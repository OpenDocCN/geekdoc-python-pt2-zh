- en: Part IV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ADVANCED CONCEPTS
  prefs: []
  type: TYPE_NORMAL
- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inheritance and Mixins
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Knowing when to use inheritance is even more important than knowing how. The
    technique is extraordinarily useful for some situations but fantastically ill-suited
    for most others, making it one of the more controversial topics in object-oriented
    programming. Syntactically, inheritance is simple to implement. Logistically,
    the issues surrounding it are so intricate and nuanced that it deserves a chapter
    of its own.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Inheritance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inheritance can quickly get out of hand, primarily because it feels clever.
    You must know when *not* to use it.
  prefs: []
  type: TYPE_NORMAL
- en: While many languages use inheritance and polymorphism to allow working with
    many different types of data, Python seldom has such a need. Instead, the language
    uses duck typing, accepting an argument on the sole merit of its interface. For
    example, Python does not force you to inherit from a particular base class to
    make your object an iterator; instead, it recognizes any object with the methods
    `__iter__()` and `__next__()` as an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: Since a class is defined by its constituent data, inheritance should extend
    this definition. If two or more classes need to contain the same sort of data
    and provide the same interface, inheritance is likely justified.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the built-in `BaseException` class contains several common attributes
    describing all exceptions. Other exceptions, such as `ValueError` and `RuntimeError`,
    contain the same data, justifying their inheriting from `BaseException`. The base
    class defines a common interface for interacting with this data. Derived classes
    extend the interface and attributes to serve their needs as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re tempted to use inheritance purely to require the implementation of
    a particular interface in a derived class, or to allow extending a fairly complex
    interface, consider using *abstract base classes* instead. I’ll come back to that
    topic in Chapter 14.
  prefs: []
  type: TYPE_NORMAL
- en: Crimes of Inheritance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Remember: ***decisions about object-oriented design must be based on the data
    being encapsulated***. Bearing that rule in mind will help you steer clear of
    many atrocities commonly seen in object-oriented code. There are many misuses
    of inheritance, and I’ll cover a few of the most egregious here.'
  prefs: []
  type: TYPE_NORMAL
- en: One major inheritance anti-pattern is the *god class*, which lacks a single
    clear responsibility and instead stores or provides access to a large set of shared
    resources. A god class quickly becomes bloated and unmaintainable. It’s better
    to use class attributes to store anything that needs to be shared mutably between
    objects. Even a global variable is less of an anti-pattern than a god class.
  prefs: []
  type: TYPE_NORMAL
- en: Another inheritance anti-pattern is the *stub class*, which is a class that
    contains little to no data. Stub classes usually show up because the developer’s
    motivations for inheritance were based on minimizing repeated code, rather than
    a consideration of the encapsulated data. This creates a plethora of fairly useless
    objects with unclear purposes. Better ways exist to prevent repeated code, such
    as using ordinary functions from a module, instead of writing methods or employing
    composition. Methods, and thus common code, can be shared between classes with
    techniques like mixins and abstract base classes (Chapter 14).
  prefs: []
  type: TYPE_NORMAL
- en: Mixins, which I’ll introduce later in this chapter, are really a form of composition
    that just happens to leverage Python’s inheritance mechanisms. They aren’t an
    exception to the rules.
  prefs: []
  type: TYPE_NORMAL
- en: The third reason for atrocious inheritance schemes is the devilish old desire
    to produce “clever” code. Inheritance is a nifty-looking hammer, but don’t use
    it to pound in screws. You get better architecture and fewer bugs with composition.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Inheritance in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before I dive into the deeper mechanics of inheritance, I want to properly cover
    the basics.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll use the popular personal task management technique of bullet journaling
    as an example. In real life, a *bullet journal* is a physical book made up of
    one or more *collections*, which are titled sets of items—bulleted tasks, events,
    and notes. There are different kinds of collections for different purposes. For
    an example of inheritance in Python, I’ll write some classes that emulate a bullet
    journal.
  prefs: []
  type: TYPE_NORMAL
- en: First, I will write a pared-down `Collection` class, which I’ll inherit from
    shortly. Remember, a class should be crafted around its data, not its behavior.
    To keep the example small, I’ll mainly write stub functions, with little to no
    actual behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-1: *bullet_journal.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'By itself, a `Collection` in a bullet journal only needs three things: a title
    (`self.title`), its page numbers (`self.page_start` and `self.page_end`), and
    its items (`self.items`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I add a `__str__()` special instance method for displaying the title when the
    collection is converted to a string. Implementing this method means I can directly
    `print()` a `Collection` object. I also offer two instance methods: `expand()`,
    for adding another page to the collection; and `add_item()`, for adding an entry
    to the collection. (I skipped writing the logic for this method for brevity.)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, I’ll write a class for the `MonthlyLog`, which is a specialized type of
    `Collection` for tracking events and items in the context of an entire month.
    It still has to have a title, page numbers, and a set of items. In addition, it
    needs to store events. Because it *extends* the data stored, inheritance is a
    good fit for this situation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-2: *bullet_journal.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: You will recall from Chapter 7 that, when instantiating a derived class, one
    must explicitly call the initializer of the base class. I do that here with `super().__init__()`
    ❶. I create the title from `month` and `year` ❷, and I also pass the `page_start`
    and `length` arguments directly. The base class initializer creates these instance
    attributes, which will be accessible to the `MonthlyLog` object because it inherits
    from `Collection` ❸.
  prefs: []
  type: TYPE_NORMAL
- en: I override the `__str__()` special instance method, this time appending `"(Monthly
    Log)"` to the collection title.
  prefs: []
  type: TYPE_NORMAL
- en: I also define the instance method `add_event()` specifically for `MonthlyLog`,
    for logging events on the calendar view I would store in `self.events`. I won’t
    implement this calendar behavior here because it’s pretty involved and also irrelevant
    to the example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s one more derived class, `FutureLog`, which is a collection belonging
    to one of the next six months:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-3: *bullet_journal.py:3*'
  prefs: []
  type: TYPE_NORMAL
- en: The `FutureLog` class also inherits from `Collection`, with the added attribute
    `self.months`, which is a list of months. The class also has a predefined title
    and length, which I pass to the `Collection` initializer via `super.__init__()`,
    as I did in `MonthlyLog`.
  prefs: []
  type: TYPE_NORMAL
- en: I also override the instance method `add_item()` so it now accepts `month` in
    addition to the other arguments and so would store the `bullet`, note, and `signifier`
    in the appropriate month in `FutureLog`. The `month` parameter is optional, so
    I don’t violate the Liskov Substitution Principle. As before, I’ve skipped the
    implementation here to keep things moving along.
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to briefly mention that, much like how I can check if an object is an
    instance of a class with `isinstance()`, I can check if a *class* is derived from
    another class with `issubclass()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a very basic usage of my classes, in which I create a `FutureLog`, a
    `MonthlyLog`, and a `Collection`, adding some items to each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-4: *bullet_journal.py:4*'
  prefs: []
  type: TYPE_NORMAL
- en: Because I wrote so many stub functions, this won’t do much, but the fact that
    it doesn’t fail at least proves what’s working. (Famous last words, I know.) A
    derived class has the same attributes and methods as its base class, but it can
    override any of those and add more.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Inheritance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a class inherits from multiple base classes, it gains all of the attributes
    and methods of those base classes. This is known as *multiple inheritance*.
  prefs: []
  type: TYPE_NORMAL
- en: In languages that permit it, multiple inheritance can be a powerful tool, but
    one that presents many thorny challenges. I’ll therefore discuss how Python gets
    around many of these obstacles and what issues remain. As with normal inheritance,
    your decision about whether to use multiple inheritance should be based primarily
    on the data, rather than just on the desired functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Method Resolution Order
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One potential issue with multiple inheritance occurs if more than one base class
    has a method of the same name. Suppose you have a class `Calzone`, which inherits
    from both `Pizza` and `Sandwich`, and both base classes provide a method `__str__()`.
    If I call `__str__()` on an instance of `Calzone`, Python must *resolve* which
    method to call, meaning it must decide which class’s `__str__()` method to execute.
    The rule the language uses to perform this resolution is called the *method resolution
    order*.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll explain how Python determines the method resolution order.
    To check the method resolution order on a particular class, consult that class’s
    `__mro__` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the classes for my `Calzone` multiple-inheritance scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-5: *calzone.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: The classes `Pizza` and `Sandwich` are both derived from the `Food` class. A
    `Calzone` is considered both a type of `Pizza` and a type of `Sandwich`, so it
    inherits from both of those classes. The question is, what will be printed when
    this code runs?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-6: *calzone.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: Which version of the `__str__()` special instance method is `Calzone` inheriting?
    Because both `Pizza` and `Sandwich` derive from `Food` and both override the special
    instance method `__str__()`, Python must resolve the ambiguity about which class’s
    implementation of `__str__()` to use when `Calzone.__str__()` is called.
  prefs: []
  type: TYPE_NORMAL
- en: The situation above is known in software development as the *diamond inheritance
    problem*, or sometimes more ominously as the “Deadly Diamond of Death” (cue scary
    thunder). It’s one of the nastier method resolution problems that arise with multiple
    inheritance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python solves the diamond inheritance problem with a straightforward approach:
    a technique known as the *C3 Method Resolution Order (C3 MRO)*, or more formally,
    the *C3 superclass linearization*. (Try saying that 10 times fast.) Python does
    this automatically, behind the scenes. You need only know how it functions so
    you can use it to your advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, the C3 MRO involves generating a *superclass linearization*—a list
    of base classes each class inherits from—following a simple set of rules. The
    superclass linearization is the order in which classes are searched for the method
    being called.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this, here’s the linearization list of the first class, `Food`.
    In the (non-Python) notation here, `L[Food]` is the linearization of class `Food`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Like all classes in Python, `Food` inherits from the ubiquitous `object`, so
    the linearization is `Food, object`. In this linearization, `Food` is considered
    the *head*, meaning it’s the first item in the linearization list and thus the
    next class to be considered. The rest of the list is considered the *tail*. In
    this case, the tail is just one item: `object`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Pizza` class inherits from `Food`. To do this, Python must look at the
    linearization of each class `Pizza` directly inherits from and consider each item
    in the linearization in turn.
  prefs: []
  type: TYPE_NORMAL
- en: In the following non-Python notation, I’m using `merge()` to indicate the linearizations
    from base classes I have yet to consider. By the time I’m done, `merge()` should
    be empty. Each linearization is wrapped in curly braces (`{ }`). The class being
    considered in each step is in `italics`, and a class that has just been added
    to the linearization on that step is in `bold`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this notation, I can illustrate the linearization process for `Pizza`.
    The C3 MRO here will traverse the heads from left to right. In creating the superclass
    linearization for `Pizza`, the C3 MRO doesn’t care what methods each class has;
    it only cares where a class appears in the linearizations it is merging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Python first considers whether to add the leftmost head—the current class,
    `Pizza`—to the linearization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If a head class is not in any tail for any linearization being merged, it is
    added to the new linearization and removed from any other positions. Since `Pizza`
    doesn’t appear in any tail, it is added to the linearization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, Python examines the new leftmost head, which is the head of the linearization
    that needs to be merged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Food` class doesn’t appear in any tail, so it is added to the `Pizza`
    linearization and removed from the linearization being merged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'That means `object` is the new head of the linearization being merged. Python
    now considers this new head. Since `object` doesn’t appear in any tail—obviously,
    since the only linearization being merged no longer has a tail—it can be added
    to the new linearization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There is nothing left to merge. The linearization for `Pizza` is `Pizza`, `Food`,
    and `object`. The `Sandwich` class evaluates to nearly the same linearization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This gets a little more complicated with multiple inheritance, so let’s consider
    the `Calzone` class. I will need to merge the linearizations of `Pizza` and `Sandwich`,
    in that particular order, matching the order of the classes in `Calzone`’s inheritance
    list ([Listing 13-5](#listing13-5)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The C3 MRO first inspects the leftmost head, `Calzone`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `Calzone` doesn’t appear in any tail, it’s added to the new linearization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The new leftmost head to be considered is `Pizza`. It, too, doesn’t appear in
    any tail, so it is also added to the new linearization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When `Pizza` is removed from the linearizations being merged, `Food` becomes
    the new head of that first linearization. As it’s the new leftmost head, it’s
    considered next. However, `Food` also appears in the tail of the linearization
    headed by `Sandwich`, so it cannot be added to the linearization yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next head is considered instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Sandwich` doesn’t appear in any tails, so it can be added to the new linearization
    and removed from the linearizations being merged. The C3 MRO goes back to considering
    the leftmost head, which is `Food`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `Food` class appears as the head of both linearizations being merged, but
    not in any tails, so it can be added. It is also removed from all linearizations
    to be merged.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This leaves only `object` as the head in each linearization to merge. Since
    it appears only as a head, not as a tail, it can be added.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: There’s the finished superclass linearization for `Calzone`.
  prefs: []
  type: TYPE_NORMAL
- en: To think of that another way, the linearization process will always look for
    the next-nearest ancestor of the class being considered, as long as that ancestor
    is not being inherited by any ancestor not yet considered. For `Calzone`, the
    next-nearest ancestor is `Pizza`, which isn’t inherited by either `Sandwich` or
    `Food`. The `Sandwich` class is next, and only once both `Pizza` and `Sandwich`
    are accounted for can their common ancestor, `Food`, be added.
  prefs: []
  type: TYPE_NORMAL
- en: Bearing this in mind and revisiting that question of ambiguity from [Listing
    13-6](#listing13-6), repeated below, which version of `__str__()` gets called
    here?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to determine which base class is providing the `__str__()` method
    being called, the superclass linearization for `Calzone` is consulted. According
    to the method resolution order, Python would first check `Calzone` for a `__str__()`
    method. Failing to find that, it checks `Pizza` next and finds the desired method.
    Sure enough, running this code, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Ensuring Consistent Method Resolution Order
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When using multiple inheritance, the order in which you specify base classes
    matters. Here, I’ll create a `PizzaSandwich` class, representing a sandwich where
    you use slices of pizza instead of bread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-7: *calzone.py:3a*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PizzaSandwich` class derives from `(Sandwich, Pizza)`. Recall that `Calzone`
    inherits from `(Pizza, Sandwich)`. Both `PizzaSandwich` and `Calzone` have the
    same base classes, but they inherit from them in different orders. That means
    that `PizzaSandwich` has a slightly different linearization than `Calzone`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If I went overboard and put a calzone between two slices of pizza, I’d get a
    `CalzonePizzaSandwich`, which inherits from `(Calzone, PizzaSandwich)`.
  prefs: []
  type: TYPE_NORMAL
- en: Since `Calzone` and `PizzaSandwich` inherit from the same base classes in different
    orders, what will happen when I try to resolve the `__str__()` method on `CalzonePizzaSandwich`?
    Here’s how the C3 MRO tries to solve that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The leftmost head, `CalzonePizzaSandwich`, is considered and added first, since
    it doesn’t appear in any tails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The new leftmost head, `Calzone`, is checked next and added.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Next, the C3 MRO looks at `Pizza`, the new leftmost head. It skips this class
    for now, since `Pizza` appears in the tail of one of the lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it considers the next head, `PizzaSandwich`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'That class can be added, since it’s only a head. After adding `PizzaSandwich`
    to the new linearization and removing it from the linearizations to merge, the
    C3 MRO reconsiders the leftmost head:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`Pizza` is still not eligible to be added, since it’s still in the tail of
    the second linearization. The head of the next list, `Sandwich`, is considered
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'No dice! `Sandwich` appears in the tail of the first linearization being merged.
    Python cannot determine the method resolution order here because both heads in
    the last step, `Pizza` and `Sandwich`, are also in the tail of the other linearization.
    The `CalzonePizzaSandwich` class would cause Python to raise the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The fix for this particular situation is trivial: I’d need to switch the order
    of base classes on `PizzaSandwich`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-8: *calzone.py:3b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the linearization of `CalzonePizzaSandwich` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: When using multiple inheritance, pay close attention to the order in which you
    specify base classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be aware that the fixes aren’t always trivial, like in my example here. As
    you can imagine, the problem is made worse when inheriting with three or more
    classes. I won’t go into these here, but know that understanding the C3 MRO is
    a major part of the solution. Raymond Hettinger outlines some other techniques
    and considerations in his article “Python’s super() considered super!” which you
    can read here: [https://rhettinger.wordpress.com/2011/05/26/super-considered-super/](https://rhettinger.wordpress.com/2011/05/26/super-considered-super/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about the C3 MRO, I recommend the article that accompanied the
    addition of this MRO to Python 2.3: [https://www.python.org/download/releases/2.3/mro/](https://www.python.org/download/releases/2.3/mro/).'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, aside from Python, only a handful of relatively obscure languages
    use the C3 MRO by default; Perl 5 and onward offers it optionally. It’s one of
    the relatively unique advantages of Python.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit Resolution Order
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While you must always work out the correct inheritance order for your code
    to run, you can also explicitly call methods on the base classes you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-9: *calzone.py:3c*'
  prefs: []
  type: TYPE_NORMAL
- en: This will ensure that `CalzonePizzaSandwich.__str__()` calls `Calzone.__str__()`,
    regardless of the method resolution order. You will notice that I have to pass
    `self` explicitly, since I’m calling the `__str__()` instance method on the `Calzone`
    class and not on an instance.
  prefs: []
  type: TYPE_NORMAL
- en: Resolving Base Class in Multiple Inheritance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another challenge with multiple inheritance is ensuring the initializers for
    all the base classes are called, with the right arguments passed to each. By default,
    if a class doesn’t declare its own initializer, Python will use the method resolution
    order to find one. Otherwise, if an initializer is declared by a derived class,
    it won’t implicitly call the base class initializers; that must be done explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Your first thought might be to use `super()` for this. Indeed, that can work,
    but only if you have planned it out in advance! The `super()` function looks at
    the next class (not the current class) in the superclass linearization for the
    instance. If you’re not expecting this, it can lead to some freaky and unexpected
    behavior or errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how this should be handled, I’ll add initializers on my first
    three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-10: *make_calzone.py:1a*'
  prefs: []
  type: TYPE_NORMAL
- en: Because `Pizza` and `Sandwich` both inherit from `Food`, they need to call the
    initializer on `Food` via `super().__init__()` and pass the required argument,
    `name` ❶. All is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'But `Calzone` is trickier, since it needs to call `__init__()` on both `Pizza`
    *and* `Sandwich`. Calling `super()` only provides access to the first base class
    in the method resolution order, so this would still only call the initializer
    on `Pizza`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-11: *make_calzone.py:2a*'
  prefs: []
  type: TYPE_NORMAL
- en: The method resolution order on `Calzone` means that `super().__init__()` calls
    the initializer on `Pizza`. However, the call to `super().__init__()` in `Pizza.__init__()`
    ([Listing 13-10](#listing13-10)) will now try to call `__init__()` on the next
    class in the linearization for the `Calzone` instance. That is, `Pizza`’s initializer
    will now call `Sandwich.__init__()`. Unfortunately, it will pass the wrong arguments,
    and the code will throw a rather confusing `TypeError`, complaining about a missing
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to handle initializers with multiple inheritance might seem
    to be to call the `Pizza` and `Sandwich` initializers directly and explicitly,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-12: *make_calzone.py:2b*'
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t solve the problem because my use of `super()` in the base classes
    still doesn’t play well with the multiple inheritance. Also, if I were to change
    the base classes, or even just their names, I would also have to rewrite the `Calzone`
    initializer.
  prefs: []
  type: TYPE_NORMAL
- en: The preferred method is to still use `super()` and write the base classes of
    `Sandwich` and `Pizza` to be used *cooperatively*. This means their initializers,
    or any other instance methods meant to be used with `super()`, can work either
    alone or in the context of multiple inheritance.
  prefs: []
  type: TYPE_NORMAL
- en: For the initializers to work cooperatively, they must not make assumptions about
    what class will be called with `super()`. If I initialize `Pizza` by itself, then
    `super()` will refer to `Food`, but when `Pizza.__init__()` is accessed via `super()`
    from an instance of `Calzone`, it will refer to `Sandwich` instead. It all depends
    on the method resolution order on the instance (rather than the class).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I’ll rewrite `Pizza` and `Sandwich` so their initializers are cooperative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-13: *make_calzone.py:1b*'
  prefs: []
  type: TYPE_NORMAL
- en: Both initializers will accept keyword arguments and must also accept any other
    unknown keyword arguments in the variadic parameter, `**kwargs`. This is important,
    as it will be impossible to know in advance all the arguments that may be passed
    up via `super().__init__()`.
  prefs: []
  type: TYPE_NORMAL
- en: Each initializer explicitly accepts the arguments it needs, and then it sends
    the rest up the method resolution order via `super().__init__()`. In both cases,
    however, I provide a default value for `name` for when `Pizza` or `Sandwich` is
    instantiated directly. I pass name up to the next initializer, along with all
    the leftover arguments (if any) in `**kwargs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use these cooperative initializers, the new `Calzone` class looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '**Listing 13-14: *make_calzone.py:2**c*'
  prefs: []
  type: TYPE_NORMAL
- en: I only need one call to `super().__init__()`, which will point to `Pizza.__init__()`,
    due to the method resolution order. However, I pass all the arguments for all
    the initializers in the superclass linearization. I only use keyword arguments,
    each with a unique name, to ensure that every initializer can pick up what it
    needs, regardless of the method resolution order.
  prefs: []
  type: TYPE_NORMAL
- en: '`Pizza.__init__()` uses the `toppings` keyword argument and then passes the
    rest on. `Sandwich.__init__()` is next in the method resolution order, and it
    picks up `bread` and `fillings` before passing `name` up to the next class, `Food`.
    More importantly, this code will still work, even if I swap the order of `Pizza`
    and `Sandwich` in the inheritance list for `Calzone`.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from just that simple example, designing cooperative base classes
    requires some careful planning.
  prefs: []
  type: TYPE_NORMAL
- en: Mixins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One particular upside of multiple inheritance is that you can work with mixins.
    A *mixin* is a special type of incomplete (and even invalid) class that contains
    functionality you might want to add to multiple other classes.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, mixins are used to share common methods for logging, database connections,
    networking, authentication, and much more. Whenever you need to reuse the same
    methods (not just functions) across multiple classes, mixins are one of the best
    ways to accomplish that.
  prefs: []
  type: TYPE_NORMAL
- en: Mixins do use inheritance, but they are the exception to the rule that inheritance
    decisions should be based on data. Mixins essentially rely on a form of *composition*
    that happens to leverage the inheritance mechanism. A mixin seldom has its own
    attributes; instead, it often relies its on expectations about the attributes
    and methods of the classes that use it.
  prefs: []
  type: TYPE_NORMAL
- en: In case that’s making your brain hurt a bit, here’s an example.
  prefs: []
  type: TYPE_NORMAL
- en: Say I’m creating an application that relies on a *live* settings file that can
    be updated at any time. I’ll write multiple classes that need to grab information
    from this settings file. (In reality, I’m only writing one such class for the
    example. You can imagine the rest.)
  prefs: []
  type: TYPE_NORMAL
- en: 'First, I create the file *livesettings.ini*, which I’ll store in the same directory
    as the module I’m about to write. Here are contents of that *.ini* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-15: *livesettings.ini*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next is my mixin, which contains only the functionality for working with this
    settings file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-16: *mixins.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: The class `SettingsFileMixin` is not a complete class by itself. It lacks an
    initializer and even refers to an instance attribute it doesn’t have, `self.settings_section`.
    This is okay, as mixins are never intended to be used by themselves. That missing
    attribute will need to be provided by any class that uses the mixin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mixin does have a couple of class attributes, `settings_path` and `config`.
    Most importantly, it has a `read_setting()` method, which reads a setting from
    the *.ini* file. This method uses the `configparser` module to read and return
    a setting specified by `key` from a particular section: `self.settings_section`,
    in the *.ini* file that the class attribute `settings_path` points to. If the
    section, the key, or even the file does not exist, the method will raise a `KeyError`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a class that prints a greeting to the user. I want this class to acquire
    the username from the *livesetting.ini* file. To do that, I’ll have this new class
    use the mixin `SettingsFileMixin` by inheriting from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-17: *mixins.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: The `Greeter` class is initialized with a string to use as the greeting. In
    the initializer, I define that `self.settings_section` instance attribute upon
    which the `SettingsFileMixin` relies. (In a production-quality mixin, you’d document
    the necessity for this attribute.)
  prefs: []
  type: TYPE_NORMAL
- en: The `__str__()` instance method uses the `self.read_setting()` method from the
    mixin, as if it had been defined as part of this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usefulness of this becomes obvious if I add another class, such as one
    that works with the `MagicNumber` value from *livesetting.ini*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-18: *mixins.py:3*'
  prefs: []
  type: TYPE_NORMAL
- en: I can have as many classes as I want read from *livesetting.ini* by having them
    inherit `SettingsFileMixin`. That mixin provides the single canonical source of
    that functionality in my project, so any improvements or bug fixes I make to the
    mixin will be picked up by all the classes that use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example usage of my `Greeter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 13-19: *mixins.py:4*'
  prefs: []
  type: TYPE_NORMAL
- en: I run the `print()` statement in a loop to demonstrate the effects of changing
    the `livesettings.ini` file live.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re trying this out along with the book, open the *.ini* file before
    starting the module and change `UserName` to yours, *but do not save the changes
    yet*. Now run the *mixins.py* module. Once it starts, save the changes to *livesettings.ini*
    and observe the change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Inheritance* isn’t quite the dirty word in Python that it sometimes is in
    other languages. It provides the mechanism for extending classes and enforcing
    interfaces, resulting in clean and well-structured code in situations that would
    otherwise lead to spaghetti.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple inheritance works well in Python, thanks to the C3 linearization method
    resolution order, which sidesteps most of the problems usually presented by the
    diamond inheritance problem. This, in turn, makes it possible to use mixins to
    add methods to classes.
  prefs: []
  type: TYPE_NORMAL
- en: With all these shiny, clever-looking tools, it is important to remember that
    inheritance in its many forms can also easily get out of hand. Before employing
    any of the tactics in this chapter, you should fully determine what problem you’re
    trying to solve. At the end of the day, your goal is to create readable, maintainable
    code. Although inheritance can detract from this when used wrong, if employed
    judiciously, these techniques can make your code significantly easier to read
    and maintain.**  **# 14
  prefs: []
  type: TYPE_NORMAL
- en: Metaclasses and ABCs
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Python developers are well familiar with the mantra, “Everything is an object.”
    When looking at the class system in Python, however, this becomes a paradox: If
    everything is an object, then what is a class? The seemingly arcane answer to
    that question unlocks another powerful tool in the Python toolbox: abstract base
    classes, which are one way of outlining expected behaviors of a type when using
    duck typing.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll be digging into metaclasses, abstract base classes, and
    how you can use them to write more maintainable classes.
  prefs: []
  type: TYPE_NORMAL
- en: Metaclasses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classes are instances of *metaclasses*, in the same way that objects are instances
    of classes. More precisely, every class is an instance of `type`, and `type` is
    a metaclass. Metaclasses allow you to override how a class is created.
  prefs: []
  type: TYPE_NORMAL
- en: To build on the analogy from Chapter 13, just as a house can be constructed
    from a blueprint, the blueprint can be made from a template. A metaclass is that
    template. One template can be used to produce many different blueprints, and many
    different houses can be built from any one of those blueprints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before I go any further, a disclaimer is appropriate: you may reasonably go
    through your entire career without even once using metaclasses directly. By themselves,
    they’re almost certainly not the solution to whatever problem you’re thinking
    about using them for. Tim Peters summarizes this warning exceptionally well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Metaclasses] are deeper magic than 99% of users should ever worry about. If
    you wonder whether you need them, you don’t (the people who actually need them
    know with certainty that they need them, and don’t need an explanation about why).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, understanding metaclasses *does* help with comprehending other Python
    features, including abstract base classes. Django, a Python web framework, also
    makes frequent use of metaclasses internally. Rather than trying to contrive a
    somewhat believable usage for metaclasses, I’ll stick to the bare minimum to demonstrate
    how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Classes with type
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You might have used the `type()` callable in the past to return the type of
    a value or object, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-1: *types.py*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `type()` callable is actually a metaclass, rather than a function, meaning
    it can be used to create classes the same way in which a class is used to create
    instances. Here’s an example of creating a class from `type()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-2: *classes_from_type.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, I create a class `Food`. It inherits from nothing and has no methods
    or attributes. I’m literally instantiating the metaclass. This is an effective
    equivalent to the code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '(In production code, I’d never define an empty base class, but doing so here
    is helpful for demonstration purposes.) Next, I’ll instantiate the `type` metaclass
    again to create another class, `Pizza`, which derives from `Food`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-3: *classes_from_type.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: I define the function `__init__()`, which will be the initializer of the upcoming
    `Pizza` class. I named the first parameter `obj`, since this isn’t *actually*
    a member of a class yet.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I create the `Pizza` class by calling `type()` and passing the name for
    the class ❶, a tuple of base classes ❷, and a dictionary of methods and class
    attributes ❸. This is where I pass the `__init__` function I wrote.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the functional equivalent of the preceding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the normal syntax for creating a class is a lot more readable
    and practical. The benefit of the `type` metaclass is that you can create classes
    somewhat dynamically during runtime, although there is seldom a practical reason
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'The familiar approach of creating a class with the `class` keyword is really
    just syntactic sugar for instantiating the `type` metaclass. Either way, the end
    result is the same, as this usage indicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-4: *classes_from_type.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: Custom Metaclasses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can create a custom metaclass to use as a blueprint for classes. In this
    way, metaclasses are really only useful for modifying the deep internal behavior
    of how the language instantiates and works with the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'A metaclass will often override the `__new__()` method, as this is the constructor
    method that governs the creation of the class. Here’s an example of this, via
    an admittedly pointless metaclass, `Gadget`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-5: *metaclass.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: The special method `__new__()` is what is called behind the scenes when you
    invoke `type()` or any other metaclass, such as you saw in [Listing 14-2](#listing14-2).
    The `__new__()` method here prints a message that the class is being created,
    and it then invokes the `__new__()` method from the `type` base metaclass. This
    method is expected by the language to accept four arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter here is `self`. The `__new__()` method is written as an
    instance method on the metaclass, because it is supposed to be a class method
    on any instance of this metaclass. If you’re feeling lost, read that a few times
    and let it sink in, remembering that a class is an instance of a metaclass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another special method often implemented by metaclasses is `__prepare__()`.
    Its purpose is to create the dictionary that stores all the methods and class
    attributes for the class being created (see Chapter 15). Here’s one for my `Gadget`
    metaclass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-6: *metaclass.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: The `@classmethod` decorator indicates that this method belongs to the metaclass
    itself, not to a class instantiated from this metaclass. (If your brain starts
    to overheat at this point, I highly recommend eating a scoop of ice cream.) The
    `__prepare__()` method also must accept two more parameters, conventionally named
    `name` and `bases`.
  prefs: []
  type: TYPE_NORMAL
- en: The `__prepare__()` special method returns the dictionary that stores all the
    attributes and methods on the class. In this case, I’m returning a dictionary
    that already has a value, so all classes created from the `Gadget` metaclass will
    have a `color` class attribute, with the value `'white'`.
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, I would just return an empty dictionary that each class can fill.
    In fact, I can omit the `__prepare__()` method in that case; the `type` metaclass
    already provides this method via inheritance, and the Python interpreter is smart
    about handling a lack of a `__prepare__` method anyway.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it for the `Gadget` metaclass!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I’ll create an ordinary class using the `Gadget` metaclass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-7: *metaclass.py:3*'
  prefs: []
  type: TYPE_NORMAL
- en: The interesting feature here is that I’ve specified the metaclass in the inheritance
    list using `metaclass=Gadget`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The added behavior from the `Gadget` metaclass is present, as you can see from
    this example usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-8: *metaclass.py:4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Thingamajig` class is instantiated, and I can use it in the same way as
    any other class, except for certain key differences: instantiating the class prints
    a message, and `Thingamajig` has a `color` class attribute with the default value
    `"white"`.'
  prefs: []
  type: TYPE_NORMAL
- en: Those are the basic principles of creating and using metaclasses. If you are
    still tracking with me, congratulate yourself, as this is a difficult topic.
  prefs: []
  type: TYPE_NORMAL
- en: You may have observed that I could have implemented those same behaviors with
    `Gadget` and `Thingamajig` via ordinary inheritance, instead of mucking about
    with a custom metaclass, and you are absolutely right! The trouble is, it’s nearly
    impossible to think of good uses for metaclasses, so we always wind up contriving
    some awful example like the above, just to demonstrate *how* they work. As Tim
    Peters said, “The people who actually need them know with certainty that they
    need them, and don’t need an explanation about why.”
  prefs: []
  type: TYPE_NORMAL
- en: In my own work, I once used a metaclass to implement `__getattr__()` (discussed
    in Chapter 15), which provides fallback behavior when a class attribute is not
    defined. A metaclass was undeniably the right solution to the problem. (Then my
    co-worker Patrick Viafore pointed out that I was also solving the wrong problem.
    Go figure.)
  prefs: []
  type: TYPE_NORMAL
- en: Metaclasses are also the best way to implement the singleton design pattern
    in Python, wherein you only ever have one instance of an object in existence.
    However, the singleton is almost never useful in Python, as you can accomplish
    the same thing with static methods.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a reason Python developers have struggled to come up with viable examples
    for metaclasses for decades. Metaclasses are something you will seldom, if ever,
    use directly, except in that rare instance when you instinctively *know* it’s
    the right tool for the job.
  prefs: []
  type: TYPE_NORMAL
- en: Type Expectations with Duck Typing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metaclasses do enable the powerful concept of abstract base classes in Python.
    These allow you to codify expectations for a type in terms of their behaviors.
    Before I can explain abstract base classes, however, I need to unpack some important
    principles of duck typing in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Chapter 13, I asserted that in Python, you do not need to use inheritance
    in order to write a function that accepts objects of different types as arguments.
    Python employs duck typing, meaning that instead of caring about an object’s *type*,
    it only expects an object to provide the needed interface. When working with duck
    typing, there are three ways of ensuring a particular argument has the necessary
    functionality: catching exceptions, testing for attributes, or checking for a
    particular interface.'
  prefs: []
  type: TYPE_NORMAL
- en: 'EAFP: Catching Exceptions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Back in Chapter 8, I introduced the philosophy of *Easier to Ask Forgiveness
    than Permission (EAFP)*, which advocates raising an exception if an argument is
    missing functionality. This is ideal in situations where you’re providing the
    arguments yourself in your code, since unhandled exceptions will alert you to
    the places where the code needs to be improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is unwise to use this technique anywhere an unhandled exception
    might evade detection until a user attempts to use the program in an unexpected
    or untested way. This consideration is known as *fail-fast*: a program in an erroneous
    state should fail as early in the call stack as possible, to reduce the chances
    of bugs evading detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LBYL: Checking for Attributes'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more complex or brittle code, it may be better to adhere to the philosophy
    of *Look Before You Leap (LBYL)*, in which you check for the functionality you
    need on an argument or value before proceeding. There are two ways to do this.
    For situations where you rely on one or two methods on an object, you can use
    the `hasattr()` function to check for the methods, or even attributes, that you
    need.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, using `hasattr()` isn’t necessarily as simple or clear as one might
    hope. Here’s an example of a function that multiplies every third element in the
    collection passed to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-9: *product_of_thirds.py:1a*'
  prefs: []
  type: TYPE_NORMAL
- en: Right at the top of my `product_of_thirds` function, I use the `hasattr()` function
    to check that the argument sequence has an attribute named `__iter__` ❶. This
    works because all methods are technically attributes. If the argument doesn’t
    have an attribute by that name, I raise an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this technique can be subtly wrong. For one thing, not everything
    that is iterable is necessarily subscriptable, and the code in [Listing 14-9](#listing14-9)
    is erroneously assuming that it is. Meanwhile, consider what would happen if I
    passed an instance of the following class to `product_of_thirds()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: While this example is contrived, there is nothing stopping a developer from
    hackishly repurposing a name that is assumed to mean something else—yes, nasty
    things like this show up in real code. The result would cause the `hasattr()`
    test to pass anyway. The `hasattr()` function only checks that the object has
    *some* attribute with that name; it doesn’t concern itself with the attribute’s
    type or interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, one must be careful about making assumptions regarding what any single
    function will actually do. Building on my example in [Listing 14-9](#listing14-9),
    I might add the following logic to try to check that my sequence contained values
    that could be multiplied by one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-10: *product_of_thirds.py:1b*'
  prefs: []
  type: TYPE_NORMAL
- en: By checking for `__getitem__()` along with `__iter__()`, I know the object has
    to be subscriptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still another problem is that a string object does implement `__mul__()` but
    doesn’t use it as expected. Trying to run this version of the code raises a `TypeError`
    when passing a string to `product_of_thirds()`, but with the wrong message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Hmm, that’s not the message I specified. The problem is that the test failed
    to identify that the function logic—namely, multiplication between collection
    items—makes no sense on a string.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, sometimes inheritance itself can create situations where a `hasattr()`
    test result can be subtly wrong. For example, if you wanted to ensure that an
    object implemented the special method `__ge__` (for the `>=` operator), you might
    expect this to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately for this test, `__ge__` is implemented on the base class `object`,
    from which all classes inherit, so this test will *virtually never fail*, even
    when you expect it should.
  prefs: []
  type: TYPE_NORMAL
- en: All this is to say that while `hasattr()` is appropriate for extremely simple
    scenarios, as soon as your expectations about an argument’s type get complicated,
    you need a better way to look before leaping with duck typing.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An *abstract base class (ABC)* allows you to specify particular interfaces
    that must be implemented by any class that inherits from the ABC. If the derived
    class does not provide the expected interface, the class instantiation will fail.
    This provides a more robust means of checking whether an object has particular
    traits, such as being iterable or subscriptable. In one sense, you can consider
    an ABC to be a sort of interface contract: the class agrees to implement the methods
    specified by the ABC.'
  prefs: []
  type: TYPE_NORMAL
- en: ABCs cannot be directly instantiated; they can only be inherited by another
    class. Typically, an ABC only defines what methods are expected, and it leaves
    the actual implementation of those methods to the derived class. Under some circumstances,
    an ABC may provide the implementations of some methods.
  prefs: []
  type: TYPE_NORMAL
- en: You can use ABCs to check that your object actually implements an interface.
    This technique avoids the subtly wrong situation of a method being defined on
    some distant base class. If an ABC mandates that `__str__()` be implemented, any
    class that inherits from that ABC will be expected to implement `__str__()` itself,
    or else I won’t be able to instantiate the class; it will not matter that `object.__str__()`
    is valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'A word of caution: Python’s concept of abstract base classes should not be
    compared to virtual and abstract inheritance in C++, Java, or other object-oriented
    languages. Despite some similarities, they work in fundamentally different ways.
    Treat them as separate concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: Built-in ABCs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python provides an ABC for iterators and a few other common interfaces, but
    it doesn’t *require* you to inherit from a particular base class to make your
    object an iterator. The distinction between an ABC and ordinary inheritance is
    that an ABC seldom provides the actual functionality—instead, inheriting from
    the ABC means the class is required to implement the expected methods.
  prefs: []
  type: TYPE_NORMAL
- en: The `collections.abc` and `numbers` modules contain nearly all of the built-in
    abstract classes, with a few others floating around `contextlib` (for `with` statements),
    `selectors`, and `asyncio`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how ABCs fit into an LBYL strategy, I’ll rewrite the example
    I started in [Listing 14-10](#listing14-10). I will use two ABCs to ensure the
    argument `sequence` has the interface my `product_of_thirds()` function expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-11: *product_of_thirds.py:1c*'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the `product_of_thirds()` function expects the argument
    `sequence` to be a sequence, and thus an iterable—or else it wouldn’t work with
    the `for` loop—and its elements to support multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: I check for an expected interface using `isinstance()` to find out if the given
    object is an instance of a class or an instance of a subclass thereof. I ensure
    that `sequence` itself is derived from `collections.abc.Sequence`, meaning it
    implements the `__iter__()` instance method.
  prefs: []
  type: TYPE_NORMAL
- en: I also check the first element of the sequence to ensure it is derived from
    `numeric.Complex`, which implies (among other things) that it supports basic numeric
    operations, including multiplication. Although a string implements the special
    method `__mul__()`, it does *not* derive from `numeric.Complex`. It couldn’t reasonably
    do so, since it doesn’t support the rest of the expected mathematical operators
    and methods. Thus, it fails the test here, as it should.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving from ABCs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ABCs are useful for identifying which classes implement a particular interface,
    so it’s beneficial to consider which ABCs your own classes should inherit from,
    especially when you are writing a library for other Python developers to use.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this, I’ll rewrite my example from the end of Chapter 9 with
    custom iterable and iterator classes to use ABCs, thereby allowing the interface
    checking with `isinstance()`.
  prefs: []
  type: TYPE_NORMAL
- en: First, I need to import a few ABCs from the `collections.abc` module. I’ll explain
    why I’m importing these ABCs shortly, when I actually use them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-12: *cafe_queue_**abc**.py:1a*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, I’ll modify my `CafeQueue` class from the Chapter 9 example to use three
    abstract base classes that promise important components of the class’s functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 14-13: *cafe_queue_**abc**.py:2a*'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve made no changes to the implementation of this class, but I am now inheriting
    from three different ABCs, all of which come from the ``collections.abc module.
    I selected these particular ABCs based on the methods I’m implementing on the
    class. The `CafeQueue` class implements `__iter__()` to work with iteration, so
    I inherit from the ABC `Iterable`. The `Container` ABC requires `__contains__()`,
    which allows `CafeQueue` to work with the `in` operator. The `Sized` ABC requires
    `__len__()`, which means `CafeQueue` objects work with `len()`. The functionality
    is the same as it was in Chapter 9, but now, there is a reliable way of testing
    that this class supports iteration, `in`, and `len()`.``
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65] from collections.abc import **Collection**, Iterator [PRE66] class
    CafeQueue(**Collection**):      def __init__(self):         self._queue = []         self._orders
    = {}         self._togo = {}      def __iter__(self):         return CafeQueueIterator(self)      def
    __len__(self):         return len(self._queue)      def __contains__(self, customer):         return
    (customer in self._queue)      def add_customer(self, customer, *orders, to_go=True):         self._queue.append(customer)         self._orders[customer]
    = tuple(orders)         self._togo[customer] = to_go [PRE67] class CafeQueueIterator(**Iterator**):      def
    __init__(self, iterable):  self._iterable = iterable         self._position =
    0      def __next__(self):         if self._position >= len(self._iterable):             raise
    StopIteration          customer = self._iterable._queue[self._position]         orders
    = self._iterable._orders[customer]         togo = self._iterable._togo[customer]          self._position
    += 1          return (customer, orders, togo)      def __iter__(self):         return
    self [PRE68] def serve_customers(queue):   ❶ if not isinstance(queue, Collection):         raise
    TypeError("serve_next() requires a collection.")      if not len(queue):         print("Queue
    is empty.")         return      def brew(order):         print(f"(Making {order}...)")      for
    customer, orders, to_go in queue:         for order in orders: brew(order)         if
    to_go:             print(f"Order for {customer}!")         else:             print(f"(Takes
    order to {customer})")   queue = CafeQueue() queue.add_customer(''Raquel'', ''double
    macchiato'', to_go=False) queue.add_customer(''Naomi'', ''large mocha, skim'')
    queue.add_customer(''Anmol'', ''mango lassi'')  serve_customers(queue) [PRE69]
    (Making double macchiato...) (Takes order to Raquel) (Making large mocha, skim...)
    Order for Naomi! (Making mango lassi...) Order for Anmol! [PRE70] from collections.abc
    import Collection, Iterator **from abc import abstractmethod**   **class CustomerQueue(Collection):**   **@abstractmethod**  **def
    add_customer(self, customer): pass**   **@property**  **@abstractmethod**  **def
    first(self): pass** [PRE71] class CafeQueue(**CustomerQueue**):      def __init__(self):         self._queue
    = []         self._orders = {}         self._togo = {}      def __iter__(self):         return
    CafeQueueIterator(self)      def __len__(self):         return len(self._queue)      def
    __contains__(self, customer):         return (customer in self._queue)      def
    add_customer(self, customer, *orders, to_go=True):         self._queue.append(customer)         self._orders[customer]
    = tuple(orders)         self._togo[customer] = to_go   **@property**  **def first(self):**  **return
    self._queue[0]** [PRE72] TypeError: Can''t instantiate abstract class CafeQueue
    with abstract method first [PRE73] def serve_customers(queue):     if not isinstance(queue,
    **CustomerQueue**):         raise TypeError(**"serve_next() requires a customer
    queue."**)      if not len(queue):         print("Queue is empty.")         return      def
    brew(order):         print(f"(Making {order}...)")      for customer, orders,
    to_go in queue:         for order in orders: brew(order)         if to_go:             print(f"Order
    for {customer}!")         else:             print(f"(Takes order to {customer})")   queue
    = CafeQueue() queue.add_customer(''Raquel'', ''double macchiato'', to_go=False)
    queue.add_customer(''Naomi'', ''large mocha, skim'') queue.add_customer(''Anmol'',
    ''mango lassi'')  **print(f"The first person in line is {queue.first}.")** serve_customers(queue)
    [PRE74] The first person in line is Raquel. (Making double macchiato...) (Takes
    order to Raquel) (Making large mocha, skim...) Order for Naomi! (Making mango
    lassi...) Order for Anmol! [PRE75] from abc import ABC, abstractmethod   class
    Palindromable(ABC):      @abstractmethod     def __reversed__(self): pass      @abstractmethod     def
    __iter__(self): pass      @abstractmethod     def __str__(self): pass [PRE76]
    class LetterPalindrome(Palindromable):      def __init__(self, string):         self._raw
    = string         self._stripped = ''''.join(filter(str.isalpha, string.lower()))      def
    __str__(self):         return self._raw      def __iter__(self):         return
    self._stripped.__iter__()      def __reversed__(self):         return reversed(self._stripped)
    [PRE77] def check_palindrome(sequence):      if not isinstance(sequence, Palindromable):         raise
    TypeError("Cannot check for palindrome on that type.")      for c, r in zip(sequence,
    reversed(sequence)):         if c != r:  print(f"NON-PALINDROME: {sequence}")             return
    False     print(f"PALINDROME: {sequence}")     return True [PRE78] canal = LetterPalindrome("A
    man, a plan, a canal - Panama!") print(check_palindrome(canal))   # prints ''True''  bolton
    = LetterPalindrome("Bolton") print(check_palindrome(bolton))  # prints ′False''
    [PRE79] PALINDROME: A man, a plan, a canal - Panama! True NON-PALINDROME: Bolton
    False [PRE80] print(check_palindrome([1, 2, 3, 2, 1]))  # raises TypeError [PRE81]
    **Palindromable.register(list)** print(check_palindrome([1, 2, 3, 2, 1]))  # prints
    ''True'' [PRE82] **from collections.abc import Sequence  # This should be at the
    top of the file**  # `--snip--`  Palindromable.register(**Sequence**) print(check_palindrome([1,
    2, 3, 2, 1]))  # prints ''True'' [PRE83] from abc import ABC, abstractmethod from
    collections.abc import Sequence  class Palindromable(ABC):      @abstractmethod  def
    __reversed__(self): pass      @abstractmethod     def __iter__(self): pass      @abstractmethod     def
    __str__(self): pass      **@classmethod**  **def __subclasshook__(cls, C):**  **if
    issubclass(C, Sequence):**  **return True**  **return NotImplemented** [PRE84]
    print(check_palindrome([1, 2, 3, 2, 1]))               # prints ′True′ print(check_palindrome((1,
    2, 3, 2, 1)))               # prints ′True′  print(check_palindrome(''racecar''))                     #
    prints ''True'' print(check_palindrome(''race car''))                    # prints
    ''False'' print(check_palindrome(LetterPalindrome(''race car'')))  # prints ''True''  print(check_palindrome({1,
    2, 3, 2, 1}))               # raises TypeError [PRE85]`  [PRE86] if __name__ ==
    "__main__":     main() [PRE87] from pathlib import Path  path = Path(__file__)
    / Path("../resources/about.txt") with path.open() as file:     about = file.read()
    [PRE88] class Quadruped:     leg_count = 4      def __init__(self, species):         self.species
    = species   class Llama(Quadruped):     """A quadruped that lives in large rivers."""     dangerous
    = True      def __init__(self):         self.swimming = False         super().__init__("llama")      def
    warn(self):         if self.swimming:             print("Cuidado, llamas!")      @classmethod     def
    feed(cls):         print("Eats honey with beak.") [PRE89] llama = Llama()  from
    pprint import pprint  print("Instance __dict__:") pprint(llama.__dict__)  print("\nLlama
    class __dict__:") pprint(Llama.__dict__)  print("\nQuadruped class __dict__")
    pprint(Quadruped.__dict__) [PRE90] Instance __dict__: { ❶ ''species'': ''llama'',
    ''swimming'': False}  Llama class __dict__: mappingproxy({''__doc__'': ''A quadruped
    that lives in large rivers.'',               ''__init__'': <function Llama.__init__
    at 0x7f191b6170d0>,               ''__module__'': ''__main__'',               ''dangerous'':
    True,               ''feed'': <classmethod object at 0x7f191b619d60>,               2
    ''warn'': <function Llama.warn at 0x7f191b617160>})  Quadruped class __dict__
    mappingproxy({''__dict__'': <attribute ''__dict__'' of ''Quadruped'' objects>,               ''__doc__'':
    None,               ''__init__'': <function Quadruped.__init__ at 0x7f191b617040>,               ''__module__'':
    ''__main__'',               ''__weakref__'': <attribute ''__weakref__'' of ''Quadruped''
    objects>,                ❸ ''leg_count'': 4}) [PRE91] llama = Llama()  from pprint
    import pprint  print("Instance __dict__:") pprint(**vars(llama)**)  print("\nLlama
    class __dict__:") pprint(**vars(Llama)**)  print("\nQuadruped class __dict__")
    pprint(**vars(Quadruped)**) [PRE92] print(llama.swimming)   # prints ''False''
    print(Llama.leg_count)  # prints ''4'' [PRE93] print(**getattr(llama, ''swimming'')**)   #
    prints ''False'' print(**getattr(Llama, ''leg_count'')**)  # prints ''4'' [PRE94]
    print(**object.__getattribute__(llama, ''swimming'')**)  # prints ''False'' print(**type.__getattribute__(Llama,
    ''leg_count'')**)   # prints ''4'' [PRE95] llama = Llama()  **try:**     print(object.__getattribute__(llama,
    ''swimming'')) **except AttributeError as e:**  **try:**  **__getattr__ = object.__getattribute__(llama,
    ''__getattr__'')**  **except AttributeError:**  **raise e**  **else:**  **print(__getattr__(llama,
    ''swimming''))**  **try:**     print(type.__getattribute__(Llama, ''leg_count''))
    **except AttributeError as e:**  **try:**  **__getattr__ = type.__getattribute__(Llama,
    ''__getattr__'')**  **except AttributeError:**  **raise e**  **print(__getattr__(Llama,
    ''leg_count''))** [PRE96] # Either of these works! print(llama.swimming)              #
    prints ''False'' print(getattr(Llama, ''leg_count'')  # prints ''4'' [PRE97] if
    hasattr(llama, ''larger_than_frogs''):     print("¡Las llamas son más grandes
    que las ranas!") [PRE98] try:     getattr(llama, ''larger_than_frogs'') except
    AttributeError:     pass else:     print("¡Las llamas son más grandes que las
    ranas!") [PRE99] setattr(llama, ''larger_than_frogs'', True) print(llama.larger_than_frogs)  #
    prints ''True''  setattr(Llama, ''leg_count'', 3) print(Llama.leg_count)          #
    prints ''3'' [PRE100] **object.__setattr__(**llama, ''larger_than_frogs'', True**)**
    print(llama.larger_than_frogs)    # prints ''True''  **type.__setattr__(**Llama,
    ''leg_count'', 3**)** print(Llama.leg_count)            # prints ''3'' [PRE101]
    setattr(llama, ''dangerous'', False)  # uh oh, shadowing! print(llama.dangerous)              #
    prints ''False'', looks OK? print(Llama.dangerous)              # prints ''True'',
    still dangerous!! [PRE102] **llama.dangerous = False  # same problem** print(llama.dangerous)   #
    prints ''False'', looks OK? print(Llama.dangerous)   # prints ''True'', still
    dangerous!! [PRE103] **Llama.dangerous = False  # this is better** print(llama.dangerous)   #
    prints ''False'', looks OK? print(Llama.dangerous)   # prints ''False'', we are
    safe now [PRE104] print(llama.larger_than_frogs)  # prints ''True'' del llama.larger_than_frogs
    print(llama.larger_than_frogs)  # raises AttributeError [PRE105] print(llama.larger_than_frogs)  #
    prints ''True'' delattr(llama, ''larger_than_frogs'') print(llama.larger_than_frogs)  #
    raises AttributeError [PRE106] def multiplier(n):     factor = 0     print(n *
    factor)   ❶ multiplier.factor = 3 ❷ multiplier(2)             # prints 0 print(multiplier.factor)  #
    prints 3 [PRE107] def multiplier(n):     factor = 0     print(n * factor)   **print(multiplier.__dict__)  #
    prints {}** multiplier.factor = 3 **print(multiplier.__dict__)  # prints {''factor'':
    3}** [PRE108] def multiplier(n):     print(n * **multiplier.factor**)   print(multiplier.__dict__)  #
    prints {} multiplier.factor = 3 print(multiplier.__dict__)  # prints {''factor'':
    3} **multiplier(2)               # prints 6** [PRE109] def multiplier(n):  **if
    not hasattr(multiplier, ''factor''):**  **multiplier.factor = 0**     print(n
    * multiplier.factor)   **multiplier(2)               # prints 0** print(multiplier.__dict__)  #
    prints {**''factor'': 0**} ❶ multiplier.factor = 3 print(multiplier.__dict__)  #
    prints {''factor'': 3} multiplier(2)               # prints 6 [PRE110]`In other
    words, *function attributes are attributes on a mutable object*. This is a logic
    error waiting to bite you! Consider the following simplistic example, where I
    try to change a function attribute on one function and it changes elsewhere, too:    [PRE111]    Listing
    15-22: *bad_function_attribute.py*    When I assign `sketch` to `skit`, I’m binding
    `sketch` to the same mutable function object as `skit`. When I then assign a new
    value to the function attribute `sketch.actor`, it is the same as assigning it
    to the function attribute `skit.actor`; it’s an attribute on the same function
    object. If you’re familiar with the troubles with mutable objects, such as lists
    passed as arguments, that behavior may not look surprising, especially packed
    into a dozen-line example. However, imagine this being scattered into a production
    code base of thousands of lines. This could be a horrible bug to attempt to locate
    and resolve.    As to my `multiplier()` function ([Listing 15-21](#listing15-21)),
    if I really needed to be able to provide a `factor` in some manner other than
    as an argument, I’d write that function as a closure instead. That way, each callable
    would be, itself, stateless. (See Chapter 6 for more on that topic.)    If you
    do need to use function attributes, you should be careful to only modify them
    in ways that are clear, predictable, and easy to debug. One possible usage is
    to employ a decorator to provide a default value up front to a callable, and to
    never change that value at any point during the execution of the program. While
    a similar outcome can be achieved with a closure, using a decorator places the
    extension immediately before the definition of the function. This leaves the attribute
    open to inspection, something that is squarely impossible with a parameter in
    a closure.    ## Descriptors    *Descriptors* are objects with *binding behavior*,
    meaning that they control how the objects are used as attributes. You can think
    of a descriptor as a property, whose getter, setter, and deleter methods are encapsulated
    in a class with the data those methods work with.    For example, you could have
    a `Book` descriptor that contains a book’s title, author, publisher, and publication
    year. When the descriptor is used as an attribute, all this information could
    be assigned directly via a string, and the descriptor could parse the information
    out of that string.    All methods, including static and class methods, as well
    as the `super()` function (discussed in Chapter 13), are actually descriptor objects.
    Properties are descriptors behind the scenes, too. Properties are only defined
    in the context of the class using them, while descriptors can be defined outside
    of the class and reused. This is similar to the difference between lambdas, which
    are defined where they’re used, and functions, which are defined separately from
    their usage.    ### The Descriptor Protocol    An object is a descriptor if it
    implements at least one of the three special methods in the *descriptor protocol*:
    `__get__()`, `__set__()`, or `__delete__()`. If the object only implements `__get__()`,
    it is a *non-data descriptor*, which is typically used for methods behind the
    scenes. If it also implements `__set__()` and/or `__delete__()`, it’s a *data
    descriptor*, which is what properties are an example of.    This matters to the
    *lookup chain* used by `object.__getattribute__()` and `type.__getattribute__()`.
    The lookup chain determines where Python searches for an attribute, and in what
    order. Data descriptors get first priority, followed by ordinary attributes stored
    on the object’s `__dict__`, then non-data descriptors, and finally any attributes
    on the class and its base classes. This means that a data descriptor named `foo`
    will shadow, or even prevent the creation of, an attribute by the same name. Similarly,
    an attribute named `update` will shadow a method (non-data descriptor) named `update()`.    A
    *read-only data descriptor* would still have `__set__()` defined, but that method
    would only raise an `AttributeError`. This is important for the descriptor to
    be considered a data descriptor in the lookup chain.    Descriptors also have
    a `__set_name__()` method, which is called when the descriptor is bound to a name.
    I’ll demonstrate this later in this section.    ### Writing a Descriptor Class
    (the Slightly Wrong Way)    While it is possible to write a descriptor class as
    a property on a class, you’d normally write a separate descriptor class to reduce
    code repetition. This can be useful if you want to use the descriptor in multiple
    unrelated classes, or if you want several instances of the same descriptor in
    the same instance.    As an example, I’ll write a descriptor class that stores
    details about a book. I want to parse these details out of a string following
    the APA 7 citation format. Here’s the first part of that descriptor class. Be
    advised, there is a logical error in this code, which I’ll cover shortly:    [PRE112]    Listing
    15-23: *book_club.py:1a*    This class is a data descriptor (instead of a non-data
    descriptor) because it defines `__set__()`. (I’ll define `__get__()` in [Listing
    15-24](#listing15-24).) When the descriptor is an attribute of another class,
    a value can be assigned directly to that attribute, and the `__set__()` method
    is called. This method accepts exactly three arguments: `self`, the object to
    access on (`instance`), and the value being assigned to the descriptor (`value`).    Within
    `__set__()`, I use a regular expression that I precompiled and stored in the class
    attribute pattern to extract the author, title, year, and publisher from the string
    passed to the value parameter. These extracted values are stored in instance attributes.
    If value is not a string that matches the expectations of the regular expression,
    a `ValueError` is raised.    For this to be a descriptor, I must also provide
    a `__get__()` method in the `Book` descriptor class:    [PRE113]    Listing 15-24:
    *book_club.py:2a*    When the descriptor is accessed as an attribute, this `__get__()`
    method is called, returning a new string containing the book’s title and author.
    If the expected attributes haven’t been defined, I return a string `"nothing right
    now"` instead of re-raising the `AttributeError`.    The `__get__()` method must
    accept the arguments `self` and `instance`, just like `__set__()` does, as well
    as the optional argument `owner`, which specifies which class the descriptor belongs
    to. When `owner` is set to the default value of `None`, the owning class is considered
    to be the same as `type(instance)`.    You’ll notice that the `Book` class has
    no `__init__()` method. Although a descriptor class *may* have an initializer
    if desired, you should not use it to initialize instance attributes as you would
    with an ordinary class. This is because only one instance of the descriptor is
    shared between all classes that use it, so all instance attributes will be shared,
    too. In fact, this unexpected behavior has already set me up for a problem in
    the example I’m crafting. Stay tuned.    ### Using a Descriptor    A descriptor
    only exhibits its binding behavior when used as an attribute in another class.
    To demonstrate this, I’ll define a `BookClub` class, which will use the `Book`
    descriptor class to keep track of what book the club is currently reading:    [PRE114]    Listing
    15-25: *book_club.py:3a*    I put the `Book` descriptor to work by binding an
    instance of `Book` to the class attribute `reading`. I also defined a `new_member()`
    method for adding new members to the book club and welcoming them with information
    about the book the club is currently reading.    There’s one important detail
    here: *the descriptor must be a class attribute!* Otherwise, all of the descriptor
    behavior will be ignored, and assignment will merely rebind the attribute to the
    value being assigned. This isn’t too surprising if you think of where else descriptors
    show up: all methods and properties are declared at class scope, rather than as
    names on `self` (instance attributes).    Given that the descriptor is a class
    attribute with attributes of its own, a problem emerges when using the `BookClub`
    class. I’ll demonstrate by creating two new book clubs: `mystery_lovers` and `lattes_and_lit`:    [PRE115]    Listing
    15-26: *book_club.py:4*    The first club is reading a mystery novel from some
    weird programmer guy, so I assign a string containing the appropriately formatted
    book information to the `reading` attribute of `mystery_lovers`. This assignment
    is invoking the `__set__()` method on the `Book` data descriptor object bound
    to `reading`.    Meanwhile, the folks in the `lattes_and_lit` club are reading
    a classic Agatha Christie novel, so I assign the appropriate book information
    to `lattes_and_lit.reading`.    However, since `reading` is a class attribute,
    this second assignment changes what both clubs are reading, as you can see from
    the `print()` statements. How do I fix that?    ### Writing a Descriptor Class
    the Right Way    While the `reading` descriptor must be a class attribute on `BookClub`,
    I can modify the descriptor class by storing attributes on the class instance
    it exists on:    [PRE116]    Listing 15-27: *book_club.py:1b*    Instead of having
    the `Book` descriptor store its own attributes, it should store them on the instance
    it is a member of, accessed via the `instance` argument.    Since I’m defining
    attributes on the instance, I provide a `__delete__()` method as well, so deleting
    the `Book` descriptor via the `reading` attribute on a `BookClub` instance will
    work appropriately:    [PRE117]    Listing 15-28: *book_club.py:2b*    If I hadn’t
    defined this, calling `del` on the reading attribute would have raised an exception.    With
    the descriptor’s data safely stored on the appropriate owning instances, I find
    that the usage from earlier now works as expected:    [PRE118]    Listing 15-29:
    *book_club.py:4*    Here’s a little more usage of this `BookClub` class, demonstrating
    calling `del` on the descriptor and adding a new member:    [PRE119]    Listing
    15-30: *book_club.py:5*    I clear the current book so that the Lattes and Lit
    book club isn’t reading anything right now. This calls the `reading.__del__()`
    method. Then, I add a new member, Jaime; the `new_member()` method will print
    out a welcome message announcing what the club is reading, which is currently
    nothing.    Next, I choose a book to be read by the club by assigning a string
    to the `reading` attribute; this calls `reading.__set__()`.    Finally, I add
    one more member via `new_member()`, which once again prints out a welcome message
    and the current book.    Here’s the complete output of that usage:    [PRE120]    ###
    Using Multiple Descriptors in the Same Class    There’s one remaining problem
    with my design: the descriptor looks for the attributes `title`, `author`, and
    the like on the instance, so multiple `Book` descriptors on the same `BookClub`
    instance would mutate these same values repeatedly.    Consider if a book club
    wanted to track both their current selection and the book they’re reading next:    [PRE121]    Listing
    15-31: *book_club.py:3b*    To demonstrate, I’ll assign different books to the
    `reading` and `reading_next` descriptors. Logically, those two descriptors should
    behave separately, but that’s not what happens:    [PRE122]    Listing 15-32:
    *book_club.py:6*    This code outputs the following:    [PRE123]    That’s wrong:
    the club is supposed to be reading *Noah Clue, P.I.* right now and *The Innocence
    of Father Brown* later. The trouble is, both the `reading` and `reading_later`
    descriptors are storing their data in the same instance attributes on `mystery_lovers`.    To
    get around that, I should instead store the desired attributes with the namespace
    of the descriptor it relates to, creating names like `reading.author` and `reading_later.title`.
    That requires a couple of additional methods on the descriptor, for a start:    [PRE124]    Listing
    15-33: *book_club.py:1c*    The `__set_name__()` special method is called when
    the descriptor is first bound to a name on the owning class. In this case, I’m
    using it to store the name the descriptor is bound to.    I define another method
    that I’ve chosen to name `attr()`, where I attach the namespace of the descriptor’s
    name to the beginning of the requested name. As a result, calling `attr(''title'')`
    on a descriptor bound to `reading` would return `reading.title`.    I implement
    this behavior throughout the `__set__()` method by using the `setattr()` function,
    to assign a value to the given attribute on `instance`.    I must similarly modify
    `__get__()` and `__delete__()`:    [PRE125]    Listing 15-34: *book_club.py:2c*    Here,
    I’m using `getattr()` and `delattr()` to respectively access and delete the given
    attributes, as composed by `self.attr()`, on `instance`.    Rerunning the usage
    in [Listing 15-32](#listing15-32), I get the following:    [PRE126]    The two
    descriptors are storing their attributes separately. This can be confirmed by
    printing out the names of all the attributes on the `mystery_lovers` object:    [PRE127]    Listing
    15-35: *book_club.py:7*    This produces the following:    [PRE128]    ## Slots    There’s
    one downside to the fact that all attributes are stored and accessed on a dictionary:
    a dictionary collection has significant performance and memory overhead. Ordinarily,
    this is a reasonable tradeoff, given all the versatility that this approach makes
    possible.    If you need to improve performance of your class, you can use *slots*
    to predeclare the attributes you want. Accessing an attribute on a slot is faster
    than accessing one on a dictionary, and it reduces the memory taken up by the
    attributes.    Switching your class to use slots instead of an instance `__dict__`
    is as simple as adding the `__slots__` class attribute, which is a tuple of valid
    attribute names. This list should contain names of instance attributes, not methods
    or class attributes (which are stored on the class `__dict__`).    For example,
    here’s a class for storing data about chemical elements:    [PRE129]    Listing
    15-36: *element.py:1a*    The `__slots__` tuple contains five names. These will
    be the only valid instance attribute names on an `Element` instance, and working
    with these attributes will be faster than when using `__dict__`. Notice that none
    of the methods have to be listed in `__slots__`; only the instance attribute names
    do. What’s more, the slots must never conflict with any names elsewhere on the
    class (with two exceptions, mentioned shortly).    ### Binding Attribute Names
    to Values    Although the attribute names are declared in `__slots__`, they don’t
    have a value (not even `None`) until they’re bound to values in the usual manner,
    such as within `__init__()`:    [PRE130]    Listing 15-37: *element.py:2*    Here,
    I’ve added my initializer, as well as a function for converting the instance to
    a string.    From the outside, the class seems to behave the same as a typical
    class, although if I were to measure the performance, it would be improved:    [PRE131]    Listing
    15-38: *element.py:3a*    ### Using Arbitrary Attributes with Slots    The `__slots__`
    attribute completely takes over attribute storage from the instance `__dict__`,
    preventing `__dict__` from even being created for the instance, as you can see
    here:    [PRE132]    Listing 15-39: *element.py:4a*    However, if I want the
    benefits of `__slots__` for the primary attributes, while still allowing additional
    attributes to be defined later, I only need to add `__dict__` to `__slots__`,
    like this:    [PRE133]    Listing 15-40: *element.py:1b*    The `__dict__` special
    attribute is one of the two exceptions to the rule that slots must not conflict
    with class attribute names. The other exception is _`_weakref__`, which you would
    add to `__slots__` if you wanted your slotted class to support weak references
    or references to a value that don’t increase the reference count or block garbage
    collection during their lifetime. I want both arbitrary attributes and weak references
    for `Element` instances, so I add the names to `__slots__`.    With this one change,
    the code in [Listing 15-39](#listing15-39) works correctly, instead of raising
    an `AttributeError`. This technique will diminish the space savings normally afforded
    by slots, but you will still have the performance gains on all slotted names.    ###
    Slots and Inheritance    Slots have some important effects on inheritance. First,
    you should only declare any given slot once in an inheritance tree. If I were
    to derive a class from `Element`, I should not redeclare any of the slots. Doing
    so would bloat the size of the derived class because all the slots are declared
    on each instance, even if some of the base class’s slots are shadowed by the derived
    class’s slots.    Second, you cannot inherit from multiple parents classes with
    non-empty slots. If you need to use slots in multiple inheritance scenarios, the
    best approach is to ensure that the base classes have only an empty tuple assigned
    to `__slots__`. That way, you can make the derived class use `__dict__`, `__slots__`,
    or both.    ## Immutable Classes    Technically, there’s no formal mechanism for
    creating an immutable class. This fact can make it unfortunately tricky to implement
    a hashable class, since the `__hash__()` method must produce a hash value that
    never changes during the instance’s lifetime, according to the documentation.    Although
    you cannot create a truly immutable class, you can get close enough that the fact
    it’s technically mutable doesn’t matter. Consider the core trait of an immutable
    object: once its attributes have been initially set, those attributes can never
    be modified by any means, nor can additional attributes be added. This is why
    all immutable objects are hashable. The most obvious way to emulate an immutable
    class (at least in my view), and the one that gives you the most control, is implemented
    using slots.    I want to make the `Element` class from earlier into an immutable
    class, and a hashable one at that. To accomplish that, I need to do the following:    *   Implement
    all attributes as `__slots__`. *   Restrict adding further attributes by omitting
    `__dict__` from `__slots__`. *   Allow the creation of weak references by including
    `__weakref__` in `__slots__` (not strictly necessary, but helpful enough in some
    use cases to be good practice). *   Implement `__setattr__()` and `__delattr__()`
    to prevent modifying or deleting existing attributes. *   Implement `__hash__()`
    to make instances hashable. *   Implement `__eq__()` and `__gt__()` to make instances
    comparable.    I’ll start by defining the `__slots__`, as before:    [PRE134]    Listing
    15-41: *element_immutable.py:1*    If I wanted to store additional attributes
    about elements, I could use a dictionary to associate `Element` keys with instances
    of some other mutable object as values that contain the rest of the data. For
    brevity, I won’t do that here.    I’ll add the special methods for converting
    to string, for hashing, and for comparing between `Element` instances:    [PRE135]    Listing
    15-42: *element_immutable.py:2*    In all these cases, I’m using `self.symbol`
    as the key attribute. Remember that `__eq__()`, `__lt__()`, and `__le__()` correspond
    to the equals (`==`), less-than (`<`), and less-than-or-equals (`<=`) operators,
    respectively. Not equals (`!=`), greater than (`>`), and greater than or equals
    (`>=`) are mirrors of these three, respectively, so I typically only need to implement
    one special method in each pair.    For objects of this class to be immutable,
    I have to prevent any modification of its attributes. However, I can’t just make
    `__setattr__()` do nothing, as it’s needed for initial assignment of values as
    well. Instead, I write this method to only allow assignment to uninitialized attributes:    [PRE136]    Listing
    15-43: *element_immutable.py:3*    If the attribute already exists on the instance,
    I raise an `AttributeError`. The message here is designed to exactly match the
    one raised by modifying an attribute on any true immutable class.    Because I’m
    using slots, I don’t need to worry about new attributes being added, so long as
    `__dict__` is not specified on `__slots__`.    If the attribute doesn’t already
    exist, I use `object.__setattr__()` to assign the value to that attribute. I cannot
    just call the `setattr()` function, or I’ll get infinite recursion.    I also
    must define `__delattr__()` to prevent deletion of an attribute:    [PRE137]    Listing
    15-44: *element_immutable.py:4*    The `__delattr__()` method is simpler to implement,
    as I don’t ever want to allow deleting an attribute from an immutable instance.
    Thus, any use of `del` on an attribute of this class raises an `AttributeError`.    This
    class now behaves as immutable, as you can see from the usage:    [PRE138]    Listing
    15-45: *element_immutable.py:5*    Some Python developers will happily point out
    that one can bypass the simulated immutability of the `Element` class by calling
    `__setattr__()` on the object directly:    [PRE139]    While this indeed modifies
    the `iron.symbol` attribute, this nasty hack is a straw man argument against the
    pattern. No code outside of the class itself should ever call `__setattr__()`;
    Python and its standard library certainly never will.    *Python does not pretend
    to be Java!* While it’s possible to bypass safety barriers—as is possible with
    most things in the Python language—if someone employs such an irrational and dirty
    hack, they deserve whatever bugs they’ve got coming to them. The hope of preventing
    such deliberate abuses does not justify the complexity and fragility of other
    immutability techniques, like inheriting from tuple, simulating objects with `namedtuple`,
    and so forth. If you want an immutable object, use `__slots__` and `__setattr__()`.    Alternatively,
    you can achieve something functionally similar with the `@dataclasses.dataclass(frozen=True)`
    class decorator, which is provided by the *dataclasses* module in the standard
    library. Dataclasses have some differences from normal classes, so if you want
    to use them, see the documentation at [https://docs.python.org/3/library/dataclasses.html](https://docs.python.org/3/library/dataclasses.html).    ##
    Single-Dispatch Generic Functions    By now, you’re probably used to the idea
    of duck typing and its implications for function design. However, every now and
    then, you’ll need a function to behave differently for parameters of different
    types. In Python, as in most languages, you can write *generic functions* to adapt
    to parameter types.    Generic functions in Python are made possible by two decorators
    from the *functools* standard library module: `@singledispatch` and `@singledispatchmethod`.
    Both of these decorators create a *single-dispatch generic function*, which can
    switch between multiple function implementations, based on the type of the first
    parameter (when using `@singledispatch`) or the first parameter that isn’t `self`
    or `cls` (when using `@singledispatchmethod`). This is the only difference between
    the two decorators.    As an example, I’ll expand on my `Element` class from earlier.
    I want to be able to compare `Element` instances to each other, as well as to
    a string containing an element symbol or an integer representing an element number.
    Instead of writing one big function with an `if` statement that checks the argument
    against `isinstance()`, I can use single-dispatch generic functions.    I’ll begin
    by adding two `import` statements before the `Element` class definition to get
    the `@singledispatchmethod` and `@overload` decorators:    [PRE140]    Listing
    15-46: *element_generic.py:1*    There are three slightly different ways to write
    a single-dispatch generic function, which I’ll cover in a moment. These techniques
    all work regardless of whether you’re using `@singledispatch` or `@singledispatchmethod`,
    except that the second decorator allows you to have `self` or `cls` as the first
    argument, which is why I use it here.    Regardless of which technique is used,
    the `__eq__()` method must be declared first. This first version of the method
    should be the most type-dynamic version, since it’ll be used as the fallback.    [PRE141]    Listing
    15-47: *element_generic.py:2*    This method is declared with the `@singledispatchmethod`
    decorator, but it is otherwise the same as if it were an ordinary implementation
    of the `__eq__()` instance method.    The `@singledispatchmethod` decorator must
    be the outermost (first) decorator for it to work with other decorators, such
    as `@classmethod`. The `@singledispatch` decorator can typically exist anywhere
    in the stack of decorators, although you’re best off ensuring it’s first, to avoid
    surprises and because consistency is helpful.    ### Registering Single-Dispatch
    Functions with Type Hints    My single-dispatch `__eq__()` method above still
    accepts any type. I want to add versions based on the type of the first argument.
    One way to do that is by registering them with the automatically created `@__eq__.register`
    decorator. In this case, I’ll create two more versions of the function: one that
    works with a string argument and another that works with either an integer or
    a floating-point number argument:    [PRE142]    Listing 15-48: *element_generic.py:3*    The
    first of these methods accepts a string argument. The first parameter, the one
    being switched on, is annotated with a type hint for the expected type, which
    is a string (`str`) in this first case.    The second method here accepts either
    an integer or a float, and it is made possible with the `@typing.overload` decorator.
    When type hinting, you can mark one or more function headings with `@overload`,
    to indicate that they overload an upcoming function or method with the same name.
    The *Ellipsis* (`...`) is used in place of the suite of the overloaded method,
    so it can instead share the suite of the method below it. The function or method
    not decorated with `@overload` must come immediately after all the overloaded
    versions thereof.    Every single dispatch method conventionally has an underscore
    (`_`) as a name, to avoid undesired shadowing. The fact that they shadow each
    other won’t matter, since they’re being wrapped and registered, and thus, they
    won’t need to be bound to names themselves.    When the `__eq__()` method is called,
    the type of the first parameter is checked. If it matches the type annotation
    for any of the registered methods, that method is used. Otherwise, the fallback
    method, the one marked with the `@singledispatchmethod` decorator, is called instead.    ###
    Registering Single-Dispatch Functions with Explicit Type    You can also achieve
    the same result without type annotations. In this case, instead of type hinting,
    I pass the expected type of the first non-`self` parameter to the `register()`
    decorator. I’ll use this technique to define my `__lt__()` method:    [PRE143]    Listing
    15-49: *element_generic.py:4*    As before, the first version is the most dynamic,
    the second accepts a string, and the third accepts either an integer or a floating-point
    number.    Although it’s not seen in this example, your single-dispatch function
    can accept as many arguments as you need and even different arguments on the different
    functions, but you can only switch method definitions on the data type of the
    first parameter.    ### Registering Single-Dispatch Functions with the register()
    Method    The third way to register to a single-dispatch function is to call `register()`
    as a method, rather than as a decorator, and directly pass any callable to it.
    I’ll use this technique with the `__le__()` method.    [PRE144]    Listing 15-50:
    *element_generic.py:5*    In this case, I define the generic single-dispatch method
    first, and then I directly register lambdas for handling strings, integers, and
    floating-point numbers. I could pass *any* callable in place of that lambda, whether
    it be a previously defined function, a callable, or anything else that accepts
    the appropriate argument.    Of these three techniques, I like the lambdas the
    best for these basic operator special methods, since they have less boilerplate.
    Otherwise, for more involved functions, I prefer to work with type annotations
    instead.    ## Using the Element Class    I’ve put a lot of work into this `Element`
    class, making it immutable and allowing the comparison of instances to strings
    and numbers. The benefits of all this work are apparent in the usage of the class,
    which I’ll demonstrate by writing a `Compound` class to represent chemical compounds:    [PRE145]    Listing
    15-51: *element_generic.py:6*    I’ll wager you could read through that code and
    understand everything going on. In short, this class allows me to instantiate
    a chemical compound with a name and add elements to the compound. Because `Element`
    is hashable and immutable, I can safely use `Element` instances as dictionary
    keys.    Because I can compare `Element` instances, either to strings representing
    element symbols or to integers representing element numbers, I can fairly easily
    implement the Hill system for outputting an empirical chemical formula for the
    compound.    Here’s the `Compound` class in use:    [PRE146]    Listing 15-52:
    *element_generic.py:7*    I define four `Element` objects: `hydrogen`, `carbon`,
    `oxygen`, and `iron`. Then I use these to construct three `Compound` instances:
    `rust`, `aspirin`, and `water`. I print each `Compound` using the canonical string
    representation (from `__repr__()`) via the `!r` formatting flag.    As you can
    see, the `Compound` class and its usage are quite simple and clean, all because
    I designed `Element` with slots, `__setattr__()`, and single-dispatch generic
    functions.    ## Arbitrary Execution    Introspection also enables *arbitrary
    execution*, whereby strings can be directly executed as Python code. To this end,
    there are some built-in functions that you will encounter sooner or later and
    that may appeal to your inner hacker: `eval()`, `compile()`, and `exec()`. Yet
    hidden dangers lurk.    Here’s a contrived little version of how this can go very
    wrong:    [PRE147]    Listing 15-53: *arbitrary.py*    I read all the lines in
    from a file, *input.dat*, which I naively assume will contain only mathematical
    expressions.    For each line I read from *input.dat*, I compose a string containing
    a Python expression, which I bind to `expression`. Then I pass that string to
    the `eval()` built-in function, which evaluates it as a Python expression and
    converts it to a value that I bind to `answer`.    For the sake of demonstration,
    I compose a string containing a line of Python code, bound to `code`. I could
    execute it immediately as Python code by passing the string to the `exec()` built-in
    function. Instead, I compile it into a Python code object using `compile()`, and
    then I run that code object using `exec()`. This approach is slower for a single
    use but faster for code being called repeatedly. Again, I only have it here to
    demonstrate the technique.    The problem here is that arbitrary execution is
    a major security risk, especially as soon as it involves data provided from an
    external source, such as a file or user input. I’m expecting my *input.dat* to
    look something like this:    [PRE148]    Listing 15-54: *input.dat:1a*    These
    values produce some neat, safe-looking output:    [PRE149]    The danger here
    is a potential security threat. What would happen if an attacker somehow modified
    *input.dat* to look like this?    [PRE150]    Listing 15-55: *input.dat:1b*    What
    would happen if I ran that code on a POSIX system, such as Linux?    [PRE151]    That
    `jason is DOOMED` message should make your blood run cold, as that was *not* from
    a print statement; it was produced by a shell command executed directly on the
    operating system. This is known as a *code injection attack*, and it can lead
    to some pretty horrific security issues. (I’ll revisit security in Chapter 19.)    There
    are many clever and esoteric ways to inject code into a string being passed to
    `eval()`, `compile()`, or `exec()`. As a result, although these functions may
    look like the key to some truly brilliant Python code, they’re almost always best
    left alone. If you really, really need something like `eval()`, you should almost
    certainly use `ast.literal_eval()` instead, although it is unable to evaluate
    with operators (and thus, it cannot work with my *input.dat*). There are rare,
    advanced techniques that use `eval()`, `compile()`, or `exec()` safely, but that
    involve ensuring those functions can only ever receive *trusted* data, as opposed
    to external data, which is *untrusted*.    To learn more about how dangerous `eval()`
    (and `exec()`, by extension) is, check out Ned Batchelder’s article, *Eval really
    is dangerous:* [https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html.](https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html)
    The discussion in the comments is also insightful.    Some of my cleverness-loving
    readers will have noticed `os.system()` can be used to execute shell commands.
    This, too, should seldom (if ever) be employed. Use the `subprocess` module instead:
    [https://docs.python.org/3/library/subprocess.html](https://docs.python.org/3/library/subprocess.html).    ##
    Wrapping Up    Classes and class instances store their attributes inside of special
    dictionaries, and this one detail empowers Python to know a lot about the internal
    composition of objects during runtime.    Descriptors—the magic behind properties,
    methods, and many other tricks—can be used to make your code easier to maintain.
    Slots unlock performance and enable you to write effectively immutable classes.
    Single-dispatch generic functions bring the versatility of overloaded functions
    to dynamic typing.    Python certainly looks magical at first blush, but it freely
    unlocks the backstage door and lets us in on all its illusions and secrets. By
    knowing how the tricks work, you, too, can write elegant classes and libraries
    that feel, well, dead simple to use.[PRE152]`# 16 Asynchrony and Concurrency  ![](Images/chapterart.png)  You
    know the situation: you have to finish that TPS report for your boss, fix a bug
    that shipped to production, and figure out which of your co-workers borrowed your
    stapler (it’s Jeff again, isn’t it?), all before day’s end. How will you get it
    all done? You can’t make copies of yourself—and even if you could, the line for
    the copier is out the door—so you tackle these tasks with *concurrency*.    It’s
    the same in Python. If your program needs to wait on user input, send data over
    a network, and crunch numbers, all while still updating the user interface, you
    can handle these tasks concurrently, thus improving your program’s responsiveness.    There
    are two options for achieving concurrency in Python: either threading (see Chapter
    17), wherein the operating system manages the multitasking, or asynchrony, where
    Python handles it. This chapter focuses on the latter.    ## Asynchrony in Python    As
    mentioned, there are two ways to achieve concurrency in Python. *Threading*, also
    known as *pre-emptive multitasking*, involves letting the operating system manage
    the multitasking by running each task in a single flow of execution called a *thread*.
    These multiple threads still share the same system *process*, which is an instance
    of a running computer program. If you open the system monitor on your computer,
    you can see a list of running processes on your machine. Any one of those processes
    can have multiple threads.    Traditional threading has a number of pitfalls,
    which is why I’ll come back to it in Chapter 17. The alternative is *asynchrony*,
    also known as *cooperative multitasking*. It is the easiest way to achieve concurrency
    in Python—but that doesn’t make it easy! The operating system only sees your code
    as running in a single process, with a single thread; it is Python itself that
    manages the multitasking, with some help from you, thereby sidestepping some of
    the issues that crop up with threading. Still, writing good asynchronous code
    in Python requires some forethought and planning.    It’s important to keep in
    mind that asynchrony is not parallelism. In Python, a mechanism called the *Global
    Interpreter Lock (GIL)* ensures that any single Python process is constrained
    to a single CPU core, regardless of how many cores are available to the system.
    For this reason, parallelism cannot be achieved with either asynchrony or threading.
    This may sound like a design flaw, but efforts to eliminate the GIL from CPython
    have proven more technically challenging than imagined, and the results have had
    poorer performance thus far. As of this writing, progress has all but stalled
    on the most prominent of these efforts, Larry Hastings’s Gilectomy. The GIL makes
    things run smoother in Python.    Asynchrony was originally possible in Python
    through third-party libraries, like Twisted. Later, Python 3.5 added syntax and
    functionality for natively achieving asynchrony. These features did not become
    stable until Python 3.7; as a result, many articles and online discussions about
    asynchrony are profoundly out-of-date. Your best source of information is always
    the official documentation. I’ve done my best to be up-to-date with Python 3.10.    Python
    makes asynchrony possible with two keywords borrowed from the C# language: `async`
    and `await`, as well as a special type of coroutine. (Many other languages implement
    similar syntax; these include JavaScript, Dart, and Scala.) Asynchronous execution
    is managed and run by an *event* loop, which is responsible for the multitasking.
    Python provides the *asyncio* module in the standard library for this purpose,
    and we’ll use it in this chapter’s examples.    It’s worth noting that at the
    time of this writing, `asyncio` is dauntingly complicated beyond basic usage,
    even to some Python experts. For that reason, I’ll stick to the essential concepts
    universal to asynchrony and avoid explaining or using `asyncio` any more than
    is absolutely unavoidable. Most of what you’ll see are just plain asynchrony techniques;
    I’ll call out the exceptions.    When you’re ready to go further into this topic
    of asynchrony, pick up either the Trio or Curio library. Both are written to be
    user-friendly. They’re well documented, with the beginner in mind, and they regularly
    provide design cues to `asyncio`’s developers. Armed with the knowledge from this
    chapter, you should be able to learn either of those libraries from their documentation.    *Curio*
    was developed by David Beazley, an expert in Python and concurrency, with the
    goal of making asynchrony in Python more approachable. The official documentation
    can be found at [https://curio.readthedocs.io/](https://curio.readthedocs.io/).
    That main page also has links to a number of excellent talks on Python asynchronous
    programming, including several talks that guide you through writing your *own*
    asynchrony module (although you likely won’t ever need to do this).    *Trio*
    is based on Curio, and it furthers the library’s goals of simplicity and usability.
    As of this writing, it’s considered somewhat experimental, but it is still stable
    enough to use in production. Python developers most often recommend using Trio.
    Find the official documentation at [https://trio.readthedocs.io/en/stable/](https://trio.readthedocs.io/en/stable/).    Earlier
    in this section, I mentioned the *Twisted* library, which added asynchronous behavior
    to Python some 20 years before asynchrony was added to the core language. It uses
    a number of dated patterns, rather than the modern asynchronous workflow model,
    but it’s still an active and viable library with many use cases. A number of popular
    libraries use it under the hood. Learn more at [https://twistedmatrix.com/](https://twistedmatrix.com/).    The
    `asyncio` official documentation can be found at [https://docs.python.org/3/library/asyncio.html](https://docs.python.org/3/library/asyncio.html).
    I recommend digging into `asyncio` in more detail only once you’re comfortable
    with asynchronous programming via either Trio or Curio, as well as with the analogous
    concepts of threading (Chapter 17). Your grasp of the concepts and patterns of
    asynchrony and concurrency will help you make sense of the `asyncio` documentation.    In
    the midst of all this, remember that asynchrony is still in its relative infancy,
    both in Python and in the field of computer science as a whole. The asynchronous
    workflow model first appeared in the F# language in 2007, based on concepts introduced
    in Haskell around 1999 and several papers from the early 1990s. By contrast, the
    related concept of threading dates to the late 1960s. Many problems in asynchrony
    still don’t have clear or established solutions. Who knows—you might be the first
    to solve one of them!    ## The Example Scenario: Collatz Game, Synchronous Version    To
    properly demonstrate how these concepts work, I’ll create a small program that
    would benefit from concurrency. Because of the complexity of the issues at hand,
    I’ll focus entirely on this one example for this chapter and the next, so you
    can get used to the working details.    I’ll start with a *synchronous* working
    version, so you’ll have a solid idea of what I’m doing. The complexity of this
    example will demonstrate some of the common issues involved in concurrency. Simplicity
    is the enemy of effectiveness for examples with these concepts.    For the example,
    I’ll play with a strange phenomenon in math known as the *Collatz conjecture*,
    which works like this:    1.  Start with any positive integer *n*. 2.  If *n*
    is even, the next term in the sequence should be `n / 2`. 3.  If *n* is odd, the
    next term in the sequence should be `3 * n + 1`. 4.  If *n* is `1`, stop.    Even
    if you start with a fantastically large number, you’ll always wind up at `1` after
    relatively few steps. Starting with `942,488,749,153,153`, for example, the Collatz
    sequence arrives at `1` in only 1,863 steps.    There are all sorts of things
    you can do with the Collatz conjecture. For this example, I’ll create a simple
    game that challenges the user to guess how many Collatz sequences have a particular
    length. I’ll restrict the range of starting numbers to integers between `2` and
    `100,000` (which I can also represent as `10**5`).    For example, exactly 782
    starting numbers will yield Collatz sequences with a length of exactly 42 values.
    To play the game in the example, the user would enter 42 (the target length) and
    then guess how many starting numbers would produce Collatz sequences of the target
    length. If the user guessed 782, they’d win. (Okay, yes, it’s a lousy game premise,
    but it works for demonstrating concurrency.)    At the top of my module, I’ll
    define a constant, `BOUND`, for the maximum starting number. Counting zeros in
    constants is an invitation to error, so I’ll define 100,000 as a power of 10 instead:    [PRE153]    Listing
    16-1: *collatz_sync.py:1*    Next is the function for finding the number of steps
    in a single Collatz sequence:    [PRE154]    Listing 16-2: *collatz_sync.py:2*    Nothing
    should surprise you here. This function follows the rules for calculating a Collatz
    sequence and returns the number of steps it took to reach `1`.    I need another
    function for tracking how many times a target sequence length is met:    [PRE155]    Listing
    16-3: *collatz_sync.py:3*    This function runs `collatz()` on every possible
    starting integer from `2` to `BOUND` and counts how many times a Collatz sequence
    had exactly `target` steps. It returns this count at the end.    Next, I create
    a function for getting a positive integer from the user, which I’ll need to do
    a couple of times in the program run, first to get the desired Collatz target
    length, and second to get the user’s guess of how many starting values produce
    Collatz sequences of the target length:    [PRE156]    Listing 16-4: *collatz_sync.py:4*    This
    function should also look pretty familiar by now. I get a string from the user
    with `input()`, attempt to convert it to an integer, and ensure that integer is
    not negative. If anything goes wrong, I display a message for the user and let
    them try again.    Tying it all together is the main function:    [PRE157]    Listing
    16-5: *collatz_sync.py:5*    I display the name of the program and ask the user
    to enter a target sequence length to search for. I perform the search for the
    sequence length with `length_counter()` and bind the result to `count`. Next,
    I get the user’s guess, which I bind to `guess`, and compare it to `count`, giving
    the user some feedback on how close their guess was.    Lastly, I need to execute
    the main function:    [PRE158]    Listing 16-6: *collatz_sync.py:6*    All in
    all, I’ve stuck with syntax and patterns you should be familiar with by now. But
    running this module shows why the program could do with some concurrency:    [PRE159]    At
    this point, the program hangs for several seconds, before continuing:    [PRE160]    It
    works, but it doesn’t feel particularly responsive. What’s more, if I were to
    increase the value of `BOUND` by even one exponential value, to `10**6`, the delay
    would increase dramatically; on my system, it went from 7 to 63 seconds!    Thankfully,
    there are a number of ways I could make this program feel more responsive. Over
    the next two chapters, I’ll point these out to you and implement the changes.    ##
    Asynchrony    Let’s see how asynchrony can help my Collatz program. First, notice
    that the several-second delay on running `length_counter()` is *CPU-bound*, as
    it relates to how long it takes the CPU to perform the math. That delay will remain
    until I apply parallelism in the next chapter.    But there’s another source of
    delay in this program: the user. The program has to wait for an indefinite period
    of time until the user enters a valid number. This part of the program is *IO-bound*,
    because it is limited by the response time of something external, such as user
    input, a network response, or another program, rather than the working speed of
    the CPU.    I can improve the program’s *perceived responsiveness*, how fast it
    seems to the user, by running the math concurrently with waiting for the user’s
    guess. The calculations themselves won’t actually happen any faster, but my users
    probably won’t realize that: they’ll be focusing on entering a guess while Python
    is running that heavy-duty math.    I’ll use the built-in `asyncio` module to
    work with asynchrony in Python, so I import it at the start of my program:    [PRE161]    Listing
    16-7: *collatz_async.py:1a*    As in [Listing 16-1](#listing16-1), I’m defining
    my `BOUND` constant here. Now I can begin rewriting my code to be asynchronous.    ###
    Native Coroutines    In Chapter 10, I introduced simple coroutines, which are
    based on generators. Simple coroutines run until they hit a `yield` statement
    and then wait for data to be sent to the coroutine object with the `send()` method.    I’ll
    make my game’s code asynchronous by turning some of my program’s functions into
    *native coroutines*. Also called *coroutine functions*, native coroutines build
    on the idea of simple coroutines: instead of waiting for data to be sent, they
    can be paused and resumed at specific places to achieve multitasking. In the rest
    of this chapter, I use the terms *coroutine function* and *native coroutine* somewhat
    interchangeably. Be advised that when most Python developers refer to coroutines,
    they almost always mean native coroutines, although it never hurts to clarify.    You
    declare a native coroutine by placing the `async` keyword in front of the definition,
    like this:    [PRE162]    However, don’t get the idea that to implement asynchrony,
    you only need to put the `async` keyword in front of all your function definitions.
    This is only the first of many steps toward making the code asynchronous.    When
    called, a coroutine function returns a native coroutine object, which is a special
    kind of object called an *awaitable*. These are callables that can pause and resume
    mid-execution. Awaitables must be called using the `await` keyword, which acts
    much like `yield from`. Here, I use the `await` keyword to call the awaitable
    coroutine function `some_function()`:    [PRE163]    There’s another catch: the
    `await` keyword can only be used within an awaitable. When a native coroutine
    reaches an `await`, it pauses execution until the called awaitable is finished.    A
    function should only be turned into a coroutine function when it calls another
    awaitable, performs an IO-bound task, or is specifically intended to run concurrently
    with another awaitable. In the Collatz game, I’ll need to make some decisions
    about which functions to turn into coroutine functions and which to leave as ordinary
    functions.    To begin with, consider the synchronous `collatz()` function:    [PRE164]    Listing
    16-8: *collatz_async.py:2*    This function will always return almost instantly,
    so it neither needs to call an awaitable nor run concurrently with another awaitable.
    It can remain as a normal function.    Meanwhile, `length_counter()` is labor
    intensive and CPU-bound. I want it to run concurrently with the code that waits
    for the user to input a guess, so it’s a good candidate for a coroutine function.
    I’ll rewrite the synchronous version from [Listing 16-3](#listing16-3):    [PRE165]    Listing
    16-9: *collatz_async.py:3*    I turn this function into a coroutine function with
    `async` and use `await asyncio.sleep(0)` to tell Python where the coroutine function
    can pause and let something else work. If I don’t `await` something in the coroutine
    function, it will never be paused, which would defeat the purpose of making it
    a coroutine function in the first place. (Trio, Curio, and `asyncio` all offer
    a `sleep()` awaitable.)    I also want to turn the IO-bound `get_input()` function
    into a coroutine function, as the very nature of waiting for user input involves
    the ability to pause and resume. This first version of the coroutine function
    doesn’t yet await anything else; I’ll revisit that in a bit.    [PRE166]    Listing
    16-10: *collatz_async.py:4a*    There’s one critical limitation to `await`: it
    can only be called from within an awaitable, such as a coroutine function. I want
    to call `get_input()` from `main()`, so `main()` must also be a coroutine function,
    as you’ll see in [Listing 16-11](#listing16-11).    Behind the scenes, native
    coroutines are still used in a strikingly similar manner to simple coroutines.
    Because `length_counter()` is a coroutine function, I can force it to be executed
    manually (and synchronously) the same as I would a simple coroutine. This is just
    a side example that runs the coroutine function synchronously:    [PRE167]    I
    wouldn’t ever use this approach in production, as coroutine functions need to
    be run in a special way to be useful.    ### Tasks    Now that `get_input()` and
    `length_counter()` are coroutine functions, I must call them using the `await`
    keyword. There are two different ways to invoke them, depending on how I want
    them to be run: by directly awaiting them or by scheduling them as *tasks*, which
    are special objects that run coroutine functions without blocking.    Both approaches
    require turning the Collatz example’s `main()` function into a coroutine function,
    so I’ll begin by doing that:    [PRE168]   **Listing 16-11: *collatz_async.py:5a*    Deciding
    how to call each awaitable requires some thought. First, before I can do anything
    else, I need to know what Collatz sequence length the user wants to search for.
    I call the `get_input()` coroutine function with the `await` keyword. Calling
    the coroutine function like this will block the program while literally awaiting
    the user input. This blocking is acceptable here, since we can’t do any math calculations
    (or, really, anything else) without that initial user input.    Once I have the
    input, I can start the calculation in `length_counter()`, which I want to run
    concurrently with getting the user guess via another call to `get_input()`. To
    do this, I schedule the native coroutines as tasks. Always schedule a `Task` object
    rather than instantiating it directly. `Here`, I use `asyncio.create_task()` to
    schedule a task.    The two native coroutines are now scheduled to run as soon
    as there’s an opening, that is, as soon as `main()` is awaiting something. I relinquish
    the `main()` coroutine function’s control of execution by calling `await` on one
    of my tasks—at the moment, it doesn’t matter which—thereby allowing another task
    to have a turn. Because both tasks are already scheduled, they’ll take turns until
    `length_counter_task` returns the value returned by the `length_counter()` coroutine
    function. Then the program waits on the other task, `guess_task`, until it, too,
    returns a value. Depending on how quickly the user entered input, `guess_task`
    may have been waiting to return a value even while `length_counter_task` was still
    running.    The Trio and Curio libraries have tasks, just like `asyncio` does,
    although they’re created a little differently. Consult the documentation for those
    libraries to learn more.    ### The Event Loop    At this point, it may seem like
    I’ve coded myself into a corner: coroutine functions and other awaitables must
    be called with `await`, but only coroutine functions can contain the `await` keyword.
    How can I start this program?    The *event loop* is the heart of asynchrony.
    It manages the multitasking between awaitables and provides the means of calling
    the first awaitable in the stack. Every asynchrony module provides an event loop
    mechanism. You can even write your own if you’re feeling brave. In this example,
    I’ll employ the default event loop provided by `asyncio`, like this:    [PRE169]    Listing
    16-12: *collatz_async.py:6a*    I acquire an event loop❶ and bind it to the name
    `loop`. An event loop object has a number of methods for controlling execution;
    in this case, I use `loop.run_until_complete()` to schedule and run the `main()`
    coroutine function ❷.    Since this is the most common way to start an event loop,
    it’s not surprising that `asyncio` provides a shorter equivalent way to use the
    default event loop:    [PRE170]    Listing 16-13: *collatz_async.py:6b*    I can
    now run my module, and it works.    Alternative asynchrony modules offer some
    important additional mechanisms for handling multiple tasks: Curio has `TaskGroup`,
    while Trio has (and requires use of) `Nursery`. See the documentation for those
    libraries to learn more. The `asyncio` module has no analogous structures yet,
    and implementing them is decidedly non-trivial.    If you run this code, you’ll
    notice a remaining problem: *collatz_async.py* still hangs in the same place it
    did before! That’s not very practical.    ### Making It (Actually) Asynchronous    The
    code isn’t yet behaving concurrently because of this line from `get_input()` in
    [Listing 16-10](#listing16-10), which I warned you about earlier:    [PRE171]    No
    matter how I write the rest of the program, `input()` is an IO-bound blocking
    function, so it will hog the process until the user inputs something, such as
    their guess. It doesn’t know how to take turns.    To get user input asynchronously,
    I must use a coroutine function equivalent to `input()`. There’s nothing of the
    sort in the standard library, but the third-party library `aioconsole` provides
    an asynchronous equivalent for `input()`, among a few other functions. I’ll need
    to install this package to my virtual environment.    Once it’s installed, I import
    the `ainput` coroutine function I need:    [PRE172]    Listing 16-14: *collatz_async.py:1b*    The
    `ainput` coroutine function works exactly like the built-in function `input()`,
    except that it’s an awaitable, so it will periodically relinquish control of the
    process, allowing other awaitables to run.    Conventionally, asynchronous equivalents
    to standard library functions and modules are prepended with an `a` (for `async`).
    This is helpful to know because there’s not much in the way of documentation for
    `aioconsole`, as of this writing. With any library of this sort, assume this naming
    convention and identical usage to the standard library equivalent, unless otherwise
    informed by documentation.    I adjust my `get_input()` coroutine function to
    use `ainput`:    [PRE173]    Listing 16-15: *collatz_async.py:4b*    Now, if I
    run the module, it works asynchronously. If I take more than a couple of seconds
    to input a valid guess, the results are printed immediately after I press enter.
    In contrast, if I input a valid guess immediately, the delay from the CPU-bound
    task can still be observed. As mentioned before, concurrency only improves the
    perceived responsiveness of the program, not the execution speed.    ## Scheduling
    and Asynchronous Execution Flow    When you’re used to the execution flow of ordinary
    synchronous code, asynchrony can take more than a little bit of getting used to.
    To help reinforce the principles I’ve introduced, I’ll break down the call stack
    of the complete *collatz_async.py* file.    At the outset of execution, I start
    the event loop:    [PRE174]    This schedules the `main()` coroutine function
    as a task, which I’ll refer to as `main` throughout this section. Because it’s
    the only task scheduled, the event loop runs it immediately.    Next, the code
    needs some user input before it can logically do anything, so I `await` the return
    value of the coroutine function `get_input()`:    [PRE175]    The `await` statement
    causes the `main` task to relinquish control to the event loop so something else
    can run. The coroutine function `get_input()` is scheduled as an event in the
    background, it runs, and the value is returned and assigned to `target`. With
    that fulfilled, the `main` task proceeds.    Next, I schedule the `get_input()`
    coroutine function as a task to get the user’s guess:    [PRE176]    The task
    bound to `guess_task` is scheduled, but it does not immediately start. The `main`
    task still has control, and it hasn’t relinquished it yet.    I also schedule
    the `length_counter()` coroutine function in the same manner:    [PRE177]    Now,
    `length_counter_task` is scheduled to run at some point in the future.    Next,
    this line is executed in the `main` task:    [PRE178]    This code causes `main`
    to relinquish control to the event loop, in that it pauses and waits for `length_counter_task`
    to have a value for return. The event loop now has control.    The next scheduled
    task in the queue is `guess_task`, so that’s started next. The `get_input()` coroutine
    function runs up to the following line:    [PRE179]    Now, `get_input()` is waiting
    on another awaitable, `ainput()`, which is scheduled. Control is handed back to
    the event loop, which runs the next scheduled task, `length_counter_task`. The
    `length_counter()` coroutine function is started, and it runs up to its `await`
    command before returning control to the event loop.    Perhaps the user hasn’t
    yet input anything—only a few milliseconds have passed, after all—so the event
    loop checks in with `main` and `guess_task`, which are both still waiting. The
    event loop again checks `length_counter_task`, which does some more work before
    pausing again. Then, the event loop checks back with `ainput` to see if the user
    has entered anything yet.    Execution continues in this manner until something
    finishes.    Bear in mind that `await` isn’t a magical keyword that *itself* returns
    control to the event loop. Rather, some non-trivial logic under the hood determines
    which task is run next, but I won’t go into that here, in the interests of time
    and sanity.    Once the `length_counter_task` has completed and is ready to return
    a value, the `await` in `main` is fulfilled, and that returned value is assigned
    to `count`. The next line in the `main()` coroutine function is run:    [PRE180]    For
    the purposes of this example, suppose the `guess_task` isn’t done yet. The `main`
    task must wait some more, so it hands control back to the event loop, which now
    checks in on `guess_task`—still waiting—before checking on `ainput`. Notice it
    doesn’t need to check on `length_counter_task` anymore, as that task is completed.    Once
    the user enters something, `ainput` has a value to return. The event loop checks
    in with the still-waiting `main` task and then allows `guess_task` to store the
    returned value from its await in `n` and continue its execution. There are no
    more `await` statements in the `get_input()` coroutine function, so aside from
    the event loop looking in on the napping `main` task, `guess_task` is able to
    return a value. With its `await` fulfilled and no other tasks in the queue, `main`
    is given priority again, and it finishes up.    There’s an important rule here:
    the order in which concurrent tasks complete is never guaranteed! As you’ll see
    in the next chapter, this can lead to some interesting problems.    ### Simplifying
    the Code    I can use the `asyncio.gather()` coroutine function instead of the
    prior method to run two tasks concurrently. This won’t change the functionality
    of the program at all from what I’m already doing, but it will make the code cleaner:    [PRE181]    Listing
    16-16: *collatz_async.py:5b*    I pass the awaitables I want to run to `asyncio.gather()`
    in the order I want their values returned. While `asyncio.gather()` will create
    and schedule tasks for all native coroutines passed to it, remember not to depend
    on the order in which tasks will be started and run. The return values from the
    native coroutines are packed into a list, which is then returned from `asyncio.gather()`.
    In this case, I unpack the two values from the list into `guess` and `count`.
    The outcome is the same as that of [Listing 16-11](#listing16-11).    ## Asynchronous
    Iteration    Iterators work with asynchrony much the same as functions do: only
    iterators marked as `async` support the pause-and-resume behavior. By default,
    looping on an iterator is blocking, unless there’s an explicit `await` somewhere
    in the suite. This particular feature of asynchrony has evolved quite a lot in
    the last several versions of Python and has only achieved some modicum of API
    stability in 3.7, so older code you may find will likely use outdated techniques.    To
    demonstrate this behavior, I’ll rework my Collatz example to use an asynchronously
    iterable class, instead of a coroutine function. Understand that this technique
    is overpowered for this use case. In production code, I’d have stuck with the
    simpler native coroutine and saved the asynchronous iterator class for more complex
    logic.    All the new concepts below are part of the core Python language and
    not from `asyncio`. I start simply enough by creating a `Collatz` class, setting
    the bound and starting values as before:    [PRE182]    Listing 16-17: *collatz_aiter.py:1*    Next,
    I’ll write a new coroutine function that will contain all the logic for counting
    the steps in a single Collatz sequence. This really isn’t much of a coroutine
    function, as it lacks a `yield`, but this approach will be convenient for the
    example:    [PRE183]    Listing 16-18: *collatz_aiter.py:2*    For an object to
    be an ordinary, synchronous iterator, it needs to implement the special methods
    `__iter__()` and `__next__()`. Similarly, to be an *asynchronous iterator*, it
    must implement the special methods `__aiter__()` and `__anext__()`. You can define
    an *asynchronous iterable* in the same manner, by implementing `__aiter__()`.    Here,
    I define the two special methods necessary to make Collatz an asynchronous iterator:    [PRE184]    Listing
    16-19: *collatz_aiter.py:3*    The `__aiter__()` method must return an asynchronous
    iterator object, which is just `self` in this case. You will notice that this
    method is not made awaitable with `async`. It must be directly callable.    The
    `__anext__()` special method has a couple of differences from `__next__()`. First
    and most importantly, it is marked `async`, making it awaitable. Otherwise, iterating
    over the iterator object would be blocking. Second, when there are no more values
    to iterate over, I raise `StopAsyncIteration`, instead of `StopIteration`, as
    with ordinary iterators.    In my `__anext__()` coroutine function, I also chose
    to include an `await` statement, which allows the coroutine function to pause
    if necessary and hand control back to the event loop. (Here, I really only do
    this to demonstrate that it’s possible, especially since asynchronous iterators
    are particularly useful when a single iterative step is time consuming. However,
    since the execution time of the coroutine function is so brief in this example,
    I could have omitted it, as the mere usage of the asynchronous iterator involves
    an `await` under the hood.)    In my `length_counter()` coroutine function, I
    must use an `async for` to iterate over the asynchronous iterator:    [PRE185]    Listing
    16-20: *collatz_aiter.py:4*    The `async for` compound statement is specifically
    for iterating over an asynchronous iterator, which in this case is a `Collatz`
    instance ❶.    If you want to understand what’s happening under the hood here,
    take a look at the equivalent logic for the `async for` loop in the context of
    this function:    [PRE186]    Notice the use of the `await` keyword when calling
    `__anext__()` on the iterator. An `async for` loop will hand control back to the
    event loop on each iteration, but if a single iteration were to take a long time,
    I’d need the additional `await` statements in the `__anext__()` coroutine function
    to keep it from blocking.    As for the rest of this version of my code, I’m reusing
    Listings 16-16 and 16-13\. The output and behavior are the same as before.    As
    I mentioned, asynchronous iterators are definitely overpowered for my Collatz
    example. Most of the time, asynchronous iterators are only useful if iteration
    is either IO-blocking or computationally heavy enough to justify some sort of
    progress indicator, or perhaps for concurrency with another IO-blocking task.    ##
    Asynchronous Context Managers    Context managers must also be written in a specific
    way to have them work with asynchrony. An asynchronous context manager has the
    special coroutine functions `__aenter__()` and `__aexit__()`, instead of the usual
    `__enter__()` and `__exit__()` special methods. Typically, you’d only write an
    asynchronous context manager if you needed to await something in `__aenter__()`
    or `__aexit__()`, such as a network connection.    An asynchronous context manager
    is used with `async with`, instead of `with`, but using it is otherwise the same
    as working with a regular context manager.    ## Asynchronous Generators    You
    can also create *asynchronous generators*, which are identical in all manners
    to ordinary generators (discussed in Chapter 10), except for being compatible
    with asynchrony. They were introduced in Python 3.6 via PEP 525.    You define
    an asynchronous generator with `async def` but use `yield` statements as with
    regular generators. As these generators are asynchronous, you can use `await`,
    `async for`, and `async with` in their suites as needed. When the asynchronous
    generator is called as normal (without `await`), it produces an *asynchronous
    generator iterator*, which can be used just as you would an asynchronous iterator.    ##
    Other Asynchrony Concepts    There are quite a number of other tools in your asynchrony
    toolkit: locks, pools, events, futures, and the like. Most of these concepts are
    borrowed from the older and considerably better-documented technique of threading,
    which I’ll cover in Chapter 17. The main reason I’ve skipped over these concepts
    in this chapter is that the exact usage of each one varies between asynchrony
    modules. If you’ve done any amount of work in concurrency before, you’ve also
    noticed that I skipped some important problems, including race conditions and
    deadlocks. I’ll introduce these issues in the next chapter as well.    One other
    advanced concept relating to asynchrony that you should be aware of is *context
    variables*, or `contextvars`. These allow you to store different values in variables
    depending on a context, meaning two different tasks can work with the same apparent
    variables but in fact retrieve entirely distinct values. If you want to learn
    more about context variables, see the official documentation at [https://docs.python.org/3/library/contextvars.html](https://docs.python.org/3/library/contextvars.html).
    (The analogous threading concept is *thread-local storage*, which I won’t cover
    in this book.)    Anyone considering a dive into asynchrony should push onward
    into the next chapter, as the same problems experienced in threading and traditional
    concurrency can creep into asynchronous programming. The exact solutions vary
    from one asynchrony library to the next and may even require a bit of trailblazing.
    If you want to excel at asynchrony, become comfortable with threading, even if
    you never plan on using threads in production.    ## Wrapping Up    The distinction
    between concurrency and asynchrony can be bewildering at first, so here’s a quick
    recap. Concurrency improves your program’s perceived responsiveness by allowing
    the code to do something else while waiting on an IO-bound process. It does not
    make your code faster, and it thus isn’t useful for CPU-bound processes where
    the delays come from the code itself. Asynchrony is a relatively new way of achieving
    concurrency entirely within the code, without needing to resort to the more complex
    techniques.    In Python, asynchronous programming is achieved with the `async`/`await`
    model, which introduces two keywords. Roughly, `async` means, “This structure
    can be used asynchronously,” and `await` means, “I’m waiting for a value, so you
    can do something else now, if you like.” The `await` keyword can only be used
    inside of a native coroutine (also known as a coroutine function), which is a
    function declared with `async def`.    Asynchronous programming primarily consists
    of writing native coroutines and either awaiting them with `await` or scheduling
    them as tasks. The order in which current tasks will complete is never guaranteed.    Ultimately,
    asynchrony relies on an event loop to manage the execution of native coroutines
    and concurrent tasks. The Python standard library includes `asyncio` for this
    purpose, although this module is sometimes considered quite complex and obtuse.
    There are some much more intuitive alternatives, especially Trio. Alternatively,
    you could write your own custom event loop that’s fit to your particular purpose,
    if you’re feeling brave.**  **# 17 Threading and Parallelism  ![](Images/chapterart.png)  Before
    the advent of asynchrony in Python, you had two options to improve the responsiveness
    of your program: *threading* and *multiprocessing*. Although the two concepts
    are often seen as related, and even interchangeable in some languages, they couldn’t
    be more different in Python.    *Threading* is a means of achieving concurrency,
    which is useful in working around IO-blocking tasks, wherein the code is limited
    by the speed of something external, like user input, the network, or another program.
    It is not useful by itself for working around CPU-blocking tasks, wherein the
    magnitude of processing is the cause of code slowdowns.    *Parallelism* is a
    technique used to deal with CPU-blocking tasks, by running different tasks at
    the same time on separate CPU cores. *Multiprocessing* is the way we accomplish
    parallelism in Python. It was introduced to the language in Python 2.6.    Concurrency
    and parallelism are often essential when programming user interfaces, scheduling
    events, working with networks, and performing labor-intensive tasks in code.    Unsurprisingly,
    there is a lot to threading and multiprocessing, far beyond the conceivable scope
    of this book. This chapter will anchor you in the core concepts of concurrency
    and parallelism in Python. From there, you can consult the official documentation:
    [https://docs.python.org/3/library/concurrency.html](https://docs.python.org/3/library/concurrency.html).
    I’ll assume you have already read Chapter 16, as I’ll be reworking the Collatz
    example I introduced throughout that chapter.    ## Threading    A single sequence
    of instructions in a program is called a *thread of execution*, which is usually
    just referred to as a *thread*. Any Python program not written with concurrency
    or multiprocessing is contained within a single thread. *Multithreading*, typically
    just called *threading*, achieves concurrency by running multiple threads simultaneously
    in the same *process*, which is an instance of a running computer program.    In
    Python, only one thread can run at a time within a single process, so multiple
    threads have to take turns. Threading is also known as preemptive multitasking,
    because the operating system is *preempting*, or seizing control from, one running
    thread to give another thread a turn. This contrasts with asynchrony, also known
    as cooperative multitasking, wherein a particular task voluntarily gives up control.    While
    threading is mediated by the operating system, your code is responsible for starting
    the threads and managing the data they share. This is not a simple task, and a
    large portion of this chapter will focus on the difficulties of sharing data between
    threads.    ### Concurrency vs. Parallelism    Although they are often confused,
    concurrency and parallelism are not the same thing! According to Go co-creator
    Rob Pike, *concurrency* is the composition of multiple tasks, while *parallelism*
    involves running multiple tasks at the same time. Parallelism can be brought in
    as part of a concurrent solution, but you should first understand the concurrent
    design of your code before you invite multiprocessing to the party. (I highly
    recommend watching Pike’s talk “Concurrency is not Parallelism” from the Heroku
    Waza conference: [https://blog.golang.org/waza-talk/](https://blog.golang.org/waza-talk/).)    In
    many programming languages, threading also achieves parallelism as a side effect
    of the language and system architecture. This is part of the reason why many confuse
    concurrency with parallelism. However, Python’s Global Interpreter Lock prevents
    this implicit parallelism, since any Python process is constrained to run on a
    single CPU core.    ### Basic Threading    In Python, the `threading`, `concurrent.futures`,
    and `queue` modules provide all the classes, functions, and tools you’ll need
    to work with threading. I’ll use all three in this chapter.    To use threading
    effectively, first identify the IO-bound tasks in your code and isolate each such
    task behind a single function call. This design will make it easier to thread
    individual tasks later.    In the case of my Collatz example from Chapter 16,
    the function `get_input()` is IO-bound, since it waits on input from the user.
    The rest of the code is not IO-bound, so it can run synchronously. I want to run
    `get_input()` on a separate thread, so it can run concurrently with the rest of
    the program.    If you’re coding along with me, open a fresh copy of *collatz_sync.py*
    (Listings 16-1 through 16-6) in your code editor. In [Listing 17-1](#listing17-1),
    I have the original synchronous version of the `collatz()` and `length_counter()`
    methods. I import the `threading` module at the top, as I’ll be using functions
    from that module throughout this version of my program:    [PRE187]    Listing
    17-1: *collatz_threaded.py:1*    I need to iron out one little wrinkle in my prior
    design as I introduce threading: a function being run in a separate thread cannot
    return a value to the caller, but all my original functions return values. I therefore
    need a different solution for passing those values around.    The naive solution
    would be to create some sort of central location where the threaded function can
    store its data, and the most quick-and-dirty way to accomplish this is with a
    global name:    [PRE188]    Listing 17-2: *collatz_threaded.py:2a*    The `get_input()`
    function stores its return value in the new global name `guess` and then returns
    directly anyway.    Okay, yes, that design is repulsively non-Pythonic for this
    use case, and I’ll build a cleaner solution a bit later, but it’ll do for the
    moment. However, this version is analogous to a real-world pattern that *can*
    be Pythonic: you may need to allow threads to store data in a central, shared
    location, such as a database.    Now for the interesting part. I need to thread
    the function call—namely, `get_input()`—that I want to execute concurrently:    [PRE189]    Listing
    17-3: *collatz_threaded.py:3a*    The program can’t do anything until it gets
    the target value from the user, so I call `get_input()` the ordinary (synchronous)
    way the first time.    I want the *second* call to `get_input()`, wherein the
    user enters their guess, to run concurrently with the CPU-intensive Collatz calcuations.
    To run this concurrently, I create a thread with `threading.Thread()`. I pass
    the function to run in the thread to the `target=` keyword argument of `Thread()`.
    Any arguments that must be passed to the function being threaded have to be passed
    as a tuple to `args=`. (Note the trailing comma in the code above!) When the thread
    starts, it will call `get_input()` and pass it the arguments specified in `args=`.
    In this case, I am threading the call to `get_input()` and passing the input prompt
    message as a string.    I bind the thread object I created to `t_guess` and then
    start it in the background with `t_guess.start()`, so my code can continue as
    normal without waiting for the `get_input()` function to return.    Now I can
    start the CPU-intensive step of calling `length_counter()` synchronously. While
    I could thread this, too, there’s no point, as my program can’t really do anything
    else until `length_counter()` returns a value. Threads are quite a bit more expensive
    than asynchronous tasks, in terms of performance overhead for creating them. Therefore,
    you should only create threads when they provide a direct benefit to your program’s
    performance or perceived responsiveness.    Once `length_counter()` has finished,
    the program really can’t do anything more until the thread for `get_input()` is
    done working and has stored its return value in `guess`. I *join* the thread with
    `t_guess.join()`, meaning the code will wait for the thread to finish its work
    before continuing. If the thread is already done, the call to `t_guess.join()`
    it will immediately return.    From there, the program carries on as normal. If
    you run this complete program, you’ll find it has the same behavior as the asynchronous
    version in Chapter 16: the calculations happen while the program waits on the
    user to input their guess.    ### Timeouts    Notice that the program hangs while
    waiting for a thread to finish up. For [Listing 17-3](#listing17-3), this is perfectly
    safe, as any delay in this scenario would come from the user being slow about
    entering a value. However, you would be justified in being a bit wary of this
    indefinite suspension. If your thread is IO-bound from using a network connection
    or another system process, an unexpected error might cause the thread to never
    return! Your program would hang indefinitely, without an explanation or error
    message, until the operating system reports to your bewildered user, `The program
    has stopped responding`.    To help mitigate this issue, you can introduce a *timeout*.
    This specifies the maximum time until the program gives up waiting on a thread
    to join, after which, it carries on with or without it.    To demonstrate this,
    I’ll annoy my Collatz game user by making the program a bit impatient. (Yes, this
    is an absolutely terrible game design choice, but it’s better than dragging you
    through a fresh example, right?)    [PRE190]    Listing 17-4: *collatz_threaded.py:3b*    I
    pass the `timeout=1.5` keyword argument to `join()` to specify that, once the
    `join()` statement is reached, the program should only wait for one and a half
    seconds before continuing, regardless of whether the thread has finished. In effect,
    this means the user only has one and a half seconds to enter their answer.    Bear
    in mind, if the `join()` times out, it doesn’t actually affect the thread at all.
    It’s still running in the background. The main program just doesn’t wait around
    for it anymore.    To determine whether a timeout has occurred, I check whether
    the thread is still alive. If it is, I complain to the user and quit the program.    ###
    Daemonic Threads    Another consideration is that exiting the main thread will
    *not* terminate the other threads. In [Listing 17-4](#listing17-4), if I timed
    out waiting for the `t_guess` thread, even when I reached the return statement,
    ending the main flow of execution, that `t_guess` thread would keep running in
    the background indefinitely. That’s a problem, especially as the user would expect
    the program to have quit in its entirety.    Even so, Python deliberately supplies
    no obvious way of killing a thread, because doing so can result in some horrible
    effects, including utterly mangling your program state. However, without a way
    to abort the thread, it would hang forever after the complaint, and entering data
    would then do nothing. Once again, if the hanging thread were due to something
    like a network error, then either your program would go unresponsive, or the waiting
    thread would keep running in the background long after the main program was closed.    To
    mitigate this here, I make my thread *daemonic*, meaning I tie its lifespan to
    the lifespan of the process (the main program). When the main thread ends, all
    the associated daemonic threads are killed as well.    I define a thread as daemonic
    when I create it, by specifying the keyword argument `daemon=True`, as I did in
    [Listing 17-4](#listing17-4). Now, when I exit the main program, the thread is
    aborted too.    ### Futures and Executors    That quick-and-dirty global-name
    technique for passing things around is less than ideal for this example, largely
    because global names are too easily shadowed or improperly mutated. On the other
    hand, I don’t want to introduce side effects into my `get_input()` function by
    passing some mutable collection to store the data in instead. I need a more resilient
    way to return a value from the thread.    This is possible, thanks to *futures*,
    which are sometimes known as promises or delays in other languages. A future is
    an object that will contain a value at some point in the future but can be passed
    around like a normal object, even before it contains that value.    Carrying on
    with our current example, I import the `concurrent.futures` module that provides
    futures. Futures also provide a way to create threads directly, so I no longer
    need the `threading` module.    [PRE191]    Listing 17-5: *collatz_threaded.py:1b*    In
    this version, I won’t be needing the global name `guess` for storing the return
    value from `get_input()`, so I’ll just remove the two lines that use it:    [PRE192]    Listing
    17-6: *collatz_threaded.py:2b*    The function now looks like the synchronous
    version, although it will still work with threading via futures.    In my main
    method, I start the thread with a `ThreadPoolExecutor` object. This is a type
    of *executor*—an object that creates and manages threads:    [PRE193]    Listing
    17-7: *collatz_threaded.py:3c*    I create a new `ThreadPoolExecutor` and bind
    it to the name `executor`. This name is conventional for thread pools and other
    executors. Then, I create a new thread in that pool with `executor.submit()`,
    passing the function to be threaded, as well as all its arguments. Unlike when
    I’m instantiating from a `threading.Thread` object, I do *not* need to wrap the
    arguments in a tuple.    The call to `executor.submit()` returns a future, which
    I bind to the name `future_guess`. The future object will eventually contain the
    value returned by `get_input()`, but at the moment, it’s nothing more than a promise.    From
    here, I continue as normal, running those heavy calculations via `length_counter()`.    Once
    that’s done, I get the final value of the future with `future_guess.result()`.
    Like when joining a thread, this will hang until the thread returns a value.    After
    all the threads managed by the executor are done, I need to tell the executor
    to clean up after itself, which I accomplish with `executor.shutdown()`. This
    is safe to call, even before the threads are finished, as it will shut down the
    executor once all threads are finished. Once you’ve called `shutdown()` on an
    executor, attempting to start new threads with it will raise a `RuntimeError`.    A
    `with` statement can automatically shut down an executor in the same way it can
    automatically close files. This is useful, in case you forget to shut down your
    executor.    [PRE194]    Listing 17-8: *collatz_threaded.py:3d*    The call to
    `executor.shutdown()` occurs automatically at the end of the `with` statement.
    Any statements that should run concurrently with the thread(s) must be in the
    suite of the `with`, as it won’t be possible for main control flow to leave the
    `with` statement until all the threads are finished.    In this case, I chose
    to retrieve the result of the future after the thread pool has been shut down,
    outside of the `with` statement. This order isn’t strictly necessary, but it doesn’t
    hurt, since the thread pool’s preceding shutdown also implies the thread is done,
    so there will be no waiting to retrieve from the future.    ### Timeouts with
    Futures    The `result()` method on a future accepts a `timeout=` keyword argument,
    just as `join()` does on a `Thread` object. Unlike with `Thread` objects, you
    can determine if a timeout has occurred by catching the `concurrent.futures.TimeoutError`
    exception. However, this isn’t as simple as it seems. While you can time out on
    waiting, there remains the problem of stopping the hanging thread.    Here’s an
    example, although you probably shouldn’t run this, as it will hang forever:    [PRE195]    The
    trouble is, executors do not properly support daemonic threads and do not support
    them at all as of Python 3.9\. Executors also do not offer any mechanism for canceling
    a thread that is already running. In handling the `TimeoutError` above, I cancel
    any threads that haven’t started yet❶, but once a thread has been started by an
    executor, it cannot be stopped externally without some horrible and inexcusable
    hackery.    If a thread started by an executor might need to abort under some
    circumstances, you would need to plan ahead and write custom code for the thread
    to handle its own timeout internally. This is easier said than done, and in the
    case of `get_input()`, it is non-trivial and even approaches impossible. To create
    a timeout for user input, I have to stick with the thread-based technique.    ##
    Race Conditions    Race conditions are particularly hard to detect because a single
    line of code may hide many steps. For example, consider something as seemingly
    innocuous as incrementing an integer bound to a global name:    [PRE196]    Listing
    17-9: *increment.py:1a*    The augmented addition operator `+=` is not *atomic*,
    meaning it consists of multiple instructions under the hood. You can see this
    by disassembling the `increment()` function with the `dis` module:    [PRE197]    Listing
    17-10: *increment.py:1b*    Running [Listing 17-10](#listing17-10) produces the
    following output:    [PRE198]    The 7 in the leftmost column tells us that the
    bytecode here corresponds to the line `7` in the Python code, which is `count
    += 1`. All of these Python bytecode instructions, except the last two, take place
    on that one line of code! The value of `count` is read, the value `1` is added,
    and then the new value is stored. These three steps (across five instructions)
    must take place in uninterrupted succession. But consider what would happen if
    two threads both called `increment()` at the same time, as illustrated in [Table
    17-1](#table17-1).      Table 17-1: Model of Race Condition with Two Threads       |  |
    **Thread A** | **`count` (global)** | **Thread B** |  | | --- | --- | --- | ---
    | --- | | `0` | ← Read | `0` | *(Waiting)* |  | | `1` | Increment | `0` | *(Waiting)*
    |  | | `1` | *(Waiting)* | `0` | Read → | `0` | | `1` | Write → | `1` | *(Waiting)*
    | `0` | |  | *(Done)* | `1` | Increment | `1` | |  |  | `1` | ← Write | `1` |
    |  |  | `1` | *(Done)* |  |    Although two separate threads are supposed to increment
    the global `count` value, Thread B has read the value `0` from the global count
    before Thread A has a chance to write its updated value.    The worst thing about
    a race condition is that there is no such thing as a `RaceConditionError` exception
    that can be raised. There’s no error message and no linter error. Nothing is going
    to tell you that a race condition is happening—you can only determine it with
    some in-depth detective work. Since there’s no way to predict when threads will
    pause and resume, a race condition can hide in plain sight for ages until the
    perfect conditions arise for it to manifest. This accounts for a terrifying number
    of “can’t reproduce” bug reports.    ### A Race Condition Example    To demonstrate
    thread safety techniques, I’m going to rather uselessly thread the Collatz calculations.
    As mentioned before, concurrency will actually *slow down* CPU-bound tasks further.
    However, the threading pattern I’m about to apply would be useful if the functions
    involved had only been IO-bound. I’ll also use this scenario to demonstrate multiple
    concurrency techniques, although some of them are ill-suited to the situation.
    Roll with it.    To reliably demonstrate a race condition, I will need to create
    a class to serve as a counter. I’ll use this counter, instead of a normal integer,
    for storing global shared state between different threads working on the Collatz
    calculation. Again, this would be nonsense in real life, but it ensures I can
    reliably reproduce a race condition for demonstration purposes:    [PRE199]    Listing
    17-11: *collatz_pool.py:1a*    The likelihood of a race condition increases the
    more time there is between sequential steps in a process, like between reading
    and updating data. By slipping that `time.sleep()` call into the `increment()`
    class method ❶, I increase the time between calculating and storing the new count,
    and I thus practically guarantee the race condition will manifest.    Now, I’ll
    thread my `collatz()` function for this example and have it accept a target number
    as an argument. Every time the Collatz sequence generated by the function has
    the target number of values, I increment the `Counter` instead of returning a
    value:    [PRE200]    Listing 17-12: *collatz_pool.py:2a*    The situation is
    now prime for a race condition. Now, I only need to dispatch multiple threads—and
    my problem is aliiiiive! (Cue thunder and evil laughter.)    In the next section,
    I’ll make some adjustments to the code, merely to create the race condition I’m
    trying to demonstrate. Understand that the problem isn’t the threading technique
    itself. The code in the next section is valid. It’s the code in Listings 17-11
    and 17-12 that contains the real problem, and I’ll come back around to fixing
    it in a little while.    ### Creating Multiple Threads with ThreadPoolExecutor    To
    demonstrate the race condition, I will first do away with the `for` loop in my
    original `length_counter()` method and replace it with a `ThreadPoolExecutor`.
    This will allow me to dispatch a new thread for each individual Collatz sequence
    calculation:    [PRE201]    Listing 17-13: *collatz_pool.py:3a*    I start by
    resetting the `Counter` to `0` ❶. I define a `ThreadPoolExecutor` in a `with`
    statement, and I specify that it may run a maximum of five threads (also known
    as *workers*) at once ❷.    For this arbitrary example, I pulled this maximum
    of five workers out of thin air. The maximum number of workers you permit can
    have a significant impact on your program’s performance: too few workers won’t
    improve responsiveness enough, but too many can bloat overhead. When using threading,
    it’s worth doing a bit of trial and error to find the sweet spot!    I need to
    pass two arguments to the function `collatz()`: the target number of steps (`target`)
    and the sequence’s starting value (`n`). The target value never changes, but each
    value for `n` comes from an iterable, `range(2, BOUND)`.    The `executor.map()`
    method can dispatch multiple threads iteratively. However, this method is only
    able to pass a single value, provided by an iterable, to the given function. Since
    I’m trying to dispatch my `collatz()` function, which accepts two arguments, I
    need another way to handle the first argument. To accomplish this, I generate
    a callable object with `functools.partial()`, with the `target` argument effectively
    passed in advance ❸. I bind this callable object to `func`.    The `executor.map()`
    method uses the `range()` iterable to provide the values for the remaining `collatz()`
    argument via `func` ❹. Each resulting function call will take place in a separate
    worker thread. The rest of this program is the same as in Listings 17-6 and 17-8.    If
    you run this code as is, you’ll see that nasty race condition at work:    [PRE202]    My
    guess of `210` should be exactly right, but the race condition interfered so badly
    that the calculated result was wildly inaccurate. If you run this on your computer
    or update the `time.sleep()` duration, you might get a completely different number,
    maybe even coincidentally the right number at times. That unpredictability is
    why race conditions are so hard to debug.    Now that I’ve finished creating the
    problem, I can begin to fix it.    ## Locks    A *lock* can prevent race conditions
    by ensuring that only one thread can access a shared resource or perform an operation
    at a time. Any thread that wants to access a resource has to lock it first. If
    the resource already has a lock on it, the thread must wait until that lock is
    released.    I can resolve the race condition in the Collatz example by adding
    a lock to `Counter.increment()`:    [PRE203]    Listing 17-14: *collatz_pool.py:1b*    I
    create a new `Lock` object, which in this case, I bind to a class attribute, `_lock`
    ❶. Every time `Counter.increment()` is called, the thread will try to *acquire*
    (take ownership of) the lock with the lock’s `acquire()` method ❷. If another
    thread has ownership of the lock, any other call to `acquire()` will hang until
    that owning thread releases the lock. Once a thread acquires the lock, it can
    continue as before.    A thread must also *release* the lock, via the lock’s `release()`
    method, as soon as possible after finishing work with the protected resource,
    so other threads can continue ❸. *Every lock that is acquired must be released*.    Because
    of this requirement, locks are also context managers. Instead of manually calling
    `acquire()` and `release()` on my `Lock`, I can handle both implicitly via a `with`,
    like this:    [PRE204]    Listing 17-15: *collatz_pool.py:1c*    The `with` statement
    will acquire and release the lock automatically.    There’s nothing inherently
    magical about a `Lock`; it’s merely a glorified boolean value. Any thread can
    acquire an unowned `Lock`, and any thread can release a `Lock`, regardless of
    owner. You must ensure any code that would be prone to a race condition is hemmed
    in by the lock acquisition and release. Nothing prevents you from breaking the
    rules, but debugging such violations can be fraught with danger, or at least considerable
    annoyance.    ## Deadlock, Livelock, and Starvation    A *deadlock* situation
    occurs when the combined current status of your locks causes all your threads
    to wait, with no way forward. You can visualize a deadlock as two cars moving
    toward each other and blocking each other from crossing a one-lane bridge.    The
    similar *livelock* situation occurs when threads keep infinitely repeating the
    same interactions, instead of merely waiting, which also results in no real progress.
    Have you ever had a conversation with a significant other to the effect of “Where
    do you want to eat?” “I don’t care, where do you want to eat?” If so, you have
    experienced a real-world example of livelock. Both threads perform work involved
    in waiting for or deferring to one another, but neither thread gets anywhere.    Deadlocks
    and livelocks will usually cause your program to go unresponsive, often without
    any messages or errors to suggest why. Whenever you’re using locks, you must be
    extraordinarily careful to foresee and prevent deadlock and livelock scenarios.    To
    prevent deadlock and livelock, one must be mindful of potential circular wait
    conditions, in which two or more threads are all mutually waiting on one another
    to release resources. Because these are hard to depict believably in a code example,
    I’ll illustrate a common circular wait condition in [Figure 17-1](#figure17-1).  ![](Images/f17001.png)    Figure
    17-1: Deadlock between two processes      Threads A and B both require access
    to shared Resources X and Y at the same time. Thread A acquires a lock on Resource
    X, while Thread B concurrently acquires a lock on Resource Y. Now, Thread A is
    waiting on Lock B, and Thread B is waiting on Lock A. They’re deadlocked.    The
    right way to resolve a deadlock or livelock will always depend on your particular
    situation, but you have a couple of tools at your disposal. First, you can specify
    a `timeout=` on the lock’s `acquire()` method; if the call times out, it will
    return `False`. In the situation in [Figure 17-1](#figure17-1), if either thread
    encountered such a timeout, it could then release any locks it held, thereby allowing
    the other thread to proceed.    Second, any thread can release a lock, so you
    can forcibly break the deadlock if necessary. If Thread A recognizes a deadlock,
    it could release Thread B’s lock on Resource Y and proceed anyway, thereby breaking
    the deadlock. The difficulty with this method is that you risk breaking a lock
    even if you’re *not* in deadlock, which creates a race condition.    Locks are
    not the only culprits in a deadlock or livelock scenario. *Starvation* occurs
    when a thread is stuck waiting for a future or to join a thread, especially to
    acquire some data or resource it needs, but that thread or future never returns
    for some reason. This can even occur if two or more futures or threads wind up
    waiting for one another to complete.    A thread can even deadlock itself! If
    a single thread tries to acquire a `Lock` twice in a row without releasing it
    first, then it is stuck waiting for itself to release the lock it’s waiting on!
    If there’s any risk of this situation arising, you can use `threading.RLock`,
    instead of a `threading.Lock`. With an `RLock`, a single thread can acquire the
    same lock multiple times without deadlocking, and only the thread that acquired
    the lock may release it. While you still must release as many times as you acquire,
    a thread cannot directly deadlock itself with an `RLock`.    There is one catch:
    because an `RLock` may only be released by the owning thread, it’s much harder
    to break a multithread deadlock with an `RLock` than with an ordinary `Lock`.    ##
    Passing Messages with Queue    You can sidestep the risk of race conditions and
    deadlocks by *passing messages*, at the cost of a bit more memory overhead. This
    is safer than using futures or a shared data source. Anytime futures don’t work
    for your situation, your default strategy for exchanging and collating data with
    multiple threads should be to have those threads pass messages.    You would typically
    pass messages between threads with a queue. One or more threads can push data
    to the queue, and one or more other threads can pull data from the queue. This
    is analogous to a waiter passing written orders to a kitchen in a restaurant.
    Neither the sender nor the receiver needs to wait on the other, unless the message
    queue is full or empty.    Python’s standard library includes the `queue` module,
    which provides collections that already implement thread safety and proper locking,
    thus negating the risk of deadlocks. Alternatively, you can use `collections.deque`
    in the same way for passing messages, because that collection has atomic operations
    like `append()` and `popleft()`, which make locking unnecessary.    I’ll update
    my *collatz_pool.py* example to pass results from worker threads back to the main
    thread via a queue, rather than using a shared object.    [PRE205]    Listing
    17-16: *collatz_pool.py:1d*    I import the `queue` module and remove that pesky
    `Counter`.    Next, I’ll adjust my `collatz()` function to push the results onto
    the queue:    [PRE206]    Listing 17-17: *collatz_pool.py:2b*    I accept a `queue.Queue`
    object on the `results` parameter, and I add an item to that collection via `results.put()`.    My
    design decisions here are deliberate. Data should only flow in one direction via
    the queue, either input to the worker threads or output from the worker threads.
    Most design patterns involving queues react to the queue being empty, non-empty,
    or full, rather than to the contents of the data. If you try to create a queue
    for moving multiple types of data, it’s all too easy to create a starvation or
    infinite loop situation.    In this case, the worker threads running `collatz()`
    will push their output data to the queue, and `length_counter()` will pull that
    data in from the queue. If I had needed two-way communication, I would have implemented
    a second queue to handle data flow in the other direction.    Each worker thread
    running `collatz()` must also have a dedicated queue for storing results, lest
    concurrent calls mix up their results. To do this, I pass the queue as an argument,
    instead of binding it to a global name. Although this technically violates the
    “no side effects” principle, it’s acceptable because the queue is intended purely
    as a data transfer medium.    Here’s my updated `length_counter()` method, using
    the queue:    [PRE207]    Listing 17-18: *collatz_pool.py:3b*    I create the
    queue object and pass it to each of the workers ❶ in the same way I passed `target`
    in [Listing 17-13](#listing17-13). The workers will append the length of each
    generated Collatz sequence to the queue. Once they’re all done, I convert the
    `results` queue to a list and return the number of times the target appears in
    that queue.    ## Futures with Multiple Workers    As I mentioned before, you
    can also solve deadlocks with futures. In fact, that’s the best option for avoiding
    a deadlock in this multithreaded Collatz example. Futures have little risk of
    deadlocking, as long as you avoid having multiple threads waiting on futures from
    one another.    I’m revising my deadlock example further, to implement futures.
    I only need to import the `concurrent.futures` module to use this technique:    [PRE208]    Listing
    17-19: *collatz_**pool.py:1e*    I can also restore my `collatz()` method to its
    original form, where I am only returning a single value:    [PRE209]    Listing
    17-20: *collatz_pool.py:2c*    The `executor.map()` method returns an iterable
    of futures, which I can use to collect the return values from the worker threads:    [PRE210]    Listing
    17-21: *collatz_pool.py:3c*    I iterate over each of the values returned by `executor.map()`
    and count how many of those values match the target. The `executor.map()` method
    is essentially a threaded drop-in replacement for the built-in `map()` function;
    while the input processing order is not guaranteed, the output order is. With
    most other techniques, you cannot rely on the order in which values are returned.    This
    is the cleanest approach of all, with minimal overhead.    ## Achieving Parallelism
    with Multiprocessing    Since Python 2.6, parallelism is possible via *multiprocessing*,
    wherein parallel tasks are handled by entirely separate system processes, each
    with its own dedicated instance of the Python interpreter. Multiprocessing bypasses
    the limitations imposed by Python’s Global Interpreter Lock (GIL), since each
    process has its own Python interpreter, and thus its own GIL. This allows a single
    Python program to employ parallelism. A computer can run these multiple processes
    simultaneously by distributing them among different CPU cores, to be worked on
    at the same time. How processes are divided up among cores is the prerogative
    of the operating system.    Remember that multiprocessing has performance costs
    of its own, so merely adding it to your code will not automatically make everything
    faster. Improving performance requires dedicated thought and work. As with threading
    and asynchrony, you must carefully consider your code’s design when implementing
    multiprocessing. You’ll see these principles in action shortly.    Multiprocessing
    in Python follows a very similar structure to threading. `Process` objects are
    used exactly like `Thread` objects. The `multiprocessing` module also provides
    classes like `Queue` and `Event`, which are analogous to their threading-based
    cousins but specifically designed for multiprocessing. The `concurrent.futures`
    module provides `ProcessPoolExecutor`, which looks and acts much the same as `ThreadPoolExecutor`
    and makes it possible to use futures.    ### Pickling Data    Some Python developers
    recoil at the thought of using `multiprocessing` for one reason: it uses `pickle`
    behind the scenes. If you recall from Chapter 12, `pickle` is extraordinarily
    slow and insecure as a data serialization format, so much so that Pythonistas
    avoid it like the plague, with good reason.    Even so, `pickle` does work reasonably
    well in the context of multiprocessing. First, we don’t need to worry about `pickle`
    being insecure, because it is being used to transfer data directly between active
    processes started and managed by your code, so that data is considered trusted.
    Second, many of `pickle`’s performance issues are offset by the tangible performance
    gains afforded by parallelism and the fact that the serialized data is never being
    written to a file, which is itself a CPU-intensive task.    Moreover, because
    `pickle` is used in multiprocessing, it is still actively maintained and improved;
    Python 3.8 saw the implementation of `pickle` protocol 5\. You typically don’t
    need to worry about `pickle` when using multiprocessing; it’s merely an implementation
    detail most of the time.    It is important to remember that data must be *picklable*,
    meaning it can be serialized by the `pickle` protocol, for it to be passed between
    processes. According to the documentation, you can pickle the following data types:    1.  `None`
    2.  `True` and `False` 3.  Integers 4.  Floating-point numbers 5.  Complex numbers
    6.  Strings 7.  Bytes-like objects 8.  Tuples, lists, sets, and dictionaries only
    containing picklable objects 9.  Functions (*but not lambdas*) at global scope
    10.  Classes at global scope, with additional requirements    For a class to be
    picklable, all of its instance attributes must be picklable and stored in the
    instance `__dict__` attribute. When a class is pickled, methods and class attributes
    are omitted, along with anything else in the class `__dict__` attribute. Alternatively,
    if a class uses slots or otherwise cannot fulfill this criteria, you can make
    it picklable by implementing the special instance method `__getstate__()`, which
    should return a picklable object. Typically, this would be a dictionary of picklable
    attributes. If the method returns `False`, it signals the class as unpicklable.    You
    can also implement the special instance method `__setstate__(state)`, which accepts
    an unpickled object, which you would unpack into the instance attributes as appropriate.
    This is a more time-consuming approach, but it’s a good way around the restrictions.
    If you don’t define this method, one will be created automatically that accepts
    a dictionary and assigns it directly to the instance’s `__dict__` attribute.    If
    you’re going to work a lot with picklable data, especially in the context of multithreading,
    it may be helpful to see the official documentation for the `pickle` module: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html).    ###
    Speed Considerations and ProcessPoolExecutor    Multiprocessing gets around the
    GIL, allowing the code to use multiple processes, so you might assume it will
    speed up that CPU-bound activity of calculating the Collatz sequences. Let’s test
    out that idea.    Continuing from the previous example, I can make use of multiprocessing
    by swapping my `ThreadPoolExecutor` to a `ProcessPoolExecutor`. I can reuse Listings
    17-19 and 17-20 (not shown below) and modify [Listing 17-21](#listing17-21) to
    look like this:    [PRE211]    Listing 17-22: *collatz_multi.py:3a*    All I needed
    to do was to replace `ThreadPoolExecutor` with `ProcessPoolExecutor`. Here, I
    didn’t specify `max_workers` on the `ProcessPoolExecutor`, so it defaults to one
    worker per processor core on the machine. I happen to be on an 8-core machine,
    so when I run this code on my machine, the `ProcessPoolExecutor` will default
    to a `max_workers` value of 8\. Your machine may be different.    I’m still using
    Listings 17-6 and 17-8 for the rest of the program, which threads the IO-bound
    task of getting user input. There’s no sense in using multiprocessing to handle
    an IO-bound task.    If I run this program, however, it is actually the slowest
    version yet! My computer has an Intel i7 8-core processor, but it took a whopping
    *21 seconds* to get the results. The version with the `ThreadPoolExecutor` and
    futures ([Listing 17-18](#listing17-18)) took 8 seconds, and the version that
    didn’t thread the calculations at all ([Listing 17-1](#listing17-1)) took less
    than 3 seconds.    Rest assured, the code is indeed creating multiple *subprocesses*—separate
    processes linked to the main process—and bypassing the GIL. The problem is, subprocesses
    themselves are extremely expensive to create and manage! The overhead of multiprocessing
    is outweighing any performance gains I might have.    Alternatively, I could drop
    the threading on the IO-bound task and instead move the CPU-bound task out to
    a single subprocess. However, that results in roughly the same performance as
    seen with threading alone, negating the point of multiprocessing here.    One
    cannot simply throw parallelism at a problem and expect the code to run faster.
    Effective multiprocessing requires planning. In order to take proper advantage
    of multiprocessing, I need to give each subprocess a reasonable amount of work
    to do. As you’ve seen, if I create too many subprocesses, the overhead of the
    multiprocessing nullifies any performance gains. If I create too few, there will
    be little to no difference from running it on a single subprocess.    I don’t
    want to create a new subprocess for each call to `collatz()`, as I did before—a
    hundred thousand subprocesses is a huge strain on the system resources! Instead,
    I’ll divide those among four separate subprocesses, each of which performs a quarter
    of the work. I can do this by *chunking*: defining how much of the work is given
    to a single subprocess:    [PRE212]    Listing 17-23: *collatz_multi.py:3b*    Within
    the `executor.map()` method, I use the keyword argument `chunksize` to specify
    that roughly one-quarter of the values being passed should go to each subprocess.    When
    I run the code, I find this version to be the fastest yet! With a `BOUND = 10**5`,
    it completes almost instantaneously. If I increase `BOUND` to `10**6`, this version
    takes 5 seconds, versus 16 seconds for the final version of *collatz_threaded.py*.
    I can play with the chunking to find the ideal value, which is indeed `4`; any
    higher value in this scenario won’t make the program run faster than 5 seconds,
    and a value smaller than `4` is slower.    By carefully applying parallelism,
    I can bypass the GIL and speed up CPU-bound tasks.    ## The Producer/Consumer
    Problem    In parallelism and concurrency, one often classifies threads or processes
    as either *producers*, which provide data, or *consumers*, which intake that data
    and process it. It’s possible for a thread or process to be both a producer and
    a consumer. The *producer/consumer problem* is a common situation in which one
    or more producers provide data, which is then processed by one or more consumers.
    The producers and consumers are working independently of one another, at potentially
    different speeds. The producer might produce values faster than the consumers
    can process them, or the consumers might process the data faster than it is produced.
    The challenge is preventing the queue of data being processed from getting too
    full, lest the consumers wait forever because they don’t know whether the producer
    is slow or finished.    I’ll model the producer/consumer problem with the Collatz
    example, merely so I don’t need to build up a brand-new example. I’ll have one
    producer provide the starting values of the Collatz sequences, and I’ll have four
    consumers working in parallel to generate the sequences from those starting values
    and to determine how many have the target number of steps. (In practice, this
    pattern is overkill for something as simple as the Collatz example.)    At first
    blush, this program seems like it should be easy to write: create a queue, fill
    it with the starting values, and then start four subprocesses on the executor
    to take values out of that queue. In practice, there are a few problems that need
    to be solved.    First, I cannot wait until the producer has produced all the
    values before processing. If I tried to pack all the values into the queue at
    once, I’d have a whopping 3.8 MiB if I set my `BOUND` to `10**7`; real-world examples
    of the producer-consumer problem can involve gigabytes or terabytes of data. Instead,
    the producer should provide more values only when there’s space in the queue for
    them.    Second, the consumers need to know the difference between the queue being
    empty because the values are exhausted and the queue being empty because the producer
    is working on adding more. If something goes wrong to prevent values being added—or,
    conversely, if something prevents the consumers from processing values—then the
    code should be able to handle that error gracefully, rather than waiting forever
    for something to happen. Similarly, when the program is ready to end, each thread
    or subprocess should clean up after itself—closing files and streams, for example—rather
    than aborting abruptly.    Third, and perhaps hardest to implement, producers
    and consumers must be reentrant. In other words, if a consumer is paused mid-execution
    and another consumer is started before the first is resumed, the two consumers
    should not interfere with one another. Otherwise, they may be in danger of deadlocking
    (or livelocking) in subtle and unexpected ways.    ### Importing the Modules    I’m
    going to need quite a few modules to accomplish this program:    [PRE213]    Listing
    17-24: *collatz_producer_consumer.py:1*    As before, the `concurrent.futures`
    module allows me to work with both threads and processes. The `multiprocessing`
    module provides the parallelism-specific versions of concurrency classes, including
    `multiprocessing.Queue` and `multiprocessing.Event`. The `queue` module provides
    the exceptions related to `Queue`. I’ll also need `repeat` from `itertools` when
    dispatching consumers in my particular use case.    The `signal` module, which
    may be new to you, allows me to asynchronously monitor for and respond to process
    control signals coming from the operating system. This is important to ensuring
    that all my subprocesses shut down cleanly. I’ll come back to this in a moment.    ###
    Monitoring the Queue    I need two shared objects from the `multiprocessing` module
    for the producer/consumer model to work. The `Queue` stores the data being passed
    from producer to consumer, and `Event` signals when no more data will be added
    to the `Queue` by the producer.    [PRE214]    Listing 17-25: *collatz_producer_consumer.py:2*    I’m
    using global names for these two objects because, unlike with threads, you cannot
    pass shared objects via arguments when dispatching subprocesses.    The producer
    subprocess will fill the queue with starting values, and the consumer subprocesses
    will read from it. The `multiprocessing.Queue` class has atomic `put()` and `get()`
    methods, like `queue.Queue`, so it isn’t prone to race conditions. I pass the
    argument `100` to the `Queue` initializer here, which sets the queue to hold a
    maximum of `100` items. The maximum is somewhat arbitrary; you should experiment
    for your use cases.    The `exit_event` object is an *event*, a special flag that
    processes can monitor and react to. In this case, the event signals either that
    the producer will be adding no more values to the queue, or that the program itself
    has been aborted. The `threading` module provides an analogous `Event` object
    for concurrency.    You’ll see both of these at work shortly.    ### Subprocess
    Cleanup    It is each subprocess’s duty to clean up after itself and shut down
    properly. When the producer is finished producing data, the consumers need to
    process the remaining data in the queue and then clean themselves up. Similarly,
    if the main program is aborted, the subprocesses must all be informed so they
    can shut themselves down.    To handle both of these situations, I’ll write my
    consumer and producer functions to monitor and respond to `exit_event`.    [PRE215]    Listing
    17-26: *collatz_producer_consumer.py:3*    I define an *event handler function*,
    `exit_handler()`, which is designed to respond to operating system events. The
    event handler function must accept two arguments: the *signal number* that corresponds
    to a particular system event and the current *stack frame*, which is roughly the
    current location and scope in the code. Both values will be provided automatically,
    so there’s no need to worry about figuring out what they should be.    The `exit_handler()`
    function will set `exit_event` ❶. Later, I’ll write my processes to clean up and
    shut down in response to `exit_event`.    I attach the handler to the two signals
    I want to monitor: `signal.SIGTERM` occurs when the main process terminates ❷,
    and `signal.SIGINT` occurs when it is interrupted, such as with Ctrl-C on POSIX
    systems ❸.    ### Consumers    Here’s my usual `collatz()` function, as well as
    my consumer function:    [PRE216]    Listing 17-27: *collatz_producer_consumer.py:4*    The
    `collatz_consumer()` function accepts one argument: the `target` it is looking
    for. It loops infinitely, until explicitly exited with a `return` statement.    On
    each iteration, the consumer function checks whether the shared queue, `in_queue`,
    has anything in it. If it does, the function attempts to get the next item from
    the queue with `in_queue.get()`, which waits until there’s an item in the queue.
    I provide a `timeout` of one second ❶, which is important to prevent deadlocks
    and to give the subprocess an opportunity to check and respond to events. One
    second is more than ample time for my producer process to put a new item into
    the queue. If `in_queue.get()` times out, the exception `queue.Empty` is raised,
    and I exit the subprocess immediately.    If the consumer is able to get a value
    from the queue, it runs the value through `collatz()`, checks the returned value
    against `target`, and updates `count` accordingly.    Finally, I check if `exit_event`
    is set; if it is, I return `count`, ending the subprocess cleanly.    ### Checking
    for an Empty Queue    If you look at the documentation for `multiprocessing.Queue.empty()`
    (`queue.empty()` in [Listing 17-27](#listing17-27)) and similar methods, there’s
    a rather ominous statement:    > Because of multithreading/multiprocessing semantics,
    this is not reliable.    This does not mean you cannot use the `empty()` method.
    The “unreliability” has to do with the dynamics I’ve already described at work
    in concurrency. By the time the subprocess determines that the queue is empty
    via the `empty()` method, it might *not* be empty anymore, since the producer
    is operating in parallel. This is okay, however. There’s no harm in another iteration
    passing if the timing is off.    This dynamic really only becomes treacherous
    if you rely on `empty()` to ensure you can `put()` a new item in the queue. The
    queue might be filled up before the subprocess even reaches `put()`, even if it’s
    the next statement. The same is true of checking `full()` before calling `get()`.
    That’s why I invert the logic and check that the queue *is not* empty, while still
    wrapping the `get()` statement in a `try` clause.    ### Producers    Enough about
    the consumer. It’s time to put some values in that queue with the producer. The
    producer function for the Collatz example pushes values to the queue when there’s
    an empty spot to do so:    [PRE217]    Listing 17-28: *collatz_producer_consumer.py:5*    In
    addition to pushing values to the queue when it isn’t full, the producer function
    always checks if `exit_event` is set, so it can exit cleanly as soon as possible
    ❶.    I next try to put a value on the queue with `put()` ❷. In this particular
    example, if the operation takes more than a second, I want it to time out and
    thus indicate that the consumers have deadlocked or crashed in some way. If there’s
    a timeout, the `queue.Full` exception is raised, and I set the `exit_event` ❸
    and end the subprocess. Bear in mind, each situation will have its own ideal timeouts,
    which you’ll have to figure out for yourself. It’s better to have too long a timeout,
    rather than one that’s too short.    If the loop reaches its end without a timeout,
    I don’t want to set the `exit_event` right away, as that might cause the consumers
    to quit too early and not process some waiting items. Instead, I loop infinitely,
    checking whether the queue is empty. I can rely on the `empty()` method here to
    inform me when the consumers are finished processing the data, since this is the
    only producer adding values to the queue. On each iteration here, I also sleep
    for a few milliseconds, so the producer doesn’t chew through processing power
    while it waits. Once the queue is empty, I set the `exit_event`. Then I exit the
    function, ending the subprocess.    ### Starting the Processes    Now that I have
    written my producers and consumers, it’s time to dispatch them. I’ll run the producer
    and all four consumers as subprocesses, which I start with a `ProcessPoolExecutor`:    [PRE218]    Listing
    17-29: *collatz_producer_consumer.py:6*    I submit my producer function from
    [Listing 17-28](#listing17-28), `range_producer()`, to a single subprocess ❶.    I
    use `executor.map()` as a convenient way to dispatch multiple consumer subprocesses
    ❷, but I don’t need to iteratively provide any data, which is the usual purpose
    of `map()`. Since that function requires an iterable as its second argument, I
    use `itertools.repeat()` to create an iterator providing exactly four copies of
    the value `target` ❸. The values in this iterable will be mapped to four separate
    subprocesses.    Finally, I collect and sum all the counts returned by the finished
    consumer subprocesses via `results` ❹. Because this statement is outside of the
    `with` statement’s suite, it will only run once the producer subprocess and all
    four consumer subprocesses have exited.    I’ve designed my code architecture
    to work with a single producer. If I wanted more than one producer subprocess,
    I would need to refactor the code.    As before, I’m using Listings 17-6 and 17-8
    for the rest of the program, which will still be threaded.    ### Performance
    Results    Running the code in [Listing 17-29](#listing17-29) is a bit slower
    than running the version in [Listing 17-23](#listing17-23)—five seconds on my
    machine for a `BOUND` of `10**5`, versus the nearly instant return of the previous
    version—but you can see that it works as expected:    [PRE219]    I chose `128`
    as my test target, as that’s the length of the Collatz sequence starting with
    the value `10**5`, which was the last value provided. This allowed me to confirm
    that the consumer subprocesses didn’t exit before the queue was empty. There are
    608 Collatz sequences that are 128 steps long, and that is what is reported after
    a few seconds.    Be aware that the design of this particular code isn’t necessarily
    going to work for your producer/consumer scenario. You will need to carefully
    consider how messages and data are passed, how events are set and checked, and
    how subprocesses (or threads, for that matter) clean up after themselves. I highly
    recommend reading “Things I Wish They Told Me About Multiprocessing in Python”
    by Pamela McA’Nulty: [https://www.cloudcity.io/blog/2019/02/27/things-i-wish-they-told-me-about-multiprocessing-in-python/](https://www.cloudcity.io/blog/2019/02/27/things-i-wish-they-told-me-about-multiprocessing-in-python/).    ###
    Logging with Multiprocessing    There’s one piece of multiprocessing that I’ve
    chosen to skip over. Normally, when working with multiple processes, you’d use
    a logging system that includes timestamps and unique identifiers for processes.
    This is extraordinarily helpful in debugging parallelized code, especially as
    most other debugging tools will not work across processes. Logging will allow
    you to see the status of different processes at any given moment in time, so you
    can tell which ones are working, which are waiting, and which have crashed. I’ll
    cover logging in general in Chapter 19, and you can fit it to your purposes.    ##
    Wrapping Up    There are many helpful tools for threading and multiprocessing
    that I simply didn’t have the opportunity to cover here. Now that you have a grasp
    of the fundamentals of concurrency and parallelism in Python, I highly recommend
    skimming through the official documentation for the modules I’ve been using, as
    well as two I’ve skipped:    **`concurrent.futures`**    [https://docs.python.org/3/library/concurrent.futures.html](https://docs.python.org/3/library/concurrent.futures.html)    **`queue`**    [https://docs.python.org/3/library/queue.html](https://docs.python.org/3/library/queue.html)    **`multiprocessing`**    [https://docs.python.org/3/library/multiprocessing.html](https://docs.python.org/3/library/multiprocessing.html)    **`sched`**    [https://docs.python.org/3/library/sched.html](https://docs.python.org/3/library/sched.html)    **`subprocess`**    [https://docs.python.org/3/library/subprocess.html](https://docs.python.org/3/library/subprocess.html)    **`_thread`**    [https://docs.python.org/3/library/_thread.html](https://docs.python.org/3/library/_thread.html)    **`threading`**    [https://docs.python.org/3/library/threading.html](https://docs.python.org/3/library/threading.html)     Remember
    also that asynchrony libraries have analogous structures and patterns for nearly
    everything in threading, including locks, events, pools, and futures. Most of
    the time, if you can thread it, you can write it asynchronously.    In summary,
    there are three primary ways to multitask in Python: asynchrony (Chapter 16),
    threading, and multiprocessing. Asynchrony and threading are both for working
    with IO-bound tasks, while multiprocessing is for speeding up CPU-bound tasks.    Even
    so, remember that concurrency and parallelism are not magic bullets! They can
    improve your program’s responsiveness and, at least in the case of multiprocessing,
    speed up execution times, but always at the cost of added complexity and the risk
    of some pretty snarly bugs. They demand careful planning and code architecture,
    and it’s easy to get their usage wrong. I spent considerable time writing, testing,
    debugging, and fighting with the Collatz examples until I got them right—more
    so than with any other examples in this book! Even now, you may discover flaws
    in the design that I overlooked.    Concurrency and parallelism are essential
    when programming user interfaces, scheduling events, and performing labor-intensive
    tasks in code. Even so, there’s often nothing wrong with old-fashioned synchronous
    code. If you don’t need the extra power, then avoid the extra complexity.    For
    further understanding, you may want to look up other concurrency and parallelism
    patterns, such as the `subprocess` module, job queues like `celery`, and even
    external operating system utilities like `xargs -P` and `systemd`. There is far
    more to this than can fit into one chapter. Always carefully research the options
    for your particular situation.**[PRE220]``**'
  prefs: []
  type: TYPE_NORMAL
