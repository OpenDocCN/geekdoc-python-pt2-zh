## **11**

**可扩展性与架构**

![image](img/common01.jpg)

迟早，你的开发过程将不得不考虑弹性和可扩展性。应用程序的可扩展性、并发性和并行性在很大程度上取决于其初始架构和设计。正如本章所见，有些范式——例如多线程——并不适合 Python，而其他技术，如面向服务架构，则更为有效。

涵盖可扩展性全貌需要一本书的篇幅，实际上已经有很多书籍涉及这一主题。本章将介绍可扩展性的基本原理，即使你不打算构建拥有数百万用户的应用程序。

### **Python 中的多线程及其局限性**

默认情况下，Python 进程仅在一个线程上运行，称为*主线程*。该线程在单个处理器上执行代码。*多线程*是一种编程技术，通过同时运行多个线程，使代码在单个 Python 进程中并发执行。这是我们在 Python 中引入并发的主要机制。如果计算机配备了多个处理器，你甚至可以使用*并行性*，在多个处理器上并行运行线程，以加速代码执行。

多线程最常用的场景（尽管并不总是合适）是：

+   你需要在不停止主线程执行的情况下运行后台或面向 I/O 的任务。例如，图形用户界面的主循环忙于等待事件（例如用户点击或键盘输入），但代码仍需要执行其他任务。

+   你需要将工作负载分配到多个 CPU 上。

第一个场景是多线程的一个好例子。尽管在这种情况下实现多线程会引入额外的复杂性，但控制多线程是可管理的，除非 CPU 工作负载非常密集，否则性能不会受到影响。当 I/O 密集型任务的 I/O 延迟较高时，使用并发会带来更大的性能提升：你需要等待读写操作时，越是能在这段时间里做其他事情，就越能提高效率。

在第二种情况下，你可能希望为每个新请求启动一个新线程，而不是一个一个地处理它们。这看起来像是使用多线程的一个好例子。然而，如果你像这样分配工作负载，你会遇到 Python 的*全局解释器锁（GIL）*，这是一个每次 CPython 需要执行字节码时必须获取的锁。这个锁意味着在任何时刻，只有一个线程可以控制 Python 解释器。这个规则最初是为了防止竞争条件而引入的，但不幸的是，这意味着如果你尝试通过让应用程序运行多个线程来进行扩展，你将始终受到这个全局锁的限制。

所以，虽然使用线程看起来是理想的解决方案，但大多数在多个线程中运行请求的应用程序很难达到 150%的 CPU 使用率，或者相当于 1.5 个核心的使用率。大多数计算机有 4 个或 8 个核心，服务器提供 24 个或 48 个核心，但 GIL（全局解释器锁）限制了 Python 无法充分利用 CPU。虽然有一些计划在进行中，以移除 GIL，但这个工作非常复杂，因为它需要在性能和向后兼容性之间做出权衡。

虽然 CPython 是 Python 语言最常用的实现，但也有其他实现没有 GIL。例如，*Jython*可以有效地并行运行多个线程。不幸的是，像 Jython 这样的项目天生落后于 CPython，因此并不是非常有用的目标；创新发生在 CPython 中，其他实现只是跟随 CPython 的步伐。

所以，让我们根据现在所知道的，重新审视我们的两个用例，并找出一个更好的解决方案：

+   当你需要运行后台任务时，你*可以*使用多线程，但更简单的解决方案是围绕事件循环构建你的应用程序。有许多 Python 模块提供这种功能，当前的标准是 asyncio。也有一些框架，如 Twisted，围绕同样的概念构建。最先进的框架将使你能够基于信号、定时器和文件描述符活动来访问事件——我们将在本章的“事件驱动架构”中进一步讨论，具体在第 181 页。

+   当你需要分散工作负载时，使用多个进程是最有效的方法。我们将在下一节中探讨这一技术。

开发人员在使用多线程时应该始终三思而后行。举个例子，我曾经在我几年前编写的 Debian 构建守护进程 rebuildd 中使用过多线程来分配任务。虽然有一个独立的线程来控制每个正在运行的构建任务似乎很方便，但我很快就陷入了 Python 中线程并行的陷阱。如果我有机会重新开始，我会基于异步事件处理或多进程来构建，而不是担心 GIL。

多线程是复杂的，要使多线程应用程序正确运行非常困难。你需要处理线程同步和锁定，这意味着有很多引入错误的机会。考虑到总体增益较小，在投入过多精力之前最好再三考虑。

### **多进程与多线程**

由于 GIL 限制了多线程作为可扩展解决方案的有效性，可以考虑 Python 的*多进程*包提供的替代解决方案。这个包暴露了你在使用多线程模块时能够获得的相同接口，只是它启动的是新的*进程*（通过 os.fork()），而不是新的系统线程。

示例 11-1 展示了一个简单的例子，其中对一百万个随机整数进行了八次求和，并且这一操作同时在八个线程上分布执行。

import random

import threading

results = []

def compute():

results.append(sum(

[random.randint(1, 100) for i in range(1000000)]))

workers = [threading.Thread(target=compute) for x in range(8)] for worker in workers:

worker.start()

for worker in workers:

worker.join()

print("结果: %s" % results)

*示例 11-1：使用多线程进行并发操作*

在示例 11-1 中，我们使用 threading.Thread 类创建了八个线程，并将它们存储在 workers 数组中。这些线程将执行 compute()函数。然后，它们使用 start()方法启动线程。join()方法只有在线程执行完毕后才会返回。在这个阶段，结果可以被打印出来。

运行这个程序返回以下结果：

$ time python worker.py

结果：[50517927, 50496846, 50494093, 50503078, 50512047, 50482863,

50543387, 50511493]

python worker.py  13.04s 用户 2.11s 系统 129% CPU 11.662 总计

这是在一个空闲的四核 CPU 上运行的，这意味着 Python 本来可以利用最多 400%的 CPU。但这些结果表明，即使同时运行了八个线程，它也显然未能达到这一点。相反，CPU 使用率最多为 129%，这只是硬件能力的 32%（129/400）。

现在，让我们使用*多进程*重写这个实现。对于像这样的简单案例，切换到多进程是相当直接的，如示例 11-2 所示。

import multiprocessing

import random

def compute(n):

return sum(

[random.randint(1, 100) for i in range(1000000)])

# 启动 8 个工作进程

pool = multiprocessing.Pool(processes=8)

print("结果: %s" % pool.map(compute, range(8)))

*示例 11-2：使用多进程进行并发操作*

多进程模块提供了一个 Pool 对象，它接受一个参数，该参数是要启动的进程数量。它的 map()方法与原生 map()方法的工作方式相同，不同之处在于不同的 Python 进程将负责执行 compute()函数。

在与示例 11-1 相同的条件下运行示例 11-2 程序，得到以下结果：

$ time python workermp.py

结果：[50495989, 50566997, 50474532, 50531418, 50522470, 50488087, 0498016, 50537899]

python workermp.py  16.53s 用户 0.12s 系统 363% CPU 4.581 总计

多进程减少了 60%的执行时间。此外，我们还能够消耗高达 363%的 CPU 功率，这比计算机 CPU 容量的 90%（363/400）还要高。

每当你认为可以并行化某些工作时，几乎总是更好地依赖多进程，并通过 fork 你的工作，以便将工作负载分布到多个 CPU 核心上。这对非常短的执行时间来说不是一个好解决方案，因为 fork() 调用的开销会太大，但对于更大的计算需求来说，它能很好地工作。

### **事件驱动架构**

*事件驱动编程*的特点是使用事件（例如用户输入）来决定程序控制流的走向，它是组织程序流的一种良好解决方案。事件驱动程序监听队列中的各种事件，并根据这些传入事件做出反应。

假设你想构建一个应用程序，它监听一个套接字上的连接，并处理接收到的连接。基本上有三种方法可以解决这个问题：

+   每次建立新连接时，依赖类似 multiprocessing 模块的工具来分叉一个新进程。

+   每次建立新连接时启动一个新线程，可以依赖类似 threading 模块的工具。

+   将这个新连接添加到你的事件循环中，并在事件发生时做出响应。

确定现代计算机如何同时处理成千上万的连接被称为 *C10K 问题*。其中，C10K 的解决策略解释了如何使用事件循环监听成百上千个事件源，比起每个连接一个线程的方式，事件驱动的方式扩展性更好。这并不意味着这两种技术不可兼容，而是通常可以用事件驱动机制替代多线程方式。

事件驱动架构使用事件循环：程序调用一个函数，该函数会阻塞执行，直到接收到并准备好处理的事件。其思想是，在等待输入和输出完成时，程序可以忙于执行其他任务。最基本的事件是“数据准备好读取”和“数据准备好写入”。

在 Unix 中，构建这种事件循环的标准函数是系统调用 select(2) 或 poll(2)。这些函数期望一个文件描述符列表进行监听，一旦至少有一个文件描述符准备好读取或写入，它们就会返回。

在 Python 中，我们可以通过 select 模块访问这些系统调用。使用这些调用来构建事件驱动系统是相当容易的，尽管这么做可能会很繁琐。Listing 11-3 展示了一个事件驱动系统，它完成我们指定的任务：在一个套接字上监听并处理任何接收到的连接。

import select

import socket

server = socket.socket(socket.AF_INET,

socket.SOCK_STREAM)

# 永远不要在读/写操作上阻塞

server.setblocking(0)

# 将套接字绑定到端口

server.bind(('localhost', 10000))

server.listen(8)

while True:

# select() 返回包含对象（套接字、文件等）的 3 个数组

# 准备好读取、写入或触发错误的文件描述符

输入，

输出，例外 = select.select([server], [], [server])

如果服务器在输入列表中：

connection, client_address = server.accept()

connection.send("hello!\n")

*清单 11-3：监听并处理连接的事件驱动程序*

在清单 11-3 中，创建了一个服务器套接字并设置为*非阻塞模式*，意味着对该套接字进行的任何读取或写入操作都不会阻塞程序。如果程序尝试从套接字读取数据，但没有数据准备好读取，套接字的 recv() 方法将引发 OSError，表明套接字未准备好。如果我们没有调用 setblocking(0)，套接字将保持阻塞模式，而不是引发错误，这不是我们在这里想要的行为。然后，套接字绑定到一个端口，并以最多八个连接的回溯队列进行监听。

主循环使用 select() 构建，该函数接收我们要读取的文件描述符列表（在本例中是套接字）、我们要写入的文件描述符列表（在本例中没有）以及我们希望从中获取异常的文件描述符列表（在本例中是套接字）。select() 函数在选定的文件描述符准备好读取、写入或引发异常时立即返回。返回值是符合请求的文件描述符的列表。然后，很容易检查我们的套接字是否在准备好读取的列表中，如果是，接受连接并发送消息。

### **其他选项和 asyncio**

另外，还有许多框架，如 Twisted 或 Tornado，它们以更加集成的方式提供这种功能；在这方面，Twisted 多年来一直是事实上的标准。导出 Python 接口的 C 库，如 libevent、libev 或 libuv，也提供了非常高效的事件循环。

这些选项都解决了相同的问题。缺点是，尽管有各种各样的选择，但大多数选项不兼容。许多选项也是*基于回调的*，这意味着在阅读代码时，程序流程并不十分清晰；你必须跳转到许多不同的地方才能理解程序。

另一种选择是使用 gevent 或 greenlet 库，它们避免了回调的使用。然而，这些库的实现细节包括 CPython x86 特定代码和在运行时动态修改标准函数，这意味着你不希望长期使用并维护这些库的代码。

2012 年，Guido Van Rossum 开始了名为*tulip*的解决方案的工作，并在 PEP 3156 中进行了文档化（*[`www.python.org/dev/peps/pep-3156`](https://www.python.org/dev/peps/pep-3156)*）。该软件包的目标是提供一个标准的事件循环接口，与所有框架和库兼容，并且具有互操作性。

自那时以来，郁金香代码已经更名并合并到 Python 3.4 中，成为 asyncio 模块，并且现在已成为事实上的标准。并不是所有的库都与 asyncio 兼容，大多数现有的绑定需要重写。

从 Python 3.6 开始，asyncio 被如此深入集成，以至于它拥有了自己的 `await` 和 `async` 关键字，使得使用起来非常简单。Listing 11-4 展示了如何使用 aiohttp 库（提供异步 HTTP 绑定）与 asyncio 一起运行多个网页获取操作。

import aiohttp

import asyncio

async def get(url):

async with aiohttp.ClientSession() as session:

async with session.get(url) as response:

return response

loop = asyncio.get_event_loop()

coroutines = [get("http://example.com") for _ in range(8)]

results = loop.run_until_complete(asyncio.gather(*coroutines))

print("Results: %s" % results)

*Listing 11-4：使用 aiohttp 并发获取网页*

我们将 get() 函数定义为异步函数，因此它从技术上来说是一个协程。get() 函数的两个步骤，即连接和页面获取，被定义为异步操作，直到准备好时才将控制权交还给调用者。这使得 asyncio 可以在任何时候安排另一个协程。模块在连接建立或页面准备好被读取时恢复协程的执行。这八个协程同时启动并提供给事件循环，asyncio 的任务是高效地安排它们。

asyncio 模块是编写异步代码和利用事件循环的一个很好的框架。它支持文件、套接字等，许多第三方库也可用来支持各种协议。不要犹豫，尽管使用它！

### **面向服务架构**

绕过 Python 的扩展性不足可能看起来有些棘手。然而，Python *非常*擅长实现 *面向服务架构（SOA）*，这是一种软件设计风格，其中不同的组件通过通信协议提供一组服务。例如，OpenStack 在其所有组件中使用 SOA 架构。这些组件使用 HTTP REST 与外部客户端（最终用户）通信，并使用基于高级消息队列协议（AMQP）的抽象远程过程调用（RPC）机制。

在你的开发场景中，知道在这些模块之间使用哪个通信渠道，主要取决于你将与谁进行通信。

当将服务暴露给外部世界时，首选的通信渠道是 HTTP，尤其是对于像 REST 风格（表述性状态转移风格）架构这样的无状态设计。这些架构使得实现、扩展、部署和理解服务变得更容易。

然而，当在内部暴露和使用你的 API 时，HTTP 可能不是最佳协议。有许多其他的通信协议，甚至描述其中一个就可能填满整本书。

在 Python 中，有很多用于构建 RPC 系统的库。Kombu 很有趣，因为它在许多后端之上提供了一个 RPC 机制，AMQ 协议是其中的主要之一。它还支持 Redis、MongoDB、Beanstalk、Amazon SQS、CouchDB 或 ZooKeeper。

最终，通过使用这种松耦合架构，你可以间接地获得巨大的性能提升。如果我们考虑到每个模块提供并暴露一个 API，我们可以运行多个守护进程，这些守护进程也可以暴露该 API，允许多个进程——因此也能利用多个 CPU——来处理工作负载。例如，*Apache httpd* 会创建一个新的工作进程来处理新的连接；然后我们可以将连接分配给在同一节点上运行的另一个工作进程。为了做到这一点，我们只需要一个用于将工作分配到各个工作进程的系统，而这个 API 就提供了这样的功能。每个模块将是一个不同的 Python 进程，正如我们之前所看到的，这种方法比多线程更适合分担工作负载。你将能够在每个节点上启动多个工作进程。即使无状态的模块并非绝对必要，每当有选择时，你都应该优先使用它们。

### **进程间通信与 ZeroMQ**

正如我们刚才讨论的，构建分布式系统时总是需要一个消息总线。你的进程需要相互通信以传递消息。ZeroMQ 是一个可以充当并发框架的套接字库。清单 11-5 实现了与 清单 11-1 中看到的相同的工作进程，但使用 ZeroMQ 来分配工作并在进程间进行通信。

import multiprocessing

import random

import zmq

def compute():

return sum(

[random.randint(1, 100) for i in range(1000000)])

def worker():

context = zmq.Context()

work_receiver = context.socket(zmq.PULL)

work_receiver.connect("tcp://0.0.0.0:5555")

result_sender = context.socket(zmq.PUSH)

result_sender.connect("tcp://0.0.0.0:5556")

poller = zmq.Poller()

poller.register(work_receiver, zmq.POLLIN)

while True:

socks = dict(poller.poll())

if socks.get(work_receiver) == zmq.POLLIN:

obj = work_receiver.recv_pyobj()

result_sender.send_pyobj(obj())

context = zmq.Context()

# 构建一个通道来发送待完成的工作

➊ work_sender = context.socket(zmq.PUSH)

work_sender.bind("tcp://0.0.0.0:5555")

# 构建一个通道来接收计算结果

➋ result_receiver = context.socket(zmq.PULL)

result_receiver.bind("tcp://0.0.0.0:5556")

# 启动 8 个工作进程

processes = []

for x in range(8):

➌     p = multiprocessing.Process(target=worker)

p.start()

processes.append(p)

# 发送 8 个任务

for x in range(8):

work_sender.send_pyobj(compute)

# 读取 8 个结果

results = []

for x in range(8):

➍     results.append(result_receiver.recv_pyobj()) # 终止所有进程

for p in processes:

p.terminate()

print("Results: %s" % results)

*清单 11-5：使用 ZeroMQ 的工作进程*

我们创建了两个套接字，一个用于发送函数（work_sender） ➊，另一个用于接收任务（result_receiver） ➋。每个由 `multiprocessing.Process` 启动的工作进程 ➌ 都会创建自己的套接字并将其连接到主进程。然后，工作进程执行发送给它的任何函数并返回结果。主进程只需通过发送套接字发送八个任务，并通过接收套接字等待八个结果返回 ➍。

如你所见，ZeroMQ 提供了一种简便的方式来构建通信通道。我在这里选择使用 TCP 传输层来说明我们可以在网络上运行这个过程。需要注意的是，ZeroMQ 还提供了一个本地的进程间通信通道（没有任何网络层参与），通过使用 Unix 套接字来实现。显然，为了保持清晰简洁，本示例中基于 ZeroMQ 构建的通信协议非常简单，但不难想象，在此基础上构建一个更复杂的通信层。同样，也可以很容易地想象，构建一个完全分布式的应用，使用网络消息总线进行通信，比如 ZeroMQ 或 AMQP。

请注意，像 HTTP、ZeroMQ 和 AMQP 这样的协议是与语言无关的：你可以使用不同的语言和平台来实现系统的每个部分。虽然我们都认为 Python 是一种很好的语言，但其他团队可能有不同的偏好，或者某个问题的某个部分可能需要其他语言来提供更好的解决方案。

最终，使用传输总线将你的应用解耦为多个部分是一个不错的选择。这种方法使你能够构建同步和异步的 API，并可以从一台计算机分布到几千台计算机。它不会将你绑定到特定的技术或语言，因此你可以将一切朝着正确的方向发展。

### **总结**

在 Python 中的经验法则是：只对 I/O 密集型工作负载使用线程，而当 CPU 密集型工作负载出现时，立即切换到多进程。将工作负载在更大范围内分布——比如在网络上构建分布式系统——需要外部库和协议。Python 本身支持这些内容，但它们是由外部提供的。
