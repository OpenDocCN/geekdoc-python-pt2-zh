## **8

神经网络简介**

![image](Images/common.jpg)

神经网络是深度学习的核心。在[第9章](ch09.xhtml#ch09)中，我们将深入探讨我们所称的*传统神经网络*。然而，在此之前，我们将介绍神经网络的结构，接着展示一个简单的示例。

具体来说，我们将介绍一个*全连接前馈神经网络*的组成部分。从视觉上看，你可以将该网络想象为[图8-1](ch08.xhtml#ch8fig1)中所示的样子。在本章及下章中，我们将经常提到这一图形。你的任务，如果你选择接受的话，是将这一图形记住，以减少翻阅书籍时的磨损。

![image](Images/08fig01.jpg)

*图8-1：一个示例神经网络*

在讨论了神经网络的结构和组成部分之后，我们将探索如何训练我们的示例网络来分类鸢尾花。通过这个初步实验，[第9章](ch09.xhtml#ch09)将引导我们进入梯度下降和反向传播算法——这是神经网络，包括先进的深度神经网络，训练的标准方法。本章旨在作为热身，真正的重头戏将在[第9章](ch09.xhtml#ch09)开始。

### 神经网络的结构

*神经网络*是一个图。在计算机科学中，*图*是由*节点*（通常绘制为圆形）通过*边*（短线段）连接而成的。这个抽象结构对于表示多种不同类型的关系非常有用：城市之间的道路、社交媒体上谁认识谁、互联网的结构，或是用来逼近任何数学函数的基本计算单元系列。

上一个示例，当然是故意为之。神经网络是通用的函数逼近器。它们使用图结构来表示一系列计算步骤，将输入特征向量映射到输出值，通常被解释为概率。神经网络是分层构建的。从概念上讲，它们从左到右作用，通过将值沿着边传递到节点来将输入特征向量映射到输出。需要注意的是，神经网络的节点通常被称为*神经元*。我们稍后将解释原因。节点根据它们的输入计算新的值，这些新值随后传递到下一层节点，依此类推，直到到达输出节点。在[图8-1](ch08.xhtml#ch8fig1)中，左侧有一个输入层，右侧有一个隐藏层，再右边还有一个隐藏层，输出层中有一个单独的节点。

前一部分提到了*全连接前馈神经网络*这一术语，却没有做太多解释。让我们来解析一下。*全连接*部分意味着一层中的每个节点的输出都会传送到下一层的每个节点。*前馈*部分意味着信息从左到右通过网络传递，不会被送回到前一层；网络结构中没有*反馈*，也没有循环。这就剩下了*神经网络*部分。

#### 神经元

就个人而言，我对*神经网络*这个词有一种爱恨交织的情感。这个词本身源于这样一个事实：在非常粗略的近似中，网络的基本单元类似于大脑中的神经元。看看[图 8-2](ch08.xhtml#ch8fig2)，我们很快会详细描述它。

![image](Images/08fig02.jpg)

*图 8-2：一个神经网络节点*

回想一下我们总是从左到右地可视化一个网络，我们看到节点（圆圈）从左侧接收输入，并且右侧有一个单一的输出。这里有两个输入，但它也可能是成百上千个。

多个输入映射到单一输出的方式与大脑中神经元的工作方式相似：被称为树突的结构接受来自其他许多神经元的输入，单一的轴突是输出。我喜欢这个类比，因为它引出了一个很酷的方式来谈论和思考网络。但我又讨厌这个类比，因为这些人工神经元在操作上与真实神经元差异很大，这个类比很快就会崩塌。它与实际神经元在解剖结构上有相似之处，但它们并不完全相同，这会导致那些不熟悉机器学习的人产生困惑，有些人甚至认为计算机科学家真的在构建人工大脑，或者认为网络会“思考”。“思考”这个词的意义难以捉摸，但在我看来，它并不适用于神经网络的行为。

现在回到[图 8-2](ch08.xhtml#ch8fig2)，我们看到左侧有两个方框，一些线条，一个圆圈，右侧有一条线，还有一些带下标的标签。让我们理清楚这些。如果我们理解了[图 8-2](ch08.xhtml#ch8fig2)，我们就能很好地理解神经网络。稍后，我们会看到我们视觉模型的代码实现，并惊讶于它竟然如此简单。

在[图 8-2](ch08.xhtml#ch8fig2)中，一切都集中在圆圈上。这就是实际的节点。实际上，它实现了一个称为*激活函数*的数学函数，用于计算节点的输出，即一个单一的数字。两个方框是节点的输入。这个节点接受来自输入特征向量的特征；我们用方框来与圆圈区分开，但输入也可以来自前一网络层中另一组圆形节点。

每个输入都是一个数字，一个标量值，我们称其为*x*[0]和*x*[1]。这些输入沿着标记为*w*[0]和*w*[1]的两条线段传递到节点。这些线段表示*权重*，即连接的强度。在计算上，输入（*x*[0]、*x*[1]）与权重（*w*[0]、*w*[1]）相乘、求和，然后传递给节点的激活函数。在这里，我们将激活函数称为*h*，这是一个相当常见的命名方式。

激活函数的值就是节点的输出值。在这里，我们将这个输出值称为*a*。输入值与权重相乘后相加，再传递给激活函数，产生输出值。我们还没有提到*b*[0]值，它也被加进去并传递给激活函数。这就是*偏置*项。它是一个偏移量，用于调整输入范围，使其适合激活函数。在[图8-2](ch08.xhtml#ch8fig2)中，我们添加了一个零下标。每个层中的每个节点都有一个偏置值，所以这里的下标意味着这是该层中的第一个节点。（记住，计算机领域的人总是从零开始计数，而不是从一开始。）

这就是神经网络节点的全部工作：神经网络节点接受多个输入，*x*[0]、*x*[1]、…，将每个输入与一个权重值* w*[0]、*w*[1]、…相乘，和偏置项*b*一起求和，然后将这个和传递给激活函数*h*，产生一个标量输出值*a*：

*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + … + *b*)

就是这样。把一堆节点组合在一起，正确地连接它们，弄清楚如何训练它们来设定权重和偏置，你就得到了一个有用的神经网络。正如你在下一章看到的那样，训练一个神经网络并不容易。但一旦训练完成，它们的使用非常简单：给它输入一个特征向量，它就会输出一个分类。

顺便提一下，我们一直在称这些图为*神经网络*，并将继续使用这个名称，有时也会使用缩写*NN*。如果你读过其他书籍或论文，可能会看到它们被称为*人工神经网络（ANNs）*，甚至是*多层感知器（MLPs）*，比如在sklearn的MLPClassifier类名中。我建议坚持使用*神经网络*，不过那只是我的看法。

#### 激活函数

让我们来谈谈激活函数。节点的激活函数接收一个标量输入，即输入的加权和再加上偏置，然后对其进行处理。特别地，我们需要激活函数是非线性的，这样模型才能学习复杂的函数。从数学上讲，最容易理解非线性函数是什么的方法是先了解什么是线性函数，然后得出任何非线性映射都是...非线性的结论。

*线性函数*，*g*，的输出与输入成正比，*g*(*x*) ∝ *x*，其中 ∝ 表示 *与...成正比*。或者说，线性函数的图形是一条直线。因此，任何图形不是直线的函数都是非线性函数。

例如，函数

*g*(*x*) = 3*x* + 2

是一个线性函数，因为它的图形是一条直线。像 *g*(*x*) = 1 这样的常数函数也是线性的。然而，函数

*g*(*x*) = *x*² + 2

是非线性函数，因为 *x* 的指数是 2。超越函数也是非线性的。*超越函数*是像 *g*(*x*) = log*x* 或 *g*(*x*) = *e*^(*x*) 这样的函数，其中 *e* = 2.718*...* 是自然对数的底数。像正弦和余弦这样的三角函数、它们的反函数，以及从正弦和余弦构建的正切等函数也是超越函数。这些函数被称为超越函数，是因为你无法通过有限的代数运算将它们表示出来。它们是非线性的，因为它们的图形不是直线。

网络需要非线性激活函数；否则，它只能学习线性映射，而线性映射不足以使网络具有广泛的实用性。考虑一个简单的网络，包含两个节点，每个节点有一个输入。这意味着每个节点都有一个权重和一个偏置值，第一个节点的输出就是第二个节点的输入。如果我们设置 *h*(*x*) = 5*x –* 3，这是一个线性函数，那么对于输入 *x*，网络计算出的输出 *a*[1] 为

| *a*[1] | = *h*(*w*[1]*a*[0] + *b*[1]) |
| --- | --- |
|  | = *h*(*w*[1]*h*(*w*[0]*x* + *b*[0]) + *b*[1]) |
|  | = *h*(*w*[1](5(*w*[0]*x* + *b*[0]) – 3) + *b*[1]) |
|  | = *h*(*w*[1](5*w*[0]*x* + 5*b*[0] – 3) + *b*[1]) |
|  | = *h*(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) |
|  | = 5(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) – 3 |
|  | = (25*w*[1]*w*[0])*x* + (25*w*[1]*b*[0] – [1]5*w*[1] + 5*b*[1] – 3) |
|  | = *W**x* + *B* |

对于 *W* = 25*w*[1]*w*[0] 和 *B* = 25*w*[1]*b*[0] *–* 15*w*[1] + 5*b*[1] *–* 3，这也是一个线性函数，另一条斜率为 *W* 和截距为 *B* 的直线，因为 *W* 和 *B* 都不依赖于 *x*。因此，使用线性激活函数的神经网络只能学习线性模型，因为线性函数的组合仍然是线性的。正是由于线性激活函数的这个局限性，导致了1970年代第一次神经网络“冬天”：对神经网络的研究几乎被放弃，因为人们认为它们过于简单，无法学习复杂的函数。

好的，我们需要非线性激活函数。哪些函数呢？实际上，可能的选择是无限的。经过实践验证，一些函数因其有效性、优良性质或两者兼具而脱颖而出。传统的神经网络使用的是 sigmoid 激活函数或双曲正切函数。*Sigmoid* 是

![image](Images/174equ01.jpg)

*双曲正切*是

![image](Images/174equ02.jpg)

这两个函数的图像见于[图8-3](ch08.xhtml#ch8fig3)，其中Sigmoid位于上方，双曲正切位于下方。

首先要注意的是，这两个函数都有大致相同的“S”形状。Sigmoid函数在x轴向左延伸时从0开始，到向右延伸时达到1。在x=0时，函数值为0.5。双曲正切函数也有相同的形状，但从-1到+1变化，并且在*x* = 0时为0。

最近，Sigmoid和双曲正切函数已被*修正线性单元*，简称*ReLU*所取代。ReLU简单且具有适用于神经网络的便捷特性。尽管名字中有*线性*，但ReLU是一个非线性函数——它的图像并不是一条直线。当我们在[第9章](ch09.xhtml#ch09)讨论神经网络的反向传播训练时，我们将了解为什么会发生这种变化。

![image](Images/08fig03.jpg)

*图8-3：Sigmoid函数（上）和双曲正切函数（下）。请注意，y轴的尺度不同*。

ReLU如下所示，并展示在[图8-4](ch08.xhtml#ch8fig4)中。

![image](Images/176equ01.jpg)![image](Images/08fig04.jpg)

*图8-4：修正线性激活函数，ReLU(x) = max(0,x)*

ReLU之所以被称为*修正*，是因为它去除了负值并将其替换为0。事实上，机器学习社区使用了几种不同版本的这个函数，但所有版本本质上都是将负值替换为常数或其他值。ReLU的分段性质使它成为一个非线性函数，因此适合用作神经网络的激活函数。它在计算上也非常简单，比Sigmoid或双曲正切要快得多。这是因为后两者函数使用*e*^(*x*)，在计算机术语中，这意味着调用exp函数。这个函数通常是通过一系列展开的和来实现的，这意味着它会在实际计算中产生数十次浮点运算，而实现ReLU只需一个简单的if语句。像这样的微小节省在一个可能有成千上万个节点的庞大网络中积累起来。

#### 网络的架构

我们已经讨论了节点及其工作原理，并暗示了节点是如何连接形成网络的。让我们更仔细地看看节点是如何连接的，以及网络的*架构*。

像本章中我们正在使用的标准神经网络一样，通常是分层构建的，正如你在[图8-1](ch08.xhtml#ch8fig1)中看到的。我们不一定要这样做，但正如我们将看到的，这样做能带来一些计算上的简便性，并大大简化训练过程。前馈网络包括一个输入层、一个或多个隐藏层和一个输出层。输入层仅仅是特征向量，而输出层则是预测（概率）。如果网络是用于多类问题，输出层可能有多个节点，每个节点表示模型对每个可能类别的预测。

隐藏层由节点构成，层 *i* 的节点接受来自层 *i –* 1 的节点输出作为输入，并将它们的输出传递给层 *i* + 1 的节点输入。层与层之间的连接通常是全连接的，这意味着层 *i –* 1 的每个节点输出都作为输入用于层 *i* 中的每个节点，因此称为*全连接*。再次强调，我们不一定要这样做，但它简化了实现过程。

隐藏层的数量和每个隐藏层中节点的数量定义了网络的架构。已经证明，单一隐藏层且节点足够多时，能够学习任何函数映射。这是件好事，因为这意味着神经网络可以应用于机器学习问题，因为最终，模型作为一个复杂的函数，将输入映射到输出标签和概率。然而，像许多理论结果一样，这并不意味着在所有情况下单层网络都是实用的。随着网络中节点（和层数）数量的增加，学习的参数数量（权重和偏置）也会增加，因此所需的训练数据量也会增加。这又是维度诅咒的体现。

这些问题在1980年代再次困扰了神经网络。计算机太慢，无法训练大型网络，而且，无论如何，通常也没有足够的数据来训练网络。实践者们知道，如果这两种情况发生变化，那么训练大型网络就变得可能，这些网络将比当时的小型网络更强大。幸运的是，到了2000年代初，情况发生了变化。

选择合适的神经网络架构对模型是否能够学习到有效的内容有着巨大影响。这就是经验和直觉发挥作用的地方。选择正确的架构是使用神经网络的“黑魔法”。为了更有帮助，我们可以给出一些（粗略的）经验法则：

+   如果你的输入有明确的空间关系，比如图像的各个部分，你可能想使用卷积神经网络（[第12章](ch12.xhtml#ch12)）。

+   最多使用三个隐藏层。回想一下，理论上，一个足够大的隐藏层就足够了，因此尽量只使用必要的隐藏层。如果模型在使用一个隐藏层时能够学习，那么可以尝试增加第二个隐藏层，看是否能够提高效果。

+   第一个隐藏层的节点数应与输入特征向量的数量相匹配或（理想情况下）超过该数量。

+   除了第一个隐藏层（参见前面的规则）外，每个隐藏层的节点数应与上一层和下一层的节点数相等或介于两者之间。如果第 *i –* 1 层有 *N* 个节点，第 *i* + 1 层有 *M* 个节点，那么第 *i* 层可能适合有 *N* ≤ *x* ≤ *M* 个节点。

第一个规则表明，传统神经网络最适用于输入没有空间关系的情况——也就是说，输入是特征向量，而不是图像。此外，当输入维度较小，或者没有足够的数据，导致训练一个更大的卷积网络困难时，应该尝试使用传统网络。如果你认为自己处于需要传统神经网络的情况，建议从小开始，随着性能的提升逐步扩展。

#### 输出层

神经网络的最后一层是输出层。如果网络建模的是连续值，也就是 *回归*，这是本书忽略的用例，那么输出层是一个不使用激活函数的节点；它仅仅报告 *h* 的参数，如[图 8-2](ch08.xhtml#ch8fig2)所示。请注意，这与说激活函数是身份函数 *h*(*x*) = *x* 是一样的。

我们的神经网络是用于分类的；我们希望它们输出一个决策值。如果我们有两个类别，分别标记为 0 和 1，我们将最终节点的激活函数设置为 sigmoid。这将输出一个介于 0 和 1 之间的值，我们可以将其解释为输入属于类别 1 的可能性或概率。我们根据输出值使用简单规则做出分类决策：如果激活值小于 0.5，则将输入分类为 0 类；否则，分类为 1 类。我们将在[第 11 章](ch11.xhtml#ch11)看到如何通过改变 0.5 的阈值来调整模型在特定任务中的表现。

如果我们有多个类别，我们需要采取不同的方法。我们将不再使用输出层中的单个节点，而是拥有 *N* 个输出节点，每个类别一个，每个节点都使用身份函数作为 *h*。然后，我们对这 *N* 个输出应用 *softmax* 操作，并选择具有最大 softmax 值的输出。

让我们来说明一下 softmax 的含义。假设我们有一个包含四个类别的数据集。它们表示什么并不重要；网络也不知道它们代表什么。这些类别被标记为 0、1、2 和 3。因此，*N* = 4 表示我们的网络将有四个输出节点，每个节点都使用身份函数作为 *h*。这看起来像是 [图 8-5](ch08.xhtml#ch8fig5)，其中我们也展示了 softmax 操作和由此产生的输出向量。

![image](Images/08fig05.jpg)

*图 8-5：具有四个类别的神经网络的最后一层隐藏层 *n*-1 和输出层 (*n*，节点编号)。应用 softmax 操作，生成一个四元素输出向量，* [p*0,*p*1,*p*2,*p*3*]*。

我们选择该输出向量中最大值的索引作为给定输入特征向量的类别标签。softmax 操作确保该向量的元素总和为 1，因此我们可以稍微不那么严格地称这些值为属于四个类别中每个类别的概率。这就是为什么我们只取最大值来决定输出类别标签的原因。

softmax 操作是直接的：每个输出的概率就是

![image](Images/179equ01.jpg)

其中 *a*[*i*] 是第 *i* 个输出，分母是所有输出的总和。对于这个示例，*i* = 0,1,2,3，最大值的索引将是分配给输入的类别标签。

作为示例，假设四个最后一层节点的输出为

*a*[0] = 0.2

*a*1 = 1.3

*a*2 = 0.8

*a*3 = 2.1

然后按照以下方式计算 softmax：

*p*[0] = *e*^(0.2)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.080

*p*[1] = *e*^(1.3)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.240

*p*[2] = *e*^(0.8)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.146

*p*[3] = *e*^(2.1)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.534

选择类 3，因为 *p*[3] 是最大的。注意，*p*[*i*] 值的总和是 1.0，这是我们预期的。

这里有两点需要提到。在前面的公式中，我们使用了 sigmoid 函数来计算网络的输出。如果我们将类别数设为 2 并计算 softmax，那么我们将得到两个输出值：一个是某个 *p*，另一个是 1 *– p*。这与仅使用 sigmoid 是一致的，后者选择了输入属于类 1 的概率。

第二点涉及到 softmax 的实现。如果网络输出的 *a* 值很大，那么 *e*^(*a*) 可能会非常大，这会是计算机不喜欢的情况。至少会丢失精度，或者值可能溢出使得输出没有意义。从数值上看，如果我们在计算 softmax 之前从所有其他值中减去最大 *a* 值，那么我们就会对较小的值取指数，这些值不太可能发生溢出。对前面的例子进行这样的处理后，我们得到新的 *a* 值

![image](Images/180equ02.jpg)

在这里，我们减去 2.1，因为这是最大的 *a* 值。这导致了与之前找到的完全相同的 *p* 值，但这次对溢出的情况进行了保护，以防任何 *a* 值过大。

#### 表示权重和偏置

在我们继续进行神经网络示例之前，先回顾一下权重和偏置，并且可以看到通过将神经网络视为矩阵和向量的形式，我们可以大大简化神经网络的实现。

考虑将一个包含两个元素的输入特征向量映射到第一个具有三个节点的隐藏层（*a*[1]，见[图8-1](ch08.xhtml#ch8fig1)）。我们将两层之间的边（权重）标记为 *w*[*ij*]，其中 *i* = 0,1 对应输入 *x*[0] 和 *x*[1]，*j* = 0,1,2 对应三个隐藏层节点，从图中自上而下编号。此外，我们还需要三个偏置值，这些值在图中未显示，每个隐藏节点一个。我们将它们称为 *b*[0]、*b*[1] 和 *b*[2]，同样是自上而下排列。

为了计算三个隐藏节点的激活函数 *h* 的输出，我们需要找出以下内容。

*a*[0] = *h*(*w*[00]*x*[0] + *w*[10]*x*[1] + *b*[0])

*a*[1] = *h*(*w*[01]*x*[0] + *w*[11]*x*[1] + *b*[1])

*a*[2] = *h*(*w*[02]*x*[0] + *w*[12]*x*[1] + *b*[2])

但是，记住矩阵乘法和向量加法的工作原理，我们可以看到这正是

![image](Images/181equ02.jpg)

其中 ![Image](Images/181equ03.jpg)，*W* 是一个 3 × 2 的权重值矩阵。在这种情况下，激活函数 *h* 接收一个输入值的向量，并生成一个输出值的向量。这实际上是将 *h* 应用到 ![Image](Images/181equ04.jpg) 的每个元素。例如，将 *h* 应用于一个包含三个元素的向量 ![Image](Images/xbar1.jpg)

![image](Images/181equ05.jpg)

将 *h* 分别应用于 ![Image](Images/xbar1.jpg) 的每个元素。

由于 NumPy Python 模块设计用于处理数组，而矩阵和向量都是数组，我们得出一个令人愉快的结论，即神经网络的权重和偏置可以存储在 NumPy 数组中，只需简单的矩阵运算（调用 np.dot）和加法即可操作完全连接的神经网络。请注意，这也是我们希望使用全连接网络的原因：它们的实现是直接的。

为了存储[图8-1](ch08.xhtml#ch8fig1)中的网络，我们需要在每一层之间设置权重矩阵和偏置向量，从而得到三个矩阵和三个向量：每一层的输入到第一个隐藏层，第一个隐藏层到第二个隐藏层，第二个隐藏层到输出层的矩阵和向量。权重矩阵的维度分别为 3 × 2、2 × 3 和 1 × 2。偏置向量的长度分别为 3、2 和 1。

### 实现一个简单的神经网络

在这一部分，我们将实现[图 8-1](ch08.xhtml#ch8fig1)中的示例神经网络，并使用鸢尾花数据集的两个特征进行训练。我们将从头实现网络，但使用 sklearn 来训练它。本节的目标是看看实现一个简单神经网络有多直接。希望这能清除上一节讨论中可能存在的一些迷雾。

[图 8-1](ch08.xhtml#ch8fig1)中的网络接受一个具有两个特征的输入特征向量。它有两个隐藏层，一个包含三个节点，另一个包含两个节点。它有一个 sigmoid 输出层。隐藏层节点的激活函数也是 sigmoid。

#### 构建数据集

在我们查看神经网络代码之前，先构建一下我们要训练的数据集，看看它是什么样的。我们已经知道鸢尾花数据集，但在这个例子中，我们只使用两个类别，并且只使用四个特征中的两个。构建训练和测试数据集的代码在[清单 8-1](ch08.xhtml#ch8lis1)中。

import numpy as np

❶ d = np.load("iris_train_features_augmented.npy")

l = np.load("iris_train_labels_augmented.npy")

d1 = d[np.where(l==1)]

d2 = d[np.where(l==2)]

❷ a=len(d1)

b=len(d2)

x = np.zeros((a+b,2))

x[:a,:] = d1[:,2:]

x[a:,:] = d2[:,2:]

❸ y = np.array([0]*a+[1]*b)

i = np.argsort(np.random.random(a+b))

x = x[i]

y = y[i]

❹ np.save("iris2_train.npy", x)

np.save("iris2_train_labels.npy", y)

❺ d = np.load("iris_test_features_augmented.npy")

l = np.load("iris_test_labels_augmented.npy")

d1 = d[np.where(l==1)]

d2 = d[np.where(l==2)]

a=len(d1)

b=len(d2)

x = np.zeros((a+b,2))

x[:a,:] = d1[:,2:]

x[a:,:] = d2[:,2:]

y = np.array([0]*a+[1]*b)

i = np.argsort(np.random.random(a+b))

x = x[i]

y = y[i]

np.save("iris2_test.npy", x)

np.save("iris2_test_labels.npy", y)

*清单 8-1：构建简单的示例数据集。见* nn_iris_dataset.py。

这段代码是直接的数据处理。我们从增强数据集开始，加载样本和标签❶。我们只需要类别 1 和类别 2，所以我们找到这些样本的索引并将它们提取出来。我们只保留特征 2 和 3，并将它们放入 x ❷。接下来，我们构建标签（y）❸。注意，我们将类别标签重新编码为 0 和 1。最后，我们打乱样本的顺序并将新的数据集写入磁盘❹。最后，我们重复这个过程来构建测试样本❺。

[图 8-6](ch08.xhtml#ch8fig6)展示了训练集。因为我们只有两个特征，所以可以绘制它。

![image](Images/08fig06.jpg)

*图 8-6：两类两特征鸢尾花数据集的训练数据*

我们很快就会发现这个数据集并不是简单地可分的。没有简单的线可以将训练集正确地分成两组，一组全是类别 0，另一组全是类别 1。这使得问题变得更有趣。

#### 实现神经网络

让我们看看如何使用 NumPy 在 Python 中实现 [图 8-1](ch08.xhtml#ch8fig1) 中的网络。我们假设网络已经训练好，也就是说我们已经知道所有的权重和偏置。代码见 [清单 8-2](ch08.xhtml#ch8lis2)。

import numpy as np

import pickle

import sys

def sigmoid(x):

return 1.0 / (1.0 + np.exp(-x))

def evaluate(x, y, w):

❶ w12, b1, w23, b2, w34, b3 = w

nc = nw = 0

prob = np.zeros(len(y))

for i in range(len(y)):

a1 = sigmoid(np.dot(x[i], w12) + b1)

a2 = sigmoid(np.dot(a1, w23) + b2)

prob[i] = sigmoid(np.dot(a2, w34) + b3)

z = 0 if prob[i] < 0.5 else 1

❷ if (z == y[i]):

nc += 1

else:

nw += 1

return [float(nc) / float(nc + nw), prob]

❸ xtest = np.load("iris2_test.npy")

ytest = np.load("iris2_test_labels.npy")

❹ weights = pickle.load(open("iris2_weights.pkl", "rb"))

score, prob = evaluate(xtest, ytest, weights)

print()

for i in range(len(prob)):

print("%3d:  实际: %d  预测: %d  概率: %0.7f" %

(i, ytest[i], 0 if (prob[i] < 0.5) else 1, prob[i]))

print("得分 = %0.4f" % score)

*清单 8-2：使用训练好的权重和偏置对保留的测试样本进行分类。见* nn_iris_evaluate.py。

也许我们首先需要注意的是代码的简洁。evaluate 函数实现了网络。我们还需要定义 sigmoid，因为 NumPy 本身没有提供这个函数。主代码加载了测试样本（xtest）和相关标签（ytest）❸。这些文件是之前代码生成的，因此我们知道 xtest 的形状是 23 × 2，因为我们有 23 个测试样本，每个样本有两个特征。同样，ytest 是一个包含 23 个标签的向量。

当我们训练这个网络时，我们会将权重和偏置存储为一个 NumPy 数组列表。Python 中存储列表到磁盘的方式是使用 pickle 模块，因此我们使用 pickle 从磁盘加载列表 ❹。这个列表 weights 有六个元素，代表定义网络的三个权重矩阵和三个偏置向量。这些就是我们训练过程中调节到数据集的“魔法”数字。最后，我们调用 evaluate 函数将每个测试样本通过网络运行。这个函数返回每个样本的分数（准确度）和输出的类别 1 的概率（prob）。其余的代码会显示样本编号、实际标签、预测标签以及与类别 1 相关的输出概率。最后，显示分数（准确度）。

网络在 evaluate 中实现；我们来看看它是如何实现的。首先，从提供的权重列表 ❶ 中提取单独的权重矩阵和偏置向量。这些是 NumPy 数组：w12 是一个 2 × 3 的矩阵，将两个输入元素映射到具有三个节点的第一隐藏层，w23 是一个 3 × 2 的矩阵，将第一隐藏层映射到第二隐藏层，w34 是一个 2 × 1 的矩阵，将第二隐藏层映射到输出。偏置向量分别是 b1（三个元素）、b2（两个元素）和 b3（单个元素，标量）。

请注意，权重矩阵的形状与我们之前表示的并不相同。它们是转置的。这是因为我们将向量（被视为1 × 2的矩阵）与权重矩阵相乘。由于标量乘法是可交换的，即*ab* = *ba*，我们可以看到我们仍然在计算相同的激活函数参数值。

接下来，evaluate将正确计数器(nc)和错误计数器(nw)初始化为0。它们用于计算整个测试集的总体得分。类似地，我们定义了prob，一个向量，用来存储每个测试样本的输出概率值。

循环将整个网络应用于每个测试样本。首先，我们将输入向量映射到第一个隐藏层，并计算*a*[1]，这是一个包含三个数值的向量，表示每个隐藏节点的激活值。然后，我们将这些第一个隐藏层的激活值用于计算第二个隐藏层的激活值*a*[2]。这是一个包含两个元素的向量，因为第二个隐藏层有两个节点。接下来，我们计算当前输入向量的输出值，并将其存储在prob数组中。通过检查网络输出值是否小于0.5来为类标签z赋值。最后，根据该样本的实际标签(y[i]) ❷，我们递增正确计数器(nc)或错误计数器(nw)。当所有样本通过网络处理后，返回总体准确率，它是正确分类样本数与总样本数的比值。

这很好，我们可以实现一个网络，并将输入向量通过它来观察它的表现。如果网络有第三个隐藏层，我们将在计算最终输出值之前，将第二个隐藏层的输出(a2)传递给它。

#### 训练和测试神经网络

[清单 8-2](ch08.xhtml#ch8lis2)中的代码将训练好的模型应用于测试数据。为了首先训练模型，我们将使用sklearn。训练模型的代码在[清单 8-3](ch08.xhtml#ch8lis3)中。

import numpy as np

import pickle

from sklearn.neural_network import MLPClassifier

xtrain = np.load("iris2_train.npy")

ytrain = np.load("iris2_train_labels.npy")

xtest = np.load("iris2_test.npy")

ytest = np.load("iris2_test_labels.npy")

❶ clf = MLPClassifier(

❷ hidden_layer_sizes=(3,2),

❸ activation="logistic",

solver="adam", tol=1e-9,

max_iter=5000,

verbose=True)

clf.fit(xtrain, ytrain)

prob = clf.predict_proba(xtest)

score = clf.score(xtest, ytest)

❹ w12 = clf.coefs_[0]

w23 = clf.coefs_[1]

w34 = clf.coefs_[2]

b1 = clf.intercepts_[0]

b2 = clf.intercepts_[1]

b3 = clf.intercepts_[2]

weights = [w12,b1,w23,b2,w34,b3]

pickle.dump(weights, open("iris2_weights.pkl","wb"))

print()

print("测试结果：")

print("  总体得分: %0.7f" % score)

print()

for i in range(len(ytest)):

p = 0 if (prob[i,1] < 0.5) else 1

print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))

print()

*清单 8-3：使用 sklearn 训练鸢尾花神经网络。请参见* nn_iris_mlpclassifier.py。

首先，我们从磁盘加载训练和测试数据。这些数据是我们之前创建的文件。接着，我们设置神经网络对象，这是 MLPClassifier 的一个实例❶。网络有两个隐藏层，第一个有三个节点，第二个有两个节点❷。这与[图 8-1](ch08.xhtml#ch8fig1)中的架构相匹配。该网络还使用了*logistic*层❸。这是另一个名称，用于表示 Sigmoid 层。我们通过调用 fit 方法来训练模型，正如我们对其他 sklearn 模型类型所做的那样。由于我们将 verbose 设置为 True，因此我们将看到每次迭代的损失输出。

调用 predict_proba 方法可以得到测试数据的输出概率。大多数其他 sklearn 模型也支持此方法。这表示模型对分配的输出标签的确定性。接着，我们调用 score 方法计算测试集上的分数，正如我们之前所做的那样。

我们想要存储学习到的权重和偏置，以便在测试代码中使用它们。我们可以直接从训练好的模型中提取这些数据❹。这些数据被打包成一个列表（weights），然后被保存为 Python pickle 文件。

剩下的代码打印出运行 sklearn 训练模型并与保留的测试数据进行比较的结果。例如，某次运行此代码输出：

测试结果：

总体得分：1.0000000

000: 0 - 0, 0.0705069

001: 1 - 1, 0.8066224

002: 0 - 0, 0.0308244

003: 0 - 0, 0.0205917

004: 1 - 1, 0.9502825

005: 0 - 0, 0.0527558

006: 1 - 1, 0.9455174

007: 0 - 0, 0.0365360

008: 1 - 1, 0.9471218

009: 0 - 0, 0.0304762

010: 0 - 0, 0.0304762

011: 0 - 0, 0.0165365

012: 1 - 1, 0.9453844

013: 0 - 0, 0.0527558

014: 1 - 1, 0.9495079

015: 1 - 1, 0.9129983

016: 1 - 1, 0.8931552

017: 0 - 0, 0.1197567

018: 0 - 0, 0.0406094

019: 0 - 0, 0.0282220

020: 1 - 1, 0.9526721

021: 0 - 0, 0.1436263

022: 1 - 1, 0.9446458

表明模型在小型测试数据集上的表现完美。输出显示了样本编号、实际类标签、分配的类标签以及属于类 1 的输出概率。如果我们通过评估代码运行保存了 sklearn 网络权重和偏置的 pickle 文件，我们会发现输出概率与前面的代码完全相同，表明我们手动生成的神经网络实现是正确的。

### 总结

在本章中，我们讨论了神经网络的结构。我们描述了其架构、节点的排列以及它们之间的连接。我们讨论了输出层节点及其计算的函数。然后，我们看到所有的权重和偏置可以方便地通过矩阵和向量来表示。最后，我们展示了一个简单的网络，用于分类鸢尾花数据的子集，并展示了如何训练和评估它。

现在我们已经初步了解了神经网络，接下来让我们深入探讨其背后的理论。
