## **11

**评估模型**

![image](Images/common.jpg)

到目前为止，我们通过查看模型在保留的测试集上的准确率来评估模型。这是自然且直观的，但正如我们在本章中将要学习的那样，这并不是我们能做的、或者应该做的所有评估工作。

本章开始时，我们将定义指标并阐明一些基本假设。然后我们会讨论为什么我们需要的不仅仅是准确率。我们将引入混淆矩阵的概念，并花时间讨论我们可以从中衍生出的指标。从那里，我们将讨论性能曲线，这是比较不同模型的最佳方式。最后，我们将混淆矩阵的概念扩展到多分类问题。我们不会说完所有关于性能指标的内容，因为这一领域仍在不断发展。然而，在本章结束时，你将熟悉机器学习领域人士常用的各种数字，并能很好地理解它们的含义。

### 定义与假设

除了准确率，我们还有许多其他的指标可以帮助我们评估模型的表现。这些指标使我们能够合理地比较不同的模型。让我们从定义“指标”这个词开始。对我们来说，指标是一个数字或一组数字，代表模型表现的某些方面。

当模型的表现增加或减少时，指标的值也会随之增加或减少，或者可能相反。有时我们会有些随意，把图表也称作指标，因为我们用它们来评估模型的表现。

我们关注的是评估一个只有单一保留测试集的模型。我们假设我们遵循了[第4章](ch04.xhtml#ch04)的建议，构建了三个数据集：一个训练集来训练模型，一个验证集来决定模型何时训练完成，以及一个保留的测试集来评估训练后的模型。现在，我们已经训练好了模型，利用了训练集和验证集，并希望知道我们的表现如何。

本章中还有一个隐含的假设，这是一个至关重要的假设：我们假设保留的测试集能够很好地代表生成数据的母体分布。换句话说，保留的测试集必须尽可能地代表模型在实际应用中可能遇到的数据类型。例如，测试集中各类出现的频率应该与模型实际使用时遇到的预期比例尽可能一致。

这是必要的，因为训练集在训练过程中将模型条件化为期望特定的分布和特定的特征，如果模型在使用时接收到的数据具有不同的特征，模型的表现将不佳。训练集与模型实际使用时接收到的数据集之间的分布差异，是部署的机器学习模型在实际使用中失败的最常见原因之一。

### 为什么准确度不足够

二分类器对特定输入输出一个决策：类 0 或类 1。我们定义以下内容，

*N*[*c*]，模型正确分类的测试样本数量

*N*[*w*]，模型分类错误的测试样本数量

那么，这个模型的整体准确度是一个介于 0 和 1 之间的数字：

![image](Images/252equ01.jpg)

这是我们在整本书中一直使用的准确度。请注意，在本章中，当我们提到整体准确度时，我们将使用 *ACC*。

这个似乎是一个相当合理的度量，但有几个充分的理由不应该过于相信这个数字。例如，*N*[*c*] 和 *N*[*w*] 并没有告诉我们每个类别的相对频率。如果某个类别很稀有呢？让我们看看这会如何影响结果。

如果模型的准确度是 95%（ACC = 0.95），我们可能会感到满意。然而，假设类 1 的频率（即 *先验概率*）只有 5%，意味着平均来说，如果我们从测试集中抽取 100 个样本，约有 5 个属于类 1，其余 95 个属于类 0。我们看到一个预测所有输入都是类 0 的模型将有 95% 的正确率。但考虑一下：我们的模型可能会将所有输入都预测为类 0。如果我们仅关注整体准确度，我们可能会认为我们有一个好的模型，实际上我们拥有的只是一个糟糕的模型，且用两行 Python 就可以实现：

def predict(x):

返回 0

在这段代码中，我们说无论输入特征向量 *x* 是什么，类别都是 0。没有人会对这样的模型感到满意。

各类别的先验概率会影响我们如何看待整体准确度。然而，如果我们知道以下内容：

*N*[0]，测试集中类 0 实例的数量

*N*[1]，测试集中类 1 实例的数量

*C*[0]，我们的模型找到的类 0 实例的数量

*C*[1]，我们的模型找到的类 1 实例的数量

我们可以很容易地计算每个类别的准确度：

![image](Images/253equ01.jpg)

最终的表达式只是计算整体准确度的另一种方式，因为它将所有正确分类的数量除以测试的样本数量。

每个类别的准确度比整体准确度更好，因为它考虑了测试集中各类别频率的不平衡。对于我们之前假设的测试集，其中类 1 的频率为 5%，如果分类器对所有输入都预测为类 0，我们会发现这一点，因为我们的每个类别的准确度将是 ACC[0] = 1.0 和 ACC[1] = 0.0。这是有道理的。我们会将每个类 0 的样本预测正确，而将每个类 1 的样本预测错误（我们无论如何会将其视为类 0）。每个类别的准确度将在我们考虑评估多类模型时再次出现。

不仅仅是因为错误的成本，这种做法更微妙的原因是，错误可能带来的成本远高于正确的结果。这引入了测试集之外的因素：它引入了我们对类别 0 和类别 1 所赋予的意义。例如，如果我们的模型用于乳腺癌检测，可能使用了我们在[第 5 章](ch05.xhtml#ch05)中创建的数据集，当实际样本并不是恶性病例时，报告为类别 1（恶性），这可能会导致等待检查结果的女性产生焦虑。然而，经过进一步检测后，结果证明她并没有患乳腺癌。但考虑另一种情况。如果一个良性的结果实际上是恶性的，可能意味着她不会接受治疗，或者治疗过晚，这可能是致命的。不同类别的相对成本并不相同，甚至可能意味着生死之间的差异。对于一辆自驾车来说，这种情况同样成立，如果它认为在路中间玩耍的孩子是一个空的汽水罐，或者其他许多现实世界中的例子。

我们在现实世界中使用模型，因此它们的输出与现实世界相连接，有时输出的成本是相当大的。仅使用模型的总体准确率可能会产生误导，因为它没有考虑到错误的成本。

### 2 × 2 混淆矩阵

到目前为止，我们处理的模型最终都会为每个输入分配一个类别标签。例如，一个具有逻辑输出的神经网络被解释为类别 1 的成员概率。使用常见的 0.5 阈值，我们可以为输入分配一个类别标签：如果输出小于 0.5，则将输入标记为类别 0；否则，标记为类别 1。对于其他模型类型，决策规则不同（例如，*k*-最近邻投票法），但效果相同：我们为输入分配一个类别。

如果我们将整个测试集输入到模型中，并应用决策规则，那么每个样本的分配类别标签和真实类别标签就会被给出。同样，假设我们只考虑二分类器的情况，对于每个输入样本，在分配类别和真实类别之间有四种可能的结果（见[表 11-1](ch11.xhtml#ch11tab1)）。

**表 11-1：** 二分类器中真实类别标签与分配类别标签之间的可能关系

| **分配的类别** | **真实类别** | **情况** |
| --- | --- | --- |
| 0 | 0 | 真阴性 (TN) |
| 0 | 1 | 假阴性 (FN) |
| 1 | 0 | 假阳性 (FP) |
| 1 | 1 | 真阳性 (TP) |

*Case*标签定义了我们如何描述这些情况。如果输入的实际类别是类别0，并且模型分配了类别0，那么我们就有一个正确识别的负例，这就是*真正负例*，或*TN*。如果实际类别是类别1，并且模型分配了类别1，那么我们就有一个正确识别的正例，这就是*真正正例*，或*TP*。然而，如果实际类别是类别1，并且模型分配了类别0，那么我们就有一个正例被错误地称为负例，这就是*假负例*，或*FN*。最后，如果实际类别是类别0，并且模型分配了类别1，那么我们就有一个负例被错误地称为正例，这就是*假正例*，或*FP*。  

我们可以将测试集中的每个输入放入其中的一个且仅一个情况中。这样做可以让我们统计每个情况在测试集中出现的次数，我们可以将其整齐地展示为一个表格（见[表 11-2](ch11.xhtml#ch11tab2)）。  

**表 11-2：** 2 × 2 表中的类别标签定义  

|  | **实际类别1** | **实际类别0** |   |
| --- | --- | --- | --- |
| --- | --- | --- |   |
| *模型分配类别1* | TP | FP |   |
| *模型分配类别0* | FN | TN |   |

我已经将案例标签（TP、FP等）放在了实际计数的位置，以便对应每个案例的计数。  

这个表格叫做2 × 2 *混淆矩阵*（或2 × 2 *列联表*）。它是2 × 2的，因为有两行两列。它之所以叫做混淆矩阵，是因为它让我们一目了然地看到分类器的表现，尤其是分类器在哪些地方出现了混淆。当分类器将某一类实例误分为另一类时，它就产生了混淆。在2 × 2表格中，这种混淆表现为不在主对角线（从左上到右下）上的计数。这些就是FP和FN条目。一个在测试集上表现完美的模型将会有FP = 0 和 FN = 0；它在分配类别标签时不会犯任何错误。  

在[第7章](ch07.xhtml#ch07)中，我们对在[第5章](ch05.xhtml#ch05)构建的乳腺癌数据集进行了实验。我们通过查看模型的整体准确率，报告了经典模型在该数据集上的表现。这就是sklearn得分方法返回的内容。现在，让我们来看看从这些模型的测试集生成的一些2 × 2表格。  

我们正在查看的代码在文件*bc_experiments.py*中。此代码训练了多种经典模型类型。然而，我们不再使用整体准确率，而是引入了一个新函数来计算2 × 2表中的条目（见[清单 11-1](ch11.xhtml#ch11lis1)）：  

def tally_predictions(clf, x, y):  

p = clf.predict(x)

score = clf.score(x, y)  

tp = tn = fp = fn = 0  

for i in range(len(y)):  

if (p[i] == 0) and (y[i] == 0):  

tn += 1  

elif (p[i] == 0) and (y[i] == 1):  

fn += 1  

(*\pagebreak*)  

elif (p[i] == 1) and (y[i] == 0):  

fp += 1  

else:  

tp += 1  

return [tp, tn, fp, fn, score]  

*清单 11-1：生成计数统计*  

这个函数接受一个训练好的 sklearn 模型对象（clf）、测试样本（x）和相应的实际测试标签（y）。该函数首先使用 sklearn 模型对每个测试样本预测一个类别标签；预测结果存储在 p 中。然后，它会计算总体得分，并遍历每个测试样本，将预测的类别标签（p）与实际的已知类别标签（y）进行比较，以查看该样本是正例（True Positive）、负例（True Negative）、假正例（False Positive）还是假负例（False Negative）。完成后，所有这些值将被返回。

将 tally_predictions 应用到 *bc_experiments.py* 的输出上，得到了 [表 11-3](ch11.xhtml#ch11tab3)。这里给出了 sklearn 模型类型。

**表 11-3：** 乳腺癌测试集的 2 × 2 表格

![image](Images/table11-3.jpg)

在 [表 11-3](ch11.xhtml#ch11tab3) 中，我们可以看到四个 2 × 2 表格，分别对应于应用于各个模型的测试集：最近质心法（Nearest Centroid）、3-NN、决策树（Decision Tree）和线性支持向量机（linear SVM）。仅从表格中，我们可以看到表现最好的模型是 3-NN，因为它只有一个假正例，没有假负例。这意味着该模型从未将真正的恶性病例误判为良性，且只将一个良性病例误判为恶性。根据我们在上一节的讨论，这个结果是令人鼓舞的。

现在来看一下最近质心法和决策树的结果。这两个模型的总体准确率分别为 94.7% 和 93.9%。仅从准确率来看，我们可能会倾向于认为最近质心法模型更好。然而，如果我们查看 2 × 2 表格，我们会发现，尽管决策树有更多的假正例（6 个），但它只有一个假负例，而最近质心法有两个假负例。再次强调，在这种情况下，假负例意味着漏诊癌症，可能会导致严重后果。因此，对于这个数据集，我们希望尽量减少假负例，即使这意味着我们需要容忍假正例稍微增多。因此，我们会选择决策树而不是最近质心法模型。

### 从 2 × 2 混淆矩阵派生的度量

查看原始的 2 × 2 表格很有帮助，但更有帮助的是从中派生出的度量。在本节中，我们将查看几个度量，了解它们如何帮助我们解读 2 × 2 表格中的信息。不过，在我们开始之前，需要记住的是，我们将讨论的度量有时会引起一些争议。关于何时使用哪种度量，学术界仍然存在激烈的讨论。我们在这里的目的是通过示例介绍这些度量，并描述它们衡量的内容。作为机器学习从业者，你几乎会时不时地遇到这些度量，所以至少要对它们有所了解。

#### 从 2 × 2 表格中派生的度量

第一个指标直接来源于 2 × 2 表中的值：TP、TN、FP、FN。可以把这些看作是基础指标。它们计算简单，理解容易。回忆一下 2 × 2 表的一般形式，见[表 11-2](ch11.xhtml#ch11tab2)。我们现在将定义另外两个量：

![image](Images/257equ01.jpg)

*真正例率（TPR）*是指实际类 1 的实例被模型正确识别的概率。TPR 常常有其他名称：*敏感度*、*召回率*和*命中率*。你可能在医学文献中看到它被称为*敏感度*。

*真负例率（TNR）*是指实际类 0 的实例被模型正确识别的概率。TNR 也叫做*特异性*，特别是在医学文献中经常使用这个术语。这两个量作为概率，其值介于 0 和 1 之间，值越高越好。一个完美的分类器会有 TPR = TNR = 1.0；这发生在它不犯任何错误时，因此 FP = FN = 0，始终如此。

TPR 和 TNR 需要一起理解才能评估一个模型。例如，我们之前提到过，如果类 1 很少见，而模型总是预测类 0，那么它的准确率会很高。如果我们查看该情况下的 TPR 和 TNR，就会发现 TNR 为 1，因为模型从不将类 0 的实例预测为类 1（FP = 0）。然而，TPR 也是 0，原因恰恰相反，所有实际的类 1 实例都会被错误地识别为假阴性；它们被归类为类 0。因此，这两个指标一起立即表明该模型并不好。

那么，在乳腺癌的例子中，假阴性可能是致命的，我们希望在这种情况下 TPR 和 TNR 是怎样的呢？理想情况下，当然是希望它们都尽可能高，但如果 TPR 非常高而 TNR 较低，我们可能还是愿意使用该模型。在这种情况下，我们知道实际的乳腺癌几乎总是能被检测到。为什么？因为假阴性（FN）的数量几乎为 0，所以 TPR 的分母接近 TP，这意味着 TPR 大约为 1.0。另一方面，如果我们容忍假阳性（实际为负类的实例被模型判定为恶性），我们会发现 TNR 可能远低于 1.0，因为 TNR 的分母包含了假阳性计数。

TPR 和 TNR 告诉我们模型识别实际类 1 和类 0 实例的可能性。然而，它们并没有告诉我们我们应该多大程度上相信模型的输出。例如，如果模型说“类 1”，我们应该相信吗？为了做出这样的评估，我们需要另外两个直接来源于 2 × 2 表的指标：

![image](Images/258equ01.jpg)

*正向预测值（PPV）*通常被称为*精准度（precision）*。它是指当模型预测某个实例为类1时，实际它属于类1的概率。同样，*负向预测值（NPV）*是模型预测某个实例为类0时，该预测是正确的概率。这两个值的范围都是0到1之间，值越大越好。

TPR和PPV之间唯一的区别是是否考虑假阴性（FN）或假阳性（FP）在分母中的影响。通过包括假阳性，即模型预测为类1但实际为类0的实例，我们可以得到模型输出正确的概率。

对于一个始终预测为类0的模型，PPV是未定义的，因为TP和FP都是零。所有的类1实例都被计入假阴性（FN），而真阴性（TN）计数包括所有实际为类0的实例。对于TPR很高，但TNR不高的情况，我们有一个非零的FP计数，因此PPV会下降。让我们编造一个例子来看看为什么会这样，以及我们如何理解它。

假设我们的乳腺癌模型生成了以下2 × 2表格（[表11-4](ch11.xhtml#ch11tab4)）。

**表11-4：** 乳腺癌数据集的假设2 × 2表格

|  | **实际 1** | **实际 0** |
| --- | --- | --- |
| *模型预测为1* | 312 | 133 |
| *模型预测为0* | 6 | 645 |

在这个例子中，我们到目前为止覆盖的指标是

*TPR* = 0.9811

*TNR* = 0.8398

*PPV* = 0.7011

*NPV* = 0.9908

这意味着一个真正恶性的病例，模型有98%的概率将其预测为恶性，而一个良性病例只有84%的概率被预测为良性。70%的PPV意味着当模型说“恶性”时，实际上只有70%的机会该病例是真正的恶性；然而，由于高TPR，我们知道“恶性”输出中几乎包含了所有实际的乳腺癌病例。还要注意，这意味着NPV很高，所以当模型说“良性”时，我们非常有信心该实例不是乳腺癌。这就是为什么即使PPV低于100%，模型仍然有用。在临床环境中，当模型说“恶性”时，可能需要进一步检查，但一般情况下，如果它说“良性”，通常不需要进一步测试。当然，这些指标的可接受水平取决于模型的应用场景。有些人可能认为，考虑到错过癌症检测的潜在高昂成本，99.1%的NPV可能太低了。像这样的思考，可能也促使了推荐的筛查频率。

我们还可以从2 × 2表格中轻松得出两个额外的基本指标：

![image](Images/259equ01.jpg)

这些指标告诉我们，如果实际类别是类别0，则样本为假阳性的可能性，或者如果实际类别是类别1，则样本为假阴性的可能性。FPR将在稍后使用曲线评估模型时再次出现。注意，FPR = 1 – TNR 和 FNR = 1 – TPR。

计算这些基本指标非常简单，尤其是如果我们使用之前定义的`tally_predictions`函数的输出作为输入（见[清单 11-2](ch11.xhtml#ch11lis2)）：

def basic_metrics(tally):

tp, tn, fp, fn, _ = tally

return {

"TPR": tp / (tp + fn),

"TNR": tn / (tn + fp),

"PPV": tp / (tp + fp),

"NPV": tn / (tn + fn),

(*\pagebreak*)

"FPR": fp / (fp + tn),

"FNR": fn / (fn + tp)

}

*清单 11-2：计算基本指标*

我们将通过`tally_predictions`返回的列表进行分解，忽略准确度，然后构建并返回一个字典，包含我们描述的六个基本指标。当然，健壮的代码应该检查分母为零的病态情况，但为了保持展示的清晰性，我们在此忽略了这些代码。

#### 使用我们的指标来解释模型

我们将使用`tally_predictions`和`basic_metrics`来解释一些模型。我们将使用MNIST数据的向量形式，但只保留数字3和5，以便我们有一个二分类器。代码与我们在[第7章](ch07.xhtml#ch07)中使用的 *mnist_experiments.py* 中的代码类似。

只保留数字3和5后，我们有11,552个训练样本（6,131个3，5,421个5）和1,902个测试样本，其中1,010个是3，892个是5。实际代码位于*mnist_2x2_tables.py* 中，选定的输出见[表格 11-5](ch11.xhtml#ch11tab5)。

**表格 11-5：** 来自 MNIST 3 与 5 模型的选定输出及对应的基本指标

| **模型** | **TP** | **TN** | **FP** | **FN** |
| --- | --- | --- | --- | --- |
| *Nearest Centroid* | 760 | 909 | 101 | 132 |
| *3-NN* | 878 | 994 | 16 | 14 |
| *Naïve Bayes* | 612 | 976 | 34 | 280 |
| *RF 500* | 884 | 1,003 | 7 | 8 |
| *LinearSVM* | 853 | 986 | 24 | 39 |
| **模型** | **TPR** | **TNR** | **PPV** | **NPV** | **FPR** | **FNR** |
| --- | --- | --- | --- | --- | --- | --- |
| *Nearest Centroid* | 0.8520 | 0.9000 | 0.8827 | 0.8732 | 0.1000 | 0.1480 |
| *3-NN* | 0.9843 | 0.9842 | 0.9821 | 0.9861 | 0.0158 | 0.0157 |
| *Naïve Bayes* | 0.6851 | 0.9663 | 0.9474 | 0.7771 | 0.0337 | 0.3139 |
| *RF 500* | 0.9910 | 0.9931 | 0.9921 | 0.9921 | 0.0069 | 0.0090 |
| *LinearSVM* | 0.9563 | 0.9762 | 0.9726 | 0.9620 | 0.0238 | 0.0437 |

在[表格 11-5](ch11.xhtml#ch11tab5)中，我们可以看到顶部的原始计数和底部定义的指标。数字很多！我们稍微解析一下，看看到底发生了什么。我们将重点关注表格底部的指标。前两列显示了真正阳性率（敏感性，召回率）和真正阴性率（特异性）。这些值应该一起考察。

如果我们查看最近质心分类器的结果，我们看到 TPR = 0.8520 和 TNR = 0.9000。这里类 1 是五，类 0 是三。因此，最近质心分类器将 85% 的五正确地标为“五”。同样，它会将 90% 的三正确地标为“三”。虽然不算太差，但也不值得大惊小怪。看一下各列数据，我们可以看到两种模型在这些度量上表现得非常好：3-NN 和 500 棵树的随机森林。在这两种情况下，TPR 和 TNR 几乎相同，并且接近 1.0。这是模型表现良好的标志。绝对完美的情况是 TPR = TNR = PPV = NPV = 1.0，且 FPR = FNR = 0.0。我们越接近完美，模型就越好。如果要为这个分类器选择最佳模型，我们可能会选择随机森林，因为它在测试集上最接近完美。

简单看一下朴素贝叶斯的结果。TNR（特异性）相当高，大约为 97%。然而，68.5% 的 TPR（敏感性）就显得相当差劲。大致来说，模型只能正确分类每三个五个中的两个。如果我们检查接下来的两列，即正预测值（PPV）和负预测值（NPV），我们可以看到 PPV 为 94.7%，这意味着当模型确实预测输入为五时，我们可以有一定信心它是五。然而，负预测值却不那么理想，为 77.7%。查看[表 11-5](ch11.xhtml#ch11tab5)的顶部部分，可以看出这种情况的原因。测试集中 1010 个三的样本中，假阳性（FP）只有 34 个，但假阴性（FN）很高：有 280 个五被标记为“三”。这就是该模型负预测值较低的原因。

这里有一个关于这些度量的好经验法则：表现良好的模型应具有接近 1.0 的 TPR、TNR、PPV 和 NPV，FPR 和 FNR 应接近 0.0。

再看一下[表 11-5](ch11.xhtml#ch11tab5)，特别是随机森林的较低度量值。正如其名称所示，FPR 和 FNR 值是比率。我们可以利用它们来估计在使用该模型时，假阳性（FP）和假阴性（FN）发生的频率。例如，如果我们给模型呈现 *N* = 1000 个实际为三（类别 0）的案例，我们可以使用 FPR 来估算模型将多少个三误判为五（类别 1）：

估算的假阳性数 = FPR × N = 0.0069(1000) ≈ 7

通过类似的计算，我们可以得到 *N* = 1000 时的假阴性估算数：实际上是五的实例：

估算的假阴性数 = FNR × N = 0.0090(1000) = 9

同样适用于 TPR 和 TNR，它们的名称中也包含“比率”一词（*N* = 1000，分别表示实际的三和五）：

估算的真阳性数 = TPR × N = 0.9910(1000) = 991

估算的假阴性数 = FNR × N = 0.9931(1000) = 993

这些计算展示了该模型在测试数据上的表现。

### 更高级的度量

让我们在这一部分中看一下我随意称之为*更高级的度量*的内容。我之所以说它们更高级，是因为它们并不是直接使用2×2表格中的条目，而是基于从表格本身计算得出的值。特别地，我们将探讨五个高级度量：知情度、标记度、F1分数、Cohen's kappa和Matthews相关系数（MCC）。

#### 知情度和标记度

*知情度*和*标记度*是相互关联的。它们在这一部分中的其他度量可能没有那么知名，但希望它们在未来会被更多人了解。我之前提到过，TPR（敏感性）和TNR（特异性）应该一起解释。知情度（也称为Youden的*J*统计量）正是做到了这一点：

知情度 = TPR + TNR − 1

知情度是一个在[*–*1,+1]区间的数字，结合了TPR和TNR。知情度越高越好。知情度为0意味着随机猜测，而知情度为1意味着完美（在测试集上）。知情度小于0可能意味着模型比随机猜测还要差。知情度为–1意味着所有真实正例都被错误地标记为负类，反之亦然。在这种情况下，我们可以交换模型想要分配给每个输入的标签，从而得到一个相当不错的模型。只有病态模型才会导致负的知情度值。

标记度将正预测值和负预测值结合起来，就像知情度将TPR和TNR结合起来一样：

标记度 = PPV + NPV − 1

我们看到它与知情度具有相同的范围。知情度说明模型在正确标记来自每个类别的输入时表现如何。标记度说明模型在做出特定标签的预测时，是否能正确预测这个标签，无论它是类别0还是类别1。随机猜测会使标记度接近0，完美的模型则会使标记度接近1。

我喜欢知情度和标记度各自能够在一个数字中捕捉到模型性能的关键方面。有人声称，这些度量不受特定类别的先验概率的影响。这意味着如果类别1显著少于类别0，知情度和标记度也不会受到影响。欲了解详细信息，请参见David Martin Powers的《评估：从精确度、召回率和F值到ROC、知情度、标记度和相关性》。

#### F1分数

*F1分数*，无论对与错，广泛使用，我们应该对它有所了解。F1分数将两个基本度量合并为一个。它的定义可以通过精确度（PPV）和召回率（TPR）来简单描述：

![image](Images/263equ01.jpg)

F1分数是一个位于[0,1]之间的数值，值越高越好。这个公式从哪里来呢？它在这种形式下并不直观，但F1分数是精确度和召回率的调和均值。调和均值是倒数均值的倒数，计算方式如下：

![image](Images/263equ02.jpg)

F1 分数的一个批评是它没有像信息度（informedness）那样考虑真正的负样本（通过 TNR）。如果我们查看 PPV 和 TPR 的定义，我们会发现这两个量完全依赖于来自 2 × 2 表格的 TP、FP 和 FN 计数，而不是 TN 计数。此外，F1 分数对精确度和召回率赋予了相等的权重。精确度受假阳性影响，而召回率受假阴性影响。从之前的乳腺癌模型中，我们看到假阴性的人工成本远高于假阳性。有些人认为，在评估模型性能时，必须考虑这一点，确实应该考虑。然而，如果假阳性和假阴性的相对成本相同，那么 F1 分数将更具意义。

#### Cohen’s Kappa

*Cohen’s kappa* 是机器学习中常见的另一项统计量。它试图考虑模型可能会因偶然而将输入分配到正确类别的可能性。在数学上，该指标定义为：

![image](Images/263equ03.jpg)

其中 *p*[*o*] 是观察到的准确率，*p*[*e*] 是随机情况下的预期准确率。对于 2 × 2 表格，这些值被定义为：

![image](Images/264equ01.jpg)

其中 N 是测试集中的样本总数。

Cohen’s kappa 通常介于 0 和 1 之间。0 表示分配的类别标签和给定的类别标签之间完全不同。负值表示比随机一致性还差。接近 1 的值表示强一致性。

#### Matthews 相关系数

我们的最终指标是 *Matthews 相关系数 (MCC)*。它是信息度和标记度的几何平均值。从这个意义上说，它就像 F1 分数一样，将两个指标合并成一个。

MCC 定义为：

![image](Images/264equ02.jpg)

在数学上，它的计算结果是信息度和标记度的几何平均值：

![image](Images/264equ03.jpg)

许多人青睐 MCC，因为它考虑了完整的 2 × 2 表格，包括两个类别的相对频率（类别先验概率）。这是 F1 分数所没有做的，因为它忽略了真正的负样本。

MCC 是一个介于 0 和 1 之间的数字，值越高越好。如果只考虑一个指标来评估二分类模型，那就选 MCC。请注意，MCC 的分母中有四个求和项。如果其中一个求和项为 0，整个分母将为 0，这就成了一个问题，因为我们无法除以 0。幸运的是，在这种情况下，分母可以替换为 1，从而得到一个仍然有意义的结果。表现良好的模型的 MCC 接近 1.0。

#### 实现我们的指标

让我们编写一个函数，从给定的 2 × 2 表格构建这些指标。代码见 [Listing 11-3](ch11.xhtml#ch11lis3)：

from math import sqrt

def advanced_metrics(tally, m):

tp, tn, fp, fn, _ = tally

n = tp+tn+fp+fn

po = (tp+tn)/n

pe = (tp+fn)*(tp+fp)/n**2 + (tn+fp)*(tn+fn)/n**2

return {

"F1"：2.0 * m["PPV"] * m["TPR"] / (m["PPV"] + m["TPR"]),

"MCC"： (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)),

"kappa"： (po - pe) / (1.0 - pe),

"知情性"：m["TPR"] + m["TNR"] - 1.0,

"标记性"：m["PPV"] + m["NPV"] - 1.0

}

*清单 11-3：计算高级指标*

为了简化，我们不检查 MCC 分母是否为 0，完整实现中会进行此检查。

这段代码将计数和基本指标作为参数，并返回一个包含更高级指标的新字典。让我们看看当我们计算高级指标时，来自 [表 11-5](ch11.xhtml#ch11tab5) 的 MNIST 示例表现如何。

[表 11-6](ch11.xhtml#ch11tab6) 显示了此部分针对 MNIST 3 对 5 模型的指标。有几点值得注意。首先，F1 得分总是高于 MCC 或 Cohen 的 *κ*。在某种程度上，F1 得分过于乐观。如前所述，F1 得分没有考虑真正负例，而 MCC 和 Cohen 的 *κ* 都考虑了这一点。

**表 11-6：** MNIST 3 对 5 模型的选定输出及其相应的高级指标

| **模型** | **F1** | **MCC** | **Cohen 的 *κ*** | **知情性** | **标记性** |
| --- | --- | --- | --- | --- | --- |
| *最近质心* | 0.8671 | 0.7540 | 0.7535 | 0.7520 | 0.7559 |
| *3-NN* | 0.9832 | 0.9683 | 0.9683 | 0.9685 | 0.9682 |
| *朴素贝叶斯* | 0.7958 | 0.6875 | 0.6631 | 0.6524 | 0.7244 |
| *RF 500* | 0.9916 | 0.9842 | 0.9842 | 0.9841 | 0.9842 |
| *线性SVM* | 0.9644 | 0.9335 | 0.9334 | 0.9325 | 0.9346 |

另外需要注意的是，表现良好的模型，如 3-NN 和随机森林，在所有这些指标中得分都很高。当模型表现良好时，F1 得分和 MCC 之间的差异小于模型表现不佳时的差异（例如，朴素贝叶斯）。还需要注意的是，MCC 总是在知情性和标记性之间，正如几何均值所表现的那样。最后，从 [表 11-5](ch11.xhtml#ch11tab5) 和 [表 11-6](ch11.xhtml#ch11tab6) 中的数值可以看出，基于 MCC 值 0.9842，表现最好的模型是随机森林。

在本节以及前两节中，我们查看了许多指标，并了解了它们如何计算和解读。一个表现良好的模型会在所有这些指标上得分很高。这是一个好模型的标志。当我们评估的模型表现不如预期时，各指标之间的相对差异以及这些指标的意义才真正显现出来。这时我们需要考虑特定指标值以及模型错误（FP 和 FN）所带来的成本。在这种情况下，我们必须结合判断力和具体问题因素来决定最终选择哪个模型。

现在，让我们换个角度，看看评估模型性能的图形化方法。

### 接收者操作特征曲线

他们说一张图片胜过千言万语。在这一节中，我们将学习到，事实上，一张图片——更准确地说，是一条曲线——可以比前几节中的度量指标捕捉到更多的模型性能。具体而言，我们将学习一种广泛使用的*接收者操作特征（ROC）曲线*：它是什么，如何绘制，以及如何使用 sklearn 来为我们绘制它。

#### 收集我们的模型

为了绘制曲线，我们需要一个输出属于类别 1 的概率的模型。在前几节中，我们使用的是输出类别标签的模型，以便我们可以统计 TP、TN、FP 和 FN 的数量。对于我们的 ROC 曲线，我们仍然需要这些计数，但与模型输出类别标签不同，我们需要的是类别 1 的概率。我们将对这些概率应用不同的阈值，以决定给输入分配什么标签。

幸运的是，传统神经网络（以及我们将在[第 12 章](ch12.xhtml#ch12)中看到的深度网络）输出所需的概率。如果我们使用 sklearn，其他经典模型也可以输出概率估计，但为了简化问题，我们在此忽略这一点。

我们的测试案例是通过神经网络训练来决定偶数 MNIST 数字（类别 0）和奇数 MNIST 数字（类别 1）之间的区分。我们的输入是我们在本书中一直使用的数字的向量形式。我们可以使用在[第 5 章](ch05.xhtml#ch05)中创建的训练和测试数据——我们只需要重新编码标签，将数字 0、2、4、6 和 8 定义为类别 0，而数字 1、3、5、7 和 9 定义为类别 1。只需几行代码即可轻松完成：

old = np.load("mnist_train_labels.npy")

new = np.zeros(len(old), dtype="uint8")

new[np.where((old % 2) == 0)] = 0

new[np.where((old % 2) == 1)] = 1

np.save("mnist_train_even_odd_labels.npy", new)

old = np.load("mnist_test_labels.npy")

new = np.zeros(len(old), dtype="uint8")

new[np.where((old % 2) == 0)] = 0

new[np.where((old % 2) == 1)] = 1

np.save("mnist_test_even_odd_labels.npy", new)

目录路径指向存储其他 MNIST 数据的相同位置。我们利用偶数除以 2 的余数总是 0 或 1，具体取决于数字是偶数还是奇数这一事实。

我们将测试哪些模型？为了强调各个模型之间的差异，我们将故意训练一些我们知道远非理想的模型。具体而言，我们将使用以下代码来生成模型并生成概率估计：

import numpy as np

from sklearn.neural_network import MLPClassifier

def run(x_train, y_train, x_test, y_test, clf):

clf.fit(x_train, y_train)

return clf.predict_proba(x_test)

def nn(layers):

return MLPClassifier(solver="sgd", verbose=False, tol=1e-8,

nesterovs_momentum=False, early_stopping=False, batch_size=64,

learning_rate_init=0.001, momentum=0.9, max_iter=200,

hidden_layer_sizes=layers, activation="relu")

def main():

x_train = np.load("mnist_train_vectors.npy").astype("float64")/256.0

y_train = np.load("mnist_train_even_odd_labels.npy")

x_test = np.load("mnist_test_vectors.npy").astype("float64")/256.0

y_test = np.load("mnist_test_even_odd_labels.npy")

x_train = x_train[:1000]

y_train = y_train[:1000]

layers = [(2,), (100,), (100,50), (500,250)]

mlayers = ["2", "100", "100x50", "500x250"]

for i, layer in enumerate(layers):

prob = run(x_train, y_train, x_test, y_test, nn(layer))

np.save("mnist_even_odd_probs_%s.npy" % mlayers[i], prob)

代码可以在文件 *mnist_even_odd.py* 中找到。run 和 nn 函数应该很熟悉。我们在[第 10 章](ch10.xhtml#ch10)中使用了几乎相同的版本，其中 nn 返回配置好的 MLPClassifier 对象，run 训练分类器并返回测试集上的预测概率。主函数加载训练集和测试集，将训练集限制为前 1000 个样本（大约 500 个偶数和 500 个奇数），然后循环遍历我们将训练的隐藏层大小。前两个是单隐藏层网络，分别有 2 和 100 个节点。最后两个是双隐藏层网络，每层有 100 × 50 和 500 × 250 个节点。

#### 绘制我们的指标

clf.predict_proba 的输出是一个矩阵，行数与测试样本的数量相同（此例中为一万）。矩阵的列数与类别的数量相同；由于我们处理的是二分类器，每个样本有两列。第一列是样本为偶数（类别 0）的概率，第二列是样本为奇数（类别 1）的概率。例如，某个模型的前 10 个输出见[表 11-7](ch11.xhtml#ch11tab7)。

**表 11-7：** 示例模型输出，显示每个类别的概率分配及实际原始类别标签

| **类别 0** | **类别 1** | **实际标签** |
| --- | --- | --- |
| 0.009678 | 0.990322 | 3 |
| 0.000318 | 0.999682 | 3 |
| 0.001531 | 0.998469 | 7 |
| 0.007464 | 0.992536 | 3 |
| 0.011103 | 0.988897 | 1 |
| 0.186362 | 0.813638 | 7 |
| 0.037229 | 0.962771 | 7 |
| 0.999412 | 0.000588 | 2 |
| 0.883890 | 0.116110 | 6 |
| 0.999981 | 0.000019 | 6 |

第一列是偶数的概率，第二列是奇数的概率。第三列是样本的实际类别标签，显示预测结果完全正确。奇数数字的类别 1 概率较高，类别 0 概率较低，而偶数样本则正好相反。

当我们从一个保留的测试集的模型表现构建一个 2 × 2 表时，我们会得到一组 TP、TN、FP 和 FN 数值，从中可以计算出前面章节中提到的所有指标。这包括真正率（TPR，灵敏度）和假正率（FPR，等于 1 - 特异度）。在表中隐含的是我们用来决定模型输出何时应视为类别 1 或类别 0 的阈值。在前面的章节中，这个阈值为 0.5。如果输出值 ≥ 0.5，我们将样本分配给类别 1；否则，我们将其分配给类别 0。有时你会看到这个阈值作为下标出现，例如 TPR[0.5] 或 FPR[0.5]。

从数学上讲，我们可以将从 2 × 2 表中计算出的 TPR 和 FPR 视为 FPR（x 轴）与 TPR（y 轴）平面上的一个点，具体来说，就是点（FPR，TPR）。由于 FPR 和 TPR 都在 0 到 1 之间变化，点（FPR，TPR）将位于边长为 1 的正方形内，该正方形的左下角位于点（0,0），右上角位于点（1,1）。每次我们改变决策阈值时，就会得到一个新的 2 × 2 表，进而得到 FPR 与 TPR 平面上的一个新点。例如，如果我们将决策阈值从 0.5 改为 0.3，使得每个输出的类别 1 概率为 0.3 或更高时被归为类别 1，我们将得到一个新的 2 × 2 表和一个新点（FPR[0.3]，TPR[0.3]）在平面上。当我们系统地将决策阈值从高到低变化时，我们生成一系列的点，可以将这些点连接起来形成一条曲线。

通过这种方式改变参数生成的曲线被称为 *参数曲线*。这些点是阈值的函数。我们将阈值值称为 *θ*（希腊字母 theta），并让其从接近 1 变化到接近 0\. 这样，我们可以计算出一组点（FPR[*θ*]，TPR[*θ*]），这些点绘制出来后，会在 FPR 与 TPR 平面上形成一条曲线。正如前面所提到的，这条曲线有一个名字：接收者操作特征（ROC）曲线。让我们来看一下 ROC 曲线，并探讨这样的曲线能告诉我们什么。

#### 探索 ROC 曲线

[图 11-1](ch11.xhtml#ch11fig1) 显示了带有 100 个节点单隐层的 MNIST 奇偶模型的 ROC 曲线。

![image](Images/11fig01.jpg)

*图 11-1：标记了关键元素的 ROC 曲线*

标记点表示给定阈值下的假阳性率（FPR）和真正率（TPR）。虚线是从(0,0)到(1,1)的对角线。这条虚线表示一个随机猜测输出的分类器。我们的曲线越接近这条虚线，模型的性能就越差。如果你的曲线与虚线重合，那你不如抛个硬币随机分配标签。任何低于虚线的曲线都比随机猜测*差*。如果模型完全错误，即它将所有类1实例都标为类0，*反之亦然*，就会发生一件有趣的事：我们可以通过将所有类1的输出改为类0，所有类0的输出改为类1，将这个完全错误的模型转变为一个完美正确的模型。不过，遇到如此糟糕的模型的可能性不大。

[图11-1](ch11.xhtml#ch11fig1)中的ROC曲线在图表的左上角标有一个点，标记为*完美*。这就是我们所追求的理想状态。我们希望ROC曲线朝着这个点上升并向左移动。曲线越接近这个点，模型在测试集上的表现就越好。一个完美的模型将会有一条ROC曲线，先垂直跃升到这个点，然后水平延伸到(1,1)点。[图11-1](ch11.xhtml#ch11fig1)中的ROC曲线朝着正确的方向移动，代表了一个表现相当不错的模型。

注意标记的*θ*值。我们可以通过调整*θ*来选择模型的表现水平。在这种情况下，典型的默认值0.5能为我们提供最佳的性能，因为该阈值返回的TPR和FPR有最佳的平衡，是图表上最接近左上角的点。然而，我们可能有理由使用不同的*θ*值。如果我们将*θ*设置得很小，比如0.1，我们就会沿着曲线向右移动。这样会发生两件事。首先，TPR上升到约0.99，意味着我们正确地将大约99%的实际类1实例分配给类1。其次，FPR也会上升，达到约0.32，这意味着我们会同时将大约32%的真实负类（类0）错误地标为类1。如果我们的任务是可以容忍将一些负类实例判定为“正类”，而知道我们几乎不会把正类判定为负类，我们可能会选择将阈值改为0.1。以之前的乳腺癌例子为例：我们绝不希望将正类判定为负类，所以我们宁可容忍更多的假阳性，以确保不会错标任何实际的正类。

将阈值(*θ*)调整为0.9意味着什么？在这种情况下，我们沿着曲线向左移动，来到了一个假阳性率非常低的点。如果我们希望在模型判断为“类1”时，能够以较高的信心确认它确实是类1的实例，我们可能会这么做。这意味着我们希望获得较高的正预测值（PPV，精度）。回想一下PPV的定义：

![image](Images/270equ01.jpg)

如果FP较低，PPV较高。将*θ*设为0.9可以使任意给定测试集的FP较低。对于[图11-1](ch11.xhtml#ch11fig1)中的ROC曲线，移动到*θ* = 0.9时，FPR约为0.02，TPR约为0.71，PPV约为0.97。在*θ* = 0.9时，当模型输出“类1”时，模型正确的概率为97%。相比之下，在*θ* = 0.1时，PPV约为76%。高阈值可以用于我们关注于确定位于类1的样本，而不在意可能会漏掉某些类1样本的情况。

改变阈值*θ*会让我们沿着ROC曲线移动。这样做时，我们应该预期上一节中的指标也会随着*θ*的变化而变化。[图11-2](ch11.xhtml#ch11fig2)展示了MCC和PPV随着*θ*变化的情况。

![image](Images/11fig02.jpg)

*图11-2：随着决策阈值（*θ*）变化，MNIST奇偶模型的MCC（圆圈）和PPV（方块）如何变化，[图11-1](ch11.xhtml#ch11fig1)中的模型*

在图中，我们看到随着阈值的增大，PPV也在增大。当模型判定一个输入属于类1时，它变得更加自信。然而，这一点被MCC的变化所制约，正如我们之前所看到的，MCC是衡量模型整体表现的优秀单一指标。在这种情况下，最高的MCC出现在*θ* = 0.5时，随着阈值的增加或减少，MCC会下降。

#### 使用ROC分析比较模型

ROC曲线为我们提供了大量信息。它也非常适合用于比较模型，即使这些模型在架构或方法上有很大的不同。然而，在进行比较时必须小心，确保生成曲线的测试集理想情况下是相同的，或者非常接近。

让我们使用ROC分析来比较我们之前训练的不同MNIST奇偶数字模型。我们将看看这是否有助于我们在它们之间做出选择。

[图11-3](ch11.xhtml#ch11fig3)展示了这些模型的ROC曲线，图中有一个插图，放大了图表的左上角，便于区分不同的模型。每个隐藏层中的节点数量已标明，用于识别模型。

![image](Images/11fig03.jpg)

*图11-3：MNIST奇偶模型的ROC曲线。模型的隐藏层大小已标明。*

我们立刻看到一条ROC曲线明显不同于其他三条。这是具有单一隐藏层和两个节点的模型的ROC曲线。所有其他ROC曲线都位于这条曲线之上。一般来说，如果一条ROC曲线完全位于另一条之上，那么生成该曲线的模型可以认为是更优的。所有较大的MNIST奇偶模型都优于只有两个节点的隐藏层模型。

另外三种模型彼此之间非常接近，那我们该如何选择呢？这个决策并不总是明确的。根据我们关于ROC曲线的经验法则，我们应该选择包含500和250节点的两层模型，因为它的ROC曲线优于其他模型。然而，根据具体应用场景，我们可能会犹豫。这个模型有超过50万个参数，运行它需要使用所有这些参数。而100 × 50模型包含略多于80,000个参数，只有大模型的不到五分之一。我们可能会决定，处理速度的考虑超过了大模型性能略微提升的需求，从而选择了较小的模型。ROC分析向我们表明，选择较小模型只会导致轻微的性能损失。

比较ROC曲线时需要考虑的另一个因素是当FPR较小时曲线的斜率。一个完美的模型具有垂直的斜率，因为它会立即从点(0,0)跳到(0,1)。因此，更好的模型会在低FPR区域拥有更陡的斜率。

从ROC曲线派生出的一个常用度量是其下方的面积。这个面积通常缩写为*AUC*，在医学领域也称为*Az*。完美的ROC曲线具有AUC为1.0，因为曲线从(0,0)跳到(0,1)，然后到(1,1)，形成一个边长为1的正方形，面积为1。一个随机猜测的模型（ROC图中的对角线）具有AUC为0.5，即由虚线对角线形成的三角形的面积。要计算任意ROC曲线下的面积，需要进行数值积分。幸运的是，sklearn知道如何做到这一点，因此我们不需要自己去做。稍后我们将看到这一点。

人们常常报告AUC（曲线下面积），但随着时间的推移，我对它的支持越来越少。主要原因是AUC将高度信息化的图表替换成了一个单一的数字，但不同的ROC曲线可能会产生相同的AUC。如果两条曲线的AUC相同，但一条曲线偏向右侧而另一条在低FPR（假阳性率）区域有较陡的斜率，我们可能会认为这两个模型在性能上大致相当，然而实际上，具有较陡斜率的模型更可能是我们想要的，因为它能在没有过多假阳性的情况下达到合理的TPR（真正率）。

使用AUC时的另一个注意事项是，AUC在其他参数发生相当显著变化时变化很小。这使得基于AUC值的轻微差异做出准确判断变得非常困难。例如，MNIST偶数/奇数模型的AUC值为0.9373，而隐藏层有100个节点的模型AUC为0.9722。两者都远高于0.9（满分为1.0），那么，它们是否差不多呢？我们知道它们不一样，因为ROC曲线清楚地显示，具有两个节点的模型明显低于另一个。

#### 生成ROC曲线

我们现在已经准备好学习如何创建 ROC 曲线。获得 ROC 曲线和 AUC 的简单方法是使用 sklearn：

import os

import sys

import numpy as np

import matplotlib.pylab as plt

from sklearn.metrics import roc_auc_score, roc_curve

def main():

labels = np.load(sys.argv[1])

probs = np.load(sys.argv[2])

pname = sys.argv[3]

auc = roc_auc_score(labels, probs[:,1])

roc = roc_curve(labels, probs[:,1])

print("AUC = %0.6f" % auc)

plt.plot(roc[0], roc[1], color='r')

plt.plot([0,1],[0,1], color='k', linestyle=':')

plt.xlabel("FPR")

plt.ylabel("TPR")

plt.tight_layout(pad=0, w_pad=0, h_pad=0)

plt.savefig(pname, dpi=300)

plt.show()

该例程读取一组标签和相关的每个类别的概率，例如前面代码部分生成的输出。然后调用 sklearn 函数 roc_auc_score 和 roc_curve 分别返回 AUC 和 ROC 点。ROC 曲线将被绘制、保存到磁盘并显示。

我们不必将 sklearn 视为黑盒。我们可以快速生成 ROC 曲线的点。我们加载相同的输入，标签和每个类别的概率，但不是调用库函数，我们遍历感兴趣的阈值并为每个阈值计算 TP、TN、FP 和 FN。通过这些，我们可以直接计算 FPR 和 TPR，从而得到需要绘制的点集。实现这一点的代码是简单直接的：

def table(labels, probs, t):

tp = tn = fp = fn = 0

for i,l in enumerate(labels):

c = 1 if (probs[i,1] >= t) else 0

如果 (l == 0) 且 (c == 0):

tn += 1

如果 (l == 0) 且 (c == 1):

fp += 1

如果 (l == 1) 且 (c == 0):

fn += 1

如果 (l == 1) 且 (c == 1):

tp += 1

return [tp, tn, fp, fn]

def main():

labels = np.load(sys.argv[1])

probs = np.load(sys.argv[2])

pname = sys.argv[3]

th = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]

roc = []

对于 t 在 th 中：

tp, tn, fp, fn = table(labels, probs, t)

tpr = tp / (tp + fn)

fpr = fp / (tn + fp)

roc.append([fpr, tpr])

roc = np.array(roc)

xy = np.zeros((roc.shape[0]+2, roc.shape[1]))

xy[1:-1,:] = roc

xy[0,:] = [0,0]

xy[-1,:] = [1,1]

plt.plot(xy[:,0], xy[:,1], color='r', marker='o')

plt.plot([0,1],[0,1], color='k', linestyle=':')

plt.xlabel("FPR")

plt.ylabel("TPR")

plt.savefig(pname)

plt.show()

main 函数加载标签和概率。对 th 的循环应用不同的阈值，通过调用 table 函数将 ROC 点累积到 roc 中，table 函数计算当前阈值的 TP、TN、FP 和 FN。

table 函数遍历每个类别的概率，如果类别 1 的概率大于或等于当前的阈值，就分配类别标签为 1。然后将这个类别分配与实际的类别标签进行比较，并增加相应的计数器。

一旦计算出 ROC 点，就通过在点列表的开头添加 (0,0) 和在列表末尾添加 (1,1) 来绘制图形。这样做可以确保绘图扩展到 FPR 值的完整范围。然后将这些点绘制并保存到磁盘上。

#### 精确度-召回率曲线

在离开这一部分之前，我们应该提到另一种在机器学习中时常遇到的评估曲线。这就是 *精确度-召回率 (PR) 曲线*。顾名思义，它绘制了 PPV（精确度）和 TPR（召回率，敏感度）随着决策阈值变化的关系，就像 ROC 曲线一样。一个好的 PR 曲线应该朝向右上角移动，而不是像一个好的 ROC 曲线那样朝向左上角。这个曲线的点可以通过 sklearn 中的 metrics 模块的 precision_recall_curve 函数轻松生成。

我们没有花时间讨论这条曲线，因为它没有考虑到真正的负例。请参考 PPV 和 TPR 的定义来验证这一点。 我对 PR 曲线的偏见来源于与我对 F1 分数的偏见相同的担忧。由于没有考虑到真正的负例，PR 曲线和 F1 分数未能提供分类器质量的完整图像。当真正的正类稀有或真正的负类性能不重要时，PR 曲线确实有其用途。然而，通常来说，在评估分类器性能时，我认为最好坚持使用 ROC 曲线和我们已定义的度量标准。

### 处理多类

到目前为止，我们讨论的所有度量标准仅适用于二元分类器。当然，我们知道许多分类器是多类的：它们输出多个标签，而不仅仅是 0 或 1。为了评估这些模型，我们将把混淆矩阵的概念扩展到多类情况，并且会看到我们已经熟悉的一些度量标准也可以扩展到多类情况。

我们需要一些多类模型的结果。幸运的是，MNIST 数据本身就是多类的。回想一下，我们曾经为了将数据集转换为二元数据而麻烦地重新编码了标签。在这里，我们将使用相同的架构训练模型，但这次我们保留原始标签，使得模型输出十个标签之一：它为测试输入分配的数字标签，作为 MLPClassifier 类的 predict 方法的输出。我们不会展示代码，因为它与前一节中的代码完全相同，只不过是调用了 predict 而不是 predict_proba。

#### 扩展混淆矩阵

我们的二元度量标准基于 2 × 2 混淆矩阵。混淆矩阵可以很容易地扩展到多类情况。为此，我们让矩阵的行表示实际的类别标签，列表示模型的预测。矩阵是方形的，行数和列数与数据集中类别的数量相等。因此，对于 MNIST 数据集，我们得到一个 10 × 10 的混淆矩阵，因为有 10 个数字。

我们从实际已知的测试标签和模型的预测标签中计算混淆矩阵。sklearn 的 metrics 模块中有一个函数 confusion_matrix，我们可以使用它，但自己计算也很简单：

def confusion_matrix(y_test, y_predict, n=10):

cmat = np.zeros((n,n), dtype="uint32")

for i, y in enumerate(y_test):

cmat[y, y_predict[i]] += 1

return cmat

这里的 n 是类别的数量，MNIST 固定为 10。如果需要，我们也可以从提供的测试标签中确定它。

代码很简单。输入是实际标签（y_test）和预测标签（y_predict）向量，混淆矩阵（cmat）通过增加每个由实际标签和预测标签形成的索引来填充。例如，如果实际标签是 3，预测标签是 8，那么我们就将 cmat[3,8] 的值加 1。

让我们看一下具有一个 100 节点单隐藏层的模型的混淆矩阵（[表 11-8](ch11.xhtml#ch11tab8)）。

**表 11-8：** 具有 100 个节点单隐藏层模型的混淆矩阵

|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **9** |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **0** | 943 | 0 | 6 | 9 | 0 | 10 | 7 | 1 | 4 | 0 |
| **1** | 0 | 1102 | 14 | 5 | 1 | 1 | 3 | 1 | 8 | 0 |
| **2** | 16 | 15 | 862 | 36 | 18 | 1 | 17 | 24 | 41 | 2 |
| **3** | 3 | 1 | 10 | 937 | 0 | 20 | 3 | 13 | 17 | 6 |
| **4** | 2 | 8 | 4 | 2 | 879 | 0 | 14 | 1 | 6 | 66 |
| **5** | 19 | 3 | 3 | 53 | 13 | 719 | 17 | 3 | 44 | 18 |
| **6** | 14 | 3 | 4 | 2 | 21 | 15 | 894 | 1 | 4 | 0 |
| **7** | 3 | 21 | 32 | 7 | 10 | 1 | 0 | 902 | 1 | 51 |
| **8** | 17 | 14 | 11 | 72 | 11 | 46 | 21 | 9 | 749 | 24 |
| **9** | 10 | 11 | 1 | 13 | 42 | 5 | 2 | 31 | 10 | 884 |

行表示实际的测试样本标签，[0,9]。列表示模型分配的标签。如果模型完美，实际标签和预测标签之间会有一对一的匹配。这就是混淆矩阵的主对角线。因此，完美的模型会在主对角线上有条目，其他所有元素都为 0。尽管[表 11-8](ch11.xhtml#ch11tab8)不是完美的，但最大的计数值位于主对角线附近。

看看第 4 行和第 4 列。行列交汇处的值是 879。这个数字意味着，实际类别为 4 时，模型正确地预测了“4”作为标签，共有 879 次。如果我们查看第 4 行，会看到其他非零的数字。每个数字都代表实际为 4 的样本被模型错误地分类为其他数字。例如，有 66 次情况下，4 被错误地分类为“9”，但只有一次情况下，4 被标记为“7”。

第 4 列表示模型将输入分类为“4”的情况。正如我们所看到的，它正确分类了 879 次。然而，也有其他数字被模型错误地标记为“4”，例如 21 次将 6 错误地标记为“4”，或者一次将 1 错误地标记为“4”。没有将 3 错误地标记为“4”的情况。

混淆矩阵可以一目了然地告诉我们模型在测试集上的表现。我们可以迅速判断矩阵是否主要是对角线。如果是，这说明模型在测试集上表现良好。如果不是，我们需要仔细查看哪些类别被误分类为其他类别。对矩阵进行简单的调整可以帮助我们。我们可以将每一行的值除以该行的总和，而不是使用原始计数，这样可以将计数转换为分数。然后我们可以将每个值乘以 100，转换为百分比。这将混淆矩阵转换为我们所称的*准确率矩阵*。转换过程是直接的：

acc = 100.0*(cmat / cmat.sum(axis=1))

这里的 cmat 是混淆矩阵。它生成一个准确率矩阵，[表 11-9](ch11.xhtml#ch11tab9)。

**表 11-9：** 按类别准确率呈现的混淆矩阵

|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **9** |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **0** | **96.2** | 0. | 0.6 | 0.9 | 0. | 1.1 | 0.7 | 0.1 | 0.4 | 0. |
| **1** | 0. | **97.1** | 1.4 | 0.5 | 0.1 | 0.1 | 0.3 | 0.1 | 0.8 | 0. |
| **2** | 1.6 | 1.3 | **83.5** | 3.6 | 1.8 | 0.1 | 1.8 | 2.3 | 4.2 | 0.2 |
| **3** | 0.3 | 0.1 | 1. | **92.8** | 0. | 2.2 | 0.3 | 1.3 | 1.7 | 0.6 |
| **4** | 0.2 | 0.7 | 0.4 | 0.2 | **89.5** | 0. | 1.5 | 0.1 | 0.6 | 6.5 |
| **5** | 1.9 | 0.3 | 0.3 | 5.2 | 1.3 | **80.6** | 1.8 | 0.3 | 4.5 | 1.8 |
| **6** | 1.4 | 0.3 | 0.4 | 0.2 | 2.1 | 1.7 | **93.3** | 0.1 | 0.4 | 0. |
| **7** | 0.3 | 1.9 | 3.1 | 0.7 | 1. | 0.1 | 0. | **87.7** | 0.1 | 5.1 |
| **8** | 1.7 | 1.2 | 1.1 | 7.1 | 1.1 | 5.2 | 2.2 | 0.9 | **76.9** | 2.4 |
| **9** | 1. | 1. | 0.1 | 1.3 | 4.3 | 0.6 | 0.2 | 3. | 1. | **87.6** |

对角线显示了每个类别的准确率。表现最差的类别是 8，准确率为 76.9%，表现最好的类别是 1，准确率为 97.1%。非对角线元素表示模型将实际类别标记为其他类别的百分比。对于类别 0，模型将真实的零类错误标记为“5” 1.1% 的时间。

为什么8类的表现如此糟糕？通过查看8类的行，我们发现模型将实际的8类实例中7.1%的错误识别为“3”，5.2%的实例识别为“5”。将8误识为“3”是模型犯的最大单一错误，尽管也有6.5%的4类实例被标为“9”。稍加思考就能理解这些错误。人们多久会将8和3或4和9搞混？这个模型犯的错误类似于人类常犯的错误。

混淆矩阵也可以揭示出病态的表现。考虑[图11-3](ch11.xhtml#ch11fig3)中的MNIST模型，该模型只有一个隐藏层，且该层仅包含两个节点。它生成的准确度矩阵显示在[表11-10](ch11.xhtml#ch11tab10)中。

我们立刻可以看出这是一个较差的模型。第5列完全是零，意味着该模型对于任何输入都不会输出“5”。对于输出标签“8”和“9”也是如此。另一方面，模型更倾向于将输入识别为“0”、“1”、“2”或“3”，因为这些列对于各种输入数字都被密集填充。看着对角线，我们可以看到，只有1和3有合理的几率被正确识别，尽管其中许多会被标为“7”。8类很少被正确标记（1.3%）。一个表现差的模型通常会有这样的混淆矩阵，输出结果异常且有较大的非对角线值。

**表 11-10：** 只有两个节点的隐藏层模型的准确度矩阵

|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **9** |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **0** | **51.0** | 1.0 | 10.3 | 0.7 | 1.8 | 0.0 | 34.1 | 0.7 | 0.0 | 0.4 |
| **1** | 0.4 | **88.3** | 0.4 | 1.1 | 0.8 | 0.0 | 0.0 | 9.3 | 1.0 | 0.0 |
| **2** | 8.6 | 2.8 | **75.2** | 6.9 | 1.7 | 0.0 | 1.4 | 3.0 | 0.3 | 0.6 |
| **3** | 0.2 | 1.0 | 4.9 | **79.4** | 0.3 | 0.0 | 0.0 | 13.5 | 0.0 | 0.2 |
| **4** | 28.4 | 31.3 | 7.3 | 2.1 | **9.7** | 0.0 | 0.3 | 13.6 | 1.0 | 0.5 |
| **5** | 11.4 | 42.5 | 2.2 | 4.9 | 4.4 | **0.0** | 0.1 | 16.5 | 0.9 | 0.3 |
| **6** | 35.4 | 1.0 | 5.4 | 0.2 | 1.4 | 0.0 | **55.0** | 0.0 | 0.0 | 0.1 |
| **7** | 0.4 | 5.2 | 2.0 | 66.2 | 0.8 | 0.0 | 0.0 | **25.5** | 0.2 | 0.3 |
| **8** | 10.5 | 41.9 | 2.8 | 8.0 | 4.1 | 0.0 | 0.1 | 22.1 | **1.3** | 0.4 |
| **9** | 4.7 | 9.1 | 5.8 | 26.2 | 5.8 | 0.0 | 0.2 | 41.2 | 2.2 | **3.1** |

#### 计算加权准确度

准确率矩阵的对角元素告诉我们模型在每个类别上的准确率。我们可以通过平均这些值来计算整体准确率。然而，如果一个或多个类别在测试数据中比其他类别更为普遍，这可能会误导结果。我们应该使用加权平均，而不是简单平均。权重是基于每个类别的测试样本数与提供给模型的总测试样本数的比值。假设我们有三个类别，并且它们在测试集中的频率和每个类别的准确率如下表所示：[表 11-11](ch11.xhtml#ch11tab11)：

**表 11-11：** 三个类别模型的假设每个类别准确率

| **类别** | **频率** | **准确率** |
| --- | --- | --- |
| 0 | 4,004 | 88.1 |
| 1 | 6,502 | 76.6 |
| 2 | 8,080 | 65.2 |

在这里，*N* = 4,004 + 6,502 + 8,080 = 18586 个测试样本。然后，每个类别的权重显示在[表 11-12](ch11.xhtml#ch11tab12)中。

**表 11-12：** 每个类别的权重示例

| 类别 | 权重 |
| --- | --- |
| 0 | 4,004 / 18,586 = 0.2154 |
| 1 | 6,502 / 18,586 = 0.3498 |
| 2 | 8,080 / 18,586 = 0.4347 |

平均准确率可以计算为：

ACC = 0.2154 × 88.1 + 0.3498 × 76.6 + 0.4347 × 65.2 = 74.1

从哲学角度来看，如果我们知道每个类别的先验概率，应该将权重替换为实际的每个类别的先验概率。这些概率是类别在实际数据中出现的真实可能性。然而，如果我们假设测试集构造合理，那么仅使用每个类别的频率也是安全的。我们认为，正确构建的测试集将合理地代表真实的先验类别概率。

在代码中，加权平均准确率可以简洁地从混淆矩阵中计算得出：

def weighted_mean_acc(cmat):

N = cmat.sum()

C = cmat.sum(axis=1)

return ((C/N)*(100*np.diag(cmat)/C)).sum()

*N* 是测试的样本总数，它只是混淆矩阵中条目的总和，因为测试集中的每个样本都落在矩阵的某个位置，*C* 是每个类别的样本数量向量。这只是混淆矩阵各行的总和。每个类别的准确率（以百分比表示）是通过将混淆矩阵的对角元素（np.diag(cmat)）除以该类别在测试集中出现的次数 *C* 计算得出的。乘以 100 后，即可得到这些百分比准确率。

如果我们将这些每个类别的值相加并除以类别数量，就会得到（可能具有误导性的）无权重平均准确率。相反，我们首先乘以 *C*/*N*，即每个类别在所有测试样本中的占比（回想一下，*C* 是一个向量），然后求和得到加权准确率。此代码适用于任何大小的混淆矩阵。

对于前一节中的 MNIST 模型，我们计算出的加权平均准确率如下表所示：[表 11-13](ch11.xhtml#ch11tab13)。

**表 11-13：** MNIST 模型的加权平均准确率

| **架构** | **加权平均准确率** |
| --- | --- |
| 2 | 40.08% |
| 100 | 88.71% |
| 100 × 50 | 88.94% |
| 500 × 250 | 89.63% |

[表 11-13](ch11.xhtml#ch11tab13) 显示了随着模型大小增加，我们之前看到的收益递减的趋势。100 个节点的单隐层几乎与具有 100 和 50 个节点的双隐层模型完全相同，且仅比具有 500 个节点和 250 个节点的更大模型差 1%。只有两个节点的隐藏层模型表现较差。由于有 10 个类别，随机猜测的准确率通常为 1/10 = 0.1 = 10%，所以即使这个非常奇怪的模型将 784 个输入值（28×28 像素）映射到只有两个节点，然后映射到十个输出节点，仍然比随机猜测准确四倍。然而，这种模型的混淆矩阵如我们在[表 11-10](ch11.xhtml#ch11tab10)中所看到的那样非常奇怪，我们当然不想使用这个模型。仔细分析混淆矩阵是至关重要的。

#### 多分类 Matthews 相关系数

2 × 2 混淆矩阵导致了许多可能的度量指标。虽然可以将其中一些度量扩展到多分类情况，但我们这里只考虑主要度量：Matthews 相关系数（MCC）。对于二分类情况，我们看到 MCC 为

![image](Images/281equ01.jpg)

这可以通过使用混淆矩阵中的项来扩展到多分类情况，如下所示

![image](Images/281equ02.jpg)

其中

![image](Images/281equ03.jpg)

这里，*K* 是类别数，*C* 是混淆矩阵。这个符号来源于 sklearn 网站上对 MCC 的描述，直接展示了它是如何实现的。我们不需要详细跟踪方程式，我们只需要知道 MCC 是根据多分类情况中的混淆矩阵构建的，就像在二分类情况下那样。直观上，这是有意义的。二分类 MCC 的值在[*–*1, +1] 范围内。多分类情况根据类别数改变下界，但上界保持为 1.0，因此 MCC 越接近 1.0，模型表现越好。

计算 MNIST 模型的 MCC，就像我们计算加权平均准确度一样，见[表 11-14](ch11.xhtml#ch11tab14)。

**表 11-14：** MNIST 模型的 MCC

| **架构** | **MCC** |
| --- | --- |
| 2 | 0.3440 |
| 100 | 0.8747 |
| 100 × 50 | 0.8773 |
| 500 × 250 | 0.8849 |

再次，这表明最小的模型性能较差，而其他三个模型的性能非常相似。然而，在10,000个测试样本上进行预测的时间因模型而异。具有100个节点的单隐层模型需要0.052秒，而最大模型则需要0.283秒，时间是前者的五倍多。如果速度至关重要，较小的模型可能更可取。选择使用哪个模型时有许多因素需要考虑。本章讨论的指标是指南，但不应盲目遵循。最终，只有你知道什么对你要解决的问题最合适。

### 总结

在本章中，我们了解了为什么准确度不能作为模型性能的充分衡量标准。我们学习了如何为二分类器生成2 × 2混淆矩阵，以及这个矩阵能告诉我们关于模型在保留测试集上的表现。我们从2 × 2混淆矩阵中推导出基本指标，并使用这些基本指标推导出更高级的指标。我们讨论了各种指标的实用性，帮助我们建立直觉，了解如何以及何时使用它们。接着，我们了解了接收器操作特征（ROC）曲线，包括它展示了模型的哪些信息，以及如何解释它来对比不同模型。最后，我们介绍了多分类混淆矩阵，并举例说明如何解读它以及如何将一些二分类器的指标扩展到多分类情况。

在下一章，我们将达到机器学习模型的巅峰：卷积神经网络（CNN）。下一章将介绍CNN背后的基本思想；后续章节将通过实验使用这一深度学习架构。
