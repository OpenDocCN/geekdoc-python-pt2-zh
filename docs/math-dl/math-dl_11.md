## **11

梯度下降**

![图片](img/common.jpg)

在这一章的最后，我们将稍微放慢速度，重新审视梯度下降。我们将从通过插图回顾梯度下降的概念开始，讨论它是什么以及如何工作。接下来，我们将探讨*随机*在*随机梯度下降*中的含义。梯度下降是一个简单的算法，允许我们进行调整，因此在探讨随机梯度下降后，我们将考虑一个有用且常用的调整：动量。最后，我们将通过讨论更先进的自适应梯度下降算法来结束本章，具体包括 RMSprop、Adagrad 和 Adam。

这是一本数学书，但梯度下降非常应用数学，因此我们将通过实验来学习。方程式很简单，之前章节看到的数学内容作为背景是相关的。因此，可以将本章视为一个将我们迄今为止所学应用的机会。

### 基本概念

我们已经遇到过几次梯度下降。我们知道基本的梯度下降更新方程的形式，来自方程式 10.14：

![图片](img/11equ01.jpg)

在这里，**Δ*W*** 和 **Δ*b*** 是基于权重和偏差的偏导数的误差；η（希腊字母 eta）是步长或学习率，一个用来调整我们如何移动的值。

方程式 11.1 并不限于机器学习。我们可以使用相同的形式来对任意函数实现梯度下降。让我们通过一维和二维的例子来讨论梯度下降，奠定它如何运作的基础。我们将使用一种未经修改的梯度下降形式，称为*原始梯度下降*。

#### 一维梯度下降

让我们从一个标量函数*x*开始：

![图片](img/11equ02.jpg)

方程式 11.2 是一个向上的抛物线。因此，它有一个最小值。让我们通过将导数设为零并解出*x*来解析地找到最小值：

![图片](img/272equ01.jpg)

抛物线的最小值在*x* = 1。现在，让我们改用梯度下降来找到方程式 11.2 的最小值。我们应该如何开始？

首先，我们需要写出适当的更新方程式，适用于此情况的方程式 11.1 的形式。我们需要梯度，对于一维函数，梯度就是导数，*f*′(*x*) = 12*x* − 12。通过导数，梯度下降变为

![图片](img/11equ03.jpg)

注意，我们减去了η (12*x* − 12)。这就是为什么这个算法被称为梯度*下降*。回想一下，梯度指向函数值变化最大的方向。我们关心的是最小化函数，而不是最大化它，因此我们朝着与梯度相反的方向移动，趋向较小的函数值；因此，我们要减去。

方程 11.3 是一个梯度下降步骤。它根据当前位置的斜率值，将位置从初始位置 *x* 移动到新位置。同样，η（学习率）决定了我们移动的距离。

现在我们有了方程，让我们实现梯度下降。我们将绘制 方程 11.2，选择一个起始位置，比如 *x* = −0.9，并迭代 方程 11.3，在每个新位置 *x* 处绘制函数值。如果我们这样做，我们应该会看到一系列的点，这些点在函数上逐步靠近 *x* = 1 的最小值位置。让我们写些代码吧。

首先，我们实现 方程 11.2 及其导数：

def f(x):

return 6*x**2 - 12*x + 3

def d(x):

return 12*x - 12

接下来，我们绘制函数图像，然后迭代 方程 11.3，每次绘制新的点对（*x*，*f*（*x*））：

import numpy as np

import matplotlib.pylab as plt

❶ x = np.linspace(-1,3,1000)

plt.plot(x,f(x))

❷ x = -0.9

eta = 0.03

❸ for i in range(15):

plt.plot(x, f(x), marker='o', color='r')

❹ x = x - eta * d(x)

让我们逐步走过代码。在导入 NumPy 和 Matplotlib 后，我们绘制 方程 11.2 ❶。接下来，我们设置初始 *x* 位置 ❷ 并进行 15 步梯度下降 ❸。我们在步进之前绘制，所以我们可以看到初始的 *x*，但不绘制最后一步，这在这种情况下是可以的。

最后一行 ❹ 很关键。它实现了 方程 11.3。我们通过将当前位置 *x* 处的导数值乘以 η = 0.03 作为步长来更新当前的 *x* 位置。上面的代码位于文件 *gd_1d.py* 中。如果我们运行它，将得到 图 11-1。

![图片](img/11fig01.jpg)

*图 11-1：带有小步长的单维梯度下降（η = 0.03）*

我们的初始位置可以看作是对最小值位置的初步猜测，*x* = −0.9。显然，这不是最小值。当我们进行梯度下降步伐时，我们会逐步接近最小值，正如一系列向最小值移动的圆圈所示。

在这里注意两点。首先，我们确实越来越接近最小值。经过 14 步后，我们基本上已经到达最小值：*x* = 0.997648。其次，每一步梯度下降都会导致 *x* 的变化越来越小。学习率保持为 *η* = 0.03，因此 *x* 的更小更新源于每个 *x* 位置处的导数值逐渐减小。如果我们仔细想想，这是有道理的。当我们接近最小位置时，导数会变得越来越小，直到在最小值处为零，因此使用导数的更新也会逐步变小。

我们为图 11-1 选择的步长使得梯度下降平滑地朝着抛物线的最小值移动。如果我们改变步长会怎样呢？在*gd_1d.py*中，代码重复了上述步骤，从*x* = 0.75 开始，并设置*η* = 0.15，步长是图 11-1 中绘制步长的五倍。结果是图 11-2。

![图片](img/11fig02.jpg)

*图 11-2：带有大步长（η = 0.15）的单维梯度下降*

在这种情况下，步长超过了最小值。新的*x*位置振荡，在最小值位置前后来回跳动。虚线连接了连续的*x*位置。总体搜索依然接近最小值，但因为步长较大，每次更新*x*时都倾向于越过最小值，因此需要更多的时间才能达到最小值。

小的梯度下降步长沿着函数移动的距离较短，而大的步长则移动较大的距离。如果学习率太小，需要很多梯度下降步骤。如果学习率过大，搜索就会超越最小值并在最小位置附近振荡。合适的学习率并不立即显现，因此在选择时需要凭直觉和经验。此外，这些示例中固定了*η*。*η*不必是一个常数，许多深度学习应用中，学习率并不是常数，而是随着训练的进行而逐步变化，实际上使*η*成为梯度下降步骤数的函数。

#### 二维梯度下降

一维梯度下降足够简单。现在让我们进入二维梯度下降，以便更好地理解这个算法。下面引用的代码位于文件*gd_2d.py*中。我们将首先考虑函数有一个最小值的情况，然后再看有多个最小值的情况。

##### 带有单一最小值的梯度下降法

要在二维中工作，我们需要一个向量的标量函数，*f*(***x***) = *f*(*x*, *y*)，为了便于理解，我们将向量分解为其分量，***x*** = (*x*, *y*)。

我们将要处理的第一个函数是

*f*(*x*, *y*) = 6*x*² + 9*y*² − 12*x* − 14*y* + 3

要实现梯度下降，我们还需要偏导数：

![图片](img/276equ01.jpg)

我们的更新方程变为

![图片](img/276equ02.jpg)

在代码中，我们定义了函数和偏导数：

def f(x,y):

return 6*x**2 + 9*y**2 - 12*x - 14*y + 3

def dx(x):

return 12*x - 12

def dy(y):

return 18*y - 14

由于偏导数与另一个变量无关，我们可以只传递*x*或*y*。稍后在本节中，我们会看到一个不同的例子，其中并非如此。

梯度下降遵循与之前相同的模式：选择一个初始位置，这次是一个向量，进行若干步的迭代并绘制路径。由于函数是二维的，我们首先使用等高线图来绘制它，如下所示。

N = 100

x,y = np.meshgrid(np.linspace(-1,3,N), np.linspace(-1,3,N))

z = f(x,y)

plt.contourf(x,y,z,10, cmap="Greys")

plt.contour(x,y,z,10, colors='k', linewidths=1)

plt.plot([0,0],[-1,3],color='k',linewidth=1)

plt.plot([-1,3],[0,0],color='k',linewidth=1)

plt.plot(1,0.7777778,color='k',marker='+')

这段代码需要一些解释。为了绘制等高线，我们需要在 (*x*, *y*) 点对的网格上表示该函数。为了生成这个网格，我们使用 NumPy，特别是 np.meshgrid。np.meshgrid 的参数是 *x* 和 *y* 点，这里由 np.linspace 提供，后者本身生成从 −1 到 3 的 *N* = 100 个均匀间隔的数值。np.meshgrid 函数返回两个 100 × 100 的矩阵。第一个矩阵包含给定范围内的 *x* 值，第二个矩阵包含 *y* 值。所有可能的 (*x*, *y*) 点对都在返回值中表示，从而形成一个覆盖 −1 到 3 范围的网格。将这些点传递给函数后，会返回 z，这是一个 100 × 100 的矩阵，包含每个 (*x*, *y*) 点对的函数值。

我们可以在 3D 中绘制该函数，但这样不容易观察，且在这种情况下并不必要。相反，我们将使用 *x*、*y* 和 *z* 的函数值来生成等高线图。等高线图通过一系列具有相同 *z* 值的线条展示 3D 信息。可以将其想象为地形图上环绕山丘的等高线，其中每条线代表相同的海拔高度。随着山丘的升高，线条会围绕越来越小的区域。

等高线图有两种形式，一种是相等函数值的线条，另一种是函数值范围的阴影区域。我们将使用灰度图来绘制这两种形式。这是调用 Matplotlib 的 plt.contourf 和 plt.contour 函数的最终结果。其余的 plt.plot 调用展示了坐标轴，并用加号标记了函数的最小值。等高线图的阴影较浅意味着函数值较低。

现在我们准备绘制梯度下降步骤的序列。我们将绘制序列中的每个位置，并用虚线将它们连接起来，以便清晰地显示路径（见 Listing 11-1）。代码如下：

x = xold = -0.5

y = yold = 2.9

for i in range(12):

plt.plot([xold,x],[yold,y], marker='o', linestyle='dotted', color='k')

xold = x

yold = y

x = x - 0.02 * dx(x)

y = y - 0.02 * dy(y)

*Listing 11-1: 二维梯度下降*

我们从 (*x*, *y*) = (−0.5, 2.9) 开始，并进行 12 步梯度下降。为了用虚线连接最后的位置和新位置，我们追踪当前的 *x* 和 *y* 位置以及之前的位置 (*x*[old], *y*[old])。梯度下降步骤使用 *η* = 0.02 更新 *x* 和 *y*，并调用相应的偏导数函数 dx 和 dy。

Figure 11-3 展示了梯度下降路径，该路径沿 Listing 11-1（圆圈）与从 (1.5, −0.8)（方块）和 (2.7, 2.3)（三角形）出发的另外两条路径。

![image](img/11fig03.jpg)

*图 11-3：对于小步长的二维梯度下降法*

所有三个梯度下降路径都收敛到该函数的最小值。 这并不令人惊讶，因为该函数只有一个最小值。如果函数只有一个最小值，那么梯度下降最终会找到它。如果步长太小，可能需要很多步，但它们最终会收敛到最小值。如果步长太大，梯度下降可能会围绕最小值振荡，但会不断越过最小值。

让我们稍微改变一下函数，将其在 *x* 方向上相对于 *y* 方向拉伸：

*f*(*x*, *y*) = 6*x*² + 40*y*² − 12*x* − 30*y* + 3

这个函数的偏导数为 ∂*f*/∂*x* = 12*x* − 12 和 ∂*f*/∂*y* = 80*y* − 30。

此外，让我们选择两个起始位置（−0.5，2.3）和（2.3，2.3），并分别生成步长为 *η* = 0.02 和 *η* = 0.01 的梯度下降步骤序列。图 11-4 显示了结果路径。

![image](img/11fig04.jpg)

*图 11-4：具有较大步长和略有不同的函数的二维梯度下降法*

首先考虑 *η* = 0.02（圆形）路径。新的函数像一个峡谷，*y* 方向较窄，而 *x* 方向较长。较大的步长在 *y* 方向上上下振荡，同时向 *x* 方向的最小值移动。跳跃过峡谷壁 aside，我们仍然能够找到最小值。

现在，看看 *η* = 0.01（方形）路径。它快速进入峡谷，然后沿着峡谷底部的平坦区域缓慢地向最小位置移动。沿 *x* 方向的梯度向量分量（*x* 和 *y* 的偏导数值）在峡谷中很小，因此在 *x* 方向的运动相对较慢。沿 *y* 方向没有运动——峡谷很陡峭，而且相对较小的学习率已经定位到峡谷底部，在那里梯度主要沿 *x* 方向。

这里的教训是什么？再次强调，步长很重要。然而，函数的形状更为重要。函数的最小值位于一个长而窄的峡谷底部。峡谷中的梯度很小；峡谷底部在 *x* 方向是平坦的，所以运动很慢，因为它依赖于梯度值。我们在深度学习中经常遇到这种现象：如果梯度很小，学习速度会很慢。这就是为什么修正线性单元（ReLU）在深度学习中占据主导地位的原因；对于正输入，梯度是常数。而对于 sigmoid 或双曲正切函数，当输入远离零时，梯度接近于零。

##### 具有多个最小值的梯度下降法

到目前为止，我们考察的函数都有一个最小值。如果情况不是这样呢？让我们看看当函数有多个最小值时梯度下降会发生什么。考虑这个函数：

![Image](img/11equ04.jpg)

公式 11.4 是两个倒转高斯函数的和，一个在（-1, 1）处有最小值-2，另一个在（1, -1）处有最小值-1。如果梯度下降要找到全局最小值，它应该在（-1, 1）找到它。此示例的代码位于 *gd_multiple.py* 中。

偏导数为

![图片](img/280equ01.jpg)

这转换成以下代码：

def f(x,y):

return -2*np.exp(-0.5*((x+1)**2+(y-1)**2)) + \

-np.exp(-0.5*((x-1)**2+(y+1)**2))

def dx(x,y):

return 2*(x+1)*np.exp(-0.5*((x+1)**2+(y-1)**2)) + \

(x-1)*np.exp(-0.5*((x-1)**2+(y+1)**2))

def dy(x,y):

return (y+1)*np.exp(-0.5*((x-1)**2+(y+1)**2)) + \

2*(y-1)*np.exp(-0.5*((x+1)**2+(y-1)**2))

注意，在这种情况下，偏导数确实依赖于 *x* 和 *y*。

梯度下降部分的代码与之前的 *gd_multiple.py* 相同。让我们运行表 11-1 中的案例。

**表 11-1：** 不同起始位置和梯度下降步骤数量

| **起始点** | **步骤数** | **符号** |
| --- | --- | --- |
| (-1.5,1.2) | 9 | 圆形 |
| (1.5,–1.8) | 9 | 方块 |
| (0,0) | 20 | 加号 |
| (0.7,–0.2) | 20 | 三角形 |
| (1.5,1.5) | 30 | 星号 |

符号列指的是图 11-5 中使用的绘图符号。对于所有情况，*η* = 0.4。

![图片](img/11fig05.jpg)

*图 11-5：具有两个最小值的函数的梯度下降*

在图 11-5 中指示的梯度下降路径是合理的。在五种情况中，有三种路径确实进入了由较深的最小值定义的深坑——这是一次成功的搜索。然而，对于三角形和方块，梯度下降进入了错误的最小值。显然，在这种情况下，梯度下降的成功与我们开始过程的位置有关。一旦路径向下滑动到一个更深的位置，梯度下降就无法向上逃逸去寻找一个潜在的更好的最小值。

当前的观点是，深度学习模型的损失景观包含许多最小值。现在也认为，在大多数情况下，这些最小值非常相似，这部分解释了深度学习模型的成功——训练它们时，你不需要找到那个唯一的魔法全局最小值，只需要找到（可能）许多最小值中的一个，它们（可能）和其他任何一个一样好。

我有意选择了本节示例中使用的初始位置，基于对函数形式的理解。对于深度学习模型，选择起始点意味着对权重和偏差进行随机初始化。通常，我们不知道损失函数的具体形式，所以初始化就是一次盲目的尝试。大多数情况下，或者至少大部分情况下，梯度下降会产生一个表现良好的模型。然而，有时它会失败，表现得非常糟糕。在这些情况下，可能是因为初始位置就像图 11-5 中的方形位置一样：它陷入了一个劣质的局部最小值，因为一开始就处于一个不好的位置。

现在我们已经掌握了梯度下降的基本概念、原理和工作方式，接下来让我们探讨如何在深度学习中应用它。

### 随机梯度下降

训练神经网络的主要任务是最小化损失函数，同时通过各种正则化形式保持泛化能力。在第十章中，我们将损失写为 *L*(**θ**; ***x***, *y*)，其中 **θ**（theta）是权重和偏差的向量，***x*** 和 *y* 是训练实例，***x*** 是输入向量，y 是已知标签。注意，这里 ***x*** 是代表*所有*训练数据的符号，而不仅仅是单个样本。

梯度下降需要 ∂*L*/∂**θ**，我们通过反向传播获得这个结果。表达式 ∂*L*/∂**θ** 是指代反向传播给我们提供的所有单独的权重和偏差误差项的简洁方式。我们通过对训练数据进行误差平均化来得到 ∂*L*/∂**θ**。这就引出了一个问题：我们是对所有训练数据进行平均，还是只对部分训练数据进行平均？

在每次梯度下降步骤之前将所有训练数据通过模型的过程称为批量训练。乍一看，批量训练似乎很合理。毕竟，如果我们的训练集是从生成模型数据的母体分布中抽取的一个良好样本，那么为什么不利用这个完整的样本来进行梯度下降呢？

当数据集较小的时候，批量训练是自然而然的选择。然而，随着模型和数据集的增大，逐渐地将*所有*训练数据通过模型进行每一步梯度下降的计算负担变得过重。本章的示例已经暗示了，要找到一个良好的最小值位置，可能需要多次梯度下降步骤，尤其是在学习率很小的情况下。

因此，实践者开始在每次梯度下降步骤中使用训练数据的子集——*小批量*。小批量训练最初可能被视为一种妥协，因为它计算的梯度是“错误的”，因为它不是基于完整训练集的表现。

当然，*批量*（batch）和*mini-batch* 之间的区别只是一个约定的虚构。实际上，它是从一个 mini-batch 到包含所有可用样本的 mini-batch 之间的一个连续体。考虑到这一点，在网络训练过程中计算的所有梯度都是“错误的”，或者至少是不完整的，因为它们是基于对数据生成器和它能够生成的完整数据集的部分了解而得出的。

因此，mini-batch 训练是合理的，而非一种让步。与较大 mini-batch 计算的梯度相比，小 mini-batch 上的梯度更加嘈杂，因为小 mini-batch 的梯度是“真实”梯度的粗略估计。当事物嘈杂或随机时，*随机*一词通常会出现，就像这里一样。使用 mini-batch 进行梯度下降就是*随机梯度下降（SGD）*。

在实践中，使用较小 mini-batch 的梯度下降通常会得到比使用较大 mini-batch 训练的模型更好的效果。通常给出的理由是，小 mini-batch 的嘈杂梯度帮助梯度下降避免陷入损失函数的较差局部最小值。我们在图 11-5 中看到了这个效果，那里三角形和正方形都陷入了错误的最小值。

再次，我们发现自己在某种程度上是幸运的。之前我们之所以幸运，是因为一阶梯度下降成功地训练了那些由于非线性损失函数的形状而无法训练的模型，而现在，通过故意使用少量数据来估计梯度，我们得到了一个额外的助力，从而避免了可能会使深度学习在许多情况下变得过于繁琐的计算负担。

我们的 mini-batch 应该有多大？mini-batch 大小是一个*超参数*，是我们在训练模型时需要选择的内容，但它不属于模型本身。合适的 mini-batch 大小取决于数据集。例如，在极端情况下，我们可以对每个样本进行一次梯度下降，这有时效果很好。这个情况通常被称为*在线学习*。然而，特别是当我们使用诸如批量归一化（batch normalization）这样的层时，我们需要一个足够大的 mini-batch 来使计算得到的均值和标准差成为合理的估计。再次强调，像当前深度学习中的大多数问题一样，这是经验性的，你需要既有直觉，也要尝试多种变化来优化模型的训练。这也是人们研究*AutoML*系统的原因，AutoML 系统旨在为你完成所有超参数调整的工作。

另一个值得思考的问题是：mini-batch 中应该包含什么？也就是说，我们应该使用数据集中的哪个小子集？通常，训练集中的样本顺序是随机的，mini-batches 是从训练集中依次抽取样本，直到所有样本都被使用。使用数据集中的所有样本定义一个训练周期（epoch），因此训练集中的样本数除以 mini-batch 大小决定了每个训练周期中的 mini-batch 数量。

另外，正如我们对*NN.py*所做的那样，一个小批次可能真的是从可用数据中随机采样的。可能某个特定的训练样本从未被使用，而另一个被多次使用，但总体来说，训练过程中大部分数据集都会被使用。

一些工具包会训练指定数量的小批次。*NN.py*和 Caffe 都以这种方式工作。其他工具包，如 Keras 和 sklearn，使用训练周期（epochs）。梯度下降步骤在每个小批次处理后进行。较大的小批次会导致每个训练周期中较少的梯度下降步骤。为了补偿，使用训练周期的工具包的从业者需要确保随着小批次大小的增加，梯度下降步骤的数量也增加——较大的小批次需要更多的训练周期才能很好地训练。

总结一下，深度学习不使用全批次训练，至少有以下几个原因：

1.  将整个训练集传递通过模型进行每一次梯度下降步骤的计算负担过于沉重。

1.  从一个小批次的平均损失计算出的梯度是一个嘈杂但合理的估计值，代表了真实的梯度，而真实的梯度是最终不可知的。

1.  嘈杂的梯度在损失地形中指向一个稍微错误的方向，从而可能避开不良的极小值。

1.  小批次训练在实际中对于许多数据集效果更好。

不应低估理由#4：深度学习中的许多实践最初被采用，是因为它们更有效。只有后来，理论才会对这些实践提供解释，甚至有时没有解释。

我们已经在第十章中实现了 SGD（参见*NN.py*），因此这里不再重新实现，但在下一节中，我们将加入动量，看看它如何影响神经网络的训练。

### 动量

普通梯度下降仅依赖于偏导数的值乘以学习率。如果损失地形有很多局部极小值，尤其是当它们很陡峭时，普通梯度下降可能会陷入某个极小值并无法恢复。为了弥补这一点，我们可以修改普通梯度下降，加入一个*动量*项，这个项使用前一步更新的部分值。将动量加入到梯度下降中，为算法在损失地形中的运动增加了惯性，从而可能使梯度下降越过不良的局部极小值。

让我们先定义动量，然后通过 1D 和 2D 的示例进行实验，正如我们之前所做的。之后，我们将更新我们的*NN.py*工具包，使用动量来观察它如何影响在更复杂数据集上训练的模型。

#### 什么是动量？

在物理学中，运动物体的动量定义为质量乘以速度，***p*** = *m**v***。然而，速度本身是位置的第一导数，***v*** = *d**x***/*dt*，因此动量就是质量与物体位置随时间变化速度的乘积。

对于梯度下降，*位置*是函数值，*时间*是函数的自变量。因此，*速度*是函数值如何随着自变量变化而变化，即∂*f*/∂***x***。因此，我们可以将*动量*视为一个缩放的速度项。在物理学中，缩放因子是质量。对于梯度下降，缩放因子是*μ*（mu），它是一个介于 0 和 1 之间的数字。

如果我们将包含动量项的梯度称为***v***，那么梯度下降的更新方程是

![Image](img/285equ01.jpg)

变为

![Image](img/11equ05.jpg)

对于某些初始速度，***v*** = 0，以及“质量”，*μ*。

让我们通过方程 11.5 来理解它的含义。这个两步更新，先更新***v***，再更新***x***，使得迭代变得简单，因为我们知道这正是梯度下降的要求。如果我们将***v***代入***x***的更新方程，我们得到

![Image](img/285equ02.jpg)

这清楚地表明，更新包含了我们之前的梯度步骤，同时还加入了前一步大小的一部分。它是一个分数，因为我们将*μ*限制在[0, 1]之间。如果*μ* = 0，我们就回到了普通的梯度下降。可以将*μ*视为一个缩放因子，它表示保留前一步速度与当前梯度值之间的比例。

动量项倾向于保持在损失函数空间中沿着先前的方向移动。*μ*的值决定了这一趋势的强度。深度学习实践者通常使用*μ* = 0.9，因此大部分先前更新的方向会在下一步中得以保留，而当前的梯度则提供了一个小的调整。同样，像许多深度学习中的参数一样，这个值是通过经验选择的。

牛顿的第一运动定律指出，物体一旦运动，除非受到外力作用，否则将保持运动状态。对外力的抵抗与物体的质量有关，称为*惯性*。因此，我们也可以将*μ**v***项视为惯性，它本可以是一个更合适的名称。

无论名称如何，既然我们有了它，接下来让我们看看它对之前使用普通梯度下降处理过的一维和二维示例的影响。

#### 一维动量

让我们修改上面的 1D 和 2D 示例，加入动量项。我们将从一维情况开始。更新后的代码在文件*gd_1d_momentum.py*中，下面是清单 11-2。

import matplotlib.pylab as plt

def f(x):

return 6*x**2 - 12*x + 3

def d(x):

return 12*x - 12

❶ m = ['o','s','>','<','*','+','p','h','P','D']

x = np.linspace(0.75,1.25,1000)

plt.plot(x,f(x))

❷ x = xold = 0.75

eta = 0.09

mu = 0.8

v = 0.0

for i in range(10):

❸ plt.plot([xold,x], [f(xold),f(x)], marker=m[i], linestyle='dotted',

color='r')

xold = x

v = mu*v - eta * d(x)

x = x + v

for i in range(40):

v = mu*v - eta * d(x)

x = x + v

❹ plt.plot(x,f(x),marker='X', color='k')

*清单 11-2：带动量的一维梯度下降*

清单 11-2 有些复杂，所以我们来逐步解析。首先，我们要绘图，因此需要导入 Matplotlib。接下来，我们定义函数 f(x) 和它的导数 d(x)，就像之前那样。为了配置绘图，我们定义了一组标记 ❶，然后绘制函数本身。像之前一样，我们从 *x* = 0.75 ❷ 开始，设置步长 (eta)，动量 (mu)，以及初始速度 (v)。

我们现在准备好进行迭代了。我们将使用两个梯度下降循环。第一个循环绘制每一步 ❸，第二个循环继续梯度下降，演示我们最终确实能够找到最小值，并用“X” ❹ 标记它。对于每一步，我们通过模仿方程 11.5 计算新的速度，然后将速度加到当前的位置，得到下一个位置。

图 11-6 展示了 *gd_1d_momentum.py* 的输出。

![image](img/11fig06.jpg)

*图 11-6：带动量的一维梯度下降*

请注意，我们故意使用了较大的步长 (*η*)，因此会超过最小值。动量项也会倾向于超越最小值。如果你沿着虚线和绘图标记的顺序走，你可以看到前 10 步梯度下降的过程。虽然有振荡，但振荡逐渐减弱，最终会稳定在最小值处，正如标记所示。由于较大的步长，动量增强了这种超调现象。然而，即使有动量项，在这里并没有特别的优势，因为这里只有一个最小值，但经过足够的梯度下降步骤后，我们最终还是找到了最小值。

#### 2D 动量

现在，让我们更新我们的二维示例。我们正在使用 *gd_momentum.py* 中的代码。回想一下，对于二维示例，函数是两个反向高斯函数的和。包括动量使得代码稍有更新，如清单 11-3 所示：

def gd(x,y, eta,mu, steps, marker):

xold = x

yold = y

❶ vx = vy = 0.0

for i in range(steps):

plt.plot([xold,x],[yold,y], marker=marker,

linestyle='dotted', color='k')

xold = x

yold = y

❷ vx = mu*vx - eta * dx(x,y)

vy = mu*vy - eta * dy(x,y)

❸ x = x + vx

y = y + vy

❹ gd( 0.7,-0.2, 0.1, 0.9, 25, '>')

gd( 1.5, 1.5, 0.02, 0.9, 90, '*')

*清单 11-3：带动量的二维梯度下降*

这里是新的函数 gd，它执行从 (x,y) 开始的带动量的梯度下降，使用给定的 *μ* 和 *η*，并运行指定的步骤次数。

初始速度设为❶，然后开始循环。方程 11.5 的速度更新为 vx = mu*vx - eta * dx(x,y) ❷，位置更新为 x = x + vx ❸。像之前一样，绘制一条线连接上一个位置和当前的位置，以跟踪函数景观中的运动。

*gd_momentum.py* 中的代码追踪了从我们之前使用的两个点（0.7, −0.2）和（1.5, 1.5）❹ 开始的运动。请注意，每个点的步骤数和学习率不同，以避免图表过于拥挤。*gd_momentum.py* 的输出是 图 11-7。

![image](img/11fig07.jpg)

*图 11-7：带动量的二维梯度下降*

将 图 11-7 中的路径与 图 11-5 中的路径进行对比。添加动量后，路径发生了偏移，因此它们倾向于保持在同一方向上移动。注意从（1.5, 1.5）开始的路径如何螺旋向最小值，而另一条路径则弯向更浅的最小值，超过它后又返回。

动量项改变了通过函数空间的运动动态。然而，动量是否有助于并不马上显现出来。毕竟，使用普通梯度下降法，从（1.5, 1.5）起始位置直接移动到最小值位置，而没有螺旋式下降。

让我们将动量加入到 *NN.py* 工具包中，看看在训练实际神经网络时它是否有任何效果。

#### 使用动量训练模型

为了在 *NN.py* 中支持动量，我们需要在两个地方调整 FullyConnectedLayer 方法。首先，如 清单 11-4 所示，我们修改构造函数，允许使用动量关键词：

def __init__(self, input_size, output_size, momentum=0.0):

self.delta_w = np.zeros((input_size, output_size))

self.delta_b = np.zeros((1, output_size))

self.passes = 0

self.weights = np.random.rand(input_size, output_size) - 0.5

self.bias = np.random.rand(1, output_size) - 0.5

❶ self.vw = np.zeros((input_size, output_size))

self.vb = np.zeros((1, output_size))

self.momentum = momentum

*清单 11-4：添加动量关键词*

在这里，我们将动量关键词添加到参数列表中，默认值为零。然后，我们为权重（vw）和偏置（vb）定义初始速度 ❶。这些是初始化为零的适当形状的矩阵。我们还保留动量参数，以供后续使用。

第二个修改是针对 step 方法的，如 清单 11-5 所示：

def step(self, eta):

❶ self.vw = self.momentum * self.vw - eta * self.delta_w / self.passes

self.vb = self.momentum * self.vb - eta * self.delta_b / self.passes

❷ self.weights = self.weights + self.vw

self.bias = self.bias + self.vb

self.delta_w = np.zeros(self.weights.shape)

self.delta_b = np.zeros(self.bias.shape)

self.passes = 0

*清单 11-5：更新 step 方法以包含动量*

我们实现了公式 11.5，首先处理权重 ❶，然后是下一行的偏置。我们将动量（*μ*）与之前的速度相乘，然后减去平均误差乘以学习率。接着，我们通过加上速度 ❷ 来更新权重和偏置。这就是我们融入动量所需的全部步骤。然后，要使用它，我们在构建网络时，在每个全连接层中添加动量关键字，如示例 11-6 所示：

net = Network()

net.add(FullyConnectedLayer(14*14, 100, momentum=0.9))

net.add(ActivationLayer())

net.add(FullyConnectedLayer(100, 50, momentum=0.9))

net.add(ActivationLayer())

net.add(FullyConnectedLayer(50, 10, momentum=0.9))

net.add(ActivationLayer())

*示例 11-6：构建网络时指定动量*

每层添加动量使得可以为每一层设置特定的动量值。虽然我不知道有任何研究这么做，但这似乎是一个很明显的尝试方向，因此现在可能已经有人对此进行了实验。对于我们的目的，我们将所有层的动量设为 0.9，继续进行下去。

我们如何测试新的动量呢？我们可以使用上面提到的 MNIST 数据集，但它并不是一个好的选择，因为它太简单了。即使是一个简单的全连接网络，也能达到超过 97% 的准确率。因此，我们将用另一个已知更具挑战性的类似数据集来替代 MNIST：Fashion-MNIST 数据集。（参见 Han Xiao 等人的《Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms》，arXiv:1708.07747 [2017]。）

*Fashion-MNIST 数据集（FMNIST）* 是现有 MNIST 数据集的替代品。它包含来自 10 类服装的图像，所有图像为 28×28 像素的灰度图。为了我们的目的，我们将像 MNIST 一样，将 28×28 像素的图像缩小到 14×14 像素。图像存储在数据集目录中，格式为 NumPy 数组。让我们使用它们训练一个模型。模型的代码与示例 10-7 相似，不同之处在于，在示例 11-7 中，我们将 MNIST 数据集替换为 FMNIST：

x_train = np.load("fmnist_train_images_small.npy")/255

x_test = np.load("fmnist_test_images_small.npy")/255

y_train = np.load("fmnist_train_labels_vector.npy")

y_test = np.load("fmnist_test_labels.npy")

*示例 11-7：加载 Fashion-MNIST 数据集*

我们还包括了计算测试数据集上的 Matthews 相关系数（MCC）的代码。在第四章中，我们第一次遇到了 MCC，我们了解到它比准确率更能衡量模型的表现。运行代码位于 *fmnist.py* 中。在一台较旧的 Intel i5 计算机上运行，约 18 分钟后，得到的结果是：

[[866   1  14  28   8   1  68   0  14   0]

[ 5 958   2  25   5   0   3   0   2   0]

[ 20  1 790  14 126   0  44   1   3   1]

[ 29  21  15 863  46   1  20   0   5   0]

[ 0   0  91  22 849   1  32   0   5   0]

[  0   0   0   1   0 960   0  22   2  15]

[161   2 111  38 115   0 556   0  17   0]

[  0   0   0   0   0  29   0 942   0  29]

[  1   0   7   5   6   2   2   4 973   0]

[  0   0   0   0   0   6   0  29   1 964]]

准确率 = 0.8721000

MCC = 0.8584048

混淆矩阵，仍然是 10 × 10，因为 FMNIST 有 10 个类别，相较于我们在 MNIST 数据集中看到的非常干净的混淆矩阵，它相当嘈杂。这是一个对于全连接模型来说具有挑战性的数据集。回想一下，MCC 是一个衡量标准，值越接近 1 表示模型越好。

上面的混淆矩阵是为一个没有动量训练的模型生成的。学习率是 1.0，并且训练了 40,000 个包含 64 个样本的小批量。如果我们为每个全连接层添加 0.9 的动量并将学习率降低到 0.2，会发生什么呢？当我们添加动量时，降低学习率是有道理的，这样我们就不会因为动量已经在某个方向上移动而采取过大的步伐。请尝试运行 *fmnist.py*，学习率为 0.2 且没有动量，看看会发生什么。

带动量的代码版本在 *fmnist_momentum.py* 中。大约 20 分钟后，这段代码运行一次产生了

[[766   5  14  61   2   1 143   0   8   0]

[  1 958   2  30   3   0   6   0   0   0]

[ 12   0 794  16  98   0  80   0   0   0]

[  8  11  13 917  21   0  27   0   3   0]

[  0   0  84  44 798   0  71   0   3   0]

[  0   0   0   1   0 938   0  31   1  29]

[  76   2  87  56  60   0 714   0   5   0]

[  0   0   0   0   0  11   0 963   0  26]

[  1   1   6   8   5   1  10   4 964   0]

[  0   0   0   0   0   6   0  33   0 961]]

准确率 = 0.8773000

MCC = 0.8638721

给我们带来稍微更高的 MCC。这意味着动量有帮助吗？也许有。正如我们现在所理解的，训练神经网络是一个随机过程。因此，我们不能依赖于单次训练结果。我们需要多次训练模型并对结果进行统计检验。太好了！这给了我们一个机会，充分利用我们在第四章中学到的假设检验知识。

不要一次性运行 *fmnist.py* 和 *fmnist_momentum.py*，而是分别运行它们 22 次。这在我旧的英特尔 i5 系统上花费了一整天的时间，但耐心是美德。最终结果是有 22 个带动量的模型的 MCC 值和 22 个不带动量的模型的 MCC 值。22 个样本并没有什么神奇之处，但我们打算使用曼-惠特尼 U 检验，而该检验的经验法则是每个数据集至少要有 20 个样本。

图 11-8 显示了结果的直方图。

![图片](img/11fig08.jpg)

*图 11-8：显示带动量（浅灰色）和不带动量（深灰色）训练的模型的 MCC 分布的直方图*

深灰色的条形表示没有动量的 MCC 值，浅灰色的条形表示有动量的 MCC 值。从视觉上看，两者有很大的不同。生成图 11-8 的代码位于*fmnist_analyze.py*文件中。请务必查看该代码。它使用了 SciPy 的 ttest_ind 和 mannwhitneyu，以及我们在 Cohen’s *d*中给出的实现来计算效应大小。MCC 值本身位于代码中列出的 NumPy 文件中。

除了图表，*fmnist_analyze.py*还生成了以下输出：

无动量：0.85778 +/- 0.00056

动量         : 0.86413 +/- 0.00075

t 检验动量与无动量 (t,p)：(6.77398299, 0.00000003)

Mann-Whitney U         : (41.00000000, 0.00000126)

Cohen’s d          : 2.04243

其中前两行是均值和均值标准误差。t 检验结果为(*t*, *p*)，即*t*统计量和相关的*p*值。同样，Mann-Whitney U 检验的结果为(*U*, *p*)，即*U*统计量及其*p*值。回顾一下，Mann-Whitney U 检验是一种非参数检验，它并不假设 MCC 值的分布形态，而 t 检验则假设数据服从正态分布。由于我们每组只有 22 个样本，因此我们无法就结果是否符合正态分布做出明确的结论；直方图看起来也不像高斯曲线。这就是为什么我们还包括了 Mann-Whitney U 检验结果。

看一下各自的*p*值，我们可以发现，带动量和不带动量的 MCC 值均值差异在统计学上非常显著，支持带动量的结果。*t*值为正，且带动量的结果为第一个参数。那么 Cohen 的*d*值呢？它略高于 2.0，表示（非常）大的效应大小。

我们*现在*可以说动量在这种情况下有效吗？可能可以。它在我们使用的超参数下产生了更好的模型性能。训练神经网络的随机性使得我们可能通过调整两种模型的超参数来消除我们在现有数据中看到的差异。两者的架构是固定的，但没有什么表明学习率和小批量大小是为任何一个模型优化的。

一位细致的研究者可能会觉得有必要对超参数进行优化过程，并且在确认找到最佳模型后，通过重复实验来做出更明确的结论。幸运的是，我们不是那种细致的研究者。相反，我们将利用现有的证据，结合世界各地机器学习研究者对梯度下降中动量的有效性所积累的数十年智慧，来说明：是的，动量有助于模型学习，在大多数情况下你应该使用它。

然而，正态性问题需要进一步研究。毕竟，我们是在寻求提高我们关于深度学习的数学*和*实践直觉。因此，我们将使用动量模型对 FMNIST 进行训练，不是训练 22 次，而是训练 100 次。作为让步，我们将小批量的数量从 40,000 减少到 10,000。尽管如此，仍然预计你将花费大部分时间等待程序完成。代码（我们在这里不逐步讲解）位于*fmnist_repeat.py*中。

图 11-9 展示了结果的直方图。

很明显，这个分布看起来一点也不像正态曲线。*fmnist_repeat.py*的输出包含了 SciPy 的 normaltest 函数的结果。这个函数对一组数据进行统计检验，假设数据是正态分布的。因此，如果*p*值低于比如说 0.05 或 0.01，就表示数据不是正态分布的。我们的*p*值几乎为零。

如何解读图 11-9？首先，由于结果显然不是正态分布的，因此我们没有理由使用 t 检验。然而，我们也使用了非参数的 Mann-Whitney U 检验，并且得到了高度统计显著的结果，所以我们上述的结论依然有效。其次，图 11-9 中分布的长尾在左边。我们甚至可以认为结果可能是双峰的：一个峰值接近 0.83，另一个较小的峰值接近 MCC 为 0.75。

![image](img/11fig09.jpg)

*图 11-9：FMNIST 模型 100 次训练的 MCC 值分布*

大多数模型的训练性能相对一致，MCC 接近 0.83。然而，长尾表明，当模型表现不佳时，它通常是非常糟糕的。

从直觉上看，图 11-9 对我来说是合理的。我们知道随机梯度下降容易受到不当初始化的影响，而我们的小工具包使用的是老式的小随机值初始化。似乎更可能的情况是，我们有较大的机会从一个不好的位置开始，并且之后的表现注定会很差。

如果尾巴在右边呢？那可能意味着什么？右边的长尾表示大多数模型的表现平庸或差劲，但偶尔会有一个特别“亮眼”的模型出现。这样的情况意味着更好的模型是存在的，但我们的训练和/或初始化策略并不擅长找到它们。我认为左边的长尾更可取——大多数模型能找到合理的局部最小值，所以大多数训练，除非非常糟糕，否则最终会在性能上差不多。

现在，让我们来看看动量的一个常见变体，这个变体你在深度学习的旅程中肯定会遇到。

#### Nesterov 动量

许多深度学习工具包包括在梯度下降过程中使用 *Nesterov 动量* 的选项。Nesterov 动量是梯度下降的一个修改版本，在优化社区中广泛使用。深度学习中通常实现的版本将标准动量更新为

![图片](img/295equ01.jpg)

变为

![图片](img/11equ06.jpg)

在这里，我们使用梯度符号而不是损失函数的偏导数，表示该技术是通用的，适用于任何函数，*f*(***x***).

标准动量与深度学习中的 Nesterov 动量之间的区别是微妙的，只是添加了一个项到梯度的参数中。其思想是使用现有的动量来计算梯度，而不是在当前位置 ***x*** 计算，而是在使用当前动量继续前进时，梯度下降将会到达的位置，即 ***x*** + *μ **v***。然后我们使用该位置的梯度值来更新当前的位置，如同之前一样。

这个声明在优化中得到了很好的验证，即这一调整有助于更快的收敛，意味着梯度下降会在更少的步骤内找到最小值。然而，尽管工具包已经实现了它，但有理由相信，随机梯度下降与小批量引入的噪声会抵消这种调整，使得 Nesterov 动量在训练深度学习模型时，不太可能比常规动量更有用。（有关更多内容，请参阅 *深度学习*（Ian Goodfellow 等人）第 292 页的评论。）

然而，本章的二维示例使用实际函数来计算梯度，因此我们可以期待在这种情况下 Nesterov 动量是有效的。让我们更新二维示例，最小化两个反转高斯的和，看看 Nesterov 动量是否能如声明的那样提高收敛性。我们将运行的代码在 *gd_nesterov.py* 中，与 *gd_momentum.py* 中的代码几乎相同。此外，我稍微修改了这两个文件，以便在梯度下降完成后返回最终位置。这样，我们就能看到我们与已知最小值的接近程度。

实现 方程 11.6 是直接的，仅影响速度更新，导致

vx = mu*vx - eta * dx(x,y)

vy = mu*vy - eta * dy(x,y)

变为

vx = mu * vx - eta * dx(x + mu * vx,y)

vy = mu * vy - eta * dy(x,y + mu * vy)

为每个分量 *x* 和 *y* 添加动量。其他部分保持不变。

图 11-10 比较了标准动量（顶部，来自 图 11-7）和 Nesterov 动量（底部）。

![图片](img/11fig10.jpg)

*图 11-10：标准动量（顶部）和 Nesterov 动量（底部）*

从视觉上看，Nesterov 动量显示出较少的过冲，特别是对于从 (1.5, 1.5) 开始的螺旋路径。那么，每种方法返回的最终位置如何呢？我们可以查看 表 11-2。

**表 11-2**：使用和不使用 Nesterov 动量的梯度下降最终位置

| **初始点** | **标准** | **Nesterov** | **最小值** |
| --- | --- | --- | --- |
| (1.5,1.5) | (–0.9496, 0.9809) | (–0.9718, 0.9813) | (–1,1) |
| (0.7,–0.2) | (0.8807, –0.9063) | (0.9128, –0.9181) | (1,–1) |

Nesterov 动量的结果比标准动量在相同数量的梯度下降步骤后更接近已知的最小值。

### 自适应梯度下降

梯度下降算法几乎是简单的，这使得它适合进行适应性调整。在本节中，我们将介绍三种在深度学习社区中非常流行的梯度下降变体：RMSprop、Adagrad 和 Adam。在这三者中，Adam 无疑是最受欢迎的，但其他两个也非常值得理解，因为它们是逐步构建，最终到达 Adam。这三种算法都以某种方式动态地调整学习率。

#### RMSprop

Geoffrey Hinton 在他 2012 年的 Coursera 讲座中介绍了 *RMSprop*，即 *均方根传播*。与动量类似（它们可以结合使用），RMSprop 是一种梯度下降方法，它跟踪梯度值的变化，并使用该值来修改步长。

RMSprop 使用一个*衰减项*，γ（gamma），来计算梯度的移动平均值，随着算法的进展而变化。在他的讲座中，Hinton 使用 γ = 0.9。

梯度下降更新变为

![Image](img/11equ07.jpg)

首先，我们更新 *m*，即梯度平方的移动平均值，并通过 γ 来加权，γ 是衰减项。接下来是速度项，几乎与普通的梯度下降相同，但我们将学习率除以移动平均值的平方根，这就是 RMSprop 中的 RMS 部分。然后，我们将缩放后的速度从当前位置中减去，以便迈出一步。我们将这一步写作加法，类似于上面的动量方程（方程 11.5 和 11.6）；注意速度更新前的负号。

RMSprop 也可以与动量一起使用。例如，扩展 RMSprop 与 Nesterov 动量结合是很直接的：

![Image](img/11equ08.jpg)

其中 *μ* 是动量因子，与之前一样。

据称，RMSprop 是一种稳健的分类器。我们将在下面看到它在一个测试中的表现。我们将其视为一种自适应技术，因为学习率（*η*）是通过梯度的移动均值的平方根进行缩放的；因此，实际的学习率是基于下降历史进行调整的——它不是一成不变的。

RMSprop 常用于强化学习，这是机器学习的一个分支，旨在学习如何行动。例如，玩 Atari 视频游戏就使用强化学习。RMSprop 被认为在优化过程中是*非平稳*的，即统计量随时间变化时具有鲁棒性。相反，*平稳*过程是指统计量在时间上不发生变化。使用监督学习训练分类器是平稳的，因为训练集通常是固定的，不会变化，尽管输入给分类器的数据可能会随时间改变，这一点更难强制执行。在强化学习中，时间是一个因素，数据集的统计量可能会随时间变化；因此，强化学习可能涉及非平稳优化。

#### Adagrad 和 Adadelta

*Adagrad* 出现于 2011 年（参见 John Duchi 等人所著的《在线学习与随机优化的自适应子梯度方法》，*机器学习研究杂志* 12[7]，[2011]）。乍一看，它与 RMSprop 很相似，但也有一些重要的区别。

我们可以将 Adagrad 的基本更新规则写为

![Image](img/11equ09.jpg)

这需要一些解释。

首先，注意速度更新中的 *i* 下标，无论是在速度 ***v*** 还是梯度 *▽f **(x*** )上。这里，*i* 指的是速度的一个分量，意味着更新必须针对每个分量进行。 方程 11.9 的顶部对系统的所有分量都重复一次。对于深度神经网络，这意味着所有的权重和偏置。

接下来，看看每个分量速度更新的分母中的和。这里，τ（tau）是一个计数器，记录在优化过程中所采取的*所有*梯度步骤，这意味着对于系统的每个分量，Adagrad 跟踪每一步计算的梯度平方和。如果我们正在使用方程 11.9 来进行第 11 步梯度下降，那么分母中的和将有 11 项，依此类推。如前所述，*η* 是学习率，这里是全局性的，适用于所有分量。

Adagrad 的一个变种也得到了广泛使用：*Adadelta*。（参见 Matthew Zeiler 的《Adadelta：一种自适应学习率方法》，[2012]。）Adadelta 用最近几步的滑动平均值替代了速度更新中对所有步骤求和后的平方根，类似于 RMSprop 的滑动平均值。Adadelta 还用前几次速度更新的滑动平均值替代了手动选择的全局学习率 *η*。这消除了选择合适的 *η* 的需要，但引入了一个新参数 γ，用来设置窗口的大小，正如在 RMSprop 中所做的那样。γ 可能对数据集的性质比 *η* 更不敏感。请注意，在原始的 Adadelta 论文中，γ 被写作 *ρ*（rho）。

#### Adam

Kingma 和 Ba 于 2015 年发布了 *Adam*，来自“自适应矩估计”一词，截至目前已被引用超过 66,000 次。Adam 和 RMSprop、Adagrad 一样使用梯度的平方，但也跟踪类似动量的项。我们将呈现更新方程并逐步讲解：

![Image](img/11equ10.jpg)

方程 11.10 的前两行将 ***m*** 和 ***v*** 定义为一阶和二阶矩的移动平均值。一阶矩是均值；二阶矩类似于方差，即数据点与均值之间差值的二阶矩。请注意，在定义 ***v*** 时对梯度值进行了平方操作。移动矩被两个标量参数 *β*[1] 和 *β*[2] 加权。

接下来的两行定义了 ![Image](img/300equ01.jpg) 和 ![Image](img/300equ02.jpg)。这些是偏差修正项，用来使 ***m*** 和 ***v*** 更好地估计一阶和二阶矩。这里，*t* 是从零开始的整数，表示时间步。

实际的步更新通过从偏差修正后的第一矩 ![Image](img/300equ01.jpg) 中减去，乘以全局学习率 *η* 和偏差修正后第二矩 ![Image](img/300equ02.jpg) 的平方根的比率。∊ 项是一个常数，用于避免除零错误。

方程 11.10 有四个参数，看起来有些过多，但其中三个设置起来比较直接，并且很少更改。原文建议 *β*[1] = 0.9，*β*[2] = 0.999，和 ∊ = 10^(−8)。因此，像传统的梯度下降一样，用户仍需选择 *η*。例如，Keras 默认 *η* = 0.001，这在很多情况下效果不错。

Kingma 和 Ba 的论文通过实验表明，Adam 通常优于带有 Nesterov 动量的 SGD、RMSprop、Adagrad 和 Adadelta。这也可能是为什么 Adam 当前是许多深度学习任务首选优化器的原因。

#### 关于优化器的一些思考

使用哪个优化算法以及何时使用取决于数据集。如前所述，Adam 当前被许多任务青睐，尽管适当调整的 SGD 也能非常有效，有些人对此深信不疑。虽然无法对哪个算法是最好的作出普遍性陈述，因为没有绝对的最佳算法，但我们可以做一个小实验，并讨论结果。

这个实验，我只呈现结果，训练了一个小型卷积神经网络，使用 16,384 个随机样本作为训练集，批量大小为 128，训练 12 个周期。结果展示了每个优化器（SGD、RMSprop、Adagrad 和 Adam）五次运行的均值和标准误差。关注点是测试集的准确率和训练的时钟时间。我在同一台机器上训练了所有模型，所以我们应该关注相对时间。未使用 GPU。

图 11-11 展示了按优化器划分的总体测试集准确率（上）和训练时间（下）。

平均来说，SGD 和 RMSprop 的准确度比其他优化器低约 0.5%，其中 RMSprop 的表现差异较大，但始终没有达到 Adagrad 或 Adam 的效果。可以说，Adam 在准确度方面表现最好。在训练时间上，SGD 最快，而 Adam 最慢，正如我们所预期的那样，因为相较于 SGD 的简单性，Adam 需要进行多次每步计算。总体而言，结果支持了社区对 Adam 作为优化器的直觉判断。

![image](img/11fig11.jpg)

*图 11-11：MNIST 模型的准确度（顶部）和训练时间（底部）按优化器分类*

### 总结

本章介绍了梯度下降，讲解了其基本形式——原始梯度下降，并通过 1D 和 2D 示例加以说明。接着，我们介绍了随机梯度下降，并解释了其在深度学习中的应用理由。

接下来，我们讨论了动量，包括标准动量和 Nesterov 动量。通过标准动量，我们证明它在训练深度模型（好吧，相对“深”的模型）时确实有所帮助。我们使用 2D 示例直观地展示了 Nesterov 动量的效果，并讨论了为什么 Nesterov 动量和随机梯度下降可能相互抵消。

本章通过查看高级算法的梯度下降更新方程作总结，从而说明了原始梯度下降如何受到修改。一个简单的实验让我们对算法的表现有所了解，并似乎证实了深度学习社区对 Adam 优化器优于 SGD 的普遍认同。

随着本章的结束，我们对深度学习数学的探索也接近尾声。剩下的只是最后的附录，指引你去了解更多学习资源。

### 后记

正如伟大的计算机科学家 Edsger W. Dijkstra 所说：“不应存在无聊的数学。”我真诚地希望你不会觉得这本书无聊。我可不想冒犯 Dijkstra 的幽灵。如果你现在还在读下去，我猜测你确实找到了有价值的内容。好样的！感谢你一直坚持阅读。数学绝不应是无聊的。

我们已经涵盖了你需要理解和使用深度学习的基本内容。然而，千万不要停步于此：请利用附录中的参考资料，继续你的数学探索。你永远不应满足于现有的知识体系——永远追求扩展它。

如果你有任何问题或评论，请随时通过* mathfordeeplearning@gmail.com*与我联系。
