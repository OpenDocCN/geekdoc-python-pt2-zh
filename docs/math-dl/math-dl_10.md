## **10

反向传播**

![image](img/common.jpg)

反向传播目前是深度学习的*核心算法*。没有它，我们无法在合理的时间内训练深度神经网络，甚至无法训练。因此，深度学习的从业者需要理解什么是反向传播，它对训练过程带来了什么，以及如何实现它，至少对于简单的网络来说。在本章中，我将假设你对反向传播没有任何了解。

我们将从讨论反向传播是什么以及它不是做什么开始本章。然后，我们将通过一个简单网络的数学推导。之后，我们将介绍适合构建全连接前馈神经网络的反向传播矩阵描述。我们将深入探索数学原理，并尝试基于 NumPy 的实现。

深度学习工具包如 TensorFlow 并不像我们将在本章的前两节中所做的那样实现反向传播。相反，它们使用计算图，我们将在本章的最后简要讨论计算图。

### 什么是反向传播？

在第七章，我们介绍了标量函数对向量的梯度的概念。在第八章中，我们再次处理了梯度，并看到了它们与雅可比矩阵的关系。回想一下那一章，我们讨论了训练神经网络本质上是一个优化问题。我们知道训练神经网络涉及一个损失函数，这个函数是网络的权重和偏置的函数，用来告诉我们网络在训练集上的表现如何。当我们进行梯度下降时，我们会利用梯度来决定如何从损失景观的一个部分移动到另一个部分，从而找到网络表现最好的地方。训练的目标是最小化训练集上的损失函数。

这是高层次的概述。现在，让我们把它变得更具体一些。梯度适用于接受向量输入并返回标量值的函数。对于神经网络，向量输入是权重和偏置，这些参数定义了在架构固定后网络如何表现。符号上，我们可以将损失函数写作*L*(**θ**)，其中**θ**（theta）是网络中所有权重和偏置的向量。我们的目标是通过损失函数定义的空间找到最小值，即导致最小损失的特定**θ**，*L*。我们通过使用*L*(**θ**的梯度来实现这一目标。因此，要通过梯度下降训练神经网络，我们需要知道每个权重和偏置值对损失函数的贡献；也就是说，我们需要知道∂*L*/∂*w*，对于某个权重（或偏置）*w*。

反向传播是告诉我们每个网络的权重和偏置的∂*L*/∂*w*是什么的算法。通过这些偏导数，我们可以应用梯度下降来改进网络在下一次训练数据中的表现。

在继续之前，我们来谈一下术语。你经常会听到机器学习领域的人用*反向传播*作为训练神经网络整个过程的代名词。经验丰富的实践者明白他们的意思，但对于机器学习的新手来说，有时会有些困惑。为了明确，*反向传播*是用来找出每个权重和偏置值对网络误差贡献的算法，即 ∂*L*/∂*w*。*梯度下降*是使用 ∂*L*/∂*w* 来调整权重和偏置，以改进网络在训练集上的表现的算法。

Rumelhart、Hinton 和 Williams 在他们 1986 年的论文《通过反向传播误差学习表示》中介绍了反向传播。最终，反向传播是我们在第七章和第八章中讨论的链式法则的应用。反向传播从网络的输出开始，使用损失函数。它向*后*传播，因此得名“反向传播”，到达网络的更低层，传播误差信号以计算每个权重和偏置的 ∂*L*/∂*w*。请注意，实践者常常将其缩写为“反向传播”，你将经常遇到这个术语。

我们将在接下来的两个部分中通过示例讲解反向传播。目前，最重要的理解是，它是训练神经网络所需的两部分中的第一部分。它提供了第二部分——梯度下降——所需的信息，梯度下降将在第十一章中详细讨论。

### 手动反向传播

让我们定义一个简单的神经网络，它接受两个输入值，隐藏层中有两个节点，并且有一个输出节点，如图 10-1 所示。

![image](img/10fig01.jpg)

*图 10-1：一个简单的神经网络*

图 10-1 显示了网络及其六个权重，*w*[0] 到 *w*[5]，以及三个偏置值，*b*[0]、*b*[1] 和 *b*[2]。每个值都是标量。

我们将在隐藏层使用 sigmoid 激活函数，

![Image](img/245equ01.jpg)

输出节点不使用激活函数。为了训练网络，我们将使用平方误差损失函数，

![Image](img/245equ02.jpg)

其中，*y* 是训练样本的标签，值为 0 或 1，*a*[2] 是网络对于与 *y* 相关的输入（即 *x*[0] 和 *x*[1]）的输出。

让我们写出这个网络前向传播的方程式，这个过程从输入 *x*[0] 和 *x*[1] 向右到输出 *a*[2]。方程式为

![Image](img/10equ01.jpg)

在这里，我们引入了中间值 *z*[0] 和 *z*[1] 作为激活函数的参数。请注意，*a*[2] 没有激活函数。我们本可以在这里使用 sigmoid，但由于我们的标签要么是 0，要么是 1，无论如何我们都能学到一个好的输出值。

如果我们将一个单独的训练示例通过网络，输出是*a*[2]。如果与训练示例相关的标签，***x*** = (*x*[0], *x*[1])，是*y*，则平方误差损失如图 10-1 所示。

损失函数的参数是*a*[2]；*y*是一个固定常量。然而，*a*[2]直接依赖于*w*[4]，*w*[5]，*b*[2]，以及*a*[1]和*a*[0]的值，而这些值本身又依赖于*w*[0]，*w*[1]，*w*[2]，*w*[3]，*b*[0]，*b*[1]，*x*[0]和*x*[1]。因此，从权重和偏置的角度思考，我们可以将损失函数写为

*L* = *L*(*w*[0], *w*[1], *w*[2], *w*[3], *w*[4], *w*[5], *b*[0], *b*[1], *b*[2];*x*[0], *x*[1], *y*) = *L*(**θ**; ***x***, *y*)

这里，**θ**表示权重和偏置；它被视为变量。分号后的部分在这种情况下是常量：输入向量***x*** = (*x*[0], *x*[1])和相关标签，*y*。

我们需要损失函数的梯度，▽*L*(**θ**; ***x***, *y*)。为了明确起见，我们需要所有的偏导数，∂*L*/∂*w*[5]，∂*L*/∂*b*[0]，等等，针对所有的权重和偏置：总共九个偏导数。

这是我们的攻击计划。首先，我们将通过数学推导计算所有九个值的偏导数表达式。其次，我们将编写一些 Python 代码来实现这些表达式，以便我们可以训练图 10-1 中的网络来分类鸢尾花。在这个过程中，我们将学到一些东西。也许最重要的一点是，通过手工计算偏导数是相当繁琐的。我们会成功，但在接下来的章节中，幸运的是，我们将看到有一种更加简洁的方式来表示反向传播，尤其是对于完全连接的前馈网络。让我们开始吧。

#### 计算偏导数

我们需要图 10-1 中网络的所有偏导数的表达式。我们还需要激活函数——sigmoid 函数的导数表达式。让我们从 sigmoid 函数开始，因为一个巧妙的技巧可以将其导数写成 sigmoid 本身的形式，这是在前向传递时计算的值。

sigmoid 的导数如下所示。

![Image](img/10equ02.jpg)![Image](img/10equ03.jpg)

方程式 10.2 的技巧是在分子中加减 1，以改变因子的形式，使其成为 sigmoid 本身的另一个副本。因此，sigmoid 的导数是 sigmoid 和 1 减去 sigmoid 的乘积。回顾方程式 10.1，我们看到前向传递计算了 sigmoid 函数，即激活函数，作为*a*[0]和*a*[1]。因此，在反向传播偏导数的推导过程中，我们将能够通过方程式 10.3 用*a*[0]和*a*[1]代替 sigmoid 的导数，以避免第二次计算。

我们从导数开始。遵循反向传播的名字，我们将从损失函数开始反向计算，并应用链式法则得出所需的表达式。损失函数的导数，

![图片](img/247equ01.jpg)

是

![图片](img/10equ04.jpg)

这意味着在接下来的表达式中，我们可以将 ∂*L*/∂*a*[2] 替换为 *a*[2] − *y*。回想一下，*y* 是当前训练样本的标签，而我们在前向传播中计算 *a*[2] 作为网络的输出。

现在让我们找到 *w*[5]、*w*[4] 和 *b*[2] 的表达式，这些参数用于计算 *a*[2]。链式法则告诉我们

![图片](img/10equ05.jpg)

since

![图片](img/248equ01.jpg)

我们已经将 *a*[2] 的表达式从 方程 10.1 中替代了。

类似的逻辑得出了 *w*[4] 和 *b*[2] 的表达式：

![图片](img/10equ06.jpg)

太棒了！我们已经得到了三个需要的偏导数—只剩下六个了。现在让我们写出 *b*[1]、*w*[1] 和 *w*[3] 的表达式，

![图片](img/10equ07.jpg)

在这里我们使用

![图片](img/248equ02.jpg)

用 *a*[1] 替代 σ(*z*[1])，因为我们在前向传播过程中计算 *a*[1]。

一个类似的计算给出了最后三个偏导数的表达式：

![图片](img/10equ08.jpg)

呼！那真是繁琐，但现在我们得到了所需的内容。不过请注意，这是一个非常严格的过程—如果我们改变网络架构、激活函数或损失函数，我们需要重新推导这些表达式。现在让我们用这些表达式来分类鸢尾花。

#### 转换为 Python 代码

我在这里展示的代码在文件 *nn_by_hand.py* 中。打开它看看整体结构。我们从主函数开始（清单 10-1）：

❶ epochs = 1000

eta = 0.1

❷ xtrn, ytrn, xtst, ytst = 构建数据集()

❸ net = {}

net["b2"] = 0.0

net["b1"] = 0.0

net["b0"] = 0.0

net["w5"] = 0.0001*(np.random.random() - 0.5)

net["w4"] = 0.0001*(np.random.random() - 0.5)

net["w3"] = 0.0001*(np.random.random() - 0.5)

net["w2"] = 0.0001*(np.random.random() - 0.5)

net["w1"] = 0.0001*(np.random.random() - 0.5)

net["w0"] = 0.0001*(np.random.random() - 0.5)

❹ tn0,fp0,fn0,tp0,pred0 = 评估(net, xtst, ytst)

❺ net = 梯度下降(net, xtrn, ytrn, epochs, eta)

❻ tn,fp,fn,tp,pred = 评估(net, xtst, ytst)

print("训练 %d 个 epoch，学习率 %0.5f" % (epochs, eta))

print()

print("训练前：")

print("   TN:%3d FP:%3d" % (tn0, fp0))

print("   FN:%3d TP:%3d" % (fn0, tp0))

print()

print("训练后：")

print("   TN:%3d FP:%3d" % (tn, fp))

print("   FN:%3d TP:%3d" % (fn, tp))

*清单 10-1：* *主* *函数*

首先，我们设置迭代次数和学习率 η（eta）❶。迭代次数是指通过训练集多次更新网络权重和偏置的次数。网络结构简单，而我们的数据集很小，只有 70 个样本，因此需要很多次迭代来训练。梯度下降使用学习率来决定如何根据梯度值进行调整。我们将在第十一章中更深入地探讨学习率。

接下来，我们加载数据集 ❷。我们使用的是在第六章和第九章中使用的同一个鸢尾花数据集，保留前两个特征和类 0 与类 1。见 *nn_by_hand.py* 中的 BuildDataset 函数。返回值是 NumPy 数组：xtrn（70 × 2）和 xtst（30 × 2）分别表示训练和测试数据，以及对应的标签 ytrn 和 ytst。

我们需要一个地方来存储网络的权重和偏置。Python 字典可以胜任，所以我们接下来用默认值 ❸ 来设置它。注意，我们将偏置值设为零，将权重设为在 [−0.00005, +0.00005] 范围内的小随机值。在这个案例中，这些设置似乎效果不错。

main 的其余部分对随机初始化的网络进行评估（Evaluate ❹）在测试数据上，执行梯度下降训练模型（GradientDescent ❺），然后再次评估测试数据以证明训练有效 ❻。

列表 10-2 展示了 Evaluate 和它所调用的 Forward 函数。

def Evaluate(net, x, y):

out = Forward(net, x)

tn = fp = fn = tp = 0

pred = []

for i in range(len(y)):

❶ c = 0 if (out[i] < 0.5) else 1

pred.append(c)

if (c == 0) and (y[i] == 0):

tn += 1

elif (c == 0) and (y[i] == 1):

fn += 1

elif (c == 1) and (y[i] == 0):

fp += 1

else:

tp += 1

return tn, fp, fn, tp, pred

def Forward(net, x):

out = np.zeros(x.shape[0])

for k in range(x.shape[0]):

❷ z0 = net["w0"]*x[k,0] + net["w2"]*x[k,1] + net["b0"]

a0 = sigmoid(z0)

z1 = net["w1"]*x[k,0] + net["w3"]*x[k,1] + net["b1"]

a1 = sigmoid(z1)

out[k] = net["w4"]*a0 + net["w5"]*a1 + net["b2"]

return out

*列表 10-2: The* *Evaluate* *函数*

让我们从 Forward 开始，它执行对数据 x 的前向传递。在创建一个用于保存网络输出的地方（out）后，每个输入都会通过网络，使用当前的参数值 ❷。注意，代码直接实现了方程 10.1，其中 out[k] 替代了 *a*[2]。当所有输入都处理完成后，我们将收集到的输出返回给调用者。

现在我们来看一下 Evaluate。它的参数包括一组输入特征 x、关联标签 y 和网络参数 net。Evaluate 首先通过调用 Forward 将数据输入网络以生成输出。这些是网络的原始浮点输出。为了将其与实际标签进行比较，我们应用一个阈值❶，将输出小于 0.5 的标记为类 0，输出大于等于 0.5 的标记为类 1。预测的标签被附加到 pred 中，并通过与 y 中的实际标签进行比较来统计。

如果实际标签和预测标签都是零，则模型正确识别了*真正负类*（TN），即类 0 的真实实例。如果网络预测为类 0，但实际标签是类 1，则出现*假负类*（FN），即一个类 1 的实例被标记为类 0。反之，将类 0 实例标记为类 1 是*假正类*（FP）。唯一剩下的情况是实际的类 1 实例被标记为类 1，这是*真正正类*（TP）。最后，我们将统计结果和预测返回给调用者。

列表 10-3 展示了 GradientDescent 函数，它是由列表 10-1 调用的❺。在这里我们实现了上面计算的偏导数。

def GradientDescent(net, x, y, epochs, eta):

❶ for e in range(epochs):

dw0 = dw1 = dw2 = dw3 = dw4 = dw5 = db0 = db1 = db2 = 0.0

❷ for k in range(len(y)):

❸ z0 = net["w0"]*x[k,0] + net["w2"]*x[k,1] + net["b0"]

a0 = sigmoid(z0)

z1 = net["w1"]*x[k,0] + net["w3"]*x[k,1] + net["b1"]

a1 = sigmoid(z1)

a2 = net["w4"]*a0 + net["w5"]*a1 + net["b2"]

❹ db2 += a2 - y[k]

dw4 += (a2 - y[k]) * a0

dw5 += (a2 - y[k]) * a1

db1 += (a2 - y[k]) * net["w5"] * a1 * (1 - a1)

dw1 += (a2 - y[k]) * net["w5"] * a1 * (1 - a1) * x[k,0]

dw3 += (a2 - y[k]) * net["w5"] * a1 * (1 - a1) * x[k,1]

db0 += (a2 - y[k]) * net["w4"] * a0 * (1 - a0)

dw0 += (a2 - y[k]) * net["w4"] * a0 * (1 - a0) * x[k,0]

dw2 += (a2 - y[k]) * net["w4"] * a0 * (1 - a0) * x[k,1]

m = len(y)

❺ net["b2"] = net["b2"] - eta * db2 / m

net["w4"] = net["w4"] - eta * dw4 / m

net["w5"] = net["w5"] - eta * dw5 / m

net["b1"] = net["b1"] - eta * db1 / m

net["w1"] = net["w1"] - eta * dw1 / m

net["w3"] = net["w3"] - eta * dw3 / m

net["b0"] = net["b0"] - eta * db0 / m

net["w0"] = net["w0"] - eta * dw0 / m

net["w2"] = net["w2"] - eta * dw2 / m

return net

*列表 10-3：使用* *GradientDescent* *训练网络*

GradientDescent 函数包含一个双重循环。外层循环❶是针对 epochs，即训练集的完整遍历次数。内层循环❷是针对每个训练样本，逐个处理。首先进行前向传播❸以计算输出 a2 和中间值。

下一段代码实现了使用偏导数的反向传递，方程 10.4 到 10.8，将误差（损失）反向传递通过网络 ❹。我们使用训练集的平均损失来更新权重和偏置。因此，我们累计每个训练示例对每个权重和偏置值的损失贡献。这解释了为什么要将每个新的贡献加到训练集的总和中。

在将每个训练示例通过网络并累计其对损失的贡献后，我们更新权重和偏置 ❺。偏导数为我们提供了梯度，即最大变化的方向；然而，我们希望最小化损失，因此我们会朝着与梯度*相反*的方向移动，从当前值中减去每个权重和偏置的损失平均值。

例如，

net["b2"] = net["b2"] - eta * db2 / m

是

![Image](img/252equ01.jpg)

其中 η = 0.1 是学习率，*m* 是训练集中样本的数量。求和是针对每个输入样本的 *b*[2] 的偏导数，***x**[i]*，其平均值乘以学习率，用于调整 *b*[2] 以便进入下一个时期。我们常用另一个名字来表示学习率，即*步长*。该参数控制网络的权重和偏置在损失地形中向最小值迈进的速度。

我们的实现已完成。让我们运行它，看看它的表现如何。

#### 训练和测试模型

让我们来看一下训练数据。我们可以将特征绘制成每个坐标轴一个，以查看区分这两个类别的难易程度。结果是图 10-2，其中类别 0 用圆圈表示，类别 1 用方块表示。

![image](img/10fig02.jpg)

*图 10-2：鸢尾花训练数据，显示类别 0（圆圈）和类别 1（方块）*

很容易看出，这两个类别彼此之间相当分离，因此即使是我们拥有两个隐藏神经元的基础网络，也应该能够学习到它们之间的差异。将这个图与图 6-2 的左侧进行比较，后者显示了所有三类鸢尾花的前两个特征。如果我们在数据集中包括类别 2，那么两个特征将不足以区分所有三个类别。

运行代码：

python3 nn_by_hand.py

对我来说，这会输出：

训练 1000 轮，学习率 0.10000

训练前：

TN: 15 FP: 0

FN: 15 TP: 0

训练后：

TN: 14 FP: 1

FN: 1 TP: 14

我们被告知训练使用了 1,000 次遍历包含 70 个示例的训练集。这是清单 10-3 中的外部循环。接着，我们看到两张数字表格，分别描述了训练前和训练后的网络状态。让我们逐一查看这些表格，理解它们所讲述的故事。

这些表有几个名称：*列联表*、*2* × *2 表*，或 *混淆矩阵*。术语 *混淆矩阵* 最为通用，但通常用于多类分类器。标签用于计算测试集中真正的正例、真正的负例、假正例和假负例的数量。测试集包含 30 个样本，每个类别 15 个。如果网络是完美的，那么所有类别 0 的样本将被计入 TN（真正负例）中，所有类别 1 的样本将被计入 TP（真正正例）中。错误则为 FP（假正例）或 FN（假负例）计数。

随机初始化的网络将所有样本都标记为类别 0。我们之所以知道这一点，是因为有 15 个 TN 样本（真正类别 0 的样本）和 15 个 FN 样本（15 个实际类别 1 的样本被标记为类别 0）。因此，训练前的总体准确率为 15/(15 + 15) = 0.5 = 50%。

经过训练后，代码在列表 10-3 的外循环中运行 1,000 次，测试数据几乎被完美分类，其中类别 0 的 15 个样本中有 14 个被正确分类，类别 1 的 15 个样本中有 14 个被正确分类。总体准确率为(14 + 14)/(15 + 15) = 28/30 = 93.3%，考虑到我们的模型只有一个由两个节点组成的单隐藏层，这个结果还算不错。

再次强调，这个练习的主要目的是看到手动计算导数是多么繁琐且容易出错。上面的代码仅处理标量；它并未处理向量或矩阵，也没有利用通过更好的反向传播算法表示所可能利用的任何对称性。幸运的是，我们可以做得更好。让我们再次看一下全连接网络的反向传播算法，看看能否通过使用向量和矩阵，提出一种更简洁的方法。

### 全连接网络的反向传播

在本节中，我们将探讨允许我们将误差项从网络输出反向传递到输入的方程式。此外，我们还将看到如何利用这个误差项来计算一层权重和偏置的必要偏导数，以便实现梯度下降。掌握了所有必要的表达式后，我们将实现 Python 类，帮助我们构建和训练任意深度和形状的全连接前馈神经网络。最后，我们将用 MNIST 数据集来测试这些类。

#### 误差的反向传播

让我们从一个有用的观察开始：一个全连接神经网络的各层可以被看作是向量函数：

***y*** = ***f***(***x***)

其中层的输入是***x***，输出是***y***。输入***x***可以是训练样本的实际输入，或者如果是在模型的某一隐藏层工作，则是上一层的输出。这些都是向量；每一层中的每个节点产生一个标量输出，这些标量汇聚在一起，形成***y***，表示该层的输出。

正向传播按顺序通过网络的各层，映射***x**[i]* 到 ***y**[i]*，使得 ***y**[i]* 成为 ***x**[i]*+1，即层 *i* + 1 的输入。所有层处理完后，我们使用最后一层的输出，称为 ***h***，来计算损失，*L*(***h***, ***y***[true])。损失是衡量网络在输入 ***x*** 上的误差，我们通过将其与真实标签 ***y***[true] 比较来确定。注意，如果模型是多分类的，输出 ***h*** 是一个向量，每个元素代表一个类别，而真实标签是一个零向量，只有实际类别的索引位置为 1。这就是为什么许多工具包，比如 Keras，将整数类别标签映射为独热向量的原因。

我们需要将损失值，或称为*误差*，反向传播通过网络；这就是反向传播步骤。为了使用每层向量和权重矩阵在全连接网络中实现这一点，我们首先需要了解如何执行正向传播。正如我们在上面构建的网络中所做的那样，我们将激活函数的应用与全连接层的操作分开。

例如，对于任何一个输入向量***x***来自下层的层，我们需要计算一个输出向量，***y***。对于一个全连接层，正向传播是

***y*** = ***Wx*** + ***b***

其中，***W*** 是权重矩阵，***x*** 是输入向量，***b*** 是偏置向量。

对于一个激活层，我们有

***y*** = **σ**(***x***)

对于我们选择的任何激活函数，**σ**，我们将在本章其余部分中使用 sigmoid 函数。请注意，我们将函数设置为一个向量值函数。为此，我们将标量 sigmoid 函数应用于输入向量的每个元素，以生成输出向量：

**σ**(***x***) = [*σ*(*x*[0]) *σ*(*x*[1]) ... *σ*(*x[n]*[−1])]^⊤

一个全连接网络由一系列全连接层和后续的激活层组成。因此，正向传播是一系列操作的链条，首先是将模型的输入传递给第一层，生成输出，然后将输出传递给下一层的输入，依此类推，直到所有层都被处理完。

正向传播得到最终输出和损失函数。损失函数对网络输出的导数是第一个误差项。为了将误差项反向传递到模型中，我们需要计算误差项如何随着层输入的变化而变化，使用误差如何随着层输出的变化而变化的方式。具体来说，对于每一层，我们需要知道如何计算

![Image](img/256equ01.jpg)

也就是说，我们需要知道误差项如何随着给定层输入的变化而变化

![Image](img/256equ02.jpg)

这就是误差项如何随着层输出的变化而变化。链式法则告诉我们如何做到这一点：

![Image](img/10equ09.jpg)

当我们向后传递网络时，层*i*的∂*E*/∂***x***变为层*i* − 1 的∂*E*/∂***y***。

在操作上，反向传播算法变为

1.  运行一次前向传播，将***x***映射到***y***，逐层得到最终输出***h***。

1.  使用***h***和***y***[true]计算损失函数的导数值；这将变成输出层的∂*E*/∂***y***。

1.  对所有早期层重复此操作，计算从∂*E*/∂***y***到∂*E*/∂***x***，使得层*i*的∂*E*/∂***x***变为层*i* − 1 的∂*E*/∂***y***。

这个算法将误差项向后通过网络传递。让我们从激活层开始，逐层计算出所需的偏导数。

我们将假设我们已知∂*E*/∂***y***，并正在寻找∂*E*/∂***x***。链式法则告诉我们

![Image](img/10equ10.jpg)

在这里，我们引入⊙来表示 Hadamard 积。回想一下，Hadamard 积是两个向量或矩阵的逐元素相乘。（见第五章以获得复习）

我们现在知道如何将误差项通过激活层。我们正在考虑的唯一其他层是全连接层。如果我们展开方程 10.9，我们得到

![Image](img/10equ11.jpg)

由于

![Image](img/257equ01.jpg)

结果是***W***^⊤，而不是***W***，因为在分母表示法中，矩阵与向量相乘的导数是矩阵的转置，而不是矩阵本身。

让我们稍作停顿，回顾并思考方程 10.10 和 10.11 的形式。这些方程告诉我们如何将误差项从一层传递到另一层。它们的形状是什么样的？对于激活层，如果输入有*k*个元素，那么输出也有*k*个元素。因此，方程 10.10 中的关系应该将一个*k*元素的向量映射到另一个*k*元素的向量。误差项∂*E*/∂***y***是一个*k*元素的向量，激活函数的导数σ′(***x***)也是如此。最后，二者的 Hadamard 积同样输出一个*k*元素的向量，这是所需要的。

对于全连接层，我们有一个*m*元素的输入，***x***；一个*n* × *m*元素的权重矩阵，***W***；以及一个*n*元素的输出向量，***y***。因此，我们需要从*n*元素的误差项∂*E*/∂***y***生成一个*m*元素的向量∂*E*/∂***x***。将权重矩阵的转置，一个*m* × *n*元素的矩阵，与误差项相乘，结果是一个*m*元素的向量，因为*m* × *n*乘以*n* × 1 得到了*m* × 1，这是一个*m*元素的列向量。

#### 计算权重和偏置的偏导数

方程 10.10 和 10.11 告诉我们如何将误差项反向传播通过网络。然而，反向传播的重点在于计算权重和偏置的变化如何影响误差，以便我们能够使用梯度下降。具体来说，对于每一个全连接层，我们需要得到以下表达式：

![Image](img/258equ01.jpg)

给定

![Image](img/258equ02.jpg)

让我们从∂*E*/∂***b***开始。再次应用链式法则，得到

![Image](img/10equ12.jpg)

这意味着全连接层中由于偏置项产生的误差与输出误差相同。

权重矩阵的计算方式类似：

![Image](img/10equ13.jpg)

上面的方程告诉我们，由于权重矩阵的误差是输出误差和输入***x***的乘积。权重矩阵是一个*n* × *m*元素矩阵，因为前向传播时会乘以*m*元素的输入向量。因此，来自权重的误差贡献∂*E*/∂***W***也必须是一个*n* × *m*矩阵。我们知道∂*E*/∂***y***是一个*n*元素的列向量，而***x***的转置是一个*m*元素的行向量。二者的外积是一个*n* × *m*矩阵，符合要求。

方程 10.10，10.11，10.12 和 10.13 适用于单个训练样本。这意味着，对于特定的网络输入，这些方程，特别是 10.12 和 10.13，告诉我们偏置和权重对*该输入样本*的损失贡献。

为了实现梯度下降，我们需要对这些误差，即∂*E*/∂***W***和∂*E*/∂***b***，在训练样本上进行累积。然后我们使用这些误差的平均值来更新每个周期末的权重和偏置，或者如我们所实现的那样，更新小批量。由于梯度下降是第十一章的主题，下面我们将仅概述如何使用反向传播来实现梯度下降，详细的内容将留给该章节以及我们接下来要实现的代码。

然而，一般来说，为了训练网络，我们需要对小批量中的每个样本执行以下操作：

1.  将样本通过网络进行前向传播以产生输出。在此过程中，我们需要存储每一层的输入，因为在实现反向传播时需要用到它（即，我们需要从方程 10.13 中获取***x***^⊤）。

1.  计算损失函数的导数值，对我们来说，损失函数是均方误差，作为反向传播中的第一个误差项使用。

1.  按反向顺序通过网络的各层，计算每个全连接层的∂*E*/∂***W***和∂*E*/∂***b***。这些值会在每个小批量样本中累积（**Δ*W***，**Δ*b***）。

当小批量样本处理完毕并且误差累计完成时，便可以进行一次梯度下降步伐。在这一过程中，每一层的权重和偏置都会通过更新。

![Image](img/10equ14.jpg)

其中**Δ*W***和**Δ*b***是小批量数据上的累计误差，*m*是小批量的大小。重复的梯度下降步骤会导致最终的权重和偏置集——一个训练好的网络。

这一部分内容数学量较大。接下来的部分将数学转化为代码，我们会看到由于 NumPy 和面向对象设计的帮助，代码非常简洁和优雅。如果你对数学不太清晰，我猜代码将大大帮助你理清思路。

#### 一个 Python 实现

我们的实现风格类似于像 Keras 这样的工具包。我们希望能够创建任意的全连接网络，因此我们将使用 Python 类来表示每一层，并将网络架构存储为层的列表。每一层维护自己的权重和偏置，并具备进行前向传播、反向传播和梯度下降步骤的能力。为简便起见，我们将使用 sigmoid 激活函数和平方误差损失。

我们需要两个类：ActivationLayer 和 FullyConnectedLayer。另一个 Network 类将各个部分组合在一起并处理训练。所有类都位于文件 *NN.py* 中。（这里的代码是对 Omar Aflak 原始代码的修改，并且已获得他的许可。参见 *NN.py* 中的 GitHub 链接。我对代码做了修改，使其支持小批量处理，并支持除了每个样本外的梯度下降步骤。）

让我们逐步走过这三个类，从 ActivationLayer 开始（见列表 10-4）。我们所做的数学运算转化为代码形式非常优雅，在大多数情况下只需要一行 NumPy 代码。

class ActivationLayer:

def forward(self, input_data):

self.input = input_data

返回 sigmoid(input_data)

def backward(self, output_error):

返回 sigmoid_prime(self.input) * 输出误差

def step(self, eta):

返回

*列表 10-4：* *ActivationLayer* *类*

列表 10-4 显示了 ActivationLayer，并且只包含三个方法：forward、backward 和 step。最简单的是 step。它什么也不做，因为在梯度下降过程中，激活层没有任何操作，因为没有权重或偏置值。

forward 方法接受输入向量，***x***，将其存储以备后用，然后通过应用 sigmoid 激活函数来计算输出向量，***y***。

backward 方法接受来自上一层的输出误差 ∂*E*/∂***y***，然后通过在前向传播时将 sigmoid 的导数（sigmoid_prime）应用于输入集，并按元素逐个相乘以计算误差，最终返回公式 10.10。

sigmoid 和 sigmoid_prime 辅助函数是

def sigmoid(x):

返回 1.0 / (1.0 + np.exp(-x))

def sigmoid_prime(x):

返回 sigmoid(x)*(1.0 - sigmoid(x))

接下来是 FullyConnectedLayer 类。它比 ActivationLayer 类复杂一些，但没有显著复杂。见列表 10-5。

class FullyConnectedLayer:

def __init__(self, input_size, output_size):

❶ self.delta_w = np.zeros((input_size, output_size))

self.delta_b = np.zeros((1,output_size))

self.passes = 0

❷ self.weights = np.random.rand(input_size, output_size) - 0.5

self.bias = np.random.rand(1, output_size) - 0.5

def forward(self, input_data):

self.input = input_data

❸ return np.dot(self.input, self.weights) + self.bias

def backward(self, output_error):

input_error = np.dot(output_error, self.weights.T)

weights_error = np.dot(self.input.T, output_error)

self.delta_w += np.dot(self.input.T, output_error)

self.delta_b += output_error

self.passes += 1

return input_error

def step(self, eta):

❹ self.weights -= eta * self.delta_w / self.passes

self.bias -= eta * self.delta_b / self.passes

❺ self.delta_w = np.zeros(self.weights.shape)

self.delta_b = np.zeros(self.bias.shape)

self.passes = 0

*列表 10-5：* *FullyConnectedLayer* *类*

我们告诉构造函数输入节点和输出节点的数量。输入节点的数量（input_size）指定进入该层的向量的元素个数。同样，output_size 指定输出向量的元素个数。

全连接层在小批量上累积权重和偏置误差，delta_w 中的 ∂*E*/∂***W*** 项和 delta_b 中的 ∂*E*/∂***b*** 项 ❶。每处理一个样本，就会增加一个 pass。

我们必须用随机的权重和偏置值初始化神经网络；因此，构造函数使用范围 [−0.5, 0.5] 内的均匀随机值来设置初始的权重矩阵和偏置向量 ❷。请注意，偏置向量是 1 × *n* 的行向量。代码翻转了上述方程的顺序，以匹配通常存储训练样本的方式：每行是一个样本，每列是一个特征的矩阵。由于标量乘法是交换的：*ab* = *ba*，所以计算结果是相同的。

forward 方法将输入向量存储以供反向传播使用，然后计算层的输出，将输入与权重矩阵相乘并加上偏置项 ❸。

只剩下两个方法。backward 方法接收 ∂*E*/∂***y***（output_error）并计算 ∂*E*/∂***x***（input_error）、∂*E*/∂***W***（weights_error）和 ∂*E*/∂***b***（output_error）。我们将这些误差添加到该层的累计误差中，delta_w 和 delta_b，供 step 使用。

step 方法包括一个全连接层的梯度下降步骤。与 ActivationLayer 的空方法不同，FullyConnectedLayer 有很多事情要做。我们使用平均误差来更新权重矩阵和偏置向量，如 方程 10.14 所示 ❹。这实现了小批量上的梯度下降步骤。最后，我们重置累加器和计数器，为下一个小批量做准备 ❺。

Network 类将一切整合在一起，如 列表 10-6 所示。

class Network:

def __init__(self, verbose=True):

self.verbose = verbose

❶ self.layers = []

def add(self, layer):

❷ self.layers.append(layer)

def predict(self, input_data):

result = []

对于输入数据中的每个样本：

output = input_data[i]

对于网络中的每一层，执行以下操作：

output = layer.forward(output)

result.append(output)

❸ return result

def fit(self, x_train, y_train, minibatches, learning_rate, batch_size=64):

❹ for i in range(minibatches):

err = 0

idx = np.argsort(np.random.random(x_train.shape[0]))[:batch_size]

x_batch = x_train[idx]

y_batch = y_train[idx]

❺ for j in range(batch_size):

output = x_batch[j]

对于网络中的每一层，执行以下操作：

output = layer.forward(output)

❻ err += mse(y_batch[j], output)

❼ error = mse_prime(y_batch[j], output)

对网络中的每一层，执行以下操作：

error = layer.backward(error)

❽ 对网络中的每一层，执行以下操作：

layer.step(learning_rate)

if (self.verbose) and ((i%10) == 0):

err /= batch_size

print('minibatch %5d/%d error=%0.9f' % (i, minibatches, err))

*列表 10-6：* *Network* *类*

Network 类的构造函数很简单。我们设置一个 verbose 标志，用来切换在训练过程中是否显示每个 minibatch 的平均误差。成功的训练应该显示误差随着时间的推移逐渐减少。随着网络中层的增加，它们会存储在 layers 中，构造函数会初始化这些层❶。add 方法通过将层对象附加到 layers 中来将层添加到网络❷。

网络训练完成后，predict 方法通过网络的各层进行前向传递，生成每个输入样本在 input_data 中的输出。注意这个模式：输入样本被赋值给 output；然后，遍历各层的循环依次调用每一层的 forward 方法，将上一层的输出作为输入传递给下一层；如此类推，直到整个网络完成。当循环结束时，output 包含最后一层的输出，并将其附加到 result 中，最终返回给调用者❸。

训练网络是 fit 方法的工作。这个名字与 sklearn 的标准训练方法相匹配。参数是样本向量的 NumPy 数组，每行一个（x_train），以及它们的标签作为 one-hot 向量（y_train）。接下来是训练的 minibatch 数量。稍后我们会讨论 minibatch。我们还提供学习率η（eta）和一个可选的 minibatch 大小 batch_size。

fit 方法使用双重循环。第一个循环是遍历所需的 minibatches 数量❹。正如我们之前所学，minibatch 是全体训练集的一个子集，一个 epoch 是训练集的完整遍历。使用整个训练集被称为*批量训练*，而批量训练使用 epoch。然而，不进行批量训练是有充分理由的，正如你将在第十一章中看到的那样，因此引入了*minibatch*的概念。典型的 minibatch 大小通常是每次 16 到 128 个样本。常使用 2 的幂来使得基于 GPU 的深度学习工具包运行得更顺畅。对我们来说，64 或 63 个样本的 minibatch 在性能上没有区别。

我们选择大多数小批量数据作为训练数据的顺序集，以确保所有数据都被使用。在这里，我们稍微懒惰一些，每次需要小批量时选择随机子集。这简化了代码，并增加了一个随机性可以发挥作用的地方。那就是 idx 为我们提供的，它给出了训练集的索引的随机顺序，并只保留了前 batch_size 个。然后我们使用 x_batch 和 y_batch 进行实际的前向和反向传播。

第二个循环是对小批量样本 ❺ 进行处理。样本将逐一通过网络的各层，像 predict 函数一样调用 forward。为了显示目的，实际的均方误差（即前向传播输出与样本标签之间的误差）会在小批量中累积 ❻。

反向传播从输出误差项开始，即损失函数的导数 mse_prime ❼。然后，反向传播继续*向后*通过网络的各层，将前一层的输出误差作为输入传递给下层，这一过程直接镜像了前向传播的过程。

一旦循环处理完所有的小批量样本 ❺，就该根据每层网络在样本 ❽ 上累积的均方误差进行一次梯度下降步伐。step 函数的参数只需要学习率。小批量的处理通过报告平均误差来结束，如果 verbose 设置为每 10 个小批量进行一次输出。

我们将在第十一章中再次使用这段代码，探索梯度下降。现在，我们先用 MNIST 数据集来测试它的效果。

#### 使用实现代码

让我们来试试 *NN.py*。我们将用它来为 MNIST 数据集构建分类器，这是我们在第九章中首次遇到的。原始的 MNIST 数据集由 28×28 像素的手写数字灰度图像组成，背景为黑色。这是机器学习社区的常用数据集。我们将在将其转换为 196 个元素的向量（= 14 × 14）之前，将图像大小调整为 14×14 像素。

该数据集包含 60,000 张训练图像和 10,000 张测试图像。数据向量存储在 NumPy 数组中；请查看 *dataset* 目录中的文件。生成数据集的代码位于 *build_dataset.py* 中。如果你想自己运行这段代码，首先需要安装 Keras 和 OpenCV 的 Python 版本。Keras 提供了原始图像集并将训练集标签映射为独热编码向量，OpenCV 则将图像从 28×28 像素缩放为 14×14 像素。

我们需要的代码在 *mnist.py* 中，如列表 10-7 所示。

import numpy as np

from NN import *

❶ x_train = np.load("dataset/train_images_small.npy")

x_test = np.load("dataset/test_images_small.npy")

y_train = np.load("dataset/train_labels_vector.npy")

y_test = np.load("dataset/test_labels.npy")

❷ x_train = x_train.reshape(x_train.shape[0], 1, 14*14)

x_train /= 255

x_test = x_test.reshape(x_test.shape[0], 1, 14*14)

x_test /= 255

❸ net = Network()

net.add(FullyConnectedLayer(14*14, 100))

net.add(ActivationLayer())

net.add(FullyConnectedLayer(100, 50))

net.add(ActivationLayer())

net.add(FullyConnectedLayer(50, 10))

net.add(ActivationLayer())

❹ net.fit(x_train, y_train, minibatches=40000, learning_rate=1.0)

❺ out = net.predict(x_test)

cm = np.zeros((10,10), dtype="uint32")

for i in range(len(y_test)):

cm[y_test[i],np.argmax(out[i])] += 1

print()

print(np.array2string(cm))

print()

print("accuracy = %0.7f" % (np.diag(cm).sum() / cm.sum(),))

*清单 10-7：对 MNIST 数字的分类*

请注意，我们在导入*NN.py*之后立即导入了 NumPy。接下来，我们加载训练图像、测试图像和标签 ❶。Network 类期望每个样本向量是一个 1 × *n*的行向量，因此我们将训练数据从(60000,196)重塑为(60000,1,196)—这与测试数据相同 ❷。同时，我们将 8 位数据从[0, 255]缩放到[0, 1]。这是图像数据的标准预处理步骤，因为这样做可以使网络更容易学习。

接下来是构建模型 ❸。首先，我们创建一个 Network 类的实例。然后，通过定义一个具有 196 个输入和 100 个输出的 FullyConnectedLayer（全连接层）来添加输入层。接着添加一个 sigmoid 激活层。然后，我们再添加第二个全连接层，将第一层的 100 个输出映射到 50 个输出，并附加一个激活层。最后，添加一个全连接层，将前一层的 50 个输出映射到 10 个输出，表示类别的数量，并添加相应的激活层。这个方法模拟了常见的工具包，比如 Keras。

训练通过调用 fit ❹来进行。我们使用默认的 64 个样本的小批量大小，指定了 40,000 个小批量。我们将学习率设置为 1.0，这在此实例中表现良好。训练大约需要 17 分钟，在我的旧版 Intel i5 Ubuntu 系统上。随着模型训练的进行，会报告每个小批量的平均误差。当训练完成时，我们通过网络传递 10,000 个测试样本，并计算一个 10 × 10 的混淆矩阵 ❺。回想一下，混淆矩阵的行是实际类别标签，这里是 0 到 9 的数字。列则对应于预测标签，即每个输入样本的 10 个输出中最大的值。矩阵的元素表示真实标签是*i*，并且分配的标签是*j*的次数。如果模型完美，矩阵将完全是对角线；即不会出现真实标签和模型标签不一致的情况。最后，会打印出整体准确率，作为对角线和矩阵总和的比值，表示所有测试样本的准确度。

我运行的*mnist.py*结果是

minibatch 39940/40000  error=0.003941790

minibatch 39950/40000  error=0.001214253

minibatch 39960/40000  error=0.000832551

minibatch 39970/40000  error=0.000998448

minibatch 39980/40000  error=0.002377286

minibatch 39990/40000  error=0.000850956

[[ 965    0    1   1   1   5   2   3   2    0]

[   0 1121    3   2   0   1   3   0   5    0]

[   6    0 1005   4   2   0   3   7   5    0]

[   0    1    6 981   0   4   0   9   4    5]

[   2    0    3   0 953   0   5   3   1   15]

[   4    0    0  10   0 864   5   1   4    4]

[   8    2    1   1   3   4 936   0   3    0]

[   2    7   19   2   1   0   0 989   1    7]

[   5    0    4   5   3   5   7   3 939    3]

[   5    5    2  10   8    2   1   3   6 967]]

accuracy = 0.9720000

混淆矩阵呈现出强烈的对角线分布，总体准确率为 97.2%。对于像*NN.py*这样简单的工具包和一个全连接的前馈网络来说，这个结果还算不错。网络最大的错误是将七与二混淆了 19 次（混淆矩阵中的元素[7,2]）。下一个错误是将四与九混淆了 15 次（元素[4,9]）。这两个错误是有道理的：七和二看起来很相似，四和九也常常被混淆。

我们在本章开始时创建了一个包含两个输入、两个隐藏层节点和一个输出的网络。文件*iris.py*通过将数据集调整为网络所期望的格式，实现了相同的模型。我们不会逐行讲解代码，但请务必运行它。当我运行时，我在测试集上的表现稍有提高：类 0 的正确率是 14/15，类 1 的正确率是 15/15。

很遗憾，这里和上一节所详细介绍的反向传播方法对于深度学习来说并不够灵活。现代的工具包并不使用这些方法。让我们探索一下深度学习工具包在反向传播方面是如何处理的。

### 计算图

在计算机科学中，*图*是由节点（顶点）和连接它们的边组成的集合。我们一直在使用图来表示神经网络。在本节中，我们将使用图来表示表达式。

考虑这个简单的表达式：

*y* = *mx* + *b*

要评估这个表达式，我们遵循关于操作符优先级的公认规则。遵循这些规则意味着一系列的原始操作，我们可以将其表示为图，如图 10-3 所示。

![image](img/10fig03.jpg)

*图 10-3：实现 y = mx + b 的计算图*

数据通过图 10-3 中的箭头从左到右流动。数据源来自*x*、*m*和*b*，然后流经*操作符*（如*和+），最终输出结果*y*。

图 10-3 是一个*计算图*——一个指定如何评估表达式的图。像 C 语言这样的编译器会生成某种形式的计算图，将高级表达式转换为机器语言指令的序列。对于上面的表达式，首先将*x*和*m*的值相乘，乘法运算的结果会传递到加法运算中，与*b*一起计算最终的输出*y*。

我们可以将表达式，包括那些表示复杂深度神经网络的表达式，表示为计算图。我们以这种方式表示了全连接前馈模型，数据从输入 ***x*** 流向隐藏层，再到输出，即损失函数。

计算图是深度学习工具包（如 TensorFlow 和 PyTorch）管理模型结构并实现反向传播的方式。与本章早期的刚性计算不同，计算图是通用的，能够表示深度学习中使用的所有架构。

当你浏览深度学习文献并开始使用特定工具包时，你会遇到两种不同的计算图使用方法。第一种方法在数据可用时动态生成图。PyTorch 使用这种方法，称为 *符号到数字*。TensorFlow 使用第二种方法，*符号到符号*，提前构建静态计算图。两种方法都实现了计算图，并且都能自动计算反向传播所需的导数。

TensorFlow 生成反向传播所需的导数，方式与我们在前一节中所做的非常相似。像加法一样，每个操作都知道如何根据其输入生成输出的导数。加上链式法则，这就是实现反向传播所需的全部内容。图的遍历方式取决于*图计算引擎*和具体的模型架构，但图会根据需要在前向和后向传播中进行遍历。请注意，由于计算图将表达式分解为更小的操作，每个操作都知道如何在反向步骤中处理梯度（就像我们上面为激活层和全连接层所做的那样），因此可以在层中使用自定义函数，而无需手动处理导数。图引擎会为你完成这项工作，只要你使用的是引擎已经支持的基本操作。

让我们一起走一遍计算图的前向和后向传播过程。这个示例来自 2015 年论文《TensorFlow：在异构分布式系统上进行大规模机器学习》（* [`arxiv.org/pdf/1603.04467.pdf`](https://arxiv.org/pdf/1603.04467.pdf) *）。

在全连接模型中，隐藏层表示为

***y*** = **σ**(***Wx*** + ***b***)

对于权重矩阵 ***W***，偏置向量 ***b***，输入 ***x*** 和输出 ***y***。

图 10-4 将相同的方程表示为计算图。

![image](img/10fig04.jpg)

*图 10-4：表示前向和后向传播通过一个前馈神经网络层的计算图*

图 10-4 提供了两个版本。图的顶部显示了前向传播，其中数据从 ***x***、***W*** 和 ***b*** 流向输出。注意箭头是从左到右的。

请注意，源是张量，这里可以是向量或矩阵。操作的输出也是张量。张量在图中流动，这也就是*TensorFlow*名称的由来。图 10-4 将矩阵乘法表示为@，即 NumPy 的矩阵乘法操作符。激活函数是**σ**。

对于反向传递，导数的序列从∂***y***/∂***y*** = 1 开始，并从操作符输出回流到输入。如果有多个输入，则有多个输出导数。在实践中，图的评估引擎会按正确的顺序处理正确的操作符集合。当该操作符的轮次到来时，它所需的输入导数会自动可用。

图 10-4 使用∂表示操作符所生成的导数。例如，加法操作符（∂+）生成两个输出，因为有两个输入，***Wx***和***b***。矩阵乘法（∂@）也遵循相同的原则。激活函数的导数表示为**σ**′。

注意，箭头从***W***和***x***在前向传递中指向矩阵乘法操作符的导数，在反向传递中也是如此。***W***和***x***都必须用来计算∂***y***/∂***W***和∂***y***/∂***x***—分别见方程 10.13 和方程 10.11。没有箭头从***b***指向矩阵乘法操作符，因为∂***y***/∂***b***不依赖于***b***—见方程 10.12。如果有一层位于图 10-4 所示层的下方，那么矩阵乘法操作符的∂***y***/∂***x***输出将成为该层反向传递的输入，依此类推。

计算图的强大功能使得现代深度学习工具包具有高度的通用性，并且支持几乎所有类型的网络和架构，而无需用户处理繁琐且详细的梯度计算。随着你继续深入探索深度学习，务必欣赏工具包在仅用几行代码的情况下所能实现的可能性。

### 总结

本章介绍了反向传播，这是使深度学习具有实用性的两大组成部分之一。首先，我们手动计算了一个小型网络所需的导数，并看到这一过程是多么繁琐。然而，我们成功地训练了这个小型网络。

接下来，我们运用第八章中的矩阵微积分知识，找到了多层全连接网络的方程，并创建了一个类似于 Keras 等工具包的简单工具包。在这个工具包的帮助下，我们成功地使用 MNIST 数据集训练了一个高精度的模型。虽然该工具包在隐藏层数量和大小方面具有高度的通用性，但它仅限于全连接模型。

我们以简单的方式结束了本章，简要介绍了现代深度学习工具包，如 TensorFlow，如何实现模型并自动化反向传播。计算图使得基本操作的任意组合成为可能，每个操作都可以根据需要向后传递梯度，从而实现我们在深度学习中看到的复杂模型架构。

训练深度模型的后半部分是梯度下降，它将通过反向传播计算得到的梯度付诸实践。现在让我们把注意力转向这一部分。
