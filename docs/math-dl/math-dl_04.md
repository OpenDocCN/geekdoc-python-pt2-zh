## **4

统计学**

![image](img/common.jpg)

不良的数据集会导致糟糕的模型。在构建模型之前，我们希望能够理解我们的数据，然后利用这种理解来创建一个有用的数据集，一个能够生成符合我们预期模型的数据集。了解基本的统计学将使我们能够做到这一点。

一个*统计量*是从样本中计算得出的任何数字，用来以某种方式描述样本。在深度学习中，当我们谈论样本时，我们通常是在谈论数据集。也许最基本的统计量是算术平均数，通常称为平均值。数据集的平均值是对数据集的单一数字总结。

在本章中，我们将看到许多不同的统计学内容。我们将从学习数据类型和通过摘要统计量来描述数据集开始。接下来，我们将学习分位数和绘制数据图表，以便了解数据包含的内容。之后，我们将讨论离群值和缺失数据。数据集很少是完美的，所以我们需要某种方法来检测不良数据并处理缺失数据。我们将在讨论完不完美的数据集后，讨论变量之间的相关性。最后，我们将通过讨论假设检验来结束本章，假设检验帮助我们回答诸如“同一父进程生成两个数据集的可能性有多大？”这样的问题。假设检验在科学中广泛应用，包括深度学习。

### 数据类型

四种数据类型分别是名义数据、序数数据、区间数据和比率数据。我们将逐一查看每种数据类型。

#### 名义数据

*名义数据*，有时称为*类别数据*，是指没有不同值之间顺序的数据。例如，眼睛颜色就是一个例子；棕色、蓝色和绿色之间没有任何关系。

#### 序数数据

对于*序数数据*，数据具有排名或顺序，尽管在数学意义上，差异并不具有实际意义。例如，如果问卷要求你选择“强烈不同意”、“不同意”、“中立”、“同意”和“强烈同意”，很明显这是一种顺序。然而，也很明显，“同意”并不比“强烈不同意”多三倍。我们能说的仅仅是“强烈不同意”位于“同意”的左侧（以及“中立”和“不同意”）。

另一个序数数据的例子是教育水平。如果一个人有四年级的教育水平，而另一个人有八年级的教育水平，我们可以说后者的教育水平更高，但不能说后者的教育水平是前者的两倍，因为“教育水平是两倍”没有固定的意义。

#### 区间数据

*区间数据*具有有意义的差异。例如，如果一杯水的温度是 40 华氏度，另一杯是 80 华氏度，我们可以说两杯水之间有 40 度的差异。然而，我们不能说第二杯水的热量是第一杯的两倍，因为华氏度标尺的零点是任意设定的。口语上，我们确实会说第二杯水热得是第一杯的两倍，但实际上并非如此。要看清这一点，可以想象如果我们将温度标尺改成另一个更合理的标尺——摄氏度标尺。我们会看到第一杯水的温度大约是 4.4 摄氏度，第二杯是 26.7 摄氏度。显然，第二杯水并不会突然变成第一杯的六倍热。

#### 比率数据

最后，*比率数据*是指差异有意义且存在真实零点的数据。身高就是比率值，因为零身高就意味着完全没有身高。同样，年龄也是比率值，因为零年龄意味着没有年龄。如果我们采用一种新的年龄尺度，并将某人定义为零年龄，比如当他们达到投票年龄时，那么我们就得到一个区间尺度，而非比率尺度。

让我们再次看看温度。我们在上面说过，温度是区间量，这并不总是正确的。如果我们用华氏度或摄氏度来测量温度，那么它确实是区间量。然而，如果我们使用绝对温标——开尔文度来测量温度，那么它就变成了比率值。为什么？因为 0 开尔文（或*K*）表示的就是完全没有温度。如果我们的第一杯水温是 40°F（277.59 K），第二杯是 80°F（299.82 K），那么我们可以真实地说第二杯水比第一杯热 1.08 倍，因为（277.59）（1.08）≈ 299.8。

图 4-1 列出了数据量表并展示了它们之间的关系。

![image](img/04fig01.jpg)

*图 4-1：四种数据类型*

图 4-1 从左到右的每一步都为数据添加了缺失的部分。对于从名义数据到有序数据，我们增加了顺序性。对于从有序数据到区间数据，我们增加了有意义的差异。最后，从区间数据到比率数据则增加了一个真实的零点。

在实际应用中，统计学方面我们应该了解数据的类型，以免做出没有意义的操作。如果我们有一个问卷，问题 A 的平均值在 1 到 5 的评分尺度上是 2，而问题 B 的平均值是 4，我们不能说 B 的评分是 A 的两倍，只能说 B 的评分高于 A。在这种情况下，“两倍”是什么意思并不明确，而且很可能是没有意义的。

区间数据和比率数据可以是连续的（浮动点数）或离散的（整数）。从深度学习的角度来看，模型通常将连续数据和离散数据视为相同，不需要对离散数据做特别处理。

#### 使用名义数据进行深度学习

如果我们有一个名义值，例如一组颜色，如红色、绿色和蓝色，我们想将该值传递给深度网络，那么在使用之前，我们需要先修改数据。正如我们刚才看到的，名义数据没有顺序性，因此虽然将红色赋值为 1，绿色为 2，蓝色为 3 看起来很有诱惑力，但这样做是错误的，因为网络会将这些数字解释为区间数据。在这种情况下，对网络来说，蓝色 = 3（红色），这显然是没有意义的。如果我们想要将名义数据用于深度网络，就需要改变它，使得间隔是有意义的。我们通过 *独热编码* 来做到这一点。

在独热编码中，我们将单一的名义变量转换为一个向量，其中向量的每个元素对应名义值之一。以颜色为例，单一名义变量变成一个三元素的向量，表示红色的元素为一个，绿色为另一个，最后一个表示蓝色。然后，我们将对应颜色的值设置为 1，其余的设置为 0，如下所示：

| **值** |  | **向量** |
| --- | --- | --- |
| 红色 | → | 1 0 0 |
| 绿色 | → | 0 1 0 |
| 蓝色 | → | 0 0 1 |

现在，向量的值是有意义的，因为它要么是红色（1），要么不是（0）；要么是绿色（1），要么不是（0）；要么是蓝色（1），要么不是（0）。零和一之间的间隔具有数学意义，因为例如红色的存在（1）确实大于其不存在（0），对每种颜色都是如此。现在这些值是区间数据，因此网络可以使用它们。在一些工具包中，比如 Keras，类别标签在传递给模型之前会进行独热编码。这样做是为了让向量输出在计算损失函数时能与独热编码的类别标签良好配合。

### 汇总统计

我们得到了一个数据集。我们如何理解它？在用它构建模型之前，应该如何描述它，以便更好地理解它？

为了回答这些问题，我们需要了解 *汇总统计*。计算汇总统计是当你得到一个新数据集时应该做的第一件事。在构建模型之前不查看数据集，就像买了一辆二手车却没有检查轮胎、没有试驾，也不看发动机一样。

每个人对什么是好的汇总统计有不同的看法。我们将重点关注以下内容：均值；中位数；以及变化度量，包括方差、标准差和标准误差。范围和众数也常被提到。*范围* 是数据集中的最大值与最小值之间的差值。*众数* 是数据集中出现频率最高的值。我们通常可以通过直方图从视觉上了解众数，因为直方图展示了数据的分布形状。

#### 均值和中位数

我们大多数人在小学时就学会了如何计算一组数字的平均值：将数字相加，再除以数字的个数。这就是 *算术均值*，更具体地说，是 *无权重* 算术均值。如果数据集由一组值组成，*{x[0]*, *x*[1], *x*[2], . . . , *x[n–1]*}，那么算术均值是数据的总和除以数据集中元素的个数 (*n*)。符号表示为：

![image](img/04equ01.jpg)

![image](img/xbar.jpg) 是表示样本均值的典型方式。

方程式 4.1 计算的是无权重的均值。每个值都被赋予一个权重 1/*n*，其中所有权重的总和为 1.0。有时，我们可能希望对数据集的元素赋予不同的权重；换句话说，并非所有元素都应当平等计数。在这种情况下，我们计算加权均值，

![image](img/071equ01.jpg)

其中 *w[i]* 是赋给 *x[i]* 的权重，Σ*[i]w[i]* = 1。权重不是数据集的一部分；它们需要来自其他地方。许多大学使用的平均成绩点（GPA）就是一个加权均值的例子。每门课程的成绩乘以该课程的学分数，然后将所有结果相加，再除以总学分数。在代数上，这等同于将每个成绩乘以一个权重，*w[i]* = *c[i]* / Σ*[i]**c[i]*，其中 *c[i]* 是课程 *i* 的学分数，Σ*[i]**c[i]* 是该学期的总学分数。

##### 几何均值

算术均值是最常用的均值。然而，还有其他类型的均值。两个正数 *a* 和 *b* 的 *几何均值* 是它们乘积的平方根：

![image](img/071equ02.jpg)

通常，*n* 个正数的几何均值是它们乘积的 *n* 次方根：

![image](img/071equ03.jpg)

几何均值在金融学中用于计算平均增长率。在图像处理中，几何均值可以用作滤波器，帮助减少图像噪声。在深度学习中，几何均值出现在 *Matthews 相关系数（MCC）* 中，这是我们用来评估深度学习模型的一个指标。MCC 是两个其他指标的几何均值，即信息度和标记度。

##### 调和均值

两个数 *a* 和 *b* 的 *调和均值* 是它们倒数算术均值的倒数：

![image](img/071equ04.jpg)

通常，

![image](img/072equ01.jpg)

调和均值出现在深度学习中作为 F1 分数。这是评估分类器时常用的一个指标。F1 分数是召回率（灵敏度）和精确度的调和均值：

![image](img/072equ02.jpg)

尽管 F1 分数被频繁使用，但不建议使用它来评估深度学习模型。要理解这一点，考虑召回率和精确度的定义：

![image](img/072equ03.jpg)

这里，TP 是正确分类为正样本的数量，FN 是错误分类为负样本的数量，FP 是错误分类为正样本的数量。这些值来自用于评估模型的测试集。对于分类器来说，另一个重要的数字是 TN，它是正确分类为负样本的数量（假设是二分类器）。F1 得分忽略了 TN，但为了理解模型的表现，我们需要同时考虑正负分类。因此，F1 得分可能具有误导性，通常过于乐观。更好的度量标准是上面提到的 MCC 或 Cohen’s κ（卡帕系数），它与 MCC 类似，通常会紧密跟踪 MCC 的变化。

##### 中位数

在讨论变异度量之前，我们需要提到另一个常用的汇总统计量，它将在本章稍后再次出现。数据集的*中位数*是中间值。当数据集按数字顺序排序时，中位数是值位于其中一半数据点的上方和另一半数据点的下方的位置。我们使用这个数据集：

*X* = {55,63,65,37,74,71,73,87,69,44}

如果我们对*X*进行排序，得到：

{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}

我们立刻看到一个潜在的问题。我提到过，当数据排序时我们需要找到中间值。对于 10 个数据点的*X*，并没有一个明确的中间值。中间值位于 65 和 69 之间。当数据集中的元素数量为偶数时，中位数是两个中间数字的算术平均值。因此，在这种情况下，中位数是

![image](img/073equ01.jpg)

数据的算术均值是 63.8。均值和中位数之间有什么区别？

从设计上讲，中位数告诉我们将数据集分开的值，因此，上方的样本数量与下方的样本数量相等。这里关注的是样本数量。对于均值来说，它是所有实际数据值的总和。因此，均值对值本身非常敏感，而中位数则对值的排序更为敏感。

如果我们观察*X*，会发现大部分值集中在 60 到 70 之间，只有一个较低的值为 37。正是这个低值 37 将均值拉低，造成均值低于中位数。收入就是这个效应的一个典型例子。美国当前的年家庭收入中位数大约为 62,000 美元。最近测量的美国家庭收入的均值更接近 72,000 美元。这个差异源于一小部分收入远高于其他人的群体。他们将整体均值拉高。因此，对于收入而言，最有意义的统计量是中位数。

请参考图 4-2。

![image](img/04fig02.jpg)

*图 4-2：均值（实线）和中位数（虚线）绘制在样本数据集的直方图上*

图 4-2 显示了从 1,000 个模拟数据样本中生成的直方图。图中还绘制了均值（实线）和中位数（虚线）。这两者并不相同；直方图中的长尾将均值拉高。如果我们统计，500 个样本会落在虚线以下的区间，而 500 个样本会落在虚线以上的区间。

平均值和中位数有可能相同吗？是的。如果数据分布完全对称，那么均值和中位数将相同。这种情况的经典例子是正态分布。图 3-4 显示了一个正态分布，其中左右对称性非常明显。正态分布是特殊的。我们将在本章中多次看到它。目前，请记住，数据集的分布越接近正态分布，均值和中位数就越接近。

相反的情况也值得记住：如果数据集的分布远离正态分布，比如图 4-2 所示，那么中位数在总结数据时可能是更好的统计量。

#### 变化度量

一位初学者射手向靶子射了 10 支箭。八支箭命中靶心，两支箭完全偏离，命中的八支箭均匀地分布在靶子上。一个专家射手向靶子射了 10 支箭。专家的所有箭都命中在离靶心几厘米的范围内。想想箭的平均位置。对于专家来说，所有的箭都靠近靶心，所以我们可以看出箭的均值位置会接近靶心。对于初学者来说，虽然没有箭命中靶心，但它们大致均匀地分布在靶心的左侧和右侧或上下。因此，平均位置会平衡并接近靶心。

然而，第一位射手的箭散落不齐；箭的位置变化很大。另一方面，第二位射手的箭紧密集中，位置变化很小。总结和理解数据集的一种有意义方式是量化其变化。让我们看看如何做到这一点。

##### 偏差与方差

衡量数据集变化的一种方式是找出*范围*，即最大值与最小值之间的差异。然而，范围是一个粗略的度量，它只关注数据集中的极端值，而忽略了大部分值。我们可以通过计算数据值与数据均值之间差异的均值来做得更好。其公式为：

![image](img/04equ02.jpg)

方程 4.2 是*均差*。它是一种自然的度量方法，正好符合我们的需求：提供一个关于每个样本平均偏离均值多少的概念。虽然计算均差没有问题，但你会发现它在实际中很少使用。一个原因与代数和微积分有关。绝对值在数学处理中是个麻烦。

与其使用自然的变异度量方法，不如用平方差来计算：

![image](img/04equ03.jpg)

方程 4.3 被称为*有偏样本方差*。它是数据集中每个值与均值之间平方差的均值。这是另一种描述数据集散布的方法。为什么它有偏，我们稍后会讨论。我们将很快讨论为什么它是![image](img/ssub-bar.jpg)而不是*s[n]*。

在此之前，值得注意的是，你会经常看到一个略有不同的方程：

![image](img/04equ04.jpg)

这个方程是*无偏样本方差*。用*n* - 1 代替*n* 被称为贝塞尔修正。它与残差的自由度数量有关，残差是指从数据集中的每个值减去均值后所剩下的部分。残差的总和为零，因此，如果数据集中有*n*个值，知道*n* - 1 个残差就能计算出最后一个残差。这给我们提供了残差的自由度。我们可以“自由”地计算*n* - 1 个残差，因为我们知道通过残差的总和为零可以得出最后一个残差。用*n* - 1 来除可以得到一个较少偏差的方差估计，假设![image](img/ssub-bar.jpg)一开始有某种偏差。

为什么我们要讨论偏差方差和无偏方差呢？偏差在哪里？我们应该始终记住，数据集是来自某个母体数据生成过程的一个样本，即母体。真正的母体方差(σ²)是母体数据围绕真实母体均值(μ)的散布。然而，我们并不知道μ或σ²，因此我们只能从已有的数据集中估算它们。样本的均值是![image](img/xbar.jpg)，它是我们对μ的估计。然后，计算围绕![image](img/xbar.jpg)的平方偏差的均值，并将其作为我们对σ²的估计。这就是![image](img/ssub-bar.jpg)（方程 4.3）。这个结论虽然是真的，但超出了我们演示的范围：![image](img/ssub-bar.jpg)是有偏的，并不是σ²的最佳估计，但如果应用贝塞尔修正，我们将得到更好的母体方差估计。所以我们应该使用*s*²（方程 4.4）来描述数据集围绕均值的方差。

总结一下，我们应该使用 ![image](img/xbar.jpg) 和 *s*² 来量化数据集的方差。那么，为什么是 *s*² 呢？方差的平方根就是*标准差*，在总体中用 σ 表示，而在从数据集计算的 σ 估计值中用 *s* 表示。通常，我们更希望使用标准差。写平方根变得很麻烦，因此约定采用 σ 或 *s* 表示标准差，而在讨论方差时使用平方形式。

而且，由于生活本来就足够模糊，您经常会看到 σ 被用来表示 *s*，并且 方程 4.3 被用作实际应该使用 方程 4.4 的情况。一些工具包，包括我们钟爱的 NumPy，使得使用错误公式变得轻而易举。

然而，随着数据集中样本数量的增加，偏差方差和无偏方差之间的差异逐渐减小，因为无论是除以 *n* 还是 *n* – 1，其影响越来越小。以下几行代码说明了这一点：

>>> import numpy as np

>>> n = 10

>>> a = np.random.random(n)

>>> (1/n)*((a-a.mean())**2).sum()

0.08081748204006689

>>> (1/(n-1))*((a-a.mean())**2).sum()

0.08979720226674098

这里，只有 10 个值的样本（a）显示了偏差方差和无偏方差之间在第三位小数上的差异。如果我们将数据集的大小从 10 增加到 10,000，则得到

>>> n = 10000

>>> a = np.random.random(n)

>>> (1/n)*((a-a.mean())**2).sum()

0.08304350577482553

>>> (1/(n-1))*((a-a.mean())**2).sum()

0.08305181095592111

偏差估计和无偏估计之间的方差差异现在出现在第五位小数。因此，对于我们在深度学习中通常使用的大型数据集，实际上无论使用 *s[n]* 还是 *s* 来表示标准差几乎没有区别。

**中位数绝对偏差**

标准差是基于均值的。如上所示，均值对极端值敏感，而标准差则更加敏感，因为我们对每个样本的偏差进行平方。一个对数据集中的极端值不敏感的变异性度量是 *中位数绝对偏差 (MAD)*。MAD 定义为数据与中位数之差的绝对值的中位数：

MAD = 中位数(|*X[i]* – 中位数(*X*)|)

操作步骤是，首先计算数据的中位数，然后从每个数据值中减去它，使结果变为正数，最后报告这个集合的中位数。实现起来很简单：

def MAD(x):

return np.median(np.abs(x-np.median(x)))

MAD（中位数绝对偏差）虽然不常用，但它对数据集中极端值的不敏感性使得它在检测异常值时具有较高的实用性，因此应更多使用。

##### 标准误差与标准差

还有一个我们需要讨论的方差度量：*均值的标准误差（SEM）*。标准误差通常简称为*标准误差（SE）*。我们需要回到总体中去理解标准误差是什么以及何时使用它。如果我们从总体中选取一个样本数据集，我们可以计算该样本的均值，![image](img/xbar.jpg)。如果我们选择多个重复的样本并计算这些样本的均值，我们就会得到一个来自总体的样本均值数据集。这可能听起来很熟悉；它是我们用来说明中心极限定理的过程，见第三章。这些均值的标准差就是标准误差。

从标准差得到标准误差的公式是直接的，

![image](img/077equ01.jpg)

它只是将样本标准差按样本数量的平方根进行缩放。

我们什么时候使用标准差，什么时候使用标准误差？使用标准差来了解样本围绕均值的分布情况。使用标准误差来描述样本均值对总体均值估计的准确性。从某种意义上说，标准误差与中心极限定理有关，因为它影响从母体总体中提取多个样本均值的标准差，以及大数法则，因为较大的数据集更有可能给出总体均值的更好估计。

从深度学习的角度来看，我们可能会使用标准差来描述用于训练模型的数据集。如果我们训练并测试多个模型，考虑到深度网络初始化的随机性，我们可以计算多个模型的某个指标的均值，比如准确度。在这种情况下，我们可能会报告均值准确度加减标准误差。当我们训练更多的模型，并且逐渐确信均值准确度代表了模型架构能够提供的准确度时，我们应该预期模型的均值准确度误差会减少。

回顾一下，在这一部分中，我们讨论了不同的总结性统计量，这些是我们用来开始理解数据集的值。这些统计量包括各种均值（算术均值、几何均值和调和均值）、中位数、标准差，以及在适当的情况下，标准误差。现在，让我们看看如何利用图表来帮助理解数据集。

### 分位数与箱形图

要计算中位数，我们需要找到中间值，即将数据集分成两半的数值。从数学上讲，我们说中位数将数据集分成了两个分位数。

一个*四分位数*将数据集划分为固定大小的组，其中固定大小是四分位数中的数据值数量。由于中位数将数据集分为两个大小相等的组，因此它是一个*2-分位数*。有时你会看到中位数被称为*50 百分位数*，意味着 50%的数据值小于该值。类似地，95 百分位数是数据集中 95%的值小于它。研究人员通常计算 4-分位数，并称它们为*四分位数*，因为它们将数据集分成四个组，使得 25%的数据值位于第一个四分位数，50%的数据值位于第一个和第二个四分位数，75%的数据值位于第一个、第二个和第三个四分位数，最后 25%的数据值位于第四个四分位数。

让我们通过一个例子来理解四分位数的含义。该例子使用了一个合成的考试数据集，表示 1,000 个测试成绩。请参见文件*exams.npy*。我们将使用 NumPy 来计算四分位数值，并绘制数据集的直方图，并标记四分位数值。首先，让我们计算四分位数的位置：

d = np.load("exams.npy")

p = d[:,0].astype("uint32")

q = np.quantile(p, [0.0, 0.25, 0.5, 0.75, 1.0])

print("四分位数: ", q)

print("按四分位数计数:")

print("    %d" % ((q[0] <= p) & (p < q[1])).sum())

print("    %d" % ((q[1] <= p) & (p < q[2])).sum())

print("    %d" % ((q[2] <= p) & (p < q[3])).sum())

print("    %d" % ((q[3] <= p) & (p < q[4])).sum())

这段代码以及生成图表的代码在文件*quantiles.py*中。

首先，我们加载合成考试数据并保留第一次考试的成绩（p）。请注意，我们将 p 设为整数数组，以便稍后可以使用 np.bincount 来生成直方图。（上面没有展示那段代码。）然后，我们使用 NumPy 的 np.quantile 函数计算四分位数值。该函数接受源数组和一个量化值数组，范围在[0, 1]之间。这些值是数组的最小值到最大值之间的距离的分数。因此，要求 0.5 四分位数即是要求值，它是 p 的最小值和最大值之间距离的一半，使得每个组中的值数量相等。

为了获得四分位数，我们要求 0.25、0.5 和 0.75 四分位数，以获得使得 25%、50%和 75%的 p 元素小于该值的四分位数值。我们还要求 0.0 和 1.0 四分位数，即 p 的最小值和最大值。我们这样做是为了方便计算每个范围内的元素数量。请注意，我们本可以使用 np.percentile 函数来代替。它返回与 np.quantile 相同的值，但使用百分比值而不是分数。在这种情况下，第二个参数将是[0,25,50,75,100]。

返回的四分位数值存储在 q 中。我们打印它们以获得

18.0, 56.75, 68.0, 78.0, 100.0

这里，18 是最小值，100 是最大值，三个四分位数的切分值分别是 56.75、68 和 78。请注意，第二个四分位数的切分值是中位数，68。

剩下的代码用于统计 p 中每个范围内的值的数量。对于 1,000 个值，我们预计每个范围内会有 250 个值，但由于数学运算并不总是精确对齐到现有的数据值，因此我们实际上得到的是

250, 237, 253, 248

这意味着，p 中有 250 个元素小于 56.75，237 个在 [56.75, 68] 之间，以此类推。

上面的代码使用了一个巧妙的计数技巧，值得解释一下。我们想要统计 p 中某个范围内的数值个数。我们不能使用 NumPy 的 np.where 函数，因为它不支持复合条件语句。然而，如果我们使用像 10 <= p 这样的表达式，我们将得到一个与 p 同样大小的数组，其中每个元素如果条件为真则为 True，否则为 False。因此，要求 10 <= p 和 p < 90 会返回两个布尔数组。为了获取同时满足两个条件的元素，我们需要对它们进行逻辑与运算（&）。这样我们就得到了一个与 p 大小和形状相同的数组，其中所有为 True 的元素表示 p 中在 10, 90) 范围内的值。为了获取计数，我们使用 sum 方法，这个方法对布尔数组处理时，将 True 视为 1，False 视为 0。

[图 4-3 展示了标注四分位数的考试数据直方图。

![image](img/04fig03.jpg)

*图 4-3：标注四分位数的 1,000 个考试成绩直方图*

上面的例子再次展示了直方图在可视化和理解数据方面的巨大作用。我们应该尽可能使用直方图，帮助我们理解数据集的情况。图 4-3 将四分位数值叠加在直方图上。这有助于我们理解四分位数及其与数据值的关系，但这并不是一种典型的展示方式。更典型且实用的方式是 *箱型图*，因为它能显示数据集的多个特征。现在我们就使用箱型图来展示上面的考试成绩，并且这次我们还会包括之前忽略的另外两个考试成绩数据集。

我们将首先展示一个箱型图，然后再进行解释。要查看 *exams.npy* 文件中三个考试的箱型图，请使用

d = np.load("exams.npy")

plt.boxplot(d)

plt.xlabel("测试")

plt.ylabel("成绩")

plt.show()

我们正在加载完整的考试成绩数据集，并使用 Matplotlib 的 boxplot 函数。

请看一下输出结果，如图 4-4 所示。

![image](img/04fig04.jpg)

*图 4-4：三个考试的箱型图（顶部），以及第一个考试的箱型图和标注组件（底部）*

图 4-4 中的顶部图表展示了 *exams.npy* 文件中三个考试成绩的箱型图。这些数据的第一个箱型图再次绘制在 图 4-4 的底部，并附有描述箱型图各部分的标签。

箱线图为我们提供了数据的可视化总结。图 4-4 中底部图表的箱体展示了第一四分位数（Q1）和第三四分位数（Q3）之间的范围。Q3 和 Q1 之间的数值差异被称为*四分位距（IQR）*。IQR 越大，数据围绕中位数的分布就越分散。注意，这次的分数在 y 轴上。我们本可以轻松地将图表做成水平的，但垂直是默认设置。中位数（Q2）标记在箱体的中间。箱线图中并不显示均值。

箱线图还包括两条附加的线，即*须弯*，不过 Matplotlib 称它们为*离群值线*。如图所示，须弯位于 Q3 以上或 Q1 以下 1.5 倍的 IQR 处。最后，还有一些被标记为“可能的异常值”的圆点。根据约定，位于须弯之外的值被视为*可能的异常值*，这意味着它们可能代表错误的数据，这些数据可能是手动输入错误的，或者更常见的是来自故障传感器的数据。例如，CCD 相机上的亮点或坏点可能被视为异常值。在评估潜在的数据集时，我们应该对异常值保持敏感，并运用我们的最佳判断来决定如何处理它们。通常，异常值只有几个，我们可以在不造成伤害的情况下将它们从数据集中剔除。然而，也有可能这些异常值实际上是真实的，且可能高度指示某一特定类别。如果是这种情况，我们希望将它们保留在数据集中，以期望模型能够有效地利用它们。经验、直觉和常识必须在这里引导我们。

让我们解释一下图 4-4 中显示的三个考试成绩的箱线图。每次须弯的顶部都在 100 处，这很有道理：100 是满分，数据集中确实有 100 分的成绩。注意，图中的箱体部分并没有在须弯中垂直居中。回想一下，50% 的数据值位于 Q1 和 Q3 之间，箱体内 Q2 上下各有 25% 的数据，因此我们可以看到数据并不严格符合正态分布；其分布偏离了正态曲线。回头看看图 4-3 中的直方图，我们可以确认第一场考试的数据分布不符合正态性。同样，我们也可以看到第二场和第三场考试的数据分布也偏离正态性。因此，箱线图可以告诉我们数据集的分布与正态分布的相似程度。当我们下面讨论假设检验时，我们需要知道数据是否符合正态分布。

那么，低于 Q1 – 1.5 × IQR 的可能异常值呢？我们知道数据集代表的是考试成绩，因此常识告诉我们，这些并不是异常值，而是特别困惑（或懒惰）的学生所做出的有效成绩。如果数据集包含了超过 100 或低于零的值，那些就可以被认为是异常值。

有时丢弃包含异常值的样本是正确的做法。然而，如果异常值是由缺失数据引起的，删除样本可能不是一个合适的选择。让我们看看如何处理缺失数据，以及为什么我们通常应该尽量避免它。

### 缺失数据

缺失数据就是没有的数据。如果数据集由代表特征向量的样本组成，缺失数据通常表现为某个样本中的一个或多个特征未被测量。缺失数据通常会以某种方式进行编码。如果值是正数，缺失的特征可能用–1 或历史上用–999 来标记。如果特征是以字符串形式给出的，字符串可能为空。对于浮点值，可能使用“不是一个数字”（NaN）来表示。NumPy 提供了简便的方法来检查数组中的 NaN 值，使用 np.isnan：

>>> a = np.arange(10, dtype="float64")

>>> a[3] = np.nan

>>> np.isnan(a[3])

True

>>> a[3] == np.nan

False

>>> a[3] is np.nan

False

注意，直接用==或 is 与 np.nan 进行比较是行不通的；只有使用 np.isnan 来测试才有效。

检测缺失数据是数据集特定的。假设我们已经确认有缺失数据，接下来该如何处理呢？

让我们生成一个包含缺失值的小数据集，并利用现有的统计学知识来看如何处理它们。以下代码在 missing.py 中。首先，我们生成一个包含 1,000 个样本的数据集，每个样本有四个特征：

N = 1000

np.random.seed(73939133)

x = np.zeros((N,4))

x[:,0] = 5*np.random.random(N)

x[:,1] = np.random.normal(10,1,size=N)

x[:,2] = 3*np.random.beta(5,2,N)

x[:,3] = 0.3*np.random.lognormal(size=N)

数据集在 x 中。我们设置随机数种子以获得可重复的结果。第一个特征是均匀分布的，第二个特征是正态分布的，第三个特征遵循 Beta 分布，第四个特征遵循对数正态分布。

此时，x 没有缺失值。我们通过将随机元素设为 NaN 来添加一些缺失值：

i = np.random.randint(0,N, size=int(0.05*N))

x[i,0] = np.nan

i = np.random.randint(0,N, size=int(0.05*N))

x[i,1] = np.nan

i = np.random.randint(0,N, size=int(0.05*N))

x[i,2] = np.nan

i = np.random.randint(0,N, size=int(0.05*N))

x[i,3] = np.nan

现在数据集的 5%值为 NaN。

如果一个大数据集中的少量样本缺少数据，我们可以放心地将其从数据集中移除。然而，如果 5%的样本缺失数据，我们可能不希望丢失这么多数据。更让人担心的是，如果缺失数据与某个特定类别相关呢？丢弃这些样本可能会导致数据集出现偏差，从而使得模型的效果变差。

那么，我们该怎么做呢？我们刚刚花了很多页学习如何通过基本的描述性统计来概括一个数据集。我们可以使用这些方法吗？当然可以。我们可以查看特征的分布，忽略缺失值，然后用这些分布来决定如何替换缺失的数据。简单来说，我们可能会使用现有数据的均值，但查看分布可能会让我们更倾向于使用中位数，具体取决于分布是否偏离正态分布。这听起来像是箱型图的工作。幸运的是，Matplotlib 的 boxplot 函数很聪明，它会忽略 NaN 值。因此，绘制箱型图只是一个简单的 boxplot(x) 调用。

图 4-5 展示了忽略 NaN 后的数据集。

![image](img/04fig05.jpg)

*图 4-5：忽略缺失值的数据集的箱型图*

图 4-5 中的箱体图对于特征的分布是有意义的。特征 1 是均匀分布的，所以我们期望围绕均值/中位数有一个对称的箱体。（对于均匀分布，均值和中位数是相同的。）特征 2 是正态分布的，因此我们得到的箱体结构与特征 1 类似，但由于只有 1,000 个样本，某些不对称现象是显而易见的。特征 3 的贝塔分布偏向其范围的上端，这在箱型图中得以体现。最后，特征 4 的对数正态分布应该偏向较低的值，且上方有许多“离群值”出现在须状线以上，长尾显现出来，这为我们提供了一个反面教材，提醒我们不要盲目地将这些值称为离群值。

由于我们有些特征的分布高度偏离正态分布，我们将用中位数而不是均值来更新缺失值。代码非常简单：

good_idx = np.where(np.isnan(x[:,0]) == False)

m = np.median(x[good_idx,0])

bad_idx = np.where(np.isnan(x[:,0]) == True)

x[bad_idx,0] = m

在这里，i 首先保存了特征 1 中不是 NaN 的索引。我们用这些索引来计算中位数（m）。接着，我们将 i 设置为 NaN 的索引，并用中位数替换它们。我们可以对其他特征做同样的操作，更新整个数据集，这样就不再有缺失值了。

我们是否对早期的分布造成了较大变化？没有，因为我们只更新了 5% 的值。例如，对于特征 3，根据贝塔分布，均值和标准差的变化如下：

非 NaN 均值，标准差 = 2.169986, 0.474514

更新后的均值，标准差 = 2.173269, 0.462957

这个故事的寓意是，如果缺失数据足够多，以至于删除它可能会让数据集产生偏差，最安全的做法是用均值或中位数来替换缺失数据。要决定使用均值还是中位数，可以参考描述性统计、箱型图或直方图。

此外，如果数据集是有标签的，如同深度学习数据集那样，上述过程需要使用按类别分组的样本的均值或中位数完成。否则，计算出的值可能不适用于该类别。

去除缺失数据后，可以在数据集上训练深度学习模型。

### 相关性

有时候，数据集中的特征之间存在某种关联。如果一个特征增大，另一个特征也可能增大，但不一定是简单的线性关系。或者，另一个特征可能会减小——这是一种负相关。描述这种关联的恰当词是*相关性*。衡量相关性的统计量是理解数据集中各特征之间关系的一个便捷方式。

例如，很容易看出大多数图像的像素是高度相关的。这意味着如果我们随机选择一个像素，然后选择一个相邻像素，第二个像素很有可能与第一个像素相似。图像中如果没有这种相关性，看起来就像是随机噪声。

在传统的机器学习中，高度相关的特征是不受欢迎的，因为它们并未提供新的信息，反而容易混淆模型。特征选择的整个艺术部分就是为了解决这一问题。对于现代深度学习来说，网络本身会学习输入数据的新表示，因此输入不相关并不像传统机器学习那样关键。这也部分解释了为什么图像作为输入在深度网络中表现良好，而在旧的机器学习模型中通常会失败。

无论学习是传统的还是现代的，作为总结和探索数据集的一部分，检查和理解特征之间的相关性是值得的。在本节中，我们将讨论两种类型的相关性。每种类型都会返回一个数字，用来衡量数据集中两个特征之间的相关性强度。

#### Pearson 相关性

*Pearson 相关系数*返回一个数字，*r* ϵ [–1, +1]，表示两个特征之间*线性*相关性的强度。这里的*线性*是指我们能通过一条直线多强地描述两个特征之间的相关性。如果相关性使得一个特征的增大恰好伴随着另一个特征的增大，那么相关系数为+1。如果第二个特征恰好随着第一个特征的增大而减小，则相关系数为–1。零相关意味着两个特征之间没有关联，它们是（可能）独立的。

我在上面的句子中加入了*可能*这个词，因为在某些情况下，两个特征之间的非线性依赖关系可能会导致 Pearson 相关系数为零。然而，这种情况并不常见，对于我们的目的来说，我们可以认为相关系数接近零意味着这两个特征是独立的。相关系数越接近零，无论是正数还是负数，特征之间的相关性就越弱。

Pearson 相关系数是通过两个特征的均值或这两个特征的乘积均值定义的。输入是两个特征，即数据集中的两列。我们将这些输入称为*X*和*Y*，其中大写字母表示数据值的向量。注意，由于这两个特征来自数据集，因此*X[i]*与*Y[i]*是配对的，意味着它们来自同一特征向量。

Pearson 相关系数的公式是：

![image](img/04equ05.jpg)

我们引入了一种新的但常用的符号。*X*的均值是*X*的*期望*，记作 E(*X*)。因此，在公式 4.5 中，我们看到了*X*的均值 E(*X*)和*Y*的均值 E(*Y*)。正如我们可能猜到的，E(*XY*)是*X*与*Y*的乘积的均值，按元素计算。类似地，E(*X*²)是*X*与其自身乘积的均值，E(*X*)²则是*X*的均值的平方。有了这种符号，我们就可以轻松编写自己的函数来计算两个特征向量的 Pearson 相关系数：

import numpy as np

def pearson(x,y):

exy = (x*y).mean()

ex = x.mean()

ey = y.mean()

exx = (x*x).mean()

ex2 = x.mean()**2

eyy = (y*y).mean()

ey2 = y.mean()**2

return (exy - ex*ey)/(np.sqrt(exx-ex2)*np.sqrt(eyy-ey2))

pearson 函数直接实现了公式 4.5。

让我们设定一个场景，在其中使用 pearson 并与 NumPy 和 SciPy 提供的结果进行比较。接下来的代码，包括上面定义的 pearson，位于文件*correlation.py*中。

首先，我们将创建三个相关向量 x、y 和 z。我们假设这些是数据集中的特征，因此 x[0]与 y[0]和 z[0]配对。我们需要的代码是：

np.random.seed(8675309)

N = 100

x = np.linspace(0,1,N) + (np.random.random(N)-0.5)

y = np.random.random(N)*x

z = -0.1*np.random.random(N)*x

注意，我们再次固定了 NumPy 伪随机种子以确保输出可复现。第一个特征 x 是从零到一的噪声线。第二个特征 y 与 x 相跟踪，但也因为乘以一个在 0, 1)区间内的随机值而变得有噪声。最后，z 与 x 呈负相关，因为系数为-0.1。

[图 4-6 中的上方图表依次绘制了三个特征值，展示它们如何相互跟踪。下方图表则将三个特征作为配对的点显示，一个值在 x 轴上，另一个在 y 轴上。

![image](img/04fig06.jpg)

*图 4-6：三个特征按顺序展示它们如何跟踪（上图），以及特征作为对的散点图（下图）*

计算 Pearson 相关系数的 NumPy 函数是 np.corrcoef。与我们的版本不同，这个函数返回一个矩阵，显示传入的所有变量对之间的相关性。例如，使用我们的 pearson 函数，我们得到以下 x、y 和 z 之间的相关系数：

pearson(x,y): 0.682852

pearson(x,z): -0.850475

pearson(y,z): -0.565361

NumPy 返回以下内容，x、y 和 z 堆叠为一个单一的 3 × 100 数组：

>>> d = np.vstack((x,y,z))

>>> print(np.corrcoef(d))

[[ 1.          0.68285166 -0.85047468]

[ 0.68285166  1.         -0.56536104]

[-0.85047468 -0.56536104  1.        ]

对角线对应的是每个特征与自身的相关性，显然是完美的，因此为 1.0。x 和 y 之间的相关性位于元素 0,1，并与我们的 pearson 函数值匹配。类似地，x 和 z 之间的相关性位于元素 0,2，而 y 和 z 之间的相关性位于元素 1,2。请注意，矩阵是对称的，这符合我们的预期，因为 corr(*X, Y*) = corr(*Y, X*)。

SciPy 的相关性函数是 stats.pearsonr，类似于我们的函数，但会返回一个 *p*-值和 *r* 值。我们将在本章稍后讨论 *p*-值。我们使用返回的 *p*-值作为一个未相关系统产生计算相关性值的概率。对于我们的示例特征，*p*-值几乎为零，这意味着没有合理的可能性表明一个未相关的系统生成了这些特征。

我们之前提到，对于图像，附近的像素通常高度相关。让我们看看这一点是否对于一张示例图像也成立。我们将使用 sklearn 附带的 China 图像，并将绿色带的特定行作为配对向量。我们将计算两行相邻的相关系数、一行更远的行和一个随机向量：

>>> from sklearn.datasets import load_sample_image

>>> china = load_sample_image('china.jpg')

>>> a = china[230,:,1].astype("float64")

>>> b = china[231,:,1].astype("float64")

>>> c = china[400,:,1].astype("float64")

>>> d = np.random.random(640)

>>> pearson(a,b)

0.8979360

>>> pearson(a,c)

-0.276082

>>> pearson(a,d)

-0.038199

比较第 230 行和第 231 行，发现它们高度正相关。比较第 230 行和第 400 行时，相关性较弱，且在这种情况下为负相关性。最后，正如我们所预期的，与随机向量的相关性接近零。

Pearson 相关系数被广泛使用，以至于你常常会看到它被简单地称为 *相关系数*。现在让我们看一下第二个相关性函数，看看它与 Pearson 系数有何不同。

#### Spearman 相关性

我们将探索的第二个相关性度量是 *Spearman 相关系数*，ρ ϵ [–1, +1]。它是基于特征值的排名，而不是值本身的度量。

要对 *X* 排序，我们将 *X* 中的每个值替换为在 *X* 排序版本中的索引。如果 *X* 是

[86, 62, 28, 43, 3, 92, 38, 87, 74, 11]

那么排名是

[7, 5, 2, 4, 0, 9, 3, 8, 6, 1]

因为当 *X* 被排序时，86 排在第八位（从零开始计算），而 3 排在第一位。

Pearson 相关性寻找的是线性关系，而 Spearman 寻找的是输入之间的任何单调关系。

如果我们已经获得了特征值的排名，那么 Spearman 系数就是

![image](img/04equ06.jpg)

其中 *n* 是样本数量，*d* = rank(*X*) – rank(*Y*) 是配对 *X* 和 *Y* 值的排名差。注意，公式 4.6 仅在排名唯一的情况下有效（即 *X* 或 *Y* 中没有重复值）。

在公式 4.6 中计算 *d* 时，我们需要对 *X* 和 *Y* 进行排名，并使用排名的差值。Spearman 相关系数是排名的 Pearson 相关系数。

上面的例子展示了 Spearman 相关系数实现的方式：

import numpy as np

def spearman(x,y):

n = len(x)

t = x[np.argsort(x)]

rx = []

for i in range(n):

rx.append(np.where(x[i] == t)[0][0])

rx = np.array(rx, dtype="float64")

t = y[np.argsort(y)]

ry = []

for i in range(n):

ry.append(np.where(y[i] == t)[0][0])

ry = np.array(ry, dtype="float64")

d = rx - ry

return 1.0 - (6.0/(n*(n*n-1)))*(d**2).sum()

为了获取排名，我们首先需要对 *X* (t) 进行排序。然后，对于 *X* (x) 中的每个值，我们通过 np.where 找出它在 t 中的位置，并取第一个元素，即第一个匹配项。构建完 rx 列表后，我们将其转化为浮点型的 NumPy 数组。对 *Y* 也进行同样的操作，得到 ry。通过这些排名，d 设置为它们的差值，然后使用公式 4.6 来返回 Spearman ρ 值。

请注意，这个版本的 Spearman 相关系数受限于公式 4.6，当 *X* 或 *Y* 中没有重复值时使用。本节中的例子使用的是随机浮点数值，因此完全重复的概率非常低。

我们将把我们实现的 Spearman 相关系数与 SciPy 版本的 stats.spearmanr 进行比较。就像 SciPy 版本的 Pearson 相关系数一样，stats.spearmanr 返回一个 *p* 值。我们将忽略它。让我们看看我们的函数和 SciPy 版本的比较：

>>> from scipy.stats import spearmanr

>>> print(spearman(x,y), spearmanr(x,y)[0])

0.694017401740174 0.6940174017401739

>>> print(spearman(x,z), spearmanr(x,z)[0])

-0.8950855085508551 -0.895085508550855

>>> print(spearman(y,z), spearmanr(y,z)[0])

-0.6414041404140414 -0.6414041404140414

我们与 SciPy 函数的结果完全一致，直到浮点值的最后一位。

重要的是要记住 Pearson 相关系数和 Spearman 相关系数的根本区别。例如，考虑一个线性 ramp 和 sigmoid 函数之间的相关性：

ramp = np.linspace(-20,20,1000)

sig = 1.0 / (1.0 + np.exp(-ramp))

print(pearson(ramp,sig))

print(spearman(ramp,sig))

在这里，ramp 从 -20 到 20 线性增加，而 sig 则遵循一个 S 型的 sigmoid 曲线。由于两者都在增加，所以 Pearson 相关系数会较高，因为 *x* 越来越正，但关联关系并非完全线性。运行示例结果为：

0.905328

1.0

该图表明 Pearson 相关系数为 0.9，但 Spearman 相关系数为 1.0，因为每当坡道增加时，信号也随之增加，而且*仅仅*是增加。Spearman 相关系数捕捉了两个变量之间的非线性关系，而 Pearson 相关系数只显示了它的线性趋势。如果我们正在分析一个经典机器学习算法的数据集，Spearman 相关系数可能帮助我们决定保留哪些特征，丢弃哪些特征。

这就是我们对描述和理解数据的统计学考察的总结。现在让我们学习如何使用假设检验来解释实验结果，并回答诸如“这两组数据样本是否来自同一母体分布？”等问题。

### 假设检验

我们有两组独立的 50 名细胞生物学的学生。我们没有理由认为这两组存在显著差异，因为这些学生是从更大的群体中随机分配的。第一组学生参加了讲座，并且完成了一组结构化的计算机练习。第二组学生仅参加了讲座。两组学生都参加了相同的期末考试，并得到了表 4-1 中的成绩。我们想知道要求学生完成计算机练习是否对他们的期末考试成绩产生了影响。

**表 4-1：** 第一组和第二组的测试成绩

| **第一组** | 81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88 89 92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85 85 78 94 100 |
| --- | --- |
| **第二组** | 92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86 77 94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88 91 83 85 73 |

图 4-7 展示了表 4-1 的箱线图。

为了了解两组之间的最终考试成绩是否有显著变化，我们需要进行假设检验。我们将使用的假设检验方法被称为*假设检验*，这是现代科学中的一个关键部分。

![image](img/04fig07.jpg)

*图 4-7：表 4-1 数据的箱线图*

假设检验是一个广泛的主题，太过庞大，无法在此提供全面的介绍。由于这是一本关于深度学习的书，我们将专注于深度学习研究人员可能遇到的情景。我们将仅考虑两种假设检验：用于具有不同方差的未配对样本的 t 检验（参数检验）和 Mann-Whitney U 检验（非参数检验）。在接下来的讨论中，我们将了解这些检验是什么，为什么我们仅限于这两种检验，以及*参数检验*和*非参数检验*的含义。

为了成功进行假设检验，我们需要理解*假设*的含义，因此我们首先会讨论这个概念，以及我们为何限制只考虑某些类型的假设检验。掌握了假设的概念后，我们将依次讨论 t 检验和 Mann-Whitney U 检验，并使用表 4-1 中的数据作为示例。让我们开始吧。

#### 假设

为了了解两组数据是否来自相同的母体分布，我们可以查看汇总统计数据。图 4-7 展示了第一组和第二组的箱型图。看起来这两组数据的均值和标准差不同。我们怎么知道呢？箱型图展示了中位数的位置，胡须部分则告诉我们一些关于方差的信息。这两者结合起来提示均值会有所不同，因为中位数不同，而且两组数据在中位数周围大致对称。胡须之间的空间则暗示了标准差。因此，我们可以利用数据集的均值来提出假设。

在假设检验中，我们有两个假设。第一个是假设被称为*原假设*（*H*[0]），即两组数据*来自*相同的母体分布，它们之间没有任何特别的区别。第二个假设是*备择假设*（*H[a]*），即这两组数据不来自相同的分布。由于我们将使用均值，*H*[0] 表示均值，实际上是生成数据的母体的均值，是相同的。同样，如果我们拒绝*H*[0]，我们隐含地接受了*H[a]*，并声明我们有证据表明均值不同。由于我们没有真实的母体均值，因此我们将使用样本均值和标准差。

假设检验并不能明确告诉我们*H*[0]是否成立。相反，它为我们提供了接受或拒绝原假设的证据。记住这一点至关重要。

我们正在测试两个独立样本，看看是否应该认为它们来自相同的母体分布。还有其他使用假设检验的方法，但在深度学习中我们很少遇到它们。对于当前任务，我们需要样本的均值和样本的标准差。我们的检验会提出这样一个问题：“这两组数据的均值是否存在显著差异？”

我们只关心检测这两组数据是否来自相同的母体分布，因此我们做的另一个简化假设是，所有的检验将是*双侧*的，或称*双尾*的。当我们使用检验时，比如接下来我们将描述的 t 检验，我们将计算的检验统计量（t 值）与检验统计量的分布进行比较，并询问我们的计算 t 值有多可能。如果我们想了解检验统计量在该分布的某个分位数之上或之下的可能性，我们就是在做双侧检验。如果我们只关心检验统计量是否超过某个特定值，而不关心其是否低于该值，或反之，则我们在做单侧检验。

让我们列出我们的假设和方法：

1.  我们有两组独立的数据，想要进行比较。

1.  我们不假设数据的标准差是相同的。

1.  我们的原假设是数据集的母体分布的均值相同，*H*[0] : μ[1] = μ[2]。我们将使用样本均值 ![image](img/094equ01a.jpg) 和样本标准差 (*s*[1], *s*[2]) 来帮助我们决定是否接受或拒绝 *H*[0]。

1.  假设检验假定数据是*独立同分布（i.i.d.）*的。我们可以将其解释为数据是一个公平的随机样本。

理解了这些假设后，让我们从 t 检验开始，它是最广泛使用的假设检验。

#### t 检验

*t 检验*依赖于*t*，即检验统计量。这个统计量与 t 分布进行比较，并用来生成*p*-值，这个概率值将帮助我们做出关于 *H*[0] 的结论。t 检验和相关的 z 检验有着丰富的历史背景，但在这里我们不做讨论。我鼓励你在有机会时深入了解假设检验，或者至少回顾一下关于如何正确进行假设检验并解读结果的有见地的文章。

t 检验是一个*参数检验*。这意味着有关于数据和数据分布的假设。具体来说，t 检验假设，除了数据是 i.i.d.之外，数据的分布（直方图）是正态的。我们之前已经提到，许多物理过程似乎遵循正态分布，所以可以合理推测实际测量的数据也可能如此。

测试一个数据集是否符合正态分布有很多方法，但我们将在此忽略这些方法，因为关于这些测试的实用性存在一些争议。相反，我（有些鲁莽地）建议你同时使用 t 检验和曼-惠特尼 U 检验，来帮助你决定是否接受或拒绝*H*[0]。使用这两种检验可能会导致一种情况，即它们得出不一致的结果，一种检验表明有证据反对原假设，而另一种则表明没有。一般来说，如果非参数检验表明有证据反对*H*[0]，那么无论 t 检验结果如何，你可能都应该接受这个证据。如果 t 检验结果反对*H*[0]，而曼-惠特尼 U 检验没有，而且你认为数据是正态的，那么你也可能接受 t 检验的结果。

t 检验有不同的版本。我们在上面明确指出，我们将使用为大小和方差不同的数据集设计的版本。我们将使用的 t 检验的具体版本是*Welch 的 t 检验*，它不假设两个数据集的方差相同。

Welch 的 t 检验的 t 得分是

![image](img/095equ01.jpg)

其中*n*[1]和*n*[2]分别是两个组的样本大小。

t 得分和一个相关值，称为*自由度*，它与上面提到的自由度类似，但也有所不同，生成了适当的 t 分布曲线。为了得到*p*值，我们计算曲线下方的面积，包括正负 t 得分的部分，并返回它。由于概率分布的积分为 1，从正负 t 得分到正负无穷大的尾部面积将是*p*值。我们将在下面使用自由度来帮助我们计算置信区间。

*p*值告诉我们什么？它告诉我们，在原假设为真的情况下，观察到的两个均值之间的差异或更大差异的概率。通常，如果这个概率低于我们选择的某个阈值，我们就拒绝原假设，并认为我们有证据表明这两个组的均值不同——也就是说，它们来自不同的母体分布。当我们拒绝*H*[0]时，我们称这个差异是*统计学显著*的。接受/拒绝*H*[0]的阈值叫做α，通常情况下，α = 0.05 是一个典型的值，尽管这个值存在一些问题。我们将在下面讨论为什么 0.05 是一个问题。

需要记住的一点是，*p*值假设零假设为真。它告诉我们，零假设*H*[0]至少产生我们所观察到的差异，或更大的差异，发生在两个组之间的可能性。如果*p*值较小，可能有两种含义：（1）零假设为假，或者（2）随机抽样误差使我们得到的样本超出了我们预期的范围。由于*p*值假设*H*[0]为真，较小的*p*值帮助我们越来越不相信（2），并增强了我们对（1）可能正确的信心。然而，单独的*p*值不能确认（1）；其他知识也需要发挥作用。

我提到过使用α = 0.05 是有问题的。它的主要问题在于它过于宽松，导致过多地拒绝了真实的零假设。根据 James Berger 和 Thomas Sellke 在其文章《测试点零假设：*P*值与证据的不可调和性》（《美国统计学会期刊》，1987 年）中的观点，当α = 0.05 时，大约 30%的真实零假设会被拒绝。当我们使用类似α ≤ 0.001 时，错误拒绝真实零假设的机会降至不到 3%。故事的教训是，*p* < 0.05 并不是魔法值，坦率地说，对于单一研究来说是缺乏说服力的。寻找至少为 0.001，或者更好的是更小的高度显著的*p*值。当*p* = 0.05 时，所有你得到的只是一个暗示，你应该重复实验。如果重复实验的*p*值都在 0.05 左右，那么拒绝零假设就开始变得有意义。

##### 置信区间

除了*p*值，你还会经常看到*置信区间（CIs）*。置信区间给出了我们相信真实的总体均值差异所在的区间范围，并为我们所比较的两个数据集的重复样本提供了给定置信度的估算。通常，我们报告 95%的置信区间。我们的假设检验通过询问样本均值之间的差异是否为零来检查均值的相等性。因此，任何包含零的置信区间都向我们传递了不能拒绝零假设的信号。

Welch’s t 检验中的自由度是

![image](img/04equ07.jpg)

我们可以用它来计算置信区间。

![image](img/04equ08.jpg)

其中*t*[1–α/2,*df*]是临界值，t 值对应给定的置信度（α）和自由度*df*，可从方程 4.7 中得出。

我们应如何解读 95%的置信区间？存在一个总体值：即组均值之间的真实差异。95%的置信区间的含义是，如果我们能够从产生这两个数据集的分布中抽取重复样本，95%的计算置信区间将包含均值之间的真实差异。这并*不是*在 95%的置信度下包含均值差异真实值的区间。

除了检查零是否在置信区间内外，置信区间还很有用，因为它的宽度能告诉我们效应的大小。这里，效应与均值之间的差异相关。我们可能根据 *p*-值有统计显著的差异，但这个效应在实际中可能没有意义。因为小的置信区间意味着包含真实效应的范围很窄，效应较大时，置信区间会很窄。稍后我们将看到如何在可能的情况下计算另一个有用的效应度量。

最后，当 *p*-值小于 α 时，相应的 *CI*[α] 也不会包含 *H*[0]。换句话说，*p*-值和置信区间所告诉我们的信息是一致的——它们不会互相矛盾。

##### 效应大小

拥有一个统计显著的 *p*-值是一回事。另一个问题是，该 *p*-值所代表的差异在现实世界中是否有意义。一个常用的效应大小度量是 *Cohen’s d*。对于我们来说，由于我们使用的是 Welch’s t 检验，Cohen 的 *d* 通过计算得到：

![image](img/04equ09.jpg)

Cohen 的 *d* 通常是主观解释的，尽管我们也应该报告数值。主观上，效应的大小可以是

| ***d*** | **效应** |
| --- | --- |
| 0.2 | 小 |
| 0.5 | 中等 |
| 0.8 | 大 |

Cohen 的 *d* 是合理的。均值之间的差异是理解效应的一种自然方式。通过均值方差进行缩放将其置于一个一致的范围内。从 公式 4.9 中，我们看到一个对应于统计显著结果的 *p*-值可能会导致一个效应很小但在实际中并不重要的结果。

##### 评估测试分数

让我们把上面的所有内容放在一起，应用 t 检验到 表 4-1 中的测试数据。你可以在文件 *hypothesis.py* 中找到代码。我们首先生成数据集：

np.random.seed(65535)

a = np.random.normal(85,6,50).astype("int32")

a[np.where(a > 100)] = 100

b = np.random.normal(82,7,50).astype("int32")

b[np.where(b > 100)] = 100

再次强调，我们使用固定的 NumPy 伪随机数种子以保证可重复性。我们从一个均值为 85，标准差为 6.0 的正态分布中生成样本 a。从一个均值为 82，标准差为 7.0 的正态分布中生成样本 b。对于两个样本，我们将所有超过 100 的值限制为 100。这些毕竟是测试分数，没有额外加分。

接下来，我们应用 t 检验：

from scipy.stats import ttest_ind

t,p = ttest_ind(a,b, equal_var=False)

print("(t=%0.5f, p=%0.5f)" % (t,p))

我们得到 *(t* = 2.40234, *p* = 0.01852)。*t* 是统计量，*p* 是计算出的*p*-值。它是 0.019，低于 0.05，但仅低了一倍。我们得到一个较弱的结果，表明我们可能想要拒绝零假设，认为 a 组和 b 组来自不同的分布。当然，我们知道它们确实来自不同的分布，因为是我们生成的，但看到检验结果朝着正确的方向发展，还是很不错的。

注意，我们从 SciPy 导入的函数是 ttest_ind。这是用于独立样本（未配对样本）的函数。另外，注意我们在调用时添加了 equal_var=False。这是使用 Welch t 检验的方法，它不假设两个数据集之间的方差相等。我们知道它们不相等，因为 a 的标准差是 6.0，而 b 的标准差是 7.0。

为了获得置信区间，我们将编写一个 CI 函数，因为 NumPy 和 SciPy 并没有内置此函数。该函数直接实现了公式 4.7 和 4.8：

from scipy import stats

def CI(a, b, alpha=0.05):

n1, n2 = len(a), len(b)

s1, s2 = np.std(a, ddof=1)**2, np.std(b, ddof=1)**2

df = (s1/n1 + s2/n2)**2 / ((s1/n1)**2/(n1-1) + (s2/n2)**2/(n2-1))

tc = stats.t.ppf(1 - alpha/2, df)

lo = (a.mean()-b.mean()) - tc*np.sqrt(s1/n1 + s2/n2)

hi = (a.mean()-b.mean()) + tc*np.sqrt(s1/n1 + s2/n2)

return lo, hi

临界的*t*值是通过调用 stats.t.ppf 来获得的，传入α/2 值和适当的自由度（*df*）。临界的*t*值是 97.5 百分位值，对于α = 0.05，这是*百分位点函数 (ppf)* 返回的值。我们将其除以 2，以涵盖 t 分布的两端。

对于我们的测试示例，置信区间是[0.56105, 5.95895]。注意，这个区间不包括零，因此 CI 也表示一个统计显著的结果。然而，区间相当大，所以这不是一个特别稳健的结果。CI 区间本身可能很难解释，所以最后，让我们计算 Cohen 的*d*，看看它是否根据置信区间的宽度有意义。在代码中，我们实现了公式 4.9：

def Cohen_d(a,b):

s1 = np.std(a, ddof=1)**2

s2 = np.std(b, ddof=1)**2

return (a.mean() - b.mean()) / np.sqrt(0.5*(s1+s2))

我们得到 *d* = 0.48047，表示中等效应大小。

#### Mann-Whitney U 检验

t 检验假设源数据的分布是正态分布。如果数据不是正态分布，我们应该使用*非参数检验*。非参数检验不对数据的底层分布做任何假设。*Mann-Whitney U 检验*，有时称为*Wilcoxon 秩和检验*，是一种非参数检验，帮助判断两个不同的数据集是否来自同一母体分布。Mann-Whitney U 检验不直接依赖于数据的值，而是使用数据的排名。

该检验的原假设如下：从组 1 随机选择的值大于从组 2 随机选择的值的概率是 0.5。我们来稍微思考一下。如果数据来自相同的总体分布，那么我们应该期望从两个组中随机选择的任何一对值没有偏好地显示哪个值大于另一个。

替代假设是，随机选择的组 1 中的一个值大于随机选择的组 2 中的一个值的概率不是 0.5。请注意，这里没有关于概率是否大于或小于 0.5 的说明，只是说它不是 0.5；因此，我们将使用的曼-惠特尼 U 检验是双侧的。

曼-惠特尼 U 检验的原假设与 t 检验的原假设不同。对于 t 检验，我们询问两组之间的均值是否相同。（实际上，我们询问的是均值差异是否为零。）然而，如果两组数据来自不同的总体分布，两个原假设都为假，因此我们可以在 t 检验的基础上使用曼-惠特尼 U 检验，尤其是在数据不服从正态分布时。

为了生成 *U*，即曼-惠特尼统计量，我们首先将两组数据合并并进行排序。相同的值用该值的排名和下一个排名值之间的平均值替代。我们还会跟踪数据来源组，以便在排序后能再次分开列表。按组分别求和得到 *R*[1] 和 *R*[2]（使用合并数据的排名）。我们计算两个值，

![image](img/100equ01.jpg)

使用较小的值叫做 *U*，即检验统计量。可以根据 *U* 生成 *p*-值，记住我们之前讨论过的关于 *p*-值的意义和用途。和之前一样，*n*[1] 和 *n*[2] 分别是两组中的样本数。曼-惠特尼 U 检验要求这两个数字中较小的值至少为 21 个样本。如果样本数不够，使用 SciPy 的 mannwhitneyu 函数时，结果可能不可靠。

我们可以对来自表 4-1 的测试数据进行曼-惠特尼 U 检验，

from scipy.stats import mannwhitneyu

u,p = mannwhitneyu(a,b)

print("(U=%0.5f, p=%0.5f)" % (u,p))

使用我们在 t 检验中使用的 a 和 b。得到的结果是 *(U* = 997.00000, *p* = 0.04058)。*p*-值稍微低于 0.05 的最小阈值。

a 和 b 的均值分别为 85 和 82。如果我们将 b 的均值改为 83 或 81，*p*-值会发生什么变化？改变 b 的均值意味着改变 np.random.normal 中的第一个参数。这样做的结果见表 4-2，我已为完整性提供了所有结果。

**表 4-2：** 不同均值下模拟测试分数的曼-惠特尼 U 检验和 t 检验结果（*n*[1]=*n*[2]=50）

| **均值** | **曼-惠特尼 U** | **t 检验** |
| --- | --- | --- |
| 85 对比 83 | (*U*=1104.50000, *p*=0.15839) | (*t*=1.66543, *p*=0.09959) |
| 85 对 82 | (*U*=997.00000, *p*=0.04058) | (*t*=2.40234, *p*=0.01852) |
| 85 对 81 | (*U*=883.50000, *p*=0.00575) | (*t*=3.13925, *p*=0.00234) |

表 4-2 应该对我们来说很有意义。当均值接近时，更难区分它们，因此我们预期 *p*-值较大。回想一下我们每组只有 50 个样本。当均值之间的差异增大时，*p*-值会下降。均值差异为三时，*p*-值勉强显著。当差异更大时，*p*-值变得真正显著——这正是我们所预期的。

上述分析引出了一个问题：当两组均值之间的差异较小时，*p*-值如何随样本量的变化而变化？

图 4-8 显示了 Mann-Whitney U 检验和 t 检验的 *p*-值（均值 ± 标准误差）在 25 次实验中的变化，作为样本量的函数，其中均值为 85 和 84。

![image](img/04fig08.jpg)

*图 4-8：均值 *p*-值随样本量变化的函数，其中样本均值差异为 1，![image](img/101equ01.jpg)*

小数据集使得在均值差异较小时，很难区分不同情况。我们还发现，较大的样本量能够揭示差异，无论使用什么测试。在 图 4-8 中，尽管基础数据呈正态分布，Mann-Whitney U 的 *p*-值却低于 t 检验的 *p*-值，这一点很有趣。传统观点通常认为应该是反过来的。

图 4-8 是一个关于大样本测试能够发现实际差异的典型示例。当样本量足够大时，即使是微弱的差异也会变得显著。然而，我们需要平衡这一点与效应大小。当每组有 1,000 个样本时，我们得到一个统计学上显著的 *p*-值，但 Cohen's *d* 值大约为 0.13，表明效应较弱。大样本研究可能会发现一个显著的效应，但它可能如此微弱，以至于在实践中几乎没有意义。

### 总结

本章涉及了你在深入学习深度学习过程中会遇到的统计学关键内容。具体来说，我们了解了不同类型的数据，以及如何确保数据对构建模型有用。然后，我们学习了汇总统计量，并通过实际示例帮助我们理解数据集。理解我们的数据是成功进行深度学习的关键。我们探讨了不同类型的均值，了解了变异度的测量方法，并看到了通过箱线图可视化数据的实用性。

缺失数据是深度学习中的一大难题。在本章中，我们探讨了如何弥补缺失数据。接下来，我们讨论了相关性，如何检测和衡量数据集元素之间的关系。最后，我们介绍了假设检验。在深度学习中，我们限制自己只考虑最可能遇到的情境，并学习了如何应用 t 检验和曼-惠特尼 U 检验。假设检验还让我们了解了*p*-值。我们通过示例看到了它，并讨论了如何正确解读它。

在下一章，我们将告别统计学，深入探索线性代数的世界。线性代数是我们实现神经网络的基础。
