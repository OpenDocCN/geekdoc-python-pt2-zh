## **2

文本处理管道**

![图片](img/comm1.jpg)

现在你已经了解了 NLP 应用的结构，是时候看到这些基础概念的实际应用了。在这一章中，你将安装 spaCy 并设置工作环境。接着，你将学习*文本处理管道*，这是你用来确定语篇意义和意图的一系列基本 NLP 操作。这些操作包括分词、词形还原、词性标注、句法依存分析和命名实体识别。

### **设置工作环境**

在开始使用 spaCy 之前，你需要通过在机器上安装以下软件组件来设置工作环境：

+   Python 2.7 或更高版本，或者 3.4 或更高版本

+   spaCy 库

+   spaCy 的统计模型

你需要安装 Python 2.7 或更高版本，或者 3.4 或更高版本，才能使用 spaCy v2.0.*x*。可以在*[`www.python.org/downloads/`](https://www.python.org/downloads/)*下载，并按照指示设置 Python 环境。接下来，使用 pip 在你的 Python 环境中安装 spaCy，通过运行以下命令：

$ pip install spacy

如果系统上有多个 Python 安装，选择与要使用的 Python 安装相关联的 pip 可执行文件。例如，如果你想使用 Python 3.5 与 spaCy，运行以下命令：

$ pip3.5 install spacy

如果你的系统已经安装了 spaCy，你可能希望将其升级到新版本。本书中的示例假设你使用的是 spaCy v2.0.*x*或更高版本。你可以通过以下命令验证已安装的 spaCy 版本：

$ python -m spacy info

再次提醒，你可能需要将 python 命令替换为在你的特定环境中使用的 python 可执行文件命令，比如 python3.5。从现在开始，我们将使用 python 和 pip，无论你的系统使用的可执行文件是什么。

如果你决定将已安装的 spaCy 包升级到最新版本，可以使用以下 pip 命令：

$ pip install -U spacy

### **安装 spaCy 的统计模型**

spaCy 的安装不包括你开始使用库时所需的统计模型。统计模型包含了从一组来源收集的关于特定语言的知识。你必须单独下载并安装每个想要使用的模型。

有多种预训练的统计模型可用于不同语言。例如，英语的以下模型可以从 spaCy 的网站上下载：en_core_web_sm、en_core_web_md、en_core_web_lg 和 en_vectors_web_lg。模型使用以下命名约定：*lang_type_genre_size*。*Lang* 指定语言。*Type* 表示模型的功能（例如，core 是一个通用模型，包括词汇、语法、实体和向量）。*Genre* 表示模型训练的文本类型：web（例如 Wikipedia 或类似的媒体资源）或 news（新闻文章）。*Size* 表示模型的大小：lg 为大，md 为中，sm 为小。模型越大，所需的磁盘空间越大。例如，en_vectors_web_lg-2.1.0 模型需要 631MB，而 en_core_web_sm-2.1.0 只需要 10MB。

为了跟随本书中的示例，en_core_web_sm（最轻量级的模型）就足够使用。使用 spaCy 的下载命令时，spaCy 会默认选择该模型：

$ python -m spacy download en

命令中的 en 快捷方式指示 spaCy 下载并安装适合英语语言的最佳默认模型。在这个上下文中，最佳匹配的模型意味着为指定语言（本例为英语）生成的模型、通用模型以及最轻量级的模型。

要下载特定的模型，您必须指定其名称，像这样：

$ python -m spacy download en_core_web_md

安装完成后，您可以使用在安装过程中指定的相同快捷方式加载该模型：

nlp = spacy.load('en')

### **使用 spaCy 的基本 NLP 操作**

让我们开始执行一系列基本的 NLP 操作，这些操作被称为处理流程。spaCy 会在后台为您执行所有这些操作，让您可以专注于应用程序的具体逻辑。图 2-1 提供了这个过程的简化示意图。

![image](img/fig2-1.jpg)

*图 2-1：处理流程的高级视图*

处理流程通常包括分词、词形还原、词性标注、句法依赖分析和命名实体识别。在本节中，我们将介绍这些任务。

#### ***分词***

任何 NLP 应用程序通常在处理文本时执行的第一个操作是将文本解析为*词元*，它可以是单词、数字或标点符号。分词是第一个操作，因为所有其他操作都需要已经处理好的词元。

以下代码展示了分词过程：

➊ import spacy

➋ nlp = spacy.load('en')

➌ doc = nlp(u'I am flying to Frisco')

➍ print([w.text for w in doc])

我们首先导入 spaCy 库 ➊，以便访问其功能。然后，我们使用 en 快捷方式链接加载模型包，以创建 spaCy 的 Language 类的实例。Language 对象包含语言的词汇表和来自统计模型的其他数据。我们称语言对象为 nlp。

接下来，我们将刚刚创建的对象应用到一个样本句子中，创建一个 *Doc 对象* 实例。Doc 对象是 Token 对象序列的容器。spaCy 根据您提供的文本隐式生成它。

到这一点为止，仅用三行代码，spaCy 就已经生成了样本句子的语法结构。如何使用它完全取决于您。在这个非常简单的例子中，您只是打印出样本句子的每个令牌的 *文本内容* ➍。

该脚本将样本句子的令牌输出为一个列表：

['I', 'am', 'flying', 'to', 'Frisco']

*文本内容* —— 组成令牌的字符组，例如令牌“am”中的字母“a”和“m”，仅仅是 Token 对象的众多属性之一。您还可以提取分配给令牌的各种语言特征，正如您将在以下示例中看到的那样。

#### ***词形归并***

*词形* 是令牌的基本形式。您可以将其视为令牌在字典中列出时的形式。例如，令牌“flying”的词形是“fly”。*词形归并* 是将词形减少为它们的词形的过程。以下脚本提供了使用 spaCy 进行词形归并的简单示例：

import spacy

nlp = spacy.load('en')

doc = nlp(u'这个产品集成了用于下载和库

应用补丁')

for token in doc:

print(➊token.text, ➋token.lemma_)

脚本的前三行与之前的脚本相同。回想一下，它们导入 spaCy 库，使用 en 快捷方式加载英语模型，并创建一个文本处理管道，并将管道应用于一个样本句子 —— 通过它您可以访问句子的语法结构。

**注意**

*在语法中，句子结构是句子中个别词语、短语和从句的排列。句子的语法含义取决于这种结构组织。*

一旦您有一个包含样例句子中令牌的 Doc 对象，您可以在循环中迭代这些令牌，然后打印出令牌的文本内容 ➊ 以及其对应的词形 ➋。此脚本生成以下输出（我已制表以使其更易读）：

this        这

product     产品

integrates  集成

both        两者

libraries   库

for         为了

downloading 下载

and         和

applying    应用

patches     补丁

左侧列出了令牌，右侧列出了它们的词形。

#### ***应用词形归并进行意义识别***

词形还原是意义识别任务中的一个重要步骤。为了更好地理解，我们回到前一部分中的示例句子：

我将飞往弗里斯科。

假设这个句子提交给一个与在线系统交互的 NLP 应用程序，该系统提供一个 API 用于预定旅行票务。该应用程序处理客户请求，提取必要的信息，并将这些信息传递给底层 API。这个设计可能类似于图 2-2 所示。

![image](img/fig2-2.jpg)

*图 2-2：在提取客户请求中必要信息的过程中使用词形还原*

NLP 应用程序尝试从客户请求中获取以下信息：旅行方式（飞机、火车、巴士等）和目的地。应用程序首先需要确定客户是想要机票、火车票还是巴士票。为此，应用程序会搜索一个与预定义关键字列表中的某个词匹配的单词。简化搜索这些关键字的一种简单方法是，首先将处理中的句子中的所有单词转换为它们的词根形式。在这种情况下，预定义的关键字列表会短得多，也更清晰。例如，你不需要将“fly”（飞）、“flying”（飞行）、“flew”（飞过）和“flown”（飞行过）等所有形式都列入其中，因为它们都可以作为客户想要机票的指示，所有这些变体都会被简化为该词的基础形式——即“fly”。

词形还原在应用程序尝试从提交的请求中确定目的地时也非常有用。全球的城市有很多别名。但用于预定机票的系统要求使用官方名称。当然，执行词形还原的默认 Tokenizer 不会知道城市、国家等的别名和官方名称之间的区别。为了解决这个问题，你可以为现有的 Tokenizer 实例添加特例规则。

以下脚本演示了如何为目的地城市示例实现词形还原。它会打印出组成句子的单词的词根形式。

import spacy

from spacy.symbols import ORTH, LEMMA

nlp = spacy.load('en')

doc = nlp(u'I am flying to Frisco')

print([w.text for w in doc])

➊ special_case = [{ORTH: u'Frisco', LEMMA: u'San Francisco'}]

➋ nlp.tokenizer.add_special_case(u'Frisco', special_case)

➌ print([w.lemma_ for w in nlp(u'I am flying to Frisco')])

你为单词 Frisco 定义了一个*特例* ➊，通过将其默认的词根形式替换为“San Francisco”。然后你将这个特例添加到 Tokenizer 实例 ➋中。一旦添加，Tokenizer 实例将在每次被请求 Frisco 的词根时使用这个特例。为了确保一切按预期工作，你会打印出句子中单词的词根形式 ➌。

脚本生成以下输出：

['I', 'am', 'flying', 'to', 'Frisco']

['-PRON-', 'be', 'fly', 'to', 'San Francisco']

输出列出了句子中所有单词的词元，除了“Frisco”，对于它会列出“San Francisco”。

#### ***词性标注***

*词性标签*会告诉你给定句子中某个单词的词性（名词、动词等）。（请回顾第一章，一个单词根据其出现的上下文可能充当多个词性。）

在 spaCy 中，词性标签可以包含关于标记的详细信息。对于动词，它们可能会告诉你以下特征：时态（过去式、现在式或将来式）、体（简单、进行或完成）、人称（第一、第二或第三人称）以及数（单数或复数）。

提取这些动词的词性标签有助于在仅有分词和词元化时无法充分识别用户意图的情况下。比如，前一节中提到的票务预订应用的词元化脚本无法决定 NLP 应用如何选择句子中的词来向底层 API 发出请求。在实际情况中，这可能会相当复杂。例如，客户的请求可能包含多个句子：

我已经飞到洛杉矶。现在我正飞往 Frisco。

对于这些句子，词元化的结果如下：

['-PRON-', 'have', 'fly', 'to', 'LA', '.', 'now', '-PRON-', 'be', 'fly', 'to', 'San Francisco', '.']

仅执行词元化在这里是不够的；应用程序可能会将第一句中的词元“fly”和“LA”作为关键字，表示客户打算飞往洛杉矶，实际上客户打算飞往旧金山。问题的一部分是，词元化将动词转换为不定式形式，难以了解它们在句子中的角色。

这就是词性标签发挥作用的地方。在英语中，核心词性包括名词、代词、限定词、形容词、动词、副词、介词、连词和感叹词。（有关这些词性的更多信息，请参阅附录中的语言学入门。）在 spaCy 中，这些相同的类别—以及一些用于符号、标点符号和其他的附加类别—被称为*粗粒度词性*，并通过 Token.pos（int）和 Token.pos_（unicode）属性作为固定标签集提供。

此外，spaCy 还提供了*精细粒度词性*标签，它提供了关于标记的更多详细信息，涵盖了形态学特征，例如动词时态和代词类型。显然，精细粒度词性标签的数量要比粗粒度词性标签更多。精细粒度词性标签可以通过 Token.tag（int）和 Token.tag_（unicode）属性获得。

表 2-1 列出了 spaCy 中用于英语模型的一些常见词性标签。

**表 2-1：** 一些常见的 spaCy 词性标签

| **标记（精细化词性）** | **词性（粗粒度词性）** | **形态学** | **描述** |
| --- | --- | --- | --- |
| NN | 名词 | 数量=单数 | 单数名词 |
| NNS | 名词 | 数量=复数 | 复数名词 |
| PRP | 代词 | 代词类型=人称 | 个人代词 |
| PRP$ | 代词 | 代词类型=人称 所有格=有 | 所有格代词 |
| VB | 动词 | 动词形式=原形 | 动词，基本形式 |
| VBD | 动词 | 动词形式=完成时 时态=过去时 | 动词，过去时 |

| VBG | 动词 | 动词形式=动名词 时态=现在时

时态=进行时 | 动词，动名词或现在分词 |

| JJ | 形容词 | 程度=普通 | 形容词 |
| --- | --- | --- | --- |

**注意**

*你可以在注释规范手册的“词性标注”章节中找到 spaCy 使用的精细化词性标记的完整列表，链接如下* *[`spacy.io/api/annotation#pos-tagging`](https://spacy.io/api/annotation#pos-tagging)*。

时态和体貌可能是动词对于自然语言处理应用来说最有趣的属性。它们一起表示动词在时间中的参考位置。例如，我们使用*现在时进行时*形式的动词来描述当前正在发生的事情或即将发生的事情。要构成现在时进行时的动词，您需要在动词“to be”的现在时形式之前加上一个-ing 动词。例如，在句子“I am looking into it”中，我们在动词“looking”之前加上了“am”——动词“to be”在第一人称现在时的形式。在这个例子中，“am”表示现在时，“looking”则指示进行时。

#### ***使用词性标注来找到相关的动词***

票务预订应用可以使用 spaCy 中可用的精细化词性标记来过滤话语中的动词，选择那些可能有助于确定客户意图的动词。

在进入此过程的代码之前，让我们先尝试弄清楚客户可能用什么样的表达方式来表达他们预定机票的意图，比如说，飞往洛杉矶。我们可以从以下包含词元组合“fly”、“to”和“LA”的句子开始。以下是一些简单的例子：

我飞到了洛杉矶。

我已经飞到了洛杉矶。

我需要飞往洛杉矶。

我正在飞往洛杉矶。

我将飞往洛杉矶。

请注意，尽管所有这些句子在简化为词元时都会包含“fly to LA”这个组合，但只有部分句子能够暗示客户有预定飞往洛杉矶的意图。前两句显然不合适。

快速分析表明，“fly”这个动词的过去时和过去完成时形式——第一和第二个句子中使用的时态——并不暗示我们所寻找的意图。只有不定式和现在进行时形式是合适的。以下脚本展示了如何在样本话语中找到这些形式：

import spacy

nlp = spacy.load('en')

doc = nlp(u'我已经飞到了洛杉矶。现在我正在飞往旧金山。')

print([w.text for w in doc if ➊w.tag_== ➋'VBG' or w.tag_== ➌'VB'])

Token 对象的 tag_ 属性 ➊ 包含分配给该对象的细粒度词性属性。你可以通过对构成语篇的词元执行循环来检查分配给某个词元的细粒度词性标签是否为 VB（动词的基本形式或不定式形式） ➌ 或 VBG（动词的现在进行时形式） ➋。

在示例语篇中，只有第二句中的动词“flying”符合指定的条件。因此，你应该看到以下输出：

['flying']

当然，细粒度的词性标签不仅会分配给动词；它们还会分配给句子中的其他词性。例如，spaCy 会将 LA 和 Frisco 识别为专有名词——指代个人、地点、物体或组织的名词——并标记为 PROPN。如果需要，你可以在之前的脚本中添加以下代码行：

print([w.text for w in doc if w.pos_ == 'PROPN'])

添加代码应该输出以下列表：

['LA', 'Frisco']

来自两个句子的专有名词都出现在列表中。

#### ***上下文很重要***

细粒度词性标签可能并不总是足够用来确定一个话语的意义。为此，你可能仍然需要依赖上下文。例如，考虑以下话语：“I am flying to LA。” 在这个例子中，词性标注器会为动词“flying”分配 VBG 标签，因为它是现在进行时形式。但由于我们使用这个动词形式来描述当前发生的事情或即将发生的事情，因此该话语的意思可能是“我已经在空中，飞往 LA。”或“我要飞往 LA。”当提交到机票预订 NLP 应用程序时，应用程序应该只将这句话解释为“我需要一张飞往 LA 的机票。”类似地，考虑以下语篇：“I am flying to LA. In the evening, I have to be back in Frisco。”这很可能意味着说话者想要一张从 LA 飞往 Frisco 的晚间航班机票。你可以在 “使用上下文改进机票预订聊天机器人” 第 91 页 (page 91) 中找到更多关于基于上下文识别意义的例子。

#### ***句法关系***

现在，让我们将专有名词与词性标注器之前选择的动词结合起来。回想一下，你用来识别语篇意图的动词列表中，只有第二个句子中的动词“flying”符合条件。如何获取最能描述语篇背后意图的动词/专有名词对呢？人类显然会从同一句话中的词语构成动词/专有名词对。由于第一句中的动词“flown”不符合指定条件（记住，只有不定式和现在进行时形式符合条件），你只能为第二个句子构建这样的对： “flying, Frisco。”

为了以编程方式处理这些情况，spaCy 提供了一个句法依赖解析器，它可以发现句子中各个词之间的句法关系，并通过单一的弧连接句法相关的词对。

与前面章节中讨论的词根和词性标签类似，*句法依赖标签*是 spaCy 赋予构成 Doc 对象中文本的 Token 对象的语言特征。例如，依赖标签 dobj 表示“直接宾语”。我们可以用箭头弧来表示它所代表的句法关系，如图 2-3 所示。

**头词与子词**

句法依赖标签描述了句子中两个词之间的句法关系类型。在这样的词对中，一个词是句法主导词（也称为头或父词），另一个词是依赖词（也称为子词）。spaCy 会将句法依赖标签赋给依赖词。例如，在“need, ticket”这一对词中，摘自句子“I need a plane ticket”，其中“ticket”是子词，“need”是头词，因为“need”是动词短语中的动词。在同一句子中，“a plane ticket”是名词短语：“ticket”是头词，而“a”和“plane”是它的子词。想了解更多信息，请参考《依赖语法与短语结构语法》，见第 185 页。

每个句子中的词只有一个头词。因此，一个词只能是一个头词的子词。而反过来并不总是成立。相同的词可以在多个词对中充当头词。这意味着头词有多个子词。这也解释了为什么依赖标签总是赋给子词。

![image](img/fig2-3.jpg)

*图 2-3：句法依赖弧的图形表示*

dobj 标签被赋给“ticket”这个词，因为它是该关系的子词。依赖标签总是赋给子词。在你的脚本中，你可以使用 Token.head 属性来确定关系的头词。

你可能还想查看句子中其他的头/子关系，例如图 2-4 所示的那些。

![image](img/fig2-4.jpg)

*图 2-4：整个句子中的头/子关系*

如你所见，句子中的相同词可以参与多个句法关系。表 2-2 列出了一些常用的英语依赖标签。

**表 2-2：一些常见的依赖标签**

| **依赖标签** | **描述** |
| --- | --- |
| acomp | 形容词补语 |
| amod | 形容词修饰语 |
| aux | 助动词 |
| compound | 复合词 |
| dative | 与格 |
| det | 限定词 |
| dobj | 直接宾语 |
| nsubj | 名词主语 |
| pobj | 介词宾语 |
| ROOT | 根词 |

ROOT 标签标记的是其主词是自身的词语。通常，spaCy 会将其分配给句子的主谓动词（即谓语动词）。每个完整的句子应有一个带有 ROOT 标签的动词和一个带有 nsubj 标签的主语。其他成分是可选的。

**注意**

*本书中的大部分示例假定提交的文本是一个完整的句子，并使用 ROOT 标签定位句子的主要动词。请记住，这并不适用于所有可能的输入。*

以下脚本演示了如何访问示例中“词性标注”第 21 页中的语篇词语的句法依赖标签：

import spacy

nlp = spacy.load('en')

doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')

for token in doc:

print(token.text, ➊token.pos_, ➋token.dep_)

脚本输出了粗粒度的词性标签➊（参见表 2-1）和分配给示例语篇中词语的依赖标签➋：

I      代词   主语

have   动词   助动词

flown  动词   根词

to     介词   prep

LA     专有名词  宾语

.      标点符号  punct

Now    副词    副词修饰语

I      代词   主语

am     动词   助动词

flying 动词   根词

to     介词   prep

Frisco 专有名词  宾语

.      标点符号  punct

但是，它没有显示的是词语之间是如何通过通常所称的*依赖弧*在句子中相互关联的，这在本节开头已经解释过。要查看示例语篇中的依赖弧，可以将前面脚本中的循环替换为以下内容：

for token in doc:

print(➊token.head.text, token.dep_, token.text)

词语对象的主词属性➊指的是该词语的句法主词。当你打印这一行时，你会看到语篇中的词语是如何通过句法依赖关系相互连接的。如果这些关系以图形方式呈现，你将看到每一行都有一个弧线，唯一例外的是 ROOT 关系。原因是，被分配到这个标签的词是句子中唯一没有主词的词：

flown  主语  I

flown  助动词    have

flown  根词   flown

flown  介词   to

to     宾语   LA

flown  标点符号  .

flying 副词修饰语 Now

flying 主语   I

flying 助动词    am

flying 根词   flying

flying 介词   to

to     宾语   Frisco

flying 标点符号  .

看一下前面列出的句法依赖关系，让我们试着弄清楚哪些标签指向的词语可以最好地描述顾客的意图：换句话说，你需要找到一个可以单独恰当地描述顾客意图的词对。

你可能对带有 ROOT 和 pobj 依赖标签的词语感兴趣，因为在这个示例中，它们在意图识别中起着关键作用。如前所述，ROOT 标签标记的是句子的主要动词，而 pobj 在这个例子中标记的是与动词一起总结整个话语含义的实体。

以下脚本查找分配给这两个依赖标签的单词：

import spacy

nlp = spacy.load('en')

doc = nlp(u'我已经飞到了洛杉矶。现在我正飞往旧金山。')

➊ for sent in doc.sents:

➋ print([w.text for w in sent ➌if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])

在这个脚本中，你*将话语分解* ➊，通过 doc.sents 属性将句子分开，它会遍历文档中的每个句子。将文本分解成独立的句子在你需要找到某些句子中的特定词性时非常有用（我们将在下一章讨论 doc.sents，你将看到如何通过句子级索引引用文档中的标记）。这使你能够根据分配给标记的特定依赖标签，为每个句子创建一个潜在关键词的列表 ➋。这个示例中使用的过滤条件是根据前一个脚本生成的句法相关词对的检查来选择的。特别是，你会挑选出标记为 ROOT 和 pobj 的词 ➌，因为这些词构成了你感兴趣的词对。

脚本的输出应如下所示：

['飞到', '洛杉矶']

['飞往', '旧金山']

在这两个句子对中，输出的名词是标记为 pobj 的那些词。你可以在你的票务应用程序中使用这个方法来选择与动词最匹配的名词。在这种情况下，应该选择“飞往”，它与“旧金山”搭配。

这是一个简化的使用依赖标签进行信息提取的示例。在接下来的章节中，你将看到更多复杂的示例，展示如何遍历一个句子或甚至整个话语的依赖树，提取所需的信息。

#### ***试试这个***

现在你已经知道如何利用词形还原、词性标注和句法依赖标签，你可以将它们结合起来做一些有用的事情。尝试将前面各节的示例合并为一个正确识别飞往旧金山的意图的脚本。

你的脚本应该生成以下输出：

['飞往', '旧金山']

为了实现这一点，从本节中最新的脚本开始，并增强循环中的条件语句，添加条件以处理细粒度的词性标签，如在“词性标注”中第 21 页所讨论的那样。然后将词形还原功能添加到脚本中，如在“词形还原”中第 18 页所讨论的那样。

#### ***命名实体识别***

*命名实体*是可以通过专有名词来指代的实际对象。它可以是人、组织、地点或其他实体。命名实体在自然语言处理中很重要，因为它们揭示了用户谈论的地点或组织。以下脚本在前面示例中使用的样本文本中找到命名实体：

import spacy

nlp = spacy.load('en')

doc = nlp(u'我已经飞到洛杉矶。现在我正飞往旧金山。')

for token in doc:

➊ if token.ent_type != 0:

print(token.text, ➋token.ent_type_)

如果一个词语的 ent_type 属性没有被设置为 0 ➊，那么该词语就是一个命名实体。如果是这样，你将打印该词语的 ent_type_ 属性 ➋，它包含该命名实体的类型（以 unicode 表示）。因此，脚本应输出以下内容：

洛杉矶     GPE

旧金山 GPE

洛杉矶和旧金山都被标记为 GPE，这是“地理政治实体”的缩写，包括国家、城市、州和其他地名。

### **总结**

在本章中，你为使用 spaCy 设置了工作环境。然后，你学习了简单的脚本，展示了如何使用 spaCy 的功能来执行提取重要信息的基本 NLP 操作。这些操作包括分词、词形还原以及识别句子中各个词语之间的句法关系。本章提供的示例被简化了，并不反映现实世界的场景。为了编写更复杂的脚本使用 spaCy，你需要实现一个算法，从依赖树中派生出所需的词语，利用分配给词语的语言特征。我们将在第四章回到提取和使用语言特征的话题，并将在第六章详细讨论依赖树。

在下一章中，你将了解 spaCy API 的关键对象，包括容器和处理管道组件。此外，你还将学习如何使用 spaCy 的 C 级数据结构和接口创建能够处理大量文本的 Python 模块。
