## **6

寻找模式和遍历依赖树**

![Image](img/comm1.jpg)

如果你希望应用程序能够对文本进行分类，提取其中的特定短语，或者确定它与另一篇文本的语义相似度，它必须能够“理解”用户提交的句子并生成有意义的响应。

你已经学习了一些执行这些任务的技巧。本章讨论另外两种方法：使用词序列模式进行文本分类和生成，以及遍历句子的句法依赖树来提取所需的信息。我将向你介绍 spaCy 的 Matcher 工具来寻找模式。我还会讨论在何时你仍然需要依赖上下文来决定正确的处理方法。

### **词序列模式**

一个*词序列模式*由词语的特征组成，这些特征对序列中每个词语提出了特定要求。例如，短语“I can”将匹配以下词序列模式：“代词 + 情态动词”。通过寻找词序列模式，你可以识别出具有相似语言特征的词序列，从而能够对输入进行分类并妥善处理。

例如，当你收到一个以“情态动词 + 专有名词”这一模式开头的问题时，例如“Can George”，你就知道这个问题是在询问某个或某物的能力、可能性、许可或义务，这个专有名词所指代的对象。

在接下来的章节中，你将通过识别语言特征的常见模式来学习如何分类句子。

#### ***基于语言特征的模式识别***

我们需要在文本中寻找模式，因为在大多数情况下，我们无法在文本中找到两个完全相同的句子。通常，一篇文本由不同的句子组成，每个句子包含不同的单词。为每个句子编写处理代码将是非常不现实的。

幸运的是，一些看似完全不同的句子可能遵循相同的词序列模式。例如，考虑以下两个句子：“我们可以超过他们。”和“你必须指定它。”这两个句子没有共同的词语。但如果你看一下句子中词语所分配的句法依赖标签，就会发现一个模式，如以下脚本所示：

import spacy

nlp = spacy.load('en')

doc1 = nlp(u'我们可以超过他们。')

doc2 = nlp(u'你必须指定它。')

➊ for i in range(len(doc1)-1):

➋ 如果 doc1[i].dep_ == doc2[i].dep_：

➌ print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))

因为两个句子的词数相同，我们可以在一个循环中遍历两个句子中的单词 ➊。如果在两个句子中，具有相同索引的单词的依赖标签相同 ➋，我们就会打印这些单词及其对应的标签，以及每个标签的描述 ➌。

输出应如下所示：

我们      你      nsubj  主语

can      must    aux    助动词

overtake specify ROOT   无

them     it      dobj   直接宾语

如你所见，依赖标签的列表对于这两个句子是相同的。这意味着这两个句子遵循相同的词序模式，基于以下句法依赖标签：“主语 + 助动词 + 动词 + 直接宾语”。

还要注意，词性标签（粗粒度和细粒度）的列表对于这些示例句子也是相同的。如果我们将前面脚本中所有对 .dep_ 的引用替换为 .pos_，我们将得到以下结果：

我们      你      PRON  代词

can      must    VERB  动词

overtake specify VERB  动词

them     it      PRON  代词

这些示例句子不仅匹配了句法依赖标签模式，还匹配了词性标签的模式。

#### ***尝试这个***

在前面的例子中，我们创建了两个 Doc 对象——每个示例句子一个。但实际上，一篇文本通常包含多个句子，这使得按句子创建 Doc 对象的方法不太实用。请重写脚本，使其创建一个单一的 Doc 对象。然后使用在第二章中介绍的 doc.sents 属性来处理每个句子。

但请注意，doc.sents 是一个生成器对象，这意味着它是不可下标访问的——你不能通过索引来引用它的项。为了解决这个问题，可以将 doc.sents 转换为列表，如下所示：

sents = list(doc.sents)

当然，你还可以在 for 循环中迭代 doc.sents，以便按顺序获取句子，正如它们在循环中被请求的那样。

#### ***检查言论是否符合模式***

在前面的例子中，我们比较了两个示例句子，以便找到基于它们共享的语言特征的模式。但实际上，我们很少需要将句子相互比较以确定它们是否共享一个共同的模式。相反，将已提交的句子与我们已经感兴趣的模式进行对比会更有用。

例如，假设我们试图在用户输入中找到表达以下之一的言论：能力、可能性、许可或义务（与描述已发生、正在发生或定期发生的实际行为的言论相对）。例如，我们想找到“I can do it.”，而不是“I’ve done it.”。

为了区分不同的言论，我们可以检查某个言论是否满足以下模式：“主语 + 助动词 + 动词 + … + 直接宾语 …”。省略号表示直接宾语不一定紧跟在动词后面，这使得这个模式与前面例子中的模式稍有不同。

以下句子满足这个模式：“I might send them a card as a reminder.”。在这个句子中，名词“card”是直接宾语，而代词“them”是间接宾语，将其与动词“send”分开。这个模式没有指定直接宾语在句子中的位置；它只是要求有直接宾语的存在。

图 6-1 显示了此设计的图形表示：

![image](img/fig6-1.jpg)

*图 6-1：根据语言特征检查提交的语句是否符合词序列模式*

在以下脚本中，我们定义了一个实现该模式的函数，然后在一个示例句子上测试它：

import spacy

nlp = spacy.load('en')

➊ def dep_pattern(doc):

➋ for i in range(len(doc)-1):

➌ if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and

doc[i+2].dep_ == 'ROOT':

➍ for tok in doc[i+2].children:

if tok.dep_ == 'dobj':

➎ return True

➏ return False

➐ doc = nlp(u'我们可以超越他们。')

if ➑dep_pattern(doc):

print('找到')

else:

print('未找到')

在这个脚本中，我们定义了 dep_pattern 函数，它接受一个 Doc 对象作为参数 ➊。在函数中，我们遍历 Doc 对象的标记 ➋，寻找“主语 + 助动词 + 动词”模式 ➌。如果找到该模式，我们检查动词是否有直接宾语作为其语法子节点 ➍。最后，如果找到直接宾语，函数返回 True ➎。否则，它返回 False ➏。

在主代码中，我们将文本处理管道应用于示例句子 ➐ 并将 Doc 对象传递给 dep_pattern 函数 ➑，如果示例符合函数中实现的模式，则输出 Found，否则输出 Not found。

因为此示例中使用的示例符合该模式，所以脚本应产生以下输出：

找到

在接下来的几节中，你将看到如何使用 dep_pattern 函数的一些示例。

#### ***使用 spaCy 的 Matcher 查找词序列模式***

在上一节中，你学习了如何通过遍历文档中的标记并检查它们的语言特征来查找词序列模式。实际上，spaCy 为此任务提供了一个预定义的功能，称为 *Matcher*，它是一个专门设计用于根据模式规则查找标记序列的工具。例如，使用 Matcher 实现“主语 + 助动词 + 动词”模式可能看起来像这样：

import spacy

from spacy.matcher import Matcher

nlp = spacy.load("en")

➊ matcher = Matcher(nlp.vocab)

➋ pattern = [{"DEP": "nsubj"}, {"DEP": "aux"}, {"DEP": "ROOT"}]

➌ matcher.add("NsubjAuxRoot", None, pattern)

doc = nlp(u"我们可以超越他们。")

➍ matches = matcher(doc)

➎ for match_id, start, end in matches:

span = doc[start:end]

➏ print("Span: ", span.text)

print("在 doc 中的位置是：", start, "-", end)

我们创建一个 Matcher 实例，将与文档共享的词汇对象传递给它 ➊。然后，我们定义一个模式，指定单词序列应匹配的依赖标签 ➋。我们将新创建的模式添加到 Matcher 中 ➌。

接下来，我们可以将 Matcher 应用到示例文本中，并获取匹配的标记列表 ➍。然后我们遍历这个列表 ➎，打印出匹配模式标记在文本中的起始和结束位置 ➏。

脚本应产生以下输出：

Span: 我们可以超越

doc 中的位置是：0 - 3

Matcher 允许你在文本中找到模式，而不需要显式地遍历文本的每个标记，从而隐藏了实现细节。因此，你可以获得组成满足指定模式的序列的单词的起始和结束位置。这种方法在你对紧接在一起的单词序列感兴趣时非常有用。

但通常你需要一个包括散布在整个句子中的单词的模式。例如，你可能需要实现类似于我们在“检查句子模式”中使用的“主语 + 助动词 + 动词 + . . . + 直接宾语 . . .”的模式。问题是，你无法提前知道“主语 + 助动词 + 动词”序列和直接宾语之间可能出现多少个单词。Matcher 不允许你定义这样的模式。出于这个原因，本章其余部分我将手动定义模式。

#### ***应用多个模式***

你可以应用多个匹配模式来检查一句话，确保它符合所有条件。例如，你可以将一句话与两个模式进行比较：一个实现依赖标签序列（如在“检查句子模式”中讨论的，见第 77 页）和一个检查词性标签序列的模式。如果你想确保句子中的直接宾语是人称代词，这可能会很有帮助。这样，你就可以开始确定赋予代词意义的名词，并且该名词在对话中其他地方有所提及。

在图示上，这种设计可能看起来像图 6-2。

![image](img/fig6-2.jpg)

*图 6-2：将多个匹配模式应用于用户输入*

除了使用在“检查句子模式”中定义的依赖标签序列外，你还可以通过实现一个基于词性标签的模式来定义一个新函数。词性标签模式可能会检查句子，确保主语和直接宾语是人称代词。这个新函数可能会实现如下模式：“人称代词 + 情态助动词 + 基本形式动词 + . . . + 人称代词 . . .”。

这是代码：

导入 spacy

nlp = spacy.load('en')

# 在此插入之前列表中的 dep_pattern 函数

#...

➊ 定义 pos_pattern(doc)：

➋ 对于 token 在 doc 中：

如果 token.dep_ == 'nsubj' 并且 token.tag_ != 'PRP'：

返回 False

如果 token.dep_ == 'aux' 并且 token.tag_ != 'MD'：

返回 False

如果 token.dep_ == 'ROOT' 并且 token.tag_ != 'VB'：

返回 False

如果 token.dep_ == 'dobj' 并且 token.tag_ != 'PRP'：

返回 False

➌ 返回 True

# 测试代码

doc = nlp(u'我们可以超过他们。')

➍ 如果 dep_pattern(doc) 和 pos_pattern(doc)：

打印('已找到')

其他情况：

打印('未找到')

我们首先添加在前一个脚本中定义的 dep_pattern 函数的代码。为了创建第二个模式，我们定义了 pos_pattern 函数 ➊，其中包含一个带有一系列 if 语句的 for 循环 ➋。每个 if 语句检查句子中的某个部分是否与特定的词性标签匹配。当函数检测到不匹配时，它返回 False。否则，在所有检查完成且没有检测到不匹配时，函数返回 True ➌。

为了测试这些模式，我们将管道应用于一个句子，然后检查该句子是否同时匹配两个模式 ➍。由于本示例中使用的样本同时匹配两个模式，我们应该看到以下输出：

已找到

但是，如果我们将示例句子替换为：“I might send them a card as a reminder.”，我们应该看到以下输出：

未找到

原因在于，句子没有匹配词性标签模式，因为直接宾语“card”不是一个人称代词，即使该句子完全满足第一个模式的条件。

#### ***基于自定义特征创建模式***

在创建词序列模式时，你可能需要通过定制语言特征来增强 spaCy 提供的功能，以满足你的需求。例如，你可能希望上面的脚本识别另一个模式，根据代词的数目（单数或复数）来区分代词。这在你需要找出代词所指代的前述名词时非常有用。

虽然 spaCy 通过数目区分名词，但对于代词并不如此。但识别代词是单数还是复数的能力在语义识别或信息提取任务中非常有用。例如，考虑以下话语：

卡车正在缓慢行驶。我们可以超越它们。

如果我们能够确认第二个句子中的直接宾语“them”是复数代词，那么我们有理由相信它指代第一个句子中的复数名词“trucks”。我们经常使用这个技巧根据上下文来识别代词的意义。

以下脚本定义了一个 pron_pattern 函数，该函数查找提交句子中的任何直接宾语，确定该直接宾语是否为人称代词，并进一步确定该代词是单数还是复数。然后，脚本将函数应用于一个样本句子，并测试在第 77 页中的“检查话语的模式”和第 80 页中的“应用多个模式”中定义的两个模式。

import spacy

nlp = spacy.load('en')

#插入之前的 dep_pattern 和 pos_pattern 函数

这里列出

#...

➊ def pron_pattern(doc):

➋ plural = ['we','us','they','them']

for token in doc:

➌ if token.dep_ == 'dobj' and token.tag_ == 'PRP':

➍ if token.text in plural:

➎ return 'plural'

否则：

➏ 返回 'singular'

➐ 返回 '未找到'

doc = nlp(u'我们可以超过他们。')

如果 dep_pattern(doc) 和 pos_pattern(doc):

print('找到：', '直接宾语位置的代词是',

pron_pattern(doc))

否则：

print('未找到')

我们首先通过在脚本中添加在“检查句子是否符合模式”和“应用多个模式”中定义的 dep_pattern 和 pos_pattern 函数开始。在 pron_pattern 函数 ➊ 中，我们定义了一个包含所有可能的复数人称代词的 Python 列表 ➋。接下来，我们定义一个循环，遍历提交句子中的词项，寻找作为直接宾语的人称代词 ➌。如果我们找到了这样的词项，我们就检查它是否在复数人称代词的列表中 ➍。如果是，那么函数返回复数 ➎。否则，它返回单数 ➏。如果函数未能检测到直接宾语，或者找到了一个不是人称代词的直接宾语，它将返回未找到 ➐。

对于句子“我们可以超过他们。”，我们应该得到以下输出：

找到：直接宾语位置的代词是复数

我们可以利用这些信息来找到前一句中的代词对应的名词。

#### ***选择应用哪些模式***

一旦你定义了这些模式，你可以根据每种情况选择应用哪些模式。注意，即使一个句子未能完全满足 dep_pattern 和 pos_pattern 函数，它仍然可能匹配 pron_pattern 函数。例如，句子“我知道它。”不匹配 dep_pattern 或 pos_pattern 函数，因为它没有情态助动词。但它满足 pron_pattern，因为它包含一个作为句子直接宾语的人称代词。

这些模式之间的松散耦合使你可以将它们与其他模式一起使用，或者独立使用。例如，如果你想确保句子中的主语和直接宾语是名词，你可以将 dep_pattern（检查句子是否符合“主语 + 助动词 + 动词 + . . . + 直接宾语 . . .”模式）与“名词 + 情态助动词 + 基本形式动词 + . . . + 名词 . . .”模式结合使用。这两个模式将匹配以下示例：

开发者可能会遵循这个规则。

正如你可能猜到的，能够以不同方式组合模式使你可以用更少的代码处理更多场景。

#### ***在聊天机器人中使用词序列模式生成陈述***

如前所述，NLP 中最具挑战性的任务是理解和生成自然语言文本。聊天机器人必须理解用户的输入，并生成适当的回应。基于语言学特征的词序列模式可以帮助你实现这些功能。

在第四章中，你学习了如何将陈述句转化为相关问题，从而继续与用户对话。通过使用词序模式，你还可以生成其他类型的响应，例如相关的陈述句。

假设你的聊天机器人收到以下用户输入：

符号显然是可以区分的。我也能迅速识别它们。

聊天机器人可能会做出如下反应：

我也能迅速识别符号。

你可以使用前面部分实现的模式来完成这个文本生成任务。步骤列表可能如下所示：

1.  检查对话输入是否符合之前定义的`dep_pattern`和`pos_pattern`函数，分别找出符合“主语 + 助动词 + 动词 + ... + 直接宾语...”以及“代词 + 情态动词 + 基本动词 + ... + 代词...”模式的表达。

1.  检查步骤 1 中找到的表达式是否符合`pron_pattern`模式，以确定直接宾语代词是单数还是复数。

1.  通过搜索与人称代词具有相同数量的名词，找出赋予代词意义的名词。

1.  用步骤 3 中找到的名词替换步骤 1 中作为直接宾语的代词。

1.  在生成的句子末尾添加“too”一词。

以下脚本实现了这些步骤。它使用了本章前面定义的`dep_pattern`、`pos_pattern`和`pron_pattern`函数（为节省空间，省略了它们的代码）。它还引入了两个新函数：`find_noun`和`gen_utterance`。为了方便，我们将分三个步骤讲解代码：初步操作和`find_noun`函数，用于找到与人称代词匹配的名词；`gen_utterance`函数，用于从问题生成相关的陈述；最后是测试表达的代码。以下是第一部分：

导入 spacy

nlp = spacy.load('en')

#从...插入`dep_pattern`、`pos_pattern`和`pron_pattern`函数

这里的前面列表

#...

➊ def find_noun(➋sents, ➌num):

如果 num == '复数':

➍ taglist = ['NNS','NNPS']

如果 num == '单数':

➎ taglist = ['NN','NNP']

➏ for sent in reversed(sents):

➐ for token in sent:

➑ 如果 token.tag_ 在 taglist 中:

返回 token.text

返回 '未找到名词'

在插入`dep_pattern`、`pos_pattern`和`pron_pattern`函数的代码之后，我们定义了`find_noun`函数，它需要两个参数➊。第一个参数包含从话语开头到满足所有模式的句子的句子列表。在这个例子中，这个列表将包含所有话语中的句子，因为只有最后一个句子满足所有模式➋。但是，赋予代词意义的名词可能出现在前面的某个句子中。

传递给 find_noun 的第二个参数是满足所有模式的直接宾语代词在句子中的位置 ➌。pron_pattern 函数决定了这一点。如果这个参数的值是“复数”，我们定义一个包含 spaCy 用来标记复数名词的细粒度词性标签的 Python 列表 ➍。如果它是“单数”，我们创建一个包含用来标记单数名词的细粒度词性标签的标签列表 ➎。

在一个 for 循环中，我们逆序遍历句子，从距离包含要替换的人称代词的句子最近的句子开始 ➏。我们从最接近的句子开始，因为我们要找的名词最有可能出现在那里。这里，我们使用 Python 的 reversed 函数，它返回一个反向迭代器。在内层循环中，我们遍历每个句子中的标记 ➐，寻找一个其细粒度的词性标签在之前定义的标签列表中的标记 ➑。

然后我们定义了 gen_utterance 函数，它生成我们新的语句：

定义 gen_utterance 函数(doc, noun)：

sent = ''

➊ 对于 i，token 在 enumerate(doc) 中：

➋ 如果 token.dep_ == 'dobj' 且 token.tag_ == 'PRP'：

➌ sent = doc[:i].text + ' ' + 名词 + ' ' + doc[i+1:len(doc)-2].text + 'too.'

➍ 返回 sent

➎ 返回 '生成语句失败'

我们使用一个 for 循环遍历句子 ➊ 中的标记，寻找一个是人称代词的直接宾语 ➋。一旦找到，我们就生成一个新的语句。我们通过将人称代词替换为匹配的名词，并在其后附加“too”来修改原始句子 ➌。然后函数返回这个新生成的语句 ➍。如果没有找到人称代词形式的直接宾语，函数会返回一个错误信息 ➎。

现在我们已经有了所有的函数，可以用以下代码在示例语句上进行测试：

➊ doc = nlp(u'这些符号是明显可区分的。我能识别它们。')

promptly.')

➋ sents = list(doc.sents)

response = ''

名词 = ''

➌ 对于 i，sent 在 enumerate(sents) 中：

如果 dep_pattern(sent) 和 pos_pattern(sent)：

➍ 名词 = find_noun(sents[:i], pron_pattern(sent))

如果名词 != '未找到名词'：

➎ response = gen_utterance(sents[i], noun)

break

print(response)

在将管道应用于示例语句 ➊ 后，我们将其转换为句子列表 ➋。然后我们遍历这个列表 ➌，寻找符合 dep_pattern 和 pos_pattern 函数定义的模式的句子。接下来，我们使用 find_noun 函数确定在前一步找到的句子中赋予代词意义的名词 ➍。最后，我们调用 get_utterance 函数生成回应语句 ➎。

上述代码的输出应如下所示：

我也能识别符号。

#### ***尝试这个***

请注意，前面代码仍有改进的空间，因为原始句子在名词 "symbols" 前面包含了冠词 "the"。一个更好的输出应该在名词前面也加入这个冠词。为了生成一个在这个上下文中最有意义的句子，可以扩展脚本，使其在名词前加上冠词 "the"，变成 "I can recognize the symbols too"。为此，你需要检查名词前是否有冠词，然后加上那个冠词。

### **从句法依存树中提取关键词**

在话语中找到符合某种模式的词序，可以帮助你构造一个语法正确的回应——无论是陈述句还是疑问句，都是基于提交的文本。但这些模式并不总是有助于提取文本的含义。

例如，在第二章的机票预定应用中，用户可能会提交如下句子：

我需要一张去柏林的机票。

你可以通过搜索模式 "to + GPE" 来轻松找到用户的预期目的地，其中 GPE 是指国家、城市和州的命名实体。这个模式可以匹配类似 "to London"（去伦敦）、"to California"（去加利福尼亚）等短语。

但假设用户提交了以下其中一个话语：

我将去柏林参加会议。我需要一张机票。

我将去柏林参加会议。我想

预定一张机票。

如你所见，"to + GPE" 的模式在这两个例子中都无法找到目的地。在这两种情况下，"to" 直接指的是 "the conference"（会议），而不是柏林。你需要类似 "to + . . . + GPE" 的模式。但你如何知道在 "to" 和 "GPE" 之间需要或允许什么呢？例如，以下句子包含了 "to + . . . + GPE" 模式，但与预定去柏林的机票无关：

我想预定一张直飞不经柏林的机票。

经常需要检查句子中单词之间的关系，以获得必要的信息。这时候，遍历句子的依存树可能会非常有帮助。

遍历依存树意味着按自定义顺序遍历——不一定是从第一个标记到最后一个标记。例如，你可以在找到所需的组成部分后立即停止遍历依存树。记住，句子的依存树展示了单词对之间的句法关系。我们通常用箭头连接关系的主词和子词来表示这些关系。句子中的每个词都参与至少一个关系。这保证了你在遍历整个为该句子生成的依存树时，如果从 ROOT 开始，就会经过句子中的每个单词。

在这一节中，我们将分析句子的结构，以便了解用户的意图。

#### ***遍历依存树进行信息提取***

让我们回到票务预订应用程序的例子。为了找出用户的预期目的地，你可能需要遍历句子的依赖树，以确定“to”是否与“Berlin”在语义上相关。如果你记住了构成依赖树的头/子关系（如在第 25 页的“头与子”框中介绍的那样），这将变得很容易实现。

图 6-3 展示了句子“I am going to the conference in Berlin”的依赖树：

![image](img/fig6-3.jpg)

*图 6-3：话语的句法依赖树*

动词“going”是句子的根词，意味着它不是任何其他词的子节点。它右侧的直接子词是“to”。如果你按照依赖树进行遍历，每次走到每个单词的直接右子节点，最终你会到达“Berlin”。这表明句子中“to”和“Berlin”之间存在语义连接。

#### ***遍历词的头词***

现在让我们弄清楚如何以编程方式表达句子中“to”和“Berlin”之间的关系。一种方法是从左到右遍历依赖树，从“to”开始，在过程中每次选择每个单词的直接右子节点。如果你可以通过这种方式从“to”走到“Berlin”，那么可以合理地假设这两个词之间存在语义连接。

但这种方法有一个缺点。在某些情况下，一个词可能有多个右子节点。例如，在句子“I am going to the conference on spaCy, which will be held in Berlin”中，词“conference”有两个直接的右子节点：“on”和“held”。这迫使你检查多个分支，增加了代码的复杂性。

另一方面，虽然一个头词可以有多个子节点，但句子中的每个词都有且只有一个头词。这意味着你可以从右到左移动，从“Berlin”开始，试图到达“to”。以下脚本在 det_destination 函数中实现了这一过程：

import spacy

nlp = spacy.load('en')

# 这是确定目的地的函数

➊ def det_destination(doc):

for i, token in enumerate(doc):

➋ if token.ent_type != 0 and token.ent_type_ == 'GPE':

➌ while True:

➍ token = token.head

if token.text == 'to':

➎ return doc[i].text

➏ if token.head == token:

return '无法确定'

return '无法确定'

# 测试 det_destination 函数

doc = nlp(u'I am going to the conference in Berlin.')

➐ dest = det_destination(doc)

print('看起来用户想要订票去' + dest)

在 det_destination 函数➊中，我们遍历提交的语句中的标记，寻找一个 GPE 实体➋。如果找到了，我们启动一个 while 循环➌，该循环遍历每个标记的头部，从包含 GPE 实体的标记开始 ➍。当它到达包含“to”的标记➎或句子的根节点时，循环停止。我们可以通过将标记与其头部进行比较来检查根节点，因为根节点的头部总是指向它自身。（或者，我们可以检查 ROOT 标签。）

为了测试这个功能，我们将管道应用于示例句子，并在其上调用 det_destination 函数➐。

脚本应生成以下输出：

看起来用户想要一张前往柏林的机票。

如果我们修改示例句子，使其不包含“to”或 GPE 命名实体，我们应该得到以下输出：

看起来用户想要一张前往“未能确定”的机票。

我们可以改进脚本，使其在未能确定用户的目的地时，使用另一条消息。

#### ***使用依赖树简化文本***

语法依赖树方法当然不仅仅局限于聊天机器人。例如，你可以在报告处理应用中使用它。假设，你需要开发一个应用程序，必须通过提取最重要的信息来精简零售报告。

例如，你可能想要选择包含数字的句子，生成关于销售量、收入和成本的简明摘要。（你在第四章学习了如何提取数字。）然后，为了使你的新报告更简洁，你可能会缩短选择的句子。

作为一个快速示例，考虑以下句子：

该产品销量在第一季度创下新纪录，售出 1860 万件。

处理后，它应该是这样的：

产品销量达到 1860 万件。

为了实现这一点，你可以通过以下步骤分析句子的依赖树：

1.  通过从包含数字的标记开始，按从左到右的顺序，遍历标记的头部，提取包含该数字的整个短语（在本例中是 18.6）。

1.  从提取的短语的主要词（其头部在短语之外的词）开始，遍历依赖树，直到到达句子的主要动词，通过遍历头部并将其收集到新的句子中。

1.  拿到主要动词的主语，以及其左侧的子节点，通常包括限定词，可能还有其他修饰词。

图 6-4 表示此过程。

![image](img/fig6-4.jpg)

*图 6-4：一个仅包含重要元素的简化句子的示例*

让我们从第一步开始，在这个例子中，应该提取出“1860 万件售出”这个短语。以下代码片段演示了如何以编程方式做到这一点：

doc = nlp(u"该产品销量在第一季度创下新纪录，售出 1860 万件。")

phrase = ''

对于 doc 中的每一个 token：

➊ 如果 token.pos_ == 'NUM'：

while True:

phrase = phrase + ' ' + token.text

➋ token = token.head

➌ 如果 token 不在 token.head.lefts 列表中：

phrase = phrase + ' ' + token.text

➍ break

➎ break

print(phrase.strip())

我们遍历句子的 tokens，寻找一个代表数字的 token ➊。若找到，我们开始一个 while 循环，遍历右侧的头部 ➋，从数字 token 开始，然后将每个头部的文本附加到 phrase 变量，形成一个新的短语。为了确保下一个 token 的头部位于该 token 的右侧，我们检查 token 是否在其头部左侧子节点的列表中 ➌。一旦这个条件返回 false，我们就退出 while 循环 ➍，然后退出外部的 for 循环 ➎。

接下来，我们从包含数字的短语的主词开始，遍历 token 的头部，直到到达句子的主谓动词（在此例中为“hit”），排除介词（在此例中为“with”）。我们可以像下面这样实现：

while True:

➊ token = doc[token.i].head

如果 token.pos_ != 'ADP'：

➋ phrase = token.text + phrase

➌ 如果 token.dep_ == 'ROOT'：

➍ break

我们在 while 循环中遍历 token 的头部 ➊，将每个头部的文本附加到正在形成的短语中 ➋。到达主谓动词（标记为 ROOT）后 ➌，我们退出循环 ➍。

最后，我们提取句子的主语及其左侧子节点：“The”和“product”。在这个例子中，主语是“sales”，因此我们提取以下名词短语：“The product sales”。这可以通过以下代码实现：

➊ 对于 token.lefts 中的每一个 tok：

➋ 如果 tok.dep_ == 'nsubj'：

➌ phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' '

+ phrase

break

➍ print(phrase)

我们首先遍历主谓动词的子节点 ➊，寻找主语 ➋。然后我们将主语的子节点和短语的主语加到前面 ➌。为了查看生成的短语，我们打印它 ➍。

输出应该是这样的：

产品销售达到了 1860 万件。

结果是一个简化版的原句。

#### ***试试看***

编写一个脚本，通过提取包含金额信息的句子来简化财务报告。此外，脚本还需要将选中的句子简化，只保留主语、谓语动词、表示金额的短语以及从金额短语的主词开始，直到句子主谓动词的过程中可以获取的 token。例如，给定以下句子：

公司今年的利润创下了新高，主要归因于

对于管理层的变动，收入总额为 426 万美元。

你的脚本应该返回这句话：

公司获得了 426 万美元的收入。

在这个例子中，“million”是短语“$4.26 million”中的主要单词。“million”的主词是“of”，它是“revenue”的子词，而“revenue”又是“earned”的子词，后者是句子的主要动词。

### **利用上下文提升票务预订聊天机器人的功能**

如你所料，所有智能文本处理任务都没有单一的解决方案。例如，本章前面展示的票务预订脚本只有在提交的句子中包含“to”一词时，才会找到目的地。

提高这些脚本有用性的一个方法是考虑上下文，从而确定合适的响应。让我们增加票务预订脚本的功能，使其能够处理更广泛的用户输入，包括那些不包含任何“to + GPE”组合的语句。例如，看看以下的语句：

我将在柏林参加会议。

在这里，用户表达了要去柏林的意图，但没有使用“to”一词。句子中只有 GPE 实体“柏林”。在这种情况下，聊天机器人提出一个确认性问题是合理的，比如以下这个：

你是想要一张去柏林的票，对吧？

改进后的票务预订聊天机器人应该根据三种不同的情况输出不同的结果：

+   用户明确表示希望预定前往某个目的地的票。

+   目前还不清楚用户是否想要预定提到的目的地的票。

+   用户没有提到任何目的地。

根据用户输入的类别，聊天机器人生成相应的响应。图 6-5 展示了如何在图表中表示这种用户输入处理方式。

![image](img/fig6-5.jpg)

*图 6-5：票务预订应用中用户输入处理的示例*

以下脚本实现了这个设计。为了方便，代码被分为几个部分。

第一个代码片段包含 guess_destination 函数，该函数在句子中搜索 GPE 实体。此外，我们还需要插入在“Iterating Over the Heads of Tokens”一节中定义和讨论的 dep_destination 函数，见 第 87 页。回顾一下，这个函数用于在句子中搜索“to + GPE”模式。我们将需要 dep_destination 和 guess_destination 函数来处理用户输入的第一和第二种场景。

import spacy

nlp = spacy.load('en')

#在这里插入之前列表中的 dep_destination 函数

#...

def guess_destination(doc):

for token in doc:

➊ if token.ent_type != 0 and token.ent_type_ == 'GPE':

➋ return token.text

➌ return '无法确定'

guess_destination 函数中的代码遍历句子中的词元，查找 GPE 实体 ➊。找到后，函数将其返回给调用代码 ➋。如果没有找到，函数返回“无法确定” ➌，意味着句子中没有 GPE 实体。

在下面的 `gen_function` 中，我们基于前面代码段中定义的函数返回的内容来生成回应。

def gen_response(doc):

➊ dest = det_destination(doc)

if dest != '无法确定':

➋ return '你需要什么时候到 ' + dest + '？'

➌ dest = guess_destination(doc)

if dest != '无法确定':

➍ return '你想要一张到 ' + dest + ' 的机票，对吧？'

➎ return '你要飞到某个地方吗？'

`gen_response` 函数中的代码首先调用 `det_destination` 函数 ➊，该函数确定一个话语中是否包含“to + GPE”对。如果找到了，我们假设用户想要一张到目的地的机票，并且需要澄清他们的出发时间 ➋。

如果 `det_destination` 函数没有找到话语中的“to + GPE”对，我们会调用 `guess_destination` 函数 ➌。这个函数试图找到一个 GPE 实体。如果找到该实体，它会询问用户是否想飞往该目的地 ➍。否则，如果话语中没有找到 GPE 实体，脚本会询问用户是否要飞往某个地方 ➎。

要测试代码，我们将管道应用于一个句子，然后将文档发送到我们在前一段代码中使用的 `gen_response` 函数：

doc = nlp(u'我将前往柏林参加会议。')

print(gen_response(doc))

对于这个示例中的话语，你应该看到以下输出：

你需要什么时候到柏林？

你可以试验这个示例话语，以查看不同的输出。

### **通过寻找合适的修饰词来制作更智能的聊天机器人**

让你的聊天机器人更智能的一种方法是使用依赖树来查找特定单词的修饰词。例如，你可以教你的应用程序识别适用于给定名词的形容词。然后你可以告诉机器人：“我想读一本书”，智能机器人可能会这样回答：“你想读一本小说吗？”

*修饰词* 是短语或从句中的可选成分，用来改变其他成分的意思。去掉修饰词通常不会改变句子的基本意思，但会使句子变得不那么具体。举个例子，考虑以下两句话：

我想读一本书。

我想读一本关于 Python 的书。

第一条句子没有使用修饰词。第二条句子使用了修饰词“on Python”，使你的请求更为详细。

如果你想要更具体，你必须使用修饰词。例如，为了生成一个恰当的回应，你可能需要学习哪些修饰词可以与某个名词或动词搭配使用。

考虑以下短语：

那个来自非洲的异国水果。

在这个名词短语中，“fruit”是中心词，“that”和“exotic”是*前修饰语*——位于被修饰词前的修饰词，而“from Africa”是*后修饰语*短语——修饰词跟随它所限定或修饰的词。图 6-6 展示了这个短语的依赖树。

![image](img/fig6-6.jpg)

*图 6-6：前置修饰语和后置修饰语的示例*

假设你想确定单词“fruit”的可能形容词修饰语。（形容词修饰语始终是前修饰语。）同时，你还想查看在该词的后置修饰语中可以找到哪些 GPE 实体。这些信息可能在与水果的对话中帮助你生成相关的句子。

以下脚本实现了这个设计：

导入 spacy

nlp = spacy.load('en')

➊ doc = nlp(u"Kiwanos 的果肉像果冻一样，带有令人耳目一新的果味。这个

是一种美味又异国情调的水果，来自非洲，绝对值得一试。")

➋ fruit_adjectives = []

➌ fruit_origins = []

对于 doc 中的每个 token：

➍ 如果 token.text == 'fruit':

➎ fruit_adjectives = fruit_adjectives + [modifier.text for modifier in

token.lefts 如果 modifier.pos_ == 'ADJ']

➏ fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier

在 token.rights 中，如果修饰词的文本是'from'且 doc[modifier.i + 1]是实体类型

type != 0]

print('水果的形容词修饰词列表：', fruit_adjectives)

print('适用于水果的 GPE 名称作为后置修饰词：',

fruit_origins)

我们首先对包含“fruit”一词并同时具有前置和后置修饰语的短文本应用管道 ➊。我们定义了两个空列表：fruit_adjectives ➋ 和 fruit_origins ➌。第一个列表将保存所有为“fruit”找到的形容词修饰语。第二个列表将保存所有在“fruit”后置修饰语中找到的 GPE 实体。

接下来，在一个遍历整个文本的循环中，我们寻找单词“fruit” ➍。一旦找到这个词，我们首先通过选择其左侧的句法子节点并仅选择形容词（限定词和复合词也可以是前置修饰语）来确定它的形容词前修饰词。我们将形容词修饰词添加到 fruit_adjectives 列表中 ➎。

然后，我们通过检查单词“fruit”的右侧句法子节点来搜索后置修饰语。特别地，我们查找命名实体，并将其添加到 fruit_origins 列表中 ➏。

脚本输出以下两个列表：

水果的形容词修饰词列表：['nice', 'exotic']

适用于“fruit”的 GPE 名称作为后置修饰词：['Africa']

现在你的机器人“知道”水果可以是美味的、异国情调的（或者同时既美味又异国情调），并且可能来自非洲。

### **总结**

当你需要处理一句话，甚至只是一个短语时，通常需要查看它的结构，以确定它匹配哪些一般模式。使用 spaCy 的语言学特性，你可以检测这些模式，使你的脚本能够理解用户的意图并做出适当回应。

基于语言特征的模式在你需要识别句子的基本结构时非常有效，这些结构包括主语、情态动词、主要动词和直接宾语。但是，实际应用需要识别更复杂的句子结构，并且要准备处理更广泛的用户输入。这时，句子的句法依赖树就显得非常有用。你可以以不同的方式遍历句子的依赖树，从中提取必要的信息。例如，你可以使用依赖树找到特定单词的修饰语，然后将这些信息在后续生成智能文本时加以利用。
