## **4

提取和使用语言特征**

![Image](img/comm1.jpg)

在前几章中，你已经学习了如何访问语言特征，如词性标签、句法依赖关系和命名实体，这些都是文本处理管道的一部分。本章将展示如何使用词性标签和句法依赖标签来提取和生成文本，从而帮助你构建提问的聊天机器人、在文本中定位特定短语等。

几乎每一个自然语言处理（NLP）应用都需要从文本中提取特定的信息，并生成与特定情境相关的新文本。例如，一个聊天机器人必须能够与用户进行对话，这意味着它必须能够识别用户文本中的特定部分，然后生成相应的回复。让我们来看看如何利用语言特征做到这一切。

### **使用词性标签提取和生成文本**

词性标签可以帮助你从文本中提取特定的信息，它们还可以帮助你基于给定的句子生成全新的句子。在这一部分，我们将介绍一些新的词性标签，编写一个脚本来查找描述金额的短语，并将陈述句转换成疑问句。有关 spaCy 中用于英语模型的常见词性标签的列表，请参阅表 2-1 在第 22 页上的内容。

#### ***数字、符号和标点标签***

除了句子中名词、动词和其他词的词性标签外，spaCy 还为符号、数字和标点符号提供了标签。让我们通过处理以下句子来看看这些标签：

该公司在 2017 年赚取了 150 万美元。

首先，让我们从句子中的词元提取粗粒度的词性特征，以查看 spaCy 如何区分不同的词性类别。我们可以使用以下脚本来实现：

>>> import spacy

>>> nlp = spacy.load('en')

>>> doc = nlp(u"该公司在 2017 年赚取了 150 万美元。")

>>> for token in doc:

...   print(token.text, ➊token.pos_, ➋spacy.explain(token.pos_))

...

我们为提交的句子创建了一个 Doc 对象，然后输出粗粒度的词性标签 ➊。我们还使用了 spacy.explain() 函数，它会返回给定语言特征的描述 ➋。

输出结果应该如下所示：

The     DET   限定词

firm    NOUN  名词

earned  VERB  动词

$       SYM   符号

1.5     NUM   数字

million NUM   数字

in      ADP   介词

2017    NUM   数字

.       PUNCT 标点符号

请注意，粗粒度标签器将数字、符号和标点符号作为独立类别进行区分。正如你所看到的，它甚至能够识别出“million”这个单词的拼写。

现在，为了进行比较，我们将输出这个示例句子的粗粒度和细粒度词性标签，并附上细粒度标签的描述列：

>>> for token in doc:

...   print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))

输出应该如下所示：

The     DET   DT  限定词

firm    NOUN  NN  名词，单数

earned  VERB  VBD  动词，过去式

$       SYM   $   符号，货币

1.5     NUM   CD  基数词

million NUM   CD  基数词

in      ADP   IN  连词，附属或介词

2017    NUM   CD  基数词

.       PUNCT .   标点符号，句末符号

第二列和第三列分别包含粗粒度和细粒度的词性标签。第四列给出了第三列中细粒度标签的描述。

细粒度标注将每个类别划分为子类别。例如，粗粒度类别 SYM（符号）有三个细粒度子类别。它们分别是：$表示货币符号，#表示数字符号，SYM 表示所有其他符号，如 +、−、×、÷、=。这种细分在你需要区分不同类型的符号时非常有用。例如，你可能正在处理数学文章，想让脚本识别数学公式中常见的符号。或者，你可能在编写一个脚本，需要识别财务报告中的货币符号。

**注意**

*因为 spaCy 的词性标注器依赖于标记的上下文来生成其标签，所以在不寻常的上下文中，你可能会得到不同的标签。*

现在让我们看看如何利用这些特定的词性标签来提取和生成文本。

#### ***提取货币描述***

假设你正在开发一个用于处理财务报告的应用程序，该程序必须从冗长、无聊的文本中提取必要的信息。在实践中，财务报告可能非常庞大，但你真正需要的只是数字。特别是，你感兴趣的是指代金额的短语，并且这些短语以货币符号开头。例如，你的脚本应该从前面的示例句子中提取出短语“$1.5 百万”，而不是“2017”。

以下脚本演示了如何仅依赖标记的词性标签从句子中提取这个短语。你可以将这个脚本保存到文件中，然后运行它，或者在 Python 会话中执行代码：

import spacy

nlp = spacy.load('en')

doc = nlp(u"公司在 2017 年赚取了 150 万美元。")

phrase = ''

➊ for token in doc:

➋ if token.tag_ == '$':

phrase = token.text

i = token.i+1

➌ while doc[i].tag_ == 'CD':

phrase += doc[i].text + ' '

i += 1

➍ break

phrase = phrase[:-1]

print(phrase)

我们遍历句子的每个词 ➊，搜索一个精细词性标签为$的词 ➋。这个标签表示一个货币符号，通常开始一个表示金额的短语。一旦找到货币符号，我们就开始通过检查紧随其后的词是否为数字来构建短语。为此，我们实现了一个 while 循环，在该循环中我们取出位于货币符号右侧的词，并检查它们是否有 CD 标签，这个标签是基数数字的精细词性标签 ➌。当我们遇到一个非数字词时，就退出 while 循环并中断遍历句子词的 for 循环 ➍。

当我们运行脚本时，输出应如下所示：

$1.5 million

这正是我们想要的输出结果。

请记住，分配给$的精细词性标签不一定是“$”。这个词性标签可能会标记其他常见的货币符号，比如£和€。例如，前面的脚本会识别短语“£1.500.000”。

#### ***试试看***

我们编写了这个脚本，以从提交的句子中提取一个表示金额的短语。一旦脚本找到了该短语，它就会完成执行。但在实际应用中，可能会有包含多个此类短语的句子，如以下示例：“公司在 2017 年赚取了 150 万美元，与 2016 年相比，赚取了 120 万美元。”

修改脚本，使其能够提取句子中每个表示金额的短语。为了实现这一点，移除 break 语句，以防止在找到第一个感兴趣的短语后就结束循环。然后将负责准备和打印已找到短语的代码（脚本的最后两行）移动到循环中，这样就可以为每个在提交句子中找到的感兴趣的短语调用这两行代码。

#### ***将陈述转化为问题***

假设你的 NLP 应用必须能够根据提交的陈述生成一个问题。例如，聊天机器人维持与用户对话的一种方式是通过向用户提出确认性问题。当用户说“我确定”时，聊天机器人可能会问类似“你真的确定吗？”的问题。为了做到这一点，聊天机器人必须能够生成相关的问题。

假设用户提交的句子是这样的：

我可以保证这值得你花时间。

这个句子包含了多个动词和代词，每个词的形态不同。为了更清楚地查看这一点，我们来看一下 spaCy 为这个句子中的每个词分配的词性标签：

>>> doc = nlp(u"我可以保证这值得你花时间。")

>>> for token in doc:

...   print(token.text, token.pos_, token.tag_)

...

我们打印出这些词、它们的粗粒度词性标签以及精细词性标签，生成以下输出：

I       代词  PRP

can     动词  MD

promise 动词  VB

it      代词  PRP

is      动词  VBZ

worth   形容词  JJ

your    ADJ  PRP$

time    NOUN NN

.      PUNCT .

从细粒度的词性标签中，你可以区分句子中动词和代词的形态类别。例如，细粒度的词性标签 PRP 标记个人代词，而 PRP$标记所有格代词，这使你能够在程序中区分这两种代词。我们在处理这个示例时需要这些信息。

对于这里讨论的句子，确认问题可能如下所示（当然，另一个陈述需要另一个确认问题）：

Can you really promise it is worth my time?

从人类的角度来看，从陈述中形成这个问题看起来相当简单：你只需更改一些单词的顺序，相应地修改代词，并将副词修饰语“really”添加到主谓之间（即紧跟主语后的动词）。但如何以编程的方式实现这些操作呢？

让我们看看一些词性标签。在示例句子中，构成问题的动词是“can”和“promise”。细粒度的词性标签将第一个动词“can”标记为情态助动词，将第二个动词“promise”标记为基本形式的动词。请注意，在前面的确认问题中，情态助动词与个人代词交换了位置，这个过程叫做*倒装*。我们需要在脚本中实现这一点。

在涉及代词时，聊天机器人应遵循常见的对话模式。表 4-1 总结了在此类应用中代词的使用。

**表 4-1：** 聊天机器人中代词的使用

|  | **个人代词** | **所有格代词** |
| --- | --- | --- |
| chatbot | I, me | my, mine |
| user | you | your, yours |

换句话说，聊天机器人会将自己称为“I”或“me”，并将用户称为“you”。

以下步骤概述了我们需要做什么，以便从原始陈述中生成一个问题：

1.  将原始句子中的单词顺序从“主语 + 情态助动词 + 不定式动词”更改为“情态助动词 + 主语 + 不定式动词”。

1.  将个人代词“I”（句子的主语）替换为“you”。

1.  将所有格代词“your”替换为“my”。

1.  将副词修饰语“really”放在动词“promise”之前，以强调后者。

1.  将句末的标点符号“.”替换为“?”。

以下脚本实现了这些步骤：

import spacy

nlp = spacy.load('en')

doc = nlp(u"我可以保证这值得你的时间。")

sent = ''

for i,token in enumerate(doc):

➊ if token.tag_ == 'PRP' and doc[i+1].tag_ == 'MD' and doc[i+2].tag_ == 'VB':

➋ sent = doc[i+1].text.capitalize() + ' ' + doc[i].text

sent = sent + ' ' + ➌doc[i+2:].text

➍ break

#到现在为止，你应该已经有了：“Can I promise it is worth your time.”

#重标记

➎ doc=nlp(sent)

for i,token in enumerate(doc):

➏ if token.tag_ == 'PRP' and token.text == 'I':

sent = doc[:i].text + ' you ' + doc[i+1:].text

break

#到此为止，你应该得到：'Can you promise it is worth your time.'

doc=nlp(sent)

for i,token in enumerate(doc):

➐ if token.tag_ == 'PRP$' and token.text == 'your':

sent = doc[:i].text + ' my ' + doc[i+1:].text

break

#到此为止，你应该得到：'Can you promise it is worth my time.'

doc=nlp(sent)

for i,token in enumerate(doc):

if token.tag_ == 'VB':

➑ sent = doc[:i].text + ' really ' + doc[i:].text

break

#到此为止，你应该得到：'Can you really promise it is worth my time.'

doc=nlp(sent)

➒ sent = doc[:len(doc)-1].text + '?'

#最后，你应该得到：'Can you really promise it is worth my time?'

print(sent)

我们将前四个步骤分开执行。首先，我们遍历句子中的词元并改变主语和动词的顺序，使句子变为一个疑问句。在这个例子中，我们正在寻找跟在人称代词后面的情态助动词（标记为 MD），并且后面跟着一个不定式动词 ➊。找到这一系列词后，我们将情态助动词移到人称代词之前，放在句子的开头 ➋。

为了构造一个新句子，我们使用在 Python 中称为 *切片* 的技术，它允许我们通过指定开始和结束索引，从序列对象中提取子序列，比如字符串或列表。在这个例子中，我们可以对 Doc 对象应用切片，从中提取出给定的词元子序列。例如，切片 doc[2:] 将包含从索引 2 开始直到文档结尾的词元，在这种情况下，就是“promise it is worth your time.” ➌。一旦我们将情态动词移动到新位置，我们就退出 for 循环 ➍。

你可能会问，为什么我们不直接使用人称代词和助动情态动词的索引来执行倒装呢？因为我们知道人称代词在索引 0 处，情态动词在索引 1 处，为什么还要使用一个遍历所有词元的循环来找到情态动词的位置呢？动词不总是跟随主语并且是句子中的第二个词吗？

事实是，句子并不总是以主语开始。例如，如果句子是 “Sure enough, I can promise it is worth your time.” 呢？在这种情况下，脚本会知道省略前两个词，并从主语开始处理。

由于倒装，我们得到了新的句子作为字符串。为了让这个句子可用于进一步处理，我们需要为其获取一个 Doc 对象 ➎。

接下来，我们创建一个新的 for 循环，将人称代词 “I” 替换为人称代词 “you”。为此，我们搜索人称代词（标记为 PRP）。如果人称代词是 “I”，我们将其替换为 “you” ➏。然后我们退出 for 循环。

我们重复这个过程，通过搜索 PRP$标签 ➐来将所有物主代词“your”替换为“my”。然后，在一个新的 for 循环中，我们找到不定式形式的动词，并在它前面插入副词修饰语“really” ➑。

最后，我们将句子的句号替换为问号。这是唯一一个不需要使用循环的步骤。原因是，在所有可能的句子中，句号和问号都位于句子的末尾，因此我们可以通过使用它们的索引来可靠地找到它们，方法是 len(doc)-1 ➒。

当我们运行这段代码时，我们应该得到以下输出：

你真的能保证这是值得我花时间做的吗？

这段脚本是一个不错的开始，但它不能适用于每个提交的句子。例如，句子可能包含“我”以外的其他人称代词，但我们的脚本并没有明确检查这一点。此外，一些句子不包含助动词，比如“我喜欢吃冰淇淋”这个句子。在这些情况下，我们必须使用“do”来构成疑问句，而不是像“can”或“should”这样的词，像这样：“你真的喜欢吃冰淇淋吗？”但是，如果句子中包含动词“to be”，例如“我困了”，我们就必须将动词移到句首，像这样：“你困吗？”

这个聊天机器人的真实实现必须能够为提交的句子选择合适的选项。你将在“确定聊天机器人应提问的问题”中看到一个“do”示例，位于第 56 页。

#### ***试试看***

仔细检查“将陈述句转化为疑问句”中的脚本，你可能会注意到其中一些代码块看起来非常相似，包含重复的操作。在每个步骤中，你都会对句子进行替换，然后重新标记。这意味着你可能需要尝试将代码进行概括，将重复的操作放入一个单一的函数中。

在编写这样的函数之前，花些时间理解它需要哪些参数才能执行你在脚本中看到的文本处理操作。特别是，你需要明确指定你要搜索的标记，以及你希望对其执行的操作，无论是将其替换为另一个标记，还是在它之前添加一个标记。

一旦你定义了这个函数，你就可以编写调用它的主代码，实现与原始脚本相同的功能。

### **在文本处理中过滤句法依赖标签**

正如你在“通过词性标记提取和生成文本”中学习的那样，词性标记是智能文本处理的有力工具。但在实践中，你可能需要了解更多关于句子的标记，以便智能地处理它。

例如，你可能需要知道一个人称代词是句子的主语还是语法上的宾语。有时，这个任务很简单。人称代词“I”，“he”，“she”，“they”和“we”几乎总是主语。作为宾语时，“I”变成了“me”，例如在句子“A postman brought me a letter.”中。

但当涉及到其他一些人称代词时，情况可能不那么明确，例如“you”或“it”，这些代词无论作为主语还是宾语时形式都相同。考虑以下两个句子：“I know you. You know me.” 在第一个句子中，“you”是动词“know”的直接宾语。在第二个句子中，“you”是动词的主语。

让我们通过使用句法依存关系标签和词性标签来解决这个问题。然后，我们将应用句法依存关系标签来构建一个更好的问答聊天机器人。

#### ***区分主语和宾语***

要程序化地确定像“you”或“it”这样的代词在特定句子中的角色，你需要检查分配给它的依存关系标签。通过结合使用词性标签和依存关系标签，你可以获得更多关于句子中单词的信息。

让我们回到前面的句子，看看对它进行依存句法分析后的结果：

>>> doc = nlp(u"我可以保证这值得你的时间。")

>>> for token in doc:

...   print(token.text, token.pos_, token.tag_, token.dep_, spacy.explain(token.dep_))

我们提取词性标签、依存关系标签以及依存关系标签的描述：

I       PRON  PRP  nsubj     主语

can     VERB  MD   aux       auxiliary

promise VERB  VB   ROOT      无

it      PRON  PRP  nsubj     主语

is      VERB  VBZ  ccomp     从句补语

worth   ADJ   JJ   acomp     形容词性补语

your    ADJ   PRP$ poss      所有格修饰词

time    NOUN  NN   npadvmod  作状语的名词短语

.       PUNCT .    punct     标点符号

第二列和第三列分别包含粗粒度和细粒度的词性标签。第四列包含依存关系标签，第五列包含这些依存关系标签的描述。

结合词性标签和依存关系标签可以更清晰地了解句子中每个词的语法角色——比单纯的词性标签或依存关系标签更具信息性。例如，在这个例子中，分配给“is”的词性标签 VBZ 表示第三人称单数现在时动词，而分配给同一词的依存关系标签 ccomp 则表示“is”是一个*从句补语*（带有内部主语的依赖子句）。在这个例子中，“is”是动词“promise”的从句补语，内部主语是“it”。

为了弄清楚句子“I know you. You know me.”中“you”的角色，我们需要检查以下词性标签和依存关系标签：

I     PRON  PRP  nsubj  名词性主语

know  VERB  VBP  ROOT   无

you   PRON  PRP  dobj   直接宾语

.     PUNCT .    标点符号  标点符号

You   PRON  PRP  nsubj  名词性主语

know  VERB  VBP  ROOT   无

me    PRON  PRP  dobj   直接宾语

.     PUNCT .    标点符号  标点符号

在这两种情况下，“你”被分配了相同的词性标签：PRON 和 PRP（粗粒度和细粒度）。但这两种情况的依赖标签不同：第一个句子的标签是 dobj，第二个句子的标签是 nsubj。

#### ***决定聊天机器人应提什么问题***

有时，你可能需要浏览一个句子的依赖树，以提取必要的信息。例如，考虑以下聊天机器人与用户之间的对话：

用户：我想要一个苹果。

机器人：你想要一个红色的苹果吗？

用户：我想要一个绿色的苹果。

机器人：你为什么想要一个绿色的？

聊天机器人通过提问能够继续对话。但请注意，“苹果”这个名词是否有形容词修饰语对于决定它应该问什么类型的问题起着关键作用。

英语中有两种基本问题类型：是/否问题和信息性问题。像我们在 “将陈述转为问题”（第 51 页）中讨论的例子那样的是/否问题只有两个可能的答案：是或否。要形成这种类型的问题，你将情态助动词置于主语之前，主语后面跟主谓动词。例如：“你能修改它吗？”

信息性问题应该通过提供比简单的“是”或“否”更多的信息来回答。它们通常以疑问词开头，如“什么”、“哪里”、“何时”、“为什么”或“怎么”。在疑问词之后，信息性问题的构成与是/否问题相同。例如：“你怎么看待这个？”

在前面苹果例子的第一个案例中，聊天机器人提出了一个是/否问题。在第二个案例中，当用户用形容词“绿色”修饰名词“苹果”时，聊天机器人则提出了一个信息性问题。

图 4-1 中的流程图总结了这种方法。

![image](img/fig4-1.jpg)

*图 4-1：输入句子中是否有修饰语决定了聊天机器人应该提什么问题。*

以下脚本仅分析提交的句子，以决定应该提什么类型的问题，然后形成正确的问题。我们将分别讲解代码，但你应该将整个程序保存为一个名为 *question.py* 的文件。

首先导入 sys 模块，它提供了将句子作为参数传递给程序进行处理的功能：

import spacy

import sys

这是对之前脚本的改进，以前我们硬编码了需要分析的句子。现在用户可以提交他们自己的句子作为输入。

接下来，我们定义一个函数，该函数识别并提取提交的文档中任何作为直接宾语的名词短语。例如，如果你提交的文档包含句子 “I want a green apple.”，它会返回名词短语 “a green apple”：

定义 find_chunk(doc) 函数：

chunk = ''

➊ 对于 i, token 在 enumerate(doc) 中：

➋ 如果 token.dep_ == 'dobj'：

➌ shift = len([w for w in token.children])

➍ #print([w for w in token.children])

➎ chunk = doc[i-shift:i+1]

break

返回 chunk

我们遍历提交句子中的 token ➊，并通过检查它的依赖标签是否为 dobj 来寻找作为直接宾语的 token ➋。在句子 “I want a green apple.” 中，直接宾语是名词 “apple”。一旦我们找到了直接宾语，我们需要确定它的语法子节点 ➌，因为它们构成了我们用来决定提问类型的 chunk。为了调试，我们可能还需要查看直接宾语的子节点 ➍。

为了提取 chunk，我们切片 Doc 对象，计算切片的起始和结束索引，方法如下：起始索引是直接宾语的索引减去其语法子节点的数量。你可能猜到，这就是最左边子节点的索引。结束索引是直接宾语的索引加一，所以 chunk 中包含的最后一个 token 就是直接宾语 ➎。

为了简化，本文所实现的算法假设直接宾语只有向左的子节点。实际上，情况并非总是如此。例如，在以下句子中，“I want to touch a wall painted green.”，我们需要检查直接宾语 “wall” 的左右子节点。此外，由于 “green” 不是 “wall” 的直接子节点，我们需要遍历依赖树来确定 “green” 是 “wall” 的修饰语。我们将在第六章中更深入地讨论前修饰语和后修饰语。

以下函数检查 chunk 并决定聊天机器人应该问哪种类型的问题：

定义 determine_question_type(chunk) 函数：

➊ question_type = 'yesno'

对于 token 在 chunk 中：

➋ 如果 token.dep_ == 'amod'：

➌ question_type = 'info'

返回 question_type

我们将 question_type 变量初始化为 yesno，表示是/否问题类型 ➊。然后，在提交的 chunk 中，我们搜索标记为 amod 的 token，表示形容词修饰语 ➋。如果找到，我们将 question_type 变量设置为 'info'，表示信息性问题类型 ➌。

一旦我们确定了要使用的提问类型，以下函数会根据提交的句子生成问题：

定义 generate_question(doc, question_type) 函数：

sent = ''

对于 i, token 在 enumerate(doc) 中：

如果 token.tag_ == 'PRP' 且 doc[i+1].tag_ == 'VBP'：

sent = 'do ' + doc[i].text

sent = sent + ' ' + doc[i+1:].text

break

doc = nlp(sent)

对于 i, token 在 enumerate(doc) 中：

如果 token.tag_ == 'PRP' 且 token.text == 'I'：

sent = doc[:i].text + ' you ' + doc[i+1:].text

break

doc = nlp(sent)

➊ 如果 question_type == 'info'：

for i,token in enumerate(doc):

if token.dep_ == 'dobj':

sent = '为什么 ' + doc[:i].text + ' 一个 ' + doc[i+1:].text

break

➋ if question_type == 'yesno':

for i,token in enumerate(doc):

if token.dep_ == 'dobj':

➌ sent = doc[:i-1].text + ' a red ' + doc[i:].text

break

doc=nlp(sent)

sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'

return sent

在一系列的 for 循环中，我们通过倒装和更改人称代词将提交的陈述句转换为疑问句。在这个例子中，由于句子中没有情态助动词，我们在人称代词前加上动词“do”来形成疑问句。（请记住，这仅适用于某些句子；在更完整的实现中，我们必须程序性地决定采用哪种处理方法。）

如果 question_type 设置为 info，我们会在问题的开头加上“为什么”一词 ➊。如果 question_type 变量设置为 yesno ➋，我们会插入一个形容词来修饰问题中的直接宾语。在这个例子中，为了简化，我们硬编码了形容词。我们选择了形容词“红色的”， ➌ 这个在某些句子中可能听起来有些奇怪。例如，我们可以说：“你想要一个红色的橙子吗？”但不能说：“你想要一个红色的想法吗？”在更好的聊天机器人实现中，我们可以找到一种方法，根据上下文程序性地确定合适的形容词来修饰直接宾语。我们将在第六章中回到这个话题。

另外，请注意这里使用的算法假设提交的句子以标点符号结尾，例如“。”或“！”。

现在我们已经定义了所有的函数，以下是脚本的主要部分：

➊ if len(sys.argv) > 1:

sent = sys.argv[1]

nlp = spacy.load('en')

➋ doc = nlp(sent)

➌ chunk = find_chunk(doc)

➍ if str(chunk) == '':

print('句子不包含直接宾语。')

sys.exit()

➎ question_type = determine_question_type(chunk)

➏ question = generate_question(doc, question_type)

print(question)

else:

print('你没有提交句子！')

首先，我们检查用户是否通过命令行参数传递了一个句子 ➊。若提交了句子，我们应用 spaCy 的管道处理它，创建一个 Doc 对象实例 ➋。

然后我们将文档传递给 find_chunk 函数，该函数应该返回一个包含直接宾语的名词短语，例如“一个绿色的苹果”，以便进一步处理 ➌。如果提交的句子中没有这样的名词短语 ➍，我们会收到“句子不包含直接宾语。”的提示信息。

接下来，我们将刚刚提取的 chunk 传递给 determine_question_type 函数，该函数根据 chunk 的结构确定要提出的问题类型 ➎。

最后，我们将提交的句子和问题类型传递给 generate_question 函数，该函数将生成一个适当的问题并以字符串形式返回 ➏。

脚本的输出取决于提交的具体句子。以下是一些可能的变体：

➊ $ python question.py 'I want a green apple.'

你为什么想要一个绿色的？

➋ $ python question.py 'I want an apple.'

你想要一个红色的苹果吗？

➌ $ python question.py 'I want...'

该句子不包含直接宾语。

➍ $ python question.py

你没有提交句子！

如果我们提交一个包含形容词修饰语的句子，例如“green”作为“apple”这样的直接宾语的修饰词，脚本应生成一个信息性疑问句 ➊。

如果句子包含没有形容词修饰语的直接宾语，脚本应该生成是/否疑问句 ➋。

如果我们提交的句子没有直接宾语，脚本应该立刻识别并要求我们重新提交 ➌。

最后，如果我们忘记提交句子，脚本应该返回一个适当的消息 ➍。

#### ***试试这个***

如前所述，前面章节讨论的脚本并不适用于所有句子。该脚本通过添加“do”来构成疑问句，这只适用于不含有助动词的句子。

增强该脚本的功能，使其也能处理包含情态助动词的陈述句。例如，针对以下陈述句：“I might want a green apple”，脚本应生成“Why might you want a green one？”有关如何将包含情态助动词的陈述句转为疑问句，请参考《将陈述句转为疑问句》（见第 51 页）。

### **总结**

语言学特征是所有 NLP 任务的核心。本章教会了你一些利用语言学特征进行智能文本处理和文本生成的技巧。你学会了如何提取某一类型的短语（例如那些指代金额的短语），然后编写了一个脚本，使用依赖标签和词性标签生成对用户提交的句子作出有意义的回应。

我们将在第六章中回到语言学特征，您将在更复杂的场景中实现它们。
