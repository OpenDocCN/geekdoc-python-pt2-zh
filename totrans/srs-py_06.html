<html><head></head><body>
<h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_75"/><strong><span class="big">6</span></strong><br/><strong>UNIT TESTING</strong></h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Many find unit testing to be arduous and time-consuming, and some people and projects have no testing policy. This chapter assumes that you see the wisdom of unit testing! Writing code that is not tested is fundamentally useless, as there’s no way to conclusively prove that it works. If you need convincing, I suggest you start by reading about the benefits of test-driven development.</p>&#13;
<p class="indent">In this chapter you’ll learn about the Python tools you can use to construct a comprehensive suite of tests that will make testing simpler and more automated. We’ll talk about how you can use tools to make your software rock solid and regression-free. We’ll cover creating reusable test objects, running tests in parallel, revealing untested code, and using virtual environments to make sure your tests are clean, as well as some other good-practice methods and ideas.</p>&#13;
<h3 class="h3" id="lev1sec30"><span epub:type="pagebreak" id="page_76"/><strong>The Basics of Testing</strong></h3>&#13;
<p class="noindent">Writing and running unit tests is uncomplicated in Python. The process is not intrusive or disruptive, and unit testing will greatly help you and other developers in maintaining your software. Here I’ll discuss some of the absolute basics of testing that will make things easier for you.</p>&#13;
<h4 class="h4" id="lev2sec21"><strong><em>Some Simple Tests</em></strong></h4>&#13;
<p class="noindent">First, you should store tests inside a <code>tests</code> submodule of the application or library they apply to. Doing so will allow you to ship the tests as part of your module so that they can be run or reused by anyone—even after your software is installed—without necessarily using the source package. Making the tests a submodule of your main module also prevents them from being installed by mistake in a top-level <code>tests</code> module.</p>&#13;
<p class="indent">Using a hierarchy in your test tree that mimics the hierarchy of your module tree will make the tests more manageable. This means that the tests covering the code of <em>mylib/foobar.py</em> should be stored inside <em>mylib/tests/test_foobar.py</em>. Consistent nomenclature makes things simpler when you’re looking for the tests related to a particular file. <a href="ch06.xhtml#ch6list1">Listing 6-1</a> shows the simplest unit test you can write.</p>&#13;
<pre>def test_true():<br/>    assert True</pre>&#13;
<p class="listing1"><a id="ch6list1"/><em>Listing 6-1: A really simple test in <span class="roman">test_true.py</span></em></p>&#13;
<p class="indent">This will simply assert that the behavior of the program is what you expect. To run this test, you need to load the <em>test_true.py</em> file and run the <code>test_true()</code> function defined within.</p>&#13;
<p class="indent">However, writing and running an individual test for each of your test files and functions would be a pain. For small projects with simple usage, the <code>pytest</code> package comes to the rescue—once installed via <code>pip</code>, pytest provides the <code>pytest</code> command, which loads every file whose name starts with <em>test_</em> and then executes all functions within that start with <code>test_</code>.</p>&#13;
<p class="indent">With just the <em>test_true.py</em> file in our source tree, running <code>pytest</code> gives us the following output:</p>&#13;
<pre> $ <span class="codestrong1">pytest -v test_true.py</span><br/>========================== test session starts ===========================<br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 --<br/>/usr/local/opt/python/bin/python3.6<br/>cachedir: .cache<br/>rootdir: examples, inifile:<br/>collected 1 item<br/><br/>test_true.py::test_true PASSED                                     [100%]<br/><br/>======================== 1 passed in 0.01 seconds ========================</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_77"/>The <code>-v</code> option tells <code>pytest</code> to be verbose and print the name of each test run on a separate line. If a test fails, the output changes to indicate the failure, accompanied by the whole traceback.</p>&#13;
<p class="indent">Let’s add a failing test this time, as shown in <a href="ch06.xhtml#ch6list2">Listing 6-2</a>.</p>&#13;
<pre>def test_false():<br/>    assert False</pre>&#13;
<p class="listing1"><a id="ch6list2"/><em>Listing 6-2: A failing test in <span class="roman">test_true.py</span></em></p>&#13;
<p class="indent">If we run the test file again, here’s what happens:</p>&#13;
<pre> $ <span class="codestrong1">pytest -v test_true.py</span><br/>========================== test session starts ===========================<br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/<br/>local/opt/python/bin/python3.6<br/>cachedir: .cache<br/>rootdir: examples, inifile:<br/>collected 2 items<br/><br/>test_true.py::test_true PASSED                                     [ 50%]<br/>test_true.py::test_false FAILED                                    [100%]<br/><br/>================================ FAILURES ================================<br/>_______________________________ test_false _______________________________<br/><br/>    def test_false():<br/>&gt;       assert False<br/>E       assert False<br/><br/>test_true.py:5: AssertionError<br/>=================== 1 failed, 1 passed in 0.07 seconds ===================</pre>&#13;
<p class="indent">A test fails as soon as an <code>AssertionError</code> exception is raised; our <code>assert</code> test will raise an <code>AssertionError</code> when its argument is evaluated to something false (<code>False</code>, <code>None</code>, 0, etc.). If any other exception is raised, the test also errors out.</p>&#13;
<p class="indent">Simple, isn’t it? While simplistic, a lot of small projects use this approach and it works very well. Those projects require no tools or libraries other than pytest and thus can rely on simple <code>assert</code> tests.</p>&#13;
<p class="indent">As you start to write more sophisticated tests, pytest will help you understand what’s wrong in your failing tests. Imagine the following test:</p>&#13;
<pre>def test_key():<br/>    a = ['a', 'b']<br/>    b = ['b']<br/>    assert a == b</pre>&#13;
<p class="indent">When <code>pytest</code> is run, it gives the following output:</p>&#13;
<pre> $ <span class="codestrong1">pytest test_true.py</span><br/>========================== test session starts ===========================<span epub:type="pagebreak" id="page_78"/><br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0<br/>rootdir: /Users/jd/Source/python-book/examples, inifile:<br/>plugins: celery-4.1.0<br/>collected 1 item<br/><br/>test_true.py F                                                     [100%]<br/><br/>================================ FAILURES ================================<br/>________________________________ test_key ________________________________<br/><br/>    def test_key():<br/>        a = ['a', 'b']<br/>        b = ['b']<br/>&gt;       assert a == b<br/>E       AssertionError: assert ['a', 'b'] == ['b']<br/>E         At index 0 diff: 'a' != 'b'<br/>E         Left contains more items, first extra item: 'b'<br/>E         Use -v to get the full diff<br/><br/>test_true.py:10: AssertionError<br/>======================== 1 failed in 0.07 seconds ========================</pre>&#13;
<p class="indent">This tells us that <code>a</code> and <code>b</code> are different and that this test does not pass. It also tells us exactly how they are different, making it easy to fix the test or code.</p>&#13;
<h4 class="h4" id="lev2sec22"><strong><em>Skipping Tests</em></strong></h4>&#13;
<p class="noindent">If a test cannot be run, you will probably want to skip that test—for example, you may wish to run a test conditionally based on the presence or absence of a particular library. To that end, you can use the <code>pytest.skip()</code> function, which will mark the test as skipped and move on to the next one. The <code>pytest.mark.skip</code> decorator skips the decorated test function unconditionally, so you’ll use it when a test always needs to be skipped. <a href="ch06.xhtml#ch6list3">Listing 6-3</a> shows how to skip a test using these methods.</p>&#13;
<pre>import pytest<br/><br/>try:<br/>    import mylib<br/>except ImportError:<br/>    mylib = None<br/><br/>@pytest.mark.skip("Do not run this")<br/>def test_fail():<br/>    assert False<br/><br/>@pytest.mark.skipif(mylib is None, reason="mylib is not available")<br/>def test_mylib():<br/>    assert mylib.foobar() == 42<span epub:type="pagebreak" id="page_79"/><br/>def test_skip_at_runtime():<br/>    if True:<br/>        pytest.skip("Finally I don't want to run it")</pre>&#13;
<p class="listing1"><a id="ch6list3"/><em>Listing 6-3: Skipping tests</em></p>&#13;
<p class="indent">When executed, this test file will output the following:</p>&#13;
<pre> $ <span class="codestrong1">pytest -v examples/test_skip.py</span><br/>========================== test session starts ===========================<br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/<br/>local/opt/python/bin/python3.6<br/>cachedir: .cache<br/>rootdir: examples, inifile:<br/>collected 3 items<br/><br/>examples/test_skip.py::test_fail SKIPPED<br/>[ 33%]<br/>examples/test_skip.py::test_mylib SKIPPED<br/>[ 66%]<br/>examples/test_skip.py::test_skip_at_runtime SKIPPED<br/>[100%]<br/><br/>================= 3 skipped in 0.01 seconds =================</pre>&#13;
<p class="indent">The output of the test run in <a href="ch06.xhtml#ch6list3">Listing 6-3</a> indicates that, in this case, all the tests have been skipped. This information allows you to ensure you didn’t accidentally skip a test you expected to run.</p>&#13;
<h4 class="h4" id="lev2sec23"><strong><em>Running Particular Tests</em></strong></h4>&#13;
<p class="noindent">When using <code>pytest</code>, you often want to run only a particular subset of your tests. You can select which tests you want to run by passing their directory or files as an argument to the <code>pytest</code> command line. For example, calling <code>pytest test_one.py</code> will only run the <em>test_one.py</em> test. Pytest also accepts a directory as argument, and in that case, it will recursively scan the directory and run any file that matches the <em>test_*.py</em> pattern.</p>&#13;
<p class="indent">You can also add a filter with the <code>-k</code> argument on the command line in order to execute only the test matching a name, as shown in <a href="ch06.xhtml#ch6list4">Listing 6-4</a>.</p>&#13;
<pre>$ pytest -v examples/test_skip.py -k test_fail<br/>========================== test session starts ===========================<br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/<br/>local/opt/python/bin/python3.6<br/>cachedir: .cache<br/>rootdir: examples, inifile:<br/>collected 3 items<br/><br/>examples/test_skip.py::test_fail SKIPPED<br/>[100%]<span epub:type="pagebreak" id="page_80"/><br/>=== 2 tests deselected ===<br/>=== 1 skipped, 2 deselected in 0.04 seconds ===</pre>&#13;
<p class="listing1"><a id="ch6list4"/><em>Listing 6-4: Filtering tests run by name</em></p>&#13;
<p class="indent">Names are not always the best way to filter which tests will run. Commonly, a developer would group tests by functionalities or types instead. Pytest provides a dynamic marking system that allows you to mark tests with a keyword that can be used as a filter. To mark tests in this way, use the <code>-m</code> option. If we set up a couple of tests like this:</p>&#13;
<pre>import pytest<br/><br/>@pytest.mark.dicttest<br/>def test_something():<br/>    a = ['a', 'b']<br/>    assert a == a<br/><br/>def test_something_else():<br/>    assert False</pre>&#13;
<p class="noindent">we can use the <code>-m</code> argument with <code>pytest</code> to run only one of those tests:</p>&#13;
<pre>$ <span class="codestrong1">pytest -v test_mark.py -m dicttest</span><br/>=== test session starts ===<br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 -- /usr/<br/>local/opt/python/bin/python3.6<br/>cachedir: .cache<br/>rootdir: examples, inifile:<br/>collected 2 items<br/><br/>test_mark.py::test_something PASSED<br/>[100%]<br/><br/>=== 1 tests deselected ===<br/>=== 1 passed, 1 deselected in 0.01 seconds ===</pre>&#13;
<p class="indent">The <code>-m</code> marker accepts more complex queries, so we can also run all tests that are <em>not</em> marked:</p>&#13;
<pre>$ <span class="codestrong1">pytest test_mark.py -m 'not dicttest'</span><br/>=== test session starts ===<br/>platform darwin -- Python 3.6.4, pytest-3.3.2, py-1.5.2, pluggy-0.6.0<br/>rootdir: examples, inifile:<br/>collected 2 items<br/><br/>test_mark.py F<br/>[100%]<br/><br/>=== FAILURES ===<br/>test_something_else<span epub:type="pagebreak" id="page_81"/><br/>    def test_something_else():<br/>&gt;       assert False<br/>E       assert False<br/><br/>test_mark.py:10: AssertionError<br/>=== 1 tests deselected ===<br/>=== 1 failed, 1 deselected in 0.07 seconds ===</pre>&#13;
<p class="indent">Here pytest executed every test that was not marked as <code>dicttest</code>—in this case, the <code>test_something_else</code> test, which failed. The remaining marked test, <code>test_something</code>, was not executed and so is listed as <code>deselected</code>.</p>&#13;
<p class="indent">Pytest accepts complex expressions composed of the <code>or</code>, <code>and</code>, and <code>not</code> keywords, allowing you to do more advanced filtering.</p>&#13;
<h4 class="h4" id="lev2sec24"><strong><em>Running Tests in Parallel</em></strong></h4>&#13;
<p class="noindent">Test suites can take a long time to run. It’s not uncommon for a full suite of unit tests to take tens of minutes to run in large software projects. By default, pytest runs all tests serially, in an undefined order. Since most computers have several CPUs, you can usually speed things up if you split the list of tests and run them on multiple CPUs.</p>&#13;
<p class="indent">To handle this approach, pytest provides the plugin <code>pytest-xdist</code>, which you can install with <code>pip</code>. This plugin extends the pytest command line with the <code>--numprocesses</code> argument (shortened as <code>-n</code>), which accepts as its argument the number of CPUs to use. Running <code>pytest -n 4</code> would run your test suite using four parallel processes, balancing the load across the available CPUs.</p>&#13;
<p class="indent">Because the number of CPUs can change from one computer to another, the plugin also accepts the <code>auto</code> keyword as a value. In this case, it will probe the machine to retrieve the number of CPUs available and start this number of processes.</p>&#13;
<h4 class="h4" id="lev2sec25"><strong><em>Creating Objects Used in Tests with Fixtures</em></strong></h4>&#13;
<p class="noindent">In unit testing, you’ll often need to execute a set of common instructions before and after running a test, and those instructions will use certain components. For example, you might need an object that represents the configuration state of your application, and you’ll likely want that object to be initialized before each test, then reset to its default values when the test is achieved. Similarly, if your test relies on the temporary creation of a file, the file must be created before the test starts and deleted once the test is done. These components, known as <em>fixtures</em>, are set up before a test and cleaned up after the test has finished.</p>&#13;
<p class="indent">With pytest, fixtures are defined as simple functions. The fixture function should return the desired object(s) so that a test using that fixture can use that object.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_82"/>Here’s a simple fixture:</p>&#13;
<pre>import pytest<br/><br/>@pytest.fixture<br/>def database():<br/>    return <span class="codeitalic1">&lt;some database connection&gt;</span><br/><br/>def test_insert(database):<br/>    database.insert(123)</pre>&#13;
<p class="indent">The database fixture is automatically used by any test that has <code>database</code> in its argument list. The <code>test_insert()</code> function will receive the result of the <code>database()</code> function as its first argument and use that result as it wants. When we use a fixture this way, we don’t need to repeat the database initialization code several times.</p>&#13;
<p class="indent">Another common feature of code testing is tearing down after a test has used a fixture. For example, you may need to close a database connection. Implementing the fixture as a generator allows us to add teardown functionality, as shown in <a href="ch06.xhtml#ch6list5">Listing 6-5</a>.</p>&#13;
<pre>import pytest<br/><br/>@pytest.fixture<br/>def database():<br/>    db = &lt;<span class="codeitalic1">some database connection</span>&gt;<br/>    yield db<br/>    db.close()<br/><br/>def test_insert(database):<br/>    database.insert(123)</pre>&#13;
<p class="listing1"><a id="ch6list5"/><em>Listing 6-5: Teardown functionality</em></p>&#13;
<p class="indent">Because we used the <code>yield</code> keyword and made <code>database</code> a generator, the code after the <code>yield</code> statement runs when the test is done. That code will close the database connection at the end of the test.</p>&#13;
<p class="indent">However, closing a database connection for each test might impose an unnecessary runtime cost, as tests may be able to reuse that same connection. In that case, you can pass the <code>scope</code> argument to the fixture decorator, specifying the scope of the fixture:</p>&#13;
<pre>import pytest<br/><br/>@pytest.fixture(scope="module")<br/>def database():<br/>    db = <span class="codeitalic1">&lt;some database connection&gt;</span><br/>    yield db<br/>    db.close()<br/><br/>def test_insert(database):<br/>    database.insert(123)</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_83"/>By specifying the <code>scope="module"</code> parameter, you initialize the fixture once for the whole module, and the same database connection will be passed to all test functions requesting a database connection.</p>&#13;
<p class="indent">Finally, you can run some common code before and after your tests by marking fixtures as <em>automatically used</em> with the <code>autouse</code> keyword, rather than specifying them as an argument for each of the test functions. Specifying the <code>autouse=True</code> keyword argument to the <code>pytest.fixture()</code> function will make sure the fixture is called before running any test in the module or class it is defined in, as in this example:</p>&#13;
<pre>import os<br/><br/>import pytest<br/><br/>@pytest.fixture(autouse=True)<br/>def change_user_env():<br/>    curuser = os.environ.get("USER")<br/>    os.environ["USER"] = "foobar"<br/>    yield<br/>    os.environ["USER"] = curuser<br/><br/>def test_user():<br/>    assert os.getenv("USER") == "foobar"</pre>&#13;
<p class="indent">Such automatically enabled features are handy, but make sure not to abuse fixtures: they are run before each and every test covered by their scope, so they can slow down a test run significantly.</p>&#13;
<h4 class="h4" id="lev2sec26"><strong><em>Running Test Scenarios</em></strong></h4>&#13;
<p class="noindent">When unit testing, you may want to run the same error-handling test with several different objects that trigger that error, or you may want to run an entire test suite against different drivers.</p>&#13;
<p class="indent">We relied heavily on this latter approach when developing <em>Gnocchi</em>, a time series database. Gnocchi provides an abstract class that we call the <em>storage API</em>. Any Python class can implement this abstract base and register itself to become a driver. The software loads the configured storage driver when required and uses the implemented storage API to store or retrieve data. In this case, we need a class of unit tests that runs against each driver—thus running against each implementation of this storage API—to be sure all drivers conform to what the callers expect.</p>&#13;
<p class="indent">An easy way to achieve this is by using <em>parameterized fixtures</em>, which will run all the tests that use them several times, once for each of the defined parameters. <a href="ch06.xhtml#ch6list6">Listing 6-6</a> shows an example of using parameterized fixtures to run a single test twice with different parameters: once for <code>mysql</code> and once for <code>postgresql</code>.</p>&#13;
<pre>import pytest<br/>import myapp<span epub:type="pagebreak" id="page_84"/><br/>@pytest.fixture(params=["mysql", "postgresql"])<br/>def database(request):<br/>    d = myapp.driver(request.param)<br/>    d.start()<br/>    yield d<br/>    d.stop()<br/><br/>def test_insert(database):<br/>    database.insert("somedata")</pre>&#13;
<p class="listing1"><a id="ch6list6"/><em>Listing 6-6: Running a test using parameterized fixtures</em></p>&#13;
<p class="indent">In <a href="ch06.xhtml#ch6list6">Listing 6-6</a>, the <code>driver</code> fixture is parameterized with two different values, each the name of a database driver that is supported by the application. When <code>test_insert</code> is run, it is actually run twice: once with a MySQL database connection and once with a PostgreSQL database connection. This allows us to easily reuse the same test with different scenarios, without adding many lines of code.</p>&#13;
<h4 class="h4" id="lev2sec27"><strong><em>Controlled Tests Using Mocking</em></strong></h4>&#13;
<p class="noindent">Mock objects are simulated objects that mimic the behavior of real application objects, but in particular and controlled ways. These are especially useful in creating environments that describe precisely the conditions for which you would like to test code. You can replace all objects but one with mock objects to isolate the behavior of your focus object and create an enviroment for testing your code.</p>&#13;
<p class="indent">One use case is in writing an HTTP client, since it is likely impossible (or at least extremely complicated) to spawn the HTTP server and test it through all scenarios to return every possible value. HTTP clients are especially difficult to test for all failure scenarios.</p>&#13;
<p class="indent">The standard library for creating mock objects in Python is <code>mock</code>. Starting with Python 3.3, <code>mock</code> has been merged into the Python Standard Library as <code>unittest.mock</code>. You can, therefore, use a snippet like the following to maintain backward compatibility between Python 3.3 and earlier versions:</p>&#13;
<pre>try:<br/>    from unittest import mock<br/>except ImportError:<br/>    import mock</pre>&#13;
<p class="indent">The <code>mock</code> library is pretty simple to use. Any attribute accessed on a <code>mock.Mock</code> object is dynamically created at runtime. Any value can be set to such an attribute. <a href="ch06.xhtml#ch6list7">Listing 6-7</a> shows <code>mock</code> being used to create a fake object with a fake attribute.</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from unittest import mock</span><br/>&gt;&gt;&gt; <span class="codestrong1">m = mock.Mock()</span><br/>&gt;&gt;&gt; <span class="codestrong1">m.some_attribute = "hello world"</span><span epub:type="pagebreak" id="page_85"/><br/>&gt;&gt;&gt; <span class="codestrong1">m.some_attribute</span><br/>"hello world"</pre>&#13;
<p class="listing1"><a id="ch6list7"/><em>Listing 6-7: Accessing the <span class="codeitalic">mock.Mock</span> attribute</em></p>&#13;
<p class="indent">You can also dynamically create a method on a malleable object, as in <a href="ch06.xhtml#ch6list8">Listing 6-8</a> where we create a fake method that always returns 42 and accepts anything as an argument.</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from unittest import mock</span><br/>&gt;&gt;&gt; <span class="codestrong1">m = mock.Mock()</span><br/>&gt;&gt;&gt; <span class="codestrong1">m.some_method.return_value = 42</span><br/>&gt;&gt;&gt; <span class="codestrong1">m.some_method()</span><br/>42<br/>&gt;&gt;&gt; <span class="codestrong1">m.some_method("with", "arguments")</span><br/>42</pre>&#13;
<p class="listing1"><a id="ch6list8"/><em>Listing 6-8: Creating methods on a <span class="codeitalic">mock.Mock</span> object</em></p>&#13;
<p class="indent">In just a few lines, your <code>mock.Mock</code> object now has a <code>some_method()</code> method that returns 42. It accepts any kind of argument, and there is no check on what the values are—yet.</p>&#13;
<p class="indent">Dynamically created methods can also have (intentional) side effects. Rather than being boilerplate methods that just return a value, they can be defined to execute useful code.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch6list9">Listing 6-9</a> creates a fake method that has the side effect of printing the "<code>hello world</code>" string.</p>&#13;
<pre>   &gt;&gt;&gt; <span class="codestrong1">from unittest</span> <span class="codestrong1">import mock</span><br/>   &gt;&gt;&gt; <span class="codestrong1">m = mock.Mock()</span><br/>   &gt;&gt;&gt; <span class="codestrong1">def print_hello():</span><br/>   ...     <span class="codestrong1">print("hello world!")</span><br/>   ...     <span class="codestrong1">return 43</span><br/>   ...<br/><span class="ent">➊</span> &gt;&gt;&gt; <span class="codestrong1">m.some_method.side_effect = print_hello</span><br/>   &gt;&gt;&gt; <span class="codestrong1">m.some_method()</span><br/>   hello world!<br/>   43<br/><span class="ent">➋</span> &gt;&gt;&gt; <span class="codestrong1">m.some_method.call_count</span><br/>   1</pre>&#13;
<p class="listing1"><a id="ch6list9"/><em>Listing 6-9: Creating methods on a <span class="codeitalic">mock.Mock</span> object with side effects</em></p>&#13;
<p class="indent">We assign an entire function to the <code>some_method</code> attribute <span class="ent">➊</span>. This technique allows us to implement more complex scenarios in a test because we can plug any code needed for testing into a mock object. We then just need to pass this mock object to whichever function expects it.</p>&#13;
<p class="indent">The <code>call_count</code> attribute <span class="ent">➋</span> is a simple way of checking the number of times a method has been called.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_86"/>The <code>mock</code> library uses the action/assertion pattern: this means that once your test has run, it’s up to you to check that the actions you are mocking were correctly executed. <a href="ch06.xhtml#ch6list10">Listing 6-10</a> applies the <code>assert()</code> method to our mock objects to perform these checks.</p>&#13;
<pre>   &gt;&gt;&gt; <span class="codestrong1">from unittest import mock</span><br/>   &gt;&gt;&gt; <span class="codestrong1">m = mock.Mock()</span><br/><span class="ent">➊</span> &gt;&gt;&gt; <span class="codestrong1">m.some_method('foo', 'bar')</span><br/>   <span class="codeitalic1">&lt;Mock name</span>='<span class="codeitalic1">mock.some_method()</span>' id='26144272'<span class="codeitalic1">&gt;</span><br/><span class="ent">➋</span> &gt;&gt;&gt; <span class="codestrong1">m.some_method.assert_called_once_with('foo', 'bar')</span><br/>   &gt;&gt;&gt; <span class="codestrong1">m.some_method.assert_called_once_with('foo',</span> <span class="ent">➌</span><span class="codestrong1">mock.ANY)</span><br/>   &gt;&gt;&gt; <span class="codestrong1">m.some_method.assert_called_once_with('foo', 'baz')</span><br/>   Traceback (most recent call last):<br/>     File "<span class="codeitalic1">&lt;stdin&gt;</span>", line 1, in <span class="codeitalic1">&lt;module&gt;</span><br/>     File "/usr/lib/python2.7/dist-packages/mock.py", line 846, in assert_called_<br/>   once_with<br/>       return self.assert_called_with(*args, **kwargs)<br/>     File "/usr/lib/python2.7/dist-packages/mock.py", line 835, in assert_called_<br/>   with<br/>       raise AssertionError(msg)<br/>   AssertionError: Expected call: some_method('foo', 'baz')<br/>   Actual call: some_method('foo', 'bar')</pre>&#13;
<p class="listing1"><a id="ch6list10"/><em>Listing 6-10: Checking method calls</em></p>&#13;
<p class="indent">We create a method with the arguments <code>foo</code> and <code>bar</code> to stand in as our tests by calling the method <span class="ent">➊</span>. The usual way to check calls to a mock object is to use the <code>assert_called()</code> methods, such as <code>assert_called_once_with()</code> <span class="ent">➋</span>. To these methods, you need to pass the values that you expect callers to use when calling your mock method. If the values passed are not the ones being used, then <code>mock</code> raises an <code>AssertionError</code>. If you don’t know what arguments may be passed, you can use <code>mock.ANY</code> as a value <span class="ent">➌</span>; that will match any argument passed to your mock method.</p>&#13;
<p class="indent">Th <code>mock</code> library can also be used to patch some function, method, or object from an external module. In <a href="ch06.xhtml#ch6list11">Listing 6-11</a>, we replace the <code>os.unlink()</code> function with a fake function we provide.</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from unittest import mock</span><br/>&gt;&gt;&gt; <span class="codestrong1">import os</span><br/>&gt;&gt;&gt; <span class="codestrong1">def fake_os_unlink(path):</span><br/>...     <span class="codestrong1">raise IOError("Testing!")</span><br/>...<br/>&gt;&gt;&gt; <span class="codestrong1">with mock.patch('os.unlink', fake_os_unlink):</span><br/>...     <span class="codestrong1">os.unlink('foobar')</span><br/>...<br/>Traceback (most recent call last):<br/>  File "&lt;<span class="codeitalic1">stdin</span>&gt;", line 2, in &lt;<span class="codeitalic1">module</span>&gt;<br/>  File "&lt;<span class="codeitalic1">stdin</span>&gt;", line 2, in fake_os_unlink<br/>IOError: Testing!</pre>&#13;
<p class="listing1"><a id="ch6list11"/><em>Listing 6-11: Using <span class="codeitalic">mock.patch</span></em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_87"/>When used as a context manager, <code>mock.patch()</code> replaces the target function with the function we provide so the code executed inside the context uses that patched method. With the <code>mock.patch()</code> method, it’s possible to change any part of an external piece of code, making it behave in a way that lets you test all conditions in your application, as shown in <a href="ch06.xhtml#ch6list12">Listing 6-12</a>.</p>&#13;
<pre>   from unittest import mock<br/><br/>   import pytest<br/>   import requests<br/><br/>   class WhereIsPythonError(Exception):<br/>       pass<br/><br/><span class="ent">➊</span> def is_python_still_a_programming_language():<br/>       try:<br/>           r = requests.get("http://python.org")<br/>       except IOError:<br/>           pass<br/>       else:<br/>           if r.status_code == 200:<br/>               return 'Python is a programming language' in r.content<br/>       raise WhereIsPythonError("Something bad happened")<br/><br/>   def get_fake_get(status_code, content):<br/>       m = mock.Mock()<br/>       m.status_code = status_code<br/>       m.content = content<br/><br/>       def fake_get(url):<br/>           return m<br/><br/>       return fake_get<br/><br/>   def raise_get(url):<br/>       raise IOError("Unable to fetch url %s" % url)<br/><br/><span class="ent">➋</span> @mock.patch('requests.get', get_fake_get(<br/>       200, 'Python is a programming language for sure'))<br/>   def test_python_is():<br/>       assert is_python_still_a_programming_language() is True<br/><br/>   @mock.patch('requests.get', get_fake_get(<br/>       200, 'Python is no more a programming language'))<br/>   def test_python_is_not():<br/>       assert is_python_still_a_programming_language() is False<br/><br/>   @mock.patch('requests.get', get_fake_get(404, 'Whatever'))<br/>   def test_bad_status_code():<br/>       with pytest.raises(WhereIsPythonError):<br/>           is_python_still_a_programming_language()<br/><br/>   @mock.patch('requests.get', raise_get)<br/>   def test_ioerror():<br/><span epub:type="pagebreak" id="page_88"/>       with pytest.raises(WhereIsPythonError):<br/>           is_python_still_a_programming_language()</pre>&#13;
<p class="listing1"><a id="ch6list12"/><em>Listing 6-12: Using <span class="codeitalic">mock.patch()</span> to test a set of behaviors</em></p>&#13;
<p class="indent"><a href="ch06.xhtml#ch6list12">Listing 6-12</a> implements a test suite that searches for all instances of the string “Python is a programming language” on the <em><a href="http://python.org/">http://python.org/</a></em> web page <span class="ent">➊</span>. There is no way to test negative scenarios (where this sentence is not on the web page) without modifying the page itself—something we’re not able to do, obviously. In this case, we’re using <code>mock</code> to cheat and change the behavior of the request so it returns a mocked reply with a fake page that doesn’t contain that string. This allows us to test the negative scenario in which <em><a href="http://python.org/">http://python.org/</a></em> does not contain this sentence, making sure the program handles that case correctly.</p>&#13;
<p class="indent">This example uses the decorator version of <code>mock.patch()</code> <span class="ent">➋</span>. Using the decorator does not change the mocking behavior, but it is simpler when you need to use mocking within the context of an entire test function.</p>&#13;
<p class="indent">Using mocking, we can simulate any problem, such as a web server returning a 404 error, an I/O error, or a network latency issue. We can make sure code returns the correct values or raises the correct exception in every case, ensuring our code always behaves as expected.</p>&#13;
<h4 class="h4" id="lev2sec28"><strong><em>Revealing Untested Code with coverage</em></strong></h4>&#13;
<p class="noindent">A great complement to unit testing, the <code>coverage</code> tool identifies whether any of your code has been missed during testing. It uses code analysis tools and tracing hooks to determine which lines of your code have been executed; when used during a unit test run, it can show you which parts of your codebase have been crossed over and which parts have not. Writing tests is useful, but having a way to know what part of your code you may have missed during the testing process is the cherry on the cake.</p>&#13;
<p class="indent">Install the <code>coverage</code> Python module on your system via <code>pip</code> to have access to the <code>coverage</code> program command from your shell.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The command may also be named <span class="codeitalic">python-coverage</span>, if you install <span class="codeitalic">coverage</span> through your operating system installation software. This is the case on Debian, for example.</em></p>&#13;
</div>&#13;
<p class="indent">Using <code>coverage</code> in stand-alone mode is straightforward. It can show you parts of your programs that are never run and which code might be “dead code,” that is, code that could be removed without modifying the normal workflow of the program. All the test tools we’ve talked about so far in this chapter are integrated with <code>coverage</code>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_89"/>When using <code>pytest</code>, just install the <code>pytest-cov</code> plugin via <code>pip install pytest-pycov</code> and add a few option switches to generate a detailed code coverage output, as shown in <a href="ch06.xhtml#ch6list13">Listing 6-13</a>.</p>&#13;
<pre>$ <span class="codestrong1">pytest --cov=gnocchiclient gnocchiclient/tests/unit</span><br/>---------- coverage: platform darwin, python 3.6.4-final-0 -----------<br/>Name                                          Stmts   Miss Branch BrPart  Cover<br/>---------------------------<br/>gnocchiclient/__init__.py                         0      0      0      0   100%<br/>gnocchiclient/auth.py                            51     23      6      0    49%<br/>gnocchiclient/benchmark.py                      175    175     36      0     0%<br/>--<span class="codeitalic1">snip</span>--<br/>---------------------------<br/>TOTAL                                          2040   1868    424      6     8%<br/><br/>=== passed in 5.00 seconds ===</pre>&#13;
<p class="listing1"><a id="ch6list13"/><em>Listing 6-13: Using coverage with <span class="codeitalic">pytest</span></em></p>&#13;
<p class="indent">The <code>--cov</code> option enables the coverage report at the end of the test run. You need to pass the package name as an argument for the plugin to filter the coverage report properly. The output includes the lines of code that were not run and therefore have no tests. All you need to do now is spawn your favorite text editor and start writing tests for that code.</p>&#13;
<p class="indent">However, <code>coverage</code> goes one better, allowing you to generate clear HTML reports. Simply add the <code>--cov-report=html</code> flag, and the <em>htmlcov</em> directory from which you ran the command will be populated with HTML pages. Each page will show you which parts of your source code were or were not run.</p>&#13;
<p class="indent">If you want to be <em>that</em> person, you can use the option <code>--cover-fail-under=COVER_MIN_PERCENTAGE</code>, which will make the test suite fail if a minimum percentage of the code is not executed when the test suite is run. While having a good coverage percentage is a decent goal, and while the tool is useful to gain insight into the state of your test coverage, defining an arbitrary percentage value does not provide much insight. <a href="ch06.xhtml#ch6fig1">Figure 6-1</a> shows an example of a coverage report with the percentage at the top.</p>&#13;
<p class="indent">For example, a code coverage score of 100 percent is a respectable goal, but it does not necessarily mean the code is entirely tested and you can rest. It only proves that your whole code path has been run; there is no indication that every possible condition has been tested.</p>&#13;
<p class="indent">You should use coverage information to consolidate your test suite and add tests for any code that is currently not being run. This facilitates later project maintenance and increases your code’s overall quality.</p>&#13;
<div class="image"><span epub:type="pagebreak" id="page_90"/><a id="ch6fig1"/><img alt="image" src="../images/f06-01.jpg"/></div>&#13;
<p class="figcap"><em>Figure 6-1: Coverage of <span class="codeitalic">ceilometer.publisher</span></em></p>&#13;
<h3 class="h3" id="lev1sec31"><strong>Virtual Environments</strong></h3>&#13;
<p class="noindent">Earlier we mentioned the danger that your tests may not capture the absence of dependencies. Any application of significant size inevitably depends on external libraries to provide features the application needs, but there are many ways external libraries might cause issues on your operating system. Here are a few:</p>&#13;
<ul>&#13;
<li><p class="noindent">Your system does not have the library you need packaged.</p></li>&#13;
<li><p class="noindent">Your system does not have the right <em>version</em> of the library you need packaged.</p></li>&#13;
<li><p class="noindent">You need two different versions of the same library for two different applications.</p></li>&#13;
</ul>&#13;
<p class="indent">These problems can happen when you first deploy your application or later on, while it’s running. Upgrading a Python library installed via your <span epub:type="pagebreak" id="page_91"/>system manager might break your application in a snap without warning, for reasons as simple as an API change in the library being used by the application.</p>&#13;
<p class="indent">The solution is for each application to use a library directory that contains all the application’s dependencies. This directory is then used to load the needed Python modules rather than the system-installed ones.</p>&#13;
<p class="indent">Such a directory is known as a <em>virtual environment</em>.</p>&#13;
<h4 class="h4" id="lev2sec29"><strong><em>Setting Up a Virtual Environment</em></strong></h4>&#13;
<p class="noindent">The tool <code>virtualenv</code> handles virtual environments automatically for you. Until Python 3.2, you’ll find it in the <code>virtualenv</code> package that you can install using <code>pip install virtualenv</code>. If you use Python 3.3 or later, it’s available directly via Python under the <code>venv</code> name.</p>&#13;
<p class="indent">To use the module, load it as the main program with a destination directory as its argument, like so:</p>&#13;
<pre>$ <span class="codestrong1">python3 -m venv myvenv</span><br/>$ <span class="codestrong1">ls foobar</span><br/>bin        include    lib        pyvenv.cfg</pre>&#13;
<p class="indent">Once run, <code>venv</code> creates a <em>lib/pythonX.Y</em> directory and uses it to install <code>pip</code> into the virtual environment, which will be useful to install further Python packages.</p>&#13;
<p class="indent">You can then activate the virtual environment by “sourcing” the <code>activate</code> command. Use the following on Posix systems:</p>&#13;
<pre>$ <span class="codestrong1">source myvenv/bin/activate</span></pre>&#13;
<p class="indent">On Windows systems, use this code:</p>&#13;
<pre>&gt; <span class="codestrong1">\myvenv\Scripts\activate</span></pre>&#13;
<p class="indent">Once you do that, your shell prompt should appear prefixed by the name of your virtual environment. Executing <code>python</code> will call the version of Python that has been copied into the virtual environment. You can check that it’s working by reading the <code>sys.path</code> variable and checking that it has your virtual environment directory as its first component.</p>&#13;
<p class="indent">You can stop and leave the virtual environment at any time by calling the <code>deactivate</code> command:</p>&#13;
<pre>$ <span class="codestrong1">deactivate</span></pre>&#13;
<p class="indent">That’s it. Also note that you are not forced to run <code>activate</code> if you want to use the Python installed in your virtual environment just once. Calling the <code>python</code> binary will also work:</p>&#13;
<pre>$ <span class="codestrong1">myvenv/bin/python</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_92"/>Now, while we’re in our activated virtual environment, we do not have access to any of the modules installed and available on the main system. That is the point of using a virtual environment, but it does mean we probably need to install the packages we need. To do that, use the standard <code>pip</code> command to install each package, and the packages will install in the right place, without changing anything about your system:</p>&#13;
<pre>$ <span class="codestrong1">source myvenv/bin/activate</span><br/>(myvenv) $ <span class="codestrong1">pip install six</span><br/>Downloading/unpacking six<br/>  Downloading six-1.4.1.tar.gz<br/>  Running setup.py egg_info for package six<br/><br/>Installing collected packages: six<br/>  Running setup.py install for six<br/><br/>Successfully installed six<br/>Cleaning up...</pre>&#13;
<p class="indent">Voilà! We can install all the libraries we need and then run our application from this virtual environment, without breaking our system. It’s easy to see how we can script this to automate the installation of a virtual environment based on a list of dependencies, as in <a href="ch06.xhtml#ch6list14">Listing 6-14</a>.</p>&#13;
<pre>virtualenv myappvenv<br/>source myappvenv/bin/activate<br/>pip install -r requirements.txt<br/>deactivate</pre>&#13;
<p class="listing1"><a id="ch6list14"/><em>Listing 6-14: Automatic virtual environment creation</em></p>&#13;
<p class="indent">It can still be useful to have access to your system-installed packages, so <code>virtualenv</code> allows you to enable them when creating your virtual environment by passing the <code>--system-site-packages</code> flag to the <code>virtualenv</code> command.</p>&#13;
<p class="indent">Inside <code>myvenv</code>, you will find a <em>pyvenv.cfg</em>, the configuration file for this environment. It doesn’t have a lot of configuration options by default. You should recognize <code>include-system-site-package</code>, whose purpose is the same as the <code>--system-site-packages</code> of <code>virtualenv</code> that we described earlier.</p>&#13;
<p class="indent">As you might guess, virtual environments are incredibly useful for automated runs of unit test suites. Their use is so widespread that a particular tool has been built to address it.</p>&#13;
<h4 class="h4" id="lev2sec30"><strong><em>Using virtualenv with tox</em></strong></h4>&#13;
<p class="noindent">One of the central uses of virtual environments is to provide a clean environment for running unit tests. It would be detrimental if you were under the impression that your tests were working, when they were not, for example, respecting the dependency list.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_93"/>One way to ensure you’re accounting for all the dependencies would be to write a script to deploy a virtual environment, install <code>setuptools</code>, and then install all of the dependencies required for both your application/library runtime and unit tests. Luckily, this is such a popular use case that an application dedicated to this task has already been built: <code>tox</code>.</p>&#13;
<p class="indent">The <code>tox</code> management tool aims to automate and standardize how tests are run in Python. To that end, it provides everything needed to run an entire test suite in a clean virtual environment, while also installing your application to check that the installation works.</p>&#13;
<p class="indent">Before using <code>tox</code>, you need to provide a configuration file named <em>tox.ini</em> that should be placed in the root directory of your project, beside your <em>setup.py</em> file:</p>&#13;
<pre>$ <span class="codestrong1">touch tox.ini</span></pre>&#13;
<p class="indent">You can then run <code>tox</code> successfully:</p>&#13;
<pre>% <span class="codestrong1">tox</span><br/>GLOB sdist-make: /home/jd/project/setup.py<br/>python create: /home/jd/project/.tox/python<br/>python inst: /home/jd/project/.tox/dist/project-1.zip<br/>____________________ summary _____________________<br/>  python: commands succeeded<br/>  congratulations :)</pre>&#13;
<p class="indent">In this instance, <code>tox</code> creates a virtual environment in <em>.tox/python</em> using the default Python version. It uses <em>setup.py</em> to create a distribution of your package, which it then installs inside this virtual environment. No commands are run, because we did not specify any in the configuration file. This alone is not particularly useful.</p>&#13;
<p class="indent">We can change this default behavior by adding a command to run inside our test environment. Edit <em>tox.ini</em> to include the following:</p>&#13;
<pre>[testenv]<br/>commands=pytest</pre>&#13;
<p class="indent">Now <code>tox</code> runs the command <code>pytest</code>. However, since we do not have <code>pytest</code> installed in the virtual environment, this command will likely fail. We need to list <code>pytest</code> as a dependency to be installed:</p>&#13;
<pre>[testenv]<br/>deps=pytest<br/>commands=pytest</pre>&#13;
<p class="indent">When run now, <code>tox</code> re-creates the environment, installs the new dependency, and runs the command <code>pytest</code>, which executes all of the unit tests. To add more dependencies, you can either list them in the <code>deps</code> configuration option, as is done here, or use the <code>-rfile</code> syntax to read from a file.</p>&#13;
<h4 class="h4" id="lev2sec31"><span epub:type="pagebreak" id="page_94"/><strong><em>Re-creating an Environment</em></strong></h4>&#13;
<p class="noindent">Sometimes you’ll need to re-create an environment to, for example, ensure things work as expected when a new developer clones the source code repository and runs <code>tox</code> for the first time. For this, <code>tox</code> accepts a <code>--recreate</code> option that will rebuild the virtual environment from scratch based on parameters you lay out.</p>&#13;
<p class="indent">You define the parameters for all virtual environments managed by <code>tox</code> in the <code>[testenv]</code> section of <em>tox.ini</em>. And, as mentioned, <code>tox</code> can manage multiple Python virtual environments—indeed, it is possible to run our tests under a Python version other than the default one by passing the <code>-e</code> flag to <code>tox</code>, like so:</p>&#13;
<pre> % <span class="codestrong1">tox -e py26</span><br/> GLOB sdist-make: /home/jd/project/setup.py<br/> py26 create: /home/jd/project/.tox/py26<br/> py26 installdeps: nose<br/> py26 inst: /home/jd/project/.tox/dist/rebuildd-1.zip<br/> py26 runtests: commands[0] | pytests<br/> --<span class="codeitalic1">snip</span>--<br/>== test session starts ==<br/>=== 5 passed in 4.87 seconds ====</pre>&#13;
<p class="indent">By default, <code>tox</code> simulates any environment that matches an existing Python version: <code>py24</code>, <code>py25</code>, <code>py26</code>, <code>py27</code>, <code>py30</code>, <code>py31</code>, <code>py32</code>, <code>py33</code>, <code>py34</code>, <code>py35</code>, <code>py36</code>, <code>py37</code>, <code>jython</code>, and <code>pypy</code>! Furthermore, you can define your own environments. You just need to add another section named <code>[testenv:_envname_]</code>. If you want to run a particular command for just one of the environments, you can do so easily by listing the following in the <em>tox.ini</em> file:</p>&#13;
<pre>[testenv]<br/>deps=pytest<br/>commands=pytest<br/><br/>[testenv:py36-coverage]<br/>deps={[testenv]deps}<br/>      pytest-cov<br/>commands=pytest --cov=myproject</pre>&#13;
<p class="indent">By using <code>pytest --cov=myproject</code> under the <code>py36-coverage</code> section as shown here, you override the commands for the <code>py36-coverage</code> environment, meaning when you run <code>tox -e py36-coverage</code>, <code>pytest</code> is installed as part of the dependencies, but the command <code>pytest</code> is actually run instead with the coverage option. For that to work, the <code>pytest-cov</code> extension must be installed: to this end, we replace the <code>deps</code> value with the <code>deps</code> from <code>testenv</code> and add the <code>pytest-cov</code> dependency. Variable interpolation is also supported by <code>tox</code>, so you can refer to any other field from the <em>tox.ini</em> file and use it as a variable, the syntax being <code>{[<span class="codeitalic">env_name</code>]<span class="codeitalic">variable_name</span>}</span>. This allows us to avoid repeating the same things over and over again.</p>&#13;
<h4 class="h4" id="lev2sec32"><span epub:type="pagebreak" id="page_95"/><strong><em>Using Different Python Versions</em></strong></h4>&#13;
<p class="noindent">We can also create a new environment with an unsupported version of Python right away with the following in <em>tox.ini</em>:</p>&#13;
<pre>[testenv]<br/>deps=pytest<br/>commands=pytest<br/><br/>[testenv:py21]<br/>basepython=python2.1</pre>&#13;
<p class="indent">When we run this, it will now (attempt to) use Python 2.1 to run the test suite—although since it is very unlikely you have this ancient Python version installed on your system, I doubt this would work for you!</p>&#13;
<p class="indent">It’s likely that you’ll want to support multiple Python versions, in which case it would be useful to have <code>tox</code> run all the tests for all the Python versions you want to support by default. You can do this by specifying the environment list you want to use when <code>tox</code> is run without arguments:</p>&#13;
<pre>[tox]<br/>envlist=py35,py36,pypy<br/><br/>[testenv]<br/>deps=pytest<br/>commands=pytest</pre>&#13;
<p class="indent">When <code>tox</code> is launched without any further arguments, all four environments listed are created, populated with the dependencies and the application, and then run with the command <code>pytest</code>.</p>&#13;
<h4 class="h4" id="lev2sec33"><strong><em>Integrating Other Tests</em></strong></h4>&#13;
<p class="noindent">We can also use <code>tox</code> to integrate tests like <code>flake8</code>, as discussed in <a href="ch01.xhtml#ch01">Chapter 1</a>. The following <em>tox.ini</em> file provides a PEP 8 environment that will install <code>flake8</code> and run it:</p>&#13;
<pre>[tox]<br/>envlist=py35,py36,pypy,pep8<br/><br/>[testenv]<br/>deps=pytest<br/>commands=pytest<br/><br/>[testenv:pep8]<br/>deps=flake8<br/>commands=flake8</pre>&#13;
<p class="indent">In this case, the <code>pep8</code> environment is run using the default version of Python, which is probably fine, though you can still specify the <code>basepython</code> option if you want to change that.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_96"/>When running <code>tox</code>, you’ll notice that all the environments are built and run sequentially. This can make the process very long, but since virtual environments are isolated, nothing prevents you from running <code>tox</code> commands in parallel. This is exactly what the <code>detox</code> package does, by providing a <code>detox</code> command that runs all of the default environments from <em>envlist</em> in parallel. You should <code>pip install</code> it!</p>&#13;
<h3 class="h3" id="lev1sec32"><strong>Testing Policy</strong></h3>&#13;
<p class="noindent">Embedding testing code in your project is an excellent idea, but how that code is run is also extremely important. Too many projects have test code lying around that fails to run for some reason or other. This topic is not strictly limited to Python, but I consider it important enough to emphasize here: you should have a zero-tolerance policy regarding untested code. No code should be merged without a proper set of unit tests to cover it.</p>&#13;
<p class="indent">The minimum you should aim for is that each of the commits you push passes all the tests. Automating this process is even better. For example, OpenStack relies on a specific workflow based on <em>Gerrit</em> (a web-based code review service) and <em>Zuul</em> (a continuous integration and delivery service). Each commit pushed goes through the code review system provided by Gerrit, and Zuul is in charge of running a set of testing jobs. Zuul runs the unit tests and various higher-level functional tests for each project. This code review, which is executed by a couple of developers, makes sure all code committed has associated unit tests.</p>&#13;
<p class="indent">If you’re using the popular GitHub hosting service, <em>Travis CI</em> is a tool that allows you to run tests after each push or merge or against pull requests that are submitted. While it is unfortunate that this testing is done post-push, it’s still a fantastic way to track regressions. Travis supports all significant Python versions out of the box, and it can be customized significantly. Once you’ve activated Travis on your project via the web interface at <em><a href="https://www.travis-ci.org/">https://www.travis-ci.org/</a></em>, just add a <em>.travis.yml</em> file that will determine how the tests are run. <a href="ch06.xhtml#ch6list15">Listing 6-15</a> shows an example of a .<em>travis.yml</em> file.</p>&#13;
<pre>language: python<br/>python:<br/>  - "2.7"<br/>  - "3.6"<br/># command to install dependencies<br/>install: "pip install -r requirements.txt --use-mirrors"<br/># command to run tests<br/>script: pytest</pre>&#13;
<p class="listing1"><a id="ch6list15"/><em>Listing 6-15: A .<span class="roman">travis.yml</span> example file</em></p>&#13;
<p class="indent">With this file in place in your code repository and Travis enabled, the latter will spawn a set of jobs to test your code with the associated unit tests. It’s easy to see how you can customize this by simply adding dependencies and tests. Travis is a paid service, but the good news is that for open source projects, it’s entirely free!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_97"/>The <code>tox-travis</code> package (<em><a href="https://pypi.python.org/pypi/tox-travis/">https://pypi.python.org/pypi/tox-travis/</a></em>) is also worth looking into, as it will polish the integration between <code>tox</code> and Travis by running the correct <code>tox</code> target depending on the Travis environment being used. <a href="ch06.xhtml#ch6list16">Listing 6-16</a> shows an example of a <em>.travis.yml</em> file that will install <code>tox-travis</code> before running <code>tox</code>.</p>&#13;
<pre>sudo: false<br/>language: python<br/>python:<br/>  - "2.7"<br/>  - "3.4"<br/>install: pip install tox-travis<br/>script: tox</pre>&#13;
<p class="listing1"><a id="ch6list16"/><em>Listing 6-16: A <span class="roman">.travis.yml</span> example file with <span class="codeitalic">tox-travis</span></em></p>&#13;
<p class="indent">Using <code>tox-travis</code>, you can simply call <code>tox</code> as the script on Travis, and it will call <code>tox</code> with the environment you specify here in the <em>.travis.yml</em> file, building the necessary virtual environment, installing the dependency, and running the commands you specified in <em>tox.ini</em>. This makes it easy to use the same workflow both on your local development machine and on the Travis continuous integration platform.</p>&#13;
<p class="indent">These days, wherever your code is hosted, it is always possible to apply some automatic testing of your software and to make sure your project is moving forward, not being held back by the addition of bugs.</p>&#13;
<h3 class="h3" id="lev1sec33"><strong>Robert Collins on Testing</strong></h3>&#13;
<p class="noindent">Robert Collins is, among other things, the original author of the <em>Bazaar</em> distributed version control system. Today, he is a Distinguished Technologist at HP Cloud Services, where he works on OpenStack. Robert is also the author of many of the Python tools described in this book, such as fixtures, <code>testscenarios</code>, <code>testrepository</code>, and even <code>python-subunit</code>—you may have used one of his programs without knowing it!</p>&#13;
<p class="noindentt"><strong>What kind of testing policy would you advise using? Is it ever acceptable not to test code?</strong></p>&#13;
<p class="noindent">I think testing is an engineering trade-off: you must consider the likelihood of a failure slipping through to production undetected, the cost and size of an undetected failure, and cohesion of the team doing the work. Take OpenStack, which has 1,600 contributors: it’s difficult to work with a nuanced policy with so many people with their own opinions. Generally speaking, a project needs some automated testing to check that the code will do what it is intended to do, and that what it is intended to do is what is needed. Often that requires functional tests that might be in different codebases. Unit tests are excellent for speed and pinning down corner cases. I think it is okay to vary the balance between styles of testing, as long as there is testing.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_98"/>Where the cost of testing is very high and the returns are very low, I think it’s fine to make an informed decision not to test, but that situation is relatively rare: most things can be tested reasonably cheaply, and the benefit of catching errors early is usually quite high.</p>&#13;
<p class="noindentt"><strong>What are the best strategies when writing Python code to make testing manageable and improve the quality of the code?</strong></p>&#13;
<p class="noindent">Separate out concerns and don’t do multiple things in one place; this makes reuse natural, and that makes it easier to put test doubles in place. Take a purely functional approach when possible; for example, in a single method either calculate something or change some state, but avoid doing both. That way you can test all of the calculating behaviors without dealing with state changes, such as writing to a database or talking to an HTTP server. The benefit works the other way around too—you can replace the calculation logic for tests to provoke corner case behavior and use mocks and test doubles to check that the expected state propagation happens as desired. The most heinous things to test are deeply layered stacks with complex cross-layer behavioral dependencies. There you want to evolve the code so that the contract between layers is simple, predictable, and—most usefully for testing—replaceable.</p>&#13;
<p class="noindentt"><strong>What’s the best way to organize unit tests in source code?</strong></p>&#13;
<p class="noindent">Have a clear hierarchy, like <em>$ROOT/$PACKAGE/tests</em>. I tend to do just one hierarchy for a whole source tree, for example <em>$ROOT/$PACKAGE/$SUBPACKAGE/tests</em>.</p>&#13;
<p class="indent">Within tests, I often mirror the structure of the rest of the source tree: <em>$ROOT/$PACKAGE/foo.py</em> would be tested in <em>$ROOT/$PACKAGE/tests/test_foo.py</em>.</p>&#13;
<p class="indent">The rest of the tree should not import from the tests tree, except perhaps in the case of a <code>test_suite</code>/<code>load_tests</code> function in the top level <code>__init__</code>. This permits you to easily detach the tests for small-footprint installations.</p>&#13;
<p class="noindentt"><strong>What do you see as the future of unit-testing libraries and frameworks in Python?</strong></p>&#13;
<p class="noindent">The significant challenges I see are these:</p>&#13;
<ul>&#13;
<li><p class="noindent">The continued expansion of parallel capabilities in new machines, like phones with four CPUs. Existing unit test internal APIs are not optimized for parallel workloads. My work on the StreamResult Java class is aimed directly at resolving this.</p></li>&#13;
<li><p class="noindent">More complex scheduling support—a less ugly solution for the problems that class and module-scoped setup aim at.</p></li>&#13;
<li><p class="noindent">Finding some way to consolidate the vast variety of frameworks we have today: for integration testing, it would be great to be able to get a consolidated view across multiple projects that have different test runners in use.</p></li>&#13;
</ul>&#13;
</body></html>