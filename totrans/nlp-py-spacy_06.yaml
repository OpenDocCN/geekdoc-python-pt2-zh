- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: FINDING PATTERNS AND WALKING DEPENDENCY TREES**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找模式和遍历依赖树**
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: If you want your application to categorize a text, extract specific phrases
    from it, or determine how semantically similar it is to another text, it must
    be able to “understand” an utterance submitted by a user and generate a meaningful
    response to it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望应用程序能够对文本进行分类，提取其中的特定短语，或者确定它与另一篇文本的语义相似度，它必须能够“理解”用户提交的句子并生成有意义的响应。
- en: 'You’ve already learned some techniques for performing these tasks. This chapter
    discusses two more approaches: using word sequence patterns to classify and generate
    text, and walking the syntactic dependency tree of an utterance to extract necessary
    pieces of information from it. I’ll introduce you to spaCy’s Matcher tool to find
    patterns. I’ll also discuss when you might still need to rely on context to determine
    the proper processing approach.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了一些执行这些任务的技巧。本章讨论另外两种方法：使用词序列模式进行文本分类和生成，以及遍历句子的句法依赖树来提取所需的信息。我将向你介绍spaCy的Matcher工具来寻找模式。我还会讨论在何时你仍然需要依赖上下文来决定正确的处理方法。
- en: '**Word Sequence Patterns**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**词序列模式**'
- en: 'A *word sequence pattern* consists of features of words that impose a certain
    requirement on each word in the sequence. For example, the phrase “I can” will
    match the following word sequence pattern: “pronoun + modal auxiliary verb.” By
    searching for word sequence patterns, you can recognize word sequences with similar
    linguistic features, making it possible to categorize input and handle it properly.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*词序列模式*由词语的特征组成，这些特征对序列中每个词语提出了特定要求。例如，短语“I can”将匹配以下词序列模式：“代词 + 情态动词”。通过寻找词序列模式，你可以识别出具有相似语言特征的词序列，从而能够对输入进行分类并妥善处理。
- en: For example, when you receive a question that begins with a word sequence that
    uses the pattern “modal auxiliary verb + proper noun,” such as “Can George,” you
    know that this question is about the ability, possibility, permission, or obligation
    of someone or something that the proper noun refers to.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你收到一个以“情态动词 + 专有名词”这一模式开头的问题时，例如“Can George”，你就知道这个问题是在询问某个或某物的能力、可能性、许可或义务，这个专有名词所指代的对象。
- en: In the following sections, you’ll learn to classify sentences by identifying
    common patterns of linguistic features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将通过识别语言特征的常见模式来学习如何分类句子。
- en: '***Finding Patterns Based on Linguistic Features***'
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基于语言特征的模式识别***'
- en: We need to find patterns in texts because, in most cases, we won’t be able to
    find even two identical sentences within a text. Typically, a text is composed
    of different sentences, each of which contains different words. It would be impractical
    to write the code to process each sentence in a text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在文本中寻找模式，因为在大多数情况下，我们无法在文本中找到两个完全相同的句子。通常，一篇文本由不同的句子组成，每个句子包含不同的单词。为每个句子编写处理代码将是非常不现实的。
- en: 'Fortunately, some sentences that look completely different might follow the
    same word sequence patterns. For example, consider the following two sentences:
    “We can overtake them.” and “You must specify it.”. These sentences have no words
    in common. But if you look at the syntactic dependency labels assigned to the
    words in the sentences, a pattern emerges, as shown in the following script:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一些看似完全不同的句子可能遵循相同的词序列模式。例如，考虑以下两个句子：“我们可以超过他们。”和“你必须指定它。”这两个句子没有共同的词语。但如果你看一下句子中词语所分配的句法依赖标签，就会发现一个模式，如以下脚本所示：
- en: import spacy
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc1 = nlp(u'We can overtake them.')
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: doc1 = nlp(u'我们可以超过他们。')
- en: doc2 = nlp(u'You must specify it.')
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: doc2 = nlp(u'你必须指定它。')
- en: '➊ for i in range(len(doc1)-1):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ for i in range(len(doc1)-1):'
- en: '➋ if doc1[i].dep_ == doc2[i].dep_:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 doc1[i].dep_ == doc2[i].dep_：
- en: ➌ print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))
- en: Because both sentences have the same number of words, we can iterate over the
    words in both sentences within a single loop ➊. If the dependency label is the
    same for the words that have the same index in both sentences ➋, we print these
    words along with the label assigned to them, as well as a description for each
    label ➌.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因为两个句子的词数相同，我们可以在一个循环中遍历两个句子中的单词 ➊。如果在两个句子中，具有相同索引的单词的依赖标签相同 ➋，我们就会打印这些单词及其对应的标签，以及每个标签的描述
    ➌。
- en: 'The output should look as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: We       You     nsubj  nominal subject
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们      你      nsubj  主语
- en: can      must    aux    auxiliary
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: can      must    aux    助动词
- en: overtake specify ROOT   None
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: overtake specify ROOT   无
- en: them     it      dobj   direct object
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: them     it      dobj   直接宾语
- en: 'As you can see, the list of dependency labels is identical for both sentences.
    This means that these sentences follow the same word sequence pattern based on
    the following syntactic dependency labels: “subject + auxiliary + verb + direct
    object.”'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，依赖标签的列表对于这两个句子是相同的。这意味着这两个句子遵循相同的词序模式，基于以下句法依赖标签：“主语 + 助动词 + 动词 + 直接宾语”。
- en: 'Also notice that the list of part-of-speech tags (coarse-grained and fined-grained)
    is also identical for these sample sentences. If we replace all references to
    .dep_ with .pos_ in the previous script, we’ll get the following results:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，词性标签（粗粒度和细粒度）的列表对于这些示例句子也是相同的。如果我们将前面脚本中所有对 .dep_ 的引用替换为 .pos_，我们将得到以下结果：
- en: We       You     PRON  pronoun
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们      你      PRON  代词
- en: can      must    VERB  verb
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: can      must    VERB  动词
- en: overtake specify VERB  verb
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: overtake specify VERB  动词
- en: them     it      PRON  pronoun
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: them     it      PRON  代词
- en: The sample sentences match not only the syntactic dependency label pattern,
    but also the pattern of part-of-speech tags.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例句子不仅匹配了句法依赖标签模式，还匹配了词性标签的模式。
- en: '***Try This***'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***尝试这个***'
- en: In the previous example, we created two Doc objects—one for each sample sentence.
    But in practice, a text usually consists of numerous sentences, which makes a
    Doc-per-sentence approach impractical. Rewrite the script so it creates a single
    Doc object. Then use the doc.sents property introduced in [Chapter 2](../Text/ch02.xhtml#ch02)
    to operate on each sentence.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们创建了两个 Doc 对象——每个示例句子一个。但实际上，一篇文本通常包含多个句子，这使得按句子创建 Doc 对象的方法不太实用。请重写脚本，使其创建一个单一的
    Doc 对象。然后使用在[第 2 章](../Text/ch02.xhtml#ch02)中介绍的 doc.sents 属性来处理每个句子。
- en: 'But note that doc.sents is a generator object, which means it’s not subscriptable—you
    can’t refer to its items by index. To solve this issue, convert the doc.sents
    to a list, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，doc.sents 是一个生成器对象，这意味着它是不可下标访问的——你不能通过索引来引用它的项。为了解决这个问题，可以将 doc.sents
    转换为列表，如下所示：
- en: sents = list(doc.sents)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: sents = list(doc.sents)
- en: And, of course, you can iterate over a doc.sents in a for loop to obtain the
    sents in order, as they’re requested by the loop.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你还可以在 for 循环中迭代 doc.sents，以便按顺序获取句子，正如它们在循环中被请求的那样。
- en: '***Checking an Utterance for a Pattern***'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***检查言论是否符合模式***'
- en: In the preceding example, we compared two sample sentences to find a pattern
    based on their shared linguistic features. But in practice, we’ll rarely want
    to compare sentences to one another to determine whether they share a common pattern.
    Instead, it’ll be more useful to check a submitted sentence against the pattern
    we’re already interested in.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们比较了两个示例句子，以便找到基于它们共享的语言特征的模式。但实际上，我们很少需要将句子相互比较以确定它们是否共享一个共同的模式。相反，将已提交的句子与我们已经感兴趣的模式进行对比会更有用。
- en: 'For example, let’s say we were trying to find utterances in user input that
    express one of the following: ability, possibility, permission, or obligation
    (as opposed to utterances that describe real actions that have occurred, are occurring,
    or occur regularly). For instance, we want to find “I can do it.” but not “I’ve
    done it.”'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们试图在用户输入中找到表达以下之一的言论：能力、可能性、许可或义务（与描述已发生、正在发生或定期发生的实际行为的言论相对）。例如，我们想找到“I
    can do it.”，而不是“I’ve done it.”。
- en: 'To distinguish between utterances, we might check whether an utterance satisfies
    the following pattern: “subject + auxiliary + verb + . . . + direct object . .
    .”. The ellipses indicate that the direct object isn’t necessarily located immediately
    behind the verb, making this pattern a little different from the one in the preceding
    example.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分不同的言论，我们可以检查某个言论是否满足以下模式：“主语 + 助动词 + 动词 + … + 直接宾语 …”。省略号表示直接宾语不一定紧跟在动词后面，这使得这个模式与前面例子中的模式稍有不同。
- en: 'The following sentence satisfies the pattern: “I might send them a card as
    a reminder.”. In this sentence, the noun “card” is a direct object, and the pronoun
    “them” is an indirect object that separates it from the verb “send.” The pattern
    doesn’t specify a position for the direct object in the sentence; it simply requires
    its presence.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子满足这个模式：“I might send them a card as a reminder.”。在这个句子中，名词“card”是直接宾语，而代词“them”是间接宾语，将其与动词“send”分开。这个模式没有指定直接宾语在句子中的位置；它只是要求有直接宾语的存在。
- en: '[Figure 6-1](../Text/ch06.xhtml#ch06fig01) shows a graphical depiction of this
    design:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](../Text/ch06.xhtml#ch06fig01) 显示了此设计的图形表示：'
- en: '![image](../Images/fig6-1.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig6-1.jpg)'
- en: '*Figure 6-1: Checking submitted utterances against a word sequence pattern
    based on linguistic features*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-1：根据语言特征检查提交的语句是否符合词序列模式*'
- en: 'In the following script, we define a function that implements this pattern,
    and then test it on a sample sentence:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下脚本中，我们定义了一个实现该模式的函数，然后在一个示例句子上测试它：
- en: import spacy
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '➊ def dep_pattern(doc):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def dep_pattern(doc):'
- en: '➋ for i in range(len(doc)-1):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ for i in range(len(doc)-1):'
- en: ➌ if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and
- en: 'doc[i+2].dep_ == ''ROOT'':'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'doc[i+2].dep_ == ''ROOT'':'
- en: '➍ for tok in doc[i+2].children:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ for tok in doc[i+2].children:'
- en: 'if tok.dep_ == ''dobj'':'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'if tok.dep_ == ''dobj'':'
- en: ➎ return True
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ return True
- en: ➏ return False
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ return False
- en: ➐ doc = nlp(u'We can overtake them.')
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ doc = nlp(u'我们可以超越他们。')
- en: 'if ➑dep_pattern(doc):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'if ➑dep_pattern(doc):'
- en: print('Found')
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: print('找到')
- en: 'else:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: print('Not found')
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: print('未找到')
- en: In this script, we define the dep_pattern function that takes a Doc object as
    parameter ➊. In the function, we iterate over the Doc object’s tokens ➋, searching
    for a “subject + auxiliary + verb” pattern ➌. If we find this pattern, we check
    whether the verb has a direct object among its syntactic children ➍. Finally,
    if we find a direct object, the function returns True ➎. Otherwise, it returns
    False ➏.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们定义了 dep_pattern 函数，它接受一个 Doc 对象作为参数 ➊。在函数中，我们遍历 Doc 对象的标记 ➋，寻找“主语 +
    助动词 + 动词”模式 ➌。如果找到该模式，我们检查动词是否有直接宾语作为其语法子节点 ➍。最后，如果找到直接宾语，函数返回 True ➎。否则，它返回 False
    ➏。
- en: In the main code, we apply the text-processing pipeline to the sample sentence
    ➐ and send the Doc object to the dep_pattern function ➑, outputting Found if the
    sample satisfies the pattern implemented in the function or Not found otherwise.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在主代码中，我们将文本处理管道应用于示例句子 ➐ 并将 Doc 对象传递给 dep_pattern 函数 ➑，如果示例符合函数中实现的模式，则输出 Found，否则输出
    Not found。
- en: 'Because the sample used in this example satisfies the pattern, the script should
    produce the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因为此示例中使用的示例符合该模式，所以脚本应产生以下输出：
- en: Found
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 找到
- en: You’ll see some examples of using the dep_pattern function in some of the following
    sections.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将看到如何使用 dep_pattern 函数的一些示例。
- en: '***Using spaCy’s Matcher to Find Word Sequence Patterns***'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用 spaCy 的 Matcher 查找词序列模式***'
- en: 'In the previous section, you learned how to find a word sequence pattern in
    a doc by iterating over its tokens and checking their linguistic features. In
    fact, spaCy has a predefined feature for this task called *Matcher*, a tool that
    is specially designed to find sequences of tokens based on pattern rules. For
    example, an implementation of the “subject + auxiliary + verb” pattern with Matcher
    might look like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了如何通过遍历文档中的标记并检查它们的语言特征来查找词序列模式。实际上，spaCy 为此任务提供了一个预定义的功能，称为 *Matcher*，它是一个专门设计用于根据模式规则查找标记序列的工具。例如，使用
    Matcher 实现“主语 + 助动词 + 动词”模式可能看起来像这样：
- en: import spacy
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: from spacy.matcher import Matcher
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: from spacy.matcher import Matcher
- en: nlp = spacy.load("en")
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load("en")
- en: ➊ matcher = Matcher(nlp.vocab)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ matcher = Matcher(nlp.vocab)
- en: ➋ pattern = [{"DEP": "nsubj"}, {"DEP": "aux"}, {"DEP": "ROOT"}]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ pattern = [{"DEP": "nsubj"}, {"DEP": "aux"}, {"DEP": "ROOT"}]
- en: ➌ matcher.add("NsubjAuxRoot", None, pattern)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ matcher.add("NsubjAuxRoot", None, pattern)
- en: doc = nlp(u"We can overtake them.")
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u"我们可以超越他们。")
- en: ➍ matches = matcher(doc)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ matches = matcher(doc)
- en: '➎ for match_id, start, end in matches:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '➎ for match_id, start, end in matches:'
- en: span = doc[start:end]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: span = doc[start:end]
- en: ➏ print("Span: ", span.text)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ print("Span: ", span.text)
- en: print("The positions in the doc are: ", start, "-", end)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: print("在 doc 中的位置是：", start, "-", end)
- en: We create a Matcher instance, passing in the vocabulary object shared with the
    documents the Matcher will work on ➊. Then we define a pattern, specifying the
    dependency labels that a word sequence should match ➋. We add the newly created
    pattern to the Matcher ➌.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个 Matcher 实例，将与文档共享的词汇对象传递给它 ➊。然后，我们定义一个模式，指定单词序列应匹配的依赖标签 ➋。我们将新创建的模式添加到
    Matcher 中 ➌。
- en: Next, we can apply the Matcher to a sample text and obtain the matching tokens
    in a list ➍. Then we iterate over this list ➎, printing out the start and end
    positions of the pattern tokens in the text ➏.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将 Matcher 应用到示例文本中，并获取匹配的标记列表 ➍。然后我们遍历这个列表 ➎，打印出匹配模式标记在文本中的起始和结束位置 ➏。
- en: 'The script should produce the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本应产生以下输出：
- en: 'Span: We can overtake'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'Span: 我们可以超越'
- en: 'The positions in the doc are: 0 - 3'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: doc 中的位置是：0 - 3
- en: Matcher allows you to find a pattern in a text without iterating explicitly
    over the text’s tokens, thus hiding implementation details from you. As a result,
    you can obtain the start and end positions of the words composing a sequence that
    satisfies the specified pattern. This approach can be very useful when you’re
    interested in a sequence of words that immediately follow one another.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Matcher 允许你在文本中找到模式，而不需要显式地遍历文本的每个标记，从而隐藏了实现细节。因此，你可以获得组成满足指定模式的序列的单词的起始和结束位置。这种方法在你对紧接在一起的单词序列感兴趣时非常有用。
- en: But often you need a pattern that includes words scattered over the sentence.
    For example, you might need to implement such patterns as the “subject + auxiliary
    + verb + . . . + direct object . . .” pattern we used in “[Checking an Utterance
    for a Pattern](../Text/ch06.xhtml#lev73)” on [page 77](../Text/ch06.xhtml#page_77).
    The problem is that you don’t know in advance how many words can occur between
    the “subject + auxiliary + verb” sequence and the direct object. Matcher doesn’t
    allow you to define such patterns. For this reason, I’ll define patterns manually
    for the remainder of this chapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常你需要一个包括散布在整个句子中的单词的模式。例如，你可能需要实现类似于我们在“[检查句子模式](../Text/ch06.xhtml#lev73)”中使用的“主语
    + 助动词 + 动词 + . . . + 直接宾语 . . .”的模式。问题是，你无法提前知道“主语 + 助动词 + 动词”序列和直接宾语之间可能出现多少个单词。Matcher
    不允许你定义这样的模式。出于这个原因，本章其余部分我将手动定义模式。
- en: '***Applying Several Patterns***'
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***应用多个模式***'
- en: 'You can apply several matching patterns to an utterance to make sure it satisfies
    all your conditions. For example, you might check an utterance against two patterns:
    one that implements a dependency label sequence (as discussed in “[Checking an
    Utterance for a Pattern](../Text/ch06.xhtml#lev73)” on [page 77](../Text/ch06.xhtml#page_77))
    and one that checks against a sequence of part-of-speech tags. This might be helpful,
    if, say, you want to make sure that the direct object in an utterance is a personal
    pronoun. If so, you can start the procedure of determining the noun that gives
    its meaning to the pronoun and is mentioned elsewhere in the discourse.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以应用多个匹配模式来检查一句话，确保它符合所有条件。例如，你可以将一句话与两个模式进行比较：一个实现依赖标签序列（如在“[检查句子模式](../Text/ch06.xhtml#lev73)”中讨论的，见[第77页](../Text/ch06.xhtml#page_77)）和一个检查词性标签序列的模式。如果你想确保句子中的直接宾语是人称代词，这可能会很有帮助。这样，你就可以开始确定赋予代词意义的名词，并且该名词在对话中其他地方有所提及。
- en: Diagrammatically, this design might look like [Figure 6-2](../Text/ch06.xhtml#ch06fig02).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在图示上，这种设计可能看起来像[图6-2](../Text/ch06.xhtml#ch06fig02)。
- en: '![image](../Images/fig6-2.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig6-2.jpg)'
- en: '*Figure 6-2: Applying several matching patterns to user input*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-2：将多个匹配模式应用于用户输入*'
- en: 'In addition to using the dependency label sequence defined in “[Checking an
    Utterance for a Pattern](../Text/ch06.xhtml#lev73)”, you can define a new function
    by implementing a pattern based on part-of-speech tags. The part-of-speech tag
    pattern might search the sentence to make sure that the subject and the direct
    object are personal pronouns. This new function might implement the following
    pattern: “personal pronoun + modal auxiliary verb + base form verb + . . . + personal
    pronoun . . .”.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用在“[检查句子模式](../Text/ch06.xhtml#lev73)”中定义的依赖标签序列外，你还可以通过实现一个基于词性标签的模式来定义一个新函数。词性标签模式可能会检查句子，确保主语和直接宾语是人称代词。这个新函数可能会实现如下模式：“人称代词
    + 情态助动词 + 基本形式动词 + . . . + 人称代词 . . .”。
- en: 'Here is the code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: import spacy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 spacy
- en: nlp = spacy.load('en')
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Insert the dep_pattern function from a previous listing here'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '# 在此插入之前列表中的 dep_pattern 函数'
- en: '#...'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#...'
- en: '➊ def pos_pattern(doc):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 定义 pos_pattern(doc)：
- en: '➋ for token in doc:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 对于 token 在 doc 中：
- en: 'if token.dep_ == ''nsubj'' and token.tag_ != ''PRP'':'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.dep_ == 'nsubj' 并且 token.tag_ != 'PRP'：
- en: return False
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 False
- en: 'if token.dep_ == ''aux'' and token.tag_ != ''MD'':'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.dep_ == 'aux' 并且 token.tag_ != 'MD'：
- en: return False
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 False
- en: 'if token.dep_ == ''ROOT'' and token.tag_ != ''VB'':'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.dep_ == 'ROOT' 并且 token.tag_ != 'VB'：
- en: return False
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 False
- en: 'if token.dep_ == ''dobj'' and token.tag_ != ''PRP'':'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.dep_ == 'dobj' 并且 token.tag_ != 'PRP'：
- en: return False
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 False
- en: ➌ return True
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 返回 True
- en: '#Testing code'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '# 测试代码'
- en: doc = nlp(u'We can overtake them.')
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'我们可以超过他们。')
- en: '➍ if dep_pattern(doc) and pos_pattern(doc):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 如果 dep_pattern(doc) 和 pos_pattern(doc)：
- en: print('Found')
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 打印('已找到')
- en: 'else:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其他情况：
- en: print('Not found')
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 打印('未找到')
- en: We start by adding the code for the dep_pattern function defined in a previous
    script. To create the second pattern, we define the pos_pattern function ➊, which
    contains a for loop with a series of if statements in it ➋. Each if statement
    checks whether a certain part of a sentence matches a certain part-of-speech tag.
    When the function detects a mismatch, it returns False. Otherwise, after all the
    checks have occurred and no mismatch has been detected, the function returns True
    ➌.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先添加在前一个脚本中定义的dep_pattern函数的代码。为了创建第二个模式，我们定义了pos_pattern函数 ➊，其中包含一个带有一系列if语句的for循环
    ➋。每个if语句检查句子中的某个部分是否与特定的词性标签匹配。当函数检测到不匹配时，它返回False。否则，在所有检查完成且没有检测到不匹配时，函数返回True
    ➌。
- en: 'To test the patterns, we apply the pipeline to a sentence, and then check whether
    the sentence matches both patterns ➍. Because the sample used in this example
    matches both patterns, we should see the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这些模式，我们将管道应用于一个句子，然后检查该句子是否同时匹配两个模式 ➍。由于本示例中使用的样本同时匹配两个模式，我们应该看到以下输出：
- en: Found
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 已找到
- en: 'But if we replace the sample sentence with this one: “I might send them a card
    as a reminder.”, we should see this output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们将示例句子替换为：“I might send them a card as a reminder.”，我们应该看到以下输出：
- en: Not found
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 未找到
- en: The reason is that the sentence doesn’t match the part-of-speech tag pattern,
    because the direct object “card” isn’t a personal pronoun, even though the sentence
    fully satisfies the conditions of the first pattern.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于，句子没有匹配词性标签模式，因为直接宾语“card”不是一个人称代词，即使该句子完全满足第一个模式的条件。
- en: '***Creating Patterns Based on Customized Features***'
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基于自定义特征创建模式***'
- en: When creating a word sequence pattern, you might need to enhance the functionality
    of the linguistic features spaCy provides by customizing them for your needs.
    For example, you might want the preceding script to recognize another pattern
    that distinguishes pronouns according to number (whether they’re singular or plural).
    Once again, this could be useful when you need to find the noun in a previous
    utterance to which the pronoun refers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建词序列模式时，你可能需要通过定制语言特征来增强spaCy提供的功能，以满足你的需求。例如，你可能希望上面的脚本识别另一个模式，根据代词的数目（单数或复数）来区分代词。这在你需要找出代词所指代的前述名词时非常有用。
- en: 'Although spaCy separates nouns by number, it doesn’t do this for pronouns.
    But the ability to recognize whether a pronoun is plural or singular can be very
    useful in the task of meaning recognition or information extraction. For example,
    consider the following discourse:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然spaCy通过数目区分名词，但对于代词并不如此。但识别代词是单数还是复数的能力在语义识别或信息提取任务中非常有用。例如，考虑以下话语：
- en: The trucks are traveling slowly. We can overtake them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 卡车正在缓慢行驶。我们可以超越它们。
- en: If we can establish that the direct object “them” in the second sentence is
    a plural pronoun, we’ll have reason to believe that it refers to the plural noun
    “trucks” in the first sentence. We often use this technique to recognize a pronoun’s
    meaning based on the context.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够确认第二个句子中的直接宾语“them”是复数代词，那么我们有理由相信它指代第一个句子中的复数名词“trucks”。我们经常使用这个技巧根据上下文来识别代词的意义。
- en: The following script defines a pron_pattern function, which finds any direct
    object in the submitted sentence, determines whether that direct object is a personal
    pronoun, and then determines whether the pronoun is singular or plural. The script
    then applies the function to a sample sentence after testing for the two patterns
    defined in “[Checking an Utterance for a Pattern](../Text/ch06.xhtml#lev73)” on
    [page 77](../Text/ch06.xhtml#page_77) and “[Applying Several Patterns](../Text/ch06.xhtml#lev75)”
    on [page 80](../Text/ch06.xhtml#page_80).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本定义了一个pron_pattern函数，该函数查找提交句子中的任何直接宾语，确定该直接宾语是否为人称代词，并进一步确定该代词是单数还是复数。然后，脚本将函数应用于一个样本句子，并测试在[第77页](../Text/ch06.xhtml#page_77)中的“[检查话语的模式](../Text/ch06.xhtml#lev73)”和[第80页](../Text/ch06.xhtml#page_80)中的“[应用多个模式](../Text/ch06.xhtml#lev75)”中定义的两个模式。
- en: import spacy
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Insert the dep_pattern and pos_pattern functions from the previous'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#插入之前的dep_pattern和pos_pattern函数'
- en: listings here
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出
- en: '#...'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#...'
- en: '➊ def pron_pattern(doc):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def pron_pattern(doc):'
- en: ➋ plural = ['we','us','they','them']
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ plural = ['we','us','they','them']
- en: 'for token in doc:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in doc:'
- en: '➌ if token.dep_ == ''dobj'' and token.tag_ == ''PRP'':'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ if token.dep_ == ''dobj'' and token.tag_ == ''PRP'':'
- en: '➍ if token.text in plural:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ if token.text in plural:'
- en: ➎ return 'plural'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ return 'plural'
- en: 'else:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: ➏ return 'singular'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ 返回 'singular'
- en: ➐ return 'not found'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ 返回 '未找到'
- en: doc = nlp(u'We can overtake them.')
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'我们可以超过他们。')
- en: 'if dep_pattern(doc) and pos_pattern(doc):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 dep_pattern(doc) 和 pos_pattern(doc):'
- en: print('Found:', 'the pronoun in position of direct object is',
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: print('找到：', '直接宾语位置的代词是',
- en: pron_pattern(doc))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: pron_pattern(doc))
- en: 'else:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: print('Not found')
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: print('未找到')
- en: We start by adding the dep_pattern and pos_pattern functions defined in “[Checking
    an Utterance for a Pattern](../Text/ch06.xhtml#lev73)” and “[Applying Several
    Patterns](../Text/ch06.xhtml#lev75)” to the script. In the pron_pattern function
    ➊, we define a Python list that includes all the possible plural personal pronouns
    ➋. Next, we define a loop that iterates over the tokens in the submitted sentence,
    looking for a direct object that is a personal pronoun ➌. If we find such a token,
    we check whether it’s in the list of plural personal pronouns ➍. If so, the function
    returns plural ➎. Otherwise, it returns singular ➏. If the function either failed
    to detect a direct object or found one that isn’t a personal pronoun, it returns
    Not found ➐.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过在脚本中添加在“[检查句子是否符合模式](../Text/ch06.xhtml#lev73)”和“[应用多个模式](../Text/ch06.xhtml#lev75)”中定义的
    dep_pattern 和 pos_pattern 函数开始。在 pron_pattern 函数 ➊ 中，我们定义了一个包含所有可能的复数人称代词的 Python
    列表 ➋。接下来，我们定义一个循环，遍历提交句子中的词项，寻找作为直接宾语的人称代词 ➌。如果我们找到了这样的词项，我们就检查它是否在复数人称代词的列表中
    ➍。如果是，那么函数返回复数 ➎。否则，它返回单数 ➏。如果函数未能检测到直接宾语，或者找到了一个不是人称代词的直接宾语，它将返回未找到 ➐。
- en: 'For the sentence “We can overtake them.”, we should get the following output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子“我们可以超过他们。”，我们应该得到以下输出：
- en: 'Found: the pronoun in position of direct object is plural'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 找到：直接宾语位置的代词是复数
- en: We could use this information to find a corresponding noun for the pronoun in
    the previous sentence.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这些信息来找到前一句中的代词对应的名词。
- en: '***Choosing Which Patterns to Apply***'
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***选择应用哪些模式***'
- en: Once you define these patterns, you can choose which ones to apply for each
    situation. Notice that even if a sentence fails to fully satisfy the dep_pattern
    and pos_pattern functions, it might still match the pron_pattern function. For
    example, the sentence “I know it.” doesn’t match either the dep_pattern or pos_pattern
    functions, because it doesn’t have a modal auxiliary verb. But it satisfies pron_pattern
    because it contains a personal pronoun that is the direct object of the sentence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了这些模式，你可以根据每种情况选择应用哪些模式。注意，即使一个句子未能完全满足 dep_pattern 和 pos_pattern 函数，它仍然可能匹配
    pron_pattern 函数。例如，句子“我知道它。”不匹配 dep_pattern 或 pos_pattern 函数，因为它没有情态助动词。但它满足 pron_pattern，因为它包含一个作为句子直接宾语的人称代词。
- en: 'This loose coupling between the patterns lets you use them with other patterns
    or independently. For example, you might use dep_pattern, which checks a sentence
    against the “subject + auxiliary + verb + . . . + direct object . . .” pattern
    in conjunction with, say, a “noun + modal auxiliary verb + base form verb + .
    . . + noun . . .” pattern, if you wanted to be sure that the subject and the direct
    object in the sentence are nouns. These two patterns would match the following
    example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式之间的松散耦合使你可以将它们与其他模式一起使用，或者独立使用。例如，如果你想确保句子中的主语和直接宾语是名词，你可以将 dep_pattern（检查句子是否符合“主语
    + 助动词 + 动词 + . . . + 直接宾语 . . .”模式）与“名词 + 情态助动词 + 基本形式动词 + . . . + 名词 . . .”模式结合使用。这两个模式将匹配以下示例：
- en: Developers might follow this rule.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可能会遵循这个规则。
- en: As you might guess, the ability to combine patterns in different ways allows
    you to handle more scenarios with less code.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，能够以不同方式组合模式使你可以用更少的代码处理更多场景。
- en: '***Using Word Sequence Patterns in Chatbots to Generate Statements***'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***在聊天机器人中使用词序列模式生成陈述***'
- en: As stated earlier, the most challenging tasks in NLP are understanding and generating
    natural language text. A chatbot must understand a user’s input and then generate
    a proper response to it. Word sequence patterns based on linguistic features can
    help you implement these functions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，NLP 中最具挑战性的任务是理解和生成自然语言文本。聊天机器人必须理解用户的输入，并生成适当的回应。基于语言学特征的词序列模式可以帮助你实现这些功能。
- en: In [Chapter 4](../Text/ch04.xhtml#ch04), you learned how to turn a statement
    into a relevant question to continue a conversation with a user. Using word sequence
    patterns, you could generate other kinds of responses, too, such as relevant statements.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](../Text/ch04.xhtml#ch04)中，你学习了如何将陈述句转化为相关问题，从而继续与用户对话。通过使用词序模式，你还可以生成其他类型的响应，例如相关的陈述句。
- en: 'Suppose your chatbot has received the following user input:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的聊天机器人收到以下用户输入：
- en: The symbols are clearly distinguishable. I can recognize them promptly.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 符号显然是可以区分的。我也能迅速识别它们。
- en: 'The chatbot might react as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人可能会做出如下反应：
- en: I can recognize symbols promptly too.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我也能迅速识别符号。
- en: 'You can use the patterns implemented in the previous sections to accomplish
    this text generation task. The list of steps might look like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用前面部分实现的模式来完成这个文本生成任务。步骤列表可能如下所示：
- en: Check the conversational input against the dep_pattern and pos_pattern functions
    defined previously to find an utterance that follows the “subject + auxiliary
    + verb + . . . + direct object . . .” and “pronoun + modal auxiliary verb + base
    form verb + . . . + pronoun . . .” patterns, respectively.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查对话输入是否符合之前定义的`dep_pattern`和`pos_pattern`函数，分别找出符合“主语 + 助动词 + 动词 + ... + 直接宾语...”以及“代词
    + 情态动词 + 基本动词 + ... + 代词...”模式的表达。
- en: Check the utterance found in step 1 against the pron_pattern pattern to determine
    whether the direct object personal pronoun is plural or singular.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查步骤1中找到的表达式是否符合`pron_pattern`模式，以确定直接宾语代词是单数还是复数。
- en: Find the noun that gives its meaning to the pronoun by searching for a noun
    that has the same number as the personal pronoun.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过搜索与人称代词具有相同数量的名词，找出赋予代词意义的名词。
- en: Replace the pronoun that acts as the direct object in the sentence located in
    step 1 with the noun found in step 3.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用步骤3中找到的名词替换步骤1中作为直接宾语的代词。
- en: Append the word “too” to the end of the generated utterance.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成的句子末尾添加“too”一词。
- en: 'The following script implements these steps. It uses the dep_pattern, pos_pattern,
    and pron_pattern functions defined earlier in this chapter (their code is omitted
    to save space). It also introduces two new functions: find_noun and gen_utterance.
    For convenience, we’ll walk through the code in three steps: the initial operations
    and the find_noun function, which finds the noun that matches the personal pronoun;
    the gen_utterance function, which generates a relevant statement from that question;
    and finally, the code that tests an utterance. Here is the first part:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本实现了这些步骤。它使用了本章前面定义的`dep_pattern`、`pos_pattern`和`pron_pattern`函数（为节省空间，省略了它们的代码）。它还引入了两个新函数：`find_noun`和`gen_utterance`。为了方便，我们将分三个步骤讲解代码：初步操作和`find_noun`函数，用于找到与人称代词匹配的名词；`gen_utterance`函数，用于从问题生成相关的陈述；最后是测试表达的代码。以下是第一部分：
- en: import spacy
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 spacy
- en: nlp = spacy.load('en')
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Insert the dep_pattern, pos_pattern and pron_pattern functions from the'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#从...插入`dep_pattern`、`pos_pattern`和`pron_pattern`函数'
- en: previous listings here
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的前面列表
- en: '#...'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#...'
- en: '➊ def find_noun(➋sents, ➌num):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def find_noun(➋sents, ➌num):'
- en: 'if num == ''plural'':'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 num == ''复数'':'
- en: ➍ taglist = ['NNS','NNPS']
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ taglist = ['NNS','NNPS']
- en: 'if num == ''singular'':'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 num == ''单数'':'
- en: ➎ taglist = ['NN','NNP']
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ taglist = ['NN','NNP']
- en: '➏ for sent in reversed(sents):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '➏ for sent in reversed(sents):'
- en: '➐ for token in sent:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '➐ for token in sent:'
- en: '➑ if token.tag_ in taglist:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '➑ 如果 token.tag_ 在 taglist中:'
- en: return token.text
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 token.text
- en: return 'Noun not found'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 '未找到名词'
- en: After inserting the code of the dep_pattern, pos_pattern, and pron_pattern functions,
    we define the find_noun function, which takes two parameters ➊. The first one
    contains a list of the sentences from the beginning of the discourse up to the
    sentence that satisfies all the patterns here. In this example, this list will
    include all the sentences from the discourse, because only the last sentence satisfies
    all the patterns ➋. But the noun that gives its meaning to the pronoun can be
    found in one of the previous sentences.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在插入`dep_pattern`、`pos_pattern`和`pron_pattern`函数的代码之后，我们定义了`find_noun`函数，它需要两个参数➊。第一个参数包含从话语开头到满足所有模式的句子的句子列表。在这个例子中，这个列表将包含所有话语中的句子，因为只有最后一个句子满足所有模式➋。但是，赋予代词意义的名词可能出现在前面的某个句子中。
- en: The second parameter sent to find_noun is the number of the direct object pronoun
    in the sentence that satisfies all the patterns ➌. The pron_pattern function determines
    this. If the value of this argument is 'plural', we define a Python list containing
    fine-grained part-of-speech tags used in spaCy to mark plural nouns ➍. If it’s
    'singular', we create a tag list containing fine-grained part-of-speech tags used
    to mark singular nouns ➎.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 find_noun 的第二个参数是满足所有模式的直接宾语代词在句子中的位置 ➌。pron_pattern 函数决定了这一点。如果这个参数的值是“复数”，我们定义一个包含
    spaCy 用来标记复数名词的细粒度词性标签的 Python 列表 ➍。如果它是“单数”，我们创建一个包含用来标记单数名词的细粒度词性标签的标签列表 ➎。
- en: In a for loop, we iterate over the sentences in reverse order, starting from
    the sentence that is the closest to the sentence containing the pronoun to be
    replaced ➏. We start with the closest sentence, because the noun we’re searching
    for will most likely be there. Here, we use Python’s reversed function that returns
    a reverse iterator over the list. In the inner loop, we iterate over the tokens
    in each sentence ➐, looking for a token whose fine-grained part-of-speech tag
    is in the tag list defined earlier ➑.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 for 循环中，我们逆序遍历句子，从距离包含要替换的人称代词的句子最近的句子开始 ➏。我们从最接近的句子开始，因为我们要找的名词最有可能出现在那里。这里，我们使用
    Python 的 reversed 函数，它返回一个反向迭代器。在内层循环中，我们遍历每个句子中的标记 ➐，寻找一个其细粒度的词性标签在之前定义的标签列表中的标记
    ➑。
- en: 'Then we define the gen_utterance function, which generates our new statement:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了 gen_utterance 函数，它生成我们新的语句：
- en: 'def gen_utterance(doc, noun):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 gen_utterance 函数(doc, noun)：
- en: sent = ''
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: sent = ''
- en: '➊ for i,token in enumerate(doc):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 对于 i，token 在 enumerate(doc) 中：
- en: '➋ if token.dep_ == ''dobj'' and token.tag_ == ''PRP'':'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 token.dep_ == 'dobj' 且 token.tag_ == 'PRP'：
- en: ➌ sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)-2].text + 'too.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ sent = doc[:i].text + ' ' + 名词 + ' ' + doc[i+1:len(doc)-2].text + 'too.'
- en: ➍ return sent
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 返回 sent
- en: ➎ return 'Failed to generate an utterance'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ 返回 '生成语句失败'
- en: We use a for loop to iterate over the tokens in the sentence ➊, looking for
    a direct object that is a personal pronoun ➋. Once we’ve found one, we generate
    a new utterance. We change the original sentence by replacing the personal pronoun
    with the matching noun and appending “too” to the end of it ➌. The function then
    returns this newly generated utterance ➍. If we haven’t found a direct object
    in the form of a personal pronoun, the function returns an error message ➎.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个 for 循环遍历句子 ➊ 中的标记，寻找一个是人称代词的直接宾语 ➋。一旦找到，我们就生成一个新的语句。我们通过将人称代词替换为匹配的名词，并在其后附加“too”来修改原始句子
    ➌。然后函数返回这个新生成的语句 ➍。如果没有找到人称代词形式的直接宾语，函数会返回一个错误信息 ➎。
- en: 'Now that we have all the functions in place, we can test them on a sample utterance
    using the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的函数，可以用以下代码在示例语句上进行测试：
- en: ➊ doc = nlp(u'The symbols are clearly distinguishable. I can recognize them
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ doc = nlp(u'这些符号是明显可区分的。我能识别它们。')
- en: promptly.')
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: promptly.')
- en: ➋ sents = list(doc.sents)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ sents = list(doc.sents)
- en: response = ''
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: response = ''
- en: noun = ''
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 名词 = ''
- en: '➌ for i, sent in enumerate(sents):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 对于 i，sent 在 enumerate(sents) 中：
- en: 'if dep_pattern(sent) and pos_pattern(sent):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 dep_pattern(sent) 和 pos_pattern(sent)：
- en: ➍ noun = find_noun(sents[:i], pron_pattern(sent))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 名词 = find_noun(sents[:i], pron_pattern(sent))
- en: 'if noun != ''Noun not found'':'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果名词 != '未找到名词'：
- en: ➎ response = gen_utterance(sents[i],noun)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ response = gen_utterance(sents[i], noun)
- en: break
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: print(response)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: print(response)
- en: After applying the pipeline to the sample discourse ➊, we convert it to the
    list of sentences ➋. Then we iterate over this list ➌, searching for the sentence
    that matches the patterns defined in the dep_pattern and pos_pattern functions.
    Next, we determine the noun that gives the meaning to the pronoun in the sentence
    found in the previous step, using the find_noun function ➍. Finally, we call the
    get_utterance function to generate a response utterance ➎.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在将管道应用于示例语句 ➊ 后，我们将其转换为句子列表 ➋。然后我们遍历这个列表 ➌，寻找符合 dep_pattern 和 pos_pattern 函数定义的模式的句子。接下来，我们使用
    find_noun 函数确定在前一步找到的句子中赋予代词意义的名词 ➍。最后，我们调用 get_utterance 函数生成回应语句 ➎。
- en: 'The output of the preceding code should look like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出应如下所示：
- en: I can recognize symbols too.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我也能识别符号。
- en: '***Try This***'
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***尝试这个***'
- en: Notice that there’s still room for improvement in the preceding code, because
    the original statement included the article “the” in front of the noun “symbols.”
    A better output would include the same article in front of the noun. To generate
    a statement that makes the most sense in this context, expand on the script so
    that it inserts the article “the” in front of the noun, making it “I can recognize
    the symbols too.” For that, you’ll need to check whether the noun is preceded
    by an article, and then add that article.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面代码仍有改进的空间，因为原始句子在名词 "symbols" 前面包含了冠词 "the"。一个更好的输出应该在名词前面也加入这个冠词。为了生成一个在这个上下文中最有意义的句子，可以扩展脚本，使其在名词前加上冠词
    "the"，变成 "I can recognize the symbols too"。为此，你需要检查名词前是否有冠词，然后加上那个冠词。
- en: '**Extracting Keywords from Syntactic Dependency Trees**'
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**从句法依存树中提取关键词**'
- en: Finding a sequence of words in an utterance that satisfies a certain pattern
    allows you to construct a grammatically correct response—either a statement or
    a question, based on the submitted text. But these patterns aren’t always useful
    for extracting the meaning of texts.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在话语中找到符合某种模式的词序，可以帮助你构造一个语法正确的回应——无论是陈述句还是疑问句，都是基于提交的文本。但这些模式并不总是有助于提取文本的含义。
- en: 'For example, in the ticket-booking application in [Chapter 2](../Text/ch02.xhtml#ch02),
    a user might submit a sentence like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第2章](../Text/ch02.xhtml#ch02)的机票预定应用中，用户可能会提交如下句子：
- en: I need an air ticket to Berlin.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要一张去柏林的机票。
- en: You could easily find the user’s intended destination by searching for the pattern
    “to + GPE” where GPE is a named entity for countries, cities, and states. This
    pattern would match phrases like “to London,” “to California,” and so on.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过搜索模式 "to + GPE" 来轻松找到用户的预期目的地，其中 GPE 是指国家、城市和州的命名实体。这个模式可以匹配类似 "to London"（去伦敦）、"to
    California"（去加利福尼亚）等短语。
- en: 'But suppose the user submitted one of the following utterances instead:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设用户提交了以下其中一个话语：
- en: I am going to the conference in Berlin. I need an air ticket.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我将去柏林参加会议。我需要一张机票。
- en: I am going to the conference, which will be held in Berlin. I would like to
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我将去柏林参加会议。我想
- en: book an air ticket.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 预定一张机票。
- en: 'As you can see, the “to + GPE” pattern wouldn’t find the destination in either
    example. In both cases, “to” directly refers to “the conference,” not to Berlin.
    You’d need something like “to + . . . + GPE” instead. But how would you know what’s
    required—or what’s allowed—between “to” and “GPE”? For example, the following
    sentence contains the “to + . . . + GPE” pattern but has nothing to do with booking
    a ticket to Berlin:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，"to + GPE" 的模式在这两个例子中都无法找到目的地。在这两种情况下，"to" 直接指的是 "the conference"（会议），而不是柏林。你需要类似
    "to + . . . + GPE" 的模式。但你如何知道在 "to" 和 "GPE" 之间需要或允许什么呢？例如，以下句子包含了 "to + . . .
    + GPE" 模式，但与预定去柏林的机票无关：
- en: I want to book a ticket on a direct flight without landing in Berlin.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我想预定一张直飞不经柏林的机票。
- en: Often, you need to examine relations between the words in a sentence to obtain
    necessary pieces of information. This is where walking the dependency tree of
    the sentence could help a lot.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 经常需要检查句子中单词之间的关系，以获得必要的信息。这时候，遍历句子的依存树可能会非常有帮助。
- en: Walking a dependency tree means navigating through it in custom order—not necessarily
    from the first token to the last one. For example, you can stop iterating a dependency
    tree just after the required component is found. Remember that a sentence’s dependency
    tree shows the syntactic relationships between pairs of words. We often represent
    these as arrows connecting the head with the child of a relation. Every word in
    a sentence is involved in at least one of the relations. This guarantees that
    you’ll pass through each word in a sentence when walking through the entire dependency
    tree generated for that sentence if you start from ROOT.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历依存树意味着按自定义顺序遍历——不一定是从第一个标记到最后一个标记。例如，你可以在找到所需的组成部分后立即停止遍历依存树。记住，句子的依存树展示了单词对之间的句法关系。我们通常用箭头连接关系的主词和子词来表示这些关系。句子中的每个词都参与至少一个关系。这保证了你在遍历整个为该句子生成的依存树时，如果从
    ROOT 开始，就会经过句子中的每个单词。
- en: In this section, we’ll examine a sentence’s structure to figure out a user’s
    intended meaning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将分析句子的结构，以便了解用户的意图。
- en: '***Walking a Dependency Tree for Information Extraction***'
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***遍历依存树进行信息提取***'
- en: Let’s return to the ticket-booking application example. To find a user’s intended
    destination, you might need to iterate over the dependency tree of a sentence
    to determine whether “to” is semantically related to “Berlin.” This is easy to
    accomplish if you remember the head/child syntactic relations that compose a dependency
    tree, which was introduced in the “Head and Child” box on [page 25](../Text/ch02.xhtml#page_25).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到票务预订应用程序的例子。为了找出用户的预期目的地，你可能需要遍历句子的依赖树，以确定“to”是否与“Berlin”在语义上相关。如果你记住了构成依赖树的头/子关系（如在[第25页](../Text/ch02.xhtml#page_25)的“头与子”框中介绍的那样），这将变得很容易实现。
- en: '[Figure 6-3](../Text/ch06.xhtml#ch06fig03) shows the dependency tree for the
    sentence, “I am going to the conference in Berlin”:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-3](../Text/ch06.xhtml#ch06fig03)展示了句子“I am going to the conference in Berlin”的依赖树：'
- en: '![image](../Images/fig6-3.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig6-3.jpg)'
- en: '*Figure 6-3: A syntactic dependency tree of an utterance*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-3：话语的句法依赖树*'
- en: The verb “going” is the root of the sentence, meaning it’s not a child of any
    other word. Its child to the immediate right is “to.” If you walk through the
    dependency tree, moving to the child to the immediate right of each word, you’ll
    finally reach “Berlin.” This shows that there’s a semantic connection between
    “to” and “Berlin” in this sentence.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 动词“going”是句子的根词，意味着它不是任何其他词的子节点。它右侧的直接子词是“to”。如果你按照依赖树进行遍历，每次走到每个单词的直接右子节点，最终你会到达“Berlin”。这表明句子中“to”和“Berlin”之间存在语义连接。
- en: '***Iterating over the Heads of Tokens***'
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***遍历词的头词***'
- en: Now let’s figure out how to express the relation between “to” and “Berlin” in
    the sentence programmatically. One way is to walk the dependency tree from left
    to right, starting from “to,” choosing only the immediate right child of each
    word along the way. If you can go from “to” to “Berlin” this way, you can reasonably
    assume that there’s a semantic connection between the two words.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们弄清楚如何以编程方式表达句子中“to”和“Berlin”之间的关系。一种方法是从左到右遍历依赖树，从“to”开始，在过程中每次选择每个单词的直接右子节点。如果你可以通过这种方式从“to”走到“Berlin”，那么可以合理地假设这两个词之间存在语义连接。
- en: 'But this approach has a drawback. In some cases, a word might have more than
    one right child. For example, in the sentence “I am going to the conference on
    spaCy, which will be held in Berlin,” the word “conference” has two immediate
    right children: the words “on” and “held.” This forces you to check multiple branches,
    complicating the code.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法有一个缺点。在某些情况下，一个词可能有多个右子节点。例如，在句子“I am going to the conference on spaCy,
    which will be held in Berlin”中，词“conference”有两个直接的右子节点：“on”和“held”。这迫使你检查多个分支，增加了代码的复杂性。
- en: 'On the other hand, although a head can have multiple children, each word in
    a sentence has exactly one head. This means you can instead move from right to
    left, starting from “Berlin” and trying to reach “to.” The following script implements
    this process in the det_destination function:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，虽然一个头词可以有多个子节点，但句子中的每个词都有且只有一个头词。这意味着你可以从右到左移动，从“Berlin”开始，试图到达“to”。以下脚本在
    det_destination 函数中实现了这一过程：
- en: import spacy
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Here''s the function that figures out the destination'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '# 这是确定目的地的函数'
- en: '➊ def det_destination(doc):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def det_destination(doc):'
- en: 'for i, token in enumerate(doc):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i, token in enumerate(doc):'
- en: '➋ if token.ent_type != 0 and token.ent_type_ == ''GPE'':'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ if token.ent_type != 0 and token.ent_type_ == ''GPE'':'
- en: '➌ while True:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ while True:'
- en: ➍ token = token.head
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ token = token.head
- en: 'if token.text == ''to'':'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'if token.text == ''to'':'
- en: ➎ return doc[i].text
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ return doc[i].text
- en: '➏ if token.head == token:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '➏ if token.head == token:'
- en: return 'Failed to determine'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: return '无法确定'
- en: return 'Failed to determine'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: return '无法确定'
- en: '#Testing the det_destination function'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '# 测试 det_destination 函数'
- en: doc = nlp(u'I am going to the conference in Berlin.')
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'I am going to the conference in Berlin.')
- en: ➐ dest = det_destination(doc)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ dest = det_destination(doc)
- en: print('It seems the user wants a ticket to ' + dest)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: print('看起来用户想要订票去' + dest)
- en: In the det_destination function ➊, we iterate over the tokens in the submitted
    utterance, looking for a GPE entity ➋. If it’s found, we start a while loop ➌
    that iterates over the head of each token, starting from the token containing
    the GPE entity ➍. The loop stops when it reaches either the token containing “to”
    ➎ or the root of the sentence. We can check for the root by comparing a token
    to its head ➏, because the head of the root token always refers to itself. (Alternatively,
    we can check for the ROOT tag.)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在det_destination函数➊中，我们遍历提交的语句中的标记，寻找一个GPE实体➋。如果找到了，我们启动一个while循环➌，该循环遍历每个标记的头部，从包含GPE实体的标记开始
    ➍。当它到达包含“to”的标记➎或句子的根节点时，循环停止。我们可以通过将标记与其头部进行比较来检查根节点，因为根节点的头部总是指向它自身。（或者，我们可以检查ROOT标签。）
- en: To test this function, we apply the pipeline to the sample sentence and then
    invoke the det_destination function on it ➐.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个功能，我们将管道应用于示例句子，并在其上调用det_destination函数➐。
- en: 'The script should generate the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本应生成以下输出：
- en: It seems the user wants a ticket to Berlin
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来用户想要一张前往柏林的机票。
- en: 'If we change the sample sentence so it doesn’t contain “to” or a GPE named
    entity, we should get the following output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们修改示例句子，使其不包含“to”或GPE命名实体，我们应该得到以下输出：
- en: It seems the user wants a ticket to Failed to determine
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来用户想要一张前往“未能确定”的机票。
- en: We can improve the script so it uses another message for cases when it fails
    to determine the user’s destination.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以改进脚本，使其在未能确定用户的目的地时，使用另一条消息。
- en: '***Condensing a Text Using Dependency Trees***'
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用依赖树简化文本***'
- en: The syntactic dependency tree approach isn’t limited to chatbots, of course.
    You could use it, for example, in report-processing applications. Say, you need
    to develop an application that has to condense retail reports by extracting only
    the most important information from them.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 语法依赖树方法当然不仅仅局限于聊天机器人。例如，你可以在报告处理应用中使用它。假设，你需要开发一个应用程序，必须通过提取最重要的信息来精简零售报告。
- en: For example, you might want to select the sentences containing numbers, producing
    a concise summary of the data on sales volume, revenue, and costs. (You learned
    how to extract numbers in [Chapter 4](../Text/ch04.xhtml#ch04).) Then, to make
    your new report more concise, you might shorten the selected sentences.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能想要选择包含数字的句子，生成关于销售量、收入和成本的简明摘要。（你在[第4章](../Text/ch04.xhtml#ch04)学习了如何提取数字。）然后，为了使你的新报告更简洁，你可能会缩短选择的句子。
- en: 'As a quick example, consider the following sentence:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个快速示例，考虑以下句子：
- en: The product sales hit a new record in the first quarter, with 18.6 million units
    sold.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 该产品销量在第一季度创下新纪录，售出1860万件。
- en: 'After processing, it should look like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后，它应该是这样的：
- en: The product sales hit 18.6 million units sold.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 产品销量达到1860万件。
- en: 'To accomplish this, you can analyze the dependency trees of sentences by following
    these steps:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，你可以通过以下步骤分析句子的依赖树：
- en: Extract the entire phrase containing the number (it’s 18.6 in this example)
    by walking the heads of tokens, starting from the token containing the number
    and moving from left to right.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从包含数字的标记开始，按从左到右的顺序，遍历标记的头部，提取包含该数字的整个短语（在本例中是18.6）。
- en: Walk the dependency tree from the main word of the extracted phrase (the one
    whose head is out of the phrase) to the main verb of the sentence, iterating over
    the heads and picking them up to be used in a new sentence.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从提取的短语的主要词（其头部在短语之外的词）开始，遍历依赖树，直到到达句子的主要动词，通过遍历头部并将其收集到新的句子中。
- en: Pick up the main verb’s subject, along with its leftward children, which typically
    include a determiner and possibly some other modifiers.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拿到主要动词的主语，以及其左侧的子节点，通常包括限定词，可能还有其他修饰词。
- en: '[Figure 6-4](../Text/ch06.xhtml#ch06fig04) represents this process.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-4](../Text/ch06.xhtml#ch06fig04)表示此过程。'
- en: '![image](../Images/fig6-4.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig6-4.jpg)'
- en: '*Figure 6-4: An example of condensing a sentence to include only important
    elements*'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-4：一个仅包含重要元素的简化句子的示例*'
- en: 'Let’s start with the first step, which in this example should extract the phrase
    “18.6 million units sold.”. The following code snippet illustrates how to do this
    programmatically:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始，在这个例子中，应该提取出“1860万件售出”这个短语。以下代码片段演示了如何以编程方式做到这一点：
- en: doc = nlp(u"The product sales hit a new record in the first quarter, with 18.6
    million units sold.")
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u"该产品销量在第一季度创下新纪录，售出1860万件。")
- en: phrase = ''
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: phrase = ''
- en: 'for token in doc:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 doc 中的每一个 token：
- en: '➊ if token.pos_ == ''NUM'':'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 如果 token.pos_ == 'NUM'：
- en: 'while True:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 'while True:'
- en: phrase = phrase + ' ' + token.text
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: phrase = phrase + ' ' + token.text
- en: ➋ token = token.head
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ token = token.head
- en: '➌ if token not in list(token.head.lefts):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 如果 token 不在 token.head.lefts 列表中：
- en: phrase = phrase + ' ' + token.text
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: phrase = phrase + ' ' + token.text
- en: ➍ break
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ break
- en: ➎ break
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ break
- en: print(phrase.strip())
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: print(phrase.strip())
- en: We iterate over the sentence’s tokens, looking for one that represents a number
    ➊. If we find one, we start a while loop that iterates over right-hand heads ➋,
    starting from the number token and then appending the text of each head to the
    phrase variable to form a new phrase. To make sure the head of the next token
    is to the right of that token, we check whether the token is in the list of its
    head’s left children ➌. Once this condition returns false, we break from the while
    loop ➍ and then from the outer for loop ➎.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历句子的 tokens，寻找一个代表数字的 token ➊。若找到，我们开始一个 while 循环，遍历右侧的头部 ➋，从数字 token 开始，然后将每个头部的文本附加到
    phrase 变量，形成一个新的短语。为了确保下一个 token 的头部位于该 token 的右侧，我们检查 token 是否在其头部左侧子节点的列表中 ➌。一旦这个条件返回
    false，我们就退出 while 循环 ➍，然后退出外部的 for 循环 ➎。
- en: 'Next, we walk the heads of tokens, starting from the main word of the phrase
    containing the number (“sold” in this example) until we reach the main verb of
    the sentence (“hit” in this example), excluding the adposition (“with” in this
    example). We can implement this as shown in the following listing:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从包含数字的短语的主词开始，遍历 token 的头部，直到到达句子的主谓动词（在此例中为“hit”），排除介词（在此例中为“with”）。我们可以像下面这样实现：
- en: 'while True:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 'while True:'
- en: ➊ token = doc[token.i].head
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ token = doc[token.i].head
- en: 'if token.pos_ != ''ADP'':'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.pos_ != 'ADP'：
- en: ➋ phrase = token.text + phrase
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ phrase = token.text + phrase
- en: '➌ if token.dep_ == ''ROOT'':'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 如果 token.dep_ == 'ROOT'：
- en: ➍ break
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ break
- en: We walk the heads of tokens ➊ in a while loop, appending the text of each head
    to the phrase being formed ➋. After reaching the main verb (marked as ROOT) ➌,
    we break from the loop ➍.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 while 循环中遍历 token 的头部 ➊，将每个头部的文本附加到正在形成的短语中 ➋。到达主谓动词（标记为 ROOT）后 ➌，我们退出循环
    ➍。
- en: 'Finally, we pick up the subject of the sentence, along with its left children:
    “The” and “product.” In this example, the subject is “sales,” so we pick up the
    following noun chunk: “The product sales.” This can be done with the following
    code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提取句子的主语及其左侧子节点：“The”和“product”。在这个例子中，主语是“sales”，因此我们提取以下名词短语：“The product
    sales”。这可以通过以下代码实现：
- en: '➊ for tok in token.lefts:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 对于 token.lefts 中的每一个 tok：
- en: '➋ if tok.dep_ == ''nsubj'':'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 tok.dep_ == 'nsubj'：
- en: ➌ phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' '
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' '
- en: + phrase
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: + phrase
- en: break
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: ➍ print(phrase)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ print(phrase)
- en: We start by iterating over the main verb’s children ➊, searching for the subject
    ➋. Then we prepend the subject’s children and the subject of the phrase ➌. To
    see the resulting phrase, we print it ➍.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先遍历主谓动词的子节点 ➊，寻找主语 ➋。然后我们将主语的子节点和短语的主语加到前面 ➌。为了查看生成的短语，我们打印它 ➍。
- en: 'The output should look like this:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是这样的：
- en: The product sales hit 18.6 million units sold.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 产品销售达到了 1860 万件。
- en: The result is a condensed version of the original sentence.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个简化版的原句。
- en: '***Try This***'
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: 'Write a script that condenses financial reports by extracting only those sentences
    that contain phrases referring to an amount of money. Also, the script needs to
    condense the selected sentences so they include only the subject, the main verb,
    the phrase referring to an amount of money, and the tokens you can pick up when
    walking the heads starting from the main word of the money phrase up to the main
    verb of the sentence. For example, given the following sentence:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个脚本，通过提取包含金额信息的句子来简化财务报告。此外，脚本还需要将选中的句子简化，只保留主语、谓语动词、表示金额的短语以及从金额短语的主词开始，直到句子主谓动词的过程中可以获取的
    token。例如，给定以下句子：
- en: The company, whose profits reached a record high this year, largely attributed
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 公司今年的利润创下了新高，主要归因于
- en: to changes in management, earned a total revenue of $4.26 million.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 对于管理层的变动，收入总额为 426 万美元。
- en: 'Your script should return this sentence:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你的脚本应该返回这句话：
- en: The company earned revenue of $4.26 million.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 公司获得了 426 万美元的收入。
- en: In this example, “million” is the main word in the phrase “$4.26 million.” The
    head of “million” is “of,” which is a child of “revenue,” which, in turn, is a
    child of “earned,” the main verb of the sentence.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，“million”是短语“$4.26 million”中的主要单词。“million”的主词是“of”，它是“revenue”的子词，而“revenue”又是“earned”的子词，后者是句子的主要动词。
- en: '**Using Context to Improve the Ticket-Booking Chatbot**'
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**利用上下文提升票务预订聊天机器人的功能**'
- en: As you’ve no doubt realized by now, there’s no single solution for all intelligent
    text-processing tasks. For example, the ticket-booking script shown earlier in
    this chapter will only find a destination if the submitted sentence contains the
    word “to.”
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，所有智能文本处理任务都没有单一的解决方案。例如，本章前面展示的票务预订脚本只有在提交的句子中包含“to”一词时，才会找到目的地。
- en: 'One way to make these scripts more useful is to take context into account to
    determine an appropriate response. Let’s increase the functionality of the ticket-booking
    script so it can handle a wider set of user input, including utterances that don’t
    contain a “to + GPE” pair in any combination. For example, look at the following
    utterance:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 提高这些脚本有用性的一个方法是考虑上下文，从而确定合适的响应。让我们增加票务预订脚本的功能，使其能够处理更广泛的用户输入，包括那些不包含任何“to +
    GPE”组合的语句。例如，看看以下的语句：
- en: I am attending the conference in Berlin.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在柏林参加会议。
- en: 'Here, the user has expressed an intention to go to Berlin without “to.” Only
    the GPE entity “Berlin” is in the sentence. In such cases, it would be reasonable
    for a chatbot to ask a confirmatory question, such as the following:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，用户表达了要去柏林的意图，但没有使用“to”一词。句子中只有 GPE 实体“柏林”。在这种情况下，聊天机器人提出一个确认性问题是合理的，比如以下这个：
- en: You want a ticket to Berlin, right?
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 你是想要一张去柏林的票，对吧？
- en: 'The improved ticket-booking chatbot should produce different outputs based
    on three different situations:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 改进后的票务预订聊天机器人应该根据三种不同的情况输出不同的结果：
- en: The user expresses a clear intention to book a ticket to a certain destination.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户明确表示希望预定前往某个目的地的票。
- en: It’s not immediately clear whether the user wants a ticket to the destination
    mentioned.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前还不清楚用户是否想要预定提到的目的地的票。
- en: The user doesn’t mention any destination.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户没有提到任何目的地。
- en: Depending on which category the user input falls under, the chatbot generates
    an appropriate response. [Figure 6-5](../Text/ch06.xhtml#ch06fig05) illustrates
    how to represent this user input handling on a diagram.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户输入的类别，聊天机器人生成相应的响应。[图 6-5](../Text/ch06.xhtml#ch06fig05)展示了如何在图表中表示这种用户输入处理方式。
- en: '![image](../Images/fig6-5.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig6-5.jpg)'
- en: '*Figure 6-5: An example of user input handling in a ticket-booking application*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-5：票务预订应用中用户输入处理的示例*'
- en: The following script implements this design. For convenience, the code is divided
    into several parts.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本实现了这个设计。为了方便，代码被分为几个部分。
- en: The first snippet contains the guess_destination function, which searches a
    sentence for a GPE entity. Also, we’ll need to insert the dep_destination function
    defined and discussed in “[Iterating Over the Heads of Tokens](../Text/ch06.xhtml#lev82)”
    on [page 87](../Text/ch06.xhtml#page_87). Recall that this function searches a
    sentence for the “to + GPE” pattern. We’ll need the dep_destination and guess_destination
    functions to handle the first and second scenarios of user input, respectively.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个代码片段包含 guess_destination 函数，该函数在句子中搜索 GPE 实体。此外，我们还需要插入在“[Iterating Over
    the Heads of Tokens](../Text/ch06.xhtml#lev82)”一节中定义和讨论的 dep_destination 函数，见
    [第 87 页](../Text/ch06.xhtml#page_87)。回顾一下，这个函数用于在句子中搜索“to + GPE”模式。我们将需要 dep_destination
    和 guess_destination 函数来处理用户输入的第一和第二种场景。
- en: import spacy
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Insert the dep_destination function from a previous listing here'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '#在这里插入之前列表中的 dep_destination 函数'
- en: '#...'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '#...'
- en: 'def guess_destination(doc):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 'def guess_destination(doc):'
- en: 'for token in doc:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in doc:'
- en: '➊ if token.ent_type != 0 and token.ent_type_ == ''GPE'':'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ if token.ent_type != 0 and token.ent_type_ == ''GPE'':'
- en: ➋ return token.text
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ return token.text
- en: ➌ return 'Failed to determine'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ return '无法确定'
- en: The code in the guess_destination function iterates over the tokens in a sentence,
    looking for a GPE entity ➊. Once it finds one, the function returns it to the
    calling code ➋. If it fails to find one, the function returns 'Failed to determine'
    ➌, meaning the sentence doesn’t contain a GPE entity.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: guess_destination 函数中的代码遍历句子中的词元，查找 GPE 实体 ➊。找到后，函数将其返回给调用代码 ➋。如果没有找到，函数返回“无法确定”
    ➌，意味着句子中没有 GPE 实体。
- en: In the gen_function that follows, we generate a response based on what the functions
    defined in the preceding snippet return.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的 `gen_function` 中，我们基于前面代码段中定义的函数返回的内容来生成回应。
- en: 'def gen_response(doc):'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 'def gen_response(doc):'
- en: ➊ dest = det_destination(doc)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ dest = det_destination(doc)
- en: 'if dest != ''Failed to determine'':'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 'if dest != ''无法确定'':'
- en: ➋ return 'When do you need to be in ' + dest + '?'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ return '你需要什么时候到 ' + dest + '？'
- en: ➌ dest = guess_destination(doc)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ dest = guess_destination(doc)
- en: 'if dest != ''Failed to determine'':'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 'if dest != ''无法确定'':'
- en: ➍ return 'You want a ticket to ' + dest +', right?'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ return '你想要一张到 ' + dest + ' 的机票，对吧？'
- en: ➎ return 'Are you flying somewhere?'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ return '你要飞到某个地方吗？'
- en: The code in the gen_response function starts by invoking the det_destination
    function ➊, which determines whether an utterance contains a “to + GPE” pair.
    If one is found, we assume that the user wants a ticket to the destination and
    they need to clarify their departure time ➋.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`gen_response` 函数中的代码首先调用 `det_destination` 函数 ➊，该函数确定一个话语中是否包含“to + GPE”对。如果找到了，我们假设用户想要一张到目的地的机票，并且需要澄清他们的出发时间
    ➋。'
- en: If the det_destination function hasn’t found a “to + GPE” pair in the utterance,
    we invoke the guess_destination function ➌. This function tries to find a GPE
    entity. If it finds such an entity, it asks the user a confirmatory question about
    whether they want to fly to that destination ➍. Otherwise, if it finds no GPE
    entity in the utterance, the script asks the user whether they want to fly somewhere
    ➎.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `det_destination` 函数没有找到话语中的“to + GPE”对，我们会调用 `guess_destination` 函数 ➌。这个函数试图找到一个
    GPE 实体。如果找到该实体，它会询问用户是否想飞往该目的地 ➍。否则，如果话语中没有找到 GPE 实体，脚本会询问用户是否要飞往某个地方 ➎。
- en: 'To test the code, we apply the pipeline to a sentence and then send the doc
    to the gen_response function we used in the previous listing:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试代码，我们将管道应用于一个句子，然后将文档发送到我们在前一段代码中使用的 `gen_response` 函数：
- en: doc = nlp(u'I am going to the conference in Berlin.')
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'我将前往柏林参加会议。')
- en: print(gen_response(doc))
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: print(gen_response(doc))
- en: 'For the utterance submitted in this example, you should see the following output:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例中的话语，你应该看到以下输出：
- en: When do you need to be in Berlin?
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要什么时候到柏林？
- en: You can experiment with the sample utterance to see different output.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以试验这个示例话语，以查看不同的输出。
- en: '**Making a Smarter Chatbot by Finding Proper Modifiers**'
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**通过寻找合适的修饰词来制作更智能的聊天机器人**'
- en: 'One way to make your chatbot smarter is to use dependency trees to find modifiers
    for particular words. For example, you might teach your application to recognize
    the adjectives that are applicable to a given noun. Then you could tell the bot,
    “I’d like to read a book,” to which the smart bot could respond like this: “Would
    you like a fiction book?”'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让你的聊天机器人更智能的一种方法是使用依赖树来查找特定单词的修饰词。例如，你可以教你的应用程序识别适用于给定名词的形容词。然后你可以告诉机器人：“我想读一本书”，智能机器人可能会这样回答：“你想读一本小说吗？”
- en: 'A *modifier* is an optional element in a phrase or a clause used to change
    the meaning of another element. Removing a modifier doesn’t typically change the
    basic meaning of the sentence, but it does make it less specific. As a quick example,
    consider the following two sentences:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '*修饰词* 是短语或从句中的可选成分，用来改变其他成分的意思。去掉修饰词通常不会改变句子的基本意思，但会使句子变得不那么具体。举个例子，考虑以下两句话：'
- en: I want to read a book.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我想读一本书。
- en: I want to read a book on Python.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我想读一本关于 Python 的书。
- en: The first sentence doesn’t use modifiers. The second uses the modifier “on Python,”
    making your request more detailed.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条句子没有使用修饰词。第二条句子使用了修饰词“on Python”，使你的请求更为详细。
- en: If you want to be specific, you must use modifiers. For example, to generate
    a proper response to a user, you might need to learn which modifiers you can use
    in conjunction with a given noun or verb.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更具体，你必须使用修饰词。例如，为了生成一个恰当的回应，你可能需要学习哪些修饰词可以与某个名词或动词搭配使用。
- en: 'Consider the following phrase:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下短语：
- en: That exotic fruit from Africa.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 那个来自非洲的异国水果。
- en: In this noun phrase, “fruit” is the head, “that” and “exotic” are *premodifiers*—modifiers
    located in front of the word being modified—and “from Africa” is a *postmodifier*
    phrase—a modifier that follows the word it limits or qualifies. [Figure 6-6](../Text/ch06.xhtml#ch06fig06)
    shows the dependency tree for this phrase.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个名词短语中，“fruit”是中心词，“that”和“exotic”是*前修饰语*——位于被修饰词前的修饰词，而“from Africa”是*后修饰语*短语——修饰词跟随它所限定或修饰的词。[图6-6](../Text/ch06.xhtml#ch06fig06)展示了这个短语的依赖树。
- en: '![image](../Images/fig6-6.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig6-6.jpg)'
- en: '*Figure 6-6: An example of premodifiers and postmodifiers*'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-6：前置修饰语和后置修饰语的示例*'
- en: Suppose you want to determine possible adjectival modifiers for the word “fruit.”
    (Adjectival modifiers are always premodifiers.) Also, you want to look at what
    GPE entities you can find in the postmodifiers of this same word. This information
    could later help you generate an utterance during a conversation on fruits.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想确定单词“fruit”的可能形容词修饰语。（形容词修饰语始终是前修饰语。）同时，你还想查看在该词的后置修饰语中可以找到哪些GPE实体。这些信息可能在与水果的对话中帮助你生成相关的句子。
- en: 'The following script implements this design:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本实现了这个设计：
- en: import spacy
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 导入spacy
- en: nlp = spacy.load('en')
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: ➊ doc = nlp(u"Kiwano has jelly-like flesh with a refreshingly fruity taste.
    This
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ doc = nlp(u"Kiwanos的果肉像果冻一样，带有令人耳目一新的果味。这个
- en: is a nice exotic fruit from Africa. It is definitely worth trying.")
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 是一种美味又异国情调的水果，来自非洲，绝对值得一试。")
- en: ➋ fruit_adjectives = []
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ fruit_adjectives = []
- en: ➌ fruit_origins = []
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ fruit_origins = []
- en: 'for token in doc:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 对于doc中的每个token：
- en: '➍ if token.text == ''fruit'':'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ 如果token.text == ''fruit'':'
- en: ➎ fruit_adjectives = fruit_adjectives + [modifier.text for modifier in
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ fruit_adjectives = fruit_adjectives + [modifier.text for modifier in
- en: token.lefts if modifier.pos_ == 'ADJ']
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: token.lefts 如果modifier.pos_ == 'ADJ']
- en: ➏ fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier
- en: in token.rights if modifier.text == 'from' and doc[modifier.i + 1].ent_
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在token.rights中，如果修饰词的文本是'from'且doc[modifier.i + 1]是实体类型
- en: type != 0]
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: type != 0]
- en: print('The list of adjectival modifiers for word fruit:', fruit_adjectives)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: print('水果的形容词修饰词列表：', fruit_adjectives)
- en: print('The list of GPE names applicable to word fruit as postmodifiers:',
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: print('适用于水果的GPE名称作为后置修饰词：',
- en: fruit_origins)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: fruit_origins)
- en: 'We start by applying the pipeline to a short text that contains the word “fruit”
    with both premodifiers and postmodifiers ➊. We define two empty lists: fruit_adjectives
    ➋ and fruit_origins ➌. The first one will hold any adjectival modifiers found
    for the word “fruit.” The second list will hold any GPE entities found among the
    postmodifiers of “fruit.”'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对包含“fruit”一词并同时具有前置和后置修饰语的短文本应用管道 ➊。我们定义了两个空列表：fruit_adjectives ➋ 和 fruit_origins
    ➌。第一个列表将保存所有为“fruit”找到的形容词修饰语。第二个列表将保存所有在“fruit”后置修饰语中找到的GPE实体。
- en: Next, in a loop iterating over the tokens of the entire text, we look for the
    word “fruit” ➍. Once this word is found, we first determine its adjectival premodifiers
    by picking up its syntactic children to the left and choosing only adjectives
    (determiners and compounds can also be premodifiers). We append the adjectival
    modifiers to the fruit_adjectives list ➎.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在一个遍历整个文本的循环中，我们寻找单词“fruit” ➍。一旦找到这个词，我们首先通过选择其左侧的句法子节点并仅选择形容词（限定词和复合词也可以是前置修饰语）来确定它的形容词前修饰词。我们将形容词修饰词添加到fruit_adjectives列表中
    ➎。
- en: Then we search for postmodifiers by checking the right-hand syntactic children
    of the word “fruit.” In particular, we look for named entities, and then append
    them to the fruit_origins list ➏.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过检查单词“fruit”的右侧句法子节点来搜索后置修饰语。特别地，我们查找命名实体，并将其添加到fruit_origins列表中 ➏。
- en: 'The script outputs the following two lists:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本输出以下两个列表：
- en: 'The list of adjectival modifiers for word fruit: [''nice'', ''exotic'']'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 水果的形容词修饰词列表：['nice', 'exotic']
- en: 'The list of GPE names applicable to word fruit as postmodifiers: [''Africa'']'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于“fruit”的GPE名称作为后置修饰词：['Africa']
- en: Now your bot “knows” that a fruit can be nice, exotic (or both nice and exotic),
    and might come from Africa.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的机器人“知道”水果可以是美味的、异国情调的（或者同时既美味又异国情调），并且可能来自非洲。
- en: '**Summary**'
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: When you need to process an utterance, or even just a phrase, it’s often important
    to look at its structure to determine which general patterns it matches. Using
    spaCy’s linguistic features, you can detect these patterns, allowing your script
    to understand the user’s intention and respond properly.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要处理一句话，甚至只是一个短语时，通常需要查看它的结构，以确定它匹配哪些一般模式。使用spaCy的语言学特性，你可以检测这些模式，使你的脚本能够理解用户的意图并做出适当回应。
- en: Using patterns based on linguistic features works well when you need to recognize
    the general structure of a sentence, which involves the subject, modal auxiliary
    verb, main verb, and direct object. But a real-world application needs to recognize
    more complicated sentence structures and be prepared for a wider set of user input.
    This is where the syntactic dependency tree of a sentence becomes very useful.
    You can walk the dependency tree of a sentence in different ways, extracting necessary
    pieces of information from it. For example, you can use dependency trees to find
    modifiers for particular words, and then use this information later to generate
    intelligent text.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语言特征的模式在你需要识别句子的基本结构时非常有效，这些结构包括主语、情态动词、主要动词和直接宾语。但是，实际应用需要识别更复杂的句子结构，并且要准备处理更广泛的用户输入。这时，句子的句法依赖树就显得非常有用。你可以以不同的方式遍历句子的依赖树，从中提取必要的信息。例如，你可以使用依赖树找到特定单词的修饰语，然后将这些信息在后续生成智能文本时加以利用。
