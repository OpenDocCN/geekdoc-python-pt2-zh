- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: FINDING PATTERNS AND WALKING DEPENDENCY TREES**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找模式和遍历依赖树**
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: If you want your application to categorize a text, extract specific phrases
    from it, or determine how semantically similar it is to another text, it must
    be able to “understand” an utterance submitted by a user and generate a meaningful
    response to it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望应用程序能够对文本进行分类，提取其中的特定短语，或者确定它与另一篇文本的语义相似度，它必须能够“理解”用户提交的句子并生成有意义的响应。
- en: 'You’ve already learned some techniques for performing these tasks. This chapter
    discusses two more approaches: using word sequence patterns to classify and generate
    text, and walking the syntactic dependency tree of an utterance to extract necessary
    pieces of information from it. I’ll introduce you to spaCy’s Matcher tool to find
    patterns. I’ll also discuss when you might still need to rely on context to determine
    the proper processing approach.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了一些执行这些任务的技巧。本章讨论另外两种方法：使用词序列模式进行文本分类和生成，以及遍历句子的句法依赖树来提取所需的信息。我将向你介绍spaCy的Matcher工具来寻找模式。我还会讨论在何时你仍然需要依赖上下文来决定正确的处理方法。
- en: '**Word Sequence Patterns**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**词序列模式**'
- en: 'A *word sequence pattern* consists of features of words that impose a certain
    requirement on each word in the sequence. For example, the phrase “I can” will
    match the following word sequence pattern: “pronoun + modal auxiliary verb.” By
    searching for word sequence patterns, you can recognize word sequences with similar
    linguistic features, making it possible to categorize input and handle it properly.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*词序列模式*由词语的特征组成，这些特征对序列中每个词语提出了特定要求。例如，短语“I can”将匹配以下词序列模式：“代词 + 情态动词”。通过寻找词序列模式，你可以识别出具有相似语言特征的词序列，从而能够对输入进行分类并妥善处理。
- en: For example, when you receive a question that begins with a word sequence that
    uses the pattern “modal auxiliary verb + proper noun,” such as “Can George,” you
    know that this question is about the ability, possibility, permission, or obligation
    of someone or something that the proper noun refers to.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你收到一个以“情态动词 + 专有名词”这一模式开头的问题时，例如“Can George”，你就知道这个问题是在询问某个或某物的能力、可能性、许可或义务，这个专有名词所指代的对象。
- en: In the following sections, you’ll learn to classify sentences by identifying
    common patterns of linguistic features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将通过识别语言特征的常见模式来学习如何分类句子。
- en: '***Finding Patterns Based on Linguistic Features***'
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基于语言特征的模式识别***'
- en: We need to find patterns in texts because, in most cases, we won’t be able to
    find even two identical sentences within a text. Typically, a text is composed
    of different sentences, each of which contains different words. It would be impractical
    to write the code to process each sentence in a text.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在文本中寻找模式，因为在大多数情况下，我们无法在文本中找到两个完全相同的句子。通常，一篇文本由不同的句子组成，每个句子包含不同的单词。为每个句子编写处理代码将是非常不现实的。
- en: 'Fortunately, some sentences that look completely different might follow the
    same word sequence patterns. For example, consider the following two sentences:
    “We can overtake them.” and “You must specify it.”. These sentences have no words
    in common. But if you look at the syntactic dependency labels assigned to the
    words in the sentences, a pattern emerges, as shown in the following script:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一些看似完全不同的句子可能遵循相同的词序列模式。例如，考虑以下两个句子：“我们可以超过他们。”和“你必须指定它。”这两个句子没有共同的词语。但如果你看一下句子中词语所分配的句法依赖标签，就会发现一个模式，如以下脚本所示：
- en: import spacy
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc1 = nlp(u'We can overtake them.')
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: doc1 = nlp(u'我们可以超过他们。')
- en: doc2 = nlp(u'You must specify it.')
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: doc2 = nlp(u'你必须指定它。')
- en: '➊ for i in range(len(doc1)-1):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ for i in range(len(doc1)-1):'
- en: '➋ if doc1[i].dep_ == doc2[i].dep_:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 doc1[i].dep_ == doc2[i].dep_：
- en: ➌ print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))
- en: Because both sentences have the same number of words, we can iterate over the
    words in both sentences within a single loop ➊. If the dependency label is the
    same for the words that have the same index in both sentences ➋, we print these
    words along with the label assigned to them, as well as a description for each
    label ➌.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因为两个句子的词数相同，我们可以在一个循环中遍历两个句子中的单词 ➊。如果在两个句子中，具有相同索引的单词的依赖标签相同 ➋，我们就会打印这些单词及其对应的标签，以及每个标签的描述
    ➌。
- en: 'The output should look as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: We       You     nsubj  nominal subject
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们      你      nsubj  主语
- en: can      must    aux    auxiliary
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: can      must    aux    助动词
- en: overtake specify ROOT   None
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: overtake specify ROOT   无
- en: them     it      dobj   direct object
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: them     it      dobj   直接宾语
- en: 'As you can see, the list of dependency labels is identical for both sentences.
    This means that these sentences follow the same word sequence pattern based on
    the following syntactic dependency labels: “subject + auxiliary + verb + direct
    object.”'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，依赖标签的列表对于这两个句子是相同的。这意味着这两个句子遵循相同的词序模式，基于以下句法依赖标签：“主语 + 助动词 + 动词 + 直接宾语”。
- en: 'Also notice that the list of part-of-speech tags (coarse-grained and fined-grained)
    is also identical for these sample sentences. If we replace all references to
    .dep_ with .pos_ in the previous script, we’ll get the following results:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，词性标签（粗粒度和细粒度）的列表对于这些示例句子也是相同的。如果我们将前面脚本中所有对 .dep_ 的引用替换为 .pos_，我们将得到以下结果：
- en: We       You     PRON  pronoun
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们      你      PRON  代词
- en: can      must    VERB  verb
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: can      must    VERB  动词
- en: overtake specify VERB  verb
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: overtake specify VERB  动词
- en: them     it      PRON  pronoun
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: them     it      PRON  代词
- en: The sample sentences match not only the syntactic dependency label pattern,
    but also the pattern of part-of-speech tags.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例句子不仅匹配了句法依赖标签模式，还匹配了词性标签的模式。
- en: '***Try This***'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***尝试这个***'
- en: In the previous example, we created two Doc objects—one for each sample sentence.
    But in practice, a text usually consists of numerous sentences, which makes a
    Doc-per-sentence approach impractical. Rewrite the script so it creates a single
    Doc object. Then use the doc.sents property introduced in [Chapter 2](../Text/ch02.xhtml#ch02)
    to operate on each sentence.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们创建了两个 Doc 对象——每个示例句子一个。但实际上，一篇文本通常包含多个句子，这使得按句子创建 Doc 对象的方法不太实用。请重写脚本，使其创建一个单一的
    Doc 对象。然后使用在[第 2 章](../Text/ch02.xhtml#ch02)中介绍的 doc.sents 属性来处理每个句子。
- en: 'But note that doc.sents is a generator object, which means it’s not subscriptable—you
    can’t refer to its items by index. To solve this issue, convert the doc.sents
    to a list, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，doc.sents 是一个生成器对象，这意味着它是不可下标访问的——你不能通过索引来引用它的项。为了解决这个问题，可以将 doc.sents
    转换为列表，如下所示：
- en: sents = list(doc.sents)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: sents = list(doc.sents)
- en: And, of course, you can iterate over a doc.sents in a for loop to obtain the
    sents in order, as they’re requested by the loop.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你还可以在 for 循环中迭代 doc.sents，以便按顺序获取句子，正如它们在循环中被请求的那样。
- en: '***Checking an Utterance for a Pattern***'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***检查言论是否符合模式***'
- en: In the preceding example, we compared two sample sentences to find a pattern
    based on their shared linguistic features. But in practice, we’ll rarely want
    to compare sentences to one another to determine whether they share a common pattern.
    Instead, it’ll be more useful to check a submitted sentence against the pattern
    we’re already interested in.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们比较了两个示例句子，以便找到基于它们共享的语言特征的模式。但实际上，我们很少需要将句子相互比较以确定它们是否共享一个共同的模式。相反，将已提交的句子与我们已经感兴趣的模式进行对比会更有用。
- en: 'For example, let’s say we were trying to find utterances in user input that
    express one of the following: ability, possibility, permission, or obligation
    (as opposed to utterances that describe real actions that have occurred, are occurring,
    or occur regularly). For instance, we want to find “I can do it.” but not “I’ve
    done it.”'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们试图在用户输入中找到表达以下之一的言论：能力、可能性、许可或义务（与描述已发生、正在发生或定期发生的实际行为的言论相对）。例如，我们想找到“I
    can do it.”，而不是“I’ve done it.”。
- en: 'To distinguish between utterances, we might check whether an utterance satisfies
    the following pattern: “subject + auxiliary + verb + . . . + direct object . .
    .”. The ellipses indicate that the direct object isn’t necessarily located immediately
    behind the verb, making this pattern a little different from the one in the preceding
    example.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分不同的言论，我们可以检查某个言论是否满足以下模式：“主语 + 助动词 + 动词 + … + 直接宾语 …”。省略号表示直接宾语不一定紧跟在动词后面，这使得这个模式与前面例子中的模式稍有不同。
- en: 'The following sentence satisfies the pattern: “I might send them a card as
    a reminder.”. In this sentence, the noun “card” is a direct object, and the pronoun
    “them” is an indirect object that separates it from the verb “send.” The pattern
    doesn’t specify a position for the direct object in the sentence; it simply requires
    its presence.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子满足这个模式：“I might send them a card as a reminder.”。在这个句子中，名词“card”是直接宾语，而代词“them”是间接宾语，将其与动词“send”分开。这个模式没有指定直接宾语在句子中的位置；它只是要求有直接宾语的存在。
- en: '[Figure 6-1](../Text/ch06.xhtml#ch06fig01) shows a graphical depiction of this
    design:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig6-1.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-1: Checking submitted utterances against a word sequence pattern
    based on linguistic features*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following script, we define a function that implements this pattern,
    and then test it on a sample sentence:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '➊ def dep_pattern(doc):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '➋ for i in range(len(doc)-1):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: ➌ if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'doc[i+2].dep_ == ''ROOT'':'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '➍ for tok in doc[i+2].children:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'if tok.dep_ == ''dobj'':'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: ➎ return True
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: ➏ return False
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: ➐ doc = nlp(u'We can overtake them.')
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'if ➑dep_pattern(doc):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: print('Found')
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: print('Not found')
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: In this script, we define the dep_pattern function that takes a Doc object as
    parameter ➊. In the function, we iterate over the Doc object’s tokens ➋, searching
    for a “subject + auxiliary + verb” pattern ➌. If we find this pattern, we check
    whether the verb has a direct object among its syntactic children ➍. Finally,
    if we find a direct object, the function returns True ➎. Otherwise, it returns
    False ➏.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: In the main code, we apply the text-processing pipeline to the sample sentence
    ➐ and send the Doc object to the dep_pattern function ➑, outputting Found if the
    sample satisfies the pattern implemented in the function or Not found otherwise.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the sample used in this example satisfies the pattern, the script should
    produce the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Found
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see some examples of using the dep_pattern function in some of the following
    sections.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '***Using spaCy’s Matcher to Find Word Sequence Patterns***'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous section, you learned how to find a word sequence pattern in
    a doc by iterating over its tokens and checking their linguistic features. In
    fact, spaCy has a predefined feature for this task called *Matcher*, a tool that
    is specially designed to find sequences of tokens based on pattern rules. For
    example, an implementation of the “subject + auxiliary + verb” pattern with Matcher
    might look like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: from spacy.matcher import Matcher
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load("en")
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: ➊ matcher = Matcher(nlp.vocab)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: ➋ pattern = [{"DEP": "nsubj"}, {"DEP": "aux"}, {"DEP": "ROOT"}]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: ➌ matcher.add("NsubjAuxRoot", None, pattern)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u"We can overtake them.")
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: ➍ matches = matcher(doc)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '➎ for match_id, start, end in matches:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: span = doc[start:end]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: ➏ print("Span: ", span.text)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: print("The positions in the doc are: ", start, "-", end)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: We create a Matcher instance, passing in the vocabulary object shared with the
    documents the Matcher will work on ➊. Then we define a pattern, specifying the
    dependency labels that a word sequence should match ➋. We add the newly created
    pattern to the Matcher ➌.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can apply the Matcher to a sample text and obtain the matching tokens
    in a list ➍. Then we iterate over this list ➎, printing out the start and end
    positions of the pattern tokens in the text ➏.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The script should produce the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Span: We can overtake'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The positions in the doc are: 0 - 3'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Matcher allows you to find a pattern in a text without iterating explicitly
    over the text’s tokens, thus hiding implementation details from you. As a result,
    you can obtain the start and end positions of the words composing a sequence that
    satisfies the specified pattern. This approach can be very useful when you’re
    interested in a sequence of words that immediately follow one another.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: But often you need a pattern that includes words scattered over the sentence.
    For example, you might need to implement such patterns as the “subject + auxiliary
    + verb + . . . + direct object . . .” pattern we used in “[Checking an Utterance
    for a Pattern](../Text/ch06.xhtml#lev73)” on [page 77](../Text/ch06.xhtml#page_77).
    The problem is that you don’t know in advance how many words can occur between
    the “subject + auxiliary + verb” sequence and the direct object. Matcher doesn’t
    allow you to define such patterns. For this reason, I’ll define patterns manually
    for the remainder of this chapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '***Applying Several Patterns***'
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can apply several matching patterns to an utterance to make sure it satisfies
    all your conditions. For example, you might check an utterance against two patterns:
    one that implements a dependency label sequence (as discussed in “[Checking an
    Utterance for a Pattern](../Text/ch06.xhtml#lev73)” on [page 77](../Text/ch06.xhtml#page_77))
    and one that checks against a sequence of part-of-speech tags. This might be helpful,
    if, say, you want to make sure that the direct object in an utterance is a personal
    pronoun. If so, you can start the procedure of determining the noun that gives
    its meaning to the pronoun and is mentioned elsewhere in the discourse.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Diagrammatically, this design might look like [Figure 6-2](../Text/ch06.xhtml#ch06fig02).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig6-2.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-2: Applying several matching patterns to user input*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to using the dependency label sequence defined in “[Checking an
    Utterance for a Pattern](../Text/ch06.xhtml#lev73)”, you can define a new function
    by implementing a pattern based on part-of-speech tags. The part-of-speech tag
    pattern might search the sentence to make sure that the subject and the direct
    object are personal pronouns. This new function might implement the following
    pattern: “personal pronoun + modal auxiliary verb + base form verb + . . . + personal
    pronoun . . .”.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '#Insert the dep_pattern function from a previous listing here'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '#...'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '➊ def pos_pattern(doc):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '➋ for token in doc:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'if token.dep_ == ''nsubj'' and token.tag_ != ''PRP'':'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: return False
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'if token.dep_ == ''aux'' and token.tag_ != ''MD'':'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: return False
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'if token.dep_ == ''ROOT'' and token.tag_ != ''VB'':'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: return False
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'if token.dep_ == ''dobj'' and token.tag_ != ''PRP'':'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: return False
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: ➌ return True
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '#Testing code'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u'We can overtake them.')
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '➍ if dep_pattern(doc) and pos_pattern(doc):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: print('Found')
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: print('Not found')
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: We start by adding the code for the dep_pattern function defined in a previous
    script. To create the second pattern, we define the pos_pattern function ➊, which
    contains a for loop with a series of if statements in it ➋. Each if statement
    checks whether a certain part of a sentence matches a certain part-of-speech tag.
    When the function detects a mismatch, it returns False. Otherwise, after all the
    checks have occurred and no mismatch has been detected, the function returns True
    ➌.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先添加在前一个脚本中定义的dep_pattern函数的代码。为了创建第二个模式，我们定义了pos_pattern函数 ➊，其中包含一个带有一系列if语句的for循环
    ➋。每个if语句检查句子中的某个部分是否与特定的词性标签匹配。当函数检测到不匹配时，它返回False。否则，在所有检查完成且没有检测到不匹配时，函数返回True
    ➌。
- en: 'To test the patterns, we apply the pipeline to a sentence, and then check whether
    the sentence matches both patterns ➍. Because the sample used in this example
    matches both patterns, we should see the following output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这些模式，我们将管道应用于一个句子，然后检查该句子是否同时匹配两个模式 ➍。由于本示例中使用的样本同时匹配两个模式，我们应该看到以下输出：
- en: Found
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 已找到
- en: 'But if we replace the sample sentence with this one: “I might send them a card
    as a reminder.”, we should see this output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们将示例句子替换为：“I might send them a card as a reminder.”，我们应该看到以下输出：
- en: Not found
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 未找到
- en: The reason is that the sentence doesn’t match the part-of-speech tag pattern,
    because the direct object “card” isn’t a personal pronoun, even though the sentence
    fully satisfies the conditions of the first pattern.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于，句子没有匹配词性标签模式，因为直接宾语“card”不是一个人称代词，即使该句子完全满足第一个模式的条件。
- en: '***Creating Patterns Based on Customized Features***'
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基于自定义特征创建模式***'
- en: When creating a word sequence pattern, you might need to enhance the functionality
    of the linguistic features spaCy provides by customizing them for your needs.
    For example, you might want the preceding script to recognize another pattern
    that distinguishes pronouns according to number (whether they’re singular or plural).
    Once again, this could be useful when you need to find the noun in a previous
    utterance to which the pronoun refers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建词序列模式时，你可能需要通过定制语言特征来增强spaCy提供的功能，以满足你的需求。例如，你可能希望上面的脚本识别另一个模式，根据代词的数目（单数或复数）来区分代词。这在你需要找出代词所指代的前述名词时非常有用。
- en: 'Although spaCy separates nouns by number, it doesn’t do this for pronouns.
    But the ability to recognize whether a pronoun is plural or singular can be very
    useful in the task of meaning recognition or information extraction. For example,
    consider the following discourse:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然spaCy通过数目区分名词，但对于代词并不如此。但识别代词是单数还是复数的能力在语义识别或信息提取任务中非常有用。例如，考虑以下话语：
- en: The trucks are traveling slowly. We can overtake them.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 卡车正在缓慢行驶。我们可以超越它们。
- en: If we can establish that the direct object “them” in the second sentence is
    a plural pronoun, we’ll have reason to believe that it refers to the plural noun
    “trucks” in the first sentence. We often use this technique to recognize a pronoun’s
    meaning based on the context.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够确认第二个句子中的直接宾语“them”是复数代词，那么我们有理由相信它指代第一个句子中的复数名词“trucks”。我们经常使用这个技巧根据上下文来识别代词的意义。
- en: The following script defines a pron_pattern function, which finds any direct
    object in the submitted sentence, determines whether that direct object is a personal
    pronoun, and then determines whether the pronoun is singular or plural. The script
    then applies the function to a sample sentence after testing for the two patterns
    defined in “[Checking an Utterance for a Pattern](../Text/ch06.xhtml#lev73)” on
    [page 77](../Text/ch06.xhtml#page_77) and “[Applying Several Patterns](../Text/ch06.xhtml#lev75)”
    on [page 80](../Text/ch06.xhtml#page_80).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本定义了一个pron_pattern函数，该函数查找提交句子中的任何直接宾语，确定该直接宾语是否为人称代词，并进一步确定该代词是单数还是复数。然后，脚本将函数应用于一个样本句子，并测试在[第77页](../Text/ch06.xhtml#page_77)中的“[检查话语的模式](../Text/ch06.xhtml#lev73)”和[第80页](../Text/ch06.xhtml#page_80)中的“[应用多个模式](../Text/ch06.xhtml#lev75)”中定义的两个模式。
- en: import spacy
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Insert the dep_pattern and pos_pattern functions from the previous'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#插入之前的dep_pattern和pos_pattern函数'
- en: listings here
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出
- en: '#...'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#...'
- en: '➊ def pron_pattern(doc):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def pron_pattern(doc):'
- en: ➋ plural = ['we','us','they','them']
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ plural = ['we','us','they','them']
- en: 'for token in doc:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in doc:'
- en: '➌ if token.dep_ == ''dobj'' and token.tag_ == ''PRP'':'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ if token.dep_ == ''dobj'' and token.tag_ == ''PRP'':'
- en: '➍ if token.text in plural:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ if token.text in plural:'
- en: ➎ return 'plural'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ return 'plural'
- en: 'else:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: ➏ return 'singular'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ 返回 'singular'
- en: ➐ return 'not found'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ 返回 '未找到'
- en: doc = nlp(u'We can overtake them.')
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'我们可以超过他们。')
- en: 'if dep_pattern(doc) and pos_pattern(doc):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 dep_pattern(doc) 和 pos_pattern(doc):'
- en: print('Found:', 'the pronoun in position of direct object is',
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: print('找到：', '直接宾语位置的代词是',
- en: pron_pattern(doc))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: pron_pattern(doc))
- en: 'else:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: print('Not found')
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: print('未找到')
- en: We start by adding the dep_pattern and pos_pattern functions defined in “[Checking
    an Utterance for a Pattern](../Text/ch06.xhtml#lev73)” and “[Applying Several
    Patterns](../Text/ch06.xhtml#lev75)” to the script. In the pron_pattern function
    ➊, we define a Python list that includes all the possible plural personal pronouns
    ➋. Next, we define a loop that iterates over the tokens in the submitted sentence,
    looking for a direct object that is a personal pronoun ➌. If we find such a token,
    we check whether it’s in the list of plural personal pronouns ➍. If so, the function
    returns plural ➎. Otherwise, it returns singular ➏. If the function either failed
    to detect a direct object or found one that isn’t a personal pronoun, it returns
    Not found ➐.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过在脚本中添加在“[检查句子是否符合模式](../Text/ch06.xhtml#lev73)”和“[应用多个模式](../Text/ch06.xhtml#lev75)”中定义的
    dep_pattern 和 pos_pattern 函数开始。在 pron_pattern 函数 ➊ 中，我们定义了一个包含所有可能的复数人称代词的 Python
    列表 ➋。接下来，我们定义一个循环，遍历提交句子中的词项，寻找作为直接宾语的人称代词 ➌。如果我们找到了这样的词项，我们就检查它是否在复数人称代词的列表中
    ➍。如果是，那么函数返回复数 ➎。否则，它返回单数 ➏。如果函数未能检测到直接宾语，或者找到了一个不是人称代词的直接宾语，它将返回未找到 ➐。
- en: 'For the sentence “We can overtake them.”, we should get the following output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子“我们可以超过他们。”，我们应该得到以下输出：
- en: 'Found: the pronoun in position of direct object is plural'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 找到：直接宾语位置的代词是复数
- en: We could use this information to find a corresponding noun for the pronoun in
    the previous sentence.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这些信息来找到前一句中的代词对应的名词。
- en: '***Choosing Which Patterns to Apply***'
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***选择应用哪些模式***'
- en: Once you define these patterns, you can choose which ones to apply for each
    situation. Notice that even if a sentence fails to fully satisfy the dep_pattern
    and pos_pattern functions, it might still match the pron_pattern function. For
    example, the sentence “I know it.” doesn’t match either the dep_pattern or pos_pattern
    functions, because it doesn’t have a modal auxiliary verb. But it satisfies pron_pattern
    because it contains a personal pronoun that is the direct object of the sentence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了这些模式，你可以根据每种情况选择应用哪些模式。注意，即使一个句子未能完全满足 dep_pattern 和 pos_pattern 函数，它仍然可能匹配
    pron_pattern 函数。例如，句子“我知道它。”不匹配 dep_pattern 或 pos_pattern 函数，因为它没有情态助动词。但它满足 pron_pattern，因为它包含一个作为句子直接宾语的人称代词。
- en: 'This loose coupling between the patterns lets you use them with other patterns
    or independently. For example, you might use dep_pattern, which checks a sentence
    against the “subject + auxiliary + verb + . . . + direct object . . .” pattern
    in conjunction with, say, a “noun + modal auxiliary verb + base form verb + .
    . . + noun . . .” pattern, if you wanted to be sure that the subject and the direct
    object in the sentence are nouns. These two patterns would match the following
    example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式之间的松散耦合使你可以将它们与其他模式一起使用，或者独立使用。例如，如果你想确保句子中的主语和直接宾语是名词，你可以将 dep_pattern（检查句子是否符合“主语
    + 助动词 + 动词 + . . . + 直接宾语 . . .”模式）与“名词 + 情态助动词 + 基本形式动词 + . . . + 名词 . . .”模式结合使用。这两个模式将匹配以下示例：
- en: Developers might follow this rule.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可能会遵循这个规则。
- en: As you might guess, the ability to combine patterns in different ways allows
    you to handle more scenarios with less code.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，能够以不同方式组合模式使你可以用更少的代码处理更多场景。
- en: '***Using Word Sequence Patterns in Chatbots to Generate Statements***'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***在聊天机器人中使用词序列模式生成陈述***'
- en: As stated earlier, the most challenging tasks in NLP are understanding and generating
    natural language text. A chatbot must understand a user’s input and then generate
    a proper response to it. Word sequence patterns based on linguistic features can
    help you implement these functions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，NLP 中最具挑战性的任务是理解和生成自然语言文本。聊天机器人必须理解用户的输入，并生成适当的回应。基于语言学特征的词序列模式可以帮助你实现这些功能。
- en: In [Chapter 4](../Text/ch04.xhtml#ch04), you learned how to turn a statement
    into a relevant question to continue a conversation with a user. Using word sequence
    patterns, you could generate other kinds of responses, too, such as relevant statements.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](../Text/ch04.xhtml#ch04)中，你学习了如何将陈述句转化为相关问题，从而继续与用户对话。通过使用词序模式，你还可以生成其他类型的响应，例如相关的陈述句。
- en: 'Suppose your chatbot has received the following user input:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的聊天机器人收到以下用户输入：
- en: The symbols are clearly distinguishable. I can recognize them promptly.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 符号显然是可以区分的。我也能迅速识别它们。
- en: 'The chatbot might react as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人可能会做出如下反应：
- en: I can recognize symbols promptly too.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我也能迅速识别符号。
- en: 'You can use the patterns implemented in the previous sections to accomplish
    this text generation task. The list of steps might look like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用前面部分实现的模式来完成这个文本生成任务。步骤列表可能如下所示：
- en: Check the conversational input against the dep_pattern and pos_pattern functions
    defined previously to find an utterance that follows the “subject + auxiliary
    + verb + . . . + direct object . . .” and “pronoun + modal auxiliary verb + base
    form verb + . . . + pronoun . . .” patterns, respectively.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查对话输入是否符合之前定义的`dep_pattern`和`pos_pattern`函数，分别找出符合“主语 + 助动词 + 动词 + ... + 直接宾语...”以及“代词
    + 情态动词 + 基本动词 + ... + 代词...”模式的表达。
- en: Check the utterance found in step 1 against the pron_pattern pattern to determine
    whether the direct object personal pronoun is plural or singular.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查步骤1中找到的表达式是否符合`pron_pattern`模式，以确定直接宾语代词是单数还是复数。
- en: Find the noun that gives its meaning to the pronoun by searching for a noun
    that has the same number as the personal pronoun.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过搜索与人称代词具有相同数量的名词，找出赋予代词意义的名词。
- en: Replace the pronoun that acts as the direct object in the sentence located in
    step 1 with the noun found in step 3.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用步骤3中找到的名词替换步骤1中作为直接宾语的代词。
- en: Append the word “too” to the end of the generated utterance.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成的句子末尾添加“too”一词。
- en: 'The following script implements these steps. It uses the dep_pattern, pos_pattern,
    and pron_pattern functions defined earlier in this chapter (their code is omitted
    to save space). It also introduces two new functions: find_noun and gen_utterance.
    For convenience, we’ll walk through the code in three steps: the initial operations
    and the find_noun function, which finds the noun that matches the personal pronoun;
    the gen_utterance function, which generates a relevant statement from that question;
    and finally, the code that tests an utterance. Here is the first part:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本实现了这些步骤。它使用了本章前面定义的`dep_pattern`、`pos_pattern`和`pron_pattern`函数（为节省空间，省略了它们的代码）。它还引入了两个新函数：`find_noun`和`gen_utterance`。为了方便，我们将分三个步骤讲解代码：初步操作和`find_noun`函数，用于找到与人称代词匹配的名词；`gen_utterance`函数，用于从问题生成相关的陈述；最后是测试表达的代码。以下是第一部分：
- en: import spacy
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 spacy
- en: nlp = spacy.load('en')
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#Insert the dep_pattern, pos_pattern and pron_pattern functions from the'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#从...插入`dep_pattern`、`pos_pattern`和`pron_pattern`函数'
- en: previous listings here
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的前面列表
- en: '#...'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#...'
- en: '➊ def find_noun(➋sents, ➌num):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def find_noun(➋sents, ➌num):'
- en: 'if num == ''plural'':'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 num == ''复数'':'
- en: ➍ taglist = ['NNS','NNPS']
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ taglist = ['NNS','NNPS']
- en: 'if num == ''singular'':'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 num == ''单数'':'
- en: ➎ taglist = ['NN','NNP']
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ taglist = ['NN','NNP']
- en: '➏ for sent in reversed(sents):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '➏ for sent in reversed(sents):'
- en: '➐ for token in sent:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '➐ for token in sent:'
- en: '➑ if token.tag_ in taglist:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '➑ 如果 token.tag_ 在 taglist中:'
- en: return token.text
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 token.text
- en: return 'Noun not found'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 '未找到名词'
- en: After inserting the code of the dep_pattern, pos_pattern, and pron_pattern functions,
    we define the find_noun function, which takes two parameters ➊. The first one
    contains a list of the sentences from the beginning of the discourse up to the
    sentence that satisfies all the patterns here. In this example, this list will
    include all the sentences from the discourse, because only the last sentence satisfies
    all the patterns ➋. But the noun that gives its meaning to the pronoun can be
    found in one of the previous sentences.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在插入`dep_pattern`、`pos_pattern`和`pron_pattern`函数的代码之后，我们定义了`find_noun`函数，它需要两个参数➊。第一个参数包含从话语开头到满足所有模式的句子的句子列表。在这个例子中，这个列表将包含所有话语中的句子，因为只有最后一个句子满足所有模式➋。但是，赋予代词意义的名词可能出现在前面的某个句子中。
- en: The second parameter sent to find_noun is the number of the direct object pronoun
    in the sentence that satisfies all the patterns ➌. The pron_pattern function determines
    this. If the value of this argument is 'plural', we define a Python list containing
    fine-grained part-of-speech tags used in spaCy to mark plural nouns ➍. If it’s
    'singular', we create a tag list containing fine-grained part-of-speech tags used
    to mark singular nouns ➎.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 find_noun 的第二个参数是满足所有模式的直接宾语代词在句子中的位置 ➌。pron_pattern 函数决定了这一点。如果这个参数的值是“复数”，我们定义一个包含
    spaCy 用来标记复数名词的细粒度词性标签的 Python 列表 ➍。如果它是“单数”，我们创建一个包含用来标记单数名词的细粒度词性标签的标签列表 ➎。
- en: In a for loop, we iterate over the sentences in reverse order, starting from
    the sentence that is the closest to the sentence containing the pronoun to be
    replaced ➏. We start with the closest sentence, because the noun we’re searching
    for will most likely be there. Here, we use Python’s reversed function that returns
    a reverse iterator over the list. In the inner loop, we iterate over the tokens
    in each sentence ➐, looking for a token whose fine-grained part-of-speech tag
    is in the tag list defined earlier ➑.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个 for 循环中，我们逆序遍历句子，从距离包含要替换的人称代词的句子最近的句子开始 ➏。我们从最接近的句子开始，因为我们要找的名词最有可能出现在那里。这里，我们使用
    Python 的 reversed 函数，它返回一个反向迭代器。在内层循环中，我们遍历每个句子中的标记 ➐，寻找一个其细粒度的词性标签在之前定义的标签列表中的标记
    ➑。
- en: 'Then we define the gen_utterance function, which generates our new statement:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了 gen_utterance 函数，它生成我们新的语句：
- en: 'def gen_utterance(doc, noun):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 gen_utterance 函数(doc, noun)：
- en: sent = ''
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: sent = ''
- en: '➊ for i,token in enumerate(doc):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 对于 i，token 在 enumerate(doc) 中：
- en: '➋ if token.dep_ == ''dobj'' and token.tag_ == ''PRP'':'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 token.dep_ == 'dobj' 且 token.tag_ == 'PRP'：
- en: ➌ sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)-2].text + 'too.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ sent = doc[:i].text + ' ' + 名词 + ' ' + doc[i+1:len(doc)-2].text + 'too.'
- en: ➍ return sent
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 返回 sent
- en: ➎ return 'Failed to generate an utterance'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ 返回 '生成语句失败'
- en: We use a for loop to iterate over the tokens in the sentence ➊, looking for
    a direct object that is a personal pronoun ➋. Once we’ve found one, we generate
    a new utterance. We change the original sentence by replacing the personal pronoun
    with the matching noun and appending “too” to the end of it ➌. The function then
    returns this newly generated utterance ➍. If we haven’t found a direct object
    in the form of a personal pronoun, the function returns an error message ➎.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个 for 循环遍历句子 ➊ 中的标记，寻找一个是人称代词的直接宾语 ➋。一旦找到，我们就生成一个新的语句。我们通过将人称代词替换为匹配的名词，并在其后附加“too”来修改原始句子
    ➌。然后函数返回这个新生成的语句 ➍。如果没有找到人称代词形式的直接宾语，函数会返回一个错误信息 ➎。
- en: 'Now that we have all the functions in place, we can test them on a sample utterance
    using the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的函数，可以用以下代码在示例语句上进行测试：
- en: ➊ doc = nlp(u'The symbols are clearly distinguishable. I can recognize them
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ doc = nlp(u'这些符号是明显可区分的。我能识别它们。')
- en: promptly.')
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: promptly.')
- en: ➋ sents = list(doc.sents)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ sents = list(doc.sents)
- en: response = ''
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: response = ''
- en: noun = ''
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 名词 = ''
- en: '➌ for i, sent in enumerate(sents):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 对于 i，sent 在 enumerate(sents) 中：
- en: 'if dep_pattern(sent) and pos_pattern(sent):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 dep_pattern(sent) 和 pos_pattern(sent)：
- en: ➍ noun = find_noun(sents[:i], pron_pattern(sent))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 名词 = find_noun(sents[:i], pron_pattern(sent))
- en: 'if noun != ''Noun not found'':'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果名词 != '未找到名词'：
- en: ➎ response = gen_utterance(sents[i],noun)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ response = gen_utterance(sents[i], noun)
- en: break
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: print(response)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: print(response)
- en: After applying the pipeline to the sample discourse ➊, we convert it to the
    list of sentences ➋. Then we iterate over this list ➌, searching for the sentence
    that matches the patterns defined in the dep_pattern and pos_pattern functions.
    Next, we determine the noun that gives the meaning to the pronoun in the sentence
    found in the previous step, using the find_noun function ➍. Finally, we call the
    get_utterance function to generate a response utterance ➎.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在将管道应用于示例语句 ➊ 后，我们将其转换为句子列表 ➋。然后我们遍历这个列表 ➌，寻找符合 dep_pattern 和 pos_pattern 函数定义的模式的句子。接下来，我们使用
    find_noun 函数确定在前一步找到的句子中赋予代词意义的名词 ➍。最后，我们调用 get_utterance 函数生成回应语句 ➎。
- en: 'The output of the preceding code should look like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出应如下所示：
- en: I can recognize symbols too.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我也能识别符号。
- en: '***Try This***'
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***尝试这个***'
- en: Notice that there’s still room for improvement in the preceding code, because
    the original statement included the article “the” in front of the noun “symbols.”
    A better output would include the same article in front of the noun. To generate
    a statement that makes the most sense in this context, expand on the script so
    that it inserts the article “the” in front of the noun, making it “I can recognize
    the symbols too.” For that, you’ll need to check whether the noun is preceded
    by an article, and then add that article.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '**Extracting Keywords from Syntactic Dependency Trees**'
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finding a sequence of words in an utterance that satisfies a certain pattern
    allows you to construct a grammatically correct response—either a statement or
    a question, based on the submitted text. But these patterns aren’t always useful
    for extracting the meaning of texts.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the ticket-booking application in [Chapter 2](../Text/ch02.xhtml#ch02),
    a user might submit a sentence like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: I need an air ticket to Berlin.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: You could easily find the user’s intended destination by searching for the pattern
    “to + GPE” where GPE is a named entity for countries, cities, and states. This
    pattern would match phrases like “to London,” “to California,” and so on.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'But suppose the user submitted one of the following utterances instead:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: I am going to the conference in Berlin. I need an air ticket.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: I am going to the conference, which will be held in Berlin. I would like to
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: book an air ticket.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the “to + GPE” pattern wouldn’t find the destination in either
    example. In both cases, “to” directly refers to “the conference,” not to Berlin.
    You’d need something like “to + . . . + GPE” instead. But how would you know what’s
    required—or what’s allowed—between “to” and “GPE”? For example, the following
    sentence contains the “to + . . . + GPE” pattern but has nothing to do with booking
    a ticket to Berlin:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: I want to book a ticket on a direct flight without landing in Berlin.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Often, you need to examine relations between the words in a sentence to obtain
    necessary pieces of information. This is where walking the dependency tree of
    the sentence could help a lot.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Walking a dependency tree means navigating through it in custom order—not necessarily
    from the first token to the last one. For example, you can stop iterating a dependency
    tree just after the required component is found. Remember that a sentence’s dependency
    tree shows the syntactic relationships between pairs of words. We often represent
    these as arrows connecting the head with the child of a relation. Every word in
    a sentence is involved in at least one of the relations. This guarantees that
    you’ll pass through each word in a sentence when walking through the entire dependency
    tree generated for that sentence if you start from ROOT.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll examine a sentence’s structure to figure out a user’s
    intended meaning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '***Walking a Dependency Tree for Information Extraction***'
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s return to the ticket-booking application example. To find a user’s intended
    destination, you might need to iterate over the dependency tree of a sentence
    to determine whether “to” is semantically related to “Berlin.” This is easy to
    accomplish if you remember the head/child syntactic relations that compose a dependency
    tree, which was introduced in the “Head and Child” box on [page 25](../Text/ch02.xhtml#page_25).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-3](../Text/ch06.xhtml#ch06fig03) shows the dependency tree for the
    sentence, “I am going to the conference in Berlin”:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig6-3.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-3: A syntactic dependency tree of an utterance*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The verb “going” is the root of the sentence, meaning it’s not a child of any
    other word. Its child to the immediate right is “to.” If you walk through the
    dependency tree, moving to the child to the immediate right of each word, you’ll
    finally reach “Berlin.” This shows that there’s a semantic connection between
    “to” and “Berlin” in this sentence.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '***Iterating over the Heads of Tokens***'
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now let’s figure out how to express the relation between “to” and “Berlin” in
    the sentence programmatically. One way is to walk the dependency tree from left
    to right, starting from “to,” choosing only the immediate right child of each
    word along the way. If you can go from “to” to “Berlin” this way, you can reasonably
    assume that there’s a semantic connection between the two words.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'But this approach has a drawback. In some cases, a word might have more than
    one right child. For example, in the sentence “I am going to the conference on
    spaCy, which will be held in Berlin,” the word “conference” has two immediate
    right children: the words “on” and “held.” This forces you to check multiple branches,
    complicating the code.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, although a head can have multiple children, each word in
    a sentence has exactly one head. This means you can instead move from right to
    left, starting from “Berlin” and trying to reach “to.” The following script implements
    this process in the det_destination function:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '#Here''s the function that figures out the destination'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '➊ def det_destination(doc):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'for i, token in enumerate(doc):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '➋ if token.ent_type != 0 and token.ent_type_ == ''GPE'':'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '➌ while True:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ➍ token = token.head
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'if token.text == ''to'':'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: ➎ return doc[i].text
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '➏ if token.head == token:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: return 'Failed to determine'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: return 'Failed to determine'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '#Testing the det_destination function'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u'I am going to the conference in Berlin.')
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: ➐ dest = det_destination(doc)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: print('It seems the user wants a ticket to ' + dest)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In the det_destination function ➊, we iterate over the tokens in the submitted
    utterance, looking for a GPE entity ➋. If it’s found, we start a while loop ➌
    that iterates over the head of each token, starting from the token containing
    the GPE entity ➍. The loop stops when it reaches either the token containing “to”
    ➎ or the root of the sentence. We can check for the root by comparing a token
    to its head ➏, because the head of the root token always refers to itself. (Alternatively,
    we can check for the ROOT tag.)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: To test this function, we apply the pipeline to the sample sentence and then
    invoke the det_destination function on it ➐.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The script should generate the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: It seems the user wants a ticket to Berlin
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'If we change the sample sentence so it doesn’t contain “to” or a GPE named
    entity, we should get the following output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: It seems the user wants a ticket to Failed to determine
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: We can improve the script so it uses another message for cases when it fails
    to determine the user’s destination.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '***Condensing a Text Using Dependency Trees***'
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The syntactic dependency tree approach isn’t limited to chatbots, of course.
    You could use it, for example, in report-processing applications. Say, you need
    to develop an application that has to condense retail reports by extracting only
    the most important information from them.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might want to select the sentences containing numbers, producing
    a concise summary of the data on sales volume, revenue, and costs. (You learned
    how to extract numbers in [Chapter 4](../Text/ch04.xhtml#ch04).) Then, to make
    your new report more concise, you might shorten the selected sentences.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'As a quick example, consider the following sentence:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The product sales hit a new record in the first quarter, with 18.6 million units
    sold.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'After processing, it should look like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The product sales hit 18.6 million units sold.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this, you can analyze the dependency trees of sentences by following
    these steps:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Extract the entire phrase containing the number (it’s 18.6 in this example)
    by walking the heads of tokens, starting from the token containing the number
    and moving from left to right.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Walk the dependency tree from the main word of the extracted phrase (the one
    whose head is out of the phrase) to the main verb of the sentence, iterating over
    the heads and picking them up to be used in a new sentence.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick up the main verb’s subject, along with its leftward children, which typically
    include a determiner and possibly some other modifiers.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 6-4](../Text/ch06.xhtml#ch06fig04) represents this process.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig6-4.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-4: An example of condensing a sentence to include only important
    elements*'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first step, which in this example should extract the phrase
    “18.6 million units sold.”. The following code snippet illustrates how to do this
    programmatically:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u"The product sales hit a new record in the first quarter, with 18.6
    million units sold.")
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: phrase = ''
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in doc:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '➊ if token.pos_ == ''NUM'':'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'while True:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: phrase = phrase + ' ' + token.text
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: ➋ token = token.head
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '➌ if token not in list(token.head.lefts):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: phrase = phrase + ' ' + token.text
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: ➍ break
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: ➎ break
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: print(phrase.strip())
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: We iterate over the sentence’s tokens, looking for one that represents a number
    ➊. If we find one, we start a while loop that iterates over right-hand heads ➋,
    starting from the number token and then appending the text of each head to the
    phrase variable to form a new phrase. To make sure the head of the next token
    is to the right of that token, we check whether the token is in the list of its
    head’s left children ➌. Once this condition returns false, we break from the while
    loop ➍ and then from the outer for loop ➎.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we walk the heads of tokens, starting from the main word of the phrase
    containing the number (“sold” in this example) until we reach the main verb of
    the sentence (“hit” in this example), excluding the adposition (“with” in this
    example). We can implement this as shown in the following listing:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'while True:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: ➊ token = doc[token.i].head
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'if token.pos_ != ''ADP'':'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: ➋ phrase = token.text + phrase
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '➌ if token.dep_ == ''ROOT'':'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: ➍ break
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: We walk the heads of tokens ➊ in a while loop, appending the text of each head
    to the phrase being formed ➋. After reaching the main verb (marked as ROOT) ➌,
    we break from the loop ➍.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we pick up the subject of the sentence, along with its left children:
    “The” and “product.” In this example, the subject is “sales,” so we pick up the
    following noun chunk: “The product sales.” This can be done with the following
    code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '➊ for tok in token.lefts:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '➋ if tok.dep_ == ''nsubj'':'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: ➌ phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' '
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: + phrase
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: break
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: ➍ print(phrase)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: We start by iterating over the main verb’s children ➊, searching for the subject
    ➋. Then we prepend the subject’s children and the subject of the phrase ➌. To
    see the resulting phrase, we print it ➍.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should look like this:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The product sales hit 18.6 million units sold.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The result is a condensed version of the original sentence.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '***Try This***'
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Write a script that condenses financial reports by extracting only those sentences
    that contain phrases referring to an amount of money. Also, the script needs to
    condense the selected sentences so they include only the subject, the main verb,
    the phrase referring to an amount of money, and the tokens you can pick up when
    walking the heads starting from the main word of the money phrase up to the main
    verb of the sentence. For example, given the following sentence:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The company, whose profits reached a record high this year, largely attributed
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: to changes in management, earned a total revenue of $4.26 million.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Your script should return this sentence:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The company earned revenue of $4.26 million.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: In this example, “million” is the main word in the phrase “$4.26 million.” The
    head of “million” is “of,” which is a child of “revenue,” which, in turn, is a
    child of “earned,” the main verb of the sentence.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Context to Improve the Ticket-Booking Chatbot**'
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you’ve no doubt realized by now, there’s no single solution for all intelligent
    text-processing tasks. For example, the ticket-booking script shown earlier in
    this chapter will only find a destination if the submitted sentence contains the
    word “to.”
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to make these scripts more useful is to take context into account to
    determine an appropriate response. Let’s increase the functionality of the ticket-booking
    script so it can handle a wider set of user input, including utterances that don’t
    contain a “to + GPE” pair in any combination. For example, look at the following
    utterance:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: I am attending the conference in Berlin.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the user has expressed an intention to go to Berlin without “to.” Only
    the GPE entity “Berlin” is in the sentence. In such cases, it would be reasonable
    for a chatbot to ask a confirmatory question, such as the following:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: You want a ticket to Berlin, right?
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'The improved ticket-booking chatbot should produce different outputs based
    on three different situations:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: The user expresses a clear intention to book a ticket to a certain destination.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not immediately clear whether the user wants a ticket to the destination
    mentioned.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user doesn’t mention any destination.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on which category the user input falls under, the chatbot generates
    an appropriate response. [Figure 6-5](../Text/ch06.xhtml#ch06fig05) illustrates
    how to represent this user input handling on a diagram.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig6-5.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-5: An example of user input handling in a ticket-booking application*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The following script implements this design. For convenience, the code is divided
    into several parts.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: The first snippet contains the guess_destination function, which searches a
    sentence for a GPE entity. Also, we’ll need to insert the dep_destination function
    defined and discussed in “[Iterating Over the Heads of Tokens](../Text/ch06.xhtml#lev82)”
    on [page 87](../Text/ch06.xhtml#page_87). Recall that this function searches a
    sentence for the “to + GPE” pattern. We’ll need the dep_destination and guess_destination
    functions to handle the first and second scenarios of user input, respectively.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '#Insert the dep_destination function from a previous listing here'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '#...'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'def guess_destination(doc):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in doc:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '➊ if token.ent_type != 0 and token.ent_type_ == ''GPE'':'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ➋ return token.text
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: ➌ return 'Failed to determine'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The code in the guess_destination function iterates over the tokens in a sentence,
    looking for a GPE entity ➊. Once it finds one, the function returns it to the
    calling code ➋. If it fails to find one, the function returns 'Failed to determine'
    ➌, meaning the sentence doesn’t contain a GPE entity.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: In the gen_function that follows, we generate a response based on what the functions
    defined in the preceding snippet return.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'def gen_response(doc):'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ➊ dest = det_destination(doc)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'if dest != ''Failed to determine'':'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ➋ return 'When do you need to be in ' + dest + '?'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ➌ dest = guess_destination(doc)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'if dest != ''Failed to determine'':'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: ➍ return 'You want a ticket to ' + dest +', right?'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ➎ return 'Are you flying somewhere?'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: The code in the gen_response function starts by invoking the det_destination
    function ➊, which determines whether an utterance contains a “to + GPE” pair.
    If one is found, we assume that the user wants a ticket to the destination and
    they need to clarify their departure time ➋.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: If the det_destination function hasn’t found a “to + GPE” pair in the utterance,
    we invoke the guess_destination function ➌. This function tries to find a GPE
    entity. If it finds such an entity, it asks the user a confirmatory question about
    whether they want to fly to that destination ➍. Otherwise, if it finds no GPE
    entity in the utterance, the script asks the user whether they want to fly somewhere
    ➎.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the code, we apply the pipeline to a sentence and then send the doc
    to the gen_response function we used in the previous listing:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u'I am going to the conference in Berlin.')
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: print(gen_response(doc))
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'For the utterance submitted in this example, you should see the following output:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: When do you need to be in Berlin?
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with the sample utterance to see different output.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '**Making a Smarter Chatbot by Finding Proper Modifiers**'
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One way to make your chatbot smarter is to use dependency trees to find modifiers
    for particular words. For example, you might teach your application to recognize
    the adjectives that are applicable to a given noun. Then you could tell the bot,
    “I’d like to read a book,” to which the smart bot could respond like this: “Would
    you like a fiction book?”'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'A *modifier* is an optional element in a phrase or a clause used to change
    the meaning of another element. Removing a modifier doesn’t typically change the
    basic meaning of the sentence, but it does make it less specific. As a quick example,
    consider the following two sentences:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: I want to read a book.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: I want to read a book on Python.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: The first sentence doesn’t use modifiers. The second uses the modifier “on Python,”
    making your request more detailed.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: If you want to be specific, you must use modifiers. For example, to generate
    a proper response to a user, you might need to learn which modifiers you can use
    in conjunction with a given noun or verb.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following phrase:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: That exotic fruit from Africa.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: In this noun phrase, “fruit” is the head, “that” and “exotic” are *premodifiers*—modifiers
    located in front of the word being modified—and “from Africa” is a *postmodifier*
    phrase—a modifier that follows the word it limits or qualifies. [Figure 6-6](../Text/ch06.xhtml#ch06fig06)
    shows the dependency tree for this phrase.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig6-6.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-6: An example of premodifiers and postmodifiers*'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want to determine possible adjectival modifiers for the word “fruit.”
    (Adjectival modifiers are always premodifiers.) Also, you want to look at what
    GPE entities you can find in the postmodifiers of this same word. This information
    could later help you generate an utterance during a conversation on fruits.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script implements this design:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: ➊ doc = nlp(u"Kiwano has jelly-like flesh with a refreshingly fruity taste.
    This
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: is a nice exotic fruit from Africa. It is definitely worth trying.")
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: ➋ fruit_adjectives = []
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: ➌ fruit_origins = []
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in doc:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '➍ if token.text == ''fruit'':'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: ➎ fruit_adjectives = fruit_adjectives + [modifier.text for modifier in
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: token.lefts if modifier.pos_ == 'ADJ']
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: ➏ fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: in token.rights if modifier.text == 'from' and doc[modifier.i + 1].ent_
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: type != 0]
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: print('The list of adjectival modifiers for word fruit:', fruit_adjectives)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: print('The list of GPE names applicable to word fruit as postmodifiers:',
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: fruit_origins)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by applying the pipeline to a short text that contains the word “fruit”
    with both premodifiers and postmodifiers ➊. We define two empty lists: fruit_adjectives
    ➋ and fruit_origins ➌. The first one will hold any adjectival modifiers found
    for the word “fruit.” The second list will hold any GPE entities found among the
    postmodifiers of “fruit.”'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Next, in a loop iterating over the tokens of the entire text, we look for the
    word “fruit” ➍. Once this word is found, we first determine its adjectival premodifiers
    by picking up its syntactic children to the left and choosing only adjectives
    (determiners and compounds can also be premodifiers). We append the adjectival
    modifiers to the fruit_adjectives list ➎.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Then we search for postmodifiers by checking the right-hand syntactic children
    of the word “fruit.” In particular, we look for named entities, and then append
    them to the fruit_origins list ➏.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'The script outputs the following two lists:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of adjectival modifiers for word fruit: [''nice'', ''exotic'']'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of GPE names applicable to word fruit as postmodifiers: [''Africa'']'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Now your bot “knows” that a fruit can be nice, exotic (or both nice and exotic),
    and might come from Africa.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you need to process an utterance, or even just a phrase, it’s often important
    to look at its structure to determine which general patterns it matches. Using
    spaCy’s linguistic features, you can detect these patterns, allowing your script
    to understand the user’s intention and respond properly.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Using patterns based on linguistic features works well when you need to recognize
    the general structure of a sentence, which involves the subject, modal auxiliary
    verb, main verb, and direct object. But a real-world application needs to recognize
    more complicated sentence structures and be prepared for a wider set of user input.
    This is where the syntactic dependency tree of a sentence becomes very useful.
    You can walk the dependency tree of a sentence in different ways, extracting necessary
    pieces of information from it. For example, you can use dependency trees to find
    modifiers for particular words, and then use this information later to generate
    intelligent text.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
