- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: MORE LINEAR ALGEBRA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 更多线性代数**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/common.jpg)'
- en: In this chapter, we’ll continue our exploration of linear algebra concepts.
    Some of these concepts are only tangentially related to deep learning, but they’re
    the sort of math you’ll eventually encounter. Think of this chapter as assumed
    background knowledge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探讨线性代数的概念。虽然其中一些概念与深度学习的关系较为间接，但它们是你最终会遇到的数学知识。可以将本章视为假定的背景知识。
- en: Specifically, we’ll learn more about the properties of and operations on square
    matrices, introducing terms you’ll encounter in the deep learning literature.
    After that, I’ll introduce the ideas behind the eigenvalues and eigenvectors of
    a square matrix and how to find them. Next, we’ll explore vector norms and other
    ways of measuring distance that are often encountered in deep learning. At that
    point, I’ll introduce the important concept of a covariance matrix.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将学习更多关于方阵的性质和运算，介绍在深度学习文献中常遇到的术语。之后，我将介绍方阵的特征值和特征向量的概念，以及如何求解它们。接下来，我们将探索向量范数和其他常见的距离度量方法，这些方法通常出现在深度学习中。此时，我将介绍协方差矩阵这一重要概念。
- en: We’ll conclude the chapter by demonstrating principal component analysis (PCA)
    and singular value decomposition (SVD). These frequently used approaches depend
    heavily on the concepts and operators introduced throughout the chapter. We will
    see what PCA is, how to do it, and what it can buy us from a machine learning
    perspective. Similarly, we will work with SVD and see how we can use it to implement
    PCA as well as compute the pseudoinverse of a rectangular matrix.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的结尾通过展示主成分分析（PCA）和奇异值分解（SVD）来总结。这些常用的方法在很大程度上依赖于本章所介绍的概念和运算符。我们将了解PCA是什么，如何执行PCA，以及从机器学习的角度来看，PCA能为我们带来什么。同样，我们也会学习SVD，并看看如何用它来实现PCA，以及计算矩形矩阵的伪逆。
- en: Square Matrices
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方阵
- en: Square matrices occupy a special place in the world of linear algebra. Let’s
    explore them in more detail. The terms used here will show up often in deep learning
    and other areas.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 方阵在线性代数中占有特殊地位。让我们更详细地探讨它们。这里使用的术语在深度学习和其他领域中将频繁出现。
- en: Why Square Matrices?
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么是方阵？
- en: 'If we multiply a matrix by a column vector, we’ll get another column vector
    as output:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一个矩阵与一个列向量相乘，结果将是另一个列向量：
- en: '![Image](Images/128equ01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/128equ01.jpg)'
- en: Interpreted geometrically, the 2 × 4 matrix has mapped the 4 × 1 column vector,
    a point in ℝ⁴, to a new point in ℝ². The mapping is linear because the point values
    are only being multiplied by the elements of the 2 × 4 matrix; there are no nonlinear
    operations, such as raising the components of the vector to a power, for example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何角度解释，2 × 4的矩阵已经将4 × 1的列向量，一个ℝ⁴中的点，映射到了ℝ²中的一个新点。该映射是线性的，因为点值只是与2 × 4矩阵的元素相乘；没有进行非线性操作，比如将向量的组件进行幂运算等。
- en: Viewed this way, we can use a matrix to transform points between spaces. If
    the matrix is square, say, *n* × *n*, the mapping is from ℝ^(*n*) back to ℝ^(*n*).
    For example, consider
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们可以使用矩阵在空间之间变换点。如果矩阵是方阵，比如*n* × *n*，那么映射是从ℝ^(*n*)回到ℝ^(*n*)。例如，考虑
- en: '![Image](Images/128equ02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/128equ02.jpg)'
- en: where the point (11, 12, 13) is mapped to the point (74, 182, 209), both in
    ℝ³.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，点（11，12，13）被映射到点（74，182，209），这两个点都在ℝ³中。
- en: Using a matrix to map points from one space to another makes it possible to
    rotate a set of points about an axis by using a *rotation matrix*. For simple
    rotations, we can define matrices in 2D,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵将点从一个空间映射到另一个空间，可以通过使用*旋转矩阵*来实现围绕一个轴旋转一组点。对于简单的旋转，我们可以定义二维矩阵，
- en: '![Image](Images/06equ01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06equ01.jpg)'
- en: and in 3D,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中，
- en: '![Image](Images/128equ03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/128equ03.jpg)'
- en: Rotations are by an angle, *θ*, and for 3D, about the x-, y-, or z-axis, as
    indicated by the subscript.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转是由一个角度*θ*确定的，对于三维空间，旋转围绕x轴、y轴或z轴进行，如下标所示。
- en: Using a matrix, we can create an *affine transformation*. An affine transformation
    maps a set of points into another set of points so that points on a line in the
    original space are still on a line in the mapped space. The transformation is
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵，我们可以创建一个*仿射变换*。仿射变换将一组点映射到另一组点，使得原空间中一条直线上的点在映射空间中仍然处于直线上。该变换为：
- en: '***y*** = ***Ax*** + ***b***'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***y*** = ***Ax*** + ***b***'
- en: The affine transform combines a matrix transform, ***A***, with a translation,
    ***b***, to map a vector, ***x***, to a new vector, ***y***. We can combine this
    operation into a single matrix multiplication by putting ***A*** in the upper-left
    corner of the matrix and adding ***b*** as a new column on the right. A row of
    all zeros at the bottom with a single 1 in the rightmost column completes the
    augmented transformation matrix. For an affine transformation matrix
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 仿射变换将矩阵变换***A***与平移***b***结合起来，将向量***x***映射到新向量***y***。我们可以通过将***A***放在矩阵的左上角，并将***b***作为新列添加到右侧，来将这一操作合并成一个单一的矩阵乘法。矩阵底部添加一行全零，并在最右列放置一个1，完成扩展的变换矩阵。对于仿射变换矩阵
- en: '![Image](Images/129equ01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/129equ01.jpg)'
- en: and translation vector
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 和平移向量
- en: '![Image](Images/129equ02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/129equ02.jpg)'
- en: we get
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到
- en: '![Image](Images/129equ03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/129equ03.jpg)'
- en: This form maps a point, *(x*, *y*), to a new point, (*x*′, *y*′).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式将一个点，*(x*, *y*)，映射到一个新点，(*x*′, *y*′)。
- en: This maneuver is identical to the *bias trick* sometimes used when implementing
    a neural network to bury the bias in an augmented weight matrix by including an
    extra feature vector input set to 1\. In fact, we can view a feedforward neural
    network as a series of affine transformations, where the transformation matrix
    is the weight matrix between the layers, and the bias vector provides the translation.
    The activation function at each layer alters the otherwise linear relationship
    between the layers. It is this nonlinearity that lets the network learn a new
    way to map inputs so that the final output reflects the functional relationship
    the network is designed to learn.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作与在实现神经网络时常用的*偏置技巧*相同，通过在扩展权重矩阵中包含一个额外的特征向量输入设置为1来隐藏偏置。事实上，我们可以将前馈神经网络视为一系列仿射变换，其中变换矩阵是层与层之间的权重矩阵，偏置向量提供平移。每一层的激活函数改变了层之间的线性关系。正是这种非线性使得网络能够学习一种新的方式来映射输入，以便最终输出反映网络设计要学习的功能关系。
- en: We use square matrices, then, to map points from one space back into the same
    space, for example to rotate them about an axis. Let’s look now at some special
    properties of square matrices.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用方阵，将点从一个空间映射回同一空间，例如绕轴旋转它们。现在，让我们来看看方阵的一些特殊属性。
- en: Transpose, Trace, and Powers
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转置、迹和幂
- en: '[Chapter 5](ch05.xhtml#ch05) showed us the vector transpose to move between
    column and row vectors. The transpose operation is not restricted to vectors.
    It works for any matrix by flipping the rows and columns along the main diagonal.
    For example,'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](ch05.xhtml#ch05)向我们展示了向量转置，用于在列向量和行向量之间移动。转置操作不仅限于向量，它适用于任何矩阵，通过沿主对角线翻转行和列。例如，'
- en: '![Image](Images/129equ04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/129equ04.jpg)'
- en: 'The transpose is formed by flipping the indices of the matrix elements:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 转置是通过翻转矩阵元素的索引来形成的：
- en: '*a[ij]* ← *a[ij]*, *i* = 0, 1, . . . , *n* − 1, *j* = 0, 1, . . . , *m* – 1'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*a[ij]* ← *a[ij]*, *i* = 0, 1, . . . , *n* − 1, *j* = 0, 1, . . . , *m* – 1'
- en: This changes an *n* × *m* matrix into an *m* × *n* matrix. Notice that the order
    of a square matrix remains the same under the transpose operation, and the values
    on the main diagonal don’t change.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将一个 *n* × *m* 矩阵变成 *m* × *n* 矩阵。注意，在转置操作下，方阵的顺序保持不变，且主对角线上的值不会改变。
- en: In NumPy, you can call the transpose method on an array, but the transpose is
    so common that a shorthand notation (.T) also exists. For example,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中，您可以对数组调用转置方法，但由于转置如此常见，因此也有简写符号 (.T)。例如，
- en: '>>> import numpy as np'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import numpy as np'
- en: '>>> a = np.array([[1,2,3],[4,5,6],[7,8,9]])'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.array([[1,2,3],[4,5,6],[7,8,9]])'
- en: '>>> print(a)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a)'
- en: '[[1 2 3]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2 3]'
- en: '[4 5 6]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[4 5 6]'
- en: '[7 8 9]]'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[7 8 9]]'
- en: '>>> print(a.transpose())'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a.transpose())'
- en: '[[1 4 7]'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 4 7]'
- en: '[2 5 8]'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[2 5 8]'
- en: '[3 6 9]]'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[3 6 9]]'
- en: '>>> print(a.T)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a.T)'
- en: '[[1 4 7]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 4 7]'
- en: '[2 5 8]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[2 5 8]'
- en: '[3 6 9]]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[3 6 9]]'
- en: 'The *trace* is another common operation applied to square matrices:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*迹*是应用于方阵的另一个常见操作：'
- en: '![Image](Images/130equ01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/130equ01.jpg)'
- en: 'As an operator, the trace has certain properties. For example, it’s linear:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个运算符，迹有一定的属性。例如，它是线性的：
- en: tr(***A*** + ***B***) = tr***A*** + tr***B***
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: tr(***A*** + ***B***) = tr***A*** + tr***B***
- en: It’s also true that tr(***A***) = tr(***A****^T*) and tr(***AB***) = tr(***BA***).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，tr(***A***) = tr(***A****^T*) 并且 tr(***AB***) = tr(***BA***)。
- en: NumPy uses np.trace to quickly calculate the trace of a matrix and np .diag
    to return the diagonal elements of a matrix as a 1D array,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy使用np.trace快速计算矩阵的迹，并使用np.diag返回矩阵的对角元素作为一维数组，
- en: (*a*[00], *a*[11], . . . , *a[n−1,n−1]*)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (*a*[00], *a*[11], . . . , *a[n−1,n−1]*)
- en: for an *n* × *n* or *n* × *m* matrix.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*n* × *n*或*n* × *m*矩阵。
- en: 'A matrix doesn’t need to be square for NumPy to return the elements along its
    diagonal. And although mathematically the trace generally only applies to square
    matrices, NumPy will calculate the trace of any matrix, returning the sum of the
    diagonal elements:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NumPy来说，矩阵不必是方阵，仍然可以返回其对角线上的元素。虽然在数学上，迹一般只适用于方阵，但NumPy会计算任何矩阵的迹，返回对角线元素的和：
- en: '>>> b = np.array([[1,2,3,4],[5,6,7,8]])'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> b = np.array([[1,2,3,4],[5,6,7,8]])'
- en: '>>> print(b)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(b)'
- en: '[[1 2 3 4]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2 3 4]'
- en: '[5 6 7 8]]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[5 6 7 8]]'
- en: '>>> print(np.diag(b))'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.diag(b))'
- en: '[1 6]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 6]'
- en: '>>> print(np.trace(b))'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.trace(b))'
- en: '7'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '7'
- en: Lastly, you can multiply a square matrix by itself, implying that you can raise
    a square matrix to an integer power, *n*, by multiplying itself *n* times. Note
    that this is not the same as raising the elements of the matrix to a power. For
    example,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以将一个方阵与自身相乘，这意味着你可以将一个方阵提升到一个整数次幂*n*，通过将它自己乘*n*次。请注意，这与将矩阵的元素提升到某个幂次不同。例如，
- en: '![Image](Images/131equ01.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/131equ01.jpg)'
- en: 'The matrix power follows the same rules as raising any number to a power:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的幂遵循与将数字提升为幂次相同的规则：
- en: '***A**^n**A**^m* = ***A**^(n+m)*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***A**^n**A**^m* = ***A**^(n+m)*'
- en: (***A**^n*)^(*m*) = ***A**^(nm)*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (***A**^n*)^(*m*) = ***A**^(nm)*
- en: for ![Image](Images/131equ01a.jpg) (positive integers) and where ***A*** is
    a square matrix.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于![Image](Images/131equ01a.jpg)（正整数）且***A***是一个方阵。
- en: 'NumPy provides a function to compute the power of a square matrix more efficiently
    than repeated calls to np.dot:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy提供了一种比重复调用np.dot更高效地计算方阵幂的方法：
- en: '>>> from numpy.linalg import matrix_power'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from numpy.linalg import matrix_power'
- en: '>>> a = np.array([[1,2],[3,4]])'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.array([[1,2],[3,4]])'
- en: '>>> print(matrix_power(a,2))'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(matrix_power(a,2))'
- en: '[[ 7 10]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 7 10]'
- en: '[15 22]]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[15 22]]'
- en: '>>> print(matrix_power(a,10))'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(matrix_power(a,10))'
- en: '[[ 4783807 6972050]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 4783807 6972050]'
- en: '[10458075 15241882]]'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[10458075 15241882]]'
- en: Now let’s consider some special square matrices that you’ll encounter from time
    to time.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一些你可能会时常遇到的特殊方阵。
- en: Special Square Matrices
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特殊方阵
- en: 'Many square (and nonsquare) matrices have received special names. Some are
    rather obvious, like matrices that are all zero or one, which are called *zeros
    matrices* and *ones matrices*, respectively. NumPy uses these extensively:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方阵（和非方阵）都有特殊名称。一些非常直观，比如全为零或全为一的矩阵，分别称为*零矩阵*和*单位矩阵*。NumPy在这方面应用得很广泛：
- en: '>>> print(np.zeros((3,5)))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.zeros((3,5)))'
- en: '[[0\. 0\. 0\. 0\. 0.]'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[[0\. 0\. 0\. 0\. 0.]]'
- en: '[0\. 0\. 0\. 0\. 0.]'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 0\. 0\. 0.]'
- en: '[0\. 0\. 0\. 0\. 0.]]'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 0\. 0\. 0.]]'
- en: '>>> print(np.ones(3,3))'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.ones(3,3))'
- en: '[[1\. 1\. 1.]'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1\. 1\. 1.]'
- en: '[1\. 1\. 1.]'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. 1\. 1.]'
- en: '[1\. 1\. 1.]]'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[1\. 1\. 1.]]'
- en: Note that you can find a matrix of any constant value, *c*, by multiplying the
    ones matrix by *c*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过将全1矩阵乘以*c*，你可以得到一个常数值为*c*的矩阵。
- en: Notice above that NumPy defaults to matrices of 64-bit floating-point numbers
    corresponding to a C-language type of double. See [Table 1-1](ch01.xhtml#ch01tab01)
    on page 6 for a list of possible numeric data types. You can specify the desired
    data type with the dtype keyword. In pure mathematics, we don’t care much about
    data types, but to work in deep learning, you need to pay attention to avoid defining
    arrays that are far more memory-hungry than needed. Many deep learning models
    are happy with arrays of 32-bit floats, which use half the memory per element
    than the NumPy default. Also, many toolkits make use of new or previously seldom-used
    data types, like 16-bit floats, to allow for even better use of memory. NumPy
    does support 16-bit floats by specifying float16 as the dtype.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意上面，NumPy默认矩阵使用64位浮点数，这对应于C语言中的double类型。请参见[表1-1](ch01.xhtml#ch01tab01)第6页，了解可能的数据类型列表。你可以通过dtype关键字指定所需的数据类型。在纯数学中，我们不太关注数据类型，但在深度学习中，你需要特别注意，避免定义比实际需要的更多的内存消耗的数组。许多深度学习模型对于32位浮点数组都能很好地工作，它们每个元素的内存占用是NumPy默认值的一半。另外，许多工具包开始使用新型或以前很少用的数据类型，如16位浮点数，以便更高效地使用内存。NumPy通过将dtype指定为float16，支持16位浮点数。
- en: The Identity Matrix
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单位矩阵
- en: 'By far, the most important special matrix is the *identity matrix*. This is
    a square matrix with all ones on the diagonal:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，最重要的特殊矩阵是*单位矩阵*。这是一个对角线全为1的方阵：
- en: '![Image](Images/06equ02.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ02.jpg)'
- en: The identity matrix acts like the number 1 when multiplying a matrix. Therefore,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 单位矩阵在矩阵乘法中起到类似数字1的作用。因此，
- en: '***AI*** = ***IA*** = ***A***'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '***AI*** = ***IA*** = ***A***'
- en: for an *n* × *n* square matrix ***A*** and an *n* × *n* identity matrix ***I***.
    When necessary, we’ll add a subscript to indicate the order of the identity matrix,
    for example, ***I**[n]*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个*n* × *n*的方阵***A***和一个*n* × *n*的单位矩阵***I***。在必要时，我们会加上下标来表示单位矩阵的阶数，例如***I**[n]*。
- en: 'NumPy uses np.identity or np.eye to generate identity matrices of a given size:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 使用 np.identity 或 np.eye 来生成给定大小的单位矩阵：
- en: '>>> a = np.array([[1,2],[3,4]])'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.array([[1,2],[3,4]])'
- en: '>>> i = np.identity(2)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> i = np.identity(2)'
- en: '>>> print(i)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(i)'
- en: '[[1\. 0.]'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1. 0.]'
- en: '[0\. 1.]]'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[0. 1.]]'
- en: '>>> print(a @ i)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a @ i)'
- en: '[[1\. 2.]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1. 2.]'
- en: '[3\. 4.]]'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[3. 4.]]'
- en: Look carefully at the example above. Mathematically, we said that multiplication
    of a square matrix by the identity matrix of the same order returns the matrix.
    NumPy, however, did something we might not want. Matrix a was defined with integer
    elements, so it has a data type of int64, the NumPy default for integers. However,
    since we didn’t explicitly provide np.identity with a data type, NumPy defaulted
    to a 64-bit float. Therefore, matrix multiplication (@) between a and i returned
    a floating-point version of a. This subtle change of data type might be important
    for later calculations, so, again, we need to pay attention to data types when
    using NumPy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细看一下上面的例子。从数学角度讲，我们说将方阵与同阶的单位矩阵相乘会返回原矩阵。然而，NumPy 做了一些我们可能不希望发生的事情。矩阵a是用整数元素定义的，因此它的数据类型是int64，这是NumPy默认的整数类型。然而，由于我们没有显式地为np.identity指定数据类型，NumPy
    默认为64位浮点数。因此，矩阵a与i的矩阵乘法（@）返回了a的浮点数版本。这种数据类型的细微变化可能会影响后续的计算，因此我们在使用NumPy时需要特别注意数据类型。
- en: It doesn’t matter if you use np.identity or np.eye. In fact, internally, np.identity
    is just a wrapper for np.eye.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是使用 np.identity 还是 np.eye 都没有关系。实际上，np.identity 只是 np.eye 的一个封装。
- en: Triangular Matrices
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 三角矩阵
- en: 'Occasionally, you’ll hear about *triangular* matrices. There are two kinds:
    upper and lower. As you may intuit from the name, an upper triangular matrix is
    one with nonzero elements in the part on or above the main diagonal, whereas a
    lower triangular matrix only has elements on or below the main diagonal. For example,'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔你会听到*三角矩阵*的概念。三角矩阵有两种类型：上三角矩阵和下三角矩阵。正如你从名字中可以直观推测的那样，上三角矩阵是指主对角线及其上方的部分包含非零元素，而下三角矩阵只有主对角线及其下方的部分包含非零元素。例如，
- en: '![Image](Images/133equ01.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/133equ01.jpg)'
- en: is an upper triangular matrix, whereas
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个上三角矩阵，而
- en: '![Image](Images/133equ02.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/133equ02.jpg)'
- en: is a lower triangular matrix. A matrix that has elements only on the main diagonal
    is, not surprisingly, a *diagonal matrix*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个下三角矩阵。一个仅在主对角线上有元素的矩阵，毫不意外的是，它是一个*对角矩阵*。
- en: NumPy has two functions, np.triu and np.tril, to return the upper or lower triangular
    part of the given matrix, respectively. So,
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 提供了两个函数，np.triu 和 np.tril，分别用于返回给定矩阵的上三角部分或下三角部分。因此，
- en: '>>> a = np.arange(16).reshape((4,4))'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.arange(16).reshape((4,4))'
- en: '>>> print(a)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a)'
- en: '[[ 0  1  2  3]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 0 1 2 3]'
- en: '[ 4  5  6  7]'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 4 5 6 7]'
- en: '[ 8  9 10 11]'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 8 9 10 11]'
- en: '[12 13 14 15]]'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[12 13 14 15]]'
- en: '>>> print(np.triu(a))'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.triu(a))'
- en: '[[ 0 1  2  3]'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 0 1 2 3]'
- en: '[ 0 5  6  7]'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0 5 6 7]'
- en: '[ 0 0 10 11]'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0 0 10 11]'
- en: '[ 0 0 0 15]]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0 0 0 15]]'
- en: '>>> print(np.tril(a))'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.tril(a))'
- en: '[[ 0  0  0  0]'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 0 0 0 0]'
- en: '[ 4  5  0  0]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 4 5 0 0]'
- en: '[ 8  9 10  0]'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 8 9 10 0]'
- en: '[12 13 14 15]]'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[12 13 14 15]]'
- en: We don’t frequently use triangular matrices in deep learning, but we do use
    them in linear algebra, in part to compute determinants, to which we now turn.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中我们不常使用三角矩阵，但在线性代数中我们确实会用到它们，部分原因是为了计算行列式，现在我们将讨论这一点。
- en: Determinants
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 行列式
- en: We can think of the *determinant* of a square matrix, *n* × *n*, as a function
    mapping square matrices to a scalar. The primary use of the determinant in deep
    learning is to compute the eigenvalues of a matrix. We’ll see what that means
    later in this chapter, but for now think of eigenvalues as special scalar values
    associated with a matrix. The determinant also tells us something about whether
    or not a matrix has an inverse, as we’ll also see below. Notationally, we write
    the determinant of a matrix with vertical bars. For example, if ***A*** is a 3
    × 3 matrix, we write the determinant as
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个方阵的*行列式*，*n* × *n*，看作是一个将方阵映射到标量的函数。行列式在深度学习中的主要用途是计算矩阵的特征值。我们稍后会详细了解这一点，但现在可以把特征值看作是与矩阵相关的特殊标量值。行列式还可以告诉我们矩阵是否有逆矩阵，我们稍后也会看到这一点。在记法上，我们用竖线表示矩阵的行列式。例如，如果***A***是一个3
    × 3的矩阵，我们写行列式为
- en: '![Image](Images/134equ01.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/134equ01.jpg)'
- en: 'where we state explicitly that the value of the determinant is a scalar (element
    of ℝ). All square matrices have a determinant. For now, let’s consider some of
    the properties of the determinant:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们明确指出行列式的值是一个标量（实数集合 ℝ 的元素）。所有方阵都有行列式。现在，我们来考虑一些行列式的性质：
- en: If any row or column of ***A*** is zero, then det(***A***) = 0.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 的任意一行或一列为零，则 det(***A***) = 0。
- en: If any two rows of ***A*** are identical, then det(***A***) = 0.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 的任意两行相同，那么 det(***A***) = 0。
- en: If ***A*** is an upper or lower triangular, then det ![Image](Images/134equ02.jpg).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 是上三角矩阵或下三角矩阵，那么 det ![Image](Images/134equ02.jpg)。
- en: If ***A*** is a diagonal matrix, then det ![Image](Images/134equ03.jpg).
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 是对角矩阵，则 det ![Image](Images/134equ03.jpg)。
- en: The determinant of the identity matrix, regardless of size, is 1.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单位矩阵的行列式，无论大小如何，都是 1。
- en: The determinant of a product of matrices is the product of the determinants,
    det(***AB***) = det(***A***)det(***B***).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘积的行列式是各自行列式的乘积，det(***AB***) = det(***A***)det(***B***)。
- en: det(***A***) = det(***A***^⊤).
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: det(***A***) = det(***A***^⊤)。
- en: det(***A**^n*) = det(***A***)^(*n*).
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: det(***A**^n*) = det(***A***)^(*n*)。
- en: Property 7 indicates that the transpose operation does not change the value
    of a determinant. Property 8 is a consequence of Property 6.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 属性 7 表明转置操作不会改变行列式的值。属性 8 是属性 6 的一个推论。
- en: We have multiple ways we can calculate the determinant of a square matrix. We’ll
    examine only one way here, which involves using a recursive formula. All recursive
    formulas apply themselves, just as recursive functions in code call themselves.
    The general idea is that each recursion works on a simpler version of the problem,
    which can be combined to return the solution to the larger problem.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方法可以计算方阵的行列式。在这里我们只讨论一种方法，它涉及使用递归公式。所有递归公式都是自我调用的，就像代码中的递归函数调用自己一样。一般来说，每个递归操作处理问题的简化版本，然后可以将它们结合起来，得到更大问题的解决方案。
- en: For example, we can calculate the factorial of an integer,
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以计算一个整数的阶乘，
- en: '*n*! = *n*(*n* − 1)(*n* − 2)(*n* − 3) . . . 1'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*! = *n*(*n* − 1)(*n* − 2)(*n* − 3) . . . 1'
- en: 'recursively if we notice the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 递归地，如果我们注意到以下几点：
- en: '![Image](Images/135equ01.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/135equ01.jpg)'
- en: 'The first statement says that the factorial of *n* is *n* times the factorial
    of (*n* − 1). The second statement says that the factorial of zero is one. The
    recursion is the first statement, but this recursion will never end without some
    condition that returns a value. That’s the point of the second statement, the
    *base case*: it says the recursion ends when we get to zero.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一句话表示 *n* 的阶乘是 *n* 乘以 (*n* − 1) 的阶乘。第二句话表示零的阶乘是 1。递归是第一句话所描述的内容，但如果没有某种条件返回一个值，递归将永远不会结束。这就是第二句话的意义，*基准情况*：它说明当我们得到零时，递归就结束了。
- en: 'This might be clearer in code. We can define the factorial like so:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这在代码中可能更清晰。我们可以这样定义阶乘：
- en: 'def factorial(n):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'def factorial(n):'
- en: 'if (n == 0):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (n == 0):'
- en: return 1
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: return 1
- en: return n*factorial(n-1)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: return n*factorial(n-1)
- en: Notice that factorial calls itself on the argument minus one, unless the argument
    is zero, in which case it immediately returns one. The code works because of the
    Python call stack. The call stack keeps track of all the computations of n*factorial(n-1).
    When we encounter the base case, all the pending multiplications are done, and
    we return the final value.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，阶乘函数会在参数减一时进行自我调用，除非参数为零，在这种情况下它会立即返回 1。这个代码之所以能工作，是因为 Python 的调用栈。调用栈会追踪所有的
    n*factorial(n-1) 计算。当我们遇到基准情况时，所有待完成的乘法操作都已完成，并且我们返回最终结果。
- en: To calculate determinants recursively, then, we need a recursion statement,
    something that defines the determinant of a matrix in terms of simpler determinants.
    We also need a base case that gives us a definitive value. For determinants, the
    base case is when we get to a 1 × 1 matrix. For any 1 × 1 matrix, ***A***, we
    have
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了递归地计算行列式，我们需要一个递归语句，定义行列式的计算方法，并且用简化版的行列式来表示。我们还需要一个基准情况，给我们一个确定的值。对于行列式来说，基准情况是当我们得到一个
    1 × 1 的矩阵时。对于任何 1 × 1 的矩阵 ***A***，我们有
- en: det(***A***) = *a*[00]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A***) = *a*[00]
- en: meaning the determinant of a 1 × 1 matrix is the single value it contains.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 1 × 1 矩阵的行列式就是它所包含的唯一值。
- en: Our plan is to calculate the determinant by breaking the calculation into successively
    simpler determinants until we reach the base case above. To do this, we need a
    statement involving recursion. However, we need to define a few things before
    we can make the statement. First, we need to define the *minor* of a matrix. The
    (*i*, *j*)-minor of a matrix, ***A***, is the matrix left after removing the *i*th
    row and *j*th column of ***A***. We’ll denote a minor matrix by ***A****[ij]*.
    For example, given
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计划是通过将计算分解成逐步简化的行列式，直到我们达到上面的基本情况。为此，我们需要一个涉及递归的语句。然而，在编写这个语句之前，我们需要先定义一些概念。首先，我们需要定义矩阵的*余子式*。矩阵***A***的(*i*,
    *j*)-余子式，是在去掉矩阵***A***的第*i*行和第*j*列之后剩下的矩阵。我们用***A****[ij]*表示一个余子式矩阵。例如，给定
- en: '![Image](Images/135equ02.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/135equ02.jpg)'
- en: then
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![Image](Images/136equ01.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/136equ01.jpg)'
- en: where the minor, ***A***[11], is found by deleting row 1 and column 1 to leave
    only the underlined values.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，余子式***A***[11]是通过删除第1行和第1列得到的，仅保留下划线部分的值。
- en: Second, we need to define the *cofactor*, *C**[ij]*, of the minor, ***A****[ij]*.
    This is where our recursive statement appears. The definition is
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们需要定义*余因子*，*C**[ij]*，即余子式***A****[ij]*的余因子。这里就是我们的递归语句出现的地方。定义为
- en: '*C**[ij]* = (−1)^(*i*+*j*+2)det(***A****[ij]*)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*C**[ij]* = (−1)^(*i*+*j*+2)det(***A****[ij]*)'
- en: 'The cofactor depends on the determinant of the minor. Notice the exponent on
    −1, written as *i* + *j* + 2\. If you look at most math books, you’ll see the
    exponent as *i* + *j*. We’ve made a conscious choice to define matrices with zero-based
    indices so the math and implementation in code match without being off by one.
    Here’s one place where that choice forces us to be less elegant than the math
    texts. Because our indices are “off” by one, we need to add that one back into
    the exponent of the cofactor so the pattern of positive and negative values that
    the cofactor uses is correct. This means adding one to each of the variables in
    the exponent: *i* → *i* + 1 and *j* → *j* + 1\. This makes the exponent *i* +
    *j* → (*i* + 1) + (*j* + 1) = *i* + *j* + 2.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 余因子依赖于余子式的行列式。请注意，指数是-1的幂，写作*i* + *j* + 2。如果你翻阅大多数数学书籍，你会看到指数是*i* + *j*。我们特意选择了用从零开始的索引来定义矩阵，这样数学推导和代码实现就能匹配，而不会因为索引偏差而出错。这里正是我们选择这种方式让数学公式不够优雅的地方。由于我们的索引从0开始，我们需要将这一偏差加回到余因子的指数中，以确保余因子使用的正负值模式正确。这意味着要将指数中的每个变量加1：*i*
    → *i* + 1 和 *j* → *j* + 1。这样，指数*i* + *j*就变成了(*i* + 1) + (*j* + 1) = *i* + *j*
    + 2。
- en: We’re now ready for our full recursive definition of the determinant of ***A***
    by using *cofactor expansion*. It turns out that summing the product of the matrix
    values and associated cofactors for any row or column of a square matrix will
    give us the determinant. So, we’ll use the first row of the matrix and calculate
    the determinant as
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好通过使用*余因子展开法*来完整地递归定义矩阵***A***的行列式。事实证明，对于任何一个方阵，计算某一行或某一列的矩阵值与相应的余因子的乘积并求和，就能得到行列式。因此，我们将使用矩阵的第一行，并将行列式计算为
- en: '![Image](Images/06equ03.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06equ03.jpg)'
- en: 'You may be wondering: Where’s the recursion in [Equation 6.3](ch06.xhtml#ch06equ03)?
    It shows up on the determinant of the minor. If ***A*** is an *n* × *n* matrix,
    the minor, ***A****[ij]*, is an *(n* − 1) × (*n* − 1) matrix. Therefore, to calculate
    the cofactors to find the determinant of an *n* × *n* matrix, we need to know
    how to find the determinant of an *(n* − 1) × (*n* − 1) matrix. However, we can
    use cofactor expansion to find the *(n* − 1) × (*n* − 1) determinant, which involves
    finding the determinant of an *(n* − 2) × (*n* − 2) matrix. This process continues
    until we get to a 1 × 1 matrix. We already know the determinant of a 1 × 1 matrix
    is the single value it contains.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：在[方程 6.3](ch06.xhtml#ch06equ03)中，递归在哪里呢？它出现在余子式的行列式中。如果***A***是一个*n* ×
    *n*矩阵，余子式***A****[ij]*是一个*(n* − 1) × (*n* − 1)矩阵。因此，为了计算余因子以求出*n* × *n*矩阵的行列式，我们需要知道如何计算*(n*
    − 1) × (*n* − 1)矩阵的行列式。然而，我们可以使用余因子展开法来求*(n* − 1) × (*n* − 1)矩阵的行列式，这需要求出*(n*
    − 2) × (*n* − 2)矩阵的行列式。这个过程会一直持续，直到我们得到一个1 × 1矩阵。我们已经知道，1 × 1矩阵的行列式就是它所包含的唯一值。
- en: 'Let’s work through this process for a 2 × 2 matrix:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个2 × 2矩阵来演示这个过程：
- en: '![Image](Images/136equ02.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/136equ02.jpg)'
- en: Using cofactor expansion, we get
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余因子展开法，我们得到
- en: '![Image](Images/137equ01.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/137equ01.jpg)'
- en: which is the formula for the determinant of a 2 × 2 matrix. The minors of a
    2 × 2 matrix are 1 × 1 matrices, each returning either *d* or *c* in this case.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是2 × 2矩阵行列式的公式。2 × 2矩阵的余子式是1 × 1矩阵，在这种情况下，它们分别返回*d*或*c*。
- en: In NumPy, we calculate determinants with np.linalg.det. For example,
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，我们使用 np.linalg.det 计算行列式。例如，
- en: '>>> a = np.array([[1,2],[3,4]])'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.array([[1,2],[3,4]])'
- en: '>>> print(a)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a)'
- en: '[[1 2]'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2]'
- en: '[3 4]]'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[3 4]]'
- en: '>>> np.linalg.det(a)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.linalg.det(a)'
- en: '-2.0000000000000004'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '-2.0000000000000004'
- en: '>>> 1*4 - 2*3'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> 1*4 - 2*3'
- en: '-2'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '-2'
- en: 'The last line of code uses the formula for a 2 × 2 matrix we derived above
    for comparison purposes. Internally, NumPy does not use recursive cofactor expansion
    to calculate the determinant. Instead, it factors the matrix into the product
    of three matrices: (1) a *permutation matrix*, which looks like a scrambled identity
    matrix with only a single one in each row and column, (2) a lower triangular matrix,
    and (3) an upper triangular matrix. The determinant of the permutation matrix
    is either +1 or −1\. The determinant of a triangular matrix is the product of
    the diagonal elements, while the determinant of a product of matrices is the product
    of the per-matrix determinants.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后一行使用了我们上面推导出的 2 × 2 矩阵的公式以便进行比较。实际上，NumPy 并不使用递归的代数余子式展开法来计算行列式。相反，它将矩阵分解为三个矩阵的乘积：（1）一个*排列矩阵*，看起来像一个被打乱的单位矩阵，每一行和每一列只有一个
    1，（2）一个下三角矩阵，和（3）一个上三角矩阵。排列矩阵的行列式为 +1 或 −1。三角矩阵的行列式是其对角元素的乘积，而矩阵乘积的行列式是各个矩阵行列式的乘积。
- en: We can use determinants to determine whether a matrix has an inverse. Let’s
    turn there now.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用行列式来判断一个矩阵是否有逆矩阵。现在让我们来了解这个内容。
- en: Inverses
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逆矩阵
- en: '[Equation 6.2](ch06.xhtml#ch06equ02) defines the identity matrix. We said that
    this matrix acts like the number 1, so when it multiplies a square matrix, the
    same square matrix is returned. When multiplying scalars, we know that for any
    number, *x* ≠ 0, there exists another number, call it *y*, such that *xy* = 1\.
    This number is the multiplicative inverse of *x*. Furthermore, we know exactly
    what *y* is; it’s 1/*x* = *x*^(−1).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 6.2](ch06.xhtml#ch06equ02)定义了单位矩阵。我们说这个矩阵像数字 1 一样工作，因此当它与一个方阵相乘时，返回的还是同一个方阵。在乘法中，我们知道对于任何数字
    *x* ≠ 0，存在另一个数字，称为 *y*，使得 *xy* = 1。这个数字就是 *x* 的乘法逆元。此外，我们知道 *y* 就是 1/*x* = *x*^(−1)。'
- en: By analogy, then, we might wonder if, since we have an identity matrix that
    acts like the number 1, there is another square matrix, call it ***A***^(−1),
    for a given square matrix, ***A***, such that
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 类比而言，我们可能会想，既然我们有一个像数字 1 一样工作的单位矩阵，那么是否存在另一个方阵，记为 **A**^(−1)，对于给定的方阵 **A**，使得
- en: '***AA***^(−1) = ***A***^(−1) ***A*** = ***I***'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**AA**^(−1) = **A**^(−1) **A** = **I**'
- en: 'If ***A***^(−1) exists, it’s known as the *inverse matrix* of ***A***, and
    ***A*** is said to be *invertable*. For real numbers, all numbers except zero
    have an inverse. For matrices, it isn’t so straightforward. Many square matrices
    don’t have inverses. To check if ***A*** has an inverse, we use the determinant:
    det(***A***) = 0 tells us that ***A*** has no inverse. Furthermore, if ***A***^(−1)
    exists, then'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 **A**^(−1) 存在，它被称为 **A** 的*逆矩阵*，而 **A** 被称为*可逆的*。对于实数，所有非零数字都有逆元。对于矩阵情况则没那么简单，许多方阵没有逆矩阵。为了检查
    **A** 是否有逆矩阵，我们使用行列式：det(**A**) = 0 表示 **A** 没有逆矩阵。此外，如果 **A**^(−1) 存在，则
- en: '![Image](Images/138equ01.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/138equ01.jpg)'
- en: Note also that (***A***^(−1))^(−1) = ***A***, as is the case for real numbers.
    Another useful property of inverses is
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，(**A**^(−1))^(−1) = **A**，这与实数的情况相同。逆矩阵的另一个有用性质是
- en: (***AB***)^(−1) = ***B***^(−1) ***A***^(−1)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: (**AB**)^(−1) = **B**^(−1) **A**^(−1)
- en: 'where the order of the product on the right-hand side is important. Finally,
    note that the inverse of a diagonal matrix is simply the reciprocal of the diagonal
    elements:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中右边乘积的顺序是很重要的。最后，注意对角矩阵的逆矩阵只是对角元素的倒数：
- en: '![Image](Images/138equ02.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/138equ02.jpg)'
- en: It’s possible to calculate the inverse by hand using row operations, which we’ve
    conveniently ignored here because they are seldom used in deep learning. Cofactor
    expansion techniques can also calculate the inverse, but to save time, we won’t
    elaborate on the process here. What’s important for us is to know that square
    matrices often have an inverse, and that we can calculate inverses with NumPy
    via np.linalg.inv.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过手动使用行变换来计算逆矩阵，尽管我们在这里方便地忽略了这一点，因为在深度学习中很少使用行变换。代数余子式展开技术也可以用来计算逆矩阵，但为了节省时间，我们在此不展开讨论。对我们而言，重要的是知道方阵通常有逆矩阵，并且我们可以通过
    NumPy 使用 np.linalg.inv 来计算逆矩阵。
- en: If a matrix is *not* invertible, the matrix is said to be *singular*. Therefore,
    the determinant of a singular matrix is zero. If a matrix has an inverse, it is
    a *nonsingular* or *nondegenerate* matrix.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个矩阵 *不可* 逆，那么该矩阵被称为 *奇异矩阵*。因此，奇异矩阵的行列式为零。如果一个矩阵有逆，它就是 *非奇异矩阵* 或 *非退化矩阵*。
- en: In NumPy, we use np.linalg.inv to calculate the inverse of a square matrix.
    For example,
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，我们使用 np.linalg.inv 来计算方阵的逆。例如，
- en: '>>> a = np.array([[1,2,1],[2,1,2],[1,2,2]])'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.array([[1,2,1],[2,1,2],[1,2,2]])'
- en: '>>> print(a)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a)'
- en: '[[1 2 1]'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2 1]'
- en: '[2 1 2]'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[2 1 2]'
- en: '[1 2 2]]'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 2 2]]'
- en: '>>> b = np.linalg.inv(a)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> b = np.linalg.inv(a)'
- en: '>>> print(b)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(b)'
- en: '[[ 0.66666667 0.66666667 -1\. ]'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 0.66666667 0.66666667 -1\. ]'
- en: '[ 0.66666667 -0.33333333 0\. ]'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.66666667 -0.33333333 0\. ]'
- en: '[-1.          0.         1\. ]]'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.          0.          1\. ]]'
- en: '>>> print(a @ b)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(a @ b)'
- en: '[[1\. 0\. 0.]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1\. 0\. 0.]'
- en: '[0\. 1\. 0.]'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 1\. 0.]'
- en: '[0\. 0\. 1.]]'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 1.]]'
- en: '>>> print(b @ a)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(b @ a)'
- en: '[[1\. 0\. 0.]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1\. 0\. 0.]'
- en: '[0\. 1\. 0.]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 1\. 0.]'
- en: '[0\. 0\. 1.]]'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[0\. 0\. 1.]]'
- en: Notice the inverse (b) working as we expect and giving the identity matrix when
    multiplying a from the left or right.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意逆矩阵（b）按预期工作，并且在从左或右相乘时给出了单位矩阵。
- en: Symmetric, Orthogonal, and Unitary Matrices
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对称矩阵、正交矩阵和酉矩阵
- en: If for a square matrix, ***A***, we have
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于方阵 ***A***，我们有
- en: '***A***^⊤ = ***A***'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***^⊤ = ***A***'
- en: then ***A*** is said to be a *symmetric matrix*. For example,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 ***A*** 被称为 *对称矩阵*。例如，
- en: '![Image](Images/139equ01.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/139equ01.jpg)'
- en: is a symmetric matrix, since ***A***^⊤ = ***A***.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 是对称矩阵，因为 ***A***^⊤ = ***A***。
- en: 'Notice that diagonal matrices are symmetric, and the product of two symmetric
    matrices is commutative: ***AB*** = ***BA***. The inverse of a symmetric matrix,
    if it exists, is also a symmetric matrix.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意对角矩阵是对称的，且两个对称矩阵的乘积是可交换的： ***AB*** = ***BA***。对称矩阵的逆矩阵（如果存在）也是对称矩阵。
- en: If the following is true,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立，
- en: '***AA***^⊤ = ***A***^⊤ ***A*** = ***I***'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '***AA***^⊤ = ***A***^⊤ ***A*** = ***I***'
- en: then ***A*** is an orthogonal matrix. If ***A*** is an orthogonal matrix, then
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 ***A*** 是一个正交矩阵。如果 ***A*** 是一个正交矩阵，那么
- en: '***A***^(−1) = ***A***^⊤'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***^(−1) = ***A***^⊤'
- en: and, as a result,
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，结果是，
- en: det(***A***) = ±1
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A***) = ±1
- en: If the values in the matrix are allowed to be complex, which does not happen
    often in deep learning, and
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果矩阵中的值可以是复数，这在深度学习中不常见，并且
- en: '***U***^****U*** = ***UU***^* = ***I***'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '***U***^****U*** = ***UU***^* = ***I***'
- en: then ***U*** is a *unitary matrix* with ***U***^* being the *conjugate transpose*
    of ***U***. The conjugate transpose is the ordinary matrix transpose followed
    by the complex conjugate operation to change ![Image](Images/140equ01.jpg) to
    −*i*. So, we might have
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 ***U*** 是一个 *酉矩阵*，其中 ***U***^* 是 ***U*** 的 *共轭转置*。共轭转置是普通矩阵转置后再进行复共轭操作，将
    ![Image](Images/140equ01.jpg) 改为 −*i*。因此，我们可能有
- en: '![Image](Images/140equ02.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ02.jpg)'
- en: Sometimes, especially in physics, the conjugate transpose is called the *Hermitian
    adjoint* and is denoted as ***A***^†. If a matrix is equal to its conjugate transpose,
    it is called a *Hermitian matrix*. Notice that real symmetric matrices are also
    Hermitian matrices because the conjugate transpose is the same as the ordinary
    transpose when the values are real numbers. Therefore, you might encounter the
    term *Hermitian* in place of *symmetric* when referring to matrices with real
    elements.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，特别是在物理学中，共轭转置被称为 *埃尔米特伴随*，记作 ***A***^†。如果一个矩阵等于它的共轭转置，则称该矩阵为 *埃尔米特矩阵*。注意，实对称矩阵也是埃尔米特矩阵，因为当数值是实数时，共轭转置与普通转置相同。因此，当涉及到具有实元素的矩阵时，你可能会遇到
    *埃尔米特* 这个术语，代替 *对称*。
- en: Definiteness of a Symmetric Matrix
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对称矩阵的正定性
- en: We saw at the beginning of this section that an *n* × *n* square matrix maps
    a vector in ℝ^(*n*) to another vector in ℝ^(*n*). Let’s consider now a symmetric
    *n* × *n* matrix, ***B***, with real-valued elements. We can characterize this
    matrix by how it maps vectors using the inner product between the mapped vector
    and the original vector. Specifically, if ***x*** is a column vector (*n* × 1),
    then ***Bx*** is also an *n* × 1 column vector. Therefore, the inner product of
    this vector and the original vector, ***x***, is ***x***^⊤***Bx***, a scalar.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节开始时看到，*n* × *n* 方阵将 ℝ^(*n*) 中的一个向量映射到 ℝ^(*n*) 中的另一个向量。现在我们考虑一个对称的 *n* ×
    *n* 矩阵，***B***，其元素为实数。我们可以通过它如何使用原向量与映射向量之间的内积来表征这个矩阵。具体而言，如果 ***x*** 是一个列向量（*n*
    × 1），那么 ***Bx*** 也是一个 *n* × 1 的列向量。因此，这个向量与原始向量 ***x*** 的内积是 ***x***^⊤***Bx***，一个标量。
- en: 'If the following is true:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立：
- en: '![Image](Images/140equ03.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ03.jpg)'
- en: then ***B*** is said to be *positive definite*. Here, the bolded **0** is the
    *n* × 1 column vector of all zeros, and ∀ is math notation meaning “for all.”
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后***B***被称为*正定*。这里，加粗的**0**是一个* n * × 1的全零列向量，∀是数学符号，表示“对所有”。
- en: Similarly, if
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果
- en: '![Image](Images/140equ04.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ04.jpg)'
- en: then ***B*** is *negative definite*. Relaxing the inner product relationship
    and the nonzero requirement on ***x*** gives two additional cases. If
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后***B***是*负定*的。放宽内积关系和***x***的非零要求，得到两个额外的情况。如果
- en: '![Image](Images/140equ05.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ05.jpg)'
- en: then ***B*** is said to be *positive semidefinite*, and
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然后***B***被称为*正半定*，并且
- en: '![Image](Images/140equ06.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ06.jpg)'
- en: makes ***B*** a *negative semidefinite* matrix. Finally, a real square symmetric
    matrix that is neither positive nor negative semidefinite is called an *indefinite
    matrix*. The definiteness of a matrix tells us something about the eigenvalues,
    which we’ll learn more about in the next section. If a symmetric matrix is positive
    definite, then all of its eigenvalues are positive. Similarly, a symmetric negative
    definite matrix has all negative eigenvalues. Positive and negative semidefinite
    symmetric matrices have eigenvalues that are all positive or zero or all negative
    or zero, respectively.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使***B***成为一个*负半定*矩阵。最后，一个既不是正半定也不是负半定的实数对称方阵被称为*不定矩阵*。矩阵的定性告诉我们一些关于特征值的信息，关于这一点我们将在下一节中进一步了解。如果一个对称矩阵是正定的，那么它的所有特征值都是正的。同样，负定的对称矩阵具有所有负的特征值。正半定和负半定的对称矩阵分别具有所有正或零的特征值，或所有负或零的特征值。
- en: Let’s shift gears now from talking about types of matrices to discovering the
    importance of eigenvectors and eigenvalues, key properties of a matrix that we
    use frequently in deep learning.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从讨论矩阵类型转向发现特征向量和特征值的重要性，这些是我们在深度学习中经常使用的矩阵关键属性。
- en: Eigenvectors and Eigenvalues
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征向量和特征值
- en: We learned above that a square matrix maps a vector into another vector in the
    same dimensional space, ***v***′ = ***Av***, where both ***v***′ and ***v*** are
    *n*-dimensional vectors, if ***A*** is an *n* × *n* matrix.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面学到，方阵将一个向量映射到另一个同维度空间中的向量，***v***′ = ***Av***，其中***v***′和***v***都是*n*维向量，如果***A***是一个*n*
    × *n*矩阵。
- en: Consider this equation,
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个方程，
- en: '![Image](Images/06equ04.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ04.jpg)'
- en: for some square matrix, ***A***, where λ is a scalar value and ***v*** is a
    nonzero column vector.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个方阵***A***，其中λ是一个标量值，***v***是一个非零列向量。
- en: '[Equation 6.4](ch06.xhtml#ch06equ04) says that the vector, ***v***, is mapped
    by ***A*** back into a scalar multiple of itself. We call ***v*** an *eigenvector*
    of ***A*** with *eigenvalue* λ. The prefix *eigen* comes from German and is often
    translated as “self,” “characteristic,” or even “proper.” Thinking geometrically,
    [Equation 6.4](ch06.xhtml#ch06equ04) says that the action of ***A*** on its eigenvectors
    in ℝ^(*n*) is to shrink or expand the vector without changing its direction. Note,
    while ***v*** is nonzero, it’s possible for λ to be zero.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 6.4](ch06.xhtml#ch06equ04)表明，向量***v***被***A***映射回它自身的标量倍数。我们称***v***为***A***的*特征向量*，其*特征值*为λ。前缀*eigen*来源于德语，常被翻译为“自我”、“特征”或“固有”。从几何角度看，[公式
    6.4](ch06.xhtml#ch06equ04)说明，***A***对其在ℝ^(*n*)中的特征向量的作用是缩小或放大向量，而不改变其方向。请注意，虽然***v***是非零的，但λ可能为零。'
- en: How does [Equation 6.4](ch06.xhtml#ch06equ04) relate to the identity matrix,
    ***I*** ? By definition, the identity matrix maps a vector back into itself without
    scaling it. Therefore, the identity matrix has an infinite number of eigenvectors,
    and all of them have an eigenvalue of 1, since, for any ***x***, ***Ix*** = ***x***.
    Therefore, the same eigenvalue may apply to more than one eigenvector.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 6.4](ch06.xhtml#ch06equ04)如何与单位矩阵***I***相关？根据定义，单位矩阵将一个向量映射回其自身，而不进行缩放。因此，单位矩阵有无限多个特征向量，而且所有这些特征向量的特征值都是1，因为对于任何***x***，***Ix***
    = ***x***。因此，同一个特征值可能适用于多个特征向量。'
- en: Recall that [Equation 6.1](ch06.xhtml#ch06equ01) defines a rotation matrix in
    2D space for some given angle, *θ*. This matrix has no eigenvectors, because,
    for any nonzero vector, it rotates the vector by *θ*, so it can never map a vector
    back into its original direction. Therefore, not every matrix has eigenvectors.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，[公式 6.1](ch06.xhtml#ch06equ01)定义了一个二维空间中某个给定角度*θ*的旋转矩阵。这个矩阵没有特征向量，因为对于任何非零向量，它会将向量旋转*θ*，因此它永远不能将一个向量映射回其原始方向。因此，并不是每个矩阵都有特征向量。
- en: Finding Eigenvalues and Eigenvectors
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 寻找特征值和特征向量
- en: 'To find the eigenvalues of a matrix, if there are any, we go back to [Equation
    6.4](ch06.xhtml#ch06equ04) and rewrite it:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出矩阵的特征值（如果有的话），我们回到 [方程 6.4](ch06.xhtml#ch06equ04)，并重写它：
- en: '![Image](Images/06equ05.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ05.jpg)'
- en: We can insert the identity matrix, ***I***, between λ and ***v*** because ***Iv***
    = ***v***. Therefore, to find the eigenvalues of ***A***, we need to find values
    of λ that cause the matrix ***A*** − λ***I*** to map a nonzero vector, ***v***,
    to the zero vector. [Equation 6.5](ch06.xhtml#ch06equ05) only has solutions other
    than the zero vector if the determinant of ***A*** − λ***I*** is zero.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 λ 和 ***v*** 之间插入单位矩阵 ***I***，因为 ***Iv*** = ***v***。因此，要找出矩阵 ***A*** 的特征值，我们需要找到使得矩阵
    ***A*** − λ***I*** 映射非零向量 ***v*** 到零向量的 λ 值。[方程 6.5](ch06.xhtml#ch06equ05) 只有在
    ***A*** − λ***I*** 的行列式为零时，才有除零向量之外的解。
- en: 'The above gives us a way to find the eigenvalues. For example, consider what
    ***A*** − λ***I*** looks like for a 2 × 2 matrix:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法为我们提供了找到特征值的途径。例如，考虑一个 2 × 2 矩阵，看看 ***A*** − λ***I*** 是什么样的：
- en: '![Image](Images/142equ01.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/142equ01.jpg)'
- en: We learned above that the determinant of a 2 × 2 matrix has a simple form; therefore,
    the determinant of the matrix above is
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，2 × 2 矩阵的行列式有一个简单的形式；因此，上面矩阵的行列式是
- en: det(***A*** − λ***I***) = (*a* − λ)(*d* − λ) − *bc*
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A*** − λ***I***) = (*a* − λ)(*d* − λ) − *bc*
- en: This equation is a second-degree polynomial in λ. Since we need the determinant
    to be zero, we set this polynomial to zero and find the roots. The roots are the
    eigenvalues of ***A***. The polynomial that this process finds is called the *characteristic
    polynomial*, and [Equation 6.5](ch06.xhtml#ch06equ05) is the *characteristic equation*.
    Notice above that the characteristic polynomial is a second-degree polynomial.
    In general, the characteristic polynomial of an *n* × *n* matrix is of degree
    *n*, so a matrix has at most *n* distinct eigenvalues, since an *n*th degree polynomial
    has at most *n* roots.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是 λ 的二次多项式。因为我们需要行列式为零，所以我们将这个多项式设为零，并找到其根。根就是矩阵 ***A*** 的特征值。这个过程得到的多项式叫做
    *特征多项式*，[方程 6.5](ch06.xhtml#ch06equ05) 是 *特征方程*。注意到上面特征多项式是一个二次多项式。一般来说，*n* ×
    *n* 矩阵的特征多项式是 *n* 次多项式，因此一个矩阵最多有 *n* 个不同的特征值，因为 *n* 次多项式最多有 *n* 个根。
- en: Once we know the roots of the characteristic polynomial, we can go back to [Equation
    6.5](ch06.xhtml#ch06equ05), substitute each root for λ, and solve to find the
    associated eigenvectors, the ***v***’s of [Equation 6.5](ch06.xhtml#ch06equ05).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了特征多项式的根，就可以回到 [方程 6.5](ch06.xhtml#ch06equ05)，将每个根代入 λ，并解出相应的特征向量，也就是
    [方程 6.5](ch06.xhtml#ch06equ05) 中的 ***v***。
- en: The eigenvalues of a triangular matrix, which includes diagonal matrices, are
    straightforward to calculate because the determinant of such a matrix is simply
    the product of the main diagonal. For example, for a 4 × 4 triangular matrix,
    the determinant of the characteristic equation is
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三角矩阵（包括对角矩阵），特征值的计算是直接的，因为这种矩阵的行列式只是主对角线元素的乘积。例如，对于一个 4 × 4 的三角矩阵，特征方程的行列式是
- en: det(***A*** − λ***I***) = (*a*[00] − λ)(*a*[11] − λ)(*a*[22] − λ)(*a*[33] −
    λ)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A*** − λ***I***) = (*a*[00] − λ)(*a*[11] − λ)(*a*[22] − λ)(*a*[33] −
    λ)
- en: 'which has four roots: the values of the diagonal. For triangular and diagonal
    matrices, the entries on the main diagonal *are* the eigenvalues.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 它有四个根：对角线上的值。对于三角矩阵和对角矩阵，主对角线上的元素 *就是* 特征值。
- en: 'Let’s see a worked eigenvalue example for the following matrix:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个已完成的特征值例子，使用以下矩阵：
- en: '![Image](Images/142equ02.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/142equ02.jpg)'
- en: I selected this matrix to make the math nicer, but the process works for any
    matrix. The characteristic equation means we need the λ values that make the determinant
    zero, as shown next.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了这个矩阵以使数学计算更加简洁，但这个过程适用于任何矩阵。特征方程意味着我们需要找到使行列式为零的 λ 值，具体见下文。
- en: '![Image](Images/143equ01.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/143equ01.jpg)'
- en: The characteristic polynomial is easily factored to give λ = −1, −2.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 特征多项式可以轻松分解为 λ = −1, −2。
- en: 'In code, to find the eigenvalues and eigenvectors of a matrix, we use np.linalg.eig.
    Let’s check our calculation above to see if NumPy agrees:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，要找出一个矩阵的特征值和特征向量，我们使用 np.linalg.eig。让我们检查一下上面的计算，看看 NumPy 是否同意：
- en: '>>> a = np.array([[0,1],[-2,-3]])'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.array([[0,1],[-2,-3]])'
- en: '>>> print(np.linalg.eig(a)[0])'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.linalg.eig(a)[0])'
- en: '[-1\. -2.]'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1\. -2.]'
- en: The np.linalg.eig function returns a list. The first element is a vector of
    the eigenvalues of the matrix. The second element, which we are ignoring for the
    moment, is a matrix, the *columns* of which are the eigenvectors associated with
    each of the eigenvalues. Note that we also could have used np.linalg.eigvals to
    return just the eigenvalues. Regardless, we see that our calculation of the eigenvalues
    of ***A*** is correct.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: np.linalg.eig 函数返回一个列表。第一个元素是矩阵的特征值向量。第二个元素，我们暂时忽略，是一个矩阵，其*列*是与每个特征值关联的特征向量。请注意，我们也可以使用
    np.linalg.eigvals 来只返回特征值。不管怎样，我们看到我们对***A***的特征值计算是正确的。
- en: To find the associated eigenvectors, we put each of the eigenvalues back into
    [Equation 6.5](ch06.xhtml#ch06equ05) and solve for ***v***. For example, for λ
    = −1, we get
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到关联的特征向量，我们将每个特征值代入[公式 6.5](ch06.xhtml#ch06equ05)，并求解***v***。例如，对于 λ = −1，我们得到：
- en: '![Image](Images/143equ02.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/143equ02.jpg)'
- en: 'which leads to the system of equations:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下方程系统：
- en: '*v*[0] + *v*[1] = 0'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*v*[0] + *v*[1] = 0'
- en: −2*v*[0] − 2*v*[1] = 0
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: −2*v*[0] − 2*v*[1] = 0
- en: This system has many solutions, as long as *v*[0] = −*v*[1]. That means we can
    pick *v*[0] and *v*[1], as long as the relationship between them is preserved.
    Therefore, we have our eigenvector for
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统有很多解，只要*v*[0] = −*v*[1]。这意味着我们可以选择*v*[0]和*v*[1]，只要它们之间的关系得以保持。因此，我们得到了我们的特征向量。
- en: '![Image](Images/143equ03.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/143equ03.jpg)'
- en: If we repeat this process for λ = −2, we get the relationship between the components
    of ***v*****[2]** to be 2*v*[0] = −*v*[1]. Therefore, we select ![Image](Images/143equ04.jpg)
    as the second eigenvector.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对 λ = −2 重复此过程，我们得到***v***[2]的分量关系为 2*v*[0] = −*v*[1]。因此，我们选择 ![Image](Images/143equ04.jpg)
    作为第二个特征向量。
- en: 'Let’s see if NumPy agrees with us. This time, we’ll display the second list
    element returned by np.linalg.eig. This is a matrix where the columns of the matrix
    are the eigenvectors:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 NumPy 是否同意我们的计算。这次，我们将显示 np.linalg.eig 返回的第二个列表元素。这是一个矩阵，其中矩阵的列是与每个特征值关联的特征向量：
- en: '>>> print(np.linalg.eig(a)[1])'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(np.linalg.eig(a)[1])'
- en: '[[ 0.70710678 -0.4472136 ]'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 0.70710678 -0.4472136 ]'
- en: '[-0.70710678  0.89442719]]'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[-0.70710678  0.89442719]]'
- en: 'Hmm . . . the columns of this matrix do not appear to match our selected eigenvectors.
    But don’t worry—we didn’t make a mistake. Recall that the eigenvectors were not
    uniquely determined, only the relationship between the components was determined.
    If we’d wanted to, we could have selected other values, as long as for one eigenvector
    they were of equal magnitude and opposite sign, and for the other they were in
    the ratio of 2:1 with opposite signs. What NumPy returns is a set of eigenvectors
    that are of unit length. So, to see that our hand calculation is correct, we need
    to make our eigenvectors unit vectors by dividing each component by the square
    root of the sum of the squares of the components. In code, it’s succinct, if a
    bit messy:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯……这个矩阵的列似乎与我们选择的特征向量不匹配。但别担心——我们没有犯错。请记住，特征向量并不是唯一确定的，只有分量之间的关系是确定的。如果我们愿意的话，我们可以选择其他值，只要对于一个特征向量，它们的大小相等且符号相反，而对于另一个，它们的比例为
    2:1 且符号相反。NumPy 返回的是一组单位长度的特征向量。因此，要验证我们手工计算的结果是否正确，我们需要通过将每个分量除以分量平方和的平方根，将我们的特征向量转换为单位向量。代码如下，简洁但稍显凌乱：
- en: '>>> np.array([1,-1])/np.sqrt((np.array([1,-1])**2).sum())'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.array([1,-1])/np.sqrt((np.array([1,-1])**2).sum())'
- en: array([ 0.70710678, -0.70710678])
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: array([ 0.70710678, -0.70710678])
- en: '>>> np.array([-1,2])/np.sqrt((np.array([-1,2])**2).sum())'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.array([-1,2])/np.sqrt((np.array([-1,2])**2).sum())'
- en: array([-0.4472136, 0.89442719])
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: array([-0.4472136, 0.89442719])
- en: Now we see that we’re correct. The unit vector versions of the eigenvectors
    do match the columns of the matrix NumPy returned.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到我们是正确的。特征向量的单位向量版本确实与 NumPy 返回的矩阵列匹配。
- en: We’ll use the eigenvectors and eigenvalues of a matrix often when we’re doing
    deep learning. For example, we’ll see them again later in the chapter when we
    investigate principal component analysis. But before we can learn about PCA, we
    need to change focus once again and learn about vector norms and distance metrics
    commonly used in deep learning, especially about the covariance matrix.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行深度学习时，我们会经常使用矩阵的特征向量和特征值。例如，当我们研究主成分分析时，我们会在本章后面再次看到它们。但在我们学习 PCA 之前，我们需要再次转移焦点，学习深度学习中常用的向量范数和距离度量，尤其是关于协方差矩阵的内容。
- en: Vector Norms and Distance Metrics
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量范数和距离度量
- en: In common deep learning parlance, people are somewhat sloppy and use the terms
    *norm* and *distance* interchangeably. We can forgive them for doing so; the difference
    between the terms is small in practice, as we’ll see below.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的常用术语中，人们有些随意地将 *范数* 和 *距离* 这两个术语互换使用。我们可以宽容地理解他们这样做；正如下面所看到的，在实践中，这两个术语的差别非常小。
- en: A *vector norm* is a function that maps a vector, real or complex, to some value,
    *x* ∈ ℝ, *x* ≥ 0\. A norm must satisfy some specific properties in a mathematical
    sense, but in practice, not everything that’s called a norm is, in fact, a norm.
    In deep learning, we usually use norms as distances between pairs of vectors.
    In practice, an important property for a distance measure is that the order of
    the inputs doesn’t matter. If *f*(*x*, *y*) is a distance, then *f*(*x*, *y*)
    = *f*(*y*, *x*). Again, this is not rigorously followed; for example, you’ll often
    see the Kullback-Leibler divergence (KL-divergence) used as a distance even though
    this property doesn’t hold.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量范数*是一个将向量（无论是实数还是复数）映射到某个值的函数，*x* ∈ ℝ，*x* ≥ 0。范数必须满足一些特定的数学性质，但在实践中，并非所有被称为范数的东西，实际上都是范数。在深度学习中，我们通常将范数作为向量对之间的距离。实际上，距离度量的一个重要性质是输入的顺序不重要。如果
    *f*(*x*, *y*) 是一个距离，那么 *f*(*x*, *y*) = *f*(*y*, *x*)。不过，这个性质并不总是严格遵循的；例如，你经常会看到
    Kullback-Leibler 散度（KL 散度）被用作距离，尽管它并不满足这个性质。'
- en: 'Let’s start with vector norms and see how we can easily use them as a distance
    measure between vectors. Then we’ll introduce the important concept of a covariance
    matrix, heavily used on its own in deep learning, and see how we can create a
    distance measure from it: the Mahalanobis distance. We’ll end the section by introducing
    the KL-divergence, which we can view as a measure between two discrete probability
    distributions.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向量范数开始，看看如何将它们轻松地用作向量之间的距离度量。接下来，我们将介绍深度学习中常用的一个重要概念——协方差矩阵，并看看如何从中创建一个距离度量：马哈拉诺比斯距离。最后，我们将通过介绍
    KL 散度结束这一部分，KL 散度可以视为两种离散概率分布之间的度量。
- en: L-Norms and Distance Metrics
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: L-范数与距离度量
- en: For an *n*-dimensional vector, ***x***, we define the *p*-norm of the vector
    to be
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 *n* 维向量 ***x***，我们定义该向量的 *p*-范数为
- en: '![Image](Images/06equ06.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ06.jpg)'
- en: where *p* is a real number. Although we use *p* in the definition, people generally
    refer to these as L*[p]* norms. We saw one of these norms in [Chapter 5](ch05.xhtml#ch05)
    when we defined the magnitude of a vector. In that case, we were calculating the
    *L2-norm*,
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 是一个实数。尽管我们在定义中使用了 *p*，人们通常将这些称为 L*[p]* 范数。我们在 [第五章](ch05.xhtml#ch05)
    中已经看到了其中一种范数，当时我们定义了一个向量的大小。在那个例子中，我们计算的是 *L2 范数*，
- en: '![Image](Images/145equ01.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/145equ01.jpg)'
- en: which is the square root of the inner product of ***x*** with itself.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 ***x*** 与自身的内积的平方根。
- en: The norms we use most often in deep learning are the L2-norm and the *L1-norm*,
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在深度学习中最常使用的范数是 L2 范数和 *L1 范数*，
- en: '![Image](Images/145equ02.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/145equ02.jpg)'
- en: which is nothing more than the sum of the absolute values of the components
    of ***x***. Another norm you’ll encounter is the *L**[∞]-norm*,
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是 ***x*** 的各分量的绝对值之和。你还会遇到另外一种范数，即 *L**[∞]-范数*，
- en: L[∞] = max |*x**[i]*|
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: L[∞] = max |*x**[i]*|
- en: the maximum absolute value of the components of ***x***.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '***x*** 的各分量的最大绝对值。'
- en: If we replace ***x*** with the difference of two vectors, ***x*** − ***y***,
    we can treat the norms as distance measures between the two vectors. Alternatively,
    we can picture the process as computing the vector norm on the vector that is
    the difference between ***x*** and ***y***.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 ***x*** 替换为两个向量的差 ***x*** − ***y***，我们就可以将范数视为这两个向量之间的距离度量。或者，我们可以将这个过程想象为计算
    ***x*** 和 ***y*** 之间差向量的范数。
- en: 'Switching from norm to distance makes a trivial change in [Equation 6.6](ch06.xhtml#ch06equ06):'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 从范数转到距离在 [方程 6.6](ch06.xhtml#ch06equ06) 中只需做一个微小的修改：
- en: '![Image](Images/06equ07.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ07.jpg)'
- en: The L2-distance becomes
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: L2 距离变为
- en: '![Image](Images/145equ03.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/145equ03.jpg)'
- en: 'This is the *Euclidean distance* between two vectors. The L1-distance is often
    called the *Manhattan distance* (also called *city block distance*, *boxcar distance*,
    or *taxicab distance*):'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个向量之间的 *欧几里得距离*。L1 距离通常被称为 *曼哈顿距离*（也称为 *城市街区距离*、*箱车距离* 或 *出租车距离*）：
- en: '![Image](Images/146equ01.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/146equ01.jpg)'
- en: It’s so named because it corresponds to the length a taxicab would travel on
    the grid of streets in Manhattan. The L[∞]-distance is sometimes known as the
    *Chebyshev distance*.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 它之所以如此命名，是因为它对应的是出租车在曼哈顿街道网格上行驶的距离。L[∞]距离有时被称为*切比雪夫距离*。
- en: Norm equations have other uses in deep learning. For example, weight decay,
    used in deep learning as a regularizer, uses the L2-norm of the weights of the
    model to keep the weights from getting too large. The L1-norm of the weights is
    also sometimes used as a regularizer.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 范数方程在深度学习中有其他用途。例如，深度学习中用于正则化的权重衰减，使用模型权重的L2范数来防止权重变得过大。权重的L1范数有时也用作正则化项。
- en: Let’s move now to consider the important concept of a covariance matrix. It
    isn’t a distance metric itself but is used by one, and it will show up again later
    in the chapter.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来考虑一个重要概念——协方差矩阵。它本身不是一个距离度量，但它被用于某个距离度量，并且它会在本章稍后出现。
- en: Covariance Matrices
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: 'If we have a collection of measurements on multiple variables, like a training
    set with feature vectors, we can calculate the variance of the features with respect
    to each other. For example, here’s a matrix of observations of four variables,
    one per row:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们拥有多变量的多个测量数据，例如具有特征向量的训练集，我们可以计算特征之间的方差。例如，下面是四个变量的观测矩阵，每行对应一个变量：
- en: '![Image](Images/146equ02.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/146equ02.jpg)'
- en: 'In reality, ***X*** is the first five samples from the famous iris dataset.
    For the iris dataset, the features are measurements of the parts of iris flowers
    from three different species. You can load this dataset into NumPy using sklearn:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，***X***是著名鸢尾花数据集中的前五个样本。对于鸢尾花数据集，这些特征是来自三种不同物种的鸢尾花各个部位的测量值。你可以使用sklearn将该数据集加载到NumPy中：
- en: '>>> from sklearn import datasets'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from sklearn import datasets'
- en: '>>> iris = datasets.load_iris()'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> iris = datasets.load_iris()'
- en: '>>> X = iris.data[:5]'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X = iris.data[:5]'
- en: '>>> X'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X'
- en: array([[5.1, 3.5, 1.4, 0.2],
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: array([[5.1, 3.5, 1.4, 0.2],
- en: '[4.9, 3.0, 1.4, 0.2],'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.9, 3.0, 1.4, 0.2],'
- en: '[4.7, 3.2, 1.3, 0.2],'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.7, 3.2, 1.3, 0.2],'
- en: '[4.6, 3.1, 1.5, 0.2],'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.6, 3.1, 1.5, 0.2],'
- en: '[5.0, 3.6, 1.4, 0.2]])'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[5.0, 3.6, 1.4, 0.2]])'
- en: We could calculate the standard deviation of each of the features, the columns
    of ***A***, but that would only tell us about the variance of the values of that
    feature around its mean. Since we have multiple features, it would be nice to
    know something about how the features in, say, column zero and column one vary
    together. To determine this, we need to calculate the *covariance matrix*. This
    matrix captures the variance of the individual features along the main diagonal.
    Meanwhile, the off-diagonal values represent how one feature varies as another
    varies—these are the covariances. Since there are four features, the covariance
    matrix, which is always square, is, in this case, a 4 × 4 matrix. We find the
    elements of the covariance matrix, Σ, by calculating
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算每个特征的标准差，即***A***的列，但这只会告诉我们该特征值围绕其均值的方差。由于我们有多个特征，了解不同特征之间的关系会更有帮助。例如，我们可能想知道第零列和第一列的特征如何一起变化。为了确定这一点，我们需要计算*协方差矩阵*。该矩阵在主对角线上捕捉各个特征的方差。与此同时，非对角线的值表示一个特征如何随着另一个特征的变化而变化——这些就是协方差。由于有四个特征，协方差矩阵总是方阵，在这种情况下是一个4
    × 4的矩阵。我们通过计算来找到协方差矩阵Σ的元素：
- en: '![Image](Images/06equ08.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ08.jpg)'
- en: 'assuming the rows of the matrix, ***X***, are the observations, and the columns
    of ***X*** represent the different features. The means of the features across
    all rows are ![Image](Images/147equ01.jpg) and ![Image](Images/147equ02.jpg) for
    features *i* and *j*. Here, *n* is the number of observations, the number of rows
    in ***X***. We can see that when *i* = *j*, the covariance value is the normal
    variance for that feature. When *i* ≠ *j*, the value is how *i* and *j* vary together.
    We often denote the covariance matrix as Σ, and it is always symmetric: ∑[*ij*]
    = ∑*[ji]*.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 假设矩阵***X***的行是观测值，列表示不同的特征。所有行中各个特征的均值分别是![Image](Images/147equ01.jpg)和![Image](Images/147equ02.jpg)，对应特征*i*和*j*。这里，*n*是观测值的数量，即***X***中的行数。我们可以看到，当*i*
    = *j*时，协方差值是该特征的常规方差；当*i* ≠ *j*时，值表示*i*和*j*一起变化的程度。我们通常将协方差矩阵表示为Σ，它始终是对称的：∑[*ij*]
    = ∑[*ji]*。
- en: 'Let’s calculate some elements of the covariance matrix for ***X*** above. The
    per-feature means are ![Image](Images/147equ03.jpg). Let’s find the first row
    of Σ. This will tell us the variance of the first feature (column of **X**) and
    how that feature varies with the second, third, and fourth features. Therefore,
    we need to calculate ∑[00], ∑[01], ∑[02], and ∑[03]:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/147equ04.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: 'We can repeat this calculation for all the rows of Σ to give the complete covariance
    matrix:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/147equ05.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: The elements along the diagonal represent the variance of the features of ***X***.
    Notice that for the fourth feature of ***X***, all the variances and covariances
    are zero. This makes sense because all the values for this feature in ***X***
    are the same; there is no variance.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the covariance matrix for a set of observations in code by
    using np.cov:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.cov(X, rowvar=False))'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 0.043   0.0365 -0.0025  0.    ]'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0.0365  0.067  -0.0025  0.    ]'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[-0.0025 -0.0025  0.005   0.    ]'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0.      0.      0.      0.    ]]'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the call to np.cov includes rowvar=False. By default, np.cov expects
    each row of its argument to be a variable and the columns to be the observations
    of that variable. This is the opposite of the usual way a set of observations
    is typically stored in a matrix for deep learning. Therefore, we use the rowvar
    keyword to tell NumPy that the rows, not the columns, are the observations.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: I claimed above that the diagonal of the covariance matrix returns the variances
    of the features in ***X***. NumPy has a function, np.std, to calculate the standard
    deviation, and squaring the output of this function should give us the variances
    of the features by themselves. For ***X***, we get
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.std(X, axis=0)**2)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[0.0344 0.0536 0.004 0.     ]'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: These variances don’t look like the diagonal of the covariance matrix. The difference
    is due to the *n* − 1 in the denominator of the covariance equation, [Equation
    6.8](ch06.xhtml#ch06equ08). By default, np.std calculates what is known as a biased
    estimate of the sample variance. This means that instead of dividing by *n* −
    1, it divides by *n*. To get np.std to calculate the unbiased estimator of the
    variance, we need to add the ddof=1 keyword,
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.std(X, axis=0, ddof=1)**2)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[0.043 0.067 0.005  0.   ]'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: then we’ll get the same values as along the diagonal of Σ.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to calculate the covariance matrix, let’s use it in a distance
    metric.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Mahalanobis Distance
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Above, we represented a dataset by a matrix where the rows of the dataset are
    observations and the columns are the values of variables that make up each observation.
    In machine learning terms, the rows are the feature vectors. As we saw above,
    we can calculate the mean of each feature across all the observations, and we
    can calculate the covariance matrix. With these values, we can define a distance
    metric called the *Mahalanobis distance*,
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ09.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: where ***x*** is a vector, **μ** is the vector formed by the mean values of
    each feature, and Σ is the covariance matrix. Notice that this metric uses the
    *inverse* of the covariance matrix, not the covariance matrix itself.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 6.9](ch06.xhtml#ch06equ09) is, in some sense, measuring the distance
    between a vector and a distribution with the mean vector **μ**. The dispersion
    of the distribution is captured in Σ. If there is no covariance between the features
    in the dataset and each feature has the same standard deviation, then Σ becomes
    the identity matrix, which is its own inverse. In that case, Σ^(−1) effectively
    drops out of [Equation 6.9](ch06.xhtml#ch06equ09), and the Mahalanobis distance
    becomes the L2-distance (Euclidean distance).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of the Mahalanobis distance is to replace **μ** with another
    vector, call it ***y***, that comes from the same dataset as ***x***. Then *D*[M]
    is the distance between the two vectors, taking the variance of the dataset into
    account.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: We can use the Mahalanobis distance to build a simple classifier. If, given
    a dataset, we calculate the mean feature vector of each class in the dataset (this
    vector is also called the *centroid*), we can use the Mahalanobis distance to
    assign a label to an unknown feature vector, ***x***. We can do so by calculating
    all the Mahalanobis distances to the class centroids and assigning ***x*** to
    the class returning the smallest value. This type of classifier is sometimes called
    a *nearest centroid* classifier, and you’ll often see it implemented using the
    L2-distance in place of the Mahalanobis distance. Arguably, you can expect the
    Mahalanobis distance to be the better metric because it takes the variance of
    the dataset into account.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the breast cancer dataset included with sklearn to build the nearest
    centroid classifier using the Mahalanobis distance. The breast cancer dataset
    has two classes: benign (0) and malignant (1). The dataset contains 569 observations,
    each of which has 30 features derived from histology slides. We’ll build two versions
    of the nearest centroid classifier: one using the Mahalanobis distance and the
    other using the Euclidean distance. Our expectation is that the classifier using
    the Mahalanobis distance will perform better.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we need is straightforward:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn import datasets
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: ❶ from scipy.spatial.distance import mahalanobis
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: bc = datasets.load_breast_cancer()
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: d = bc.data; l = bc.target
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: ❷ i = np.argsort(np.random.random(len(d)))
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: d = d[i]; l = l[i]
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: xtrn, ytrn = d[:400], l[:400]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: xtst, ytst = d[400:], l[400:]
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: ❸ i = np.where(ytrn == 0)
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: m0 = xtrn[i].mean(axis=0)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: i = np.where(ytrn == 1)
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: m1 = xtrn[i].mean(axis=0)
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: S = np.cov(xtrn, rowvar=False)
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: SI= np.linalg.inv(S)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'def score(xtst, ytst, m, SI):'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: nc = 0
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(ytst)):'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: d = np.array([mahalanobis(xtst[i],m[0],SI),
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: mahalanobis(xtst[i],m[1],SI)])
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: c = np.argmin(d)
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'if (c == ytst[i]):'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: nc += 1
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: return nc / len(ytst)
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: mscore = score(xtst, ytst, [m0,m1], SI)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: mscore = score(xtst, ytst, [m0,m1], SI)
- en: ❹ escore = score(xtst, ytst, [m0,m1], np.identity(30))
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ escore = score(xtst, ytst, [m0,m1], np.identity(30))
- en: print("Mahalanobis score = %0.4f" % mscore)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: print("Mahalanobis 得分 = %0.4f" % mscore)
- en: print("Euclidean score = %0.4f" % escore)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: print("欧几里得得分 = %0.4f" % escore)
- en: We start by importing the modules we need, including mahalanobis from SciPy
    ❶. This function accepts two vectors and the inverse of a covariance matrix and
    returns *D*[M]. We get the dataset next in d with labels in l. We randomize the
    order ❷ and pull out the first 400 observations as training data (xtrn) with labels
    (ytrn). We hold back the remaining observations for testing (xtst, ytst).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的模块，包括来自 SciPy 的 mahalanobis ❶。这个函数接受两个向量和协方差矩阵的逆，并返回 *D*[M]。接着，我们获取包含标签的
    d 数据集和 l 标签。我们随机化顺序 ❷ 并提取前 400 个观测值作为训练数据 (xtrn) 和标签 (ytrn)。其余观测值保留用于测试 (xtst,
    ytst)。
- en: We *train* the model next. Training consists of pulling out all the observations
    belonging to each class ❸ and calculating m0 and m1. These are the mean values
    of each of the 30 features for all class 0 and class 1 observations. We then calculate
    the covariance matrix of the entire training set (S) and its inverse (SI).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们进行 *训练* 模型。训练过程包括提取每个类别的所有观测值 ❸，并计算 m0 和 m1。这些是所有类别 0 和类别 1 观测值的 30 个特征的均值。然后，我们计算整个训练集的协方差矩阵
    (S) 及其逆 (SI)。
- en: The score function takes the test observations, a list of the class mean vectors,
    and the inverse of the covariance matrix. It runs through each test observation
    and calculates the Mahalanobis distances (d). It then uses the smallest distance
    to assign the class label (c). If the assigned label matches the actual test label,
    we count it (nc). At the end of the function, we return the overall accuracy.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 得分函数接受测试观测值、类别均值向量的列表以及协方差矩阵的逆。它遍历每个测试观测值并计算马哈拉诺比斯距离 (d)。然后，它使用最小的距离来分配类别标签
    (c)。如果分配的标签与实际测试标签匹配，我们就统计它 (nc)。函数结束时，我们返回整体准确率。
- en: We call the score function twice. The first call uses the inverse covariance
    matrix (SI), while the second call uses an identity matrix, thereby making score
    calculate the Euclidean distance instead. Finally, we print both results.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用得分函数两次。第一次调用使用逆协方差矩阵 (SI)，而第二次调用使用单位矩阵，从而使得得分计算欧几里得距离。最后，我们打印两个结果。
- en: The randomization of the dataset ❷ means that each time the code is run, it
    will output slightly different scores. Running the code 100 times gives the following
    mean scores (± the standard deviation).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的随机化 ❷ 意味着每次运行代码时，输出的得分会略有不同。运行代码 100 次，得到以下的平均得分（±标准差）。
- en: '| **Distance** | **Mean score** |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| **距离** | **平均得分** |'
- en: '| --- | --- |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mahalanobis | 0.9595 ± 0.0142 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 马哈拉诺比斯距离 | 0.9595 ± 0.0142 |'
- en: '| Euclidean | 0.8914 ± 0.0185 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 欧几里得距离 | 0.8914 ± 0.0185 |'
- en: This clearly shows that using the Mahalanobis distance leads to better model
    performance, with about a 7 percent improvement in accuracy.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地显示了使用马哈拉诺比斯距离能提高模型性能，准确率提高了大约 7%。
- en: One recent use of the Mahalanobis distance in deep learning is to take the top-level
    embedding layer values, a vector, and use the Mahalanobis distance to detect out-of-domain
    or adversarial inputs. An *out-of-domain input* is one that is significantly different
    from the type of data the model was trained to use. An *adversarial input* is
    one where an adversary is deliberately attempting to fool the model by supplying
    an input that isn’t of class X but that the model will label as class X.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，马哈拉诺比斯距离在深度学习中的应用之一是利用顶层嵌入层值（一个向量），使用马哈拉诺比斯距离来检测域外或对抗性输入。*域外输入*是指与模型训练时使用的数据类型有显著差异的输入。*对抗性输入*是指对手故意尝试通过提供不是类
    X 的输入来欺骗模型，使得模型错误地将其标记为类 X。
- en: Kullback-Leibler Divergence
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kullback-Leibler 散度
- en: 'The *Kullback-Leibler divergence (KL-divergence)*, or *relative entropy*, is
    a measure of the similarity between two probability distributions: the lower the
    value, the more similar the distributions.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kullback-Leibler 散度（KL 散度）*，或称为 *相对熵*，是衡量两个概率分布相似度的指标：值越低，分布越相似。'
- en: If *P* and *Q* are discrete probability distributions, the KL-divergence is
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *P* 和 *Q* 是离散的概率分布，那么 KL 散度是
- en: '![Image](Images/151equ01.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/151equ01.jpg)'
- en: where log[2] is the logarithm base-2\. This is an information-theoretic measure;
    the output is in bits of information. Sometimes the natural log, ln, is used,
    in which case the measure is said to be in *nats*. The SciPy function that implements
    the KL-divergence is in scipy.special as rel_entr. Note that rel_entr uses the
    natural log, not log base-2\. Note also that the KL-divergence isn’t a distance
    metric in the mathematical sense because it violates the symmetry property, *D*[KL](*P**||Q*)
    ≠ *D*[KL](*Q*||*P*), but that doesn’t stop people from using it as one from time
    to time.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 log[2] 是以 2 为底的对数。这是一个信息论度量；输出是信息的比特。自然对数 ln 有时也会被使用，在这种情况下，度量被称为 *nats*。实现
    KL 散度的 SciPy 函数位于 scipy.special 中，名为 rel_entr。注意，rel_entr 使用的是自然对数，而不是以 2 为底的对数。还要注意，KL
    散度在数学意义上不是一个距离度量，因为它违反了对称性属性，*D*[KL](*P**||Q*) ≠ *D*[KL](*Q*||*P*)，但这并不妨碍人们偶尔将其作为距离度量来使用。
- en: Let’s see an example of how we might use the KL-divergence to measure between
    different discrete probability distributions. We’ll measure the divergence between
    two different binomial distributions and a uniform distribution. Then, we’ll plot
    the distributions to see if, visually, we believe the numbers.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个示例，展示如何使用 KL 散度来衡量不同离散概率分布之间的差异。我们将衡量两个不同的二项分布和一个均匀分布之间的散度。然后，我们将绘制这些分布，看看在视觉上是否与数值一致。
- en: To generate the distributions, we’ll take many draws from a uniform distribution
    with 12 possible outputs. We can do this quickly in code by using np.random.randint.
    Then, we’ll take draws from two different binomial distributions, *B*(12, 0.4)
    and *B*(12, 0.9), meaning 12 trials with probabilities of 0.4 and 0.9 per trial.
    We’ll generate histograms of the resulting draws, divide by the sum of the counts,
    and use the rescaled histograms as our probability distributions. We can then
    measure the divergences between them.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成分布，我们将从一个具有 12 个可能输出的均匀分布中进行多次抽样。我们可以通过使用 np.random.randint 快速地在代码中实现这一点。然后，我们将从两个不同的二项分布
    *B*(12, 0.4) 和 *B*(12, 0.9) 中抽样，这意味着每次试验有 12 次，概率分别为 0.4 和 0.9。我们将生成这些抽样的直方图，除以计数的总和，并将重新缩放的直方图用作我们的概率分布。然后，我们可以衡量它们之间的散度。
- en: The code we need is
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码是
- en: from scipy.special import rel_entr
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.special import rel_entr
- en: N = 1000000
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: N = 1000000
- en: ❶ p = np.random.randint(0,13,size=N)
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ p = np.random.randint(0,13,size=N)
- en: ❷ p = np.bincount(p)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ p = np.bincount(p)
- en: ❸ p = p / p.sum()
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ p = p / p.sum()
- en: q = np.random.binomial(12,0.9,size=N)
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: q = np.random.binomial(12,0.9,size=N)
- en: q = np.bincount(q)
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: q = np.bincount(q)
- en: q = q / q.sum()
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: q = q / q.sum()
- en: w = np.random.binomial(12,0.4,size=N)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: w = np.random.binomial(12,0.4,size=N)
- en: w = np.bincount(w)
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: w = np.bincount(w)
- en: w = w / w.sum()
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: w = w / w.sum()
- en: print(rel_entr(q,p).sum())
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: print(rel_entr(q,p).sum())
- en: print(rel_entr(w,p).sum())
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: print(rel_entr(w,p).sum())
- en: We load rel_entr from SciPy and set the number of draws for each distribution
    to 1,000,000 (N). The code to generate the respective probability distributions
    follows the same method for each distribution. We draw N samples from the distribution,
    starting with the uniform ❶. We use randint because it returns integers in the
    range [0, 12] so we can match the discrete [0, 12] values that binomial returns
    for 12 trials. We get the histogram from the vector of draws by using np.bincount.
    This function counts the frequency of unique values in a vector ❷. Finally, we
    change the counts into fractions by dividing the histogram by the sum ❸. This
    gives us a 12-element vector in p representing the probability that randint will
    return the values 0 through 12\. Assuming randint uses a good pseudorandom number
    generator, we expect the probabilities to be roughly equal for each value in p.
    (NumPy uses the Mersenne Twister pseudorandom number generator, one of the better
    ones out there, so we’re confident that we’ll get good results.)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 SciPy 中加载 rel_entr，并将每个分布的抽样次数设置为 1,000,000（N）。生成相应概率分布的代码对于每个分布采用相同的方法。我们从分布中抽取
    N 个样本，首先是均匀分布 ❶。我们使用 randint，因为它返回 [0, 12] 范围内的整数，这样我们就可以与二项分布在 12 次试验中返回的离散 [0,
    12] 值相匹配。我们通过使用 np.bincount 从抽样向量中获得直方图。这个函数统计向量中唯一值的频率 ❷。最后，我们通过将直方图除以总和来将计数转换为分数
    ❸。这将为我们提供一个包含 12 个元素的向量 p，表示 randint 返回值 0 到 12 的概率。假设 randint 使用了一个良好的伪随机数生成器，我们预计
    p 中每个值的概率大致相等。（NumPy 使用 Mersenne Twister 伪随机数生成器，它是一个非常好的生成器，因此我们有信心得到良好的结果。）
- en: We repeat this process, substituting binomial for randint, sampling from binomial
    distributions using probabilities of 0.9 and 0.4\. Again, histogramming the draws
    and converting the counts to fractions gives us the remaining probability distributions,
    q and w, based on 0.9 and 0.4, respectively.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程，使用0.9和0.4的概率从二项分布中抽样，而不是使用randint。再次通过对抽样结果进行直方图分析，并将计数转换为分数，得到了基于0.9和0.4的剩余概率分布q和w。
- en: We are finally ready to measure the divergence. The rel_entr function is a bit
    different from other functions in that it does not return *D*[KL] directly. Instead,
    it returns a vector of the same length as its arguments, where each element of
    the vector is part of the overall sum leading to *D*[KL]. Therefore, to get the
    actual divergence number, we need to add the elements of this vector. So, we print
    the sum of the output of rel_entr, comparing the two binomial distributions to
    the uniform distribution.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好测量散度了。rel_entr函数与其他函数略有不同，它并不会直接返回 *D*[KL]。相反，它返回一个与其参数长度相同的向量，其中向量的每个元素都是总体和的一部分，最终得出
    *D*[KL]。因此，要获得实际的散度数值，我们需要将这个向量的元素相加。因此，我们打印rel_entr输出的和，比较这两个二项分布与均匀分布的差异。
- en: The random nature of the draws means we’ll get slightly different numbers each
    time we run the code. One run gave
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样的随机性意味着每次运行代码时我们会得到略有不同的数字。一次运行结果为
- en: '| **Distributions** | **Divergence** |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| **分布** | **散度** |'
- en: '| --- | --- |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *D*[KL](*Q**&#124;&#124;P*) | 1.1826 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| *D*[KL](*Q**&#124;&#124;P*) | 1.1826 |'
- en: '| *D*[KL](*W&#124;&#124;P*) | 0.6218 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| *D*[KL](*W&#124;&#124;P*) | 0.6218 |'
- en: This shows that the binomial distribution with probability 0.9 diverges more
    from a uniform distribution than the binomial distribution with probability 0.4\.
    Recall, the smaller the divergence, the closer the two probability distributions
    are to each other.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，概率为0.9的二项分布比概率为0.4的二项分布更偏离均匀分布。回忆一下，偏差越小，两个概率分布越接近。
- en: Do we believe this result? One way to check is visually, by plotting the three
    distributions and seeing if *B*(12, 0.4) looks more like a uniform distribution
    than *B*(12, 0.9) does. This leads to [Figure 6-1](ch06.xhtml#ch06fig01).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这个结果吗？一种检查方法是通过可视化，即绘制三个分布并查看 *B*(12, 0.4) 是否比 *B*(12, 0.9) 更像均匀分布。这引出了[图6-1](ch06.xhtml#ch06fig01)。
- en: '![image](Images/06fig01.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig01.jpg)'
- en: '*Figure 6-1: Three different, discrete probability distributions: uniform (forward
    hash),* B(*12,0.4) (backward hash), and* B(*12,0.9) (horizontal hash)*'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-1：三种不同的离散概率分布：均匀分布（正向哈希），* B(*12,0.4)（反向哈希），和* B(*12,0.9)（水平哈希）*'
- en: Although it is clear that neither binomial distribution is particularly uniform,
    the *B*(12, 0.4) distribution is relatively centered in the range and spread across
    more values than the *B*(12, 0.9) distribution is. It seems reasonable to think
    of *B*(12, 0.4) as more like the uniform distribution, which is precisely what
    the KL-divergence told us by returning a smaller value.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然显然没有一个二项分布特别均匀，但 *B*(12, 0.4) 的分布相对集中在范围内，并且分布的值比 *B*(12, 0.9) 更加广泛。将 *B*(12,
    0.4) 看作更像均匀分布似乎是合理的，这正是KL散度通过返回较小的值所告诉我们的。
- en: We now have everything we need to implement principal component analysis.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了实施主成分分析所需的一切。
- en: Principal Component Analysis
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Assume we have a matrix, ***X***, representing a dataset. We understand that
    the variance of each of the features need not be the same. If we think of each
    observation as a point in an *n*-dimensional space, where *n* is the number of
    features in each observation, we can imagine a cloud of points with a different
    amount of scatter in different directions.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个矩阵，***X***，表示一个数据集。我们理解每个特征的方差不需要相同。如果我们将每个观测值看作是一个*n*维空间中的点，其中 *n* 是每个观测值中的特征数，我们可以想象一组点在不同方向上的散布程度不同。
- en: '*Principal component analysis (PCA)* is a technique to learn the directions
    of the scatter in the dataset, starting with the direction aligned along the greatest
    scatter. This direction is called the *principal component*. You then find the
    remaining components in order of decreasing scatter, with each new component orthogonal
    to all the others. The top part of [Figure 6-2](ch06.xhtml#ch06fig02) shows a
    2D dataset and two arrows. Without knowing anything about the dataset, we can
    see that the largest arrow points along the direction of the greatest scatter.
    This is what we mean by the principal component.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/06fig02.jpg)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-2: The first two features of the iris dataset and principal component
    directions (top), and the iris dataset after transformation by PCA (bottom)*'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: We often use PCA to reduce the dimensionality of a dataset. If there are 100
    variables per observation, but the first two principal components explain 95 percent
    of the scatter in the data, then mapping the dataset along those two components
    and discarding the remaining 98 components might adequately characterize the dataset
    with only two variables. We can use PCA to augment a dataset as well, assuming
    continuous features.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how does PCA work? All this talk about the scatter of the data implies
    that PCA might be able to make use of the covariance matrix, and, indeed, it does.
    We can break the PCA algorithm down into a few steps:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Mean center the data.
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the covariance matrix, Σ, of the mean-centered data.
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvalues and eigenvectors of Σ.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues by decreasing absolute value.
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discard the weakest eigenvalues/eigenvectors (optional).
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Form a transformation matrix, ***W***, using the remaining eigenvectors.
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate new transformed values from the existing dataset, ***x***′ = ***Wx***.
    These are sometimes referred to as *derived variables*.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s work through an example of this process using the iris dataset ([Listing
    6-1](ch06.xhtml#ch06ex01)). We’ll reduce the dimensionality of the data from four
    features to two. First the code, then the explanation:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.datasets import load_iris
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: iris = load_iris().data.copy()
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: ❶ m = iris.mean(axis=0)
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: ir = iris - m
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: ❷ cv = np.cov(ir, rowvar=False)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: ❸ val, vec = np.linalg.eig(cv)
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: val = np.abs(val)
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: ❹ idx = np.argsort(val)[::-1]
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: ex = val[idx] / val.sum()
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'print("fraction explained: ", ex)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: ❺ w = np.vstack((vec[:,idx[0]],vec[:,idx[1]]))
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: ❻ d = np.zeros((ir.shape[0],2))
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(ir.shape[0]):'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: d[i,:] = np.dot(w,ir[i])
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6-1: Principal component analysis (PCA)*'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: We start by loading the iris dataset, courtesy of sklearn. This gives us iris
    as a 150 × 4 matrix, since there are 150 observations, each with four features.
    We calculate the mean value of each feature ❶ and subtract it from the dataset,
    relying on NumPy’s broadcasting rules to subtract m from each row of iris. We’ll
    work with the mean-centered matrix ir going forward.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to compute the covariance matrix ❷. The output, cv, is a 4
    × 4 matrix, since we have four features per observation. We follow this by calculating
    the eigenvalues and eigenvectors of cv ❸ and then take the absolute value of the
    eigenvalues to get the magnitude. We want the eigenvalues in decreasing order
    of magnitude, so we get the indices that sort them that way ❹ using the Python
    idiom of [::-1] to reverse the order of a list or array.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude of the eigenvalues is proportional to the fraction of the variance
    in the dataset along each principal component; therefore, if we scale the eigenvalues
    by their overall sum, we get the proportion explained by each principal component
    (ex). The fraction of variance explained is
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 'fraction explained: [0.92461872 0.05306648 0.01710261 0.00521218]'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: indicating that two principal components explain nearly 98 percent of the variance
    in the iris dataset. Therefore, we’ll only keep the first two principal components
    going forward.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: We create the transformation matrix, w, from the eigenvectors that go with the
    two largest eigenvalues ❺. Recall, eig returns the eigenvectors as the columns
    of the matrix vec. The transformation matrix, w, is a 2 × 4 matrix because it
    maps a four-component feature vector to a new two-component vector.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: All that’s left is to create a place to hold the transformed observations and
    fill them in ❻. The new, reduced-dimension dataset is in d. We can now plot the
    entire transformed dataset, labeling each point by the class to which it belongs.
    The result is the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: In the top part of [Figure 6-2](ch06.xhtml#ch06fig02) is a plot of the original
    dataset using only the first two features. The arrows indicate the first two principal
    components, and the size of the arrows shows how much of the variance in the data
    these components explain. The first component explains most of the variance, which
    makes sense visually.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the derived variables in the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02)
    have made the dataset easier to work with, as the classes are better separated
    than on the top using only two of the original features. Sometimes, PCA makes
    it easier for a model to learn because of the reduced feature vector size. However,
    this is not always the case. During PCA, you may lose a critical feature allowing
    class separation. As with most things in machine learning, experimentation is
    vital.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is commonly used and is therefore well supported in multiple tool-kits.
    Instead of the dozen or so lines of code we used above, we can accomplish the
    same thing by using the PCA class from the sklearn.decomposition module:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.decomposition import PCA
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: pca = PCA(n_components=2)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(ir)
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: d = pca.fit_transform(ir)
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: The new, reduced-dimension dataset is in d. Like other sklearn classes, after
    we tell PCA how many components we want it to learn, it uses fit to set up the
    transformation matrix (w in [Listing 6-1](ch06.xhtml#ch06ex01)). We then apply
    the transform by calling fit_transform.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition and Pseudoinverse
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll end this chapter with an introduction to *singular value decomposition
    (SVD)*. This is a powerful technique to factor any matrix into the product of
    three matrices, each with special properties. The derivation of SVD is beyond
    the scope of this book. I trust motivated readers to dig into the vast literature
    on linear algebra to locate a satisfactory presentation of where SVD comes from
    and how it is best understood. Our goal is more modest: to become familiar with
    the mathematics found in deep learning. Therefore, we’ll content ourselves with
    the definition of SVD, some idea of what it gives us, some of its uses, and how
    to work with it in Python. For deep learning, you’ll most likely encounter SVD
    when calculating the pseudoinverse of a nonsquare matrix. We’ll also see how that
    works in this section.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: The output of SVD for an input matrix, ***A***, with real elements and shape
    *m* × *n*, where *m* does not necessarily equal *n* (though it could) is
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ10.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
- en: '***A*** has been decomposed into three matrices: ***U***, Σ, and ***V***. Note
    that you might sometimes see ***V***^⊤ written as ***V***^*, the conjugate transpose
    of ***V***. This is the more general form that works with complex-valued matrices.
    We’ll restrict ourselves to real-valued matrices, so we only need the ordinary
    matrix transpose.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVD of an *m* × *n* matrix, ***A***, returns the following: ***U***, which
    is *m* × *m* and orthogonal; Σ, which is *m* × *n* and diagonal; and ***V***,
    which is *n* × *n* and orthogonal. Recall that the transpose of an orthogonal
    matrix is its inverse, so ***UU***^⊤ = ***I****[m]* and ***VV***^⊤ = ***I****[n]*,
    where the subscript on the identity matrix gives the order of the matrix, *m*
    × *m* or *n* × *n*.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: At this point in the chapter, you may have raised an eyebrow at the statement
    “Σ, which is *m* × *n* and diagonal,” since we’ve only considered square matrices
    to be diagonal. Here, when we say *diagonal*, we mean a *rectangular diagonal
    matrix*. This is the natural extension to a diagonal matrix, where the elements
    of what would be the diagonal are nonzero and all others are zero. For example,
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/157equ01.jpg)'
  id: totrans-502
  prefs: []
  type: TYPE_IMG
- en: is a 3 × 5 rectangular diagonal matrix because only the main diagonal is nonzero.
    The “singular” in “singular value decomposition” comes from the fact that the
    elements of the diagonal matrix, Σ, are the singular values, the square roots
    of the positive eigenvalues of the matrix ***A**^T**A***.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: SVD in Action
  id: totrans-504
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s be explicit and use SVD to decompose a matrix. Our test matrix is
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/158equ01.jpg)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
- en: We’ll show SVD in action as a series of steps. To get the SVD, we use svd from
    scipy.linalg,
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from scipy.linalg import svd'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([[3,2,2],[2,3,-2]])'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '>>> u,s,vt = svd(a)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 'where u is ***U***, vt is ***V***^⊤, and s contains the singular values:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(u)'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '[[-0.70710678 -0.70710678]'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '[-0.70710678  0.70710678]]'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(s)'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '[5\. 3.]'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(vt)'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '[[-7.07106781e-01 -7.07106781e-01 -5.55111512e-17]'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '[-2.35702260e-01  2.35702260e-01 -9.42809042e-01]'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '[-6.66666667e-01  6.66666667e-01  3.33333333e-01]]'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check that the singular values are indeed the square roots of the positive
    eigenvalues of ***A**^T**A***:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.linalg.eig(a.T @ a)[0])'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '[2.5000000e+01 5.0324328e-15 9.0000000e+00]'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows us that, yes, 5 and 3 are the square roots of 25 and 9\. Recall
    that eig returns a list, the first element of which is a vector of the eigenvalues.
    Also note that there is a third eigenvalue: zero. You might ask: “How small a
    numeric value should we interpret as zero?” That’s a good question with no hard
    and fast answer. Typically, I interpret values below 10^(−9) to be zero.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: 'The claim of SVD is that ***U*** and ***V*** are unitary matrices. If so, their
    products with their transposes should be the identity matrix:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(u.T @ u)'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: '[[1.00000000e+00 3.33066907e-16]'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '[3.33066907e-16 1.00000000e+00]]'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(vt @ vt.T)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 1.00000000e+00  8.00919909e-17 -1.85037171e-17]'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '[ 8.00919909e-17  1.00000000e+00 -5.55111512e-17]'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '[-1.85037171e-17 -5.55111512e-17  1.00000000e+00]]'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Given the comment above about numeric values that we should interpret as zero,
    this is indeed the identity matrix. Notice that svd returned ***V***^⊤, not ***V***.
    However, since (***V***^⊤)^⊤ = ***V***, we’re still multiplying ***V***^⊤***V***.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 'The svd function returns not Σ but the diagonal values of Σ. Therefore, let’s
    reconstruct Σ and use it to see that SVD works, meaning we can use ***U***, Σ,
    and ***V***^⊤ to recover ***A***:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '>>> S = np.zeros((2,3))'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '>>> S[0,0], S[1,1] = s'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(S)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[[5\. 0\. 0.]'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '[0\. 3\. 0.]]'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '>>> A = u @ S @ vt'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(A)'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 3\. 2\. 2.]'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '[ 2\. 3\. -2.]]'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the ***A*** we started with—almost: the recovered ***A*** is no longer
    of integer type, a subtle change worth remembering when writing code.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Two Applications
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SVD is a cute trick, but what can we do with it? The short answer is “a lot.”
    Let’s see two applications. The first is using SVD for PCA. The sklearn PCA class
    we used in the previous section uses SVD under the hood. The second example shows
    up in deep learning: using SVD to calculate the Moore-Penrose pseudoinverse, a
    generalization of the inverse of a square matrix to *m* × *n* matrices.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: SVD for PCA
  id: totrans-547
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To see how to use SVD for PCA, let’s use the iris data from the previous section
    so we can compare with those results. The key is to truncate the Σ and ***V***^⊤
    matrices to keep only the desired number of largest singular values. The decomposition
    code will put the singular values in decreasing order along the diagonal of Σ
    for us, we need only retain the first *k* columns of Σ. In code, then,
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: u,s,vt = svd(ir)
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: ❶ S = np.zeros((ir.shape[0], ir.shape[1]))
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(4):'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: S[i,i] = s[i]
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: ❷ S = S[:, :2]
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: T = u @ S
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’re using ir from [Listing 6-1](ch06.xhtml#ch06ex01). This is the mean-centered
    version of the iris dataset matrix, with 150 rows of four features each. A call
    to svd gives us the decomposition of ir. The next three lines ❶ create the full
    Σ matrix in S. Because the iris dataset has four features, the s vector that svd
    returns will have four singular values.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: The truncation comes by keeping the first two columns of S ❷. Doing this changes
    Σ from a 150 × 4 matrix to a 150 × 2 matrix. Multiplying ***U*** by the new Σ
    gives us the transformed iris dataset. Since ***U*** is 150 × 150 and Σ is 150
    × 2, we get a 150 × 2 dataset in T. If we plot this as T[:,0] versus T[:,1], we
    get the exact same plot as the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: The Moore-Penrose Pseudoinverse
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As promised, our second application is to compute ***A***^+, the Moore-Penrose
    pseudoinverse of an *m* × *n* matrix ***A***. The matrix ***A***^+ is called a
    pseudo-inverse because, in conjunction with ***A***, it acts like an inverse in
    that
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ11.jpg)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
- en: where ***AA***^+ is somewhat like the identity matrix, making ***A***^+ somewhat
    like the inverse of ***A***.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that the pseudoinverse of a rectangular diagonal matrix is simply the
    reciprocal of the diagonal values, leaving zeros as zero, followed by a transpose,
    we can calculate the pseudoinverse of any general matrix as
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '***A***^+ = ***V***Σ^+ ***U****'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: for ***A*** = ***U***Σ***V****, the SVD of ***A***. Notice, we’re using the
    conjugate transpose, ***V***^*, instead of the ordinary transpose, ***V***^⊤.
    If ***A*** is real, then the ordinary transpose is the same as the conjugate transpose.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if the claim regarding ***A***^+ is true. We’ll start with the ***A***
    matrix we used in the section above, compute the SVD, and use the parts to find
    the pseudoinverse. Finally, we’ll validate [Equation 6.11](ch06.xhtml#ch06equ11).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with ***A***, the same array we used above for the SVD example:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: '>>> A = np.array([[3,2,2],[2,3,-2]])'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(A)'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 3 2  2]'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[ 2 3 -2]]'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying SVD will give us ***U*** and ***V***^⊤ along with the diagonal of
    Σ. We’ll use the diagonal elements to construct Σ^+ by hand. Recall, Σ^+ is the
    transpose of Σ, where the diagonal elements that are not zero are changed to their
    reciprocals:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '>>> u,s,vt = svd(A)'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: '>>> Splus = np.array([[1/s[0],0],[0,1/s[1]],[0,0]])'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(Splus)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '[[0.2        0.        ]'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '[0.         0.33333333]'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '[0.         0.        ]]'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can calculate ***A***^+ and verify that ***AA***^+***A*** = ***A***:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: '>>> Aplus = vt.T @ Splus @ u.T'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(Aplus)'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 0.15555556  0.04444444]'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0.04444444  0.15555556]'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0.22222222 -0.22222222]]'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(A @ Aplus @ A)'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 3\. 2.  2.]'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '[ 2\. 3\. -2.]]'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: 'And, in this case, ***AA***^+ is the identity matrix:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(A @ Aplus)'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '[[1.00000000e+00 5.55111512e-17]'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '[1.66533454e-16 1.00000000e+00]]'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our whirlwind look at SVD and our discussion of linear algebra.
    We barely scratched the surface, but we’ve covered what we need to know.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-591
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This heavy chapter and [Chapter 5](ch05.xhtml#ch05) before it plowed through
    a lot of linear algebra. As a mathematical topic, linear algebra is vastly richer
    than our presentation here.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: We focused the chapter on square matrices, as they have a special place in linear
    algebra. Specifically, we discussed general properties of square matrices, with
    examples. We learned about eigenvalues and eigenvectors, how to find them, and
    why they are useful. Next, we looked at vector norms and other ways to measure
    distance, as they show up often in deep learning. Finally, we ended the chapter
    by learning what PCA is and how it works, followed by a look at singular value
    decomposition, with two applications relevant to deep learning.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter shifts gears and covers differential calculus. This is, fortunately,
    the “easy” part of calculus, and is, in general, all that we need to understand
    the algorithms specific to deep learning. So, fasten your seat belts, make sure
    your arms and legs are fully within the vehicle, and prepare for departure to
    the world of differential calculus.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
