- en: '**6'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MORE LINEAR ALGEBRA**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’ll continue our exploration of linear algebra concepts.
    Some of these concepts are only tangentially related to deep learning, but they’re
    the sort of math you’ll eventually encounter. Think of this chapter as assumed
    background knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we’ll learn more about the properties of and operations on square
    matrices, introducing terms you’ll encounter in the deep learning literature.
    After that, I’ll introduce the ideas behind the eigenvalues and eigenvectors of
    a square matrix and how to find them. Next, we’ll explore vector norms and other
    ways of measuring distance that are often encountered in deep learning. At that
    point, I’ll introduce the important concept of a covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll conclude the chapter by demonstrating principal component analysis (PCA)
    and singular value decomposition (SVD). These frequently used approaches depend
    heavily on the concepts and operators introduced throughout the chapter. We will
    see what PCA is, how to do it, and what it can buy us from a machine learning
    perspective. Similarly, we will work with SVD and see how we can use it to implement
    PCA as well as compute the pseudoinverse of a rectangular matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Square Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Square matrices occupy a special place in the world of linear algebra. Let’s
    explore them in more detail. The terms used here will show up often in deep learning
    and other areas.
  prefs: []
  type: TYPE_NORMAL
- en: Why Square Matrices?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If we multiply a matrix by a column vector, we’ll get another column vector
    as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/128equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interpreted geometrically, the 2 × 4 matrix has mapped the 4 × 1 column vector,
    a point in ℝ⁴, to a new point in ℝ². The mapping is linear because the point values
    are only being multiplied by the elements of the 2 × 4 matrix; there are no nonlinear
    operations, such as raising the components of the vector to a power, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Viewed this way, we can use a matrix to transform points between spaces. If
    the matrix is square, say, *n* × *n*, the mapping is from ℝ^(*n*) back to ℝ^(*n*).
    For example, consider
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/128equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the point (11, 12, 13) is mapped to the point (74, 182, 209), both in
    ℝ³.
  prefs: []
  type: TYPE_NORMAL
- en: Using a matrix to map points from one space to another makes it possible to
    rotate a set of points about an axis by using a *rotation matrix*. For simple
    rotations, we can define matrices in 2D,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and in 3D,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/128equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Rotations are by an angle, *θ*, and for 3D, about the x-, y-, or z-axis, as
    indicated by the subscript.
  prefs: []
  type: TYPE_NORMAL
- en: Using a matrix, we can create an *affine transformation*. An affine transformation
    maps a set of points into another set of points so that points on a line in the
    original space are still on a line in the mapped space. The transformation is
  prefs: []
  type: TYPE_NORMAL
- en: '***y*** = ***Ax*** + ***b***'
  prefs: []
  type: TYPE_NORMAL
- en: The affine transform combines a matrix transform, ***A***, with a translation,
    ***b***, to map a vector, ***x***, to a new vector, ***y***. We can combine this
    operation into a single matrix multiplication by putting ***A*** in the upper-left
    corner of the matrix and adding ***b*** as a new column on the right. A row of
    all zeros at the bottom with a single 1 in the rightmost column completes the
    augmented transformation matrix. For an affine transformation matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/129equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and translation vector
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/129equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: we get
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/129equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This form maps a point, *(x*, *y*), to a new point, (*x*′, *y*′).
  prefs: []
  type: TYPE_NORMAL
- en: This maneuver is identical to the *bias trick* sometimes used when implementing
    a neural network to bury the bias in an augmented weight matrix by including an
    extra feature vector input set to 1\. In fact, we can view a feedforward neural
    network as a series of affine transformations, where the transformation matrix
    is the weight matrix between the layers, and the bias vector provides the translation.
    The activation function at each layer alters the otherwise linear relationship
    between the layers. It is this nonlinearity that lets the network learn a new
    way to map inputs so that the final output reflects the functional relationship
    the network is designed to learn.
  prefs: []
  type: TYPE_NORMAL
- en: We use square matrices, then, to map points from one space back into the same
    space, for example to rotate them about an axis. Let’s look now at some special
    properties of square matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Transpose, Trace, and Powers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Chapter 5](ch05.xhtml#ch05) showed us the vector transpose to move between
    column and row vectors. The transpose operation is not restricted to vectors.
    It works for any matrix by flipping the rows and columns along the main diagonal.
    For example,'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/129equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The transpose is formed by flipping the indices of the matrix elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a[ij]* ← *a[ij]*, *i* = 0, 1, . . . , *n* − 1, *j* = 0, 1, . . . , *m* – 1'
  prefs: []
  type: TYPE_NORMAL
- en: This changes an *n* × *m* matrix into an *m* × *n* matrix. Notice that the order
    of a square matrix remains the same under the transpose operation, and the values
    on the main diagonal don’t change.
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, you can call the `transpose` method on an array, but the transpose
    is so common that a shorthand notation (`.T`) also exists. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The *trace* is another common operation applied to square matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/130equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As an operator, the trace has certain properties. For example, it’s linear:'
  prefs: []
  type: TYPE_NORMAL
- en: tr(***A*** + ***B***) = tr***A*** + tr***B***
  prefs: []
  type: TYPE_NORMAL
- en: It’s also true that tr(***A***) = tr(***A****^T*) and tr(***AB***) = tr(***BA***).
  prefs: []
  type: TYPE_NORMAL
- en: NumPy uses `np.trace` to quickly calculate the trace of a matrix and `np .diag`
    to return the diagonal elements of a matrix as a 1D array,
  prefs: []
  type: TYPE_NORMAL
- en: (*a*[00], *a*[11], . . . , *a[n−1,n−1]*)
  prefs: []
  type: TYPE_NORMAL
- en: for an *n* × *n* or *n* × *m* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'A matrix doesn’t need to be square for NumPy to return the elements along its
    diagonal. And although mathematically the trace generally only applies to square
    matrices, NumPy will calculate the trace of any matrix, returning the sum of the
    diagonal elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, you can multiply a square matrix by itself, implying that you can raise
    a square matrix to an integer power, *n*, by multiplying itself *n* times. Note
    that this is not the same as raising the elements of the matrix to a power. For
    example,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/131equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix power follows the same rules as raising any number to a power:'
  prefs: []
  type: TYPE_NORMAL
- en: '***A**^n**A**^m* = ***A**^(n+m)*'
  prefs: []
  type: TYPE_NORMAL
- en: (***A**^n*)^(*m*) = ***A**^(nm)*
  prefs: []
  type: TYPE_NORMAL
- en: for ![Image](Images/131equ01a.jpg) (positive integers) and where ***A*** is
    a square matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy provides a function to compute the power of a square matrix more efficiently
    than repeated calls to `np.dot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s consider some special square matrices that you’ll encounter from time
    to time.
  prefs: []
  type: TYPE_NORMAL
- en: Special Square Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many square (and nonsquare) matrices have received special names. Some are
    rather obvious, like matrices that are all zero or one, which are called *zeros
    matrices* and *ones matrices*, respectively. NumPy uses these extensively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that you can find a matrix of any constant value, *c*, by multiplying the
    ones matrix by *c*.
  prefs: []
  type: TYPE_NORMAL
- en: Notice above that NumPy defaults to matrices of 64-bit floating-point numbers
    corresponding to a C-language type of `double`. See [Table 1-1](ch01.xhtml#ch01tab01)
    on page 6 for a list of possible numeric data types. You can specify the desired
    data type with the `dtype` keyword. In pure mathematics, we don’t care much about
    data types, but to work in deep learning, you need to pay attention to avoid defining
    arrays that are far more memory-hungry than needed. Many deep learning models
    are happy with arrays of 32-bit floats, which use half the memory per element
    than the NumPy default. Also, many toolkits make use of new or previously seldom-used
    data types, like 16-bit floats, to allow for even better use of memory. NumPy
    does support 16-bit floats by specifying `float16` as the `dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: The Identity Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By far, the most important special matrix is the *identity matrix*. This is
    a square matrix with all ones on the diagonal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The identity matrix acts like the number 1 when multiplying a matrix. Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '***AI*** = ***IA*** = ***A***'
  prefs: []
  type: TYPE_NORMAL
- en: for an *n* × *n* square matrix ***A*** and an *n* × *n* identity matrix ***I***.
    When necessary, we’ll add a subscript to indicate the order of the identity matrix,
    for example, ***I**[n]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy uses `np.identity` or `np.eye` to generate identity matrices of a given
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Look carefully at the example above. Mathematically, we said that multiplication
    of a square matrix by the identity matrix of the same order returns the matrix.
    NumPy, however, did something we might not want. Matrix `a` was defined with integer
    elements, so it has a data type of `int64`, the NumPy default for integers. However,
    since we didn’t explicitly provide `np.identity` with a data type, NumPy defaulted
    to a 64-bit float. Therefore, matrix multiplication (`@`) between `a` and `i`
    returned a floating-point version of `a`. This subtle change of data type might
    be important for later calculations, so, again, we need to pay attention to data
    types when using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t matter if you use `np.identity` or `np.eye`. In fact, internally,
    `np.identity` is just a wrapper for `np.eye`.
  prefs: []
  type: TYPE_NORMAL
- en: Triangular Matrices
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Occasionally, you’ll hear about *triangular* matrices. There are two kinds:
    upper and lower. As you may intuit from the name, an upper triangular matrix is
    one with nonzero elements in the part on or above the main diagonal, whereas a
    lower triangular matrix only has elements on or below the main diagonal. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/133equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is an upper triangular matrix, whereas
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/133equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is a lower triangular matrix. A matrix that has elements only on the main diagonal
    is, not surprisingly, a *diagonal matrix*.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy has two functions, `np.triu` and `np.tril`, to return the upper or lower
    triangular part of the given matrix, respectively. So,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We don’t frequently use triangular matrices in deep learning, but we do use
    them in linear algebra, in part to compute determinants, to which we now turn.
  prefs: []
  type: TYPE_NORMAL
- en: Determinants
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can think of the *determinant* of a square matrix, *n* × *n*, as a function
    mapping square matrices to a scalar. The primary use of the determinant in deep
    learning is to compute the eigenvalues of a matrix. We’ll see what that means
    later in this chapter, but for now think of eigenvalues as special scalar values
    associated with a matrix. The determinant also tells us something about whether
    or not a matrix has an inverse, as we’ll also see below. Notationally, we write
    the determinant of a matrix with vertical bars. For example, if ***A*** is a 3
    × 3 matrix, we write the determinant as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/134equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where we state explicitly that the value of the determinant is a scalar (element
    of ℝ). All square matrices have a determinant. For now, let’s consider some of
    the properties of the determinant:'
  prefs: []
  type: TYPE_NORMAL
- en: If any row or column of ***A*** is zero, then det(***A***) = 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any two rows of ***A*** are identical, then det(***A***) = 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If ***A*** is an upper or lower triangular, then det ![Image](Images/134equ02.jpg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If ***A*** is a diagonal matrix, then det ![Image](Images/134equ03.jpg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The determinant of the identity matrix, regardless of size, is 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The determinant of a product of matrices is the product of the determinants,
    det(***AB***) = det(***A***)det(***B***).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: det(***A***) = det(***A***^⊤).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: det(***A**^n*) = det(***A***)^(*n*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Property 7 indicates that the transpose operation does not change the value
    of a determinant. Property 8 is a consequence of Property 6.
  prefs: []
  type: TYPE_NORMAL
- en: We have multiple ways we can calculate the determinant of a square matrix. We’ll
    examine only one way here, which involves using a recursive formula. All recursive
    formulas apply themselves, just as recursive functions in code call themselves.
    The general idea is that each recursion works on a simpler version of the problem,
    which can be combined to return the solution to the larger problem.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can calculate the factorial of an integer,
  prefs: []
  type: TYPE_NORMAL
- en: '*n*! = *n*(*n* − 1)(*n* − 2)(*n* − 3) . . . 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'recursively if we notice the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/135equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first statement says that the factorial of *n* is *n* times the factorial
    of (*n* − 1). The second statement says that the factorial of zero is one. The
    recursion is the first statement, but this recursion will never end without some
    condition that returns a value. That’s the point of the second statement, the
    *base case*: it says the recursion ends when we get to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This might be clearer in code. We can define the factorial like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `factorial` calls itself on the argument minus one, unless the argument
    is zero, in which case it immediately returns one. The code works because of the
    Python call stack. The call stack keeps track of all the computations of `n*factorial(n-1)`.
    When we encounter the base case, all the pending multiplications are done, and
    we return the final value.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate determinants recursively, then, we need a recursion statement,
    something that defines the determinant of a matrix in terms of simpler determinants.
    We also need a base case that gives us a definitive value. For determinants, the
    base case is when we get to a 1 × 1 matrix. For any 1 × 1 matrix, ***A***, we
    have
  prefs: []
  type: TYPE_NORMAL
- en: det(***A***) = *a*[00]
  prefs: []
  type: TYPE_NORMAL
- en: meaning the determinant of a 1 × 1 matrix is the single value it contains.
  prefs: []
  type: TYPE_NORMAL
- en: Our plan is to calculate the determinant by breaking the calculation into successively
    simpler determinants until we reach the base case above. To do this, we need a
    statement involving recursion. However, we need to define a few things before
    we can make the statement. First, we need to define the *minor* of a matrix. The
    (*i*, *j*)-minor of a matrix, ***A***, is the matrix left after removing the *i*th
    row and *j*th column of ***A***. We’ll denote a minor matrix by ***A****[ij]*.
    For example, given
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/135equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/136equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the minor, ***A***[11], is found by deleting row 1 and column 1 to leave
    only the underlined values.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we need to define the *cofactor*, *C**[ij]*, of the minor, ***A****[ij]*.
    This is where our recursive statement appears. The definition is
  prefs: []
  type: TYPE_NORMAL
- en: '*C**[ij]* = (−1)^(*i*+*j*+2)det(***A****[ij]*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cofactor depends on the determinant of the minor. Notice the exponent on
    −1, written as *i* + *j* + 2\. If you look at most math books, you’ll see the
    exponent as *i* + *j*. We’ve made a conscious choice to define matrices with zero-based
    indices so the math and implementation in code match without being off by one.
    Here’s one place where that choice forces us to be less elegant than the math
    texts. Because our indices are “off” by one, we need to add that one back into
    the exponent of the cofactor so the pattern of positive and negative values that
    the cofactor uses is correct. This means adding one to each of the variables in
    the exponent: *i* → *i* + 1 and *j* → *j* + 1\. This makes the exponent *i* +
    *j* → (*i* + 1) + (*j* + 1) = *i* + *j* + 2.'
  prefs: []
  type: TYPE_NORMAL
- en: We’re now ready for our full recursive definition of the determinant of ***A***
    by using *cofactor expansion*. It turns out that summing the product of the matrix
    values and associated cofactors for any row or column of a square matrix will
    give us the determinant. So, we’ll use the first row of the matrix and calculate
    the determinant as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You may be wondering: Where’s the recursion in [Equation 6.3](ch06.xhtml#ch06equ03)?
    It shows up on the determinant of the minor. If ***A*** is an *n* × *n* matrix,
    the minor, ***A****[ij]*, is an *(n* − 1) × (*n* − 1) matrix. Therefore, to calculate
    the cofactors to find the determinant of an *n* × *n* matrix, we need to know
    how to find the determinant of an *(n* − 1) × (*n* − 1) matrix. However, we can
    use cofactor expansion to find the *(n* − 1) × (*n* − 1) determinant, which involves
    finding the determinant of an *(n* − 2) × (*n* − 2) matrix. This process continues
    until we get to a 1 × 1 matrix. We already know the determinant of a 1 × 1 matrix
    is the single value it contains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work through this process for a 2 × 2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/136equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using cofactor expansion, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/137equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is the formula for the determinant of a 2 × 2 matrix. The minors of a
    2 × 2 matrix are 1 × 1 matrices, each returning either *d* or *c* in this case.
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, we calculate determinants with `np.linalg.det`. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of code uses the formula for a 2 × 2 matrix we derived above
    for comparison purposes. Internally, NumPy does not use recursive cofactor expansion
    to calculate the determinant. Instead, it factors the matrix into the product
    of three matrices: (1) a *permutation matrix*, which looks like a scrambled identity
    matrix with only a single one in each row and column, (2) a lower triangular matrix,
    and (3) an upper triangular matrix. The determinant of the permutation matrix
    is either +1 or −1\. The determinant of a triangular matrix is the product of
    the diagonal elements, while the determinant of a product of matrices is the product
    of the per-matrix determinants.'
  prefs: []
  type: TYPE_NORMAL
- en: We can use determinants to determine whether a matrix has an inverse. Let’s
    turn there now.
  prefs: []
  type: TYPE_NORMAL
- en: Inverses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Equation 6.2](ch06.xhtml#ch06equ02) defines the identity matrix. We said that
    this matrix acts like the number 1, so when it multiplies a square matrix, the
    same square matrix is returned. When multiplying scalars, we know that for any
    number, *x* ≠ 0, there exists another number, call it *y*, such that *xy* = 1\.
    This number is the multiplicative inverse of *x*. Furthermore, we know exactly
    what *y* is; it’s 1/*x* = *x*^(−1).'
  prefs: []
  type: TYPE_NORMAL
- en: By analogy, then, we might wonder if, since we have an identity matrix that
    acts like the number 1, there is another square matrix, call it ***A***^(−1),
    for a given square matrix, ***A***, such that
  prefs: []
  type: TYPE_NORMAL
- en: '***AA***^(−1) = ***A***^(−1) ***A*** = ***I***'
  prefs: []
  type: TYPE_NORMAL
- en: 'If ***A***^(−1) exists, it’s known as the *inverse matrix* of ***A***, and
    ***A*** is said to be *invertable*. For real numbers, all numbers except zero
    have an inverse. For matrices, it isn’t so straightforward. Many square matrices
    don’t have inverses. To check if ***A*** has an inverse, we use the determinant:
    det(***A***) = 0 tells us that ***A*** has no inverse. Furthermore, if ***A***^(−1)
    exists, then'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/138equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note also that (***A***^(−1))^(−1) = ***A***, as is the case for real numbers.
    Another useful property of inverses is
  prefs: []
  type: TYPE_NORMAL
- en: (***AB***)^(−1) = ***B***^(−1) ***A***^(−1)
  prefs: []
  type: TYPE_NORMAL
- en: 'where the order of the product on the right-hand side is important. Finally,
    note that the inverse of a diagonal matrix is simply the reciprocal of the diagonal
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/138equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It’s possible to calculate the inverse by hand using row operations, which we’ve
    conveniently ignored here because they are seldom used in deep learning. Cofactor
    expansion techniques can also calculate the inverse, but to save time, we won’t
    elaborate on the process here. What’s important for us is to know that square
    matrices often have an inverse, and that we can calculate inverses with NumPy
    via `np.linalg.inv`.
  prefs: []
  type: TYPE_NORMAL
- en: If a matrix is *not* invertible, the matrix is said to be *singular*. Therefore,
    the determinant of a singular matrix is zero. If a matrix has an inverse, it is
    a *nonsingular* or *nondegenerate* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, we use `np.linalg.inv` to calculate the inverse of a square matrix.
    For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Notice the inverse (`b`) working as we expect and giving the identity matrix
    when multiplying `a` from the left or right.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric, Orthogonal, and Unitary Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If for a square matrix, ***A***, we have
  prefs: []
  type: TYPE_NORMAL
- en: '***A***^⊤ = ***A***'
  prefs: []
  type: TYPE_NORMAL
- en: then ***A*** is said to be a *symmetric matrix*. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/139equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is a symmetric matrix, since ***A***^⊤ = ***A***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that diagonal matrices are symmetric, and the product of two symmetric
    matrices is commutative: ***AB*** = ***BA***. The inverse of a symmetric matrix,
    if it exists, is also a symmetric matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: If the following is true,
  prefs: []
  type: TYPE_NORMAL
- en: '***AA***^⊤ = ***A***^⊤ ***A*** = ***I***'
  prefs: []
  type: TYPE_NORMAL
- en: then ***A*** is an orthogonal matrix. If ***A*** is an orthogonal matrix, then
  prefs: []
  type: TYPE_NORMAL
- en: '***A***^(−1) = ***A***^⊤'
  prefs: []
  type: TYPE_NORMAL
- en: and, as a result,
  prefs: []
  type: TYPE_NORMAL
- en: det(***A***) = ±1
  prefs: []
  type: TYPE_NORMAL
- en: If the values in the matrix are allowed to be complex, which does not happen
    often in deep learning, and
  prefs: []
  type: TYPE_NORMAL
- en: '***U***^****U*** = ***UU***^* = ***I***'
  prefs: []
  type: TYPE_NORMAL
- en: then ***U*** is a *unitary matrix* with ***U***^* being the *conjugate transpose*
    of ***U***. The conjugate transpose is the ordinary matrix transpose followed
    by the complex conjugate operation to change ![Image](Images/140equ01.jpg) to
    −*i*. So, we might have
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/140equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes, especially in physics, the conjugate transpose is called the *Hermitian
    adjoint* and is denoted as ***A***^†. If a matrix is equal to its conjugate transpose,
    it is called a *Hermitian matrix*. Notice that real symmetric matrices are also
    Hermitian matrices because the conjugate transpose is the same as the ordinary
    transpose when the values are real numbers. Therefore, you might encounter the
    term *Hermitian* in place of *symmetric* when referring to matrices with real
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: Definiteness of a Symmetric Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We saw at the beginning of this section that an *n* × *n* square matrix maps
    a vector in ℝ^(*n*) to another vector in ℝ^(*n*). Let’s consider now a symmetric
    *n* × *n* matrix, ***B***, with real-valued elements. We can characterize this
    matrix by how it maps vectors using the inner product between the mapped vector
    and the original vector. Specifically, if ***x*** is a column vector (*n* × 1),
    then ***Bx*** is also an *n* × 1 column vector. Therefore, the inner product of
    this vector and the original vector, ***x***, is ***x***^⊤***Bx***, a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/140equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: then ***B*** is said to be *positive definite*. Here, the bolded **0** is the
    *n* × 1 column vector of all zeros, and ∀ is math notation meaning “for all.”
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/140equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: then ***B*** is *negative definite*. Relaxing the inner product relationship
    and the nonzero requirement on ***x*** gives two additional cases. If
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/140equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: then ***B*** is said to be *positive semidefinite*, and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/140equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: makes ***B*** a *negative semidefinite* matrix. Finally, a real square symmetric
    matrix that is neither positive nor negative semidefinite is called an *indefinite
    matrix*. The definiteness of a matrix tells us something about the eigenvalues,
    which we’ll learn more about in the next section. If a symmetric matrix is positive
    definite, then all of its eigenvalues are positive. Similarly, a symmetric negative
    definite matrix has all negative eigenvalues. Positive and negative semidefinite
    symmetric matrices have eigenvalues that are all positive or zero or all negative
    or zero, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s shift gears now from talking about types of matrices to discovering the
    importance of eigenvectors and eigenvalues, key properties of a matrix that we
    use frequently in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvectors and Eigenvalues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned above that a square matrix maps a vector into another vector in the
    same dimensional space, ***v***′ = ***Av***, where both ***v***′ and ***v*** are
    *n*-dimensional vectors, if ***A*** is an *n* × *n* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this equation,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for some square matrix, ***A***, where λ is a scalar value and ***v*** is a
    nonzero column vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 6.4](ch06.xhtml#ch06equ04) says that the vector, ***v***, is mapped
    by ***A*** back into a scalar multiple of itself. We call ***v*** an *eigenvector*
    of ***A*** with *eigenvalue* λ. The prefix *eigen* comes from German and is often
    translated as “self,” “characteristic,” or even “proper.” Thinking geometrically,
    [Equation 6.4](ch06.xhtml#ch06equ04) says that the action of ***A*** on its eigenvectors
    in ℝ^(*n*) is to shrink or expand the vector without changing its direction. Note,
    while ***v*** is nonzero, it’s possible for λ to be zero.'
  prefs: []
  type: TYPE_NORMAL
- en: How does [Equation 6.4](ch06.xhtml#ch06equ04) relate to the identity matrix,
    ***I*** ? By definition, the identity matrix maps a vector back into itself without
    scaling it. Therefore, the identity matrix has an infinite number of eigenvectors,
    and all of them have an eigenvalue of 1, since, for any ***x***, ***Ix*** = ***x***.
    Therefore, the same eigenvalue may apply to more than one eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that [Equation 6.1](ch06.xhtml#ch06equ01) defines a rotation matrix in
    2D space for some given angle, *θ*. This matrix has no eigenvectors, because,
    for any nonzero vector, it rotates the vector by *θ*, so it can never map a vector
    back into its original direction. Therefore, not every matrix has eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Eigenvalues and Eigenvectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To find the eigenvalues of a matrix, if there are any, we go back to [Equation
    6.4](ch06.xhtml#ch06equ04) and rewrite it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can insert the identity matrix, ***I***, between λ and ***v*** because ***Iv***
    = ***v***. Therefore, to find the eigenvalues of ***A***, we need to find values
    of λ that cause the matrix ***A*** − λ***I*** to map a nonzero vector, ***v***,
    to the zero vector. [Equation 6.5](ch06.xhtml#ch06equ05) only has solutions other
    than the zero vector if the determinant of ***A*** − λ***I*** is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above gives us a way to find the eigenvalues. For example, consider what
    ***A*** − λ***I*** looks like for a 2 × 2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/142equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We learned above that the determinant of a 2 × 2 matrix has a simple form; therefore,
    the determinant of the matrix above is
  prefs: []
  type: TYPE_NORMAL
- en: det(***A*** − λ***I***) = (*a* − λ)(*d* − λ) − *bc*
  prefs: []
  type: TYPE_NORMAL
- en: This equation is a second-degree polynomial in λ. Since we need the determinant
    to be zero, we set this polynomial to zero and find the roots. The roots are the
    eigenvalues of ***A***. The polynomial that this process finds is called the *characteristic
    polynomial*, and [Equation 6.5](ch06.xhtml#ch06equ05) is the *characteristic equation*.
    Notice above that the characteristic polynomial is a second-degree polynomial.
    In general, the characteristic polynomial of an *n* × *n* matrix is of degree
    *n*, so a matrix has at most *n* distinct eigenvalues, since an *n*th degree polynomial
    has at most *n* roots.
  prefs: []
  type: TYPE_NORMAL
- en: Once we know the roots of the characteristic polynomial, we can go back to [Equation
    6.5](ch06.xhtml#ch06equ05), substitute each root for λ, and solve to find the
    associated eigenvectors, the ***v***’s of [Equation 6.5](ch06.xhtml#ch06equ05).
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues of a triangular matrix, which includes diagonal matrices, are
    straightforward to calculate because the determinant of such a matrix is simply
    the product of the main diagonal. For example, for a 4 × 4 triangular matrix,
    the determinant of the characteristic equation is
  prefs: []
  type: TYPE_NORMAL
- en: det(***A*** − λ***I***) = (*a*[00] − λ)(*a*[11] − λ)(*a*[22] − λ)(*a*[33] −
    λ)
  prefs: []
  type: TYPE_NORMAL
- en: 'which has four roots: the values of the diagonal. For triangular and diagonal
    matrices, the entries on the main diagonal *are* the eigenvalues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see a worked eigenvalue example for the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/142equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I selected this matrix to make the math nicer, but the process works for any
    matrix. The characteristic equation means we need the λ values that make the determinant
    zero, as shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/143equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The characteristic polynomial is easily factored to give λ = −1, −2.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, to find the eigenvalues and eigenvectors of a matrix, we use `np.linalg.eig`.
    Let’s check our calculation above to see if NumPy agrees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `np.linalg.eig` function returns a list. The first element is a vector of
    the eigenvalues of the matrix. The second element, which we are ignoring for the
    moment, is a matrix, the *columns* of which are the eigenvectors associated with
    each of the eigenvalues. Note that we also could have used `np.linalg.eigvals`
    to return just the eigenvalues. Regardless, we see that our calculation of the
    eigenvalues of ***A*** is correct.
  prefs: []
  type: TYPE_NORMAL
- en: To find the associated eigenvectors, we put each of the eigenvalues back into
    [Equation 6.5](ch06.xhtml#ch06equ05) and solve for ***v***. For example, for λ
    = −1, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/143equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'which leads to the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*v*[0] + *v*[1] = 0'
  prefs: []
  type: TYPE_NORMAL
- en: −2*v*[0] − 2*v*[1] = 0
  prefs: []
  type: TYPE_NORMAL
- en: This system has many solutions, as long as *v*[0] = −*v*[1]. That means we can
    pick *v*[0] and *v*[1], as long as the relationship between them is preserved.
    Therefore, we have our eigenvector for
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/143equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we repeat this process for λ = −2, we get the relationship between the components
    of ***v*****[2]** to be 2*v*[0] = −*v*[1]. Therefore, we select ![Image](Images/143equ04.jpg)
    as the second eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if NumPy agrees with us. This time, we’ll display the second list
    element returned by `np.linalg.eig`. This is a matrix where the columns of the
    matrix are the eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmm . . . the columns of this matrix do not appear to match our selected eigenvectors.
    But don’t worry—we didn’t make a mistake. Recall that the eigenvectors were not
    uniquely determined, only the relationship between the components was determined.
    If we’d wanted to, we could have selected other values, as long as for one eigenvector
    they were of equal magnitude and opposite sign, and for the other they were in
    the ratio of 2:1 with opposite signs. What NumPy returns is a set of eigenvectors
    that are of unit length. So, to see that our hand calculation is correct, we need
    to make our eigenvectors unit vectors by dividing each component by the square
    root of the sum of the squares of the components. In code, it’s succinct, if a
    bit messy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we see that we’re correct. The unit vector versions of the eigenvectors
    do match the columns of the matrix NumPy returned.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the eigenvectors and eigenvalues of a matrix often when we’re doing
    deep learning. For example, we’ll see them again later in the chapter when we
    investigate principal component analysis. But before we can learn about PCA, we
    need to change focus once again and learn about vector norms and distance metrics
    commonly used in deep learning, especially about the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Norms and Distance Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In common deep learning parlance, people are somewhat sloppy and use the terms
    *norm* and *distance* interchangeably. We can forgive them for doing so; the difference
    between the terms is small in practice, as we’ll see below.
  prefs: []
  type: TYPE_NORMAL
- en: A *vector norm* is a function that maps a vector, real or complex, to some value,
    *x* ∈ ℝ, *x* ≥ 0\. A norm must satisfy some specific properties in a mathematical
    sense, but in practice, not everything that’s called a norm is, in fact, a norm.
    In deep learning, we usually use norms as distances between pairs of vectors.
    In practice, an important property for a distance measure is that the order of
    the inputs doesn’t matter. If *f*(*x*, *y*) is a distance, then *f*(*x*, *y*)
    = *f*(*y*, *x*). Again, this is not rigorously followed; for example, you’ll often
    see the Kullback-Leibler divergence (KL-divergence) used as a distance even though
    this property doesn’t hold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with vector norms and see how we can easily use them as a distance
    measure between vectors. Then we’ll introduce the important concept of a covariance
    matrix, heavily used on its own in deep learning, and see how we can create a
    distance measure from it: the Mahalanobis distance. We’ll end the section by introducing
    the KL-divergence, which we can view as a measure between two discrete probability
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: L-Norms and Distance Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For an *n*-dimensional vector, ***x***, we define the *p*-norm of the vector
    to be
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *p* is a real number. Although we use *p* in the definition, people generally
    refer to these as L*[p]* norms. We saw one of these norms in [Chapter 5](ch05.xhtml#ch05)
    when we defined the magnitude of a vector. In that case, we were calculating the
    *L2-norm*,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/145equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is the square root of the inner product of ***x*** with itself.
  prefs: []
  type: TYPE_NORMAL
- en: The norms we use most often in deep learning are the L2-norm and the *L1-norm*,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/145equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is nothing more than the sum of the absolute values of the components
    of ***x***. Another norm you’ll encounter is the *L**[∞]-norm*,
  prefs: []
  type: TYPE_NORMAL
- en: L[∞] = max |*x**[i]*|
  prefs: []
  type: TYPE_NORMAL
- en: the maximum absolute value of the components of ***x***.
  prefs: []
  type: TYPE_NORMAL
- en: If we replace ***x*** with the difference of two vectors, ***x*** − ***y***,
    we can treat the norms as distance measures between the two vectors. Alternatively,
    we can picture the process as computing the vector norm on the vector that is
    the difference between ***x*** and ***y***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Switching from norm to distance makes a trivial change in [Equation 6.6](ch06.xhtml#ch06equ06):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The L2-distance becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/145equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the *Euclidean distance* between two vectors. The L1-distance is often
    called the *Manhattan distance* (also called *city block distance*, *boxcar distance*,
    or *taxicab distance*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/146equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It’s so named because it corresponds to the length a taxicab would travel on
    the grid of streets in Manhattan. The L[∞]-distance is sometimes known as the
    *Chebyshev distance*.
  prefs: []
  type: TYPE_NORMAL
- en: Norm equations have other uses in deep learning. For example, weight decay,
    used in deep learning as a regularizer, uses the L2-norm of the weights of the
    model to keep the weights from getting too large. The L1-norm of the weights is
    also sometimes used as a regularizer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move now to consider the important concept of a covariance matrix. It
    isn’t a distance metric itself but is used by one, and it will show up again later
    in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If we have a collection of measurements on multiple variables, like a training
    set with feature vectors, we can calculate the variance of the features with respect
    to each other. For example, here’s a matrix of observations of four variables,
    one per row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/146equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In reality, ***X*** is the first five samples from the famous iris dataset.
    For the iris dataset, the features are measurements of the parts of iris flowers
    from three different species. You can load this dataset into NumPy using `sklearn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We could calculate the standard deviation of each of the features, the columns
    of ***A***, but that would only tell us about the variance of the values of that
    feature around its mean. Since we have multiple features, it would be nice to
    know something about how the features in, say, column zero and column one vary
    together. To determine this, we need to calculate the *covariance matrix*. This
    matrix captures the variance of the individual features along the main diagonal.
    Meanwhile, the off-diagonal values represent how one feature varies as another
    varies—these are the covariances. Since there are four features, the covariance
    matrix, which is always square, is, in this case, a 4 × 4 matrix. We find the
    elements of the covariance matrix, Σ, by calculating
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'assuming the rows of the matrix, ***X***, are the observations, and the columns
    of ***X*** represent the different features. The means of the features across
    all rows are ![Image](Images/147equ01.jpg) and ![Image](Images/147equ02.jpg) for
    features *i* and *j*. Here, *n* is the number of observations, the number of rows
    in ***X***. We can see that when *i* = *j*, the covariance value is the normal
    variance for that feature. When *i* ≠ *j*, the value is how *i* and *j* vary together.
    We often denote the covariance matrix as Σ, and it is always symmetric: ∑[*ij*]
    = ∑*[ji]*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate some elements of the covariance matrix for ***X*** above. The
    per-feature means are ![Image](Images/147equ03.jpg). Let’s find the first row
    of Σ. This will tell us the variance of the first feature (column of **X**) and
    how that feature varies with the second, third, and fourth features. Therefore,
    we need to calculate ∑[00], ∑[01], ∑[02], and ∑[03]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/147equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can repeat this calculation for all the rows of Σ to give the complete covariance
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/147equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The elements along the diagonal represent the variance of the features of ***X***.
    Notice that for the fourth feature of ***X***, all the variances and covariances
    are zero. This makes sense because all the values for this feature in ***X***
    are the same; there is no variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the covariance matrix for a set of observations in code by
    using `np.cov`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the call to `np.cov` includes `rowvar=False`. By default, `np.cov`
    expects each row of its argument to be a variable and the columns to be the observations
    of that variable. This is the opposite of the usual way a set of observations
    is typically stored in a matrix for deep learning. Therefore, we use the `rowvar`
    keyword to tell NumPy that the rows, not the columns, are the observations.
  prefs: []
  type: TYPE_NORMAL
- en: I claimed above that the diagonal of the covariance matrix returns the variances
    of the features in ***X***. NumPy has a function, `np.std`, to calculate the standard
    deviation, and squaring the output of this function should give us the variances
    of the features by themselves. For ***X***, we get
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These variances don’t look like the diagonal of the covariance matrix. The difference
    is due to the *n* − 1 in the denominator of the covariance equation, [Equation
    6.8](ch06.xhtml#ch06equ08). By default, `np.std` calculates what is known as a
    biased estimate of the sample variance. This means that instead of dividing by
    *n* − 1, it divides by *n*. To get `np.std` to calculate the unbiased estimator
    of the variance, we need to add the `ddof=1` keyword,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: then we’ll get the same values as along the diagonal of Σ.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to calculate the covariance matrix, let’s use it in a distance
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Mahalanobis Distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Above, we represented a dataset by a matrix where the rows of the dataset are
    observations and the columns are the values of variables that make up each observation.
    In machine learning terms, the rows are the feature vectors. As we saw above,
    we can calculate the mean of each feature across all the observations, and we
    can calculate the covariance matrix. With these values, we can define a distance
    metric called the *Mahalanobis distance*,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ***x*** is a vector, **μ** is the vector formed by the mean values of
    each feature, and Σ is the covariance matrix. Notice that this metric uses the
    *inverse* of the covariance matrix, not the covariance matrix itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 6.9](ch06.xhtml#ch06equ09) is, in some sense, measuring the distance
    between a vector and a distribution with the mean vector **μ**. The dispersion
    of the distribution is captured in Σ. If there is no covariance between the features
    in the dataset and each feature has the same standard deviation, then Σ becomes
    the identity matrix, which is its own inverse. In that case, Σ^(−1) effectively
    drops out of [Equation 6.9](ch06.xhtml#ch06equ09), and the Mahalanobis distance
    becomes the L2-distance (Euclidean distance).'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of the Mahalanobis distance is to replace **μ** with another
    vector, call it ***y***, that comes from the same dataset as ***x***. Then *D*[M]
    is the distance between the two vectors, taking the variance of the dataset into
    account.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the Mahalanobis distance to build a simple classifier. If, given
    a dataset, we calculate the mean feature vector of each class in the dataset (this
    vector is also called the *centroid*), we can use the Mahalanobis distance to
    assign a label to an unknown feature vector, ***x***. We can do so by calculating
    all the Mahalanobis distances to the class centroids and assigning ***x*** to
    the class returning the smallest value. This type of classifier is sometimes called
    a *nearest centroid* classifier, and you’ll often see it implemented using the
    L2-distance in place of the Mahalanobis distance. Arguably, you can expect the
    Mahalanobis distance to be the better metric because it takes the variance of
    the dataset into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the breast cancer dataset included with `sklearn` to build the nearest
    centroid classifier using the Mahalanobis distance. The breast cancer dataset
    has two classes: benign (0) and malignant (1). The dataset contains 569 observations,
    each of which has 30 features derived from histology slides. We’ll build two versions
    of the nearest centroid classifier: one using the Mahalanobis distance and the
    other using the Euclidean distance. Our expectation is that the classifier using
    the Mahalanobis distance will perform better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we need is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We start by importing the modules we need, including `mahalanobis` from SciPy
    ❶. This function accepts two vectors and the inverse of a covariance matrix and
    returns *D*[M]. We get the dataset next in `d` with labels in `l`. We randomize
    the order ❷ and pull out the first 400 observations as training data (`xtrn`)
    with labels (`ytrn`). We hold back the remaining observations for testing (`xtst`,
    `ytst`).
  prefs: []
  type: TYPE_NORMAL
- en: We *train* the model next. Training consists of pulling out all the observations
    belonging to each class ❸ and calculating `m0` and `m1`. These are the mean values
    of each of the 30 features for all class 0 and class 1 observations. We then calculate
    the covariance matrix of the entire training set (`S`) and its inverse (`SI`).
  prefs: []
  type: TYPE_NORMAL
- en: The `score` function takes the test observations, a list of the class mean vectors,
    and the inverse of the covariance matrix. It runs through each test observation
    and calculates the Mahalanobis distances (`d`). It then uses the smallest distance
    to assign the class label (`c`). If the assigned label matches the actual test
    label, we count it (`nc`). At the end of the function, we return the overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We call the `score` function twice. The first call uses the inverse covariance
    matrix (`SI`), while the second call uses an identity matrix, thereby making `score`
    calculate the Euclidean distance instead. Finally, we print both results.
  prefs: []
  type: TYPE_NORMAL
- en: The randomization of the dataset ❷ means that each time the code is run, it
    will output slightly different scores. Running the code 100 times gives the following
    mean scores (± the standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Distance** | **Mean score** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mahalanobis | 0.9595 ± 0.0142 |'
  prefs: []
  type: TYPE_TB
- en: '| Euclidean | 0.8914 ± 0.0185 |'
  prefs: []
  type: TYPE_TB
- en: This clearly shows that using the Mahalanobis distance leads to better model
    performance, with about a 7 percent improvement in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: One recent use of the Mahalanobis distance in deep learning is to take the top-level
    embedding layer values, a vector, and use the Mahalanobis distance to detect out-of-domain
    or adversarial inputs. An *out-of-domain input* is one that is significantly different
    from the type of data the model was trained to use. An *adversarial input* is
    one where an adversary is deliberately attempting to fool the model by supplying
    an input that isn’t of class X but that the model will label as class X.
  prefs: []
  type: TYPE_NORMAL
- en: Kullback-Leibler Divergence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *Kullback-Leibler divergence (KL-divergence)*, or *relative entropy*, is
    a measure of the similarity between two probability distributions: the lower the
    value, the more similar the distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: If *P* and *Q* are discrete probability distributions, the KL-divergence is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/151equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where log[2] is the logarithm base-2\. This is an information-theoretic measure;
    the output is in bits of information. Sometimes the natural log, ln, is used,
    in which case the measure is said to be in *nats*. The SciPy function that implements
    the KL-divergence is in `scipy.special` as `rel_entr`. Note that `rel_entr` uses
    the natural log, not log base-2\. Note also that the KL-divergence isn’t a distance
    metric in the mathematical sense because it violates the symmetry property, *D*[KL](*P**||Q*)
    ≠ *D*[KL](*Q*||*P*), but that doesn’t stop people from using it as one from time
    to time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see an example of how we might use the KL-divergence to measure between
    different discrete probability distributions. We’ll measure the divergence between
    two different binomial distributions and a uniform distribution. Then, we’ll plot
    the distributions to see if, visually, we believe the numbers.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the distributions, we’ll take many draws from a uniform distribution
    with 12 possible outputs. We can do this quickly in code by using `np.random.randint`.
    Then, we’ll take draws from two different binomial distributions, *B*(12, 0.4)
    and *B*(12, 0.9), meaning 12 trials with probabilities of 0.4 and 0.9 per trial.
    We’ll generate histograms of the resulting draws, divide by the sum of the counts,
    and use the rescaled histograms as our probability distributions. We can then
    measure the divergences between them.
  prefs: []
  type: TYPE_NORMAL
- en: The code we need is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We load `rel_entr` from SciPy and set the number of draws for each distribution
    to 1,000,000 (`N`). The code to generate the respective probability distributions
    follows the same method for each distribution. We draw `N` samples from the distribution,
    starting with the uniform ❶. We use `randint` because it returns integers in the
    range [0, 12] so we can match the discrete [0, 12] values that `binomial` returns
    for 12 trials. We get the histogram from the vector of draws by using `np.bincount`.
    This function counts the frequency of unique values in a vector ❷. Finally, we
    change the counts into fractions by dividing the histogram by the sum ❸. This
    gives us a 12-element vector in `p` representing the probability that `randint`
    will return the values 0 through 12\. Assuming `randint` uses a good pseudorandom
    number generator, we expect the probabilities to be roughly equal for each value
    in `p`. (NumPy uses the Mersenne Twister pseudorandom number generator, one of
    the better ones out there, so we’re confident that we’ll get good results.)
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process, substituting `binomial` for `randint`, sampling from
    binomial distributions using probabilities of 0.9 and 0.4\. Again, histogramming
    the draws and converting the counts to fractions gives us the remaining probability
    distributions, `q` and `w`, based on 0.9 and 0.4, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We are finally ready to measure the divergence. The `rel_entr` function is a
    bit different from other functions in that it does not return *D*[KL] directly.
    Instead, it returns a vector of the same length as its arguments, where each element
    of the vector is part of the overall sum leading to *D*[KL]. Therefore, to get
    the actual divergence number, we need to add the elements of this vector. So,
    we print the sum of the output of `rel_entr`, comparing the two binomial distributions
    to the uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The random nature of the draws means we’ll get slightly different numbers each
    time we run the code. One run gave
  prefs: []
  type: TYPE_NORMAL
- en: '| **Distributions** | **Divergence** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *D*[KL](*Q**&#124;&#124;P*) | 1.1826 |'
  prefs: []
  type: TYPE_TB
- en: '| *D*[KL](*W&#124;&#124;P*) | 0.6218 |'
  prefs: []
  type: TYPE_TB
- en: This shows that the binomial distribution with probability 0.9 diverges more
    from a uniform distribution than the binomial distribution with probability 0.4\.
    Recall, the smaller the divergence, the closer the two probability distributions
    are to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Do we believe this result? One way to check is visually, by plotting the three
    distributions and seeing if *B*(12, 0.4) looks more like a uniform distribution
    than *B*(12, 0.9) does. This leads to [Figure 6-1](ch06.xhtml#ch06fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/06fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-1: Three different, discrete probability distributions: uniform (forward
    hash),* B(*12,0.4) (backward hash), and* B(*12,0.9) (horizontal hash)*'
  prefs: []
  type: TYPE_NORMAL
- en: Although it is clear that neither binomial distribution is particularly uniform,
    the *B*(12, 0.4) distribution is relatively centered in the range and spread across
    more values than the *B*(12, 0.9) distribution is. It seems reasonable to think
    of *B*(12, 0.4) as more like the uniform distribution, which is precisely what
    the KL-divergence told us by returning a smaller value.
  prefs: []
  type: TYPE_NORMAL
- en: We now have everything we need to implement principal component analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assume we have a matrix, ***X***, representing a dataset. We understand that
    the variance of each of the features need not be the same. If we think of each
    observation as a point in an *n*-dimensional space, where *n* is the number of
    features in each observation, we can imagine a cloud of points with a different
    amount of scatter in different directions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Principal component analysis (PCA)* is a technique to learn the directions
    of the scatter in the dataset, starting with the direction aligned along the greatest
    scatter. This direction is called the *principal component*. You then find the
    remaining components in order of decreasing scatter, with each new component orthogonal
    to all the others. The top part of [Figure 6-2](ch06.xhtml#ch06fig02) shows a
    2D dataset and two arrows. Without knowing anything about the dataset, we can
    see that the largest arrow points along the direction of the greatest scatter.
    This is what we mean by the principal component.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/06fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-2: The first two features of the iris dataset and principal component
    directions (top), and the iris dataset after transformation by PCA (bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: We often use PCA to reduce the dimensionality of a dataset. If there are 100
    variables per observation, but the first two principal components explain 95 percent
    of the scatter in the data, then mapping the dataset along those two components
    and discarding the remaining 98 components might adequately characterize the dataset
    with only two variables. We can use PCA to augment a dataset as well, assuming
    continuous features.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how does PCA work? All this talk about the scatter of the data implies
    that PCA might be able to make use of the covariance matrix, and, indeed, it does.
    We can break the PCA algorithm down into a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean center the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the covariance matrix, Σ, of the mean-centered data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvalues and eigenvectors of Σ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvalues by decreasing absolute value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discard the weakest eigenvalues/eigenvectors (optional).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Form a transformation matrix, ***W***, using the remaining eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate new transformed values from the existing dataset, ***x***′ = ***Wx***.
    These are sometimes referred to as *derived variables*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s work through an example of this process using the iris dataset ([Listing
    6-1](ch06.xhtml#ch06ex01)). We’ll reduce the dimensionality of the data from four
    features to two. First the code, then the explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-1: Principal component analysis (PCA)*'
  prefs: []
  type: TYPE_NORMAL
- en: We start by loading the iris dataset, courtesy of `sklearn`. This gives us `iris`
    as a 150 × 4 matrix, since there are 150 observations, each with four features.
    We calculate the mean value of each feature ❶ and subtract it from the dataset,
    relying on NumPy’s broadcasting rules to subtract `m` from each row of `iris`.
    We’ll work with the mean-centered matrix `ir` going forward.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to compute the covariance matrix ❷. The output, `cv`, is a
    4 × 4 matrix, since we have four features per observation. We follow this by calculating
    the eigenvalues and eigenvectors of `cv` ❸ and then take the absolute value of
    the eigenvalues to get the magnitude. We want the eigenvalues in decreasing order
    of magnitude, so we get the indices that sort them that way ❹ using the Python
    idiom of `[::-1]` to reverse the order of a list or array.
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude of the eigenvalues is proportional to the fraction of the variance
    in the dataset along each principal component; therefore, if we scale the eigenvalues
    by their overall sum, we get the proportion explained by each principal component
    (`ex`). The fraction of variance explained is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: indicating that two principal components explain nearly 98 percent of the variance
    in the iris dataset. Therefore, we’ll only keep the first two principal components
    going forward.
  prefs: []
  type: TYPE_NORMAL
- en: We create the transformation matrix, `w`, from the eigenvectors that go with
    the two largest eigenvalues ❺. Recall, `eig` returns the eigenvectors as the columns
    of the matrix `vec`. The transformation matrix, `w`, is a 2 × 4 matrix because
    it maps a four-component feature vector to a new two-component vector.
  prefs: []
  type: TYPE_NORMAL
- en: All that’s left is to create a place to hold the transformed observations and
    fill them in ❻. The new, reduced-dimension dataset is in `d`. We can now plot
    the entire transformed dataset, labeling each point by the class to which it belongs.
    The result is the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02).
  prefs: []
  type: TYPE_NORMAL
- en: In the top part of [Figure 6-2](ch06.xhtml#ch06fig02) is a plot of the original
    dataset using only the first two features. The arrows indicate the first two principal
    components, and the size of the arrows shows how much of the variance in the data
    these components explain. The first component explains most of the variance, which
    makes sense visually.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the derived variables in the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02)
    have made the dataset easier to work with, as the classes are better separated
    than on the top using only two of the original features. Sometimes, PCA makes
    it easier for a model to learn because of the reduced feature vector size. However,
    this is not always the case. During PCA, you may lose a critical feature allowing
    class separation. As with most things in machine learning, experimentation is
    vital.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is commonly used and is therefore well supported in multiple tool-kits.
    Instead of the dozen or so lines of code we used above, we can accomplish the
    same thing by using the `PCA` class from the `sklearn.decomposition` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The new, reduced-dimension dataset is in `d`. Like other `sklearn` classes,
    after we tell `PCA` how many components we want it to learn, it uses `fit` to
    set up the transformation matrix (`w` in [Listing 6-1](ch06.xhtml#ch06ex01)).
    We then apply the transform by calling `fit_transform`.
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition and Pseudoinverse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll end this chapter with an introduction to *singular value decomposition
    (SVD)*. This is a powerful technique to factor any matrix into the product of
    three matrices, each with special properties. The derivation of SVD is beyond
    the scope of this book. I trust motivated readers to dig into the vast literature
    on linear algebra to locate a satisfactory presentation of where SVD comes from
    and how it is best understood. Our goal is more modest: to become familiar with
    the mathematics found in deep learning. Therefore, we’ll content ourselves with
    the definition of SVD, some idea of what it gives us, some of its uses, and how
    to work with it in Python. For deep learning, you’ll most likely encounter SVD
    when calculating the pseudoinverse of a nonsquare matrix. We’ll also see how that
    works in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: The output of SVD for an input matrix, ***A***, with real elements and shape
    *m* × *n*, where *m* does not necessarily equal *n* (though it could) is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '***A*** has been decomposed into three matrices: ***U***, Σ, and ***V***. Note
    that you might sometimes see ***V***^⊤ written as ***V***^*, the conjugate transpose
    of ***V***. This is the more general form that works with complex-valued matrices.
    We’ll restrict ourselves to real-valued matrices, so we only need the ordinary
    matrix transpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVD of an *m* × *n* matrix, ***A***, returns the following: ***U***, which
    is *m* × *m* and orthogonal; Σ, which is *m* × *n* and diagonal; and ***V***,
    which is *n* × *n* and orthogonal. Recall that the transpose of an orthogonal
    matrix is its inverse, so ***UU***^⊤ = ***I****[m]* and ***VV***^⊤ = ***I****[n]*,
    where the subscript on the identity matrix gives the order of the matrix, *m*
    × *m* or *n* × *n*.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point in the chapter, you may have raised an eyebrow at the statement
    “Σ, which is *m* × *n* and diagonal,” since we’ve only considered square matrices
    to be diagonal. Here, when we say *diagonal*, we mean a *rectangular diagonal
    matrix*. This is the natural extension to a diagonal matrix, where the elements
    of what would be the diagonal are nonzero and all others are zero. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/157equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is a 3 × 5 rectangular diagonal matrix because only the main diagonal is nonzero.
    The “singular” in “singular value decomposition” comes from the fact that the
    elements of the diagonal matrix, Σ, are the singular values, the square roots
    of the positive eigenvalues of the matrix ***A**^T**A***.
  prefs: []
  type: TYPE_NORMAL
- en: SVD in Action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s be explicit and use SVD to decompose a matrix. Our test matrix is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/158equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We’ll show SVD in action as a series of steps. To get the SVD, we use `svd`
    from `scipy.linalg`,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'where `u` is ***U***, `vt` is ***V***^⊤, and `s` contains the singular values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check that the singular values are indeed the square roots of the positive
    eigenvalues of ***A**^T**A***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that, yes, 5 and 3 are the square roots of 25 and 9\. Recall
    that `eig` returns a list, the first element of which is a vector of the eigenvalues.
    Also note that there is a third eigenvalue: zero. You might ask: “How small a
    numeric value should we interpret as zero?” That’s a good question with no hard
    and fast answer. Typically, I interpret values below 10^(−9) to be zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The claim of SVD is that ***U*** and ***V*** are unitary matrices. If so, their
    products with their transposes should be the identity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Given the comment above about numeric values that we should interpret as zero,
    this is indeed the identity matrix. Notice that `svd` returned ***V***^⊤, not
    ***V***. However, since (***V***^⊤)^⊤ = ***V***, we’re still multiplying ***V***^⊤***V***.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `svd` function returns not Σ but the diagonal values of Σ. Therefore, let’s
    reconstruct Σ and use it to see that SVD works, meaning we can use ***U***, Σ,
    and ***V***^⊤ to recover ***A***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the ***A*** we started with—almost: the recovered ***A*** is no longer
    of integer type, a subtle change worth remembering when writing code.'
  prefs: []
  type: TYPE_NORMAL
- en: Two Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SVD is a cute trick, but what can we do with it? The short answer is “a lot.”
    Let’s see two applications. The first is using SVD for PCA. The `sklearn PCA`
    class we used in the previous section uses SVD under the hood. The second example
    shows up in deep learning: using SVD to calculate the Moore-Penrose pseudoinverse,
    a generalization of the inverse of a square matrix to *m* × *n* matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: SVD for PCA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To see how to use SVD for PCA, let’s use the iris data from the previous section
    so we can compare with those results. The key is to truncate the Σ and ***V***^⊤
    matrices to keep only the desired number of largest singular values. The decomposition
    code will put the singular values in decreasing order along the diagonal of Σ
    for us, we need only retain the first *k* columns of Σ. In code, then,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’re using `ir` from [Listing 6-1](ch06.xhtml#ch06ex01). This is the
    mean-centered version of the iris dataset matrix, with 150 rows of four features
    each. A call to `svd` gives us the decomposition of `ir`. The next three lines
    ❶ create the full Σ matrix in `S`. Because the iris dataset has four features,
    the `s` vector that `svd` returns will have four singular values.
  prefs: []
  type: TYPE_NORMAL
- en: The truncation comes by keeping the first two columns of `S` ❷. Doing this changes
    Σ from a 150 × 4 matrix to a 150 × 2 matrix. Multiplying ***U*** by the new Σ
    gives us the transformed iris dataset. Since ***U*** is 150 × 150 and Σ is 150
    × 2, we get a 150 × 2 dataset in `T`. If we plot this as `T[:,0]` versus `T[:,1]`,
    we get the exact same plot as the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02).
  prefs: []
  type: TYPE_NORMAL
- en: The Moore-Penrose Pseudoinverse
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As promised, our second application is to compute ***A***^+, the Moore-Penrose
    pseudoinverse of an *m* × *n* matrix ***A***. The matrix ***A***^+ is called a
    pseudo-inverse because, in conjunction with ***A***, it acts like an inverse in
    that
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/06equ11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ***AA***^+ is somewhat like the identity matrix, making ***A***^+ somewhat
    like the inverse of ***A***.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that the pseudoinverse of a rectangular diagonal matrix is simply the
    reciprocal of the diagonal values, leaving zeros as zero, followed by a transpose,
    we can calculate the pseudoinverse of any general matrix as
  prefs: []
  type: TYPE_NORMAL
- en: '***A***^+ = ***V***Σ^+ ***U****'
  prefs: []
  type: TYPE_NORMAL
- en: for ***A*** = ***U***Σ***V****, the SVD of ***A***. Notice, we’re using the
    conjugate transpose, ***V***^*, instead of the ordinary transpose, ***V***^⊤.
    If ***A*** is real, then the ordinary transpose is the same as the conjugate transpose.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if the claim regarding ***A***^+ is true. We’ll start with the ***A***
    matrix we used in the section above, compute the SVD, and use the parts to find
    the pseudoinverse. Finally, we’ll validate [Equation 6.11](ch06.xhtml#ch06equ11).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with ***A***, the same array we used above for the SVD example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying SVD will give us ***U*** and ***V***^⊤ along with the diagonal of
    Σ. We’ll use the diagonal elements to construct Σ^+ by hand. Recall, Σ^+ is the
    transpose of Σ, where the diagonal elements that are not zero are changed to their
    reciprocals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can calculate ***A***^+ and verify that ***AA***^+***A*** = ***A***:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And, in this case, ***AA***^+ is the identity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our whirlwind look at SVD and our discussion of linear algebra.
    We barely scratched the surface, but we’ve covered what we need to know.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This heavy chapter and [Chapter 5](ch05.xhtml#ch05) before it plowed through
    a lot of linear algebra. As a mathematical topic, linear algebra is vastly richer
    than our presentation here.
  prefs: []
  type: TYPE_NORMAL
- en: We focused the chapter on square matrices, as they have a special place in linear
    algebra. Specifically, we discussed general properties of square matrices, with
    examples. We learned about eigenvalues and eigenvectors, how to find them, and
    why they are useful. Next, we looked at vector norms and other ways to measure
    distance, as they show up often in deep learning. Finally, we ended the chapter
    by learning what PCA is and how it works, followed by a look at singular value
    decomposition, with two applications relevant to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter shifts gears and covers differential calculus. This is, fortunately,
    the “easy” part of calculus, and is, in general, all that we need to understand
    the algorithms specific to deep learning. So, fasten your seat belts, make sure
    your arms and legs are fully within the vehicle, and prepare for departure to
    the world of differential calculus.
  prefs: []
  type: TYPE_NORMAL
