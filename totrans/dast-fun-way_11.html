<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_187" title="187"/>11</span><br/>
<span class="ChapterTitle">Caches</span></h1>
</header>
<figure class="opener">
<img alt="" height="99" src="image_fi/book_art/chapterart.png" width="98"/>
</figure>
<p class="ChapterIntro">This chapter introduces <em>caches</em>, data structures that seek to alleviate high data access costs by storing some data closer to the computation. As we have seen throughout the previous chapters, the cost of data access is a critical factor in the efficiency of our algorithms. This is important not just to how we organize the data in storage but also to the type of storage we use. We say data is <em>local</em> when it is stored closer to the processor and thus quickly available for processing. If we copy some of the data from more expensive, distant locations to local storage, we can read the data significantly faster.</p>
<p>For example, we might use caches to accelerate access to web pages. Loading a web page consists of querying a server for the information contained on the page, transferring that data to your local computer, and rendering this information visually. If the web page contains large elements, such as images or videos, we may need to transfer a lot of data. To reduce this cost, browsers cache commonly accessed data. Instead of <span epub:type="pagebreak" id="Page_188" title="188"/>redownloading the logo of our favorite site every time we access the page, the browser keeps this image in a local cache on the computer’s hard drive so that it can quickly read it from disk.</p>
<p>How should we choose which data to load into our cache? Obviously, we can’t store everything. If we could, we wouldn’t need the cache in the first place—we’d just copy the entire data set into the closest memory. There are a range of caching strategies whose effectiveness depends, as always, on the use case. In this chapter, we combine data structures we’ve previously explored to build one such strategy, the least recently used (LRU) cache, which can greatly facilitate the operations of our favorite coffee shop. We also briefly discuss a few other caching strategies for comparison.</p>
<h2 id="h1-502604c11-0001">Introducing Caches</h2>
<p class="BodyFirst">Up to this point, we’ve treated all of the computer’s storage equally, a single bookshelf from which we can grab any item we need with approximately the same amount of effort. However, data storage is not this straightforward. We can think of the storage as being like a large, multistory library. We have shelves of popular books arranged near the entrance to satisfy the majority of patrons, while we relegate the musty stacks of old computer science journals to the basement. We might even have a warehouse offsite where rarely used books are stored until a patron specifically requests them. Want that resource on the PILOT programming language? Odds are, it won’t be in the popular picks section. </p>
<p>In addition to paying attention to how we organize data within our program, we need to consider <em>where</em> the data is stored. Not all data storage is equal. In fact, for different mediums, there is often a tradeoff among the size of storage available, its speed, and its cost. Memory on the CPU itself (registers or local caches) is incredibly fast but can only hold a very limited amount of data. A computer’s random-access memory (RAM) provides a larger space at slower speeds. Hard drives are significantly larger, but also slower than RAM. Calling out to a network allows access to a huge range of storage, such as the whole internet, but incurs corresponding overhead. When dealing with very large data sets, it might not be possible to load the entirety of the data into memory, which can have dramatic impact on the algorithms’ performance. It’s important to understand how these tradeoffs impact the performance of algorithms.</p>
<p>Compare this to our own morning coffee-making routine. While it would be ideal to have thousands of varieties at our fingertips, our apartment can store only a limited amount of coffee. Down the street is a coffee distributor stocked with hundreds of varieties, but do we really want to walk there every time we want to make a new cup of coffee? Instead, we store small quantities of our preferred coffees in our apartment. This local storage speeds up the production of our morning brew and lets us enjoy our first cup before venturing out into the world.</p>
<p>Caches are a step before accessing expensive data, as shown in <a href="#figure11-1" id="figureanchor11-1">Figure 11-1</a>. Before calling out to a remote server or even accessing our <span epub:type="pagebreak" id="Page_189" title="189"/>local hard drive, we check whether we already have the data stored locally in the cache. If we do, we access the data directly, and cheaply, from the cache. When we find the data in the cache, that’s a <em>cache hit</em>. We can stop the lookup and avoid calling out to the more expensive storage. If the data isn’t in the cache, we have a <em>cache miss</em>. We give a defeated sigh and proceed with the more expensive access.</p>
<figure>
<img alt="The cache sits between the algorithm and expensive data store. Arrows labeled “query” point from the algorithm to the cache and from the cache to the data store. Arrows labeled “reply” point the other direction." class="" height="117" src="image_fi/502604c11/f11001.png" width="490"/>
<figcaption><p><a id="figure11-1">Figure 11-1</a>: A cache sits between the algorithm and a slow, expensive data store.</p></figcaption>
</figure>
<p>Picture this in the context of a busy coffee counter. The daily menu contains 10 coffees ranging in popularity from the mega-popular house brew to the rarely purchased mint-cinnamon-pumpkin explosion. At the far end of the counter sits a coffee station with 10 pots of coffee, each on its own heater. Upon receiving an order, the barista walks to the coffee station and fills a cup with the appropriate coffee. After miles of walking each day, the barista asks the owner to install another heater right beside the register so they can give their tired feet a rest. The owner agrees but asks, “Which coffee will you keep near the register?” This question weighs on the barista’s mind and they try a few strategies. </p>
<p>The barista quickly realizes that it isn’t just a question of which coffee, but also when they should change it. They can store different coffees locally at different points of the day. They try keeping the house brew in the cache all day, moving the decaf closer at night, and swapping out the current selection for the house brew when there hasn’t been a customer in the last 10 minutes. Some strategies work and others, especially anything involving caching mint-cinnamon-pumpkin explosion, fail miserably. If the barista chooses correctly, most orders result in a cache hit, and they can avoid the long walk. If they choose incorrectly, the local coffee is useless for most orders. </p>
<p>The barista could further improve the situation by installing a second heater by the register. Instead of storing a single flavor nearby, they can now store two. The cost, however, is that this second heater uses up precious counter real estate. Both computers and coffee shops have a limited amount of local space. We cannot store every coffee next to the register, and we cannot store the entire internet in our computer’s RAM. If we make the cache too large, we will need to use slower storage for it. This is the fundamental tradeoff with a cache—memory used versus the speedup provided.</p>
<p>When the cache fills up, we determine which data to keep and which data to replace. We say the replaced data is <em>evicted</em> from the cache. Perhaps the triple-caffeinated blend replaces the decaf during a fall morning rush. By evicting the decaf from the nearby heater, the barista saves hours of walking to retrieve the trendier blend. Different caching approaches use <span epub:type="pagebreak" id="Page_190" title="190"/>different eviction strategies to determine which data to replace, from counting how often the data has been accessed thus far to making predictions about whether the data will be used again in the near future.</p>
<h2 id="h1-502604c11-0002">LRU Eviction and Caches</h2>
<p class="BodyFirst">The <em>least recently used (LRU) cache</em> stores recently used information. LRU caches are a simple and common approach that provide a good illustration of the types of tradeoffs we face in using caches. The intuition behind this caching strategy is that we are likely to re-access information that we recently needed, which fits well with our web-browsing example above. If we commonly access the same sets of pages over and over, it pays to store some of the (unchanging) elements of these pages locally. We don’t need to re-request the site’s logo with each visit. LRU caches consist of a set amount of storage filled with the most recently accessed items. The cache starts evicting older items—those least recently used—when the cache is full. </p>
<p>If our barista decides to treat the heater closest to the register as an LRU cache, they keep the last ordered coffee on that heater. When they receive an order for something different, they take the current cached coffee back to the coffee station, place it on the heater there, and retrieve the new coffee. They bring this new coffee back to the register, use it to fulfill the order, and leave it there.</p>
<p>If every customer orders something different, this strategy just adds overhead. Instead of walking to the end of the counter with a cup, the barista is now making the same journey with a pot of coffee—probably complaining under their breadth about the length of the counter or resentfully judging customers’ tastes: “Who orders mint-cinnamon-pumpkin explosion?” However, if most customers order in clumps of similar coffees, this strategy can be hugely effective. Three customers ordering regular coffee in a row means two round trips saved. The advantage can compound with the effect of customer interaction. After seeing the person in front of them order the pumpkin concoction, the next customer makes the (questionable) decision to copy their predecessor, saying, “I’ll have the same.” </p>
<p>This is exactly the scenario we can run into when browsing our favorite website. We move from one page of the site to another page of the same site. Certain elements, such as logos, will reappear on subsequent pages, leading to a significant savings from keeping those items in the cache.</p>
<h3 id="h2-502604c11-0001">Building an LRU Cache</h3>
<p class="BodyFirst">Caching with the LRU eviction policy requires us to support two operations: looking up an arbitrary element and finding the least recently used element (for eviction). We must be able to perform both operations quickly. <span epub:type="pagebreak" id="Page_191" title="191"/>After all, the purpose of a cache is to accelerate lookups. If we have to scan through an entire unstructured data set to check for a cache hit, we are more likely to add overhead than to save time.</p>
<p>We can construct an LRU cache using two previously explored components: a hash table and a queue. The hash table allows us to perform fast lookups, efficiently retrieving any items that are in the cache. The queue is a FIFO data structure that allows us to track which item hasn’t been used recently. Instead of scanning through each item and checking the timestamp, we can dequeue the <em>earliest</em> item in the queue, as shown in the following code. This provides an efficient way to determine which data to remove.</p>
<pre><code>LRUCache {
    HashTable: ht
    Queue: q
    Integer: max_size
    Integer: current_size
 }</code></pre>
<p>Each entry in the hash table stores a composite data structure containing at least three entries: the key, the corresponding value (or data for that entry), and a pointer to the corresponding node in the cache’s queue. This last piece of information is essential, because we need a way to look up and modify entries in the queue. As in <a href="#listing11-1" id="listinganchor11-1">Listing 11-1</a>, we store those pieces of information directly in the value entry of the hash table’s nodes.</p>
<pre><code>CacheEntry {
    Type: key
    Type: value
    QueueListNode: node
}</code></pre>
<p class="CodeListingCaption"><a id="listing11-1">Listing 11-1</a>:  The data structure for a cache entry containing the data’s key, value, and a link to its node in the queue</p>
<p><a href="#figure11-2" id="figureanchor11-2">Figure 11-2</a> shows the resulting diagram of how these pieces fit together. The diagram looks a bit complex but becomes easier to understand when we break it down into two parts. On the left is the hash table. As in <span class="xref" itemid="xref_target_Chapter 10">Chapter 10</span>, each hash value corresponds to a linked list of entries. The value for these entries is the <code>CacheEntry</code> data structure in <a href="#listing11-1">Listing 11-1</a>. On the right-hand side is the queue data structure, which stores the entry’s keys. A single pointer links across these data structures, pointing from the <code>CacheEntry</code> data structure to the queue node with the corresponding key. </p>
<p>Of course, the real-world layout of the components in <a href="#figure11-2">Figure 11-2</a> throughout the computer’s memory is even messier than it appears in the diagram, because the nodes do not actually reside next to each other.</p>
<span epub:type="pagebreak" id="Page_192" title="192"/><figure>
<img alt="A hash table on the left includes an entry for each cache entry with pointers to an additional CacheEntry node. Each of the CacheEntry nodes has a single pointer that links to an entry in the queue on the right." class="" height="549" src="image_fi/502604c11/f11002.png" width="694"/>
<figcaption><p><a id="figure11-2">Figure 11-2</a>: An LRU cache implemented as a combination of a hash table and queue</p></figcaption>
</figure>
<p>In <a href="#listing11-2" id="listinganchor11-2">Listing 11-2</a>, we define a single lookup function, <code>CacheLookup</code>, that returns the value for a given lookup key. If there is a cache hit, the lookup function returns the value directly and updates how recently that data was accessed. If there is a cache miss, the function fetches the data via the expensive lookup, inserts it into the cache, and, if necessary, removes the oldest piece of data.</p>
<pre><code>CacheLookup(LRUCache: cache, Type: key):
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> CacheEntry: entry = HashTableLookup(cache.ht, key)

    IF entry == null:
      <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> IF cache.current_size &gt;= cache.max_size:
            Type: key_to_remove = Dequeue(cache.q)
            HashTableRemove(cache.ht, key_to_remove)
            cache.current_size = cache.current_size - 1

      <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> Type: data = retrieve data for the key from 
                     the slow data source.

      <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> Enqueue(cache.q, key)
        entry = CacheEntry(key, data, cache.q.back)
      <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> HashTableInsert(cache.ht, key, entry)
        cache.current_size = cache.current_size + 1
    ELSE:
        # Reset this key's location in the queue.
<span epub:type="pagebreak" id="Page_193" title="193"/>      <span aria-label="annotation6" class="CodeAnnotationCode">❻</span> RemoveNode(cache.q, entry.node)
      <span aria-label="annotation7" class="CodeAnnotationCode">❼</span> Enqueue(cache.q, key)

        # Update the CacheEntry's pointer.
      <span aria-label="annotation8" class="CodeAnnotationCode">❽</span> entry.node = cache.q.back
    return entry.value</code></pre>
<p class="CodeListingCaption"><a id="listing11-2">Listing 11-2</a>: Code for looking up an item by its key</p>
<p>This code starts by checking whether <code>key</code> occurs in our cache table <span aria-label="annotation1" class="CodeAnnotation">❶</span>. If so, we have a cache hit. Otherwise, we have a cache miss. </p>
<p>We deal with the case of a cache miss first. If the hash table returns <code>null</code>, we need to fetch the data from the more expensive data store. We store this newly retrieved value in the cache, evicting the (oldest) item from the front of the queue. We do this in three steps. First, if the cache is full <span aria-label="annotation2" class="CodeAnnotation">❷</span>, we dequeue the key of the oldest item from the queue and use it to remove the corresponding entry in the hash table. This completes the eviction of the oldest item. Second, we retrieve the new data <span aria-label="annotation3" class="CodeAnnotation">❸</span>. Third, we insert the (<code>key</code>, <code>data</code>) pair into the cache. We enqueue <code>key</code> into the back of the queue <span aria-label="annotation4" class="CodeAnnotation">❹</span>. Then we create a new hash table entry with the new <code>key</code>, <code>data</code>, and pointer to the key’s corresponding location in the queue (using the queue’s <code>back</code> pointer). We store this entry in the hash table <span aria-label="annotation5" class="CodeAnnotation">❺</span>.</p>
<p>The final block of code handles the case of a cache hit. When we see a cache hit, we want to move the element for this key from its current position to the back of the queue. After all, we’ve just seen it. It should now be the last element we want to discard. We move this element in two steps. First, we remove it from the queue with the <code>RemoveNode</code> function, using the pointer to the node <span aria-label="annotation6" class="CodeAnnotation">❻</span>. Second, we re-enqueue the key at the back of the queue <span aria-label="annotation7" class="CodeAnnotation">❼</span> and update the pointer to that queue node <span aria-label="annotation8" class="CodeAnnotation">❽</span>. </p>
<p>We can picture this update operation in the context of a line of customers in a coffee shop. If a customer leaves the line, they lose their position. When they rejoin the line in the future, they do so at the back.</p>
<h3 id="h2-502604c11-0002">Updating an Element’s Recency </h3>
<p class="BodyFirst">In order to support the <code>RemoveNode</code> operation in <a href="#listing11-2">Listing 11-2</a>, we need to change our queue to support <em>updating</em> an element’s position. We need to move recently accessed items from their current position in the queue to the back in order to indicate that they were the most recently accessed. First, we modify our queue implementation from <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span> to use a doubly linked list, which will allow us to efficiently remove items from the middle of the queue: </p>
<pre><code>QueueListNode {
    Type: value
    QueueListNode: next
    QueueListNode: prev
}</code></pre>
<p><span epub:type="pagebreak" id="Page_194" title="194"/>As illustrated in <a href="#figure11-3" id="figureanchor11-3">Figure 11-3</a>, the <code>next</code> field refers to the node directly behind the current node (the next node after this one to be dequeued), where the <code>prev</code> field refers to the node preceding this one.</p>
<figure>
<img alt="A queue with elements 31, 41, 5, 92, and 65. Each node contains a single number and links to both the next and previous number in the queue." class="" height="110" src="image_fi/502604c11/f11003.png" width="522"/>
<figcaption><p><a id="figure11-3">Figure 11-3</a>: A queue implemented as a doubly linked list</p></figcaption>
</figure>
<p>Second, we modify the enqueue and dequeue operations accordingly. For both enqueue and dequeue, we add extra logic to update the previous pointers. The primary change from <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span> is in how we set the <code>prev</code> pointer:</p>
<pre><code>Enqueue(Queue: q, Type: value):
    QueueListNode: node = QueueListNode(value)
    IF q.back == null:
        q.front = node
        q.back = node
    ELSE:
        q.back.next = node
      <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> node.prev = q.back
        q.back = node</code></pre>
<p class="BodyContinued">In the enqueue operation, the code needs to set the new node’s <code>prev</code> pointer to the previous last element <code>q.back</code> <span aria-label="annotation1" class="CodeAnnotation">❶</span>.</p>
<pre><code>Dequeue(Queue: q):
    IF q.front == null:
        return null

    Type: value = q.front.value
    q.front = q.front.next
    IF q.front == null:
        q.back = null
    ELSE:
      <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> q.front.prev = null
    return value</code></pre>
<p class="BodyContinued">The dequeue operation sets the <code>prev</code> pointer of the new front node to <code>null</code> to indicate that there is no node in front of it <span aria-label="annotation1" class="CodeAnnotation">❶</span>.</p>
<p>Finally, we add the <code>RemoveNode</code> operation, which provides the ability to remove a node from the middle of our queue. This is where the use of a doubly linked list helps. By keeping pointers in both directions, we do not need to scan the entire queue to find the entry before the current one. The code for <code>RemoveNode</code> adjusts the pointers into the node from the adjacent nodes in the linked list (<code>node.prev</code> and <code>node.next</code>), the front of the queue (<code>q.front</code>), and the back of the queue (<code>q.back</code>).</p>
<pre><code><span epub:type="pagebreak" id="Page_195" title="195"/>RemoveNode(Queue: q, QueueListNode: node):
    IF node.prev != null:
        node.prev.next = node.next
    IF node.next != null:
        node.next.prev = node.prev
    IF node == q.front:
       q.front = q.front.next
    IF node == q.back:
       q.back = q.back.prev</code></pre>
<p class="BodyContinued">This code contains multiple <code>IF</code> statements to handle the special cases of adding or removing from either end of the queue.</p>
<p>One question remains: How can we efficiently find the element we want to remove and reinsert? We can’t afford to scan through the entire queue while searching for the node to update. Supporting a cache hit requires speedy lookups. In the case of our cache above, we maintain a pointer directly from the cache’s entry to the element in the queue. This allows us to access, remove, and reinsert the element in constant time by following the pointers. </p>
<h2 id="h1-502604c11-0003">Other Eviction Strategies</h2>
<p class="BodyFirst">Let’s consider how three of the many alternate eviction strategies—most recently used, least frequently used, and predictive eviction—compare to LRU eviction. The goal isn’t to provide an in-depth analysis of these methods but rather to help develop your intuition about the types of tradeoffs that arise when picking a caching strategy. The optimal caching strategy for a particular scenario will depend on the specifics of that scenario. Understanding the tradeoffs will help you pick the best strategy for your use case.</p>
<p>LRU is a good eviction strategy when we see bursts of common usage among certain cache items but expect the distribution of cached items to change over time. The local pot of coffee near the register described earlier is an excellent example. When someone orders a new type of coffee, the barista walks to the end of the counter, returns the old blend, retrieves the new blend, and leaves that carafe by the register.</p>
<p>Let’s contrast that with evicting the <em>most recently used (MRU)</em> element. You can picture this in the context of an ebook reader that prefetches and caches books that you might enjoy. Since it is unlikely that we will read a book twice in a row, it might make sense to discard the recently completed book from our cache in order to free up space for a new one. While MRU can be a good approach for items with infrequent repetition, the same eviction would create constant tragedy in our coffee pantry. Imagine if we were forced to evict our favorite, go-to blend anytime we brewed a cup.</p>
<p>Instead of considering <em>when</em> the item was last accessed, we could also track how many times it has been accessed in total. The <em>least frequently used (LFU)</em> eviction strategy discards the element with the smallest count. This <span epub:type="pagebreak" id="Page_196" title="196"/>approach is advantageous when our cached population remains stable, such as with the tried-and-true coffee varieties we keep at home. We don’t evict one of our three go-to varieties simply because we recently tried the seasonal blend at our local coffeehouse. The items in the cache have a proven track record and are there to stay—at least, until we find a new favorite. Unfortunately, when preferences change, it can take a while for newly popular items to accumulate enough counts to make it into our cache. If we encounter a new brew that is better than anything in our home collection, we’d need to sample individual cups of the coffee many times, making numerous trips to the coffee shop, before finally buying beans for our own pantry. </p>
<p>The <em>predictive evictio</em><em>n</em> strategy provides a forward-looking approach that tries to predict what elements will be needed in the future. Instead of relying on simple counts or timestamps, we can build a model to predict what cache entries are most likely to be accessed in the future. The effectiveness of this cache depends on the accuracy of the model. If we have a highly accurate model, such as predicting that we will switch from our go-to java blends to a seasonal fall blend only in October and November, we greatly improve the hit rate. However, if the model is inaccurate, perhaps associating each month with the first coffee we consumed that month last year, we can find ourselves reliving that onetime mistake of drinking mint-cinnamon-pumpkin lattes in August. Another downside of predictive eviction is that it adds complexity to the cache itself. It is no longer sufficient to track simple counts or timestamps; instead, the cache must learn a model.</p>
<h2 id="h1-502604c11-0004">Why This Matters</h2>
<p class="BodyFirst">Caches can alleviate some of the access cost when dealing with expensive storage mediums. Instead of calling out to the expensive storage, caches store a portion of this data in a closer and faster location. If we choose the correct caching strategy, we can save significant amounts of time by pulling data from the cache instead of the slower locations. This makes caches a common and powerful tool in our computational toolbox. They are used throughout the real world, from web browsers to library shelves to coffee shops.</p>
<p>Caches also illustrate several key concepts. First, they highlight the potential tradeoffs to consider when accessing data from different mediums. If we could store everything in the fastest memory, we wouldn’t need caches. Unfortunately, this usually isn’t possible; our algorithms will need to access larger, slower data stores, and it’s worth understanding the potential cost this adds. In the next chapter, we introduce the B-tree, a tree-based data structure that reduces the number of data accesses needed. This optimization helps reduce the overall cost when we cannot fit our data in nearby, fast memory.</p>
<p>Second, caches revisit the data structure–tuning question that we saw in previous chapters. Both the size of the cache and the eviction strategy are parameters that can have massive impact on your cache’s performance. <span epub:type="pagebreak" id="Page_197" title="197"/>Consider the problem of selecting the size of the cache. If the cache is too small, it might not store enough to provide a benefit. The single nearby pot of coffee in our café only helps so much. On the other hand, too large a cache will take important memory resources that might be needed for other parts of our algorithm.</p>
<p>Finally, and most importantly for this book’s purposes, caches illustrate how we can combine basic data structures, such as the hash table and the queue, to provide more complex and impactful behaviors. In this case, by using pointers to tie hash table entries to nodes in a queue, we can efficiently track which entry in our cache should be removed next. </p>
</section>
</div></body></html>