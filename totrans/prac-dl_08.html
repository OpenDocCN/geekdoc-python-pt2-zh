<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch08"><span epub:type="pagebreak" id="page_169"/><strong><span class="big">8</span><br/>INTRODUCTION TO NEURAL NETWORKS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">Neural networks are the heart of deep learning. In <a href="ch09.xhtml#ch09">Chapter 9</a>, we’ll take a deep dive into what we’ll call <em>traditional neural networks</em>. However, before we do that, we’ll introduce the anatomy of a neural network, followed by a quick example.</p>&#13;
<p class="indent">Specifically, we’ll present the components of a <em>fully connected feed- forward neural network</em>. Visually, you can imagine the network as shown in <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>. We’ll refer to this figure often in this chapter and the next. Your mission, should you choose to accept it, is to commit this figure to memory to save wear and tear on the book by flipping back to it repeatedly.</p>&#13;
<div class="image" id="ch8fig1"><span epub:type="pagebreak" id="page_170"/><img src="Images/08fig01.jpg" alt="image" width="471" height="299"/></div>&#13;
<p class="figcap"><em>Figure 8-1: A sample neural network</em></p>&#13;
<p class="indent">After discussing the structure and parts of a neural network, we’ll explore training our example network to classify irises. From this initial experiment, <a href="ch09.xhtml#ch09">Chapter 9</a> will lead us to gradient descent and the backpropagation algorithm—the standard way that neural networks, including advanced deep neural networks, are trained. This chapter is intended as a warm-up. The heavy lifting starts in <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<h3 class="h3" id="lev1_50">Anatomy of a Neural Network</h3>&#13;
<p class="noindent">A <em>neural network</em> is a graph. In computer science, a <em>graph</em> is a series of <em>nodes</em>, universally drawn as circles, connected by <em>edges</em> (short line segments). This abstraction is useful for representing many different kinds of relationships: roads between cities, who knows whom on social media, the structure of the internet, or a series of basic computational units that can be used to approximate any mathematical function.</p>&#13;
<p class="indent">The last example is, of course, deliberate. Neural networks are universal function approximators. They use a graph structure to represent a series of computational steps mapping an input feature vector to an output value, typically interpreted as a probability. Neural networks are built in layers. Conceptually, they act from left to right, mapping an input feature vector to the output(s) by passing values along the edges to the nodes. Note, the nodes of a neural network are often referred to as <em>neurons</em>. We’ll see why shortly. The nodes calculate new values based on their inputs. The new values are then passed to the next layer of nodes and so on until the output nodes are reached. In <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>, there’s an input layer on the left, a hidden layer to its right, another hidden layer right of that, and a single node in the output layer.</p>&#13;
<p class="indent">The previous section included the phrase <em>fully connected feedforward neural network</em> without much explanation. Let’s break it down. The <em>fully connected</em> part means every node of a layer has its output sent to every node of the next layer. The <em>feedforward</em> part means that information passes from left <span epub:type="pagebreak" id="page_171"/>to right through the network without being sent back to a previous layer; there is no <em>feedback</em>, no looping, in the network structure. This leaves only the <em>neural network</em> part.</p>&#13;
<h4 class="h4" id="lev2_73">The Neuron</h4>&#13;
<p class="noindent">Personally, I have a love/hate relationship with the phrase <em>neural network</em>. The phrase itself comes from the fact that in a very crude approximation, the basic unit of the network resembles a neuron in a brain. Consider <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>, which we’ll describe in detail shortly.</p>&#13;
<div class="image" id="ch8fig2"><img src="Images/08fig02.jpg" alt="image" width="528" height="211"/></div>&#13;
<p class="figcap"><em>Figure 8-2: A single neural network node</em></p>&#13;
<p class="indent">Recalling that our visualization of a network always moves from left to right, we see that the node (the circle) accepts input from the left, and has a single output on the right. Here there are two inputs, but it might be hundreds.</p>&#13;
<p class="indent">Many inputs mapped to a single output echo how a neuron in the brain works: structures called dendrites accept input from many other neurons, and the single axon is the output. I love this analogy because it leads to a cool way of talking and thinking about the networks. But I hate the analogy because these artificial neurons are, operationally, quite different from real ones, and the analogy quickly falls apart. There is an anatomical similarity to actual neurons, but they’re not the same, and it leads to confusion on the part of those who are not familiar with machine learning, causing some to believe that computer scientists are truly building artificial brains or that the networks think. The meaning of the word <em>think</em> is hard to pin down, but to me it doesn’t apply to what a neural network does.</p>&#13;
<p class="indent">Returning now to <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>, we see two squares on the left, a bunch of lines, a circle, a line on the right, and a bunch of labels with subscripts. Let’s sort this out. If we understand <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>, we’ll be well on our way to understanding neural networks. Later, we’ll see an implementation of our visual model in code and be surprised to learn how simple it can be.</p>&#13;
<p class="indent">Everything in <a href="ch08.xhtml#ch8fig2">Figure 8-2</a> focuses on the circle. This is the actual node. In reality, it implements a mathematical function called the <em>activation function</em>, which calculates the output of the node, a single number. The two squares are the inputs to the node. This node accepts features from an input feature vector; we use squares to differentiate from circles, but the input might just <span epub:type="pagebreak" id="page_172"/>as well have come from another group of circular nodes in a previous network layer.</p>&#13;
<p class="indent">Each input is a number, a single scalar value, which we’re calling <em>x</em><sub>0</sub> and <em>x</em><sub>1</sub>. These inputs move to the node along the two line segments labeled <em>w</em><sub>0</sub> and <em>w</em><sub>1</sub>. These line segments represent <em>weights</em>, the strength of the connection. Computationally, the inputs (<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>) are multiplied by the weights (<em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>), summed, and given to the activation function of the node. Here we’re calling the activation function <em>h</em>, a fairly common thing to call it.</p>&#13;
<p class="indent">The value of the activation function is the output of the node. Here we’re calling this output <em>a</em>. The inputs, multiplied by the weights, are added together and given to the activation function to produce an output value. We have yet to mention the <em>b</em><sub>0</sub> value, which is also added in and passed to the activation function. This is the <em>bias</em> term. It’s an offset used to adjust the input range to make it suitable for the activation function. In <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>, we added a zero subscript. There is a bias value for each node in each layer, so the subscript here implies that this node is the first node in the layer. (Remember computer people always count from zero, not one.)</p>&#13;
<p class="indent">This is all that a neural network node does: a neural network node accepts multiple inputs, <em>x</em><sub>0</sub>,<em>x</em><sub>1</sub>,…, multiplies each by a weight value, <em>w</em><sub>0</sub>,<em>w</em><sub>1</sub>,…, sums these products along with the bias term, <em>b</em>, and passes this sum to the activation function, <em>h</em>, to produce a single scalar output value, <em>a</em>:</p>&#13;
<p class="center"><em>a</em> = <em>h</em>(<em>w</em><sub>0</sub><em>x</em><sub>0</sub> + <em>w</em><sub>1</sub><em>x</em><sub>1</sub> + … + <em>b</em>)</p>&#13;
<p class="indent">That’s it. Get a bunch of nodes together, link them appropriately, figure out how to train them to set the weights and biases, and you have a useful neural network. As you’ll see in the next chapter, training a neural network is no easy feat. But once trained, they’re simple to use: feed it a feature vector, and out comes a classification.</p>&#13;
<p class="indent">As an aside, we’ve been calling these graphs <em>neural networks</em>, and will continue to do so, sometimes using the abbreviation <em>NN</em>. If you read other books or papers, you might see them called <em>artificial neural networks (ANNs)</em> or even <em>multi-layer perceptrons (MLPs)</em>, as in the sklearn <span class="literal">MLPClassifier</span> class name. I recommend sticking with <em>neural network</em>, but that’s just me.</p>&#13;
<h4 class="h4" id="lev2_74">Activation Functions</h4>&#13;
<p class="noindent">Let’s talk about activation functions. The activation function for a node takes a single scalar input, the sum of the inputs times the weights plus the bias, and does something to it. In particular, we need the activation function to be nonlinear so that the model can learn complex functions. Mathematically, it’s easiest to see what a nonlinear function is by stating what a linear function is and then saying that any mapping that is not linear is . . . nonlinear.</p>&#13;
<p class="indent">A <em>linear function</em>, <em>g</em>, has output that is directly proportional to the input, <em>g</em>(<em>x</em>) ∝ <em>x</em>, where ∝ means <em>proportional to</em>. Alternatively, the graph of a linear function is a straight line. Therefore, any function whose graph is not a straight line is a nonlinear function.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_173"/>For example, the function</p>&#13;
<p class="center"><em>g</em>(<em>x</em>) = 3<em>x</em> + 2</p>&#13;
<p class="noindent">is a linear function because its graph is a straight line. A constant function like <em>g</em>(<em>x</em>) = 1 is also linear. However, the function</p>&#13;
<p class="center"><em>g</em>(<em>x</em>) = <em>x</em><sup>2</sup> + 2</p>&#13;
<p class="noindent">is a nonlinear function because the exponent of <em>x</em> is 2. Transcendental functions are also nonlinear. <em>Transcendental functions</em> are functions like <em>g</em>(<em>x</em>) = log<em>x</em>, or <em>g</em>(<em>x</em>) = <em>e</em><sup><em>x</em></sup>, where <em>e</em> = 2.718<em>...</em> is the base of the natural logarithm. <em>Trigonometric functions</em> like sine and cosine, their inverses, and functions like tangent that are built from sine and cosine are also transcendental functions. These functions are transcendental because you cannot form them as finite combinations of elementary algebra operations. They are nonlinear because their graphs are not straight lines.</p>&#13;
<p class="indent">The network needs nonlinear activation functions; otherwise, it will be able to learn only linear mappings, and linear mappings are not sufficient to make the networks generally useful. Consider a trivial network of two nodes, each with one input. This means there’s one weight and one bias value per node, and the output of the first node is the input of the second. If we set <em>h</em>(<em>x</em>) = 5<em>x –</em> 3, a linear function, then for input <em>x</em> the network computes output <em>a</em><sub>1</sub> to be</p>&#13;
<table>&#13;
<tbody>&#13;
<tr>&#13;
<td><p class="tab-r"><em>a</em><sub>1</sub></p></td>&#13;
<td><p class="tab">= <em>h</em>(<em>w</em><sub>1</sub><em>a</em><sub>0</sub> + <em>b</em><sub>1</sub>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= <em>h</em>(<em>w</em><sub>1</sub><em>h</em>(<em>w</em><sub>0</sub><em>x</em> + <em>b</em><sub>0</sub>) + <em>b</em><sub>1</sub>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= <em>h</em>(<em>w</em><sub>1</sub>(5(<em>w</em><sub>0</sub><em>x</em> + <em>b</em><sub>0</sub>) – 3) + <em>b</em><sub>1</sub>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= <em>h</em>(<em>w</em><sub>1</sub>(5<em>w</em><sub>0</sub><em>x</em> + 5<em>b</em><sub>0</sub> – 3) + <em>b</em><sub>1</sub>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= <em>h</em>(5<em>w</em><sub>1</sub><em>w</em><sub>0</sub><em>x</em> + 5<em>w</em><sub>1</sub><em>b</em><sub>0</sub> – 3<em>w</em><sub>1</sub> + <em>b</em><sub>1</sub>)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= 5(5<em>w</em><sub>1</sub><em>w</em><sub>0</sub><em>x</em> + 5<em>w</em><sub>1</sub><em>b</em><sub>0</sub> – 3<em>w</em><sub>1</sub> + <em>b</em><sub>1</sub>) – 3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= (25<em>w</em><sub>1</sub><em>w</em><sub>0</sub>)<em>x</em> + (25<em>w</em><sub>1</sub><em>b</em><sub>0</sub> – <sub>1</sub>5<em>w</em><sub>1</sub> + 5<em>b</em><sub>1</sub> – 3)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p class="tab">= <em>W</em><em>x</em> + <em>B</em></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">for <em>W</em> = 25<em>w</em><sub>1</sub><em>w</em><sub>0</sub> and <em>B</em> = 25<em>w</em><sub>1</sub><em>b</em><sub>0</sub> <em>–</em> 15<em>w</em><sub>1</sub> + 5<em>b</em><sub>1</sub> <em>–</em> 3, which is also a linear function, another line with slope <em>W</em> and intercept <em>B</em> since neither <em>W</em> nor <em>B</em> depend on <em>x</em>. Therefore, a neural network with linear activation functions <span epub:type="pagebreak" id="page_174"/>would learn only a linear model since the composition of linear functions is also linear. It’s precisely this limitation of linear activation functions that caused the first neural network “winter” in the 1970s: research into neural networks was effectively abandoned because they were thought to be too simple to learn complex functions.</p>&#13;
<p class="indent">Okay, so we want nonlinear activation functions. Which ones? There are an infinite number of possibilities. In practice, a few have risen to the top because of their proven usefulness or nice properties or both. Traditional neural networks used either sigmoid activation functions or hyperbolic tangents. A <em>sigmoid</em> is</p>&#13;
<div class="imagec"><img src="Images/174equ01.jpg" alt="image" width="116" height="44"/></div>&#13;
<p class="indent">and the <em>hyperbolic tangent</em> is</p>&#13;
<div class="imagec"><img src="Images/174equ02.jpg" alt="image" width="231" height="49"/></div>&#13;
<p class="noindent">Plots of both of these functions are in <a href="ch08.xhtml#ch8fig3">Figure 8-3</a>, with the sigmoid on the top and the hyperbolic tangent on the bottom.</p>&#13;
<p class="indent">The first thing to notice is that both of these functions have roughly the same “S” shape. The sigmoid runs from 0 as you go further left along the x-axis to 1 as you go to the right. At 0, the function value is 0.5. The hyperbolic tangent does the same but goes from –1 to +1 and is 0 at <em>x</em> = 0.</p>&#13;
<p class="indent">More recently, the sigmoid and hyperbolic tangent have been replaced by the <em>rectified linear unit</em>, or <em>ReLU</em> for short. The ReLU is simple, and has convenient properties for neural networks. Even though the word <em>linear</em> is in the name, the ReLU is a nonlinear function—its graph is not a straight line. When we discuss backpropagation training of neural networks in <a href="ch09.xhtml#ch09">Chapter 9</a>, we’ll learn why this change has happened.</p>&#13;
<div class="image" id="ch8fig3"><span epub:type="pagebreak" id="page_175"/><img src="Images/08fig03.jpg" alt="image" width="675" height="1043"/></div>&#13;
<p class="figcap"><em>Figure 8-3: A sigmoid function (top) and a hyperbolic tangent function (bottom). Note that the y-axis scales are not the same</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_176"/>The ReLU is as follows and is shown in <a href="ch08.xhtml#ch8fig4">Figure 8-4</a>.</p>&#13;
<div class="imagec"><img src="Images/176equ01.jpg" alt="image" width="329" height="63"/></div>&#13;
<div class="image" id="ch8fig4"><img src="Images/08fig04.jpg" alt="image" width="675" height="502"/></div>&#13;
<p class="figcap"><em>Figure 8-4: The rectified linear activation function, ReLU(x) = max(0,x)</em></p>&#13;
<p class="indent">ReLU is called <em>rectified</em> because it removes the negative values and replaces them with 0. In truth, the machine learning community uses several different versions of this function, but all are essentially replacing negative values with a constant or some other value. The piecewise nature of the ReLU is what makes it nonlinear and, therefore, suitable for use as a neural network activation function. It’s also computationally simple, far faster to calculate than either the sigmoid or the hyperbolic tangent. This is because the latter functions use <em>e</em><sup><em>x</em></sup>, which, in computer terms, means a call to the <span class="literal">exp</span> function. This function is typically implemented as a sum of terms of a series expansion, translating into dozens of floating-point operations in place of the single <span class="literal">if</span> statement necessary to implement a ReLU. Small savings like this add up in an extensive network with potentially thousands of nodes.</p>&#13;
<h4 class="h4" id="lev2_75">Architecture of a Network</h4>&#13;
<p class="noindent">We’ve discussed nodes and how they work, and hinted that nodes are connected to form networks. Let’s look more closely at how nodes are connected, the <em>architecture</em> of the network.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_177"/>Standard neural networks like the ones we are working with in this chapter are built in layers, as you saw in <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>. We don’t need to do this, but as we’ll see, this buys us some computational simplicity and greatly simplifies training. A feedforward network has an input layer, one or more hidden layers, and an output layer. The input layer is simply the feature vector, and the output layer is the prediction (probability). If the network is for a multiclass problem, the output layer might have more than one node, with each node representing the model’s prediction for each of the possible classes of inputs.</p>&#13;
<p class="indent">The hidden layers are made of nodes, and the nodes of layer <em>i</em> accept as input the output of the nodes of layer <em>i –</em> 1 and pass their outputs to the inputs of the nodes of layer <em>i</em> + 1. The connections between the layers are typically fully connected, meaning every output of every node of layer <em>i –</em> 1 is used as an input to every node of layer <em>i</em>, hence <em>fully connected</em>. Again, we don’t need to do this, but it simplifies the implementation.</p>&#13;
<p class="indent">The number of hidden layers and the number of nodes in each hidden layer define the architecture of the network. It has been proven that a single hidden layer with enough nodes can learn any function mapping. This is good because it means neural networks are applicable to machine learning problems since, in the end, the model acts as a complex function mapping inputs to output labels and probabilities. However, like many theoretical results, this does not mean that it’s practical for a single layer network to be used in all situations. As the number of nodes (and layers) in a network grows, so, too, does the number of parameters to learn (weights and biases), and therefore the amount of training data needed goes up as well. It’s the curse of dimensionality again.</p>&#13;
<p class="indent">Issues like these stymied neural networks for a second time in the 1980s. Computers were too slow to train large networks, and, regardless, there was usually too little data available to train the network anyway. Practitioners knew that if both of these situations changed, then it would become possible to train large networks that would be far more capable than the small networks of the time. Fortunately for the world, the situation changed in the early 2000s.</p>&#13;
<p class="indent">Selecting the proper neural network architecture has a huge impact on whether or not your model will learn anything. This is where experience and intuition come in. Selecting the right architecture is the dark art of using neural networks. Let’s try to be more helpful by giving some (crude) rules of thumb:</p>&#13;
<ul>&#13;
<li class="noindent">If your input has definite spatial relationships, like the parts of an image, you might want to use a convolutional neural network instead (<a href="ch12.xhtml#ch12">Chapter 12</a>).</li>&#13;
<li class="noindent">Use no more than three hidden layers. Recall, in theory, one sufficiently large hidden layer is all that is needed, so use as few hidden layers as necessary. If the model learns with one hidden layer, then add a second to see if that improves things.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_178"/>The number of nodes in the first hidden layer should match or (ideally) exceed the number of input vector features.</li>&#13;
<li class="noindent">Except for the first hidden layer (see previous rule), the number of nodes per hidden layer should be the same as or some value between the number of nodes in the previous layer and the following layer. If layer <em>i –</em> 1 has <em>N</em> nodes and layer <em>i</em> + 1 has <em>M</em> nodes, then layer <em>i</em> might be good with <em>N</em> ≤ <em>x</em> ≤ <em>M</em> nodes.</li>&#13;
</ul>&#13;
<p class="noindent">The first rule says that a traditional neural network best applies to situations where your input does not have spatial relationships—that is, you have a feature vector, not an image. Also, when your input dimension is small, or when you do not have a lot of data, which makes it hard to train a larger convolutional network, you should give a traditional network a try. If you do think you are in a situation where a traditional neural network is called for, start small, and grow it as long as performance improves.</p>&#13;
<h4 class="h4" id="lev2_76">Output Layers</h4>&#13;
<p class="noindent">The last layer of a neural network is the output layer. If the network is modeling a continuous value, known as <em>regression</em>, a use case we’re ignoring in this book, then the output layer is a node that doesn’t use an activation function; it simply reports the argument to <em>h</em> in <a href="ch08.xhtml#ch8fig2">Figure 8-2</a>. Note that this is the same as saying that the activation function is the identity function, <em>h</em>(<em>x</em>) = <em>x</em>.</p>&#13;
<p class="indent">Our neural networks are for classification; we want them to output a decision value. If we have two classes labeled 0 and 1, we make the activation function of the final node a sigmoid. This will output a value between 0 and 1 that we can interpret as a likelihood or probability that the input belongs to class 1. We make our classification decision based on the output value with a simple rule: if the activation value is less than 0.5, call the input class 0; otherwise, call it class 1. We’ll see in <a href="ch11.xhtml#ch11">Chapter 11</a> how changing this threshold of 0.5 can be used to tune a model’s performance for the task at hand.</p>&#13;
<p class="indent">If we have more than two classes, we need to take a different approach. Instead of a single node in the output layer, we’ll have <em>N</em> output nodes, one for each class, each one using the identity function for <em>h</em>. Then, we apply a <em>softmax</em> operation to these <em>N</em> outputs and select the output with the largest softmax value.</p>&#13;
<p class="indent">Let’s illustrate what we mean by softmax. Suppose we have a dataset with four classes in it. What they represent doesn’t really matter; the network doesn’t know what they represent, either. The classes are labeled 0, 1, 2, and 3. So, <em>N</em> = 4 means our network will have four output nodes, each one using the identity function for <em>h</em>. This looks like <a href="ch08.xhtml#ch8fig5">Figure 8-5</a>, where we have also shown the softmax operation and the resulting output vector.</p>&#13;
<div class="image" id="ch8fig5"><span epub:type="pagebreak" id="page_179"/><img src="Images/08fig05.jpg" alt="image" width="494" height="398"/></div>&#13;
<p class="figcap"><em>Figure 8-5: The last hidden layer <em>n</em>-1 and output layer (<em>n</em>, nodes numbered) for a neural network with four classes. The softmax operation is applied, producing a four-element output vector, <em>[p</em>0,<em>p</em>1,<em>p</em>2,<em>p</em>3<em>]</em>.</em></p>&#13;
<p class="indent">We select the index of the largest value in this output vector as the class label for the given input feature vector. The softmax operation ensures that the elements of this vector sum to 1, so we can again be a bit sloppy and call these values the probability of belonging to each of the four classes. That is why we take only the largest value to decide the output class label.</p>&#13;
<p class="indent">The softmax operation is straightforward: the probability for each of the outputs is simply</p>&#13;
<div class="imagec"><img src="Images/179equ01.jpg" alt="image" width="96" height="55"/></div>&#13;
<p class="noindent">where <em>a</em><sub><em>i</em></sub> is the <em>i</em>-th output, and the denominator is the sum over all the outputs. For the example, <em>i</em> = 0,1,2,3, and the index of the largest value will be the class label assigned to the input.</p>&#13;
<p class="indent">As an example, assume the output of the four last layer nodes is</p>&#13;
<p class="center"><em>a</em><sub>0</sub> = 0.2</p>&#13;
<p class="center"><em>a</em>1 = 1.3</p>&#13;
<p class="center"><em>a</em>2 = 0.8</p>&#13;
<p class="center"><em>a</em>3 = 2.1</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_180"/>Then calculate the softmax as follows:</p>&#13;
<p class="center"><em>p</em><sub>0</sub> = <em>e</em><sup>0.2</sup>/(<em>e</em><sup>0.2</sup> + <em>e</em><sup>1.3</sup> + <em>e</em><sup>0.8</sup> + <em>e</em><sup>2.1</sup>) = 0.080</p>&#13;
<p class="center"><em>p</em><sub>1</sub> = <em>e</em><sup>1.3</sup>/(<em>e</em><sup>0.2</sup> + <em>e</em><sup>1.3</sup> + <em>e</em><sup>0.8</sup> + <em>e</em><sup>2.1</sup>) = 0.240</p>&#13;
<p class="center"><em>p</em><sub>2</sub> = <em>e</em><sup>0.8</sup>/(<em>e</em><sup>0.2</sup> + <em>e</em><sup>1.3</sup> + <em>e</em><sup>0.8</sup> + <em>e</em><sup>2.1</sup>) = 0.146</p>&#13;
<p class="center"><em>p</em><sub>3</sub> = <em>e</em><sup>2.1</sup>/(<em>e</em><sup>0.2</sup> + <em>e</em><sup>1.3</sup> + <em>e</em><sup>0.8</sup> + <em>e</em><sup>2.1</sup>) = 0.534</p>&#13;
<p class="noindent">Select class 3 because <em>p</em><sub>3</sub> is the largest. Note that the sum of the <em>p</em><sub><em>i</em></sub> values is 1.0, as we would expect.</p>&#13;
<p class="indent">Two points should be mentioned here. In the preceding equations, we used the sigmoid to calculate the output of the network. If we set the number of classes to 2 and calculate the softmax, we’ll get two output values: one will be some <em>p</em>, and the other will be 1 <em>– p</em>. This is identical to the sigmoid alone, selecting the probability of the input being of class 1.</p>&#13;
<p class="indent">The second point has to do with implementing the softmax. If the network outputs, the <em>a</em> values, are large, then <em>e</em><sup><em>a</em></sup> might be very large, which is something the computer will not like. Precision will be lost, at least, or the value might overflow and make the output meaningless. Numerically, if we subtract the largest <em>a</em> value from all the others before calculating the softmax, we’ll take the exponential over smaller values that are less likely to overflow. Doing this for the preceding example gives new <em>a</em> values</p>&#13;
<div class="imagec"><img src="Images/180equ02.jpg" alt="image" width="176" height="224"/></div>&#13;
<p class="noindent">where we subtract 2.1 because that is the largest <em>a</em> value. This leads to precisely the same <em>p</em> values we found before, but this time protected against overflow in the case that any of the <em>a</em> values are too large.</p>&#13;
<h4 class="h4" id="lev2_77">Representing Weights and Biases</h4>&#13;
<p class="noindent">Before we move on to an example neural network, let’s revisit the weights and biases and see that we can greatly simplify the implementation of a neural network by viewing it in terms of matrices and vectors.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_181"/>Consider the mapping from an input feature vector of two elements to the first hidden layer with three nodes (<em>a</em><sub>1</sub> in <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>). Let’s label the edges between the two layers (the weights) as <em>w</em><sub><em>ij</em></sub> with <em>i</em> = 0,1 for the inputs <em>x</em><sub>0</sub> and <em>x</em><sub>1</sub> and <em>j</em> = 0,1,2 for the three hidden layer nodes numbered from top to bottom of the figure. Additionally, we need three bias values that are not shown in the figure, one for each hidden node. We’ll call these <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, and <em>b</em><sub>2</sub>, again, top to bottom.</p>&#13;
<p class="indent">In order to calculate the outputs of the activation functions, <em>h</em>, for the three hidden nodes, we need to find the following.</p>&#13;
<p class="center"><em>a</em><sub>0</sub> = <em>h</em>(<em>w</em><sub>00</sub><em>x</em><sub>0</sub> + <em>w</em><sub>10</sub><em>x</em><sub>1</sub> + <em>b</em><sub>0</sub>)</p>&#13;
<p class="center"><em>a</em><sub>1</sub> = <em>h</em>(<em>w</em><sub>01</sub><em>x</em><sub>0</sub> + <em>w</em><sub>11</sub><em>x</em><sub>1</sub> + <em>b</em><sub>1</sub>)</p>&#13;
<p class="center"><em>a</em><sub>2</sub> = <em>h</em>(<em>w</em><sub>02</sub><em>x</em><sub>0</sub> + <em>w</em><sub>12</sub><em>x</em><sub>1</sub> + <em>b</em><sub>2</sub>)</p>&#13;
<p class="noindent">But, remembering how matrix multiplication and vector addition work, we see that this is exactly</p>&#13;
<div class="imagec"><img src="Images/181equ02.jpg" alt="image" width="410" height="75"/></div>&#13;
<p class="noindent">where <span class="middle"><img src="Images/181equ03.jpg" alt="Image" width="382" height="26"/></span>, and <em>W</em> is a 3 × 2 matrix of weight values. In this case, the activation function, <em>h</em>, is given a vector of input values and produces a vector of output values. This is simply applying <em>h</em> to every element of <span class="middle"><img src="Images/181equ04.jpg" alt="Image" width="58" height="21"/></span>. For example, applying <em>h</em> to a vector <span class="middle"><img src="Images/xbar1.jpg" alt="Image"/></span> with three elements is</p>&#13;
<div class="imagec"><img src="Images/181equ05.jpg" alt="image" width="353" height="21"/></div>&#13;
<p class="noindent">with <em>h</em> applied separately to each element of <span class="middle"><img src="Images/xbar1.jpg" alt="Image" width="11" height="16"/></span>.</p>&#13;
<p class="indent">Since the NumPy Python module is designed to work with arrays, and matrices and vectors are arrays, we arrive at the pleasant conclusion that the weights and biases of a neural network can be stored in NumPy arrays and we need only simple matrix operations (calls to <span class="literal">np.dot</span>) and addition to work with a fully connected neural network. Note this is why we want to use fully connected networks: their implementation is straightforward.</p>&#13;
<p class="indent">To store the network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>, we need a weight matrix and bias vector between each layer, giving us three matrices and three vectors: a matrix and vector each for the input to the first hidden layer, the first hidden layer to the second, and the second hidden layer to the output. The weight matrices are of dimensions 3 × 2, 2 × 3, and 1 × 2, respectively. The bias vectors are of length 3, 2, and 1.</p>&#13;
<h3 class="h3" id="lev1_51"><span epub:type="pagebreak" id="page_182"/>Implementing a Simple Neural Network</h3>&#13;
<p class="noindent">In this section, we’ll implement the sample neural network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> and train it on two features from the iris dataset. We’ll implement the network from scratch but use sklearn to train it. The goal of this section is to see how straightforward it is to implement a simple neural network. Hopefully, this will clear some of the fog that might be hanging around from the discussion of the previous sections.</p>&#13;
<p class="indent">The network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> accepts an input feature vector with two features. It has two hidden layers, one with three nodes and the other with two nodes. It has one sigmoid output. The activation functions of the hidden nodes are also sigmoids.</p>&#13;
<h4 class="h4" id="lev2_78">Building the Dataset</h4>&#13;
<p class="noindent">Before we look at the neural network code, let’s build the dataset we’ll train against and see what it looks like. We know the iris dataset already, but for this example, we’ll use only two classes and only two of the four features. The code to build the train and test datasets is in <a href="ch08.xhtml#ch8lis1">Listing 8-1</a>.</p>&#13;
<p class="programs" id="ch8lis1">   import numpy as np<br/>&#13;
<span class="ent">❶</span> d = np.load("iris_train_features_augmented.npy")<br/>&#13;
   l = np.load("iris_train_labels_augmented.npy")<br/>&#13;
   d1 = d[np.where(l==1)]<br/>&#13;
   d2 = d[np.where(l==2)]<br/>&#13;
<span class="ent">❷</span> a=len(d1)<br/>&#13;
   b=len(d2)<br/>&#13;
   x = np.zeros((a+b,2))<br/>&#13;
   x[:a,:] = d1[:,2:]<br/>&#13;
   x[a:,:] = d2[:,2:]<br/>&#13;
<span class="ent">❸</span> y = np.array([0]*a+[1]*b)<br/>&#13;
   i = np.argsort(np.random.random(a+b))<br/>&#13;
   x = x[i]<br/>&#13;
   y = y[i]<br/>&#13;
<span class="ent">❹</span> np.save("iris2_train.npy", x)<br/>&#13;
   np.save("iris2_train_labels.npy", y)<br/>&#13;
<span class="ent">❺</span> d = np.load("iris_test_features_augmented.npy")<br/>&#13;
   l = np.load("iris_test_labels_augmented.npy")<br/>&#13;
   d1 = d[np.where(l==1)]<br/>&#13;
   d2 = d[np.where(l==2)]<br/>&#13;
   a=len(d1)<br/>&#13;
   b=len(d2)<br/>&#13;
   x = np.zeros((a+b,2))<br/>&#13;
   x[:a,:] = d1[:,2:]<br/>&#13;
   x[a:,:] = d2[:,2:]<br/>&#13;
   y = np.array([0]*a+[1]*b)<br/>&#13;
   i = np.argsort(np.random.random(a+b))<br/>&#13;
   x = x[i]<br/>&#13;
<span epub:type="pagebreak" id="page_183"/>   y = y[i]<br/>&#13;
   np.save("iris2_test.npy", x)<br/>&#13;
   np.save("iris2_test_labels.npy", y)</p>&#13;
<p class="figcap"><em>Listing 8-1: Building the simple example dataset. See</em> nn_iris_dataset.py.</p>&#13;
<p class="indent">This code is straightforward data munging. We start with the augmented dataset and load the samples and labels <span class="ent">❶</span>. We want only class 1 and class 2, so we find the indices of those samples and pull them out. We’re keeping only features 2 and 3 and put them in <span class="literal">x</span> <span class="ent">❷</span>. Next, we build the labels (<span class="literal">y</span>) <span class="ent">❸</span>. Note, we recode the class labels to 0 and 1. Finally, we scramble the order of the samples and write the new dataset to disk <span class="ent">❹</span>. Last of all, we repeat this process to build the test samples <span class="ent">❺</span>.</p>&#13;
<p class="indent"><a href="ch08.xhtml#ch8fig6">Figure 8-6</a> shows the training set. We can plot it in this case because we have only two features.</p>&#13;
<div class="image" id="ch8fig6"><img src="Images/08fig06.jpg" alt="image" width="677" height="515"/></div>&#13;
<p class="figcap"><em>Figure 8-6: The training data for the two-class, two-feature iris dataset</em></p>&#13;
<p class="indent">We immediately see that this dataset is not trivially separable. There is no simple line we can draw that will correctly split the training set into two groups, one all class 0 and the other all class 1. This makes things a little more interesting.</p>&#13;
<h4 class="h4" id="lev2_79">Implementing the Neural Network</h4>&#13;
<p class="noindent">Let’s see how to implement the network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> in Python using NumPy. We’ll assume that it’s already trained, meaning we already know all the weights and biases. The code is in <a href="ch08.xhtml#ch8lis2">Listing 8-2</a>.</p>&#13;
<p class="programs" id="ch8lis2"><span epub:type="pagebreak" id="page_184"/>  import numpy as np<br/>&#13;
  import pickle<br/>&#13;
  import sys<br/>&#13;
<br/>&#13;
  def sigmoid(x):<br/>&#13;
      return 1.0 / (1.0 + np.exp(-x))<br/>&#13;
<br/>&#13;
  def evaluate(x, y, w):<br/>&#13;
  <span class="ent">❶</span> w12,b1,w23,b2,w34,b3 = w<br/>&#13;
     nc = nw = 0<br/>&#13;
     prob = np.zeros(len(y))<br/>&#13;
     for i in range(len(y)):<br/>&#13;
         a1 = sigmoid(np.dot(x[i], w12) + b1)<br/>&#13;
         a2 = sigmoid(np.dot(a1, w23) + b2)<br/>&#13;
         prob[i] = sigmoid(np.dot(a2, w34) + b3)<br/>&#13;
         z  = 0 if prob[i] &lt; 0.5 else 1<br/>&#13;
      <span class="ent">❷</span> if (z == y[i]):<br/>&#13;
             nc += 1<br/>&#13;
         else:<br/>&#13;
             nw += 1<br/>&#13;
     return [float(nc) / float(nc + nw), prob]<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> xtest = np.load("iris2_test.npy")<br/>&#13;
   ytest = np.load("iris2_test_labels.npy")<br/>&#13;
<span class="ent">❹</span> weights = pickle.load(open("iris2_weights.pkl","rb"))<br/>&#13;
   score, prob = evaluate(xtest, ytest, weights)<br/>&#13;
   print()<br/>&#13;
   for i in range(len(prob)):<br/>&#13;
       print("%3d:  actual: %d  predict: %d  prob: %0.7f" %<br/>&#13;
       (i, ytest[i], 0 if (prob[i] &lt; 0.5) else 1, prob[i]))<br/>&#13;
   print("Score = %0.4f" % score)</p>&#13;
<p class="figcap"><em>Listing 8-2: Using the trained weights and biases to classify held-out test samples. See</em> nn_iris_evaluate.py.</p>&#13;
<p class="indent">Perhaps the first thing we should notice is how short the code is. The <span class="literal">evaluate</span> function implements the network. We also need to define <span class="literal">sigmoid</span> as NumPy does not have it natively. The main code loads the test samples (<span class="literal">xtest</span>) and associated labels (<span class="literal">ytest</span>) <span class="ent">❸</span>. These are the files generated by the preceding code, so we know that <span class="literal">xtest</span> is of shape 23 × 2 because we have 23 test samples, and each has two features. Similarly, <span class="literal">ytest</span> is a vector of 23 labels.</p>&#13;
<p class="indent">When we train this network, we’ll store the weights and biases as a list of NumPy arrays. The Python way to store a list on disk is via the <span class="literal">pickle</span> module, so we use <span class="literal">pickle</span> to load the list from disk <span class="ent">❹</span>. The list <span class="literal">weights</span> has six elements representing the three weight matrices and three bias vectors that define the network. These are the “magic” numbers that our training has conditioned to the dataset. Finally, we call <span class="literal">evaluate</span> to run each of the <span epub:type="pagebreak" id="page_185"/>test samples through the network. This function returns the score (accuracy) and the output probabilities for each sample (<span class="literal">prob</span>). The remainder of the code displays the sample number, actual label, predicted label, and associated output probability of being class 1. Finally, the score (accuracy) is shown.</p>&#13;
<p class="indent">The network is implemented in <span class="literal">evaluate</span>; let’s see how. First, pull the individual weight matrices and bias vectors from the supplied weight list <span class="ent">❶</span>. These are NumPy arrays: <span class="literal">w12</span> is a 2 × 3 matrix mapping the two-element input to the first hidden layer with three nodes, <span class="literal">w23</span> is a 3 × 2 matrix mapping the first hidden layer to the second hidden layer, and <span class="literal">w34</span> is a 2 × 1 matrix mapping the second hidden layer to the output. The bias vectors are <span class="literal">b1</span>, three elements; <span class="literal">b2</span>, two elements; and <span class="literal">b3</span>, a single element (a scalar).</p>&#13;
<p class="indent">Notice the weight matrices are not of the same shape as we previously indicated they would be. They are transposes. This is because we’re multiplying vectors, which are treated as 1 × 2 matrices, by the weight matrices. Because scalar multiplication is commutative, meaning <em>ab</em> = <em>ba</em>, we see that we’re still calculating the same argument value for the activation function.</p>&#13;
<p class="indent">Next, <span class="literal">evaluate</span> sets the number correct (<span class="literal">nc</span>) and number wrong (<span class="literal">nw</span>) counters to 0. These are for calculating the overall score across the entire test set. Similarly, we define <span class="literal">prob</span>, a vector to hold the output probability value for each of the test samples.</p>&#13;
<p class="indent">The loop applies the entire network to each test sample. First, we map the input vectors to the first hidden layer and calculate <em>a</em><sub>1</sub>, a vector of three numbers, the activation for each of the three hidden nodes. We then take these first hidden layer activations and calculate the second hidden layer activations, <em>a</em><sub>2</sub>. This is a two-element vector as there are two nodes in the second hidden layer. Next, we calculate the output value for the current input vector and store it in the <span class="literal">prob</span> array. The class label, <span class="literal">z</span>, is assigned by checking if the output value of the network is &lt; 0.5 or not. Finally, we increment the correct (<span class="literal">nc</span>) or incorrect (<span class="literal">nw</span>) counters based on the actual label for this sample (<span class="literal">y[i]</span>) <span class="ent">❷</span>. When all samples have been passed through the network, the overall accuracy is returned as the number of correctly classified samples divided by the total number of samples.</p>&#13;
<p class="indent">This is all well and good; we can implement a network and pass input vectors through it to see how well it does. If the network had a third hidden layer, we would pass the output of the second hidden layer (<span class="literal">a2</span>) through it before calculating the final output value.</p>&#13;
<h4 class="h4" id="lev2_80">Training and Testing the Neural Network</h4>&#13;
<p class="noindent">The code in <a href="ch08.xhtml#ch8lis2">Listing 8-2</a> applies the trained model to the test data. To train the model in the first place, we’ll use sklearn. The code to train the model is in <a href="ch08.xhtml#ch8lis3">Listing 8-3</a>.</p>&#13;
<p class="programs" id="ch8lis3">   import numpy as np<br/>&#13;
   import pickle<br/>&#13;
   from sklearn.neural_network import MLPClassifier<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_186"/>   xtrain= np.load("iris2_train.npy")<br/>&#13;
   ytrain= np.load("iris2_train_labels.npy")<br/>&#13;
   xtest = np.load("iris2_test.npy")<br/>&#13;
   ytest = np.load("iris2_test_labels.npy")<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> clf = MLPClassifier(<br/>&#13;
         <span class="ent">❷</span> hidden_layer_sizes=(3,2),<br/>&#13;
         <span class="ent">❸</span> activation="logistic",<br/>&#13;
           solver="adam", tol=1e-9,<br/>&#13;
           max_iter=5000,<br/>&#13;
           verbose=True)<br/>&#13;
   clf.fit(xtrain, ytrain)<br/>&#13;
   prob = clf.predict_proba(xtest)<br/>&#13;
   score = clf.score(xtest, ytest)<br/>&#13;
<br/>&#13;
<span class="ent">❹</span> w12 = clf.coefs_[0]<br/>&#13;
   w23 = clf.coefs_[1]<br/>&#13;
   w34 = clf.coefs_[2]<br/>&#13;
   b1 = clf.intercepts_[0]<br/>&#13;
   b2 = clf.intercepts_[1]<br/>&#13;
   b3 = clf.intercepts_[2]<br/>&#13;
   weights = [w12,b1,w23,b2,w34,b3]<br/>&#13;
   pickle.dump(weights, open("iris2_weights.pkl","wb"))<br/>&#13;
<br/>&#13;
   print()<br/>&#13;
   print("Test results:")<br/>&#13;
   print("  Overall score: %0.7f" % score)<br/>&#13;
   print()<br/>&#13;
   for i in range(len(ytest)):<br/>&#13;
       p = 0 if (prob[i,1] &lt; 0.5) else 1<br/>&#13;
       print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))<br/>&#13;
   print()</p>&#13;
<p class="figcap"><em>Listing 8-3: Using sklearn to train the iris neural network. See</em> nn_iris_mlpclassifier.py.</p>&#13;
<p class="indent">First, we load the training and testing data from disk. These are the same files we created previously. Then we set up the neural network object, an instance of <span class="literal">MLPClassifier</span> <span class="ent">❶</span>. The network has two hidden layers, the first with three nodes and the second with two nodes <span class="ent">❷</span>. This matches the architecture in <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>. The network is also using <em>logistic</em> layers <span class="ent">❸</span>. This is another name for a sigmoid layer. We train the model by calling <span class="literal">fit</span> just as we did for other sklearn model types. Since we set <span class="literal">verbose</span> to <span class="literal">True</span>, we’ll get output showing us the loss for each iteration.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_187"/>Calling <span class="literal">predict_proba</span> gives us the output probabilities on the test data. This method is also supported by most other sklearn models. This is the model’s certainty as to the assigned output label. We then call <span class="literal">score</span> to calculate the score over the test set as we have done before.</p>&#13;
<p class="indent">We want to store the learned weights and biases so we can use them with our test code. We can pull them directly from the trained model <span class="ent">❹</span>. These are packed into a list (<span class="literal">weights</span>) and dumped to a Python pickle file.</p>&#13;
<p class="indent">The remaining code prints the results of running the sklearn trained model against the held-out test data. For example, a particular run of this code gives</p>&#13;
<p class="programs">Test results:<br/>&#13;
  Overall score: 1.0000000<br/>&#13;
<br/>&#13;
000: 0 - 0, 0.0705069<br/>&#13;
001: 1 - 1, 0.8066224<br/>&#13;
002: 0 - 0, 0.0308244<br/>&#13;
003: 0 - 0, 0.0205917<br/>&#13;
004: 1 - 1, 0.9502825<br/>&#13;
005: 0 - 0, 0.0527558<br/>&#13;
006: 1 - 1, 0.9455174<br/>&#13;
007: 0 - 0, 0.0365360<br/>&#13;
008: 1 - 1, 0.9471218<br/>&#13;
009: 0 - 0, 0.0304762<br/>&#13;
010: 0 - 0, 0.0304762<br/>&#13;
011: 0 - 0, 0.0165365<br/>&#13;
012: 1 - 1, 0.9453844<br/>&#13;
013: 0 - 0, 0.0527558<br/>&#13;
014: 1 - 1, 0.9495079<br/>&#13;
015: 1 - 1, 0.9129983<br/>&#13;
016: 1 - 1, 0.8931552<br/>&#13;
017: 0 - 0, 0.1197567<br/>&#13;
018: 0 - 0, 0.0406094<br/>&#13;
019: 0 - 0, 0.0282220<br/>&#13;
020: 1 - 1, 0.9526721<br/>&#13;
021: 0 - 0, 0.1436263<br/>&#13;
022: 1 - 1, 0.9446458</p>&#13;
<p class="noindent">indicating that the model was perfect against the small test dataset. The output shows the sample number, the actual class label, the assigned class label, and the output probability of being class 1. If we run the pickle file holding the sklearn network’s weights and biases through our evaluation code, we see that the output probabilities are precisely the same as the preceding code, indicating that our hand-generated neural network implementation is working correctly.</p>&#13;
<h3 class="h3" id="lev1_52"><span epub:type="pagebreak" id="page_188"/>Summary</h3>&#13;
<p class="noindent">In this chapter, we discussed the anatomy of a neural network. We described the architecture, the arrangement of nodes, and the connections between them. We discussed the output layer nodes and the functions they compute. We then saw that all the weights and biases could be conveniently represented by matrices and vectors. Finally, we presented a simple network for classifying a subset of the iris data and showed how it could be trained and evaluated.</p>&#13;
<p class="indent">Now that we have our feet wet, let’s move on and dive into the theory behind neural networks.</p>&#13;
</div></body></html>