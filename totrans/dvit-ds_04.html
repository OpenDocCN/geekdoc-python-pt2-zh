<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_75" title="75"/>4</span><br/>
<span class="ChapterTitle">A/B Testing</span></h1>
</header>
<figure class="opener">
<img alt="" height="200" src="image_fi/book_art/chapterart.png" width="200"/>
</figure>
<p class="ChapterIntro">In the preceding chapter, we discussed the scientific practice of observing two groups and making quantitative judgments about how they relate to each other. But scientists (including data scientists) do more than just observe preexisting differences. A huge part of science consists of creating differences experimentally and then drawing conclusions. In this chapter, we’ll discuss how to conduct those kinds of experiments in business.</p>
<p>We’ll start by discussing the need for experimentation and our motivations for testing. We’ll cover how to properly set up experiments, including the need for randomization. Next, we’ll detail the steps of A/B testing and the champion/challenger framework. We’ll conclude by describing nuances like the exploration/exploitation trade-off, as well as ethical concerns.</p>
<h2 id="h1-502888c04-0001"><span epub:type="pagebreak" id="Page_76" title="76"/>The Need for Experimentation</h2>
<p class="BodyFirst">Let’s return to the scenario we outlined in the second half of <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>. Imagine that you’re running a computer company and maintain email marketing lists that your customers can choose to subscribe to. One email list is designed for customers who are interested in your desktop computers, and the other email list is for customers interested in your laptops. You can download two fabricated datasets for this scenario from <a class="LinkURL" href="https://bradfordtuckfield.com/desktop.csv">https://bradfordtuckfield.com/desktop.csv</a> and <a class="LinkURL" href="https://bradfordtuckfield.com/laptop.csv">https://bradfordtuckfield.com/laptop.csv</a>. If you save them to the same directory you’re running Python from, you can read these hypothetical lists into Python as follows:</p>
<pre><code>import pandas as pd
desktop=pd.read_csv('desktop.csv')
laptop=pd.read_csv('laptop.csv')</code></pre>
<p>You can run <code class="bold">print(desktop.head())</code> and <code class="bold">print(laptop.head())</code> to see the first five rows of each dataset.</p>
<p>In <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>, you learned how to use simple t-tests to detect differences between our datasets, as follows:</p>
<pre><code>import scipy.stats
print(scipy.stats.ttest_ind(desktop['spending'],laptop['spending']))
print(scipy.stats.ttest_ind(desktop['age'],laptop['age']))
print(scipy.stats.ttest_ind(desktop['visits'],laptop['visits']))</code></pre>
<p>Here, we import the SciPy package’s <code>stats</code> module so we can use it for t-tests. Then we print the results of three separate t-tests: one comparing the spending of desktop and laptop subscribers, one comparing the ages of desktop and laptop subscribers, and one comparing the number of recorded website visits of desktop and laptop subscribers. We can see that the first <em>p</em>-value is less than 0.05, indicating that these groups are significantly different in their spending levels (at the 5 percent significance level), just as we concluded in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>.</p>
<p>After determining that desktop subscribers are different from laptop subscribers, we can conclude that we should send them different marketing emails. However, this fact alone is not enough to completely guide our marketing strategy. Just knowing that our desktop subscriber group spends a little less than the laptop subscriber group doesn’t tell us whether crafting long messages or short ones would lead to better sales, or whether using red text or blue text would get us more clicks, or whether informal or formal language would improve customer loyalty most. In some cases, past research published in academic marketing journals can give us hints about what will work best. But even when relevant research exists, every company has its own unique set of customers that may not respond to marketing in exactly the same way that past research indicates.</p>
<p>We need a way to generate new data that’s never been collected or published before, so we can use that data to answer new questions about the new situations that we regularly face. Only if we can generate this kind of new data can we reliably learn about what will work best in our efforts to <span epub:type="pagebreak" id="Page_77" title="77"/>grow our business with our particular set of unique customers. We’ll spend the rest of this chapter discussing an approach that will accomplish this.</p>
<p><em>A/B testing</em>, the focus of this chapter, uses experiments to help businesses determine which practices will give them the greatest chances of success. It consists of a few steps: experimental design, random assignment into treatment and control groups, careful measurement of outcomes, and finally, statistical comparison of outcomes between groups.</p>
<p>The way we’ll do statistical comparisons will be familiar: we’ll use the t-tests introduced in the previous chapter. While t-tests are a part of the A/B testing process, they are not the only part. A/B testing is a process for collecting new data, which can then be analyzed using tests like the t-test. Since we’ve already introduced t-tests, we won’t focus on them in this chapter. Instead, we’ll focus on all the other steps of A/B testing.</p>
<h2 id="h1-502888c04-0002">Running Experiments to Test New Hypotheses</h2>
<p class="BodyFirst">Let’s consider just one hypothesis about our customers that might interest us. Suppose we’re interested in studying whether changing the color of text in our marketing emails from black to blue will increase the revenue we earn as a result of the emails. Let’s express two hypotheses related to this:</p>
<p class="RunInPara"><span class="RunInHead">Hypothesis 0</span>  Changing the color of text in our emails from black to blue will have no effect on revenues.</p>
<p class="RunInPara"><span class="RunInHead">Hypothesis 1</span>  Changing the color of text in our emails from black to blue will lead to a change in revenues (either an increase or a decrease).</p>
<p>We can use the testing hypothesis framework covered in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span> to test our null hypothesis (Hypothesis 0) and decide whether we want to reject it in favor of its alternative hypothesis (Hypothesis 1). The only difference is that in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>, we tested hypotheses related to data we had already collected. Here, our datasets do not include information about blue-text and black-text emails. So, extra steps are required before we perform hypothesis testing: designing an experiment, running an experiment, and collecting data related to the experiment’s results.</p>
<p>Running experiments may not sound so difficult, but some tricky parts are important to get exactly right. To do the hypothesis test we just outlined, we’ll need data from two groups: a group that has received a blue-text email and a group that has received a black-text email. We’ll need to know how much revenue we received from each member of the group that received the blue-text email and how much revenue we received from each member of the group that received the black-text email.</p>
<p>After we have that, we can do a simple t-test to determine whether the revenue collected from the blue-text group differed significantly from the revenue collected from the black-text group. In this chapter, we’ll use a 5 percent significance level for all of our tests—that is, we’ll reject the null hypothesis and accept our alternative hypothesis if our <em>p</em>-value is less than 0.05. When we do our t-test, if the revenues are significantly different, we can reject our null hypothesis (Hypothesis 0). <span epub:type="pagebreak" id="Page_78" title="78"/>Otherwise, we won’t reject our null hypothesis, and until something convinces us otherwise, we’ll accept its assertion that blue and black text lead to equal revenues.</p>
<p>We need to split our population of interest into two subgroups and send a blue-text email to one subgroup and a black-text email to our other subgroup so we can compare revenues from each group. For now, let’s focus on desktop subscribers only and split our desktop dataframe into two subgroups.</p>
<p>We can split a group into two subgroups in many ways. One possible choice is to split our dataset into a group of younger people and a group of older people. We might split our data this way because we believe that younger people and older people might be interested in different products, or we might do it this way just because age is one of the few variables that appears in our data. Later, we’ll see that this way of splitting our group into subgroups will lead to problems in our analysis, and we’ll discuss better ways to create subgroups. But since this method of splitting into subgroups is simple and easy, let’s start by trying it to see what happens:</p>
<pre><code>import numpy as np
medianage=np.median(desktop['age'])
groupa=desktop.loc[desktop['age']&lt;=medianage,:]
groupb=desktop.loc[desktop['age']&gt;medianage,:]</code></pre>
<p>Here, we import the NumPy package, giving it the alias <code>np</code>, so we can use its <code>median()</code> method. Then we simply take the median age of our group of desktop subscribers and create <code>groupa</code>, a subset of our desktop subscribers whose age is below or equal to the median age, and <code>groupb</code>, a subset of our desktop subscribers whose age is above the median age.</p>
<p>After creating <code>groupa</code> and <code>groupb</code>, you can send these two dataframes to your marketing team members and instruct them to send different emails to each group. Suppose they send the black-text email to <code>groupa</code> and the blue-text email to <code>groupb</code>. In every email, they include links to new products they want to sell, and by tracking who clicks which links and their purchases, the team members can measure the total revenue earned from each individual email recipient.</p>
<p>Let’s read in some fabricated data that shows hypothetical outcomes for members of our two groups. This data can be downloaded from <a class="LinkURL" href="https://bradfordtuckfield.com/emailresults1.csv">https://bradfordtuckfield.com/emailresults1.csv</a>; store it in the same directory where you run Python. Then you can read it into your Python session as follows:</p>
<pre><code>emailresults1=pd.read_csv('emailresults1.csv')</code></pre>
<p>If you run <code class="bold">print(emailresults1.head())</code> in Python, you can see the first rows of this new data. It’s a simple dataset: each row corresponds to one individual desktop email subscriber, whose ID is identified in the <code>userid</code> column. The <code>revenue</code> column records the revenue your company earned from each user as a result of this email campaign.</p>
<p><span epub:type="pagebreak" id="Page_79" title="79"/>It will be useful to have this new revenue information in the same dataframe as our other information about each user. Let’s join the datasets:</p>
<pre><code>groupa_withrevenue=groupa.merge(emailresults1,on='userid')
groupb_withrevenue=groupb.merge(emailresults1,on='userid')</code></pre>
<p>In this snippet, we use the pandas <code>merge()</code> method to combine our dataframes. We specify <code>on='userid'</code>, meaning that we take the row of <code>emailresults1</code> that corresponds to a particular <code>userid</code> and merge it with the row of <code>groupa</code> that corresponds to that same <code>userid</code>. The end result of using <code>merge()</code> is a dataframe in which every row corresponds to a particular user identified by their unique <code>userid</code>. The columns tell us not only about their characteristics like age but also about the revenue we earned from them as a result of our recent email campaign.</p>
<p>After preparing our data, it’s simple to perform a t-test to check whether our groups are different. We can do it in one line, as follows:</p>
<pre><code>print(scipy.stats.ttest_ind(groupa_withrevenue['revenue'],groupb_withrevenue['revenue']))</code></pre>
<p>When you run this code, you’ll get the following result:</p>
<pre><code>Ttest_indResult(statistic=-2.186454851070545, pvalue=0.03730073920038287)</code></pre>
<p>The important part of this output is the <code>pvalue</code> variable, which tells us the <em>p</em>-value of our test. We can see that the result says that <em>p </em>= 0.037, approximately. Since <em>p</em> &lt; 0.05, we can conclude that this is a statistically significant difference. We can check the size of the difference:</p>
<pre><code>print(np.mean(groupb_withrevenue['revenue'])-np.mean(groupa_withrevenue['revenue']))</code></pre>
<p>The output is 125.0. The average <code>groupb</code> customer has outspent the average <code>groupa</code> customer by $125. This difference is statistically significant, so we reject Hypothesis 0 in favor of Hypothesis 1, concluding (for now, at least) that the blue text in marketing emails leads to about $125 more in revenue per user than black text.</p>
<p>What we have just done was an <em>experiment</em>. We split a population into two groups, performed different actions on each group, and compared the results. In the context of business, such an experiment is often called an <em>A/B test</em>. The <em>A/B</em> part of the name refers to the two groups, Group A and Group B, whose different responses to emails we compared. Every A/B test follows the same pattern we went through here: a split into two groups, application of a different treatment (for example, sending different emails) to each group, and statistical analysis to compare the groups’ outcomes and draw conclusions about which treatment is better.</p>
<p>Now that we’ve successfully conducted an A/B test, we may want to conclude that the effect of blue text is to increase spending by $125. However, something is wrong with the A/B test we ran: it’s <em>confounded</em>. To see what we mean, consider <a href="#table4-1" id="tableanchor4-1">Table 4-1</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table4-1">Table 4-1</a>: Differences Between Groups<span epub:type="pagebreak" id="Page_80" title="80"/></p></figcaption>
<table border="1" id="table-502888c04-0001">
<thead>
<tr>
<td><b/></td>
<td><b>Group A</b></td>
<td><b>Group B</b></td>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="1"><b>Personal characteristics</b></td>
<td>Younger<br/>(Same as B in other ways)</td>
<td>Older<br/>(Same as A in other ways)</td>
</tr>
<tr>
<td><b>Email text color</b></td>
<td>Black</td>
<td>Blue</td>
</tr>
<tr>
<td><b>Average revenue per user</b></td>
<td>$104</td>
<td>$229</td>
</tr>
</tbody>
</table>
</figure>
<p>We can see the important features of Group A and Group B. Our t-test comparing spending found that their spending levels were significantly different. We want an explanation for why they’re different, and any explanation of different outcomes will have to rely on the differences listed in <a href="#table4-1">Table 4-1</a>. We want to be able to conclude that the difference in spending can be explained by the difference in the text color. However, that difference coexists with another difference: age.</p>
<p>We can’t be certain that the difference in spending levels is due to text color rather than age. For example, perhaps no one even noticed the text difference, but older people tend to be wealthier and more eager to buy your products than young people. If so, our A/B test didn’t test for the effect of blue text, but rather for the effect of age or wealth. We intended to study only the effect of text color in this A/B test, and now we don’t know whether we truly studied that or whether we studied age, wealth, or something else. It would be better if our A/B test had a simpler, non-confounded design like the one illustrated in <a href="#table4-2" id="tableanchor4-2">Table 4-2</a>.</p>
<figure>
<figcaption class="TableTitle"><p><a id="table4-2">Table 4-2</a>: A Non-confounded A/B Test Design</p></figcaption>
<table border="1" id="table-502888c04-0002">
<thead>
<tr>
<td><b/></td>
<td><b>Group C</b></td>
<td><b>Group D</b></td>
</tr>
</thead>
<tbody>
<tr>
<td><b>Personal characteristics</b></td>
<td>(Same as D in every way)</td>
<td>(Same as C in every way)</td>
</tr>
<tr>
<td><b>Email text color</b></td>
<td>Black</td>
<td>Blue</td>
</tr>
<tr>
<td><b>Average revenue per user</b></td>
<td>$104</td>
<td>$229</td>
</tr>
</tbody>
</table>
</figure>
<p><a href="#table4-2">Table 4-2</a> imagines that we had split the users into hypothetical groups called C and D, which are identical in all personal characteristics, but differ only in the text of the emails they received. In this hypothetical scenario, the spending difference can be explained only by the different text colors sent to each group because that’s the only difference between them. We should have split our groups in a way that ensured that the only differences between groups were in our experimental treatment, not in the group members’ preexisting characteristics. If we had done so, we would have avoided having a confounded experiment.</p>
<h3 id="h2-502888c04-0001">Understanding the Math of A/B Testing</h3>
<p class="BodyFirst">We can also express these notions mathematically. We can use the common statistical notation <em>E</em>() to refer to the expected value. So <em>E</em>(<em>A’s revenue <span epub:type="pagebreak" id="Page_81" title="81"/>with blk text</em>) will mean <em>the expected value of revenue we would earn by sending a black-text email to Group A</em>. We can write two simple equations that describe the relationship between the revenue we expect to earn from black text, the effect of our experiment, and the revenue we expect to earn from blue text:</p>
<p class="Equation"><em>E</em>(<em>A’s revenue with blk text</em>) + <em>E</em>(<em>effect of changing blk → blue on A</em>) = <em>E</em>(<em>A’s revenue with blue text</em>)</p>
<p class="Equation"><em>E</em>(<em>B’s revenue with blk text</em>) + <em>E</em>(<em>effect of changing blk → blue on B</em>) = <em>E</em>(<em>B’s revenue with blue text</em>)</p>
<p>To decide whether to reject Hypothesis 0, we need to solve for the effect sizes: <em>E</em>(<em>effect of changing blk → blue on A</em>) and <em>E</em>(<em>effect of changing blk → blue on B</em>). If either of these effect sizes is different from 0, we should reject Hypothesis 0. By performing our experiment, we found <em>E</em>(<em>A’s revenue with blk text</em>) = 104 and <em>E</em>(<em>B’s revenue with blue text</em>) = 229. After knowing these values, we have the following equations:</p>
<p class="Equation">104 + <em>E</em>(<em>effect of changing blk → blue on A</em>) = <em>E</em>(<em>A’s revenue with blue text</em>)</p>
<p class="Equation"><em>E</em>(<em>B’s revenue with blk text</em>) + <em>E</em>(<em>effect of changing blk → blue on B</em>) = 229</p>
<p>But this still leaves many variables we don’t know, and we’re not yet able to solve for <em>E</em>(<em>effect of changing blk → blue on A</em>) and <em>E</em>(<em>effect of changing blk → blue on B</em>). The only way we’ll be able to solve for our effect sizes will be if we can simplify these two equations. For example, if we knew that <em>E</em>(<em>A’s revenue with blk text</em>) = <em>E</em>(<em>B’s revenue with blk text</em>), and <em>E</em>(<em>effect of changing blk → blue on A</em>) = <em>E</em>(<em>effect of changing blk → blue on B</em>), and <em>E</em>(<em>A’s revenue with blue Text</em>) = <em>E</em>(<em>B’s revenue with blue text</em>), then we could reduce these two equations to just one simple equation. If we knew that our groups were identical before our experiment, we would know that all of these expected values were equal, and we could simplify our two equations to the following easily solvable equation:</p>
<p class="Equation">104 + <em>E</em>(<em>effect of changing blk → blue on everyone</em>) = 229</p>
<p>With this, we can be sure that the effect of blue text is a $125 revenue increase. This is why we consider it so important to design non-confounded experiments in which the groups have equal expected values for personal characteristics. By doing so, we’re able to solve the preceding equations and be confident that our measured effect size is actually the effect of what we’re studying and not the result of different underlying characteristics.</p>
<h3 id="h2-502888c04-0002">Translating the Math into Practice</h3>
<p class="BodyFirst">We know what to do mathematically, but we need to translate that into practical action. How should we ensure that <em>E</em>(<em>A’s revenue with blk text</em>) = <em>E</em>(<em>B’s revenue with blk text</em>), and how should we ensure that the other expected values are all the same? In other words, how can we ensure that <span epub:type="pagebreak" id="Page_82" title="82"/>our study design looks like <a href="#table4-2">Table 4-2</a> instead of <a href="#table4-1">Table 4-1</a>? We need to find a way to select subgroups of our desktop subscriber list that are expected to be identical.</p>
<p>The simplest way to select subgroups that are expected to be identical is to select them randomly. We mentioned this briefly in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>: every random sample from a population has an expected value equal to the population mean. So, we expect that two random samples from the same population won’t differ from each other significantly.</p>
<p>Let’s perform an A/B test on our laptop subscriber list, but this time we’ll use randomization to select our groups to avoid having a confounded experimental design. Suppose that in this new A/B test, we want to test whether adding a picture to a marketing email will improve revenue. We can proceed just as we did before: we split the laptop subscriber list into two subgroups, and we send different emails to each subgroup. The difference is that this time, instead of splitting based on age, we perform a random split:</p>
<pre><code>np.random.seed(18811015)
laptop.loc[:,'groupassignment1']=1*(np.random.random(len(laptop.index))&gt;0.5)
groupc=laptop.loc[laptop['groupassignment1']==0,:].copy()
groupd=laptop.loc[laptop['groupassignment1']==1,:].copy()</code></pre>
<p>In this snippet, we use the NumPy <code>random.random()</code> method to generate a column that consists of randomly generated 0s and 1s. We can interpret a 0 to mean that a user belongs to Group C, and a 1 to mean that a user belongs to group D. When we generate 0s and 1s randomly like this, the groups could end up with different sizes. However, here we use a <em>random seed</em> (in the first line, <code>np.random.seed(18811015)</code>). Every time anyone uses this random seed, their “randomly” generated column of 0s and 1s will be identical. That means that if you use this random seed, your results at home should be the same as the results here in the book. Using a random seed is not necessary, but if you use the same random seed we used here, you should find that both Group C and Group D have 15 members.</p>
<p>After generating this random column of 0s and 1s that indicates the group assignment of each customer, we create two smaller dataframes, <code>groupc</code> and <code>groupd</code>, that contain user IDs and information about the users in each subgroup.</p>
<p>You can send the group membership information to your marketing team members and ask them to send the right emails to the right groups. One group, either C or D, should receive an email without a picture, and the other group, either D or C, should receive an email with a picture. Then, suppose that the marketing team sends you a file containing the results of this latest A/B test. You can download a fabricated dataset containing hypothetical results from <a class="LinkURL" href="https://bradfordtuckfield.com/emailresults2.csv">https://bradfordtuckfield.com/emailresults2.csv</a>. After you store it in the same place where you’re running Python, let’s read the results of this email campaign into Python as follows:</p>
<pre><code>emailresults2=pd.read_csv('emailresults2.csv')</code></pre>
<p><span epub:type="pagebreak" id="Page_83" title="83"/>Again, let’s join our email results to our group dataframes, just as we did before:</p>
<pre><code>groupc_withrevenue=groupc.merge(emailresults2,on='userid')
groupd_withrevenue=groupd.merge(emailresults2,on='userid')</code></pre>
<p>And again, we can use a t-test to check whether the revenue resulting from Group C is different from the revenue we get from Group D:</p>
<pre><code>print(scipy.stats.ttest_ind(groupc_withrevenue['revenue'],groupd_withrevenue['revenue']))</code></pre>
<p>We find that the <em>p</em>-value is less than 0.05, indicating that the difference between the groups is statistically significant. This time, our experiment isn’t confounded, because we used random assignment to ensure that the differences between groups are the result of our different emails, not the result of different characteristics of each group. Since our experiment isn’t confounded, and since we find a significant difference between the revenues earned from Group C and Group D, we conclude that including the picture in the email has a nonzero effect. If the marketing team tells us that it sent the picture only to Group D, we can find the estimated size of the effect easily:</p>
<pre><code>print(np.mean(groupd_withrevenue['revenue'])-np.mean(groupc_withrevenue['revenue']))</code></pre>
<p>We calculate the estimated effect here with subtraction: the mean revenue obtained from subjects in Group D minus the mean revenue obtained from subjects in Group C. The difference between mean revenue from Group C and mean revenue from Group D, about $260, is the size of the effect of our experiment.</p>
<p>The process we follow for A/B testing is really quite simple, but it’s also powerful. We can use it for a wide variety of questions that we might want to answer. Anytime you’re unsure about an approach to take in business, especially in user interactions and product design, considering an A/B test as an approach to learn the answer is worthwhile. Now that you know the process, let’s move on and understand its nuances.</p>
<h2 id="h1-502888c04-0003">Optimizing with the Champion/Challenger Framework</h2>
<p class="BodyFirst">When we’ve crafted a great email, we might call it our <em>champion</em> email design: the one that, according to what we know so far, we think will perform the best. After we have a champion email design, we may wish to stop doing A/B testing and simply rest on our laurels, collecting money indefinitely from our “perfect” email campaigns.</p>
<p>But this isn’t a good idea, for a few reasons. The first is that times change. Fads in design and marketing change quickly, and a marketing effort that seems exciting and effective today may soon seem dated and outmoded. Like all champions, your champion email design will become weaker and less effective as it ages. Even if design and marketing fads <em>don’t</em> change, your champion will eventually seem boring as the novelty wears off: new stimuli are more likely to get people’s attention.</p>
<p><span epub:type="pagebreak" id="Page_84" title="84"/>Another reason that you shouldn’t stop A/B testing is that your customer base will change. You’ll lose some old customers and gain new ones. You’ll release new products and enter new markets. As your customer mix changes, the types of emails that they tend to respond to will change as well, and constant A/B testing will enable you to keep up with their changing characteristics and preferences.</p>
<p>A final reason to continue A/B testing is that although your champion likely is good, you might not have optimized it in every possible way. A dimension you haven’t tested yet could enable you to have an even better champion that gets even better performance. If we can successfully run one A/B test and learn one thing, we’ll naturally want to continue to use our A/B testing skills to learn more and more and to increase profits higher and higher.</p>
<p>Suppose you have a champion email and want to continue A/B testing to try to improve it. You do another random split of your users, into a new Group A and a new Group B. You send the champion email to Group A. You send another email to Group B that differs from the champion email in one way that you want to learn about; for example, maybe it uses formal rather than informal language. When we compare the revenues from Group A and Group B after the email campaign, we’ll be able to see whether this new email performs better than the champion email.</p>
<p>Since the new email is in direct competition with the champion email, we call it the <em>challenger</em>. If the champion performs better than the challenger, the champion retains its champion status. If the challenger performs better than the champion, that challenger becomes the new champion.</p>
<p>This process can continue indefinitely: we have a champion that represents the state of the art of whatever we’re doing (marketing emails, in this case). We constantly test the champion by putting it in direct competition with a succession of challengers in A/B tests. Each challenger that leads to significantly better outcomes than the champion becomes the new champion and is, in turn, put into competition against new challengers later.</p>
<p>This endless process is called the <em>champion/challenger framework</em> for A/B tests. It’s meant to lead to continuous improvement, continuous refinement, and asymptotic optimization to get to the best-possible performance in all aspects of business. The biggest tech companies in the world run literally hundreds of A/B tests per day, with hundreds of challengers taking on hundreds of champions, sometimes defeating them and sometimes being defeated. The champion/challenger framework is a common approach for setting up and running A/B tests for the most important and most challenging parts of your business.</p>
<h2 id="h1-502888c04-0004">Preventing Mistakes with Twyman’s Law and A/A Testing</h2>
<p class="BodyFirst">A/B testing is a relatively simple process from beginning to end. Nevertheless, we are all human and make mistakes. In any data science effort, not just A/B <span epub:type="pagebreak" id="Page_85" title="85"/>testing, it’s important to proceed carefully and constantly check whether we’ve done something wrong. One piece of evidence that often indicates that we’ve done something wrong is that things are going too well.</p>
<p>How could it be bad for things to go too well? Consider a simple example. You perform an A/B test: Group A gets one email, and Group B gets a different one. You measure revenue from each group afterward and find that the average revenue earned from members of Group A is about $25, while the average revenue earned from members of Group B is $99,999. You feel thrilled about the enormous revenue you earned from Group B. You call all your colleagues to an emergency meeting and tell them to stop everything they’re doing and immediately work on implementing the email that Group B got and pivot the whole company strategy around this miracle email.</p>
<p>As your colleagues are working around the clock on sending the new email to everyone they know, you start to feel a nagging sense of doubt. You think about how unlikely it is that a single email campaign could plausibly earn almost $100,000 in revenue per recipient, especially when your other campaigns are earning only about $25 per user. You think about how $99,999, the amount of revenue you supposedly earned per user, is five identical digits repeated. Maybe you remember a conversation you had with a database administrator who told you that your company database automatically inserts 99999 every time a database error occurs or data is missing. Suddenly, you realize that your email campaign didn’t really earn $99,999 per user, but rather a database error for Group B caused the appearance of the apparently miraculous result.</p>
<p>A/B testing is a simple process from a data science point of view, but it can be quite complex from a practical and social point of view. For example, in any company larger than a tiny startup, the creative people designing marketing emails will be different from the technology people who maintain the databases that record revenues per user. Other groups may be involved in little parts of A/B testing: maybe a group that maintains the software used to schedule and send out emails, maybe a group that creates art that the email marketing team asks for, and maybe others.</p>
<p>With all these groups and steps involved, many possible chances exist for miscommunication and small errors. Maybe two different emails are designed, but the person who’s in charge of sending them out doesn’t understand A/B testing and copies and pastes the same email to both groups. Maybe they accidentally paste in something that’s not even supposed to be in the A/B test at all. In our example, maybe the database that records revenues encounters an error and puts 99999 in the results as an error code, which others mistakenly interpret as a high revenue. No matter how careful we try to be, mistakes and miscommunications will always find a way to happen.</p>
<p>The inevitability of mistakes should lead us to be naturally suspicious of anything that seems too good, bad, interesting, or strange to be true. This natural suspicion is advocated by<em> Twyman’s law</em>, which states that “any figure that looks interesting or different is usually wrong.” This law has been restated in several ways, including “any statistic that appears interesting is <span epub:type="pagebreak" id="Page_86" title="86"/>almost certainly a mistake” and “the more unusual or interesting the data, the more likely it is to have been the result of an error.”</p>
<p>Besides extreme carefulness and natural suspicion of good news, we have another good way to prevent the kinds of interpretive mistakes that Twyman’s law warns against: <em>A/A testing</em>. This type of testing is just what it sounds like; we go through the steps of randomization, treatment, and comparison of two groups just as in A/B testing, but instead of sending two different emails to our two randomized groups, we send the identical email to each group. In this case, we expect the null hypothesis to be true, and we won’t be gullibly convinced by a group that appears to get $100,000 more revenue than the other group.</p>
<p>If we consistently find that A/A tests lead to statistically significant differences between groups, we can conclude that our process has a problem: a database gone haywire, a t-test being run incorrectly, an email being pasted wrong, randomization performed incorrectly, or something else. An A/A test would also help us realize that the first test described in this chapter (where Group A consists of younger people and Group B consists of older people) was confounded, since we would know that differences between the results of an A/A test must be due to the differences in age rather than differences between emails. A/A testing can be a useful sanity check that can prevent us from getting carried away by the kind of unusual, interesting, too-good-to-be-true results that Twyman’s law warns us about.</p>
<h2 id="h1-502888c04-0005">Understanding Effect Sizes</h2>
<p class="BodyFirst">In the first A/B test we ran, we observed a difference of $125 between the Group A users who received a black-text email and the Group B users who received a blue-text email. This $125 difference between groups is also called the A/B test’s <em>effect size</em>. It’s natural to try to form a judgment about whether we should consider this $125 effect size a small effect, a medium effect, or a large effect.</p>
<p>To judge whether an effect is small or large, we have to compare it to something else. Consider the following list of nominal GDP figures (in US dollars, as of 2019) for Malaysia, Myanmar, and the Marshall Islands, respectively:</p>
<pre><code>gdps=[365303000000,65994000000,220000000]</code></pre>
<p>When we look at these numbers, $125 starts to seem pretty small. For example, consider the standard deviation of our <code>gdps</code> list:</p>
<pre><code>print(np.std(gdps))</code></pre>
<p>The result is 158884197328.32672, or about $158,884,197,328 (almost $159 billion). The standard deviation is a common way to measure how dispersed a dataset is. If we observe a difference between two countries’ GDPs that’s about $80 billion, we don’t think of that as outrageously big or outrageously small, because it means those countries are about half of a standard <span epub:type="pagebreak" id="Page_87" title="87"/>deviation apart, a common magnitude of difference. Instead of expressing the difference as an $80 billion difference, you might say that the two countries’ GDPs differed by about half of a standard deviation, and expect to be understood by anyone with some statistical training.</p>
<p>By contrast, if someone tells you that two countries have GDPs that differ by 112 trillion kyat (the currency of Myanmar), you might be unsure whether that difference is large or small if you’ve never learned the value of 1 kyat (112 trillion kyat is equal to about $80 billion at the time of writing). Many currencies exist in the world, and their relative and absolute values change all the time. A standard deviation, on the other hand, isn’t specific to any particular country and is not affected by inflation, making it a useful unit of measurement.</p>
<p>We can use standard deviations as measurements in other domains as well. Someone from Europe may be used to using meters to express heights. When you tell your European data scientist friend about a man who’s 75 inches tall, they may feel puzzled about whether that’s tall or short or average if they’re not used to conversion from inches. But, if you tell them that he’s about two standard deviations taller than the mean, they should immediately be able to understand that he’s pretty tall but not a record-breaking height. Observing someone who’s more than three standard deviations above the mean height will be much rarer, and we can know that whether we’re measuring in meters or inches or any other units.</p>
<p>When we talk about the $125 effect size of our A/B test, let’s try to think of it in terms of standard deviations as well. Compared to the standard deviation of the GDP measurements we’ve seen, $125 is small potatoes:</p>
<pre><code>print(125/np.std(gdps))</code></pre>
<p>The output is about 7.9 · 10<sup>–10</sup>, which shows us that the $125 effect size is a little more than 1 one-billionth of the standard deviation of our GDP figures. Compared to the world of GDP measurements, a $125 difference in GDP is like being a micrometer taller than your friend—not even enough to notice without extremely precise measurement technology.</p>
<p>By contrast, suppose we conduct a survey of the prices of burgers at local restaurants. Maybe we find the following prices:</p>
<pre><code>burgers=[9.0,12.99,10.50]</code></pre>
<p>We can check this standard deviation as well:</p>
<pre><code>print(np.std(burgers))</code></pre>
<p>The standard deviation of our burger price data is about 1.65. So, two countries’ GDPs differing by about $80 billion is roughly comparable to two burger prices differing by about 80 cents: both represent about half of a standard deviation in their respective domains. When we compare a $125 effect size to this, we see that it’s huge:</p>
<pre><code>print(125/np.std(burgers))</code></pre>
<p><span epub:type="pagebreak" id="Page_88" title="88"/>We see that $125 is about 75.9 burger price standard deviations. Seeing a $125 difference in burger prices in your town is therefore something like seeing a man who is over 20 feet tall—unheard of.</p>
<p>By measuring our effect size in terms of the standard deviation of different datasets, we can easily make comparisons, not just between different domains with the same units (GDP in dollars versus burger prices in dollars) but also between different domains that use totally different units (burger prices in dollars versus height in inches). The metric we’ve calculated several times here—an effect size divided by a relevant standard deviation—is called <em>Cohen’s d</em>, a common metric for measuring effect sizes. Cohen’s <em>d</em> is just the number of standard deviations that two populations’ means are apart from each other. We can calculate Cohen’s <em>d</em> for our first A/B test as follows:</p>
<pre><code>print(125/np.std(emailresults1['revenue']))</code></pre>
<p>We see that the result is about 0.76. A common convention when we’re working with Cohen’s <em>d</em> is to say that if Cohen’s <em>d</em> is about 0.2 or lower, we have a small effect; if Cohen’s <em>d</em> is about 0.5, we have a medium effect; and if Cohen’s <em>d</em> is around 0.8 or even higher, we have a large effect. Since our result is about 0.76—quite close to 0.8—we can say that we’re working with a large effect size.</p>
<h2 id="h1-502888c04-0006">Calculating the Significance of Data</h2>
<p class="BodyFirst">We typically use statistical significance as the key piece of evidence that convinces us that an effect that we study in an A/B test is real. Mathematically, statistical significance depends on three things:</p>
<ul class="disc">
<li>The size of the effect being studied (like the increase in revenue that results from changing an email’s text color). Bigger effects make statistical significance more likely.</li>
<li>The size of the sample being studied (the number of people on a subscriber list who are receiving our marketing emails). Bigger samples make statistical significance more likely.</li>
<li>The significance threshold we’re using (typically 0.05). A higher threshold makes statistical significance more likely.</li>
</ul>
<p>If we have a big sample size, and we’re studying a big effect, our t-tests will likely reach statistical significance. On the other hand, if we study an effect that’s very small, with a sample that’s very small, we may have predestined our own failure: the probability that we detect a statistically significant result is essentially 0—even if the email truly does have an effect. Since running an A/B test costs time and money, we’d rather not waste resources running tests like this that are predestined to fail to reach statistical significance.</p>
<p>The probability that a correctly run A/B test will reject a false null hypothesis is called the A/B test’s <em>statistical power</em>. If changing the color of text leads to a $125 increase in revenue per user, we can say that $125 is the <span epub:type="pagebreak" id="Page_89" title="89"/>effect size, and since the effect size is nonzero, we know the null hypothesis (that changing the text color has no effect on revenue) is false. But if we study this true effect by using a sample of only three or four email subscribers, it’s very possible that, by chance, none of these subscribers purchase anything, so we fail to detect the true $125 effect. By contrast, if we study the effect of changing the text color by using an email list of a million subscribers, we’re much more likely to detect the $125 effect and measure it as statistically significant. With the million-subscriber list, we have greater statistical power.</p>
<p>We can import a module into Python that makes calculating statistical power easy:</p>
<pre><code>from statsmodels.stats.power import TTestIndPower</code></pre>
<p>To calculate power with this module, we’ll need to define parameters for the three things that determine statistical significance (see the preceding bulleted list). We’ll define <code>alpha</code>, which is our chosen statistical significance threshold, as discussed in <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span>:</p>
<pre><code>alpha=0.05</code></pre>
<p>We choose the standard 0.05 threshold for <code>alpha</code>, as is standard in much empirical research. We also need to define our sample size. Suppose we’re running an A/B test on a group of email subscribers that consists of 90 people total. That means we’ll have 45 people in Group A and 45 people in Group B, so we define the number of observations in each of our groups as 45. We’ll store this number in a variable called <code>nobs</code>, short for <em>number of observations</em>:</p>
<pre><code>nobs=45</code></pre>
<p>We also have to define an estimated effect size. In our previous A/B test, we observed an effect size of $125. However, for the statistical power calculations this module performs, we can’t express the effect size in dollar units or units of any other currency. We’ll use Cohen’s<em> d </em>instead, and we’ll specify a medium size:</p>
<pre><code>effectsize=0.5</code></pre>
<p>Finally, we can use a function that will take the three parameters we’ve defined and calculate the statistical power we should expect:</p>
<pre><code>analysis = TTestIndPower()
power = analysis.solve_power(effect_size=effectsize, nobs1=nobs, alpha=alpha)</code></pre>
<p>If you run <code>print(power)</code>, you can see that the estimated statistical power for our hypothetical A/B test is about 0.65. This means that we expect about a 65 percent chance of detecting an effect from our A/B test and about a 35 percent chance that even though a true effect exists, our A/B test doesn’t find it. These odds might seem unfavorable if a given <span epub:type="pagebreak" id="Page_90" title="90"/>A/B test is expected to be expensive; you’ll have to make your own decisions about the minimum level of power that is acceptable to you. Power calculations can help at the planning stage to understand what to expect and be prepared. One common convention is to authorize only A/B tests that are expected to have at least 80 percent power.</p>
<p>You can also use the same <code>solve_power()</code> method we used in the previous snippet to “reverse” the power calculation: you’d start by assuming a certain power level and then calculate the parameters required to achieve that level of statistical power. For example, in the following snippet, we define <code>power</code>, <code>alpha</code>, and our effect size, and run the <code>solve_power()</code> command not to calculate the power but to calculate <code>observations</code>, the number of observations we’ll need in each group to achieve the power level we specified:</p>
<pre><code>analysis = TTestIndPower()
alpha = 0.05
effect = 0.5
power = 0.8
observations = analysis.solve_power(effect_size=effect, power=power, alpha=alpha)</code></pre>
<p>If you run <code>print(observations)</code>, you’ll see that the result is about 63.8. This means that if we want to have 80 percent statistical power for our planned A/B test, we’ll need to recruit at least 64 participants for both groups. Being able to perform these kinds of calculations can be helpful in the planning stages of A/B tests.</p>
<h2 id="h1-502888c04-0007">Applications and Advanced Considerations</h2>
<p class="BodyFirst">So far, we’ve considered only A/B tests related to marketing emails. But A/B tests are applicable to a wide variety of business challenges beyond optimal email design. One of the most common applications of A/B testing is user interface/experience design. A website might randomly assign visitors to two groups (called Group A and Group B, as usual) and show different versions of the site to each group. The site can then measure which version leads to more user satisfaction, higher revenue, more link clicks, more time spent on the site, or whatever else interests the company. The whole process can be completely automated, which is what enables the high-speed, high-volume A/B testing that today’s top tech companies are doing.</p>
<p>E-commerce companies run tests, including A/B tests, on product pricing. By running an A/B test on pricing, you can measure what economists call the <em>price elasticity of demand</em>, meaning how much demand changes in response to price changes. If your A/B test finds only a very small change in demand when you increase the price, you should increase the price for everyone and take advantage of their greater willingness to pay. If your A/B test finds that demand drops off significantly when you increase the price slightly, you can conclude that customers are sensitive to price, and their purchase decisions depend heavily on price considerations. If customers are sensitive to price and constantly thinking about it, they’ll likely respond <span epub:type="pagebreak" id="Page_91" title="91"/>positively to a price decrease. If so, you should decrease the price for everyone instead, and expect a large increase in demand. Some businesses have to set prices based on intuition or other painstaking calculations, but A/B testing makes determining the right price relatively simple.</p>
<p>Email design, user-interface design, and product pricing are all common concerns for <em>business-to-consumer (B2C)</em> business models, in which businesses sell directly to consumers. B2C scenarios are a natural fit for A/B testing because the number of customers, products, and transactions tends to be higher for B2C businesses than for other businesses, so we can get large sample sizes and higher statistical power.</p>
<p>This doesn’t mean that business-to-business (B2B) companies can’t do A/B testing. Indeed, A/B testing has been practiced for centuries around the world and in many domains, though it used to be called just <em>science</em>. For example, medical researchers do <em>randomized controlled trials</em> for new drugs, with an approach that’s often essentially the same as A/B testing in a champion/challenger framework. Businesses of all kinds have always needed to learn about the market and their customers, and A/B testing is a natural, rigorous way to learn nearly anything.</p>
<p>As you apply A/B testing in your business, you should try to learn as much as you can about it, beyond the content in the limited space of this chapter. One huge field you might want to wade into is Bayesian statistics. Some data scientists prefer to use Bayesian methods instead of significance tests and <em>p</em>-values to test for the success of A/B tests.</p>
<p>Another interesting, useful topic to learn more about is the <em>exploration/exploitation trade-off </em>in A/B tests. In this trade-off, two goals are in constant tension: to explore (for example, to run A/B tests with possibly bad email designs to learn which is best) and to exploit (for example, to send out only the champion email because it seems to perform the best). Exploration can lead to missed opportunities if one of your challengers performs much worse than the champion; you would have been better off just sending out the champion to everyone. Exploitation can lead to missed opportunities if your champion is not as good as another challenger that you haven’t tested yet because you’re too busy exploiting your champion to do the requisite exploration.</p>
<p>In operations research, you’ll find a huge body of research on the <em>multi-armed bandit problem</em>, which is a mathematical formalization of the exploration/exploitation dilemma. If you’re really interested in doing A/B testing optimally, you can peer into some of the strategies that researchers have come up with to solve the multi-armed bandit problem and run A/B tests as efficiently as possible.</p>
<h2 id="h1-502888c04-0008">The Ethics of A/B Testing</h2>
<p class="BodyFirst">A/B testing is fraught with difficult ethical issues. This may seem surprising, but remember, A/B testing is an experimental method in which we intentionally alter human subjects’ experiences in order to study the results <span epub:type="pagebreak" id="Page_92" title="92"/>for our own gain. This means that A/B testing is human experimentation. Think about other examples of human experimentation to see why people have ethical concerns about it:</p>
<ol class="decimal">
<li value="1">Jonas Salk developed an untested, unprecedented polio vaccine, tried it on himself and his family, and then tried it on millions of American children to ensure that it worked. (It worked and helped eliminate a terrible disease from much of the world.)</li>
<li value="2">My grandmother made a pie for her grandchildren, observed how we reacted to it, and then the next day made a different pie and checked whether we reacted more or less positively. (Both were delicious.)</li>
<li value="3">A professor posed as a student and emailed 6,300 professors to ask them to schedule time to talk to her, lying about herself and her intentions in an attempt to determine whether her false persona would be a target of discrimination so she could publish a paper about the replies. She didn’t compensate any of the unwitting study participants for the deception or the schedule disruption, nor did she receive their consent beforehand to be an experimental subject. (Every detail of this study was approved by a university ethics board.)</li>
<li value="4">A corporation intentionally manipulated the emotions of its users to better understand and sell products to them.</li>
<li value="5">Josef Mengele performed painful and deadly sadistic experiments on unwilling human subjects in the Auschwitz concentration camp.</li>
<li value="6">You perform an A/B test.</li>
</ol>
<p>The first five entries on this list of human experiments actually happened, and all but the second inspired public discussions about ethics among social scientists. You’ll have to decide whether the sixth will happen and what position you will take concerning the ethical issues involved. Because of the broad range of activities that could be called human experimentation, making a single ethical judgment about all of its forms isn’t possible. We have to consider several important ethical concepts when we’re deciding whether our A/B tests make us a hero like my grandmother or Salk, a villain like Mengele, or something in between.</p>
<p>The first concept we should consider is consent. Salk tested his vaccine on himself before large-scale tests of others. Mengele, by contrast, performed experiments on unwilling subjects confined in concentration camps. Informed consent always makes human experimentation more ethical. In some cases, obtaining informed consent is not feasible. For example, if we perform experiments about which outdoor billboard designs are most effective, we can’t obtain informed consent from every possible human research subject, since any person in the world could conceivably see a public billboard, and we don’t have a way to contact every living human.</p>
<p>Other cases form a large gray area. For example, a website that performs A/B tests may have a Terms and Conditions section, with small print and legalese that claims that every website visitor provides consent to be experimented on (via A/B tests of user-interface features) whenever they <span epub:type="pagebreak" id="Page_93" title="93"/>navigate to the site. This may technically meet the definition of informed consent, but only a tiny percentage of any website’s visitors likely visit and understand these conditions. In gray-area cases, it helps to consider other ethical concepts.</p>
<p>Another important ethical consideration related to A/B testing is risk. Risk itself involves two considerations: potential downsides to participation as a human subject and the probability of experiencing those downsides. Salk’s vaccine had a large potential downside—contracting polio—but because of Salk’s preparation and knowledge, the probability of subjects experiencing it was remarkably low. A/B testing for marketing campaigns usually has potential downsides that are minuscule or smaller, as it’s hard to even imagine any downside that could occur because (for example) someone was exposed to blue rather than black text in one marketing email. Experiments with low risks to subjects are more ethical than risky experiments.</p>
<p>We should also consider the potential benefits that could result from our experimentation. Salk’s vaccine experiments had the potential (later realized) of eradicating polio from most of the Earth. A/B tests are designed to improve profits, not cure diseases, so your judgment of their benefits will have to depend on your opinion of the moral status of corporate profits. The only other benefit likely to come from a corporation’s marketing experiment would be an advance in understanding of human psychology. Indeed, corporate marketing practitioners occasionally publish the results of marketing experiments in psychology journals, so this isn’t unheard of.</p>
<p>Ethical and philosophical questions can never reach a definitive, final conclusion that everyone agrees on. You can make up your own mind about whether you feel that A/B testing is fundamentally good, like Salk’s vaccine experiments, or fundamentally abhorrent, like Mengele’s horrors. Most people agree that the extremely low risks of most online A/B testing, and the fact that people rarely refuse consent to benign A/B tests, mean that A/B testing is an ethically justifiable activity when performed properly. Regardless, you should think carefully through your own situation and come to your own conclusion.</p>
<h2 id="h1-502888c04-0009">Summary</h2>
<p class="BodyFirst">In this chapter, we discussed A/B testing. We started with a simple t-test, and then looked at the need for random, non-confounded data collection as part of the A/B testing process. We covered some nuances of A/B testing, including the champion/challenger framework and Twyman’s law, as well as ethical concerns. In the next chapter, we’ll discuss binary classification, an essential skill for any data scientist.</p>
</section>
</div></body></html>