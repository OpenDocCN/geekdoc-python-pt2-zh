- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8'
- en: INTRODUCTION TO NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络简介**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Neural networks are the heart of deep learning. In [Chapter 9](ch09.xhtml#ch09),
    we’ll take a deep dive into what we’ll call *traditional neural networks*. However,
    before we do that, we’ll introduce the anatomy of a neural network, followed by
    a quick example.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是深度学习的核心。在[第 9 章](ch09.xhtml#ch09)中，我们将深入探讨我们称之为*传统神经网络*的内容。然而，在此之前，我们将介绍神经网络的解剖学，并且进行一个快速的示例。
- en: Specifically, we’ll present the components of a *fully connected feed- forward
    neural network*. Visually, you can imagine the network as shown in [Figure 8-1](ch08.xhtml#ch8fig1).
    We’ll refer to this figure often in this chapter and the next. Your mission, should
    you choose to accept it, is to commit this figure to memory to save wear and tear
    on the book by flipping back to it repeatedly.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将介绍*全连接前馈神经网络*的组成部分。在视觉上，你可以想象网络如[图 8-1](ch08.xhtml#ch8fig1)所示。我们将在本章和下一章经常提到这个图。如果你选择接受这个任务，你应该将这个图形记忆下来，以免频繁翻书。
- en: '![image](Images/08fig01.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig01.jpg)'
- en: '*Figure 8-1: A sample neural network*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-1：一个样本神经网络*'
- en: After discussing the structure and parts of a neural network, we’ll explore
    training our example network to classify irises. From this initial experiment,
    [Chapter 9](ch09.xhtml#ch09) will lead us to gradient descent and the backpropagation
    algorithm—the standard way that neural networks, including advanced deep neural
    networks, are trained. This chapter is intended as a warm-up. The heavy lifting
    starts in [Chapter 9](ch09.xhtml#ch09).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论神经网络的结构和部分之后，我们将探索训练我们的示例网络以对鸢尾花进行分类。从这个初步实验开始，[第 9 章](ch09.xhtml#ch09)将引导我们进入梯度下降和反向传播算法——这是训练神经网络（包括先进的深度神经网络）的标准方法。本章旨在作为一个热身。真正的挑战从[第
    9 章](ch09.xhtml#ch09)开始。
- en: Anatomy of a Neural Network
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的解剖学
- en: 'A *neural network* is a graph. In computer science, a *graph* is a series of
    *nodes*, universally drawn as circles, connected by *edges* (short line segments).
    This abstraction is useful for representing many different kinds of relationships:
    roads between cities, who knows whom on social media, the structure of the internet,
    or a series of basic computational units that can be used to approximate any mathematical
    function.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*是一个图。在计算机科学中，*图*是一系列*节点*，通常以圆圈表示，通过*边*（短线段）连接。这种抽象有助于表示许多不同类型的关系：城市之间的道路、社交媒体上的人际关系、互联网的结构，或者一系列可以用来逼近任何数学函数的基本计算单元。'
- en: The last example is, of course, deliberate. Neural networks are universal function
    approximators. They use a graph structure to represent a series of computational
    steps mapping an input feature vector to an output value, typically interpreted
    as a probability. Neural networks are built in layers. Conceptually, they act
    from left to right, mapping an input feature vector to the output(s) by passing
    values along the edges to the nodes. Note, the nodes of a neural network are often
    referred to as *neurons*. We’ll see why shortly. The nodes calculate new values
    based on their inputs. The new values are then passed to the next layer of nodes
    and so on until the output nodes are reached. In [Figure 8-1](ch08.xhtml#ch8fig1),
    there’s an input layer on the left, a hidden layer to its right, another hidden
    layer right of that, and a single node in the output layer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个例子显然是有意为之。神经网络是通用的函数逼近器。它们使用图结构来表示一系列计算步骤，将输入特征向量映射到输出值，通常解释为概率。神经网络是分层构建的。从概念上讲，它们从左到右操作，通过沿着边传递值到节点来将输入特征向量映射到输出。注意，神经网络的节点通常被称为*神经元*。我们很快就会看到为什么这样说。节点根据它们的输入计算新值。然后将新值传递给下一层节点，依此类推直到达到输出节点。在[图
    8-1](ch08.xhtml#ch8fig1)中，左边有一个输入层，其右边是一个隐藏层，再右边是另一个隐藏层，并且输出层有一个单独的节点。
- en: The previous section included the phrase *fully connected feedforward neural
    network* without much explanation. Let’s break it down. The *fully connected*
    part means every node of a layer has its output sent to every node of the next
    layer. The *feedforward* part means that information passes from left to right
    through the network without being sent back to a previous layer; there is no *feedback*,
    no looping, in the network structure. This leaves only the *neural network* part.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 前一部分提到了*全连接前馈神经网络*这个词，却没有过多解释。让我们来分解一下。*全连接*部分意味着每一层的每个节点的输出都会发送到下一层的每个节点。*前馈*部分意味着信息从左到右通过网络传递，而不会被送回到前一层；网络结构中没有*反馈*，没有循环。这就只剩下*神经网络*部分。
- en: The Neuron
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经元
- en: Personally, I have a love/hate relationship with the phrase *neural network*.
    The phrase itself comes from the fact that in a very crude approximation, the
    basic unit of the network resembles a neuron in a brain. Consider [Figure 8-2](ch08.xhtml#ch8fig2),
    which we’ll describe in detail shortly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，我对*神经网络*这个词有一种爱恨交加的感觉。这个词本身来源于一个非常粗略的近似——网络的基本单元类似于大脑中的神经元。考虑一下[图 8-2](ch08.xhtml#ch8fig2)，我们稍后会详细描述它。
- en: '![image](Images/08fig02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig02.jpg)'
- en: '*Figure 8-2: A single neural network node*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-2：单个神经网络节点*'
- en: Recalling that our visualization of a network always moves from left to right,
    we see that the node (the circle) accepts input from the left, and has a single
    output on the right. Here there are two inputs, but it might be hundreds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们的网络可视化总是从左到右移动，我们看到节点（圆圈）从左侧接受输入，并在右侧有一个输出。这里有两个输入，但它也可能有数百个输入。
- en: 'Many inputs mapped to a single output echo how a neuron in the brain works:
    structures called dendrites accept input from many other neurons, and the single
    axon is the output. I love this analogy because it leads to a cool way of talking
    and thinking about the networks. But I hate the analogy because these artificial
    neurons are, operationally, quite different from real ones, and the analogy quickly
    falls apart. There is an anatomical similarity to actual neurons, but they’re
    not the same, and it leads to confusion on the part of those who are not familiar
    with machine learning, causing some to believe that computer scientists are truly
    building artificial brains or that the networks think. The meaning of the word
    *think* is hard to pin down, but to me it doesn’t apply to what a neural network
    does.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多个输入映射到单个输出，类似于大脑中神经元的工作方式：称为树突的结构接收来自其他许多神经元的输入，单一的轴突则是输出。我喜欢这个类比，因为它引出了一个很酷的方式来讨论和思考网络。但我也讨厌这个类比，因为这些人工神经元在操作上与真实的神经元有很大的不同，这个类比很快就会崩溃。它与真实神经元在解剖上有相似之处，但它们并不相同，这会导致那些不熟悉机器学习的人产生困惑，有些人甚至认为计算机科学家正在真正地构建人工大脑，或者认为这些网络会“思考”。“思考”这个词的含义很难界定，但对我来说，它并不适用于神经网络的行为。
- en: Returning now to [Figure 8-2](ch08.xhtml#ch8fig2), we see two squares on the
    left, a bunch of lines, a circle, a line on the right, and a bunch of labels with
    subscripts. Let’s sort this out. If we understand [Figure 8-2](ch08.xhtml#ch8fig2),
    we’ll be well on our way to understanding neural networks. Later, we’ll see an
    implementation of our visual model in code and be surprised to learn how simple
    it can be.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到[图 8-2](ch08.xhtml#ch8fig2)，我们看到左边有两个方块，一堆线，一个圆圈，右边有一条线，还有一些带下标的标签。我们来理清楚这个图。如果我们理解了[图
    8-2](ch08.xhtml#ch8fig2)，我们就能很好地理解神经网络。稍后，我们会看到这个视觉模型的代码实现，并会惊讶地发现它可以是如此简单。
- en: Everything in [Figure 8-2](ch08.xhtml#ch8fig2) focuses on the circle. This is
    the actual node. In reality, it implements a mathematical function called the
    *activation function*, which calculates the output of the node, a single number.
    The two squares are the inputs to the node. This node accepts features from an
    input feature vector; we use squares to differentiate from circles, but the input
    might just as well have come from another group of circular nodes in a previous
    network layer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-2](ch08.xhtml#ch8fig2)中的一切都集中在圆圈上。这就是实际的节点。实际上，它实现了一个叫做*激活函数*的数学函数，用于计算节点的输出，也就是一个单一的数字。两个方块是节点的输入。这个节点接受来自输入特征向量的特征；我们使用方块来与圆圈区分，但输入也可能来自前一个网络层中的另一组圆形节点。'
- en: Each input is a number, a single scalar value, which we’re calling *x*[0] and
    *x*[1]. These inputs move to the node along the two line segments labeled *w*[0]
    and *w*[1]. These line segments represent *weights*, the strength of the connection.
    Computationally, the inputs (*x*[0], *x*[1]) are multiplied by the weights (*w*[0],
    *w*[1]), summed, and given to the activation function of the node. Here we’re
    calling the activation function *h*, a fairly common thing to call it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入是一个数字，一个单一的标量值，我们称之为 *x*[0] 和 *x*[1]。这些输入沿着标有 *w*[0] 和 *w*[1] 的两条线段移动到节点。这些线段代表*权重*，即连接的强度。在计算上，输入
    (*x*[0], *x*[1]) 被权重 (*w*[0], *w*[1]) 相乘，求和，并提供给节点的激活函数。这里我们将激活函数称为 *h*，这是一个相当常见的称呼。
- en: The value of the activation function is the output of the node. Here we’re calling
    this output *a*. The inputs, multiplied by the weights, are added together and
    given to the activation function to produce an output value. We have yet to mention
    the *b*[0] value, which is also added in and passed to the activation function.
    This is the *bias* term. It’s an offset used to adjust the input range to make
    it suitable for the activation function. In [Figure 8-2](ch08.xhtml#ch8fig2),
    we added a zero subscript. There is a bias value for each node in each layer,
    so the subscript here implies that this node is the first node in the layer. (Remember
    computer people always count from zero, not one.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的值是节点的输出。这里我们称这个输出为 *a*。输入乘以权重相加，并传递给激活函数以产生输出值。我们还没有提到 *b*[0] 值，它也被加入并传递给激活函数。这是*偏置*项。它是一个偏移量，用于调整输入范围，使其适合激活函数。在[图8-2](ch08.xhtml#ch8fig2)中，我们加了一个零下标。每个层中的每个节点都有一个偏置值，因此这里的下标表明该节点是该层中的第一个节点。（请记住，计算机人员总是从零开始计数，而不是从一开始。）
- en: 'This is all that a neural network node does: a neural network node accepts
    multiple inputs, *x*[0],*x*[1],…, multiplies each by a weight value, *w*[0],*w*[1],…,
    sums these products along with the bias term, *b*, and passes this sum to the
    activation function, *h*, to produce a single scalar output value, *a*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经网络节点的全部功能：神经网络节点接受多个输入，*x*[0], *x*[1], ...，将每个输入与权重值 *w*[0], *w*[1], ...
    相乘，然后将这些乘积与偏置项 *b* 相加，并将这个总和传递给激活函数 *h*，以产生单个标量输出值 *a*：
- en: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + … + *b*)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + ... + *b*)'
- en: 'That’s it. Get a bunch of nodes together, link them appropriately, figure out
    how to train them to set the weights and biases, and you have a useful neural
    network. As you’ll see in the next chapter, training a neural network is no easy
    feat. But once trained, they’re simple to use: feed it a feature vector, and out
    comes a classification.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。将一堆节点组合在一起，适当地链接它们，找出如何训练它们来设置权重和偏置，你就有了一个有用的神经网络。正如你将在下一章看到的那样，训练神经网络并不容易。但是一旦训练好，它们就很简单：输入一个特征向量，输出一个分类。
- en: As an aside, we’ve been calling these graphs *neural networks*, and will continue
    to do so, sometimes using the abbreviation *NN*. If you read other books or papers,
    you might see them called *artificial neural networks (ANNs)* or even *multi-layer
    perceptrons (MLPs)*, as in the sklearn `MLPClassifier` class name. I recommend
    sticking with *neural network*, but that’s just me.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，我们一直将这些图形称为*神经网络*，并将继续这样称呼，有时使用缩写*NN*。如果你读其他书籍或论文，可能会看到它们被称为*人工神经网络 (ANNs)*或者甚至是*多层感知器
    (MLPs)*，例如sklearn中的`MLPClassifier`类名。我建议坚持使用*神经网络*，但这只是我的建议。
- en: Activation Functions
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活函数
- en: Let’s talk about activation functions. The activation function for a node takes
    a single scalar input, the sum of the inputs times the weights plus the bias,
    and does something to it. In particular, we need the activation function to be
    nonlinear so that the model can learn complex functions. Mathematically, it’s
    easiest to see what a nonlinear function is by stating what a linear function
    is and then saying that any mapping that is not linear is . . . nonlinear.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈激活函数。节点的激活函数接受一个单一的标量输入，即输入乘以权重加偏置的总和，并对其进行某些处理。特别是，我们需要激活函数是非线性的，以便模型可以学习复杂的函数。从数学上来说，通过说明线性函数来定义非线性函数可能更容易理解。任何不是线性的映射都是非线性的。
- en: A *linear function*, *g*, has output that is directly proportional to the input,
    *g*(*x*) ∝ *x*, where ∝ means *proportional to*. Alternatively, the graph of a
    linear function is a straight line. Therefore, any function whose graph is not
    a straight line is a nonlinear function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*线性函数*，*g*，其输出与输入直接成比例，*g*(*x*) ∝ *x*，其中 ∝ 表示*与...成比例*。或者，线性函数的图形是一条直线。因此，任何图形不是直线的函数都是非线性函数。
- en: For example, the function
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，函数
- en: '*g*(*x*) = 3*x* + 2'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*g*(*x*) = 3*x* + 2'
- en: is a linear function because its graph is a straight line. A constant function
    like *g*(*x*) = 1 is also linear. However, the function
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个线性函数，因为它的图形是一条直线。像 *g*(*x*) = 1 这样的常数函数也是线性的。然而，函数
- en: '*g*(*x*) = *x*² + 2'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*g*(*x*) = *x*² + 2'
- en: is a nonlinear function because the exponent of *x* is 2\. Transcendental functions
    are also nonlinear. *Transcendental functions* are functions like *g*(*x*) = log*x*,
    or *g*(*x*) = *e*^(*x*), where *e* = 2.718*...* is the base of the natural logarithm.
    *Trigonometric functions* like sine and cosine, their inverses, and functions
    like tangent that are built from sine and cosine are also transcendental functions.
    These functions are transcendental because you cannot form them as finite combinations
    of elementary algebra operations. They are nonlinear because their graphs are
    not straight lines.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个非线性函数，因为 *x* 的指数是 2。超越函数也是非线性的。*超越函数* 是像 *g*(*x*) = log*x*，或者 *g*(*x*) =
    *e*^(*x*) 这样的函数，其中 *e* = 2.718*...* 是自然对数的底数。像正弦和余弦这样的*三角函数*，它们的反函数，以及由正弦和余弦构建的像正切这样的函数也都是超越函数。这些函数是超越的，因为你不能将它们表示为有限的初等代数运算的组合。它们是非线性的，因为它们的图形不是直线。
- en: The network needs nonlinear activation functions; otherwise, it will be able
    to learn only linear mappings, and linear mappings are not sufficient to make
    the networks generally useful. Consider a trivial network of two nodes, each with
    one input. This means there’s one weight and one bias value per node, and the
    output of the first node is the input of the second. If we set *h*(*x*) = 5*x
    –* 3, a linear function, then for input *x* the network computes output *a*[1]
    to be
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 网络需要非线性激活函数；否则，它只能学习线性映射，而线性映射不足以使网络具有普遍的实用性。考虑一个简单的由两个节点组成的网络，每个节点有一个输入。这意味着每个节点有一个权重和一个偏置值，且第一个节点的输出是第二个节点的输入。如果我们设置
    *h*(*x*) = 5*x* –* 3，一个线性函数，那么对于输入 *x*，网络计算得到的输出 *a*[1] 为
- en: '| *a*[1] | = *h*(*w*[1]*a*[0] + *b*[1]) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| *a*[1] | = *h*(*w*[1]*a*[0] + *b*[1]) |'
- en: '|  | = *h*(*w*[1]*h*(*w*[0]*x* + *b*[0]) + *b*[1]) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(*w*[1]*h*(*w*[0]*x* + *b*[0]) + *b*[1]) |'
- en: '|  | = *h*(*w*[1](5(*w*[0]*x* + *b*[0]) – 3) + *b*[1]) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(*w*[1](5(*w*[0]*x* + *b*[0]) – 3) + *b*[1]) |'
- en: '|  | = *h*(*w*[1](5*w*[0]*x* + 5*b*[0] – 3) + *b*[1]) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(*w*[1](5*w*[0]*x* + 5*b*[0] – 3) + *b*[1]) |'
- en: '|  | = *h*(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) |'
- en: '|  | = 5(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) – 3 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | = 5(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) – 3 |'
- en: '|  | = (25*w*[1]*w*[0])*x* + (25*w*[1]*b*[0] – [1]5*w*[1] + 5*b*[1] – 3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | = (25*w*[1]*w*[0])*x* + (25*w*[1]*b*[0] – [1]5*w*[1] + 5*b*[1] – 3) |'
- en: '|  | = *W**x* + *B* |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | = *W**x* + *B* |'
- en: 'for *W* = 25*w*[1]*w*[0] and *B* = 25*w*[1]*b*[0] *–* 15*w*[1] + 5*b*[1] *–*
    3, which is also a linear function, another line with slope *W* and intercept
    *B* since neither *W* nor *B* depend on *x*. Therefore, a neural network with
    linear activation functions would learn only a linear model since the composition
    of linear functions is also linear. It’s precisely this limitation of linear activation
    functions that caused the first neural network “winter” in the 1970s: research
    into neural networks was effectively abandoned because they were thought to be
    too simple to learn complex functions.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *W* = 25*w*[1]*w*[0] 和 *B* = 25*w*[1]*b*[0] *–* 15*w*[1] + 5*b*[1] *–* 3，这也是一个线性函数，另一条具有斜率
    *W* 和截距 *B* 的直线，因为 *W* 和 *B* 都不依赖于 *x*。因此，具有线性激活函数的神经网络只能学习线性模型，因为线性函数的组合仍然是线性的。正是线性激活函数的这个局限性导致了1970年代首次神经网络的“寒冬”：由于被认为过于简单，无法学习复杂的函数，神经网络研究实际上被放弃了。
- en: Okay, so we want nonlinear activation functions. Which ones? There are an infinite
    number of possibilities. In practice, a few have risen to the top because of their
    proven usefulness or nice properties or both. Traditional neural networks used
    either sigmoid activation functions or hyperbolic tangents. A *sigmoid* is
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们需要非线性激活函数。哪些是可选的？有无数种可能性。在实践中，少数几种因其经过验证的有效性或优良特性或两者兼备而脱颖而出。传统的神经网络使用的是sigmoid激活函数或双曲正切激活函数。一个*sigmoid*函数是
- en: '![image](Images/174equ01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/174equ01.jpg)'
- en: and the *hyperbolic tangent* is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 和 *双曲正切* 是
- en: '![image](Images/174equ02.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/174equ02.jpg)'
- en: Plots of both of these functions are in [Figure 8-3](ch08.xhtml#ch8fig3), with
    the sigmoid on the top and the hyperbolic tangent on the bottom.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的图形绘制在 [图 8-3](ch08.xhtml#ch8fig3) 中，Sigmoid 在顶部，双曲正切在底部。
- en: The first thing to notice is that both of these functions have roughly the same
    “S” shape. The sigmoid runs from 0 as you go further left along the x-axis to
    1 as you go to the right. At 0, the function value is 0.5\. The hyperbolic tangent
    does the same but goes from –1 to +1 and is 0 at *x* = 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，这两个函数都大致具有相同的 “S” 形状。Sigmoid 在沿 x 轴向左移动时从 0 变为 1，在 x = 0 时函数值为 0.5。双曲正切也是如此，但从
    –1 变为 +1，在 *x* = 0 时为 0。
- en: More recently, the sigmoid and hyperbolic tangent have been replaced by the
    *rectified linear unit*, or *ReLU* for short. The ReLU is simple, and has convenient
    properties for neural networks. Even though the word *linear* is in the name,
    the ReLU is a nonlinear function—its graph is not a straight line. When we discuss
    backpropagation training of neural networks in [Chapter 9](ch09.xhtml#ch09), we’ll
    learn why this change has happened.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 较近期，**Sigmoid** 和 **双曲正切** 被简称为 **ReLU** 的 *修正线性单元* 所取代。ReLU 简单且对神经网络有便利的特性。尽管名称中有
    *线性* 一词，ReLU 是一种非线性函数——其图形并非直线。当我们在 [第 9 章](ch09.xhtml#ch09) 中讨论神经网络的反向传播训练时，我们将了解为何发生了这种变化。
- en: '![image](Images/08fig03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig03.jpg)'
- en: '*Figure 8-3: A sigmoid function (top) and a hyperbolic tangent function (bottom).
    Note that the y-axis scales are not the same*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-3：Sigmoid 函数（顶部）和双曲正切函数（底部）。请注意 y 轴的比例尺不相同*。'
- en: The ReLU is as follows and is shown in [Figure 8-4](ch08.xhtml#ch8fig4).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 如下，并在 [图 8-4](ch08.xhtml#ch8fig4) 中显示。
- en: '![image](Images/176equ01.jpg)![image](Images/08fig04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/176equ01.jpg)![image](Images/08fig04.jpg)'
- en: '*Figure 8-4: The rectified linear activation function, ReLU(x) = max(0,x)*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-4：修正线性激活函数，ReLU(x) = max(0,x)*'
- en: ReLU is called *rectified* because it removes the negative values and replaces
    them with 0\. In truth, the machine learning community uses several different
    versions of this function, but all are essentially replacing negative values with
    a constant or some other value. The piecewise nature of the ReLU is what makes
    it nonlinear and, therefore, suitable for use as a neural network activation function.
    It’s also computationally simple, far faster to calculate than either the sigmoid
    or the hyperbolic tangent. This is because the latter functions use *e*^(*x*),
    which, in computer terms, means a call to the `exp` function. This function is
    typically implemented as a sum of terms of a series expansion, translating into
    dozens of floating-point operations in place of the single `if` statement necessary
    to implement a ReLU. Small savings like this add up in an extensive network with
    potentially thousands of nodes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 被称为 *修正*，因为它消除了负值并用 0 替换。事实上，机器学习社区使用了几种不同版本的这个函数，但所有这些版本本质上都是用常数或其他值替换负值。ReLU
    的分段特性使其成为非线性函数，因此适合作为神经网络的激活函数。它还计算简单，比 Sigmoid 或双曲正切要快得多。这是因为后两者使用 *e*^(*x*)，在计算机术语中意味着调用
    `exp` 函数。该函数通常被实现为级数展开的项的和，这意味着在执行 ReLU 所需的单个 `if` 语句中，需要执行数十次浮点运算。这类小节省在具有数千个节点的大规模网络中将会累积。
- en: Architecture of a Network
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络的架构
- en: We’ve discussed nodes and how they work, and hinted that nodes are connected
    to form networks. Let’s look more closely at how nodes are connected, the *architecture*
    of the network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了节点及其工作方式，并暗示节点是如何连接形成网络的。让我们更仔细地看看节点是如何连接的，网络的 *架构*。
- en: Standard neural networks like the ones we are working with in this chapter are
    built in layers, as you saw in [Figure 8-1](ch08.xhtml#ch8fig1). We don’t need
    to do this, but as we’ll see, this buys us some computational simplicity and greatly
    simplifies training. A feedforward network has an input layer, one or more hidden
    layers, and an output layer. The input layer is simply the feature vector, and
    the output layer is the prediction (probability). If the network is for a multiclass
    problem, the output layer might have more than one node, with each node representing
    the model’s prediction for each of the possible classes of inputs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 标准神经网络，如本章节中使用的那些，是分层构建的，就像您在[图 8-1](ch08.xhtml#ch8fig1)中看到的那样。我们不一定非要这样做，但正如我们将看到的那样，这样做可以简化计算，并极大简化训练。前馈网络包括一个输入层，一个或多个隐藏层以及一个输出层。输入层只是特征向量，输出层是预测（概率）。如果网络用于多类问题，则输出层可能有多个节点，每个节点代表输入可能类别的模型预测。
- en: The hidden layers are made of nodes, and the nodes of layer *i* accept as input
    the output of the nodes of layer *i –* 1 and pass their outputs to the inputs
    of the nodes of layer *i* + 1\. The connections between the layers are typically
    fully connected, meaning every output of every node of layer *i –* 1 is used as
    an input to every node of layer *i*, hence *fully connected*. Again, we don’t
    need to do this, but it simplifies the implementation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层由节点组成，第*i*层的节点接受第*i-1*层节点的输出作为输入，并将它们的输出传递给第*i+1*层的节点的输入。层之间的连接通常是全连接的，这意味着第*i-1*层的每个节点的每个输出都用作第*i*层的每个节点的输入，因此*全连接*。再次强调，我们不一定非要这样做，但这简化了实现。
- en: The number of hidden layers and the number of nodes in each hidden layer define
    the architecture of the network. It has been proven that a single hidden layer
    with enough nodes can learn any function mapping. This is good because it means
    neural networks are applicable to machine learning problems since, in the end,
    the model acts as a complex function mapping inputs to output labels and probabilities.
    However, like many theoretical results, this does not mean that it’s practical
    for a single layer network to be used in all situations. As the number of nodes
    (and layers) in a network grows, so, too, does the number of parameters to learn
    (weights and biases), and therefore the amount of training data needed goes up
    as well. It’s the curse of dimensionality again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的数量和每个隐藏层中的节点数量定义了网络的架构。已经证明，具有足够节点的单隐藏层可以学习任何函数映射。这很好，因为这意味着神经网络适用于机器学习问题，因为最终，模型充当将输入映射到输出标签和概率的复杂函数。然而，就像许多理论结果一样，这并不意味着单层网络在所有情况下都是实用的。随着网络中节点（和层）的数量增加，需要学习的参数（权重和偏差）的数量也增加，因此需要的训练数据量也增加。这又是维度的诅咒。
- en: Issues like these stymied neural networks for a second time in the 1980s. Computers
    were too slow to train large networks, and, regardless, there was usually too
    little data available to train the network anyway. Practitioners knew that if
    both of these situations changed, then it would become possible to train large
    networks that would be far more capable than the small networks of the time. Fortunately
    for the world, the situation changed in the early 2000s.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题使得神经网络在20世纪80年代第二次停滞不前。计算机训练大型网络的速度太慢，而且无论如何，训练网络可用的数据通常太少。从业者们知道，如果这两种情况都改变了，那么训练大型网络将成为可能，这些网络将比当时的小型网络更加强大。对世界而言，幸运的是，情况在21世纪初发生了改变。
- en: 'Selecting the proper neural network architecture has a huge impact on whether
    or not your model will learn anything. This is where experience and intuition
    come in. Selecting the right architecture is the dark art of using neural networks.
    Let’s try to be more helpful by giving some (crude) rules of thumb:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的神经网络架构对于模型是否能够学习任何内容具有巨大影响。这就是经验和直觉发挥作用的地方。选择正确的架构是使用神经网络的黑暗艺术。让我们试着通过提供一些（粗略的）经验法则来更有帮助：
- en: If your input has definite spatial relationships, like the parts of an image,
    you might want to use a convolutional neural network instead ([Chapter 12](ch12.xhtml#ch12)).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的输入具有明确定义的空间关系，例如图像的部分，您可能希望改用卷积神经网络（[第12章](ch12.xhtml#ch12)）。
- en: Use no more than three hidden layers. Recall, in theory, one sufficiently large
    hidden layer is all that is needed, so use as few hidden layers as necessary.
    If the model learns with one hidden layer, then add a second to see if that improves
    things.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层不要超过三个。回想一下，理论上，一个足够大的隐藏层就足够了，所以只使用必要的隐藏层。如果模型在一个隐藏层下能够学习，那么就添加第二个隐藏层，看看是否能提高效果。
- en: The number of nodes in the first hidden layer should match or (ideally) exceed
    the number of input vector features.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个隐藏层的节点数应与输入特征向量的数量匹配或（理想情况下）超过输入特征向量的数量。
- en: Except for the first hidden layer (see previous rule), the number of nodes per
    hidden layer should be the same as or some value between the number of nodes in
    the previous layer and the following layer. If layer *i –* 1 has *N* nodes and
    layer *i* + 1 has *M* nodes, then layer *i* might be good with *N* ≤ *x* ≤ *M*
    nodes.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了第一个隐藏层（参见前一条规则）外，每个隐藏层的节点数应与前一层和下一层的节点数相同或在其之间。如果第*i-1*层有*N*个节点，第*i+1*层有*M*个节点，那么第*i*层可能需要*N*
    ≤ *x* ≤ *M*个节点。
- en: The first rule says that a traditional neural network best applies to situations
    where your input does not have spatial relationships—that is, you have a feature
    vector, not an image. Also, when your input dimension is small, or when you do
    not have a lot of data, which makes it hard to train a larger convolutional network,
    you should give a traditional network a try. If you do think you are in a situation
    where a traditional neural network is called for, start small, and grow it as
    long as performance improves.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条规则指出，传统的神经网络最适用于输入没有空间关系的情况——也就是说，你有一个特征向量，而不是图像。此外，当输入维度较小，或者数据不多时，这会使得训练较大的卷积网络变得困难，此时你应该尝试使用传统网络。如果你认为自己正处于需要传统神经网络的情况，建议从小做起，随着性能的提高逐步扩展。
- en: Output Layers
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出层
- en: The last layer of a neural network is the output layer. If the network is modeling
    a continuous value, known as *regression*, a use case we’re ignoring in this book,
    then the output layer is a node that doesn’t use an activation function; it simply
    reports the argument to *h* in [Figure 8-2](ch08.xhtml#ch8fig2). Note that this
    is the same as saying that the activation function is the identity function, *h*(*x*)
    = *x*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的最后一层是输出层。如果网络建模的是一个连续值，称为*回归*，这是我们在本书中忽略的用例，那么输出层是一个不使用激活函数的节点；它只报告[图8-2](ch08.xhtml#ch8fig2)中*h*的参数。请注意，这与激活函数是身份函数*h*(*x*)
    = *x*是一样的。
- en: 'Our neural networks are for classification; we want them to output a decision
    value. If we have two classes labeled 0 and 1, we make the activation function
    of the final node a sigmoid. This will output a value between 0 and 1 that we
    can interpret as a likelihood or probability that the input belongs to class 1\.
    We make our classification decision based on the output value with a simple rule:
    if the activation value is less than 0.5, call the input class 0; otherwise, call
    it class 1\. We’ll see in [Chapter 11](ch11.xhtml#ch11) how changing this threshold
    of 0.5 can be used to tune a model’s performance for the task at hand.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络用于分类；我们希望它们输出一个决策值。如果我们有两个类别，分别标记为0和1，我们将最终节点的激活函数设置为sigmoid函数。这将输出一个介于0和1之间的值，我们可以将其解释为输入属于类别1的可能性或概率。我们根据输出值使用简单的规则做出分类决策：如果激活值小于0.5，则将输入归类为类别0；否则，将其归类为类别1。我们将在[第11章](ch11.xhtml#ch11)中看到，如何通过改变0.5的阈值来调整模型的性能以应对当前任务。
- en: If we have more than two classes, we need to take a different approach. Instead
    of a single node in the output layer, we’ll have *N* output nodes, one for each
    class, each one using the identity function for *h*. Then, we apply a *softmax*
    operation to these *N* outputs and select the output with the largest softmax
    value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有超过两个类别，我们需要采取不同的方法。我们将在输出层中使用*N*个输出节点，每个类别一个，每个节点都使用身份函数处理*h*。然后，我们对这*N*个输出应用*softmax*操作，并选择具有最大softmax值的输出。
- en: Let’s illustrate what we mean by softmax. Suppose we have a dataset with four
    classes in it. What they represent doesn’t really matter; the network doesn’t
    know what they represent, either. The classes are labeled 0, 1, 2, and 3\. So,
    *N* = 4 means our network will have four output nodes, each one using the identity
    function for *h*. This looks like [Figure 8-5](ch08.xhtml#ch8fig5), where we have
    also shown the softmax operation and the resulting output vector.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解释一下 softmax 的含义。假设我们有一个包含四个类别的数据集。它们代表的是什么并不重要；网络也不知道它们代表什么。这些类别标记为 0、1、2
    和 3。所以，*N* = 4 表示我们的网络将有四个输出节点，每个节点使用恒等函数作为 *h*。这看起来像是 [图 8-5](ch08.xhtml#ch8fig5)，我们还展示了
    softmax 操作和最终的输出向量。
- en: '![image](Images/08fig05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig05.jpg)'
- en: '*Figure 8-5: The last hidden layer *n*-1 and output layer (*n*, nodes numbered)
    for a neural network with four classes. The softmax operation is applied, producing
    a four-element output vector, *[p*0,*p*1,*p*2,*p*3*]*.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-5：最后一个隐藏层 *n*-1 和输出层 (*n*，节点编号) 对应一个有四个类别的神经网络。应用 softmax 操作，产生一个包含四个元素的输出向量，*
    [p*0,*p*1,*p*2,*p*3]*。*'
- en: We select the index of the largest value in this output vector as the class
    label for the given input feature vector. The softmax operation ensures that the
    elements of this vector sum to 1, so we can again be a bit sloppy and call these
    values the probability of belonging to each of the four classes. That is why we
    take only the largest value to decide the output class label.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这个输出向量中最大值的索引作为给定输入特征向量的类别标签。softmax 操作确保该向量的元素之和为 1，因此我们可以稍微马虎地称这些值为属于每个类别的概率。这就是为什么我们只取最大值来决定输出类别标签的原因。
- en: 'The softmax operation is straightforward: the probability for each of the outputs
    is simply'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 操作是直接的：每个输出的概率就是
- en: '![image](Images/179equ01.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/179equ01.jpg)'
- en: where *a*[*i*] is the *i*-th output, and the denominator is the sum over all
    the outputs. For the example, *i* = 0,1,2,3, and the index of the largest value
    will be the class label assigned to the input.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a*[*i*] 是第 *i* 个输出，分母是所有输出值的总和。以此为例，*i* = 0, 1, 2, 3，最大值的索引将是分配给输入的类别标签。
- en: As an example, assume the output of the four last layer nodes is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设四个最后一层节点的输出为：
- en: '*a*[0] = 0.2'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[0] = 0.2'
- en: '*a*1 = 1.3'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*1 = 1.3'
- en: '*a*2 = 0.8'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*2 = 0.8'
- en: '*a*3 = 2.1'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*3 = 2.1'
- en: 'Then calculate the softmax as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按如下方式计算 softmax：
- en: '*p*[0] = *e*^(0.2)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.080'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[0] = *e*^(0.2)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.080'
- en: '*p*[1] = *e*^(1.3)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.240'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[1] = *e*^(1.3)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.240'
- en: '*p*[2] = *e*^(0.8)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.146'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[2] = *e*^(0.8)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.146'
- en: '*p*[3] = *e*^(2.1)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.534'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[3] = *e*^(2.1)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.534'
- en: Select class 3 because *p*[3] is the largest. Note that the sum of the *p*[*i*]
    values is 1.0, as we would expect.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 选择类别 3，因为 *p*[3] 是最大的。注意，*p*[*i*] 值的总和是 1.0，这是我们预期的。
- en: 'Two points should be mentioned here. In the preceding equations, we used the
    sigmoid to calculate the output of the network. If we set the number of classes
    to 2 and calculate the softmax, we’ll get two output values: one will be some
    *p*, and the other will be 1 *– p*. This is identical to the sigmoid alone, selecting
    the probability of the input being of class 1.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两点需要提到。在前面的公式中，我们使用了 sigmoid 函数来计算网络的输出。如果我们将类别数设置为 2 并计算 softmax，我们将得到两个输出值：一个是某个
    *p*，另一个是 1 *– p*。这与单独使用 sigmoid 是相同的，选择输入属于类别 1 的概率。
- en: The second point has to do with implementing the softmax. If the network outputs,
    the *a* values, are large, then *e*^(*a*) might be very large, which is something
    the computer will not like. Precision will be lost, at least, or the value might
    overflow and make the output meaningless. Numerically, if we subtract the largest
    *a* value from all the others before calculating the softmax, we’ll take the exponential
    over smaller values that are less likely to overflow. Doing this for the preceding
    example gives new *a* values
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点涉及到如何实现 softmax。如果网络的输出，即 *a* 值，比较大，那么 *e*^(*a*) 可能会非常大，这可能会导致计算机不喜欢。至少会丧失精度，或者该值可能会溢出，从而使输出变得没有意义。从数值上看，如果我们在计算
    softmax 之前，将最大的 *a* 值从所有其他值中减去，我们就可以对较小的值取指数，这样更不容易发生溢出。对前面的例子进行这种处理得到新的 *a* 值：
- en: '![image](Images/180equ02.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/180equ02.jpg)'
- en: where we subtract 2.1 because that is the largest *a* value. This leads to precisely
    the same *p* values we found before, but this time protected against overflow
    in the case that any of the *a* values are too large.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们减去 2.1，因为这是最大的 *a* 值。这将导致我们之前找到的相同的 *p* 值，但这一次可以防止溢出情况，假如任何 *a* 值过大。
- en: Representing Weights and Biases
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表示权重和偏置
- en: Before we move on to an example neural network, let’s revisit the weights and
    biases and see that we can greatly simplify the implementation of a neural network
    by viewing it in terms of matrices and vectors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讲解一个示例神经网络之前，先回顾一下权重和偏置，并看到通过将其视为矩阵和向量，我们可以大大简化神经网络的实现。
- en: Consider the mapping from an input feature vector of two elements to the first
    hidden layer with three nodes (*a*[1] in [Figure 8-1](ch08.xhtml#ch8fig1)). Let’s
    label the edges between the two layers (the weights) as *w*[*ij*] with *i* = 0,1
    for the inputs *x*[0] and *x*[1] and *j* = 0,1,2 for the three hidden layer nodes
    numbered from top to bottom of the figure. Additionally, we need three bias values
    that are not shown in the figure, one for each hidden node. We’ll call these *b*[0],
    *b*[1], and *b*[2], again, top to bottom.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑从一个包含两个元素的输入特征向量到第一隐层（有三个节点）(*a*[1] 在[图 8-1](ch08.xhtml#ch8fig1)中的位置)。我们将两个层之间的边（即权重）标记为
    *w*[*ij*]，其中 *i* = 0,1 对应输入 *x*[0] 和 *x*[1]，*j* = 0,1,2 对应图中从上到下编号的三个隐层节点。此外，我们还需要三个偏置值，这些在图中未显示，每个隐节点一个。我们将它们分别命名为
    *b*[0]、*b*[1] 和 *b*[2]，同样是从上到下。
- en: In order to calculate the outputs of the activation functions, *h*, for the
    three hidden nodes, we need to find the following.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算三个隐层节点的激活函数 *h* 的输出，我们需要找到以下内容。
- en: '*a*[0] = *h*(*w*[00]*x*[0] + *w*[10]*x*[1] + *b*[0])'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[0] = *h*(*w*[00]*x*[0] + *w*[10]*x*[1] + *b*[0])'
- en: '*a*[1] = *h*(*w*[01]*x*[0] + *w*[11]*x*[1] + *b*[1])'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[1] = *h*(*w*[01]*x*[0] + *w*[11]*x*[1] + *b*[1])'
- en: '*a*[2] = *h*(*w*[02]*x*[0] + *w*[12]*x*[1] + *b*[2])'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[2] = *h*(*w*[02]*x*[0] + *w*[12]*x*[1] + *b*[2])'
- en: But, remembering how matrix multiplication and vector addition work, we see
    that this is exactly
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，记住矩阵乘法和向量加法的工作原理，我们可以看到这实际上是：
- en: '![image](Images/181equ02.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/181equ02.jpg)'
- en: where ![Image](Images/181equ03.jpg), and *W* is a 3 × 2 matrix of weight values.
    In this case, the activation function, *h*, is given a vector of input values
    and produces a vector of output values. This is simply applying *h* to every element
    of ![Image](Images/181equ04.jpg). For example, applying *h* to a vector ![Image](Images/xbar1.jpg)
    with three elements is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/181equ03.jpg)，*W* 是一个 3 × 2 的权重矩阵。在这种情况下，激活函数 *h* 接收一组输入值，并输出一组输出值。这实际上是将
    *h* 应用到 ![Image](Images/181equ04.jpg) 的每个元素。例如，将 *h* 应用到一个包含三个元素的向量 ![Image](Images/xbar1.jpg)
    中时，结果是：
- en: '![image](Images/181equ05.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/181equ05.jpg)'
- en: with *h* applied separately to each element of ![Image](Images/xbar1.jpg).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *h* 分别应用于 ![Image](Images/xbar1.jpg) 中的每个元素。
- en: 'Since the NumPy Python module is designed to work with arrays, and matrices
    and vectors are arrays, we arrive at the pleasant conclusion that the weights
    and biases of a neural network can be stored in NumPy arrays and we need only
    simple matrix operations (calls to `np.dot`) and addition to work with a fully
    connected neural network. Note this is why we want to use fully connected networks:
    their implementation is straightforward.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 NumPy Python 模块是专为数组设计的，而矩阵和向量本身就是数组，因此我们得出一个令人愉快的结论：神经网络的权重和偏置可以存储在 NumPy
    数组中，我们只需要简单的矩阵操作（调用 `np.dot`）和加法即可处理一个全连接神经网络。请注意，这也是我们为何要使用全连接网络的原因：它们的实现非常简单。
- en: 'To store the network of [Figure 8-1](ch08.xhtml#ch8fig1), we need a weight
    matrix and bias vector between each layer, giving us three matrices and three
    vectors: a matrix and vector each for the input to the first hidden layer, the
    first hidden layer to the second, and the second hidden layer to the output. The
    weight matrices are of dimensions 3 × 2, 2 × 3, and 1 × 2, respectively. The bias
    vectors are of length 3, 2, and 1.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储[图 8-1](ch08.xhtml#ch8fig1)中的网络，我们需要在每一层之间存储一个权重矩阵和一个偏置向量，这样我们就得到了三个矩阵和三个向量：每层输入到第一隐层、第一隐层到第二隐层、第二隐层到输出的矩阵和向量。权重矩阵的维度分别为
    3 × 2、2 × 3 和 1 × 2。偏置向量的长度分别为 3、2 和 1。
- en: Implementing a Simple Neural Network
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个简单的神经网络
- en: In this section, we’ll implement the sample neural network of [Figure 8-1](ch08.xhtml#ch8fig1)
    and train it on two features from the iris dataset. We’ll implement the network
    from scratch but use sklearn to train it. The goal of this section is to see how
    straightforward it is to implement a simple neural network. Hopefully, this will
    clear some of the fog that might be hanging around from the discussion of the
    previous sections.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现[图 8-1](ch08.xhtml#ch8fig1)中的示例神经网络，并在来自鸢尾花数据集的两个特征上训练它。我们将从头实现网络，但使用
    sklearn 来训练它。本节的目标是看看实现一个简单神经网络有多直接。希望这可以清除前几节讨论中可能存在的一些困惑。
- en: The network of [Figure 8-1](ch08.xhtml#ch8fig1) accepts an input feature vector
    with two features. It has two hidden layers, one with three nodes and the other
    with two nodes. It has one sigmoid output. The activation functions of the hidden
    nodes are also sigmoids.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-1](ch08.xhtml#ch8fig1)中的网络接受一个包含两个特征的输入特征向量。它有两个隐藏层，一个有三个节点，另一个有两个节点。它有一个sigmoid输出。隐藏节点的激活函数也是sigmoid。'
- en: Building the Dataset
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建数据集
- en: Before we look at the neural network code, let’s build the dataset we’ll train
    against and see what it looks like. We know the iris dataset already, but for
    this example, we’ll use only two classes and only two of the four features. The
    code to build the train and test datasets is in [Listing 8-1](ch08.xhtml#ch8lis1).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看神经网络代码之前，让我们先构建用于训练的数据集，并看看它长什么样。我们已经知道鸢尾花数据集，但在此示例中，我们只使用两个类别和四个特征中的两个。构建训练集和测试集的代码见[清单
    8-1](ch08.xhtml#ch8lis1)。
- en: import numpy as np
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: ❶ d = np.load("iris_train_features_augmented.npy")
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ d = np.load("iris_train_features_augmented.npy")
- en: l = np.load("iris_train_labels_augmented.npy")
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: l = np.load("iris_train_labels_augmented.npy")
- en: d1 = d[np.where(l==1)]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: d1 = d[np.where(l==1)]
- en: d2 = d[np.where(l==2)]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: d2 = d[np.where(l==2)]
- en: ❷ a=len(d1)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ a=len(d1)
- en: b=len(d2)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: b=len(d2)
- en: x = np.zeros((a+b,2))
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.zeros((a+b,2))
- en: x[:a,:] = d1[:,2:]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: x[:a,:] = d1[:,2:]
- en: x[a:,:] = d2[:,2:]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: x[a:,:] = d2[:,2:]
- en: ❸ y = np.array([0]*a+[1]*b)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ y = np.array([0]*a+[1]*b)
- en: i = np.argsort(np.random.random(a+b))
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.argsort(np.random.random(a+b))
- en: x = x[i]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: ❹ np.save("iris2_train.npy", x)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ np.save("iris2_train.npy", x)
- en: np.save("iris2_train_labels.npy", y)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris2_train_labels.npy", y)
- en: ❺ d = np.load("iris_test_features_augmented.npy")
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ d = np.load("iris_test_features_augmented.npy")
- en: l = np.load("iris_test_labels_augmented.npy")
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: l = np.load("iris_test_labels_augmented.npy")
- en: d1 = d[np.where(l==1)]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: d1 = d[np.where(l==1)]
- en: d2 = d[np.where(l==2)]
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: d2 = d[np.where(l==2)]
- en: a=len(d1)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: a=len(d1)
- en: b=len(d2)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: b=len(d2)
- en: x = np.zeros((a+b,2))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.zeros((a+b,2))
- en: x[:a,:] = d1[:,2:]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: x[:a,:] = d1[:,2:]
- en: x[a:,:] = d2[:,2:]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: x[a:,:] = d2[:,2:]
- en: y = np.array([0]*a+[1]*b)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.array([0]*a+[1]*b)
- en: i = np.argsort(np.random.random(a+b))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.argsort(np.random.random(a+b))
- en: x = x[i]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: np.save("iris2_test.npy", x)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris2_test.npy", x)
- en: np.save("iris2_test_labels.npy", y)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris2_test_labels.npy", y)
- en: '*Listing 8-1: Building the simple example dataset. See* nn_iris_dataset.py.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：构建简单示例数据集。见* nn_iris_dataset.py。'
- en: This code is straightforward data munging. We start with the augmented dataset
    and load the samples and labels ❶. We want only class 1 and class 2, so we find
    the indices of those samples and pull them out. We’re keeping only features 2
    and 3 and put them in `x` ❷. Next, we build the labels (`y`) ❸. Note, we recode
    the class labels to 0 and 1\. Finally, we scramble the order of the samples and
    write the new dataset to disk ❹. Last of all, we repeat this process to build
    the test samples ❺.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是简单的数据预处理。我们从增强后的数据集开始，加载样本和标签 ❶。我们只想要类别 1 和类别 2，因此我们找到这些样本的索引并将它们提取出来。我们只保留特征
    2 和特征 3，并将它们放入 `x` ❷。接下来，我们构建标签 (`y`) ❸。请注意，我们将类别标签重新编码为 0 和 1。最后，我们打乱样本的顺序并将新数据集写入磁盘
    ❹。最后，我们重复此过程构建测试样本 ❺。
- en: '[Figure 8-6](ch08.xhtml#ch8fig6) shows the training set. We can plot it in
    this case because we have only two features.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-6](ch08.xhtml#ch8fig6) 显示了训练集。我们可以绘制它，因为这里只有两个特征。'
- en: '![image](Images/08fig06.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig06.jpg)'
- en: '*Figure 8-6: The training data for the two-class, two-feature iris dataset*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-6：两类两特征鸢尾花数据集的训练数据*'
- en: We immediately see that this dataset is not trivially separable. There is no
    simple line we can draw that will correctly split the training set into two groups,
    one all class 0 and the other all class 1\. This makes things a little more interesting.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立刻发现这个数据集并不是简单可分的。没有一条简单的线可以将训练集正确地分成两个组，一个全是类别 0，另一个全是类别 1。这使得问题变得稍微有趣一些。
- en: Implementing the Neural Network
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现神经网络
- en: Let’s see how to implement the network of [Figure 8-1](ch08.xhtml#ch8fig1) in
    Python using NumPy. We’ll assume that it’s already trained, meaning we already
    know all the weights and biases. The code is in [Listing 8-2](ch08.xhtml#ch8lis2).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用NumPy在Python中实现[图8-1](ch08.xhtml#ch8fig1)所示的网络。我们假设网络已经训练好，这意味着我们已经知道所有的权重和偏置。代码见[列表8-2](ch08.xhtml#ch8lis2)。
- en: import numpy as np
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pickle
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: import pickle
- en: import sys
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: 'def sigmoid(x):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'def sigmoid(x):'
- en: return 1.0 / (1.0 + np.exp(-x))
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: return 1.0 / (1.0 + np.exp(-x))
- en: 'def evaluate(x, y, w):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'def evaluate(x, y, w):'
- en: ❶ w12,b1,w23,b2,w34,b3 = w
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ w12,b1,w23,b2,w34,b3 = w
- en: nc = nw = 0
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: nc = nw = 0
- en: prob = np.zeros(len(y))
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: prob = np.zeros(len(y))
- en: 'for i in range(len(y)):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(y)):'
- en: a1 = sigmoid(np.dot(x[i], w12) + b1)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: a1 = sigmoid(np.dot(x[i], w12) + b1)
- en: a2 = sigmoid(np.dot(a1, w23) + b2)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: a2 = sigmoid(np.dot(a1, w23) + b2)
- en: prob[i] = sigmoid(np.dot(a2, w34) + b3)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: prob[i] = sigmoid(np.dot(a2, w34) + b3)
- en: z  = 0 if prob[i] < 0.5 else 1
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: z  = 0 if prob[i] < 0.5 else 1
- en: '❷ if (z == y[i]):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ if (z == y[i]):'
- en: nc += 1
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: nc += 1
- en: 'else:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: nw += 1
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: nw += 1
- en: return [float(nc) / float(nc + nw), prob]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: return [float(nc) / float(nc + nw), prob]
- en: ❸ xtest = np.load("iris2_test.npy")
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xtest = np.load("iris2_test.npy")
- en: ytest = np.load("iris2_test_labels.npy")
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ytest = np.load("iris2_test_labels.npy")
- en: ❹ weights = pickle.load(open("iris2_weights.pkl","rb"))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ weights = pickle.load(open("iris2_weights.pkl","rb"))
- en: score, prob = evaluate(xtest, ytest, weights)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: score, prob = evaluate(xtest, ytest, weights)
- en: print()
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'for i in range(len(prob)):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(prob)):'
- en: 'print("%3d:  actual: %d  predict: %d  prob: %0.7f" %'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%3d:  actual: %d  predict: %d  prob: %0.7f" %'
- en: (i, ytest[i], 0 if (prob[i] < 0.5) else 1, prob[i]))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (i, ytest[i], 0 if (prob[i] < 0.5) else 1, prob[i]))
- en: print("Score = %0.4f" % score)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: print("Score = %0.4f" % score)
- en: '*Listing 8-2: Using the trained weights and biases to classify held-out test
    samples. See* nn_iris_evaluate.py.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表8-2：使用训练好的权重和偏置来分类持出的测试样本。见* nn_iris_evaluate.py。'
- en: Perhaps the first thing we should notice is how short the code is. The `evaluate`
    function implements the network. We also need to define `sigmoid` as NumPy does
    not have it natively. The main code loads the test samples (`xtest`) and associated
    labels (`ytest`) ❸. These are the files generated by the preceding code, so we
    know that `xtest` is of shape 23 × 2 because we have 23 test samples, and each
    has two features. Similarly, `ytest` is a vector of 23 labels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们应该注意到的第一件事是代码是多么简短。`evaluate`函数实现了网络。我们还需要定义`sigmoid`，因为NumPy本身没有这个函数。主代码加载了测试样本（`xtest`）和相关标签（`ytest`）❸。这些是前面的代码生成的文件，因此我们知道`xtest`的形状是23
    × 2，因为我们有23个测试样本，每个样本有两个特征。类似地，`ytest`是一个包含23个标签的向量。
- en: When we train this network, we’ll store the weights and biases as a list of
    NumPy arrays. The Python way to store a list on disk is via the `pickle` module,
    so we use `pickle` to load the list from disk ❹. The list `weights` has six elements
    representing the three weight matrices and three bias vectors that define the
    network. These are the “magic” numbers that our training has conditioned to the
    dataset. Finally, we call `evaluate` to run each of the test samples through the
    network. This function returns the score (accuracy) and the output probabilities
    for each sample (`prob`). The remainder of the code displays the sample number,
    actual label, predicted label, and associated output probability of being class
    1\. Finally, the score (accuracy) is shown.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练这个网络时，我们会将权重和偏置存储为一个NumPy数组列表。Python存储列表到磁盘的方式是通过`pickle`模块，因此我们使用`pickle`从磁盘加载列表❹。该列表`weights`有六个元素，表示三个权重矩阵和三个偏置向量，定义了网络。这些是我们训练过程中根据数据集调整的“魔法”数字。最后，我们调用`evaluate`函数，将每个测试样本通过网络进行计算。该函数返回每个样本的得分（准确率）和输出概率（`prob`）。其余的代码显示样本编号、实际标签、预测标签和对应的输出概率。最后，显示得分（准确率）。
- en: 'The network is implemented in `evaluate`; let’s see how. First, pull the individual
    weight matrices and bias vectors from the supplied weight list ❶. These are NumPy
    arrays: `w12` is a 2 × 3 matrix mapping the two-element input to the first hidden
    layer with three nodes, `w23` is a 3 × 2 matrix mapping the first hidden layer
    to the second hidden layer, and `w34` is a 2 × 1 matrix mapping the second hidden
    layer to the output. The bias vectors are `b1`, three elements; `b2`, two elements;
    and `b3`, a single element (a scalar).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在`evaluate`中实现；我们来看一下如何实现。首先，从提供的权重列表中提取各个权重矩阵和偏置向量❶。这些是NumPy数组：`w12`是一个2
    × 3的矩阵，将两个输入元素映射到具有三个节点的第一隐藏层，`w23`是一个3 × 2的矩阵，将第一隐藏层映射到第二隐藏层，`w34`是一个2 × 1的矩阵，将第二隐藏层映射到输出层。偏置向量有`b1`，包含三个元素；`b2`，包含两个元素；以及`b3`，是一个单独的元素（标量）。
- en: Notice the weight matrices are not of the same shape as we previously indicated
    they would be. They are transposes. This is because we’re multiplying vectors,
    which are treated as 1 × 2 matrices, by the weight matrices. Because scalar multiplication
    is commutative, meaning *ab* = *ba*, we see that we’re still calculating the same
    argument value for the activation function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，权重矩阵的形状与我们之前指出的不同。它们是转置矩阵。这是因为我们正在将向量（视为1×2矩阵）与权重矩阵相乘。由于标量乘法是可交换的，即*ab*
    = *ba*，所以我们看到我们仍然在计算激活函数的相同参数值。
- en: Next, `evaluate` sets the number correct (`nc`) and number wrong (`nw`) counters
    to 0\. These are for calculating the overall score across the entire test set.
    Similarly, we define `prob`, a vector to hold the output probability value for
    each of the test samples.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`evaluate` 将正确计数器（`nc`）和错误计数器（`nw`）初始化为0。这些用于计算整个测试集的总体得分。同样，我们定义了 `prob`，这是一个向量，用于存储每个测试样本的输出概率值。
- en: The loop applies the entire network to each test sample. First, we map the input
    vectors to the first hidden layer and calculate *a*[1], a vector of three numbers,
    the activation for each of the three hidden nodes. We then take these first hidden
    layer activations and calculate the second hidden layer activations, *a*[2]. This
    is a two-element vector as there are two nodes in the second hidden layer. Next,
    we calculate the output value for the current input vector and store it in the
    `prob` array. The class label, `z`, is assigned by checking if the output value
    of the network is < 0.5 or not. Finally, we increment the correct (`nc`) or incorrect
    (`nw`) counters based on the actual label for this sample (`y[i]`) ❷. When all
    samples have been passed through the network, the overall accuracy is returned
    as the number of correctly classified samples divided by the total number of samples.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 循环将整个网络应用于每个测试样本。首先，我们将输入向量映射到第一隐藏层，并计算*a*[1]，这是一个包含三个数的向量，表示每个隐藏节点的激活值。然后，我们将这些第一隐藏层的激活值作为输入，计算第二隐藏层的激活值*a*[2]。由于第二隐藏层有两个节点，因此它是一个包含两个元素的向量。接下来，我们计算当前输入向量的输出值，并将其存储在`prob`数组中。通过检查网络的输出值是否小于0.5来分配类别标签`z`。最后，我们根据该样本的实际标签（`y[i]`）递增正确（`nc`）或错误（`nw`）的计数器❷。当所有样本都通过网络处理完毕时，整体准确率通过正确分类的样本数与总样本数之比返回。
- en: This is all well and good; we can implement a network and pass input vectors
    through it to see how well it does. If the network had a third hidden layer, we
    would pass the output of the second hidden layer (`a2`) through it before calculating
    the final output value.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，我们可以实现一个网络，并将输入向量传递其中，查看其表现如何。如果网络有第三个隐藏层，我们将把第二个隐藏层的输出（`a2`）传递给它，然后计算最终的输出值。
- en: Training and Testing the Neural Network
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络的训练与测试
- en: The code in [Listing 8-2](ch08.xhtml#ch8lis2) applies the trained model to the
    test data. To train the model in the first place, we’ll use sklearn. The code
    to train the model is in [Listing 8-3](ch08.xhtml#ch8lis3).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 8-2](ch08.xhtml#ch8lis2) 中的代码将训练好的模型应用于测试数据。为了训练模型，我们将使用 sklearn。训练模型的代码在
    [Listing 8-3](ch08.xhtml#ch8lis3) 中。'
- en: import numpy as np
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pickle
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: import pickle
- en: from sklearn.neural_network import MLPClassifier
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neural_network import MLPClassifier
- en: xtrain= np.load("iris2_train.npy")
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: xtrain= np.load("iris2_train.npy")
- en: ytrain= np.load("iris2_train_labels.npy")
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ytrain= np.load("iris2_train_labels.npy")
- en: xtest = np.load("iris2_test.npy")
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: xtest = np.load("iris2_test.npy")
- en: ytest = np.load("iris2_test_labels.npy")
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ytest = np.load("iris2_test_labels.npy")
- en: ❶ clf = MLPClassifier(
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ clf = MLPClassifier(
- en: ❷ hidden_layer_sizes=(3,2),
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ hidden_layer_sizes=(3,2),
- en: ❸ activation="logistic",
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ activation="logistic",
- en: solver="adam", tol=1e-9,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: solver="adam", tol=1e-9,
- en: max_iter=5000,
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: max_iter=5000,
- en: verbose=True)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=True)
- en: clf.fit(xtrain, ytrain)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(xtrain, ytrain)
- en: prob = clf.predict_proba(xtest)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: prob = clf.predict_proba(xtest)
- en: score = clf.score(xtest, ytest)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: score = clf.score(xtest, ytest)
- en: ❹ w12 = clf.coefs_[0]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ w12 = clf.coefs_[0]
- en: w23 = clf.coefs_[1]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: w23 = clf.coefs_[1]
- en: w34 = clf.coefs_[2]
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: w34 = clf.coefs_[2]
- en: b1 = clf.intercepts_[0]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: b1 = clf.intercepts_[0]
- en: b2 = clf.intercepts_[1]
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: b2 = clf.intercepts_[1]
- en: b3 = clf.intercepts_[2]
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: b3 = clf.intercepts_[2]
- en: weights = [w12,b1,w23,b2,w34,b3]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: weights = [w12,b1,w23,b2,w34,b3]
- en: pickle.dump(weights, open("iris2_weights.pkl","wb"))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: pickle.dump(weights, open("iris2_weights.pkl","wb"))
- en: print()
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: print("Test results:")
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: print("测试结果：")
- en: 'print("  Overall score: %0.7f" % score)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("  总体得分: %0.7f" % score)'
- en: print()
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'for i in range(len(ytest)):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(ytest)):'
- en: p = 0 if (prob[i,1] < 0.5) else 1
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: p = 0 if (prob[i,1] < 0.5) else 1
- en: 'print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))'
- en: print()
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: '*Listing 8-3: Using sklearn to train the iris neural network. See* nn_iris_mlpclassifier.py.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-3：使用 sklearn 训练鸢尾花神经网络。请参见* nn_iris_mlpclassifier.py。'
- en: First, we load the training and testing data from disk. These are the same files
    we created previously. Then we set up the neural network object, an instance of
    `MLPClassifier` ❶. The network has two hidden layers, the first with three nodes
    and the second with two nodes ❷. This matches the architecture in [Figure 8-1](ch08.xhtml#ch8fig1).
    The network is also using *logistic* layers ❸. This is another name for a sigmoid
    layer. We train the model by calling `fit` just as we did for other sklearn model
    types. Since we set `verbose` to `True`, we’ll get output showing us the loss
    for each iteration.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从磁盘加载训练数据和测试数据。这些是我们之前创建的相同文件。然后我们设置神经网络对象，即 `MLPClassifier` 的实例 ❶。该网络有两个隐藏层，第一个包含三个节点，第二个包含两个节点
    ❷。这与[图 8-1](ch08.xhtml#ch8fig1)中的架构相匹配。该网络还使用 *logistic* 层 ❸。这就是 sigmoid 层的另一个名称。我们通过调用
    `fit` 来训练模型，就像我们为其他 sklearn 模型类型所做的那样。由于我们将 `verbose` 设置为 `True`，因此我们会看到输出显示每次迭代的损失。
- en: Calling `predict_proba` gives us the output probabilities on the test data.
    This method is also supported by most other sklearn models. This is the model’s
    certainty as to the assigned output label. We then call `score` to calculate the
    score over the test set as we have done before.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `predict_proba` 可以给我们测试数据的输出概率。此方法也被大多数其他 sklearn 模型支持。这是模型对分配的输出标签的确定性。然后我们调用
    `score` 来计算测试集上的得分，就像我们之前做的那样。
- en: We want to store the learned weights and biases so we can use them with our
    test code. We can pull them directly from the trained model ❹. These are packed
    into a list (`weights`) and dumped to a Python pickle file.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望存储学习到的权重和偏置，以便我们可以在测试代码中使用它们。我们可以直接从训练好的模型 ❹ 中提取它们。这些被打包成一个列表（`weights`）并导出为
    Python pickle 文件。
- en: The remaining code prints the results of running the sklearn trained model against
    the held-out test data. For example, a particular run of this code gives
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码输出的是运行经过 sklearn 训练的模型在保留的测试数据上的结果。例如，运行此代码的一次结果显示
- en: '[PRE0]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: indicating that the model was perfect against the small test dataset. The output
    shows the sample number, the actual class label, the assigned class label, and
    the output probability of being class 1\. If we run the pickle file holding the
    sklearn network’s weights and biases through our evaluation code, we see that
    the output probabilities are precisely the same as the preceding code, indicating
    that our hand-generated neural network implementation is working correctly.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表明该模型在小型测试数据集上的表现完美。输出显示了样本编号、实际类别标签、分配的类别标签以及属于类别 1 的输出概率。如果我们将包含 sklearn 网络权重和偏置的
    pickle 文件通过我们的评估代码运行，我们会看到输出概率与前面的代码完全相同，表明我们手动生成的神经网络实现是正确的。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the anatomy of a neural network. We described
    the architecture, the arrangement of nodes, and the connections between them.
    We discussed the output layer nodes and the functions they compute. We then saw
    that all the weights and biases could be conveniently represented by matrices
    and vectors. Finally, we presented a simple network for classifying a subset of
    the iris data and showed how it could be trained and evaluated.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了神经网络的结构。我们描述了架构、节点的排列以及它们之间的连接。我们讨论了输出层节点及其计算的函数。然后我们看到，所有的权重和偏置可以方便地通过矩阵和向量来表示。最后，我们展示了一个简单的网络，用于对鸢尾花数据集的子集进行分类，并展示了如何训练和评估该网络。
- en: Now that we have our feet wet, let’s move on and dive into the theory behind
    neural networks.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经初步了解，让我们继续深入研究神经网络背后的理论。
