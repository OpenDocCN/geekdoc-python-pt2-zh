- en: '**8'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: INTRODUCTION TO NEURAL NETWORKS**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Neural networks are the heart of deep learning. In [Chapter 9](ch09.xhtml#ch09),
    we’ll take a deep dive into what we’ll call *traditional neural networks*. However,
    before we do that, we’ll introduce the anatomy of a neural network, followed by
    a quick example.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we’ll present the components of a *fully connected feed- forward
    neural network*. Visually, you can imagine the network as shown in [Figure 8-1](ch08.xhtml#ch8fig1).
    We’ll refer to this figure often in this chapter and the next. Your mission, should
    you choose to accept it, is to commit this figure to memory to save wear and tear
    on the book by flipping back to it repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/08fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-1: A sample neural network*'
  prefs: []
  type: TYPE_NORMAL
- en: After discussing the structure and parts of a neural network, we’ll explore
    training our example network to classify irises. From this initial experiment,
    [Chapter 9](ch09.xhtml#ch09) will lead us to gradient descent and the backpropagation
    algorithm—the standard way that neural networks, including advanced deep neural
    networks, are trained. This chapter is intended as a warm-up. The heavy lifting
    starts in [Chapter 9](ch09.xhtml#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of a Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A *neural network* is a graph. In computer science, a *graph* is a series of
    *nodes*, universally drawn as circles, connected by *edges* (short line segments).
    This abstraction is useful for representing many different kinds of relationships:
    roads between cities, who knows whom on social media, the structure of the internet,
    or a series of basic computational units that can be used to approximate any mathematical
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: The last example is, of course, deliberate. Neural networks are universal function
    approximators. They use a graph structure to represent a series of computational
    steps mapping an input feature vector to an output value, typically interpreted
    as a probability. Neural networks are built in layers. Conceptually, they act
    from left to right, mapping an input feature vector to the output(s) by passing
    values along the edges to the nodes. Note, the nodes of a neural network are often
    referred to as *neurons*. We’ll see why shortly. The nodes calculate new values
    based on their inputs. The new values are then passed to the next layer of nodes
    and so on until the output nodes are reached. In [Figure 8-1](ch08.xhtml#ch8fig1),
    there’s an input layer on the left, a hidden layer to its right, another hidden
    layer right of that, and a single node in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The previous section included the phrase *fully connected feedforward neural
    network* without much explanation. Let’s break it down. The *fully connected*
    part means every node of a layer has its output sent to every node of the next
    layer. The *feedforward* part means that information passes from left to right
    through the network without being sent back to a previous layer; there is no *feedback*,
    no looping, in the network structure. This leaves only the *neural network* part.
  prefs: []
  type: TYPE_NORMAL
- en: The Neuron
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Personally, I have a love/hate relationship with the phrase *neural network*.
    The phrase itself comes from the fact that in a very crude approximation, the
    basic unit of the network resembles a neuron in a brain. Consider [Figure 8-2](ch08.xhtml#ch8fig2),
    which we’ll describe in detail shortly.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/08fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-2: A single neural network node*'
  prefs: []
  type: TYPE_NORMAL
- en: Recalling that our visualization of a network always moves from left to right,
    we see that the node (the circle) accepts input from the left, and has a single
    output on the right. Here there are two inputs, but it might be hundreds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many inputs mapped to a single output echo how a neuron in the brain works:
    structures called dendrites accept input from many other neurons, and the single
    axon is the output. I love this analogy because it leads to a cool way of talking
    and thinking about the networks. But I hate the analogy because these artificial
    neurons are, operationally, quite different from real ones, and the analogy quickly
    falls apart. There is an anatomical similarity to actual neurons, but they’re
    not the same, and it leads to confusion on the part of those who are not familiar
    with machine learning, causing some to believe that computer scientists are truly
    building artificial brains or that the networks think. The meaning of the word
    *think* is hard to pin down, but to me it doesn’t apply to what a neural network
    does.'
  prefs: []
  type: TYPE_NORMAL
- en: Returning now to [Figure 8-2](ch08.xhtml#ch8fig2), we see two squares on the
    left, a bunch of lines, a circle, a line on the right, and a bunch of labels with
    subscripts. Let’s sort this out. If we understand [Figure 8-2](ch08.xhtml#ch8fig2),
    we’ll be well on our way to understanding neural networks. Later, we’ll see an
    implementation of our visual model in code and be surprised to learn how simple
    it can be.
  prefs: []
  type: TYPE_NORMAL
- en: Everything in [Figure 8-2](ch08.xhtml#ch8fig2) focuses on the circle. This is
    the actual node. In reality, it implements a mathematical function called the
    *activation function*, which calculates the output of the node, a single number.
    The two squares are the inputs to the node. This node accepts features from an
    input feature vector; we use squares to differentiate from circles, but the input
    might just as well have come from another group of circular nodes in a previous
    network layer.
  prefs: []
  type: TYPE_NORMAL
- en: Each input is a number, a single scalar value, which we’re calling *x*[0] and
    *x*[1]. These inputs move to the node along the two line segments labeled *w*[0]
    and *w*[1]. These line segments represent *weights*, the strength of the connection.
    Computationally, the inputs (*x*[0], *x*[1]) are multiplied by the weights (*w*[0],
    *w*[1]), summed, and given to the activation function of the node. Here we’re
    calling the activation function *h*, a fairly common thing to call it.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the activation function is the output of the node. Here we’re calling
    this output *a*. The inputs, multiplied by the weights, are added together and
    given to the activation function to produce an output value. We have yet to mention
    the *b*[0] value, which is also added in and passed to the activation function.
    This is the *bias* term. It’s an offset used to adjust the input range to make
    it suitable for the activation function. In [Figure 8-2](ch08.xhtml#ch8fig2),
    we added a zero subscript. There is a bias value for each node in each layer,
    so the subscript here implies that this node is the first node in the layer. (Remember
    computer people always count from zero, not one.)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all that a neural network node does: a neural network node accepts
    multiple inputs, *x*[0],*x*[1],…, multiplies each by a weight value, *w*[0],*w*[1],…,
    sums these products along with the bias term, *b*, and passes this sum to the
    activation function, *h*, to produce a single scalar output value, *a*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + … + *b*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s it. Get a bunch of nodes together, link them appropriately, figure out
    how to train them to set the weights and biases, and you have a useful neural
    network. As you’ll see in the next chapter, training a neural network is no easy
    feat. But once trained, they’re simple to use: feed it a feature vector, and out
    comes a classification.'
  prefs: []
  type: TYPE_NORMAL
- en: As an aside, we’ve been calling these graphs *neural networks*, and will continue
    to do so, sometimes using the abbreviation *NN*. If you read other books or papers,
    you might see them called *artificial neural networks (ANNs)* or even *multi-layer
    perceptrons (MLPs)*, as in the sklearn MLPClassifier class name. I recommend sticking
    with *neural network*, but that’s just me.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s talk about activation functions. The activation function for a node takes
    a single scalar input, the sum of the inputs times the weights plus the bias,
    and does something to it. In particular, we need the activation function to be
    nonlinear so that the model can learn complex functions. Mathematically, it’s
    easiest to see what a nonlinear function is by stating what a linear function
    is and then saying that any mapping that is not linear is . . . nonlinear.
  prefs: []
  type: TYPE_NORMAL
- en: A *linear function*, *g*, has output that is directly proportional to the input,
    *g*(*x*) ∝ *x*, where ∝ means *proportional to*. Alternatively, the graph of a
    linear function is a straight line. Therefore, any function whose graph is not
    a straight line is a nonlinear function.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the function
  prefs: []
  type: TYPE_NORMAL
- en: '*g*(*x*) = 3*x* + 2'
  prefs: []
  type: TYPE_NORMAL
- en: is a linear function because its graph is a straight line. A constant function
    like *g*(*x*) = 1 is also linear. However, the function
  prefs: []
  type: TYPE_NORMAL
- en: '*g*(*x*) = *x*² + 2'
  prefs: []
  type: TYPE_NORMAL
- en: is a nonlinear function because the exponent of *x* is 2\. Transcendental functions
    are also nonlinear. *Transcendental functions* are functions like *g*(*x*) = log*x*,
    or *g*(*x*) = *e*^(*x*), where *e* = 2.718*...* is the base of the natural logarithm.
    *Trigonometric functions* like sine and cosine, their inverses, and functions
    like tangent that are built from sine and cosine are also transcendental functions.
    These functions are transcendental because you cannot form them as finite combinations
    of elementary algebra operations. They are nonlinear because their graphs are
    not straight lines.
  prefs: []
  type: TYPE_NORMAL
- en: The network needs nonlinear activation functions; otherwise, it will be able
    to learn only linear mappings, and linear mappings are not sufficient to make
    the networks generally useful. Consider a trivial network of two nodes, each with
    one input. This means there’s one weight and one bias value per node, and the
    output of the first node is the input of the second. If we set *h*(*x*) = 5*x
    –* 3, a linear function, then for input *x* the network computes output *a*[1]
    to be
  prefs: []
  type: TYPE_NORMAL
- en: '| *a*[1] | = *h*(*w*[1]*a*[0] + *b*[1]) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = *h*(*w*[1]*h*(*w*[0]*x* + *b*[0]) + *b*[1]) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = *h*(*w*[1](5(*w*[0]*x* + *b*[0]) – 3) + *b*[1]) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = *h*(*w*[1](5*w*[0]*x* + 5*b*[0] – 3) + *b*[1]) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = *h*(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = 5(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) – 3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | = (25*w*[1]*w*[0])*x* + (25*w*[1]*b*[0] – [1]5*w*[1] + 5*b*[1] – 3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | = *W**x* + *B* |'
  prefs: []
  type: TYPE_TB
- en: 'for *W* = 25*w*[1]*w*[0] and *B* = 25*w*[1]*b*[0] *–* 15*w*[1] + 5*b*[1] *–*
    3, which is also a linear function, another line with slope *W* and intercept
    *B* since neither *W* nor *B* depend on *x*. Therefore, a neural network with
    linear activation functions would learn only a linear model since the composition
    of linear functions is also linear. It’s precisely this limitation of linear activation
    functions that caused the first neural network “winter” in the 1970s: research
    into neural networks was effectively abandoned because they were thought to be
    too simple to learn complex functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so we want nonlinear activation functions. Which ones? There are an infinite
    number of possibilities. In practice, a few have risen to the top because of their
    proven usefulness or nice properties or both. Traditional neural networks used
    either sigmoid activation functions or hyperbolic tangents. A *sigmoid* is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/174equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and the *hyperbolic tangent* is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/174equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Plots of both of these functions are in [Figure 8-3](ch08.xhtml#ch8fig3), with
    the sigmoid on the top and the hyperbolic tangent on the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to notice is that both of these functions have roughly the same
    “S” shape. The sigmoid runs from 0 as you go further left along the x-axis to
    1 as you go to the right. At 0, the function value is 0.5\. The hyperbolic tangent
    does the same but goes from –1 to +1 and is 0 at *x* = 0.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the sigmoid and hyperbolic tangent have been replaced by the
    *rectified linear unit*, or *ReLU* for short. The ReLU is simple, and has convenient
    properties for neural networks. Even though the word *linear* is in the name,
    the ReLU is a nonlinear function—its graph is not a straight line. When we discuss
    backpropagation training of neural networks in [Chapter 9](ch09.xhtml#ch09), we’ll
    learn why this change has happened.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/08fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-3: A sigmoid function (top) and a hyperbolic tangent function (bottom).
    Note that the y-axis scales are not the same*.'
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU is as follows and is shown in [Figure 8-4](ch08.xhtml#ch8fig4).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/176equ01.jpg)![image](Images/08fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-4: The rectified linear activation function, ReLU(x) = max(0,x)*'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU is called *rectified* because it removes the negative values and replaces
    them with 0\. In truth, the machine learning community uses several different
    versions of this function, but all are essentially replacing negative values with
    a constant or some other value. The piecewise nature of the ReLU is what makes
    it nonlinear and, therefore, suitable for use as a neural network activation function.
    It’s also computationally simple, far faster to calculate than either the sigmoid
    or the hyperbolic tangent. This is because the latter functions use *e*^(*x*),
    which, in computer terms, means a call to the exp function. This function is typically
    implemented as a sum of terms of a series expansion, translating into dozens of
    floating-point operations in place of the single if statement necessary to implement
    a ReLU. Small savings like this add up in an extensive network with potentially
    thousands of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of a Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve discussed nodes and how they work, and hinted that nodes are connected
    to form networks. Let’s look more closely at how nodes are connected, the *architecture*
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Standard neural networks like the ones we are working with in this chapter are
    built in layers, as you saw in [Figure 8-1](ch08.xhtml#ch8fig1). We don’t need
    to do this, but as we’ll see, this buys us some computational simplicity and greatly
    simplifies training. A feedforward network has an input layer, one or more hidden
    layers, and an output layer. The input layer is simply the feature vector, and
    the output layer is the prediction (probability). If the network is for a multiclass
    problem, the output layer might have more than one node, with each node representing
    the model’s prediction for each of the possible classes of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layers are made of nodes, and the nodes of layer *i* accept as input
    the output of the nodes of layer *i –* 1 and pass their outputs to the inputs
    of the nodes of layer *i* + 1\. The connections between the layers are typically
    fully connected, meaning every output of every node of layer *i –* 1 is used as
    an input to every node of layer *i*, hence *fully connected*. Again, we don’t
    need to do this, but it simplifies the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden layers and the number of nodes in each hidden layer define
    the architecture of the network. It has been proven that a single hidden layer
    with enough nodes can learn any function mapping. This is good because it means
    neural networks are applicable to machine learning problems since, in the end,
    the model acts as a complex function mapping inputs to output labels and probabilities.
    However, like many theoretical results, this does not mean that it’s practical
    for a single layer network to be used in all situations. As the number of nodes
    (and layers) in a network grows, so, too, does the number of parameters to learn
    (weights and biases), and therefore the amount of training data needed goes up
    as well. It’s the curse of dimensionality again.
  prefs: []
  type: TYPE_NORMAL
- en: Issues like these stymied neural networks for a second time in the 1980s. Computers
    were too slow to train large networks, and, regardless, there was usually too
    little data available to train the network anyway. Practitioners knew that if
    both of these situations changed, then it would become possible to train large
    networks that would be far more capable than the small networks of the time. Fortunately
    for the world, the situation changed in the early 2000s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Selecting the proper neural network architecture has a huge impact on whether
    or not your model will learn anything. This is where experience and intuition
    come in. Selecting the right architecture is the dark art of using neural networks.
    Let’s try to be more helpful by giving some (crude) rules of thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: If your input has definite spatial relationships, like the parts of an image,
    you might want to use a convolutional neural network instead ([Chapter 12](ch12.xhtml#ch12)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use no more than three hidden layers. Recall, in theory, one sufficiently large
    hidden layer is all that is needed, so use as few hidden layers as necessary.
    If the model learns with one hidden layer, then add a second to see if that improves
    things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes in the first hidden layer should match or (ideally) exceed
    the number of input vector features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Except for the first hidden layer (see previous rule), the number of nodes per
    hidden layer should be the same as or some value between the number of nodes in
    the previous layer and the following layer. If layer *i –* 1 has *N* nodes and
    layer *i* + 1 has *M* nodes, then layer *i* might be good with *N* ≤ *x* ≤ *M*
    nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first rule says that a traditional neural network best applies to situations
    where your input does not have spatial relationships—that is, you have a feature
    vector, not an image. Also, when your input dimension is small, or when you do
    not have a lot of data, which makes it hard to train a larger convolutional network,
    you should give a traditional network a try. If you do think you are in a situation
    where a traditional neural network is called for, start small, and grow it as
    long as performance improves.
  prefs: []
  type: TYPE_NORMAL
- en: Output Layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last layer of a neural network is the output layer. If the network is modeling
    a continuous value, known as *regression*, a use case we’re ignoring in this book,
    then the output layer is a node that doesn’t use an activation function; it simply
    reports the argument to *h* in [Figure 8-2](ch08.xhtml#ch8fig2). Note that this
    is the same as saying that the activation function is the identity function, *h*(*x*)
    = *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our neural networks are for classification; we want them to output a decision
    value. If we have two classes labeled 0 and 1, we make the activation function
    of the final node a sigmoid. This will output a value between 0 and 1 that we
    can interpret as a likelihood or probability that the input belongs to class 1\.
    We make our classification decision based on the output value with a simple rule:
    if the activation value is less than 0.5, call the input class 0; otherwise, call
    it class 1\. We’ll see in [Chapter 11](ch11.xhtml#ch11) how changing this threshold
    of 0.5 can be used to tune a model’s performance for the task at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: If we have more than two classes, we need to take a different approach. Instead
    of a single node in the output layer, we’ll have *N* output nodes, one for each
    class, each one using the identity function for *h*. Then, we apply a *softmax*
    operation to these *N* outputs and select the output with the largest softmax
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate what we mean by softmax. Suppose we have a dataset with four
    classes in it. What they represent doesn’t really matter; the network doesn’t
    know what they represent, either. The classes are labeled 0, 1, 2, and 3\. So,
    *N* = 4 means our network will have four output nodes, each one using the identity
    function for *h*. This looks like [Figure 8-5](ch08.xhtml#ch8fig5), where we have
    also shown the softmax operation and the resulting output vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/08fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-5: The last hidden layer *n*-1 and output layer (*n*, nodes numbered)
    for a neural network with four classes. The softmax operation is applied, producing
    a four-element output vector, *[p*0,*p*1,*p*2,*p*3*]*.*'
  prefs: []
  type: TYPE_NORMAL
- en: We select the index of the largest value in this output vector as the class
    label for the given input feature vector. The softmax operation ensures that the
    elements of this vector sum to 1, so we can again be a bit sloppy and call these
    values the probability of belonging to each of the four classes. That is why we
    take only the largest value to decide the output class label.
  prefs: []
  type: TYPE_NORMAL
- en: 'The softmax operation is straightforward: the probability for each of the outputs
    is simply'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/179equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *a*[*i*] is the *i*-th output, and the denominator is the sum over all
    the outputs. For the example, *i* = 0,1,2,3, and the index of the largest value
    will be the class label assigned to the input.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, assume the output of the four last layer nodes is
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[0] = 0.2'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*1 = 1.3'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*2 = 0.8'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*3 = 2.1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then calculate the softmax as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[0] = *e*^(0.2)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.080'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[1] = *e*^(1.3)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.240'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[2] = *e*^(0.8)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.146'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[3] = *e*^(2.1)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.534'
  prefs: []
  type: TYPE_NORMAL
- en: Select class 3 because *p*[3] is the largest. Note that the sum of the *p*[*i*]
    values is 1.0, as we would expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two points should be mentioned here. In the preceding equations, we used the
    sigmoid to calculate the output of the network. If we set the number of classes
    to 2 and calculate the softmax, we’ll get two output values: one will be some
    *p*, and the other will be 1 *– p*. This is identical to the sigmoid alone, selecting
    the probability of the input being of class 1.'
  prefs: []
  type: TYPE_NORMAL
- en: The second point has to do with implementing the softmax. If the network outputs,
    the *a* values, are large, then *e*^(*a*) might be very large, which is something
    the computer will not like. Precision will be lost, at least, or the value might
    overflow and make the output meaningless. Numerically, if we subtract the largest
    *a* value from all the others before calculating the softmax, we’ll take the exponential
    over smaller values that are less likely to overflow. Doing this for the preceding
    example gives new *a* values
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/180equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we subtract 2.1 because that is the largest *a* value. This leads to precisely
    the same *p* values we found before, but this time protected against overflow
    in the case that any of the *a* values are too large.
  prefs: []
  type: TYPE_NORMAL
- en: Representing Weights and Biases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before we move on to an example neural network, let’s revisit the weights and
    biases and see that we can greatly simplify the implementation of a neural network
    by viewing it in terms of matrices and vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the mapping from an input feature vector of two elements to the first
    hidden layer with three nodes (*a*[1] in [Figure 8-1](ch08.xhtml#ch8fig1)). Let’s
    label the edges between the two layers (the weights) as *w*[*ij*] with *i* = 0,1
    for the inputs *x*[0] and *x*[1] and *j* = 0,1,2 for the three hidden layer nodes
    numbered from top to bottom of the figure. Additionally, we need three bias values
    that are not shown in the figure, one for each hidden node. We’ll call these *b*[0],
    *b*[1], and *b*[2], again, top to bottom.
  prefs: []
  type: TYPE_NORMAL
- en: In order to calculate the outputs of the activation functions, *h*, for the
    three hidden nodes, we need to find the following.
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[0] = *h*(*w*[00]*x*[0] + *w*[10]*x*[1] + *b*[0])'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[1] = *h*(*w*[01]*x*[0] + *w*[11]*x*[1] + *b*[1])'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[2] = *h*(*w*[02]*x*[0] + *w*[12]*x*[1] + *b*[2])'
  prefs: []
  type: TYPE_NORMAL
- en: But, remembering how matrix multiplication and vector addition work, we see
    that this is exactly
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/181equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![Image](Images/181equ03.jpg), and *W* is a 3 × 2 matrix of weight values.
    In this case, the activation function, *h*, is given a vector of input values
    and produces a vector of output values. This is simply applying *h* to every element
    of ![Image](Images/181equ04.jpg). For example, applying *h* to a vector ![Image](Images/xbar1.jpg)
    with three elements is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/181equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with *h* applied separately to each element of ![Image](Images/xbar1.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the NumPy Python module is designed to work with arrays, and matrices
    and vectors are arrays, we arrive at the pleasant conclusion that the weights
    and biases of a neural network can be stored in NumPy arrays and we need only
    simple matrix operations (calls to np.dot) and addition to work with a fully connected
    neural network. Note this is why we want to use fully connected networks: their
    implementation is straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To store the network of [Figure 8-1](ch08.xhtml#ch8fig1), we need a weight
    matrix and bias vector between each layer, giving us three matrices and three
    vectors: a matrix and vector each for the input to the first hidden layer, the
    first hidden layer to the second, and the second hidden layer to the output. The
    weight matrices are of dimensions 3 × 2, 2 × 3, and 1 × 2, respectively. The bias
    vectors are of length 3, 2, and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Simple Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll implement the sample neural network of [Figure 8-1](ch08.xhtml#ch8fig1)
    and train it on two features from the iris dataset. We’ll implement the network
    from scratch but use sklearn to train it. The goal of this section is to see how
    straightforward it is to implement a simple neural network. Hopefully, this will
    clear some of the fog that might be hanging around from the discussion of the
    previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: The network of [Figure 8-1](ch08.xhtml#ch8fig1) accepts an input feature vector
    with two features. It has two hidden layers, one with three nodes and the other
    with two nodes. It has one sigmoid output. The activation functions of the hidden
    nodes are also sigmoids.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before we look at the neural network code, let’s build the dataset we’ll train
    against and see what it looks like. We know the iris dataset already, but for
    this example, we’ll use only two classes and only two of the four features. The
    code to build the train and test datasets is in [Listing 8-1](ch08.xhtml#ch8lis1).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: ❶ d = np.load("iris_train_features_augmented.npy")
  prefs: []
  type: TYPE_NORMAL
- en: l = np.load("iris_train_labels_augmented.npy")
  prefs: []
  type: TYPE_NORMAL
- en: d1 = d[np.where(l==1)]
  prefs: []
  type: TYPE_NORMAL
- en: d2 = d[np.where(l==2)]
  prefs: []
  type: TYPE_NORMAL
- en: ❷ a=len(d1)
  prefs: []
  type: TYPE_NORMAL
- en: b=len(d2)
  prefs: []
  type: TYPE_NORMAL
- en: x = np.zeros((a+b,2))
  prefs: []
  type: TYPE_NORMAL
- en: x[:a,:] = d1[:,2:]
  prefs: []
  type: TYPE_NORMAL
- en: x[a:,:] = d2[:,2:]
  prefs: []
  type: TYPE_NORMAL
- en: ❸ y = np.array([0]*a+[1]*b)
  prefs: []
  type: TYPE_NORMAL
- en: i = np.argsort(np.random.random(a+b))
  prefs: []
  type: TYPE_NORMAL
- en: x = x[i]
  prefs: []
  type: TYPE_NORMAL
- en: y = y[i]
  prefs: []
  type: TYPE_NORMAL
- en: ❹ np.save("iris2_train.npy", x)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris2_train_labels.npy", y)
  prefs: []
  type: TYPE_NORMAL
- en: ❺ d = np.load("iris_test_features_augmented.npy")
  prefs: []
  type: TYPE_NORMAL
- en: l = np.load("iris_test_labels_augmented.npy")
  prefs: []
  type: TYPE_NORMAL
- en: d1 = d[np.where(l==1)]
  prefs: []
  type: TYPE_NORMAL
- en: d2 = d[np.where(l==2)]
  prefs: []
  type: TYPE_NORMAL
- en: a=len(d1)
  prefs: []
  type: TYPE_NORMAL
- en: b=len(d2)
  prefs: []
  type: TYPE_NORMAL
- en: x = np.zeros((a+b,2))
  prefs: []
  type: TYPE_NORMAL
- en: x[:a,:] = d1[:,2:]
  prefs: []
  type: TYPE_NORMAL
- en: x[a:,:] = d2[:,2:]
  prefs: []
  type: TYPE_NORMAL
- en: y = np.array([0]*a+[1]*b)
  prefs: []
  type: TYPE_NORMAL
- en: i = np.argsort(np.random.random(a+b))
  prefs: []
  type: TYPE_NORMAL
- en: x = x[i]
  prefs: []
  type: TYPE_NORMAL
- en: y = y[i]
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris2_test.npy", x)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris2_test_labels.npy", y)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8-1: Building the simple example dataset. See* nn_iris_dataset.py.'
  prefs: []
  type: TYPE_NORMAL
- en: This code is straightforward data munging. We start with the augmented dataset
    and load the samples and labels ❶. We want only class 1 and class 2, so we find
    the indices of those samples and pull them out. We’re keeping only features 2
    and 3 and put them in x ❷. Next, we build the labels (y) ❸. Note, we recode the
    class labels to 0 and 1\. Finally, we scramble the order of the samples and write
    the new dataset to disk ❹. Last of all, we repeat this process to build the test
    samples ❺.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-6](ch08.xhtml#ch8fig6) shows the training set. We can plot it in
    this case because we have only two features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/08fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-6: The training data for the two-class, two-feature iris dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: We immediately see that this dataset is not trivially separable. There is no
    simple line we can draw that will correctly split the training set into two groups,
    one all class 0 and the other all class 1\. This makes things a little more interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s see how to implement the network of [Figure 8-1](ch08.xhtml#ch8fig1) in
    Python using NumPy. We’ll assume that it’s already trained, meaning we already
    know all the weights and biases. The code is in [Listing 8-2](ch08.xhtml#ch8lis2).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pickle
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  prefs: []
  type: TYPE_NORMAL
- en: 'def sigmoid(x):'
  prefs: []
  type: TYPE_NORMAL
- en: return 1.0 / (1.0 + np.exp(-x))
  prefs: []
  type: TYPE_NORMAL
- en: 'def evaluate(x, y, w):'
  prefs: []
  type: TYPE_NORMAL
- en: ❶ w12,b1,w23,b2,w34,b3 = w
  prefs: []
  type: TYPE_NORMAL
- en: nc = nw = 0
  prefs: []
  type: TYPE_NORMAL
- en: prob = np.zeros(len(y))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(y)):'
  prefs: []
  type: TYPE_NORMAL
- en: a1 = sigmoid(np.dot(x[i], w12) + b1)
  prefs: []
  type: TYPE_NORMAL
- en: a2 = sigmoid(np.dot(a1, w23) + b2)
  prefs: []
  type: TYPE_NORMAL
- en: prob[i] = sigmoid(np.dot(a2, w34) + b3)
  prefs: []
  type: TYPE_NORMAL
- en: z  = 0 if prob[i] < 0.5 else 1
  prefs: []
  type: TYPE_NORMAL
- en: '❷ if (z == y[i]):'
  prefs: []
  type: TYPE_NORMAL
- en: nc += 1
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: nw += 1
  prefs: []
  type: TYPE_NORMAL
- en: return [float(nc) / float(nc + nw), prob]
  prefs: []
  type: TYPE_NORMAL
- en: ❸ xtest = np.load("iris2_test.npy")
  prefs: []
  type: TYPE_NORMAL
- en: ytest = np.load("iris2_test_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: ❹ weights = pickle.load(open("iris2_weights.pkl","rb"))
  prefs: []
  type: TYPE_NORMAL
- en: score, prob = evaluate(xtest, ytest, weights)
  prefs: []
  type: TYPE_NORMAL
- en: print()
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(prob)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'print("%3d:  actual: %d  predict: %d  prob: %0.7f" %'
  prefs: []
  type: TYPE_NORMAL
- en: (i, ytest[i], 0 if (prob[i] < 0.5) else 1, prob[i]))
  prefs: []
  type: TYPE_NORMAL
- en: print("Score = %0.4f" % score)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8-2: Using the trained weights and biases to classify held-out test
    samples. See* nn_iris_evaluate.py.'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the first thing we should notice is how short the code is. The evaluate
    function implements the network. We also need to define sigmoid as NumPy does
    not have it natively. The main code loads the test samples (xtest) and associated
    labels (ytest) ❸. These are the files generated by the preceding code, so we know
    that xtest is of shape 23 × 2 because we have 23 test samples, and each has two
    features. Similarly, ytest is a vector of 23 labels.
  prefs: []
  type: TYPE_NORMAL
- en: When we train this network, we’ll store the weights and biases as a list of
    NumPy arrays. The Python way to store a list on disk is via the pickle module,
    so we use pickle to load the list from disk ❹. The list weights has six elements
    representing the three weight matrices and three bias vectors that define the
    network. These are the “magic” numbers that our training has conditioned to the
    dataset. Finally, we call evaluate to run each of the test samples through the
    network. This function returns the score (accuracy) and the output probabilities
    for each sample (prob). The remainder of the code displays the sample number,
    actual label, predicted label, and associated output probability of being class
    1\. Finally, the score (accuracy) is shown.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network is implemented in evaluate; let’s see how. First, pull the individual
    weight matrices and bias vectors from the supplied weight list ❶. These are NumPy
    arrays: w12 is a 2 × 3 matrix mapping the two-element input to the first hidden
    layer with three nodes, w23 is a 3 × 2 matrix mapping the first hidden layer to
    the second hidden layer, and w34 is a 2 × 1 matrix mapping the second hidden layer
    to the output. The bias vectors are b1, three elements; b2, two elements; and
    b3, a single element (a scalar).'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the weight matrices are not of the same shape as we previously indicated
    they would be. They are transposes. This is because we’re multiplying vectors,
    which are treated as 1 × 2 matrices, by the weight matrices. Because scalar multiplication
    is commutative, meaning *ab* = *ba*, we see that we’re still calculating the same
    argument value for the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, evaluate sets the number correct (nc) and number wrong (nw) counters to
    0\. These are for calculating the overall score across the entire test set. Similarly,
    we define prob, a vector to hold the output probability value for each of the
    test samples.
  prefs: []
  type: TYPE_NORMAL
- en: The loop applies the entire network to each test sample. First, we map the input
    vectors to the first hidden layer and calculate *a*[1], a vector of three numbers,
    the activation for each of the three hidden nodes. We then take these first hidden
    layer activations and calculate the second hidden layer activations, *a*[2]. This
    is a two-element vector as there are two nodes in the second hidden layer. Next,
    we calculate the output value for the current input vector and store it in the
    prob array. The class label, z, is assigned by checking if the output value of
    the network is < 0.5 or not. Finally, we increment the correct (nc) or incorrect
    (nw) counters based on the actual label for this sample (y[i]) ❷. When all samples
    have been passed through the network, the overall accuracy is returned as the
    number of correctly classified samples divided by the total number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: This is all well and good; we can implement a network and pass input vectors
    through it to see how well it does. If the network had a third hidden layer, we
    would pass the output of the second hidden layer (a2) through it before calculating
    the final output value.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Testing the Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code in [Listing 8-2](ch08.xhtml#ch8lis2) applies the trained model to the
    test data. To train the model in the first place, we’ll use sklearn. The code
    to train the model is in [Listing 8-3](ch08.xhtml#ch8lis3).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import pickle
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neural_network import MLPClassifier
  prefs: []
  type: TYPE_NORMAL
- en: xtrain= np.load("iris2_train.npy")
  prefs: []
  type: TYPE_NORMAL
- en: ytrain= np.load("iris2_train_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: xtest = np.load("iris2_test.npy")
  prefs: []
  type: TYPE_NORMAL
- en: ytest = np.load("iris2_test_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: ❶ clf = MLPClassifier(
  prefs: []
  type: TYPE_NORMAL
- en: ❷ hidden_layer_sizes=(3,2),
  prefs: []
  type: TYPE_NORMAL
- en: ❸ activation="logistic",
  prefs: []
  type: TYPE_NORMAL
- en: solver="adam", tol=1e-9,
  prefs: []
  type: TYPE_NORMAL
- en: max_iter=5000,
  prefs: []
  type: TYPE_NORMAL
- en: verbose=True)
  prefs: []
  type: TYPE_NORMAL
- en: clf.fit(xtrain, ytrain)
  prefs: []
  type: TYPE_NORMAL
- en: prob = clf.predict_proba(xtest)
  prefs: []
  type: TYPE_NORMAL
- en: score = clf.score(xtest, ytest)
  prefs: []
  type: TYPE_NORMAL
- en: ❹ w12 = clf.coefs_[0]
  prefs: []
  type: TYPE_NORMAL
- en: w23 = clf.coefs_[1]
  prefs: []
  type: TYPE_NORMAL
- en: w34 = clf.coefs_[2]
  prefs: []
  type: TYPE_NORMAL
- en: b1 = clf.intercepts_[0]
  prefs: []
  type: TYPE_NORMAL
- en: b2 = clf.intercepts_[1]
  prefs: []
  type: TYPE_NORMAL
- en: b3 = clf.intercepts_[2]
  prefs: []
  type: TYPE_NORMAL
- en: weights = [w12,b1,w23,b2,w34,b3]
  prefs: []
  type: TYPE_NORMAL
- en: pickle.dump(weights, open("iris2_weights.pkl","wb"))
  prefs: []
  type: TYPE_NORMAL
- en: print()
  prefs: []
  type: TYPE_NORMAL
- en: print("Test results:")
  prefs: []
  type: TYPE_NORMAL
- en: 'print("  Overall score: %0.7f" % score)'
  prefs: []
  type: TYPE_NORMAL
- en: print()
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(ytest)):'
  prefs: []
  type: TYPE_NORMAL
- en: p = 0 if (prob[i,1] < 0.5) else 1
  prefs: []
  type: TYPE_NORMAL
- en: 'print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))'
  prefs: []
  type: TYPE_NORMAL
- en: print()
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8-3: Using sklearn to train the iris neural network. See* nn_iris_mlpclassifier.py.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we load the training and testing data from disk. These are the same files
    we created previously. Then we set up the neural network object, an instance of
    MLPClassifier ❶. The network has two hidden layers, the first with three nodes
    and the second with two nodes ❷. This matches the architecture in [Figure 8-1](ch08.xhtml#ch8fig1).
    The network is also using *logistic* layers ❸. This is another name for a sigmoid
    layer. We train the model by calling fit just as we did for other sklearn model
    types. Since we set verbose to True, we’ll get output showing us the loss for
    each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Calling predict_proba gives us the output probabilities on the test data. This
    method is also supported by most other sklearn models. This is the model’s certainty
    as to the assigned output label. We then call score to calculate the score over
    the test set as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: We want to store the learned weights and biases so we can use them with our
    test code. We can pull them directly from the trained model ❹. These are packed
    into a list (weights) and dumped to a Python pickle file.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining code prints the results of running the sklearn trained model against
    the held-out test data. For example, a particular run of this code gives
  prefs: []
  type: TYPE_NORMAL
- en: 'Test results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall score: 1.0000000'
  prefs: []
  type: TYPE_NORMAL
- en: '000: 0 - 0, 0.0705069'
  prefs: []
  type: TYPE_NORMAL
- en: '001: 1 - 1, 0.8066224'
  prefs: []
  type: TYPE_NORMAL
- en: '002: 0 - 0, 0.0308244'
  prefs: []
  type: TYPE_NORMAL
- en: '003: 0 - 0, 0.0205917'
  prefs: []
  type: TYPE_NORMAL
- en: '004: 1 - 1, 0.9502825'
  prefs: []
  type: TYPE_NORMAL
- en: '005: 0 - 0, 0.0527558'
  prefs: []
  type: TYPE_NORMAL
- en: '006: 1 - 1, 0.9455174'
  prefs: []
  type: TYPE_NORMAL
- en: '007: 0 - 0, 0.0365360'
  prefs: []
  type: TYPE_NORMAL
- en: '008: 1 - 1, 0.9471218'
  prefs: []
  type: TYPE_NORMAL
- en: '009: 0 - 0, 0.0304762'
  prefs: []
  type: TYPE_NORMAL
- en: '010: 0 - 0, 0.0304762'
  prefs: []
  type: TYPE_NORMAL
- en: '011: 0 - 0, 0.0165365'
  prefs: []
  type: TYPE_NORMAL
- en: '012: 1 - 1, 0.9453844'
  prefs: []
  type: TYPE_NORMAL
- en: '013: 0 - 0, 0.0527558'
  prefs: []
  type: TYPE_NORMAL
- en: '014: 1 - 1, 0.9495079'
  prefs: []
  type: TYPE_NORMAL
- en: '015: 1 - 1, 0.9129983'
  prefs: []
  type: TYPE_NORMAL
- en: '016: 1 - 1, 0.8931552'
  prefs: []
  type: TYPE_NORMAL
- en: '017: 0 - 0, 0.1197567'
  prefs: []
  type: TYPE_NORMAL
- en: '018: 0 - 0, 0.0406094'
  prefs: []
  type: TYPE_NORMAL
- en: '019: 0 - 0, 0.0282220'
  prefs: []
  type: TYPE_NORMAL
- en: '020: 1 - 1, 0.9526721'
  prefs: []
  type: TYPE_NORMAL
- en: '021: 0 - 0, 0.1436263'
  prefs: []
  type: TYPE_NORMAL
- en: '022: 1 - 1, 0.9446458'
  prefs: []
  type: TYPE_NORMAL
- en: indicating that the model was perfect against the small test dataset. The output
    shows the sample number, the actual class label, the assigned class label, and
    the output probability of being class 1\. If we run the pickle file holding the
    sklearn network’s weights and biases through our evaluation code, we see that
    the output probabilities are precisely the same as the preceding code, indicating
    that our hand-generated neural network implementation is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we discussed the anatomy of a neural network. We described
    the architecture, the arrangement of nodes, and the connections between them.
    We discussed the output layer nodes and the functions they compute. We then saw
    that all the weights and biases could be conveniently represented by matrices
    and vectors. Finally, we presented a simple network for classifying a subset of
    the iris data and showed how it could be trained and evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our feet wet, let’s move on and dive into the theory behind
    neural networks.
  prefs: []
  type: TYPE_NORMAL
