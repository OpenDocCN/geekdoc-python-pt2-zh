- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11'
- en: EVALUATING MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: So far, we’ve evaluated models by looking at their accuracy on a held-out test
    set. This is natural and intuitive, but as we’ll learn in this chapter, it’s not
    all that we can, or should, do to evaluate a model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过查看模型在保留测试集上的准确率来评估模型。这是自然且直观的，但正如我们将在本章中学习的那样，这并不是评估模型时我们能做的或应该做的全部。
- en: We’ll begin this chapter by defining metrics and delineating some basic assumptions.
    Then we’ll look at why we need more than just accuracy. We’ll introduce the concept
    of a confusion matrix and spend time discussing the metrics we can derive from
    it. From there, we’ll jump to performance curves, which are the best way to compare
    different models together. Finally, we’ll extend the idea of a confusion matrix
    to the multiclass case. We won’t say all there is to say about performance metrics,
    as this area is still somewhat evolving. However, by the end of this chapter,
    you’ll be familiar with the sorts of numbers that people involved in machine learning
    will throw around and have a good understanding of what they mean.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的开始，我们将定义度量标准并阐明一些基本假设。然后我们将讨论为什么仅仅依赖准确率是不够的。我们将介绍混淆矩阵的概念，并花时间讨论我们可以从中得出的度量标准。接下来，我们将讨论性能曲线，它们是比较不同模型的最佳方式。最后，我们将把混淆矩阵的概念扩展到多类别的情况。我们不会说完关于性能度量的所有内容，因为这一领域仍在发展。然而，到本章结束时，你将熟悉机器学习领域的人们常提到的各种数字，并且能很好地理解它们的含义。
- en: Definitions and Assumptions
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义和假设
- en: There are many other metrics besides accuracy that we can use to help us evaluate
    how well a model is performing. These allow us to reasonably compare models. Let’s
    start by defining the word *metric*. For us, a metric is a number or set of numbers
    that represents something about how well the model is doing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率，还有许多其他度量标准可以帮助我们评估模型的表现。这些度量标准可以让我们合理地比较模型。我们先来定义一下“度量”这个词。对我们而言，度量是一个数字或一组数字，表示模型表现的某个方面。
- en: The value of the metric increases or decreases as the performance of the model
    increases or decreases, or possibly vice versa. At times, we’ll be a bit sloppy
    and refer to graphs as metrics as well since we use them to judge the performance
    of a model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的性能提高或降低时，度量值也会随之增加或减少，反之亦然。有时我们会稍微不严谨，称图形为度量，因为我们用它们来判断模型的表现。
- en: 'We’re concerned with evaluating a model for which we have a single held-out
    test set. We’ll assume that we followed the advice of [Chapter 4](ch04.xhtml#ch04)
    and built three datasets: a training set to teach the model, a validation set
    to decide when the model was done training, and a held-out test set to evaluate
    the trained model. We’ve now trained our model, thereby utilizing the training
    and validation sets, and want to know how well we’ve done.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关心的是评估一个只有一个保留测试集的模型。我们将假设我们已经遵循了[第4章](ch04.xhtml#ch04)的建议，构建了三个数据集：一个用于训练模型的训练集，一个用于决定模型何时训练完成的验证集，以及一个用于评估训练好模型的保留测试集。我们现在已经训练了模型，利用了训练集和验证集，并希望知道我们做得怎么样。
- en: 'We have another, implicit assumption in this chapter. It’s a crucial one: we
    assume that the held-out test set is a good representation of the parent distribution
    that generated the data. Put another way, the held-out test set must represent
    the sort of data the model will encounter in the wild in as many ways as possible.
    For example, the frequency with which particular classes appear in the test set
    should match, as far as is practical, the expected rates that will be encountered
    when the model is used.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中还有另一个隐含的假设。它是一个关键假设：我们假设保留的测试集是生成数据的母体分布的一个良好代表。换句话说，保留的测试集必须尽可能多地代表模型在实际使用中将遇到的各种数据。例如，测试集中各个类别出现的频率应该尽可能与模型实际使用时将遇到的预期频率匹配。
- en: This is necessary because the training set is conditioning the model to expect
    a particular distribution, a particular set of characteristics, and if the data
    given to the model when it’s used has different characteristics, the model won’t
    perform well. A difference in distribution between the training set and the set
    of data presented to the model when it’s used is one of the most common reasons
    deployed machine learning models fail in actual use.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是必要的，因为训练集使模型预期一个特定的分布、一组特定的特征，如果模型使用时所接收到的数据具有不同的特征，模型的表现就会变差。训练集与模型使用时所呈现数据之间的分布差异，是已部署的机器学习模型在实际使用中失败的最常见原因之一。
- en: Why Accuracy Is Not Enough
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么准确率不足够
- en: 'A binary classifier outputs a single decision for a particular input: class
    0 or class 1\. Let’s define the following,'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类器为特定输入输出一个决策：类别0或类别1。我们定义以下内容，
- en: '*N*[*c*], the number of test examples the model correctly classified'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*[*c*]，模型正确分类的测试样本数量'
- en: '*N*[*w*], the number of test examples the model got wrong'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*[*w*]，模型预测错误的测试样本数量'
- en: Then, the overall accuracy of this model, a number between 0 and 1, is
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这个模型的整体准确率，一个介于0和1之间的数字，是
- en: '![image](Images/252equ01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/252equ01.jpg)'
- en: This is the accuracy as we have been using it throughout the book. Note, in
    this chapter, we will use *ACC* when we mean the overall accuracy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在本书中一直使用的准确率。请注意，在本章中，当我们说到整体准确率时，我们会使用*ACC*。
- en: This seems like a pretty reasonable metric, but there are a couple of good reasons
    not to trust this number too much. For example, *N*[*c*] and *N*[*w*] tell us
    nothing about the relative frequency of each class. What if one class is rare?
    Let’s see how that might affect things.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是一个相当合理的度量，但有几个很好的理由不应该过于依赖这个数字。例如，*N*[*c*]和*N*[*w*]并没有告诉我们各类别的相对频率。如果某一类别很稀有呢？我们来看一下这种情况可能会产生什么影响。
- en: 'If the model is 95 percent accurate (ACC = 0.95), we might be happy. However,
    let’s say the frequency (read *prior probability*) of class 1 is only 5 percent,
    meaning that on average, if we draw 100 samples from the test set, about 5 of
    them will be of class 1 and the other 95 will be of class 0\. We see that a model
    that predicts all inputs are of class 0 will be right 95 percent of the time.
    But consider this: our model might be returning only class 0 for all inputs. If
    we stick with the overall accuracy, we might think we have a good model when,
    in fact, we have a terrible model that we could implement in two lines of Python
    as'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的准确率是95%（ACC = 0.95），我们可能会感到满意。然而，假设类别1的频率（即*先验概率*）仅为5%，这意味着如果我们从测试集中抽取100个样本，平均来说，其中约5个将是类别1，其他95个将是类别0。我们看到，一个将所有输入预测为类别0的模型在95%的情况下是正确的。但考虑一下：我们的模型可能对所有输入都只返回类别0。如果我们仅关注整体准确率，我们可能会认为自己有一个不错的模型，而实际上我们拥有一个非常糟糕的模型，我们可以用两行Python代码实现如下
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, we say that the class is 0 regardless of the input feature vector,
    *x*. No one would be satisfied with such a model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们无论输入特征向量*x*是什么，都会认为类别是0。没有人会对这样的模型感到满意。
- en: The prior probabilities of the classes affect how we should think about the
    overall accuracy. However, if we know the following
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类别的先验概率会影响我们如何思考整体准确率。然而，如果我们知道以下信息
- en: '*N*[0], the number of class 0 instances in our test set'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*[0]，我们测试集中类别0实例的数量'
- en: '*N*[1], the number of class 1 instances in our test set'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*[1]，我们测试集中类别1实例的数量'
- en: '*C*[0], the number of class 0 instances our model found'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*C*[0]，我们模型找出的类别0实例的数量'
- en: '*C*[1], the number of class 1 instances our model found'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*C*[1]，我们模型找出的类别1实例的数量'
- en: 'we can easily compute the accuracy per class:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地计算每个类别的准确率：
- en: '![image](Images/253equ01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/253equ01.jpg)'
- en: The final expression is just another way to compute the overall accuracy because
    it tallies all of the correct classifications divided by the number of samples
    tested.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的表达式实际上是计算整体准确率的另一种方式，因为它统计了所有正确分类的数量，并除以测试样本的数量。
- en: The per class accuracy is better than the overall accuracy because it accounts
    for any imbalance in the frequency of the respective classes in the test set.
    For our previous hypothetical test set with the frequency of class 1 at 5 percent,
    if the classifier were predicting class 0 for all inputs, we would detect it because
    our per class accuracies would be ACC[0] = 1.0 and ACC[1] = 0.0\. This makes sense.
    We’d get every class 0 sample correct and every class 1 sample wrong (we’d call
    them class 0 anyway). Per class accuracies will show up again when we consider
    evaluating multiclass models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别的准确率比整体准确率更好，因为它考虑了测试集中各类别频率的不平衡。对于我们之前假设的测试集，其中类别1的频率为5%，如果分类器对所有输入都预测类别0，我们会发现这一点，因为每个类别的准确率将是ACC[0]
    = 1.0和ACC[1] = 0.0。这个很有道理。我们会将每个类别0的样本正确分类，而每个类别1的样本错误分类（我们还是会将它们归为类别0）。每个类别的准确率将在我们考虑评估多类别模型时再次出现。
- en: 'A more subtle reason to not just use the overall accuracy is that being wrong
    might bring a much higher cost than being right. This introduces something outside
    just the test set: it introduces the meaning we assign to class 0 and class 1\.
    For example, if our model is testing for breast cancer, perhaps using the dataset
    we created in [Chapter 5](ch05.xhtml#ch05), reporting class 1 (malignant) when,
    in fact, the sample does not represent a malignant case might cause anxiety for
    the woman waiting for her test results. With further testing, however, she’ll
    be shown to not have breast cancer after all. But consider the other case. A benign
    result that is actually malignant might mean that she will not receive treatment,
    or receive it too late, which could very well be fatal. The relative cost of one
    class versus another isn’t the same and might literally mean the difference between
    life and death. The same could be said of a self-driving car that thinks the child
    playing in the middle of the road is an empty soda can, or any number of other
    real-world examples.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更微妙的原因是，不仅仅使用整体准确率可能会产生误导，因为错误的结果可能带来比正确更高的成本。这引入了超出测试集范围的内容：它引入了我们赋予类别 0
    和类别 1 的意义。例如，如果我们的模型在测试乳腺癌，假设使用了我们在[第 5 章](ch05.xhtml#ch05)中创建的数据集，报告类别 1（恶性）时，实际上该样本并不代表恶性病例，可能会导致等待测试结果的女性产生焦虑。然而，经过进一步的检测后，她最终会被证明没有乳腺癌。但考虑另一种情况。一个良性的结果实际上是恶性的，可能意味着她不会接受治疗，或者治疗太晚，这很可能是致命的。不同类别之间的相对成本并不相同，可能会字面上决定生死。同样的情况也适用于自动驾驶汽车，如果它把在路中间玩耍的孩子误认为是一个空的汽水罐，或者其他许多现实生活中的例子。
- en: We use models in the real world, so their outputs are connected to the real
    world, and sometimes the cost associated with an output is significant. Using
    just the overall accuracy of a model can be misleading because it does not take
    the cost of an error into account.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在现实世界中使用模型，因此它们的输出与现实世界相关，有时与输出相关的成本是显著的。仅使用模型的整体准确率可能会产生误导，因为它没有考虑错误的成本。
- en: The 2 × 2 Confusion Matrix
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 × 2 混淆矩阵
- en: 'The models we’ve worked with so far have all ultimately assigned each input
    a class label. For example, a neural network with a logistic output is interpreted
    as a probability of membership of class 1\. Using a typical threshold of 0.5 lets
    us assign a class label: if the output is < 0.5, call the input class 0; otherwise,
    call it class 1\. For other model types, the decision rule is different (for example,
    voting in *k*-NN), but the effect is the same: we get a class assignment for the
    input.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止使用的所有模型最终都为每个输入分配了一个类别标签。例如，具有逻辑输出的神经网络被解释为属于类别 1 的概率。使用典型的阈值 0.5，可以为其分配类别标签：如果输出小于
    0.5，则将输入分配为类别 0；否则，分配为类别 1。对于其他类型的模型，决策规则可能不同（例如，*k*-最近邻中的投票），但效果是一样的：我们会为输入分配一个类别。
- en: If we run our entire test set through our model and apply the decision rule,
    we get the assigned class label along with the true class label for each sample.
    Again, thinking only of the binary classifier case, we have four possible outcomes
    for each input sample in regards to the assigned class and the true class (see
    [Table 11-1](ch11.xhtml#ch11tab1)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将整个测试集输入到模型中，并应用决策规则，我们就能获得每个样本的分配类别标签以及真实类别标签。同样，考虑到二分类器的情况，我们对于每个输入样本在分配类别和真实类别之间有四种可能的结果（见[表
    11-1](ch11.xhtml#ch11tab1)）。
- en: '**Table 11-1:** Possible Relationships Between the True Class Label and the
    Assigned Class Label for a Binary Classifier'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 11-1：** 二分类器中真实类别标签与分配类别标签之间的可能关系'
- en: '| **Assigned class** | **True class** | **Case** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **分配的类别** | **真实类别** | **情况** |'
- en: '| --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 0 | True negative (TN) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 真阴性（TN） |'
- en: '| 0 | 1 | False negative (FN) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 假阴性（FN） |'
- en: '| 1 | 0 | False positive (FP) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 假阳性（FP） |'
- en: '| 1 | 1 | True positive (TP) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 真阳性（TP） |'
- en: The *Case* label defines how we’ll talk about these situations. If the actual
    class of the input is class 0 and the model assigns class 0, we have a correctly
    identified negative case, so we have a *true negative*, or *TN*. If the actual
    class is class 1 and the model assigns class 1, we have a correctly identified
    positive case, so we have a *true positive*, or *TP*. However, if the actual class
    is class 1 and the model assigns class 0, we have a positive case wrongly called
    a negative case, so we have a *false negative*, or *FN*. Finally, if the actual
    class is 0 and the model assigns class 1, we have a negative case wrongly called
    a positive case, so we have a *false positive*, or *FP*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*案例* 标签定义了我们如何描述这些情况。如果输入的实际类是类 0 并且模型分配了类 0，那么我们就有一个正确识别的负类案例，这就是 *真正负*（*TN*）。如果实际类是类
    1 并且模型分配了类 1，那么我们就有一个正确识别的正类案例，这就是 *真正正*（*TP*）。然而，如果实际类是类 1 并且模型分配了类 0，我们就有一个正类案例被错误地称为负类，这就是
    *假负*（*FN*）。最后，如果实际类是类 0 并且模型分配了类 1，我们就有一个负类案例被错误地称为正类，这就是 *假正*（*FP*）。'
- en: We can place each of the inputs in our test set into one, and only one, of these
    cases. Doing this lets us tally the number of times each case appears in the test
    set, which we can present nicely as a table (see [Table 11-2](ch11.xhtml#ch11tab2)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将测试集中的每个输入归类到这些情况中的一个，且仅一个。这样做可以让我们统计每个情况在测试集中出现的次数，并且我们可以将其漂亮地展示为一个表格（见
    [表 11-2](ch11.xhtml#ch11tab2)）。
- en: '**Table 11-2:** Definition of the Class Labels in the 2 × 2 Table'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 11-2：** 2 × 2 表格中类标签的定义'
- en: '|  | **Actual class 1** | **Actual class 0** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际类 1** | **实际类 0** |'
- en: '| --- | --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Model assigns class 1* | TP | FP |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| *模型分配类 1* | TP | FP |'
- en: '| *Model assigns class 0* | FN | TN |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| *模型分配类 0* | FN | TN |'
- en: I have placed the case labels (TP, FP, and so forth) in the location where the
    actual tally counts would go for each case.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将案例标签（TP、FP 等）放置在每个案例的实际计数应该出现的位置。
- en: This table is called a 2 × 2 *confusion matrix* (or 2 × 2 *contingency table*).
    It is 2 × 2 because there are two rows and two columns. It is a confusion matrix
    because it shows us at a glance how the classifier is performing and, especially,
    where it is confused. The classifier is confused when it assigns an instance of
    one class to the other class. In the 2 × 2 table, this confusion shows up as counts
    that are not along the main diagonal of the table (upper left to lower right).
    These are the FP and FN entries. A model that performs flawlessly on the test
    set will have FP = 0 and FN = 0; it will make no mistakes in assigning class labels.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格被称为 2 × 2 *混淆矩阵*（或 2 × 2 *列联表*）。它是 2 × 2，因为它有两行两列。之所以称之为混淆矩阵，是因为它可以让我们一目了然地看到分类器的表现，特别是它在哪些地方发生了混淆。当分类器将某个类的实例错误地分配给另一个类时，便发生了混淆。在
    2 × 2 的表格中，这种混淆表现为不在表格主对角线（从左上到右下）上的计数。这些是 FP 和 FN 项。一个在测试集上表现完美的模型会有 FP = 0 和
    FN = 0；它不会在分配类标签时犯错误。
- en: In [Chapter 7](ch07.xhtml#ch07), we experimented with the breast cancer dataset
    built in [Chapter 5](ch05.xhtml#ch05). We reported the performance of classic
    models against this dataset by looking at their overall accuracy. This is what
    the sklearn `score` method returns. Let’s now instead look at some 2 × 2 tables
    generated from the test set for these models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 7 章](ch07.xhtml#ch07)中，我们使用在[第 5 章](ch05.xhtml#ch05)中构建的乳腺癌数据集进行了实验。我们通过查看这些经典模型的整体准确性报告了它们在该数据集上的表现。这就是
    sklearn `score` 方法返回的内容。现在我们来看看这些模型在测试集上生成的一些 2 × 2 表格。
- en: 'The code we are looking at is in the file *bc_experiments.py*. This code trains
    multiple classic model types. Instead of using the overall accuracy, however,
    let’s introduce a new function that computes the entries in the 2 × 2 table ([Listing
    11-1](ch11.xhtml#ch11lis1)):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在查看的代码位于文件 *bc_experiments.py* 中。此代码训练了多种经典模型类型。但是，除了使用整体准确率，我们将引入一个新函数来计算
    2 × 2 表格中的条目（[列表 11-1](ch11.xhtml#ch11lis1)）：
- en: 'def tally_predictions(clf, x, y):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'def tally_predictions(clf, x, y):'
- en: p = clf.predict(x)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: p = clf.predict(x)
- en: score = clf.score(x,y)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: score = clf.score(x, y)
- en: tp = tn = fp = fn = 0
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: tp = tn = fp = fn = 0
- en: 'for i in range(len(y)):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(y)):'
- en: 'if (p[i] == 0) and (y[i] == 0):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 (p[i] == 0) 且 (y[i] == 0):'
- en: tn += 1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: tn += 1
- en: 'elif (p[i] == 0) and (y[i] == 1):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif (p[i] == 0) 且 (y[i] == 1):'
- en: fn += 1
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: fn += 1
- en: (*\pagebreak*)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: 'elif (p[i] == 1) and (y[i] == 0):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif (p[i] == 1) 且 (y[i] == 0):'
- en: fp += 1
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: fp += 1
- en: 'else:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: tp += 1
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: tp += 1
- en: return [tp, tn, fp, fn, score]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: return [tp, tn, fp, fn, score]
- en: '*Listing 11-1: Generating tally counts*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-1：生成计数*'
- en: This function accepts a trained sklearn model object (`clf`), the test samples
    (`x`), and the corresponding actual test labels (`y`). The first thing this function
    does is use the sklearn model to predict a class label for each of the test samples;
    the result is stored in `p`. It then calculates the overall score, and loops over
    each of the test samples and compares the predicted class label (`p`) to the actual
    known class label (`y`) to see if that sample is a true positive, true negative,
    false positive, or false negative. When done, all of these values are returned.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受一个训练好的sklearn模型对象（`clf`），测试样本（`x`）和相应的实际测试标签（`y`）。该函数的第一步是使用sklearn模型对每个测试样本预测一个类别标签；结果存储在`p`中。接下来，它计算整体得分，并遍历每个测试样本，将预测的类别标签（`p`）与实际的已知类别标签（`y`）进行比较，以确定该样本是正类、负类、假阳性还是假阴性。完成后，返回所有这些值。
- en: Applying `tally_predictions` to the output of *bc_experiments.py* gives us [Table
    11-3](ch11.xhtml#ch11tab3). Here, the sklearn model type is given.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将`tally_predictions`应用于*bc_experiments.py*的输出结果给出了[表11-3](ch11.xhtml#ch11tab3)。在这里，给出了sklearn模型类型。
- en: '**Table 11-3:** 2 × 2 Tables for the Breast Cancer Test Set'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**表11-3：** 乳腺癌测试集的2 × 2表格'
- en: '![image](Images/table11-3.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/table11-3.jpg)'
- en: 'In [Table 11-3](ch11.xhtml#ch11tab3), we see four 2 × 2 tables corresponding
    to the test set applied to the respective models: Nearest Centroid, 3-NN, Decision
    Tree, and linear SVM. From the tables alone, we see that the best-performing model
    was the 3-NN as it had only one false positive and no false negatives. This means
    that the model never called a true malignant case benign and only once called
    a benign case malignant. Given our discussion in the previous section, we see
    that this is an encouraging result.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表11-3](ch11.xhtml#ch11tab3)中，我们看到四个2 × 2表格，分别对应于应用于各自模型的测试集：最近质心、3-NN、决策树和线性SVM。从表格来看，我们看到表现最好的模型是3-NN，因为它只有一个假阳性，没有假阴性。这意味着该模型从未将真正的恶性病例误判为良性，只是有一次将良性病例误判为恶性。根据我们在上一节的讨论，看到这是一个令人鼓舞的结果。
- en: Look now at the results for the Nearest Centroid and the Decision Tree. The
    overall accuracies for these models are 94.7 percent and 93.9 percent, respectively.
    From the accuracy alone, we might be tempted to say that the Nearest Centroid
    model is better. However, if we look at the 2 × 2 tables, we see that even though
    the Decision Tree had more false positives (6), it had only one false negative,
    while the Nearest Centroid had two false negatives. Again, in this case, a false
    negative means a missed cancer detection with potentially serious consequences.
    So, for this dataset, we want to minimize false negatives even if that means we
    need to tolerate a small increase in false positives. Therefore, we’ll select
    the Decision Tree over the Nearest Centroid model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看一下最近质心模型和决策树模型的结果。这两个模型的整体准确率分别为94.7%和93.9%。仅从准确率来看，我们可能会倾向于认为最近质心模型更好。然而，如果我们查看2
    × 2表格，我们会发现尽管决策树的假阳性（6个）更多，但它只有一个假阴性，而最近质心模型有两个假阴性。再说一次，在这种情况下，假阴性意味着漏掉了癌症的检测，可能会导致严重后果。因此，对于这个数据集，我们希望尽量减少假阴性，即使这意味着我们需要容忍假阳性稍微增加一点。因此，我们将选择决策树模型而非最近质心模型。
- en: Metrics Derived from the 2 × 2 Confusion Matrix
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从2 × 2混淆矩阵中派生的度量指标
- en: Looking at the raw 2 × 2 table is helpful, but even more helpful are the metrics
    derived from it. Let’s look at several of these in this section to see how they
    can help us interpret the information in the 2 × 2 table. Before we start, however,
    we should keep in mind that the metrics we’ll discuss are sometimes a bit controversial.
    There is still healthy academic debate as to which are best to use when. Our intention
    here is to introduce them via examples, and to describe what it is that they are
    measuring. As a machine learning practitioner, you’ll encounter virtually all
    of these from time to time, so it’s wise to at least be familiar with them.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 查看原始的2 × 2表格是有帮助的，但从中派生出的度量指标更为有用。让我们在本节中看几个这样的指标，看看它们如何帮助我们解释2 × 2表格中的信息。然而，在我们开始之前，我们应该牢记一点，接下来讨论的度量指标有时会引发争议。关于在何时使用哪些指标，学术界仍然存在积极的讨论。我们在这里的目的是通过示例介绍它们，并描述它们衡量的内容。作为一名机器学习实践者，你会时不时地遇到这些指标，因此至少应该对它们有所了解。
- en: Deriving Metrics from the 2 × 2 Table
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从2 × 2表格中派生度量指标
- en: 'The first metrics are derived directly from the values in the 2 × 2 table:
    TP, TN, FP, FN. Think of these as the bread-and-butter metrics. They’re easy to
    compute and easy to understand. Recall the general form of the 2 × 2 table from
    [Table 11-2](ch11.xhtml#ch11tab2). We’ll now define two other quantities:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个指标直接来自 2 × 2 表中的值：TP、TN、FP、FN。可以将这些看作是最基础的指标。它们计算起来简单，易于理解。回想一下 [表 11-2](ch11.xhtml#ch11tab2)
    中 2 × 2 表的一般形式。现在我们将定义另外两个量：
- en: '![image](Images/257equ01.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/257equ01.jpg)'
- en: 'The *true positive rate (TPR)* is the probability that an actual instance of
    class 1 will be correctly identified by the model. The TPR is frequently known
    by other names: *sensitivity*, *recall,* and *hit rate*. You will likely see it
    referred to as *sensitivity* in medical literature.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*真阳性率（TPR）* 是模型正确识别实际类别 1 实例的概率。TPR 经常有其他名称：*灵敏度*、*召回率* 和 *命中率*。在医学文献中，通常会称其为
    *灵敏度*。'
- en: The *true negative rate (TNR)* is the probability that an actual instance of
    class 0 will be correctly identified by the model. The TNR is also known as the
    *specificity,* again, particularly so in medical literature. Both of these quantities,
    as probabilities, have a value between 0 and 1; higher is better. A perfect classifier
    will have TPR = TNR = 1.0; this happens when it makes no mistakes so that FP =
    FN = 0, always.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*真负率（TNR）* 是模型正确识别实际类别 0 实例的概率。TNR 也被称为 *特异性*，尤其在医学文献中如此。这两个量作为概率，其值介于 0 和
    1 之间；值越高越好。完美的分类器将有 TPR = TNR = 1.0；这发生在它没有犯错时，即 FP = FN = 0，总是如此。'
- en: The TPR and TNR need to be understood together to assess a model. For example,
    we previously mentioned that if class 1 is rare and the model always predicts
    class 0, it will have high accuracy. If we look at TPR and TNR in that case, we’ll
    see that the TNR is 1 because the model never assigns an instance of class 0 to
    class 1 (FP = 0). However, the TPR is 0 for the very same reason, all actual instances
    of class 1 will be misidentified as false negatives; they get assigned to class
    0\. Therefore, the two metrics together immediately indicate that the model is
    not a good one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: TPR 和 TNR 需要一起理解，以评估一个模型。例如，我们之前提到过，如果类别 1 很少，而模型总是预测类别 0，那么它的准确率会很高。如果我们在这种情况下查看
    TPR 和 TNR，就会发现 TNR 为 1，因为模型从未将类别 0 的实例分配到类别 1（FP = 0）。然而，TPR 为 0，原因是一样的，所有实际的类别
    1 实例都会被误识别为假阴性；它们会被分配到类别 0。因此，这两个指标一起立即表明该模型并不好。
- en: What about the breast cancer case where a false negative might be fatal? How
    do we want the TPR and TNR to look in this case? Ideally, of course, we want them
    to both be as high as possible, but we might be willing to use the model anyway
    if the TPR is very high while the TNR might be lower. In that situation, we know
    that actual breast cancers, when presented, are detected almost always. Why? Because
    the false negative count (FN) is virtually 0, so the denominator of the TPR is
    about TP, which implies a TPR of about 1.0\. If, on the other hand, we tolerate
    false positives (actual negative instances called malignant by the model), we
    see that the TNR might be well below 1.0 because the denominator of the TNR includes
    the FP counts.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于乳腺癌的情况，如果假阴性可能是致命的，我们希望 TPR 和 TNR 在这种情况下应该如何表现呢？理想情况下，当然是希望它们都尽可能高，但如果
    TPR 很高而 TNR 较低，我们也可能愿意使用这个模型。在这种情况下，我们知道实际的乳腺癌，出现时几乎总能被检测到。为什么？因为假阴性数量（FN）几乎为
    0，因此 TPR 的分母大约是 TP，这意味着 TPR 接近 1.0。然而，如果我们容忍假阳性（模型将实际负实例称为恶性），我们会看到 TNR 可能远低于
    1.0，因为 TNR 的分母包括了 FP 数量。
- en: 'The TPR and TNR tell us something about the likelihood that the model will
    pick up actual class 1 and class 0 instances. What it does not tell us, however,
    is how much faith we should put into the output of the model. For example, if
    the model says “class 1,” should we believe it? To make that assessment, we need
    two other metrics derived directly from the 2 × 2 table:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TPR 和 TNR 告诉我们模型识别实际类别 1 和类别 0 实例的可能性。然而，它们并没有告诉我们，我们应该对模型输出的结果有多大的信心。例如，如果模型说“类别
    1”，我们应该相信它吗？要做出这样的评估，我们需要从 2 × 2 表中直接得出的另外两个指标：
- en: '![image](Images/258equ01.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/258equ01.jpg)'
- en: The *positive predictive value (PPV)* is most often known as the *precision*.
    It’s the probability that when the model says the instance is of class 1, it is
    of class 1\. Similarly, the *negative predictive value (NPV)* is the probability
    that the model is correct when it claims an instance is of class 0\. Both of these
    values are also numbers between 0 and 1, where higher is better.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*阳性预测值（PPV）*通常称为*精度*。它是模型预测实例为类别1时，实际为类别1的概率。同样，*阴性预测值（NPV）*是当模型声明实例为类别0时，模型正确的概率。这两个值也是介于0和1之间的数字，越高越好。'
- en: The only difference between the TPR and the PPV is whether we consider false
    negatives or false positives in the denominator. By including the false positives,
    the instances the model says are of class 1 when they are really of class 0; we
    get the probability that the model output is correct.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TPR和PPV之间唯一的区别是我们是否在分母中考虑了假阴性或假阳性。通过包括假阳性，即模型预测为类别1但实际为类别0的实例；我们得到的是模型输出正确的概率。
- en: For the case of a model that always predicts class 0, the PPV is undefined because
    both the TP and FP are zero. All of the class 1 instances are pushed into the
    FN count, and the TN count includes all the actual class 0 instances. For the
    case where TPR is high, but TNR is not, we have a nonzero FP count so that the
    PPV goes down. Let’s make up an example to see why this is so and how we might
    understand it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个总是预测类别0的模型，PPV是未定义的，因为TP和FP都是零。所有类别1的实例都被推入FN计数中，而TN计数包括所有实际的类别0实例。对于TPR很高，但TNR不高的情况，我们有一个非零的FP计数，因此PPV会下降。让我们举一个例子来看看为什么会这样，以及我们如何理解它。
- en: Let’s say that our breast cancer model has produced the following 2 × 2 table
    ([Table 11-4](ch11.xhtml#ch11tab4)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的乳腺癌模型生成了以下2 × 2表格（[表11-4](ch11.xhtml#ch11tab4)）。
- en: '**Table 11-4:** A Hypothetical 2 × 2 Table for a Breast Cancer Dataset'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**表11-4：** 一个假设的2 × 2表格用于乳腺癌数据集'
- en: '|  | **Actual 1** | **Actual 0** |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际 1** | **实际 0** |'
- en: '| --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Model assigns 1* | 312 | 133 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| *模型预测为1* | 312 | 133 |'
- en: '| *Model assigns 0* | 6 | 645 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| *模型预测为0* | 6 | 645 |'
- en: In this example, the metrics we have covered so far are
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们到目前为止覆盖的指标是
- en: '*TPR* = 0.9811'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*TPR* = 0.9811'
- en: '*TNR* = 0.8398'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*TNR* = 0.8398'
- en: '*PPV* = 0.7011'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*PPV* = 0.7011'
- en: '*NPV* = 0.9908'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*NPV* = 0.9908'
- en: This means a truly malignant case will be called malignant by the model 98 percent
    of the time, but a benign case will be called benign only 84 percent of the time.
    The PPV of 70 percent implies that when the model says “malignant,” there is only
    a 70 percent chance that the case is malignant; however, because of the high TPR,
    we know that buried in the “malignant” outputs are virtually all of the actual
    breast cancer cases. Notice also that this implies a high NPV, so when the model
    says “benign,” we have very high confidence that the instance is not breast cancer.
    This is what makes the model useful even if the PPV is less than 100 percent.
    In a clinical setting, this model will warrant further testing when it says “malignant”
    but in general, no further testing will likely be needed if it says “benign.”
    Of course, what acceptable levels of these metrics are depends upon the use case
    for the model. Some might call an NPV of only 99.1 percent too low given the potentially
    very high cost of missing a cancer detection. Thoughts like these likely also
    motivate the recommended frequency of screening.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，模型将98%的时间正确判断恶性病例为恶性，但只有84%的时间正确判断良性病例为良性。70%的PPV意味着当模型说“恶性”时，只有70%的概率该病例确实是恶性的；然而，由于较高的TPR，我们知道在“恶性”输出中几乎包含了所有实际的乳腺癌病例。还要注意，这也意味着高NPV，所以当模型说“良性”时，我们非常有信心该实例不是乳腺癌。这就是为什么即使PPV不到100%，该模型仍然有用。在临床环境中，当模型说“恶性”时，可能需要进一步测试，但通常如果它说“良性”，则不太可能需要进一步测试。当然，哪些指标的可接受水平取决于模型的使用场景。有些人可能会认为，考虑到错过癌症检测的潜在高成本，仅99.1%的NPV太低。像这样的想法可能也会推动推荐的筛查频率。
- en: 'There are two additional basic metrics we can easily derive from the 2 × 2
    table:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从2 × 2表格中轻松推导出两个额外的基本指标：
- en: '![image](Images/259equ01.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/259equ01.jpg)'
- en: These metrics tell us the likelihood that a sample will be a false positive
    if the actual class is class 0 or a false negative if the actual class is class
    1, respectively. The FPR will show up again later when we talk about using curves
    to assess models. Notice that FPR = 1 – TNR and FNR = 1 – TPR.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标告诉我们，如果实际类别是类别 0，则样本为假阳性的可能性，或者如果实际类别是类别 1，则样本为假阴性的可能性。FPR 会在后续讨论使用曲线评估模型时再次出现。请注意，FPR
    = 1 – TNR，FNR = 1 – TPR。
- en: 'Calculating these basic metrics is straightforward, especially if we use the
    output of the `tally_predictions` function defined previously as the input ([Listing
    11-2](ch11.xhtml#ch11lis2)):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这些基本指标是直接的，尤其是如果我们使用之前定义的 `tally_predictions` 函数的输出作为输入（[清单 11-2](ch11.xhtml#ch11lis2)）：
- en: 'def basic_metrics(tally):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'def basic_metrics(tally):'
- en: tp, tn, fp, fn, _ = tally
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: tp, tn, fp, fn, _ = tally
- en: return {
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 {
- en: '"TPR": tp / (tp + fn),'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '"TPR"：tp / (tp + fn)，'
- en: '"TNR": tn / (tn + fp),'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '"TNR"：tn / (tn + fp)，'
- en: '"PPV": tp / (tp + fp),'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '"PPV"：tp / (tp + fp)，'
- en: '"NPV": tn / (tn + fn),'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '"NPV"：tn / (tn + fn)，'
- en: (*\pagebreak*)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: '"FPR": fp / (fp + tn),'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '"FPR"：fp / (fp + tn)，'
- en: '"FNR": fn / (fn + tp)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '"FNR"：fn / (fn + tp)'
- en: '}'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '*Listing 11-2: Calculating basic metrics*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-2：计算基本指标*'
- en: We break up the list returned by `tally_predictions`, disregarding the accuracy,
    and then build and return a dictionary containing each of the six basic metrics
    we described. Of course, robust code would check for pathological cases where
    the denominators are zero, but we’ve ignored that code here to preserve clarity
    in the presentation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `tally_predictions` 返回的列表拆分，忽略准确率，然后构建并返回一个包含我们描述的六个基本指标的字典。当然，健壮的代码应该检查分母为零的病态情况，但为了保持清晰度，我们在此省略了相关代码。
- en: Using Our Metrics to Interpret Models
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用我们的指标来解释模型
- en: Let’s use `tally_predictions` and `basic_metrics` to interpret some models.
    We’ll work with the vector form of the MNIST data but keep only digits 3 and 5
    so that we have a binary classifier. The code is similar to that found in *mnist_experiments.py*,
    which we used in [Chapter 7](ch07.xhtml#ch07).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `tally_predictions` 和 `basic_metrics` 来解释一些模型。我们将使用 MNIST 数据的向量形式，但仅保留数字
    3 和 5，从而得到一个二分类器。代码类似于我们在[第 7 章](ch07.xhtml#ch07)中使用的 *mnist_experiments.py*。
- en: Keeping only digits 3 and 5 leaves us with 11,552 training samples (6,131 3s;
    5,421 5s) and 1,902 test samples of which 1,010 are 3s and 892 are 5s. The actual
    code is in *mnist_2x2_tables.py* with selected output in [Table 11-5](ch11.xhtml#ch11tab5).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 仅保留数字 3 和 5 后，我们得到了 11,552 个训练样本（6,131 个 3；5,421 个 5），以及 1,902 个测试样本，其中 1,010
    个是 3，892 个是 5。实际代码在 *mnist_2x2_tables.py* 中，选定输出见[表 11-5](ch11.xhtml#ch11tab5)。
- en: '**Table 11-5:** Selected Output from MNIST 3 vs. 5 Models and Corresponding
    Basic Metrics'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 11-5：** 从 MNIST 3 vs. 5 模型及其相应的基本指标中选取的输出'
- en: '| **Model** | **TP** | **TN** | **FP** | **FN** |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **TP** | **TN** | **FP** | **FN** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| *Nearest Centroid* | 760 | 909 | 101 | 132 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| *Nearest Centroid* | 760 | 909 | 101 | 132 |'
- en: '| *3-NN* | 878 | 994 | 16 | 14 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| *3-NN* | 878 | 994 | 16 | 14 |'
- en: '| *Naïve Bayes* | 612 | 976 | 34 | 280 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| *Naïve Bayes* | 612 | 976 | 34 | 280 |'
- en: '| *RF 500* | 884 | 1,003 | 7 | 8 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| *RF 500* | 884 | 1,003 | 7 | 8 |'
- en: '| *LinearSVM* | 853 | 986 | 24 | 39 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| *LinearSVM* | 853 | 986 | 24 | 39 |'
- en: '| **Model** | **TPR** | **TNR** | **PPV** | **NPV** | **FPR** | **FNR** |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **TPR** | **TNR** | **PPV** | **NPV** | **FPR** | **FNR** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| *Nearest Centroid* | 0.8520 | 0.9000 | 0.8827 | 0.8732 | 0.1000 | 0.1480
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| *Nearest Centroid* | 0.8520 | 0.9000 | 0.8827 | 0.8732 | 0.1000 | 0.1480
    |'
- en: '| *3-NN* | 0.9843 | 0.9842 | 0.9821 | 0.9861 | 0.0158 | 0.0157 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| *3-NN* | 0.9843 | 0.9842 | 0.9821 | 0.9861 | 0.0158 | 0.0157 |'
- en: '| *Naïve Bayes* | 0.6851 | 0.9663 | 0.9474 | 0.7771 | 0.0337 | 0.3139 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| *Naïve Bayes* | 0.6851 | 0.9663 | 0.9474 | 0.7771 | 0.0337 | 0.3139 |'
- en: '| *RF 500* | 0.9910 | 0.9931 | 0.9921 | 0.9921 | 0.0069 | 0.0090 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| *RF 500* | 0.9910 | 0.9931 | 0.9921 | 0.9921 | 0.0069 | 0.0090 |'
- en: '| *LinearSVM* | 0.9563 | 0.9762 | 0.9726 | 0.9620 | 0.0238 | 0.0437 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| *LinearSVM* | 0.9563 | 0.9762 | 0.9726 | 0.9620 | 0.0238 | 0.0437 |'
- en: In [Table 11-5](ch11.xhtml#ch11tab5), we see the raw counts at the top and the
    metrics defined in this section at the bottom. Lots of numbers! Let’s parse things
    a bit to see what’s going on. We’ll concentrate on the metrics at the bottom of
    the table. The first two columns show the true positive rate (sensitivity, recall)
    and the true negative rate (specificity). These values should be examined together.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 11-5](ch11.xhtml#ch11tab5)中，我们看到顶部是原始计数，底部是本节定义的指标。数据量很大！让我们稍作解析，看看发生了什么。我们将集中在表格底部的指标上。前两列显示了真正例率（敏感度，召回率）和真负例率（特异度）。这些值应该一起考察。
- en: 'If we look at the Nearest Centroid results, we see TPR = 0.8520 and TNR = 0.9000\.
    Here class 1 is a five, and class 0 is a three. So, the Nearest Centroid classifier
    will call 85 percent of the fives it sees “five.” Similarly, it will call 90 percent
    of the threes it sees “three.” While not too shabby, we should not be impressed.
    Looking down the columns, we see that two models performed very well for these
    metrics: 3-NN and the Random Forest with 500 trees. In both cases, the TPR and
    TNR were nearly identical and quite close to 1.0\. This is a sign of the model
    performing well. Absolute perfection would be TPR = TNR = PPV = NPV = 1.0 and
    FPR = FNR = 0.0\. The closer we get to perfection, the better. If attempting to
    pick the best model for this classifier, we would likely choose the Random Forest
    because it was the closest to perfection on the test set.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看最近质心分类器的结果，可以看到 TPR = 0.8520 和 TNR = 0.9000。在这里，类 1 是五，类 0 是三。所以，最近质心分类器会把它看到的
    85% 的五预测为“五”。类似地，它会把它看到的 90% 的三预测为“三”。虽然表现不错，但我们不必为此感到印象深刻。查看下方的列，我们可以看到有两个模型在这些指标上表现非常好：3-NN
    和 500 棵树的随机森林。在这两种情况下，TPR 和 TNR 非常相近，并且都接近 1.0。这是模型表现良好的标志。绝对完美的情况是 TPR = TNR
    = PPV = NPV = 1.0，FPR = FNR = 0.0。我们越接近完美，表现就越好。如果要为这个分类器选择最好的模型，我们可能会选择随机森林，因为它在测试集上最接近完美。
- en: 'Let’s look briefly at the Naïve Bayes results. The TNR (specificity) is reasonably
    high, about 97 percent. However, the TPR (sensitivity) of 68.5 percent is pathetic.
    Roughly speaking, only two out of every three 5’s presented to this model will
    be correctly classified. If we examine the next two columns, the positive and
    negative predictive values, we see a PPV of 94.7 percent, meaning when the model
    does happen to say the input is a five, we can be somewhat confident that it is
    a five. However, the negative predictive value isn’t so good at 77.7 percent.
    Looking at the top portion of [Table 11-5](ch11.xhtml#ch11tab5) shows us what
    is happening in this case. The FP count is only 34 out of 1010 threes in the test
    set, but the FN count is high: 280 of the fives were labeled “three.” This is
    the source of the low NPV for this model.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要看看朴素贝叶斯的结果。TNR（特异性）相当高，大约 97%。然而，TPR（灵敏度）只有 68.5%，表现差劲。大致来说，呈现给这个模型的每三个五中，只有两个能被正确分类。如果我们查看接下来的两列，即阳性预测值和阴性预测值，可以看到
    PPV 为 94.7%，意味着当模型预测输入为五时，我们可以有一定的信心它是五。然而，负预测值表现不佳，为 77.7%。查看 [表 11-5](ch11.xhtml#ch11tab5)
    的上方可以看到此时发生的情况。FP 数量只有 34 个假阳性，来自测试集中的 1010 个三，但 FN 数量很高：280 个五被标记为“三”。这就是该模型低
    NPV 的原因。
- en: 'Here is a good rule of thumb for these metrics: a well-performing model has
    TPR, TNR, PPV, and NPV very close to 1.0, and FPR and FNR very close to 0.0.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这些指标的一个好经验法则：一个表现良好的模型，TPR、TNR、PPV 和 NPV 应该接近 1.0，而 FPR 和 FNR 应该接近 0.0。
- en: 'Look again at [Table 11-5](ch11.xhtml#ch11tab5), particularly the lower metrics
    for the Random Forest. As their names suggest, the FPR and FNR values are rates.
    We can use them to estimate how often FP and FN will occur when using the model.
    For example, if we present the model with *N* = 1,000 cases that are threes (class
    0), we can use the FPR to estimate how many of them the model will call fives
    (class 1):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再看一下 [表 11-5](ch11.xhtml#ch11tab5)，特别是随机森林的低指标。正如它们的名称所示，FPR 和 FNR 的值是比率。我们可以利用它们来估算使用该模型时，FP
    和 FN 出现的频率。例如，如果我们向模型呈现 *N* = 1000 个实际是三（类 0）的案例，我们可以使用 FPR 来估算模型会将其中多少个判定为五（类
    1）：
- en: estimated number of FP = FPR × N = 0.0069(1000) ≈ 7
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的 FP 数量 = FPR × N = 0.0069(1000) ≈ 7
- en: 'A similar calculation gives us the estimated number of FN for *N* = 1000: instances
    that are really 5’s:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的计算给出了 *N* = 1000 时的 FN 估计数量：真正是 5 的实例：
- en: estimated number of FN = FNR × N = 0.0090(1000) = 9
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的 FN 数量 = FNR × N = 0.0090(1000) = 9
- en: 'The same holds for the TPR and TNR, which also have “rate” in their names (*N*
    = 1000 each for actual threes and fives):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: TPR 和 TNR 也是如此，它们的名称中也有“rate”字样（*N* = 1000 每个实际的三和五）：
- en: estimated number of TP = TPR × N = 0.9910(1000) = 991
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的 TP 数量 = TPR × N = 0.9910(1000) = 991
- en: estimated number of FN = FNR × N = 0.9931(1000) = 993
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 估计的 FN 数量 = FNR × N = 0.9931(1000) = 993
- en: These calculations show how well this model performs on the test data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算展示了该模型在测试数据上的表现。
- en: More Advanced Metrics
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更高级的指标
- en: 'Let’s look in this section at what I’m arbitrarily calling *more advanced metrics*.
    I say they are more advanced because instead of using the 2 × 2 table entries
    directly, they are built from values calculated from the table itself. In particular,
    we’ll examine five advanced metrics: informedness, markedness, F1 score, Cohen’s
    kappa, and the Matthews correlation coefficient (MCC).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，让我们看看我随意称之为*更高级的指标*的内容。我之所以称它们为更高级的，是因为它们不是直接使用2 × 2表格中的数据，而是基于从表格本身计算出的值构建的。特别地，我们将讨论五个高级指标：知情度、标记度、F1
    分数、科恩的卡帕（Cohen's kappa）和马修斯相关系数（MCC）。
- en: Informedness and Markedness
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知情度和标记度
- en: '*Informedness* and *markedness* go together. They are somewhat less well known
    than other metrics in this section, but they will hopefully be better known in
    the future. I said earlier that TPR (sensitivity) and TNR (specificity) should
    be interpreted together. The informedness (also called Youden’s *J* statistic)
    does just that:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*知情度*和*标记度*是密切相关的。它们可能不像本节中的其他指标那么知名，但希望未来能更加广泛地被了解。我之前说过，TPR（灵敏度）和TNR（特异度）应该一起解读。知情度（也称为尤登的*J*统计量）正是这么做的：'
- en: Informedness = TPR + TNR − 1
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 知情度 = TPR + TNR − 1
- en: Informedness is a number in [*–*1,+1] that combines both the TPR and TNR. The
    higher the informedness is, the better. An informedness of 0 implies random guessing,
    while an informedness of 1 implies perfection (on the test set). An informedness
    of less than 0 might suggest a model that is worse than random guessing. An informedness
    of –1 implies that all true positive instances were called negatives, and vice
    versa. In that case, we could swap the label the model wants to assign to each
    input and get a quite good model. Only pathological models lead to negative informedness
    values.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 知情度是一个位于[*–*1,+1]范围内的数字，它将TPR和TNR结合在一起。知情度越高越好。知情度为0意味着随机猜测，而知情度为1意味着完美（在测试集上）。知情度小于0可能意味着模型比随机猜测还差。知情度为–1表示所有真正的正实例都被标记为负，反之亦然。在这种情况下，我们可以交换模型希望分配给每个输入的标签，并得到一个相当好的模型。只有病态模型才会导致负的知情度值。
- en: 'The markedness combines the positive and negative predictive values in the
    same way that informedness combines TPR and TNR:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 标记度将正向和负向预测值结合起来，就像知情度将TPR和TNR结合起来一样：
- en: Markedness = PPV + NPV − 1
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 标记度 = PPV + NPV − 1
- en: We see that it has the same range as informedness. The informedness says something
    about how well the model is doing at correctly labeling inputs from each class.
    The markedness says something about how well the model is doing at being correct
    when it does claim a particular label for a particular input, be it class 0 or
    class 1\. Random guessing will give a markedness near 0 and perfection a markedness
    near 1.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到它与知情度有相同的范围。知情度表明模型在正确标记每个类别的输入方面表现如何。而标记度则表示模型在当它为特定输入分配特定标签时，是否准确无误，无论是类别0还是类别1。随机猜测会得到接近0的标记度，完美则接近1。
- en: 'I like that the informedness and markedness each capture essential aspects
    of the model’s performance in a single number. Some claim that these metrics are
    unbiased by the prior probabilities of the particular classes. This means if class
    1 is significantly less common than class 0, the informedness and markedness are
    not affected. For in-depth details, see “Evaluation: From Precision, Recall and
    F-measure to ROC, Informedness, Markedness, and Correlation” by David Martin Powers.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢知情度和标记度各自通过一个数字捕捉了模型表现的关键方面。有些人声称这些指标不受特定类别先验概率的影响。这意味着，如果类别1比类别0显著不常见，知情度和标记度不会受到影响。有关深入细节，请参见David
    Martin Powers的《评估：从精确度、召回率和F度量到ROC、知情度、标记度和相关性》。
- en: F1 Score
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F1 分数
- en: 'The *F1 score*, rightly or wrongly, is widely used, and we should be familiar
    with it. The F1 score combines two basic metrics into one. Its definition is straightforward
    in terms of precision (PPV) and recall (TPR):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1 分数*，无论对与错，都被广泛使用，我们应该熟悉它。F1 分数将两个基本指标合并为一个。它的定义是通过精确度（PPV）和召回率（TPR）来直观表达的：'
- en: '![image](Images/263equ01.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/263equ01.jpg)'
- en: The F1 score is a number in [0,1], where higher is better. Where does this formula
    come from? It’s not obvious in this form, but the the F1 score is the harmonic
    mean of the precision and recall. A *harmonic mean* is the reciprocal of the arithmetic
    mean of the reciprocals. Like this,
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是一个位于[0,1]之间的数字，越高越好。这个公式来自哪里？它的形式可能不太直观，但F1 分数是精确度和召回率的调和平均数。*调和平均*是倒数的倒数的算术平均数。像这样，
- en: '![image](Images/263equ02.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/263equ02.jpg)'
- en: One criticism of the F1 score is that it does not take the true negatives into
    account as informedness does (via the TNR). If we look at the definition of PPV
    and TPR, we see that both of these quantities depend entirely on the TP, FP, and
    FN counts from the 2 × 2 table, but not the TN count. Additionally, the F1 score
    places equal weight on the precision and the recall. Precision is affected by
    false positives, while recall is affected by false negatives. From the previous
    breast cancer model, we saw that the human cost of a false negative is substantially
    higher than a false positive. Some argue that this must be taken into account
    when evaluating model performance, and indeed it should. However, if the relative
    costs of a false positive and a false negative are the same, the F1 score will
    have more meaning.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数的一个批评是，它没有像信息性那样考虑真正负类（通过 TNR）。如果我们看看 PPV 和 TPR 的定义，会发现这两个量完全依赖于 2 × 2
    表格中的 TP、FP 和 FN 计数，而不依赖于 TN 计数。此外，F1 分数对精确度和召回率赋予了相同的权重。精确度受假阳性（FP）影响，而召回率受假阴性（FN）影响。从之前的乳腺癌模型中，我们看到假阴性的人工成本远高于假阳性。一些人认为，在评估模型表现时，必须考虑到这一点，实际上应该如此。然而，如果假阳性和假阴性的相对成本相同，F1
    分数将更具意义。
- en: Cohen’s Kappa
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Cohen’s Kappa
- en: '*Cohen’s kappa* is another statistic commonly found in machine learning. It
    attempts to account for the possibility that the model might put the input into
    the correct class by accident. Mathematically, the metric is defined as'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cohen’s kappa* 是机器学习中常见的另一项统计指标。它试图考虑到模型可能会因为偶然因素将输入误归到正确的类别。数学上，这个指标定义为：'
- en: '![image](Images/263equ03.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/263equ03.jpg)'
- en: where *p*[*o*] is the observed accuracy and *p*[*e*] is the accuracy expected
    by chance. For a 2 × 2 table, these values are defined to be
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[*o*] 是观察到的准确率，*p*[*e*] 是由偶然性预期的准确率。对于 2 × 2 表格，这些值定义为：
- en: '![image](Images/264equ01.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/264equ01.jpg)'
- en: with N being the total number of samples in the test set.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 N 是测试集中样本的总数。
- en: Cohen’s kappa is generally between 0 and 1\. 0 means a complete disagreement
    between the assigned class labels and the given class labels. A negative value
    indicates worse than chance agreement. A value near 1 indicates strong agreement.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Cohen’s kappa 通常介于 0 和 1 之间。0 表示分配的类别标签与给定的类别标签完全不一致。负值表示协议比偶然情况还差。接近 1 的值表示强一致性。
- en: Matthews Correlation Coefficient
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Matthews 相关系数
- en: Our final metric is *Matthews correlation coefficient (MCC)*. It is the geometric
    mean of the informedness and markedness. In that sense, it is, like the F1 score,
    a combination of two metrics into one.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的指标是 *Matthews 相关系数 (MCC)*。它是信息性和标记性的几何平均数。从这个角度看，它像 F1 分数一样，是将两个指标合并成一个。
- en: The MCC is defined as
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 定义为：
- en: '![image](Images/264equ02.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/264equ02.jpg)'
- en: 'which, mathematically, works out to the geometric mean of the informedness
    and markedness:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，这相当于信息性和标记性的几何平均数：
- en: '![image](Images/264equ03.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/264equ03.jpg)'
- en: The MCC is favored by many because it takes the full 2 × 2 table into account,
    including the relative frequency of the two classes (the class prior probabilities).
    This is something that the F1 score does not do because it ignores the true negatives.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 被许多人推崇，因为它考虑了整个 2 × 2 表格的内容，包括两个类别的相对频率（类别的先验概率）。这是 F1 分数没有做到的，因为它忽略了真正负类（TN）。
- en: The MCC is a number between 0 and 1, with higher being better. If considering
    only one value as a metric for evaluating a binary model, make it the MCC. Note,
    there are four sums in the denominator of the MCC. If one of these sums is 0,
    the entire denominator will be 0, which is a problem since we cannot divide by
    0\. Fortunately, in that case, the denominator can be replaced with 1 to give
    a still meaningful result. A well-performing model has MCC close to 1.0.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: MCC 是一个介于 0 和 1 之间的数字，数值越高越好。如果只考虑一个指标来评估二分类模型，那么就选 MCC。注意，MCC 的分母中有四个和。如果这些和中有一个为
    0，整个分母就会变为 0，这将是一个问题，因为我们无法除以 0。幸运的是，在这种情况下，分母可以用 1 来替代，从而得到一个仍然有意义的结果。表现良好的模型其
    MCC 值接近 1.0。
- en: Implementing Our Metrics
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现我们的指标
- en: 'Let’s write a function to construct these metrics from a given 2 × 2 table.
    The code is shown in [Listing 11-3](ch11.xhtml#ch11lis3):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写一个函数，从给定的 2 × 2 表格构造这些指标。代码见 [Listing 11-3](ch11.xhtml#ch11lis3)：
- en: from math import sqrt
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: from math import sqrt
- en: 'def advanced_metrics(tally, m):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'def advanced_metrics(tally, m):'
- en: tp, tn, fp, fn, _ = tally
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: tp, tn, fp, fn, _ = tally
- en: n = tp+tn+fp+fn
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: n = tp+tn+fp+fn
- en: po = (tp+tn)/n
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: po = (tp+tn)/n
- en: pe = (tp+fn)*(tp+fp)/n**2 + (tn+fp)*(tn+fn)/n**2
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: pe = (tp+fn)*(tp+fp)/n**2 + (tn+fp)*(tn+fn)/n**2
- en: return {
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: return {
- en: '"F1": 2.0*m["PPV"]*m["TPR"] / (m["PPV"] + m["TPR"]),'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '"F1"：2.0*m["PPV"]*m["TPR"] / (m["PPV"] + m["TPR"])，'
- en: '"MCC": (tp*tn - fp*fn) / sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)),'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '"MCC"： (tp*tn - fp*fn) / sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))，'
- en: '"kappa": (po - pe) / (1.0 - pe),'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '"kappa"： (po - pe) / (1.0 - pe)，'
- en: '"informedness": m["TPR"] + m["TNR"] - 1.0,'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '"informedness"：m["TPR"] + m["TNR"] - 1.0，'
- en: '"markedness": m["PPV"] + m["NPV"] - 1.0'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '"markedness"：m["PPV"] + m["NPV"] - 1.0'
- en: '}'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '*Listing 11-3: Calculating advanced metrics*'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-3：计算高级指标*'
- en: For the sake of simplicity, we’re not checking if the MCC denominator is 0 as
    a full implementation would.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们没有检查MCC分母是否为0，完整实现会检查这一点。
- en: This code takes the tallies and basic metrics as arguments and returns a new
    dictionary with the more advanced metrics. Let’s see how our MNIST example from
    [Table 11-5](ch11.xhtml#ch11tab5) looks when we calculate the advanced metrics.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将计数和基本指标作为参数，并返回一个包含更多高级指标的新字典。让我们来看一下在计算高级指标时，我们从[表11-5](ch11.xhtml#ch11tab5)中的MNIST示例会是什么样子。
- en: '[Table 11-6](ch11.xhtml#ch11tab6) shows the metrics of this section for the
    MNIST 3 versus 5 models. A few things are worth noticing. First, the F1 score
    is always higher than the MCC or Cohen’s kappa. In a way, the F1 score is overly
    optimistic. As previously noted, the F1 score does not take the true negatives
    into account, while both the MCC and Cohen’s kappa do.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[表11-6](ch11.xhtml#ch11tab6)显示了本节针对MNIST 3与5模型的指标。有几件事值得注意。首先，F1得分始终高于MCC或Cohen’s
    kappa。从某种意义上说，F1得分过于乐观。如前所述，F1得分没有考虑到真正的负样本，而MCC和Cohen’s kappa都考虑了。'
- en: '**Table 11-6:** Selected Output from MNIST 3 vs. 5 Models and Corresponding
    Advanced Metrics'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**表11-6：** 从MNIST 3与5模型中选择的输出及相应的高级指标'
- en: '| **Model** | **F1** | **MCC** | **Cohen’s *κ*** | **Informedness** | **Markedness**
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **F1** | **MCC** | **Cohen’s *κ*** | **知晓度** | **标记度** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| *Nearest Centroid* | 0.8671 | 0.7540 | 0.7535 | 0.7520 | 0.7559 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| *最近质心* | 0.8671 | 0.7540 | 0.7535 | 0.7520 | 0.7559 |'
- en: '| *3-NN* | 0.9832 | 0.9683 | 0.9683 | 0.9685 | 0.9682 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| *3-NN* | 0.9832 | 0.9683 | 0.9683 | 0.9685 | 0.9682 |'
- en: '| *Naïve Bayes* | 0.7958 | 0.6875 | 0.6631 | 0.6524 | 0.7244 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| *朴素贝叶斯* | 0.7958 | 0.6875 | 0.6631 | 0.6524 | 0.7244 |'
- en: '| *RF 500* | 0.9916 | 0.9842 | 0.9842 | 0.9841 | 0.9842 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| *RF 500* | 0.9916 | 0.9842 | 0.9842 | 0.9841 | 0.9842 |'
- en: '| *LinearSVM* | 0.9644 | 0.9335 | 0.9334 | 0.9325 | 0.9346 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| *线性支持向量机* | 0.9644 | 0.9335 | 0.9334 | 0.9325 | 0.9346 |'
- en: Another thing to note is that well-performing models, like 3-NN and the Random
    Forest, score highly in all of these metrics. When the model performs well, the
    difference between the F1 score and MCC is smaller than when the model is doing
    poorly (Naïve Bayes, for example). Notice also that the MCC is always between
    the informedness and markedness, as a geometric mean will be. Finally, from the
    values in [Table 11-5](ch11.xhtml#ch11tab5) and [Table 11-6](ch11.xhtml#ch11tab6),
    we see that the best-performing model is the Random Forest, based on the MCC of
    0.9842.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的事情是，表现良好的模型，比如3-NN和随机森林，在所有这些指标上都得分很高。当模型表现良好时，F1得分和MCC之间的差异比模型表现较差时要小（例如朴素贝叶斯）。还要注意，MCC总是位于知晓度和标记度之间，就像几何平均数一样。最后，从[表11-5](ch11.xhtml#ch11tab5)和[表11-6](ch11.xhtml#ch11tab6)中的值可以看到，表现最佳的模型是随机森林，其MCC为0.9842。
- en: In this section, and the two before it, we looked at quite a few metrics and
    saw how they could be calculated and interpreted. A well-performing model will
    score highly on all of these metrics. This is the hallmark of a good model. It’s
    when the models we’re evaluating are less than sterling that the relative differences
    between the metrics, and the meaning of the metrics, really comes into play. That’s
    when we need to consider specific metric values and the cost associated with the
    mistakes the models are making (FP and FN). In those cases, we have to use our
    judgment and problem-specific factors to decide which model is ultimately selected.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节以及前两节中，我们讨论了相当多的指标，并看到了它们如何计算和解释。一个表现良好的模型在所有这些指标上都会得分很高。这是好模型的标志。当我们评估的模型表现不如预期时，指标之间的相对差异，以及指标的含义，才真正发挥作用。那时我们需要考虑特定的指标值，以及模型所犯错误（假阳性和假阴性）所带来的成本。在这些情况下，我们必须根据判断和问题特定的因素来决定最终选择哪个模型。
- en: Now, let’s shift gears and take a look at a graphical way of evaluating model
    performance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们换个角度，看一下评估模型性能的图形化方法。
- en: The Receiver Operating Characteristics Curve
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线（ROC曲线）
- en: 'They say that a picture is worth a thousand words. In this section, we’ll learn
    that a picture—more accurately, a curve—can be worth upward of a dozen numbers.
    That is, we’ll learn how to turn the output of a model into a curve that captures
    more of the performance than the metrics of the previous sections can. Specifically,
    we’ll learn about the widely used *receiver operating characteristics (ROC) curve*:
    what it is, how to plot it, and how to use sklearn to plot it for us.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说，一张图片胜过千言万语。在这一部分中，我们将了解到，一张图片——更准确地说，一条曲线——的价值可以超过一打数字。也就是说，我们将学习如何将模型的输出转化为一条曲线，从而捕捉到比前几节的度量指标更多的性能。具体来说，我们将了解广泛使用的*接收者操作特征（ROC）曲线*：它是什么，如何绘制，以及如何使用
    sklearn 为我们绘制它。
- en: Gathering Our Models
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收集我们的模型
- en: To make the curve, we need a model that outputs a probability of belonging to
    class 1\. In the previous sections, we used models that output a class label so
    that we could tally the TP, TN, FP, and FN counts. For our ROC curves, we still
    need these counts, but instead of the class label as model output, we need the
    probability of class 1 membership. We’ll apply different thresholds to these probabilities
    to decide what label to give the input.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制曲线，我们需要一个能够输出属于类别 1 的概率的模型。在前几节中，我们使用了输出类别标签的模型，以便我们可以统计 TP、TN、FP 和 FN 数量。对于我们的
    ROC 曲线，我们仍然需要这些计数，但我们不再使用类别标签作为模型输出，而是需要类别 1 的成员概率。我们将应用不同的阈值来决定给输入数据什么标签。
- en: Fortunately for us, traditional neural networks (and the deep networks we will
    see in [Chapter 12](ch12.xhtml#ch12)) output the necessary probability. If we’re
    using sklearn, other classical models can also be made to output a probability
    estimate, but we’ll ignore that fact here to keep things simple.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说幸运的是，传统的神经网络（以及我们将在[第 12 章](ch12.xhtml#ch12)中看到的深度网络）能够输出所需的概率。如果我们使用 sklearn，其他经典模型也可以输出概率估计，但为了简化起见，我们在这里忽略这一点。
- en: 'Our test case is a series of neural networks trained to decide between even
    MNIST digits (class 0) and odd MNIST digits (class 1). Our inputs are the vector
    form of the digits that we’ve been using up to this point in the book. We can
    use the training and test data we created in [Chapter 5](ch05.xhtml#ch05)—we only
    need to recode the labels so that digits 0, 2, 4, 6, and 8 are class 0, while
    digits 1, 3, 5, 7, and 9 are class 1\. That is easily accomplished with a few
    lines of code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试案例是一个训练好的神经网络系列，旨在区分偶数 MNIST 数字（类别 0）和奇数 MNIST 数字（类别 1）。我们的输入是到目前为止我们在书中使用的数字的向量形式。我们可以使用在[第
    5 章](ch05.xhtml#ch05)中创建的训练和测试数据——我们只需要重新编码标签，使得数字 0、2、4、6 和 8 为类别 0，而数字 1、3、5、7
    和 9 为类别 1。通过几行代码就能轻松完成：
- en: '[PRE1]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The directory paths point to the same place the other MNIST data is stored.
    We use the fact that the remainder when an even number is divided by 2 is always
    0 or 1 depending on whether the number is even or odd.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目录路径指向存储其他 MNIST 数据的相同位置。我们利用一个事实：当一个偶数被 2 除时，余数总是 0 或 1，具体取决于该数是偶数还是奇数。
- en: 'What models will we test? To emphasize the difference between the respective
    models, we’ll intentionally train models that we know are far from ideal. In particular,
    we’ll use the following code to generate the models and produce the probability
    estimates:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试哪些模型？为了突出各个模型之间的区别，我们将故意训练一些我们知道远非理想的模型。具体来说，我们将使用以下代码生成模型并输出概率估计：
- en: '[PRE2]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code can be found in the file *mnist_even_odd.py*. The `run` and `nn` functions
    should be familiar. We used virtually identical versions in [Chapter 10](ch10.xhtml#ch10),
    where `nn` returns a configured `MLPClassifier` object and `run` trains the classifier
    and returns the prediction probabilities on the test set. The `main` function
    loads the train and test sets, limits the training set to the first 1,000 samples
    (about 500 even and 500 odd), then loops over the hidden layer sizes we will train.
    The first two are single hidden layer networks with 2 and 100 nodes, respectively.
    The last two are two hidden layer networks with 100 × 50 and 500 × 250 nodes per
    layer.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以在文件*mnist_even_odd.py*中找到。`run`和`nn`函数应该很熟悉。我们在[第 10 章](ch10.xhtml#ch10)中使用了几乎相同的版本，其中
    `nn` 返回一个配置好的 `MLPClassifier` 对象，`run` 训练分类器并返回测试集上的预测概率。`main` 函数加载训练和测试集，将训练集限制为前
    1,000 个样本（大约 500 个偶数和 500 个奇数），然后循环遍历我们将要训练的隐藏层大小。前两个是单一隐藏层网络，分别有 2 个和 100 个节点。最后两个是双隐藏层网络，每层分别有
    100 × 50 和 500 × 250 个节点。
- en: Plotting Our Metrics
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绘制我们的度量标准
- en: The output of `clf.predict_proba` is a matrix with as many rows as there are
    test samples (ten thousand in this case). The matrix has as many columns as there
    are classes; since we’re dealing with a binary classifier, there are two columns
    per sample. The first is the probability that the sample is even (class 0), and
    the second is the probability of the sample being odd (class 1). For example,
    the first 10 outputs for one of the models are shown in [Table 11-7](ch11.xhtml#ch11tab7).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-7:** Example Model Output Showing the Assigned per Class Probabilities
    Along with the Actual Original Class Label'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class 0** | **Class 1** | **Actual label** |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| 0.009678 | 0.990322 | 3 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| 0.000318 | 0.999682 | 3 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| 0.001531 | 0.998469 | 7 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| 0.007464 | 0.992536 | 3 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| 0.011103 | 0.988897 | 1 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| 0.186362 | 0.813638 | 7 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| 0.037229 | 0.962771 | 7 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| 0.999412 | 0.000588 | 2 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| 0.883890 | 0.116110 | 6 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| 0.999981 | 0.000019 | 6 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: The first column is the probability of being even, and the second is the probability
    of being odd. The third column is the actual class label for the sample showing
    that the predictions are spot on. The odd digits have high class 1 probabilities
    and low class 0 probabilities, while the opposite is true for the even samples.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: When we build a 2 × 2 table from the performance of a model on a held-out test
    set, we get a collection of TP, TN, FP, and FN numbers from which we can calculate
    all the metrics of the previous sections. This includes the true positive rate
    (TPR, sensitivity) and the false positive rate (FPR, equal to 1 – specificity).
    Implicit in the table is the threshold we used to decide when the model output
    should be considered class 1 or class 0\. In the previous sections, this threshold
    was 0.5\. If the output is ≥ 0.5, we assign the sample to class 1; otherwise,
    we assign it to class 0\. Sometimes you’ll see this threshold added as a subscript
    like TPR[0.5] or FPR[0.5].
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, we can consider the TPR and FPR calculated from a 2 × 2 table
    to be a point on the FPR (x-axis) versus TPR (y-axis) plane, specifically, the
    point (FPR, TPR). Since both FPR and TPR range from 0 to 1, the point (FPR, TPR)
    will lie somewhere within a square of length 1 with the lower-left corner of the
    square at the point (0,0) and the upper-right corner at the point (1,1). Every
    time we change our decision threshold, we get a new 2 × 2 table leading to a new
    point on the FPR versus TPR plane. For example, if we change our decision threshold
    from 0.5 to 0.3 so that each output class 1 probability of 0.3 or higher is called
    class 1, we’ll get a new 2 × 2 table and a new point, (FPR[0.3],TPR[0.3]), on
    the plane. As we systematically change the decision threshold from high to low,
    we generate a sequence of points that we can connect to form a curve.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Curves generated by changing a parameter in this way are called *parametric
    curves*. The points are functions of the threshold. Let’s call the threshold value
    *θ* (theta) and vary it from near 1 to near 0\. Doing so lets us calculate a set
    of points, (FPR[*θ*],TPR[*θ*]), which, when plotted, lead to a curve in the FPR
    versus TPR plane. As noted earlier, this curve has a name: the receiver operating
    characteristics (ROC) curve. Let’s look at an ROC curve and explore what such
    a curve can tell us.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the ROC Curve
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 11-1](ch11.xhtml#ch11fig1) shows the ROC curve for the MNIST even-versus-odd
    model with a single hidden layer of 100 nodes.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/11fig01.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-1: An ROC curve with key elements marked*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The labeled points represent the FPR and TPR for the given threshold values.
    The dashed line is the diagonal from (0,0) to (1,1). This dashed line represents
    a classifier that guesses its output randomly. The closer our curve is to this
    dashed line, the less powerful the model is. If your curve lies on top of the
    line, you might as well flip a coin and assign the label that way. Any curve below
    the dashed line is performing *worse* than random guessing. If the model were
    entirely wrong, meaning it calls all class 1 instances class 0, and *vice versa*,
    a curious thing happens: we can turn the entirely wrong model into a perfectly
    correct model by changing all class 1 output to class 0 and all class 0, output
    to class 1\. It’s unlikely that you will run across a model this bad.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve in [Figure 11-1](ch11.xhtml#ch11fig1) has a single point labeled
    *perfection* in the upper-left corner of the graph. This is the ideal we are striving
    for. We want our ROC curve to move up and to the left toward this point. The closer
    we get the curve to this point, the better the model is performing against our
    test set. A perfect model will have an ROC curve that jumps up vertically to this
    point and then horizontally to the point (1,1). The ROC curve in [Figure 11-1](ch11.xhtml#ch11fig1)
    is going in the right direction and represents a reasonably well-performing model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the labeled *θ* values. We can select a level of performance from the
    model by adjusting *θ*. In this case, the typical default value of 0.5 gives us
    the best performance because that threshold value returns a TPR and FPR with the
    best balance, the point closest to the upper left of the graph. However, there
    are reasons we might want to use a different *θ* value. If we make *θ* small,
    say 0.1, we move along the curve toward the right. Two things happen. First, the
    TPR goes up to about 0.99, meaning we correctly assign about 99 percent of the
    real class 1 instances handed to the model to class 1\. Second, the FPR also goes
    up, to about 0.32, meaning we will simultaneously call about 32 percent of the
    true negatives (class 0) class 1 as well. If our problem is such that we can tolerate
    calling some negative instances “positive,” knowing that we now have a meager
    chance of doing the opposite, calling a positive case “negative,” we might choose
    to change the threshold to 0.1\. Think of the previous breast cancer example:
    we never want to call a positive case “negative,” so we tolerate more false positives
    to know we are not mislabeling any actual positives.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'What does it mean to move the threshold (*θ*) to 0.9? In this case, we’ve moved
    along the curve to the left, to a point with a very low false-positive rate. We
    might do this if we want to know with a high degree of confidence that when the
    model says “class 1,” it is an instance of class 1\. This means we want a high
    positive predictive value (PPV, precision). Recall the definition of the PPV:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/270equ01.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: The PPV is high if FP is low. Setting *θ* to 0.9 makes the FP low for any given
    test set. For the ROC curve of [Figure 11-1](ch11.xhtml#ch11fig1), moving to *θ*
    = 0.9 implies an FPR of about 0.02 and a TPR of about 0.71 for a PPV of about
    0.97\. At *θ* = 0.9, when the model outputs “class 1,” there is a 97 percent chance
    that the model is correct. In contrast, at *θ* = 0.1, the PPV is about 76 percent.
    A high threshold can be used in a situation where we are interested in definitely
    locating an example of class 1 without caring that we might not detect all class
    1 instances.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Changing the threshold *θ* moves us along the ROC curve. As we do so, we should
    expect the metrics of the previous section to also change as a function of *θ*.
    [Figure 11-2](ch11.xhtml#ch11fig2) shows us how the MCC and PPV change with *θ*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/11fig02.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-2: How MCC (circles) and PPV (squares) change as the decision threshold
    (*θ*) changes for the MNIST even/odd model of [Figure 11-1](ch11.xhtml#ch11fig1)*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In the figure, we see that as the threshold goes up, so does the PPV. The model
    becomes more confident when it declares an input a member of class 1\. However,
    this is tempered by the change in MCC, which, as we previously saw, is an excellent
    single metric measure of overall model performance. In this case, the highest
    MCC is at *θ* = 0.5, with MCC falling off as the threshold increases or decreases.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Models with ROC Analysis
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ROC curve gives us a significant amount of information. It’s also handy
    for comparing models, even if those models are radically different from each other
    in architecture or approach. However, care must be taken when making the comparison
    so that the test sets used to generate the curves are ideally the same or very
    nearly the same.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use ROC analysis to compare the different MNIST even/odd digit models
    that we trained previously. We’ll see if this helps us to choose between them.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-3](ch11.xhtml#ch11fig3) shows the ROC curves for these models with
    an inset expanding the upper-left corner of the graph to make it easier to distinguish
    one model from the other. The number of nodes in each hidden layer is indicated
    to identify the models.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/11fig03.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11-3: ROC curves for the MNIST even/odd models. The model hidden layer
    sizes are indicated.*'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: We immediately see that one ROC curve is significantly different from the other
    three. This is the ROC curve for the model with a single hidden layer with two
    nodes. All the other ROC curves are above this one. As a general rule, if one
    ROC curve is entirely above another, then the model that generated the curve can
    be considered superior. All of the larger MNIST even/odd models are superior to
    the model with only two nodes in its hidden layer.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: The other three models are quite close to each other, so how do we choose one?
    The decision isn’t always clear-cut. Following our rule of thumb about ROC curves,
    we should select the two-layer model with 500 and 250 nodes, respectively, as
    its ROC curve is above the others. However, we might hesitate depending upon our
    use case. This model has over 500,000 parameters. Running it requires use of all
    of those parameters. The 100 × 50 model contains slightly more than 80,000 parameters.
    That’s less than one-fifth the number of the larger model. We might decide that
    processing speed considerations eclipse the small improvement in the overall performance
    of the larger model and select the smaller model. The ROC analysis showed us that
    doing so involves only a minor performance penalty.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Another factor to consider when comparing ROC curves visually is the slope of
    the curve when the FPR is small. A perfect model has a vertical slope since it
    jumps immediately from the point (0,0) to (0,1). Therefore, the better model will
    have an ROC curve that has a steeper slope in the low FPR region.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: A commonly used metric derived from the ROC curve is the area under it. This
    area is usually abbreviated as *AUC* or, in medical circles, *Az*. A perfect ROC
    curve has an AUC of 1.0 since the curve jumps from (0,0) to (0,1) and then over
    to (1,1), forming a square of side 1 with an area of 1\. A model that guesses
    randomly (the diagonal line in the ROC plot) has an AUC of 0.5, the area of the
    triangle formed by the dashed diagonal line. To calculate the area under an arbitrary
    ROC curve, one needs to perform numerical integration. Fortunately for us, sklearn
    knows how to do this, so we don’t need to. We’ll see this shortly.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: People often report the AUC, but as time goes by, I’m less and less in favor
    of it. The main reason is that AUC replaces the highly informative graph with
    a single number, but different ROC curves can lead to the same AUC. If the AUC
    of two curves is the same, but one leans far to the right while the other has
    a steep slope in the low FPR region, we might be tempted to think the models are
    roughly equivalent in terms of performance, when, in reality, the model with the
    steeper slope is likely the one we want because it will reach a reasonable TPR
    without too many false positives.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Another caution when using the AUC is that the AUC changes only a small amount
    for even fairly significant changes in other parameters. This makes it difficult
    for humans to judge well based on AUC values that are only slightly different
    from each other. For example, the AUC of the MNIST even/odd model with two nodes
    in its hidden layer is 0.9373, while the AUC of the model with 100 nodes is 0.9722\.
    Both are well above 0.9 out of a possible 1.0, so, are they both about the same?
    We know that they are not, since the ROC curves clearly show the two-node model
    to be well below the other.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Generating an ROC Curve
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We are now ready to learn how to create an ROC curve. The easy way to get the
    ROC curve, and AUC, is to use `sklearn`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This routine reads a set of labels and the associated per class probabilities,
    such as the output generated by the code in the previous section. It then calls
    the sklearn functions `roc_auc_score` and `roc_curve` to return the AUC and the
    ROC points, respectively. The ROC curve is plotted, saved to disk, and displayed.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'We need not use sklearn as a black box. We can generate the ROC curve points
    ourselves quickly enough. We load the same inputs, the labels, and the per class
    probabilities, but instead of calling a library function, we loop over the threshold
    values of interest and calculate TP, TN, FP, and FN for each threshold. From these,
    we can directly calculate the FPR and TPR, which gives us the set of points we
    need to plot. The code to do this is straightforward:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `main` function loads the labels and probabilities. The loop over `th` applies
    the different threshold values, accumulating the ROC points in `roc` by calling
    the `table` function, which calculates the TP, TN, FP, and FN for the current
    threshold.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The `table` function loops over all the per class probabilities assigning a
    class label of 1 if the class 1 probability is greater than or equal to the current
    threshold value. This class assignment is then compared to the actual class label,
    and the appropriate tally counter is incremented.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Once the ROC points are calculated, the plot is made by adding the point (0,0)
    to the beginning of the point list and the point (1,1) to the end of the list.
    Doing this ensures that the plot extends the full range of FPR values. The points
    are plotted and saved to disk.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The Precision–Recall Curve
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before leaving this section, we should mention one other evaluation curve that
    you will run across from time to time in machine learning. This is the *precision-recall
    (PR) curve*. As the name suggests, it plots the PPV (precision) and TPR (recall,
    sensitivity) as the decision threshold varies, just like an ROC curve. A good
    PR curve moves toward the upper right instead of the upper left as a good ROC
    curve does. The points of this curve are easily generated in sklearn using the
    `precision_recall_curve` function in the `metrics` module.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: We’re not spending time with this curve because it does not take the true negatives
    into account. Consider the definition of the PPV and TPR to see that this is so.
    My bias against the PR curve stems from the same concern as my bias against the
    F1 score. By not taking the true negatives into account, the PR curve and F1 score
    give an incomplete picture of the quality of the classifier. The PR curve does
    have utility when the true positive class is rare or when the true negative performance
    is not essential. However, in general, for evaluating classifier performance,
    I claim it is best to stick to the ROC curve and the metrics we have defined.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Handling Multiple Classes
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All of the metrics we’ve discussed so far apply to binary classifiers only.
    Of course, we know that many classifiers are multiclass: they output multiple
    labels, not just 0 or 1\. To evaluate these models, we’ll extend our idea of the
    confusion matrix to the multiclass case and see that we can also extend some of
    the metrics we’re already familiar with as well.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'We need some multiclass model results to work with. Thankfully, the MNIST data
    is already multiclass. Recall, we went to the trouble of recoding the labels to
    make the dataset binary. Here we’ll train models with the same architectures,
    but this time we’ll leave the labels as they are so that the model will output
    one of ten labels: the digit it assigned to the test input, the output of the
    `predict` method of the `MLPClassifier` class. We won’t show the code as it’s
    identical to the code in the previous section except that `predict` is called
    in place of `predict_proba`.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Extending the Confusion Matrix
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The basis for our binary metrics was the 2 × 2 confusion matrix. The confusion
    matrix is readily extended to the multiclass case. To do that, we let the rows
    of the matrix represent the actual class labels, while the columns of the matrix
    represent the model’s predictions. The matrix is square with as many rows and
    columns as there are classes in the dataset. For MNIST, then, we arrive at a 10
    × 10 confusion matrix since there are 10 digits.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate the confusion matrix from the actual known test labels and the
    predicted labels from the model. There is a function in the `metrics` module of
    sklearn, `confusion_matrix`, which we can use, but it’s straightforward enough
    to calculate it ourselves:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here `n` is the number of classes, fixed at 10 for MNIST. If needed, we could
    instead determine it from the supplied test labels.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The code is straightforward. The inputs are vectors of the actual labels (`y_test`)
    and the predicted labels (`y_predict`), and the confusion matrix (`cmat`) is filled
    in by incrementing each possible index formed from the actual label and the predicted
    label. For example, if the actual label is 3 and the predicted label is 8, then
    we add one to `cmat[3,8]`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the confusion matrix for a model with one hidden layer of 100
    nodes ([Table 11-8](ch11.xhtml#ch11tab8)).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-8:** Confusion Matrix for the Model with a Single Hidden Layer of
    100 Nodes'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8**
    | **9** |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| **0** | 943 | 0 | 6 | 9 | 0 | 10 | 7 | 1 | 4 | 0 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 1102 | 14 | 5 | 1 | 1 | 3 | 1 | 8 | 0 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| **2** | 16 | 15 | 862 | 36 | 18 | 1 | 17 | 24 | 41 | 2 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| **3** | 3 | 1 | 10 | 937 | 0 | 20 | 3 | 13 | 17 | 6 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| **4** | 2 | 8 | 4 | 2 | 879 | 0 | 14 | 1 | 6 | 66 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| **5** | 19 | 3 | 3 | 53 | 13 | 719 | 17 | 3 | 44 | 18 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| **6** | 14 | 3 | 4 | 2 | 21 | 15 | 894 | 1 | 4 | 0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| **7** | 3 | 21 | 32 | 7 | 10 | 1 | 0 | 902 | 1 | 51 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| **8** | 17 | 14 | 11 | 72 | 11 | 46 | 21 | 9 | 749 | 24 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| **9** | 10 | 11 | 1 | 13 | 42 | 5 | 2 | 31 | 10 | 884 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: The rows represent the actual test sample label, [0,9]. The columns are the
    label assigned by the model. If the model is perfect, there will be a one-to-one
    match between the actual label and the predicted label. This is the main diagonal
    of the confusion matrix. Therefore, a perfect model will have entries along the
    main diagonal, and all other elements will be 0\. [Table 11-8](ch11.xhtml#ch11tab8)
    is not perfect, but the largest counts are along the main diagonal.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Look at row 4 and column 4\. The place where the row and column meet has the
    value 879\. This means that there were 879 times when the actual class was 4 and
    the model correctly predicted “4” as the label. If we look along row 4, we see
    other numbers that are not zero. Each of these represents a case where an actual
    4 was called another digit by the model. For example, there were 66 times when
    a 4 was called a “9” but only one case of a 4 being labeled a “7”.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Column 4 represents the cases when the model called the input a “4”. As we saw,
    it was correct 879 times. However, there were other digits that the model accidentally
    labeled as “4”, like the 21 times a 6 was called a “4” or the one time a 1 was
    mistaken for a “4”. There were no cases of a 3 being labeled a “4”.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix tells us at a glance how well the model is doing on the
    test set. We can quickly see if the matrix is primarily diagonal. If it is, the
    model is doing a good job on the test set. If not, we need to take a closer look
    to see what classes are being confused with other classes. A simple adjustment
    to the matrix can help. Instead of the raw counts, which require us to remember
    how many examples of each class are in the test set, we can divide the values
    of each row by the sum of the row. Doing so converts the entries from counts to
    fractions. We can then multiply the entries by 100 to convert to percents. This
    transforms the confusion matrix into what we’ll call an *accuracy matrix*. The
    conversion is straightforward:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here `cmat` is the confusion matrix. This produces an accuracy matrix, [Table
    11-9](ch11.xhtml#ch11tab9).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-9:** A Confusion Matrix Presented as per Class Accuracies'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8**
    | **9** |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| **0** | **96.2** | 0. | 0.6 | 0.9 | 0. | 1.1 | 0.7 | 0.1 | 0.4 | 0. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0. | **97.1** | 1.4 | 0.5 | 0.1 | 0.1 | 0.3 | 0.1 | 0.8 | 0. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1.6 | 1.3 | **83.5** | 3.6 | 1.8 | 0.1 | 1.8 | 2.3 | 4.2 | 0.2 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0.3 | 0.1 | 1. | **92.8** | 0. | 2.2 | 0.3 | 1.3 | 1.7 | 0.6 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0.2 | 0.7 | 0.4 | 0.2 | **89.5** | 0. | 1.5 | 0.1 | 0.6 | 6.5 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| **5** | 1.9 | 0.3 | 0.3 | 5.2 | 1.3 | **80.6** | 1.8 | 0.3 | 4.5 | 1.8 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| **6** | 1.4 | 0.3 | 0.4 | 0.2 | 2.1 | 1.7 | **93.3** | 0.1 | 0.4 | 0. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| **7** | 0.3 | 1.9 | 3.1 | 0.7 | 1. | 0.1 | 0. | **87.7** | 0.1 | 5.1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| **8** | 1.7 | 1.2 | 1.1 | 7.1 | 1.1 | 5.2 | 2.2 | 0.9 | **76.9** | 2.4 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| **9** | 1. | 1. | 0.1 | 1.3 | 4.3 | 0.6 | 0.2 | 3. | 1. | **87.6** |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: The diagonal shows the per class accuracies. The worst performing class is 8
    with an accuracy of 76.9 percent, and the best performing class is 1 with an accuracy
    of 97.1 percent. The non-diagonal elements are the percentage of the actual class
    labeled as a different class by the model. For class 0, the model called a true
    zero class “5” 1.1 percent of the time. The row percentages sum to 100 percent
    (within rounding error).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Why did class 8 do so poorly? Looking across the row for class 8, we see that
    the model mistook 7.1 percent of the actual 8 instances for a “3” and 5.2 percent
    of the instances for a “5”. Confusing an 8 with a “3” was the biggest single mistake
    the model made, though 6.5 percent of 4 instances were labeled “9” as well. A
    moment’s reflection makes sense of the errors. How often do people confuse 8 and
    3 or 4 and 9? This model is making errors similar to those humans make.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix can reveal pathological performance as well. Consider the
    MNIST model in [Figure 11-3](ch11.xhtml#ch11fig3), with a single hidden layer
    of only two nodes. The accuracy matrix it produces is shown in [Table 11-10](ch11.xhtml#ch11tab10).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: We can immediately see that this is an inferior model. Column 5 is entirely
    zero, meaning the model never outputs “5” for any input. Much the same is true
    for output labels “8” and “9”. On the other hand, the model likes to call inputs
    “0”, “1”, “2”, or “3” as those columns are densely populated for all manner of
    input digits. Looking at the diagonal, we see that only 1 and 3 stand a reasonable
    chance of being correctly identified, though many of these will be called “7”.
    Class 8 is rarely correctly labeled (1.3 percent). A poorly performing model will
    have a confusion matrix like this, with oddball outputs and large off-diagonal
    values.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-10:** Accuracy Matrix for the Model with Only Two Nodes in Its Hidden
    Layer'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8**
    | **9** |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| **0** | **51.0** | 1.0 | 10.3 | 0.7 | 1.8 | 0.0 | 34.1 | 0.7 | 0.0 | 0.4
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.4 | **88.3** | 0.4 | 1.1 | 0.8 | 0.0 | 0.0 | 9.3 | 1.0 | 0.0 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| **2** | 8.6 | 2.8 | **75.2** | 6.9 | 1.7 | 0.0 | 1.4 | 3.0 | 0.3 | 0.6 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0.2 | 1.0 | 4.9 | **79.4** | 0.3 | 0.0 | 0.0 | 13.5 | 0.0 | 0.2 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| **4** | 28.4 | 31.3 | 7.3 | 2.1 | **9.7** | 0.0 | 0.3 | 13.6 | 1.0 | 0.5
    |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| **5** | 11.4 | 42.5 | 2.2 | 4.9 | 4.4 | **0.0** | 0.1 | 16.5 | 0.9 | 0.3
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| **6** | 35.4 | 1.0 | 5.4 | 0.2 | 1.4 | 0.0 | **55.0** | 0.0 | 0.0 | 0.1 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| **7** | 0.4 | 5.2 | 2.0 | 66.2 | 0.8 | 0.0 | 0.0 | **25.5** | 0.2 | 0.3 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| **8** | 10.5 | 41.9 | 2.8 | 8.0 | 4.1 | 0.0 | 0.1 | 22.1 | **1.3** | 0.4
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| **9** | 4.7 | 9.1 | 5.8 | 26.2 | 5.8 | 0.0 | 0.2 | 41.2 | 2.2 | **3.1** |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: Calculating Weighted Accuracy
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The diagonal elements of an accuracy matrix tell us the per class accuracies
    for the model. We can calculate an overall accuracy by averaging these values.
    However, this could be misleading if one or more classes is far more prevalent
    in the test data than the others. Instead of a simple average, we should use a
    weighted average. The weights are based on the total number of test samples from
    each class divided by the total number of test samples presented to the model.
    Say we have three classes and their frequency and per class accuracies in our
    test set are as in [Table 11-11](ch11.xhtml#ch11tab11):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-11:** Hypothetical per Class Accuracies for a Model with Three Classes'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Frequency** | **Accuracy** |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| 0 | 4,004 | 88.1 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6,502 | 76.6 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8,080 | 65.2 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: Here we have *N* = 4,004 + 6,502 + 8,080 = 18586 test samples. Then, the per
    class weights are shown in [Table 11-12](ch11.xhtml#ch11tab12).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-12:** Example per-class weights'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Weight |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| 0 | 4,004 / 18,586 = 0.2154 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6,502 / 18,586 = 0.3498 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8,080 / 18,586 = 0.4347 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: The average accuracy can be calculated to be
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: ACC = 0.2154 × 88.1 + 0.3498 × 76.6 + 0.4347 × 65.2 = 74.1
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Philosophically, we should replace the weights with the actual per class prior
    probabilities, if we know them. These probabilities are the true likelihood of
    the class appearing in the wild. However, if we assume that the test set is fairly
    constructed, we’re likely safe using only the per class frequencies. We claim
    that a properly built test set will represent the true prior class probabilities
    reasonably well.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, the weighted mean accuracy can be calculated succinctly from the confusion
    matrix:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*N* is the total number of samples that were tested, which is just the sum
    of the entries in the confusion matrix since every sample in the test set falls
    somewhere in the matrix, and *C* is a vector of the number of samples per class.
    This is just the sum of the rows of the confusion matrix. The per class accuracy,
    as a percentage, is calculated from the diagonal elements of the confusion matrix
    (`np.diag(cmat)`) divided by the number of times each class shows up in the test
    set, *C*. Multiply by 100 to make these percent accuracies.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: If we summed these per class and divided by the number of classes, we would
    have the (potentially misleading) unweighted mean accuracy. Instead, we first
    multiply by *C*/*N*, the fraction of all test samples that were of each class
    (recall, *C* is a vector), and then sum to get the weighted accuracy. This code
    works for any size confusion matrix.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: For the MNIST models of the previous section, we calculate weighted mean accuracies
    to be those in [Table 11-13](ch11.xhtml#ch11tab13).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-13:** Weighted Mean Accuracies for the MNIST Models'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '| **Architecture** | **Weighted mean accuracy** |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| 2 | 40.08% |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| 100 | 88.71% |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| 100 × 50 | 88.94% |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| 500 × 250 | 89.63% |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '[Table 11-13](ch11.xhtml#ch11tab13) shows the sort of diminishing returns we’ve
    seen previously as the model size increases. The single hidden layer of 100 nodes
    is virtually identical to the two hidden layer model with 100 and 50 nodes and
    only 1 percent worse than the much larger model with 500 nodes and 250 nodes in
    its hidden layers. The model with only two nodes in the hidden layer performs
    poorly. Since there are 10 classes, random guessing would tend to have an accuracy
    of 1/10 = 0.1 = 10 percent, so even this very strange model that maps 784 input
    values (28×28 pixels) to only two and then to ten output nodes is still four times
    more accurate than random guessing. However, this is misleading on its own because,
    as we just saw in [Table 11-10](ch11.xhtml#ch11tab10), the confusion matrix for
    this model is quite strange. We certainly would not want to use this model. Nothing
    beats careful consideration of the confusion matrix.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass Matthews Correlation Coefficient
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 2 × 2 confusion matrix led to many possible metrics. While it’s possible
    to extend several of those metrics to the multiclass case, we’ll consider only
    the main metric here: the Matthews correlation coefficient (MCC). For the binary
    case, we saw that the MCC was'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/281equ01.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: This can be extended to the multiclass case by using terms from the confusion
    matrix like so
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/281equ02.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: where
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/281equ03.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: Here, *K* is the number of classes, and *C* is the confusion matrix. This notation
    is from the sklearn website’s description of the MCC, giving us a direct view
    of how it’s implemented. We don’t need to follow the equations in detail; we need
    to know only that the MCC is built from the confusion matrix in the multiclass
    case as in the binary case. Intuitively, this makes sense. The binary MCC is a
    value in the range [*–*1,+1]. The multiclass case changes the lower bound based
    on the number of classes, but the upper bound remains 1.0, so the closer the MCC
    is to 1.0, the better the model is doing.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the MCC for the MNIST models, as we did for the weighted mean accuracy,
    gives [Table 11-14](ch11.xhtml#ch11tab14).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 11-14:** The MCC for the MNIST Models'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '| **Architecture** | **MCC** |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.3440 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| 100 | 0.8747 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| 100 × 50 | 0.8773 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| 500 × 250 | 0.8849 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: Again, this shows us that the smallest model is inferior while the other three
    models are all quite similar in terms of performance. The time to make predictions
    on the 10,000 test samples, however, varies quite a bit by model. The single hidden
    layer model with 100 nodes takes 0.052 seconds, while the largest model needs
    0.283 seconds, over five times longer. If speed is essential, the smaller model
    might be preferable. Many factors come into play when deciding on a model to use.
    The metrics discussed in this chapter are guides, but they should not be followed
    blindly. In the end, only you know what makes sense for the problem you are trying
    to solve.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we learned why accuracy is not a sufficient measure of the
    performance of a model. We learned how to generate the 2 × 2 confusion matrix
    for a binary classifier, and what this matrix tells us about the model’s performance
    on the held-out test set. We derived basic metrics from the 2 × 2 confusion matrix
    and used those basic metrics to derive more advanced metrics. We discussed the
    utility of the various metrics to build our intuition as to how and when to use
    them. We then learned about the receiver operating characteristics (ROC) curve,
    including what it illustrates about the model and how to interpret it to compare
    models against each other. Finally, we introduced the multiclass confusion matrix,
    giving examples of how to interpret it and how to extend some of the binary classifier
    metrics to the multiclass case.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll reach the pinnacle of our machine learning models:
    convolutional neural networks (CNNs). The next chapter introduces the basic ideas
    behind the CNN; later chapters will conduct many experiments using this deep learning
    architecture.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
