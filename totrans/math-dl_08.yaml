- en: '**8'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MATRIX CALCULUS**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Chapter 7](ch07.xhtml#ch07) introduced us to differential calculus. In this
    chapter, we’ll discuss *matrix calculus*, which extends differentiation to functions
    involving vectors and matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning works extensively with vectors and matrices, so it makes sense
    to develop a notation and approach to representing derivatives involving these
    objects. That’s what matrix calculus gives us. We saw a hint of this at the end
    of [Chapter 7](ch07.xhtml#ch07), when we introduced the gradient to represent
    the derivative of a scalar function of a vector—a function that accepts a vector
    argument and returns a scalar, *f*(***x***).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the table of matrix calculus derivatives and their definitions.
    Next, we’ll examine some identities involving matrix derivatives. Mathematicians
    love identities; however, to preserve our sanity, we’ll only consider a handful.
    Some special matrices come out of matrix calculus, namely the Jacobian and Hessian.
    You’ll run into both of these matrices during your sojourn through deep learning,
    so we’ll consider them next in the context of optimization. Recall that training
    a neural network is, fundamentally, an optimization problem, so understanding
    what these special matrices represent and how we use them is especially important.
    We’ll close the chapter with some examples of matrix derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: The Formulas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 8-1](ch08.xhtml#ch08tab01) summarizes the matrix calculus derivatives
    we’ll explore in this chapter. These are the ones commonly used in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 8-1:** Matrix Calculus Derivatives'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Scalar** | **Vector** | **Matrix** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Scalar** | ∂*f*/∂*x* | ∂***f***/∂*x* | ∂***F***/∂*x* |'
  prefs: []
  type: TYPE_TB
- en: '| **Vector** | ∂*f*/∂***x*** | ∂*f*/∂***x*** | — |'
  prefs: []
  type: TYPE_TB
- en: '| **Matrix** | ∂***f***/∂***X*** | — | — |'
  prefs: []
  type: TYPE_TB
- en: 'The columns of [Table 8-1](ch08.xhtml#ch08tab01) represent the function, meaning
    the return value. Notice we use three versions of the letter f: regular, bold,
    and capital. We use *f* if the return value is a scalar, ***f*** if a vector,
    and ***F*** if a matrix. The rows of [Table 8-1](ch08.xhtml#ch08tab01) are the
    variables the derivatives are calculated with respect to. The same notation applies:
    *x* is a scalar, ***x*** is a vector, and ***X*** is a matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8-1](ch08.xhtml#ch08tab01) defines six derivatives, but there are nine
    cells in the table. While possible to define, the remaining derivatives are not
    standardized or used often enough to make covering them worthwhile. That’s good
    for us, as the six are enough of a challenge for our mathematical brains.'
  prefs: []
  type: TYPE_NORMAL
- en: The first derivative in [Table 8-1](ch08.xhtml#ch08tab01), the one in the upper
    left, is the normal derivative of [Chapter 7](ch07.xhtml#ch07), a function producing
    a scalar with respect to a scalar. (Refer to [Chapter 7](ch07.xhtml#ch07) for
    everything you need to know about standard differentiation.)
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover the remaining five derivatives in the sections below. We define
    each one in terms of scalar derivatives. We’ll first show the definition and then
    explain what the notation represents. The definition will help you build a model
    in your head of what the derivative is. I suspect that by the end of this section
    you’ll be predicting the definitions in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, however, there is a complication we should discuss. Matrix
    calculus is notation heavy, but there’s no universal agreement on the notation.
    We’ve seen this before with the many ways to indicate differentiation. For matrix
    calculus, the two approaches are *numerator layout* or *denominator layout*. Specific
    disciplines seem to favor one over the other, though exceptions are almost the
    norm, as is mixing notations. For deep learning, a nonscientific perusal on my
    part seems to indicate a slight preference for numerator layout, so that’s what
    we’ll use here. Just be aware that there are two forms out there. One is typically
    the transpose of the other.
  prefs: []
  type: TYPE_NORMAL
- en: A Vector Function by a Scalar Argument
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A vector function accepting a scalar argument is our first derivative; see
    [Table 8-1](ch08.xhtml#ch08tab01), second column of the first row. We write such
    a function as ***f***(*x*) to indicate a scalar argument, *x*, and a vector output,
    ***f***. Functions like ***f*** take a scalar and map it to a multidimensional
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '***f*** : ℝ → ℝ^(*m*)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *m* is the number of elements in the output vector. Functions like ***f***
    are known as *vector-valued functions* with scalar arguments.
  prefs: []
  type: TYPE_NORMAL
- en: A parametric curve in 3D space is an excellent example of such a function. Those
    functions are often written as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/195equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![Image](Images/195equ02b.jpg), ![Image](Images/195equ02a.jpg), and ![Image](Images/195equ03.jpg)
    are unit vectors in the x, y, and z directions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-1](ch08.xhtml#ch08fig01) shows a plot of a 3D parametric curve,'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where, as *t* varies, the three axis values also vary to trace out the spiral.
    Here, each value of *t* specifies a single point in 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/08fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8-1: A 3D parametric curve*'
  prefs: []
  type: TYPE_NORMAL
- en: In matrix calculus notation, we don’t write ***f*** as shown in [Equation 8.1](ch08.xhtml#ch08equ01).
    Instead, we write ***f*** as a column vector of the functions,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/195equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and, in general,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/196equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for ***f*** with *n* elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of ***f***(*x*) is known as the *tangent vector*. What does
    the derivative look like? Since ***f*** is a vector, we might expect the derivative
    of ***f*** to be the derivatives of the functions representing each element of
    ***f***, and we’d be right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/196equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s look at a simple example. First, we will define ***f***(*x*), and then
    the derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/196equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, each element of ∂***f***/∂*x* is the derivative of the corresponding function
    in ***f***.
  prefs: []
  type: TYPE_NORMAL
- en: A Scalar Function by a Vector Argument
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [Chapter 7](ch07.xhtml#ch07), we learned that a function accepting a vector
    input but returning a scalar is a scalar field:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f* : ℝ^(*m*) → ℝ'
  prefs: []
  type: TYPE_NORMAL
- en: We also learned that the derivative of this function is the gradient. In matrix
    calculus notation, we write ∂*f*/∂***x*** for *f*(***x***) as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/196equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ***x*** = [*x*[0] *x*[1] ... *x*[*m*–1]]^⊤ is a vector of variables, and
    *f* is a function of those variables.
  prefs: []
  type: TYPE_NORMAL
- en: Notice, because we decided to use the numerator layout approach, ∂*f*/∂***x***
    is written as a *row* vector. So, to be true to our notation, we have to write
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/197equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: turning the row vector into a column vector to match the gradient. Remember
    that ▽ is the symbol for gradient; we saw an example of the gradient in Chapter
    7.
  prefs: []
  type: TYPE_NORMAL
- en: A Vector Function by a Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the derivative of a vector-valued function by a scalar produces a column
    vector, and the derivative of a scalar function by a vector results in a row vector,
    does the derivative of a vector-valued function by a vector produce a matrix?
    The answer is yes. In this case, we’re contemplating ∂***f***/∂***x*** for ***f***(***x***),
    a function that accepts a vector input and returns a vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The numerator layout convention gave us a column vector for the derivative
    of ***f***(*x*), implying we need a row for each of the functions in ***f***.
    Similarly, the derivative of *f*(***x***) produced a row vector. Therefore, merging
    the two gives us the derivative of ***f***(***x***):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is for a function, ***f***, returning an *n*-element vector and accepting
    an *m-*element vector, ***x***, as its argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f* : ℝ^(*m*) → ℝ^(*n*)'
  prefs: []
  type: TYPE_NORMAL
- en: Each row of ***f*** is a scalar function of ***x***, for example, *f*[0](***x***).
    Therefore, we can write [Equation 8.2](ch08.xhtml#ch08equ02) as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This gives us the matrix as a collection of gradients, one for each scalar function
    in ***f***. We’ll return to this matrix later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A Matrix Function by a Scalar
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If ***f***(*x*) is a function accepting a scalar argument but returning a vector,
    then we’d be correct in assuming ***F***(*x*) can be thought of as a function
    accepting a scalar argument but returning a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '***F*** : ℝ → ℝ^(*n*×*m*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, assume ***F*** is an *n* × *m* matrix of scalar functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/198equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative with respect to the argument, *x*, is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/198equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we saw above, the derivative of a vector-valued function by a scalar is called
    the tangent vector. By analogy, then, the derivative of a matrix-valued function
    by a scalar is the *tangent matrix*.
  prefs: []
  type: TYPE_NORMAL
- en: A Scalar Function by a Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s consider *f*(***X***), a function accepting a matrix and returning
    a scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f* : ℝ^(*n*×*m*) → ℝ'
  prefs: []
  type: TYPE_NORMAL
- en: We’d be correct in thinking that the derivative of *f* with respect to the matrix,
    ***X***, is itself a matrix. However, to be true to our numerator layout convention,
    the resulting matrix is not arranged like ***X***, but instead like ***X***^⊤,
    the transpose of ***X***.
  prefs: []
  type: TYPE_NORMAL
- en: Why use the transpose of ***X*** instead of ***X*** itself? To answer the question,
    we need to look back to how we defined ∂*f*/∂***x***. There, even though ***x***
    is a column vector, according to standard convention, we said the derivative is
    a *row* vector. We used ***x***^⊤ as the ordering. Therefore, to be consistent,
    we need to arrange ∂*f*/∂***X*** by the transpose of ***X*** and change the columns
    of ***X*** into rows in the derivative. As a result, we have the following definition.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is an *m* × *n* output matrix for the *n* × *m* input matrix, ***X***.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 8.4](ch08.xhtml#ch08equ04) defines the *gradient matrix*, which,
    for matrices, plays a role similar to that of the gradient, ▽*f*(**x**). [Equation
    8.4](ch08.xhtml#ch08equ04) also completes our collection of matrix calculus derivatives.
    Let’s move on to consider some matrix derivative identities.'
  prefs: []
  type: TYPE_NORMAL
- en: The Identities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Matrix calculus involves scalars, vectors, matrices, and functions thereof,
    which themselves return scalars, vectors, or matrices, implying many identities
    and relationships exist. However, here we’ll concentrate on basic identities showing
    the relationship between matrix calculus and the differential calculus of [Chapter
    7](ch07.xhtml#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: Each of the following subsections presents identities related to the specific
    type of derivative indicated. The identities cover fundamental relationships and,
    when applicable, the chain rule. In all cases, the results follow the numerator
    layout scheme we’ve used throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A Scalar Function by a Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We begin with identities related to a scalar function with a vector input. If
    not otherwise specified, *f* and *g* are functions of a vector, ***x***, and return
    a scalar. A constant vector that doesn’t depend on ***x*** is given as ***a***,
    and *a* denotes a scalar constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic rules are intuitive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These show that multiplication by a scalar constant acts as it did in [Chapter
    7](ch07.xhtml#ch07), as does the linearity of the partial derivative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The product rule also works as we expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let’s pause here and remind ourselves of the inputs and outputs for the equations
    above. We know that the derivative of a scalar function by a vector argument is
    a row vector in our notation. So, [Equation 8.5](ch08.xhtml#ch08equ05) returns
    a row vector multiplied by a scalar—each element of the derivative is multiplied
    by *a*.
  prefs: []
  type: TYPE_NORMAL
- en: As differentiation is a linear operator, it distributes over addition, so [Equation
    8.6](ch08.xhtml#ch08equ06) delivers two terms, each a row vector generated by
    the respective derivative.
  prefs: []
  type: TYPE_NORMAL
- en: For [Equation 8.7](ch08.xhtml#ch08equ07), the product rule, the result again
    includes two terms. In each case, the derivative returns a row vector, which is
    multiplied by a scalar function value, either *f*(***x***) or *g*(***x***). Therefore,
    the output of [Equation 8.7](ch08.xhtml#ch08equ07) is also a row vector.
  prefs: []
  type: TYPE_NORMAL
- en: The scalar-by-vector chain rule becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *f*(*g*) returns a scalar and accepts a scalar argument, while *g*(***x***)
    returns a scalar and accepts a vector argument. The end result is a row vector.
    Let’s work through a complete example to demonstrate.
  prefs: []
  type: TYPE_NORMAL
- en: We have a vector, ***x*** = [*x*[0], *x*[1], *x*[2]]^⊤; a function of that vector
    written in component form, *g*(***x***) = *x*[0] + *x*[1]*x*[2]; and a function
    of *g*, *f*(*g*) = *g*². According to [Equation 8.8](ch08.xhtml#ch08equ08), the
    derivative of *f* with respect to ***x*** is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/200equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To check our result, we can work through from *g*(***x***) = *x*[0] + *x*[1]*x*[2]
    and *f*(*g*) = *g*² to find *f*(***x***) directly via substitution. Doing this
    gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/201equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: from which we get
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/201equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which matches the result we found using the chain rule. Of course, in this simple
    example, it was easier to work through the substitution before taking the derivative,
    but we proved our case all the same.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not entirely through with scalar-by-vector identities, however. The dot
    product takes two vectors and produces a scalar, so it fits with the functional
    form we’re working with, even though the arguments to the dot product are vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have the derivative of the dot product between ***x*** and a vector
    ***a*** that does not depend on ***x***.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can expand on [Equation 8.9](ch08.xhtml#ch08equ09), replacing ***x*** with
    a vector-valued function, ***f(x***):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What’s the form of this result? Assume *f* accepts an *m*-element input and
    returns an *n*-element vector output. Likewise, assume ***a*** to be an *n*-element
    vector. From [Equation 8.2](ch08.xhtml#ch08equ02), we know the derivative ∂***f***/∂***x***
    to be an *n* × *m* matrix. Therefore, the final result is a (1 × *n*) × (*n* ×
    *m*) → 1 × *m* row vector. Good! We know the derivative of a scalar function by
    a vector should be a row vector when using the numerator layout convention.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the derivative of the dot product of two vector-valued functions, ***f***
    and ***g***, is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If [Equation 8.10](ch08.xhtml#ch08equ10) is a row vector, then the sum of two
    terms like it is also a row vector.
  prefs: []
  type: TYPE_NORMAL
- en: A Vector Function by a Scalar
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vector-by-scalar differentiation, [Table 8-1](ch08.xhtml#ch08tab01), first
    row, second column, is less common in machine learning, so we’ll only examine
    a few identities. The first are multiplications by constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/202equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note, we can multiply on the left by a matrix, as the derivative is a column
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: The sum rule still applies,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/202equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: as does the chain rule,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Equation 8.12](ch08.xhtml#ch08equ12) is correct, since the derivative of a
    vector by a scalar is a column vector, and the derivative of a vector by a vector
    is a matrix. Therefore, multiplying the matrix on the right by a column vector
    returns a column vector, as expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two other derivatives involving dot products with respect to a scalar are worth
    knowing about. The first is similar to [Equation 8.11](ch08.xhtml#ch08equ11) but
    with two vector-valued functions of a scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/202equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second derivative concerns the composition of *f*(***g***) and ***g***(*x*)
    with respect to *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/202equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is the dot product of a row vector and a column vector.
  prefs: []
  type: TYPE_NORMAL
- en: A Vector Function by a Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The derivatives of vector-valued functions with vector arguments are common
    in physics and engineering. In machine learning, they show up during backpropagation,
    for example, at the derivative of the loss function. Let’s begin with some straightforward
    identities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/203equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/203equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the result is the sum of two matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chain rule is next and works as it did above for scalar-by-vector and vector-by-scalar
    derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/203equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with the result being the product of two matrices.
  prefs: []
  type: TYPE_NORMAL
- en: A Scalar Function by a Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For functions of a matrix, ***X***, returning a scalar, we have the usual form
    for the sum rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/203equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with the result being the sum of two matrices. Recall, if ***X*** is an *n*
    × *m* matrix, the derivative in numerator layout notation is an *m* × *n* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The product rule also works as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/203equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the chain rule is different. It depends on *f(g*), a scalar function
    accepting a scalar input, and *g*(***X***), a scalar function accepting a matrix
    input. With this restriction, the form of the chain rule looks familiar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s see [Equation 8.13](ch08.xhtml#ch08equ13) in action. First, we need ***X***,
    a 2 × 2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/204equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we need ![Image](Images/204equ02.jpg) and *g*(***X***) = *x*[0]*x*[3]
    + *x*[1]*x*[2]. Notice, while *g*(***X***) accepts a matrix input, the result
    is a scalar calculated from the values of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: To apply the chain rule, we need two derivatives,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/204equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we are again using numerator layout for the result.
  prefs: []
  type: TYPE_NORMAL
- en: To find the overall result, we calculate
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/204equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To check, we combine the functions to write a single function, ![Image](Images/204equ05.jpg),
    and calculate the derivative using the standard chain rule for each element of
    the resulting matrix. This gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/204equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: matching the previous result.
  prefs: []
  type: TYPE_NORMAL
- en: We have our definitions and identities. Let’s revisit the derivative of a vector-valued
    function with a vector argument, as the resulting matrix is special. We’ll encounter
    it frequently in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Jacobians and Hessians
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Equation 8.2](ch08.xhtml#ch08equ02) defined the derivative of a vector-valued
    function, ***f***, with respect to a vector, ***x***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This derivative is known as the *Jacobian matrix*, ***J***, or simply the *Jacobian*,
    and you’ll encounter it from time to time in the deep learning literature, especially
    during discussions of gradient descent and other optimization algorithms used
    in training models. The Jacobian sometimes has a subscript to indicate the variable
    it is with respect to; for example, ***J******[x]*** if with respect to ***x***.
    When the context is clear, we’ll often neglect the subscript.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll discuss the Jacobian and what it means. Then we’ll introduce
    another matrix, the *Hessian matrix* (or just the *Hessian*), which is based on
    the Jacobian, and learn how to use it in optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The essence of this section is the following: the Jacobian is the generalization
    of the first derivative, and the Hessian is the generalization of the second derivative.'
  prefs: []
  type: TYPE_NORMAL
- en: Concerning Jacobians
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We saw previously that we can think of [Equation 8.14](ch08.xhtml#ch08equ14)
    as a stack of transposed gradient vectors ([Equation 8.3](ch08.xhtml#ch08equ03)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/205equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Viewing the Jacobian as a stack of gradients gives us a clue as to what it represents.
    Recall, the gradient of a scalar field, a function accepting a vector argument
    and returning a scalar, points in the direction of the maximum change in the function.
    Similarly, the Jacobian gives us information about how the vector-valued function
    behaves in the vicinity of some point, ***x******[p]***. The Jacobian is to vector-valued
    functions of vectors what the gradient is to scalar-valued functions of vectors;
    it tells us about how the function changes for a small change in the position
    of ***x******[p]***.
  prefs: []
  type: TYPE_NORMAL
- en: One way to think of the Jacobian is as a generalization of the more specific
    cases we encountered in [Chapter 7](ch07.xhtml#ch07). [Table 8-2](ch08.xhtml#ch08tab02)
    shows the relationship between the function and what its derivative measures.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 8-2:** The Relationship Between Jacobians, Gradients, and Slopes'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function** | **Derivative** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ***f***(***x***) | ∂***f***/∂***x***, Jacobian matrix |'
  prefs: []
  type: TYPE_TB
- en: '| *f*(***x***) | ∂*f*/∂***x***, gradient vector |'
  prefs: []
  type: TYPE_TB
- en: '| *f*(*x*) | *df*/*dx*, slope |'
  prefs: []
  type: TYPE_TB
- en: 'The Jacobian matrix is the most general of the three. If we limit the function
    to a scalar, then the Jacobian matrix becomes the gradient vector (row vector
    in numerator layout). If we limit the function and argument to scalars, the gradient
    becomes the slope. In a sense, they all indicate the same thing: how the function
    is changing around a point in space.'
  prefs: []
  type: TYPE_NORMAL
- en: The Jacobian has many uses. I’ll present two examples. The first is from systems
    of differential equations. The second uses Newton’s method to find the roots of
    a vector-valued function. We’ll see Jacobians again when we discuss backpropagation,
    as that requires calculating derivatives of a vector-valued function with respect
    to a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Differential Equations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A *differential equation* combines derivatives and function values in one equation.
    Differential equations show up everywhere in physics and engineering. Our example
    comes from the theory of *autonomous systems*, which are differential equations
    where the independent variable does not appear on the right-hand side. For instance,
    if the system consists of values of the function and first derivatives with respect
    to time, *t*, there is no *t* explicit in the equations governing the system.
  prefs: []
  type: TYPE_NORMAL
- en: The previous paragraph is just for background; you don’t need to memorize it.
    Working with systems of autonomous differential equations ultimately leads to
    the Jacobian, which is our goal. We can view the system as a vector-valued function,
    and we’ll use the Jacobian to characterize the critical points of that system
    (the points where the derivative is zero). We worked with critical points of functions
    in [Chapter 7](ch07.xhtml#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s explore the following system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/206equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This system includes two functions, *x*(*t*) and *y*(*t*), and they are coupled
    so that the rate of change of *x*(*t*) depends on the value of *x* and the value
    of *y*, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll view the system as a single, vector-valued function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we replace *x* with *x*[0] and *y* with *x*[1].
  prefs: []
  type: TYPE_NORMAL
- en: The system that ***f*** represents has critical points at locations where ***f***
    = **0**, with **0** being the 2 × 1 dimensional zero vector. The critical points
    are
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where substitution into ***f*** shows that each point returns the zero vector.
    For the time being, assume we were given the critical points, and now we want
    to characterize them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To characterize a critical point, we will need the Jacobian matrix that ***f***
    generates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the Jacobian describes how a function behaves in the vicinity of a point,
    we can use it to characterize the critical points. In [Chapter 7](ch07.xhtml#ch07),
    we used the derivative to tell us whether a point was a minimum or maximum of
    a function. For the Jacobian, we use the eigenvalues of ***J*** in much the same
    way to talk about the type and stability of critical points.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s find the Jacobian at each of the critical points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/207equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use NumPy to get the eigenvalues of the Jacobians:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> import numpy as np'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.linalg.eig([[4,0],[0,2]])[0]'
  prefs: []
  type: TYPE_NORMAL
- en: array([4., 2.])
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.linalg.eig([[2,0],[1,-2]])[0]'
  prefs: []
  type: TYPE_NORMAL
- en: array([-2., 2.])
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.linalg.eig([[0,-4],[2,-4]])[0]'
  prefs: []
  type: TYPE_NORMAL
- en: array([-2.+2.j, -2.-2.j])
  prefs: []
  type: TYPE_NORMAL
- en: We encountered np.linalg.eig in [Chapter 6](ch06.xhtml#ch06). The eigenvalues
    are the first values that eig returns, hence the [0] subscript to the function
    call.
  prefs: []
  type: TYPE_NORMAL
- en: For critical points of a system of autonomous differential equations, the eigenvalues
    indicate the points’ type and stability. If both eigenvalues are real and have
    the same sign, the critical point is a node. If the eigenvalues are less than
    zero, the node is stable; otherwise, it is unstable. You can think of a stable
    node as a pit; if you’re near it, you’ll fall into it. An unstable node is like
    a hill; if you move away from the top, the critical point, you’ll fall off. The
    first critical point, ***c***[0], has positive, real eigenvalues; therefore, it
    represents an unstable node.
  prefs: []
  type: TYPE_NORMAL
- en: If the eigenvalues of the Jacobian are real but of opposite signs, the critical
    point is a saddle point. We discussed saddle points in [Chapter 7](ch07.xhtml#ch07).
    A saddle point is ultimately unstable, but in two dimensions, there’s a direction
    where you can “fall into” the saddle and a direction where you can “fall off”
    the saddle. Some researchers believe most minima found when training deep neural
    networks are really saddle points of the loss function. We see that critical point
    ***c***[1] is a saddle point, since the eigenvalues are real with opposite signs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the eigenvalues of ***c***[2] are complex. Complex eigenvalues indicate
    a spiral (also called a focus). If the real part of the eigenvalues is less than
    zero, the spiral is stable; otherwise, it is unstable. As the eigenvalues are
    complex conjugates of each other, the signs of the real parts must be the same;
    one can’t be positive while the other is negative. For ***c***[2], the real parts
    are negative, so ***c***[2] indicates a stable spiral.
  prefs: []
  type: TYPE_NORMAL
- en: Newton’s Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: I presented the critical points of [Equation 8.15](ch08.xhtml#ch08equ15) by
    fiat. The system is easy enough that we can solve for the critical points algebraically,
    but that might not generally be the case. One classic method for finding the roots
    of a function (the places where it returns zero) is known as *Newton’s method*.
    This is an iterative method using the first derivative and an initial guess to
    zero in on the root. Let’s look at the method in one dimension and then extend
    it to two. We’ll see that moving to two or more dimensions requires the use of
    the Jacobian.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use Newton’s method to find the square root of 2\. To do that, we need
    an equation such that ![Image](Images/208equ01.jpg). A moment’s thought gives
    us one: *f*(*x*) = 2 − *x*². Clearly, when ![Image](Images/208equ02.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: The governing equation for Newton’s method in one dimension is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *x*[0] is some initial guess at the solution.
  prefs: []
  type: TYPE_NORMAL
- en: We substitute *x*[0] for *x[n]* on the right-hand side of [Equation 8.18](ch08.xhtml#ch08equ18)
    to find *x*[1]. We then repeat using *x*[1] on the right-hand side to get *x*[2],
    and so on until we see little change in *x[n]*. At that point, if our initial
    guess is reasonable, we have the value we’re looking for. Newton’s method converges
    quickly, so for typical examples, we only need a handful of iterations. Of course,
    we have powerful computers at our fingertips, so we’ll use them instead of working
    by hand. The Python code we need is in [Listing 8-1](ch08.xhtml#ch08ex01).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: 'def f(x):'
  prefs: []
  type: TYPE_NORMAL
- en: return 2.0 - x*x
  prefs: []
  type: TYPE_NORMAL
- en: 'def d(x):'
  prefs: []
  type: TYPE_NORMAL
- en: return -2.0*x
  prefs: []
  type: TYPE_NORMAL
- en: x = 1.0
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(5):'
  prefs: []
  type: TYPE_NORMAL
- en: x = x - f(x)/d(x)
  prefs: []
  type: TYPE_NORMAL
- en: 'print("%2d: %0.16f" % (i+1,x))'
  prefs: []
  type: TYPE_NORMAL
- en: print("NumPy says sqrt(2) = %0.16f for a deviation of %0.16f" %
  prefs: []
  type: TYPE_NORMAL
- en: (np.sqrt(2), np.abs(np.sqrt(2)-x)))
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8-1: Finding* ![Image](Images/209equ01.jpg) *via Newton’s method*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8-1](ch08.xhtml#ch08ex01) defines two functions. The first, f(x),
    returns the function value for a given x. The second, d(x), returns the derivative
    at x. If *f*(*x*) = 2 − *x*², then *f*′(*x*) = −2*x*.'
  prefs: []
  type: TYPE_NORMAL
- en: Our initial guess is *x* = 1.0\. We iterate [Equation 8.18](ch08.xhtml#ch08equ18)
    five times, printing the current estimate of the square root of 2 each time. Finally,
    we use NumPy to calculate the *true* value and see how far we are from it.
  prefs: []
  type: TYPE_NORMAL
- en: Running [Listing 8-1](ch08.xhtml#ch08ex01) produces
  prefs: []
  type: TYPE_NORMAL
- en: '1: 1.5000000000000000'
  prefs: []
  type: TYPE_NORMAL
- en: '2: 1.4166666666666667'
  prefs: []
  type: TYPE_NORMAL
- en: '3: 1.4142156862745099'
  prefs: []
  type: TYPE_NORMAL
- en: '4: 1.4142135623746899'
  prefs: []
  type: TYPE_NORMAL
- en: '5: 1.4142135623730951'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy says sqrt(2) = 1.4142135623730951 for a
  prefs: []
  type: TYPE_NORMAL
- en: deviation of 0.0000000000000000
  prefs: []
  type: TYPE_NORMAL
- en: which is impressive; we get ![Image](Images/209equ02.jpg) to 16 decimals in
    only five iterations.
  prefs: []
  type: TYPE_NORMAL
- en: To extend Newton’s method to vector-valued functions of vectors, like [Equation
    8.15](ch08.xhtml#ch08equ15), we replace the reciprocal of the derivative with
    the inverse of the Jacobian. Why the inverse? Recall, for a diagonal matrix, the
    inverse is the reciprocal of the diagonal elements. If we view the scalar derivative
    as a 1 × 1 matrix, then the reciprocal and inverse are the same. [Equation 8.18](ch08.xhtml#ch08equ18)
    is already using the inverse of the Jacobian, albeit one for a 1 × 1 matrix. Therefore,
    we’ll iterate
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for a suitable initial value, ***x***[0], and the inverse of the Jacobian evaluated
    at *x[n]*. Let’s use Newton’s method to find the critical points of [Equation
    8.15](ch08.xhtml#ch08equ15).
  prefs: []
  type: TYPE_NORMAL
- en: Before we can write some Python code, we need the inverse of the Jacobian, [Equation
    8.17](ch08.xhtml#ch08equ17). The inverse of a 2 × 2 matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/210equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/210equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: assuming the determinant is not zero. The determinant of ***A*** is *ad* − *bc*.
    Therefore, the inverse of [Equation 8.17](ch08.xhtml#ch08equ17) is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/210equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we can write our code. The result is [Listing 8-2](ch08.xhtml#ch08ex02).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: 'def f(x):'
  prefs: []
  type: TYPE_NORMAL
- en: x0,x1 = x[0,0],x[1,0]
  prefs: []
  type: TYPE_NORMAL
- en: return np.array([[4*x0-2*x0*x1],[2*x1+x0*x1-2*x1**2]])
  prefs: []
  type: TYPE_NORMAL
- en: 'def JI(x):'
  prefs: []
  type: TYPE_NORMAL
- en: x0,x1 = x[0,0],x[1,0]
  prefs: []
  type: TYPE_NORMAL
- en: d = (4-2*x1)*(2-x0-4*x1)+2*x0*x1
  prefs: []
  type: TYPE_NORMAL
- en: return (1/d)*np.array([[2-x0-4*x1,2*x0],[-x1,4-2*x0]])
  prefs: []
  type: TYPE_NORMAL
- en: 'x0 = float(input("x0: "))'
  prefs: []
  type: TYPE_NORMAL
- en: 'x1 = float(input("x1: "))'
  prefs: []
  type: TYPE_NORMAL
- en: ❶ x = np.array([[x0],[x1]])
  prefs: []
  type: TYPE_NORMAL
- en: N = 20
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(N):'
  prefs: []
  type: TYPE_NORMAL
- en: ❷ x = x - JI(x) @ f(x)
  prefs: []
  type: TYPE_NORMAL
- en: 'if (i > (N-10)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'print("%4d: (%0.8f, %0.8f)" % (i, x[0,0],x[1,0]))'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8-2: Newton’s method in 2D*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8-2](ch08.xhtml#ch08ex02) echoes [Listing 8-1](ch08.xhtml#ch08ex01)
    for the 1D case. We have f(x) to calculate the function value for a given input
    vector and JI(x) to give us the value of the inverse Jacobian at ***x***. Notice
    that f(x) returns a column vector and JI(x) returns a 2 × 2 matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: The code first asks the user for initial guesses, x0 and x1. These are formed
    into the initial vector, x. Note that we explicitly form x as a column vector
    ❶.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of [Equation 8.19](ch08.xhtml#ch08equ19) comes next ❷. The
    inverse Jacobian is a 2 × 2 matrix that we multiply on the right by the function
    value, a 2 × 1 column vector, using NumPy’s matrix multiplication operator, @.
    The result is a 2 × 1 column vector subtracted from the current value of x, itself
    a 2 × 1 column vector. If the loop is within 10 iterations of completion, the
    current value is printed at the console.
  prefs: []
  type: TYPE_NORMAL
- en: Does [Listing 8-2](ch08.xhtml#ch08ex02) work? Let’s run it and see if we can
    find initial guesses leading to each of the critical points ([Equation 8.16](ch08.xhtml#ch08equ16)).
    For an initial guess of ![Image](Images/211equ01.jpg), we get
  prefs: []
  type: TYPE_NORMAL
- en: '11: (0.00004807, -1.07511237)'
  prefs: []
  type: TYPE_NORMAL
- en: '12: (0.00001107, -0.61452262)'
  prefs: []
  type: TYPE_NORMAL
- en: '13: (0.00000188, -0.27403667)'
  prefs: []
  type: TYPE_NORMAL
- en: '14: (0.00000019, -0.07568702)'
  prefs: []
  type: TYPE_NORMAL
- en: '15: (0.00000001, -0.00755378)'
  prefs: []
  type: TYPE_NORMAL
- en: '16: (0.00000000, -0.00008442)'
  prefs: []
  type: TYPE_NORMAL
- en: '17: (0.00000000, -0.00000001)'
  prefs: []
  type: TYPE_NORMAL
- en: '18: (0.00000000, -0.00000000)'
  prefs: []
  type: TYPE_NORMAL
- en: '19: (0.00000000, -0.00000000)'
  prefs: []
  type: TYPE_NORMAL
- en: which is the first critical point of [Equation 8.15](ch08.xhtml#ch08equ15).
    To find the two remaining critical points, we need to pick our initial guesses
    with some care. Some guesses explode, while many lead back to the zero vector.
    However, some trial and error gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/211equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: showing that Newton’s method can find the critical points of [Equation 8.15](ch08.xhtml#ch08equ15).
  prefs: []
  type: TYPE_NORMAL
- en: We started this section with a system of differential equations that we interpreted
    as a vector-valued function. We then used the Jacobian to characterize the critical
    points of that system. Next, we used the Jacobian a second time to locate the
    system’s critical points via Newton’s method. We could do this because the Jacobian
    is the generalization of the gradient to vector-valued functions, and the gradient
    itself is a generalization of the first derivative of a scalar function. As mentioned
    above, we’ll see Jacobians again when we discuss backpropagation in [Chapter 10](ch10.xhtml#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Concerning Hessians
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the Jacobian matrix is like the first derivative of a function of one variable,
    then the *Hessian matrix* is like the second derivative. In this case, we’re restricted
    to scalar fields, functions returning a scalar value for a vector input. Let’s
    start with the definition and go from there. For the function *f*(***x***), the
    Hessian is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ***x*** = [*x*[0] *x*[1] . . . *x*[*n*–1]]^⊤.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at [Equation 8.20](ch08.xhtml#ch08equ20) tells us that the Hessian is
    a square matrix. Moreover, it’s a symmetric matrix implying ***H*** = ***H***^⊤.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hessian is the Jacobian of the gradient of a scalar field:'
  prefs: []
  type: TYPE_NORMAL
- en: '***H**[f]* = ***J***(▽*f*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see this with an example. Consider this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/212equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we use the definition of the Hessian in [Equation 8.20](ch08.xhtml#ch08equ20)
    directly, we see that ![Image](Images/212equ02.jpg) because ∂*f*/∂*x*[0] = 4*x*[0]
    + *x*[2]. Similar calculations give us the rest of the Hessian matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/212equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the Hessian is constant, not a function of ***x***, because the
    highest power of a variable in *f*(***x***) is 2.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient of *f*(***x***), using our column vector definition, is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/212equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with the Jacobian of the gradient giving the following, which is identical to
    the matrix we found by direct use of [Equation 8.20](ch08.xhtml#ch08equ20).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/212equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Minima and Maxima
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We saw in [Chapter 7](ch07.xhtml#ch07) that we could use the second derivative
    to test whether critical points of a function were minima (*f*′′ > 0) or maxima
    (*f*′′ < 0). We’ll see in the next section how we can use critical points in optimization
    problems. For now, let’s use the Hessian to find critical points by considering
    its eigenvalues. We’ll continue with the example above. The Hessian matrix is
    3 × 3, meaning there are three (or fewer) eigenvalues. Again, we’ll save time
    and use NumPy to tell us what they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.linalg.eig([[4,0,1],[0,-2,3],[1,3,0]])[0]'
  prefs: []
  type: TYPE_NORMAL
- en: array([ 4.34211128, 1.86236874, -4.20448002])
  prefs: []
  type: TYPE_NORMAL
- en: Two of the three eigenvalues are positive, and one is negative. If all three
    were positive, the critical point would be a minimum. Likewise, if all three were
    negative, the critical point would be a maximum. Notice that the minimum/maximum
    label is the opposite of the sign, just like the single-variable case. However,
    if at least one eigenvalue is positive and another negative, which is the case
    with our example, the critical point is a saddle point.
  prefs: []
  type: TYPE_NORMAL
- en: It seems natural to ask whether the Hessian of a vector-valued function, ***f***(***x***),
    exists. After all, we can calculate the Jacobian of such a function; we did so
    above to show that the Hessian is the Jacobian of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to extend the Hessian to a vector-valued function. However,
    the result is no longer a matrix, but an order-3 tensor. To see this is so, consider
    the definition of a vector-valued function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/213equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can think of a vector-valued function, a vector field, as a vector of scalar
    functions of a vector. We could calculate the Hessian of each of the *m* functions
    in ***f*** to get a vector of matrices,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/213equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'but a vector of matrices is a 3D object. Think of an RGB image: a 3D array
    made up of three 2D images, one each for the red, green, and blue channels. Therefore,
    while possible to define and calculate, the Hessian of a vector-valued function
    is beyond our current scope.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In deep learning, you’re most likely to see the Hessian in reference to optimization.
    Training a neural network is, to a first approximation, an optimization problem—the
    goal is to find the weights and biases leading to a minimum in the loss function
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](ch07.xhtml#ch07), we saw that the gradient provides information
    on how to move toward a minimum. An optimization algorithm, like gradient descent,
    the subject of [Chapter 11](ch11.xhtml#ch11), uses the gradient as a guide. As
    the gradient is a first derivative of the loss function, algorithms based solely
    on the gradient are known as *first-order optimization methods*.
  prefs: []
  type: TYPE_NORMAL
- en: The Hessian provides information beyond the gradient. As a second derivative,
    the Hessian contains information about how the loss landscape’s gradient is changing,
    that is, its curvature. An analogy from physics might help here. A particle’s
    motion in one dimension is described by some function of time, *x*(*t*). The first
    derivative, the velocity, is *dx*/*dt* = *v*(*t*). The velocity tells us how quickly
    the position is changing in time. However, the velocity might change with time,
    so its derivative, *dv*/*dt* = *a*(*t*), is the acceleration. And, if the velocity
    is the first derivative of the position, then the acceleration is the second,
    *d*²*x*/*dt*² = *a*(*t*). Similarly, the second derivative of the loss function,
    the Hessian, provides information on how the gradient is changing. Optimization
    algorithms using the Hessian, or an approximation of it, are known as *second-order
    optimization methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with an example in one dimension. We have a function, *f*(*x*),
    and we’re currently at some *x*[0]. We want to move from this position to a new
    position, *x*[1], closer to a minimum of *f*(*x*). A first-order algorithm will
    use the gradient, here the derivative, as a guide, since we know moving in the
    direction opposite to the derivative will move us toward a lower function value.
    Therefore, for some step size, call it η (eta), we can write
  prefs: []
  type: TYPE_NORMAL
- en: '*x*[1] = *x*[0] − η *f*′(*x*[0])'
  prefs: []
  type: TYPE_NORMAL
- en: This will move us from *x*[0] toward *x*[1], which is closer to the minimum
    of *f*(*x*), assuming the minimum exists.
  prefs: []
  type: TYPE_NORMAL
- en: The equation above makes sense, so why think about a second-order method? The
    second-order method comes into play when we move from *f*(*x*) to *f*(*x*). Now
    we have a gradient, not just a derivative, and the landscape of *f*(***x***) around
    some point can be more complex. The general form of gradient descent is
  prefs: []
  type: TYPE_NORMAL
- en: '***x***[1] = ***x***[0] − η▽*f*(***x***[0])'
  prefs: []
  type: TYPE_NORMAL
- en: 'but the information in the Hessian can be of assistance. To see how, we first
    need to introduce the idea of a *Taylor series expansion*, a way of approximating
    an arbitrary function as the sum of a series of terms. We use Taylor series frequently
    in physics and engineering to simplify complex functions in the vicinity of a
    specific point. We also often use them to calculate values of *transcendental
    functions* (functions that can’t be written as a finite set of basic algebra operations).
    For example, it’s likely that when you use cos(x) in a programming language, the
    result is generated by a Taylor series expansion with a sufficient number of terms
    to get the cosine to 32- or 64-bit floating-point precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/215equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In general, to approximate a function, *f*(*x*), in the vicinity of a point,
    *x* = *a*, the Taylor series expansion is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/215equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *f*^((*k*))(*a*) is the *k*th derivative of *f*(*x*) evaluated at point
    *a*.
  prefs: []
  type: TYPE_NORMAL
- en: A linear approximation of *f*(*x*) around *x* = *a* is
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) ≈ *f*(*a*) + *f*′(*a*)(*x* - *a*)'
  prefs: []
  type: TYPE_NORMAL
- en: while a quadratic approximation of *f*(*x*) becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we see the linear approximation using the first derivative and the quadratic
    using the first and second derivatives of *f*(*x*). A first-order optimization
    algorithm uses the linear approximation, while a second-order one uses the quadratic
    approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Moving from a scalar function of a scalar, *f*(*x*), to a scalar function of
    a vector, *f*(***x***), changes the first derivative to a gradient and the second
    derivative to the Hessian matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/215equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with ***H***[*f*](***a***) the Hessian matrix for *f*(***x***) evaluated at
    the point ***a***. The products’ order changes to make the dimensions work out
    properly, as we now have vectors and a matrix to deal with.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if ***x*** has *n* elements, then *f*(***a***) is a scalar; the
    gradient at ***a*** is an *n*-element column vector multiplying (***x*** − ***a***)^⊤,
    an *n*-element row vector, producing a scalar; and the last term is 1 × *n* times
    *n* × *n* times *n* × 1, leading to 1 × *n* times *n* × 1, which is also a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: To use the Taylor series expansions for optimization, to find the minimum of
    *f*, we can use Newton’s method in much the same way used in [Equation 8.18](ch08.xhtml#ch08equ18).
    First, we rewrite [Equation 8.21](ch08.xhtml#ch08equ21) to change our viewpoint
    to one of a displacement (Δ*x*) from a current position (*x*). [Equation 8.21](ch08.xhtml#ch08equ21)
    then becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Equation 8.22](ch08.xhtml#ch08equ22) is a parabola in Δ*x*, and we’re using
    it as a stand-in for the more complex shape of *f* in the region of *x* + Δ*x*.
    To find the minimum of [Equation 8.22](ch08.xhtml#ch08equ22), we take the derivative
    and set it to zero, then solve for Δ*x*. The derivative gives'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/216equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which, if set to zero, leads to
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Equation 8.23](ch08.xhtml#ch08equ23) tells us the offset from a current position,
    *x*, that would lead to the minimum of *f*(*x*) if *f*(*x*) were actually a parabola.
    In reality, *f*(*x*) isn’t a parabola, so the Δ*x* of [Equation 8.23](ch08.xhtml#ch08equ23)
    isn’t the actual offset to the minimum of *f*(*x*). However, since the Taylor
    series expansion used the actual slope, *f*′(*x*), and curvature, *f*′′(*x*),
    of *f*(*x*) at *x*, the offset of [Equation 8.23](ch08.xhtml#ch08equ23) is a better
    estimate of the actual minimum of *f*(*x*) than the linear approximation, assuming
    there is a minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go from *x* to *x* + Δ*x*, there’s no reason why we can’t then use [Equation
    8.23](ch08.xhtml#ch08equ23) a second time, calling the new position *x*. Thinking
    like this leads to an equation we can iterate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for *x*[0], some initial starting point.
  prefs: []
  type: TYPE_NORMAL
- en: We can work out all of the above for scalar functions with vector arguments,
    *f*(***x***), which are the kind we most often encounter in deep learning via
    the loss function. [Equation 8.24](ch08.xhtml#ch08equ24) becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/216equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the reciprocal of the second derivative becomes the inverse of the Hessian
    matrix evaluated at ***x**[n]*.
  prefs: []
  type: TYPE_NORMAL
- en: Excellent! We have an algorithm we can use to rapidly find the minimum of a
    function like *f*(***x***). We saw above that Newton’s method converges quickly,
    so using it to minimize a loss function also should converge quickly, faster than
    gradient descent, which only considers the first derivative.
  prefs: []
  type: TYPE_NORMAL
- en: If this is the case, why do we use gradient descent to train neural networks
    instead of Newton’s method?
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons. First, we haven’t discussed issues arising from the
    Hessian’s applicability, issues related to the Hessian being a positive definite
    matrix. A symmetric matrix is positive definite if all its eigenvalues are positive.
    Near saddle points, the Hessian might not be positive definite, which can cause
    the update rule to move away from the minimum. As you might expect with a simple
    algorithm like Newton’s method, some variations try to address issues like this,
    but even if problems with the eigenvalues of the Hessian are addressed, the computational
    burden of using the Hessian for updating network parameters is what stops Newton’s
    algorithm in its tracks.
  prefs: []
  type: TYPE_NORMAL
- en: Every time the network’s weights and biases are updated, the Hessian changes,
    requiring it and its inverse to be calculated again. Think of the number of minibatches
    used during network training. For even one minibatch, there are *k* parameters
    in the network, where *k* is easily in the millions to even billions. The Hessian
    is a *k* × *k* symmetric and positive definite matrix. Inverting the Hessian typically
    uses Cholesky decomposition, which is more efficient than other methods but is
    still an 𝒪(*k*³) algorithm. The *big-O* notation indicates that the algorithm’s
    resource use scales as the cube of the size of the matrix in time, memory, or
    both. This means doubling the number of parameters in the network increases the
    computational time to invert the Hessian by a factor of 2³ = 8 while tripling
    the number of parameters requires some 3³ = 27 times as much effort, and quadrupling
    some 4³ = 64 times as much. And this is to say nothing about storing the *k*²
    elements of the Hessian matrix, all floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: The computation necessary to use Newton’s method with even modest deep networks
    is staggering. Gradient-based, first-order optimization methods are about all
    we can use for training neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This statement is perhaps a bit premature. Recent work in the area of* neuroevolution
    *has demonstrated that evolutionary algorithms can successfully train deep models.
    My experimentation with swarm optimization techniques and neural networks lends
    credence to this approach as well.*'
  prefs: []
  type: TYPE_NORMAL
- en: That first-order methods work as well as they do seems, for now, to be a very
    happy accident.
  prefs: []
  type: TYPE_NORMAL
- en: Some Examples of Matrix Calculus Derivatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conclude the chapter with some examples similar to the kinds of derivatives
    we commonly find in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Derivative of Element-Wise Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s begin with the derivative of element-wise operations, which includes things
    like adding two vectors together. Consider
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/217equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is the straightforward addition of two vectors, element by element. What
    does ∂***f***/∂***a***, the Jacobian of ***f***, look like? From the definition,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/218equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: but *f*[0] only depends on *a*[0], while *f*[1] depends on *a*[1], and so on.
    Therefore, all derivatives ∂*f[i]*/∂*a[j]* for *i* ≠ *j* are zero. This removes
    all the off-diagonal elements of the matrix, leaving
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/218equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: since ∂*f*[*i*]/∂*a*[*i*] = 1 for all *i*. Similarly, ∂***f***/∂***b*** = ***I***
    as well. Also, if we change from addition to subtraction, ∂***f***/∂***a*** =
    ***I***, but ∂***f***/∂***b*** = −***I***.
  prefs: []
  type: TYPE_NORMAL
- en: If the operator is element-wise multiplication of ***a*** and ***b***, ***f***
    = ***a*** ⊗ ***b***, then we get the following, where the diag(***x***) notation
    means the *n* elements of vector ***x*** along the diagonal of an *n* × *n* matrix
    that is zero elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/218equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Derivative of the Activation Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s find the derivative of the weights and bias value for a single node of
    a hidden layer in a feedforward network. Recall, the inputs to the node are the
    outputs of the previous layer, ***x***, multiplied term by term by the weights,
    ***w***, and summed along with the bias value, *b*, a scalar. The result, a scalar,
    is passed to the activation function to produce the output value for the node.
    Here, we’re using the *rectified linear unit (ReLU)* which returns its argument
    if the argument is positive. If the argument is negative, ReLU returns zero. We
    can write this process as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In order to implement backpropagation, we need the derivatives of [Equation
    8.25](ch08.xhtml#ch08equ25) with respect to ***w*** and *b*. Let’s see how to
    find them.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by considering the pieces of [Equation 8.25](ch08.xhtml#ch08equ25).
    For example, from [Equation 8.9](ch08.xhtml#ch08equ09), we know the derivative
    of the dot product with respect to ***w*** is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/219equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we have taken advantage of the fact that the dot product is commutative,
    ***w*** • ***x*** = ***x*** • ***w***. Also, since *b* does not depend on ***w***,
    we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What about the derivative of ReLU? By definition,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/219equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: implying that
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/08equ27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: since ∂*z*/∂*z* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: To find the derivatives of [Equation 8.25](ch08.xhtml#ch08equ25) with respect
    to ***w*** and *b*, we need the chain rule and the results above. Let’s start
    with ***w***. The chain rule tells us how
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/219equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with *z* = ***w*** • ***x*** + *b* and *y* = ReLU(*z*).
  prefs: []
  type: TYPE_NORMAL
- en: We know ∂*y*/∂*z*; it’s the two cases above for the ReLU, [Equation 8.27](ch08.xhtml#ch08equ27).
    So now we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/219equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and we know ∂*z*/∂***w*** = ***x***^⊤; it’s [Equation 8.26](ch08.xhtml#ch08equ26).
    Therefore, our final result is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/220equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we’ve replaced *z* with ***w*** • ***x*** + *b*.
  prefs: []
  type: TYPE_NORMAL
- en: We follow much the same procedure to find ∂*y*/∂*b*, as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/220equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: but ∂*y*/∂*z* is 0 or 1, depending on the *z*’s sign. Likewise, ∂*z*/∂*b* =
    1, which leads to
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/220equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this dense chapter, we learned about matrix calculus, including working with
    derivatives of functions involving vectors and matrices. We worked through the
    definitions and discussed some identities. We then introduced the Jacobian and
    Hessian matrices as analogs for first and second derivatives and learned how to
    use them in optimization problems. Training a deep neural network is, fundamentally,
    an optimization problem, so the potential utility of the Jacobian and Hessian
    is clear, even if the latter can’t be easily used for large neural networks. We
    ended the chapter with some examples for derivatives of expressions found in deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the general mathematics portion of the book. We’ll now turn our
    attention to using what we’ve learned to understand the workings of deep neural
    networks. Let’s begin with a discussion of how data flows through a neural network
    model.
  prefs: []
  type: TYPE_NORMAL
