<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_127"/><strong><span class="big">6</span><br/>MORE LINEAR ALGEBRA</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In this chapter, we’ll continue our exploration of linear algebra concepts. Some of these concepts are only tangentially related to deep learning, but they’re the sort of math you’ll eventually encounter. Think of this chapter as assumed background knowledge.</p>&#13;
<p class="indent">Specifically, we’ll learn more about the properties of and operations on square matrices, introducing terms you’ll encounter in the deep learning literature. After that, I’ll introduce the ideas behind the eigenvalues and eigenvectors of a square matrix and how to find them. Next, we’ll explore vector norms and other ways of measuring distance that are often encountered in deep learning. At that point, I’ll introduce the important concept of a covariance matrix.</p>&#13;
<p class="indent">We’ll conclude the chapter by demonstrating principal component analysis (PCA) and singular value decomposition (SVD). These frequently used approaches depend heavily on the concepts and operators introduced throughout the chapter. We will see what PCA is, how to do it, and what it can buy us from a machine learning perspective. Similarly, we will work with SVD and see how we can use it to implement PCA as well as compute the pseudoinverse of a rectangular matrix.</p>&#13;
<h3 class="h3" id="ch06lev1_1"><span epub:type="pagebreak" id="page_128"/>Square Matrices</h3>&#13;
<p class="noindent">Square matrices occupy a special place in the world of linear algebra. Let’s explore them in more detail. The terms used here will show up often in deep learning and other areas.</p>&#13;
<h4 class="h4" id="ch06lev2_1">Why Square Matrices?</h4>&#13;
<p class="noindent">If we multiply a matrix by a column vector, we’ll get another column vector as output:</p>&#13;
<div class="imagec"><img src="Images/128equ01.jpg" alt="Image" width="250" height="92"/></div>&#13;
<p class="indent">Interpreted geometrically, the 2 × 4 matrix has mapped the 4 × 1 column vector, a point in ℝ<sup>4</sup>, to a new point in ℝ<sup>2</sup>. The mapping is linear because the point values are only being multiplied by the elements of the 2 × 4 matrix; there are no nonlinear operations, such as raising the components of the vector to a power, for example.</p>&#13;
<p class="indent">Viewed this way, we can use a matrix to transform points between spaces. If the matrix is square, say, <em>n</em> × <em>n</em>, the mapping is from ℝ<sup><em>n</em></sup> back to ℝ<sup><em>n</em></sup>. For example, consider</p>&#13;
<div class="imagec"><img src="Images/128equ02.jpg" alt="Image" width="226" height="68"/></div>&#13;
<p class="noindent">where the point (11, 12, 13) is mapped to the point (74, 182, 209), both in ℝ<sup>3</sup>.</p>&#13;
<p class="indent">Using a matrix to map points from one space to another makes it possible to rotate a set of points about an axis by using a <em>rotation matrix</em>. For simple rotations, we can define matrices in 2D,</p>&#13;
<div class="imagec" id="ch06equ01"><img src="Images/06equ01.jpg" alt="Image" width="453" height="51"/></div>&#13;
<p class="noindent">and in 3D,</p>&#13;
<div class="imagec"><img src="Images/128equ03.jpg" alt="Image" width="575" height="79"/></div>&#13;
<p class="noindent">Rotations are by an angle, <em>θ</em>, and for 3D, about the x-, y-, or z-axis, as indicated by the subscript.</p>&#13;
<p class="indent">Using a matrix, we can create an <em>affine transformation</em>. An affine transformation maps a set of points into another set of points so that points on a <span epub:type="pagebreak" id="page_129"/>line in the original space are still on a line in the mapped space. The transformation is</p>&#13;
<p class="center"><em><strong>y</strong></em> = <em><strong>Ax</strong></em> + <em><strong>b</strong></em></p>&#13;
<p class="noindent">The affine transform combines a matrix transform, <em><strong>A</strong></em>, with a translation, <em><strong>b</strong></em>, to map a vector, <em><strong>x</strong></em>, to a new vector, <em><strong>y</strong></em>. We can combine this operation into a single matrix multiplication by putting <em><strong>A</strong></em> in the upper-left corner of the matrix and adding <em><strong>b</strong></em> as a new column on the right. A row of all zeros at the bottom with a single 1 in the rightmost column completes the augmented transformation matrix. For an affine transformation matrix</p>&#13;
<div class="imagec"><img src="Images/129equ01.jpg" alt="Image" width="54" height="51"/></div>&#13;
<p class="noindent">and translation vector</p>&#13;
<div class="imagec"><img src="Images/129equ02.jpg" alt="Image" width="20" height="51"/></div>&#13;
<p class="noindent">we get</p>&#13;
<div class="imagec"><img src="Images/129equ03.jpg" alt="Image" width="200" height="70"/></div>&#13;
<p class="noindent">This form maps a point, <em>(x</em>, <em>y</em>), to a new point, (<em>x</em>′, <em>y</em>′).</p>&#13;
<p class="indent">This maneuver is identical to the <em>bias trick</em> sometimes used when implementing a neural network to bury the bias in an augmented weight matrix by including an extra feature vector input set to 1. In fact, we can view a feedforward neural network as a series of affine transformations, where the transformation matrix is the weight matrix between the layers, and the bias vector provides the translation. The activation function at each layer alters the otherwise linear relationship between the layers. It is this nonlinearity that lets the network learn a new way to map inputs so that the final output reflects the functional relationship the network is designed to learn.</p>&#13;
<p class="indent">We use square matrices, then, to map points from one space back into the same space, for example to rotate them about an axis. Let’s look now at some special properties of square matrices.</p>&#13;
<h4 class="h4" id="ch06lev2_2">Transpose, Trace, and Powers</h4>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05">Chapter 5</a> showed us the vector transpose to move between column and row vectors. The transpose operation is not restricted to vectors. It works for any matrix by flipping the rows and columns along the main diagonal. For example,</p>&#13;
<div class="imagec"><img src="Images/129equ04.jpg" alt="Image" width="480" height="91"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_130"/>The transpose is formed by flipping the indices of the matrix elements:</p>&#13;
<p class="center"><em>a<sub>ij</sub></em> ← <em>a<sub>ij</sub></em>, <em>i</em> = 0, 1, . . . , <em>n</em> − 1, <em>j</em> = 0, 1, . . . , <em>m</em> – 1</p>&#13;
<p class="noindent">This changes an <em>n</em> × <em>m</em> matrix into an <em>m</em> × <em>n</em> matrix. Notice that the order of a square matrix remains the same under the transpose operation, and the values on the main diagonal don’t change.</p>&#13;
<p class="indent">In NumPy, you can call the <code>transpose</code> method on an array, but the transpose is so common that a shorthand notation (<code>.T</code>) also exists. For example,</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import numpy as np</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">a = np.array([[1,2,3],[4,5,6],[7,8,9]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a)</span><br/>&#13;
[[1 2 3]<br/>&#13;
 [4 5 6]<br/>&#13;
 [7 8 9]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a.transpose())</span><br/>&#13;
[[1 4 7]<br/>&#13;
 [2 5 8]<br/>&#13;
 [3 6 9]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a.T)</span><br/>&#13;
[[1 4 7]<br/>&#13;
 [2 5 8]<br/>&#13;
 [3 6 9]]</pre>&#13;
<p class="indent">The <em>trace</em> is another common operation applied to square matrices:</p>&#13;
<div class="imagec"><img src="Images/130equ01.jpg" alt="Image" width="110" height="57"/></div>&#13;
<p class="indent">As an operator, the trace has certain properties. For example, it’s linear:</p>&#13;
<p class="center">tr(<em><strong>A</strong></em> + <em><strong>B</strong></em>) = tr<em><strong>A</strong></em> + tr<em><strong>B</strong></em></p>&#13;
<p class="noindent">It’s also true that tr(<em><strong>A</strong></em>) = tr(<em><strong>A</strong></em><em><sup>T</sup></em>) and tr(<em><strong>AB</strong></em>) = tr(<em><strong>BA</strong></em>).</p>&#13;
<p class="indent">NumPy uses <code>np.trace</code> to quickly calculate the trace of a matrix and <code>np .diag</code> to return the diagonal elements of a matrix as a 1D array,</p>&#13;
<p class="center">(<em>a</em><sub>00</sub>, <em>a</em><sub>11</sub>, . . . , <em>a<sub>n−1,n−1</sub></em>)</p>&#13;
<p class="noindent">for an <em>n</em> × <em>n</em> or <em>n</em> × <em>m</em> matrix.</p>&#13;
<p class="indent">A matrix doesn’t need to be square for NumPy to return the elements along its diagonal. And although mathematically the trace generally only applies to square matrices, NumPy will calculate the trace of any matrix, returning the sum of the diagonal elements:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">b = np.array([[1,2,3,4],[5,6,7,8]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(b)</span><br/>&#13;
<span epub:type="pagebreak" id="page_131"/>&#13;
[[1 2 3 4]<br/>&#13;
 [5 6 7 8]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.diag(b))</span><br/>&#13;
[1 6]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.trace(b))</span><br/>&#13;
7</pre>&#13;
<p class="indent">Lastly, you can multiply a square matrix by itself, implying that you can raise a square matrix to an integer power, <em>n</em>, by multiplying itself <em>n</em> times. Note that this is not the same as raising the elements of the matrix to a power. For example,</p>&#13;
<div class="imagec"><img src="Images/131equ01.jpg" alt="Image" width="469" height="109"/></div>&#13;
<p class="indent">The matrix power follows the same rules as raising any number to a power:</p>&#13;
<p class="center"><em><strong>A</strong><sup>n</sup><strong>A</strong><sup>m</sup></em> = <em><strong>A</strong><sup>n+m</sup></em></p>&#13;
<p class="center">(<em><strong>A</strong><sup>n</sup></em>)<sup><em>m</em></sup> = <em><strong>A</strong><sup>nm</sup></em></p>&#13;
<p class="noindent">for <span class="middle"><img src="Images/131equ01a.jpg" alt="Image" width="79" height="21"/></span> (positive integers) and where <em><strong>A</strong></em> is a square matrix.</p>&#13;
<p class="indent">NumPy provides a function to compute the power of a square matrix more efficiently than repeated calls to <code>np.dot</code>:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from numpy.linalg import matrix_power</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">a = np.array([[1,2],[3,4]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(matrix_power(a,2))</span><br/>&#13;
[[ 7 10]<br/>&#13;
 [15 22]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(matrix_power(a,10))</span><br/>&#13;
[[ 4783807 6972050]<br/>&#13;
 [10458075 15241882]]</pre>&#13;
<p class="indent">Now let’s consider some special square matrices that you’ll encounter from time to time.</p>&#13;
<h4 class="h4" id="ch06lev2_3">Special Square Matrices</h4>&#13;
<p class="noindent">Many square (and nonsquare) matrices have received special names. Some are rather obvious, like matrices that are all zero or one, which are called <em>zeros matrices</em> and <em>ones matrices</em>, respectively. NumPy uses these extensively:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(np.zeros((3,5)))</span><br/>&#13;
[[0. 0. 0. 0. 0.]<br/>&#13;
 [0. 0. 0. 0. 0.]<br/>&#13;
<span epub:type="pagebreak" id="page_132"/>&#13;
 [0. 0. 0. 0. 0.]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.ones(3,3))</span><br/>&#13;
[[1. 1. 1.]<br/>&#13;
 [1. 1. 1.]<br/>&#13;
 [1. 1. 1.]]</pre>&#13;
<p class="noindent">Note that you can find a matrix of any constant value, <em>c</em>, by multiplying the ones matrix by <em>c</em>.</p>&#13;
<p class="indent">Notice above that NumPy defaults to matrices of 64-bit floating-point numbers corresponding to a C-language type of <code>double</code>. See <a href="ch01.xhtml#ch01tab01">Table 1-1</a> on page 6 for a list of possible numeric data types. You can specify the desired data type with the <code>dtype</code> keyword. In pure mathematics, we don’t care much about data types, but to work in deep learning, you need to pay attention to avoid defining arrays that are far more memory-hungry than needed. Many deep learning models are happy with arrays of 32-bit floats, which use half the memory per element than the NumPy default. Also, many toolkits make use of new or previously seldom-used data types, like 16-bit floats, to allow for even better use of memory. NumPy does support 16-bit floats by specifying <code>float16</code> as the <code>dtype</code>.</p>&#13;
<h4 class="h4" id="ch06lev2_4">The Identity Matrix</h4>&#13;
<p class="noindent">By far, the most important special matrix is the <em>identity matrix</em>. This is a square matrix with all ones on the diagonal:</p>&#13;
<div class="imagec" id="ch06equ02"><img src="Images/06equ02.jpg" alt="Image" width="449" height="142"/></div>&#13;
<p class="indent">The identity matrix acts like the number 1 when multiplying a matrix. Therefore,</p>&#13;
<p class="center"><em><strong>AI</strong></em> = <em><strong>IA</strong></em> = <em><strong>A</strong></em></p>&#13;
<p class="noindent">for an <em>n</em> × <em>n</em> square matrix <em><strong>A</strong></em> and an <em>n</em> × <em>n</em> identity matrix <em><strong>I</strong></em>. When necessary, we’ll add a subscript to indicate the order of the identity matrix, for example, <em><strong>I</strong><sub>n</sub></em>.</p>&#13;
<p class="indent">NumPy uses <code>np.identity</code> or <code>np.eye</code> to generate identity matrices of a given size:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">a = np.array([[1,2],[3,4]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">i = np.identity(2)</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(i)</span><br/>&#13;
[[1. 0.]<br/>&#13;
 [0. 1.]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a @ i)</span><br/>&#13;
<span epub:type="pagebreak" id="page_133"/>&#13;
[[1. 2.]<br/>&#13;
 [3. 4.]]</pre>&#13;
<p class="indent">Look carefully at the example above. Mathematically, we said that multiplication of a square matrix by the identity matrix of the same order returns the matrix. NumPy, however, did something we might not want. Matrix <code>a</code> was defined with integer elements, so it has a data type of <code>int64</code>, the NumPy default for integers. However, since we didn’t explicitly provide <code>np.identity</code> with a data type, NumPy defaulted to a 64-bit float. Therefore, matrix multiplication (<code>@</code>) between <code>a</code> and <code>i</code> returned a floating-point version of <code>a</code>. This subtle change of data type might be important for later calculations, so, again, we need to pay attention to data types when using NumPy.</p>&#13;
<p class="indent">It doesn’t matter if you use <code>np.identity</code> or <code>np.eye</code>. In fact, internally, <code>np.identity</code> is just a wrapper for <code>np.eye</code>.</p>&#13;
<h5 class="h5" id="ch06lev3_1">Triangular Matrices</h5>&#13;
<p class="noindent">Occasionally, you’ll hear about <em>triangular</em> matrices. There are two kinds: upper and lower. As you may intuit from the name, an upper triangular matrix is one with nonzero elements in the part on or above the main diagonal, whereas a lower triangular matrix only has elements on or below the main diagonal. For example,</p>&#13;
<div class="imagec"><img src="Images/133equ01.jpg" alt="Image" width="177" height="93"/></div>&#13;
<p class="noindent">is an upper triangular matrix, whereas</p>&#13;
<div class="imagec"><img src="Images/133equ02.jpg" alt="Image" width="177" height="92"/></div>&#13;
<p class="noindent">is a lower triangular matrix. A matrix that has elements only on the main diagonal is, not surprisingly, a <em>diagonal matrix</em>.</p>&#13;
<p class="indent">NumPy has two functions, <code>np.triu</code> and <code>np.tril</code>, to return the upper or lower triangular part of the given matrix, respectively. So,</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">a = np.arange(16).reshape((4,4))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a)</span><br/>&#13;
[[ 0  1  2  3]<br/>&#13;
 [ 4  5  6  7]<br/>&#13;
 [ 8  9 10 11]<br/>&#13;
 [12 13 14 15]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.triu(a))</span><br/>&#13;
[[ 0 1  2  3]<br/>&#13;
 [ 0 5  6  7]<br/>&#13;
 [ 0 0 10 11]<br/>&#13;
<span epub:type="pagebreak" id="page_134"/>&#13;
 [ 0 0 0 15]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.tril(a))</span><br/>&#13;
[[ 0  0  0  0]<br/>&#13;
 [ 4  5  0  0]<br/>&#13;
 [ 8  9 10  0]<br/>&#13;
 [12 13 14 15]]</pre>&#13;
<p class="indent">We don’t frequently use triangular matrices in deep learning, but we do use them in linear algebra, in part to compute determinants, to which we now turn.</p>&#13;
<h4 class="h4" id="ch06lev2_5">Determinants</h4>&#13;
<p class="noindent">We can think of the <em>determinant</em> of a square matrix, <em>n</em> × <em>n</em>, as a function mapping square matrices to a scalar. The primary use of the determinant in deep learning is to compute the eigenvalues of a matrix. We’ll see what that means later in this chapter, but for now think of eigenvalues as special scalar values associated with a matrix. The determinant also tells us something about whether or not a matrix has an inverse, as we’ll also see below. Notationally, we write the determinant of a matrix with vertical bars. For example, if <em><strong>A</strong></em> is a 3 × 3 matrix, we write the determinant as</p>&#13;
<div class="imagec"><img src="Images/134equ01.jpg" alt="Image" width="262" height="76"/></div>&#13;
<p class="noindent">where we state explicitly that the value of the determinant is a scalar (element of ℝ). All square matrices have a determinant. For now, let’s consider some of the properties of the determinant:</p>&#13;
<ol>&#13;
<li><p>If any row or column of <em><strong>A</strong></em> is zero, then det(<em><strong>A</strong></em>) = 0.</p></li>&#13;
<li><p>If any two rows of <em><strong>A</strong></em> are identical, then det(<em><strong>A</strong></em>) = 0.</p></li>&#13;
<li><p>If <em><strong>A</strong></em> is an upper or lower triangular, then det <img src="Images/134equ02.jpg" alt="Image" width="127" height="30"/>.</p></li>&#13;
<li><p>If <em><strong>A</strong></em> is a diagonal matrix, then det <img src="Images/134equ03.jpg" alt="Image" width="128" height="31"/>.</p></li>&#13;
<li><p>The determinant of the identity matrix, regardless of size, is 1.</p></li>&#13;
<li><p>The determinant of a product of matrices is the product of the determinants, det(<em><strong>AB</strong></em>) = det(<em><strong>A</strong></em>)det(<em><strong>B</strong></em>).</p></li>&#13;
<li><p>det(<em><strong>A</strong></em>) = det(<em><strong>A</strong></em><sup>⊤</sup>).</p></li>&#13;
<li><p>det(<em><strong>A</strong><sup>n</sup></em>) = det(<em><strong>A</strong></em>)<sup><em>n</em></sup>.</p></li>&#13;
</ol>&#13;
<p class="noindent">Property 7 indicates that the transpose operation does not change the value of a determinant. Property 8 is a consequence of Property 6.</p>&#13;
<p class="indent">We have multiple ways we can calculate the determinant of a square matrix. We’ll examine only one way here, which involves using a recursive formula. All recursive formulas apply themselves, just as recursive functions in code call themselves. The general idea is that each recursion works on a <span epub:type="pagebreak" id="page_135"/>simpler version of the problem, which can be combined to return the solution to the larger problem.</p>&#13;
<p class="indent">For example, we can calculate the factorial of an integer,</p>&#13;
<p class="center"><em>n</em>! = <em>n</em>(<em>n</em> − 1)(<em>n</em> − 2)(<em>n</em> − 3) . . . 1</p>&#13;
<p class="noindent">recursively if we notice the following:</p>&#13;
<div class="imagec"><img src="Images/135equ01.jpg" alt="Image" width="150" height="80"/></div>&#13;
<p class="indent">The first statement says that the factorial of <em>n</em> is <em>n</em> times the factorial of (<em>n</em> − 1). The second statement says that the factorial of zero is one. The recursion is the first statement, but this recursion will never end without some condition that returns a value. That’s the point of the second statement, the <em>base case</em>: it says the recursion ends when we get to zero.</p>&#13;
<p class="indent">This might be clearer in code. We can define the factorial like so:</p>&#13;
<pre>&#13;
def factorial(n):<br/>&#13;
    if (n == 0):<br/>&#13;
        return 1<br/>&#13;
    return n*factorial(n-1)</pre>&#13;
<p class="indent">Notice that <code>factorial</code> calls itself on the argument minus one, unless the argument is zero, in which case it immediately returns one. The code works because of the Python call stack. The call stack keeps track of all the computations of <code>n*factorial(n-1)</code>. When we encounter the base case, all the pending multiplications are done, and we return the final value.</p>&#13;
<p class="indent">To calculate determinants recursively, then, we need a recursion statement, something that defines the determinant of a matrix in terms of simpler determinants. We also need a base case that gives us a definitive value. For determinants, the base case is when we get to a 1 × 1 matrix. For any 1 × 1 matrix, <em><strong>A</strong></em>, we have</p>&#13;
<p class="center">det(<em><strong>A</strong></em>) = <em>a</em><sub>00</sub></p>&#13;
<p class="noindent">meaning the determinant of a 1 × 1 matrix is the single value it contains.</p>&#13;
<p class="indent">Our plan is to calculate the determinant by breaking the calculation into successively simpler determinants until we reach the base case above. To do this, we need a statement involving recursion. However, we need to define a few things before we can make the statement. First, we need to define the <em>minor</em> of a matrix. The (<em>i</em>, <em>j</em>)-minor of a matrix, <em><strong>A</strong></em>, is the matrix left after removing the <em>i</em>th row and <em>j</em>th column of <em><strong>A</strong></em>. We’ll denote a minor matrix by <em><strong>A</strong></em><em><sub>ij</sub></em>. For example, given</p>&#13;
<div class="imagec"><img src="Images/135equ02.jpg" alt="Image" width="137" height="68"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_136"/>then</p>&#13;
<div class="imagec"><img src="Images/136equ01.jpg" alt="Image" width="249" height="69"/></div>&#13;
<p class="noindent">where the minor, <em><strong>A</strong></em><sub>11</sub>, is found by deleting row 1 and column 1 to leave only the underlined values.</p>&#13;
<p class="indent">Second, we need to define the <em>cofactor</em>, <em>C</em><em><sub>ij</sub></em>, of the minor, <em><strong>A</strong></em><em><sub>ij</sub></em>. This is where our recursive statement appears. The definition is</p>&#13;
<p class="center"><em>C</em><em><sub>ij</sub></em> = (−1)<sup><em>i</em>+<em>j</em>+2</sup>det(<em><strong>A</strong></em><em><sub>ij</sub></em>)</p>&#13;
<p class="noindent">The cofactor depends on the determinant of the minor. Notice the exponent on −1, written as <em>i</em> + <em>j</em> + 2. If you look at most math books, you’ll see the exponent as <em>i</em> + <em>j</em>. We’ve made a conscious choice to define matrices with zero-based indices so the math and implementation in code match without being off by one. Here’s one place where that choice forces us to be less elegant than the math texts. Because our indices are “off” by one, we need to add that one back into the exponent of the cofactor so the pattern of positive and negative values that the cofactor uses is correct. This means adding one to each of the variables in the exponent: <em>i</em> → <em>i</em> + 1 and <em>j</em> → <em>j</em> + 1. This makes the exponent <em>i</em> + <em>j</em> → (<em>i</em> + 1) + (<em>j</em> + 1) = <em>i</em> + <em>j</em> + 2.</p>&#13;
<p class="indent">We’re now ready for our full recursive definition of the determinant of <em><strong>A</strong></em> by using <em>cofactor expansion</em>. It turns out that summing the product of the matrix values and associated cofactors for any row or column of a square matrix will give us the determinant. So, we’ll use the first row of the matrix and calculate the determinant as</p>&#13;
<div class="imagec" id="ch06equ03"><img src="Images/06equ03.jpg" alt="Image" width="431" height="61"/></div>&#13;
<p class="indent">You may be wondering: Where’s the recursion in <a href="ch06.xhtml#ch06equ03">Equation 6.3</a>? It shows up on the determinant of the minor. If <em><strong>A</strong></em> is an <em>n</em> × <em>n</em> matrix, the minor, <em><strong>A</strong></em><em><sub>ij</sub></em>, is an <em>(n</em> − 1) × (<em>n</em> − 1) matrix. Therefore, to calculate the cofactors to find the determinant of an <em>n</em> × <em>n</em> matrix, we need to know how to find the determinant of an <em>(n</em> − 1) × (<em>n</em> − 1) matrix. However, we can use cofactor expansion to find the <em>(n</em> − 1) × (<em>n</em> − 1) determinant, which involves finding the determinant of an <em>(n</em> − 2) × (<em>n</em> − 2) matrix. This process continues until we get to a 1 × 1 matrix. We already know the determinant of a 1 × 1 matrix is the single value it contains.</p>&#13;
<p class="indent">Let’s work through this process for a 2 × 2 matrix:</p>&#13;
<div class="imagec"><img src="Images/136equ02.jpg" alt="Image" width="102" height="51"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_137"/>Using cofactor expansion, we get</p>&#13;
<div class="imagec"><img src="Images/137equ01.jpg" alt="Image" width="466" height="204"/></div>&#13;
<p class="noindent">which is the formula for the determinant of a 2 × 2 matrix. The minors of a 2 × 2 matrix are 1 × 1 matrices, each returning either <em>d</em> or <em>c</em> in this case.</p>&#13;
<p class="indent">In NumPy, we calculate determinants with <code>np.linalg.det</code>. For example,</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">a = np.array([[1,2],[3,4]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a)</span><br/>&#13;
[[1 2]<br/>&#13;
 [3 4]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.linalg.det(a)</span><br/>&#13;
-2.0000000000000004<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">1*4 - 2*3</span><br/>&#13;
-2</pre>&#13;
<p class="noindent">The last line of code uses the formula for a 2 × 2 matrix we derived above for comparison purposes. Internally, NumPy does not use recursive cofactor expansion to calculate the determinant. Instead, it factors the matrix into the product of three matrices: (1) a <em>permutation matrix</em>, which looks like a scrambled identity matrix with only a single one in each row and column, (2) a lower triangular matrix, and (3) an upper triangular matrix. The determinant of the permutation matrix is either +1 or −1. The determinant of a triangular matrix is the product of the diagonal elements, while the determinant of a product of matrices is the product of the per-matrix determinants.</p>&#13;
<p class="indent">We can use determinants to determine whether a matrix has an inverse. Let’s turn there now.</p>&#13;
<h4 class="h4" id="ch06lev2_6">Inverses</h4>&#13;
<p class="noindent"><a href="ch06.xhtml#ch06equ02">Equation 6.2</a> defines the identity matrix. We said that this matrix acts like the number 1, so when it multiplies a square matrix, the same square matrix is returned. When multiplying scalars, we know that for any number, <em>x</em> ≠ 0, there exists another number, call it <em>y</em>, such that <em>xy</em> = 1. This number is the multiplicative inverse of <em>x</em>. Furthermore, we know exactly what <em>y</em> is; it’s 1/<em>x</em> = <em>x</em><sup>−1</sup>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_138"/>By analogy, then, we might wonder if, since we have an identity matrix that acts like the number 1, there is another square matrix, call it <em><strong>A</strong></em><sup>−1</sup>, for a given square matrix, <em><strong>A</strong></em>, such that</p>&#13;
<p class="center"><em><strong>AA</strong></em><sup>−1</sup> = <em><strong>A</strong></em><sup>−1</sup> <em><strong>A</strong></em> = <em><strong>I</strong></em></p>&#13;
<p class="indent">If <em><strong>A</strong></em><sup>−1</sup> exists, it’s known as the <em>inverse matrix</em> of <em><strong>A</strong></em>, and <em><strong>A</strong></em> is said to be <em>invertable</em>. For real numbers, all numbers except zero have an inverse. For matrices, it isn’t so straightforward. Many square matrices don’t have inverses. To check if <em><strong>A</strong></em> has an inverse, we use the determinant: det(<em><strong>A</strong></em>) = 0 tells us that <em><strong>A</strong></em> has no inverse. Furthermore, if <em><strong>A</strong></em><sup>−1</sup> exists, then</p>&#13;
<div class="imagec"><img src="Images/138equ01.jpg" alt="Image" width="170" height="48"/></div>&#13;
<p class="noindent">Note also that (<em><strong>A</strong></em><sup>−1</sup>)<sup>−1</sup> = <em><strong>A</strong></em>, as is the case for real numbers. Another useful property of inverses is</p>&#13;
<p class="center">(<em><strong>AB</strong></em>)<sup>−1</sup> = <em><strong>B</strong></em><sup>−1</sup> <em><strong>A</strong></em><sup>−1</sup></p>&#13;
<p class="noindent">where the order of the product on the right-hand side is important. Finally, note that the inverse of a diagonal matrix is simply the reciprocal of the diagonal elements:</p>&#13;
<div class="imagec"><img src="Images/138equ02.jpg" alt="Image" width="390" height="71"/></div>&#13;
<p class="indent">It’s possible to calculate the inverse by hand using row operations, which we’ve conveniently ignored here because they are seldom used in deep learning. Cofactor expansion techniques can also calculate the inverse, but to save time, we won’t elaborate on the process here. What’s important for us is to know that square matrices often have an inverse, and that we can calculate inverses with NumPy via <code>np.linalg.inv</code>.</p>&#13;
<p class="indent">If a matrix is <em>not</em> invertible, the matrix is said to be <em>singular</em>. Therefore, the determinant of a singular matrix is zero. If a matrix has an inverse, it is a <em>nonsingular</em> or <em>nondegenerate</em> matrix.</p>&#13;
<p class="indent">In NumPy, we use <code>np.linalg.inv</code> to calculate the inverse of a square matrix. For example,</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">a = np.array([[1,2,1],[2,1,2],[1,2,2]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a)</span><br/>&#13;
[[1 2 1]<br/>&#13;
 [2 1 2]<br/>&#13;
 [1 2 2]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.linalg.inv(a)</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(b)</span><br/>&#13;
[[ 0.66666667 0.66666667 -1. ]<br/>&#13;
<span epub:type="pagebreak" id="page_139"/>&#13;
 [ 0.66666667 -0.33333333 0. ]<br/>&#13;
 [-1.          0.         1. ]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a @ b)</span><br/>&#13;
[[1. 0. 0.]<br/>&#13;
 [0. 1. 0.]<br/>&#13;
 [0. 0. 1.]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(b @ a)</span><br/>&#13;
[[1. 0. 0.]<br/>&#13;
 [0. 1. 0.]<br/>&#13;
 [0. 0. 1.]]</pre>&#13;
<p class="noindent">Notice the inverse (<code>b</code>) working as we expect and giving the identity matrix when multiplying <code>a</code> from the left or right.</p>&#13;
<h4 class="h4" id="ch06lev2_7">Symmetric, Orthogonal, and Unitary Matrices</h4>&#13;
<p class="noindent">If for a square matrix, <em><strong>A</strong></em>, we have</p>&#13;
<p class="center"><em><strong>A</strong></em><sup>⊤</sup> = <em><strong>A</strong></em></p>&#13;
<p class="noindent">then <em><strong>A</strong></em> is said to be a <em>symmetric matrix</em>. For example,</p>&#13;
<div class="imagec"><img src="Images/139equ01.jpg" alt="Image" width="169" height="92"/></div>&#13;
<p class="noindent">is a symmetric matrix, since <em><strong>A</strong></em><sup>⊤</sup> = <em><strong>A</strong></em>.</p>&#13;
<p class="indent">Notice that diagonal matrices are symmetric, and the product of two symmetric matrices is commutative: <em><strong>AB</strong></em> = <em><strong>BA</strong></em>. The inverse of a symmetric matrix, if it exists, is also a symmetric matrix.</p>&#13;
<p class="indent">If the following is true,</p>&#13;
<p class="center"><em><strong>AA</strong></em><sup>⊤</sup> = <em><strong>A</strong></em><sup>⊤</sup> <em><strong>A</strong></em> = <em><strong>I</strong></em></p>&#13;
<p class="noindent">then <em><strong>A</strong></em> is an orthogonal matrix. If <em><strong>A</strong></em> is an orthogonal matrix, then</p>&#13;
<p class="center"><em><strong>A</strong></em><sup>−1</sup> = <em><strong>A</strong></em><sup>⊤</sup></p>&#13;
<p class="noindent">and, as a result,</p>&#13;
<p class="center">det(<em><strong>A</strong></em>) = ±1</p>&#13;
<p class="indent">If the values in the matrix are allowed to be complex, which does not happen often in deep learning, and</p>&#13;
<p class="center"><em><strong>U</strong></em><sup>*</sup><em><strong>U</strong></em> = <em><strong>UU</strong></em><sup>*</sup> = <em><strong>I</strong></em></p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_140"/>then <em><strong>U</strong></em> is a <em>unitary matrix</em> with <em><strong>U</strong></em><sup>*</sup> being the <em>conjugate transpose</em> of <em><strong>U</strong></em>. The conjugate transpose is the ordinary matrix transpose followed by the complex conjugate operation to change <img src="Images/140equ01.jpg" alt="Image" width="78" height="21"/> to −<em>i</em>. So, we might have</p>&#13;
<div class="imagec"><img src="Images/140equ02.jpg" alt="Image" width="415" height="51"/></div>&#13;
<p class="indent">Sometimes, especially in physics, the conjugate transpose is called the <em>Hermitian adjoint</em> and is denoted as <em><strong>A</strong></em><sup>†</sup>. If a matrix is equal to its conjugate transpose, it is called a <em>Hermitian matrix</em>. Notice that real symmetric matrices are also Hermitian matrices because the conjugate transpose is the same as the ordinary transpose when the values are real numbers. Therefore, you might encounter the term <em>Hermitian</em> in place of <em>symmetric</em> when referring to matrices with real elements.</p>&#13;
<h4 class="h4" id="ch06lev2_8">Definiteness of a Symmetric Matrix</h4>&#13;
<p class="noindent">We saw at the beginning of this section that an <em>n</em> × <em>n</em> square matrix maps a vector in ℝ<sup><em>n</em></sup> to another vector in ℝ<sup><em>n</em></sup>. Let’s consider now a symmetric <em>n</em> × <em>n</em> matrix, <em><strong>B</strong></em>, with real-valued elements. We can characterize this matrix by how it maps vectors using the inner product between the mapped vector and the original vector. Specifically, if <em><strong>x</strong></em> is a column vector (<em>n</em> × 1), then <em><strong>Bx</strong></em> is also an <em>n</em> × 1 column vector. Therefore, the inner product of this vector and the original vector, <em><strong>x</strong></em>, is <em><strong>x</strong></em><sup>⊤</sup><em><strong>Bx</strong></em>, a scalar.</p>&#13;
<p class="indent">If the following is true:</p>&#13;
<div class="imagec"><img src="Images/140equ03.jpg" alt="Image" width="159" height="23"/></div>&#13;
<p class="noindent">then <em><strong>B</strong></em> is said to be <em>positive definite</em>. Here, the bolded <strong>0</strong> is the <em>n</em> × 1 column vector of all zeros, and ∀ is math notation meaning “for all.”</p>&#13;
<p class="indent">Similarly, if</p>&#13;
<div class="imagec"><img src="Images/140equ04.jpg" alt="Image" width="159" height="22"/></div>&#13;
<p class="noindent">then <em><strong>B</strong></em> is <em>negative definite</em>. Relaxing the inner product relationship and the nonzero requirement on <em><strong>x</strong></em> gives two additional cases. If</p>&#13;
<div class="imagec"><img src="Images/140equ05.jpg" alt="Image" width="186" height="22"/></div>&#13;
<p class="noindent">then <em><strong>B</strong></em> is said to be <em>positive semidefinite</em>, and</p>&#13;
<div class="imagec"><img src="Images/140equ06.jpg" alt="Image" width="186" height="22"/></div>&#13;
<p class="noindent">makes <em><strong>B</strong></em> a <em>negative semidefinite</em> matrix. Finally, a real square symmetric matrix that is neither positive nor negative semidefinite is called an <em>indefinite matrix</em>. The definiteness of a matrix tells us something about the eigenvalues, which we’ll learn more about in the next section. If a symmetric matrix is <span epub:type="pagebreak" id="page_141"/>positive definite, then all of its eigenvalues are positive. Similarly, a symmetric negative definite matrix has all negative eigenvalues. Positive and negative semidefinite symmetric matrices have eigenvalues that are all positive or zero or all negative or zero, respectively.</p>&#13;
<p class="indent">Let’s shift gears now from talking about types of matrices to discovering the importance of eigenvectors and eigenvalues, key properties of a matrix that we use frequently in deep learning.</p>&#13;
<h3 class="h3" id="ch06lev1_2">Eigenvectors and Eigenvalues</h3>&#13;
<p class="noindent">We learned above that a square matrix maps a vector into another vector in the same dimensional space, <em><strong>v</strong></em>′ = <em><strong>Av</strong></em>, where both <em><strong>v</strong></em>′ and <em><strong>v</strong></em> are <em>n</em>-dimensional vectors, if <em><strong>A</strong></em> is an <em>n</em> × <em>n</em> matrix.</p>&#13;
<p class="indent">Consider this equation,</p>&#13;
<div class="imagec" id="ch06equ04"><img src="Images/06equ04.jpg" alt="Image" width="384" height="20"/></div>&#13;
<p class="noindent">for some square matrix, <em><strong>A</strong></em>, where λ is a scalar value and <em><strong>v</strong></em> is a nonzero column vector.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06equ04">Equation 6.4</a> says that the vector, <em><strong>v</strong></em>, is mapped by <em><strong>A</strong></em> back into a scalar multiple of itself. We call <em><strong>v</strong></em> an <em>eigenvector</em> of <em><strong>A</strong></em> with <em>eigenvalue</em> λ. The prefix <em>eigen</em> comes from German and is often translated as “self,” “characteristic,” or even “proper.” Thinking geometrically, <a href="ch06.xhtml#ch06equ04">Equation 6.4</a> says that the action of <em><strong>A</strong></em> on its eigenvectors in ℝ<sup><em>n</em></sup> is to shrink or expand the vector without changing its direction. Note, while <em><strong>v</strong></em> is nonzero, it’s possible for λ to be zero.</p>&#13;
<p class="indent">How does <a href="ch06.xhtml#ch06equ04">Equation 6.4</a> relate to the identity matrix, <em><strong>I</strong></em> ? By definition, the identity matrix maps a vector back into itself without scaling it. Therefore, the identity matrix has an infinite number of eigenvectors, and all of them have an eigenvalue of 1, since, for any <em><strong>x</strong></em>, <em><strong>Ix</strong></em> = <em><strong>x</strong></em>. Therefore, the same eigenvalue may apply to more than one eigenvector.</p>&#13;
<p class="indent">Recall that <a href="ch06.xhtml#ch06equ01">Equation 6.1</a> defines a rotation matrix in 2D space for some given angle, <em>θ</em>. This matrix has no eigenvectors, because, for any nonzero vector, it rotates the vector by <em>θ</em>, so it can never map a vector back into its original direction. Therefore, not every matrix has eigenvectors.</p>&#13;
<h4 class="h4" id="ch06lev2_9">Finding Eigenvalues and Eigenvectors</h4>&#13;
<p class="noindent">To find the eigenvalues of a matrix, if there are any, we go back to <a href="ch06.xhtml#ch06equ04">Equation 6.4</a> and rewrite it:</p>&#13;
<div class="imagec" id="ch06equ05"><img src="Images/06equ05.jpg" alt="Image" width="512" height="22"/></div>&#13;
<p class="noindent">We can insert the identity matrix, <em><strong>I</strong></em>, between λ and <em><strong>v</strong></em> because <em><strong>Iv</strong></em> = <em><strong>v</strong></em>. Therefore, to find the eigenvalues of <em><strong>A</strong></em>, we need to find values of λ that cause the matrix <strong><em>A</em></strong> − λ<em><strong>I</strong></em> to map a nonzero vector, <em><strong>v</strong></em>, to the zero vector. <a href="ch06.xhtml#ch06equ05">Equation 6.5</a> only has solutions other than the zero vector if the determinant of <em><strong>A</strong></em> − λ<em><strong>I</strong></em> is zero.</p> &#13;
<p class="indent"><span epub:type="pagebreak" id="page_142"/>The above gives us a way to find the eigenvalues. For example, consider what <em><strong>A</strong></em> − λ<em><strong>I</strong></em> looks like for a 2 × 2 matrix:</p>&#13;
<div class="imagec"><img src="Images/142equ01.jpg" alt="Image" width="252" height="229"/></div>&#13;
<p class="indent">We learned above that the determinant of a 2 × 2 matrix has a simple form; therefore, the determinant of the matrix above is</p>&#13;
<p class="center">det(<em><strong>A</strong></em> − λ<em><strong>I</strong></em>) = (<em>a</em> − λ)(<em>d</em> − λ) − <em>bc</em></p>&#13;
<p class="indent">This equation is a second-degree polynomial in λ. Since we need the determinant to be zero, we set this polynomial to zero and find the roots. The roots are the eigenvalues of <em><strong>A</strong></em>. The polynomial that this process finds is called the <em>characteristic polynomial</em>, and <a href="ch06.xhtml#ch06equ05">Equation 6.5</a> is the <em>characteristic equation</em>. Notice above that the characteristic polynomial is a second-degree polynomial. In general, the characteristic polynomial of an <em>n</em> × <em>n</em> matrix is of degree <em>n</em>, so a matrix has at most <em>n</em> distinct eigenvalues, since an <em>n</em>th degree polynomial has at most <em>n</em> roots.</p>&#13;
<p class="indent">Once we know the roots of the characteristic polynomial, we can go back to <a href="ch06.xhtml#ch06equ05">Equation 6.5</a>, substitute each root for λ, and solve to find the associated eigenvectors, the <em><strong>v</strong></em>’s of <a href="ch06.xhtml#ch06equ05">Equation 6.5</a>.</p>&#13;
<p class="indent">The eigenvalues of a triangular matrix, which includes diagonal matrices, are straightforward to calculate because the determinant of such a matrix is simply the product of the main diagonal. For example, for a 4 × 4 triangular matrix, the determinant of the characteristic equation is</p>&#13;
<p class="center">det(<em><strong>A</strong></em> − λ<em><strong>I</strong></em>) = (<em>a</em><sub>00</sub> − λ)(<em>a</em><sub>11</sub> − λ)(<em>a</em><sub>22</sub> − λ)(<em>a</em><sub>33</sub> − λ)</p>&#13;
<p class="noindent">which has four roots: the values of the diagonal. For triangular and diagonal matrices, the entries on the main diagonal <em>are</em> the eigenvalues.</p>&#13;
<p class="indent">Let’s see a worked eigenvalue example for the following matrix:</p>&#13;
<div class="imagec"><img src="Images/142equ02.jpg" alt="Image" width="134" height="51"/></div>&#13;
<p class="noindent">I selected this matrix to make the math nicer, but the process works for any matrix. The characteristic equation means we need the λ values that make the determinant zero, as shown next.</p>&#13;
<span epub:type="pagebreak" id="page_143"/>&#13;
<div class="imagec"><img src="Images/143equ01.jpg" alt="Image" width="309" height="177"/></div>&#13;
<p class="noindent">The characteristic polynomial is easily factored to give λ = −1, −2.</p> &#13;
<p class="indent">In code, to find the eigenvalues and eigenvectors of a matrix, we use <code>np.linalg.eig</code>. Let’s check our calculation above to see if NumPy agrees:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">a = np.array([[0,1],[-2,-3]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.linalg.eig(a)[0])</span><br/>&#13;
[-1. -2.]</pre>&#13;
<p class="indent">The <code>np.linalg.eig</code> function returns a list. The first element is a vector of the eigenvalues of the matrix. The second element, which we are ignoring for the moment, is a matrix, the <em>columns</em> of which are the eigenvectors associated with each of the eigenvalues. Note that we also could have used <code>np.linalg.eigvals</code> to return just the eigenvalues. Regardless, we see that our calculation of the eigenvalues of <em><strong>A</strong></em> is correct.</p>&#13;
<p class="indent">To find the associated eigenvectors, we put each of the eigenvalues back into <a href="ch06.xhtml#ch06equ05">Equation 6.5</a> and solve for <em><strong>v</strong></em>. For example, for λ = −1, we get</p>&#13;
<div class="imagec"><img src="Images/143equ02.jpg" alt="Image" width="192" height="113"/></div>&#13;
<p class="noindent">which leads to the system of equations:</p>&#13;
<p class="center">    <em>v</em><sub>0</sub> + <em>v</em><sub>1</sub> = 0</p>&#13;
<p class="center">−2<em>v</em><sub>0</sub> − 2<em>v</em><sub>1</sub> = 0</p>&#13;
<p class="indent">This system has many solutions, as long as <em>v</em><sub>0</sub> = −<em>v</em><sub>1</sub>. That means we can pick <em>v</em><sub>0</sub> and <em>v</em><sub>1</sub>, as long as the relationship between them is preserved. Therefore, we have our eigenvector for</p>&#13;
<div class="imagec"><img src="Images/143equ03.jpg" alt="Image" width="167" height="50"/></div>&#13;
<p class="indent">If we repeat this process for λ = −2, we get the relationship between the components of <em><strong>v</strong></em><strong><sub>2</sub></strong> to be 2<em>v</em><sub>0</sub> = −<em>v</em><sub>1</sub>. Therefore, we select <img src="Images/143equ04.jpg" alt="Image" width="97" height="51"/> as the second eigenvector.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_144"/>Let’s see if NumPy agrees with us. This time, we’ll display the second list element returned by <code>np.linalg.eig</code>. This is a matrix where the columns of the matrix are the eigenvectors:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(np.linalg.eig(a)[1])</span><br/>&#13;
[[ 0.70710678 -0.4472136 ]<br/>&#13;
 [-0.70710678  0.89442719]]</pre>&#13;
<p class="noindent">Hmm . . . the columns of this matrix do not appear to match our selected eigenvectors. But don’t worry—we didn’t make a mistake. Recall that the eigenvectors were not uniquely determined, only the relationship between the components was determined. If we’d wanted to, we could have selected other values, as long as for one eigenvector they were of equal magnitude and opposite sign, and for the other they were in the ratio of 2:1 with opposite signs. What NumPy returns is a set of eigenvectors that are of unit length. So, to see that our hand calculation is correct, we need to make our eigenvectors unit vectors by dividing each component by the square root of the sum of the squares of the components. In code, it’s succinct, if a bit messy:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">np.array([1,-1])/np.sqrt((np.array([1,-1])**2).sum())</span><br/>&#13;
array([ 0.70710678, -0.70710678])<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.array([-1,2])/np.sqrt((np.array([-1,2])**2).sum())</span><br/>&#13;
array([-0.4472136, 0.89442719])</pre>&#13;
<p class="indent">Now we see that we’re correct. The unit vector versions of the eigenvectors do match the columns of the matrix NumPy returned.</p>&#13;
<p class="indent">We’ll use the eigenvectors and eigenvalues of a matrix often when we’re doing deep learning. For example, we’ll see them again later in the chapter when we investigate principal component analysis. But before we can learn about PCA, we need to change focus once again and learn about vector norms and distance metrics commonly used in deep learning, especially about the covariance matrix.</p>&#13;
<h3 class="h3" id="ch06lev1_3">Vector Norms and Distance Metrics</h3>&#13;
<p class="noindent">In common deep learning parlance, people are somewhat sloppy and use the terms <em>norm</em> and <em>distance</em> interchangeably. We can forgive them for doing so; the difference between the terms is small in practice, as we’ll see below.</p>&#13;
<p class="indent">A <em>vector norm</em> is a function that maps a vector, real or complex, to some value, <em>x</em> ∈ ℝ, <em>x</em> ≥ 0. A norm must satisfy some specific properties in a mathematical sense, but in practice, not everything that’s called a norm is, in fact, a norm. In deep learning, we usually use norms as distances between pairs of vectors. In practice, an important property for a distance measure is that the order of the inputs doesn’t matter. If <em>f</em>(<em>x</em>, <em>y</em>) is a distance, then <em>f</em>(<em>x</em>, <em>y</em>) = <em>f</em>(<em>y</em>, <em>x</em>). Again, this is not rigorously followed; for example, you’ll often see the Kullback-Leibler divergence (KL-divergence) used as a distance even though this property doesn’t hold.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_145"/>Let’s start with vector norms and see how we can easily use them as a distance measure between vectors. Then we’ll introduce the important concept of a covariance matrix, heavily used on its own in deep learning, and see how we can create a distance measure from it: the Mahalanobis distance. We’ll end the section by introducing the KL-divergence, which we can view as a measure between two discrete probability distributions.</p>&#13;
<h4 class="h4" id="ch06lev2_10">L-Norms and Distance Metrics</h4>&#13;
<p class="noindent">For an <em>n</em>-dimensional vector, <em><strong>x</strong></em>, we define the <em>p</em>-norm of the vector to be</p>&#13;
<div class="imagec" id="ch06equ06"><img src="Images/06equ06.jpg" alt="Image" width="438" height="72"/></div>&#13;
<p class="noindent">where <em>p</em> is a real number. Although we use <em>p</em> in the definition, people generally refer to these as L<em><sub>p</sub></em> norms. We saw one of these norms in <a href="ch05.xhtml#ch05">Chapter 5</a> when we defined the magnitude of a vector. In that case, we were calculating the <em>L2-norm</em>,</p>&#13;
<div class="imagec"><img src="Images/145equ01.jpg" alt="Image" width="346" height="38"/></div>&#13;
<p class="noindent">which is the square root of the inner product of <em><strong>x</strong></em> with itself.</p>&#13;
<p class="indent">The norms we use most often in deep learning are the L2-norm and the <em>L1-norm</em>,</p>&#13;
<div class="imagec"><img src="Images/145equ02.jpg" alt="Image" width="125" height="43"/></div>&#13;
<p class="noindent">which is nothing more than the sum of the absolute values of the components of <em><strong>x</strong></em>. Another norm you’ll encounter is the <em>L</em><em><sub>∞</sub>-norm</em>,</p>&#13;
<p class="center">L<sub>∞</sub> = max |<em>x</em><em><sub>i</sub></em>|</p>&#13;
<p class="noindent">the maximum absolute value of the components of <em><strong>x</strong></em>.</p>&#13;
<p class="indent">If we replace <em><strong>x</strong></em> with the difference of two vectors, <em><strong>x</strong></em> − <em><strong>y</strong></em>, we can treat the norms as distance measures between the two vectors. Alternatively, we can picture the process as computing the vector norm on the vector that is the difference between <em><strong>x</strong></em> and <em><strong>y</strong></em>.</p>&#13;
<p class="indent">Switching from norm to distance makes a trivial change in <a href="ch06.xhtml#ch06equ06">Equation 6.6</a>:</p>&#13;
<div class="imagec" id="ch06equ07"><img src="Images/06equ07.jpg" alt="Image" width="475" height="72"/></div>&#13;
<p class="indent">The L2-distance becomes</p>&#13;
<div class="imagec"><img src="Images/145equ03.jpg" alt="Image" width="553" height="39"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_146"/>This is the <em>Euclidean distance</em> between two vectors. The L1-distance is often called the <em>Manhattan distance</em> (also called <em>city block distance</em>, <em>boxcar distance</em>, or <em>taxicab distance</em>):</p>&#13;
<div class="imagec"><img src="Images/146equ01.jpg" alt="Image" width="154" height="43"/></div>&#13;
<p class="noindent">It’s so named because it corresponds to the length a taxicab would travel on the grid of streets in Manhattan. The L<sub>∞</sub>-distance is sometimes known as the <em>Chebyshev distance</em>.</p>&#13;
<p class="indent">Norm equations have other uses in deep learning. For example, weight decay, used in deep learning as a regularizer, uses the L2-norm of the weights of the model to keep the weights from getting too large. The L1-norm of the weights is also sometimes used as a regularizer.</p>&#13;
<p class="indent">Let’s move now to consider the important concept of a covariance matrix. It isn’t a distance metric itself but is used by one, and it will show up again later in the chapter.</p>&#13;
<h4 class="h4" id="ch06lev2_11">Covariance Matrices</h4>&#13;
<p class="noindent">If we have a collection of measurements on multiple variables, like a training set with feature vectors, we can calculate the variance of the features with respect to each other. For example, here’s a matrix of observations of four variables, one per row:</p>&#13;
<div class="imagec"><img src="Images/146equ02.jpg" alt="Image" width="231" height="117"/></div>&#13;
<p class="indent">In reality, <em><strong>X</strong></em> is the first five samples from the famous iris dataset. For the iris dataset, the features are measurements of the parts of iris flowers from three different species. You can load this dataset into NumPy using <code>sklearn</code>:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from sklearn import datasets</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">iris = datasets.load_iris()</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">X = iris.data[:5]</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">X</span><br/>&#13;
array([[5.1, 3.5, 1.4, 0.2],<br/>&#13;
       [4.9, 3.0, 1.4, 0.2],<br/>&#13;
       [4.7, 3.2, 1.3, 0.2],<br/>&#13;
       [4.6, 3.1, 1.5, 0.2],<br/>&#13;
       [5.0, 3.6, 1.4, 0.2]])</pre>&#13;
<p class="indent">We could calculate the standard deviation of each of the features, the columns of <em><strong>A</strong></em>, but that would only tell us about the variance of the values of that feature around its mean. Since we have multiple features, it would be nice to know something about how the features in, say, column zero and <span epub:type="pagebreak" id="page_147"/>column one vary together. To determine this, we need to calculate the <em>covariance matrix</em>. This matrix captures the variance of the individual features along the main diagonal. Meanwhile, the off-diagonal values represent how one feature varies as another varies—these are the covariances. Since there are four features, the covariance matrix, which is always square, is, in this case, a 4 × 4 matrix. We find the elements of the covariance matrix, Σ, by calculating</p>&#13;
<div class="imagec" id="ch06equ08"><img src="Images/06equ08.jpg" alt="Image" width="491" height="58"/></div>&#13;
<p class="noindent">assuming the rows of the matrix, <em><strong>X</strong></em>, are the observations, and the columns of <em><strong>X</strong></em> represent the different features. The means of the features across all rows are <img src="Images/147equ01.jpg" alt="Image" width="14" height="20"/> and <img src="Images/147equ02.jpg" alt="Image" width="14" height="23"/> for features <em>i</em> and <em>j</em>. Here, <em>n</em> is the number of observations, the number of rows in <em><strong>X</strong></em>. We can see that when <em>i</em> = <em>j</em>, the covariance value is the normal variance for that feature. When <em>i</em> ≠ <em>j</em>, the value is how <em>i</em> and <em>j</em> vary together. We often denote the covariance matrix as Σ, and it is always symmetric: ∑<sub><em>ij</em></sub> = ∑<em><sub>ji</sub></em>.</p>&#13;
<p class="indent">Let’s calculate some elements of the covariance matrix for <em><strong>X</strong></em> above. The per-feature means are <img src="Images/147equ03.jpg" alt="Image" width="212" height="21"/>. Let’s find the first row of Σ. This will tell us the variance of the first feature (column of <strong>X</strong>) and how that feature varies with the second, third, and fourth features. Therefore, we need to calculate ∑<sub>00</sub>, ∑<sub>01</sub>, ∑<sub>02</sub>, and ∑<sub>03</sub>:</p>&#13;
<div class="imagec"><img src="Images/147equ04.jpg" alt="Image" width="451" height="360"/></div>&#13;
<p class="indent">We can repeat this calculation for all the rows of Σ to give the complete covariance matrix:</p>&#13;
<div class="imagec"><img src="Images/147equ05.jpg" alt="Image" width="405" height="92"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_148"/>The elements along the diagonal represent the variance of the features of <em><strong>X</strong></em>. Notice that for the fourth feature of <em><strong>X</strong></em>, all the variances and covariances are zero. This makes sense because all the values for this feature in <em><strong>X</strong></em> are the same; there is no variance.</p>&#13;
<p class="indent">We can calculate the covariance matrix for a set of observations in code by using <code>np.cov</code>:</p>&#13;
<pre>&#13;
&gt;&gt;&gt;<span class="codestrong1"> print(np.cov(X, rowvar=False))</span><br/>&#13;
[[ 0.043   0.0365 -0.0025  0.    ]<br/>&#13;
 [ 0.0365  0.067  -0.0025  0.    ]<br/>&#13;
 [-0.0025 -0.0025  0.005   0.    ]<br/>&#13;
 [ 0.      0.      0.      0.    ]]</pre>&#13;
<p class="indent">Notice that the call to <code>np.cov</code> includes <code>rowvar=False</code>. By default, <code>np.cov</code> expects each row of its argument to be a variable and the columns to be the observations of that variable. This is the opposite of the usual way a set of observations is typically stored in a matrix for deep learning. Therefore, we use the <code>rowvar</code> keyword to tell NumPy that the rows, not the columns, are the observations.</p>&#13;
<p class="indent">I claimed above that the diagonal of the covariance matrix returns the variances of the features in <em><strong>X</strong></em>. NumPy has a function, <code>np.std</code>, to calculate the standard deviation, and squaring the output of this function should give us the variances of the features by themselves. For <em><strong>X</strong></em>, we get</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> print(np.std(X, axis=0)**2)</span><br/>&#13;
[0.0344 0.0536 0.004 0.     ]</pre>&#13;
<p class="indent">These variances don’t look like the diagonal of the covariance matrix. The difference is due to the <em>n</em> − 1 in the denominator of the covariance equation, <a href="ch06.xhtml#ch06equ08">Equation 6.8</a>. By default, <code>np.std</code> calculates what is known as a biased estimate of the sample variance. This means that instead of dividing by <em>n</em> − 1, it divides by <em>n</em>. To get <code>np.std</code> to calculate the unbiased estimator of the variance, we need to add the <code>ddof=1</code> keyword,</p>&#13;
<pre>&#13;
&gt;&gt;&gt;<span class="codestrong1"> print(np.std(X, axis=0, ddof=1)**2)</span><br/>&#13;
[0.043 0.067 0.005  0.   ]</pre>&#13;
<p class="noindent">then we’ll get the same values as along the diagonal of Σ.</p>&#13;
<p class="indent">Now that we know how to calculate the covariance matrix, let’s use it in a distance metric.</p>&#13;
<h4 class="h4" id="ch06lev2_12">Mahalanobis Distance</h4>&#13;
<p class="noindent">Above, we represented a dataset by a matrix where the rows of the dataset are observations and the columns are the values of variables that make up each observation. In machine learning terms, the rows are the feature vectors. As we saw above, we can calculate the mean of each feature across all the observations, and we can calculate the covariance matrix. With these values, we can define a distance metric called the <em>Mahalanobis distance</em>,</p>&#13;
<span epub:type="pagebreak" id="page_149"/>&#13;
<div class="imagec" id="ch06equ09"><img src="Images/06equ09.jpg" alt="Image" width="475" height="26"/></div>&#13;
<p class="noindent">where <em><strong>x</strong></em> is a vector, <strong>μ</strong> is the vector formed by the mean values of each feature, and Σ is the covariance matrix. Notice that this metric uses the <em>inverse</em> of the covariance matrix, not the covariance matrix itself.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch06equ09">Equation 6.9</a> is, in some sense, measuring the distance between a vector and a distribution with the mean vector <strong>μ</strong>. The dispersion of the distribution is captured in Σ. If there is no covariance between the features in the dataset and each feature has the same standard deviation, then Σ becomes the identity matrix, which is its own inverse. In that case, Σ<sup>−1</sup> effectively drops out of <a href="ch06.xhtml#ch06equ09">Equation 6.9</a>, and the Mahalanobis distance becomes the L2-distance (Euclidean distance).</p>&#13;
<p class="indent">Another way to think of the Mahalanobis distance is to replace <strong>μ</strong> with another vector, call it <em><strong>y</strong></em>, that comes from the same dataset as <em><strong>x</strong></em>. Then <em>D</em><sub>M</sub> is the distance between the two vectors, taking the variance of the dataset into account.</p>&#13;
<p class="indent">We can use the Mahalanobis distance to build a simple classifier. If, given a dataset, we calculate the mean feature vector of each class in the dataset (this vector is also called the <em>centroid</em>), we can use the Mahalanobis distance to assign a label to an unknown feature vector, <em><strong>x</strong></em>. We can do so by calculating all the Mahalanobis distances to the class centroids and assigning <em><strong>x</strong></em> to the class returning the smallest value. This type of classifier is sometimes called a <em>nearest centroid</em> classifier, and you’ll often see it implemented using the L2-distance in place of the Mahalanobis distance. Arguably, you can expect the Mahalanobis distance to be the better metric because it takes the variance of the dataset into account.</p>&#13;
<p class="indent">Let’s use the breast cancer dataset included with <code>sklearn</code> to build the nearest centroid classifier using the Mahalanobis distance. The breast cancer dataset has two classes: benign (0) and malignant (1). The dataset contains 569 observations, each of which has 30 features derived from histology slides. We’ll build two versions of the nearest centroid classifier: one using the Mahalanobis distance and the other using the Euclidean distance. Our expectation is that the classifier using the Mahalanobis distance will perform better.</p>&#13;
<p class="indent">The code we need is straightforward:</p>&#13;
<pre>&#13;
   import numpy as np<br/>&#13;
   from sklearn import datasets<br/>&#13;
<span class="ent">❶</span> from scipy.spatial.distance import mahalanobis<br/>&#13;
<br/>&#13;
   bc = datasets.load_breast_cancer()<br/>&#13;
   d = bc.data; l = bc.target<br/>&#13;
<span class="ent">❷</span> i = np.argsort(np.random.random(len(d)))<br/>&#13;
   d = d[i]; l = l[i]<br/>&#13;
   xtrn, ytrn = d[:400], l[:400]<br/>&#13;
   xtst, ytst = d[400:], l[400:]<br/>&#13;
<span epub:type="pagebreak" id="page_150"/>&#13;
<span class="ent">❸</span> i = np.where(ytrn == 0)<br/>&#13;
   m0 = xtrn[i].mean(axis=0)<br/>&#13;
   i = np.where(ytrn == 1)<br/>&#13;
   m1 = xtrn[i].mean(axis=0)<br/>&#13;
   S = np.cov(xtrn, rowvar=False)<br/>&#13;
   SI= np.linalg.inv(S)<br/>&#13;
<br/>&#13;
   def score(xtst, ytst, m, SI):<br/>&#13;
       nc = 0<br/>&#13;
       for i in range(len(ytst)):<br/>&#13;
           d = np.array([mahalanobis(xtst[i],m[0],SI),<br/>&#13;
                         mahalanobis(xtst[i],m[1],SI)])<br/>&#13;
           c = np.argmin(d)<br/>&#13;
           if (c == ytst[i]):<br/>&#13;
               nc += 1<br/>&#13;
       return nc / len(ytst)<br/>&#13;
<br/>&#13;
   mscore = score(xtst, ytst, [m0,m1], SI)<br/>&#13;
<span class="ent">❹</span> escore = score(xtst, ytst, [m0,m1], np.identity(30))<br/>&#13;
   print("Mahalanobis score = %0.4f" % mscore)<br/>&#13;
   print("Euclidean score = %0.4f" % escore)</pre>&#13;
<p class="indent">We start by importing the modules we need, including <code>mahalanobis</code> from SciPy <span class="ent">❶</span>. This function accepts two vectors and the inverse of a covariance matrix and returns <em>D</em><sub>M</sub>. We get the dataset next in <code>d</code> with labels in <code>l</code>. We randomize the order <span class="ent">❷</span> and pull out the first 400 observations as training data (<code>xtrn</code>) with labels (<code>ytrn</code>). We hold back the remaining observations for testing (<code>xtst</code>, <code>ytst</code>).</p>&#13;
<p class="indent">We <em>train</em> the model next. Training consists of pulling out all the observations belonging to each class <span class="ent">❸</span> and calculating <code>m0</code> and <code>m1</code>. These are the mean values of each of the 30 features for all class 0 and class 1 observations. We then calculate the covariance matrix of the entire training set (<code>S</code>) and its inverse (<code>SI</code>).</p>&#13;
<p class="indent">The <code>score</code> function takes the test observations, a list of the class mean vectors, and the inverse of the covariance matrix. It runs through each test observation and calculates the Mahalanobis distances (<code>d</code>). It then uses the smallest distance to assign the class label (<code>c</code>). If the assigned label matches the actual test label, we count it (<code>nc</code>). At the end of the function, we return the overall accuracy.</p>&#13;
<p class="indent">We call the <code>score</code> function twice. The first call uses the inverse covariance matrix (<code>SI</code>), while the second call uses an identity matrix, thereby making <code>score</code> calculate the Euclidean distance instead. Finally, we print both results.</p>&#13;
<p class="indent">The randomization of the dataset <span class="ent">❷</span> means that each time the code is run, it will output slightly different scores. Running the code 100 times gives the following mean scores (± the standard deviation).</p>&#13;
<span epub:type="pagebreak" id="page_151"/>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Distance</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Mean score</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Mahalanobis</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9595 ± 0.0142</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Euclidean</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8914 ± 0.0185</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">This clearly shows that using the Mahalanobis distance leads to better model performance, with about a 7 percent improvement in accuracy.</p>&#13;
<p class="indent">One recent use of the Mahalanobis distance in deep learning is to take the top-level embedding layer values, a vector, and use the Mahalanobis distance to detect out-of-domain or adversarial inputs. An <em>out-of-domain input</em> is one that is significantly different from the type of data the model was trained to use. An <em>adversarial input</em> is one where an adversary is deliberately attempting to fool the model by supplying an input that isn’t of class X but that the model will label as class X.</p>&#13;
<h4 class="h4" id="ch06lev2_13">Kullback-Leibler Divergence</h4>&#13;
<p class="noindent">The <em>Kullback-Leibler divergence (KL-divergence)</em>, or <em>relative entropy</em>, is a measure of the similarity between two probability distributions: the lower the value, the more similar the distributions.</p>&#13;
<p class="indent">If <em>P</em> and <em>Q</em> are discrete probability distributions, the KL-divergence is</p>&#13;
<div class="imagec"><img src="Images/151equ01.jpg" alt="Image" width="302" height="53"/></div>&#13;
<p class="noindent">where log<sub>2</sub> is the logarithm base-2. This is an information-theoretic measure; the output is in bits of information. Sometimes the natural log, ln, is used, in which case the measure is said to be in <em>nats</em>. The SciPy function that implements the KL-divergence is in <code>scipy.special</code> as <code>rel_entr</code>. Note that <code>rel_entr</code> uses the natural log, not log base-2. Note also that the KL-divergence isn’t a distance metric in the mathematical sense because it violates the symmetry property, <em>D</em><sub>KL</sub>(<em>P</em><em>||Q</em>) ≠ <em>D</em><sub>KL</sub>(<em>Q</em>||<em>P</em>), but that doesn’t stop people from using it as one from time to time.</p>&#13;
<p class="indent">Let’s see an example of how we might use the KL-divergence to measure between different discrete probability distributions. We’ll measure the divergence between two different binomial distributions and a uniform distribution. Then, we’ll plot the distributions to see if, visually, we believe the numbers.</p>&#13;
<p class="indent">To generate the distributions, we’ll take many draws from a uniform distribution with 12 possible outputs. We can do this quickly in code by using <code>np.random.randint</code>. Then, we’ll take draws from two different binomial distributions, <em>B</em>(12, 0.4) and <em>B</em>(12, 0.9), meaning 12 trials with probabilities of 0.4 and 0.9 per trial. We’ll generate histograms of the resulting draws, divide by the sum of the counts, and use the rescaled histograms as our probability distributions. We can then measure the divergences between them.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_152"/>The code we need is</p>&#13;
<pre>   from scipy.special import rel_entr<br/>&#13;
   N = 1000000<br/>&#13;
<span class="ent">❶</span> p = np.random.randint(0,13,size=N)<br/>&#13;
<span class="ent">❷</span> p = np.bincount(p)<br/>&#13;
<span class="ent">❸</span> p = p / p.sum()<br/>&#13;
   q = np.random.binomial(12,0.9,size=N)<br/>&#13;
   q = np.bincount(q)<br/>&#13;
   q = q / q.sum()<br/>&#13;
   w = np.random.binomial(12,0.4,size=N)<br/>&#13;
   w = np.bincount(w)<br/>&#13;
   w = w / w.sum()<br/>&#13;
   print(rel_entr(q,p).sum())<br/>&#13;
   print(rel_entr(w,p).sum())</pre>&#13;
<p class="indent">We load <code>rel_entr</code> from SciPy and set the number of draws for each distribution to 1,000,000 (<code>N</code>). The code to generate the respective probability distributions follows the same method for each distribution. We draw <code>N</code> samples from the distribution, starting with the uniform <span class="ent">❶</span>. We use <code>randint</code> because it returns integers in the range [0, 12] so we can match the discrete [0, 12] values that <code>binomial</code> returns for 12 trials. We get the histogram from the vector of draws by using <code>np.bincount</code>. This function counts the frequency of unique values in a vector <span class="ent">❷</span>. Finally, we change the counts into fractions by dividing the histogram by the sum <span class="ent">❸</span>. This gives us a 12-element vector in <code>p</code> representing the probability that <code>randint</code> will return the values 0 through 12. Assuming <code>randint</code> uses a good pseudorandom number generator, we expect the probabilities to be roughly equal for each value in <code>p</code>. (NumPy uses the Mersenne Twister pseudorandom number generator, one of the better ones out there, so we’re confident that we’ll get good results.)</p>&#13;
<p class="indent">We repeat this process, substituting <code>binomial</code> for <code>randint</code>, sampling from binomial distributions using probabilities of 0.9 and 0.4. Again, histogramming the draws and converting the counts to fractions gives us the remaining probability distributions, <code>q</code> and <code>w</code>, based on 0.9 and 0.4, respectively.</p>&#13;
<p class="indent">We are finally ready to measure the divergence. The <code>rel_entr</code> function is a bit different from other functions in that it does not return <em>D</em><sub>KL</sub> directly. Instead, it returns a vector of the same length as its arguments, where each element of the vector is part of the overall sum leading to <em>D</em><sub>KL</sub>. Therefore, to get the actual divergence number, we need to add the elements of this vector. So, we print the sum of the output of <code>rel_entr</code>, comparing the two binomial distributions to the uniform distribution.</p>&#13;
<p class="indent">The random nature of the draws means we’ll get slightly different numbers each time we run the code. One run gave</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Distributions</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Divergence</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>D</em><sub>KL</sub>(<em>Q</em><em>||P</em>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1826</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>D</em><sub>KL</sub>(<em>W||P</em>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6218</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_153"/>This shows that the binomial distribution with probability 0.9 diverges more from a uniform distribution than the binomial distribution with probability 0.4. Recall, the smaller the divergence, the closer the two probability distributions are to each other.</p>&#13;
<p class="indent">Do we believe this result? One way to check is visually, by plotting the three distributions and seeing if <em>B</em>(12, 0.4) looks more like a uniform distribution than <em>B</em>(12, 0.9) does. This leads to <a href="ch06.xhtml#ch06fig01">Figure 6-1</a>.</p>&#13;
<div class="image" id="ch06fig01"><img src="Images/06fig01.jpg" alt="image" width="694" height="519"/></div>&#13;
<p class="figcap"><em>Figure 6-1: Three different, discrete probability distributions: uniform (forward hash),</em> B(<em>12,0.4) (backward hash), and</em> B(<em>12,0.9) (horizontal hash)</em></p>&#13;
<p class="indent">Although it is clear that neither binomial distribution is particularly uniform, the <em>B</em>(12, 0.4) distribution is relatively centered in the range and spread across more values than the <em>B</em>(12, 0.9) distribution is. It seems reasonable to think of <em>B</em>(12, 0.4) as more like the uniform distribution, which is precisely what the KL-divergence told us by returning a smaller value.</p>&#13;
<p class="indent">We now have everything we need to implement principal component analysis.</p>&#13;
<h3 class="h3" id="ch06lev1_4">Principal Component Analysis</h3>&#13;
<p class="noindent">Assume we have a matrix, <em><strong>X</strong></em>, representing a dataset. We understand that the variance of each of the features need not be the same. If we think of each observation as a point in an <em>n</em>-dimensional space, where <em>n</em> is the number of features in each observation, we can imagine a cloud of points with a different amount of scatter in different directions.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_154"/><em>Principal component analysis (PCA)</em> is a technique to learn the directions of the scatter in the dataset, starting with the direction aligned along the greatest scatter. This direction is called the <em>principal component</em>. You then find the remaining components in order of decreasing scatter, with each new component orthogonal to all the others. The top part of <a href="ch06.xhtml#ch06fig02">Figure 6-2</a> shows a 2D dataset and two arrows. Without knowing anything about the dataset, we can see that the largest arrow points along the direction of the greatest scatter. This is what we mean by the principal component.</p>&#13;
<div class="image" id="ch06fig02"><img src="Images/06fig02.jpg" alt="image" width="680" height="919"/></div>&#13;
<p class="figcap"><em>Figure 6-2: The first two features of the iris dataset and principal component directions (top), and the iris dataset after transformation by PCA (bottom)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_155"/>We often use PCA to reduce the dimensionality of a dataset. If there are 100 variables per observation, but the first two principal components explain 95 percent of the scatter in the data, then mapping the dataset along those two components and discarding the remaining 98 components might adequately characterize the dataset with only two variables. We can use PCA to augment a dataset as well, assuming continuous features.</p>&#13;
<p class="indent">So, how does PCA work? All this talk about the scatter of the data implies that PCA might be able to make use of the covariance matrix, and, indeed, it does. We can break the PCA algorithm down into a few steps:</p>&#13;
<ol>&#13;
<li class="noindent">Mean center the data.</li>&#13;
<li class="noindent">Calculate the covariance matrix, Σ, of the mean-centered data.</li>&#13;
<li class="noindent">Calculate the eigenvalues and eigenvectors of Σ.</li>&#13;
<li class="noindent">Sort the eigenvalues by decreasing absolute value.</li>&#13;
<li class="noindent">Discard the weakest eigenvalues/eigenvectors (optional).</li>&#13;
<li class="noindent">Form a transformation matrix, <em><strong>W</strong></em>, using the remaining eigenvectors.</li>&#13;
<li class="noindent">Generate new transformed values from the existing dataset, <em><strong>x</strong></em>′ = <em><strong>Wx</strong></em>. These are sometimes referred to as <em>derived variables</em>.</li>&#13;
</ol>&#13;
<p class="indent">Let’s work through an example of this process using the iris dataset (<a href="ch06.xhtml#ch06ex01">Listing 6-1</a>). We’ll reduce the dimensionality of the data from four features to two. First the code, then the explanation:</p>&#13;
<pre>   from sklearn.datasets import load_iris<br/>&#13;
   iris = load_iris().data.copy()<br/>&#13;
<span class="ent">❶</span> m = iris.mean(axis=0)<br/>&#13;
   ir = iris - m<br/>&#13;
<span class="ent">❷</span> cv = np.cov(ir, rowvar=False)<br/>&#13;
<span class="ent">❸</span> val, vec = np.linalg.eig(cv)<br/>&#13;
   val = np.abs(val)<br/>&#13;
<span class="ent">❹</span> idx = np.argsort(val)[::-1]<br/>&#13;
   ex = val[idx] / val.sum()<br/>&#13;
   print("fraction explained: ", ex)<br/>&#13;
<span class="ent">❺</span> w = np.vstack((vec[:,idx[0]],vec[:,idx[1]]))<br/>&#13;
<br/>&#13;
<span class="ent">❻</span> d = np.zeros((ir.shape[0],2))<br/>&#13;
   for i in range(ir.shape[0]):<br/>&#13;
       d[i,:] = np.dot(w,ir[i])</pre>&#13;
<p class="ex-caption" id="ch06ex01"><em>Listing 6-1: Principal component analysis (PCA)</em></p>&#13;
<p class="indent">We start by loading the iris dataset, courtesy of <code>sklearn</code>. This gives us <code>iris</code> as a 150 × 4 matrix, since there are 150 observations, each with four features. We calculate the mean value of each feature <span class="ent">❶</span> and subtract it from the dataset, relying on NumPy’s broadcasting rules to subtract <code>m</code> from each row of <code>iris</code>. We’ll work with the mean-centered matrix <code>ir</code> going forward.</p>&#13;
<p class="indent">The next step is to compute the covariance matrix <span class="ent">❷</span>. The output, <code>cv</code>, is a 4 × 4 matrix, since we have four features per observation. We follow this <span epub:type="pagebreak" id="page_156"/>by calculating the eigenvalues and eigenvectors of <code>cv</code> <span class="ent">❸</span> and then take the absolute value of the eigenvalues to get the magnitude. We want the eigenvalues in decreasing order of magnitude, so we get the indices that sort them that way <span class="ent">❹</span> using the Python idiom of <code>[::-1]</code> to reverse the order of a list or array.</p>&#13;
<p class="indent">The magnitude of the eigenvalues is proportional to the fraction of the variance in the dataset along each principal component; therefore, if we scale the eigenvalues by their overall sum, we get the proportion explained by each principal component (<code>ex</code>). The fraction of variance explained is</p>&#13;
<pre>fraction explained: [0.92461872 0.05306648 0.01710261 0.00521218]</pre>&#13;
<p class="noindent">indicating that two principal components explain nearly 98 percent of the variance in the iris dataset. Therefore, we’ll only keep the first two principal components going forward.</p>&#13;
<p class="indent">We create the transformation matrix, <code>w</code>, from the eigenvectors that go with the two largest eigenvalues <span class="ent">❺</span>. Recall, <code>eig</code> returns the eigenvectors as the columns of the matrix <code>vec</code>. The transformation matrix, <code>w</code>, is a 2 × 4 matrix because it maps a four-component feature vector to a new two-component vector.</p>&#13;
<p class="indent">All that’s left is to create a place to hold the transformed observations and fill them in <span class="ent">❻</span>. The new, reduced-dimension dataset is in <code>d</code>. We can now plot the entire transformed dataset, labeling each point by the class to which it belongs. The result is the bottom part of <a href="ch06.xhtml#ch06fig02">Figure 6-2</a>.</p>&#13;
<p class="indent">In the top part of <a href="ch06.xhtml#ch06fig02">Figure 6-2</a> is a plot of the original dataset using only the first two features. The arrows indicate the first two principal components, and the size of the arrows shows how much of the variance in the data these components explain. The first component explains most of the variance, which makes sense visually.</p>&#13;
<p class="indent">In this example, the derived variables in the bottom part of <a href="ch06.xhtml#ch06fig02">Figure 6-2</a> have made the dataset easier to work with, as the classes are better separated than on the top using only two of the original features. Sometimes, PCA makes it easier for a model to learn because of the reduced feature vector size. However, this is not always the case. During PCA, you may lose a critical feature allowing class separation. As with most things in machine learning, experimentation is vital.</p>&#13;
<p class="indent">PCA is commonly used and is therefore well supported in multiple tool-kits. Instead of the dozen or so lines of code we used above, we can accomplish the same thing by using the <code>PCA</code> class from the <code>sklearn.decomposition</code> module:</p>&#13;
<pre>from sklearn.decomposition import PCA<br/>&#13;
pca = PCA(n_components=2)<br/>&#13;
pca.fit(ir)<br/>&#13;
d = pca.fit_transform(ir)</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_157"/>The new, reduced-dimension dataset is in <code>d</code>. Like other <code>sklearn</code> classes, after we tell <code>PCA</code> how many components we want it to learn, it uses <code>fit</code> to set up the transformation matrix (<code>w</code> in <a href="ch06.xhtml#ch06ex01">Listing 6-1</a>). We then apply the transform by calling <code>fit_transform</code>.</p>&#13;
<h3 class="h3" id="ch06lev1_5">Singular Value Decomposition and Pseudoinverse</h3>&#13;
<p class="noindent">We’ll end this chapter with an introduction to <em>singular value decomposition (SVD)</em>. This is a powerful technique to factor any matrix into the product of three matrices, each with special properties. The derivation of SVD is beyond the scope of this book. I trust motivated readers to dig into the vast literature on linear algebra to locate a satisfactory presentation of where SVD comes from and how it is best understood. Our goal is more modest: to become familiar with the mathematics found in deep learning. Therefore, we’ll content ourselves with the definition of SVD, some idea of what it gives us, some of its uses, and how to work with it in Python. For deep learning, you’ll most likely encounter SVD when calculating the pseudoinverse of a nonsquare matrix. We’ll also see how that works in this section.</p>&#13;
<p class="indent">The output of SVD for an input matrix, <em><strong>A</strong></em>, with real elements and shape <em>m</em> × <em>n</em>, where <em>m</em> does not necessarily equal <em>n</em> (though it could) is</p>&#13;
<div class="imagec" id="ch06equ10"><img src="Images/06equ10.jpg" alt="Image" width="397" height="25"/></div>&#13;
<p class="noindent"><em><strong>A</strong></em> has been decomposed into three matrices: <em><strong>U</strong></em>, Σ, and <em><strong>V</strong></em>. Note that you might sometimes see <em><strong>V</strong></em><sup>⊤</sup> written as <em><strong>V</strong></em><sup>*</sup>, the conjugate transpose of <em><strong>V</strong></em>. This is the more general form that works with complex-valued matrices. We’ll restrict ourselves to real-valued matrices, so we only need the ordinary matrix transpose.</p>&#13;
<p class="indent">The SVD of an <em>m</em> × <em>n</em> matrix, <em><strong>A</strong></em>, returns the following: <em><strong>U</strong></em>, which is <em>m</em> × <em>m</em> and orthogonal; Σ, which is <em>m</em> × <em>n</em> and diagonal; and <em><strong>V</strong></em>, which is <em>n</em> × <em>n</em> and orthogonal. Recall that the transpose of an orthogonal matrix is its inverse, so <em><strong>UU</strong></em><sup>⊤</sup> = <em><strong>I</strong></em><em><sub>m</sub></em> and <em><strong>VV</strong></em><sup>⊤</sup> = <em><strong>I</strong></em><em><sub>n</sub></em>, where the subscript on the identity matrix gives the order of the matrix, <em>m</em> × <em>m</em> or <em>n</em> × <em>n</em>.</p>&#13;
<p class="indent">At this point in the chapter, you may have raised an eyebrow at the statement “Σ, which is <em>m</em> × <em>n</em> and diagonal,” since we’ve only considered square matrices to be diagonal. Here, when we say <em>diagonal</em>, we mean a <em>rectangular diagonal matrix</em>. This is the natural extension to a diagonal matrix, where the elements of what would be the diagonal are nonzero and all others are zero. For example,</p>&#13;
<div class="imagec"><img src="Images/157equ01.jpg" alt="Image" width="204" height="68"/></div>&#13;
<p class="noindent">is a 3 × 5 rectangular diagonal matrix because only the main diagonal is nonzero. The “singular” in “singular value decomposition” comes from the fact that the elements of the diagonal matrix, Σ, are the singular values, the square roots of the positive eigenvalues of the matrix <em><strong>A</strong><sup>T</sup><strong>A</strong></em>.</p>&#13;
<h4 class="h4" id="ch06lev2_14"><span epub:type="pagebreak" id="page_158"/>SVD in Action</h4>&#13;
<p class="noindent">Let’s be explicit and use SVD to decompose a matrix. Our test matrix is</p>&#13;
<div class="imagec"><img src="Images/158equ01.jpg" alt="Image" width="149" height="51"/></div>&#13;
<p class="indent">We’ll show SVD in action as a series of steps. To get the SVD, we use <code>svd</code> from <code>scipy.linalg</code>,</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from scipy.linalg import svd</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">a = np.array([[3,2,2],[2,3,-2]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">u,s,vt = svd(a)</span></pre>&#13;
<p class="noindent">where <code>u</code> is <em><strong>U</strong></em>, <code>vt</code> is <em><strong>V</strong></em><sup>⊤</sup>, and <code>s</code> contains the singular values:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(u)</span><br/>&#13;
[[-0.70710678 -0.70710678]<br/>&#13;
 [-0.70710678  0.70710678]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(s)</span><br/>&#13;
[5. 3.]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(vt)</span><br/>&#13;
[[-7.07106781e-01 -7.07106781e-01 -5.55111512e-17]<br/>&#13;
 [-2.35702260e-01  2.35702260e-01 -9.42809042e-01]<br/>&#13;
 [-6.66666667e-01  6.66666667e-01  3.33333333e-01]]</pre>&#13;
<p class="indent">Let’s check that the singular values are indeed the square roots of the positive eigenvalues of <em><strong>A</strong><sup>T</sup><strong>A</strong></em>:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(np.linalg.eig(a.T @ a)[0])</span><br/>&#13;
[2.5000000e+01 5.0324328e-15 9.0000000e+00]</pre>&#13;
<p class="noindent">This shows us that, yes, 5 and 3 are the square roots of 25 and 9. Recall that <code>eig</code> returns a list, the first element of which is a vector of the eigenvalues. Also note that there is a third eigenvalue: zero. You might ask: “How small a numeric value should we interpret as zero?” That’s a good question with no hard and fast answer. Typically, I interpret values below 10<sup>−9</sup> to be zero.</p>&#13;
<p class="indent">The claim of SVD is that <em><strong>U</strong></em> and <em><strong>V</strong></em> are unitary matrices. If so, their products with their transposes should be the identity matrix:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(u.T @ u)</span><br/>&#13;
[[1.00000000e+00 3.33066907e-16]<br/>&#13;
 [3.33066907e-16 1.00000000e+00]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(vt @ vt.T)</span><br/>&#13;
[[ 1.00000000e+00  8.00919909e-17 -1.85037171e-17]<br/>&#13;
 [ 8.00919909e-17  1.00000000e+00 -5.55111512e-17]<br/>&#13;
 [-1.85037171e-17 -5.55111512e-17  1.00000000e+00]]</pre>&#13;
<p class="noindent">Given the comment above about numeric values that we should interpret as zero, this is indeed the identity matrix. Notice that <code>svd</code> returned <em><strong>V</strong></em><sup>⊤</sup>, not <em><strong>V</strong></em>. However, since (<em><strong>V</strong></em><sup>⊤</sup>)<sup>⊤</sup> = <em><strong>V</strong></em>, we’re still multiplying <em><strong>V</strong></em><sup>⊤</sup><em><strong>V</strong></em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_159"/>The <code>svd</code> function returns not Σ but the diagonal values of Σ. Therefore, let’s reconstruct Σ and use it to see that SVD works, meaning we can use <em><strong>U</strong></em>, Σ, and <em><strong>V</strong></em><sup>⊤</sup> to recover <em><strong>A</strong></em>:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">S = np.zeros((2,3))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">S[0,0], S[1,1] = s</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(S)</span><br/>&#13;
[[5. 0. 0.]<br/>&#13;
 [0. 3. 0.]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">A = u @ S @ vt</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(A)</span><br/>&#13;
[[ 3. 2. 2.]<br/>&#13;
 [ 2. 3. -2.]]</pre>&#13;
<p class="noindent">This is the <em><strong>A</strong></em> we started with—almost: the recovered <em><strong>A</strong></em> is no longer of integer type, a subtle change worth remembering when writing code.</p>&#13;
<h4 class="h4" id="ch06lev2_15">Two Applications</h4>&#13;
<p class="noindent">SVD is a cute trick, but what can we do with it? The short answer is “a lot.” Let’s see two applications. The first is using SVD for PCA. The <code>sklearn PCA</code> class we used in the previous section uses SVD under the hood. The second example shows up in deep learning: using SVD to calculate the Moore-Penrose pseudoinverse, a generalization of the inverse of a square matrix to <em>m</em> × <em>n</em> matrices.</p>&#13;
<h5 class="h5" id="ch06lev3_2">SVD for PCA</h5>&#13;
<p class="noindent">To see how to use SVD for PCA, let’s use the iris data from the previous section so we can compare with those results. The key is to truncate the Σ and <em><strong>V</strong></em><sup>⊤</sup> matrices to keep only the desired number of largest singular values. The decomposition code will put the singular values in decreasing order along the diagonal of Σ for us, we need only retain the first <em>k</em> columns of Σ. In code, then,</p>&#13;
<pre>   u,s,vt = svd(ir)<br/>&#13;
<span class="ent">❶</span> S = np.zeros((ir.shape[0], ir.shape[1]))<br/>&#13;
   for i in range(4):<br/>&#13;
       S[i,i] = s[i]<br/>&#13;
<span class="ent">❷</span> S = S[:, :2]<br/>&#13;
   T = u @ S</pre>&#13;
<p class="noindent">Here, we’re using <code>ir</code> from <a href="ch06.xhtml#ch06ex01">Listing 6-1</a>. This is the mean-centered version of the iris dataset matrix, with 150 rows of four features each. A call to <code>svd</code> gives us the decomposition of <code>ir</code>. The next three lines <span class="ent">❶</span> create the full Σ matrix in <code>S</code>. Because the iris dataset has four features, the <code>s</code> vector that <code>svd</code> returns will have four singular values.</p>&#13;
<p class="indent">The truncation comes by keeping the first two columns of <code>S</code> <span class="ent">❷</span>. Doing this changes Σ from a 150 × 4 matrix to a 150 × 2 matrix. Multiplying <em><strong>U</strong></em> by <span epub:type="pagebreak" id="page_160"/>the new Σ gives us the transformed iris dataset. Since <em><strong>U</strong></em> is 150 × 150 and Σ is 150 × 2, we get a 150 × 2 dataset in <code>T</code>. If we plot this as <code>T[:,0]</code> versus <code>T[:,1]</code>, we get the exact same plot as the bottom part of <a href="ch06.xhtml#ch06fig02">Figure 6-2</a>.</p>&#13;
<h5 class="h5" id="ch06lev3_3">The Moore-Penrose Pseudoinverse</h5>&#13;
<p class="noindent">As promised, our second application is to compute <em><strong>A</strong></em><sup>+</sup>, the Moore-Penrose pseudoinverse of an <em>m</em> × <em>n</em> matrix <em><strong>A</strong></em>. The matrix <em><strong>A</strong></em><sup>+</sup> is called a pseudo-inverse because, in conjunction with <em><strong>A</strong></em>, it acts like an inverse in that</p>&#13;
<div class="imagec" id="ch06equ11"><img src="Images/06equ11.jpg" alt="Image" width="395" height="24"/></div>&#13;
<p class="noindent">where <em><strong>AA</strong></em><sup>+</sup> is somewhat like the identity matrix, making <em><strong>A</strong></em><sup>+</sup> somewhat like the inverse of <em><strong>A</strong></em>.</p>&#13;
<p class="indent">Knowing that the pseudoinverse of a rectangular diagonal matrix is simply the reciprocal of the diagonal values, leaving zeros as zero, followed by a transpose, we can calculate the pseudoinverse of any general matrix as</p>&#13;
<p class="center"><em><strong>A</strong></em><sup>+</sup> = <em><strong>V</strong></em>Σ<sup>+</sup> <em><strong>U</strong></em>*</p>&#13;
<p class="noindent">for <em><strong>A</strong></em> = <em><strong>U</strong></em>Σ<em><strong>V</strong></em>*, the SVD of <em><strong>A</strong></em>. Notice, we’re using the conjugate transpose, <em><strong>V</strong></em><sup>*</sup>, instead of the ordinary transpose, <em><strong>V</strong></em><sup>⊤</sup>. If <em><strong>A</strong></em> is real, then the ordinary transpose is the same as the conjugate transpose.</p>&#13;
<p class="indent">Let’s see if the claim regarding <em><strong>A</strong></em><sup>+</sup> is true. We’ll start with the <em><strong>A</strong></em> matrix we used in the section above, compute the SVD, and use the parts to find the pseudoinverse. Finally, we’ll validate <a href="ch06.xhtml#ch06equ11">Equation 6.11</a>.</p>&#13;
<p class="indent">We’ll start with <em><strong>A</strong></em>, the same array we used above for the SVD example:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">A = np.array([[3,2,2],[2,3,-2]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(A)</span><br/>&#13;
[[ 3 2  2]<br/>&#13;
 [ 2 3 -2]]</pre>&#13;
<p class="noindent">Applying SVD will give us <em><strong>U</strong></em> and <em><strong>V</strong></em><sup>⊤</sup> along with the diagonal of Σ. We’ll use the diagonal elements to construct Σ<sup>+</sup> by hand. Recall, Σ<sup>+</sup> is the transpose of Σ, where the diagonal elements that are not zero are changed to their reciprocals:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">u,s,vt = svd(A)</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Splus = np.array([[1/s[0],0],[0,1/s[1]],[0,0]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(Splus)</span><br/>&#13;
[[0.2        0.        ]<br/>&#13;
 [0.         0.33333333]<br/>&#13;
 [0.         0.        ]]</pre>&#13;
<p class="noindent">Now we can calculate <em><strong>A</strong></em><sup>+</sup> and verify that <em><strong>AA</strong></em><sup>+</sup><em><strong>A</strong></em> = <em><strong>A</strong></em>:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">Aplus = vt.T @ Splus @ u.T</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(Aplus)</span><br/>&#13;
<span epub:type="pagebreak" id="page_161"/>[[ 0.15555556  0.04444444]<br/>&#13;
 [ 0.04444444  0.15555556]<br/>&#13;
 [ 0.22222222 -0.22222222]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(A @ Aplus @ A)</span><br/>&#13;
[[ 3. 2.  2.]<br/>&#13;
 [ 2. 3. -2.]]</pre>&#13;
<p class="noindent">And, in this case, <em><strong>AA</strong></em><sup>+</sup> is the identity matrix:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(A @ Aplus)</span><br/>&#13;
[[1.00000000e+00 5.55111512e-17]<br/>&#13;
 [1.66533454e-16 1.00000000e+00]]</pre>&#13;
<p class="indent">This concludes our whirlwind look at SVD and our discussion of linear algebra. We barely scratched the surface, but we’ve covered what we need to know.</p>&#13;
<h3 class="h3" id="ch06lev1_6">Summary</h3>&#13;
<p class="noindent">This heavy chapter and <a href="ch05.xhtml#ch05">Chapter 5</a> before it plowed through a lot of linear algebra. As a mathematical topic, linear algebra is vastly richer than our presentation here.</p>&#13;
<p class="indent">We focused the chapter on square matrices, as they have a special place in linear algebra. Specifically, we discussed general properties of square matrices, with examples. We learned about eigenvalues and eigenvectors, how to find them, and why they are useful. Next, we looked at vector norms and other ways to measure distance, as they show up often in deep learning. Finally, we ended the chapter by learning what PCA is and how it works, followed by a look at singular value decomposition, with two applications relevant to deep learning.</p>&#13;
<p class="indent">The next chapter shifts gears and covers differential calculus. This is, fortunately, the “easy” part of calculus, and is, in general, all that we need to understand the algorithms specific to deep learning. So, fasten your seat belts, make sure your arms and legs are fully within the vehicle, and prepare for departure to the world of differential calculus.<span epub:type="pagebreak" id="page_162"/></p>&#13;
</div></body></html>