- en: '**3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3'
- en: MORE PROBABILITY**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 更多概率**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: '[Chapter 2](ch02.xhtml#ch02) introduced us to basic concepts of probability.
    In this chapter, we’ll continue our exploration of probability by focusing on
    two essential topics often encountered in deep learning and machine learning:
    probability distributions and how to sample from them, and Bayes’ theorem. Bayes’
    theorem is one of the most important concepts in probability theory, and it has
    produced a paradigm shift in the way many researchers think about probability
    and how to apply it.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 2 章](ch02.xhtml#ch02)为我们介绍了概率的基本概念。在本章中，我们将继续探讨概率，重点关注深度学习和机器学习中常遇到的两个重要话题：概率分布及其如何采样，贝叶斯定理。贝叶斯定理是概率论中最重要的概念之一，它在许多研究人员思考概率和如何应用它的方式上引发了范式转变。'
- en: Probability Distributions
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概率分布
- en: A probability distribution can be thought of as a function that generates values
    on demand. The values generated are random—we don’t know which one will appear—but
    the likelihood of any value appearing follows a general form. For example, if
    we roll a standard die many times and tally how many times each number comes up,
    we expect that in the long run, each number is equally likely. Indeed, that’s
    the entire point of making the die in the first place. Therefore, the probability
    distribution of the die is known as a *uniform distribution*, since each number
    is equally likely to appear. We can imagine other distributions favoring one value
    or range of values over others, like a weighted die that might come up as six
    suspiciously often.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布可以看作是一个函数，根据需求生成值。生成的值是随机的——我们不知道哪个值会出现——但任何值出现的概率遵循一般形式。例如，如果我们多次掷标准骰子并统计每个数字出现的次数，我们期望从长远来看，每个数字出现的可能性相等。实际上，这正是设计骰子的目的。因此，骰子的概率分布被称为*均匀分布*，因为每个数字出现的概率相等。我们还可以想象其他分布，其中某些值或值的范围比其他值更容易出现，比如一个加权骰子，它可能会异常频繁地显示六点。
- en: Deep learning’s primary reason for sampling from a probability distribution
    is to initialize the network before training. Modern networks select the initial
    weights and sometimes biases from different distributions, most notably uniform
    and normal. The uniform distribution is familiar to us, and I’ll discuss the normal
    distribution, a continuous distribution, later.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习从概率分布中采样的主要原因是在训练之前初始化网络。现代网络从不同的分布中选择初始权重，有时还包括偏置，最常见的是均匀分布和正态分布。均匀分布对我们来说比较熟悉，接下来我会讨论正态分布，它是一种连续分布。
- en: I’ll present several different kinds of probability distributions in this section.
    Our focus is to understand the shape of the distribution and to learn how to draw
    samples from it using NumPy. I’ll start with histograms to show you that we can
    often treat histograms as approximations of a probability distribution. Then I’ll
    discuss common *discrete probability distributions*. These are distributions returning
    integer values, like 3 or 7\. Lastly, I’ll switch to continuous distributions
    yielding floating-point numbers, like 3.8 or 7.592.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将展示几种不同类型的概率分布。我们的重点是理解分布的形状，并学习如何使用 NumPy 从中抽样。我将从直方图开始，向你展示我们通常可以将直方图视为概率分布的近似。接着我将讨论常见的*离散概率分布*，这些分布返回整数值，如
    3 或 7。最后，我将转向连续分布，返回浮动点数值，如 3.8 或 7.592。
- en: Histograms and Probabilities
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直方图与概率
- en: Take a look at [Table 3-1](ch03.xhtml#ch03tab01), which we saw in [Chapter 2](ch02.xhtml#ch02).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看 [表 3-1](ch03.xhtml#ch03tab01)，我们在 [第 2 章](ch02.xhtml#ch02) 中见过它。
- en: '**Table 3-1:** The Number of Combinations of Two Dice Leading to Different
    Sums (Copied from [Table 2-1](ch02.xhtml#ch02tab01))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-1：** 两个骰子不同和的组合数（摘自 [表 2-1](ch02.xhtml#ch02tab01)）'
- en: '| **Sum** | **Combinations** | **Count** | **Probability** |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **和** | **组合** | **计数** | **概率** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 2 | 1 + 1 | 1 | 0.0278 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 + 1 | 1 | 0.0278 |'
- en: '| 3 | 1 + 2, 2 + 1 | 2 | 0.0556 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 + 2, 2 + 1 | 2 | 0.0556 |'
- en: '| 4 | 1 + 3, 2 + 2, 3 + 1 | 3 | 0.0833 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 + 3, 2 + 2, 3 + 1 | 3 | 0.0833 |'
- en: '| 5 | 1 + 4, 2 + 3, 3 + 2, 4 + 1 | 4 | 0.1111 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 + 4, 2 + 3, 3 + 2, 4 + 1 | 4 | 0.1111 |'
- en: '| 6 | 1 + 5, 2 + 4, 3 + 3, 4 + 2, 5 + 1 | 5 | 0.1389 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 + 5, 2 + 4, 3 + 3, 4 + 2, 5 + 1 | 5 | 0.1389 |'
- en: '| 7 | 1 + 6, 2 + 5, 3 + 4, 4 + 3, 5 + 2, 6 + 1 | 6 | 0.1667 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 + 6, 2 + 5, 3 + 4, 4 + 3, 5 + 2, 6 + 1 | 6 | 0.1667 |'
- en: '| 8 | 2 + 6, 3 + 5, 4 + 4, 5 + 3, 6 + 2 | 5 | 0.1389 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2 + 6, 3 + 5, 4 + 4, 5 + 3, 6 + 2 | 5 | 0.1389 |'
- en: '| 9 | 3 + 6, 4 + 5, 5 + 4, 6 + 3 | 4 | 0.1111 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 3 + 6, 4 + 5, 5 + 4, 6 + 3 | 4 | 0.1111 |'
- en: '| 10 | 4 + 6, 5 + 5, 6 + 4 | 3 | 0.0833 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 4 + 6, 5 + 5, 6 + 4 | 3 | 0.0833 |'
- en: '| 11 | 5 + 6, 6 + 5 | 2 | 0.0556 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 5 + 6, 6 + 5 | 2 | 0.0556 |'
- en: '| 12 | 6 + 6 | 1 | 0.0278 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 6 + 6 | 1 | 0.0278 |'
- en: '|  |  | 36 | 1.0000 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 36 | 1.0000 |'
- en: It shows how two dice add to different sums. Don’t look at the actual values;
    look at the shape the possible combinations make. If we chop off the last two
    columns, turn the table to the left, and replace each sum with an “X,” we should
    see something like the following.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它展示了两个骰子如何加和得到不同的和。不要看实际的数值，看看可能的组合所形成的形状。如果我们去掉最后两列，向左旋转表格，并用“X”替换每个和，我们应该能看到类似下面的内容。
- en: '|  |  |  |  |  | × |  |  |  |  |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | × |  |  |  |  |  |'
- en: '|  |  |  |  | × | × | × |  |  |  |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | × | × | × |  |  |  |  |'
- en: '|  |  |  | × | × | × | × | × |  |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | × | × | × | × | × |  |  |  |'
- en: '|  |  | × | × | × | × | × | × | × |  |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |  | × | × | × | × | × | × | × |  |  |'
- en: '|  | × | × | × | × | × | × | × | × | × |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | × | × | × | × | × | × | × | × | × |  |'
- en: '| × | × | × | × | × | × | × | × | × | × | × |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| × | × | × | × | × | × | × | × | × | × | × |'
- en: '| 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |'
- en: You can see that there’s a definite shape and symmetry to the number of ways
    to arrive at each sum. This kind of plot is called a *histogram*. A histogram
    is a plot tallying the number of things that fall into discrete bins. For [Table
    3-1](ch03.xhtml#ch03tab01), the bins are the numbers 2 through 12\. The tally
    is a possible way to get that sum. Histograms are often represented as bar graphs,
    usually vertical bars, though they need not be. [Table 3-1](ch03.xhtml#ch03tab01)
    is basically a horizontal histogram. How many bins are used in the histogram is
    up to the maker. If you use too few, the histogram will be blocky and may not
    reveal necessary detail because interesting features have all been lumped into
    the same bin. Use too many bins, and the histogram will be sparse, with many bins
    having no tallies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，达到每个和的方式数目有一个明确的形状和对称性。这种图表称为*直方图*。直方图是一种统计落入离散区间的事物数量的图表。对于[表3-1](ch03.xhtml#ch03tab01)，这些区间是2到12的数字。统计的是获得该和的可能方式。直方图通常用条形图表示，通常是垂直条形，尽管它们不必是垂直的。[表3-1](ch03.xhtml#ch03tab01)基本上是一个水平直方图。使用多少个区间由制作人决定。如果使用太少的区间，直方图将会显得块状，可能无法揭示必要的细节，因为有趣的特征都被归类到同一个区间。如果使用太多区间，直方图将变得稀疏，很多区间没有统计数据。
- en: 'Let’s generate some histograms. First, we’ll randomly sample integers in [0,9]
    and count how many of each integer we get. The code for this is straightforward:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些直方图。首先，我们将随机抽取[0,9]区间的整数，并统计每个整数出现的次数。实现这一点的代码非常简单：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We first set `n` to an array of 10,000 integers in [0, 9]. We then use `np .bincount`
    to count how many of each digit we have. We see that this run gave us 975 zeros
    and 984 nines. If the NumPy pseudorandom generator is doing its job, we expect,
    on average, to have 1,000 of each digit in a sample of 10,000 digits. We expect
    some variation, but most values are close enough to 1,000 to be convincing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将`n`设置为一个包含10,000个[0, 9]区间整数的数组。然后，我们使用`np .bincount`来统计每个数字出现的次数。我们发现这次运行得到了975个零和984个九。如果NumPy的伪随机生成器工作正常，我们期望在10,000个数字的样本中，每个数字平均出现1,000次。我们预期会有一定的波动，但大多数值应该接近1,000，足够具有说服力。
- en: The counts above tell us how many times each digit appeared. If we divide each
    bin of a histogram by the total of all the bins, we change from simple counts
    to the probability of that bin appearing. For the random digits above, we get
    the probabilities with
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的计数告诉我们每个数字出现的次数。如果我们将每个直方图的区间除以所有区间的总和，我们就从简单的计数转换为该区间出现的概率。对于上述随机数字，我们可以得到这些概率。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: which tells us that each digit did appear with a probability of about 0.1, or
    1 out of 10\. This trick of dividing histogram values by the sum of the counts
    in the histogram allows us to estimate probability distributions from samples.
    It also tells us the likelihood of particular values appearing when sampling from
    whatever process generated the data used to make the histogram. You should note
    that I said we could *estimate* the probability distribution from a set of samples
    drawn from it. The larger the number of samples, the closer the estimated probability
    distribution will be to the actual population distribution generating the samples.
    We will never get to the actual population distribution, but given the limit of
    an infinite number of samples, we can get as close as we need to.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们每个数字大约有 0.1 的概率出现，也就是每 10 次中有 1 次出现。将直方图的值除以直方图中计数的总和的这个技巧，使我们能够通过样本估计概率分布。它还告诉我们，当从生成直方图数据的任何过程中进行采样时，特定值出现的可能性。你应该注意，我说的是我们可以*估计*从一组样本中得到的概率分布。样本数量越大，估计的概率分布越接近实际生成这些样本的总体分布。我们永远无法得到实际的总体分布，但在样本数量无限的限制下，我们可以尽可能接近。
- en: Histograms are frequently used to look at the distribution of pixel values in
    an image. Let’s make a plot of the histogram of the pixels in two images. You
    can find the code in the file `ricky.py`. (I won’t show it here, as it doesn’t
    add to the discussion.) The images used are two example grayscale images included
    with SciPy in `scipy.misc`. The first shows people walking up stairs (`ascent`),
    and the second is the face of a young raccoon (`face`), as shown in [Figure 3-1](ch03.xhtml#ch03fig01).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图常用于观察图像中像素值的分布。让我们绘制两张图像中像素的直方图。你可以在文件`ricky.py`中找到代码。（我不会在这里展示它，因为它对讨论没有帮助。）所使用的图像是
    SciPy 中 `scipy.misc` 提供的两个示例灰度图像。第一张图展示了人们正在上楼梯（`ascent`），第二张是年轻浣熊的脸（`face`），如
    [图 3-1](ch03.xhtml#ch03fig01)所示。
- en: '![image](Images/03fig01.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig01.jpg)'
- en: '*Figure 3-1: People ascending (left) and “Ricky” raccoon (right)*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-1：攀爬中的人（左）和“瑞奇”浣熊（右）*'
- en: '[Figure 3-2](ch03.xhtml#ch03fig02) provides a plot of the histograms for each
    image, as probabilities. It shows two very different distributions of gray level
    values in the images. For the raccoon face, the distribution is more spread out
    and flatter, while the ascent image has a spike right around gray level 128 and
    a few bright pixels. The distributions tell us that if we pick a random pixel
    in the face image, we’re most likely to get one around gray level 100, but an
    arbitrary pixel in the ascent image will, with high relative likelihood, be closer
    to gray level 128.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](ch03.xhtml#ch03fig02)提供了每个图像的直方图作为概率的图示。它显示了图像中灰度值的两种截然不同的分布。对于浣熊的面部，分布更加分散和平坦，而上楼梯的图像则在灰度值
    128 附近有一个峰值，并且有几个亮像素。这些分布告诉我们，如果我们从面部图像中随机选择一个像素，我们最有可能得到一个灰度值接近 100 的像素，但在上楼梯的图像中，任意选择一个像素，它的灰度值很可能接近
    128。'
- en: '![image](Images/03fig02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig02.jpg)'
- en: '*Figure 3-2: Histograms as probabilities for two 512×512-pixel grayscale sample
    images*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-2：两个 512×512 像素灰度样本图像的直方图作为概率分布*'
- en: Again, histograms tally the counts of how many items fall into the predefined
    bins. We saw for the images that the histogram as probability distribution tells
    us how likely we are to get a particular gray level value if we select a random
    pixel. Likewise, the probability distribution for the random digits in the example
    before that tells us the probability of getting each digit when we ask for a random
    integer in the range [0,9].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，直方图统计了有多少项落入预定义的区间。我们看到，对于图像，作为概率分布的直方图告诉我们，如果我们随机选择一个像素，得到特定灰度值的可能性有多大。同样，前面那个例子中的随机数字的概率分布告诉我们，当我们请求一个随机整数时，得到每个数字的概率，范围是
    [0,9]。
- en: Histograms are discrete representations of a probability distribution. Let’s
    take a look at the more common discrete distributions now.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图是概率分布的离散表示。现在让我们来看看更常见的离散分布。
- en: Discrete Probability Distributions
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离散概率分布
- en: 'We’ve already encountered the most common discrete distribution several times:
    it’s the uniform distribution. That’s the one we get naturally by rolling dice
    or flipping coins. In the uniform distribution, all possible outcomes are equally
    likely. A histogram of a simulation of a process drawing from a uniform distribution
    is flat; all outcomes show up with more or less the same frequency. We’ll see
    the uniform distribution again when we look at continuous distributions. For now,
    think dice.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次遇到最常见的离散分布：均匀分布。那就是我们通过掷骰子或抛硬币自然得到的分布。在均匀分布中，所有可能的结果发生的概率是相等的。一个模拟均匀分布过程的直方图是平的；所有结果出现的频率几乎相同。当我们看连续分布时，我们会再次看到均匀分布。目前，想象骰子。
- en: Let’s look at a few other discrete distributions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其他几个离散分布。
- en: The Binomial Distribution
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二项分布
- en: Perhaps the second most common discrete distribution is the *binomial distribution*.
    This distribution represents the expected number of events happening in a given
    number of trials if each event has a specified probability. Mathematically, the
    probability of *k* events happening in *n* trials if the probability of the event
    happening is *p* can be written as
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也许第二常见的离散分布是*二项分布*。这个分布表示在给定的试验次数中，如果每次事件发生的概率是已知的，则预期发生的事件次数。数学上，*k*次事件在*n*次试验中发生的概率，如果事件发生的概率是*p*，可以表示为
- en: '![image](Images/046equ01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ01.jpg)'
- en: For example, what’s the probability of getting three heads in a row when flipping
    a fair coin three times? From the product rule, we know the probability is
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，抛三次公正的硬币，连续得到三次正面的概率是多少？根据乘法法则，我们知道概率是
- en: '![image](Images/046equ02.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ02.jpg)'
- en: Using the binomial formula, we get the same answer by calculating
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二项式公式，我们可以通过计算得到相同的结果
- en: '![image](Images/046equ03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ03.jpg)'
- en: So far, not particularly helpful. However, what if the probability of the event
    isn’t 0.5? What if we have an event, say the likelihood of a person winning *Let’s
    Make a Deal* by not changing doors, and we want to know the probability that 7
    people out of 13 will win by not changing their guess? We know the probability
    of winning the game without changing doors is 1/3—that’s *p*. We then have 13
    trials (*n*) and 7 winners (*k*). The binomial formula tells us the likelihood
    is
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，似乎并没有什么特别有用的。然而，如果事件的概率不是0.5呢？假设我们有一个事件，比如一个人通过不换门赢得*让我们来做交易*的概率，我们想知道在13个人中，有7个人不换猜测而获胜的概率是多少？我们知道不换门获胜的概率是1/3——这就是*p*。然后我们有13次试验（*n*）和7个获胜者（*k*）。二项式公式告诉我们，概率是
- en: '![image](Images/046equ04.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ04.jpg)'
- en: and, if the players *do* switch doors,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，如果玩家*确实*换门，
- en: '![image](Images/046equ05.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ05.jpg)'
- en: The binomial formula gives us the probability of a given number of events in
    a given number of trials for a specified probability per event. If we fix *n*
    and *p* and vary *k*, 0 ≤ *k* ≤ *n*, we get the probability for each *k* value.
    This gives us the distribution. For example, let *n* = 5 and *p* = 0.3, then 0
    ≤ *k* ≤ 5 with the probability for each *k* value as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 二项式公式给出了在指定的事件概率下，在给定次数的试验中发生一定数量事件的概率。如果我们固定*n*和*p*，并改变*k*，0 ≤ *k* ≤ *n*，我们可以得到每个*k*值的概率。这就给出了分布。例如，设*n*
    = 5，*p* = 0.3，那么0 ≤ *k* ≤ 5，对于每个*k*值的概率为
- en: '![image](Images/047equ01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/047equ01.jpg)'
- en: Allowing for rounding, this sums to 1.0, as we know it must because the sum
    of probabilities over an entire sample space is always 1.0\. Notice that we calculate
    all the possible values for the binomial distribution when *n* = 5\. Collectively,
    this specifies the *probability mass function (pmf)*. The probability mass function
    tells us the probability associated with all possible outcomes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到四舍五入，这个和为1.0，正如我们所知道的，因为整个样本空间的概率和总是等于1.0。注意，当*n* = 5时，我们计算了所有可能的二项分布值。总的来说，这就指定了*概率质量函数
    (pmf)*。概率质量函数告诉我们所有可能结果的概率。
- en: 'The binomial distribution is parameterized by *n* and *p*. For *n* = 5 and
    = *p* = 0.3, we see from the results above that a random sample from such a binomial
    distribution will return 1 most often—some 36 percent of the time. How can we
    draw samples from a binomial distribution? In NumPy, we need only call the `binomial`
    function in the `random` module:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布由*n*和*p*来参数化。当*n* = 5且*p* = 0.3时，我们从上面的结果可以看到，来自这种二项分布的随机样本最常返回1——大约36%的时间。我们如何从二项分布中抽取样本？在NumPy中，我们只需要调用`random`模块中的`binomial`函数：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We pass `binomial` the number of trials (`5`) and the probability of success
    for each trial (`0.3`). We then ask for 1,000 samples from a binomial distribution
    with these parameters. Using `np.bincount`, we see that the most commonly returned
    value was indeed 1, as we calculated above. By using our histogram summation trick,
    we get a probability of 0.368 for selecting a 1—close to the 0.3601 we calculated.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递`binomial`参数为试验次数（`5`）和每次试验成功的概率（`0.3`）。然后，我们请求从具有这些参数的二项分布中获取1,000个样本。通过使用`np.bincount`，我们看到最常返回的值确实是1，正如我们上面计算的那样。通过使用直方图求和技巧，我们得到选择1的概率为0.368——接近我们计算的0.3601。
- en: The Bernoulli Distribution
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 伯努利分布
- en: The *Bernoulli distribution* is a special case of the binomial distribution.
    In this case, we fix *n* = 1, meaning there’s only one trial. The only values
    we can sample are 0 or 1; either the event happens, or it doesn’t. For example,
    with *p* = 0.5, we get
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*伯努利分布*是二项分布的一个特例。在这种情况下，我们将*n* = 1，这意味着只有一次试验。我们只能抽取值为 0 或 1 的结果；要么事件发生，要么事件不发生。例如，当*p*
    = 0.5时，我们得到'
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is reasonable, since a probability of 0.5 means we’re flipping a fair coin,
    and we see that the proportion of heads or tails is roughly equal.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是合理的，因为0.5的概率意味着我们在抛一个公平的硬币，我们看到正反面出现的比例大致相等。
- en: If we change to *p* = 0.3, we get
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将*p*改为0.3，我们得到
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Again, close to 0.3, as we expect to see.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 再次接近0.3，正如我们预期的那样。
- en: Use samples from a binomial distribution when you want to simulate events with
    a known probability. With the Bernoulli form, we can sample binary outcomes, 0
    or 1, where the likelihood of the event need not be that of the flip of a fair
    coin, 0.5.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想模拟具有已知概率的事件时，使用来自二项分布的样本。通过伯努利形式，我们可以抽取二元结果，0 或 1，其中事件发生的可能性不必是公平抛硬币的概率0.5。
- en: The Poisson Distribution
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 泊松分布
- en: Sometimes, we don’t know the probability of an event happening for any particular
    trial. Instead, we might know the average number of events that happen over some
    interval, say of time. If the average number of events that happen over some time
    is λ (lambda), then the probability of *k* events happening in that interval is
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们并不知道某个事件在特定试验中发生的概率。相反，我们可能知道某个时间间隔内事件发生的平均次数，假设为λ（lambda）。那么在这个时间间隔内，发生*k*个事件的概率是
- en: '![image](Images/048equ01.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/048equ01.jpg)'
- en: This is the *Poisson distribution*, and it’s useful to model events like radioactive
    decay or the incidence of photons on an X-ray detector over some period of time.
    To sample events according to this distribution, we use `poisson` from the `random`
    module. For example, assume over some time interval there are five events on average
    (λ = 5). What sort of probability distribution do we get using the Poisson distribution?
    In code,
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*泊松分布*，它在模拟事件时非常有用，比如放射性衰变或某段时间内 X 射线探测器上光子的发生率。为了根据该分布抽样事件，我们使用`poisson`，它来自`random`模块。例如，假设某个时间间隔内，事件的平均发生次数为五次（λ
    = 5）。使用泊松分布，我们得到什么样的概率分布呢？在代码中，
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we see that, unlike the binomial distribution, which could not select
    more than *n* events, the Poisson distribution can select numbers of events that
    exceed the value of λ. In this case, the largest number of events in the time
    interval was 15, which is three times the average. You’ll find that the most frequent
    number of events is right around the average of five, as you might expect, but
    significant deviations from the average are possible.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到，与二项分布不同，二项分布不能选择超过*n*个事件，而泊松分布可以选择超过λ值的事件数量。在这种情况下，时间间隔内的最大事件数是15，是平均数的三倍。你会发现，最常见的事件数正好接近五的平均值，正如你所预期的那样，但也可能会有显著偏离平均值的情况。
- en: The Fast Loaded Dice Roller
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速加载骰子滚动器
- en: What if we need to draw samples according to an arbitrary discrete distribution?
    Earlier, we saw some histograms based on images. In that case, we could sample
    from the distribution represented by the histogram by picking pixels in the image
    at random. But what if we wanted to sample integers according to arbitrary weights?
    To do this, we can use the new Fast Loaded Dice Roller of Saad, et al.[¹](#ch03fn01)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要根据任意离散分布抽样该怎么办？之前，我们看到了一些基于图像的直方图。在那种情况下，我们可以通过随机选择图像中的像素，从表示直方图的分布中抽样。但如果我们想根据任意权重抽取整数该怎么办？为了实现这一点，我们可以使用
    Saad 等人开发的新快速加载骰子滚动器[¹](#ch03fn01)
- en: The *Fast Loaded Dice Roller (FLDR)* lets us specify an arbitrary discrete distribution
    and then draw samples from it. The code is in Python and freely available. (See
    *[https://github.com/probcomp/fast-loaded-dice-roller/](https://github.com/probcomp/fast-loaded-dice-roller/)*.)
    I’ll show how to use the code to sample according to a generic distribution. I
    recommend downloading just the `fldr.py` and `fldrf.py` files from the GitHub
    repository instead of running `setup.py`. Additionally, edit the `.fldr` import
    lines in `fldrf.py` to remove the “`.`” so they read
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*快速加载骰子滚轮（FLDR）*允许我们指定一个任意的离散分布，然后从中抽取样本。代码是用Python编写的，并且可以自由获取。（参见 *[https://github.com/probcomp/fast-loaded-dice-roller/](https://github.com/probcomp/fast-loaded-dice-roller/)*。）我将展示如何使用这段代码根据通用分布进行抽样。我建议直接从GitHub仓库下载`fldr.py`和`fldrf.py`文件，而不是运行`setup.py`。另外，在`fldrf.py`中编辑`.fldr`导入行，去掉“`.`”，使其变为：'
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using FLDR requires two steps. The first is to tell it the particular distribution
    you want to sample from. You define the distribution as ratios. (For our purposes,
    we’ll use actual probabilities, meaning our distribution will always add up to
    1.0.) This is the preprocessing step, which we only need to do once for each distribution.
    After that, we can draw samples. An example will clarify:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FLDR需要两个步骤。第一步是告诉它你想要从中抽样的特定分布。你通过比率定义分布。（为了我们的目的，我们将使用实际的概率，也就是说，我们的分布总和将始终为1.0。）这是预处理步骤，对于每个分布我们只需要做一次。之后，我们可以开始抽样。下面的示例将帮助澄清这一点：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'First, we import the two FLDR functions we need: `fldr_preprocess_float _c`
    and `fldr_sample`. Then we define the distribution using a list of four numbers.
    Four numbers imply samples will be integers in [0, 3]. However, unlike a uniform
    distribution, here we’re specifying we want zero 60 percent of the time, one 20
    percent of the time, and two and three 10 percent of the time each. The information
    that FLDR needs to sample from the distribution is returned in `x`.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的两个FLDR函数：`fldr_preprocess_float_c`和`fldr_sample`。然后，我们使用四个数字定义分布。四个数字意味着样本将是[0,
    3]范围内的整数。然而，与均匀分布不同，在这里我们指定了零出现的概率为60%，一出现的概率为20%，而二和三的概率各为10%。FLDR需要的从分布中抽样的信息存储在`x`中。
- en: 'Calling `fldr_sample` returns a single sample from the distribution. Notice
    two things: first, we need to pass `x` in, and second, FLDR doesn’t use NumPy,
    so to draw 1,000 samples, we use a standard Python list comprehension. The 1,000
    samples are in the list, `t`. Finally, we generate the histogram and see that
    nearly 60 percent of the samples are zero and slightly more than 10 percent are
    three, as we intended.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`fldr_sample`返回一个来自分布的单个样本。注意两点：首先，我们需要传入`x`，其次，FLDR不使用NumPy，因此为了绘制1,000个样本，我们使用标准的Python列表推导式。1,000个样本存储在列表`t`中。最后，我们生成直方图，可以看到近60%的样本是零，略超过10%的样本是三，这是我们预期的结果。
- en: Let’s use the histogram of the raccoon face image we used earlier to see if
    FLDR will follow a more complex distribution. We’ll load the image, generate the
    histogram, convert it to a probability distribution, and use the probabilities
    to set up FLDR. After that, we’ll draw 25,000 samples from the distribution, compute
    the histogram of the samples, and plot that histogram along with the original
    histogram to see if FLDR follows the actual distribution we give it. The code
    we need is
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前使用过的浣熊面部图像的直方图，看看FLDR是否能够遵循一个更复杂的分布。我们将加载图像，生成直方图，将其转换为概率分布，并使用这些概率来设置FLDR。之后，我们将从分布中抽取25,000个样本，计算样本的直方图，并将该直方图与原始直方图一起绘制，看看FLDR是否遵循我们提供的实际分布。我们需要的代码是：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Running this code leaves us with `b`, a probability distribution from the histogram
    of the face image, and `q`, the distribution created from 25,000 samples from
    the FLDR distribution. [Figure 3-3](ch03.xhtml#ch03fig03) shows us a plot of the
    two distributions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们得到`b`，这是来自面部图像直方图的概率分布，以及`q`，这是从FLDR分布中抽取的25,000个样本创建的分布。[图 3-3](ch03.xhtml#ch03fig03)展示了这两个分布的图形。
- en: The solid line in [Figure 3-3](ch03.xhtml#ch03fig03) is the probability distribution
    we supplied to `fldr_preprocess_float_c` representing the distribution of gray
    levels (intensities) in the raccoon image. The dashed line is the histogram of
    the 25,000 samples from this distribution. As we can see, they follow the requested
    distribution with the sort of variation we expect from such a small number of
    samples. As an exercise, change the number of samples from 25,000 to 500,000 and
    plot the two curves. You’ll see that they’re now virtually on top of each other.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-3](ch03.xhtml#ch03fig03)中的实线是我们提供给`fldr_preprocess_float_c`的概率分布，表示浣熊图像中灰度级（强度）的分布。虚线是来自该分布的25,000个样本的直方图。正如我们所看到的，它们遵循了请求的分布，并且与我们从这么少的样本中预期的变化一致。作为练习，将样本数量从25,000更改为500,000，并绘制这两条曲线。你会看到它们几乎完全重叠。'
- en: '![image](Images/03fig03.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig03.jpg)'
- en: '*Figure 3-3: Comparing the Fast Loaded Dice Roller distribution (dashed) to
    the distribution generated from the SciPy face image (solid)*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-3：比较快速加载的骰子滚动器分布（虚线）与来自SciPy面部图像生成的分布（实线）*'
- en: Discrete distributions generate integers with specific likelihoods. Let’s leave
    them now and consider continuous probability distributions, which return floating-point
    values instead.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 离散分布生成具有特定可能性的整数。现在我们暂时离开它们，来考虑返回浮动值的连续概率分布。
- en: Continuous Probability Distributions
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续概率分布
- en: I haven’t discussed continuous probabilities yet in this chapter. In part, not
    doing so was to make the concepts behind probability easier to follow. A continuous
    probability distribution, like a discrete one, has a particular shape. However,
    instead of assigning a probability to a specific integer value, as we saw above,
    the probability of selecting a particular value from a continuous distribution
    is zero. The probability of a specific value, a real number, is zero because there
    are an infinite number of possible values from a continuous distribution; this
    means no particular value can be selected. Instead, what we talk about is the
    probability of selecting values in a specific range of values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中还没有讨论连续概率。部分原因是为了让概率背后的概念更容易理解。像离散概率分布一样，连续概率分布也有一个特定的形状。然而，和我们上面看到的不同，连续分布中选择一个特定值的概率是零。一个特定值的概率为零，因为连续分布中可能的值是无限的；这意味着无法选择某个特定值。相反，我们讨论的是在某个特定范围内选择值的概率。
- en: For example, the most common continuous distribution is the uniform distribution
    over [0, 1]. This distribution returns *any* real number in that range. Although
    the probability of returning a specific real number is zero, we can talk about
    the probability of returning a value in a range, such as [0, 0.25].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，最常见的连续分布是[0, 1]范围内的均匀分布。该分布返回*任何*位于该范围内的实数。虽然返回特定实数的概率为零，但我们可以讨论返回某个范围内的值的概率，例如[0,
    0.25]。
- en: Consider again the uniform distribution over [0, 1]. We know that the sum of
    all the individual probabilities from zero to one is 1.0\. So, what is the probability
    of sampling a value from this distribution and having that value be in the range
    [0, 0.25]? All values are equally likely, and all add to 1.0, so we must have
    a 25 percent chance of returning a value in [0, 0.25]. Similarly, we have a 25
    percent chance of returning a value in [0.75, 1], as that also covers 1/4 of the
    possible range.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑在[0, 1]范围内的均匀分布。我们知道从零到一的所有单个概率之和为1.0。那么，从这个分布中抽取一个值，并且该值位于[0, 0.25]范围内的概率是多少呢？所有值的可能性相同，它们加起来为1.0，因此，我们有25%的机会返回一个位于[0,
    0.25]范围内的值。同样，我们也有25%的机会返回一个位于[0.75, 1]范围内的值，因为这个范围也覆盖了可能范围的1/4。
- en: When we talk about summing infinitely small things over a range, we’re talking
    about integration, the part of calculus that we won’t cover in this book. Conceptually,
    however, we can understand what’s happening if we think about a discrete distribution
    in the limit where the number of values it can return goes to infinity, and we’re
    summing the probabilities over some range.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论在一个范围内对无数小事物求和时，我们讨论的是积分，这是微积分的一部分，在本书中我们不会涉及。然而，从概念上讲，如果我们考虑离散分布的极限情况，其中它能返回的值数趋近于无限，并且我们在某个范围内求和概率，那么我们就能理解发生了什么。
- en: We also can think about this graphically. [Figure 3-4](ch03.xhtml#ch03fig04)
    shows the continuous probability distributions I’ll discuss.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从图形上理解这一点。[图 3-4](ch03.xhtml#ch03fig04)展示了我将要讨论的连续概率分布。
- en: '![image](Images/03fig04.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig04.jpg)'
- en: '*Figure 3-4: Some common continuous probability distributions*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3-4：一些常见的连续概率分布*'
- en: To get the probability of sampling a value in some range, we add up the area
    under the curve over that range. Indeed, this is precisely what integration does;
    the integration symbol (∫) is nothing more than a fancy “S” for *sum*. It’s the
    continuous version of ∑ for summing discrete values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得在某个范围内抽样一个值的概率，我们需要将该范围内曲线下的面积加起来。事实上，这正是积分的作用；积分符号（∫）不过是一个花哨的“S”，表示*求和*。它是用于求和离散值的∑的连续版本。
- en: The distributions in [Figure 3-4](ch03.xhtml#ch03fig04) are the most common
    ones you’ll encounter, though there are many others useful enough to be given
    names. All of these distributions have associated *probability density functions
    (pdfs)*, closed-form functions that generate the probabilities that sampling from
    the distribution will give. I generated the curves in [Figure 3-4](ch03.xhtml#ch03fig04)
    instead using the code in the file `continuous.py`. The curves are estimates of
    the probability density functions, and I created them from the histogram of a
    large number of samples. I did so intentionally to demonstrate that the NumPy
    random functions sampling from these distributions do what they claim.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-4](ch03.xhtml#ch03fig04)中的分布是你最常遇到的分布，尽管还有许多其他的分布，它们同样足够有用并且被赋予了名字。所有这些分布都有对应的*概率密度函数（pdfs）*，这些封闭形式的函数生成从分布中采样所能得到的概率。我通过使用`continuous.py`文件中的代码来生成[图3-4](ch03.xhtml#ch03fig04)中的曲线。这些曲线是概率密度函数的估计值，我是通过大量样本的直方图来创建它们的。我这样做是为了展示NumPy的随机函数从这些分布中采样时能够做到它们所宣称的效果。'
- en: Pay little attention to the x-axis in [Figure 3-4](ch03.xhtml#ch03fig04). The
    distributions have different ranges of output; they’re scaled here to fit all
    of them on the graph. The important thing to notice is their shapes. The uniform
    distribution is, well, uniform over the entire range. The normal curve, also frequently
    called a *Gaussian* or a *bell curve*, is the second most common distribution
    used in deep learning. For example, the He initialization strategy for neural
    networks samples initial weights from a normal distribution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[图3-4](ch03.xhtml#ch03fig04)，不必过多关注x轴。各个分布的输出范围不同；它们在这里被缩放，以便将所有分布都适配到图表中。需要注意的是它们的形状。均匀分布是遍布整个范围的均匀分布。正态曲线，也常被称为*高斯分布*或*钟形曲线*，是深度学习中使用的第二大常见分布。例如，神经网络的He初始化策略就是从正态分布中采样初始权重。
- en: 'The code generating the data for [Figure 3-4](ch03.xhtml#ch03fig04) is worth
    considering, as it shows us how to use NumPy to get samples:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 生成[图3-4](ch03.xhtml#ch03fig04)数据的代码值得关注，因为它向我们展示了如何使用NumPy获取样本：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**NOTE**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*We’re using the classic NumPy functions here, not the newer Generator-based
    functions. NumPy updated the pseudorandom number code in recent versions, but
    the overhead of using the new code will detract from what we want to see here.
    Unless you’re very serious about pseudorandom number generation, the older functions,
    and the Mersenne Twister pseudorandom number generator they’re based on, will
    be more than adequate.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在这里使用的是经典的NumPy函数，而不是基于生成器的新版函数。NumPy在最近的版本中更新了伪随机数代码，但使用新代码的开销会影响我们在此想要展示的内容。除非你对伪随机数生成非常认真，否则旧版函数和它们所基于的梅森旋转算法伪随机数生成器完全足够。*'
- en: To make the plots, we first use 10 million samples from each distribution (`N`).
    Then, we use 100 bins in the histogram (`B`). Again, the x-axis range when plotting
    isn’t of interest here, only the shapes of the curves.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制图表，我们首先使用来自每个分布的1000万个样本（`N`）。然后，我们使用100个区间进行直方图绘制（`B`）。同样，绘制时x轴的范围在这里并不重要，重要的是曲线的形状。
- en: The uniform samples use `random`, a function we’ve seen before. Passing the
    samples to `histogram` and applying the “divide by the sum” trick creates the
    probability curve data (`u`). We repeat this process for the Gaussian (`normal`),
    Gamma (`gamma`), and Beta (`beta`) distributions as well.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀分布的样本使用了`random`，这是我们之前见过的一个函数。将样本传递给`histogram`并应用“除以总和”的技巧就能生成概率曲线数据（`u`）。我们对正态分布（`normal`）、伽马分布（`gamma`）和贝塔分布（`beta`）也做了相同的处理。
- en: You’ll notice that `normal`, `gamma`, and `beta` accept arguments. These distributions
    are parameterized; their shape is altered by changing these parameters. For the
    normal curve, the first parameter is the mean (μ), and the second is the standard
    deviation (σ). Some 68 percent of the normal curve lies within one standard deviation
    of the mean, [μ – σ, μ + σ]. The normal curve is ubiquitous in math and nature,
    and one could write an entire book on it alone. It’s always symmetric around its
    mean value. The standard deviation controls how wide or narrow the curve is.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，`normal`、`gamma` 和 `beta` 都接受参数。这些分布是有参数的，通过改变这些参数可以改变它们的形状。对于正态分布， 第一个参数是均值（μ），第二个参数是标准差（σ）。大约
    68% 的正态分布位于均值的一个标准差范围内，即[μ - σ, μ + σ]。正态分布在数学和自然界中无处不在，甚至可以专门写一本书来讨论它。它总是围绕均值对称分布，标准差控制着曲线的宽窄。
- en: 'The gamma distribution is also parameterized. It accepts two parameters: the
    shape (*k*) and the scale (θ). Here, *k* = 5, and the scale is left at its default
    value of θ = 1\. As the shape increases, the gamma distribution becomes more and
    more like a Gaussian, with a bump that moves toward the center of the distribution.
    The scale parameter affects the horizontal size of the bump.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Gamma 分布也是有参数化的。它接受两个参数：形状参数 (*k*) 和尺度参数 (θ)。这里，*k* = 5，尺度保持默认值 θ = 1。当形状参数增大时，gamma
    分布越来越像高斯分布，峰值会向分布中心移动。尺度参数会影响峰值的横向大小。
- en: Likewise, the beta distribution uses two parameters, *a* and *b*. Here, *a*
    = 5 and *b* = 2\. If *a* > *b*, the hump of the distribution is on the right;
    if reversed, it is on the left. If *a* = *b*, the beta distribution becomes the
    uniform distribution. The flexibility of the beta distribution makes it quite
    handy for simulating different processes, as long as you can find *a* and *b*
    values approximating the probability distribution you want. However, depending
    on the precision you require, the new Fast Loaded Dice Roller we saw in the previous
    section might be a better option in practice if you have a sufficiently detailed
    discrete distribution approximation of the continuous distribution.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Beta 分布使用两个参数，*a* 和 *b*。这里，*a* = 5 和 *b* = 2。如果 *a* > *b*，则分布的峰值位于右侧；如果反过来，则位于左侧。如果
    *a* = *b*，则 Beta 分布变为均匀分布。Beta 分布的灵活性使得它在模拟不同过程时非常有用，只要你能够找到逼近所需概率分布的 *a* 和 *b*
    值。然而，根据你所需的精度，如果你有足够详细的离散分布近似值，上一节中看到的快速加载骰子滚动器可能是一个更实用的选择，尤其是在处理连续分布时。
- en: '[Table 3-2](ch03.xhtml#ch03tab02) shows us the probability density functions
    for the normal, gamma, and beta distributions. An exercise for the reader is to
    use these functions to recreate [Figure 3-4](ch03.xhtml#ch03fig04). Your results
    will be smoother still than the curves in the figure. You can calculate the *B*(*a*,
    *b*) integral in [Table 3-2](ch03.xhtml#ch03tab02) by using the function `scipy.special.beta`.
    For Γ(*k*), see `scipy.special.gamma`. Additionally, if the argument to the Γ
    function is an integer, Γ(*n* + 1) = *n*!, so Γ(5) = Γ(4 + 1) = 4! = 24.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-2](ch03.xhtml#ch03tab02) 显示了正态、Gamma 和 Beta 分布的概率密度函数。读者可以通过这些函数重新创建 [图
    3-4](ch03.xhtml#ch03fig04)。你的结果会比图中的曲线更光滑。你可以使用 `scipy.special.beta` 函数计算 [表 3-2](ch03.xhtml#ch03tab02)
    中的 *B*(*a*, *b*) 积分。对于 Γ(*k*)，请参见 `scipy.special.gamma`。此外，如果 Γ 函数的参数是整数，则 Γ(*n*
    + 1) = *n*!，所以 Γ(5) = Γ(4 + 1) = 4! = 24。'
- en: '**Table 3-2:** The Probability Density Functions for the Normal, Gamma, and
    Beta Distributions'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-2：** 正态分布、Gamma 分布和 Beta 分布的概率密度函数'
- en: '| **normal** | ![image](Images/054equ01.jpg) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **normal** | ![image](Images/054equ01.jpg) |'
- en: '| **gamma** | ![image](Images/054equ02.jpg) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **gamma** | ![image](Images/054equ02.jpg) |'
- en: '| **beta** | ![image](Images/054equ03.jpg) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **beta** | ![image](Images/054equ03.jpg) |'
- en: If you’re interested in ways to sample values from these distributions, my book
    *Random Numbers and Computers* (Springer, 2018) discusses these distributions
    and others in more depth than we can provide here, including implementations in
    C for generating samples from them. For now, let’s examine one of the most important
    theorems in probability theory.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对从这些分布中抽样值的方式感兴趣，我的书籍《*随机数与计算机*》（Springer，2018）比我们在此能提供的更深入地讨论了这些分布及其他分布，包括用
    C 语言实现从这些分布中生成样本的代码。现在，让我们来看一下概率论中最重要的定理之一。
- en: Central Limit Theorem
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 中心极限定理
- en: Imagine we draw *N* samples from some distribution and calculate the mean value,
    *m*. If we repeat this exercise many times, we’ll get a set of mean values, *{m*[0],
    *m*[1], . . .}, each from a set of samples from the distribution. It doesn’t matter
    if *N* is the same each time, but *N* shouldn’t be too small. The rule of thumb
    is that *N* should be at least 30 samples.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从某个分布中抽取 *N* 个样本并计算均值 *m*。如果我们多次重复这个操作，就会得到一组均值，*{m*[0], *m*[1], ...}，每个均值来自一个样本集合。每次
    *N* 是否相同并不重要，但 *N* 不应太小。经验法则是 *N* 至少应该为30个样本。
- en: The *central limit theorem* states that the histogram or probability distribution
    generated from this set of sample means, the *m*’s, will approach a Gaussian in
    shape regardless of the shape of the distribution the samples were drawn from
    in the first place.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*中心极限定理* 表示，基于这一组样本均值，即 *m* 的直方图或概率分布，将会呈现高斯分布的形状，无论样本最初是从什么样的分布中抽取出来的。'
- en: For example, this code
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这段代码
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: creates 10,000 sets of samples from a beta distribution, Beta(5,2), each with
    10,000 samples. The mean of each set of samples is stored in `m`. If we run this
    code and plot the histogram of *m*, we get [Figure 3-5](ch03.xhtml#ch03fig05).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了来自Beta分布Beta(5,2)的10,000组样本，每组包含10,000个样本。每组样本的均值存储在`m`中。如果我们运行这段代码并绘制 *m*
    的直方图，就会得到[图 3-5](ch03.xhtml#ch03fig05)。
- en: '![image](Images/03fig05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig05.jpg)'
- en: '*Figure 3-5: The distribution of mean values of 10,000 sets of samples of 10,000
    from Beta(5,2)*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-5：Beta(5,2)分布的10,000组样本中，每组10,000个样本的均值分布*'
- en: The shape of [Figure 3-5](ch03.xhtml#ch03fig05) is decidedly Gaussian. Again,
    the shape is a consequence of the central limit theorem and does not depend on
    the shape of the underlying distribution. [Figure 3-5](ch03.xhtml#ch03fig05) tells
    us that the sample means from many sets of samples from Beta(5,2) themselves have
    a mean of about 0.714\. The mean of the sample means (`m.mean()`) is 0.7142929
    for one run of the code above.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](ch03.xhtml#ch03fig05)的形状显然是高斯分布的。再次强调，这一形状是中心极限定理的结果，且不依赖于底层分布的形状。[图
    3-5](ch03.xhtml#ch03fig05)告诉我们，来自Beta(5,2)的多个样本均值的均值大约为0.714。对于上述代码的一次运行，样本均值的均值
    (`m.mean()`) 为0.7142929。'
- en: There’s a formula to calculate the mean value of a Beta distribution. The population
    mean value of a Beta(5,2) distribution is known to be *a*/*(a* + *b*) = 5/(5 +
    2) = 5/7 = 0.714285\. The mean of the plot in [Figure 3-5](#ch03fig05) is a measurement
    of the true population mean, of which the many means from the Beta(5,2) samples
    are only estimates.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Beta分布均值有一个公式。Beta(5,2)分布的人群均值已知为 *a*/*(a* + *b*) = 5/(5 + 2) = 5/7 = 0.714285。图中[图
    3-5](#ch03fig05)的均值是对真实人群均值的度量，而Beta(5,2)样本中得到的多个均值仅是估算值。
- en: Let’s explain this again to really follow what’s going on. For any distribution,
    like the Beta(5,2) distribution, if we draw *N* samples, we can calculate the
    mean of those samples, a single number. If we repeat this process for many sets
    of *N* samples, each with its own mean, and we make a histogram of the distribution
    of the means we measured, we’ll get a plot like [Figure 3-5](ch03.xhtml#ch03fig05).
    That plot tells us that all of the many sample means are themselves clustered
    around a mean value. The mean value of the means is a measure of the population
    mean. It’s the mean we’d get if we could draw an infinite number of samples from
    the distribution. If we change the code above to use the uniform distribution,
    we’ll get a population mean of 0.5\. Similarly, if we switch to a Gaussian distribution
    with a mean of 11, the resulting histogram will be centered at 11.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再解释一遍，真正理解这里发生了什么。对于任何分布，比如Beta(5,2)分布，如果我们抽取 *N* 个样本，就可以计算出这些样本的均值，一个单一的数字。如果我们对多个
    *N* 样本集合重复这个过程，每个集合都有自己的均值，并且我们对这些均值的分布做一个直方图，就会得到类似[图 3-5](ch03.xhtml#ch03fig05)的图形。该图告诉我们，所有多个样本均值本身都集中在某个均值附近。均值的均值是对人群均值的度量。它是我们从分布中能够抽取无数样本时得到的均值。如果我们将上述代码改为使用均匀分布，我们将得到0.5的人群均值。同样，如果我们切换到均值为11的高斯分布，生成的直方图将以11为中心。
- en: 'Let’s prove this claim again but this time with a discrete distribution. Let’s
    use the Fast Loaded Dice Roller to generate samples from a lopsided discrete distribution
    using this code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次证明这一点，但这次使用离散分布。我们使用快速加载的骰子滚动器通过以下代码从一个偏斜的离散分布中生成样本：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Figure 3-6](ch03.xhtml#ch03fig06) shows the discrete distribution (top) and
    the corresponding distribution of the sample means (bottom).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-6](ch03.xhtml#ch03fig06)展示了离散分布（上图）和相应的样本均值分布（下图）。'
- en: From the probability mass function, we can see that the most frequent value
    we expect from the sample is 1, with a probability of 60 percent. However, the
    tail on the right means we’ll also get values 2 through 4 about 30 percent of
    the time. The weighted mean of these is 0.6(1) + 0.1(2) + 0.1(3) + 0.1(4) = 1.5,
    which is precisely the mean of the sample distribution on the bottom of [Figure
    3-6](ch03.xhtml#ch03fig06). The central limit theorem works. We’ll revisit the
    central limit theorem in [Chapter 4](ch04.xhtml#ch04) when we discuss hypothesis
    testing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率质量函数中，我们可以看出，样本中最常见的值是 1，概率为 60%。然而，右侧的尾部意味着我们大约 30% 的时间会得到值 2 到 4。这些的加权均值是
    0.6(1) + 0.1(2) + 0.1(3) + 0.1(4) = 1.5，这正是[图 3-6](ch03.xhtml#ch03fig06)底部样本分布的均值。中心极限定理有效。我们将在[第
    4 章](ch04.xhtml#ch04)中讨论假设检验时重新审视中心极限定理。
- en: '![image](Images/03fig06.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig06.jpg)'
- en: '*Figure 3-6: An arbitrary discrete distribution (top) and the distribution
    of sample means drawn from it (bottom)*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-6：一个任意离散分布（上图）和从中抽取的样本均值的分布（下图）*'
- en: The Law of Large Numbers
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大数法则
- en: A concept related to the central limit theorem, and often confused with it,
    is the *law of large numbers*. The law of large numbers states that as the size
    of a sample from a distribution increases, the mean of the sample moves closer
    and closer to the mean of the population. In this case, we’re contemplating a
    single sample from the distribution and making a statement about how close we
    expect its mean to be to the true population mean. For the central limit theorem,
    we have many different sets of samples from the distribution and are making a
    statement about the distribution of the means of those sets of samples.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与中心极限定理相关的一个概念，且常常与其混淆的，是*大数法则*。大数法则指出，随着从分布中抽取的样本量增大，样本均值越来越接近总体均值。在这里，我们考虑的是从分布中抽取的单个样本，并陈述我们期望其均值与真实总体均值之间的接近程度。而对于中心极限定理，我们有多个不同的样本集，并且我们在谈论这些样本集合均值的分布。
- en: We can demonstrate the law of large numbers quite simply by selecting larger
    and larger size samples from a distribution and tracking the mean as a function
    of the sample size (the number of samples drawn). In code, then,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从一个分布中选取越来越大的样本并跟踪样本均值与样本量（抽样数量）之间的关系，简单地演示大数法则。因此，在代码中，
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: where we’re drawing ever-larger sample sizes from a normal distribution with
    a mean of 1\. The first sample size is 10, and the last is 100 million. If we
    plot the mean of the samples as a function of sample size, we see the law of large
    numbers at work.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们从均值为 1 的正态分布中抽取越来越大的样本量。第一个样本量为 10，最后一个为 1 亿。如果我们将样本均值与样本量绘制成图，就可以看到大数法则的作用。
- en: '[Figure 3-7](ch03.xhtml#ch03fig07) shows the sample means as a function of
    the number of samples for the normal distribution with a mean of 1 (dashed line).
    As the number of samples from the distribution increases, the mean of the samples
    approaches the population mean, which illustrates the law of large numbers.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-7](ch03.xhtml#ch03fig07)展示了在均值为 1（虚线）的正态分布中，样本均值随着样本数量变化的情况。随着从分布中抽取的样本数量增加，样本的均值逐渐接近总体均值，这展示了大数法则。'
- en: '![image](Images/03fig07.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig07.jpg)'
- en: '*Figure 3-7: The law of large numbers in action*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-7：大数法则的实际应用*'
- en: Let’s change gears and move on to Bayes’ theorem, the last topic for this chapter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们换个话题，进入贝叶斯定理，这是本章的最后一个主题。
- en: Bayes’ Theorem
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: In [Chapter 2](ch02.xhtml#ch02), we discussed an example where we determined
    whether a woman had cancer. There, I promised that Bayes’ theorem would tell us
    how to properly account for the probability of a randomly selected woman in her
    40s having breast cancer. Let’s fulfill that promise in this section by learning
    what Bayes’ theorem is and how to use it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](ch02.xhtml#ch02)，我们讨论了一个示例，决定一个女性是否患有癌症。在那里，我承诺贝叶斯定理将告诉我们如何正确计算一位40多岁女性患乳腺癌的概率。让我们在本节中兑现这个承诺，了解贝叶斯定理是什么以及如何使用它。
- en: 'Using the product rule, [Equation 2.8](ch02.xhtml#ch02equ08), we know the following
    two mathematical statements are true:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用乘积法则，[公式 2.8](ch02.xhtml#ch02equ08)，我们知道以下两个数学陈述是正确的：
- en: '*P*(*B*, *A*) = *P*(*B*|*A*)*P*(*A*)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*B*, *A*) = *P*(*B*|*A*)*P*(*A*)'
- en: '*P*(*A*, *B*) = *P*(*A*|*B*)*P*(*B*)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A*, *B*) = *P*(*A*|*B*)*P*(*B*)'
- en: Additionally, because the joint probability of both *A* and *B* doesn’t depend
    on which event we call *A* and which we call *B*,
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，因为*A*和*B*的联合概率与我们将哪个事件称为*A*、哪个事件称为*B*无关，
- en: '*P*(*A*, *B*) = *P*(*B*, *A*)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A*, *B*) = *P*(*B*, *A*)'
- en: Therefore,
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '*P*(*B*|*A*)*P*(*A*) = *P*(*A*|*B*)*P*(*B*)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*B*|*A*)*P*(*A*) = *P*(*A*|*B*)*P*(*B*)'
- en: Dividing by *P*(*A*), we get
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 除以*P*(*A*)，我们得到
- en: '![image](Images/03equ01.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03equ01.jpg)'
- en: 'This is *Bayes’ theorem*, the heart of the Bayesian approach to probability,
    and the proper way to compare two conditional probabilities: *P**(B*|*A*) and
    *P*(*A*|*B*). You’ll sometimes see [Equation 3.1](ch03.xhtml#ch03equ01) referred
    to as *Bayes’ rule*. You’ll also often see no apostrophe after “Bayes,” which
    is a bit sloppy and ungrammatical, but common.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*贝叶斯定理*，贝叶斯方法的核心，用来比较两个条件概率：*P**(B*|*A*) 和 *P*(*A*|*B*)。你有时会看到[方程 3.1](ch03.xhtml#ch03equ01)被称为*贝叶斯法则*。你也经常会看到“贝叶斯”后面没有撇号，这虽然有点草率和不规范，但在日常中很常见。
- en: '[Equation 3.1](ch03.xhtml#ch03equ01) has been enshrined in neon lights, tattoos,
    and even baby names: “Bayes.” The equation is named after Thomas Bayes (1701–1761),
    an English minister and statistician, and was published after his death. In words,
    [Equation 3.1](ch03.xhtml#ch03equ01) says the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 3.1](ch03.xhtml#ch03equ01)已经成为霓虹灯、纹身甚至婴儿名字的标志：“贝叶斯”。这个方程以托马斯·贝叶斯（Thomas
    Bayes，1701–1761）的名字命名，他是英国的一位牧师和统计学家，这个方程在他去世后才被发布。用文字描述，[方程 3.1](ch03.xhtml#ch03equ01)可以表述为：'
- en: The *posterior probability*, *P*(*B*|*A*), is the product of *P*(*A*|*B*), the
    *likelihood*, and *P*(*B*), the *prior*, normalized by *P*(*A*), the marginal
    probability or *evidence*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*后验概率*，*P*(*B*|*A*)，是*P*(*A*|*B*)，即*似然*，与*P*(*B*)，即*先验*，的乘积，通过*P*(*A*)，即边际概率或*证据*，进行归一化。'
- en: Now that we know what Bayes’ theorem is, let’s see it in action so we can understand
    it.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了贝叶斯定理是什么，让我们看看它是如何应用的，以便更好地理解它。
- en: Cancer or Not Redux
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 乳腺癌与否重启
- en: One way to think about the components of Bayes’ theorem is in the context of
    medical testing. At the beginning of [Chapter 2](ch02.xhtml#ch02), we calculated
    the probability of a woman having breast cancer given a positive mammogram and
    found that it was quite different from what we might naively have believed it
    to be. Let’s revisit that problem now using Bayes’ theorem. It might be helpful
    to reread the first section of [Chapter 2](ch02.xhtml#ch02) before continuing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 思考贝叶斯定理的组成部分的一种方式是将其放在医学检测的背景下。在[第2章](ch02.xhtml#ch02)的开始部分，我们计算了在乳房X光检查为阳性时，女性患乳腺癌的概率，并发现这个概率与我们可能天真认为的差异很大。现在让我们再次使用贝叶斯定理来回顾这个问题。在继续之前，重新阅读[第2章](ch02.xhtml#ch02)的第一部分可能会有所帮助。
- en: We want to use Bayes’ theorem to find the posterior probability, the probability
    of breast cancer given a positive mammogram. We’ll write this as *P**(bc* + |+),
    meaning breast cancer (*bc*+) given a positive mammogram (+).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想使用贝叶斯定理来找到后验概率，即在乳房X光检查为阳性时患乳腺癌的概率。我们将其写为*P**(bc*+ |+)，表示乳腺癌（*bc*+）在乳房X光检查为阳性（+）时的概率。
- en: In the problem, we’re told that the mammogram returns a positive result, given
    the patient has breast cancer, 90 percent of the time. We write this as
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们被告知，如果患者患有乳腺癌，乳房X光检查结果为阳性的概率是90%。我们写作
- en: '*P*(+|*bc*+) = 0.9'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(+|*bc*+) = 0.9'
- en: This is the likelihood of a positive mammogram in terms of Bayes’ equation,
    *P*(*A*|*B*) = *P*(+|*bc*+).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯方程中阳性乳房X光检查的*似然*，*P*(*A*|*B*) = *P*(+|*bc*+)。
- en: Next, we’re told the probability of a random woman having breast cancer is 0.8
    percent. Therefore, we know
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们被告知，随机女性患乳腺癌的概率是0.8%。因此，我们知道
- en: '*P*(*bc*+) = 0.008'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*bc*+) = 0.008'
- en: This is the prior probability, *P*(*B*), in Bayes’ theorem.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯定理中的先验概率，*P*(*B*)。
- en: 'We have all the components of [Equation 3.1](ch03.xhtml#ch03equ01) except one:
    *P**(A*).  What is *P*(*A*) in this context? It’s *P*(+), the marginal probability
    of a positive mammogram regardless of any *B*, any breast cancer status. It’s
    also the evidence that we have, the thing we know: the mammogram was positive.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了[方程 3.1](ch03.xhtml#ch03equ01)的所有组成部分，除了一个：*P**(A*)。在这个背景下，*P*(*A*)是什么？它是*P*(+)，即不考虑任何*B*（任何乳腺癌状态）的情况下，乳房X光检查为阳性的边际概率。这也是我们所知道的证据：乳房X光检查为阳性。
- en: In the problem, we’re told there’s a 7 percent chance a woman without breast
    cancer has a positive mammogram. Is this *P*(+)? No, it is *P*(+|*bc*–), the probability
    of a positive mammogram *given* no breast cancer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们被告知一个没有乳腺癌的女性有7%的概率会得到乳房X光检查阳性结果。这是*P*(+)? 不是，它是*P*(+|*bc*–)，即在没有乳腺癌的情况下，乳房X光检查呈阳性的概率。
- en: 'I’ve referred to *P**(A*) as the marginal probability twice now. We know what
    to do to get a marginal or total probability: we sum over all the other parts
    of a joint probability that don’t matter for what we want to know. Here, we have
    to sum over all the partitions of the sample space we don’t care about to get
    the marginal probability of a positive mammogram. What partitions are those? There
    are only two: either a woman has breast cancer or she doesn’t. Therefore, we need
    to find'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经两次提到过 *P**(A*) 是边际概率。我们知道如何计算边际或总概率：我们对联合概率中与我们想知道的内容无关的所有其他部分求和。在这里，我们必须对我们不关心的样本空间的所有分区求和，以获得阳性乳腺X光检查的边际概率。那么，哪些是我们不关心的分区呢？只有两个：一个是女性患乳腺癌，另一个是她没有乳腺癌。因此，我们需要找到
- en: '*P*(+) = *P*(+|*bc*+)*P*(*bc*+) + *P*(+|*bc*–)*P*(*bc*–)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(+) = *P*(+|*bc*+)*P*(*bc*+) + *P*(+|*bc*–)*P*(*bc*–)'
- en: We know all of these quantities already, except *P**(bc*–). This is the prior
    probability that a randomly selected woman will *not* have breast cancer, *P**(bc*–)
    = 1 – *P*(*bc*+) = 0.992.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道所有这些量，除了 *P**(bc*–)。这是一个随机选择的女性 *没有* 乳腺癌的先验概率，*P**(bc*–) = 1 – *P*(*bc*+)
    = 0.992。
- en: Sometimes, you’ll see the summation over other terms in the joint probability
    expressed in the denominator of Bayes’ theorem. Even if they’re not explicitly
    called out, they are there, implicit in what it takes to find *P**(A*).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你会在贝叶斯定理的分母中看到联合概率的其他项的求和。即使它们没有被明确指出，它们依然存在，隐含在求解 *P**(A*) 所需的条件中。
- en: 'Finally, we have all the pieces and can calculate the probability using Bayes’
    theorem:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经拥有所有必要的元素，可以使用贝叶斯定理来计算概率：
- en: '![image](Images/061equ01.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/061equ01.jpg)'
- en: This is the result we found earlier. Recall, a large percentage of doctors in
    the study claimed the probability of cancer from a positive mammogram, *P**(A*|*B*),
    was 90 percent. Their mistake was incorrectly equating *P(A*|*B*) with *P(B*|*A*).
    Bayes’ theorem correctly relates the two by using the prior and the marginal probability.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前得到的结果。回想一下，研究中有很大一部分医生声称乳腺X光检查阳性时的癌症概率 *P**(A*|*B*) 是 90%。他们的错误在于错误地将
    *P(A*|*B*) 与 *P(B*|*A*) 等同起来。贝叶斯定理通过使用先验概率和边际概率正确地将两者联系了起来。
- en: Updating the Prior
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新先验
- en: 'We don’t need to stop with this single calculation. Consider the following:
    what if, after a woman receives the news that her mammogram is positive, she decides
    to have a second mammogram at another facility with different radiologists reading
    the results, and that mammogram also comes back positive? Does she still believe
    that her probability of having breast cancer is 9 percent? Intuitively, we might
    think that she now has more reason to believe that she has cancer. Can this belief
    be quantified? It can, in the Bayesian view, by updating the prior, *P**(bc*+),
    with the posterior calculated from the first test, *P*(*bc* + |+). After all,
    she now has a stronger prior probability of cancer given the first positive mammogram.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要仅仅停留在这个单一的计算上。考虑以下情况：假设一位女性在收到乳腺X光检查呈阳性的消息后，决定在另一家有不同放射科医生解读结果的机构进行第二次乳腺X光检查，而且第二次检查也呈阳性。她还会认为自己患乳腺癌的概率是
    9% 吗？直观上，我们可能认为她现在更有理由相信自己得了癌症。这种信念是否可以量化？从贝叶斯角度来看，它是可以的，通过用第一次检查计算出来的后验概率 *P*(*bc*+|+)
    来更新先验 *P**(bc*+)。毕竟，考虑到第一次的阳性乳腺X光检查结果，她现在的乳腺癌先验概率更强。
- en: 'Let’s calculate this new posterior based on the previous mammogram result:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于之前的乳腺X光检查结果计算这个新的后验概率：
- en: '![image](Images/061equ02.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/061equ02.jpg)'
- en: As 57 percent is significantly higher than 9 percent, our hypothetical woman
    now has significantly more reason to believe she has breast cancer.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 57% 显著高于 9%，我们假设的这位女性现在有了更充足的理由相信她得了乳腺癌。
- en: Notice what has changed in this new calculation, besides a dramatic increase
    in the posterior probability of breast cancer given the second mammogram’s positive
    result. First, the prior probability of breast cancer went from 0.008 → 0.094,
    the posterior calculated based on the first test. Second, *P*(*bc*–) also changed
    from 0.992 → 0.906\. Why? Because the prior changed and *P*(*bc*–) = 1 – *P*(*bc*+).
    The sum of *P*(*bc*+) and *P*(*bc*–) must still be 1.0—either she has breast cancer,
    or she doesn’t—that’s the entire sample space.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个新的计算中，除了乳腺X光检查第二次阳性结果所导致的乳腺癌后验概率的大幅增加，还有哪些变化。首先，乳腺癌的先验概率从 0.008 → 0.094，后验概率是基于第一次检查计算得出的。其次，*P*(*bc*–)
    也从 0.992 → 0.906 发生了变化。为什么？因为先验发生了变化，且 *P*(*bc*–) = 1 – *P*(*bc*+)。*P*(*bc*+)
    和 *P*(*bc*–) 的总和仍然必须是 1.0——要么她患乳腺癌，要么她没有——这是整个样本空间。
- en: In the example above, we updated the prior based on the initial test result,
    and we had an initial prior given to us in the first example. What about the prior
    in general? In many cases, Bayesians select the prior, at least initially, based
    on an actual belief about the problem. Often the prior is a uniform distribution,
    known as the *uninformed prior* because there’s nothing to guide the selection
    of anything else. For the breast cancer example, the prior is something that can
    be estimated from an experiment using a random selection of women from the general
    population.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们基于初步的测试结果更新了先验概率，并且我们在第一个例子中获得了一个初始先验。那么，先验一般是如何选择的呢？在许多情况下，贝叶斯学派的选择先验，至少最初，是基于对问题的实际信念。通常，先验是一个均匀分布，称为*无信息先验*，因为没有任何东西来指导选择其他任何形式。以乳腺癌的例子为例，先验是可以通过对随机选择的女性样本进行实验来估计的。
- en: As mentioned earlier, don’t take the numbers here too seriously; they are for
    example use only. Also, while a woman certainly has the option to get a second
    opinion, the gold standard for a breast cancer diagnosis is biopsy, the likely
    next step after an initial positive mammogram. Finally, throughout this section,
    I’ve referred to women and breast cancer. Men also get breast cancer, though it
    is rare, with less than 1 percent of cases in men. However, it made the discussion
    simpler to refer only to women. I’ll note that breast cancer cases in men are
    more likely to be fatal, though the reasons why are not yet known.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，不要太认真对待这里的数字；它们仅用于示例。同时，虽然女性确实可以选择第二意见，但乳腺癌诊断的黄金标准是活检，这通常是初次阳性乳房X光检查后的下一步。最后，在本节中，我提到了女性和乳腺癌。男性也会得乳腺癌，尽管这种情况很少见，男性的病例不到1%。然而，讨论中仅提及女性使得讨论更简便。我需要指出，男性乳腺癌的病例更可能是致命的，尽管原因尚不清楚。
- en: Bayes’ Theorem in Machine Learning
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯定理在机器学习中的应用
- en: Bayes’ theorem is prevalent throughout machine learning and deep learning. One
    classic use of Bayes’ theorem, one that can work surprisingly well, is to use
    it as a classifier. This is known as the *Naive Bayes* classifier. Early email
    spam filters used this approach quite effectively.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理在机器学习和深度学习中广泛应用。贝叶斯定理的一个经典应用，甚至能取得惊人的效果，就是将其用作分类器。这就是所谓的*朴素贝叶斯*分类器。早期的电子邮件垃圾邮件过滤器就采用了这种方法，且非常有效。
- en: 'Assume we have a dataset consisting of class labels, *y*, and feature vectors,
    ***x***. The goal of a Naive Bayes classifier is to tell us, for each class, the
    probability that a given feature vector belongs to that class. With those probabilities,
    we can assign a class label by selecting the largest probability. That is, we
    want to find *P**(y*|***x***) for each class label, *y*. This is a conditional
    probability, so we can use Bayes’ theorem with it:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含类别标签*y*和特征向量***x***的数据集。朴素贝叶斯分类器的目标是告诉我们，每个类别的概率，即给定的特征向量属于该类别的概率。通过这些概率，我们可以通过选择最大概率来分配类别标签。也就是说，我们要为每个类别标签*y*找到*P**(y*|***x***)。这是一个条件概率，因此我们可以使用贝叶斯定理来处理它：
- en: '![image](Images/03equ02.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03equ02.jpg)'
- en: The equation above is saying that the probability of feature vector *x* representing
    an instance of class label *y* is the probability of the class label *y* generating
    a feature vector *x* times the prior probability of class label *y* occurring,
    divided by the marginal probability of the feature vector over all class labels.
    Recall the implicit sum in calculating *P*(*x*).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程表示，特征向量*x*表示类别标签*y*实例的概率，是类别标签*y*生成特征向量*x*的概率与类别标签*y*发生的先验概率的乘积，除以所有类别标签上特征向量的边际概率。回顾一下计算*P*(*x*)时的隐式求和。
- en: How is this useful to us? Since we have a dataset, we can estimate *P**(y*)
    using it, assuming the dataset class distribution is a fair representation of
    what we’d encounter when using the model. And, since we have labels, we can partition
    the dataset into smaller, per class, collections. This might help us do something
    useful to get the likelihoods per class, *P*(*x*|*y*). We’ll ignore the marginal
    *P*(*x*) completely. Let’s see why, in this case, we’re free to do so.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们有什么用呢？既然我们有一个数据集，我们可以利用它来估计*P**(y*)，假设数据集的类别分布能够公平地代表我们在使用模型时遇到的情况。而且，由于我们有标签，我们可以将数据集分割成每个类别的子集。这可能有助于我们做一些有用的事情，以获取每个类别的可能性，*P*(*x*|*y*)。我们将完全忽略边际*P*(*x*)。让我们看看为什么，在这种情况下，我们可以这么做。
- en: '[Equation 3.2](ch03.xhtml#ch03equ02) is for a particular class label, say *y*
    = 1\. We’ll have other versions of it for all the class labels in the dataset.
    We said our classifier consists of calculating the posterior probabilities for
    each class label and selecting the largest one as the label assigned to an unknown
    feature vector. The denominator of [Equation 3.2](ch03.xhtml#ch03equ02) is a scale
    factor, which makes the output a true probability. For our use case, however,
    we only care about the relative ordering of *P**(y*|***x***) over the different
    class labels. Since *P*(***x***) is the same for all *y*, it’s a common factor
    that will change the number associated with *P**(y*|***x***) but not the ordering
    over the different class labels. Therefore, we can ignore it and concentrate on
    finding the products of the likelihoods and the priors. Although the largest *P*(*y*|***x***)
    calculated this way is no longer a proper probability, it’s still the correct
    class label to assign.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 3.2](ch03.xhtml#ch03equ02)是针对一个特定的类标签，比如 *y* = 1。我们将为数据集中的所有类标签拥有它的其他版本。我们之前提到过，我们的分类器由计算每个类标签的后验概率并选择最大的那个作为分配给未知特征向量的标签组成。[方程
    3.2](ch03.xhtml#ch03equ02)的分母是一个尺度因子，使得输出成为一个真正的概率。然而，在我们的使用场景中，我们只关心不同类标签之间 *P**(y*|***x***)
    的相对排序。由于 *P*(***x***) 对所有 *y* 都是相同的，它是一个公共因子，这个因子会改变 *P**(y*|***x***) 相关的数字，但不会改变不同类标签之间的排序。因此，我们可以忽略它，专注于找到似然度和先验的乘积。尽管这种方式计算出来的最大
    *P*(*y*|***x***) 已经不再是一个合适的概率，但它仍然是正确的类标签。'
- en: Given that we can ignore *P****(x***) and the *P*(*y*) values are easily estimated
    from the dataset, we’re left with calculating *P****(x***|*y*), the likelihood
    that given the class label is *y*, we’d have a feature vector ***x***. What can
    we do in this case?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以忽略 *P****(x***) 并且 *P*(*y*) 值可以从数据集中轻松估计，我们剩下的就是计算 *P****(x***|*y*)，即给定类标签为
    *y* 时，特征向量 ***x*** 的似然度。我们在这种情况下该怎么做呢？
- en: First, we can think about what *P****(x***|*y*) is. It’s a conditional probability
    for feature vectors given the feature vectors are all representatives of class
    *y*. For the moment, let’s ignore the *y* part, since we know the feature vectors
    all come from class *y*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以考虑 *P****(x***|*y*) 是什么。它是给定特征向量属于类 *y* 时的条件概率。暂时我们可以忽略 *y* 部分，因为我们知道特征向量都来自类
    *y*。
- en: This leaves only *P****(x***) because we fixed *y*. A feature vector is a collection
    of individual features, *x* = (***x[0]***, *x*[1], *x*[2], . . ., *x[n]*[–1])
    for *n* features in the vector. Therefore, *P****(x***) is really a joint probability,
    the probability that all the individual features have their specific values *at
    the same time*. So, we can write
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这只剩下了 *P****(x***)，因为我们固定了 *y*。特征向量是单独特征的集合，*x* = (***x[0]***, *x*[1], *x*[2],
    . . ., *x[n]*[–1])，其中 *n* 是向量中的特征数量。因此，*P****(x***) 其实是一个联合概率，表示所有单独特征同时具有其特定值的概率。所以，我们可以写成
- en: '*P*(*x*) = *P*(*x*[0], *x*[1], *x*[2], . . ., *x*[*n*–1])'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x*) = *P*(*x*[0], *x*[1], *x*[2], . . ., *x*[*n*–1])'
- en: How does this help? If we make one more assumption about our data, we’ll see
    that we can break up this joint probability in a convenient way. Let’s assume
    that all the features in our feature vector are independent. Recall that *independent*
    means the value of *x*[1], say, is in no way affected by the value of any other
    feature in the vector. This is typically not quite true, and for things like pixels
    in images *definitely* not true, but we’ll assume it’s true nonetheless. We’re
    naive to believe it’s true, hence the *Naive* in *Naive Bayes*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么帮助我们呢？如果我们对数据做出一个假设，我们会发现可以方便地拆解这个联合概率。假设我们的特征向量中的所有特征都是独立的。回想一下，*独立* 的意思是，*x*[1]
    的值，比如说，并不受向量中任何其他特征值的影响。这个通常并不完全正确，对于图像中的像素来说，*肯定* 不成立，但我们还是假设它成立。我们天真地相信它是对的，这也就是
    *朴素* 贝叶斯中的 *朴素*。
- en: 'If the features are independent, then the probability of a feature taking on
    any particular value is independent of all the others. In that case, the product
    rule tells us that we can break the joint probability up like so:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征是独立的，那么一个特征取特定值的概率就与其他所有特征的取值无关。在这种情况下，乘积规则告诉我们可以像这样将联合概率拆解：
- en: '*P*(*x*) = *P*(*x*[0])*P*(*x*[1])*P*(*x*[2]) . . . *P*(*x*[*n*–1])'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x*) = *P*(*x*[0])*P*(*x*[1])*P*(*x*[2]) . . . *P*(*x*[*n*–1])'
- en: This helps tremendously. We have a dataset, labeled by class, allowing us to
    estimate the probability of any feature for any specific class by counting how
    often each feature value happens for each class.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们帮助很大。我们有一个按类别标注的数据集，使我们能够通过计算每个特征值在每个类中出现的频率，来估算任何特征对于任何特定类的概率。
- en: Let’s put it all together for a hypothetical dataset of three classes—0, 1,
    and 2—and four features. We first use the dataset, partitioned by class label,
    to estimate each feature value probability. This provides us the set of *P**(x*[0]),
    *P*(*x*[1]), and so on, for each feature for each class label. Combined with the
    prior probability of the class label, estimated from the dataset as the number
    of each class divided by the total number of samples in the dataset, we calculate
    for a new unknown feature vector, ***x***,
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有内容结合起来，考虑一个假设的数据集，其中有三个类别——0、1和2——以及四个特征。我们首先使用按类别标签划分的数据集来估计每个特征值的概率。这为我们提供了每个类别标签下每个特征的*P**(x*[0])、*P*(*x*[1])等概率。结合类别标签的先验概率，该概率可以通过数据集中每个类别的样本数除以数据集中的总样本数来估计，我们可以为一个新的未知特征向量***x***进行计算，
- en: '![image](Images/064equ01.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/064equ01.jpg)'
- en: Here, the *P*(*x*[0]) feature probabilities are specific to class 0 only, and
    *P*(0) is the prior probability of class 0 in the dataset. *P*(0|***x***) is the
    unnormalized posterior probability that the unknown feature vector ***x*** belongs
    to class 0\. We say *unnormalized* because we’re ignoring the denominator of Bayes’
    theorem, knowing that including it would not change the ordering of the posterior
    probabilities, only their values.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*P*(*x*[0])特征概率仅针对类别0，*P*(0)是数据集中类别0的先验概率。*P*(0|***x***)是未归一化的后验概率，表示未知特征向量***x***属于类别0。我们称其为*未归一化*，因为我们忽略了贝叶斯定理中的分母，知道包含它不会改变后验概率的排序，仅会改变其值。
- en: We can repeat the calculation above to get *P*(1|***x***) and *P*(2|***x***),
    making sure to use the per feature probabilities calculated for those classes
    (the *P*(*x*[0])s). Finally, we give ***x*** the class label for the largest of
    the three posteriors calculated.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重复上述计算，得到*P*(1|***x***)和*P*(2|***x***)，确保使用为这些类别计算的每个特征概率（即*P*(*x*[0])）。最后，我们为***x***分配具有最大后验概率的类别标签。
- en: The description above assumes that the feature values are discrete. Usually
    they aren’t, but there are workarounds. One is to bin the feature values to make
    them discrete. For example, if the feature ranges over [0, 3], create a new feature
    that is 0, 1, or 2, and assign the continuous feature to one of those bins by
    truncating any fractional part.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述假设特征值是离散的。通常它们不是离散的，但可以采用一些变通方法。一种方法是将特征值分箱，使其变为离散。例如，如果特征的范围是[0, 3]，可以创建一个新的特征，其值为0、1或2，并通过截断小数部分将连续特征分配到这些箱中。
- en: Another workaround is to make one more assumption about the distribution the
    feature values come from and use that distribution to calculate the *P**(x*[0])s
    per class. Features are often based on measurements in the real world, and many
    things in the real world follow a normal distribution. Therefore, typically we’d
    assume that the individual features, while continuous, are normally distributed
    and we can find estimates of the mean (μ) and standard deviation (σ) from the
    dataset, per feature, and class label.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种变通方法是对特征值的分布做出假设，并使用该分布来计算每个类别的*P**(x*[0])。特征通常基于现实世界中的测量，而现实世界中的许多事物都遵循正态分布。因此，通常我们假设各个特征虽然是连续的，但它们服从正态分布，并且我们可以从数据集中找到每个特征和类别标签的均值（μ）和标准差（σ）的估计值。
- en: Bayes’ theorem is useful for calculating probabilities. It’s helpful in machine
    learning as well. The battle between Bayesians and frequentists appears to be
    waning, though philosophical differences remain. In practice, most researchers
    are learning that both approaches are valuable, and at times tools from both camps
    should be used. We’ll continue this trend in the next chapter, where we’ll examine
    statistics from a frequentist viewpoint. We defend this decision by pointing out
    that the vast majority of published scientific results in the last century used
    statistics this way, which includes the deep learning community, at least when
    it’s presenting the results of experiments.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理对于计算概率非常有用，在机器学习中也很有帮助。尽管贝叶斯学派与频率学派之间的争斗似乎有所减弱，但哲学上的分歧依然存在。在实践中，大多数研究人员发现这两种方法都有价值，有时应该同时使用两者的方法。我们将在下一章继续这一趋势，届时我们将从频率学派的角度审视统计学。我们通过指出在过去一个世纪里，绝大多数已发布的科学结果都采用了这种统计方法来辩护这一决定，其中也包括深度学习社区，至少在呈现实验结果时是如此。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter taught us about probability distributions, what they are, and how
    to draw samples from them, both discrete and continuous. We’ll encounter different
    distributions during our exploration of deep learning. We also discovered Bayes’
    theorem and saw how it lets us properly relate conditional probabilities. We saw
    how Bayes’ theorem allows us to evaluate the true likelihood of cancer given an
    imperfect medical test—a common situation. We also learned how to use Bayes’ theorem,
    along with some of the basic probability rules we learned in [Chapter 2](ch02.xhtml#ch02),
    to build a simple but often surprisingly effective classifier.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章教会了我们概率分布，它们是什么，以及如何从中抽样，无论是离散的还是连续的。在我们探索深度学习的过程中，我们将遇到不同的分布。我们还发现了贝叶斯定理，了解了它如何让我们正确地关联条件概率。我们看到贝叶斯定理如何帮助我们评估癌症的真实可能性，给定一个不完美的医学测试——这是一个常见的情境。我们还学习了如何使用贝叶斯定理，以及我们在[第二章](ch02.xhtml#ch02)中学到的一些基本概率规则，来构建一个简单但通常出奇有效的分类器。
- en: Let’s move now into the world of statistics.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入统计学的世界。
- en: '[1](#ch03fn01a). Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash
    K. Mansinghka, “The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for
    Discrete Probability Distributions,” in AISTATS 2020: Proceedings of the 23rd
    International Conference on Artificial Intelligence and Statistics, *Proceedings
    of Machine Learning Research* 108, Palermo, Sicily, Italy, 2020.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](#ch03fn01a)。Feras A. Saad、Cameron E. Freer、Martin C. Rinard 和 Vikash K.
    Mansinghka，"快速加载骰子滚动器：一种近似最优的离散概率分布精确采样器"，载于 AISTATS 2020：第23届国际人工智能与统计学会议论文集，*机器学习研究论文集*
    108，意大利西西里岛巴勒莫，2020年。'
