- en: '**3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3'
- en: MORE PROBABILITY**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 更多概率**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: '[Chapter 2](ch02.xhtml#ch02) introduced us to basic concepts of probability.
    In this chapter, we’ll continue our exploration of probability by focusing on
    two essential topics often encountered in deep learning and machine learning:
    probability distributions and how to sample from them, and Bayes’ theorem. Bayes’
    theorem is one of the most important concepts in probability theory, and it has
    produced a paradigm shift in the way many researchers think about probability
    and how to apply it.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](ch02.xhtml#ch02)向我们介绍了概率的基本概念。在本章中，我们将继续探索概率，重点讨论深度学习和机器学习中常见的两个重要话题：概率分布及如何从中采样，以及贝叶斯定理。贝叶斯定理是概率论中最重要的概念之一，它在许多研究人员思考概率及其应用的方式上带来了范式转变。'
- en: Probability Distributions
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概率分布
- en: A probability distribution can be thought of as a function that generates values
    on demand. The values generated are random—we don’t know which one will appear—but
    the likelihood of any value appearing follows a general form. For example, if
    we roll a standard die many times and tally how many times each number comes up,
    we expect that in the long run, each number is equally likely. Indeed, that’s
    the entire point of making the die in the first place. Therefore, the probability
    distribution of the die is known as a *uniform distribution*, since each number
    is equally likely to appear. We can imagine other distributions favoring one value
    or range of values over others, like a weighted die that might come up as six
    suspiciously often.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布可以看作是一个按需生成值的函数。生成的值是随机的——我们不知道哪个会出现——但任何值出现的可能性遵循一定的形式。例如，如果我们多次掷一个标准的骰子，并记录每个数字出现的次数，我们预计从长远来看，每个数字的出现机会是相等的。实际上，这就是最初制造骰子的目的。因此，骰子的概率分布被称为*均匀分布*，因为每个数字出现的可能性是相等的。我们还可以想象其他分布倾向于某个值或某个值的范围，比如一个加权的骰子，可能会出现六的概率异常高。
- en: Deep learning’s primary reason for sampling from a probability distribution
    is to initialize the network before training. Modern networks select the initial
    weights and sometimes biases from different distributions, most notably uniform
    and normal. The uniform distribution is familiar to us, and I’ll discuss the normal
    distribution, a continuous distribution, later.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习从概率分布中采样的主要原因是初始化网络权重。现代网络从不同的分布中选择初始权重，有时也选择偏置，最常见的是均匀分布和正态分布。均匀分布我们很熟悉，我稍后会讨论正态分布，这是一种连续分布。
- en: I’ll present several different kinds of probability distributions in this section.
    Our focus is to understand the shape of the distribution and to learn how to draw
    samples from it using NumPy. I’ll start with histograms to show you that we can
    often treat histograms as approximations of a probability distribution. Then I’ll
    discuss common *discrete probability distributions*. These are distributions returning
    integer values, like 3 or 7\. Lastly, I’ll switch to continuous distributions
    yielding floating-point numbers, like 3.8 or 7.592.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍几种不同类型的概率分布。我们的重点是理解分布的形状，并学习如何使用 NumPy 从中抽取样本。我将从直方图开始，向你展示我们如何通常将直方图视为概率分布的近似。接着我会讨论常见的*离散概率分布*，这些分布返回整数值，比如
    3 或 7。最后，我将转到连续分布，返回浮动值，比如 3.8 或 7.592。
- en: Histograms and Probabilities
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直方图与概率
- en: Take a look at [Table 3-1](ch03.xhtml#ch03tab01), which we saw in [Chapter 2](ch02.xhtml#ch02).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看我们在[第二章](ch02.xhtml#ch02)中看到的[表 3-1](ch03.xhtml#ch03tab01)。
- en: '**Table 3-1:** The Number of Combinations of Two Dice Leading to Different
    Sums (Copied from [Table 2-1](ch02.xhtml#ch02tab01))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-1:** 两颗骰子组合导致不同总和的组合数（摘自[表 2-1](ch02.xhtml#ch02tab01)）'
- en: '| **Sum** | **Combinations** | **Count** | **Probability** |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **总和** | **组合数** | **计数** | **概率** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 2 | 1 + 1 | 1 | 0.0278 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 + 1 | 1 | 0.0278 |'
- en: '| 3 | 1 + 2, 2 + 1 | 2 | 0.0556 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 + 2, 2 + 1 | 2 | 0.0556 |'
- en: '| 4 | 1 + 3, 2 + 2, 3 + 1 | 3 | 0.0833 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 + 3, 2 + 2, 3 + 1 | 3 | 0.0833 |'
- en: '| 5 | 1 + 4, 2 + 3, 3 + 2, 4 + 1 | 4 | 0.1111 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 + 4, 2 + 3, 3 + 2, 4 + 1 | 4 | 0.1111 |'
- en: '| 6 | 1 + 5, 2 + 4, 3 + 3, 4 + 2, 5 + 1 | 5 | 0.1389 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 + 5, 2 + 4, 3 + 3, 4 + 2, 5 + 1 | 5 | 0.1389 |'
- en: '| 7 | 1 + 6, 2 + 5, 3 + 4, 4 + 3, 5 + 2, 6 + 1 | 6 | 0.1667 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 + 6, 2 + 5, 3 + 4, 4 + 3, 5 + 2, 6 + 1 | 6 | 0.1667 |'
- en: '| 8 | 2 + 6, 3 + 5, 4 + 4, 5 + 3, 6 + 2 | 5 | 0.1389 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2 + 6, 3 + 5, 4 + 4, 5 + 3, 6 + 2 | 5 | 0.1389 |'
- en: '| 9 | 3 + 6, 4 + 5, 5 + 4, 6 + 3 | 4 | 0.1111 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 3 + 6, 4 + 5, 5 + 4, 6 + 3 | 4 | 0.1111 |'
- en: '| 10 | 4 + 6, 5 + 5, 6 + 4 | 3 | 0.0833 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 4 + 6, 5 + 5, 6 + 4 | 3 | 0.0833 |'
- en: '| 11 | 5 + 6, 6 + 5 | 2 | 0.0556 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 5 + 6, 6 + 5 | 2 | 0.0556 |'
- en: '| 12 | 6 + 6 | 1 | 0.0278 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 6 + 6 | 1 | 0.0278 |'
- en: '|  |  | 36 | 1.0000 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 36 | 1.0000 |'
- en: It shows how two dice add to different sums. Don’t look at the actual values;
    look at the shape the possible combinations make. If we chop off the last two
    columns, turn the table to the left, and replace each sum with an “X,” we should
    see something like the following.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它展示了两个骰子加起来如何得到不同的和。不要看实际的数值；要看可能组合的形状。如果我们去掉最后两列，将表格转向左边，并将每个和替换为“X”，我们应该看到如下内容：
- en: '|  |  |  |  |  | × |  |  |  |  |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  | × |  |  |  |  |  |'
- en: '|  |  |  |  | × | × | × |  |  |  |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | × | × | × |  |  |  |  |'
- en: '|  |  |  | × | × | × | × | × |  |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | × | × | × | × | × |  |  |  |'
- en: '|  |  | × | × | × | × | × | × | × |  |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |  | × | × | × | × | × | × | × |  |  |'
- en: '|  | × | × | × | × | × | × | × | × | × |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | × | × | × | × | × | × | × | × | × |  |'
- en: '| × | × | × | × | × | × | × | × | × | × | × |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| × | × | × | × | × | × | × | × | × | × | × |'
- en: '| 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |'
- en: You can see that there’s a definite shape and symmetry to the number of ways
    to arrive at each sum. This kind of plot is called a *histogram*. A histogram
    is a plot tallying the number of things that fall into discrete bins. For [Table
    3-1](ch03.xhtml#ch03tab01), the bins are the numbers 2 through 12\. The tally
    is a possible way to get that sum. Histograms are often represented as bar graphs,
    usually vertical bars, though they need not be. [Table 3-1](ch03.xhtml#ch03tab01)
    is basically a horizontal histogram. How many bins are used in the histogram is
    up to the maker. If you use too few, the histogram will be blocky and may not
    reveal necessary detail because interesting features have all been lumped into
    the same bin. Use too many bins, and the histogram will be sparse, with many bins
    having no tallies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，到达每个和的方式有明确的形状和对称性。这种图形叫做*直方图*。直方图是一个统计落入离散区间的事物数量的图表。对于[表3-1](ch03.xhtml#ch03tab01)，这些区间是从2到12的数字。计数是达到该和的一种可能方式。直方图通常以条形图的形式表示，通常是竖直条形，尽管不一定非得如此。[表3-1](ch03.xhtml#ch03tab01)基本上是一个水平直方图。直方图使用多少个区间取决于制作人员。如果使用太少的区间，直方图将显得块状，可能无法揭示必要的细节，因为有趣的特征都被归入了同一个区间。如果使用太多的区间，直方图将显得稀疏，许多区间没有计数。
- en: 'Let’s generate some histograms. First, we’ll randomly sample integers in [0,9]
    and count how many of each integer we get. The code for this is straightforward:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来生成一些直方图。首先，我们将随机抽取[0,9]范围内的整数，并统计每个整数的出现次数。实现这一过程的代码很简单：
- en: '>>> import numpy as np'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import numpy as np'
- en: '>>> n = np.random.randint(0,10,10000)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> n = np.random.randint(0,10,10000)'
- en: '>>> h = np.bincount(n)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> h = np.bincount(n)'
- en: '>>> h'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> h'
- en: array([ 975, 987, 987, 1017, 981, 1043, 1031, 988, 1007, 984])
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: array([ 975, 987, 987, 1017, 981, 1043, 1031, 988, 1007, 984])
- en: We first set n to an array of 10,000 integers in [0, 9]. We then use np .bincount
    to count how many of each digit we have. We see that this run gave us 975 zeros
    and 984 nines. If the NumPy pseudorandom generator is doing its job, we expect,
    on average, to have 1,000 of each digit in a sample of 10,000 digits. We expect
    some variation, but most values are close enough to 1,000 to be convincing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将n设置为一个包含10,000个整数的数组，范围是[0, 9]。然后，我们使用np.bincount来统计每个数字出现的次数。我们看到这次运行给我们带来了975个零和984个九。如果NumPy的伪随机生成器工作正常，我们期望在10,000个数字中每个数字的平均出现次数为1,000。我们预计会有一些波动，但大多数值会足够接近1,000，从而令人信服。
- en: The counts above tell us how many times each digit appeared. If we divide each
    bin of a histogram by the total of all the bins, we change from simple counts
    to the probability of that bin appearing. For the random digits above, we get
    the probabilities with
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述计数告诉我们每个数字出现的次数。如果我们将每个直方图的区间除以所有区间的总和，我们就将简单的计数转换为该区间出现的概率。对于上述随机数字，我们可以通过以下方式计算概率：
- en: '>>> h = h / h.sum()'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> h = h / h.sum()'
- en: '>>> h'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> h'
- en: array([0.0975, 0.0987, 0.0987, 0.1017, 0.0981, 0.1043, 0.1031, 0.0988,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: array([0.0975, 0.0987, 0.0987, 0.1017, 0.0981, 0.1043, 0.1031, 0.0988,
- en: 0.1007, 0.0984])
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 0.1007, 0.0984])
- en: which tells us that each digit did appear with a probability of about 0.1, or
    1 out of 10\. This trick of dividing histogram values by the sum of the counts
    in the histogram allows us to estimate probability distributions from samples.
    It also tells us the likelihood of particular values appearing when sampling from
    whatever process generated the data used to make the histogram. You should note
    that I said we could *estimate* the probability distribution from a set of samples
    drawn from it. The larger the number of samples, the closer the estimated probability
    distribution will be to the actual population distribution generating the samples.
    We will never get to the actual population distribution, but given the limit of
    an infinite number of samples, we can get as close as we need to.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，每个数字出现的概率大约为0.1，即1/10。通过将直方图中的值除以直方图中计数的总和，我们可以从样本中估计概率分布。它还告诉我们，在从生成用于制作直方图的数据的任何过程进行采样时，特定值出现的可能性。你应该注意，我说过我们可以*估计*从一组样本中得出的概率分布。样本数量越大，估计的概率分布就越接近实际的生成样本的总体分布。我们永远无法得到实际的总体分布，但在无限数量样本的限制下，我们可以得到足够接近的结果。
- en: Histograms are frequently used to look at the distribution of pixel values in
    an image. Let’s make a plot of the histogram of the pixels in two images. You
    can find the code in the file ricky.py. (I won’t show it here, as it doesn’t add
    to the discussion.) The images used are two example grayscale images included
    with SciPy in scipy.misc. The first shows people walking up stairs (ascent), and
    the second is the face of a young raccoon (face), as shown in [Figure 3-1](ch03.xhtml#ch03fig01).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图常常用于查看图像中像素值的分布。让我们绘制两张图像的像素直方图。你可以在文件 ricky.py 中找到代码。（我这里不展示它，因为它与讨论无关。）使用的图像是
    SciPy 中 scipy.misc 包含的两张示例灰度图像。第一张展示了人们走上楼梯（上升），第二张是年轻浣熊的面部特写（面孔），如[图 3-1](ch03.xhtml#ch03fig01)所示。
- en: '![image](Images/03fig01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig01.jpg)'
- en: '*Figure 3-1: People ascending (left) and “Ricky” raccoon (right)*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-1：人们上楼梯（左）和“Ricky”浣熊（右）*'
- en: '[Figure 3-2](ch03.xhtml#ch03fig02) provides a plot of the histograms for each
    image, as probabilities. It shows two very different distributions of gray level
    values in the images. For the raccoon face, the distribution is more spread out
    and flatter, while the ascent image has a spike right around gray level 128 and
    a few bright pixels. The distributions tell us that if we pick a random pixel
    in the face image, we’re most likely to get one around gray level 100, but an
    arbitrary pixel in the ascent image will, with high relative likelihood, be closer
    to gray level 128.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](ch03.xhtml#ch03fig02)提供了每张图像的直方图概率图。它展示了图像中灰度级值的两种非常不同的分布。对于浣熊的面部图像，分布较为分散和平坦，而上升图像在灰度级128附近有一个尖峰，并且有一些明亮的像素。这些分布告诉我们，如果我们从面部图像中随机选择一个像素，我们最有可能得到接近灰度级100的值，而在上升图像中，任意像素相对较有可能接近灰度级128。'
- en: '![image](Images/03fig02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig02.jpg)'
- en: '*Figure 3-2: Histograms as probabilities for two 512×512-pixel grayscale sample
    images*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-2：两张512×512像素灰度样本图像的概率直方图*'
- en: Again, histograms tally the counts of how many items fall into the predefined
    bins. We saw for the images that the histogram as probability distribution tells
    us how likely we are to get a particular gray level value if we select a random
    pixel. Likewise, the probability distribution for the random digits in the example
    before that tells us the probability of getting each digit when we ask for a random
    integer in the range [0,9].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，直方图统计了多少项落入预定义的箱子中。我们看到，对于图像，作为概率分布的直方图告诉我们，如果随机选择一个像素，得到特定灰度级值的可能性有多大。同样，之前示例中的随机数字概率分布告诉我们，在请求一个范围为[0,9]的随机整数时，每个数字出现的概率。
- en: Histograms are discrete representations of a probability distribution. Let’s
    take a look at the more common discrete distributions now.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图是概率分布的离散表示。现在，让我们来看看更常见的离散分布。
- en: Discrete Probability Distributions
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离散概率分布
- en: 'We’ve already encountered the most common discrete distribution several times:
    it’s the uniform distribution. That’s the one we get naturally by rolling dice
    or flipping coins. In the uniform distribution, all possible outcomes are equally
    likely. A histogram of a simulation of a process drawing from a uniform distribution
    is flat; all outcomes show up with more or less the same frequency. We’ll see
    the uniform distribution again when we look at continuous distributions. For now,
    think dice.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次遇到过最常见的离散分布：它是均匀分布。那就是我们通过掷骰子或抛硬币自然得到的分布。在均匀分布中，所有可能的结果是等概率的。模拟均匀分布过程的直方图是平的；所有结果出现的频率大致相同。我们将在查看连续分布时再次看到均匀分布。现在，想象一下骰子。
- en: Let’s look at a few other discrete distributions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下其他几个离散分布。
- en: The Binomial Distribution
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二项分布
- en: Perhaps the second most common discrete distribution is the *binomial distribution*.
    This distribution represents the expected number of events happening in a given
    number of trials if each event has a specified probability. Mathematically, the
    probability of *k* events happening in *n* trials if the probability of the event
    happening is *p* can be written as
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可能第二常见的离散分布是*二项分布*。这种分布表示在给定次数的试验中，如果每个事件都有一个特定的概率，那么期望发生的事件数量。数学上，如果事件发生的概率是*p*，则在*n*次试验中发生*k*个事件的概率可以写作
- en: '![image](Images/046equ01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ01.jpg)'
- en: For example, what’s the probability of getting three heads in a row when flipping
    a fair coin three times? From the product rule, we know the probability is
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当抛掷公平硬币三次时，连续三次出现正面朝上的概率是多少？根据乘法规则，我们知道其概率是
- en: '![image](Images/046equ02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ02.jpg)'
- en: Using the binomial formula, we get the same answer by calculating
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二项式公式，我们可以通过计算得到相同的答案
- en: '![image](Images/046equ03.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ03.jpg)'
- en: So far, not particularly helpful. However, what if the probability of the event
    isn’t 0.5? What if we have an event, say the likelihood of a person winning *Let’s
    Make a Deal* by not changing doors, and we want to know the probability that 7
    people out of 13 will win by not changing their guess? We know the probability
    of winning the game without changing doors is 1/3—that’s *p*. We then have 13
    trials (*n*) and 7 winners (*k*). The binomial formula tells us the likelihood
    is
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，还不特别有帮助。但是，如果事件的概率不是0.5呢？如果我们有一个事件，比如一个人通过不换门赢得*Let’s Make a Deal*的概率，我们想知道在13个人中有7个人通过不更换猜测赢得比赛的概率是多少？我们知道不换门的获胜概率是1/3——那就是*p*。然后我们有13次试验（*n*）和7个赢家（*k*）。二项式公式告诉我们其可能性是
- en: '![image](Images/046equ04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ04.jpg)'
- en: and, if the players *do* switch doors,
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，如果玩家*确实*换门，
- en: '![image](Images/046equ05.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/046equ05.jpg)'
- en: The binomial formula gives us the probability of a given number of events in
    a given number of trials for a specified probability per event. If we fix *n*
    and *p* and vary *k*, 0 ≤ *k* ≤ *n*, we get the probability for each *k* value.
    This gives us the distribution. For example, let *n* = 5 and *p* = 0.3, then 0
    ≤ *k* ≤ 5 with the probability for each *k* value as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 二项式公式给出了在给定次数的试验中对于每次试验指定概率下的事件数的概率。如果我们固定*n*和*p*，并改变*k*，0 ≤ *k* ≤ *n*，我们就能得到每个*k*值的概率。这给出了分布。例如，设*n*
    = 5，*p* = 0.3，那么0 ≤ *k* ≤ 5，每个*k*值的概率为
- en: '![image](Images/047equ01.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/047equ01.jpg)'
- en: Allowing for rounding, this sums to 1.0, as we know it must because the sum
    of probabilities over an entire sample space is always 1.0\. Notice that we calculate
    all the possible values for the binomial distribution when *n* = 5\. Collectively,
    this specifies the *probability mass function (pmf)*. The probability mass function
    tells us the probability associated with all possible outcomes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到四舍五入，这总和为1.0，因为我们知道总概率在整个样本空间上总是等于1.0。请注意，我们计算了当*n* = 5时，二项分布的所有可能值。总的来说，这指定了*概率质量函数（pmf）*。概率质量函数告诉我们与所有可能结果相关的概率。
- en: 'The binomial distribution is parameterized by *n* and *p*. For *n* = 5 and
    = *p* = 0.3, we see from the results above that a random sample from such a binomial
    distribution will return 1 most often—some 36 percent of the time. How can we
    draw samples from a binomial distribution? In NumPy, we need only call the binomial
    function in the random module:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布由*n*和*p*参数化。对于*n* = 5和*p* = 0.3，从上面的结果我们可以看到，来自这样的二项分布的随机样本最常返回1——大约36%的概率。我们如何从二项分布中抽取样本呢？在NumPy中，我们只需要调用随机模块中的二项函数：
- en: '>>> t = np.random.binomial(5, 0.3, size=1000)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t = np.random.binomial(5, 0.3, size=1000)'
- en: '>>> s = np.bincount(t)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s = np.bincount(t)'
- en: '>>> s'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s'
- en: array([159, 368, 299, 155,  17,   2])
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: array([159, 368, 299, 155, 17, 2])
- en: '>>> s / s.sum()'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s / s.sum()'
- en: array([0.159, 0.368, 0.299, 0.155, 0.017, 0.002])
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: array([0.159, 0.368, 0.299, 0.155, 0.017, 0.002])
- en: We pass binomial the number of trials (5) and the probability of success for
    each trial (0.3). We then ask for 1,000 samples from a binomial distribution with
    these parameters. Using np.bincount, we see that the most commonly returned value
    was indeed 1, as we calculated above. By using our histogram summation trick,
    we get a probability of 0.368 for selecting a 1—close to the 0.3601 we calculated.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给二项分布的参数包括试验次数（5）和每次试验的成功概率（0.3）。然后我们要求从这个参数下的二项分布中取1000个样本。通过使用np.bincount，我们看到返回值最常见的确实是1，正如我们之前计算的那样。通过使用我们的直方图求和技巧，我们得到了0.368的选择1的概率——接近我们计算的0.3601。
- en: The Bernoulli Distribution
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 伯努利分布
- en: The *Bernoulli distribution* is a special case of the binomial distribution.
    In this case, we fix *n* = 1, meaning there’s only one trial. The only values
    we can sample are 0 or 1; either the event happens, or it doesn’t. For example,
    with *p* = 0.5, we get
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*伯努利分布*是二项分布的一种特殊情况。在这种情况下，我们固定*n* = 1，意味着只有一次试验。我们只能取样0或1；事件发生或不发生。例如，当*p*
    = 0.5时，我们得到'
- en: '>>> t = np.random.binomial(1, 0.5, size=1000)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t = np.random.binomial(1, 0.5, size=1000)'
- en: '>>> np.bincount(t)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.bincount(t)'
- en: array([496, 504])
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: array([496, 504])
- en: This is reasonable, since a probability of 0.5 means we’re flipping a fair coin,
    and we see that the proportion of heads or tails is roughly equal.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是合理的，因为概率为0.5意味着我们在掷一个公平的硬币，并且我们看到正面或反面的比例大致相等。
- en: If we change to *p* = 0.3, we get
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将*p*改为0.3，我们得到
- en: '>>> t = np.random.binomial(1, 0.3, size=1000)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t = np.random.binomial(1, 0.3, size=1000)'
- en: '>>> np.bincount(t)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.bincount(t)'
- en: array([665, 335])
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: array([665, 335])
- en: '>>> 335/1000'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> 335/1000'
- en: '0.335'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '0.335'
- en: Again, close to 0.3, as we expect to see.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 再次接近0.3，正如我们预期的那样。
- en: Use samples from a binomial distribution when you want to simulate events with
    a known probability. With the Bernoulli form, we can sample binary outcomes, 0
    or 1, where the likelihood of the event need not be that of the flip of a fair
    coin, 0.5.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想模拟具有已知概率的事件时，使用二项分布的样本。在伯努利形式下，我们可以采样二元结果0或1，其中事件发生的可能性不一定是公平硬币翻转的概率0.5。
- en: The Poisson Distribution
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 泊松分布
- en: Sometimes, we don’t know the probability of an event happening for any particular
    trial. Instead, we might know the average number of events that happen over some
    interval, say of time. If the average number of events that happen over some time
    is λ (lambda), then the probability of *k* events happening in that interval is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们并不知道某个特定试验发生事件的概率。相反，我们可能知道在某个时间间隔内发生事件的平均数量。假设在某段时间内，平均发生的事件数是λ（lambda），那么在该时间间隔内发生*k*个事件的概率为
- en: '![image](Images/048equ01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/048equ01.jpg)'
- en: This is the *Poisson distribution*, and it’s useful to model events like radioactive
    decay or the incidence of photons on an X-ray detector over some period of time.
    To sample events according to this distribution, we use poisson from the random
    module. For example, assume over some time interval there are five events on average
    (λ = 5). What sort of probability distribution do we get using the Poisson distribution?
    In code,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*泊松分布*，它对于模拟诸如放射性衰变或X射线探测器上光子发生的事件非常有用。为了根据这个分布来取样事件，我们使用random模块中的poisson。例如，假设在某个时间间隔内，平均发生五个事件（λ
    = 5）。我们使用泊松分布会得到什么样的概率分布？代码如下：
- en: '>>> t = np.random.poisson(5, size=1000)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t = np.random.poisson(5, size=1000)'
- en: '>>> s = np.bincount(t)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s = np.bincount(t)'
- en: '>>> s'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s'
- en: array([  6,  36,  83, 135, 179, 173, 156, 107,  58,  40,  20,   4,   2,
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: array([ 6, 36, 83, 135, 179, 173, 156, 107, 58, 40, 20, 4, 2,
- en: 0,   0,   1])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 0,   0,   1])
- en: '>>> t.max()'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t.max()'
- en: '15'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '15'
- en: '>>> s = s / s.sum()'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s = s / s.sum()'
- en: '>>> s'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> s'
- en: array([0.006, 0.036, 0.083, 0.135, 0.179, 0.173, 0.156, 0.107, 0.058,
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: array([0.006, 0.036, 0.083, 0.135, 0.179, 0.173, 0.156, 0.107, 0.058,
- en: 0.04 , 0.02 , 0.004, 0.002, 0.   , 0.   , 0.001])
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 0.04 , 0.02 , 0.004, 0.002, 0.   , 0.   , 0.001])
- en: Here, we see that, unlike the binomial distribution, which could not select
    more than *n* events, the Poisson distribution can select numbers of events that
    exceed the value of λ. In this case, the largest number of events in the time
    interval was 15, which is three times the average. You’ll find that the most frequent
    number of events is right around the average of five, as you might expect, but
    significant deviations from the average are possible.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到，与二项分布不同，二项分布不能选择超过*n*个事件，而泊松分布可以选择超过λ值的事件数量。在这种情况下，时间间隔内最大事件数为15，是平均值的三倍。正如你可能预期的那样，最常见的事件数是接近平均值5的，但也可能出现与平均值显著偏离的情况。
- en: The Fast Loaded Dice Roller
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速加载骰子滚动器
- en: What if we need to draw samples according to an arbitrary discrete distribution?
    Earlier, we saw some histograms based on images. In that case, we could sample
    from the distribution represented by the histogram by picking pixels in the image
    at random. But what if we wanted to sample integers according to arbitrary weights?
    To do this, we can use the new Fast Loaded Dice Roller of Saad, et al.[¹](#ch03fn01)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要根据任意离散分布抽取样本，该怎么办呢？之前我们看过一些基于图像的直方图。在那种情况下，我们可以通过随机选择图像中的像素来从直方图所表示的分布中抽样。但如果我们想根据任意权重抽取整数呢？为此，我们可以使用Saad等人提出的新工具——快速加载骰子滚动器（Fast
    Loaded Dice Roller）[¹](#ch03fn01)。
- en: The *Fast Loaded Dice Roller (FLDR)* lets us specify an arbitrary discrete distribution
    and then draw samples from it. The code is in Python and freely available. (See
    *[https://github.com/probcomp/fast-loaded-dice-roller/](https://github.com/probcomp/fast-loaded-dice-roller/)*.)
    I’ll show how to use the code to sample according to a generic distribution. I
    recommend downloading just the fldr.py and fldrf.py files from the GitHub repository
    instead of running setup.py. Additionally, edit the .fldr import lines in fldrf.py
    to remove the “.” so they read
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*快速加载骰子滚动器（FLDR）* 让我们可以指定一个任意的离散分布，并从中抽取样本。代码是用Python编写并且可以自由获取。（参见 *[https://github.com/probcomp/fast-loaded-dice-roller/](https://github.com/probcomp/fast-loaded-dice-roller/)*。）我将展示如何使用代码根据通用分布进行抽样。我建议从GitHub仓库中只下载fldr.py和fldrf.py文件，而不是运行setup.py。此外，编辑fldrf.py中的.fldr导入行，去掉“.”，让它们改为：'
- en: from fldr import fldr_preprocess_int
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: from fldr import fldr_preprocess_int
- en: from fldr import fldr_s
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: from fldr import fldr_s
- en: 'Using FLDR requires two steps. The first is to tell it the particular distribution
    you want to sample from. You define the distribution as ratios. (For our purposes,
    we’ll use actual probabilities, meaning our distribution will always add up to
    1.0.) This is the preprocessing step, which we only need to do once for each distribution.
    After that, we can draw samples. An example will clarify:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FLDR需要两个步骤。第一个步骤是告诉它你想要从哪个特定的分布中抽样。你通过定义分布的比率来描述该分布。（为了我们的目的，我们将使用实际概率，也就是说我们的分布的总和始终为1.0。）这是预处理步骤，每个分布我们只需要做一次。之后，我们就可以开始抽样了。下面是一个示例来说明：
- en: '>>> from fldrf import fldr_preprocess_float_c'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from fldrf import fldr_preprocess_float_c'
- en: '>>> from fldr import fldr_sample'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from fldr import fldr_sample'
- en: '>>> x = fldr_preprocess_float_c([0.6,0.2,0.1,0.1])'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> x = fldr_preprocess_float_c([0.6,0.2,0.1,0.1])'
- en: '>>> t = [fldr_sample(x) for i in range(1000)]'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t = [fldr_sample(x) for i in range(1000)]'
- en: '>>> np.bincount(t)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.bincount(t)'
- en: array([598, 190, 108, 104])
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: array([598, 190, 108, 104])
- en: 'First, we import the two FLDR functions we need: fldr_preprocess_float _c and
    fldr_sample. Then we define the distribution using a list of four numbers. Four
    numbers imply samples will be integers in [0, 3]. However, unlike a uniform distribution,
    here we’re specifying we want zero 60 percent of the time, one 20 percent of the
    time, and two and three 10 percent of the time each. The information that FLDR
    needs to sample from the distribution is returned in x.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入需要的两个FLDR函数：fldr_preprocess_float_c和fldr_sample。然后，我们使用一个包含四个数字的列表来定义分布。四个数字意味着样本将是[0,
    3]之间的整数。然而，与均匀分布不同，这里我们指定的是：零的概率为60%，一的概率为20%，二和三的概率各为10%。FLDR需要的信息以x的形式返回，用于从分布中抽样。
- en: 'Calling fldr_sample returns a single sample from the distribution. Notice two
    things: first, we need to pass x in, and second, FLDR doesn’t use NumPy, so to
    draw 1,000 samples, we use a standard Python list comprehension. The 1,000 samples
    are in the list, t. Finally, we generate the histogram and see that nearly 60
    percent of the samples are zero and slightly more than 10 percent are three, as
    we intended.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `fldr_sample` 返回从分布中抽取的单个样本。请注意两点：首先，我们需要传入 `x`，其次，FLDR 不使用 NumPy，因此为了绘制
    1,000 个样本，我们使用标准的 Python 列表推导式。1,000 个样本存储在列表 `t` 中。最后，我们生成直方图，并看到近 60% 的样本为零，略多于
    10% 的样本为三，这正是我们预期的结果。
- en: Let’s use the histogram of the raccoon face image we used earlier to see if
    FLDR will follow a more complex distribution. We’ll load the image, generate the
    histogram, convert it to a probability distribution, and use the probabilities
    to set up FLDR. After that, we’ll draw 25,000 samples from the distribution, compute
    the histogram of the samples, and plot that histogram along with the original
    histogram to see if FLDR follows the actual distribution we give it. The code
    we need is
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前提到的浣熊面部图像的直方图，看看 FLDR 是否会遵循更复杂的分布。我们将加载图像，生成直方图，将其转换为概率分布，然后使用这些概率来设置
    FLDR。之后，我们将从该分布中抽取 25,000 个样本，计算这些样本的直方图，并将这个直方图与原始直方图一起绘制，看看 FLDR 是否遵循我们提供的实际分布。我们需要的代码如下：
- en: from scipy.misc import face
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.misc import face
- en: im = face(True)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: im = face(True)
- en: b = np.bincount(im.ravel(), minlength=256)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: b = np.bincount(im.ravel(), minlength=256)
- en: b = b / b.sum()
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: b = b / b.sum()
- en: x = fldr_preprocess_float_c(list(b))
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: x = fldr_preprocess_float_c(list(b))
- en: t = [fldr_sample(x) for i in range(25000)]
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: t = [fldr_sample(x) for i in range(25000)]
- en: q = np.bincount(t, minlength=256)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: q = np.bincount(t, minlength=256)
- en: q = q / q.sum()
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: q = q / q.sum()
- en: Running this code leaves us with b, a probability distribution from the histogram
    of the face image, and q, the distribution created from 25,000 samples from the
    FLDR distribution. [Figure 3-3](ch03.xhtml#ch03fig03) shows us a plot of the two
    distributions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码后，我们得到了 `b`，它是面部图像直方图的概率分布，以及 `q`，它是从 FLDR 分布中提取的 25,000 个样本生成的分布。[图 3-3](ch03.xhtml#ch03fig03)
    展示了这两个分布的对比图。
- en: The solid line in [Figure 3-3](ch03.xhtml#ch03fig03) is the probability distribution
    we supplied to fldr_preprocess_float_c representing the distribution of gray levels
    (intensities) in the raccoon image. The dashed line is the histogram of the 25,000
    samples from this distribution. As we can see, they follow the requested distribution
    with the sort of variation we expect from such a small number of samples. As an
    exercise, change the number of samples from 25,000 to 500,000 and plot the two
    curves. You’ll see that they’re now virtually on top of each other.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-3](ch03.xhtml#ch03fig03)中的实线是我们提供给 `fldr_preprocess_float_c` 的概率分布，表示浣熊图像中的灰度级（强度）分布。虚线是来自这个分布的
    25,000 个样本的直方图。正如我们所看到的，它们遵循了请求的分布，并且有着我们从如此少量样本中预期的变化。作为练习，将样本数量从 25,000 改为 500,000
    并绘制这两条曲线。你会看到它们几乎完全重合在一起。'
- en: '![image](Images/03fig03.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig03.jpg)'
- en: '*Figure 3-3: Comparing the Fast Loaded Dice Roller distribution (dashed) to
    the distribution generated from the SciPy face image (solid)*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-3：比较快速加载骰子滚动器分布（虚线）与从 SciPy 面部图像生成的分布（实线）*'
- en: Discrete distributions generate integers with specific likelihoods. Let’s leave
    them now and consider continuous probability distributions, which return floating-point
    values instead.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 离散分布生成具有特定概率的整数。现在让我们跳过这些，考虑连续概率分布，它们返回的是浮点数值。
- en: Continuous Probability Distributions
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续概率分布
- en: I haven’t discussed continuous probabilities yet in this chapter. In part, not
    doing so was to make the concepts behind probability easier to follow. A continuous
    probability distribution, like a discrete one, has a particular shape. However,
    instead of assigning a probability to a specific integer value, as we saw above,
    the probability of selecting a particular value from a continuous distribution
    is zero. The probability of a specific value, a real number, is zero because there
    are an infinite number of possible values from a continuous distribution; this
    means no particular value can be selected. Instead, what we talk about is the
    probability of selecting values in a specific range of values.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中还没有讨论连续概率。部分原因是为了让概率背后的概念更容易理解。像离散分布一样，连续概率分布也有特定的形状。然而，和上面看到的那样，连续分布不会给特定的整数值分配概率，因为从连续分布中选择一个特定值的概率是零。一个特定值的概率为零，因为连续分布有无限多的可能值；这意味着不能选择一个特定值。相反，我们讨论的是在一个特定范围内选择值的概率。
- en: For example, the most common continuous distribution is the uniform distribution
    over [0, 1]. This distribution returns *any* real number in that range. Although
    the probability of returning a specific real number is zero, we can talk about
    the probability of returning a value in a range, such as [0, 0.25].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，最常见的连续分布是[0, 1]区间上的均匀分布。这个分布会返回该区间内的*任何*实数。尽管返回一个特定实数的概率是零，但我们可以讨论返回一个范围内的值的概率，例如[0,
    0.25]。
- en: Consider again the uniform distribution over [0, 1]. We know that the sum of
    all the individual probabilities from zero to one is 1.0\. So, what is the probability
    of sampling a value from this distribution and having that value be in the range
    [0, 0.25]? All values are equally likely, and all add to 1.0, so we must have
    a 25 percent chance of returning a value in [0, 0.25]. Similarly, we have a 25
    percent chance of returning a value in [0.75, 1], as that also covers 1/4 of the
    possible range.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑[0, 1]区间上的均匀分布。我们知道，从零到一的所有单个概率的总和是1.0。那么，从这个分布中抽取一个值，并且这个值落在[0, 0.25]区间内的概率是多少呢？所有值的概率是相等的，所有的概率加起来是1.0，所以我们必须有25%的概率返回一个位于[0,
    0.25]区间内的值。同样，我们有25%的概率返回一个位于[0.75, 1]区间内的值，因为它也涵盖了可能范围的1/4。
- en: When we talk about summing infinitely small things over a range, we’re talking
    about integration, the part of calculus that we won’t cover in this book. Conceptually,
    however, we can understand what’s happening if we think about a discrete distribution
    in the limit where the number of values it can return goes to infinity, and we’re
    summing the probabilities over some range.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论在某个区间内对无限小的量进行求和时，我们实际上是在讨论积分，这是微积分的一部分，而这一部分内容我们在本书中不会覆盖。然而，从概念上讲，如果我们考虑一个离散分布的极限，其中它可以返回的值的数量趋于无限，并且我们在某个区间内对概率进行求和，我们就能理解发生了什么。
- en: We also can think about this graphically. [Figure 3-4](ch03.xhtml#ch03fig04)
    shows the continuous probability distributions I’ll discuss.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以从图形上思考这个问题。[图 3-4](ch03.xhtml#ch03fig04)展示了我将要讨论的连续概率分布。
- en: '![image](Images/03fig04.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig04.jpg)'
- en: '*Figure 3-4: Some common continuous probability distributions*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-4：一些常见的连续概率分布*'
- en: To get the probability of sampling a value in some range, we add up the area
    under the curve over that range. Indeed, this is precisely what integration does;
    the integration symbol (∫) is nothing more than a fancy “S” for *sum*. It’s the
    continuous version of ∑ for summing discrete values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得在某个区间内抽取值的概率，我们需要加总该区间下曲线的面积。事实上，这正是积分所做的；积分符号（∫）不过是一个花哨的“ S ”，代表*求和*。它是求和离散值的∑的连续版本。
- en: The distributions in [Figure 3-4](ch03.xhtml#ch03fig04) are the most common
    ones you’ll encounter, though there are many others useful enough to be given
    names. All of these distributions have associated *probability density functions
    (pdfs)*, closed-form functions that generate the probabilities that sampling from
    the distribution will give. I generated the curves in [Figure 3-4](ch03.xhtml#ch03fig04)
    instead using the code in the file continuous.py. The curves are estimates of
    the probability density functions, and I created them from the histogram of a
    large number of samples. I did so intentionally to demonstrate that the NumPy
    random functions sampling from these distributions do what they claim.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-4](ch03.xhtml#ch03fig04)中的分布是你最常遇到的几种，虽然还有许多其他有用到足以被命名的分布。所有这些分布都有相应的*概率密度函数
    (pdfs)*，它们是生成从分布中采样所得到的概率的闭式函数。我使用文件continuous.py中的代码生成了[图 3-4](ch03.xhtml#ch03fig04)中的曲线。这些曲线是概率密度函数的估计值，我是通过大量样本的直方图创建它们的。我这样做的目的是为了展示NumPy的随机函数从这些分布中采样确实如它们所宣称的那样。'
- en: Pay little attention to the x-axis in [Figure 3-4](ch03.xhtml#ch03fig04). The
    distributions have different ranges of output; they’re scaled here to fit all
    of them on the graph. The important thing to notice is their shapes. The uniform
    distribution is, well, uniform over the entire range. The normal curve, also frequently
    called a *Gaussian* or a *bell curve*, is the second most common distribution
    used in deep learning. For example, the He initialization strategy for neural
    networks samples initial weights from a normal distribution.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对[图 3-4](ch03.xhtml#ch03fig04)中的x轴不必过多关注。这些分布的输出范围不同；它们在这里被缩放，以便所有分布都能显示在图上。重要的是要注意它们的形状。均匀分布是，嗯，在整个范围内都是均匀的。正态曲线，也常被称为*高斯曲线*或*钟形曲线*，是深度学习中第二常用的分布。例如，神经网络的He初始化策略就是从正态分布中采样初始权重。
- en: 'The code generating the data for [Figure 3-4](ch03.xhtml#ch03fig04) is worth
    considering, as it shows us how to use NumPy to get samples:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 生成[图 3-4](ch03.xhtml#ch03fig04)数据的代码值得注意，因为它向我们展示了如何使用NumPy获取样本：
- en: N = 10000000
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: N = 10000000
- en: B = 100
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: B = 100
- en: t = np.random.random(N)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.random.random(N)
- en: u = np.histogram(t, bins=B)[0]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: u = np.histogram(t, bins=B)[0]
- en: u = u / u.sum()
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: u = u / u.sum()
- en: t = np.random.normal(0, 1, size=N)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.random.normal(0, 1, size=N)
- en: n = np.histogram(t, bins=B)[0]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: n = np.histogram(t, bins=B)[0]
- en: n = n / n.sum()
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: n = n / n.sum()
- en: t = np.random.gamma(5.0, size=N)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.random.gamma(5.0, size=N)
- en: g = np.histogram(t, bins=B)[0]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: g = np.histogram(t, bins=B)[0]
- en: g = g / g.sum()
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: g = g / g.sum()
- en: t = np.random.beta(5,2, size=N)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.random.beta(5,2, size=N)
- en: b = np.histogram(t, bins=B)[0]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: b = np.histogram(t, bins=B)[0]
- en: b = b / b.sum()
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: b = b / b.sum()
- en: '**NOTE**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*We’re using the classic NumPy functions here, not the newer Generator-based
    functions. NumPy updated the pseudorandom number code in recent versions, but
    the overhead of using the new code will detract from what we want to see here.
    Unless you’re very serious about pseudorandom number generation, the older functions,
    and the Mersenne Twister pseudorandom number generator they’re based on, will
    be more than adequate.*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在这里使用的是经典的NumPy函数，而不是基于新生成器的函数。NumPy在最近的版本中更新了伪随机数代码，但使用新代码的开销会影响我们在这里想要看到的内容。除非你对伪随机数生成非常严肃，否则旧的函数及其基于的梅森旋转算法伪随机数生成器将完全足够。*'
- en: To make the plots, we first use 10 million samples from each distribution (N).
    Then, we use 100 bins in the histogram (B). Again, the x-axis range when plotting
    isn’t of interest here, only the shapes of the curves.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成这些图，我们首先从每个分布中使用1000万个样本（N）。然后，在直方图中使用100个箱子（B）。再次强调，绘制图时x轴的范围在这里并不重要，重要的是曲线的形状。
- en: The uniform samples use random, a function we’ve seen before. Passing the samples
    to histogram and applying the “divide by the sum” trick creates the probability
    curve data (u). We repeat this process for the Gaussian (normal), Gamma (gamma),
    and Beta (beta) distributions as well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀样本使用的是random函数，这是我们之前见过的。将样本传递给直方图并应用“除以总和”的技巧生成概率曲线数据（u）。我们对高斯（正态）、伽马（gamma）和贝塔（beta）分布也进行了相同的处理。
- en: You’ll notice that normal, gamma, and beta accept arguments. These distributions
    are parameterized; their shape is altered by changing these parameters. For the
    normal curve, the first parameter is the mean (μ), and the second is the standard
    deviation (σ). Some 68 percent of the normal curve lies within one standard deviation
    of the mean, [μ – σ, μ + σ]. The normal curve is ubiquitous in math and nature,
    and one could write an entire book on it alone. It’s always symmetric around its
    mean value. The standard deviation controls how wide or narrow the curve is.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，正态分布、伽马分布和贝塔分布都接受参数。这些分布是参数化的；通过改变这些参数可以改变它们的形状。对于正态曲线，第一个参数是均值 (μ)，第二个参数是标准差
    (σ)。大约 68% 的正态曲线位于均值的一个标准差范围内，[μ – σ, μ + σ]。正态曲线在数学和自然界中无处不在，单独写一本书也足够。它总是围绕其均值对称。标准差控制着曲线的宽窄。
- en: 'The gamma distribution is also parameterized. It accepts two parameters: the
    shape (*k*) and the scale (θ). Here, *k* = 5, and the scale is left at its default
    value of θ = 1\. As the shape increases, the gamma distribution becomes more and
    more like a Gaussian, with a bump that moves toward the center of the distribution.
    The scale parameter affects the horizontal size of the bump.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 伽马分布也是参数化的。它接受两个参数：形状 (*k*) 和尺度 (θ)。这里，*k* = 5，尺度保持其默认值 θ = 1。随着形状的增加，伽马分布越来越像高斯分布，峰值逐渐向分布的中心移动。尺度参数影响峰值的水平大小。
- en: Likewise, the beta distribution uses two parameters, *a* and *b*. Here, *a*
    = 5 and *b* = 2\. If *a* > *b*, the hump of the distribution is on the right;
    if reversed, it is on the left. If *a* = *b*, the beta distribution becomes the
    uniform distribution. The flexibility of the beta distribution makes it quite
    handy for simulating different processes, as long as you can find *a* and *b*
    values approximating the probability distribution you want. However, depending
    on the precision you require, the new Fast Loaded Dice Roller we saw in the previous
    section might be a better option in practice if you have a sufficiently detailed
    discrete distribution approximation of the continuous distribution.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，beta 分布使用两个参数，*a* 和 *b*。这里，*a* = 5 和 *b* = 2。如果 *a* > *b*，分布的峰值位于右侧；如果反过来，则位于左侧。如果
    *a* = *b*，beta 分布变为均匀分布。beta 分布的灵活性使其非常适合模拟不同的过程，只要你能找到接近所需概率分布的 *a* 和 *b* 值。然而，根据你需要的精度，如果你有一个足够详细的离散分布来近似连续分布，前一节中提到的新的快速加载骰子滚动器可能在实践中是更好的选择。
- en: '[Table 3-2](ch03.xhtml#ch03tab02) shows us the probability density functions
    for the normal, gamma, and beta distributions. An exercise for the reader is to
    use these functions to recreate [Figure 3-4](ch03.xhtml#ch03fig04). Your results
    will be smoother still than the curves in the figure. You can calculate the *B*(*a*,
    *b*) integral in [Table 3-2](ch03.xhtml#ch03tab02) by using the function scipy.special.beta.
    For Γ(*k*), see scipy.special.gamma. Additionally, if the argument to the Γ function
    is an integer, Γ(*n* + 1) = *n*!, so Γ(5) = Γ(4 + 1) = 4! = 24.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-2](ch03.xhtml#ch03tab02) 显示了正态分布、伽马分布和贝塔分布的概率密度函数。读者的一个练习是使用这些函数重新创建[图
    3-4](ch03.xhtml#ch03fig04)。你的结果将比图中的曲线更加平滑。你可以使用 scipy.special.beta 函数计算[表 3-2](ch03.xhtml#ch03tab02)中的
    *B*(*a*, *b*) 积分。有关 Γ(*k*)，请参见 scipy.special.gamma。此外，如果 Γ 函数的参数是整数，则 Γ(*n* +
    1) = *n*！，所以 Γ(5) = Γ(4 + 1) = 4! = 24。'
- en: '**Table 3-2:** The Probability Density Functions for the Normal, Gamma, and
    Beta Distributions'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3-2：** 正态分布、伽马分布和贝塔分布的概率密度函数'
- en: '| **normal** | ![image](Images/054equ01.jpg) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **normal** | ![image](Images/054equ01.jpg) |'
- en: '| **gamma** | ![image](Images/054equ02.jpg) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **gamma** | ![image](Images/054equ02.jpg) |'
- en: '| **beta** | ![image](Images/054equ03.jpg) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| **beta** | ![image](Images/054equ03.jpg) |'
- en: If you’re interested in ways to sample values from these distributions, my book
    *Random Numbers and Computers* (Springer, 2018) discusses these distributions
    and others in more depth than we can provide here, including implementations in
    C for generating samples from them. For now, let’s examine one of the most important
    theorems in probability theory.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对从这些分布中采样值的方法感兴趣，我的书《随机数与计算机》（Springer，2018）深入讨论了这些分布和其他分布，比我们在这里提供的内容更为详尽，包括用
    C 语言实现的采样方法。目前，让我们来看一下概率论中最重要的定理之一。
- en: Central Limit Theorem
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 中心极限定理
- en: Imagine we draw *N* samples from some distribution and calculate the mean value,
    *m*. If we repeat this exercise many times, we’ll get a set of mean values, *{m*[0],
    *m*[1], . . .}, each from a set of samples from the distribution. It doesn’t matter
    if *N* is the same each time, but *N* shouldn’t be too small. The rule of thumb
    is that *N* should be at least 30 samples.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从某个分布中抽取 *N* 个样本并计算其均值 *m*。如果我们多次重复这一操作，我们将得到一组均值，*{m*[0], *m*[1], . . .}，每个均值来自于一个分布样本集。无论
    *N* 每次是否相同，重要的是 *N* 不应过小。经验法则是，*N* 至少应该为 30 个样本。
- en: The *central limit theorem* states that the histogram or probability distribution
    generated from this set of sample means, the *m*’s, will approach a Gaussian in
    shape regardless of the shape of the distribution the samples were drawn from
    in the first place.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*中心极限定理* 表示，从这组样本均值，即 *m*，生成的直方图或概率分布将呈高斯分布的形状，而不管样本最初来自哪个分布。'
- en: For example, this code
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这段代码
- en: M = 10000
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: M = 10000
- en: m = np.zeros(M)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: m = np.zeros(M)
- en: 'for i in range(M):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(M):'
- en: t = np.random.beta(5,2,size=M)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.random.beta(5,2,size=M)
- en: m[i] = t.mean()
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: m[i] = t.mean()
- en: creates 10,000 sets of samples from a beta distribution, Beta(5,2), each with
    10,000 samples. The mean of each set of samples is stored in m. If we run this
    code and plot the histogram of *m*, we get [Figure 3-5](ch03.xhtml#ch03fig05).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了 10,000 组来自 Beta(5,2) 分布的样本，每组包含 10,000 个样本。每组样本的均值被存储在 m 中。如果我们运行这段代码并绘制
    *m* 的直方图，我们将得到[图 3-5](ch03.xhtml#ch03fig05)。
- en: '![image](Images/03fig05.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig05.jpg)'
- en: '*Figure 3-5: The distribution of mean values of 10,000 sets of samples of 10,000
    from Beta(5,2)*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-5：Beta(5,2) 分布中 10,000 组样本的均值分布*'
- en: The shape of [Figure 3-5](ch03.xhtml#ch03fig05) is decidedly Gaussian. Again,
    the shape is a consequence of the central limit theorem and does not depend on
    the shape of the underlying distribution. [Figure 3-5](ch03.xhtml#ch03fig05) tells
    us that the sample means from many sets of samples from Beta(5,2) themselves have
    a mean of about 0.714\. The mean of the sample means (m.mean()) is 0.7142929 for
    one run of the code above.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](ch03.xhtml#ch03fig05)的形状明显呈高斯分布。同样，这个形状是中心极限定理的结果，并且不依赖于底层分布的形状。[图
    3-5](ch03.xhtml#ch03fig05)告诉我们，来自 Beta(5,2) 分布的多个样本集的样本均值本身的均值大约为 0.714。对于上述代码的一次运行，样本均值的均值
    (m.mean()) 为 0.7142929。'
- en: There’s a formula to calculate the mean value of a Beta distribution. The population
    mean value of a Beta(5,2) distribution is known to be *a*/*(a* + *b*) = 5/(5 +
    2) = 5/7 = 0.714285\. The mean of the plot in [Figure 3-5](#ch03fig05) is a measurement
    of the true population mean, of which the many means from the Beta(5,2) samples
    are only estimates.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个公式可以计算 Beta 分布的均值。Beta(5,2) 分布的总体均值已知为 *a*/*(a* + *b*) = 5/(5 + 2) = 5/7
    = 0.714285。图[3-5](#ch03fig05)中的均值是对真实总体均值的测量，Beta(5,2) 样本中的多个均值只是对总体均值的估计。
- en: Let’s explain this again to really follow what’s going on. For any distribution,
    like the Beta(5,2) distribution, if we draw *N* samples, we can calculate the
    mean of those samples, a single number. If we repeat this process for many sets
    of *N* samples, each with its own mean, and we make a histogram of the distribution
    of the means we measured, we’ll get a plot like [Figure 3-5](ch03.xhtml#ch03fig05).
    That plot tells us that all of the many sample means are themselves clustered
    around a mean value. The mean value of the means is a measure of the population
    mean. It’s the mean we’d get if we could draw an infinite number of samples from
    the distribution. If we change the code above to use the uniform distribution,
    we’ll get a population mean of 0.5\. Similarly, if we switch to a Gaussian distribution
    with a mean of 11, the resulting histogram will be centered at 11.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再解释一遍，以便真正理解发生了什么。对于任何分布，例如 Beta(5,2) 分布，如果我们抽取 *N* 个样本，我们可以计算这些样本的均值，这是一个单一的数值。如果我们重复这个过程，使用多个
    *N* 样本集，每个样本集有它自己的均值，并且我们制作一个均值分布的直方图，我们将得到如[图 3-5](ch03.xhtml#ch03fig05)所示的图形。该图表明，所有的样本均值本身都围绕着一个均值聚集。均值的均值是一个总体均值的度量。如果我们能从分布中抽取无限数量的样本，我们将得到这个均值。如果我们将上面的代码更改为使用均匀分布，我们将得到
    0.5 的总体均值。同样，如果我们切换到一个均值为 11 的高斯分布，得到的直方图将会集中在 11 处。
- en: 'Let’s prove this claim again but this time with a discrete distribution. Let’s
    use the Fast Loaded Dice Roller to generate samples from a lopsided discrete distribution
    using this code:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次证明这一点，但这次使用一个离散分布。我们将使用快速加载骰子滚动器，通过以下代码从一个倾斜的离散分布中生成样本：
- en: from fldrf import fldr_preprocess_float_c
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: from fldrf import fldr_preprocess_float_c
- en: from fldr import fldr_sample
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: from fldr import fldr_sample
- en: z = fldr_preprocess_float_c([0.1,0.6,0.1,0.1,0.1])
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: z = fldr_preprocess_float_c([0.1,0.6,0.1,0.1,0.1])
- en: m = np.zeros(M)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: m = np.zeros(M)
- en: 'for i in range(M):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(M):'
- en: t = np.array([fldr_sample(z) for i in range(M)])
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.array([fldr_sample(z) for i in range(M)])
- en: m[i] = t.mean()
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: m[i] = t.mean()
- en: '[Figure 3-6](ch03.xhtml#ch03fig06) shows the discrete distribution (top) and
    the corresponding distribution of the sample means (bottom).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-6](ch03.xhtml#ch03fig06)显示了离散分布（上）及相应的样本均值分布（下）。'
- en: From the probability mass function, we can see that the most frequent value
    we expect from the sample is 1, with a probability of 60 percent. However, the
    tail on the right means we’ll also get values 2 through 4 about 30 percent of
    the time. The weighted mean of these is 0.6(1) + 0.1(2) + 0.1(3) + 0.1(4) = 1.5,
    which is precisely the mean of the sample distribution on the bottom of [Figure
    3-6](ch03.xhtml#ch03fig06). The central limit theorem works. We’ll revisit the
    central limit theorem in [Chapter 4](ch04.xhtml#ch04) when we discuss hypothesis
    testing.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率质量函数可以看出，我们期望从样本中抽取到的最频繁的值是 1，概率为 60%。然而，右侧的尾部意味着我们也会大约 30% 的时间得到 2 到 4 之间的值。这些值的加权均值是
    0.6(1) + 0.1(2) + 0.1(3) + 0.1(4) = 1.5，这恰好是[图 3-6](ch03.xhtml#ch03fig06)下方样本分布的均值。中心极限定理成立。我们将在[第
    4 章](ch04.xhtml#ch04)讨论假设检验时重新回顾中心极限定理。
- en: '![image](Images/03fig06.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig06.jpg)'
- en: '*Figure 3-6: An arbitrary discrete distribution (top) and the distribution
    of sample means drawn from it (bottom)*'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-6：一个任意的离散分布（上）及从中抽取的样本均值分布（下）*'
- en: The Law of Large Numbers
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大数法则
- en: A concept related to the central limit theorem, and often confused with it,
    is the *law of large numbers*. The law of large numbers states that as the size
    of a sample from a distribution increases, the mean of the sample moves closer
    and closer to the mean of the population. In this case, we’re contemplating a
    single sample from the distribution and making a statement about how close we
    expect its mean to be to the true population mean. For the central limit theorem,
    we have many different sets of samples from the distribution and are making a
    statement about the distribution of the means of those sets of samples.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一个与中心极限定理相关的概念，且常与之混淆的是*大数法则*。大数法则指出，随着从一个分布中抽取的样本大小的增加，样本的均值会越来越接近总体的均值。在这种情况下，我们讨论的是从分布中抽取的单个样本，并对其均值接近真实总体均值的程度做出预测。对于中心极限定理，我们有多个来自分布的样本集，并且我们对这些样本集均值的分布做出预测。
- en: We can demonstrate the law of large numbers quite simply by selecting larger
    and larger size samples from a distribution and tracking the mean as a function
    of the sample size (the number of samples drawn). In code, then,
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从一个分布中选择越来越大的样本并追踪其均值与样本大小（即抽取样本的数量）之间的关系，简单地演示大数法则。因此，在代码中，
- en: m = []
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: m = []
- en: 'for n in np.linspace(1,8,30):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'for n in np.linspace(1,8,30):'
- en: t = np.random.normal(1,1,size=int(10**n))
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.random.normal(1,1,size=int(10**n))
- en: m.append(t.mean())
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: m.append(t.mean())
- en: where we’re drawing ever-larger sample sizes from a normal distribution with
    a mean of 1\. The first sample size is 10, and the last is 100 million. If we
    plot the mean of the samples as a function of sample size, we see the law of large
    numbers at work.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们从一个均值为 1 的正态分布中抽取越来越大的样本。第一个样本大小是 10，最后一个是 1 亿。如果我们将样本的均值与样本大小作为函数绘制出来，就会看到大数法则的效果。
- en: '[Figure 3-7](ch03.xhtml#ch03fig07) shows the sample means as a function of
    the number of samples for the normal distribution with a mean of 1 (dashed line).
    As the number of samples from the distribution increases, the mean of the samples
    approaches the population mean, which illustrates the law of large numbers.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-7](ch03.xhtml#ch03fig07)显示了正态分布样本均值与样本数之间的关系，其中正态分布的均值为 1（虚线）。随着从分布中抽取的样本数增加，样本的均值逐渐接近总体均值，这说明了大数法则。'
- en: '![image](Images/03fig07.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03fig07.jpg)'
- en: '*Figure 3-7: The law of large numbers in action*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-7：大数法则的实际应用*'
- en: Let’s change gears and move on to Bayes’ theorem, the last topic for this chapter.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们换个话题，接下来讲解贝叶斯定理，这是本章的最后一个主题。
- en: Bayes’ Theorem
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: In [Chapter 2](ch02.xhtml#ch02), we discussed an example where we determined
    whether a woman had cancer. There, I promised that Bayes’ theorem would tell us
    how to properly account for the probability of a randomly selected woman in her
    40s having breast cancer. Let’s fulfill that promise in this section by learning
    what Bayes’ theorem is and how to use it.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml#ch02)，我们讨论了一个例子，确定一个女性是否患有癌症。在那里，我曾承诺，贝叶斯定理将告诉我们如何正确计算一个随机选择的四十多岁女性患乳腺癌的概率。让我们在本节中履行这个承诺，学习贝叶斯定理是什么，以及如何使用它。
- en: 'Using the product rule, [Equation 2.8](ch02.xhtml#ch02equ08), we know the following
    two mathematical statements are true:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用乘积法则，[公式 2.8](ch02.xhtml#ch02equ08)，我们知道以下两个数学陈述是正确的：
- en: '*P*(*B*, *A*) = *P*(*B*|*A*)*P*(*A*)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*B*, *A*) = *P*(*B*|*A*)*P*(*A*)'
- en: '*P*(*A*, *B*) = *P*(*A*|*B*)*P*(*B*)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A*, *B*) = *P*(*A*|*B*)*P*(*B*)'
- en: Additionally, because the joint probability of both *A* and *B* doesn’t depend
    on which event we call *A* and which we call *B*,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为 *A* 和 *B* 的联合概率不依赖于我们将哪个事件称为 *A*，哪个称为 *B*，
- en: '*P*(*A*, *B*) = *P*(*B*, *A*)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A*, *B*) = *P*(*B*, *A*)'
- en: Therefore,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '*P*(*B*|*A*)*P*(*A*) = *P*(*A*|*B*)*P*(*B*)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*B*|*A*)*P*(*A*) = *P*(*A*|*B*)*P*(*B*)'
- en: Dividing by *P*(*A*), we get
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 除以 *P*(*A*)，我们得到
- en: '![image](Images/03equ01.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03equ01.jpg)'
- en: 'This is *Bayes’ theorem*, the heart of the Bayesian approach to probability,
    and the proper way to compare two conditional probabilities: *P**(B*|*A*) and
    *P*(*A*|*B*). You’ll sometimes see [Equation 3.1](ch03.xhtml#ch03equ01) referred
    to as *Bayes’ rule*. You’ll also often see no apostrophe after “Bayes,” which
    is a bit sloppy and ungrammatical, but common.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 *贝叶斯定理*，是贝叶斯概率方法的核心，也是比较两个条件概率的正确方式：*P*(*B*|*A*) 和 *P*(*A*|*B*)。你有时会看到[公式
    3.1](ch03.xhtml#ch03equ01)被称为 *贝叶斯规则*。你还经常会看到“贝叶斯”后面没有撇号，这虽然有些草率和不规范，但很常见。
- en: '[Equation 3.1](ch03.xhtml#ch03equ01) has been enshrined in neon lights, tattoos,
    and even baby names: “Bayes.” The equation is named after Thomas Bayes (1701–1761),
    an English minister and statistician, and was published after his death. In words,
    [Equation 3.1](ch03.xhtml#ch03equ01) says the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 3.1](ch03.xhtml#ch03equ01)已经被镶嵌在霓虹灯、纹身，甚至是婴儿名字中：“贝叶斯”。这个公式以托马斯·贝叶斯（Thomas
    Bayes，1701–1761）的名字命名，他是一位英国牧师和统计学家，且在去世后才发布。用语言表达，[公式 3.1](ch03.xhtml#ch03equ01)说的是：'
- en: The *posterior probability*, *P*(*B*|*A*), is the product of *P*(*A*|*B*), the
    *likelihood*, and *P*(*B*), the *prior*, normalized by *P*(*A*), the marginal
    probability or *evidence*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*后验概率*，*P*(*B*|*A*)，是 *P*(*A*|*B*)，即 *似然*，和 *P*(*B*)，即 *先验*，的乘积，归一化由 *P*(*A*)，即边际概率或
    *证据*。'
- en: Now that we know what Bayes’ theorem is, let’s see it in action so we can understand
    it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道贝叶斯定理是什么，让我们看看它的实际应用，这样我们可以理解它。
- en: Cancer or Not Redux
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 癌症与否 Redux
- en: One way to think about the components of Bayes’ theorem is in the context of
    medical testing. At the beginning of [Chapter 2](ch02.xhtml#ch02), we calculated
    the probability of a woman having breast cancer given a positive mammogram and
    found that it was quite different from what we might naively have believed it
    to be. Let’s revisit that problem now using Bayes’ theorem. It might be helpful
    to reread the first section of [Chapter 2](ch02.xhtml#ch02) before continuing.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 思考贝叶斯定理的组成部分的一种方式是在医疗检测的背景下。我们在[第2章](ch02.xhtml#ch02)的开头计算了在乳房X光检查呈阳性时，女性患乳腺癌的概率，并发现这个概率与我们可能天真认为的不同。让我们现在用贝叶斯定理重新审视这个问题。在继续之前，重新阅读[第2章](ch02.xhtml#ch02)的第一部分可能会有所帮助。
- en: We want to use Bayes’ theorem to find the posterior probability, the probability
    of breast cancer given a positive mammogram. We’ll write this as *P**(bc* + |+),
    meaning breast cancer (*bc*+) given a positive mammogram (+).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想用贝叶斯定理来计算后验概率，即在乳房X光检查呈阳性时，患乳腺癌的概率。我们将其表示为 *P*(*bc*+ |+)，表示给定乳房X光检查呈阳性（+）的情况下，患乳腺癌（*bc*+）的概率。
- en: In the problem, we’re told that the mammogram returns a positive result, given
    the patient has breast cancer, 90 percent of the time. We write this as
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们知道，给定患者患有乳腺癌时，乳房X光检查呈阳性的概率是 90%。我们写成
- en: '*P*(+|*bc*+) = 0.9'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(+|*bc*+) = 0.9'
- en: This is the likelihood of a positive mammogram in terms of Bayes’ equation,
    *P*(*A*|*B*) = *P*(+|*bc*+).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯公式中乳房X光检查呈阳性概率的表示形式，*P*(*A*|*B*) = *P*(+|*bc*+)。
- en: Next, we’re told the probability of a random woman having breast cancer is 0.8
    percent. Therefore, we know
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们得知随机女性患乳腺癌的概率是 0.8%。因此，我们知道
- en: '*P*(*bc*+) = 0.008'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*bc*+) = 0.008'
- en: This is the prior probability, *P*(*B*), in Bayes’ theorem.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯定理中的先验概率，*P*(*B*)。
- en: 'We have all the components of [Equation 3.1](ch03.xhtml#ch03equ01) except one:
    *P**(A*).  What is *P*(*A*) in this context? It’s *P*(+), the marginal probability
    of a positive mammogram regardless of any *B*, any breast cancer status. It’s
    also the evidence that we have, the thing we know: the mammogram was positive.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 *P**(A*) 之外，我们已经具备了[公式 3.1](ch03.xhtml#ch03equ01)中的所有组件。 在这个背景下，*P*(*A*)
    是什么？它是 *P*(+)，即无论乳腺癌状态如何，阳性乳腺X光检查的边际概率。它也是我们所掌握的证据，也就是我们知道的事实：乳腺X光检查结果为阳性。
- en: In the problem, we’re told there’s a 7 percent chance a woman without breast
    cancer has a positive mammogram. Is this *P*(+)? No, it is *P*(+|*bc*–), the probability
    of a positive mammogram *given* no breast cancer.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们被告知一个没有乳腺癌的女性有7%的机会出现阳性乳腺X光检查结果。这是 *P*(+)? 不是，它是 *P*(+|*bc*–)，即在没有乳腺癌的情况下得到阳性乳腺X光检查结果的概率。
- en: 'I’ve referred to *P**(A*) as the marginal probability twice now. We know what
    to do to get a marginal or total probability: we sum over all the other parts
    of a joint probability that don’t matter for what we want to know. Here, we have
    to sum over all the partitions of the sample space we don’t care about to get
    the marginal probability of a positive mammogram. What partitions are those? There
    are only two: either a woman has breast cancer or she doesn’t. Therefore, we need
    to find'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前已经提到过两次 *P**(A*) 作为边际概率。我们知道如何计算边际概率或总概率：我们对联合概率中不相关的其他部分进行求和。这里，我们必须对我们不关心的样本空间的所有划分进行求和，以获得阳性乳腺X光检查的边际概率。这些划分是什么？只有两种情况：一个女性要么有乳腺癌，要么没有乳腺癌。因此，我们需要找到
- en: '*P*(+) = *P*(+|*bc*+)*P*(*bc*+) + *P*(+|*bc*–)*P*(*bc*–)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(+) = *P*(+|*bc*+)*P*(*bc*+) + *P*(+|*bc*–)*P*(*bc*–)'
- en: We know all of these quantities already, except *P**(bc*–). This is the prior
    probability that a randomly selected woman will *not* have breast cancer, *P**(bc*–)
    = 1 – *P*(*bc*+) = 0.992.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道了所有这些量，除了 *P**(bc*–)。这是一个随机选择的女性没有乳腺癌的先验概率，*P**(bc*–) = 1 – *P*(*bc*+)
    = 0.992。
- en: Sometimes, you’ll see the summation over other terms in the joint probability
    expressed in the denominator of Bayes’ theorem. Even if they’re not explicitly
    called out, they are there, implicit in what it takes to find *P**(A*).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你会看到贝叶斯定理的分母中会表达出对联合概率中其他项的求和。即使这些项没有明确指出，它们也是存在的，隐含在计算 *P**(A*) 所需的步骤中。
- en: 'Finally, we have all the pieces and can calculate the probability using Bayes’
    theorem:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了所有的组成部分，可以使用贝叶斯定理来计算概率：
- en: '![image](Images/061equ01.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/061equ01.jpg)'
- en: This is the result we found earlier. Recall, a large percentage of doctors in
    the study claimed the probability of cancer from a positive mammogram, *P**(A*|*B*),
    was 90 percent. Their mistake was incorrectly equating *P(A*|*B*) with *P(B*|*A*).
    Bayes’ theorem correctly relates the two by using the prior and the marginal probability.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前找到的结果。回想一下，研究中有很大一部分医生声称，从一个阳性乳腺X光检查结果中推断癌症的概率，*P**(A*|*B*)，是90%。他们的错误在于错误地将
    *P(A*|*B*) 与 *P(B*|*A*) 等同起来。贝叶斯定理通过使用先验概率和边际概率正确地将这两者联系起来。
- en: Updating the Prior
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新先验
- en: 'We don’t need to stop with this single calculation. Consider the following:
    what if, after a woman receives the news that her mammogram is positive, she decides
    to have a second mammogram at another facility with different radiologists reading
    the results, and that mammogram also comes back positive? Does she still believe
    that her probability of having breast cancer is 9 percent? Intuitively, we might
    think that she now has more reason to believe that she has cancer. Can this belief
    be quantified? It can, in the Bayesian view, by updating the prior, *P**(bc*+),
    with the posterior calculated from the first test, *P*(*bc* + |+). After all,
    she now has a stronger prior probability of cancer given the first positive mammogram.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要仅仅停留在这个单一的计算上。考虑一下：如果一位女性在得知她的乳腺X光检查结果是阳性后，决定去另一家机构进行第二次乳腺X光检查，且该检查结果也显示阳性，她还会相信她患乳腺癌的概率是9%吗？直觉上，我们可能认为她现在有更多理由相信自己患有癌症。那么这个信念能否量化？在贝叶斯的视角下，这是可以的，方法是通过使用从第一次测试计算出的后验概率
    *P*(*bc* + |+) 来更新先验 *P**(bc*+)，毕竟，第一次阳性乳腺X光检查结果使她的乳腺癌先验概率变得更强。
- en: 'Let’s calculate this new posterior based on the previous mammogram result:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于之前的乳腺X光检查结果来计算这个新的后验概率：
- en: '![image](Images/061equ02.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/061equ02.jpg)'
- en: As 57 percent is significantly higher than 9 percent, our hypothetical woman
    now has significantly more reason to believe she has breast cancer.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于57%显著高于9%，我们假设的女性现在有了更强的理由相信她患有乳腺癌。
- en: Notice what has changed in this new calculation, besides a dramatic increase
    in the posterior probability of breast cancer given the second mammogram’s positive
    result. First, the prior probability of breast cancer went from 0.008 → 0.094,
    the posterior calculated based on the first test. Second, *P*(*bc*–) also changed
    from 0.992 → 0.906\. Why? Because the prior changed and *P*(*bc*–) = 1 – *P*(*bc*+).
    The sum of *P*(*bc*+) and *P*(*bc*–) must still be 1.0—either she has breast cancer,
    or she doesn’t—that’s the entire sample space.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在这个新的计算中发生了什么变化，除了第二次乳腺X光检查结果为阳性后，乳腺癌的后验概率显著增加之外。首先，乳腺癌的先验概率从0.008 → 0.094，后验概率是基于第一次测试计算的。其次，*P*(*bc*–)也从0.992
    → 0.906发生了变化。为什么？因为先验发生了变化，而 *P*(*bc*–) = 1 – *P*(*bc*+)。*P*(*bc*+) 和 *P*(*bc*–)
    的和仍然必须是1.0——要么她得了乳腺癌，要么没有——这是整个样本空间。
- en: In the example above, we updated the prior based on the initial test result,
    and we had an initial prior given to us in the first example. What about the prior
    in general? In many cases, Bayesians select the prior, at least initially, based
    on an actual belief about the problem. Often the prior is a uniform distribution,
    known as the *uninformed prior* because there’s nothing to guide the selection
    of anything else. For the breast cancer example, the prior is something that can
    be estimated from an experiment using a random selection of women from the general
    population.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们根据初始测试结果更新了先验，并且在第一个例子中已经给出了初始的先验。那么，先验一般来说是什么呢？在许多情况下，贝叶斯方法使用者至少在最初会基于对问题的实际信念来选择先验。通常，先验是均匀分布，称为*无信息先验*，因为没有任何东西可以指导选择其他内容。对于乳腺癌的例子，先验是可以通过实验估算的，实验使用的是从一般人群中随机选取的女性。
- en: As mentioned earlier, don’t take the numbers here too seriously; they are for
    example use only. Also, while a woman certainly has the option to get a second
    opinion, the gold standard for a breast cancer diagnosis is biopsy, the likely
    next step after an initial positive mammogram. Finally, throughout this section,
    I’ve referred to women and breast cancer. Men also get breast cancer, though it
    is rare, with less than 1 percent of cases in men. However, it made the discussion
    simpler to refer only to women. I’ll note that breast cancer cases in men are
    more likely to be fatal, though the reasons why are not yet known.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，不要过于严肃看待这里的数字；它们仅用于示例。同时，虽然女性当然可以选择寻求第二意见，但乳腺癌诊断的金标准是活检，这是初次乳腺X光检查阳性后的可能下一步。最后，在本节中，我一直提到女性和乳腺癌。男性也可能得乳腺癌，尽管这种情况很少见，男性的病例不到1%。然而，为了简化讨论，我只提到了女性。我还要指出，男性的乳腺癌病例更可能是致命的，尽管其原因尚未明确。
- en: Bayes’ Theorem in Machine Learning
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯定理在机器学习中的应用
- en: Bayes’ theorem is prevalent throughout machine learning and deep learning. One
    classic use of Bayes’ theorem, one that can work surprisingly well, is to use
    it as a classifier. This is known as the *Naive Bayes* classifier. Early email
    spam filters used this approach quite effectively.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理在机器学习和深度学习中广泛应用。贝叶斯定理的一个经典应用，且往往能出奇有效，是将其用作分类器。这就是著名的*朴素贝叶斯*分类器。早期的电子邮件垃圾邮件过滤器就有效地使用了这种方法。
- en: 'Assume we have a dataset consisting of class labels, *y*, and feature vectors,
    ***x***. The goal of a Naive Bayes classifier is to tell us, for each class, the
    probability that a given feature vector belongs to that class. With those probabilities,
    we can assign a class label by selecting the largest probability. That is, we
    want to find *P**(y*|***x***) for each class label, *y*. This is a conditional
    probability, so we can use Bayes’ theorem with it:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，其中包含类标签 *y* 和特征向量 ***x***。朴素贝叶斯分类器的目标是告诉我们，对于每个类，给定特征向量属于该类的概率。通过这些概率，我们可以通过选择最大概率来分配类标签。也就是说，我们要为每个类标签
    *y* 找到 *P*( *y*|***x*** )。这是一个条件概率，所以我们可以用贝叶斯定理来计算：
- en: '![image](Images/03equ02.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/03equ02.jpg)'
- en: The equation above is saying that the probability of feature vector *x* representing
    an instance of class label *y* is the probability of the class label *y* generating
    a feature vector *x* times the prior probability of class label *y* occurring,
    divided by the marginal probability of the feature vector over all class labels.
    Recall the implicit sum in calculating *P*(*x*).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的公式表示，特征向量 *x* 表示类标签 *y* 实例的概率，是类标签 *y* 生成特征向量 *x* 的概率乘以类标签 *y* 发生的先验概率，再除以所有类标签上特征向量的边际概率。回忆一下计算
    *P*(*x*) 时的隐式求和。
- en: How is this useful to us? Since we have a dataset, we can estimate *P**(y*)
    using it, assuming the dataset class distribution is a fair representation of
    what we’d encounter when using the model. And, since we have labels, we can partition
    the dataset into smaller, per class, collections. This might help us do something
    useful to get the likelihoods per class, *P*(*x*|*y*). We’ll ignore the marginal
    *P*(*x*) completely. Let’s see why, in this case, we’re free to do so.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们有什么帮助？由于我们有一个数据集，我们可以利用它估计 *P**(y*)，假设数据集的类别分布能够公平地代表我们在使用模型时所遇到的情况。而且，由于我们有标签，我们可以将数据集分成更小的、按类别划分的集合。这或许能帮助我们做一些有用的事情，以获取每个类别的似然值
    *P*(*x*|*y*)。我们将完全忽略边际概率 *P*(*x*)。让我们来看一下为什么在这种情况下，我们可以这么做。
- en: '[Equation 3.2](ch03.xhtml#ch03equ02) is for a particular class label, say *y*
    = 1\. We’ll have other versions of it for all the class labels in the dataset.
    We said our classifier consists of calculating the posterior probabilities for
    each class label and selecting the largest one as the label assigned to an unknown
    feature vector. The denominator of [Equation 3.2](ch03.xhtml#ch03equ02) is a scale
    factor, which makes the output a true probability. For our use case, however,
    we only care about the relative ordering of *P**(y*|***x***) over the different
    class labels. Since *P*(***x***) is the same for all *y*, it’s a common factor
    that will change the number associated with *P**(y*|***x***) but not the ordering
    over the different class labels. Therefore, we can ignore it and concentrate on
    finding the products of the likelihoods and the priors. Although the largest *P*(*y*|***x***)
    calculated this way is no longer a proper probability, it’s still the correct
    class label to assign.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 3.2](ch03.xhtml#ch03equ02) 是针对特定类别标签的，比如 *y* = 1。我们会为数据集中的所有类别标签有其他版本。我们说我们的分类器是通过计算每个类别标签的后验概率，并选择最大的一个作为分配给未知特征向量的标签。
    [公式 3.2](ch03.xhtml#ch03equ02) 的分母是一个尺度因子，它使得输出成为一个真实的概率。然而，对于我们的使用案例，我们只关心不同类别标签下的
    *P**(y*|***x***) 的相对排序。由于 *P*(***x***) 对所有 *y* 来说是相同的，它是一个公共因子，会改变与 *P**(y*|***x***)
    相关的数值，但不会改变不同类别标签之间的排序。因此，我们可以忽略它，专注于寻找似然和先验的乘积。虽然通过这种方式计算出的最大 *P*(*y*|***x***)
    不再是一个正确的概率，但它仍然是正确的类别标签。'
- en: Given that we can ignore *P****(x***) and the *P*(*y*) values are easily estimated
    from the dataset, we’re left with calculating *P****(x***|*y*), the likelihood
    that given the class label is *y*, we’d have a feature vector ***x***. What can
    we do in this case?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以忽略 *P**(x***)，并且 *P*(*y*) 的值可以从数据集中轻松估计，我们剩下的就是计算 *P**(x***|*y*)，即在给定类别标签为
    *y* 的情况下，得到特征向量 ***x*** 的似然性。在这种情况下，我们能做什么呢？
- en: First, we can think about what *P****(x***|*y*) is. It’s a conditional probability
    for feature vectors given the feature vectors are all representatives of class
    *y*. For the moment, let’s ignore the *y* part, since we know the feature vectors
    all come from class *y*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以思考一下 *P**(x***|*y*) 是什么。它是给定特征向量属于类别 *y* 时的条件概率。暂时我们可以忽略 *y* 部分，因为我们知道特征向量都来自于类别
    *y*。
- en: This leaves only *P****(x***) because we fixed *y*. A feature vector is a collection
    of individual features, *x* = (***x[0]***, *x*[1], *x*[2], . . ., *x[n]*[–1])
    for *n* features in the vector. Therefore, *P****(x***) is really a joint probability,
    the probability that all the individual features have their specific values *at
    the same time*. So, we can write
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这只剩下 *P**(x***)，因为我们已经固定了 *y*。特征向量是各个单独特征的集合，*x* = (***x[0]***, *x*[1], *x*[2],
    . . ., *x[n]*[–1])，其中有 *n* 个特征。因此，*P**(x***) 实际上是一个联合概率，即所有个别特征同时具有特定值的概率。因此，我们可以写成
- en: '*P*(*x*) = *P*(*x*[0], *x*[1], *x*[2], . . ., *x*[*n*–1])'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x*) = *P*(*x*[0], *x*[1], *x*[2], . . ., *x*[*n*–1])'
- en: How does this help? If we make one more assumption about our data, we’ll see
    that we can break up this joint probability in a convenient way. Let’s assume
    that all the features in our feature vector are independent. Recall that *independent*
    means the value of *x*[1], say, is in no way affected by the value of any other
    feature in the vector. This is typically not quite true, and for things like pixels
    in images *definitely* not true, but we’ll assume it’s true nonetheless. We’re
    naive to believe it’s true, hence the *Naive* in *Naive Bayes*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么帮助我们呢？如果我们对数据做出一个额外的假设，我们会发现我们可以以一种方便的方式分解这个联合概率。假设我们特征向量中的所有特征都是独立的。回想一下，*独立*的意思是
    *x*[1] 的值，比如说，不会受到特征向量中其他特征值的影响。这通常不完全正确，像图像中的像素 *绝对* 不是独立的，但我们还是假设它们是独立的。我们天真地相信它是对的，这也是
    *Naive Bayes*（朴素贝叶斯）中的 *Naive* 的由来。
- en: 'If the features are independent, then the probability of a feature taking on
    any particular value is independent of all the others. In that case, the product
    rule tells us that we can break the joint probability up like so:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征是独立的，那么某个特征取某个特定值的概率与其他特征的取值无关。在这种情况下，乘法规则告诉我们可以像这样分解联合概率：
- en: '*P*(*x*) = *P*(*x*[0])*P*(*x*[1])*P*(*x*[2]) . . . *P*(*x*[*n*–1])'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x*) = *P*(*x*[0])*P*(*x*[1])*P*(*x*[2]) . . . *P*(*x*[*n*–1])'
- en: This helps tremendously. We have a dataset, labeled by class, allowing us to
    estimate the probability of any feature for any specific class by counting how
    often each feature value happens for each class.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这大有帮助。我们有一个按类别标记的数据集，允许我们通过计算每个特征值在每个类别中出现的频率，来估算每个特征对特定类别的概率。
- en: Let’s put it all together for a hypothetical dataset of three classes—0, 1,
    and 2—and four features. We first use the dataset, partitioned by class label,
    to estimate each feature value probability. This provides us the set of *P**(x*[0]),
    *P*(*x*[1]), and so on, for each feature for each class label. Combined with the
    prior probability of the class label, estimated from the dataset as the number
    of each class divided by the total number of samples in the dataset, we calculate
    for a new unknown feature vector, ***x***,
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有内容整合在一起，假设有一个包含三个类别（0、1 和 2）和四个特征的数据集。首先，我们使用按类别标签划分的数据集来估计每个特征值的概率。这为我们提供了每个特征对于每个类别标签的*P**(x*[0])、*P*(*x*[1])，等等的集合。结合从数据集中估算得到的类别标签的先验概率（即每个类别的样本数除以数据集中的样本总数），我们为一个新的未知特征向量***x***计算概率，
- en: '![image](Images/064equ01.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/064equ01.jpg)'
- en: Here, the *P*(*x*[0]) feature probabilities are specific to class 0 only, and
    *P*(0) is the prior probability of class 0 in the dataset. *P*(0|***x***) is the
    unnormalized posterior probability that the unknown feature vector ***x*** belongs
    to class 0\. We say *unnormalized* because we’re ignoring the denominator of Bayes’
    theorem, knowing that including it would not change the ordering of the posterior
    probabilities, only their values.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*P*(*x*[0])特征概率仅针对类别0，*P*(0)是数据集中类别0的先验概率。*P*(0|***x***)是未知特征向量***x***属于类别0的未归一化后验概率。我们称其为*未归一化*，因为我们忽略了贝叶斯定理中的分母，知道包括分母不会改变后验概率的排序，只会改变它们的值。
- en: We can repeat the calculation above to get *P*(1|***x***) and *P*(2|***x***),
    making sure to use the per feature probabilities calculated for those classes
    (the *P*(*x*[0])s). Finally, we give ***x*** the class label for the largest of
    the three posteriors calculated.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重复上面的计算，以得到*P*(1|***x***)和*P*(2|***x***)，确保使用为这些类别计算的每个特征的概率（*P*(*x*[0])s）。最后，我们为***x***赋予具有最大后验概率的类别标签。
- en: The description above assumes that the feature values are discrete. Usually
    they aren’t, but there are workarounds. One is to bin the feature values to make
    them discrete. For example, if the feature ranges over [0, 3], create a new feature
    that is 0, 1, or 2, and assign the continuous feature to one of those bins by
    truncating any fractional part.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述假设特征值是离散的。通常情况下，特征值不是离散的，但可以通过一些方法进行处理。一个方法是将特征值进行分箱，使其变为离散的。例如，如果特征的范围是[0,
    3]，则创建一个新的特征，其值为0、1或2，并通过截断任何小数部分将连续特征分配到这些箱子中的一个。
- en: Another workaround is to make one more assumption about the distribution the
    feature values come from and use that distribution to calculate the *P**(x*[0])s
    per class. Features are often based on measurements in the real world, and many
    things in the real world follow a normal distribution. Therefore, typically we’d
    assume that the individual features, while continuous, are normally distributed
    and we can find estimates of the mean (μ) and standard deviation (σ) from the
    dataset, per feature, and class label.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方法是对特征值的分布做出另一个假设，并利用该分布来计算每个类别的*P**(x*[0])s。特征通常基于现实世界中的测量，现实世界中的许多事物都遵循正态分布。因此，我们通常假设各个特征虽然是连续的，但服从正态分布，我们可以从数据集中估算出每个特征和类别标签的均值（μ）和标准差（σ）。
- en: Bayes’ theorem is useful for calculating probabilities. It’s helpful in machine
    learning as well. The battle between Bayesians and frequentists appears to be
    waning, though philosophical differences remain. In practice, most researchers
    are learning that both approaches are valuable, and at times tools from both camps
    should be used. We’ll continue this trend in the next chapter, where we’ll examine
    statistics from a frequentist viewpoint. We defend this decision by pointing out
    that the vast majority of published scientific results in the last century used
    statistics this way, which includes the deep learning community, at least when
    it’s presenting the results of experiments.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理对于计算概率非常有用。在机器学习中也很有帮助。尽管贝叶斯学派和频率学派之间的争论似乎正在减弱，但哲学上的分歧依然存在。实际上，大多数研究人员正在认识到，两种方法都有其价值，某些时候应同时使用两种学派的工具。在下一章中，我们将继续这一趋势，从频率学派的角度来审视统计学。我们为这一决定辩护，指出在过去一个世纪中，绝大多数已发表的科学结果都是通过这种方式使用统计学的，包括深度学习领域，至少在展示实验结果时是这样。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter taught us about probability distributions, what they are, and how
    to draw samples from them, both discrete and continuous. We’ll encounter different
    distributions during our exploration of deep learning. We also discovered Bayes’
    theorem and saw how it lets us properly relate conditional probabilities. We saw
    how Bayes’ theorem allows us to evaluate the true likelihood of cancer given an
    imperfect medical test—a common situation. We also learned how to use Bayes’ theorem,
    along with some of the basic probability rules we learned in [Chapter 2](ch02.xhtml#ch02),
    to build a simple but often surprisingly effective classifier.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向我们介绍了概率分布，它们是什么，以及如何从其中抽样，包括离散型和连续型。在我们探索深度学习的过程中，我们将遇到不同的分布。我们还发现了贝叶斯定理，并看到它如何帮助我们正确地关联条件概率。我们看到贝叶斯定理如何帮助我们评估癌症的真实可能性，尤其是在医学检测不完美的情况下——这是一个常见的情况。我们还学会了如何结合贝叶斯定理和我们在[第二章](ch02.xhtml#ch02)中学到的一些基本概率规则，构建一个简单但往往出奇有效的分类器。
- en: Let’s move now into the world of statistics.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入统计学的世界。
- en: '[1](#ch03fn01a). Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash
    K. Mansinghka, “The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for
    Discrete Probability Distributions,” in AISTATS 2020: Proceedings of the 23rd
    International Conference on Artificial Intelligence and Statistics, *Proceedings
    of Machine Learning Research* 108, Palermo, Sicily, Italy, 2020.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](#ch03fn01a). Feras A. Saad, Cameron E. Freer, Martin C. Rinard, 和 Vikash
    K. Mansinghka，《快速加载骰子滚动器：离散概率分布的近似最优精确采样器》，见 AISTATS 2020：第23届国际人工智能与统计会议论文集，*机器学习研究论文集*
    108，意大利西西里岛巴勒莫，2020年。'
