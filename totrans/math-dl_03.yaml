- en: '**3'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MORE PROBABILITY**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Chapter 2](ch02.xhtml#ch02) introduced us to basic concepts of probability.
    In this chapter, we’ll continue our exploration of probability by focusing on
    two essential topics often encountered in deep learning and machine learning:
    probability distributions and how to sample from them, and Bayes’ theorem. Bayes’
    theorem is one of the most important concepts in probability theory, and it has
    produced a paradigm shift in the way many researchers think about probability
    and how to apply it.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability Distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A probability distribution can be thought of as a function that generates values
    on demand. The values generated are random—we don’t know which one will appear—but
    the likelihood of any value appearing follows a general form. For example, if
    we roll a standard die many times and tally how many times each number comes up,
    we expect that in the long run, each number is equally likely. Indeed, that’s
    the entire point of making the die in the first place. Therefore, the probability
    distribution of the die is known as a *uniform distribution*, since each number
    is equally likely to appear. We can imagine other distributions favoring one value
    or range of values over others, like a weighted die that might come up as six
    suspiciously often.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning’s primary reason for sampling from a probability distribution
    is to initialize the network before training. Modern networks select the initial
    weights and sometimes biases from different distributions, most notably uniform
    and normal. The uniform distribution is familiar to us, and I’ll discuss the normal
    distribution, a continuous distribution, later.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll present several different kinds of probability distributions in this section.
    Our focus is to understand the shape of the distribution and to learn how to draw
    samples from it using NumPy. I’ll start with histograms to show you that we can
    often treat histograms as approximations of a probability distribution. Then I’ll
    discuss common *discrete probability distributions*. These are distributions returning
    integer values, like 3 or 7\. Lastly, I’ll switch to continuous distributions
    yielding floating-point numbers, like 3.8 or 7.592.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms and Probabilities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a look at [Table 3-1](ch03.xhtml#ch03tab01), which we saw in [Chapter 2](ch02.xhtml#ch02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-1:** The Number of Combinations of Two Dice Leading to Different
    Sums (Copied from [Table 2-1](ch02.xhtml#ch02tab01))'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sum** | **Combinations** | **Count** | **Probability** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 + 1 | 1 | 0.0278 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 + 2, 2 + 1 | 2 | 0.0556 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 + 3, 2 + 2, 3 + 1 | 3 | 0.0833 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 + 4, 2 + 3, 3 + 2, 4 + 1 | 4 | 0.1111 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 + 5, 2 + 4, 3 + 3, 4 + 2, 5 + 1 | 5 | 0.1389 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1 + 6, 2 + 5, 3 + 4, 4 + 3, 5 + 2, 6 + 1 | 6 | 0.1667 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2 + 6, 3 + 5, 4 + 4, 5 + 3, 6 + 2 | 5 | 0.1389 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 3 + 6, 4 + 5, 5 + 4, 6 + 3 | 4 | 0.1111 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 4 + 6, 5 + 5, 6 + 4 | 3 | 0.0833 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 5 + 6, 6 + 5 | 2 | 0.0556 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 6 + 6 | 1 | 0.0278 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 36 | 1.0000 |'
  prefs: []
  type: TYPE_TB
- en: It shows how two dice add to different sums. Don’t look at the actual values;
    look at the shape the possible combinations make. If we chop off the last two
    columns, turn the table to the left, and replace each sum with an “X,” we should
    see something like the following.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |  | × |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  | × | × | × |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | × | × | × | × | × |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | × | × | × | × | × | × | × |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | × | × | × | × | × | × | × | × | × |  |'
  prefs: []
  type: TYPE_TB
- en: '| × | × | × | × | × | × | × | × | × | × | × |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |'
  prefs: []
  type: TYPE_TB
- en: You can see that there’s a definite shape and symmetry to the number of ways
    to arrive at each sum. This kind of plot is called a *histogram*. A histogram
    is a plot tallying the number of things that fall into discrete bins. For [Table
    3-1](ch03.xhtml#ch03tab01), the bins are the numbers 2 through 12\. The tally
    is a possible way to get that sum. Histograms are often represented as bar graphs,
    usually vertical bars, though they need not be. [Table 3-1](ch03.xhtml#ch03tab01)
    is basically a horizontal histogram. How many bins are used in the histogram is
    up to the maker. If you use too few, the histogram will be blocky and may not
    reveal necessary detail because interesting features have all been lumped into
    the same bin. Use too many bins, and the histogram will be sparse, with many bins
    having no tallies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate some histograms. First, we’ll randomly sample integers in [0,9]
    and count how many of each integer we get. The code for this is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> import numpy as np'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> n = np.random.randint(0,10,10000)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> h = np.bincount(n)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> h'
  prefs: []
  type: TYPE_NORMAL
- en: array([ 975, 987, 987, 1017, 981, 1043, 1031, 988, 1007, 984])
  prefs: []
  type: TYPE_NORMAL
- en: We first set n to an array of 10,000 integers in [0, 9]. We then use np .bincount
    to count how many of each digit we have. We see that this run gave us 975 zeros
    and 984 nines. If the NumPy pseudorandom generator is doing its job, we expect,
    on average, to have 1,000 of each digit in a sample of 10,000 digits. We expect
    some variation, but most values are close enough to 1,000 to be convincing.
  prefs: []
  type: TYPE_NORMAL
- en: The counts above tell us how many times each digit appeared. If we divide each
    bin of a histogram by the total of all the bins, we change from simple counts
    to the probability of that bin appearing. For the random digits above, we get
    the probabilities with
  prefs: []
  type: TYPE_NORMAL
- en: '>>> h = h / h.sum()'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> h'
  prefs: []
  type: TYPE_NORMAL
- en: array([0.0975, 0.0987, 0.0987, 0.1017, 0.0981, 0.1043, 0.1031, 0.0988,
  prefs: []
  type: TYPE_NORMAL
- en: 0.1007, 0.0984])
  prefs: []
  type: TYPE_NORMAL
- en: which tells us that each digit did appear with a probability of about 0.1, or
    1 out of 10\. This trick of dividing histogram values by the sum of the counts
    in the histogram allows us to estimate probability distributions from samples.
    It also tells us the likelihood of particular values appearing when sampling from
    whatever process generated the data used to make the histogram. You should note
    that I said we could *estimate* the probability distribution from a set of samples
    drawn from it. The larger the number of samples, the closer the estimated probability
    distribution will be to the actual population distribution generating the samples.
    We will never get to the actual population distribution, but given the limit of
    an infinite number of samples, we can get as close as we need to.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms are frequently used to look at the distribution of pixel values in
    an image. Let’s make a plot of the histogram of the pixels in two images. You
    can find the code in the file ricky.py. (I won’t show it here, as it doesn’t add
    to the discussion.) The images used are two example grayscale images included
    with SciPy in scipy.misc. The first shows people walking up stairs (ascent), and
    the second is the face of a young raccoon (face), as shown in [Figure 3-1](ch03.xhtml#ch03fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-1: People ascending (left) and “Ricky” raccoon (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-2](ch03.xhtml#ch03fig02) provides a plot of the histograms for each
    image, as probabilities. It shows two very different distributions of gray level
    values in the images. For the raccoon face, the distribution is more spread out
    and flatter, while the ascent image has a spike right around gray level 128 and
    a few bright pixels. The distributions tell us that if we pick a random pixel
    in the face image, we’re most likely to get one around gray level 100, but an
    arbitrary pixel in the ascent image will, with high relative likelihood, be closer
    to gray level 128.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-2: Histograms as probabilities for two 512×512-pixel grayscale sample
    images*'
  prefs: []
  type: TYPE_NORMAL
- en: Again, histograms tally the counts of how many items fall into the predefined
    bins. We saw for the images that the histogram as probability distribution tells
    us how likely we are to get a particular gray level value if we select a random
    pixel. Likewise, the probability distribution for the random digits in the example
    before that tells us the probability of getting each digit when we ask for a random
    integer in the range [0,9].
  prefs: []
  type: TYPE_NORMAL
- en: Histograms are discrete representations of a probability distribution. Let’s
    take a look at the more common discrete distributions now.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete Probability Distributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’ve already encountered the most common discrete distribution several times:
    it’s the uniform distribution. That’s the one we get naturally by rolling dice
    or flipping coins. In the uniform distribution, all possible outcomes are equally
    likely. A histogram of a simulation of a process drawing from a uniform distribution
    is flat; all outcomes show up with more or less the same frequency. We’ll see
    the uniform distribution again when we look at continuous distributions. For now,
    think dice.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few other discrete distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The Binomial Distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Perhaps the second most common discrete distribution is the *binomial distribution*.
    This distribution represents the expected number of events happening in a given
    number of trials if each event has a specified probability. Mathematically, the
    probability of *k* events happening in *n* trials if the probability of the event
    happening is *p* can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/046equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, what’s the probability of getting three heads in a row when flipping
    a fair coin three times? From the product rule, we know the probability is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/046equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using the binomial formula, we get the same answer by calculating
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/046equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So far, not particularly helpful. However, what if the probability of the event
    isn’t 0.5? What if we have an event, say the likelihood of a person winning *Let’s
    Make a Deal* by not changing doors, and we want to know the probability that 7
    people out of 13 will win by not changing their guess? We know the probability
    of winning the game without changing doors is 1/3—that’s *p*. We then have 13
    trials (*n*) and 7 winners (*k*). The binomial formula tells us the likelihood
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/046equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and, if the players *do* switch doors,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/046equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The binomial formula gives us the probability of a given number of events in
    a given number of trials for a specified probability per event. If we fix *n*
    and *p* and vary *k*, 0 ≤ *k* ≤ *n*, we get the probability for each *k* value.
    This gives us the distribution. For example, let *n* = 5 and *p* = 0.3, then 0
    ≤ *k* ≤ 5 with the probability for each *k* value as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/047equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Allowing for rounding, this sums to 1.0, as we know it must because the sum
    of probabilities over an entire sample space is always 1.0\. Notice that we calculate
    all the possible values for the binomial distribution when *n* = 5\. Collectively,
    this specifies the *probability mass function (pmf)*. The probability mass function
    tells us the probability associated with all possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The binomial distribution is parameterized by *n* and *p*. For *n* = 5 and
    = *p* = 0.3, we see from the results above that a random sample from such a binomial
    distribution will return 1 most often—some 36 percent of the time. How can we
    draw samples from a binomial distribution? In NumPy, we need only call the binomial
    function in the random module:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.random.binomial(5, 0.3, size=1000)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s = np.bincount(t)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s'
  prefs: []
  type: TYPE_NORMAL
- en: array([159, 368, 299, 155,  17,   2])
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s / s.sum()'
  prefs: []
  type: TYPE_NORMAL
- en: array([0.159, 0.368, 0.299, 0.155, 0.017, 0.002])
  prefs: []
  type: TYPE_NORMAL
- en: We pass binomial the number of trials (5) and the probability of success for
    each trial (0.3). We then ask for 1,000 samples from a binomial distribution with
    these parameters. Using np.bincount, we see that the most commonly returned value
    was indeed 1, as we calculated above. By using our histogram summation trick,
    we get a probability of 0.368 for selecting a 1—close to the 0.3601 we calculated.
  prefs: []
  type: TYPE_NORMAL
- en: The Bernoulli Distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *Bernoulli distribution* is a special case of the binomial distribution.
    In this case, we fix *n* = 1, meaning there’s only one trial. The only values
    we can sample are 0 or 1; either the event happens, or it doesn’t. For example,
    with *p* = 0.5, we get
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.random.binomial(1, 0.5, size=1000)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.bincount(t)'
  prefs: []
  type: TYPE_NORMAL
- en: array([496, 504])
  prefs: []
  type: TYPE_NORMAL
- en: This is reasonable, since a probability of 0.5 means we’re flipping a fair coin,
    and we see that the proportion of heads or tails is roughly equal.
  prefs: []
  type: TYPE_NORMAL
- en: If we change to *p* = 0.3, we get
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.random.binomial(1, 0.3, size=1000)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.bincount(t)'
  prefs: []
  type: TYPE_NORMAL
- en: array([665, 335])
  prefs: []
  type: TYPE_NORMAL
- en: '>>> 335/1000'
  prefs: []
  type: TYPE_NORMAL
- en: '0.335'
  prefs: []
  type: TYPE_NORMAL
- en: Again, close to 0.3, as we expect to see.
  prefs: []
  type: TYPE_NORMAL
- en: Use samples from a binomial distribution when you want to simulate events with
    a known probability. With the Bernoulli form, we can sample binary outcomes, 0
    or 1, where the likelihood of the event need not be that of the flip of a fair
    coin, 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson Distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sometimes, we don’t know the probability of an event happening for any particular
    trial. Instead, we might know the average number of events that happen over some
    interval, say of time. If the average number of events that happen over some time
    is λ (lambda), then the probability of *k* events happening in that interval is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/048equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the *Poisson distribution*, and it’s useful to model events like radioactive
    decay or the incidence of photons on an X-ray detector over some period of time.
    To sample events according to this distribution, we use poisson from the random
    module. For example, assume over some time interval there are five events on average
    (λ = 5). What sort of probability distribution do we get using the Poisson distribution?
    In code,
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.random.poisson(5, size=1000)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s = np.bincount(t)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s'
  prefs: []
  type: TYPE_NORMAL
- en: array([  6,  36,  83, 135, 179, 173, 156, 107,  58,  40,  20,   4,   2,
  prefs: []
  type: TYPE_NORMAL
- en: 0,   0,   1])
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t.max()'
  prefs: []
  type: TYPE_NORMAL
- en: '15'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s = s / s.sum()'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s'
  prefs: []
  type: TYPE_NORMAL
- en: array([0.006, 0.036, 0.083, 0.135, 0.179, 0.173, 0.156, 0.107, 0.058,
  prefs: []
  type: TYPE_NORMAL
- en: 0.04 , 0.02 , 0.004, 0.002, 0.   , 0.   , 0.001])
  prefs: []
  type: TYPE_NORMAL
- en: Here, we see that, unlike the binomial distribution, which could not select
    more than *n* events, the Poisson distribution can select numbers of events that
    exceed the value of λ. In this case, the largest number of events in the time
    interval was 15, which is three times the average. You’ll find that the most frequent
    number of events is right around the average of five, as you might expect, but
    significant deviations from the average are possible.
  prefs: []
  type: TYPE_NORMAL
- en: The Fast Loaded Dice Roller
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: What if we need to draw samples according to an arbitrary discrete distribution?
    Earlier, we saw some histograms based on images. In that case, we could sample
    from the distribution represented by the histogram by picking pixels in the image
    at random. But what if we wanted to sample integers according to arbitrary weights?
    To do this, we can use the new Fast Loaded Dice Roller of Saad, et al.[¹](#ch03fn01)
  prefs: []
  type: TYPE_NORMAL
- en: The *Fast Loaded Dice Roller (FLDR)* lets us specify an arbitrary discrete distribution
    and then draw samples from it. The code is in Python and freely available. (See
    *[https://github.com/probcomp/fast-loaded-dice-roller/](https://github.com/probcomp/fast-loaded-dice-roller/)*.)
    I’ll show how to use the code to sample according to a generic distribution. I
    recommend downloading just the fldr.py and fldrf.py files from the GitHub repository
    instead of running setup.py. Additionally, edit the .fldr import lines in fldrf.py
    to remove the “.” so they read
  prefs: []
  type: TYPE_NORMAL
- en: from fldr import fldr_preprocess_int
  prefs: []
  type: TYPE_NORMAL
- en: from fldr import fldr_s
  prefs: []
  type: TYPE_NORMAL
- en: 'Using FLDR requires two steps. The first is to tell it the particular distribution
    you want to sample from. You define the distribution as ratios. (For our purposes,
    we’ll use actual probabilities, meaning our distribution will always add up to
    1.0.) This is the preprocessing step, which we only need to do once for each distribution.
    After that, we can draw samples. An example will clarify:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from fldrf import fldr_preprocess_float_c'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from fldr import fldr_sample'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> x = fldr_preprocess_float_c([0.6,0.2,0.1,0.1])'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = [fldr_sample(x) for i in range(1000)]'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.bincount(t)'
  prefs: []
  type: TYPE_NORMAL
- en: array([598, 190, 108, 104])
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the two FLDR functions we need: fldr_preprocess_float _c and
    fldr_sample. Then we define the distribution using a list of four numbers. Four
    numbers imply samples will be integers in [0, 3]. However, unlike a uniform distribution,
    here we’re specifying we want zero 60 percent of the time, one 20 percent of the
    time, and two and three 10 percent of the time each. The information that FLDR
    needs to sample from the distribution is returned in x.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling fldr_sample returns a single sample from the distribution. Notice two
    things: first, we need to pass x in, and second, FLDR doesn’t use NumPy, so to
    draw 1,000 samples, we use a standard Python list comprehension. The 1,000 samples
    are in the list, t. Finally, we generate the histogram and see that nearly 60
    percent of the samples are zero and slightly more than 10 percent are three, as
    we intended.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the histogram of the raccoon face image we used earlier to see if
    FLDR will follow a more complex distribution. We’ll load the image, generate the
    histogram, convert it to a probability distribution, and use the probabilities
    to set up FLDR. After that, we’ll draw 25,000 samples from the distribution, compute
    the histogram of the samples, and plot that histogram along with the original
    histogram to see if FLDR follows the actual distribution we give it. The code
    we need is
  prefs: []
  type: TYPE_NORMAL
- en: from scipy.misc import face
  prefs: []
  type: TYPE_NORMAL
- en: im = face(True)
  prefs: []
  type: TYPE_NORMAL
- en: b = np.bincount(im.ravel(), minlength=256)
  prefs: []
  type: TYPE_NORMAL
- en: b = b / b.sum()
  prefs: []
  type: TYPE_NORMAL
- en: x = fldr_preprocess_float_c(list(b))
  prefs: []
  type: TYPE_NORMAL
- en: t = [fldr_sample(x) for i in range(25000)]
  prefs: []
  type: TYPE_NORMAL
- en: q = np.bincount(t, minlength=256)
  prefs: []
  type: TYPE_NORMAL
- en: q = q / q.sum()
  prefs: []
  type: TYPE_NORMAL
- en: Running this code leaves us with b, a probability distribution from the histogram
    of the face image, and q, the distribution created from 25,000 samples from the
    FLDR distribution. [Figure 3-3](ch03.xhtml#ch03fig03) shows us a plot of the two
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The solid line in [Figure 3-3](ch03.xhtml#ch03fig03) is the probability distribution
    we supplied to fldr_preprocess_float_c representing the distribution of gray levels
    (intensities) in the raccoon image. The dashed line is the histogram of the 25,000
    samples from this distribution. As we can see, they follow the requested distribution
    with the sort of variation we expect from such a small number of samples. As an
    exercise, change the number of samples from 25,000 to 500,000 and plot the two
    curves. You’ll see that they’re now virtually on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-3: Comparing the Fast Loaded Dice Roller distribution (dashed) to
    the distribution generated from the SciPy face image (solid)*'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete distributions generate integers with specific likelihoods. Let’s leave
    them now and consider continuous probability distributions, which return floating-point
    values instead.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Probability Distributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I haven’t discussed continuous probabilities yet in this chapter. In part, not
    doing so was to make the concepts behind probability easier to follow. A continuous
    probability distribution, like a discrete one, has a particular shape. However,
    instead of assigning a probability to a specific integer value, as we saw above,
    the probability of selecting a particular value from a continuous distribution
    is zero. The probability of a specific value, a real number, is zero because there
    are an infinite number of possible values from a continuous distribution; this
    means no particular value can be selected. Instead, what we talk about is the
    probability of selecting values in a specific range of values.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the most common continuous distribution is the uniform distribution
    over [0, 1]. This distribution returns *any* real number in that range. Although
    the probability of returning a specific real number is zero, we can talk about
    the probability of returning a value in a range, such as [0, 0.25].
  prefs: []
  type: TYPE_NORMAL
- en: Consider again the uniform distribution over [0, 1]. We know that the sum of
    all the individual probabilities from zero to one is 1.0\. So, what is the probability
    of sampling a value from this distribution and having that value be in the range
    [0, 0.25]? All values are equally likely, and all add to 1.0, so we must have
    a 25 percent chance of returning a value in [0, 0.25]. Similarly, we have a 25
    percent chance of returning a value in [0.75, 1], as that also covers 1/4 of the
    possible range.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about summing infinitely small things over a range, we’re talking
    about integration, the part of calculus that we won’t cover in this book. Conceptually,
    however, we can understand what’s happening if we think about a discrete distribution
    in the limit where the number of values it can return goes to infinity, and we’re
    summing the probabilities over some range.
  prefs: []
  type: TYPE_NORMAL
- en: We also can think about this graphically. [Figure 3-4](ch03.xhtml#ch03fig04)
    shows the continuous probability distributions I’ll discuss.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-4: Some common continuous probability distributions*'
  prefs: []
  type: TYPE_NORMAL
- en: To get the probability of sampling a value in some range, we add up the area
    under the curve over that range. Indeed, this is precisely what integration does;
    the integration symbol (∫) is nothing more than a fancy “S” for *sum*. It’s the
    continuous version of ∑ for summing discrete values.
  prefs: []
  type: TYPE_NORMAL
- en: The distributions in [Figure 3-4](ch03.xhtml#ch03fig04) are the most common
    ones you’ll encounter, though there are many others useful enough to be given
    names. All of these distributions have associated *probability density functions
    (pdfs)*, closed-form functions that generate the probabilities that sampling from
    the distribution will give. I generated the curves in [Figure 3-4](ch03.xhtml#ch03fig04)
    instead using the code in the file continuous.py. The curves are estimates of
    the probability density functions, and I created them from the histogram of a
    large number of samples. I did so intentionally to demonstrate that the NumPy
    random functions sampling from these distributions do what they claim.
  prefs: []
  type: TYPE_NORMAL
- en: Pay little attention to the x-axis in [Figure 3-4](ch03.xhtml#ch03fig04). The
    distributions have different ranges of output; they’re scaled here to fit all
    of them on the graph. The important thing to notice is their shapes. The uniform
    distribution is, well, uniform over the entire range. The normal curve, also frequently
    called a *Gaussian* or a *bell curve*, is the second most common distribution
    used in deep learning. For example, the He initialization strategy for neural
    networks samples initial weights from a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code generating the data for [Figure 3-4](ch03.xhtml#ch03fig04) is worth
    considering, as it shows us how to use NumPy to get samples:'
  prefs: []
  type: TYPE_NORMAL
- en: N = 10000000
  prefs: []
  type: TYPE_NORMAL
- en: B = 100
  prefs: []
  type: TYPE_NORMAL
- en: t = np.random.random(N)
  prefs: []
  type: TYPE_NORMAL
- en: u = np.histogram(t, bins=B)[0]
  prefs: []
  type: TYPE_NORMAL
- en: u = u / u.sum()
  prefs: []
  type: TYPE_NORMAL
- en: t = np.random.normal(0, 1, size=N)
  prefs: []
  type: TYPE_NORMAL
- en: n = np.histogram(t, bins=B)[0]
  prefs: []
  type: TYPE_NORMAL
- en: n = n / n.sum()
  prefs: []
  type: TYPE_NORMAL
- en: t = np.random.gamma(5.0, size=N)
  prefs: []
  type: TYPE_NORMAL
- en: g = np.histogram(t, bins=B)[0]
  prefs: []
  type: TYPE_NORMAL
- en: g = g / g.sum()
  prefs: []
  type: TYPE_NORMAL
- en: t = np.random.beta(5,2, size=N)
  prefs: []
  type: TYPE_NORMAL
- en: b = np.histogram(t, bins=B)[0]
  prefs: []
  type: TYPE_NORMAL
- en: b = b / b.sum()
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*We’re using the classic NumPy functions here, not the newer Generator-based
    functions. NumPy updated the pseudorandom number code in recent versions, but
    the overhead of using the new code will detract from what we want to see here.
    Unless you’re very serious about pseudorandom number generation, the older functions,
    and the Mersenne Twister pseudorandom number generator they’re based on, will
    be more than adequate.*'
  prefs: []
  type: TYPE_NORMAL
- en: To make the plots, we first use 10 million samples from each distribution (N).
    Then, we use 100 bins in the histogram (B). Again, the x-axis range when plotting
    isn’t of interest here, only the shapes of the curves.
  prefs: []
  type: TYPE_NORMAL
- en: The uniform samples use random, a function we’ve seen before. Passing the samples
    to histogram and applying the “divide by the sum” trick creates the probability
    curve data (u). We repeat this process for the Gaussian (normal), Gamma (gamma),
    and Beta (beta) distributions as well.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice that normal, gamma, and beta accept arguments. These distributions
    are parameterized; their shape is altered by changing these parameters. For the
    normal curve, the first parameter is the mean (μ), and the second is the standard
    deviation (σ). Some 68 percent of the normal curve lies within one standard deviation
    of the mean, [μ – σ, μ + σ]. The normal curve is ubiquitous in math and nature,
    and one could write an entire book on it alone. It’s always symmetric around its
    mean value. The standard deviation controls how wide or narrow the curve is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gamma distribution is also parameterized. It accepts two parameters: the
    shape (*k*) and the scale (θ). Here, *k* = 5, and the scale is left at its default
    value of θ = 1\. As the shape increases, the gamma distribution becomes more and
    more like a Gaussian, with a bump that moves toward the center of the distribution.
    The scale parameter affects the horizontal size of the bump.'
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, the beta distribution uses two parameters, *a* and *b*. Here, *a*
    = 5 and *b* = 2\. If *a* > *b*, the hump of the distribution is on the right;
    if reversed, it is on the left. If *a* = *b*, the beta distribution becomes the
    uniform distribution. The flexibility of the beta distribution makes it quite
    handy for simulating different processes, as long as you can find *a* and *b*
    values approximating the probability distribution you want. However, depending
    on the precision you require, the new Fast Loaded Dice Roller we saw in the previous
    section might be a better option in practice if you have a sufficiently detailed
    discrete distribution approximation of the continuous distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-2](ch03.xhtml#ch03tab02) shows us the probability density functions
    for the normal, gamma, and beta distributions. An exercise for the reader is to
    use these functions to recreate [Figure 3-4](ch03.xhtml#ch03fig04). Your results
    will be smoother still than the curves in the figure. You can calculate the *B*(*a*,
    *b*) integral in [Table 3-2](ch03.xhtml#ch03tab02) by using the function scipy.special.beta.
    For Γ(*k*), see scipy.special.gamma. Additionally, if the argument to the Γ function
    is an integer, Γ(*n* + 1) = *n*!, so Γ(5) = Γ(4 + 1) = 4! = 24.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3-2:** The Probability Density Functions for the Normal, Gamma, and
    Beta Distributions'
  prefs: []
  type: TYPE_NORMAL
- en: '| **normal** | ![image](Images/054equ01.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| **gamma** | ![image](Images/054equ02.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| **beta** | ![image](Images/054equ03.jpg) |'
  prefs: []
  type: TYPE_TB
- en: If you’re interested in ways to sample values from these distributions, my book
    *Random Numbers and Computers* (Springer, 2018) discusses these distributions
    and others in more depth than we can provide here, including implementations in
    C for generating samples from them. For now, let’s examine one of the most important
    theorems in probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: Central Limit Theorem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine we draw *N* samples from some distribution and calculate the mean value,
    *m*. If we repeat this exercise many times, we’ll get a set of mean values, *{m*[0],
    *m*[1], . . .}, each from a set of samples from the distribution. It doesn’t matter
    if *N* is the same each time, but *N* shouldn’t be too small. The rule of thumb
    is that *N* should be at least 30 samples.
  prefs: []
  type: TYPE_NORMAL
- en: The *central limit theorem* states that the histogram or probability distribution
    generated from this set of sample means, the *m*’s, will approach a Gaussian in
    shape regardless of the shape of the distribution the samples were drawn from
    in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: For example, this code
  prefs: []
  type: TYPE_NORMAL
- en: M = 10000
  prefs: []
  type: TYPE_NORMAL
- en: m = np.zeros(M)
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(M):'
  prefs: []
  type: TYPE_NORMAL
- en: t = np.random.beta(5,2,size=M)
  prefs: []
  type: TYPE_NORMAL
- en: m[i] = t.mean()
  prefs: []
  type: TYPE_NORMAL
- en: creates 10,000 sets of samples from a beta distribution, Beta(5,2), each with
    10,000 samples. The mean of each set of samples is stored in m. If we run this
    code and plot the histogram of *m*, we get [Figure 3-5](ch03.xhtml#ch03fig05).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-5: The distribution of mean values of 10,000 sets of samples of 10,000
    from Beta(5,2)*'
  prefs: []
  type: TYPE_NORMAL
- en: The shape of [Figure 3-5](ch03.xhtml#ch03fig05) is decidedly Gaussian. Again,
    the shape is a consequence of the central limit theorem and does not depend on
    the shape of the underlying distribution. [Figure 3-5](ch03.xhtml#ch03fig05) tells
    us that the sample means from many sets of samples from Beta(5,2) themselves have
    a mean of about 0.714\. The mean of the sample means (m.mean()) is 0.7142929 for
    one run of the code above.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a formula to calculate the mean value of a Beta distribution. The population
    mean value of a Beta(5,2) distribution is known to be *a*/*(a* + *b*) = 5/(5 +
    2) = 5/7 = 0.714285\. The mean of the plot in [Figure 3-5](#ch03fig05) is a measurement
    of the true population mean, of which the many means from the Beta(5,2) samples
    are only estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explain this again to really follow what’s going on. For any distribution,
    like the Beta(5,2) distribution, if we draw *N* samples, we can calculate the
    mean of those samples, a single number. If we repeat this process for many sets
    of *N* samples, each with its own mean, and we make a histogram of the distribution
    of the means we measured, we’ll get a plot like [Figure 3-5](ch03.xhtml#ch03fig05).
    That plot tells us that all of the many sample means are themselves clustered
    around a mean value. The mean value of the means is a measure of the population
    mean. It’s the mean we’d get if we could draw an infinite number of samples from
    the distribution. If we change the code above to use the uniform distribution,
    we’ll get a population mean of 0.5\. Similarly, if we switch to a Gaussian distribution
    with a mean of 11, the resulting histogram will be centered at 11.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prove this claim again but this time with a discrete distribution. Let’s
    use the Fast Loaded Dice Roller to generate samples from a lopsided discrete distribution
    using this code:'
  prefs: []
  type: TYPE_NORMAL
- en: from fldrf import fldr_preprocess_float_c
  prefs: []
  type: TYPE_NORMAL
- en: from fldr import fldr_sample
  prefs: []
  type: TYPE_NORMAL
- en: z = fldr_preprocess_float_c([0.1,0.6,0.1,0.1,0.1])
  prefs: []
  type: TYPE_NORMAL
- en: m = np.zeros(M)
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(M):'
  prefs: []
  type: TYPE_NORMAL
- en: t = np.array([fldr_sample(z) for i in range(M)])
  prefs: []
  type: TYPE_NORMAL
- en: m[i] = t.mean()
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-6](ch03.xhtml#ch03fig06) shows the discrete distribution (top) and
    the corresponding distribution of the sample means (bottom).'
  prefs: []
  type: TYPE_NORMAL
- en: From the probability mass function, we can see that the most frequent value
    we expect from the sample is 1, with a probability of 60 percent. However, the
    tail on the right means we’ll also get values 2 through 4 about 30 percent of
    the time. The weighted mean of these is 0.6(1) + 0.1(2) + 0.1(3) + 0.1(4) = 1.5,
    which is precisely the mean of the sample distribution on the bottom of [Figure
    3-6](ch03.xhtml#ch03fig06). The central limit theorem works. We’ll revisit the
    central limit theorem in [Chapter 4](ch04.xhtml#ch04) when we discuss hypothesis
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-6: An arbitrary discrete distribution (top) and the distribution
    of sample means drawn from it (bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: The Law of Large Numbers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A concept related to the central limit theorem, and often confused with it,
    is the *law of large numbers*. The law of large numbers states that as the size
    of a sample from a distribution increases, the mean of the sample moves closer
    and closer to the mean of the population. In this case, we’re contemplating a
    single sample from the distribution and making a statement about how close we
    expect its mean to be to the true population mean. For the central limit theorem,
    we have many different sets of samples from the distribution and are making a
    statement about the distribution of the means of those sets of samples.
  prefs: []
  type: TYPE_NORMAL
- en: We can demonstrate the law of large numbers quite simply by selecting larger
    and larger size samples from a distribution and tracking the mean as a function
    of the sample size (the number of samples drawn). In code, then,
  prefs: []
  type: TYPE_NORMAL
- en: m = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for n in np.linspace(1,8,30):'
  prefs: []
  type: TYPE_NORMAL
- en: t = np.random.normal(1,1,size=int(10**n))
  prefs: []
  type: TYPE_NORMAL
- en: m.append(t.mean())
  prefs: []
  type: TYPE_NORMAL
- en: where we’re drawing ever-larger sample sizes from a normal distribution with
    a mean of 1\. The first sample size is 10, and the last is 100 million. If we
    plot the mean of the samples as a function of sample size, we see the law of large
    numbers at work.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-7](ch03.xhtml#ch03fig07) shows the sample means as a function of
    the number of samples for the normal distribution with a mean of 1 (dashed line).
    As the number of samples from the distribution increases, the mean of the samples
    approaches the population mean, which illustrates the law of large numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-7: The law of large numbers in action*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s change gears and move on to Bayes’ theorem, the last topic for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml#ch02), we discussed an example where we determined
    whether a woman had cancer. There, I promised that Bayes’ theorem would tell us
    how to properly account for the probability of a randomly selected woman in her
    40s having breast cancer. Let’s fulfill that promise in this section by learning
    what Bayes’ theorem is and how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the product rule, [Equation 2.8](ch02.xhtml#ch02equ08), we know the following
    two mathematical statements are true:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*B*, *A*) = *P*(*B*|*A*)*P*(*A*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*, *B*) = *P*(*A*|*B*)*P*(*B*)'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, because the joint probability of both *A* and *B* doesn’t depend
    on which event we call *A* and which we call *B*,
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*, *B*) = *P*(*B*, *A*)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*B*|*A*)*P*(*A*) = *P*(*A*|*B*)*P*(*B*)'
  prefs: []
  type: TYPE_NORMAL
- en: Dividing by *P*(*A*), we get
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is *Bayes’ theorem*, the heart of the Bayesian approach to probability,
    and the proper way to compare two conditional probabilities: *P**(B*|*A*) and
    *P*(*A*|*B*). You’ll sometimes see [Equation 3.1](ch03.xhtml#ch03equ01) referred
    to as *Bayes’ rule*. You’ll also often see no apostrophe after “Bayes,” which
    is a bit sloppy and ungrammatical, but common.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 3.1](ch03.xhtml#ch03equ01) has been enshrined in neon lights, tattoos,
    and even baby names: “Bayes.” The equation is named after Thomas Bayes (1701–1761),
    an English minister and statistician, and was published after his death. In words,
    [Equation 3.1](ch03.xhtml#ch03equ01) says the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The *posterior probability*, *P*(*B*|*A*), is the product of *P*(*A*|*B*), the
    *likelihood*, and *P*(*B*), the *prior*, normalized by *P*(*A*), the marginal
    probability or *evidence*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what Bayes’ theorem is, let’s see it in action so we can understand
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Cancer or Not Redux
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way to think about the components of Bayes’ theorem is in the context of
    medical testing. At the beginning of [Chapter 2](ch02.xhtml#ch02), we calculated
    the probability of a woman having breast cancer given a positive mammogram and
    found that it was quite different from what we might naively have believed it
    to be. Let’s revisit that problem now using Bayes’ theorem. It might be helpful
    to reread the first section of [Chapter 2](ch02.xhtml#ch02) before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: We want to use Bayes’ theorem to find the posterior probability, the probability
    of breast cancer given a positive mammogram. We’ll write this as *P**(bc* + |+),
    meaning breast cancer (*bc*+) given a positive mammogram (+).
  prefs: []
  type: TYPE_NORMAL
- en: In the problem, we’re told that the mammogram returns a positive result, given
    the patient has breast cancer, 90 percent of the time. We write this as
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(+|*bc*+) = 0.9'
  prefs: []
  type: TYPE_NORMAL
- en: This is the likelihood of a positive mammogram in terms of Bayes’ equation,
    *P*(*A*|*B*) = *P*(+|*bc*+).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’re told the probability of a random woman having breast cancer is 0.8
    percent. Therefore, we know
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*bc*+) = 0.008'
  prefs: []
  type: TYPE_NORMAL
- en: This is the prior probability, *P*(*B*), in Bayes’ theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have all the components of [Equation 3.1](ch03.xhtml#ch03equ01) except one:
    *P**(A*).  What is *P*(*A*) in this context? It’s *P*(+), the marginal probability
    of a positive mammogram regardless of any *B*, any breast cancer status. It’s
    also the evidence that we have, the thing we know: the mammogram was positive.'
  prefs: []
  type: TYPE_NORMAL
- en: In the problem, we’re told there’s a 7 percent chance a woman without breast
    cancer has a positive mammogram. Is this *P*(+)? No, it is *P*(+|*bc*–), the probability
    of a positive mammogram *given* no breast cancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve referred to *P**(A*) as the marginal probability twice now. We know what
    to do to get a marginal or total probability: we sum over all the other parts
    of a joint probability that don’t matter for what we want to know. Here, we have
    to sum over all the partitions of the sample space we don’t care about to get
    the marginal probability of a positive mammogram. What partitions are those? There
    are only two: either a woman has breast cancer or she doesn’t. Therefore, we need
    to find'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(+) = *P*(+|*bc*+)*P*(*bc*+) + *P*(+|*bc*–)*P*(*bc*–)'
  prefs: []
  type: TYPE_NORMAL
- en: We know all of these quantities already, except *P**(bc*–). This is the prior
    probability that a randomly selected woman will *not* have breast cancer, *P**(bc*–)
    = 1 – *P*(*bc*+) = 0.992.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, you’ll see the summation over other terms in the joint probability
    expressed in the denominator of Bayes’ theorem. Even if they’re not explicitly
    called out, they are there, implicit in what it takes to find *P**(A*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have all the pieces and can calculate the probability using Bayes’
    theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/061equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the result we found earlier. Recall, a large percentage of doctors in
    the study claimed the probability of cancer from a positive mammogram, *P**(A*|*B*),
    was 90 percent. Their mistake was incorrectly equating *P(A*|*B*) with *P(B*|*A*).
    Bayes’ theorem correctly relates the two by using the prior and the marginal probability.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the Prior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We don’t need to stop with this single calculation. Consider the following:
    what if, after a woman receives the news that her mammogram is positive, she decides
    to have a second mammogram at another facility with different radiologists reading
    the results, and that mammogram also comes back positive? Does she still believe
    that her probability of having breast cancer is 9 percent? Intuitively, we might
    think that she now has more reason to believe that she has cancer. Can this belief
    be quantified? It can, in the Bayesian view, by updating the prior, *P**(bc*+),
    with the posterior calculated from the first test, *P*(*bc* + |+). After all,
    she now has a stronger prior probability of cancer given the first positive mammogram.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate this new posterior based on the previous mammogram result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/061equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As 57 percent is significantly higher than 9 percent, our hypothetical woman
    now has significantly more reason to believe she has breast cancer.
  prefs: []
  type: TYPE_NORMAL
- en: Notice what has changed in this new calculation, besides a dramatic increase
    in the posterior probability of breast cancer given the second mammogram’s positive
    result. First, the prior probability of breast cancer went from 0.008 → 0.094,
    the posterior calculated based on the first test. Second, *P*(*bc*–) also changed
    from 0.992 → 0.906\. Why? Because the prior changed and *P*(*bc*–) = 1 – *P*(*bc*+).
    The sum of *P*(*bc*+) and *P*(*bc*–) must still be 1.0—either she has breast cancer,
    or she doesn’t—that’s the entire sample space.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, we updated the prior based on the initial test result,
    and we had an initial prior given to us in the first example. What about the prior
    in general? In many cases, Bayesians select the prior, at least initially, based
    on an actual belief about the problem. Often the prior is a uniform distribution,
    known as the *uninformed prior* because there’s nothing to guide the selection
    of anything else. For the breast cancer example, the prior is something that can
    be estimated from an experiment using a random selection of women from the general
    population.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, don’t take the numbers here too seriously; they are for
    example use only. Also, while a woman certainly has the option to get a second
    opinion, the gold standard for a breast cancer diagnosis is biopsy, the likely
    next step after an initial positive mammogram. Finally, throughout this section,
    I’ve referred to women and breast cancer. Men also get breast cancer, though it
    is rare, with less than 1 percent of cases in men. However, it made the discussion
    simpler to refer only to women. I’ll note that breast cancer cases in men are
    more likely to be fatal, though the reasons why are not yet known.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Theorem in Machine Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bayes’ theorem is prevalent throughout machine learning and deep learning. One
    classic use of Bayes’ theorem, one that can work surprisingly well, is to use
    it as a classifier. This is known as the *Naive Bayes* classifier. Early email
    spam filters used this approach quite effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have a dataset consisting of class labels, *y*, and feature vectors,
    ***x***. The goal of a Naive Bayes classifier is to tell us, for each class, the
    probability that a given feature vector belongs to that class. With those probabilities,
    we can assign a class label by selecting the largest probability. That is, we
    want to find *P**(y*|***x***) for each class label, *y*. This is a conditional
    probability, so we can use Bayes’ theorem with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/03equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The equation above is saying that the probability of feature vector *x* representing
    an instance of class label *y* is the probability of the class label *y* generating
    a feature vector *x* times the prior probability of class label *y* occurring,
    divided by the marginal probability of the feature vector over all class labels.
    Recall the implicit sum in calculating *P*(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: How is this useful to us? Since we have a dataset, we can estimate *P**(y*)
    using it, assuming the dataset class distribution is a fair representation of
    what we’d encounter when using the model. And, since we have labels, we can partition
    the dataset into smaller, per class, collections. This might help us do something
    useful to get the likelihoods per class, *P*(*x*|*y*). We’ll ignore the marginal
    *P*(*x*) completely. Let’s see why, in this case, we’re free to do so.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 3.2](ch03.xhtml#ch03equ02) is for a particular class label, say *y*
    = 1\. We’ll have other versions of it for all the class labels in the dataset.
    We said our classifier consists of calculating the posterior probabilities for
    each class label and selecting the largest one as the label assigned to an unknown
    feature vector. The denominator of [Equation 3.2](ch03.xhtml#ch03equ02) is a scale
    factor, which makes the output a true probability. For our use case, however,
    we only care about the relative ordering of *P**(y*|***x***) over the different
    class labels. Since *P*(***x***) is the same for all *y*, it’s a common factor
    that will change the number associated with *P**(y*|***x***) but not the ordering
    over the different class labels. Therefore, we can ignore it and concentrate on
    finding the products of the likelihoods and the priors. Although the largest *P*(*y*|***x***)
    calculated this way is no longer a proper probability, it’s still the correct
    class label to assign.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that we can ignore *P****(x***) and the *P*(*y*) values are easily estimated
    from the dataset, we’re left with calculating *P****(x***|*y*), the likelihood
    that given the class label is *y*, we’d have a feature vector ***x***. What can
    we do in this case?
  prefs: []
  type: TYPE_NORMAL
- en: First, we can think about what *P****(x***|*y*) is. It’s a conditional probability
    for feature vectors given the feature vectors are all representatives of class
    *y*. For the moment, let’s ignore the *y* part, since we know the feature vectors
    all come from class *y*.
  prefs: []
  type: TYPE_NORMAL
- en: This leaves only *P****(x***) because we fixed *y*. A feature vector is a collection
    of individual features, *x* = (***x[0]***, *x*[1], *x*[2], . . ., *x[n]*[–1])
    for *n* features in the vector. Therefore, *P****(x***) is really a joint probability,
    the probability that all the individual features have their specific values *at
    the same time*. So, we can write
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*x*) = *P*(*x*[0], *x*[1], *x*[2], . . ., *x*[*n*–1])'
  prefs: []
  type: TYPE_NORMAL
- en: How does this help? If we make one more assumption about our data, we’ll see
    that we can break up this joint probability in a convenient way. Let’s assume
    that all the features in our feature vector are independent. Recall that *independent*
    means the value of *x*[1], say, is in no way affected by the value of any other
    feature in the vector. This is typically not quite true, and for things like pixels
    in images *definitely* not true, but we’ll assume it’s true nonetheless. We’re
    naive to believe it’s true, hence the *Naive* in *Naive Bayes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the features are independent, then the probability of a feature taking on
    any particular value is independent of all the others. In that case, the product
    rule tells us that we can break the joint probability up like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*x*) = *P*(*x*[0])*P*(*x*[1])*P*(*x*[2]) . . . *P*(*x*[*n*–1])'
  prefs: []
  type: TYPE_NORMAL
- en: This helps tremendously. We have a dataset, labeled by class, allowing us to
    estimate the probability of any feature for any specific class by counting how
    often each feature value happens for each class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put it all together for a hypothetical dataset of three classes—0, 1,
    and 2—and four features. We first use the dataset, partitioned by class label,
    to estimate each feature value probability. This provides us the set of *P**(x*[0]),
    *P*(*x*[1]), and so on, for each feature for each class label. Combined with the
    prior probability of the class label, estimated from the dataset as the number
    of each class divided by the total number of samples in the dataset, we calculate
    for a new unknown feature vector, ***x***,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/064equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *P*(*x*[0]) feature probabilities are specific to class 0 only, and
    *P*(0) is the prior probability of class 0 in the dataset. *P*(0|***x***) is the
    unnormalized posterior probability that the unknown feature vector ***x*** belongs
    to class 0\. We say *unnormalized* because we’re ignoring the denominator of Bayes’
    theorem, knowing that including it would not change the ordering of the posterior
    probabilities, only their values.
  prefs: []
  type: TYPE_NORMAL
- en: We can repeat the calculation above to get *P*(1|***x***) and *P*(2|***x***),
    making sure to use the per feature probabilities calculated for those classes
    (the *P*(*x*[0])s). Finally, we give ***x*** the class label for the largest of
    the three posteriors calculated.
  prefs: []
  type: TYPE_NORMAL
- en: The description above assumes that the feature values are discrete. Usually
    they aren’t, but there are workarounds. One is to bin the feature values to make
    them discrete. For example, if the feature ranges over [0, 3], create a new feature
    that is 0, 1, or 2, and assign the continuous feature to one of those bins by
    truncating any fractional part.
  prefs: []
  type: TYPE_NORMAL
- en: Another workaround is to make one more assumption about the distribution the
    feature values come from and use that distribution to calculate the *P**(x*[0])s
    per class. Features are often based on measurements in the real world, and many
    things in the real world follow a normal distribution. Therefore, typically we’d
    assume that the individual features, while continuous, are normally distributed
    and we can find estimates of the mean (μ) and standard deviation (σ) from the
    dataset, per feature, and class label.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ theorem is useful for calculating probabilities. It’s helpful in machine
    learning as well. The battle between Bayesians and frequentists appears to be
    waning, though philosophical differences remain. In practice, most researchers
    are learning that both approaches are valuable, and at times tools from both camps
    should be used. We’ll continue this trend in the next chapter, where we’ll examine
    statistics from a frequentist viewpoint. We defend this decision by pointing out
    that the vast majority of published scientific results in the last century used
    statistics this way, which includes the deep learning community, at least when
    it’s presenting the results of experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter taught us about probability distributions, what they are, and how
    to draw samples from them, both discrete and continuous. We’ll encounter different
    distributions during our exploration of deep learning. We also discovered Bayes’
    theorem and saw how it lets us properly relate conditional probabilities. We saw
    how Bayes’ theorem allows us to evaluate the true likelihood of cancer given an
    imperfect medical test—a common situation. We also learned how to use Bayes’ theorem,
    along with some of the basic probability rules we learned in [Chapter 2](ch02.xhtml#ch02),
    to build a simple but often surprisingly effective classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move now into the world of statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](#ch03fn01a). Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash
    K. Mansinghka, “The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for
    Discrete Probability Distributions,” in AISTATS 2020: Proceedings of the 23rd
    International Conference on Artificial Intelligence and Statistics, *Proceedings
    of Machine Learning Research* 108, Palermo, Sicily, Italy, 2020.'
  prefs: []
  type: TYPE_NORMAL
