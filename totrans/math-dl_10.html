<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_243"/><strong><span class="big">10</span><br/>BACKPROPAGATION</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">Backpropagation is currently <em>the</em> core algorithm behind deep learning. Without it, we cannot train deep neural networks in a reasonable amount of time, if at all. Therefore, practitioners of deep learning need to understand what backpropagation is, what it brings to the training process, and how to implement it, at least for simple networks. For the purposes of this chapter, I’ll assume you have no knowledge of backpropagation.</p>&#13;
<p class="indent">We’ll begin the chapter by discussing what backpropagation is and what it isn’t. We’ll then work through the math for a trivial network. After that, we’ll introduce a matrix description of backpropagation suitable for building fully connected feedforward neural networks. We’ll explore the math and experiment with a NumPy-based implementation.</p>&#13;
<p class="indent">Deep learning toolkits like TensorFlow don’t implement backpropagation the way we will in the first two sections of this chapter. Instead, they use computational graphs, which we’ll discuss at a high level to conclude the chapter.</p>&#13;
<h3 class="h3" id="ch10lev1_1"><span epub:type="pagebreak" id="page_244"/>What Is Backpropagation?</h3>&#13;
<p class="noindent">In <a href="ch07.xhtml#ch07">Chapter 7</a>, we introduced the idea of the gradient of a scalar function of a vector. We worked with gradients again in <a href="ch08.xhtml#ch08">Chapter 8</a> and saw their connection to the Jacobian matrix. Recall in that chapter, we discussed how training a neural network is essentially an optimization problem. We know training a neural network involves a loss function, a function of the network’s weights and biases that tells us how well the network performs on the training set. When we do gradient descent, we’ll use the gradient to decide how to move from one part of the loss landscape to another to find where the network performs best. The goal of training is to minimize the loss function over the training set.</p>&#13;
<p class="indent">That’s the high-level picture. Now let’s make it a little more concrete. Gradients apply to functions that accept vector inputs and return a scalar value. For a neural network, the vector input is the weights and biases, the parameters that define how the network performs once the architecture is fixed. Symbolically, we can write the loss function as <em>L</em>(<strong>θ</strong>), where <strong>θ</strong> (theta) is a vector of all the weights and biases in the network. Our goal is to move through the space that the loss function defines to find the minimum, the specific <strong>θ</strong> leading to the smallest loss, <em>L</em>. We do this by using the gradient of <em>L</em>(<strong>θ</strong>). Therefore, to train a neural network via gradient descent, we need to know how each weight and bias value contributes to the loss function; that is, we need to know ∂<em>L</em>/∂<em>w</em>, for some weight (or bias) <em>w</em>.</p>&#13;
<p class="indent">Backpropagation is the algorithm that tells us what ∂<em>L</em>/∂<em>w</em> is for each weight and bias of the network. With the partial derivatives, we can apply gradient descent to improve the network’s performance on the next pass of the training data.</p>&#13;
<p class="indent">Before we go any further, a word on terminology. You’ll often hear machine learning folks use <em>backpropagation</em> as a proxy for the entire process of training a neural network. Experienced practitioners understand what they mean, but people new to machine learning are sometimes a bit confused. To be explicit, <em>backpropagation</em> is the algorithm that finds the contribution of each weight and bias value to the network’s error, the ∂<em>L</em>/∂<em>w</em>’s. <em>Gradient descent</em> is the algorithm that uses the ∂<em>L</em>/∂<em>w</em>’s to modify the weights and biases to improve the network’s performance on the training set.</p>&#13;
<p class="indent">Rumelhart, Hinton, and Williams introduced backpropagation in their 1986 paper “Learning Representations by Back-propagating Errors.” Ultimately, backpropagation is an application of the chain rule we discussed in <a href="ch07.xhtml#ch07">Chapters 7</a> and <a href="ch08.xhtml#ch08">8</a>. Backpropagation begins at the network’s output with the loss function. It moves <em>backward</em>, hence the name “backpropagation,” to ever-lower layers of the network, propagating the error signal to find ∂<em>L</em>/∂<em>w</em> for each weight and bias. Note, practitioners frequently shorten the name to “backprop.” You’ll encounter that term often.</p>&#13;
<p class="indent">We’ll work through backpropagation by example in the following two sections. For now, the primary thing to understand is that it is the first of two pieces we need to train neural networks. It provides the information required by the second piece, gradient descent, the subject of <a href="ch11.xhtml#ch11">Chapter 11</a>.</p>&#13;
<h3 class="h3" id="ch10lev1_2"><span epub:type="pagebreak" id="page_245"/>Backpropagation by Hand</h3>&#13;
<p class="noindent">Let’s define a simple neural network, one that accepts two input values, has two nodes in its hidden layer, and has a single output node, as shown in <a href="ch10.xhtml#ch10fig01">Figure 10-1</a>.</p>&#13;
<div class="image" id="ch10fig01"><img src="Images/10fig01.jpg" alt="image" width="316" height="176"/></div>&#13;
<p class="figcap"><em>Figure 10-1: A simple neural network</em></p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig01">Figure 10-1</a> shows the network with its six weights, <em>w</em><sub>0</sub> through <em>w</em><sub>5</sub>, and three bias values, <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, and <em>b</em><sub>2</sub>. Each value is a scalar.</p>&#13;
<p class="indent">We’ll use sigmoid activation functions in the hidden layer,</p>&#13;
<div class="imagec"><img src="Images/245equ01.jpg" alt="Image" width="130" height="44"/></div>&#13;
<p class="noindent">and no activation function for the output node. To train the network, we’ll use a squared-error loss function,</p>&#13;
<div class="imagec"><img src="Images/245equ02.jpg" alt="Image" width="134" height="43"/></div>&#13;
<p class="noindent">where <em>y</em> is the label, zero or one, for a training example and <em>a</em><sub>2</sub> is the output of the network for the input associated with <em>y</em>, namely <em>x</em><sub>0</sub> and <em>x</em><sub>1</sub>.</p>&#13;
<p class="indent">Let’s write the equations for a forward pass with this network, a pass that moves left to right from the input, <em>x</em><sub>0</sub> and <em>x</em><sub>1</sub>, to the output, <em>a</em><sub>2</sub>. The equations are</p>&#13;
<div class="imagec" id="ch10equ01"><img src="Images/10equ01.jpg" alt="Image" width="447" height="269"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_246"/>Here, we’ve introduced intermediate values <em>z</em><sub>0</sub> and <em>z</em><sub>1</sub> to be the arguments to the activation functions. Notice that <em>a</em><sub>2</sub> has no activation function. We could have used a sigmoid here as well, but as our labels are either 0 or 1, we’ll learn a good output value regardless.</p>&#13;
<p class="indent">If we pass a single training example through the network, the output is <em>a</em><sub>2</sub>. If the label associated with the training example, <em><strong>x</strong></em> = (<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>), is <em>y</em>, the squared-error loss is as indicated in <a href="ch10.xhtml#ch10fig01">Figure 10-1</a>.</p>&#13;
<p class="indent">The argument to the loss function is <em>a</em><sub>2</sub>; <em>y</em> is a fixed constant. However, <em>a</em><sub>2</sub> depends directly on <em>w</em><sub>4</sub>, <em>w</em><sub>5</sub>, <em>b</em><sub>2</sub>, and the values of <em>a</em><sub>1</sub> and <em>a</em><sub>0</sub>, which themselves depend on <em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, <em>w</em><sub>3</sub>, <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, <em>x</em><sub>0</sub>, and <em>x</em><sub>1</sub>. Therefore, thinking in terms of the weights and biases, we could write the loss function as</p>&#13;
<p class="center"><em>L</em> = <em>L</em>(<em>w</em><sub>0</sub>, <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, <em>w</em><sub>3</sub>, <em>w</em><sub>4</sub>, <em>w</em><sub>5</sub>, <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>;<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, <em>y</em>) = <em>L</em>(<strong>θ</strong>; <em><strong>x</strong></em>, <em>y</em>)</p>&#13;
<p class="noindent">Here, <strong>θ</strong> represents the weights and biases; it’s considered the variable. The parts after the semicolon are constants in this case: the input vector <em><strong>x</strong></em> = (<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>) and the associated label, <em>y</em>.</p>&#13;
<p class="indent">We need the gradient of the loss function, ▽<em>L</em>(<strong>θ</strong>; <em><strong>x</strong></em>, <em>y</em>). To be explicit, we need all the partial derivatives, ∂<em>L</em>/∂<em>w</em><sub>5</sub>, ∂<em>L</em>/∂<em>b</em><sub>0</sub>, and so on, for all weights and biases: nine partial derivatives in total.</p>&#13;
<p class="indent">Here’s our plan of attack. First, we’ll work through the math to calculate expressions for the partial derivatives of all nine values. Second, we’ll write some Python code to implement the expressions so we can train the network of <a href="ch10.xhtml#ch10fig01">Figure 10-1</a> to classify iris flowers. We’ll learn a few things during this process. Perhaps the most important is that calculating the partial derivatives by hand is, to be understated, tedious. We’ll succeed, but we’ll see in the following section that, thankfully, we have a far more compact way we can represent backpropagation, especially for fully connected feedforward networks. Let’s get started.</p>&#13;
<h4 class="h4" id="ch10lev2_1">Calculating the Partial Derivatives</h4>&#13;
<p class="noindent">We need expressions for all the partial derivatives of the loss function for the network in <a href="ch10.xhtml#ch10fig01">Figure 10-1</a>. We also need an expression for the derivative of our activation function, the sigmoid. Let’s begin with the sigmoid, as a clever trick writes the derivative in terms of the sigmoid itself, a value calculated during the forward pass.</p>&#13;
<p class="indent">The derivative of the sigmoid is shown next.</p>&#13;
<span epub:type="pagebreak" id="page_247"/>&#13;
<div class="imagec" id="ch10equ02"><img src="Images/10equ02.jpg" alt="Image" width="571" height="389"/></div>&#13;
<div class="imagec" id="ch10equ03"><img src="Images/10equ03.jpg" alt="Image" width="370" height="107"/></div>&#13;
<p class="noindent">The trick of <a href="ch10.xhtml#ch10equ02">Equation 10.2</a> is to add and subtract one in the numerator to change the form of the factor to be another copy of the sigmoid itself. So, the derivative of the sigmoid is the product of the sigmoid and one minus the sigmoid. Looking back at <a href="ch10.xhtml#ch10equ01">Equation 10.1</a>, we see that the forward pass computes the sigmoids, the activation functions, as <em>a</em><sub>0</sub> and <em>a</em><sub>1</sub>. Therefore, during the derivation of the backpropagation partial derivatives, we’ll be able to substitute <em>a</em><sub>0</sub> and <em>a</em><sub>1</sub> via <a href="ch10.xhtml#ch10equ03">Equation 10.3</a> for the derivative of the sigmoid to avoid calculating it a second time.</p>&#13;
<p class="indent">Let’s start with the derivatives. True to backpropagation’s name, we’ll work backward from the loss function and apply the chain rule to arrive at the expressions we need. The derivative of the loss function,</p>&#13;
<div class="imagec"><img src="Images/247equ01.jpg" alt="Image" width="134" height="42"/></div>&#13;
<p class="noindent">is</p>&#13;
<div class="imagec" id="ch10equ04"><img src="Images/10equ04.jpg" alt="Image" width="511" height="49"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_248"/>This means that everywhere in the expressions that follow, we can replace ∂<em>L</em>/∂<em>a</em><sub>2</sub> with <em>a</em><sub>2</sub> − <em>y</em>. Recall <em>y</em> is the label for the current training example, and we compute <em>a</em><sub>2</sub> during the forward pass as the output of the network.</p>&#13;
<p class="indent">Let’s now find expressions for <em>w</em><sub>5</sub>, <em>w</em><sub>4</sub>, and <em>b</em><sub>2</sub>, the parameters used to calculate <em>a</em><sub>2</sub>. The chain rule tells us</p>&#13;
<div class="imagec" id="ch10equ05"><img src="Images/10equ05.jpg" alt="Image" width="507" height="51"/></div>&#13;
<p class="noindent">since</p>&#13;
<div class="imagec"><img src="Images/248equ01.jpg" alt="Image" width="299" height="50"/></div>&#13;
<p class="noindent">We’ve substituted in the expression for <em>a</em><sub>2</sub> from <a href="ch10.xhtml#ch10equ01">Equation 10.1</a>.</p>&#13;
<p class="indent">Similar logic leads to expressions for <em>w</em><sub>4</sub> and <em>b</em><sub>2</sub>:</p>&#13;
<div class="imagec" id="ch10equ06"><img src="Images/10equ06.jpg" alt="Image" width="558" height="139"/></div>&#13;
<p class="indent">Fantastic! We have three of the partial derivatives we need—only six more to go. Let’s write the expressions for <em>b</em><sub>1</sub>, <em>w</em><sub>1</sub>, and <em>w</em><sub>3</sub>,</p>&#13;
<div class="imagec" id="ch10equ07"><img src="Images/10equ07.jpg" alt="Image" width="661" height="229"/></div>&#13;
<p class="noindent">where we use</p>&#13;
<div class="imagec"><img src="Images/248equ02.jpg" alt="Image" width="404" height="49"/></div>&#13;
<p class="noindent">substituting <em>a</em><sub>1</sub> for σ(<em>z</em><sub>1</sub>) as we calculate <em>a</em><sub>1</sub> during the forward pass.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_249"/>A similar calculation gives us expressions for the final three partial derivatives:</p>&#13;
<div class="imagec" id="ch10equ08"><img src="Images/10equ08.jpg" alt="Image" width="661" height="229"/></div>&#13;
<p class="indent">Whew! That was tedious, but now we have what we need. Notice, however, that this is a very rigid process—if we change the network architecture, activation function, or loss function, we need to derive these expressions again. Let’s use the expressions to classify iris flowers.</p>&#13;
<h4 class="h4" id="ch10lev2_2">Translating into Python</h4>&#13;
<p class="noindent">The code I’ve presented here is in the file <em>nn_by_hand.py</em>. Take a look at it in an editor to see the overall structure. We’ll start with the <span class="literal">main</span> function (<a href="ch10.xhtml#ch10ex01">Listing 10-1</a>):</p>&#13;
<p class="programs"><span class="ent">❶</span> epochs = 1000<br/>&#13;
   eta = 0.1<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> xtrn, ytrn, xtst, ytst = BuildDataset()<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> net = {}<br/>&#13;
   net["b2"] = 0.0<br/>&#13;
   net["b1"] = 0.0<br/>&#13;
   net["b0"] = 0.0<br/>&#13;
   net["w5"] = 0.0001*(np.random.random() - 0.5)<br/>&#13;
   net["w4"] = 0.0001*(np.random.random() - 0.5)<br/>&#13;
   net["w3"] = 0.0001*(np.random.random() - 0.5)<br/>&#13;
   net["w2"] = 0.0001*(np.random.random() - 0.5)<br/>&#13;
   net["w1"] = 0.0001*(np.random.random() - 0.5)<br/>&#13;
   net["w0"] = 0.0001*(np.random.random() - 0.5)<br/>&#13;
<br/>&#13;
<span class="ent">❹</span> tn0,fp0,fn0,tp0,pred0 = Evaluate(net, xtst, ytst)<br/>&#13;
<br/>&#13;
<span class="ent">❺</span> net = GradientDescent(net, xtrn, ytrn, epochs, eta)<br/>&#13;
<br/>&#13;
<span class="ent">❻</span> tn,fp,fn,tp,pred = Evaluate(net, xtst, ytst)<br/>&#13;
<br/>&#13;
   print("Training for %d epochs, learning rate %0.5f" % (epochs, eta))<br/>&#13;
<span epub:type="pagebreak" id="page_250"/>&#13;
   print()<br/>&#13;
   print("Before training:")<br/>&#13;
   print("   TN:%3d FP:%3d" % (tn0, fp0))<br/>&#13;
   print("   FN:%3d TP:%3d" % (fn0, tp0))<br/>&#13;
   print()<br/>&#13;
   print("After training:")<br/>&#13;
   print("   TN:%3d FP:%3d" % (tn, fp))<br/>&#13;
   print("   FN:%3d TP:%3d" % (fn, tp))</p>&#13;
<p class="ex-caption" id="ch10ex01"><em>Listing 10-1: The</em> <span class="literal"><em>main</em></span> <em>function</em></p>&#13;
<p class="indent">First, we set the number of epochs and the learning rate, η (eta) <span class="ent">❶</span>. The number of epochs is the number of passes through the training set to update the network weights and biases. The network is straightforward, and our dataset tiny, with only 70 samples, so we need many epochs for training. Gradient descent uses the learning rate to decide how to move based on the gradient values. We’ll explore the learning rate more thoroughly in <a href="ch11.xhtml#ch11">Chapter 11</a>.</p>&#13;
<p class="indent">Next, we load the dataset <span class="ent">❷</span>. We’re using the same iris dataset we used in <a href="ch06.xhtml#ch06">Chapter 6</a> and again in <a href="ch09.xhtml#ch09">Chapter 9</a>, keeping only the first two features and classes 0 and 1. See the <span class="literal">BuildDataset</span> function in <em>nn_by_hand.py</em>. The return values are NumPy arrays: <span class="literal">xtrn</span> (70 × 2) and <span class="literal">xtst</span> (30 × 2) for training and test data, and the associated labels in <span class="literal">ytrn</span> and <span class="literal">ytst</span>.</p>&#13;
<p class="indent">We need someplace to store the network weights and biases. A Python dictionary will do, so we set it up next with default values <span class="ent">❸</span>. Notice that we set the bias values to zero and the weights to small random values in [−0.00005, +0.00005]. These seem to work well enough in this case.</p>&#13;
<p class="indent">The remainder of <span class="literal">main</span> evaluates the randomly initialized network (<span class="literal">Evaluate</span> <span class="ent">❹</span>) on the test data, performs gradient descent to train the model (<span class="literal">GradientDescent</span> <span class="ent">❺</span>), and evaluates the test data again to demonstrate that training worked <span class="ent">❻</span>.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10ex02">Listing 10-2</a> shows <span class="literal">Evaluate</span> as well as <span class="literal">Forward</span>, which <span class="literal">Evaluate</span> calls.</p>&#13;
<p class="programs">&#13;
def Evaluate(net, x, y):<br/>&#13;
    out = Forward(net, x)<br/>&#13;
    tn = fp = fn = tp = 0<br/>&#13;
    pred = []<br/>&#13;
    for i in range(len(y)):<br/>&#13;
     <span class="ent">❶</span> c = 0 if (out[i] &lt; 0.5) else 1<br/>&#13;
        pred.append(c)<br/>&#13;
        if (c == 0) and (y[i] == 0):<br/>&#13;
            tn += 1<br/>&#13;
        elif (c == 0) and (y[i] == 1):<br/>&#13;
            fn += 1<br/>&#13;
        elif (c == 1) and (y[i] == 0):<br/>&#13;
            fp += 1<br/>&#13;
        else:<br/>&#13;
            tp += 1<br/>&#13;
    return tn,fp,fn,tp,pred<br/>&#13;
<span epub:type="pagebreak" id="page_251"/>&#13;
def Forward(net, x):<br/>&#13;
    out = np.zeros(x.shape[0])<br/>&#13;
    for k in range(x.shape[0]):<br/>&#13;
     <span class="ent">❷</span> z0 = net["w0"]*x[k,0] + net["w2"]*x[k,1] + net["b0"]<br/>&#13;
        a0 = sigmoid(z0)<br/>&#13;
        z1 = net["w1"]*x[k,0] + net["w3"]*x[k,1] + net["b1"]<br/>&#13;
        a1 = sigmoid(z1)<br/>&#13;
        out[k] = net["w4"]*a0 + net["w5"]*a1 + net["b2"]<br/>&#13;
    return out</p>&#13;
<p class="ex-caption" id="ch10ex02"><em>Listing 10-2: The</em> <span class="literal"><em>Evaluate</em></span> <em>function</em></p>&#13;
<p class="indent">Let’s begin with <span class="literal">Forward</span>, which performs a forward pass over the data in <span class="literal">x</span>. After creating a place to hold the output of the network (<span class="literal">out</span>), each input is run through the network using the current value of the parameters <span class="ent">❷</span>. Notice that the code is a direct implementation of <a href="ch10.xhtml#ch10equ01">Equation 10.1</a>, with <span class="literal">out[k]</span> in place of <em>a</em><sub>2</sub>. When all inputs have been processed, we return the collected outputs to the caller.</p>&#13;
<p class="indent">Now let’s look at <span class="literal">Evaluate</span>. Its arguments are a set of input features, <span class="literal">x</span>, associated labels, <span class="literal">y</span>, and the network parameters, <span class="literal">net</span>. <span class="literal">Evaluate</span> first runs the data through the network by calling <span class="literal">Forward</span> to populate <span class="literal">out</span>. These are the raw, floating-point outputs from the network. To compare them with the actual labels, we apply a threshold <span class="ent">❶</span> to call outputs &lt; 0.5 class 0 and outputs ≥ 0.5 class 1. The predicted label is appended to <span class="literal">pred</span> and tallied by comparing it to the actual label in <span class="literal">y</span>.</p>&#13;
<p class="indent">If the actual and predicted labels are both zero, the model has correctly identified a <em>true negative</em> (<span class="literal">TN</span>), a true instance of class 0. If the network predicts class 0, but the actual label is class 1, we have a <em>false negative</em> (<span class="literal">FN</span>), a class 1 instance labeled class 0. Conversely, labeling a class 0 instance class 1 is a <em>false positive</em> (<span class="literal">FP</span>). The only remaining option is an actual class 1 instance labeled as class 1, a <em>true positive</em> (<span class="literal">TP</span>). Finally, we return the tallies and predictions to the caller.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10ex03">Listing 10-3</a> presents <span class="literal">GradientDescent</span>, which <a href="ch10.xhtml#ch10ex01">Listing 10-1</a> calls <span class="ent">❺</span>. This is where we implement the partial derivatives calculated above.</p>&#13;
<p class="programs">&#13;
def GradientDescent(net, x, y, epochs, eta):<br/>&#13;
 <span class="ent">❶</span> for e in range(epochs):<br/>&#13;
        dw0 = dw1 = dw2 = dw3 = dw4 = dw5 = db0 = db1 = db2 = 0.0<br/>&#13;
<br/>&#13;
     <span class="ent">❷</span> for k in range(len(y)):<br/>&#13;
         <span class="ent">❸</span> z0 = net["w0"]*x[k,0] + net["w2"]*x[k,1] + net["b0"]<br/>&#13;
            a0 = sigmoid(z0)<br/>&#13;
            z1 = net["w1"]*x[k,0] + net["w3"]*x[k,1] + net["b1"]<br/>&#13;
            a1 = sigmoid(z1)<br/>&#13;
            a2 = net["w4"]*a0 + net["w5"]*a1 + net["b2"]<br/>&#13;
<br/>&#13;
         <span class="ent">❹</span> db2 += a2 - y[k]<br/>&#13;
            dw4 += (a2 - y[k]) * a0<br/>&#13;
<span epub:type="pagebreak" id="page_252"/><br/>&#13;
            dw5 += (a2 - y[k]) * a1<br/>&#13;
            db1 += (a2 - y[k]) * net["w5"] * a1 * (1 - a1)<br/>&#13;
            dw1 += (a2 - y[k]) * net["w5"] * a1 * (1 - a1) * x[k,0]<br/>&#13;
            dw3 += (a2 - y[k]) * net["w5"] * a1 * (1 - a1) * x[k,1]<br/>&#13;
            db0 += (a2 - y[k]) * net["w4"] * a0 * (1 - a0)<br/>&#13;
            dw0 += (a2 - y[k]) * net["w4"] * a0 * (1 - a0) * x[k,0]<br/>&#13;
            dw2 += (a2 - y[k]) * net["w4"] * a0 * (1 - a0) * x[k,1]<br/>&#13;
<br/>&#13;
        m = len(y)<br/>&#13;
     <span class="ent">❺</span> net["b2"] = net["b2"] - eta * db2 / m<br/>&#13;
        net["w4"] = net["w4"] - eta * dw4 / m<br/>&#13;
        net["w5"] = net["w5"] - eta * dw5 / m<br/>&#13;
        net["b1"] = net["b1"] - eta * db1 / m<br/>&#13;
        net["w1"] = net["w1"] - eta * dw1 / m<br/>&#13;
        net["w3"] = net["w3"] - eta * dw3 / m<br/>&#13;
        net["b0"] = net["b0"] - eta * db0 / m<br/>&#13;
        net["w0"] = net["w0"] - eta * dw0 / m<br/>&#13;
        net["w2"] = net["w2"] - eta * dw2 / m<br/>&#13;
<br/>&#13;
     return net</p>&#13;
<p class="ex-caption" id="ch10ex03"><em>Listing 10-3: Using</em> <span class="literal"><em>GradientDescent</em></span> <em>to train the network</em></p>&#13;
<p class="indent">The <span class="literal">GradientDescent</span> function contains a double loop. The outer loop <span class="ent">❶</span> is over <span class="literal">epochs</span>, the number of full passes through the training set. The inner loop <span class="ent">❷</span> is over the training examples, one at a time. The forward pass comes first <span class="ent">❸</span> to calculate the output, <span class="literal">a2</span>, and intermediate values.</p>&#13;
<p class="indent">The next block of code implements the backward pass using the partial derivatives, <a href="ch10.xhtml#ch10equ04">Equations 10.4</a> through <a href="ch10.xhtml#ch10equ08">10.8</a>, to move the error (loss) backward through the network <span class="ent">❹</span>. We use the average loss over the training set to update the weights and biases. Therefore, we accumulate the contribution to the loss for each weight and bias value for each training example. This explains adding each new contribution to the total over the training set.</p>&#13;
<p class="indent">After passing each training example through the net and accumulating its contribution to the loss, we update the weights and biases <span class="ent">❺</span>. The partial derivatives give us the gradient, the direction of maximal change; however, we want to minimize, so we move in the direction <em>opposite</em> to the gradient, subtracting the average of the loss due to each weight and bias from its current value.</p>&#13;
<p class="indent">For example,</p>&#13;
<p class="programs">net["b2"] = net["b2"] - eta * db2 / m</p>&#13;
<p class="noindent">is</p>&#13;
<div class="imagec"><img src="Images/252equ01.jpg" alt="Image" width="250" height="63"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_253"/>where η = 0.1 is the learning rate and <em>m</em> is the number of samples in the training set. The summation is over the partial for <em>b</em><sub>2</sub> evaluated for each input sample, <em><strong>x</strong><sub>i</sub></em>, the average value of which, multiplied by the learning rate, is used to adjust <em>b</em><sub>2</sub> for the next epoch. Another name we frequently use for the learning rate is <em>step size</em>. This parameter controls how quickly the weights and biases of the network step through the loss landscape toward a minimum value.</p>&#13;
<p class="indent">Our implementation is complete. Let’s run it to see how well it does.</p>&#13;
<h4 class="h4" id="ch10lev2_3">Training and Testing the Model</h4>&#13;
<p class="noindent">Let’s take a look at the training data. We can plot the features, one on each axis, to see how easy it might be to separate the two classes. The result is <a href="ch10.xhtml#ch10fig02">Figure 10-2</a>, with class 0 as circles and class 1 as squares.</p>&#13;
<div class="image" id="ch10fig02"><img src="Images/10fig02.jpg" alt="image" width="694" height="521"/></div>&#13;
<p class="figcap"><em>Figure 10-2: The iris training data showing class 0 (circles) and class 1 (squares)</em></p>&#13;
<p class="indent">It’s straightforward to see that the two classes are quite separate from each other so that even our elementary network with two hidden neurons should be able to learn the difference between them. Compare this plot with the left side of <a href="ch06.xhtml#ch06fig02">Figure 6-2</a>, which shows the first two features for all three iris classes. If we had included class 2 in our dataset, two features would not be enough to separate all three classes.</p>&#13;
<p class="indent">Run the code with</p>&#13;
<p class="programs">python3 nn_by_hand.py</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_254"/>For me, this produces</p>&#13;
<p class="program1">Training for 1000 epochs, learning rate 0.10000<br/>&#13;
<br/>&#13;
Before training:<br/>&#13;
    TN: 15 FP: 0<br/>&#13;
    FN: 15 TP: 0<br/>&#13;
<br/>&#13;
After training:<br/>&#13;
    TN: 14 FP: 1<br/>&#13;
    FN: 1 TP: 14</p>&#13;
<p class="indent">We’re told training used 1,000 passes through the training set of 70 examples. This is the outer loop of <a href="ch10.xhtml#ch10ex03">Listing 10-3</a>. We’re then presented with two tables of numbers, characterizing the network before training and after. Let’s walk through these tables to understand the story they tell.</p>&#13;
<p class="indent">The tables are known by several names: <em>contingency tables</em>, <em>2</em> × <em>2 tables</em>, or <em>confusion matrices</em>. The term <em>confusion matrix</em> is the most general, though it’s usually reserved for multiclass classifiers. The labels count the number of true positives, true negatives, false positives, and false negatives in the test set. The test set includes 30 samples, 15 from each class. If the network is perfect, all class 0 samples will be in the TN count, and all class 1 in the TP count. Errors are FP or FN counts.</p>&#13;
<p class="indent">The randomly initialized network labels everything as class 0. We know this because there are 15 TN samples (those that are truly class 0) and 15 FN samples (15 class 1 samples that are labeled class 0). The overall accuracy before training is then 15/(15 + 15) = 0.5 = 50 percent.</p>&#13;
<p class="indent">After training, the 1,000 passes through the outer loop of the code in <a href="ch10.xhtml#ch10ex03">Listing 10-3</a>, the test data is almost perfectly classified, with 14 of the 15 class 0 and 14 of the 15 class 1 labels correctly assigned. The overall accuracy is now (14 + 14)/(15 + 15) = 28/30 = 93.3 percent—not too shabby considering our model has a single hidden layer of two nodes.</p>&#13;
<p class="indent">Again, this exercise’s main point is to see how tedious and potentially error-prone it is to calculate derivatives by hand. The code above works with scalars; it doesn’t process vectors or matrices to take advantage of any symmetry possible by using a better representation of the backpropagation algorithm. Thankfully, we can do better. Let’s look again at the backpropagation algorithm for fully connected networks and see if we can use vectors and matrices to arrive at a more elegant approach.</p>&#13;
<h3 class="h3" id="ch10lev1_3">Backpropagation for Fully Connected Networks</h3>&#13;
<p class="noindent">In this section, we’ll explore the equations that allow us to pass an error term backward from the output of the network to the input. Additionally, we’ll see how to use this error term to calculate the necessary partial derivatives of the weights and biases for a layer so we can implement gradient descent. With all the essential expressions on hand, we’ll implement Python <span epub:type="pagebreak" id="page_255"/>classes that will allow us to build and train fully connected feedforward neural networks of arbitrary depth and shape. We’ll conclude by testing the classes against the MNIST dataset.</p>&#13;
<h4 class="h4" id="ch10lev2_4">Backpropagating the Error</h4>&#13;
<p class="noindent">Let’s begin with a useful observation: the layers of a fully connected neural network can be thought of as vector functions:</p>&#13;
<p class="center"><em><strong>y</strong></em> = <em><strong>f</strong></em>(<em><strong>x</strong></em>)</p>&#13;
<p class="noindent">where the input to the layer is <em><strong>x</strong></em> and the output is <em><strong>y</strong></em>. The input, <em><strong>x</strong></em>, is either the actual input to the network for a training sample or, if working with one of the hidden layers of the model, the previous layer’s output. These are both vectors; each node in a layer produces a single scalar output, which, when grouped, becomes <em><strong>y</strong></em>, a vector representing the output of the layer.</p>&#13;
<p class="indent">The forward pass runs through the layers of the network in order, mapping <em><strong>x</strong><sub>i</sub></em> to <em><strong>y</strong><sub>i</sub></em> so that <em><strong>y</strong><sub>i</sub></em> becomes <em><strong>x</strong><sub>i</sub></em>+1, the input to layer <em>i</em> + 1. After all layers are processed, we use the final layer output, call it <em><strong>h</strong></em>, to calculate the loss, <em>L</em>(<em><strong>h</strong></em>, <em><strong>y</strong></em><sub>true</sub>). The loss is a measure of how wrong the network is for the input, <strong><em>x</em></strong>, that we determine by comparing it to the true label <em><strong>y</strong></em><sub>true</sub>. Note that if the model is multiclass, the output <em><strong>h</strong></em> is a vector, with one element for each possible class, and the true label is a vector of zeros, except for the index of the actual class label, which is one. This is why many toolkits, like Keras, map integer class labels to one-hot vectors.</p>&#13;
<p class="indent">We need to move the loss value, or the <em>error</em>, back through the network; this is the backpropagation step. To do this for a fully connected network using per-layer vectors and weight matrices, we need to first see how to run the forward pass. As we did for the network we built above, we’ll separate applying the activation function from the action of a fully connected layer.</p>&#13;
<p class="indent">For example, for any layer with the input vector <em><strong>x</strong></em> coming from the layer below, we need to calculate an output vector, <em><strong>y</strong></em>. For a fully connected layer, the forward pass is</p>&#13;
<p class="center"><em><strong>y</strong></em> = <em><strong>Wx</strong></em> + <em><strong>b</strong></em></p>&#13;
<p class="noindent">where <em><strong>W</strong></em> is a weight matrix, <em><strong>x</strong></em> is the input vector, and <em><strong>b</strong></em> is the bias vector.</p>&#13;
<p class="indent">For an activation layer, we have</p>&#13;
<p class="center"><em><strong>y</strong></em> = <strong>σ</strong>(<em><strong>x</strong></em>)</p>&#13;
<p class="noindent">for whatever activation function, <strong>σ</strong>, we choose. We’ll stick with the sigmoid for the remainder of this chapter. Note we made the function a vector-valued function. To do this, we apply the scalar sigmoid function to each element of the input vector to produce the output vector:</p>&#13;
<p class="center"><strong>σ</strong>(<em><strong>x</strong></em>) = [<em>σ</em>(<em>x</em><sub>0</sub>) <em>σ</em>(<em>x</em><sub>1</sub>) ... <em>σ</em>(<em>x<sub>n</sub></em><sub>−1</sub>)]<sup>⊤</sup></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_256"/>A fully connected network consists of a series of fully connected layers followed by activation layers. Therefore, the forward pass is a chain of operations that begins with the input to the model being given to the first layer to produce an output, which is then passed to the next layer’s input, and so on until all layers have been processed.</p>&#13;
<p class="indent">The forward pass leads to the final output and the loss. The derivative of the loss function with respect to the network output is the first error term. To pass the error term back down the model, we need to calculate how the error term changes with a change to the input of a layer using how the error changes with a change to the layer’s output. Specifically, for each layer, we need to know how to calculate</p>&#13;
<div class="imagec"><img src="Images/256equ01.jpg" alt="Image" width="26" height="45"/></div>&#13;
<p class="noindent">That is, we need to know how the error term changes with a change in the input to the layer given</p>&#13;
<div class="imagec"><img src="Images/256equ02.jpg" alt="Image" width="26" height="50"/></div>&#13;
<p class="noindent">which is how the error term changes with a change in the output of the layer. The chain rule tells us how to do it:</p>&#13;
<div class="imagec" id="ch10equ09"><img src="Images/10equ09.jpg" alt="Image" width="402" height="49"/></div>&#13;
<p class="noindent">where ∂<em>E</em>/∂<em><strong>x</strong></em> for layer <em>i</em> becomes ∂<em>E</em>/∂<em><strong>y</strong></em> for layer <em>i</em> − 1 as we move backward through the network.</p>&#13;
<p class="indent">Operationally, the backpropagation algorithm becomes</p>&#13;
<ol>&#13;
<li><p>Run a forward pass to map <em><strong>x</strong></em> → <em><strong>y</strong></em>, layer by layer, to get the final output, <em><strong>h</strong></em>.</p></li>&#13;
<li><p>Calculate the value of the derivative of the loss function using <em><strong>h</strong></em> and <em><strong>y</strong></em><sub>true</sub>; this becomes ∂<em>E</em>/∂<em><strong>y</strong></em> for the output layer.</p></li>&#13;
<li><p>Repeat for all earlier layers to calculate ∂<em>E</em>/∂<em><strong>x</strong></em> from ∂<em>E</em>/∂<em><strong>y</strong></em>, causing ∂<em>E</em>/∂<em><strong>x</strong></em> for layer <em>i</em> to become ∂<em>E</em>/∂<em><strong>y</strong></em> for layer <em>i</em> − 1.</p></li>&#13;
</ol>&#13;
<p class="indent">This algorithm passes the error term backward through the network. Let’s work out how to get the necessary partial derivatives by layer type, beginning with the activation layer.</p>&#13;
<p class="indent">We will assume we know ∂<em>E</em>/∂<em><strong>y</strong></em> and are looking for ∂<em>E</em>/∂<em><strong>x</strong></em>. The chain rule says</p>&#13;
<span epub:type="pagebreak" id="page_257"/>&#13;
<div class="imagec" id="ch10equ10"><img src="Images/10equ10.jpg" alt="Image" width="498" height="329"/></div>&#13;
<p class="noindent">Here, we’re introducing ⊙ to represent the Hadamard product. Recall that the Hadamard product is the element-wise multiplication of two vectors or matrices. (See <a href="ch05.xhtml#ch05">Chapter 5</a> for a refresher.)</p>&#13;
<p class="indent">We now know how to pass the error term through an activation layer. The only other layer we’re considering is a fully connected layer. If we expand <a href="ch10.xhtml#ch10equ09">Equation 10.9</a>, we get</p>&#13;
<div class="imagec" id="ch10equ11"><img src="Images/10equ11.jpg" alt="Image" width="404" height="138"/></div>&#13;
<p class="noindent">since</p>&#13;
<div class="imagec"><img src="Images/257equ01.jpg" alt="Image" width="425" height="45"/></div>&#13;
<p class="indent">The result is <em><strong>W</strong></em><sup>⊤</sup>, not <em><strong>W</strong></em>, because the derivative of a matrix times a vector in denominator notation is the transpose of the matrix rather than the matrix itself.</p>&#13;
<p class="indent">Let us pause for a bit to recap and think about the form of <a href="ch10.xhtml#ch10equ10">Equations 10.10</a> and <a href="ch10.xhtml#ch10equ11">10.11</a>. These equations tell us how to pass the error term backward from layer to layer. What are the shapes of these values? For the activation layer, if the input has <em>k</em>-elements, then the output also has <em>k-</em>elements. Therefore, the relationship in <a href="ch10.xhtml#ch10equ10">Equation 10.10</a> should map a <em>k-</em>element vector to another <em>k</em>-element vector. The error term, ∂<em>E</em>/∂<em><strong>y</strong></em>, is a <em>k</em>-element vector, as is the derivative of the activation function, σ′(<em><strong>x</strong></em>). Finally, the Hadamard product between the two also outputs a <em>k</em>-element vector, as needed.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_258"/>For the fully connected layer, we have an <em>m</em>-element input, <em><strong>x</strong></em>; an <em>n</em> × <em>m</em>-element weight matrix, <em><strong>W</strong></em>; and an output vector, <em><strong>y</strong></em>, of <em>n</em>-elements. So we need to generate an <em>m</em>-element vector, ∂<em>E</em>/∂<em><strong>x</strong></em>, from the <em>n</em>-element error term, ∂<em>E</em>/∂<em><strong>y</strong></em>. Multiplying the transpose of the weight matrix, an <em>m</em> × <em>n-</em>element matrix, by the error term does result in an <em>m</em>-element vector, since <em>m</em> × <em>n</em> by <em>n</em> × 1 is <em>m</em> × 1, an <em>m</em>-element column vector.</p>&#13;
<h4 class="h4" id="ch10lev2_5">Calculating Partial Derivatives of the Weights and Biases</h4>&#13;
<p class="noindent"><a href="ch10.xhtml#ch10equ10">Equations 10.10</a> and <a href="ch10.xhtml#ch10equ11">10.11</a> tell us how to pass the error term backward through the network. However, the point of backpropagation is to calculate how changes in the weights and biases affect the error so we can use gradient descent. Specifically, for every fully connected layer, we need expressions for</p>&#13;
<div class="imagec"><img src="Images/258equ01.jpg" alt="Image" width="108" height="44"/></div>&#13;
<p class="noindent">given</p>&#13;
<div class="imagec"><img src="Images/258equ02.jpg" alt="Image" width="102" height="49"/></div>&#13;
<p class="indent">Let’s start with ∂<em>E</em>/∂<em><strong>b</strong></em>. Applying the chain rule yet again gives</p>&#13;
<div class="imagec" id="ch10equ12"><img src="Images/10equ12.jpg" alt="Image" width="437" height="316"/></div>&#13;
<p class="noindent">meaning the error due to the bias term for a fully connected layer is the same as the error due to the output.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_259"/>The calculation for the weight matrix is similar:</p>&#13;
<div class="imagec" id="ch10equ13"><img src="Images/10equ13.jpg" alt="Image" width="440" height="315"/></div>&#13;
<p class="noindent">The equation above tells us the error due to the weight matrix is a product of the output error and the input, <em><strong>x</strong></em>. The weight matrix is an <em>n</em> × <em>m-</em>element matrix, as the forward pass multiplies by the <em>m</em>-element input vector. Therefore, the error contribution from the weights, ∂<em>E</em>/∂<em><strong>W</strong></em>, also must be an <em>n</em> × <em>m</em> matrix. We know ∂<em>E</em>/∂<em><strong>y</strong></em> is an <em>n</em>-element column vector, and the transpose of <em><strong>x</strong></em> is an <em>m</em>-element row vector. The outer product of the two is an <em>n</em> × <em>m</em> matrix, as required.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10equ10">Equations 10.10</a>, <a href="ch10.xhtml#ch10equ11">10.11</a>, <a href="ch10.xhtml#ch10equ12">10.12</a>, and <a href="ch10.xhtml#ch10equ13">10.13</a> apply for a single training example. This means for a specific input to the network, these equations, especially 10.12 and 10.13, tell us the contribution to the loss by the biases and weights of any layer <em>for that input sample</em>.</p>&#13;
<p class="indent">To implement gradient descent, we need to accumulate these errors, the ∂<em>E</em>/∂<em><strong>W</strong></em> and ∂<em>E</em>/∂<em><strong>b</strong></em> terms, over the training samples. We then use the average value of these errors to update the weights and biases at the end of every epoch or, as we’ll implement it, minibatch. As gradient descent is the subject of <a href="ch11.xhtml#ch11">Chapter 11</a>, all we’ll do here is outline how we use backpropagation to implement gradient descent and leave the details to that chapter and the code we’ll implement next.</p>&#13;
<p class="indent">In general, however, to train the network, we need to do the following for each sample in the minibatch:</p>&#13;
<ol>&#13;
<li><p>Forward pass the sample through the network to create the output. Along the way, we need to store the input to each layer, as we need it to implement backpropagation (that is, we need <em><strong>x</strong></em><sup>⊤</sup> from <a href="ch10.xhtml#ch10equ13">Equation 10.13</a>).</p></li>&#13;
<li><p>Calculate the value of the derivative of the loss function, which for us is the mean squared error, to use as the first error term in back-propagation.</p></li>&#13;
<li><p>Run through the layers of the network in reverse order, calculating ∂<em>E</em>/∂<em><strong>W</strong></em> and ∂<em>E</em>/∂<em><strong>b</strong></em> for each fully connected layer. These values are accumulated for each sample in the minibatch (<strong>Δ<em>W</em></strong>, <strong>Δ<em>b</em></strong>).</p></li>&#13;
</ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_260"/>When the minibatch samples have been processed and the errors accumulated, it’s time to take a gradient descent step. This is where the weights and biases of each layer are updated via</p>&#13;
<div class="imagec" id="ch10equ14"><img src="Images/10equ14.jpg" alt="Image" width="442" height="129"/></div>&#13;
<p class="noindent">with <strong>Δ<em>W</em></strong> and <strong>Δ<em>b</em></strong> being the accumulated errors over the minibatch and <em>m</em> being the size of the minibatch. Repeated gradient descent steps lead to a final set of weights and biases—a trained network.</p>&#13;
<p class="indent">This section is quite math-heavy. The following section translates the math into code, where we’ll see that for all the math, the code, because of NumPy and object-oriented design, is quite compact and elegant. If you’re fuzzy on the math, I suspect the code will go a long way toward clarifying things for you.</p>&#13;
<h4 class="h4" id="ch10lev2_6">A Python Implementation</h4>&#13;
<p class="noindent">Our implementation is in the style of toolkits like Keras. We want the ability to create arbitrary, fully connected networks, so we’ll use Python classes for each layer and store the architecture as a list of layers. Each layer maintains its weights and biases, along with the ability to do a forward pass, a backward pass, and a gradient descent step. For simplicity, we’ll use sigmoid activations and the squared error loss.</p>&#13;
<p class="indent">We need two classes: <span class="literal">ActivationLayer</span> and <span class="literal">FullyConnectedLayer</span>. An additional <span class="literal">Network</span> class holds the pieces together and handles training. The classes are in the file <em>NN.py</em>. (The code here is modified from the original code by Omar Aflak and is used with his permission. See the GitHub link in <em>NN.py</em>. I modified the code to use minibatches and support gradient descent steps other than for every sample.)</p>&#13;
<p class="indent">Let’s walk through each of the three classes, starting with <span class="literal">ActivationLayer</span> (see <a href="ch10.xhtml#ch10ex04">Listing 10-4</a>). The translation of the math we’ve done to code form is quite elegant, in most cases a single line of NumPy.</p>&#13;
<p class="programs">&#13;
class ActivationLayer:<br/>&#13;
    def forward(self, input_data):<br/>&#13;
        self.input = input_data<br/>&#13;
        return sigmoid(input_data)<br/>&#13;
    def backward(self, output_error):<br/>&#13;
        return sigmoid_prime(self.input) * output_error<br/>&#13;
    def step(self, eta):<br/>&#13;
        return</p>&#13;
<p class="ex-caption" id="ch10ex04"><em>Listing 10-4: The</em> <span class="literal"><em>ActivationLayer</em></span> <em>class</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_261"/><a href="ch10.xhtml#ch10ex04">Listing 10-4</a> shows <span class="literal">ActivationLayer</span> and includes only three methods: <span class="literal">forward</span>, <span class="literal">backward</span>, and <span class="literal">step</span>. The simplest is <span class="literal">step</span>. It does nothing, as there’s nothing for an activation layer to do during gradient descent because there are no weights or bias values.</p>&#13;
<p class="indent">The <span class="literal">forward</span> method accepts the input vector, <em><strong>x</strong></em>, stores it for later use, and then calculates the output vector, <em><strong>y</strong></em>, by applying the sigmoid activation function.</p>&#13;
<p class="indent">The <span class="literal">backward</span> method accepts ∂<em>E</em>/∂<em><strong>y</strong></em>, the <span class="literal">output_error</span> from the layer above. It then returns <a href="ch10.xhtml#ch10equ10">Equation 10.10</a> by applying the derivative of the sigmoid (<span class="literal">sigmoid_prime</span>) to the input set during the forward pass, multiplied element-wise by the error.</p>&#13;
<p class="indent">The <span class="literal">sigmoid</span> and <span class="literal">sigmoid_prime</span> helper functions are</p>&#13;
<p class="programs">&#13;
def sigmoid(x):<br/>&#13;
    return 1.0 / (1.0 + np.exp(-x))<br/>&#13;
def sigmoid_prime(x):<br/>&#13;
    return sigmoid(x)*(1.0 - sigmoid(x))</p>&#13;
<p class="indent">The <span class="literal">FullyConnectedLayer</span> class is next. It’s more complex than the <span class="literal">ActivationLayer</span> class, but not significantly so. See <a href="ch10.xhtml#ch10ex05">Listing 10-5</a>.</p>&#13;
<p class="programs">&#13;
class FullyConnectedLayer:<br/>&#13;
    def __init__(self, input_size, output_size):<br/>&#13;
     <span class="ent">❶</span> self.delta_w = np.zeros((input_size, output_size))<br/>&#13;
        self.delta_b = np.zeros((1,output_size))<br/>&#13;
        self.passes = 0<br/>&#13;
     <span class="ent">❷</span> self.weights = np.random.rand(input_size, output_size) - 0.5<br/>&#13;
        self.bias = np.random.rand(1, output_size) - 0.5<br/>&#13;
<br/>&#13;
    def forward(self, input_data):<br/>&#13;
        self.input = input_data<br/>&#13;
     <span class="ent">❸</span> return np.dot(self.input, self.weights) + self.bias<br/>&#13;
<br/>&#13;
    def backward(self, output_error):<br/>&#13;
        input_error = np.dot(output_error, self.weights.T)<br/>&#13;
        weights_error = np.dot(self.input.T, output_error)<br/>&#13;
        self.delta_w += np.dot(self.input.T, output_error)<br/>&#13;
        self.delta_b += output_error<br/>&#13;
        self.passes += 1<br/>&#13;
        return input_error<br/>&#13;
<br/>&#13;
    def step(self, eta):<br/>&#13;
     <span class="ent">❹</span> self.weights -= eta * self.delta_w / self.passes<br/>&#13;
        self.bias -= eta * self.delta_b / self.passes<br/>&#13;
     <span class="ent">❺</span> self.delta_w = np.zeros(self.weights.shape)<br/>&#13;
        self.delta_b = np.zeros(self.bias.shape)<br/>&#13;
        self.passes = 0</p>&#13;
<p class="ex-caption" id="ch10ex05"><em>Listing 10-5: The</em> <span class="literal"><em>FullyConnectedLayer</em></span> <em>class</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_262"/>We tell the constructor the number of input and output nodes. The number of input nodes (<span class="literal">input_size</span>) specifies the number of elements in the vector coming into the layer. Likewise, <span class="literal">output_size</span> specifies the number of elements in the output vector.</p>&#13;
<p class="indent">Fully connected layers accumulate weight and bias errors over the minibatch, the ∂<em>E</em>/∂<em><strong>W</strong></em> terms in <span class="literal">delta_w</span> and the ∂<em>E</em>/∂<em><strong>b</strong></em> terms in <span class="literal">delta_b</span> <span class="ent">❶</span>. Each sample processed is counted in <span class="literal">passes</span>.</p>&#13;
<p class="indent">We must initialize neural networks with random weight and bias values; therefore, the constructor sets up an initial weight matrix and bias vector using uniform random values in the range [−0.5, 0.5] <span class="ent">❷</span>. Notice, the bias vector is 1 × <em>n</em>, a row vector. The code flips the ordering from the equations above to match the way training samples are usually stored: a matrix in which each row is a sample and each column a feature. The computation produces the same results because scalar multiplication is commutative: <em>ab</em> = <em>ba</em>.</p>&#13;
<p class="indent">The <span class="literal">forward</span> method stashes the input vector for later use by <span class="literal">backward</span> and then calculates the output of the layer, multiplying the input by the weight matrix and adding the bias term <span class="ent">❸</span>.</p>&#13;
<p class="indent">Only two methods remain. The <span class="literal">backward</span> method receives ∂<em>E</em>/∂<em><strong>y</strong></em> (<span class="literal">output_error</span>) and calculates ∂<em>E</em>/∂<em><strong>x</strong></em> (<span class="literal">input_error</span>), ∂<em>E</em>/∂<em><strong>W</strong></em> (<span class="literal">weights_error</span>), and ∂<em>E</em>/∂<em><strong>b</strong></em> (<span class="literal">output_error</span>). We add the errors to the running error total for the layer, <span class="literal">delta_w</span> and <span class="literal">delta_b</span>, for <span class="literal">step</span> to use.</p>&#13;
<p class="indent">The <span class="literal">step</span> method includes a gradient descent step for a fully connected layer. Unlike the empty method of <span class="literal">ActivationLayer</span>, the <span class="literal">FullyConnectedLayer</span> has plenty to do. We update the weight matrix and bias vector using the average error, as in <a href="ch10.xhtml#ch10equ14">Equation 10.14</a> <span class="ent">❹</span>. This implements the gradient descent step over the minibatch. Finally, we reset the accumulators and counter for the next minibatch <span class="ent">❺</span>.</p>&#13;
<p class="indent">The <span class="literal">Network</span> class brings everything together, as shown in <a href="ch10.xhtml#ch10ex06">Listing 10-6</a>.</p>&#13;
<p class="programs">&#13;
class Network:<br/>&#13;
    def __init__(self, verbose=True):<br/>&#13;
        self.verbose = verbose<br/>&#13;
      <span class="ent">❶</span> self.layers = []<br/>&#13;
<br/>&#13;
    def add(self, layer):<br/>&#13;
      <span class="ent">❷</span> self.layers.append(layer)<br/>&#13;
<br/>&#13;
    def predict(self, input_data):<br/>&#13;
        result = []<br/>&#13;
        for i in range(input_data.shape[0]):<br/>&#13;
            output = input_data[i]<br/>&#13;
            for layer in self.layers:<br/>&#13;
                output = layer.forward(output)<br/>&#13;
            result.append(output)<br/>&#13;
      <span class="ent">❸</span> return result<br/>&#13;
<br/>&#13;
     def fit(self, x_train, y_train, minibatches, learning_rate, batch_size=64):<br/>&#13;
      <span class="ent">❹</span> for i in range(minibatches):<br/>&#13;
<span epub:type="pagebreak" id="page_263"/>&#13;
        err = 0<br/>&#13;
        idx = np.argsort(np.random.random(x_train.shape[0]))[:batch_size]<br/>&#13;
        x_batch = x_train[idx]<br/>&#13;
        y_batch = y_train[idx]<br/>&#13;
      <span class="ent">❺</span> for j in range(batch_size):<br/>&#13;
            output = x_batch[j]<br/>&#13;
            for layer in self.layers:<br/>&#13;
                output = layer.forward(output)<br/>&#13;
<br/>&#13;
          <span class="ent">❻</span> err += mse(y_batch[j], output)<br/>&#13;
<br/>&#13;
          <span class="ent">❼</span> error = mse_prime(y_batch[j], output)<br/>&#13;
             for layer in reversed(self.layers):<br/>&#13;
                 error = layer.backward(error)<br/>&#13;
<br/>&#13;
      <span class="ent">❽</span> for layer in self.layers:<br/>&#13;
             layer.step(learning_rate)<br/>&#13;
         if (self.verbose) and ((i%10) == 0):<br/>&#13;
              err /= batch_size<br/>&#13;
              print('minibatch %5d/%d error=%0.9f' % (i, minibatches, err))</p>&#13;
<p class="ex-caption" id="ch10ex06"><em>Listing 10-6: The</em> <span class="literal"><em>Network</em></span> <em>class</em></p>&#13;
<p class="indent">The constructor for the <span class="literal">Network</span> class is straightforward. We set a <span class="literal">verbose</span> flag to toggle displaying the mean error over the minibatch during training. Successful training should show this error decreasing over time. As layers are added to the network, they are stored in <span class="literal">layers</span>, which the constructor initializes <span class="ent">❶</span>. The <span class="literal">add</span> method adds layer objects to the network by appending them to <span class="literal">layers</span> <span class="ent">❷</span>.</p>&#13;
<p class="indent">After the network is trained, the <span class="literal">predict</span> method generates output for each input sample in <span class="literal">input_data</span> with a forward pass through the layers of the network. Notice the pattern: the input sample is assigned to <span class="literal">output</span>; then the loop over <span class="literal">layers</span> calls the <span class="literal">forward</span> method of each layer, in turn passing the output of the previous layer as input to the next; and so on through the entire network. When the loop ends, <span class="literal">output</span> contains the output of the final layer, so it’s appended to <span class="literal">result</span>, which is returned to the caller <span class="ent">❸</span>.</p>&#13;
<p class="indent">Training the network is <span class="literal">fit</span>’s job. The name matches the standard training method for <span class="literal">sklearn</span>. The arguments are the NumPy array of sample vectors, one per row (<span class="literal">x_train</span>), and their labels as one-hot vectors (<span class="literal">y_train</span>). The number of minibatches to train comes next. We’ll discuss minibatches in a bit. We also provide the learning rate, η (eta), and an optional minibatch size, <span class="literal">batch_size</span>.</p>&#13;
<p class="indent">The <span class="literal">fit</span> method uses a double loop. The first is over the desired number of minibatches <span class="ent">❹</span>. As we learned earlier, a minibatch is a subset of the full training set, and an epoch is one full pass through the training set. Using the entire training set is known as <em>batch training</em>, and batch training uses epochs. However, there is good reason not to do batch training, as you’ll see in <a href="ch11.xhtml#ch11">Chapter 11</a>, so the concept of a <em>minibatch</em> was introduced. The typical <span epub:type="pagebreak" id="page_264"/>minibatch sizes are anywhere from 16 to 128 samples at a time. Powers of two are often used to make things nice for GPU-based deep learning toolkits. For us, there’s no difference between a minibatch of 64 or 63 samples in terms of performance.</p>&#13;
<p class="indent">We select most minibatches as sequential sets of the training data to ensure all the data is used. Here, we’re being a bit lazy and instead select random subsets each time we need a minibatch. This simplifies the code and adds one more place where randomness can show its utility. That’s what <span class="literal">idx</span> gives us, a random ordering of indices into the training set, keeping only the first <span class="literal">batch_size</span> worth. We then use <span class="literal">x_batch</span> and <span class="literal">y_batch</span> for the actual forward and backward passes.</p>&#13;
<p class="indent">The second loop is over the samples in the minibatch <span class="ent">❺</span>. Samples are passed individually through the layers of the network, calling <span class="literal">forward</span> just as <span class="literal">predict</span> does. For display purposes, the actual mean squared error between the forward pass output and the sample label is accumulated for the minibatch <span class="ent">❻</span>.</p>&#13;
<p class="indent">The backward pass begins with the output error term, the derivative of the loss function, <span class="literal">mse_prime</span> <span class="ent">❼</span>. The pass then continues <em>backward</em> through the layers of the network, passing the previous layer’s output error as input to the layer below, directly mirroring the forward pass process.</p>&#13;
<p class="indent">Once the loop processes all the minibatch samples <span class="ent">❺</span>, it’s time to take a gradient descent step based on the mean error each layer in the network accumulated over the samples <span class="ent">❽</span>. The argument to <span class="literal">step</span> needs only the learning rate. The minibatch concludes by reporting the average error if <span class="literal">verbose</span> is set for every 10th minibatch.</p>&#13;
<p class="indent">We’ll experiment with this code again in <a href="ch11.xhtml#ch11">Chapter 11</a> as we explore gradient descent. For now, let’s test it with the MNIST dataset to see how well it works.</p>&#13;
<h4 class="h4" id="ch10lev2_7">Using the Implementation</h4>&#13;
<p class="noindent">Let’s take <em>NN.py</em> for a spin. We’ll use it to build a classifier for the MNIST dataset, which we first encountered in <a href="ch09.xhtml#ch09">Chapter 9</a>. The original MNIST dataset consists of 28×28-pixel grayscale images of handwritten digits with black backgrounds. It’s a workhorse of the machine learning community. We’ll resize the images to 14×14 pixels before turning them into vectors of 196 elements (= 14 × 14).</p>&#13;
<p class="indent">The dataset includes 60,000 training images and 10,000 test images. The vectors are stored in NumPy arrays; see the files in the <em>dataset</em> directory. The code to generate the dataset is in <em>build_dataset.py</em>. If you want to run the code yourself, you’ll need to install Keras and OpenCV for Python first. Keras supplies the original set of images and maps the training set labels to one-hot vectors. OpenCV rescales the images from 28×28 to 14×14 pixels.</p>&#13;
<p class="indent">The code we need is in <em>mnist.py</em> and is shown in <a href="ch10.xhtml#ch10ex07">Listing 10-7</a>.</p>&#13;
<p class="programs">   import numpy as np<br/>&#13;
   from NN import *<br/>&#13;
<span epub:type="pagebreak" id="page_265"/>&#13;
<span class="ent">❶</span> x_train = np.load("dataset/train_images_small.npy")<br/>&#13;
   x_test = np.load("dataset/test_images_small.npy")<br/>&#13;
   y_train = np.load("dataset/train_labels_vector.npy")<br/>&#13;
   y_test = np.load("dataset/test_labels.npy")<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> x_train = x_train.reshape(x_train.shape[0], 1, 14*14)<br/>&#13;
   x_train /= 255<br/>&#13;
   x_test = x_test.reshape(x_test.shape[0], 1, 14*14)<br/>&#13;
   x_test /= 255<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> net = Network()<br/>&#13;
   net.add(FullyConnectedLayer(14*14, 100))<br/>&#13;
   net.add(ActivationLayer())<br/>&#13;
   net.add(FullyConnectedLayer(100, 50))<br/>&#13;
   net.add(ActivationLayer())<br/>&#13;
   net.add(FullyConnectedLayer(50, 10))<br/>&#13;
   net.add(ActivationLayer())<br/>&#13;
<br/>&#13;
<span class="ent">❹</span> net.fit(x_train, y_train, minibatches=40000, learning_rate=1.0)<br/>&#13;
<br/>&#13;
<span class="ent">❺</span> out = net.predict(x_test)<br/>&#13;
   cm = np.zeros((10,10), dtype="uint32")<br/>&#13;
   for i in range(len(y_test)):<br/>&#13;
       cm[y_test[i],np.argmax(out[i])] += 1<br/>&#13;
<br/>&#13;
   print()<br/>&#13;
   print(np.array2string(cm))<br/>&#13;
   print()<br/>&#13;
   print("accuracy = %0.7f" % (np.diag(cm).sum() / cm.sum(),))</p>&#13;
<p class="ex-caption" id="ch10ex07"><em>Listing 10-7: Classifying MNIST digits</em></p>&#13;
<p class="indent">Notice that we import <em>NN.py</em> right after NumPy. We load the training images, test images, and labels next <span class="ent">❶</span>. The <span class="literal">Network</span> class expects each sample vector to be a 1 × <em>n</em> row vector, so we reshape the training data from (60000,196) to (60000,1,196)—the same as the test data <span class="ent">❷</span>. At the same time, we scale the 8-bit data from [0, 255] to [0, 1]. This is a standard preprocessing step for image data, as doing so makes it easier for the network to learn.</p>&#13;
<p class="indent">Building the model comes next <span class="ent">❸</span>. First, we create an instance of the <span class="literal">Network</span> class. Then, we add the input layer by defining a <span class="literal">FullyConnectedLayer</span> with 196 inputs and 100 outputs. A sigmoid activation layer follows this. We then add a second fully connected layer mapping the 100 outputs of the first layer to 50 outputs, along with an activation layer. Finally, we add a last fully connected layer mapping the 50 outputs of the previous layer to 10, the number of classes, along with adding its activation layer. This approach mimics common toolkits like Keras.</p>&#13;
<p class="indent">Training happens by calling <span class="literal">fit</span> <span class="ent">❹</span>. We specify 40,000 minibatches using the default minibatch size of 64 samples. We set the learning rate to 1.0, <span epub:type="pagebreak" id="page_266"/>which works well in this instance. Training takes some 17 minutes on my old Intel i5 Ubuntu system. As the model trains, the mean error over the minibatch is reported. When training is complete, we pass the 10,000 test samples through the network and calculate a 10 × 10 confusion matrix <span class="ent">❺</span>. Recall that the rows of the confusion matrix are the true class labels, here the actual digits 0 through 9. The columns correspond to the predicted labels, the largest value of the 10 outputs for each input sample. The matrix elements are the counts of how often the true label was <em>i</em>, and the assigned label was <em>j</em>. If the model is perfect, the matrix is purely diagonal; there are no cases where the true label and model label disagree. The overall accuracy is printed last as the diagonal sum divided by the sum of the matrix, the total number of test samples.</p>&#13;
<p class="indent">My run of <em>mnist.py</em> produced</p>&#13;
<p class="program1">&#13;
minibatch 39940/40000  error=0.003941790<br/>&#13;
minibatch 39950/40000  error=0.001214253<br/>&#13;
minibatch 39960/40000  error=0.000832551<br/>&#13;
minibatch 39970/40000  error=0.000998448<br/>&#13;
minibatch 39980/40000  error=0.002377286<br/>&#13;
minibatch 39990/40000  error=0.000850956<br/>&#13;
<br/>&#13;
[[ 965    0    1   1   1   5   2   3   2    0]<br/>&#13;
 [   0 1121    3   2   0   1   3   0   5    0]<br/>&#13;
 [   6    0 1005   4   2   0   3   7   5    0]<br/>&#13;
 [   0    1    6 981   0   4   0   9   4    5]<br/>&#13;
 [   2    0    3   0 953   0   5   3   1   15]<br/>&#13;
 [   4    0    0  10   0 864   5   1   4    4]<br/>&#13;
 [   8    2    1   1   3   4 936   0   3    0]<br/>&#13;
 [   2    7   19   2   1   0   0 989   1    7]<br/>&#13;
 [   5    0    4   5   3   5   7   3 939    3]<br/>&#13;
 [   5    5    2  10   8   2   1   3   6 967]]<br/>&#13;
<br/>&#13;
accuracy = 0.9720000</p>&#13;
<p class="indent">The confusion matrix is strongly diagonal, and the overall accuracy is 97.2 percent. This isn’t too bad of a result for a simple toolkit like <em>NN.py</em> and a fully connected feedforward network. The largest error that the network made was confusing sevens for twos 19 times (element [7,2] of the confusion matrix). The next closest error was confusing fours for nines 15 times (element [4,9]). Both of these errors make sense: sevens and twos often look similar, as do fours and nines.</p>&#13;
<p class="indent">We started this chapter with a network we created that included two inputs, two nodes in the hidden layer, and an output. The file <em>iris.py</em> implements the same model by adapting the dataset to what <span class="literal">Network</span> expects. We won’t walk through the code, but do run it. When I do, I get slightly better performance on the test set: 14 out of 15 correct for class 0 and 15 out of 15 for class 1.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_267"/>Sadly, the backpropagation methods detailed here and in the previous section are not ultimately flexible enough for deep learning. Modern toolkits don’t use these approaches. Let’s explore what deep learning toolkits do when it comes to backpropagation.</p>&#13;
<h3 class="h3" id="ch10lev1_4">Computational Graphs</h3>&#13;
<p class="noindent">In computer science, a <em>graph</em> is a collection of nodes (vertices) and edges connecting them. We’ve been using graphs all along to represent neural networks. In this section, we’ll use graphs to represent expressions instead.</p>&#13;
<p class="indent">Consider this simple expression:</p>&#13;
<p class="center"><em>y</em> = <em>mx</em> + <em>b</em></p>&#13;
<p class="noindent">To evaluate this expression, we follow agreed-upon rules regarding operator precedence. Following the rules implies a sequence of primitive operations that we can represent as a graph, as shown in <a href="ch10.xhtml#ch10fig03">Figure 10-3</a>.</p>&#13;
<div class="image" id="ch10fig03"><img src="Images/10fig03.jpg" alt="image" width="163" height="80"/></div>&#13;
<p class="figcap"><em>Figure 10-3: A computational graph implementing</em> <span class="literal">y = mx + b</span></p>&#13;
<p class="indent">Data flows through the graph of <a href="ch10.xhtml#ch10fig03">Figure 10-3</a> along the arrows, from left to right. Data originates in <em>sources</em>, here <em>x</em>, <em>m</em>, and <em>b</em>, and flows through <em>operators</em>, * and +, to the output, <em>y</em>.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig03">Figure 10-3</a> is a <em>computational graph</em>—a graph specifying how to evaluate an expression. Compilers for languages like C generate computational graphs in some form to translate high-level expressions into sequences of machine language instructions. For the expression above, first the <em>x</em> and <em>m</em> values are multiplied, and the resulting output of the multiplication operation is passed to an addition operation, along with <em>b</em>, to produce the final output, <em>y</em>.</p>&#13;
<p class="indent">We can represent expressions, including those representing complex deep neural networks, as computational graphs. We represented fully connected feedforward models this way, as data flowing from the input, <em><strong>x</strong></em>, through the hidden layers to the output, the loss function.</p>&#13;
<p class="indent">Computational graphs are how deep learning toolkits like TensorFlow and PyTorch manage the structure of a model and implement backpropagation. Unlike the rigid calculations earlier in the chapter, a computational graph is generic and capable of representing all the architectures used in deep learning.</p>&#13;
<p class="indent">As you peruse the deep learning literature and begin to work with specific toolkits, you will run across two different approaches to using <span epub:type="pagebreak" id="page_268"/>computational graphs. The first generates the graph dynamically when data is available. PyTorch uses this method, called <em>symbol-to-number</em>. TensorFlow uses the second method, <em>symbol-to-symbol</em>, to build a static computational graph ahead of time. Both approaches implement graphs, and both can automatically calculate the derivatives needed for backpropagation.</p>&#13;
<p class="indent">TensorFlow generates the derivatives it needs for backpropagation in much the same way we did in the previous section. Like addition, each operation knows how to create the derivative of its outputs with respect to its inputs. That, along with the chain rule, is all that’s needed to implement backpropagation. Exactly how the graph is traversed depends on the <em>graph evaluation engine</em> and the specific model architecture, but the graph is traversed as needed for both the forward and backward passes. Note that because the computational graph breaks expressions into smaller operations, each of which knows how to process gradients during the backward step (as we did above for <span class="literal">ActivationLayer</span> and <span class="literal">FullyConnectedLayer</span>), it’s possible to use custom functions in layers without working through the derivatives. The graph engine does it for you, as long as you use primitive operations the engine already supports.</p>&#13;
<p class="indent">Let’s walk through the forward and backward passes of a computational graph. This example comes from the 2015 paper “TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems” (<em><a href="https://arxiv.org/pdf/1603.04467.pdf">https://arxiv.org/pdf/1603.04467.pdf</a></em>).</p>&#13;
<p class="indent">A hidden layer in a fully connected model is expressed as</p>&#13;
<p class="center"><em><strong>y</strong></em> = <strong>σ</strong>(<em><strong>Wx</strong></em> + <em><strong>b</strong></em>)</p>&#13;
<p class="noindent">for weight matrix <em><strong>W</strong></em>, bias vector <em><strong>b</strong></em>, input <em><strong>x</strong></em>, and output <em><strong>y</strong></em>.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig04">Figure 10-4</a> shows the same equation as a computational graph.</p>&#13;
<div class="image" id="ch10fig04"><img src="Images/10fig04.jpg" alt="image" width="402" height="218"/></div>&#13;
<p class="figcap"><em>Figure 10-4: The computational graphs representing the forward and backward passes through one layer of a feedforward neural network</em></p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig04">Figure 10-4</a> presents two versions. The top of the figure shows the forward pass, where data flows from <em><strong>x</strong></em>, <em><strong>W</strong></em>, and <em><strong>b</strong></em> to produce the output. Notice how the arrows lead left to right.</p>&#13;
<p class="indent">Note the sources are tensors, here either vectors or matrices. The outputs of operations are also tensors. The tensors flow through the graph, hence the name <em>TensorFlow</em>. <a href="ch10.xhtml#ch10fig04">Figure 10-4</a> represents matrix multiplication <span epub:type="pagebreak" id="page_269"/>as <span class="literal">@</span>, the NumPy matrix multiplication operator. The activation function is <strong>σ</strong>.</p>&#13;
<p class="indent">For the backward pass, the sequence of derivatives begins with ∂<em><strong>y</strong></em>/∂<em><strong>y</strong></em> = 1 and flows back through the graph from operator output to inputs. If there is more than one input, there is more than one output derivative. In practice, the graph evaluation engine processes the proper set of operators in the proper order. Each operator has its needed input derivatives available when it’s that operator’s turn to be processed.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig04">Figure 10-4</a> uses ∂ before an operator to indicate the derivatives the operator generates. For example, the addition operator (∂+) produces two outputs because there are two inputs, <em><strong>Wx</strong></em> and <em><strong>b</strong></em>. The same is true for matrix multiplication (∂@). The derivative of the activation function is shown as <strong>σ</strong>′.</p>&#13;
<p class="indent">Notice that arrows run from <em><strong>W</strong></em> and <em><strong>x</strong></em> in the forward pass to the derivative of the matrix multiplication operator in the backward pass. Both <em><strong>W</strong></em> and <em><strong>x</strong></em> are necessary to calculate ∂<em><strong>y</strong></em>/∂<em><strong>W</strong></em> and ∂<em><strong>y</strong></em>/∂<em><strong>x</strong></em>—see <a href="ch10.xhtml#ch10equ13">Equation 10.13</a> and <a href="ch10.xhtml#ch10equ11">Equation 10.11</a>, respectively. There is no arrow from <em><strong>b</strong></em> to the matrix multiplication operator because ∂<em><strong>y</strong></em>/∂<em><strong>b</strong></em> does not depend on <em><strong>b</strong></em>—see <a href="ch10.xhtml#ch10equ12">Equation 10.12</a>. If a layer were below what is shown in <a href="ch10.xhtml#ch10fig04">Figure 10-4</a>, the ∂<em><strong>y</strong></em>/∂<em><strong>x</strong></em> output from the matrix multiplication operator would become the input for the backward pass through that layer, and so on.</p>&#13;
<p class="indent">The power of computational graphs makes modern deep learning toolkits highly general and supports almost any network type and architecture, without burdening the user with detailed and highly tedious gradient calculations. As you continue to explore deep learning, do appreciate what the toolkits make possible with only a few lines of code.</p>&#13;
<h3 class="h3" id="ch10lev1_5">Summary</h3>&#13;
<p class="noindent">This chapter introduced backpropagation, one of the two pieces needed to make deep learning practical. First, we worked through calculating the necessary derivatives by hand for a tiny network and saw how laborious a process it was. However, we were able to train the tiny network successfully.</p>&#13;
<p class="indent">Next, we used our matrix calculus knowledge from <a href="ch08.xhtml#ch08">Chapter 8</a> to find the equations for multilayer fully connected networks and created a simple toolkit in the same vein as toolkits like Keras. With the toolkit, we successfully trained a model to high accuracy using the MNIST dataset. While effective and general in terms of the number of hidden layers and their sizes, the toolkit was restricted to fully connected models.</p>&#13;
<p class="indent">We ended the chapter with a cursory look at how modern deep learning toolkits like TensorFlow implement models and automate backpropagation. The computational graph enables arbitrary combinations of primitive operations, each of which can pass gradients backward as necessary, thereby allowing the complex model architectures we find in deep learning.</p>&#13;
<p class="indent">The second half of training a deep model is gradient descent, which puts the gradients calculated by backpropagation to work. Let’s now turn our attention that way.<span epub:type="pagebreak" id="page_270"/></p>&#13;
</div></body></html>