- en: '**7'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**7'
- en: EXPERIMENTS WITH CLASSICAL MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型实验**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In [Chapter 6](ch06.xhtml#ch06), we introduced several classical machine learning
    models. Let’s now take the datasets we built in [Chapter 5](ch05.xhtml#ch05) and
    use them with these models to see how well they perform. We’ll use sklearn to
    create the models and then we’ll compare them by looking at how well they do on
    the held-out test sets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#ch06)中，我们介绍了几种经典的机器学习模型。现在，让我们使用在[第5章](ch05.xhtml#ch05)中构建的数据集，并将其与这些模型一起使用，看看它们的表现如何。我们将使用sklearn来创建模型，然后通过查看它们在保留的测试集上的表现来进行比较。
- en: 'This will give us a good overview of how to work with sklearn and help us build
    intuition about how the different models perform relative to one another. We’ll
    use three datasets: the iris dataset, both original and augmented; the breast
    cancer dataset; and the vector form of the MNIST handwritten digits dataset.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供一个关于如何使用sklearn的良好概览，并帮助我们建立关于不同模型相互表现的直观感受。我们将使用三个数据集：鸢尾花数据集，包括原始和增强版；乳腺癌数据集；以及MNIST手写数字数据集的向量形式。
- en: Experiments with the Iris Dataset
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鸢尾花数据集实验
- en: 'We’ll start with the iris dataset. This data set has four continuous features—the
    measurements of the sepal length, sepal width, petal length, and petal width—and
    three classes—different iris species. There are 150 samples, 50 each from the
    three classes. In [Chapter 5](ch05.xhtml#ch05), we applied PCA augmentation to
    the dataset, so we actually have two versions we can work with: the original 150
    samples and the 1200 augmented training samples. Both can use the same test set.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从鸢尾花数据集开始。这个数据集有四个连续特征——萼片长度、萼片宽度、花瓣长度和花瓣宽度——以及三个类别——不同的鸢尾花种类。总共有150个样本，每个类别50个样本。在[第5章](ch05.xhtml#ch05)中，我们对数据集应用了PCA增强，因此我们实际上有两个版本可以使用：原始的150个样本和1200个增强的训练样本。两者都可以使用相同的测试集。
- en: We’ll use sklearn to implement versions of the Nearest Centroid, *k*-NN, Naïve
    Bayes, Decision Tree, Random Forest, and SVM models we outlined in [Chapter 6](ch06.xhtml#ch06).
    We’ll quickly see how powerful and elegant the sklearn toolkit is since our tests
    are virtually all identical across the models. The only thing that changes is
    the particular class we instantiate.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用sklearn实现我们在[第6章](ch06.xhtml#ch06)中概述的最近质心、*k*-NN、朴素贝叶斯、决策树、随机森林和SVM模型的版本。我们将很快看到sklearn工具包是多么强大和优雅，因为我们的测试几乎在所有模型中都是相同的。唯一变化的是我们实例化的具体类。
- en: Testing the Classical Models
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试经典模型
- en: The code for our initial tests is in [Listing 7-1](ch07.xhtml#ch7lis1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初步测试的代码在[清单7-1](ch07.xhtml#ch7lis1)中。
- en: import numpy as np
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: '❶ def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: print("    predictions  :", clf.predict(x_test))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: print("    预测结果  :", clf.predict(x_test))
- en: print("    actual labels:", y_test)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: print("    实际标签：", y_test)
- en: print("    score = %0.4f" % clf.score(x_test, y_test))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: print("    score = %0.4f" % clf.score(x_test, y_test))
- en: print()
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'def main():'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../data/iris/iris_features.npy")
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../data/iris/iris_features.npy")
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]; x_test = x[N:]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]; x_test = x[N:]
- en: y_train = y[:N]; y_test = y[N:]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]; y_test = y[N:]
- en: ❸ xa_train=np.load("../data/iris/iris_train_features_augmented.npy")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xa_train=np.load("../data/iris/iris_train_features_augmented.npy")
- en: ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")
- en: xa_test =np.load("../data/iris/iris_test_features_augmented.npy")
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: xa_test =np.load("../data/iris/iris_test_features_augmented.npy")
- en: ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")
- en: print("Nearest Centroid:")
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: print("最近质心：")
- en: ❹ run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: print("k-NN classifier (k=3):")
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN 分类器（k=3）：")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: print("Naive Bayes classifier (Gaussian):")
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器（高斯）：")
- en: ❺ run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ run(x_train, y_train, x_test, y_test, GaussianNB())
- en: print("Naive Bayes classifier (Multinomial):")
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, MultinomialNB())
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: ❻ print("Decision Tree classifier:")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: print("Random Forest classifier (estimators=5):")
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: ❼ print("SVM (linear, C=1.0):")
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: run(xa_train, ya_train, xa_test, ya_test, SVC(kernel="linear", C=1.0))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: print("SVM (RBF, C=1.0, gamma=0.25):")
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: SVC(kernel="rbf", C=1.0, gamma=0.25))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: print("SVM (RBF, C=1.0, gamma=0.001, augmented)")
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: SVC(kernel="rbf", C=1.0, gamma=0.001))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: ❽ print("SVM (RBF, C=1.0, gamma=0.001, original)")
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: SVC(kernel="rbf", C=1.0, gamma=0.001))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-1: Classic models using the iris dataset. See* iris_experiments.py.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary classes and modules. Notice that each of the
    classes represents a single type of model (classifier). For the Naïve Bayes classifier,
    we’re using two versions: the Gaussian version, GaussianNB, because the features
    are continuous values, and MultinomialNB for the discrete case to illustrate the
    effect of choosing a model that’s inappropriate for the dataset we’re working
    with. Because sklearn has a uniform interface for its classifiers, we can simplify
    things by using the same function to train and test any particular classifier.
    That function is run ❶. We pass in the training features (x_train) and labels
    (y_train) along with the test features and labels (x_test, y_test). We also pass
    in the particular classifier object (clf).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we do inside run is fit the model to the data by calling fit
    with the training data samples and labels. This is the training step. After the
    model is trained, we can test how well it does by calling the predict method with
    the held-out test data. This method returns the predicted class label for each
    sample in the test data. We held back 30 samples from the original 150 so predict
    will return a vector of 30 class label assignments, which we print. Next, we print
    the actual test labels so we can compare them visually with the predictions. Finally,
    we use the score method to apply the classifier to the test data (x_test) using
    the known test labels (y_test) to calculate the overall accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy is returned as a fraction between 0 and 1\. If every test sample
    were given the wrong label, the accuracy would be 0\. Even random guessing will
    do better than that, so a return value of 0 is a sign that something is amiss.
    Since there are three classes in the iris dataset, we’d expect a classifier that
    guesses the class at random to be right about one-third of the time and return
    a value close to 0.3333\. The actual score is calculated as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: score = *N[c]*/(*N[c]* + *N[w]*)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: where *N*[*c*] is the number of test samples for which the predicted class is
    correct; that is, it matches the class label in y_test. *N*[*w*] is the number
    of test samples where the predicted class does not match the actual class label.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a way to train and test each classifier, all we need to do
    is load the datasets and run a series of experiments by creating different classifier
    objects and passing them to run. Back inside of main, we begin by loading the
    original iris dataset and separating it into train and test cases ❷. We also load
    the augmented iris dataset that we created in [Chapter 5](ch05.xhtml#ch05) ❸.
    By design, the two test sets are identical, so regardless of which training set
    we use, the test set will be the same. This simplifies our comparisons.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define and execute the Nearest Centroid classifier ❹. The output is
    shown here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Nearest Centroid:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: predictions  :[011202120211112202201101102211]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: actual labels:[011202120211112202201101102211]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: score = 1.0000
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: We’ve removed spaces to make a visual comparison between the predicted and actual
    class labels easier. If there’s an error, the corresponding value, 0–2, will not
    match between the two lines. The score is also shown. In this case, it’s 1.0,
    which tells us that the classifier was perfect in its predictions on the held-out
    test set. This isn’t surprising; the iris dataset is a simple one. Because the
    iris dataset was randomized when created in [Chapter 5](ch05.xhtml#ch05), you
    might get a different overall score. However, unless your randomization was particularly
    unfortunate, you should have a high test score.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Based on what we learned in [Chapter 6](ch06.xhtml#ch06), we should expect that
    if the Nearest Centroid classifier is perfect on the test data, then all the other
    more sophisticated models will likewise be perfect. This is generally the case
    here, but as we’ll see, careless selection of model type or model hyperparameter
    values will result in inferior performance even from a more sophisticated model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Look again at [Listing 7-1](ch07.xhtml#ch7lis1), where we train a Gaussian
    Naïve Bayes classifier by passing an instance of GaussianNB to run ❺. This classifier
    is also perfect and returns a score of 1.0\. This is the correct way to use continuous
    values with a Naïve Bayes classifier. What happens if we instead use the discrete
    case even though we have continuous features? This is the MultinomialNB classifier,
    which assumes the features are selected from a discrete set of possible values.
    For the iris dataset, we can get away with defining such a classifier because
    the feature values are non-negative. However, because the features are not discrete,
    this model is not perfect and returns the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Naive Bayes classifier (Multinomial):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: predictions  :[011202220211122202202101102221]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: actual labels:[011202120211112202201101102211]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: score = 0.8667
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Here we see that the classifier is only 86.7 percent accurate on our test samples.
    If we need discrete counts for the probabilities, why did this approach work at
    all in this case? The answer is evident in the sklearn source code for the MultinomialNB
    classifier. The method that counts feature frequencies per class uses np.dot so
    that even if the feature values are continuous, the output will be a valid number,
    though not an integer. Still, mistakes were made, so we shouldn’t be happy. We
    should instead be careful to select the proper classifier type for the actual
    data we’re working with.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The next model we train in [Listing 7-1](ch07.xhtml#ch7lis1) is a Decision Tree
    ❻. This classifier is perfect on this dataset, as is the Random Forest trained
    next. Note, the Random Forest is using five estimators, meaning five random trees
    are created and trained; voting between the individual outputs determines the
    final class label. Note also that the Random Forest is trained on the augmented
    iris dataset, xa_train, because of the limited number of training samples in the
    unaugmented dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'We then train several SVM classifiers ❼, also on the augmented dataset. Recall
    that SVMs have two parameters we control: the margin constant, C, and gamma used
    by the Gaussian kernel.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The first is a linear SVM, meaning we need a value for the margin constant (C).
    We define C to be 1.0, the default value for sklearn. This classifier is perfect
    on the test data, as is the following classifier using the Gaussian kernel, for
    which we also set *γ* to 0.25\. The SVC class defaults to auto for gamma, which
    sets *γ* to 1/*n*, where *n* is the number of features. For the iris dataset,
    *n* = 4 so *γ* = 0.25.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train a model with very small *γ*. The classifier is still perfect
    on the test data. Lastly, we train the same type of SVM, but instead of the augmented
    training data, we use the original training data ❽. This classifier is not perfect:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: SVM (RBF, C=1.0, gamma=0.001, original)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: predictions  :[022202020222222202202202202220]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: actual labels:[011202120211112202201101102211]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: score = 0.5667
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it’s rather dismal. It never predicts class 1 and is right only 56.7
    percent of the time. This shows that data augmentation is valuable as it turned
    a lousy classifier into a good one—at least, good as far as we can know from the
    small test set we are using!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Nearest Centroid Classifier
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: What if we were stranded on a deserted island and didn’t have access to sklearn?
    Could we still quickly build a suitable classifier for the iris dataset? The answer
    is “yes,” as [Listing 7-2](ch07.xhtml#ch7lis2) shows. This code implements a quick-and-dirty
    Nearest Centroid classifier for the iris dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '❶ def centroids(x,y):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: c0 = x[np.where(y==0)].mean(axis=0)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: c1 = x[np.where(y==1)].mean(axis=0)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: c2 = x[np.where(y==2)].mean(axis=0)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: return [c0,c1,c2]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '❷ def predict(c0,c1,c2,x):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: p = np.zeros(x.shape[0], dtype="uint8")
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(x.shape[0]):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: d = [((c0-x[i])**2).sum(),
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: ((c1-x[i])**2).sum(),
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: ((c2-x[i])**2).sum()]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: p[i] = np.argmin(d)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: return p
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: ❸ x = np.load("../data/iris/iris_features.npy")
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: N = 120
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x[:N]; x_test = x[N:]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: y_train = y[:N]; y_test = y[N:]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: c0, c1, c2 = centroids(x_train, y_train)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: p = predict(c0,c1,c2, x_test)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: nc = len(np.where(p == y_test)[0])
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: nw = len(np.where(p != y_test)[0])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: acc = float(nc) / (float(nc)+float(nw))
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: print("predicted:", p)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: print("actual   :", y_test)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: print("test accuracy = %0.4f" % acc)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-2: A quick-and-dirty Nearest Centroid classifier for the iris dataset.
    See* iris_centroids.py.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: We load the iris data and separate it into train and test sets as before ❸.
    The centroids function returns the centroids of the three classes ❶. We can easily
    calculate these by finding the per feature means of each training sample of the
    desired class. This is all it takes to train this model. If we compare the returned
    centroids with those in the preceding trained NearestCentroid classifier (see
    the centroids_ member variable), we get precisely the same values.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Using the classifier is straightforward, as predict shows ❷. First, we define
    the vector of predictions, one per test sample (x). The loop defines d, a vector
    of Euclidean distances from the current test sample, x[i], to the three class
    centroids. The index of the smallest distance in d is the predicted class label
    (p[i]).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Let’s unpack d a bit more. We set d to a list of three values, the distances
    from the centroids to the current test sample. The expression
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: ((c0-x[i])**2).sum()
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: is a bit dense. The phrase c0-x[i] returns a vector of four numbers—four because
    we have four features. These are the differences between the centroid of class
    0 and the test sample feature value. This quantity is squared, which squares each
    of the four values. This squared vector is summed, element by element, to return
    the distance measure.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Strictly speaking, we’re missing a final step. The actual distance between c0
    and x[i] is the square root of this value. Since we’re simply looking for the
    smallest distance to each of the centroids, we don’t need to calculate the square
    root. The smallest value will still be the smallest value, whether we take the
    square root of all the values or not. Running this code produces the same output
    as we saw previously for the Nearest Centroid classifier, which is encouraging.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The iris dataset is extremely simple, so we shouldn’t be surprised by the excellent
    performance of our models even though we saw that careless selection of model
    type and hyperparameters will cause us trouble. Let’s now look at a larger dataset
    with more features, one that was not meant as a toy.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Experiments with the Breast Cancer Dataset
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The two-class breast cancer dataset we developed in [Chapter 5](ch05.xhtml#ch05)
    has 569 samples, each with 30 features, all measurements from a histology slide.
    There are 212 malignant cases (class 1) and 357 benign cases (class 0). Let’s
    train our classic models on this dataset and see what sort of results we get.
    As all the features are continuous, let’s use the normalized version of the dataset.
    Recall that a normalized dataset is one where, per feature in the feature vector,
    each value has the mean for that feature subtracted and then is divided by the
    standard deviation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/135equ01.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Normalization of the dataset maps all the features into the same overall range
    so that the value of one feature is similar to the value of another. This helps
    many model types and is a typical data preprocessing step, as we discussed in
    [Chapter 4](ch04.xhtml#ch04).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Two Initial Test Runs
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, we’ll do a quick run with a single test split, as we did in the previous
    section. The code is in [Listing 7-3](ch07.xhtml#ch7lis3) and mimics the code
    we described previously, where we pass in the model instance, train it, and then
    score it using the testing data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.svm import SVC
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: clf.fit(x_train, y_train)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: print("    score = %0.4f" % clf.score(x_test, y_test))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: print()
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: ❶ N = 455
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x[:N];  x_test = x[N:]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: y_train = y[:N];  y_test = y[N:]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: print("Nearest Centroid:")
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: print("k-NN classifier (k=3):")
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: print("k-NN classifier (k=7):")
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: print("Naive Bayes classifier (Gaussian):")
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: print("Decision Tree classifier:")
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: print("Random Forest classifier (estimators=5):")
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: print("Random Forest classifier (estimators=50):")
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: print("SVM (linear, C=1.0):")
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, SVC(kernel="linear", C=1.0))
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: print("SVM (RBF, C=1.0, gamma=0.03333):")
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: SVC(kernel="rbf", C=1.0, gamma=0.03333))
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-3: Initial models using the breast cancer dataset. See* bc_experiments.py.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we load the dataset and split it into training and testing data.
    We keep 455 of the 569 samples for training (80 percent), and the remaining 114
    samples are the test set (74 benign, 40 malignant). The dataset is already randomized,
    so we skip that step here. We then train nine models: Nearest Centroid (1), *k*-NN
    (2), Naïve Bayes (1), Decision Tree (1), Random Forest (2), linear SVM (1), and
    an RBF SVM (1). For the Support Vector Machines, we use the default *C* value,
    and for *γ*, we use 1/30 = 0.033333 since we have 30 features. Running this code
    gives us the scores in [Table 7-1](ch07.xhtml#ch7tab1).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-1:** Breast Cancer Model Scores'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model type** | **Score** |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.9649 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| 7-NN classifier | 0.9737 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Gaussian) | 0.9825 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.9474 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.9298 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.9737 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| Linear SVM (C = 1) | 0.9737 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9825 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: Note the number in parentheses for the Random Forest classifiers is the number
    of estimators (number of trees in the forest).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'A few things jump out at us. First, perhaps surprisingly, the simple Nearest
    Centroid classifier is right nearly 97 percent of the time. We also see that all
    the other classifiers are doing better than the Nearest Centroid, except for the
    Decision Tree and the Random Forest with five trees. Somewhat surprisingly, the
    Naïve Bayes classifier does very well, matching the RBF SVM. The *k* = 3 Nearest
    Neighbor classifier does best of all, 99 percent accurate, even though we have
    30 features, meaning our 569 samples are points scattered in a 30-dimensional
    space. Recall, a weakness of *k*-NN is the curse of dimensionality: it requires
    more and more training samples as the number of features increases. The results
    with all the classifiers are good, so this is a hint to us that the separation
    between malignant and benign is, for this dataset, distinct. There isn’t much
    overlap between the two classes using these features.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: So, are we done with this dataset? Hardly! In fact, we’ve just begun. What happens
    if we run the code a second time? Do we get the same scores? Would we expect not
    to? A second run gives us [Table 7-2](ch07.xhtml#ch7tab2).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-2:** Breast Cancer Scores, Second Run'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model type** | **Score** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.9649 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| 7-NN classifier | 0.9737 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Gaussian) | 0.9825 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | **0.9386** |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | **0.9474** |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | **0.9649** |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| Linear SVM (C = 1) | 0.9737 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9825 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: 'We’ve highlighted the scores that changed. Why would anything change? A bit
    of reflection leads to an *aha!* moment: the Random Forest is just that, random,
    so naturally we’d expect different results run to run. What about the Decision
    Tree? In sklearn, the Decision Tree classifier will randomly select a feature
    and find the best split, so different runs will also lead to different trees.
    This is a variation on the basic decision tree algorithm we discussed in [Chapter
    6](ch06.xhtml#ch06).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'All the other algorithms are fixed: for a given training dataset, they can
    lead to only one model. As an aside, the SVM implementation in sklearn does use
    a random number generator, so at times different runs will give slightly different
    results, but, conceptually, we’d expect the same model for the same input data.
    The tree-based classifiers, however, do change between training runs. We’ll explore
    this variation more next. For now, we need to add some rigor to our quick analysis.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The Effect of Random Splits
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s change the split between training and testing data and see what happens
    to our results. We don’t need to list all the code again since the only change
    is to how x_train and x_test are defined. Before splitting, we randomize the order
    of the full dataset but do so by first fixing the pseudorandom number seed so
    that each run gives the same ordering to the dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Looking again at [Listing 7-3](ch07.xhtml#ch7lis3), insert the following code
    before ❶ so that we generate a fixed permutation of the dataset (idx).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed(12345)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: x = x[idx]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: y = y[idx]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: It’s fixed because we fixed the pseudorandom number generator seed value. We
    then reorder the samples (x) and labels (y) accordingly before splitting into
    train and test subsets as before. Running this code gives us the results in [Table
    7-3](ch07.xhtml#ch7tab3).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-3:** Breast Cancer Scores After Randomizing the Dataset'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model type** | **Score** |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.9474 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| 7-NN classifier | 0.9912 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Gaussian) | 0.9474 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.9474 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.9912 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 1.0000 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| Linear SVM (C = 1) | 0.9649 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9737 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: Notice these are entirely different from our earlier results. The *k*-NN classifiers
    are both equally good, the SVM classifiers are worse, and the 50-tree Random Forest
    achieves perfection on the test set. So, what is happening? Why are we getting
    all these different results run to run?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re seeing the effect of the random sampling that builds the train and test
    splits. The first split just happened to use an ordering of samples that gave
    good results for one model type and less good results for other model types. The
    new split favors different model types. Which is correct? Both. Recall what the
    dataset represents: a sampling from some unknown parent distribution that generates
    the data that we actually have. If we think in those terms, we see that the dataset
    we have is an incomplete picture of the true parent distribution. It has biases,
    though we don’t know what they are necessarily, and is deficient in that there
    are parts of the parent distribution that the dataset does not represent well.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Further, when we split the data after randomizing the order, we might end up
    with a “bad” mix in the train or test portion—a mix of the data that does a poor
    job of representing the true distribution. If so, we might train a model to recognize
    a slightly different distribution that does not match the true distribution well,
    or the test set might be a bad mix and not be a fair representation of what the
    model has learned. This effect is even more pronounced when the proportion of
    the classes is such that one or more are rare and possibly not present in the
    train or test split. This is precisely the issue that caused us to introduce the
    idea of *k*-fold cross-validation in [Chapter 4](ch04.xhtml#ch04). With *k*-fold
    validation, we’ll be sure to use every sample as both train and test at some point
    and buy ourselves some protection against a bad split by averaging across all
    the folds.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: However, before we apply *k*-fold validation to the breast cancer dataset, we
    should notice one essential thing. We modified the code of [Listing 7-3](ch07.xhtml#ch7lis3)
    to fix the pseudorandom number seed so that we could reorder the dataset in exactly
    the same way each time we run. We then ran the code and saw the results. If we
    rerun the code, we’ll get *exactly* the same output, even for the tree-based classifiers.
    This is not what we saw earlier. The tree classifiers are *stochastic*—they will
    generate a unique tree or forest each time—so we should expect the results to
    vary somewhat from run to run. But now they don’t vary; we get the same output
    each time. By setting the NumPy pseudorandom number seed explicitly, we fixed
    not only the ordering of the dataset, but also the ordering of the *pseudorandom
    sequence* sklearn will use to generate the tree models. This is because sklearn
    is also using the NumPy pseudorandom number generator. This is a subtle effect
    with potentially serious consequences and in a larger project might be very difficult
    to pick up as a bug. The solution is to set the seed to a random value after we’re
    done reordering the dataset. We can do this by adding one line after y = y[idx]
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed()
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: so that the pseudorandom number generator is reset by using the system state,
    typically read from */dev/urandom*. Now when we run again, we’ll get different
    results for the tree models, as before.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Adding k-fold Validation
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To implement *k*-fold validation, we first need to pick a value for *k*. Our
    dataset has 569 samples. We want to split it so that there are a decent number
    of samples per fold because we want to make the test set a reasonable representation
    of the data. This argues toward making *k* small. However, we also want to average
    out the effect of a bad split, so we might want *k* to be larger. As with most
    things in life, a balance must be sought. If we set *k* = 5, we’ll get 113 samples
    per split (ignoring the final four samples, which should have no meaningful impact).
    This leaves 80 percent for training and 20 percent for test for each combination
    of folds, a reasonable thing to do. So, we’ll use *k* = 5, but we’ll write our
    code so that we can vary *k* if we want.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: We already have an approach for training multiple models on a train/ test split.
    All we need to add is code to generate each of the *k* folds and then train the
    models on them. The code is in [Listing 7-4](ch07.xhtml#ch7lis4) and [Listing
    7-5](ch07.xhtml#ch7lis5), which show the helper functions and main function, respectively.
    Let’s start with [Listing 7-4](ch07.xhtml#ch7lis4).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.svm import SVC
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: clf.fit(x_train, y_train)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: return clf.score(x_test, y_test)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'def split(x,y,k,m):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ❶ ns = int(y.shape[0]/m)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: s = []
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ❷ s.append([x[(ns*i):(ns*i+ns)],
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: y[(ns*i):(ns*i+ns)]])
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: x_test, y_test = s[k]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: x_train = []
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: y_train = []
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'if (i==k):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: a,b = s[i]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: x_train.append(a)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: y_train.append(b)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: ❸ x_train = np.array(x_train).reshape(((m-1)*ns,30))
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.array(y_train).reshape((m-1)*ns)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: return [x_train, y_train, x_test, y_test]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'def pp(z,k,s):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: m = z.shape[1]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'print("%-19s: %0.4f +/- %0.4f | " % (s, z[k].mean(),'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: z[k].std()/np.sqrt(m)), end='')
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: print("%0.4f " % z[k,i], end='')
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: print()
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-4: Using k-fold validation to evaluate the breast cancer dataset.
    Helper functions. See* bc_kfold.py.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 7-4](ch07.xhtml#ch7lis4) begins by including all the modules we used
    before and then defines three functions: run, split, and pp. The run function
    looks familiar. It takes a train set, test set, and model instance, trains the
    model, and then scores the model against the test set. The pp function is a pretty-print
    function to show the per split scores along with the average score across all
    the splits. The average is shown as the mean ± the standard error of the mean.
    Recall that an sklearn score is the overall accuracy of the model on the test
    set, or the fraction of times that the model predicted the actual class of the
    test sample. Perfection is a score of 1.0, and complete failure is 0.0\. Complete
    failure is rare because even random guessing will get it right some fraction of
    the time.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The only interesting function in [Listing 7-4](ch07.xhtml#ch7lis4) is split.
    Its arguments are the full dataset, x, the corresponding labels, y, the current
    fold number, k, and the total number of folds, m. We’ll divide the full dataset
    into *m* distinct sets, the folds, and use the *k*-th fold as test while merging
    the remaining *m –* 1 folds into a new training set. First, we set the number
    of samples per fold ❶. The loop then creates a list of folds, s. Each element
    of this list contains the feature vectors and labels of the fold ❷.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The test set is simple, it’s the *k*-th fold, so we set those values next (x_test,
    y_test). The loop then takes the remaining *m –* 1 folds and merges them into
    a new training set, x_train, with labels, y_train.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The two lines after the loop are a bit mysterious ❸. When the loop ends, x_train
    is a *list*, each element of which is a list representing the feature vectors
    of the fold we want in the training set. So we first make a NumPy array of this
    list and then reshape it so that x_train has 30 columns, the number of features
    per vector, and *n*[*s*](*m –* 1) rows, where *n*[*s*] is the number of samples
    per fold. Thus x_train becomes x minus the samples we put into the test fold,
    those of the *k*-th fold. We also build y_train so that the correct label goes
    with each the feature vector in x_train.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 7-5](ch07.xhtml#ch7lis5) shows us how to use the helper functions.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: x = x[idx]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: y = y[idx]
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: ❶ m = int(sys.argv[1])
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: z = np.zeros((8,m))
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'for k in range(m):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: z[0,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: NearestCentroid())
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: z[1,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: z[2,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: z[3,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: GaussianNB())
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: z[4,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: DecisionTreeClassifier())
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: z[5,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: z[6,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: z[7,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: SVC(kernel="linear", C=1.0))
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: pp(z,0,"Nearest"); pp(z,1,"3-NN")
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: pp(z,2,"7-NN");    pp(z,3,"Naive Bayes")
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: pp(z,4,"Decision Tree");    pp(z,5,"Random Forest (5)")
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: pp(z,6,"Random Forest (50)");    pp(z,7,"SVM (linear)")
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-5: Using k-fold validation to evaluate the breast cancer dataset.
    Main code. See* bc_kfold.py.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we do in main is load the full dataset and randomize the ordering.
    The number of folds, m, is read from the command line ❶ and used to create the
    output array, z. This array holds the per fold scores for each of the eight models
    we’ll train, so it has shape 8 × *m*. Recall, when running a Python script from
    the command line, any arguments passed after the script name are available in
    sys.argv, a list of strings. This is why the argument is passed to int to convert
    it to an integer ❶.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Next, we loop over the *m* folds, where *k* is the fold that we’ll be using
    for test data. We create the split and then use the split to train the eight model
    types we trained previously. Each call to run trains a model of the type passed
    in and returns the score found by running that model against the *k*-th fold as
    test data. We store these results in z. Finally, we use pp to display the per
    model type and per fold scores along with the average score over all the folds.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: A sample run of this code, for *k* = 5 and showing only the mean score across
    folds, gives the results in [Table 7-4](ch07.xhtml#ch7tab4).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-4:** Breast Cancer Scores as Mean Over Five Folds'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Mean** ± **SE** |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.9310 ± 0.0116 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| 3-NN | 0.9735 ± 0.0035 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| 7-NN | 0.9717 ± 0.0039 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.9363 ± 0.0140 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.9027 ± 0.0079 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.9540 ± 0.0107 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.9540 ± 0.0077 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| SVM (linear) | 0.9699 ± 0.0096 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: Here we’re showing the average performance of each model over all folds. One
    way to understand the results is that this is the sort of performance we should
    expect, per model type, if we were to train the model using *all* of the data
    in the dataset and test it against new samples from the same parent distribution.
    Indeed, in practice, we would do just this, as we can assume that the reason behind
    making the model in the first place is to use it for some purpose going forward.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Run the code a second time with *k* = 5\. A new set of outputs appears. This
    is because we’re randomizing the order of the dataset on every run ([Listing 7-5](ch07.xhtml#ch7lis5)).
    This makes a new set of splits and implies that each model will be trained on
    a different subset mix of the full dataset on each run. So, we should expect different
    results. Let’s run the code 1,000 times with *k* = 5\. Note, training this many
    models takes about 20 minutes on a very standard desktop computer. For each run
    we’ll get an average score over the five folds. We then compute the mean of these
    averages, which is known as the *grand mean*. [Table 7-5](ch07.xhtml#ch7tab5)
    shows the results.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-5:** Breast Cancer Scores as Grand Mean Over 1,000 Runs with Five
    Folds'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Grand mean** ± **SE** |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.929905 ± 0.000056 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| 3-NN | 0.966334 ± 0.000113 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| 7-NN | 0.965496 ± 0.000110 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.932973 ± 0.000095 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.925706 ± 0.000276 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.948378 ± 0.000213 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.958845 ± 0.000135 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| SVM (linear) | 0.971871 ± 0.000136 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: We can take these grand means as an indication of how well we’d expect each
    model to do against a new set of unknown feature vectors. The small standard errors
    of the mean are an indication of how well the mean value is known, not how well
    a model of that type trained on a dataset will necessarily perform. We use the
    grand mean to help us order the models so we can select one over another.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Ranking the models from highest score to lowest gives the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: SVM (linear)
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*k*-NN (*k* = 3)'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*k*-NN (*k* = 7)'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random Forest (50)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random Forest (5)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naïve Bayes (Gaussian)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nearest Centroid
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision Tree
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is interesting given that we might expect the SVM to be best, but would
    likely assume the Random Forests to do better than *k*-NN. The Decision Tree was
    not as good as we thought, and was less accurate than the Nearest Centroid classifier.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Some comments are in order here. First, note that these results are derived
    from the training of 8,000 different models on 1,000 different orderings of the
    dataset. When we study neural networks, we’ll see much longer training times.
    Experimenting with classical machine learning models is generally easy to do since
    each change to a parameter doesn’t require a lengthy training session.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Second, we didn’t try to optimize any of the model hyperparameters. Some of
    these hyperparameters are indirect, like assuming that the features are normally
    distributed so that the Gaussian Naïve Bayes classifier is a reasonable choice,
    while others are numerical, like the number of neighbors in *k*-NN or the number
    of trees in a Random Forest. If we want to thoroughly develop a good classifier
    for this dataset using a classic model, we’ll have to explore some of these hyperparameters.
    Ideally, we’d repeat the experiments many, many times for each new hyperparameter
    setting to arrive at a tight mean value for the score, as we have previously with
    the grand means over 1,000 runs. We’ll play a bit more with hyperparameters in
    the next section, where we see how we can search for good ones that work well
    with our dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Searching for Hyperparameters
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s explore the effect of some of the hyperparameters on various model types.
    Specifically, let’s see if we can optimize our choice of *k* for *k*-NN, forest
    size for Random Forest, and the *C* margin size of the linear SVM.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Our k-NN Classifier
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because the number of neighbors in a *k*-NN classifier is an integer, typically
    odd, it’s straightforward to repeat our five-fold cross validation experiment
    while varying *k* for *k* ∈ *{*1,3,5,7,9,11,13,15*}*. To do this, we need only
    change the main loop in [Listing 7-5](ch07.xhtml#ch7lis5) so that each call to
    run uses KNeighborsClassifier with a different number of neighbors, as follows.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'for k in range(m):'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: z[0,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=1))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: z[1,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: z[2,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=5))
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: z[3,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: z[4,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=9))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: z[5,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=11))
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: z[6,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=13))
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: z[7,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=15))
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The grand mean of the scores for 1,000 repetitions of the five-fold cross-validation
    code using a different random ordering of the full dataset each time gives the
    results in [Table 7-6](ch07.xhtml#ch7tab6).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-6:** Breast Cancer Scores as Grand Mean for Different k Values and
    Five-Fold Validation'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '| ***k*** | **Grand mean** ± **SE** |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.951301 ± 0.000153 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.966282 ± 0.000112 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.965998 ± 0.000097 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.96520 ± 0.000108 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| **9** | **0.967011** ± **0.000100** |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.965069 ± 0.000107 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| 13 | 0.962400 ± 0.000106 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| 15 | 0.959976 ± 0.000101 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: We’ve highlighted the *k* = 9 because it returned the highest score. This indicates
    that we might want to use *k* = 9 for this dataset.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Our Random Forest
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s look at the Random Forest model. The sklearn RandomForestClassifier class
    has quite a few hyperparameters that we could manipulate. To avoid being excessively
    pedantic, we’ll seek only an optimal number of trees in the forest. This is the
    n_estimators parameter. As we did for *k* in *k*-NN, we’ll search over a range
    of forest sizes and select the one that gives the best grand mean score for 1,000
    runs at five folds each per run.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: This is a one-dimensional grid-like search. We varied *k* by one, but for the
    number of trees in the forest, we need to cover a larger scale. We don’t expect
    there to be a meaningful difference between 10 trees in the forest or 11, especially
    considering that each Random Forest training session will lead to a different
    set of trees even if the number of trees is fixed. We saw this effect several
    times in the previous section. Instead, let’s vary the number of trees by selecting
    from *n*[*t*] ∈ *{*5,20,50,100,200,500,1000,5000*}* where *n*[*t*] is the number
    of trees in the forest (number of estimators). Running this search gives us the
    grand means in [Table 7-7](ch07.xhtml#ch7tab7).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-7:** Breast Cancer Scores as Grand Mean for Different Random Forest
    Sizes and Five-Fold Validation'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '| ***n[t]*** | **Grand mean** ± **SE** |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.948327 ± 0.000206 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0.956808 ±0.000166 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| 50 | 0.959048 ± 0.000139 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| 100 | 0.959740 ± 0.000130 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| 200 | 0.959913 ± 0.000122 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| 500 | 0.960049 ± 0.000117 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| 750 | 0.960147 ± 0.000118 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 0.960181 ± 0.000116 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: The first thing to notice is that the differences are very small, though if
    you run the Mann–Whitney U test, you’ll see that the difference between *n*[*t*]
    = 5 (worst) and *n*[*t*] = 1000 (best) is statistically significant. However,
    the difference between *n*[*t*] = 200 and *n*[*t*] = 1000 is not significant.
    Here we need to make a judgment call. Setting *n*[*t*] = 1000 did give the best
    result but it’s indistinguishable, for practical purposes, from *n*[*t*] = 500
    or even *n*[*t*] = 100\. Since runtime for a Random Forest scales linearly in
    the number of trees, using *n*[*t*] = 100 results in a classifier that is on average
    10× faster than using *n*[*t*] = 1000\. So, depending upon the task, we might
    select *n*[*t*] = 100 over *n*[*t*] = 1000 for that reason.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Our SVMs
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s turn our attention to the linear SVM. For the linear kernel, we’ll adjust
    *C*. Note, sklearn has other parameters, as it did for the Random Forest, but
    we’ll leave them at their default settings.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: What range of *C* should we search over? The answer is problem dependent but
    the sklearn default value of *C* = 1 is a good starting point. We’ll select *C*
    values around 1 but over several orders of magnitude. Specifically, we’ll select
    from *C* ∈ *{*0.001,0.01,0.1,1.0,2.0,10.0,50.0,100.0*}*. Running one thousand
    five-fold validations, each for a different random ordering of the full dataset,
    gives grand means as shown in [Table 7-8](ch07.xhtml#ch7tab8).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-8:** Breast Cancer Scores as Grand Mean for Different SVM C Values
    and Five-Fold Validation'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '| **C** | **Grand mean** ± **SE** |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| 0.001 | 0.938500 ± 0.000066 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| 0.01 | 0.967151 ± 0.000089 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.975943 ± 0.000101 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 0.971890 ± 0.000141 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| 2.0 | 0.969994 ± 0.000144 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| 10.0 | 0.966239 ± 0.000154 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| 50.0 | 0.959637 ± 0.000186 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| 100.0 | 0.957006 ± 0.000189 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '*C* = 0.1 gives the best accuracy. While, statistically, the difference between
    *C* = 0.1 and *C* = 1 is meaningful, in practice the difference is only about
    0.4 percent, so the default value of *C* = 1 would likewise be a reasonable choice.
    Further refinement of *C* is possible because we see that *C* = 0.01 and *C* =
    2 give the same accuracy, while *C* = 0.1 is higher than either, implying that
    if the *C* curve is smooth, there’s a maximum accuracy for some *C* in [0.01,2.0].'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the right *C* for our dataset is a crucial part of successfully using
    a linear SVM. Our preceding rough run used a one-dimensional grid search. We do
    expect, since *C* is continuous, that a plot of the accuracy as a function of
    *C* will also be smooth. If that’s the case, one can imagine searching for the
    right *C*, not with a grid search but with an optimization algorithm. In practice,
    however, the randomness of the ordering of the dataset and its effect on the output
    of *k*-fold cross-validation results will probably make any *C* found by an optimization
    algorithm too specific to the problem at hand. Grid search over a larger scale,
    with possibly one level of refinement, is sufficient in most cases. The take-home
    message is: do spend some time looking for the proper *C* value to maximize the
    effectiveness of the linear SVM.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Observant readers will have noticed that the preceding analysis has ignored
    the RBF kernel SVM. Let’s revisit it now and see how to do a simple two-dimensional
    grid search over *C* and *γ*, where *γ* is the parameter associated with the RBF
    (Gaussian) kernel. sklearn has the GridSearchCV class to perform sophisticated
    grid searching. We’re not using it here to be pedagogical and show how to do simple
    grid searches directly. It’s especially important for this kernel to select good
    values for both of these parameters.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: For the search, we’ll use the same range of *C* values as we used for the linear
    case. For *γ* we’ll use powers of two, 2^(*p*), times the sklearn default value,
    1/30 = 0.03333 for *p* ∈ [*–*4,3]. The search will, for the current *C* value,
    do five-fold validation over the dataset for each *γ* value before moving to the
    next *C* value so that all pairs of (*C*,*γ*) are considered. The pair that results
    in the largest score (accuracy) will be output. The code is in [Listing 7-6](ch07.xhtml#ch7lis6).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.svm import SVC
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: clf.fit(x_train, y_train)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: return clf.score(x_test, y_test)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'def split(x,y,k,m):'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: ns = int(y.shape[0]/m)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: s = []
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: x_test, y_test = s[k]
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: x_train = []
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: y_train = []
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'if (i==k):'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: a,b = s[i]
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: x_train.append(a)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: y_train.append(b)
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: x_train = np.array(x_train).reshape(((m-1)*ns,30))
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.array(y_train).reshape((m-1)*ns)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: return [x_train, y_train, x_test, y_test]
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: m = 5
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: x = x[idx]
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: y = y[idx]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: ❶ Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: zmax = 0.0
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '❷ for C in Cs:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'for g in gs:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: z = np.zeros(m)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'for k in range(m):'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: z[k] = run(x_train, y_train, x_test, y_test,
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: SVC(C=C,gamma=g,kernel="rbf"))
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '❸ if (z.mean() > zmax):'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: zmax = z.mean()
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: bestC = C
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: bestg = g
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: print("best C     = %0.5f" % bestC)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: print("     gamma = %0.5f" % bestg)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: print("   accuracy= %0.5f" % zmax)
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-6: A two-dimensional grid search for C and  for an RBF kernel SVM.
    Breast cancer dataset. See* bc_rbf_svm_search.py.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: The two helper functions, run and split, are exactly the same as we used before
    (see [Listing 7-4](ch07.xhtml#ch7lis4)); all the action is in main. We fix the
    number of folds at five and then load and randomize the full dataset.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: We then define the specific *C* and *γ* values to search over ❶. Note how gs
    is defined. The first part is 1/30, the reciprocal of the number of features.
    This is the default value for *γ* used by sklearn. We then multiply this factor
    by an array, (2^(*–*4),2^(*–*3),2^(*–*1),2⁰,2¹,2²,2³), to get the final *γ* values
    we’ll search over. Notice that one of the *γ* values is exactly the default sklearn
    uses since 2⁰ = 1.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: The double loop ❷ iterates over all possible pairs of *C* and *γ*. For each
    one, we do five-fold validation to get a set of five scores in z. We then ask
    if the mean of this set is greater than the current maximum (*z*[max]) and if
    so, update the maximum and keep the *C* and *γ* values as our current bests ❸.
    When the loops over *C* and *γ* exit, we have our best values in bestC and bestg.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: If we run this code repeatedly, we’ll get different outputs each time. This
    is because we’re randomizing the order of the full dataset, which will alter the
    subsets in the folds, leading to a different mean score over the folds. For example,
    10 runs produced the output in [Table 7-9](ch07.xhtml#ch7tab9).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-9:** Breast Cancer Scores for an RBF SVM with Different C and *γ*
    Values Averaged Over 10 Runs'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '| ***C*** | ***γ*** | ***accuracy*** |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.03333 | 0.97345 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.03333 | 0.98053 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.00417 | 0.97876 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.00417 | 0.97699 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.00417 | 0.98053 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.01667 | 0.98053 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.01667 | 0.97876 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.01667 | 0.98053 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.03333 | 0.97522 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.00417 | 0.97876 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: These results hint that (*C*,*γ*) = (10,0.00417) is a good combination. If we
    use these values to generate a grand mean over 1,000 runs of five-fold validation
    as before, we get an overall accuracy of 0.976991, or 97.70 percent, which is
    the highest grand mean accuracy of any model type we trained on the breast cancer
    histology dataset.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: The breast cancer dataset is not a large dataset. We were able to use *k*-fold
    validation to find a good model that worked well with it. Now, let’s move from
    a pure vector-only dataset to one that is actually image-based and much larger,
    the MNIST dataset.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Experiments with the MNIST Dataset
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last dataset we’ll work with in this chapter is the vector version of the
    MNIST handwritten digit dataset (see [Chapter 5](ch05.xhtml#ch05)). Recall, this
    dataset consists of 28×28 pixel grayscale images of handwritten digits, [0,9],
    one digit centered per image. This dataset is by far the most common workhorse
    dataset in machine learning, especially in deep learning, and we’ll use it throughout
    the remainder of the book.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Classical Models
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MNIST contains 60,000 training images, roughly evenly split among the digits,
    and 10,000 test images. Since we have a lot of training data, at least for classic
    models like those we’re concerned with here, we won’t make use of *k*-fold validation,
    though we certainly could. We’ll train on the training data and test on the testing
    data and trust that the two come from a common parent distribution (they do).
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Since our classic models expect vector inputs, we’ll use the vector form of
    the MNIST dataset we created in [Chapter 5](ch05.xhtml#ch05). The images are unraveled
    so that the first 28 elements of the vector are row 0, the next 28 are row 1,
    and so on for an input vector of 28 × 28 = 784 elements. The images are stored
    as 8-bit grayscale, so the data values run from 0 to 255\. We’ll consider three
    versions of the dataset. The first is the raw byte version. The second is a version
    where we scale the data to [0,1) by dividing by 256, the number of possible grayscale
    values. The third is a normalized version where, per “feature” (really, pixel),
    we subtract the mean of that feature across the dataset and then divide by the
    standard deviation. This will let us explore how the range of the feature values
    affects things, if at all.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-1](ch07.xhtml#ch7fig1) shows examples of the original images and
    the resulting normalized vectors raveled back into images and scaled [0,255].
    Normalizing affects the appearance but does not destroy spatial relationships
    among the parts of the digit images. Just scaling the data to [0,1) will result
    in images that look the same as those on the top of [Figure 7-1](ch07.xhtml#ch7fig1).'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig01.jpg)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-1: Original MNIST digits (top) and normalized versions used by the
    models (bottom)*'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: The code we’ll use is very similar to what we used previously, but for reasons
    that will be explained next, we will replace the SVC class with a new SVM class,
    LinearSVC. First, take a look at the helper functions in [Listing 7-7](ch07.xhtml#ch7lis7).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: import time
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.svm import LinearSVC
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn import decomposition
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: s = time.time()
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: clf.fit(x_train, y_train)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: e_train = time.time() - s
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: s = time.time()
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: score = clf.score(x_test, y_test)
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: e_test = time.time() - s
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: print("score = %0.4f (time, train=%8.3f, test=%8.3f)"
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '% (score, e_train, e_test))'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: 'def train(x_train, y_train, x_test, y_test):'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Nearest Centroid          : ", end='''')'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    k-NN classifier (k=3)     : ", end='''')'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    k-NN classifier (k=7)     : ", end='''')'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Naive Bayes (Gaussian)    : ", end='''')'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Decision Tree             : ", end='''')'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Random Forest (trees=  5) : ", end='''')'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Random Forest (trees= 50) : ", end='''')'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Random Forest (trees=500) : ", end='''')'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=500))
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Random Forest (trees=1000): ", end='''')'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=1000))
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=0.01)        : ", end='''')'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=0.1)         : ", end='''')'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=1.0)         : ", end='''')'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=10.0)        : ", end='''')'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-7: Training differently scaled versions of the MNIST dataset using
    classic models. Helper functions. See* mnist_experiments.py.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: The run function of [Listing 7-7](ch07.xhtml#ch7lis7) is also similar to those
    used previously, except we’ve added code to time how long training and testing
    takes. These times are reported along with the score. We added this code for MNIST
    because, unlike the tiny iris and breast cancer datasets, MNIST has a larger number
    of training samples so that runtime differences among the model types will start
    to show themselves. The train function is new, but all it does is wrap calls to
    run for the different model types.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: Now take a look at [Listing 7-8](ch07.xhtml#ch7lis8), which contains the main
    function.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: x_train = np.load("mnist_train_vectors.npy").astype("float64")
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.load("mnist_train_labels.npy")
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("mnist_test_vectors.npy").astype("float64")
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("mnist_test_labels.npy")
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on raw [0,255] images:")
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: train(x_train, y_train, x_test, y_test)
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on raw [0,1) images:")
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: train(x_train/256.0, y_train, x_test/256.0, y_test)
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: ❶ m = x_train.mean(axis=0)
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: s = x_train.std(axis=0) + 1e-8
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: x_ntrain = (x_train - m) / s
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: x_ntest  = (x_test - m) / s
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on normalized images:")
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: train(x_ntrain, y_train, x_ntest, y_test)
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: ❷ pca = decomposition.PCA(n_components=15)
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(x_ntrain)
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: x_ptrain = pca.transform(x_ntrain)
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: x_ptest = pca.transform(x_ntest)
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on first 15 PCA components of normalized images:")
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: train(x_ptrain, y_train, x_ptest, y_test)
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-8: Training differently scaled versions of the MNIST dataset using
    classic models. Main function. See* mnist_experiments.py.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: The main function of [Listing 7-8](ch07.xhtml#ch7lis8) loads the data and then
    trains the models using the raw byte values. It then repeats the training using
    a scaled [0,1) version of the data and a scaled version of the testing data. These
    are the first two versions of the dataset we’ll use.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing the data requires knowledge of the per feature means and standard
    deviations ❶. Note, we add a small value to the standard deviations to make up
    for pixels that have a standard deviation of zero. We can’t divide by zero, after
    all. We need to normalize the test data, but which means and which standard deviations
    should we use? Generally, we have more training data than testing data, so using
    the means and standard deviations from the training data makes sense; they are
    a better representation of the true means and standard deviations of the parent
    distribution that generated the data in the first place. However, at times, there
    may be slight differences between the training and testing data distributions,
    in which case it might make sense to consider the testing means and standard deviations.
    In this case, because the MNIST training and test datasets were created together,
    there’s no difference, so the training values are what we’ll use. Note that the
    same per feature means and standard deviations will need to be used for all new,
    unknown samples, too.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Next, we apply PCA to the dataset just as we did for the iris data in [Chapter
    5](ch05.xhtml#ch05) ❷. Here we’re keeping the first 15 components. These account
    for just over 33 percent of the variance in the data and reduce the feature vector
    from 784 features (the pixels) to 15 features (the principal components). Then
    we train the models using these features.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Running this code produces a wealth of output that we can learn from. Let’s
    first consider the scores per model type and data source. These are in [Table
    7-10](ch07.xhtml#ch7tab10); values in parentheses are the number of trees in the
    Random Forest.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-10:** MNIST Model Scores for Different Preprocessing Steps'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Raw [0,255]** | **Scaled [0,1)** | **Normalized** | **PCA**
    |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.8203 | 0.8203 | 0.8092 | 0.7523 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 | 0.9452 | 0.9355 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 | 0.9433 | 0.9370 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.5558 | 0.5558 | 0.5239 | 0.7996 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.8773 | 0.8784 | 0.8787 | 0.8403 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.9244 | 0.9244 | 0.9220 | 0.8845 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.9660 | 0.9661 | 0.9676 | 0.9215 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (500) | 0.9708 | 0.9709 | 0.9725 | 0.9262 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (1000) | 0.9715 | 0.9716 | 0.9719 | 0.9264 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.01) | 0.8494 | 0.9171 | 0.9158 | 0.8291 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.1) | 0.8592 | 0.9181 | 0.9163 | 0.8306 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 1.0) | 0.8639 | 0.9182 | 0.9079 | 0.8322 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 10.0) | 0.8798 | 0.9019 | 0.8787 | 0.7603 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
- en: Look at the Nearest Centroid scores. These make sense as we move from left to
    right across the different versions of the dataset. For the raw data, the center
    location of each of the 10 classes leads to a simple classifier with an accuracy
    of 82 percent—not too bad considering random guessing would have an accuracy closer
    to 10 percent (1/10 for 10 classes). Scaling the data by a constant won’t change
    the relative relationship between the per class centroids so we’d expect the same
    performance in column 2 of [Table 7-10](ch07.xhtml#ch7tab10) as in column 1.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing, however, does more than divide the data by a constant. We saw the
    effect clearly in [Figure 7-1](ch07.xhtml#ch7fig1). This alteration, at least
    for the MNIST dataset, changes the centroids’ relationships to each other and
    results in a decrease in accuracy to 80.9 percent.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: Finally, using PCA to reduce the number of features from 784 to 15 has a severe
    negative impact, resulting in an accuracy of only 75.2 percent. Note the word
    *only*. In the past, before the advent of deep learning, an accuracy of 75 percent
    on a problem with 10 classes would generally have been considered to be pretty
    good. Of course, it really isn’t. Who would get in a self-driving car that has
    an accident one time out of every four trips? We want to do better.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the *k*-NN classifiers next. We see similar performance for both
    *k* = 3 and *k* = 7 and the same sort of trend as we saw with the Nearest Centroid
    classifier. This is to be expected given how similar the two types of models actually
    are. The difference in accuracy between the two (centroid and *k*-NN) is dramatic,
    however. An accuracy of 97 percent is generally regarded as good. But still, who
    would opt for elective surgery with a 3 percent failure rate?
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: Things get interesting when we look at the Naïve Bayes classifier. Here all
    the versions of the dataset perform poorly, though still five times better than
    guessing. We see a large jump in accuracy with the PCA processed dataset, from
    56 percent to 80 percent. This is the only model type to improve after using PCA.
    Why might this be? Remember, we’re using Gaussian Naïve Bayes, which means our
    independence assumption is coupled with an assumption that the continuous feature
    values are, per feature, really drawn from a normal distribution whose parameters,
    the mean and standard deviation, we can estimate from the feature values themselves.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: Now recall what PCA does, geometrically. It’s the equivalent of rotating the
    feature vectors onto a new set of coordinates aligned with the largest orthogonal
    directions derivable from the dataset. The word *orthogonal* implies that no part
    of a direction overlaps with any other part of any other direction. Think of the
    x-, y-, and z-axes of a three-dimensional plot. No part of the *x* is along the
    *y* or *z*, and so forth. This is what PCA does. Therefore, PCA makes the first
    assumption of Naïve Bayes more likely to be true, that the new features are indeed
    independent of each other. Add in the Gaussian assumption as to the distribution
    of the per pixel values, and we have an explanation for what we see in [Table
    7-10](ch07.xhtml#ch7tab10).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: The tree-based classifiers, Decision Tree and Random Forest, perform much the
    same until we get to the PCA version of the dataset. Indeed, there is no difference
    between the raw data and the data scaled by 256\. Again, this is to be expected
    as all scaling by a constant does is scale the decision thresholds for each of
    the nodes in the body of the tree or trees. As before, working with reduced dimensionality
    vectors via PCA results in a loss of accuracy because potentially important information
    has been discarded.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: For any data source, we see scores that make sense relative to each other. As
    before, the single Decision Tree performs worst, which it should except for simple
    cases since it’s competing against a collection of trees via the Random Forests.
    For the Random Forests, we see that the score improves as the number of trees
    in the forest increases—again expected. However, the improvement comes with diminishing
    returns. There’s a significant improvement when going from 5 trees to 50 trees,
    but a minimal improvement in going from 500 trees to 1,000 trees.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at the SVM results, let’s understand why we made the switch from
    the SVC class to LinearSVC. As the name suggests, LinearSVC implements only a
    linear kernel. The SVC class is more generic and can implement other kernels,
    so why switch?
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: The reason has to do with runtime. In computer science, there are specific definitions
    of complexity and an entire branch devoted to the analysis of algorithms and how
    they perform as their inputs scale larger and larger. All we’ll concern ourselves
    with here is *big-O* notation. This is a way of characterizing how the runtime
    of an algorithm changes as the input (or the number of inputs) gets larger and
    larger.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: For example, a classic bubble sort algorithm works just fine on a few dozen
    numbers to be sorted. But, as the input gets larger (more numbers to be sorted),
    the runtime increases not linearly but quadratically, meaning the time to sort
    the numbers, *t*, is proportional to the *square* of the number of numbers to
    be sorted, *t* ∝ *n*², which is written as *O*(*n*²). So, the bubble sort is an
    order *n*² algorithm. In general, we want algorithms that are better than *n*²,
    more like *n*, written as *O*(*n*), or even independent of *n*, written as *O*(1).
    It turns out that the kernel algorithm for training an SVM is *worse* than *O*(*n*²)
    so that when the number of training samples increases, the runtime explodes. This
    is one reason for the switch from the SVC class to LinearSVC, which doesn’t use
    kernels.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reason for the switch has to do with the fact that Support Vector
    Machines are designed for binary classification—only two classes. The MNIST dataset
    has 10 classes, so something different has to be done. There are multiple approaches.
    According to the sklearn documentation, the SVC class uses a *one-versus-one*
    approach that trains pairs of classifiers, one class versus another: class 0 versus
    class 1, class 1 versus class 2, class 0 versus class 2, and so on. This means
    it ends up training not one but *m*(*m –* 1)/2 classifiers for *m* = 10 classes,
    or 10(10 *–* 1)/2 = 45 separate classifiers. This isn’t efficient in this case.
    The LinearSVC classifier uses a *one-versus-rest* approach. This means it trains
    an SVM to classify “0” versus “1–9”, then “1” versus “0, 2–9”, and so on, for
    a total of only 10 classifiers, one for each digit.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: It’s with the SVM classifiers that we see a definite benefit to scaling the
    data versus the raw byte inputs. We also see that the optimal *C* value is likely
    between *C* = 0.1 and *C* = 1.0\. Note that simple [0,1) scaling leads to SVM
    models that outperform (for this one dataset!) the models trained on the normalized
    data. The effect is small but consistent for different *C* values. And, as we
    saw before, dropping the dimensionality from 784 features to only 15 features
    via PCA leads to a rather large loss of accuracy. PCA seems not to have helped
    in this case. We’ll come back to it in a bit and see if we can understand why.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Runtimes
  id: totrans-598
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s now look at the runtime performance of the algorithms. [Table 7-11](ch07.xhtml#ch7tab11)
    shows the train and test times, in seconds, for each model type and dataset version.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: Look at the test times. This is how long each model takes to classify all 10,000
    digit images in the test set. The first thing that jumps out at us is that *k*-NN
    is slow. Classifying the test set takes over 10 minutes when full feature vectors
    are used! It’s only when we drop down to the first 15 PCA components that we see
    reasonable *k*-NN runtimes. This is a good example of the price we pay for a seemingly
    simple idea. Recall, the *k*-NN classifier finds the *k* closest training samples
    to the unknown sample we wish to classify. Here *closest* means in a Euclidean
    sense, like the distance between two points on a graph, except in this case we
    don’t have two or three dimensions but 784.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-11:** Training and Testing Times (Seconds) for Each Model Type'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Raw [0,255]** | **Scaled [0,1)** | **Normalized** | **PCA** |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '| **Model** | train | test | train | test | train | test | train | test |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.23 | 0.03 | 0.24 | 0.03 | 0.24 | 0.03 | 0.01 | 0.00
    |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '| *K*-NN (*K* = 3) | 33.24 | 747.34 | 33.63 | 747.22 | 33.66 | 699.58 | 0.09
    | 3.64 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: '| *K*-NN (*K* = 7) | 33.45 | 746.00 | 33.69 | 746.65 | 33.68 | 709.62 | 0.09
    | 4.65 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.80 | 0.88 | 0.85 | 0.90 | 0.83 | 0.94 | 0.02 | 0.01 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 25.42 | 0.03 | 25.41 | 0.02 | 25.42 | 0.02 | 2.10 | 0.00
    |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 2.65 | 0.06 | 2.70 | 0.06 | 2.61 | 0.06 | 1.20 | 0.03
    |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 25.56 | 0.46 | 25.14 | 0.46 | 25.27 | 0.46 | 12.06 |
    0.25 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (500) | 252.65 | 4.41 | 249.69 | 4.47 | 249.19 | 4.45 | 121.10
    | 2.51 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (1000) | 507.52 | 8.86 | 499.23 | 8.71 | 499.10 | 8.91 | 242.44
    | 5.00 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.01) | 169.45 | 0.02 | 5.93 | 0.02 | 232.93 | 0.02 | 16.91
    | 0.00 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.1) | 170.58 | 0.02 | 36.00 | 0.02 | 320.17 | 0.02 | 37.46
    | 0.00 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 1.0) | 170.74 | 0.02 | 96.34 | 0.02 | 488.06 | 0.02 | 66.49
    | 0.00 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 10.0) | 170.46 | 0.02 | 154.34 | 0.02 | 541.69 | 0.02 | 86.87
    | 0.00 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
- en: Therefore, for each of the test samples, we need to find the *k* = 3 or *k*
    = 7 closest points in the training data. The naïve way to do this is to calculate
    the distance between the unknown sample and each of the 60,000 training samples,
    sort them, look at the *k* smallest distances, and vote to decide the output class
    label. This is a lot of work because we have 60,000 training samples and 10,000
    test samples for a total of 600,000,000 distance calculations. It isn’t as bad
    as all that because sklearn automatically selects the algorithm used to find the
    nearest neighbors, and decades of research has uncovered “better than brute force”
    approaches. Curious readers will want to investigate the terms *K-D-tree* and
    *Ball tree* (sometimes called *Metric tree*). See “An Empirical Comparison of
    Exact Nearest Neighbor Algorithms” by Kibriya and Frank (2007). Still, because
    of the extreme difference in runtimes between the other model types and *k*-NN,
    it’s necessary to remember just how slow *k*-NN can be if the dataset is large.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: The next slowest test times are for the Random Forest classifiers. We understand
    why the forest with 500 trees takes 10 times longer to run than the forest with
    50 trees; we have 10 times as many trees to evaluate. Training times also scale
    linearly. Reducing the size of the feature vectors with PCA improves things but
    not by a factor of 50 (784 features divided by 15 PCA features ≈ 50), so the performance
    difference is not primarily influenced by the size of the feature vector.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: The linear SVMs are the next slowest to train after the Random Forests, but
    their execution time is extremely low. Long training times and short classification
    (inference) times are a hallmark of many model types. The simplest models are
    quick to train and quick to use, like Nearest Centroid or Naïve Bayes, but in
    general, “slow to train, quick to use” is a safe assumption. It’s especially true
    of neural networks.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA hurt the performance of the models except for the Naïve Bayes classifier.
    Let’s do an experiment to see the effect of PCA as the number of PCA components
    changes.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with PCA Components
  id: totrans-623
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For [Tables 7-10](ch07.xhtml#ch7tab10) and [7-11](ch07.xhtml#ch7tab11), we selected
    15 PCA components that represented about 33 percent of the variance in the dataset.
    This value was selected at random. You could imagine training models using some
    other number of principal components.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the effect of the number of PCA components used on the accuracy
    of the resulting model. We’ll vary the number of components from 10 to 780, which
    is basically all the features in the image. For each number of components, we’ll
    train a Naïve Bayes classifier, a Random Forest of 50 trees, and a linear SVM
    with *C* = 1.0\. The code to do this is in [Listing 7-9](ch07.xhtml#ch7lis9).
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: x_train = np.load("../data/mnist/mnist_train_vectors.npy")
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: .astype("float64")
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.load("../data/mnist/mnist_train_labels.npy")
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("../data/mnist/mnist_test_vectors.npy").astype("float64")
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("../data/mnist/mnist_test_labels.npy")
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: m = x_train.mean(axis=0)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: s = x_train.std(axis=0) + 1e-8
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: x_ntrain = (x_train - m) / s
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: x_ntest  = (x_test - m) / s
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: n = 78
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: pcomp = np.linspace(10,780,n, dtype="int16")
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: nb=np.zeros((n,4))
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: rf=np.zeros((n,4))
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: sv=np.zeros((n,4))
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: tv=np.zeros((n,2))
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: 'for i,p in enumerate(pcomp):'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: ❶ pca = decomposition.PCA(n_components=p)
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(x_ntrain)
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: (*\newpage*)
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: xtrain = pca.transform(x_ntrain)
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: xtest = pca.transform(x_ntest)
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: tv[i,:] = [p, pca.explained_variance_ratio_.sum()]
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: ❷ sc,etrn,etst =run(xtrain, y_train, xtest, y_test, GaussianNB())
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: nb[i,:] = [p,sc,etrn,etst]
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: sc,etrn,etst =run(xtrain, y_train, xtest, y_test,
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: rf[i,:] = [p,sc,etrn,etst]
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: sc,etrn,etst =run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0))
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: sv[i,:] = [p,sc,etrn,etst]
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_tv.npy", tv)
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_nb.npy", nb)
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_rf.npy", rf)
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_sv.npy", sv)
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-9: Model accuracy as a function of the number of PCA components
    used. See* mnist_pca.py.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: First, we load the MNIST dataset and compute the normalized version. This is
    the version that we’ll use with PCA. Next, we set up storage for the results.
    The variable pcomp stores the specific number of PCA components that will be used
    from 10 to 780 in steps of 10\. Then we start a loop over the number PCA components.
    We find the requested number of components (p) and map the dataset to the actual
    dataset trained and tested (xtrain, xtest) ❶.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: We also store the actual amount of variance in the dataset explained by the
    current number of principal components (tv). We’ll plot this value later to see
    how quickly the number of components covers the majority of the variance in the
    dataset.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: Next, we train and test a Gaussian Naïve Bayes classifier using the current
    number of features ❷. The run function called here is virtually identical to that
    used in [Listing 7-7](ch07.xhtml#ch7lis7) except that it returns the score, the
    training time, and the testing time. These are captured and put into the appropriate
    output array (nb). Then we do the same for the Random Forest and linear SVM.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: When the loop completes, we have all the data we need and we store the NumPy
    arrays on disk for plotting. Running this code takes some time, but the output,
    when plotted, leads to [Figure 7-2](ch07.xhtml#ch7fig2).
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: The solid curve shows the fraction of the total variance in the dataset explained
    by the current number of PCA components (x-axis). This curve will reach a maximum
    of 1.0 when all the features in the dataset are used. It’s helpful in this case
    because it shows how quickly adding new components explains major orientations
    of the data. For MNIST, we see that about 90 percent of the variance is explained
    by using less than half the possible number of PCA components.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig02.jpg)'
  id: totrans-666
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-2: Results of the PCA search*'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: The remaining three curves plot the accuracy of the resulting models on the
    test data. The best-performing model, in this case, is the Random Forest with
    50 trees (triangles). This is followed by the linear SVM (squares) and then Naïve
    Bayes (circles). These curves show how the number of PCA components tracks with
    accuracy, and while the Random Forest and SVM change only slowly as PCA changes,
    we see that the Naïve Bayes classifier rapidly loses accuracy as the number of
    PCA components increases. Even the Random Forest and SVM decrease as the number
    of PCA components increases, which we might expect because the curse of dimensionality
    will eventually creep in. It seems likely that the dramatically different behavior
    of the Naïve Bayes classifier is due to violations of the independence assumption
    as the number of components used increases.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: The maximum accuracy and the number of PCA components where it occurs are shown
    in [Table 7-12](ch07.xhtml#ch7tab12).
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-12:** Maximum Accuracy on MNIST by Model and Number of Components'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Accuracy** | **Components** | **Variance** |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.81390 | 20 | 0.3806 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.94270 | 100 | 0.7033 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| Linear SVM (C = 1.0) | 0.91670 | 370 | 0.9618 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '[Table 7-12](ch07.xhtml#ch7tab12) tracks with the plot in [Figure 7-2](ch07.xhtml#ch7fig2).
    Interestingly, the SVM does not reach a maximum until nearly all the features
    in the original dataset are used. Also, the best accuracy found for the Random
    Forest and SVM is not as good as seen previously for other versions of the dataset
    that did not use PCA. So, for these models, PCA is not a benefit; it is, however,
    for the Naïve Bayes classifier.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: Scrambling Our Dataset
  id: totrans-677
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before we leave this section, let’s look at one more experiment that we’ll
    come back to in [Chapter 9](ch09.xhtml#ch09) and again in [Chapter 12](ch12.xhtml#ch12).
    In [Chapter 5](ch05.xhtml#ch05), we made a version of the MNIST dataset that scrambled
    the order of the pixels in the digit images. The scrambling wasn’t random: the
    same pixel in each input image was moved to the same position in the output image,
    resulting in images that, at least to us, no longer look like the original digit,
    as [Figure 7-3](ch07.xhtml#ch7fig3) shows. How might this scrambling affect the
    accuracy of the models we’ve been using in this chapter?'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig03.jpg)'
  id: totrans-679
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-3: Original MNIST digits (top) and scrambled versions of the same
    digit (bottom).*'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: Let’s repeat the experiment code of [Listing 7-8](ch07.xhtml#ch7lis8), this
    time running only the scaled [0,1) version of the scrambled MNIST images. Since
    the only difference to the original code is the source filenames and the fact
    that we call run only once, we’ll forgo a new listing.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: Placing the accuracy results side by side gives us [Table 7-13](ch07.xhtml#ch7tab13).
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-13:** MNIST Scores by Model Type for Unscrambled and Scrambled Digits'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Unscrambled [0,1)** | **Scrambled [0,1)** |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.8203 | 0.8203 |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
- en: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.5558 | 0.5558 |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.8784 | 0.8772 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.9244 | 0.9214 |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.9661 | 0.9651 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (500) | 0.9709 | 0.9721 |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (1000) | 0.9716 | 0.9711 |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.01) | 0.9171 | 0.9171 |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.1) | 0.9181 | 0.9181 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 1.0) | 0.9182 | 0.9185 |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 10.0) | 0.9019 | 0.8885 |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
- en: Here we see virtually no difference between the scrambled and unscrambled results.
    In fact, for several models, the results are identical. For stochastic models,
    like the Random Forests, the results are still very similar. Is this surprising?
    Perhaps at first, but if we think about it for a bit, we realize that it really
    shouldn’t be.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the classic models are holistic: they operate on the entire feature
    vector as a single entity. While we can’t see the digits anymore because our vision
    does not operate holistically, the *information* present in the image is still
    there, so the models are just as happy with the scrambled as unscrambled inputs.
    When we get to [Chapter 12](ch12.xhtml#ch12), we’ll encounter a different result
    of this experiment.'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: Classical Model Summary
  id: totrans-701
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What follows is a summary of the pros and cons related to each of the classical
    model types we have explored in this chapter. This can be used as a quick list
    for future reference. It will also take some of the observations we made via our
    experiments and make them more concrete.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: Nearest Centroid
  id: totrans-703
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the simplest of all the models and can serve as a baseline. It’s seldom
    adequate unless the task at hand is particularly easy. The single centroid for
    each class is needlessly restrictive. You could use a more generalized approach
    that first finds an appropriate number of centroids for each class and then groups
    them together to build the classifier. In the extreme, this approaches *k*-NN
    but is still simpler in that the number of centroids is likely far less than the
    number of training samples. We’ll leave the implementation of this variation as
    an exercise for the motivated reader.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-705
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we saw in this chapter, the implementation of a Nearest Centroid classifier
    takes only a handful of code. Additionally, Nearest Centroid is not restricted
    to binary models and readily supports multiclass models, like the irises. Training
    is very fast and since only one centroid is stored per class, the memory overhead
    is likewise very small. When used to label an unknown sample, run time is also
    very small because the distance from the sample to each class centroid is all
    that needs to be computed.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Nearest Centroid makes a simplistic assumption about the distribution of the
    classes in the feature space—one that’s seldom met in practice. As a consequence
    of this assumption, the Nearest Centroid classifier is only highly accurate when
    the classes form a single tight group in the feature space and the groups are
    distant from each other like isolated islands.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-Nearest Neighbors'
  id: totrans-709
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the simplest model to train since there’s no training: we store the
    training set and use it to classify new instances by finding the *k* nearest training
    set vectors and voting.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-711
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As just mentioned, no training required makes *k*-NN particularly attractive.
    It also can perform quite well, especially if the number of training samples is
    large relative to the dimensionality of the feature space (that is, the number
    of features in the feature vector). Multiclass support is implicit and doesn’t
    require a special approach.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-713
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The simplicity of “training” comes at a cost: classification is slow because
    of the need to look at every training example to find the nearest neighbors to
    the unknown feature vector. Decades of research, still underway, have sped up
    the search to improve the naïve implementation of looking at every training sample
    every time, but, as we saw in this chapter, classification is still slow, especially
    when compared to the speed of other model types (for example, SVM).'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes
  id: totrans-715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This model is conceptually simple and efficient, and surprisingly valid even
    when the core assumption of feature independence isn’t met.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-717
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Naïve Bayes is fast to train and fast to classify with, both positives. It also
    supports multiclass models instead of just binary, and other than continuous features.
    As long as the probability of a particular feature value can be computed, we can
    apply Naïve Bayes.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-719
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The feature independence assumption central to Naïve Bayes is seldom true in
    practice. The more correlated the features (the more a change in, say, feature
    *x*[2] implies that *x*[3] will change), the poorer the performance of the model
    (in all likelihood).
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: While Naïve Bayes works directly with discrete valued features, using continuous
    features often involves a second level of assumption, as when we assumed that
    the continuous breast cancer dataset features were well represented as samples
    from a Gaussian distribution. This second assumption, which is also likely seldom
    true in practice, means that we need to estimate the parameters of the distribution
    from the dataset instead of using histograms to stand in for the actual feature
    probabilities.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  id: totrans-722
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This model is useful when it’s important to be able to understand, in human
    terms, why a particular class was selected.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Decision Trees are reasonably fast to train. They’re also fast to use for classifying.
    Multiclass models are not a problem and are not restricted to using just continuous
    features. A Decision Tree can justify its answer by showing the particular steps
    used to reach a decision: the series of questions asked from the root to the leaf.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-726
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Decision Trees are prone to overfitting—to learning elements of the training
    data that are not generally true of the parent distribution. Also, interpretability
    degrades as the tree increases in size. Tree depth needs to be balanced with the
    quality of the decisions (labels) as the leaves of the tree. This directly affects
    the error rate.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  id: totrans-728
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a more powerful form of Decision Tree that uses randomness to reduce
    the overfitting problem. Random Forests are one of the best performing of the
    classic model types and apply to a wide range of problem domains.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-730
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Like Decision Trees, Random Forests support multiclass models and other than
    continuous features. They are reasonably fast to train and to use for inference.
    Random Forests are also robust to differences in scale between features in the
    feature vector. In general, the accuracy improves, with diminishing returns, as
    the size of the forest grows.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-732
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The easy interpretability of a Decision Tree disappears with a Random Forest.
    While each tree in the forest can justify its decision, the combined effect of
    the forest as a whole can be difficult to understand.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
- en: The inference runtime of a forest scales linearly with the number of trees.
    However, this can be mitigated by parallelization since each tree in the forest
    is making a calculation that does not depend on any other tree until combining
    the output of all trees to make an overall decision.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: As stochastic models, the overall performance of a forest varies from training
    session to training session for the same dataset. In general, this isn’t an issue,
    but a pathological forest could exist—if possible, train the forest several times
    to get a sense of the actual performance.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  id: totrans-736
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before the “rebirth” of neural networks, Support Vector Machines were generally
    considered to provide the pinnacle of model performance when they were applicable
    and well-tuned.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SVMs can give show excellent performance when properly tuned. Inference is very
    fast once trained.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-740
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multiclass models are not directly supported. Extensions for multiclass problems
    require training multiple models whether using one-versus-one or one-versus-rest
    approaches. Additionally, SVMs expect only continuous features and feature scaling
    matters; normalization or other scaling is often necessary to get good performance.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: Large datasets are difficult to train when using other than linear kernels,
    and SVMs often require careful tuning of margin and kernel parameters (*C*, *γ*),
    though this can be mitigated somewhat by search algorithms that seek the best
    hyperparameter values.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Classical Models
  id: totrans-743
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The classical models may be *classic*, but they are still appropriate under
    the right conditions. In this section, we’ll discuss when you should consider
    a classical model instead of a more modern approach.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: Handling Small Datasets
  id: totrans-745
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the best reasons for working with a classic model is when the dataset
    is small. If you have only a few tens or hundreds of examples, then a classic
    model might be a good fit, whereas a deep learning model might not have enough
    training data to condition itself to the problem. Of course, there are exceptions.
    A deep neural network can, via transfer learning, sometimes learn from relatively
    few examples. Other approaches, like zero-shot or few-shot learning, may also
    allow a deep network to learn from a small dataset. However, these techniques
    are far beyond the scope of what we want to address in this book. For us, the
    rule of thumb is: when the dataset is small, consider using a classic model.'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Reduced Computational Requirements
  id: totrans-747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another reason to consider a classic model is when computational requirements
    must be kept to a minimum. Deep neural networks are notoriously demanding of computational
    resources. The thousands, millions, and even billions of connections in a deep
    network all require extensive calculation. Implementing such a model on a small
    handheld device, or on an embedded microcontroller, will not work, or at least
    not work in any reasonable timeframe.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: 'In such cases, you might consider a classic model that doesn’t require a lot
    of overhead. Simple models like Nearest Centroid or Naïve Bayes are good candidates.
    So are Decision Trees and Support Vector Machines, once trained. From the previous
    experiments, *k*-NN is probably not a good candidate unless the feature space
    or training set is small. This leads to our next rule of thumb: when computation
    must be kept to a minimum, consider using a classic model.'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: Having Explainable Models
  id: totrans-750
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some classic models can explain themselves by revealing exactly *how* they
    arrived at their answer for a given unknown input. This includes Decision Trees,
    by design, but also *k*-NN (by showing the labels of the *k* voters), Nearest
    Centroid (by virtue of the selected centroid), and even Naïve Bayes (by the selected
    posterior probability). By way of contrast, deep neural networks are black boxes—they
    do not explain themselves—and it’s an active area of research to learn how to
    get a deep network to give some reason for its decision. This research has not
    been entirely unsuccessful, to be sure, but it’s still far from looking like the
    decision path in a tree classifier. Therefore, we can give another rule of thumb:
    when it’s essential to know how the classifier makes its decision, consider using
    a classic model.'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: Working with Vector Inputs
  id: totrans-752
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our final rule of thumb, acknowledging, of course, that there are indeed others
    we could give, has to do with the form of the inputs to the model. Modern deep
    learning systems often work with inputs that are not an amalgamation of separate
    features put into a single vector but instead are multidimensional inputs, such
    as images, where the “features” (pixels) are not different from each other but
    of the same kind and often highly correlated (the red pixel of the apple likely
    has a red pixel next to it, for example). A color image is a three-dimensional
    beast: there are three color images, one for the red channel, one for the blue
    channel, and one for the green channel. If the inputs are images from other sources,
    like satellites, there might be four to eight or more channels per image. A convolutional
    neural network is designed precisely for inputs such as these and will look for
    spatial patterns characteristic of the classes the network is trying to learn
    about. See [Chapter 12](ch12.xhtml#ch12) for more details.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: 'But if the input to the model is a vector, especially a vector where the particular
    features are not related to each other (the key assumption of the Naïve Bayes
    classifier), then a classic model might be appropriate, since there’s no need
    to look for structure among the features beyond the global interpretation that
    the classic models perform by considering the input as a single monolithic entity.
    Therefore, we might give the rule as: when the input is a feature vector without
    spatial structure (unlike an image), especially if the features are not related
    to each other, consider using a classic model.'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that these are rule-of-thumb suggestions, and that
    they aren’t always applicable to a particular problem. Also, it’s possible to
    use deep networks even if these rules seem to apply; it’s just that they may not
    give the best performance, or might be overkill, like using a shotgun to kill
    a fly. The main point of this book is to build intuition so that when a situation
    arises, we’ll know how to use the techniques we are exploring to maximum advantage.
    Pasteur said, “In the fields of observation, chance favors only the prepared mind”
    (lecture at the University of Lille, December 1854), and we wholeheartedly agree.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-756
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we worked with six common classical machine learning models:
    Nearest Centroid, *k*-Nearest Neighbors, Naïve Bayes, Decision Trees, Random Forests,
    and Support Vector Machines. We applied them to three datasets that were developed
    in [Chapter 5](ch05.xhtml#ch05): irises, breast cancer, and MNIST digits. We used
    the results of the experiments with these datasets to gain insight into the strengths
    and weaknesses of each model type along with the effect of different data preprocessing
    steps. We ended the chapter with a discussion of the classic models and when it
    might be appropriate to use them.'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll move on from the classic models and begin our exploration
    of neural networks, the backbone of modern deep learning.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
