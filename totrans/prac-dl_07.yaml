- en: '**7'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**7'
- en: EXPERIMENTS WITH CLASSICAL MODELS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型实验**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In [Chapter 6](ch06.xhtml#ch06), we introduced several classical machine learning
    models. Let’s now take the datasets we built in [Chapter 5](ch05.xhtml#ch05) and
    use them with these models to see how well they perform. We’ll use sklearn to
    create the models and then we’ll compare them by looking at how well they do on
    the held-out test sets.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#ch06)中，我们介绍了几种经典的机器学习模型。现在，让我们使用在[第5章](ch05.xhtml#ch05)中构建的数据集，并将其与这些模型一起使用，看看它们的表现如何。我们将使用sklearn来创建模型，然后通过查看它们在保留的测试集上的表现来进行比较。
- en: 'This will give us a good overview of how to work with sklearn and help us build
    intuition about how the different models perform relative to one another. We’ll
    use three datasets: the iris dataset, both original and augmented; the breast
    cancer dataset; and the vector form of the MNIST handwritten digits dataset.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供一个关于如何使用sklearn的良好概览，并帮助我们建立关于不同模型相互表现的直观感受。我们将使用三个数据集：鸢尾花数据集，包括原始和增强版；乳腺癌数据集；以及MNIST手写数字数据集的向量形式。
- en: Experiments with the Iris Dataset
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鸢尾花数据集实验
- en: 'We’ll start with the iris dataset. This data set has four continuous features—the
    measurements of the sepal length, sepal width, petal length, and petal width—and
    three classes—different iris species. There are 150 samples, 50 each from the
    three classes. In [Chapter 5](ch05.xhtml#ch05), we applied PCA augmentation to
    the dataset, so we actually have two versions we can work with: the original 150
    samples and the 1200 augmented training samples. Both can use the same test set.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从鸢尾花数据集开始。这个数据集有四个连续特征——萼片长度、萼片宽度、花瓣长度和花瓣宽度——以及三个类别——不同的鸢尾花种类。总共有150个样本，每个类别50个样本。在[第5章](ch05.xhtml#ch05)中，我们对数据集应用了PCA增强，因此我们实际上有两个版本可以使用：原始的150个样本和1200个增强的训练样本。两者都可以使用相同的测试集。
- en: We’ll use sklearn to implement versions of the Nearest Centroid, *k*-NN, Naïve
    Bayes, Decision Tree, Random Forest, and SVM models we outlined in [Chapter 6](ch06.xhtml#ch06).
    We’ll quickly see how powerful and elegant the sklearn toolkit is since our tests
    are virtually all identical across the models. The only thing that changes is
    the particular class we instantiate.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用sklearn实现我们在[第6章](ch06.xhtml#ch06)中概述的最近质心、*k*-NN、朴素贝叶斯、决策树、随机森林和SVM模型的版本。我们将很快看到sklearn工具包是多么强大和优雅，因为我们的测试几乎在所有模型中都是相同的。唯一变化的是我们实例化的具体类。
- en: Testing the Classical Models
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试经典模型
- en: The code for our initial tests is in [Listing 7-1](ch07.xhtml#ch7lis1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初步测试的代码在[清单7-1](ch07.xhtml#ch7lis1)中。
- en: import numpy as np
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: '❶ def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: print("    predictions  :", clf.predict(x_test))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: print("    预测结果  :", clf.predict(x_test))
- en: print("    actual labels:", y_test)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: print("    实际标签：", y_test)
- en: print("    score = %0.4f" % clf.score(x_test, y_test))
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: print("    score = %0.4f" % clf.score(x_test, y_test))
- en: print()
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'def main():'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../data/iris/iris_features.npy")
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../data/iris/iris_features.npy")
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]; x_test = x[N:]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]; x_test = x[N:]
- en: y_train = y[:N]; y_test = y[N:]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]; y_test = y[N:]
- en: ❸ xa_train=np.load("../data/iris/iris_train_features_augmented.npy")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xa_train=np.load("../data/iris/iris_train_features_augmented.npy")
- en: ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")
- en: xa_test =np.load("../data/iris/iris_test_features_augmented.npy")
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: xa_test =np.load("../data/iris/iris_test_features_augmented.npy")
- en: ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")
- en: print("Nearest Centroid:")
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: print("最近质心：")
- en: ❹ run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: print("k-NN classifier (k=3):")
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN 分类器（k=3）：")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: print("Naive Bayes classifier (Gaussian):")
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器（高斯）：")
- en: ❺ run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ run(x_train, y_train, x_test, y_test, GaussianNB())
- en: print("Naive Bayes classifier (Multinomial):")
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器（多项式）：")
- en: run(x_train, y_train, x_test, y_test, MultinomialNB())
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, MultinomialNB())
- en: ❻ print("Decision Tree classifier:")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ print("决策树分类器：")
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
- en: print("Random Forest classifier (estimators=5):")
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: print("随机森林分类器（估计器=5）：")
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: ❼ print("SVM (linear, C=1.0):")
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ print("支持向量机（线性, C=1.0）：")
- en: run(xa_train, ya_train, xa_test, ya_test, SVC(kernel="linear", C=1.0))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test, SVC(kernel="linear", C=1.0))
- en: print("SVM (RBF, C=1.0, gamma=0.25):")
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: print("支持向量机（RBF, C=1.0, gamma=0.25）：")
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.25))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.25))
- en: print("SVM (RBF, C=1.0, gamma=0.001, augmented)")
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: print("支持向量机（RBF, C=1.0, gamma=0.001, 增强）")
- en: run(xa_train, ya_train, xa_test, ya_test,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: run(xa_train, ya_train, xa_test, ya_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.001))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.001))
- en: ❽ print("SVM (RBF, C=1.0, gamma=0.001, original)")
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ print("支持向量机（RBF, C=1.0, gamma=0.001, 原始）")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.001))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.001))
- en: '*Listing 7-1: Classic models using the iris dataset. See* iris_experiments.py.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7-1：使用鸢尾花数据集的经典模型。见* iris_experiments.py。'
- en: 'First, we import the necessary classes and modules. Notice that each of the
    classes represents a single type of model (classifier). For the Naïve Bayes classifier,
    we’re using two versions: the Gaussian version, GaussianNB, because the features
    are continuous values, and MultinomialNB for the discrete case to illustrate the
    effect of choosing a model that’s inappropriate for the dataset we’re working
    with. Because sklearn has a uniform interface for its classifiers, we can simplify
    things by using the same function to train and test any particular classifier.
    That function is run ❶. We pass in the training features (x_train) and labels
    (y_train) along with the test features and labels (x_test, y_test). We also pass
    in the particular classifier object (clf).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的类和模块。注意，每个类都代表一种单一类型的模型（分类器）。对于朴素贝叶斯分类器，我们使用了两个版本：高斯版本GaussianNB，因为特征是连续值，和MultinomialNB用于离散情况，以展示选择一个不适合数据集的模型的效果。由于sklearn为其分类器提供了统一的接口，我们可以通过使用相同的函数来简化操作，训练和测试任何特定的分类器。这个函数是run
    ❶。我们传入训练特征（x_train）和标签（y_train），以及测试特征和标签（x_test, y_test）。我们还传入特定的分类器对象（clf）。
- en: The first thing we do inside run is fit the model to the data by calling fit
    with the training data samples and labels. This is the training step. After the
    model is trained, we can test how well it does by calling the predict method with
    the held-out test data. This method returns the predicted class label for each
    sample in the test data. We held back 30 samples from the original 150 so predict
    will return a vector of 30 class label assignments, which we print. Next, we print
    the actual test labels so we can compare them visually with the predictions. Finally,
    we use the score method to apply the classifier to the test data (x_test) using
    the known test labels (y_test) to calculate the overall accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在run函数内做的第一件事是通过调用fit方法将模型拟合到数据上，使用训练数据样本和标签进行训练。这是训练步骤。模型训练后，我们可以通过调用predict方法并使用保留的测试数据来测试模型的表现。该方法会返回每个测试样本的预测类别标签。我们从150个原始样本中保留了30个，所以predict会返回一个包含30个类别标签的向量，我们将其打印出来。接下来，我们打印实际的测试标签，以便与预测结果进行视觉上的对比。最后，我们使用score方法，将分类器应用于测试数据（x_test），并使用已知的测试标签（y_test）来计算总体准确率。
- en: The accuracy is returned as a fraction between 0 and 1\. If every test sample
    were given the wrong label, the accuracy would be 0\. Even random guessing will
    do better than that, so a return value of 0 is a sign that something is amiss.
    Since there are three classes in the iris dataset, we’d expect a classifier that
    guesses the class at random to be right about one-third of the time and return
    a value close to 0.3333\. The actual score is calculated as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率返回的是一个介于0和1之间的分数。如果每个测试样本都被标记为错误，准确率将为0。即使是随机猜测，也能比这做得更好，所以返回值为0意味着出了问题。由于鸢尾花数据集有三个类别，我们期望一个随机猜测类别的分类器能正确约三分之一的时间，返回一个接近0.3333的值。实际得分的计算公式为：
- en: score = *N[c]*/(*N[c]* + *N[w]*)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: score = *N[c]* / (*N[c]* + *N[w]*)
- en: where *N*[*c*] is the number of test samples for which the predicted class is
    correct; that is, it matches the class label in y_test. *N*[*w*] is the number
    of test samples where the predicted class does not match the actual class label.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N*[*c*] 是预测类正确的测试样本数量；也就是说，它与y_test中的类标签匹配。*N*[*w*] 是预测类与实际类标签不匹配的测试样本数量。
- en: Now that we have a way to train and test each classifier, all we need to do
    is load the datasets and run a series of experiments by creating different classifier
    objects and passing them to run. Back inside of main, we begin by loading the
    original iris dataset and separating it into train and test cases ❷. We also load
    the augmented iris dataset that we created in [Chapter 5](ch05.xhtml#ch05) ❸.
    By design, the two test sets are identical, so regardless of which training set
    we use, the test set will be the same. This simplifies our comparisons.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种训练和测试每个分类器的方法，接下来只需要加载数据集并通过创建不同的分类器对象并传递给运行函数来进行一系列实验。在main函数中，我们首先加载原始的鸢尾花数据集，并将其分为训练集和测试集❷。我们还加载了我们在[第5章](ch05.xhtml#ch05)中创建的增强版鸢尾花数据集❸。设计上，这两个测试集是相同的，所以无论使用哪个训练集，测试集都会是一样的。这简化了我们的比较。
- en: 'We then define and execute the Nearest Centroid classifier ❹. The output is
    shown here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义并执行最近质心分类器❹。输出如下：
- en: 'Nearest Centroid:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最近质心分类器：
- en: predictions  :[011202120211112202201101102211]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果  ：[011202120211112202201101102211]
- en: actual labels:[011202120211112202201101102211]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际标签：[011202120211112202201101102211]
- en: score = 1.0000
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 得分 = 1.0000
- en: We’ve removed spaces to make a visual comparison between the predicted and actual
    class labels easier. If there’s an error, the corresponding value, 0–2, will not
    match between the two lines. The score is also shown. In this case, it’s 1.0,
    which tells us that the classifier was perfect in its predictions on the held-out
    test set. This isn’t surprising; the iris dataset is a simple one. Because the
    iris dataset was randomized when created in [Chapter 5](ch05.xhtml#ch05), you
    might get a different overall score. However, unless your randomization was particularly
    unfortunate, you should have a high test score.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们去掉了空格，以便更直观地比较预测标签和实际标签。如果有错误，0–2的相应值在两行之间将不匹配。得分也会显示出来。在这个例子中，得分为1.0，这告诉我们分类器在保留的测试集上做得非常好。这个结果并不令人惊讶；鸢尾花数据集是一个简单的数据集。由于在[第5章](ch05.xhtml#ch05)中创建鸢尾花数据集时进行了随机化，你可能会得到一个不同的总得分。然而，除非你的随机化过程特别不幸运，否则你应该会得到一个很高的测试得分。
- en: Based on what we learned in [Chapter 6](ch06.xhtml#ch06), we should expect that
    if the Nearest Centroid classifier is perfect on the test data, then all the other
    more sophisticated models will likewise be perfect. This is generally the case
    here, but as we’ll see, careless selection of model type or model hyperparameter
    values will result in inferior performance even from a more sophisticated model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在[第6章](ch06.xhtml#ch06)学到的内容，我们应该预期，如果最近的质心分类器在测试数据上完美，那么所有其他更复杂的模型也会同样完美。这里通常是这样，但正如我们将看到的，不小心选择模型类型或模型超参数值，甚至更复杂的模型也会表现较差。
- en: 'Look again at [Listing 7-1](ch07.xhtml#ch7lis1), where we train a Gaussian
    Naïve Bayes classifier by passing an instance of GaussianNB to run ❺. This classifier
    is also perfect and returns a score of 1.0\. This is the correct way to use continuous
    values with a Naïve Bayes classifier. What happens if we instead use the discrete
    case even though we have continuous features? This is the MultinomialNB classifier,
    which assumes the features are selected from a discrete set of possible values.
    For the iris dataset, we can get away with defining such a classifier because
    the feature values are non-negative. However, because the features are not discrete,
    this model is not perfect and returns the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看[Feyman学习法](ch07.xhtml#ch7lis1)，我们通过传递GaussianNB实例来训练一个高斯朴素贝叶斯分类器并运行❺。该分类器也是完美的，并返回一个1.0的分数。这是使用连续值与朴素贝叶斯分类器的正确方法。如果我们用离散情况，而我们拥有的是连续特征，结果会怎么样呢？这就是MultinomialNB分类器，它假设特征是从一个离散的可能值集合中选择的。对于鸢尾花数据集，我们可以定义这样的分类器，因为特征值是非负的。然而，由于特征不是离散的，这个模型并不完美，返回了以下结果：
- en: 'Naive Bayes classifier (Multinomial):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器（多项式）：
- en: predictions  :[011202220211122202202101102221]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果  ：[011202220211122202202101102221]
- en: actual labels:[011202120211112202201101102211]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 实际标签：[011202120211112202201101102211]
- en: score = 0.8667
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 得分 = 0.8667
- en: Here we see that the classifier is only 86.7 percent accurate on our test samples.
    If we need discrete counts for the probabilities, why did this approach work at
    all in this case? The answer is evident in the sklearn source code for the MultinomialNB
    classifier. The method that counts feature frequencies per class uses np.dot so
    that even if the feature values are continuous, the output will be a valid number,
    though not an integer. Still, mistakes were made, so we shouldn’t be happy. We
    should instead be careful to select the proper classifier type for the actual
    data we’re working with.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到分类器在测试样本上的准确率仅为86.7%。如果我们需要离散的概率计数，为什么这种方法在这种情况下仍然有效呢？答案可以在sklearn的MultinomialNB分类器源代码中找到。该方法通过np.dot计算每个类别的特征频率，即使特征值是连续的，输出也会是一个有效的数字，尽管不是整数。不过，仍然犯了一些错误，所以我们不应该感到高兴。相反，我们应该小心选择适合我们实际数据的分类器类型。
- en: The next model we train in [Listing 7-1](ch07.xhtml#ch7lis1) is a Decision Tree
    ❻. This classifier is perfect on this dataset, as is the Random Forest trained
    next. Note, the Random Forest is using five estimators, meaning five random trees
    are created and trained; voting between the individual outputs determines the
    final class label. Note also that the Random Forest is trained on the augmented
    iris dataset, xa_train, because of the limited number of training samples in the
    unaugmented dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[清单7-1](ch07.xhtml#ch7lis1)中训练的下一个模型是决策树❻。这个分类器在该数据集上表现完美，接下来训练的随机森林也是如此。请注意，随机森林使用了五个估计器，这意味着创建并训练了五棵随机树；通过在各个输出之间投票来确定最终的类别标签。还需要注意，随机森林是使用增强的鸢尾花数据集xa_train进行训练的，因为未增强的数据集中的训练样本数量有限。
- en: 'We then train several SVM classifiers ❼, also on the augmented dataset. Recall
    that SVMs have two parameters we control: the margin constant, C, and gamma used
    by the Gaussian kernel.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们训练了几个SVM分类器❼，同样是在增强的数据集上。回顾一下，SVM有两个我们可以控制的参数：边界常数C和高斯核使用的gamma。
- en: The first is a linear SVM, meaning we need a value for the margin constant (C).
    We define C to be 1.0, the default value for sklearn. This classifier is perfect
    on the test data, as is the following classifier using the Gaussian kernel, for
    which we also set *γ* to 0.25\. The SVC class defaults to auto for gamma, which
    sets *γ* to 1/*n*, where *n* is the number of features. For the iris dataset,
    *n* = 4 so *γ* = 0.25.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是线性支持向量机（SVM），意味着我们需要为边界常数（C）设置一个值。我们将C定义为1.0，这是sklearn的默认值。这个分类器在测试数据上表现完美，接下来的分类器也使用高斯核，并且我们将*γ*设置为0.25。SVC类的默认gamma值是auto，它将*γ*设置为1/*n*，其中*n*是特征的数量。对于鸢尾花数据集，*n*
    = 4，因此*γ* = 0.25。
- en: 'Next, we train a model with very small *γ*. The classifier is still perfect
    on the test data. Lastly, we train the same type of SVM, but instead of the augmented
    training data, we use the original training data ❽. This classifier is not perfect:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练了一个*γ*非常小的模型。该分类器在测试数据上的表现依然完美。最后，我们训练了相同类型的支持向量机（SVM），但是使用的是原始训练数据❽，而不是增强的训练数据。这个分类器并不完美：
- en: SVM (RBF, C=1.0, gamma=0.001, original)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: SVM (RBF, C=1.0, gamma=0.001, 原始数据)
- en: predictions  :[022202020222222202202202202220]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: predictions  :[022202020222222202202202202220]
- en: actual labels:[011202120211112202201101102211]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实际标签：[011202120211112202201101102211]
- en: score = 0.5667
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: score = 0.5667
- en: In fact, it’s rather dismal. It never predicts class 1 and is right only 56.7
    percent of the time. This shows that data augmentation is valuable as it turned
    a lousy classifier into a good one—at least, good as far as we can know from the
    small test set we are using!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，它的表现相当糟糕。它从不预测类别1，且仅56.7%的时间预测正确。这表明数据增强是有价值的，因为它将一个糟糕的分类器转变为一个不错的分类器——至少，从我们使用的小型测试集来看，它表现得还不错！
- en: Implementing a Nearest Centroid Classifier
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现最近质心分类器
- en: What if we were stranded on a deserted island and didn’t have access to sklearn?
    Could we still quickly build a suitable classifier for the iris dataset? The answer
    is “yes,” as [Listing 7-2](ch07.xhtml#ch7lis2) shows. This code implements a quick-and-dirty
    Nearest Centroid classifier for the iris dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们被困在一个荒岛上，没有访问sklearn的权限，能否仍然快速构建一个适合鸢尾花数据集的分类器呢？答案是“能”，正如[清单7-2](ch07.xhtml#ch7lis2)所示。此代码实现了一个快速而简陋的最近质心分类器，用于鸢尾花数据集。
- en: import numpy as np
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: '❶ def centroids(x,y):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def centroids(x,y):'
- en: c0 = x[np.where(y==0)].mean(axis=0)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: c0 = x[np.where(y==0)].mean(axis=0)
- en: c1 = x[np.where(y==1)].mean(axis=0)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: c1 = x[np.where(y==1)].mean(axis=0)
- en: c2 = x[np.where(y==2)].mean(axis=0)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: c2 = x[np.where(y==2)].mean(axis=0)
- en: return [c0,c1,c2]
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: return [c0,c1,c2]
- en: '❷ def predict(c0,c1,c2,x):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ def predict(c0,c1,c2,x):'
- en: p = np.zeros(x.shape[0], dtype="uint8")
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: p = np.zeros(x.shape[0], dtype="uint8")
- en: 'for i in range(x.shape[0]):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(x.shape[0]):'
- en: d = [((c0-x[i])**2).sum(),
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: d = [((c0-x[i])**2).sum(),
- en: ((c1-x[i])**2).sum(),
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ((c1-x[i])**2).sum(),
- en: ((c2-x[i])**2).sum()]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ((c2-x[i])**2).sum()]
- en: p[i] = np.argmin(d)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: p[i] = np.argmin(d)
- en: return p
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: return p
- en: 'def main():'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❸ x = np.load("../data/iris/iris_features.npy")
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x = np.load("../data/iris/iris_features.npy")
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]; x_test = x[N:]
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]; x_test = x[N:]
- en: y_train = y[:N]; y_test = y[N:]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]; y_test = y[N:]
- en: c0, c1, c2 = centroids(x_train, y_train)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: c0, c1, c2 = centroids(x_train, y_train)
- en: p = predict(c0,c1,c2, x_test)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: p = predict(c0,c1,c2, x_test)
- en: nc = len(np.where(p == y_test)[0])
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: nc = len(np.where(p == y_test)[0])
- en: nw = len(np.where(p != y_test)[0])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: nw = len(np.where(p != y_test)[0])
- en: acc = float(nc) / (float(nc)+float(nw))
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: acc = float(nc) / (float(nc)+float(nw))
- en: print("predicted:", p)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: print("预测结果:", p)
- en: print("actual   :", y_test)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: print("实际值   :", y_test)
- en: print("test accuracy = %0.4f" % acc)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: print("测试准确率 = %0.4f" % acc)
- en: '*Listing 7-2: A quick-and-dirty Nearest Centroid classifier for the iris dataset.
    See* iris_centroids.py.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 7-2：为鸢尾花数据集编写的快速简易最近质心分类器。请参见* iris_centroids.py。'
- en: We load the iris data and separate it into train and test sets as before ❸.
    The centroids function returns the centroids of the three classes ❶. We can easily
    calculate these by finding the per feature means of each training sample of the
    desired class. This is all it takes to train this model. If we compare the returned
    centroids with those in the preceding trained NearestCentroid classifier (see
    the centroids_ member variable), we get precisely the same values.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了鸢尾花数据集，并像之前一样将其分为训练集和测试集 ❸。质心函数返回三个类别的质心 ❶。我们可以通过找到每个训练样本中所需类别的每个特征的均值来轻松计算这些质心。这就是训练这个模型所需的全部。如果我们将返回的质心与之前训练的最近质心分类器中的质心（参见
    centroids_ 成员变量）进行比较，我们会得到完全相同的值。
- en: Using the classifier is straightforward, as predict shows ❷. First, we define
    the vector of predictions, one per test sample (x). The loop defines d, a vector
    of Euclidean distances from the current test sample, x[i], to the three class
    centroids. The index of the smallest distance in d is the predicted class label
    (p[i]).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类器非常直接，如predict所示 ❷。首先，我们定义了一个预测向量，每个测试样本（x）对应一个预测值。循环定义了d，这是一个从当前测试样本 x[i]
    到三个类别质心的欧几里得距离向量。d中最小距离的索引即为预测的类别标签（p[i]）。
- en: Let’s unpack d a bit more. We set d to a list of three values, the distances
    from the centroids to the current test sample. The expression
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再详细解释一下d。我们将d设置为一个包含三个值的列表，表示从质心到当前测试样本的距离。这个表达式
- en: ((c0-x[i])**2).sum()
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ((c0-x[i])**2).sum()
- en: is a bit dense. The phrase c0-x[i] returns a vector of four numbers—four because
    we have four features. These are the differences between the centroid of class
    0 and the test sample feature value. This quantity is squared, which squares each
    of the four values. This squared vector is summed, element by element, to return
    the distance measure.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有点密集。表达式 c0-x[i] 返回一个四个数字的向量——四个是因为我们有四个特征。这些是类别 0 的质心与测试样本特征值之间的差异。这个量会被平方，平方每一个值。这个平方后的向量会逐元素求和，得到距离度量。
- en: Strictly speaking, we’re missing a final step. The actual distance between c0
    and x[i] is the square root of this value. Since we’re simply looking for the
    smallest distance to each of the centroids, we don’t need to calculate the square
    root. The smallest value will still be the smallest value, whether we take the
    square root of all the values or not. Running this code produces the same output
    as we saw previously for the Nearest Centroid classifier, which is encouraging.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，我们还缺少最后一步。c0 和 x[i] 之间的实际距离是该值的平方根。由于我们只是寻找到每个质心的最小距离，因此我们不需要计算平方根。无论我们是否对所有值取平方根，最小值仍然是最小值。运行这段代码会产生与之前看到的最近质心分类器相同的输出，这令人鼓舞。
- en: The iris dataset is extremely simple, so we shouldn’t be surprised by the excellent
    performance of our models even though we saw that careless selection of model
    type and hyperparameters will cause us trouble. Let’s now look at a larger dataset
    with more features, one that was not meant as a toy.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集非常简单，因此即使我们看到不小心选择模型类型和超参数会给我们带来麻烦，我们也不应该对模型的优秀表现感到惊讶。现在我们来看一个更大的数据集，它有更多的特征，且并非一个玩具数据集。
- en: Experiments with the Breast Cancer Dataset
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 胸腺癌数据集实验
- en: 'The two-class breast cancer dataset we developed in [Chapter 5](ch05.xhtml#ch05)
    has 569 samples, each with 30 features, all measurements from a histology slide.
    There are 212 malignant cases (class 1) and 357 benign cases (class 0). Let’s
    train our classic models on this dataset and see what sort of results we get.
    As all the features are continuous, let’s use the normalized version of the dataset.
    Recall that a normalized dataset is one where, per feature in the feature vector,
    each value has the mean for that feature subtracted and then is divided by the
    standard deviation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 5 章](ch05.xhtml#ch05)中开发的二分类乳腺癌数据集包含 569 个样本，每个样本有 30 个特征，所有测量来自组织学切片。共有
    212 个恶性病例（类 1）和 357 个良性病例（类 0）。让我们在这个数据集上训练我们的经典模型，看看能得到什么样的结果。由于所有特征都是连续的，我们将使用该数据集的归一化版本。回想一下，归一化的数据集是指，在特征向量的每个特征上，先减去该特征的均值，然后再除以标准差：
- en: '![image](Images/135equ01.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/135equ01.jpg)'
- en: Normalization of the dataset maps all the features into the same overall range
    so that the value of one feature is similar to the value of another. This helps
    many model types and is a typical data preprocessing step, as we discussed in
    [Chapter 4](ch04.xhtml#ch04).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的归一化将所有特征映射到相同的总体范围内，使得一个特征的值与另一个特征的值相似。这有助于许多模型类型，并且是一个典型的数据预处理步骤，如我们在[第
    4 章](ch04.xhtml#ch04)中讨论过的。
- en: Two Initial Test Runs
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 两次初始测试运行
- en: First, we’ll do a quick run with a single test split, as we did in the previous
    section. The code is in [Listing 7-3](ch07.xhtml#ch7lis3) and mimics the code
    we described previously, where we pass in the model instance, train it, and then
    score it using the testing data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将像上一节那样，使用单次测试划分进行快速运行。代码见[列表 7-3](ch07.xhtml#ch7lis3)，该代码与我们之前描述的代码相似，其中我们传入模型实例，训练它，然后使用测试数据进行评分。
- en: import numpy as np
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import SVC
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: print("    score = %0.4f" % clf.score(x_test, y_test))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: print("    score = %0.4f" % clf.score(x_test, y_test))
- en: print()
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'def main():'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.load("../data/breast/bc_features_standard.npy")
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/breast/bc_labels.npy")
- en: ❶ N = 455
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ N = 455
- en: x_train = x[:N];  x_test = x[N:]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N];  x_test = x[N:]
- en: y_train = y[:N];  y_test = y[N:]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N];  y_test = y[N:]
- en: print("Nearest Centroid:")
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: print("最近质心:")
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: print("k-NN classifier (k=3):")
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN 分类器 (k=3):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: print("k-NN classifier (k=7):")
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: print("k-NN 分类器 (k=7):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: print("Naive Bayes classifier (Gaussian):")
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: print("朴素贝叶斯分类器 (高斯):")
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, GaussianNB())
- en: print("Decision Tree classifier:")
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: print("决策树分类器:")
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
- en: print("Random Forest classifier (estimators=5):")
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: print("随机森林分类器 (估计器=5):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: print("Random Forest classifier (estimators=50):")
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: print("随机森林分类器 (估计器=50):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: print("SVM (linear, C=1.0):")
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: print("SVM (线性, C=1.0):")
- en: run(x_train, y_train, x_test, y_test, SVC(kernel="linear", C=1.0))
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, SVC(kernel="linear", C=1.0))
- en: print("SVM (RBF, C=1.0, gamma=0.03333):")
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: print("SVM (RBF, C=1.0, gamma=0.03333):")
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: SVC(kernel="rbf", C=1.0, gamma=0.03333))
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="rbf", C=1.0, gamma=0.03333))
- en: '*Listing 7-3: Initial models using the breast cancer dataset. See* bc_experiments.py.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7-3：使用乳腺癌数据集的初始模型。见* bc_experiments.py。'
- en: 'As before, we load the dataset and split it into training and testing data.
    We keep 455 of the 569 samples for training (80 percent), and the remaining 114
    samples are the test set (74 benign, 40 malignant). The dataset is already randomized,
    so we skip that step here. We then train nine models: Nearest Centroid (1), *k*-NN
    (2), Naïve Bayes (1), Decision Tree (1), Random Forest (2), linear SVM (1), and
    an RBF SVM (1). For the Support Vector Machines, we use the default *C* value,
    and for *γ*, we use 1/30 = 0.033333 since we have 30 features. Running this code
    gives us the scores in [Table 7-1](ch07.xhtml#ch7tab1).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们加载数据集并将其拆分为训练数据和测试数据。我们保留569个样本中的455个用于训练（80%），其余的114个样本为测试集（74个良性，40个恶性）。数据集已随机化，因此我们跳过这一步。接着，我们训练了九个模型：最近质心（1），*k*-NN（2），朴素贝叶斯（1），决策树（1），随机森林（2），线性SVM（1），以及RBF
    SVM（1）。对于支持向量机，我们使用默认的*C*值，对于*γ*，我们使用1/30 = 0.033333，因为我们有30个特征。运行这段代码后，我们得到了[表7-1](ch07.xhtml#ch7tab1)中的得分。
- en: '**Table 7-1:** Breast Cancer Model Scores'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-1：** 乳腺癌模型得分'
- en: '| **Model type** | **Score** |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **得分** |'
- en: '| --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9649 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9649 |'
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN分类器 | 0.9912 |'
- en: '| 7-NN classifier | 0.9737 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN分类器 | 0.9737 |'
- en: '| Naïve Bayes (Gaussian) | 0.9825 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（高斯） | 0.9825 |'
- en: '| Decision Tree | 0.9474 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.9474 |'
- en: '| Random Forest (5) | 0.9298 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5） | 0.9298 |'
- en: '| Random Forest (50) | 0.9737 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50） | 0.9737 |'
- en: '| Linear SVM (C = 1) | 0.9737 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM（C = 1） | 0.9737 |'
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9825 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM（C = 1，*γ* = 0.03333） | 0.9825 |'
- en: Note the number in parentheses for the Random Forest classifiers is the number
    of estimators (number of trees in the forest).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随机森林分类器括号中的数字表示估计器的数量（森林中的树木数量）。
- en: 'A few things jump out at us. First, perhaps surprisingly, the simple Nearest
    Centroid classifier is right nearly 97 percent of the time. We also see that all
    the other classifiers are doing better than the Nearest Centroid, except for the
    Decision Tree and the Random Forest with five trees. Somewhat surprisingly, the
    Naïve Bayes classifier does very well, matching the RBF SVM. The *k* = 3 Nearest
    Neighbor classifier does best of all, 99 percent accurate, even though we have
    30 features, meaning our 569 samples are points scattered in a 30-dimensional
    space. Recall, a weakness of *k*-NN is the curse of dimensionality: it requires
    more and more training samples as the number of features increases. The results
    with all the classifiers are good, so this is a hint to us that the separation
    between malignant and benign is, for this dataset, distinct. There isn’t much
    overlap between the two classes using these features.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点引起了我们的注意。首先，可能出乎意料的是，简单的最近质心分类器几乎在97%的时间里都能正确分类。我们还发现，除了决策树和五棵树的随机森林，所有其他分类器的表现都超过了最近质心分类器。有些令人惊讶的是，朴素贝叶斯分类器表现得非常好，与RBF
    SVM不相上下。*k* = 3的最近邻分类器表现最好，准确率高达99%，尽管我们有30个特征，这意味着我们的569个样本散布在30维空间中。回想一下，*k*-NN的一个弱点是维度灾难：随着特征数量的增加，它需要更多的训练样本。所有分类器的结果都很好，这也提示我们，对于这个数据集，恶性和良性之间的区别是显而易见的。使用这些特征时，两个类别之间几乎没有重叠。
- en: So, are we done with this dataset? Hardly! In fact, we’ve just begun. What happens
    if we run the code a second time? Do we get the same scores? Would we expect not
    to? A second run gives us [Table 7-2](ch07.xhtml#ch7tab2).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们对这个数据集处理完成了吗？当然没有！实际上，我们才刚刚开始。如果我们第二次运行代码，会发生什么？我们会得到相同的得分吗？我们是否期望得到不同的结果？第二次运行给了我们[表7-2](ch07.xhtml#ch7tab2)中的得分。
- en: '**Table 7-2:** Breast Cancer Scores, Second Run'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-2：** 乳腺癌得分，第二次运行'
- en: '| **Model type** | **Score** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **得分** |'
- en: '| --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9649 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9649 |'
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN分类器 | 0.9912 |'
- en: '| 7-NN classifier | 0.9737 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN分类器 | 0.9737 |'
- en: '| Naïve Bayes (Gaussian) | 0.9825 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（高斯） | 0.9825 |'
- en: '| Decision Tree | **0.9386** |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | **0.9386** |'
- en: '| Random Forest (5) | **0.9474** |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5） | **0.9474** |'
- en: '| Random Forest (50) | **0.9649** |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50） | **0.9649** |'
- en: '| Linear SVM (C = 1) | 0.9737 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM（C = 1） | 0.9737 |'
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9825 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM（C = 1，*γ* = 0.03333） | 0.9825 |'
- en: 'We’ve highlighted the scores that changed. Why would anything change? A bit
    of reflection leads to an *aha!* moment: the Random Forest is just that, random,
    so naturally we’d expect different results run to run. What about the Decision
    Tree? In sklearn, the Decision Tree classifier will randomly select a feature
    and find the best split, so different runs will also lead to different trees.
    This is a variation on the basic decision tree algorithm we discussed in [Chapter
    6](ch06.xhtml#ch06).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已标出改变的得分。为什么会有变化？稍作反思就能得到一个*aha!*时刻：随机森林本身就是随机的，因此我们自然会期望每次运行时结果不同。那么决策树呢？在sklearn中，决策树分类器会随机选择一个特征并找到最佳划分，因此不同的运行也会导致不同的树。这是我们在[第6章](ch06.xhtml#ch06)中讨论的基本决策树算法的一个变体。
- en: 'All the other algorithms are fixed: for a given training dataset, they can
    lead to only one model. As an aside, the SVM implementation in sklearn does use
    a random number generator, so at times different runs will give slightly different
    results, but, conceptually, we’d expect the same model for the same input data.
    The tree-based classifiers, however, do change between training runs. We’ll explore
    this variation more next. For now, we need to add some rigor to our quick analysis.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他算法都是固定的：对于给定的训练数据集，它们只能生成一个模型。顺便提一下，sklearn中的SVM实现确实使用了随机数生成器，因此有时不同的运行会得到略微不同的结果，但从概念上讲，对于相同的输入数据，我们期望得到相同的模型。然而，基于树的分类器在训练过程中会发生变化。我们将在接下来探讨这种变化。现在，我们需要为我们的快速分析增加一些严谨性。
- en: The Effect of Random Splits
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机拆分的效果
- en: Let’s change the split between training and testing data and see what happens
    to our results. We don’t need to list all the code again since the only change
    is to how x_train and x_test are defined. Before splitting, we randomize the order
    of the full dataset but do so by first fixing the pseudorandom number seed so
    that each run gives the same ordering to the dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变训练数据和测试数据之间的拆分，看看我们的结果会发生什么。我们不需要再次列出所有代码，因为唯一的变化是如何定义x_train和x_test。在拆分之前，我们通过首先固定伪随机数种子来随机化整个数据集的顺序，以确保每次运行时数据集的顺序相同。
- en: Looking again at [Listing 7-3](ch07.xhtml#ch7lis3), insert the following code
    before ❶ so that we generate a fixed permutation of the dataset (idx).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看[Feyman学习法](ch07.xhtml#ch7lis3)，在❶之前插入以下代码，以便我们生成数据集（idx）的固定排列。
- en: np.random.seed(12345)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: np.random.seed(12345)
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y.shape[0]))
- en: x = x[idx]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: y = y[idx]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[idx]
- en: It’s fixed because we fixed the pseudorandom number generator seed value. We
    then reorder the samples (x) and labels (y) accordingly before splitting into
    train and test subsets as before. Running this code gives us the results in [Table
    7-3](ch07.xhtml#ch7tab3).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 它是固定的，因为我们固定了伪随机数生成器的种子值。然后我们重新排列样本（x）和标签（y），并像以前一样将它们分割为训练集和测试集。运行这段代码会给我们在[表7-3](ch07.xhtml#ch7tab3)中的结果。
- en: '**Table 7-3:** Breast Cancer Scores After Randomizing the Dataset'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-3：** 随机化数据集后的乳腺癌得分'
- en: '| **Model type** | **Score** |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| **模型类型** | **得分** |'
- en: '| --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9474 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9474 |'
- en: '| 3-NN classifier | 0.9912 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN分类器 | 0.9912 |'
- en: '| 7-NN classifier | 0.9912 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN分类器 | 0.9912 |'
- en: '| Naïve Bayes (Gaussian) | 0.9474 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯（高斯） | 0.9474 |'
- en: '| Decision Tree | 0.9474 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.9474 |'
- en: '| Random Forest (5) | 0.9912 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 0.9912 |'
- en: '| Random Forest (50) | 1.0000 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 1.0000 |'
- en: '| Linear SVM (C = 1) | 0.9649 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 线性SVM (C = 1) | 0.9649 |'
- en: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9737 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| RBF SVM (C = 1, *γ* = 0.03333) | 0.9737 |'
- en: Notice these are entirely different from our earlier results. The *k*-NN classifiers
    are both equally good, the SVM classifiers are worse, and the 50-tree Random Forest
    achieves perfection on the test set. So, what is happening? Why are we getting
    all these different results run to run?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些结果与我们之前的结果完全不同。*k*-NN分类器表现一样好，SVM分类器表现较差，而50棵树的随机森林在测试集上实现了完美。因此，发生了什么？为什么每次运行会得到这么不同的结果？
- en: 'We’re seeing the effect of the random sampling that builds the train and test
    splits. The first split just happened to use an ordering of samples that gave
    good results for one model type and less good results for other model types. The
    new split favors different model types. Which is correct? Both. Recall what the
    dataset represents: a sampling from some unknown parent distribution that generates
    the data that we actually have. If we think in those terms, we see that the dataset
    we have is an incomplete picture of the true parent distribution. It has biases,
    though we don’t know what they are necessarily, and is deficient in that there
    are parts of the parent distribution that the dataset does not represent well.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了构建训练集和测试集分割时随机抽样的效果。第一次分割恰好使用了一种样本排序，这对某种模型类型得到了较好的结果，而对其他模型类型则结果较差。新的分割偏向于不同的模型类型。哪个是正确的？两者都是。回想一下数据集代表的是什么：它是从某个未知的母体分布中抽样得到的，生成了我们实际拥有的数据。如果我们从这个角度思考，就会发现我们拥有的数据集是不完整的，它只是对真实母体分布的一个片面描述。它有偏差，尽管我们不一定知道这些偏差是什么，而且它也不完全代表母体分布的所有部分。
- en: Further, when we split the data after randomizing the order, we might end up
    with a “bad” mix in the train or test portion—a mix of the data that does a poor
    job of representing the true distribution. If so, we might train a model to recognize
    a slightly different distribution that does not match the true distribution well,
    or the test set might be a bad mix and not be a fair representation of what the
    model has learned. This effect is even more pronounced when the proportion of
    the classes is such that one or more are rare and possibly not present in the
    train or test split. This is precisely the issue that caused us to introduce the
    idea of *k*-fold cross-validation in [Chapter 4](ch04.xhtml#ch04). With *k*-fold
    validation, we’ll be sure to use every sample as both train and test at some point
    and buy ourselves some protection against a bad split by averaging across all
    the folds.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当我们在随机化顺序后拆分数据时，可能会得到“糟糕”的训练集或测试集混合——这种混合的数据未能很好地代表真实分布。如果是这样，我们可能会训练一个模型去识别一个与真实分布不太匹配的稍有不同的分布，或者测试集可能是一个糟糕的混合，不能公平地代表模型已经学习到的内容。当类别的比例使得一个或多个类别较为稀有，且可能在训练集或测试集中没有出现时，这种效果会更加明显。这正是导致我们在[第4章](ch04.xhtml#ch04)中引入*k*倍交叉验证的原因。通过*k*倍交叉验证，我们确保每个样本在某些时候都能作为训练集和测试集使用，并通过在所有折叠中平均结果，帮助我们避免因分割不均而带来的风险。
- en: However, before we apply *k*-fold validation to the breast cancer dataset, we
    should notice one essential thing. We modified the code of [Listing 7-3](ch07.xhtml#ch7lis3)
    to fix the pseudorandom number seed so that we could reorder the dataset in exactly
    the same way each time we run. We then ran the code and saw the results. If we
    rerun the code, we’ll get *exactly* the same output, even for the tree-based classifiers.
    This is not what we saw earlier. The tree classifiers are *stochastic*—they will
    generate a unique tree or forest each time—so we should expect the results to
    vary somewhat from run to run. But now they don’t vary; we get the same output
    each time. By setting the NumPy pseudorandom number seed explicitly, we fixed
    not only the ordering of the dataset, but also the ordering of the *pseudorandom
    sequence* sklearn will use to generate the tree models. This is because sklearn
    is also using the NumPy pseudorandom number generator. This is a subtle effect
    with potentially serious consequences and in a larger project might be very difficult
    to pick up as a bug. The solution is to set the seed to a random value after we’re
    done reordering the dataset. We can do this by adding one line after y = y[idx]
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们将 *k* 倍交叉验证应用于乳腺癌数据集之前，我们应该注意一件重要的事情。我们修改了[列表 7-3](ch07.xhtml#ch7lis3)中的代码，固定了伪随机数种子，这样每次运行时我们都能以完全相同的方式重新排序数据集。然后我们运行了代码并看到了结果。如果我们重新运行代码，我们将得到*完全*相同的输出，即使是对于基于树的分类器也是如此。这与我们之前看到的不同。树分类器是*随机的*——它们每次都会生成一个独特的树或森林——因此我们应该期望每次运行的结果会有所不同。但现在它们不再变化；我们每次都会得到相同的输出。通过显式设置NumPy伪随机数种子，我们不仅固定了数据集的排序，还固定了sklearn在生成树模型时使用的*伪随机序列*的顺序。这是因为sklearn也使用了NumPy的伪随机数生成器。这是一个微妙的效果，可能带来严重后果，在更大的项目中，可能很难察觉为一个bug。解决方案是在重新排序数据集后，将种子设置为一个随机值。我们可以在`y
    = y[idx]`之后添加一行代码来实现这一点。
- en: np.random.seed()
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: np.random.seed()
- en: so that the pseudorandom number generator is reset by using the system state,
    typically read from */dev/urandom*. Now when we run again, we’ll get different
    results for the tree models, as before.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，伪随机数生成器通过使用系统状态进行重置，通常是从 */dev/urandom* 读取。现在，当我们重新运行时，我们将得到不同的树模型结果，如之前一样。
- en: Adding k-fold Validation
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加 k 折验证
- en: To implement *k*-fold validation, we first need to pick a value for *k*. Our
    dataset has 569 samples. We want to split it so that there are a decent number
    of samples per fold because we want to make the test set a reasonable representation
    of the data. This argues toward making *k* small. However, we also want to average
    out the effect of a bad split, so we might want *k* to be larger. As with most
    things in life, a balance must be sought. If we set *k* = 5, we’ll get 113 samples
    per split (ignoring the final four samples, which should have no meaningful impact).
    This leaves 80 percent for training and 20 percent for test for each combination
    of folds, a reasonable thing to do. So, we’ll use *k* = 5, but we’ll write our
    code so that we can vary *k* if we want.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 *k*-折验证，我们首先需要为 *k* 选择一个值。我们的数据集有 569 个样本。我们希望将其拆分，使每个折叠中都有合理数量的样本，因为我们希望测试集能够合理代表数据。这表明
    *k* 应该设得比较小。但我们也希望平均掉不好的拆分结果，因此我们可能希望 *k* 设置得大一些。正如生活中的大多数事情一样，需要寻找到一个平衡。如果我们设置
    *k* = 5，那么每个拆分将得到 113 个样本（忽略最后的四个样本，因为它们不会产生重大影响）。这将每次组合折叠时训练 80% 的数据，测试 20% 的数据，这是一个合理的做法。因此，我们将使用
    *k* = 5，但我们会编写代码，以便在需要时调整 *k*。
- en: We already have an approach for training multiple models on a train/ test split.
    All we need to add is code to generate each of the *k* folds and then train the
    models on them. The code is in [Listing 7-4](ch07.xhtml#ch7lis4) and [Listing
    7-5](ch07.xhtml#ch7lis5), which show the helper functions and main function, respectively.
    Let’s start with [Listing 7-4](ch07.xhtml#ch7lis4).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了一种方法来训练多个模型，并进行训练/测试拆分。我们需要做的就是添加代码来生成每个*k*折叠，然后在它们上训练模型。相关代码见[Listing
    7-4](ch07.xhtml#ch7lis4)和[Listing 7-5](ch07.xhtml#ch7lis5)，它们分别展示了辅助函数和主函数。让我们从[Listing
    7-4](ch07.xhtml#ch7lis4)开始。
- en: import numpy as np
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 numpy 为 np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.neighbors 导入 NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.neighbors 导入 KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.naive_bayes 导入 GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.tree 导入 DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.ensemble 导入 RandomForestClassifier
- en: from sklearn.svm import SVC
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.svm 导入 SVC
- en: import sys
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 sys
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '定义 run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: return clf.score(x_test, y_test)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 clf.score(x_test, y_test)
- en: 'def split(x,y,k,m):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '定义 split(x,y,k,m):'
- en: ❶ ns = int(y.shape[0]/m)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ ns = int(y.shape[0]/m)
- en: s = []
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: s = []
- en: 'for i in range(m):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 i 在 range(m) 中:'
- en: ❷ s.append([x[(ns*i):(ns*i+ns)],
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ s.append([x[(ns*i):(ns*i+ns)],
- en: y[(ns*i):(ns*i+ns)]])
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: y[(ns*i):(ns*i+ns)]])
- en: x_test, y_test = s[k]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: x_test, y_test = s[k]
- en: x_train = []
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = []
- en: y_train = []
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = []
- en: 'for i in range(m):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 i 在 range(m) 中:'
- en: 'if (i==k):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 (i==k):'
- en: continue
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: 'else:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: a,b = s[i]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: a,b = s[i]
- en: x_train.append(a)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: x_train.append(a)
- en: y_train.append(b)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: y_train.append(b)
- en: ❸ x_train = np.array(x_train).reshape(((m-1)*ns,30))
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x_train = np.array(x_train).reshape(((m-1)*ns,30))
- en: y_train = np.array(y_train).reshape((m-1)*ns)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.array(y_train).reshape((m-1)*ns)
- en: return [x_train, y_train, x_test, y_test]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 [x_train, y_train, x_test, y_test]
- en: 'def pp(z,k,s):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '定义 pp(z,k,s):'
- en: m = z.shape[1]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: m = z.shape[1]
- en: 'print("%-19s: %0.4f +/- %0.4f | " % (s, z[k].mean(),'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '打印("%-19s: %0.4f +/- %0.4f | " % (s, z[k].mean(),'
- en: z[k].std()/np.sqrt(m)), end='')
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: z[k].std()/np.sqrt(m)), 结束='')
- en: 'for i in range(m):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 i 在 range(m) 中:'
- en: print("%0.4f " % z[k,i], end='')
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 打印("%0.4f " % z[k,i], 结束='')
- en: print()
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 打印()
- en: '*Listing 7-4: Using k-fold validation to evaluate the breast cancer dataset.
    Helper functions. See* bc_kfold.py.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 7-4: 使用 k 折验证评估乳腺癌数据集。辅助函数。请参见* bc_kfold.py。'
- en: '[Listing 7-4](ch07.xhtml#ch7lis4) begins by including all the modules we used
    before and then defines three functions: run, split, and pp. The run function
    looks familiar. It takes a train set, test set, and model instance, trains the
    model, and then scores the model against the test set. The pp function is a pretty-print
    function to show the per split scores along with the average score across all
    the splits. The average is shown as the mean ± the standard error of the mean.
    Recall that an sklearn score is the overall accuracy of the model on the test
    set, or the fraction of times that the model predicted the actual class of the
    test sample. Perfection is a score of 1.0, and complete failure is 0.0\. Complete
    failure is rare because even random guessing will get it right some fraction of
    the time.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单7-4](ch07.xhtml#ch7lis4)开始时包含了我们之前用过的所有模块，然后定义了三个函数：run、split和pp。run函数看起来很熟悉。它接受训练集、测试集和模型实例，训练模型并用测试集对模型进行评分。pp函数是一个漂亮打印函数，用于显示每个分割的分数以及所有分割的平均分数。平均值以均值±均值标准误差的形式显示。回想一下，sklearn中的分数是模型在测试集上的整体准确性，或者是模型预测测试样本实际类别的次数的比例。完美的分数是1.0，而完全失败的分数是0.0。完全失败是很少发生的，因为即使是随机猜测也会在某些情况下猜对。'
- en: The only interesting function in [Listing 7-4](ch07.xhtml#ch7lis4) is split.
    Its arguments are the full dataset, x, the corresponding labels, y, the current
    fold number, k, and the total number of folds, m. We’ll divide the full dataset
    into *m* distinct sets, the folds, and use the *k*-th fold as test while merging
    the remaining *m –* 1 folds into a new training set. First, we set the number
    of samples per fold ❶. The loop then creates a list of folds, s. Each element
    of this list contains the feature vectors and labels of the fold ❷.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单7-4](ch07.xhtml#ch7lis4)中，唯一有趣的函数是split。它的参数包括完整数据集x、相应的标签y、当前折叠数k和总折叠数m。我们将把完整数据集分成*m*个不同的子集，即折叠，并使用第*k*个折叠作为测试集，同时将其余的*m
    - 1*个折叠合并为一个新的训练集。首先，我们设置每个折叠的样本数❶。然后，循环会创建一个折叠列表s，列表中的每个元素都包含该折叠的特征向量和标签❷。
- en: The test set is simple, it’s the *k*-th fold, so we set those values next (x_test,
    y_test). The loop then takes the remaining *m –* 1 folds and merges them into
    a new training set, x_train, with labels, y_train.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集很简单，就是第*k*个折叠，所以我们接下来设置这些值（x_test，y_test）。然后，循环会将剩下的*m - 1*个折叠合并成一个新的训练集x_train，并附上标签y_train。
- en: The two lines after the loop are a bit mysterious ❸. When the loop ends, x_train
    is a *list*, each element of which is a list representing the feature vectors
    of the fold we want in the training set. So we first make a NumPy array of this
    list and then reshape it so that x_train has 30 columns, the number of features
    per vector, and *n*[*s*](*m –* 1) rows, where *n*[*s*] is the number of samples
    per fold. Thus x_train becomes x minus the samples we put into the test fold,
    those of the *k*-th fold. We also build y_train so that the correct label goes
    with each the feature vector in x_train.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 循环后的两行代码有点神秘❸。当循环结束时，x_train是一个*列表*，其中的每个元素是一个代表我们想要放入训练集的折叠特征向量的列表。因此，我们首先将这个列表转化为一个NumPy数组，然后进行重塑，使得x_train具有30列（每个特征向量的特征数），并且有*n*[*s*](*m
    - 1)行，其中*n*[*s*]是每个折叠中的样本数。因此，x_train变成了x减去我们放入测试折叠中的样本，即第*k*个折叠的样本。我们还构建了y_train，使得每个特征向量对应正确的标签。
- en: '[Listing 7-5](ch07.xhtml#ch7lis5) shows us how to use the helper functions.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单7-5](ch07.xhtml#ch7lis5)展示了如何使用这些辅助函数。'
- en: 'def main():'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.load("../data/breast/bc_features_standard.npy")
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/breast/bc_labels.npy")
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y.shape[0]))
- en: x = x[idx]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: y = y[idx]
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[idx]
- en: ❶ m = int(sys.argv[1])
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ m = int(sys.argv[1])
- en: z = np.zeros((8,m))
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: z = np.zeros((8,m))
- en: 'for k in range(m):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 'for k in range(m):'
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: x_train, y_train, x_test, y_test = split(x,y,k,m)
- en: z[0,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: z[0,k] = run(x_train, y_train, x_test, y_test,
- en: NearestCentroid())
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: NearestCentroid())
- en: z[1,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: z[1,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: z[2,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: z[2,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: z[3,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: z[3,k] = run(x_train, y_train, x_test, y_test,
- en: GaussianNB())
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: GaussianNB())
- en: z[4,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: z[4,k] = run(x_train, y_train, x_test, y_test,
- en: DecisionTreeClassifier())
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: DecisionTreeClassifier())
- en: z[5,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: z[5,k] = run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: z[6,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: z[6,k] = run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: z[7,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: z[7,k] = run(x_train, y_train, x_test, y_test,
- en: SVC(kernel="linear", C=1.0))
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: SVC(kernel="linear", C=1.0))
- en: pp(z,0,"Nearest"); pp(z,1,"3-NN")
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,0,"最近邻"); pp(z,1,"3-NN")
- en: pp(z,2,"7-NN");    pp(z,3,"Naive Bayes")
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,2,"7-NN");    pp(z,3,"朴素贝叶斯")
- en: pp(z,4,"Decision Tree");    pp(z,5,"Random Forest (5)")
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,4,"决策树");    pp(z,5,"随机森林 (5)")
- en: pp(z,6,"Random Forest (50)");    pp(z,7,"SVM (linear)")
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: pp(z,6,"随机森林 (50)");    pp(z,7,"支持向量机 (线性)")
- en: '*Listing 7-5: Using k-fold validation to evaluate the breast cancer dataset.
    Main code. See* bc_kfold.py.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 7-5：使用 k 折交叉验证评估乳腺癌数据集。主要代码。请参见* bc_kfold.py。'
- en: The first thing we do in main is load the full dataset and randomize the ordering.
    The number of folds, m, is read from the command line ❶ and used to create the
    output array, z. This array holds the per fold scores for each of the eight models
    we’ll train, so it has shape 8 × *m*. Recall, when running a Python script from
    the command line, any arguments passed after the script name are available in
    sys.argv, a list of strings. This is why the argument is passed to int to convert
    it to an integer ❶.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在main中，我们首先加载完整的数据集并随机化顺序。折叠的数量*m*从命令行读取❶，并用于创建输出数组z。这个数组保存我们将训练的每个模型在每个折叠上的得分，因此它的形状是8
    × *m*。回想一下，当从命令行运行Python脚本时，任何在脚本名后传递的参数都会出现在sys.argv中，这是一个字符串列表。这就是为什么参数会传递给int进行转换成整数❶的原因。
- en: Next, we loop over the *m* folds, where *k* is the fold that we’ll be using
    for test data. We create the split and then use the split to train the eight model
    types we trained previously. Each call to run trains a model of the type passed
    in and returns the score found by running that model against the *k*-th fold as
    test data. We store these results in z. Finally, we use pp to display the per
    model type and per fold scores along with the average score over all the folds.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历*m*折，其中*k*是我们用于测试数据的折叠。我们创建拆分，然后使用这个拆分来训练之前训练的八种模型类型。每次调用run都会训练一种指定类型的模型，并返回通过该模型对*k*折叠进行测试后得到的得分。我们将这些结果存储在z中。最后，我们使用pp显示每个模型类型和每个折叠的得分以及所有折叠的平均得分。
- en: A sample run of this code, for *k* = 5 and showing only the mean score across
    folds, gives the results in [Table 7-4](ch07.xhtml#ch7tab4).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在* k * = 5的情况下，仅显示折叠平均分数的这段代码执行示例，结果见[表 7-4](ch07.xhtml#ch7tab4)。
- en: '**Table 7-4:** Breast Cancer Scores as Mean Over Five Folds'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-4：** 乳腺癌数据集的五折平均得分'
- en: '| **Model** | **Mean** ± **SE** |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **均值** ± **标准误差** |'
- en: '| --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.9310 ± 0.0116 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心 | 0.9310 ± 0.0116 |'
- en: '| 3-NN | 0.9735 ± 0.0035 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN | 0.9735 ± 0.0035 |'
- en: '| 7-NN | 0.9717 ± 0.0039 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN | 0.9717 ± 0.0039 |'
- en: '| Naïve Bayes | 0.9363 ± 0.0140 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.9363 ± 0.0140 |'
- en: '| Decision Tree | 0.9027 ± 0.0079 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.9027 ± 0.0079 |'
- en: '| Random Forest (5) | 0.9540 ± 0.0107 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 0.9540 ± 0.0107 |'
- en: '| Random Forest (50) | 0.9540 ± 0.0077 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 0.9540 ± 0.0077 |'
- en: '| SVM (linear) | 0.9699 ± 0.0096 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 (线性) | 0.9699 ± 0.0096 |'
- en: Here we’re showing the average performance of each model over all folds. One
    way to understand the results is that this is the sort of performance we should
    expect, per model type, if we were to train the model using *all* of the data
    in the dataset and test it against new samples from the same parent distribution.
    Indeed, in practice, we would do just this, as we can assume that the reason behind
    making the model in the first place is to use it for some purpose going forward.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了每个模型在所有折叠上的平均表现。理解这些结果的一种方式是：如果我们使用数据集中的*所有*数据来训练模型，并将其在来自相同母体分布的新样本上进行测试，那么每种模型类型的表现应该大致是这样的。事实上，在实际操作中，我们确实会这样做，因为我们可以假设制作模型的初衷是为了将来某些目的使用它。
- en: Run the code a second time with *k* = 5\. A new set of outputs appears. This
    is because we’re randomizing the order of the dataset on every run ([Listing 7-5](ch07.xhtml#ch7lis5)).
    This makes a new set of splits and implies that each model will be trained on
    a different subset mix of the full dataset on each run. So, we should expect different
    results. Let’s run the code 1,000 times with *k* = 5\. Note, training this many
    models takes about 20 minutes on a very standard desktop computer. For each run
    we’ll get an average score over the five folds. We then compute the mean of these
    averages, which is known as the *grand mean*. [Table 7-5](ch07.xhtml#ch7tab5)
    shows the results.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次运行代码时设置*k* = 5，出现了一组新的输出结果。这是因为我们在每次运行时都随机化了数据集的顺序（[列表 7-5](ch07.xhtml#ch7lis5)）。这就产生了新的分割，并意味着每次运行时，每个模型都将基于数据集的不同子集进行训练。因此，我们应该期待不同的结果。让我们在*k*
    = 5时运行代码1,000次。请注意，在一台标准桌面计算机上训练这么多模型大约需要20分钟。每次运行时，我们将获得五折交叉验证的平均得分。然后我们计算这些平均值的均值，这个均值就是**总均值**。[表
    7-5](ch07.xhtml#ch7tab5)展示了结果。
- en: '**Table 7-5:** Breast Cancer Scores as Grand Mean Over 1,000 Runs with Five
    Folds'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-5：** 乳腺癌得分作为1,000次运行中五折交叉验证的总均值'
- en: '| **Model** | **Grand mean** ± **SE** |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **总均值** ± **标准误差** |'
- en: '| --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Nearest Centroid | 0.929905 ± 0.000056 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心法 | 0.929905 ± 0.000056 |'
- en: '| 3-NN | 0.966334 ± 0.000113 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 3-NN | 0.966334 ± 0.000113 |'
- en: '| 7-NN | 0.965496 ± 0.000110 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 7-NN | 0.965496 ± 0.000110 |'
- en: '| Naïve Bayes | 0.932973 ± 0.000095 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 | 0.932973 ± 0.000095 |'
- en: '| Decision Tree | 0.925706 ± 0.000276 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.925706 ± 0.000276 |'
- en: '| Random Forest (5) | 0.948378 ± 0.000213 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（5） | 0.948378 ± 0.000213 |'
- en: '| Random Forest (50) | 0.958845 ± 0.000135 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（50） | 0.958845 ± 0.000135 |'
- en: '| SVM (linear) | 0.971871 ± 0.000136 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| SVM（线性） | 0.971871 ± 0.000136 |'
- en: We can take these grand means as an indication of how well we’d expect each
    model to do against a new set of unknown feature vectors. The small standard errors
    of the mean are an indication of how well the mean value is known, not how well
    a model of that type trained on a dataset will necessarily perform. We use the
    grand mean to help us order the models so we can select one over another.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些总均值视为对每个模型在一组新的未知特征向量上的表现的预期。均值的小标准误差表明的是均值的已知程度，而不是该类型的模型在数据集上训练后的表现如何。我们使用总均值来帮助我们对模型进行排序，以便可以选择一个模型而不是另一个。
- en: 'Ranking the models from highest score to lowest gives the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 按照得分从高到低对模型进行排名，结果如下：
- en: SVM (linear)
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SVM（线性）
- en: '*k*-NN (*k* = 3)'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*k*-NN (*k* = 3)'
- en: '*k*-NN (*k* = 7)'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*k*-NN (*k* = 7)'
- en: Random Forest (50)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林（50）
- en: Random Forest (5)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林（5）
- en: Naïve Bayes (Gaussian)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯（高斯）
- en: Nearest Centroid
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最近质心法
- en: Decision Tree
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策树
- en: This is interesting given that we might expect the SVM to be best, but would
    likely assume the Random Forests to do better than *k*-NN. The Decision Tree was
    not as good as we thought, and was less accurate than the Nearest Centroid classifier.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果很有趣，因为我们可能预期SVM是最好的，但很可能会认为随机森林要比*k*-NN表现更好。决策树没有我们想象中的好，且准确度低于最近质心分类器。
- en: Some comments are in order here. First, note that these results are derived
    from the training of 8,000 different models on 1,000 different orderings of the
    dataset. When we study neural networks, we’ll see much longer training times.
    Experimenting with classical machine learning models is generally easy to do since
    each change to a parameter doesn’t require a lengthy training session.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些评论需要说明。首先，注意这些结果是通过在1,000种数据集顺序上训练8,000个不同模型得到的。当我们研究神经网络时，会看到更长的训练时间。实验经典机器学习模型通常较为简单，因为对参数的每一次更改并不需要进行长时间的训练。
- en: Second, we didn’t try to optimize any of the model hyperparameters. Some of
    these hyperparameters are indirect, like assuming that the features are normally
    distributed so that the Gaussian Naïve Bayes classifier is a reasonable choice,
    while others are numerical, like the number of neighbors in *k*-NN or the number
    of trees in a Random Forest. If we want to thoroughly develop a good classifier
    for this dataset using a classic model, we’ll have to explore some of these hyperparameters.
    Ideally, we’d repeat the experiments many, many times for each new hyperparameter
    setting to arrive at a tight mean value for the score, as we have previously with
    the grand means over 1,000 runs. We’ll play a bit more with hyperparameters in
    the next section, where we see how we can search for good ones that work well
    with our dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们没有尝试优化任何模型的超参数。这些超参数有些是间接的，例如假设特征是正态分布的，这样高斯朴素贝叶斯分类器是合理的选择，而其他超参数则是数值型的，例如*k*-NN中的邻居数量或随机森林中的树木数量。如果我们想要为这个数据集开发一个好的经典分类器，我们必须探索其中一些超参数。理想情况下，我们需要对每个新的超参数设置重复实验多次，得到一个紧密的均值评分，就像我们之前通过1,000次运行得到的总体均值那样。在下一部分，我们将进一步探讨如何搜索适合我们数据集的良好超参数。
- en: Searching for Hyperparameters
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数搜索
- en: Let’s explore the effect of some of the hyperparameters on various model types.
    Specifically, let’s see if we can optimize our choice of *k* for *k*-NN, forest
    size for Random Forest, and the *C* margin size of the linear SVM.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一些超参数对不同模型类型的影响。具体来说，让我们看看是否可以优化我们对*k*的选择，用于*k*-NN、随机森林的森林大小，以及线性SVM的*C*边际大小。
- en: Fine-Tuning Our k-NN Classifier
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调我们的*k*-NN分类器
- en: Because the number of neighbors in a *k*-NN classifier is an integer, typically
    odd, it’s straightforward to repeat our five-fold cross validation experiment
    while varying *k* for *k* ∈ *{*1,3,5,7,9,11,13,15*}*. To do this, we need only
    change the main loop in [Listing 7-5](ch07.xhtml#ch7lis5) so that each call to
    run uses KNeighborsClassifier with a different number of neighbors, as follows.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*k*-NN分类器中的邻居数量是一个整数，通常是奇数，所以通过改变*k*来重复我们的五折交叉验证实验是比较简单的，*k* ∈ *{*1,3,5,7,9,11,13,15*}*。为了做到这一点，我们只需要修改[代码清单
    7-5](ch07.xhtml#ch7lis5)中的主循环，使每次调用run时使用不同数量邻居的KNeighborsClassifier，代码如下。
- en: 'for k in range(m):'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 'for k in range(m):'
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: x_train, y_train, x_test, y_test = split(x,y,k,m)
- en: z[0,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: z[0,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=1))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=1))
- en: z[1,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: z[1,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: z[2,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: z[2,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=5))
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=5))
- en: z[3,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: z[3,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: z[4,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: z[4,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=9))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=9))
- en: z[5,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: z[5,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=11))
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=11))
- en: z[6,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: z[6,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=13))
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=13))
- en: z[7,k] = run(x_train, y_train, x_test, y_test,
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: z[7,k] = run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=15))
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=15))
- en: The grand mean of the scores for 1,000 repetitions of the five-fold cross-validation
    code using a different random ordering of the full dataset each time gives the
    results in [Table 7-6](ch07.xhtml#ch7tab6).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每次对完整数据集进行随机排序的五折交叉验证代码进行1,000次重复后的总体均值，结果见[表 7-6](ch07.xhtml#ch7tab6)。
- en: '**Table 7-6:** Breast Cancer Scores as Grand Mean for Different k Values and
    Five-Fold Validation'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-6：** 乳腺癌评分的总体均值，不同k值和五折交叉验证的结果'
- en: '| ***k*** | **Grand mean** ± **SE** |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| ***k*** | **总体均值** ± **标准误差** |'
- en: '| --- | --- |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 0.951301 ± 0.000153 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.951301 ± 0.000153 |'
- en: '| 3 | 0.966282 ± 0.000112 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.966282 ± 0.000112 |'
- en: '| 5 | 0.965998 ± 0.000097 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.965998 ± 0.000097 |'
- en: '| 7 | 0.96520 ± 0.000108 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.96520 ± 0.000108 |'
- en: '| **9** | **0.967011** ± **0.000100** |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| **9** | **0.967011** ± **0.000100** |'
- en: '| 11 | 0.965069 ± 0.000107 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.965069 ± 0.000107 |'
- en: '| 13 | 0.962400 ± 0.000106 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.962400 ± 0.000106 |'
- en: '| 15 | 0.959976 ± 0.000101 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.959976 ± 0.000101 |'
- en: We’ve highlighted the *k* = 9 because it returned the highest score. This indicates
    that we might want to use *k* = 9 for this dataset.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别强调了*k* = 9，因为它返回了最高的得分。这表明我们可能希望对这个数据集使用*k* = 9。
- en: Fine-Tuning Our Random Forest
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调我们的随机森林
- en: Let’s look at the Random Forest model. The sklearn RandomForestClassifier class
    has quite a few hyperparameters that we could manipulate. To avoid being excessively
    pedantic, we’ll seek only an optimal number of trees in the forest. This is the
    n_estimators parameter. As we did for *k* in *k*-NN, we’ll search over a range
    of forest sizes and select the one that gives the best grand mean score for 1,000
    runs at five folds each per run.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随机森林模型。sklearn的RandomForestClassifier类有许多超参数可以调整。为了避免过于繁琐，我们只寻求森林中最优的树木数量。这就是n_estimators参数。像调整*k*在*k*-NN中一样，我们将对不同的森林大小进行搜索，并选择在每次运行1000次五折交叉验证中提供最佳总体均值的那个大小。
- en: This is a one-dimensional grid-like search. We varied *k* by one, but for the
    number of trees in the forest, we need to cover a larger scale. We don’t expect
    there to be a meaningful difference between 10 trees in the forest or 11, especially
    considering that each Random Forest training session will lead to a different
    set of trees even if the number of trees is fixed. We saw this effect several
    times in the previous section. Instead, let’s vary the number of trees by selecting
    from *n*[*t*] ∈ *{*5,20,50,100,200,500,1000,5000*}* where *n*[*t*] is the number
    of trees in the forest (number of estimators). Running this search gives us the
    grand means in [Table 7-7](ch07.xhtml#ch7tab7).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种一维的网格搜索。我们将*k*调整为1，但对于森林中树木的数量，我们需要覆盖更大的范围。我们不期望森林中有10棵树和11棵树之间存在显著差异，尤其是考虑到即使树的数量固定，每次训练随机森林时也会得到不同的树木集合。我们在前面的章节中多次观察到了这一效应。相反，我们将通过从*n*[*t*]
    ∈ *{*5,20,50,100,200,500,1000,5000*}*中选择来调整树木的数量，其中*n*[*t*]是森林中的树木数量（估算器的数量）。运行此搜索将给我们在[表7-7](ch07.xhtml#ch7tab7)中的总体均值。
- en: '**Table 7-7:** Breast Cancer Scores as Grand Mean for Different Random Forest
    Sizes and Five-Fold Validation'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-7：** 不同随机森林大小和五折验证的乳腺癌分数的总体均值'
- en: '| ***n[t]*** | **Grand mean** ± **SE** |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| ***n[t]*** | **总体均值** ± **标准误差（SE）** |'
- en: '| --- | --- |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 5 | 0.948327 ± 0.000206 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.948327 ± 0.000206 |'
- en: '| 20 | 0.956808 ±0.000166 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.956808 ±0.000166 |'
- en: '| 50 | 0.959048 ± 0.000139 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 0.959048 ± 0.000139 |'
- en: '| 100 | 0.959740 ± 0.000130 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 0.959740 ± 0.000130 |'
- en: '| 200 | 0.959913 ± 0.000122 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 0.959913 ± 0.000122 |'
- en: '| 500 | 0.960049 ± 0.000117 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 0.960049 ± 0.000117 |'
- en: '| 750 | 0.960147 ± 0.000118 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 750 | 0.960147 ± 0.000118 |'
- en: '| 1000 | 0.960181 ± 0.000116 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 0.960181 ± 0.000116 |'
- en: The first thing to notice is that the differences are very small, though if
    you run the Mann–Whitney U test, you’ll see that the difference between *n*[*t*]
    = 5 (worst) and *n*[*t*] = 1000 (best) is statistically significant. However,
    the difference between *n*[*t*] = 200 and *n*[*t*] = 1000 is not significant.
    Here we need to make a judgment call. Setting *n*[*t*] = 1000 did give the best
    result but it’s indistinguishable, for practical purposes, from *n*[*t*] = 500
    or even *n*[*t*] = 100\. Since runtime for a Random Forest scales linearly in
    the number of trees, using *n*[*t*] = 100 results in a classifier that is on average
    10× faster than using *n*[*t*] = 1000\. So, depending upon the task, we might
    select *n*[*t*] = 100 over *n*[*t*] = 1000 for that reason.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，差异非常小，尽管如果你运行Mann–Whitney U检验，你会发现*n*[*t*] = 5（最差）和*n*[*t*] = 1000（最好）之间的差异在统计上是显著的。然而，*n*[*t*]
    = 200和*n*[*t*] = 1000之间的差异并不显著。在这里，我们需要做出判断。设置*n*[*t*] = 1000确实得到了最佳结果，但从实际角度来看，它与*n*[*t*]
    = 500甚至*n*[*t*] = 100没有可区分的差异。由于随机森林的运行时间与树木数量呈线性关系，因此使用*n*[*t*] = 100的分类器平均比使用*n*[*t*]
    = 1000的要快10倍。因此，根据任务的不同，可能会选择*n*[*t*] = 100而不是*n*[*t*] = 1000。
- en: Fine-Tuning Our SVMs
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调我们的支持向量机（SVM）
- en: Let’s turn our attention to the linear SVM. For the linear kernel, we’ll adjust
    *C*. Note, sklearn has other parameters, as it did for the Random Forest, but
    we’ll leave them at their default settings.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力转向线性支持向量机（SVM）。对于线性核函数，我们将调整*C*。请注意，sklearn还提供了其他参数，就像在随机森林中一样，但我们将保持其默认设置。
- en: What range of *C* should we search over? The answer is problem dependent but
    the sklearn default value of *C* = 1 is a good starting point. We’ll select *C*
    values around 1 but over several orders of magnitude. Specifically, we’ll select
    from *C* ∈ *{*0.001,0.01,0.1,1.0,2.0,10.0,50.0,100.0*}*. Running one thousand
    five-fold validations, each for a different random ordering of the full dataset,
    gives grand means as shown in [Table 7-8](ch07.xhtml#ch7tab8).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该搜索哪个范围的*C*值？答案取决于具体问题，但sklearn的默认*C* = 1是一个不错的起点。我们将选择围绕1的*C*值，但跨越几个数量级。具体来说，我们将从*C*
    ∈ {*0.001, 0.01, 0.1, 1.0, 2.0, 10.0, 50.0, 100.0*}中选择。进行一千次五折验证，每次对完整数据集进行不同的随机排序，得到的总体均值如[表7-8](ch07.xhtml#ch7tab8)所示。
- en: '**Table 7-8:** Breast Cancer Scores as Grand Mean for Different SVM C Values
    and Five-Fold Validation'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**表7-8**：不同SVM C值及五折验证下的乳腺癌评分的总体均值'
- en: '| **C** | **Grand mean** ± **SE** |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| **C** | **总体均值** ± **标准误差** |'
- en: '| --- | --- |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.001 | 0.938500 ± 0.000066 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 0.001 | 0.938500 ± 0.000066 |'
- en: '| 0.01 | 0.967151 ± 0.000089 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 0.01 | 0.967151 ± 0.000089 |'
- en: '| 0.1 | 0.975943 ± 0.000101 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.975943 ± 0.000101 |'
- en: '| 1.0 | 0.971890 ± 0.000141 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 0.971890 ± 0.000141 |'
- en: '| 2.0 | 0.969994 ± 0.000144 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | 0.969994 ± 0.000144 |'
- en: '| 10.0 | 0.966239 ± 0.000154 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 10.0 | 0.966239 ± 0.000154 |'
- en: '| 50.0 | 0.959637 ± 0.000186 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 50.0 | 0.959637 ± 0.000186 |'
- en: '| 100.0 | 0.957006 ± 0.000189 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 100.0 | 0.957006 ± 0.000189 |'
- en: '*C* = 0.1 gives the best accuracy. While, statistically, the difference between
    *C* = 0.1 and *C* = 1 is meaningful, in practice the difference is only about
    0.4 percent, so the default value of *C* = 1 would likewise be a reasonable choice.
    Further refinement of *C* is possible because we see that *C* = 0.01 and *C* =
    2 give the same accuracy, while *C* = 0.1 is higher than either, implying that
    if the *C* curve is smooth, there’s a maximum accuracy for some *C* in [0.01,2.0].'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '*C* = 0.1 给出了最佳准确率。虽然从统计上讲，*C* = 0.1和*C* = 1之间的差异是有意义的，但在实际操作中，这个差异只有大约0.4个百分点，因此默认值*C*
    = 1同样是一个合理的选择。进一步细化*C*是可能的，因为我们看到*C* = 0.01和*C* = 2给出了相同的准确率，而*C* = 0.1的准确率更高，意味着如果*C*曲线是平滑的，那么在[0.01,
    2.0]区间内存在一个最大准确率的*C*值。'
- en: 'Finding the right *C* for our dataset is a crucial part of successfully using
    a linear SVM. Our preceding rough run used a one-dimensional grid search. We do
    expect, since *C* is continuous, that a plot of the accuracy as a function of
    *C* will also be smooth. If that’s the case, one can imagine searching for the
    right *C*, not with a grid search but with an optimization algorithm. In practice,
    however, the randomness of the ordering of the dataset and its effect on the output
    of *k*-fold cross-validation results will probably make any *C* found by an optimization
    algorithm too specific to the problem at hand. Grid search over a larger scale,
    with possibly one level of refinement, is sufficient in most cases. The take-home
    message is: do spend some time looking for the proper *C* value to maximize the
    effectiveness of the linear SVM.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 为数据集找到合适的*C*是成功使用线性SVM的关键部分。我们之前的粗略运行使用了一维网格搜索。我们预计，由于*C*是连续的，准确度随*C*变化的图像也会是平滑的。如果是这样，可以想象通过优化算法而不是网格搜索来寻找合适的*C*。然而，在实践中，数据集的随机排序及其对*k*折交叉验证结果的影响，可能会使通过优化算法找到的任何*C*都过于针对当前问题。大规模的网格搜索，并可能经过一次细化，在大多数情况下已足够。要点是：确实要花些时间寻找合适的*C*值，以最大化线性SVM的有效性。
- en: Observant readers will have noticed that the preceding analysis has ignored
    the RBF kernel SVM. Let’s revisit it now and see how to do a simple two-dimensional
    grid search over *C* and *γ*, where *γ* is the parameter associated with the RBF
    (Gaussian) kernel. sklearn has the GridSearchCV class to perform sophisticated
    grid searching. We’re not using it here to be pedagogical and show how to do simple
    grid searches directly. It’s especially important for this kernel to select good
    values for both of these parameters.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能已经注意到，前面的分析忽略了RBF核SVM。现在让我们重新审视一下，并看看如何在*C*和*γ*上进行简单的二维网格搜索，其中*γ*是与RBF（高斯）核相关的参数。sklearn有一个GridSearchCV类可以执行复杂的网格搜索。我们在这里没有使用它，而是为了教学目的，展示如何直接进行简单的网格搜索。对于这个核函数来说，选择这两个参数的合适值尤其重要。
- en: For the search, we’ll use the same range of *C* values as we used for the linear
    case. For *γ* we’ll use powers of two, 2^(*p*), times the sklearn default value,
    1/30 = 0.03333 for *p* ∈ [*–*4,3]. The search will, for the current *C* value,
    do five-fold validation over the dataset for each *γ* value before moving to the
    next *C* value so that all pairs of (*C*,*γ*) are considered. The pair that results
    in the largest score (accuracy) will be output. The code is in [Listing 7-6](ch07.xhtml#ch7lis6).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn.svm import SVC
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: clf.fit(x_train, y_train)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: return clf.score(x_test, y_test)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'def split(x,y,k,m):'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: ns = int(y.shape[0]/m)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: s = []
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: x_test, y_test = s[k]
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: x_train = []
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: y_train = []
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(m):'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'if (i==k):'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: a,b = s[i]
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: x_train.append(a)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: y_train.append(b)
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: x_train = np.array(x_train).reshape(((m-1)*ns,30))
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.array(y_train).reshape((m-1)*ns)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: return [x_train, y_train, x_test, y_test]
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: m = 5
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: x = np.load("../data/breast/bc_features_standard.npy")
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/breast/bc_labels.npy")
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: x = x[idx]
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: y = y[idx]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: ❶ Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: zmax = 0.0
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '❷ for C in Cs:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'for g in gs:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: z = np.zeros(m)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'for k in range(m):'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: x_train, y_train, x_test, y_test = split(x,y,k,m)
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: z[k] = run(x_train, y_train, x_test, y_test,
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: SVC(C=C,gamma=g,kernel="rbf"))
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '❸ if (z.mean() > zmax):'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: zmax = z.mean()
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: bestC = C
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: bestg = g
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: print("best C     = %0.5f" % bestC)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: print("     gamma = %0.5f" % bestg)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: print("   accuracy= %0.5f" % zmax)
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-6: A two-dimensional grid search for C and  for an RBF kernel SVM.
    Breast cancer dataset. See* bc_rbf_svm_search.py.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: The two helper functions, run and split, are exactly the same as we used before
    (see [Listing 7-4](ch07.xhtml#ch7lis4)); all the action is in main. We fix the
    number of folds at five and then load and randomize the full dataset.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: We then define the specific *C* and *γ* values to search over ❶. Note how gs
    is defined. The first part is 1/30, the reciprocal of the number of features.
    This is the default value for *γ* used by sklearn. We then multiply this factor
    by an array, (2^(*–*4),2^(*–*3),2^(*–*1),2⁰,2¹,2²,2³), to get the final *γ* values
    we’ll search over. Notice that one of the *γ* values is exactly the default sklearn
    uses since 2⁰ = 1.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: The double loop ❷ iterates over all possible pairs of *C* and *γ*. For each
    one, we do five-fold validation to get a set of five scores in z. We then ask
    if the mean of this set is greater than the current maximum (*z*[max]) and if
    so, update the maximum and keep the *C* and *γ* values as our current bests ❸.
    When the loops over *C* and *γ* exit, we have our best values in bestC and bestg.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: If we run this code repeatedly, we’ll get different outputs each time. This
    is because we’re randomizing the order of the full dataset, which will alter the
    subsets in the folds, leading to a different mean score over the folds. For example,
    10 runs produced the output in [Table 7-9](ch07.xhtml#ch7tab9).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们反复运行这段代码，每次都会得到不同的输出。这是因为我们随机化了完整数据集的顺序，这将改变折中的子集，从而导致每个折中的均值得分不同。例如，10
    次运行产生了 [表 7-9](ch07.xhtml#ch7tab9) 中的输出。
- en: '**Table 7-9:** Breast Cancer Scores for an RBF SVM with Different C and *γ*
    Values Averaged Over 10 Runs'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-9：** 不同 C 和 *γ* 值的 RBF SVM 在 10 次运行中的乳腺癌评分平均值'
- en: '| ***C*** | ***γ*** | ***accuracy*** |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| ***C*** | ***γ*** | ***准确度*** |'
- en: '| --- | --- | --- |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 0.03333 | 0.97345 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.03333 | 0.97345 |'
- en: '| 2 | 0.03333 | 0.98053 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.03333 | 0.98053 |'
- en: '| 10 | 0.00417 | 0.97876 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.97876 |'
- en: '| 10 | 0.00417 | 0.97699 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.97699 |'
- en: '| 10 | 0.00417 | 0.98053 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.98053 |'
- en: '| 10 | 0.01667 | 0.98053 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.01667 | 0.98053 |'
- en: '| 10 | 0.01667 | 0.97876 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.01667 | 0.97876 |'
- en: '| 10 | 0.01667 | 0.98053 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.01667 | 0.98053 |'
- en: '| 1 | 0.03333 | 0.97522 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.03333 | 0.97522 |'
- en: '| 10 | 0.00417 | 0.97876 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.00417 | 0.97876 |'
- en: These results hint that (*C*,*γ*) = (10,0.00417) is a good combination. If we
    use these values to generate a grand mean over 1,000 runs of five-fold validation
    as before, we get an overall accuracy of 0.976991, or 97.70 percent, which is
    the highest grand mean accuracy of any model type we trained on the breast cancer
    histology dataset.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果暗示 (*C*,*γ*) = (10,0.00417) 是一个不错的组合。如果我们使用这些值生成一个在 1,000 次运行的五折交叉验证中的总均值，我们得到的整体准确率为
    0.976991，即 97.70%，这是我们在乳腺癌组织学数据集上训练的所有模型类型中最高的总均值准确率。
- en: The breast cancer dataset is not a large dataset. We were able to use *k*-fold
    validation to find a good model that worked well with it. Now, let’s move from
    a pure vector-only dataset to one that is actually image-based and much larger,
    the MNIST dataset.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集不是一个大数据集。我们能够使用 *k* 折交叉验证来找到一个表现良好的模型。现在，让我们从纯粹的向量数据集转到一个实际基于图像且更大的数据集——MNIST
    数据集。
- en: Experiments with the MNIST Dataset
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST 数据集的实验
- en: The last dataset we’ll work with in this chapter is the vector version of the
    MNIST handwritten digit dataset (see [Chapter 5](ch05.xhtml#ch05)). Recall, this
    dataset consists of 28×28 pixel grayscale images of handwritten digits, [0,9],
    one digit centered per image. This dataset is by far the most common workhorse
    dataset in machine learning, especially in deep learning, and we’ll use it throughout
    the remainder of the book.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们要处理的最后一个数据集是 MNIST 手写数字数据集的向量版本（见 [第 5 章](ch05.xhtml#ch05)）。回想一下，这个数据集由
    28×28 像素的灰度图像组成，图像中的手写数字为 [0,9]，每张图像中间是一个数字。这个数据集是机器学习中最常用的工作马数据集，特别是在深度学习领域，整本书接下来的部分我们将一直使用它。
- en: Testing the Classical Models
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试经典模型
- en: MNIST contains 60,000 training images, roughly evenly split among the digits,
    and 10,000 test images. Since we have a lot of training data, at least for classic
    models like those we’re concerned with here, we won’t make use of *k*-fold validation,
    though we certainly could. We’ll train on the training data and test on the testing
    data and trust that the two come from a common parent distribution (they do).
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 包含 60,000 张训练图像，数字大致均匀分布，另外还有 10,000 张测试图像。由于我们有大量的训练数据，至少对于我们关注的经典模型而言，我们不使用
    *k* 折交叉验证，尽管我们完全可以这么做。我们将在训练数据上进行训练，在测试数据上进行测试，并相信这两者来自于一个共同的父分布（它们确实如此）。
- en: Since our classic models expect vector inputs, we’ll use the vector form of
    the MNIST dataset we created in [Chapter 5](ch05.xhtml#ch05). The images are unraveled
    so that the first 28 elements of the vector are row 0, the next 28 are row 1,
    and so on for an input vector of 28 × 28 = 784 elements. The images are stored
    as 8-bit grayscale, so the data values run from 0 to 255\. We’ll consider three
    versions of the dataset. The first is the raw byte version. The second is a version
    where we scale the data to [0,1) by dividing by 256, the number of possible grayscale
    values. The third is a normalized version where, per “feature” (really, pixel),
    we subtract the mean of that feature across the dataset and then divide by the
    standard deviation. This will let us explore how the range of the feature values
    affects things, if at all.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的经典模型期望向量输入，我们将使用在 [第 5 章](ch05.xhtml#ch05) 中创建的 MNIST 数据集的向量形式。图像被展开，因此向量的前
    28 个元素是第 0 行，接下来的 28 个是第 1 行，依此类推，形成一个包含 28 × 28 = 784 个元素的输入向量。图像以 8 位灰度存储，因此数据值范围是从
    0 到 255。我们将考虑数据集的三种版本。第一种是原始字节版本。第二种是将数据通过除以 256（可能的灰度值数量）缩放到 [0,1) 的版本。第三种是归一化版本，对于每个“特征”（实际上是像素），我们减去该特征在数据集中的均值，然后除以标准差。这将帮助我们探索特征值的范围是否会产生影响。
- en: '[Figure 7-1](ch07.xhtml#ch7fig1) shows examples of the original images and
    the resulting normalized vectors raveled back into images and scaled [0,255].
    Normalizing affects the appearance but does not destroy spatial relationships
    among the parts of the digit images. Just scaling the data to [0,1) will result
    in images that look the same as those on the top of [Figure 7-1](ch07.xhtml#ch7fig1).'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-1](ch07.xhtml#ch7fig1) 显示了原始图像和结果归一化向量的示例，这些向量被重新转回图像并缩放至 [0,255]。归一化会影响外观，但不会破坏数字图像各部分之间的空间关系。仅将数据缩放到
    [0,1) 会导致图像看起来与 [图 7-1](ch07.xhtml#ch7fig1) 顶部的图像相同。'
- en: '![image](Images/07fig01.jpg)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/07fig01.jpg)'
- en: '*Figure 7-1: Original MNIST digits (top) and normalized versions used by the
    models (bottom)*'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7-1：原始 MNIST 数字（顶部）和模型使用的归一化版本（底部）*'
- en: The code we’ll use is very similar to what we used previously, but for reasons
    that will be explained next, we will replace the SVC class with a new SVM class,
    LinearSVC. First, take a look at the helper functions in [Listing 7-7](ch07.xhtml#ch7lis7).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的代码与之前使用的非常相似，但由于接下来将解释的原因，我们将用新的 SVM 类 LinearSVC 替换 SVC 类。首先，看看 [列表 7-7](ch07.xhtml#ch7lis7)
    中的辅助函数。
- en: import time
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: import time
- en: import numpy as np
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB, MultinomialNB
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB, MultinomialNB
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import LinearSVC
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import LinearSVC
- en: from sklearn import decomposition
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn import decomposition
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: s = time.time()
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: s = time.time()
- en: clf.fit(x_train, y_train)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: e_train = time.time() - s
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: e_train = time.time() - s
- en: s = time.time()
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: s = time.time()
- en: score = clf.score(x_test, y_test)
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: score = clf.score(x_test, y_test)
- en: e_test = time.time() - s
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: e_test = time.time() - s
- en: print("score = %0.4f (time, train=%8.3f, test=%8.3f)"
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: print("score = %0.4f (time, train=%8.3f, test=%8.3f)"
- en: '% (score, e_train, e_test))'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '% (score, e_train, e_test))'
- en: 'def train(x_train, y_train, x_test, y_test):'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 'def train(x_train, y_train, x_test, y_test):'
- en: 'print("    Nearest Centroid          : ", end='''')'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    最近质心              : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: 'print("    k-NN classifier (k=3)     : ", end='''')'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    k-NN 分类器 (k=3)     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=3))
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=3))
- en: 'print("    k-NN classifier (k=7)     : ", end='''')'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    k-NN 分类器 (k=7)     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: KNeighborsClassifier(n_neighbors=7))
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: KNeighborsClassifier(n_neighbors=7))
- en: 'print("    Naive Bayes (Gaussian)    : ", end='''')'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    朴素贝叶斯（高斯）    : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, GaussianNB())
- en: 'print("    Decision Tree             : ", end='''')'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    决策树              : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())
- en: 'print("    Random Forest (trees=  5) : ", end='''')'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    随机森林（树= 5） : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: 'print("    Random Forest (trees= 50) : ", end='''')'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Random Forest (trees=500) : ", end='''')'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=500))
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    Random Forest (trees=1000): ", end='''')'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=1000))
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=0.01)        : ", end='''')'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=0.1)         : ", end='''')'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=1.0)         : ", end='''')'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: 'print("    LinearSVM (C=10.0)        : ", end='''')'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-7: Training differently scaled versions of the MNIST dataset using
    classic models. Helper functions. See* mnist_experiments.py.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: The run function of [Listing 7-7](ch07.xhtml#ch7lis7) is also similar to those
    used previously, except we’ve added code to time how long training and testing
    takes. These times are reported along with the score. We added this code for MNIST
    because, unlike the tiny iris and breast cancer datasets, MNIST has a larger number
    of training samples so that runtime differences among the model types will start
    to show themselves. The train function is new, but all it does is wrap calls to
    run for the different model types.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: Now take a look at [Listing 7-8](ch07.xhtml#ch7lis8), which contains the main
    function.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: x_train = np.load("mnist_train_vectors.npy").astype("float64")
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.load("mnist_train_labels.npy")
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("mnist_test_vectors.npy").astype("float64")
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("mnist_test_labels.npy")
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on raw [0,255] images:")
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: train(x_train, y_train, x_test, y_test)
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on raw [0,1) images:")
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: train(x_train/256.0, y_train, x_test/256.0, y_test)
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: ❶ m = x_train.mean(axis=0)
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: s = x_train.std(axis=0) + 1e-8
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: x_ntrain = (x_train - m) / s
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: x_ntest  = (x_test - m) / s
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on normalized images:")
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: train(x_ntrain, y_train, x_ntest, y_test)
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: ❷ pca = decomposition.PCA(n_components=15)
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(x_ntrain)
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: x_ptrain = pca.transform(x_ntrain)
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: x_ptest = pca.transform(x_ntest)
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: print("Models trained on first 15 PCA components of normalized images:")
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: train(x_ptrain, y_train, x_ptest, y_test)
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-8: Training differently scaled versions of the MNIST dataset using
    classic models. Main function. See* mnist_experiments.py.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: The main function of [Listing 7-8](ch07.xhtml#ch7lis8) loads the data and then
    trains the models using the raw byte values. It then repeats the training using
    a scaled [0,1) version of the data and a scaled version of the testing data. These
    are the first two versions of the dataset we’ll use.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing the data requires knowledge of the per feature means and standard
    deviations ❶. Note, we add a small value to the standard deviations to make up
    for pixels that have a standard deviation of zero. We can’t divide by zero, after
    all. We need to normalize the test data, but which means and which standard deviations
    should we use? Generally, we have more training data than testing data, so using
    the means and standard deviations from the training data makes sense; they are
    a better representation of the true means and standard deviations of the parent
    distribution that generated the data in the first place. However, at times, there
    may be slight differences between the training and testing data distributions,
    in which case it might make sense to consider the testing means and standard deviations.
    In this case, because the MNIST training and test datasets were created together,
    there’s no difference, so the training values are what we’ll use. Note that the
    same per feature means and standard deviations will need to be used for all new,
    unknown samples, too.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化需要了解每个特征的均值和标准差❶。请注意，我们在标准差中加了一个小值，以补偿那些标准差为零的像素。毕竟，我们不能除以零。我们需要对测试数据进行标准化，但应该使用哪些均值和标准差呢？通常，训练数据比测试数据多，因此使用训练数据的均值和标准差是合理的；它们更能代表最初生成数据的母体分布的真实均值和标准差。然而，有时训练数据和测试数据的分布可能会略有不同，在这种情况下，考虑测试数据的均值和标准差也许更有意义。在这种情况下，由于MNIST的训练集和测试集是一起创建的，因此没有区别，所以我们将使用训练集的值。需要注意的是，所有新的未知样本也需要使用相同的每个特征的均值和标准差。
- en: Next, we apply PCA to the dataset just as we did for the iris data in [Chapter
    5](ch05.xhtml#ch05) ❷. Here we’re keeping the first 15 components. These account
    for just over 33 percent of the variance in the data and reduce the feature vector
    from 784 features (the pixels) to 15 features (the principal components). Then
    we train the models using these features.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对数据集应用PCA，就像我们在[第5章](ch05.xhtml#ch05)中对鸢尾花数据所做的那样❷。在这里，我们保留前15个主成分。这些主成分解释了数据中超过33%的方差，并将特征向量从784个特征（像素）减少到15个特征（主成分）。然后，我们使用这些特征来训练模型。
- en: Running this code produces a wealth of output that we can learn from. Let’s
    first consider the scores per model type and data source. These are in [Table
    7-10](ch07.xhtml#ch7tab10); values in parentheses are the number of trees in the
    Random Forest.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码会产生大量的输出，供我们学习。让我们首先考虑每种模型类型和数据来源的得分。这些得分在[表 7-10](ch07.xhtml#ch7tab10)中；括号中的值是随机森林中的树木数量。
- en: '**Table 7-10:** MNIST Model Scores for Different Preprocessing Steps'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 7-10：** 不同预处理步骤的 MNIST 模型得分'
- en: '| **Model** | **Raw [0,255]** | **Scaled [0,1)** | **Normalized** | **PCA**
    |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **原始 [0,255]** | **缩放 [0,1)** | **标准化** | **PCA** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Nearest Centroid | 0.8203 | 0.8203 | 0.8092 | 0.7523 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| 最近质心法 | 0.8203 | 0.8203 | 0.8092 | 0.7523 |'
- en: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 | 0.9452 | 0.9355 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 | 0.9452 | 0.9355 |'
- en: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 | 0.9433 | 0.9370 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 | 0.9433 | 0.9370 |'
- en: '| Naïve Bayes | 0.5558 | 0.5558 | 0.5239 | 0.7996 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| 天真贝叶斯 | 0.5558 | 0.5558 | 0.5239 | 0.7996 |'
- en: '| Decision Tree | 0.8773 | 0.8784 | 0.8787 | 0.8403 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.8773 | 0.8784 | 0.8787 | 0.8403 |'
- en: '| Random Forest (5) | 0.9244 | 0.9244 | 0.9220 | 0.8845 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (5) | 0.9244 | 0.9244 | 0.9220 | 0.8845 |'
- en: '| Random Forest (50) | 0.9660 | 0.9661 | 0.9676 | 0.9215 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (50) | 0.9660 | 0.9661 | 0.9676 | 0.9215 |'
- en: '| Random Forest (500) | 0.9708 | 0.9709 | 0.9725 | 0.9262 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (500) | 0.9708 | 0.9709 | 0.9725 | 0.9262 |'
- en: '| Random Forest (1000) | 0.9715 | 0.9716 | 0.9719 | 0.9264 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 (1000) | 0.9715 | 0.9716 | 0.9719 | 0.9264 |'
- en: '| LinearSVM (C = 0.01) | 0.8494 | 0.9171 | 0.9158 | 0.8291 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 0.01) | 0.8494 | 0.9171 | 0.9158 | 0.8291 |'
- en: '| LinearSVM (C = 0.1) | 0.8592 | 0.9181 | 0.9163 | 0.8306 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 0.1) | 0.8592 | 0.9181 | 0.9163 | 0.8306 |'
- en: '| LinearSVM (C = 1.0) | 0.8639 | 0.9182 | 0.9079 | 0.8322 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 1.0) | 0.8639 | 0.9182 | 0.9079 | 0.8322 |'
- en: '| LinearSVM (C = 10.0) | 0.8798 | 0.9019 | 0.8787 | 0.7603 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| LinearSVM (C = 10.0) | 0.8798 | 0.9019 | 0.8787 | 0.7603 |'
- en: Look at the Nearest Centroid scores. These make sense as we move from left to
    right across the different versions of the dataset. For the raw data, the center
    location of each of the 10 classes leads to a simple classifier with an accuracy
    of 82 percent—not too bad considering random guessing would have an accuracy closer
    to 10 percent (1/10 for 10 classes). Scaling the data by a constant won’t change
    the relative relationship between the per class centroids so we’d expect the same
    performance in column 2 of [Table 7-10](ch07.xhtml#ch7tab10) as in column 1.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下最近质心分数。这些分数随着我们从左到右遍历数据集的不同版本而变得更有意义。对于原始数据，每个10个类别的中心位置产生了一个简单的分类器，准确率为82%，考虑到随机猜测的准确率大约为10%（10个类别的1/10），这个结果还算不错。通过常数缩放数据并不会改变每个类别质心之间的相对关系，所以我们预期在[表格7-10](ch07.xhtml#ch7tab10)的第二列中，会有与第一列相同的表现。
- en: Normalizing, however, does more than divide the data by a constant. We saw the
    effect clearly in [Figure 7-1](ch07.xhtml#ch7fig1). This alteration, at least
    for the MNIST dataset, changes the centroids’ relationships to each other and
    results in a decrease in accuracy to 80.9 percent.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，归一化不仅仅是通过常数划分数据。我们在[图7-1](ch07.xhtml#ch7fig1)中清晰地看到了这一效果。至少对于MNIST数据集而言，这一变化改变了质心之间的关系，并导致准确率降至80.9%。
- en: Finally, using PCA to reduce the number of features from 784 to 15 has a severe
    negative impact, resulting in an accuracy of only 75.2 percent. Note the word
    *only*. In the past, before the advent of deep learning, an accuracy of 75 percent
    on a problem with 10 classes would generally have been considered to be pretty
    good. Of course, it really isn’t. Who would get in a self-driving car that has
    an accident one time out of every four trips? We want to do better.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用PCA将特征数量从784减少到15会产生严重的负面影响，导致准确率仅为75.2%。请注意“仅有”这个词。过去，在深度学习出现之前，解决一个有10个类别的问题，准确率达到75%通常会被认为是相当不错的。当然，实际上它并不算好。谁会乘坐一个每四次出行就发生一次事故的自动驾驶汽车呢？我们希望做得更好。
- en: Let’s consider the *k*-NN classifiers next. We see similar performance for both
    *k* = 3 and *k* = 7 and the same sort of trend as we saw with the Nearest Centroid
    classifier. This is to be expected given how similar the two types of models actually
    are. The difference in accuracy between the two (centroid and *k*-NN) is dramatic,
    however. An accuracy of 97 percent is generally regarded as good. But still, who
    would opt for elective surgery with a 3 percent failure rate?
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们考虑 *k*-最近邻分类器。我们看到无论是 *k* = 3 还是 *k* = 7，其表现都相似，并且和最近质心分类器的趋势相同。鉴于这两种模型的相似性，这样的表现是可以预期的。然而，准确率上的差异（质心和
    *k*-NN）是显著的。97%的准确率通常被认为是很好的。然而，谁会选择进行失败率为3%的择期手术呢？
- en: Things get interesting when we look at the Naïve Bayes classifier. Here all
    the versions of the dataset perform poorly, though still five times better than
    guessing. We see a large jump in accuracy with the PCA processed dataset, from
    56 percent to 80 percent. This is the only model type to improve after using PCA.
    Why might this be? Remember, we’re using Gaussian Naïve Bayes, which means our
    independence assumption is coupled with an assumption that the continuous feature
    values are, per feature, really drawn from a normal distribution whose parameters,
    the mean and standard deviation, we can estimate from the feature values themselves.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到朴素贝叶斯分类器时，事情变得有趣。这里所有版本的数据集表现都很差，尽管仍然比猜测好五倍。我们看到PCA处理后的数据集准确率有了显著提升，从56%增至80%。这是唯一一个在使用PCA后有所改进的模型类型。这是为什么呢？记住，我们使用的是高斯朴素贝叶斯，这意味着我们的独立性假设与连续特征值假设相结合，即每个特征的值实际上是从一个正态分布中抽取的，我们可以从特征值本身估计其均值和标准差。
- en: Now recall what PCA does, geometrically. It’s the equivalent of rotating the
    feature vectors onto a new set of coordinates aligned with the largest orthogonal
    directions derivable from the dataset. The word *orthogonal* implies that no part
    of a direction overlaps with any other part of any other direction. Think of the
    x-, y-, and z-axes of a three-dimensional plot. No part of the *x* is along the
    *y* or *z*, and so forth. This is what PCA does. Therefore, PCA makes the first
    assumption of Naïve Bayes more likely to be true, that the new features are indeed
    independent of each other. Add in the Gaussian assumption as to the distribution
    of the per pixel values, and we have an explanation for what we see in [Table
    7-10](ch07.xhtml#ch7tab10).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回忆一下PCA的几何意义。它相当于将特征向量旋转到一个新的坐标系，该坐标系与从数据集中推导出的最大正交方向对齐。*正交*一词意味着一个方向的任何部分都不会与任何其他方向的部分重叠。可以想象三维坐标系中的x、y和z轴。*x*轴的任何部分都不与*y*或*z*轴重合，依此类推。这就是PCA的作用。因此，PCA使得朴素贝叶斯的第一个假设更有可能成立，即新的特征确实相互独立。再加上对每个像素值分布的高斯假设，我们就能解释[表7-10](ch07.xhtml#ch7tab10)中看到的现象。
- en: The tree-based classifiers, Decision Tree and Random Forest, perform much the
    same until we get to the PCA version of the dataset. Indeed, there is no difference
    between the raw data and the data scaled by 256\. Again, this is to be expected
    as all scaling by a constant does is scale the decision thresholds for each of
    the nodes in the body of the tree or trees. As before, working with reduced dimensionality
    vectors via PCA results in a loss of accuracy because potentially important information
    has been discarded.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的分类器，决策树和随机森林，直到我们使用PCA版本的数据集时，表现基本相同。事实上，原始数据和通过256缩放后的数据没有差别。再次说明，这在预期之内，因为按常数进行缩放仅仅是缩放了树或树体中每个节点的决策阈值。如前所述，使用PCA降维后的特征向量会导致准确性损失，因为可能有重要信息被丢弃。
- en: For any data source, we see scores that make sense relative to each other. As
    before, the single Decision Tree performs worst, which it should except for simple
    cases since it’s competing against a collection of trees via the Random Forests.
    For the Random Forests, we see that the score improves as the number of trees
    in the forest increases—again expected. However, the improvement comes with diminishing
    returns. There’s a significant improvement when going from 5 trees to 50 trees,
    but a minimal improvement in going from 500 trees to 1,000 trees.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何数据源，我们看到的得分在相互之间是有意义的。如同之前一样，单一的决策树表现最差，这是应该的，除非是简单情况，因为它正在与通过随机森林组合的多棵树竞争。对于随机森林，我们看到随着森林中树木数量的增加，得分有所提高——这是预期之中的。然而，改进的幅度逐渐递减。从5棵树到50棵树时，改进显著，但从500棵树到1000棵树时，改进几乎可以忽略不计。
- en: Before we look at the SVM results, let’s understand why we made the switch from
    the SVC class to LinearSVC. As the name suggests, LinearSVC implements only a
    linear kernel. The SVC class is more generic and can implement other kernels,
    so why switch?
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看SVM结果之前，让我们先了解一下为什么我们从SVC类切换到了LinearSVC。顾名思义，LinearSVC仅实现了线性核。SVC类则更为通用，可以实现其他核。那么，为什么要切换呢？
- en: The reason has to do with runtime. In computer science, there are specific definitions
    of complexity and an entire branch devoted to the analysis of algorithms and how
    they perform as their inputs scale larger and larger. All we’ll concern ourselves
    with here is *big-O* notation. This is a way of characterizing how the runtime
    of an algorithm changes as the input (or the number of inputs) gets larger and
    larger.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 这个原因与运行时有关。在计算机科学中，有具体的复杂度定义，并且有一个专门的分支致力于算法的分析，以及它们在输入规模越来越大的情况下的表现。我们在这里关心的只是*大O*符号。这是一种描述算法运行时如何随着输入（或输入数量）增大而变化的方法。
- en: For example, a classic bubble sort algorithm works just fine on a few dozen
    numbers to be sorted. But, as the input gets larger (more numbers to be sorted),
    the runtime increases not linearly but quadratically, meaning the time to sort
    the numbers, *t*, is proportional to the *square* of the number of numbers to
    be sorted, *t* ∝ *n*², which is written as *O*(*n*²). So, the bubble sort is an
    order *n*² algorithm. In general, we want algorithms that are better than *n*²,
    more like *n*, written as *O*(*n*), or even independent of *n*, written as *O*(1).
    It turns out that the kernel algorithm for training an SVM is *worse* than *O*(*n*²)
    so that when the number of training samples increases, the runtime explodes. This
    is one reason for the switch from the SVC class to LinearSVC, which doesn’t use
    kernels.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，经典的冒泡排序算法在排序几十个数字时表现良好。但是，当输入变大（要排序的数字增多）时，运行时间的增加不是线性的，而是二次方的，意味着排序数字所需的时间
    *t* 与要排序的数字个数的 *平方* 成正比，即 *t* ∝ *n*²，记作 *O*(*n*²)。因此，冒泡排序是一个 *n*² 阶的算法。一般来说，我们希望算法的复杂度比
    *n*² 更好，接近 *n*，记作 *O*(*n*)，甚至独立于 *n*，记作 *O*(1)。结果发现，训练 SVM 的核算法的复杂度比 *O*(*n*²)
    更差，因此当训练样本数增多时，运行时会急剧增加。这也是从 SVC 类切换到 LinearSVC 类的一个原因，后者不使用核函数。
- en: 'The second reason for the switch has to do with the fact that Support Vector
    Machines are designed for binary classification—only two classes. The MNIST dataset
    has 10 classes, so something different has to be done. There are multiple approaches.
    According to the sklearn documentation, the SVC class uses a *one-versus-one*
    approach that trains pairs of classifiers, one class versus another: class 0 versus
    class 1, class 1 versus class 2, class 0 versus class 2, and so on. This means
    it ends up training not one but *m*(*m –* 1)/2 classifiers for *m* = 10 classes,
    or 10(10 *–* 1)/2 = 45 separate classifiers. This isn’t efficient in this case.
    The LinearSVC classifier uses a *one-versus-rest* approach. This means it trains
    an SVM to classify “0” versus “1–9”, then “1” versus “0, 2–9”, and so on, for
    a total of only 10 classifiers, one for each digit.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 切换的第二个原因与支持向量机（SVM）设计用于二分类（只有两个类别）这一事实有关。MNIST 数据集有 10 个类别，因此需要采用不同的处理方式。有多种方法。根据
    sklearn 文档，SVC 类使用 *一对一* 方法来训练一对一的分类器，每次训练两个类别进行对比：类别 0 与类别 1，类别 1 与类别 2，类别 0
    与类别 2，依此类推。这意味着它最终训练的不是一个，而是 *m*（*m - 1*）/2 个分类器，其中 *m* = 10 个类别，即 10（10 - 1）/2
    = 45 个分类器。在这种情况下，这种方法效率不高。LinearSVC 分类器使用 *一对其余* 方法。这意味着它训练一个 SVM 来分类“0”与“1-9”，然后是“1”与“0,
    2-9”，以此类推，总共只训练 10 个分类器，每个数字一个。
- en: It’s with the SVM classifiers that we see a definite benefit to scaling the
    data versus the raw byte inputs. We also see that the optimal *C* value is likely
    between *C* = 0.1 and *C* = 1.0\. Note that simple [0,1) scaling leads to SVM
    models that outperform (for this one dataset!) the models trained on the normalized
    data. The effect is small but consistent for different *C* values. And, as we
    saw before, dropping the dimensionality from 784 features to only 15 features
    via PCA leads to a rather large loss of accuracy. PCA seems not to have helped
    in this case. We’ll come back to it in a bit and see if we can understand why.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SVM 分类器来说，我们确实看到将数据缩放处理相较于直接使用原始字节输入有明显的好处。我们还发现，最佳的 *C* 值可能在 *C* = 0.1 和
    *C* = 1.0 之间。请注意，简单的 [0,1) 缩放会使得 SVM 模型（对于这个数据集来说！）优于在归一化数据上训练的模型。这个效果较小，但对于不同的
    *C* 值是稳定的。而且，正如我们之前所见，通过 PCA 将维度从 784 个特征减少到仅 15 个特征，导致了准确率的较大损失。在这种情况下，PCA 似乎没有带来帮助。稍后我们会回过头来看看，看看能否理解为什么。
- en: Analyzing Runtimes
  id: totrans-598
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析运行时间
- en: Let’s now look at the runtime performance of the algorithms. [Table 7-11](ch07.xhtml#ch7tab11)
    shows the train and test times, in seconds, for each model type and dataset version.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看算法的运行时性能。[表 7-11](ch07.xhtml#ch7tab11)展示了每种模型类型和数据集版本的训练和测试时间（单位：秒）。
- en: Look at the test times. This is how long each model takes to classify all 10,000
    digit images in the test set. The first thing that jumps out at us is that *k*-NN
    is slow. Classifying the test set takes over 10 minutes when full feature vectors
    are used! It’s only when we drop down to the first 15 PCA components that we see
    reasonable *k*-NN runtimes. This is a good example of the price we pay for a seemingly
    simple idea. Recall, the *k*-NN classifier finds the *k* closest training samples
    to the unknown sample we wish to classify. Here *closest* means in a Euclidean
    sense, like the distance between two points on a graph, except in this case we
    don’t have two or three dimensions but 784.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-11:** Training and Testing Times (Seconds) for Each Model Type'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Raw [0,255]** | **Scaled [0,1)** | **Normalized** | **PCA** |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '| **Model** | train | test | train | test | train | test | train | test |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.23 | 0.03 | 0.24 | 0.03 | 0.24 | 0.03 | 0.01 | 0.00
    |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '| *K*-NN (*K* = 3) | 33.24 | 747.34 | 33.63 | 747.22 | 33.66 | 699.58 | 0.09
    | 3.64 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: '| *K*-NN (*K* = 7) | 33.45 | 746.00 | 33.69 | 746.65 | 33.68 | 709.62 | 0.09
    | 4.65 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.80 | 0.88 | 0.85 | 0.90 | 0.83 | 0.94 | 0.02 | 0.01 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 25.42 | 0.03 | 25.41 | 0.02 | 25.42 | 0.02 | 2.10 | 0.00
    |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 2.65 | 0.06 | 2.70 | 0.06 | 2.61 | 0.06 | 1.20 | 0.03
    |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 25.56 | 0.46 | 25.14 | 0.46 | 25.27 | 0.46 | 12.06 |
    0.25 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (500) | 252.65 | 4.41 | 249.69 | 4.47 | 249.19 | 4.45 | 121.10
    | 2.51 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (1000) | 507.52 | 8.86 | 499.23 | 8.71 | 499.10 | 8.91 | 242.44
    | 5.00 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.01) | 169.45 | 0.02 | 5.93 | 0.02 | 232.93 | 0.02 | 16.91
    | 0.00 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.1) | 170.58 | 0.02 | 36.00 | 0.02 | 320.17 | 0.02 | 37.46
    | 0.00 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 1.0) | 170.74 | 0.02 | 96.34 | 0.02 | 488.06 | 0.02 | 66.49
    | 0.00 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 10.0) | 170.46 | 0.02 | 154.34 | 0.02 | 541.69 | 0.02 | 86.87
    | 0.00 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
- en: Therefore, for each of the test samples, we need to find the *k* = 3 or *k*
    = 7 closest points in the training data. The naïve way to do this is to calculate
    the distance between the unknown sample and each of the 60,000 training samples,
    sort them, look at the *k* smallest distances, and vote to decide the output class
    label. This is a lot of work because we have 60,000 training samples and 10,000
    test samples for a total of 600,000,000 distance calculations. It isn’t as bad
    as all that because sklearn automatically selects the algorithm used to find the
    nearest neighbors, and decades of research has uncovered “better than brute force”
    approaches. Curious readers will want to investigate the terms *K-D-tree* and
    *Ball tree* (sometimes called *Metric tree*). See “An Empirical Comparison of
    Exact Nearest Neighbor Algorithms” by Kibriya and Frank (2007). Still, because
    of the extreme difference in runtimes between the other model types and *k*-NN,
    it’s necessary to remember just how slow *k*-NN can be if the dataset is large.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个测试样本，我们需要在训练数据中找到距离最近的*k* = 3或*k* = 7个点。最简单的做法是计算未知样本与每个60,000个训练样本之间的距离，排序后查看距离最小的*k*个值，然后投票决定输出的类标签。这是一项繁琐的工作，因为我们有60,000个训练样本和10,000个测试样本，总共需要进行600,000,000次距离计算。其实并没有那么糟，因为sklearn会自动选择用于查找最近邻的算法，而且几十年的研究已经发现了“比蛮力算法更好的”方法。好奇的读者可能想了解术语*K-D树*和*Ball树*（有时称为*Metric树*）。可以参阅Kibriya和Frank（2007）的《精确最近邻算法的经验比较》。不过，由于其他模型类型和*k*-NN之间的极大运行时间差异，必须记住，如果数据集很大，*k*-NN可能会非常慢。
- en: The next slowest test times are for the Random Forest classifiers. We understand
    why the forest with 500 trees takes 10 times longer to run than the forest with
    50 trees; we have 10 times as many trees to evaluate. Training times also scale
    linearly. Reducing the size of the feature vectors with PCA improves things but
    not by a factor of 50 (784 features divided by 15 PCA features ≈ 50), so the performance
    difference is not primarily influenced by the size of the feature vector.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的测试时间较长的是随机森林分类器。我们理解为什么500棵树的森林运行时间比50棵树的森林要长10倍；我们有10倍的树需要评估。训练时间也呈线性增长。使用PCA减少特征向量的大小确实有帮助，但效果没有达到50倍（784个特征除以15个PCA特征≈50），所以性能差异主要不是由特征向量的大小影响的。
- en: The linear SVMs are the next slowest to train after the Random Forests, but
    their execution time is extremely low. Long training times and short classification
    (inference) times are a hallmark of many model types. The simplest models are
    quick to train and quick to use, like Nearest Centroid or Naïve Bayes, but in
    general, “slow to train, quick to use” is a safe assumption. It’s especially true
    of neural networks.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 线性SVM是继随机森林之后训练最慢的，但它们的执行时间非常低。长时间的训练和短时间的分类（推理）是许多模型类型的特点。最简单的模型训练快速且使用快速，比如最近邻质心或朴素贝叶斯，但一般来说，“训练慢，使用快”是一个可靠的假设。神经网络尤其如此。
- en: Using PCA hurt the performance of the models except for the Naïve Bayes classifier.
    Let’s do an experiment to see the effect of PCA as the number of PCA components
    changes.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA降低了模型的性能，除了朴素贝叶斯分类器。让我们做一个实验，看看随着PCA成分数量的变化，PCA的效果如何。
- en: Experimenting with PCA Components
  id: totrans-623
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验PCA成分
- en: For [Tables 7-10](ch07.xhtml#ch7tab10) and [7-11](ch07.xhtml#ch7tab11), we selected
    15 PCA components that represented about 33 percent of the variance in the dataset.
    This value was selected at random. You could imagine training models using some
    other number of principal components.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[表格7-10](ch07.xhtml#ch7tab10)和[7-11](ch07.xhtml#ch7tab11)，我们选择了15个PCA成分，它们代表了数据集中约33%的方差。这个值是随机选择的。你可以想象使用其他数量的主成分来训练模型。
- en: Let’s examine the effect of the number of PCA components used on the accuracy
    of the resulting model. We’ll vary the number of components from 10 to 780, which
    is basically all the features in the image. For each number of components, we’ll
    train a Naïve Bayes classifier, a Random Forest of 50 trees, and a linear SVM
    with *C* = 1.0\. The code to do this is in [Listing 7-9](ch07.xhtml#ch7lis9).
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查PCA成分数量对结果模型精度的影响。我们将把成分数量从10变到780，基本上涵盖了图像中的所有特征。对于每一个成分数量，我们将训练一个朴素贝叶斯分类器，一个50棵树的随机森林，以及一个*
    C * = 1.0的线性SVM。执行此操作的代码见[清单7-9](ch07.xhtml#ch7lis9)。
- en: 'def main():'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x_train = np.load("../data/mnist/mnist_train_vectors.npy")
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("../data/mnist/mnist_train_vectors.npy")
- en: .astype("float64")
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: .astype("float64")
- en: y_train = np.load("../data/mnist/mnist_train_labels.npy")
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("../data/mnist/mnist_train_labels.npy")
- en: x_test = np.load("../data/mnist/mnist_test_vectors.npy").astype("float64")
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("../data/mnist/mnist_test_labels.npy")
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: m = x_train.mean(axis=0)
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: s = x_train.std(axis=0) + 1e-8
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: x_ntrain = (x_train - m) / s
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: x_ntest  = (x_test - m) / s
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: n = 78
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: pcomp = np.linspace(10,780,n, dtype="int16")
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: nb=np.zeros((n,4))
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: rf=np.zeros((n,4))
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: sv=np.zeros((n,4))
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: tv=np.zeros((n,2))
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: 'for i,p in enumerate(pcomp):'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: ❶ pca = decomposition.PCA(n_components=p)
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(x_ntrain)
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: (*\newpage*)
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: xtrain = pca.transform(x_ntrain)
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: xtest = pca.transform(x_ntest)
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: tv[i,:] = [p, pca.explained_variance_ratio_.sum()]
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: ❷ sc,etrn,etst =run(xtrain, y_train, xtest, y_test, GaussianNB())
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: nb[i,:] = [p,sc,etrn,etst]
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: sc,etrn,etst =run(xtrain, y_train, xtest, y_test,
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: rf[i,:] = [p,sc,etrn,etst]
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: sc,etrn,etst =run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0))
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: sv[i,:] = [p,sc,etrn,etst]
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_tv.npy", tv)
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_nb.npy", nb)
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_rf.npy", rf)
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_pca_sv.npy", sv)
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7-9: Model accuracy as a function of the number of PCA components
    used. See* mnist_pca.py.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: First, we load the MNIST dataset and compute the normalized version. This is
    the version that we’ll use with PCA. Next, we set up storage for the results.
    The variable pcomp stores the specific number of PCA components that will be used
    from 10 to 780 in steps of 10\. Then we start a loop over the number PCA components.
    We find the requested number of components (p) and map the dataset to the actual
    dataset trained and tested (xtrain, xtest) ❶.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: We also store the actual amount of variance in the dataset explained by the
    current number of principal components (tv). We’ll plot this value later to see
    how quickly the number of components covers the majority of the variance in the
    dataset.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: Next, we train and test a Gaussian Naïve Bayes classifier using the current
    number of features ❷. The run function called here is virtually identical to that
    used in [Listing 7-7](ch07.xhtml#ch7lis7) except that it returns the score, the
    training time, and the testing time. These are captured and put into the appropriate
    output array (nb). Then we do the same for the Random Forest and linear SVM.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: When the loop completes, we have all the data we need and we store the NumPy
    arrays on disk for plotting. Running this code takes some time, but the output,
    when plotted, leads to [Figure 7-2](ch07.xhtml#ch7fig2).
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: The solid curve shows the fraction of the total variance in the dataset explained
    by the current number of PCA components (x-axis). This curve will reach a maximum
    of 1.0 when all the features in the dataset are used. It’s helpful in this case
    because it shows how quickly adding new components explains major orientations
    of the data. For MNIST, we see that about 90 percent of the variance is explained
    by using less than half the possible number of PCA components.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig02.jpg)'
  id: totrans-666
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-2: Results of the PCA search*'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: The remaining three curves plot the accuracy of the resulting models on the
    test data. The best-performing model, in this case, is the Random Forest with
    50 trees (triangles). This is followed by the linear SVM (squares) and then Naïve
    Bayes (circles). These curves show how the number of PCA components tracks with
    accuracy, and while the Random Forest and SVM change only slowly as PCA changes,
    we see that the Naïve Bayes classifier rapidly loses accuracy as the number of
    PCA components increases. Even the Random Forest and SVM decrease as the number
    of PCA components increases, which we might expect because the curse of dimensionality
    will eventually creep in. It seems likely that the dramatically different behavior
    of the Naïve Bayes classifier is due to violations of the independence assumption
    as the number of components used increases.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: The maximum accuracy and the number of PCA components where it occurs are shown
    in [Table 7-12](ch07.xhtml#ch7tab12).
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-12:** Maximum Accuracy on MNIST by Model and Number of Components'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Accuracy** | **Components** | **Variance** |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.81390 | 20 | 0.3806 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.94270 | 100 | 0.7033 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| Linear SVM (C = 1.0) | 0.91670 | 370 | 0.9618 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '[Table 7-12](ch07.xhtml#ch7tab12) tracks with the plot in [Figure 7-2](ch07.xhtml#ch7fig2).
    Interestingly, the SVM does not reach a maximum until nearly all the features
    in the original dataset are used. Also, the best accuracy found for the Random
    Forest and SVM is not as good as seen previously for other versions of the dataset
    that did not use PCA. So, for these models, PCA is not a benefit; it is, however,
    for the Naïve Bayes classifier.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: Scrambling Our Dataset
  id: totrans-677
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before we leave this section, let’s look at one more experiment that we’ll
    come back to in [Chapter 9](ch09.xhtml#ch09) and again in [Chapter 12](ch12.xhtml#ch12).
    In [Chapter 5](ch05.xhtml#ch05), we made a version of the MNIST dataset that scrambled
    the order of the pixels in the digit images. The scrambling wasn’t random: the
    same pixel in each input image was moved to the same position in the output image,
    resulting in images that, at least to us, no longer look like the original digit,
    as [Figure 7-3](ch07.xhtml#ch7fig3) shows. How might this scrambling affect the
    accuracy of the models we’ve been using in this chapter?'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig03.jpg)'
  id: totrans-679
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-3: Original MNIST digits (top) and scrambled versions of the same
    digit (bottom).*'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: Let’s repeat the experiment code of [Listing 7-8](ch07.xhtml#ch7lis8), this
    time running only the scaled [0,1) version of the scrambled MNIST images. Since
    the only difference to the original code is the source filenames and the fact
    that we call run only once, we’ll forgo a new listing.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: Placing the accuracy results side by side gives us [Table 7-13](ch07.xhtml#ch7tab13).
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-13:** MNIST Scores by Model Type for Unscrambled and Scrambled Digits'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Unscrambled [0,1)** | **Scrambled [0,1)** |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| Nearest Centroid | 0.8203 | 0.8203 |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: '| *k*-NN (*k* = 3) | 0.9705 | 0.9705 |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
- en: '| *k*-NN (*k* = 7) | 0.9694 | 0.9694 |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.5558 | 0.5558 |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.8784 | 0.8772 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (5) | 0.9244 | 0.9214 |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (50) | 0.9661 | 0.9651 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (500) | 0.9709 | 0.9721 |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (1000) | 0.9716 | 0.9711 |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.01) | 0.9171 | 0.9171 |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 0.1) | 0.9181 | 0.9181 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 1.0) | 0.9182 | 0.9185 |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '| LinearSVM (C = 10.0) | 0.9019 | 0.8885 |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
- en: Here we see virtually no difference between the scrambled and unscrambled results.
    In fact, for several models, the results are identical. For stochastic models,
    like the Random Forests, the results are still very similar. Is this surprising?
    Perhaps at first, but if we think about it for a bit, we realize that it really
    shouldn’t be.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the classic models are holistic: they operate on the entire feature
    vector as a single entity. While we can’t see the digits anymore because our vision
    does not operate holistically, the *information* present in the image is still
    there, so the models are just as happy with the scrambled as unscrambled inputs.
    When we get to [Chapter 12](ch12.xhtml#ch12), we’ll encounter a different result
    of this experiment.'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: Classical Model Summary
  id: totrans-701
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What follows is a summary of the pros and cons related to each of the classical
    model types we have explored in this chapter. This can be used as a quick list
    for future reference. It will also take some of the observations we made via our
    experiments and make them more concrete.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: Nearest Centroid
  id: totrans-703
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the simplest of all the models and can serve as a baseline. It’s seldom
    adequate unless the task at hand is particularly easy. The single centroid for
    each class is needlessly restrictive. You could use a more generalized approach
    that first finds an appropriate number of centroids for each class and then groups
    them together to build the classifier. In the extreme, this approaches *k*-NN
    but is still simpler in that the number of centroids is likely far less than the
    number of training samples. We’ll leave the implementation of this variation as
    an exercise for the motivated reader.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-705
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we saw in this chapter, the implementation of a Nearest Centroid classifier
    takes only a handful of code. Additionally, Nearest Centroid is not restricted
    to binary models and readily supports multiclass models, like the irises. Training
    is very fast and since only one centroid is stored per class, the memory overhead
    is likewise very small. When used to label an unknown sample, run time is also
    very small because the distance from the sample to each class centroid is all
    that needs to be computed.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Nearest Centroid makes a simplistic assumption about the distribution of the
    classes in the feature space—one that’s seldom met in practice. As a consequence
    of this assumption, the Nearest Centroid classifier is only highly accurate when
    the classes form a single tight group in the feature space and the groups are
    distant from each other like isolated islands.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-Nearest Neighbors'
  id: totrans-709
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the simplest model to train since there’s no training: we store the
    training set and use it to classify new instances by finding the *k* nearest training
    set vectors and voting.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-711
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As just mentioned, no training required makes *k*-NN particularly attractive.
    It also can perform quite well, especially if the number of training samples is
    large relative to the dimensionality of the feature space (that is, the number
    of features in the feature vector). Multiclass support is implicit and doesn’t
    require a special approach.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-713
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The simplicity of “training” comes at a cost: classification is slow because
    of the need to look at every training example to find the nearest neighbors to
    the unknown feature vector. Decades of research, still underway, have sped up
    the search to improve the naïve implementation of looking at every training sample
    every time, but, as we saw in this chapter, classification is still slow, especially
    when compared to the speed of other model types (for example, SVM).'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes
  id: totrans-715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This model is conceptually simple and efficient, and surprisingly valid even
    when the core assumption of feature independence isn’t met.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-717
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Naïve Bayes is fast to train and fast to classify with, both positives. It also
    supports multiclass models instead of just binary, and other than continuous features.
    As long as the probability of a particular feature value can be computed, we can
    apply Naïve Bayes.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-719
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The feature independence assumption central to Naïve Bayes is seldom true in
    practice. The more correlated the features (the more a change in, say, feature
    *x*[2] implies that *x*[3] will change), the poorer the performance of the model
    (in all likelihood).
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: While Naïve Bayes works directly with discrete valued features, using continuous
    features often involves a second level of assumption, as when we assumed that
    the continuous breast cancer dataset features were well represented as samples
    from a Gaussian distribution. This second assumption, which is also likely seldom
    true in practice, means that we need to estimate the parameters of the distribution
    from the dataset instead of using histograms to stand in for the actual feature
    probabilities.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  id: totrans-722
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This model is useful when it’s important to be able to understand, in human
    terms, why a particular class was selected.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Decision Trees are reasonably fast to train. They’re also fast to use for classifying.
    Multiclass models are not a problem and are not restricted to using just continuous
    features. A Decision Tree can justify its answer by showing the particular steps
    used to reach a decision: the series of questions asked from the root to the leaf.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-726
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Decision Trees are prone to overfitting—to learning elements of the training
    data that are not generally true of the parent distribution. Also, interpretability
    degrades as the tree increases in size. Tree depth needs to be balanced with the
    quality of the decisions (labels) as the leaves of the tree. This directly affects
    the error rate.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  id: totrans-728
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a more powerful form of Decision Tree that uses randomness to reduce
    the overfitting problem. Random Forests are one of the best performing of the
    classic model types and apply to a wide range of problem domains.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-730
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Like Decision Trees, Random Forests support multiclass models and other than
    continuous features. They are reasonably fast to train and to use for inference.
    Random Forests are also robust to differences in scale between features in the
    feature vector. In general, the accuracy improves, with diminishing returns, as
    the size of the forest grows.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-732
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The easy interpretability of a Decision Tree disappears with a Random Forest.
    While each tree in the forest can justify its decision, the combined effect of
    the forest as a whole can be difficult to understand.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
- en: The inference runtime of a forest scales linearly with the number of trees.
    However, this can be mitigated by parallelization since each tree in the forest
    is making a calculation that does not depend on any other tree until combining
    the output of all trees to make an overall decision.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: As stochastic models, the overall performance of a forest varies from training
    session to training session for the same dataset. In general, this isn’t an issue,
    but a pathological forest could exist—if possible, train the forest several times
    to get a sense of the actual performance.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  id: totrans-736
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before the “rebirth” of neural networks, Support Vector Machines were generally
    considered to provide the pinnacle of model performance when they were applicable
    and well-tuned.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  id: totrans-738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SVMs can give show excellent performance when properly tuned. Inference is very
    fast once trained.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: Cons
  id: totrans-740
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multiclass models are not directly supported. Extensions for multiclass problems
    require training multiple models whether using one-versus-one or one-versus-rest
    approaches. Additionally, SVMs expect only continuous features and feature scaling
    matters; normalization or other scaling is often necessary to get good performance.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: Large datasets are difficult to train when using other than linear kernels,
    and SVMs often require careful tuning of margin and kernel parameters (*C*, *γ*),
    though this can be mitigated somewhat by search algorithms that seek the best
    hyperparameter values.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Classical Models
  id: totrans-743
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The classical models may be *classic*, but they are still appropriate under
    the right conditions. In this section, we’ll discuss when you should consider
    a classical model instead of a more modern approach.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: Handling Small Datasets
  id: totrans-745
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the best reasons for working with a classic model is when the dataset
    is small. If you have only a few tens or hundreds of examples, then a classic
    model might be a good fit, whereas a deep learning model might not have enough
    training data to condition itself to the problem. Of course, there are exceptions.
    A deep neural network can, via transfer learning, sometimes learn from relatively
    few examples. Other approaches, like zero-shot or few-shot learning, may also
    allow a deep network to learn from a small dataset. However, these techniques
    are far beyond the scope of what we want to address in this book. For us, the
    rule of thumb is: when the dataset is small, consider using a classic model.'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Reduced Computational Requirements
  id: totrans-747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another reason to consider a classic model is when computational requirements
    must be kept to a minimum. Deep neural networks are notoriously demanding of computational
    resources. The thousands, millions, and even billions of connections in a deep
    network all require extensive calculation. Implementing such a model on a small
    handheld device, or on an embedded microcontroller, will not work, or at least
    not work in any reasonable timeframe.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: 'In such cases, you might consider a classic model that doesn’t require a lot
    of overhead. Simple models like Nearest Centroid or Naïve Bayes are good candidates.
    So are Decision Trees and Support Vector Machines, once trained. From the previous
    experiments, *k*-NN is probably not a good candidate unless the feature space
    or training set is small. This leads to our next rule of thumb: when computation
    must be kept to a minimum, consider using a classic model.'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: Having Explainable Models
  id: totrans-750
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some classic models can explain themselves by revealing exactly *how* they
    arrived at their answer for a given unknown input. This includes Decision Trees,
    by design, but also *k*-NN (by showing the labels of the *k* voters), Nearest
    Centroid (by virtue of the selected centroid), and even Naïve Bayes (by the selected
    posterior probability). By way of contrast, deep neural networks are black boxes—they
    do not explain themselves—and it’s an active area of research to learn how to
    get a deep network to give some reason for its decision. This research has not
    been entirely unsuccessful, to be sure, but it’s still far from looking like the
    decision path in a tree classifier. Therefore, we can give another rule of thumb:
    when it’s essential to know how the classifier makes its decision, consider using
    a classic model.'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: Working with Vector Inputs
  id: totrans-752
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our final rule of thumb, acknowledging, of course, that there are indeed others
    we could give, has to do with the form of the inputs to the model. Modern deep
    learning systems often work with inputs that are not an amalgamation of separate
    features put into a single vector but instead are multidimensional inputs, such
    as images, where the “features” (pixels) are not different from each other but
    of the same kind and often highly correlated (the red pixel of the apple likely
    has a red pixel next to it, for example). A color image is a three-dimensional
    beast: there are three color images, one for the red channel, one for the blue
    channel, and one for the green channel. If the inputs are images from other sources,
    like satellites, there might be four to eight or more channels per image. A convolutional
    neural network is designed precisely for inputs such as these and will look for
    spatial patterns characteristic of the classes the network is trying to learn
    about. See [Chapter 12](ch12.xhtml#ch12) for more details.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: 'But if the input to the model is a vector, especially a vector where the particular
    features are not related to each other (the key assumption of the Naïve Bayes
    classifier), then a classic model might be appropriate, since there’s no need
    to look for structure among the features beyond the global interpretation that
    the classic models perform by considering the input as a single monolithic entity.
    Therefore, we might give the rule as: when the input is a feature vector without
    spatial structure (unlike an image), especially if the features are not related
    to each other, consider using a classic model.'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that these are rule-of-thumb suggestions, and that
    they aren’t always applicable to a particular problem. Also, it’s possible to
    use deep networks even if these rules seem to apply; it’s just that they may not
    give the best performance, or might be overkill, like using a shotgun to kill
    a fly. The main point of this book is to build intuition so that when a situation
    arises, we’ll know how to use the techniques we are exploring to maximum advantage.
    Pasteur said, “In the fields of observation, chance favors only the prepared mind”
    (lecture at the University of Lille, December 1854), and we wholeheartedly agree.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-756
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we worked with six common classical machine learning models:
    Nearest Centroid, *k*-Nearest Neighbors, Naïve Bayes, Decision Trees, Random Forests,
    and Support Vector Machines. We applied them to three datasets that were developed
    in [Chapter 5](ch05.xhtml#ch05): irises, breast cancer, and MNIST digits. We used
    the results of the experiments with these datasets to gain insight into the strengths
    and weaknesses of each model type along with the effect of different data preprocessing
    steps. We ended the chapter with a discussion of the classic models and when it
    might be appropriate to use them.'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll move on from the classic models and begin our exploration
    of neural networks, the backbone of modern deep learning.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
