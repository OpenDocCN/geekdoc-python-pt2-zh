- en: '**1'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1'
- en: HOW NATURAL LANGUAGE PROCESSING WORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理是如何工作的**'
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: In the 19th century, explorers discovered *rongorongo*, a system of mysterious
    glyphs on the island of Rapa Nui (commonly known as Easter Island). Researchers
    have never succeeded in decoding rongorongo inscriptions or even figuring out
    whether those inscriptions are writing or proto-writing (pictographic symbols
    that convey information but are language independent). Moreover, although we know
    that the creators of the glyphs also erected Moai, the large statues of human
    figures for which the island is most famous, the builders’ motivations remain
    unclear. We can only speculate.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在 19 世纪，探险家们发现了 *rongorongo*，一种在拉帕努伊岛（即复活节岛）上出现的神秘符号系统。研究人员从未成功解读 rongorongo
    的铭文，甚至无法弄清这些铭文是写作还是原始写作（即传递信息的图画符号，但不依赖于特定语言）。此外，尽管我们知道这些符号的创造者也建造了摩艾巨像——这座岛屿最著名的大型人形雕像——但建造者的动机仍然不明。我们只能进行推测。
- en: If you don’t understand people’s writing—or the way in which they describe things—you
    most likely won’t understand the other aspects of their life, including what they
    do and why they do it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不理解人们的写作方式——或者他们描述事物的方式——那么你很可能也无法理解他们生活的其他方面，包括他们做什么以及为什么这么做。
- en: '*Natural language processing (NLP)* is a subfield of artificial intelligence
    that tries to process and analyze natural language data. It includes teaching
    machines to interact with humans in a natural language (a language that developed
    naturally through use). By creating machine learning algorithms designed to work
    with unknown datasets much larger than those two dozen tablets found on Rapa Nui,
    data scientists can learn how we use language. They can also do more than simply
    decipher ancient inscriptions.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理（NLP）* 是人工智能的一个子领域，旨在处理和分析自然语言数据。它包括教机器使用自然语言（通过使用自然发展起来的语言）与人类互动。通过创建专门设计的机器学习算法，可以处理远大于拉帕努伊岛上那些二十多块石板的数据集，数据科学家可以了解我们如何使用语言。他们不仅可以解读古代铭文，还能做更多事情。'
- en: Today, you can use algorithms to observe languages whose semantics and grammar
    rules are well known (unlike the rongorongo inscriptions), and then build applications
    that can programmatically “understand” utterances in that language. Businesses
    can use these applications to relieve humans from boring, monotonous tasks. For
    example, an app might take food orders or answer recurring customer questions
    requesting technical support.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，你可以使用算法观察那些语义和语法规则已知的语言（与 rongorongo 铭文不同），然后构建能够以编程方式“理解”该语言中的发音的应用程序。企业可以使用这些应用程序，减轻人类完成枯燥、单调任务的负担。例如，一个应用程序可以接收食物订单，或者回答客户关于技术支持的重复性问题。
- en: Not surprisingly, generating and understanding natural language are the most
    promising and yet challenging tasks involved in NLP. In this book, you’ll use
    the Python programming language to build a natural language processor with spaCy,
    the leading open source Python library for natural language processing. But before
    you get started, this chapter outlines what goes on behind the scenes of building
    a natural language processor.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，生成和理解自然语言是自然语言处理（NLP）中最有前景同时又最具挑战性的任务。在本书中，你将使用 Python 编程语言，通过 spaCy（一个领先的开源
    Python 自然语言处理库）构建一个自然语言处理器。但在开始之前，本章将概述构建自然语言处理器的幕后工作。
- en: '**How Can Computers Understand Language?**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**计算机如何理解语言？**'
- en: If computers are just emotionless machines, how is it possible to train them
    to understand human language and respond properly? Well, machines can’t understand
    natural language natively. If you want your computer to perform computational
    operations on language data, you need a system that can translate natural language
    words into numbers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算机仅仅是没有情感的机器，怎么可能训练它们理解人类语言并做出正确的回应呢？实际上，机器本身无法原生理解自然语言。如果你希望计算机对语言数据进行计算操作，你需要一个系统，能够将自然语言的单词转化为数字。
- en: '***Mapping Words and Numbers with Word Embedding***'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***通过词嵌入映射词与数字***'
- en: '*Word embedding* is the technique that assigns words to numbers. In word embedding,
    you map words to vectors of real numbers that distribute the meaning of each word
    between the coordinates of the corresponding word vector. Words with similar meanings
    should be nearby in such a vector space, allowing you to determine the meaning
    of a word by the company it keeps.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*词嵌入*是一种将单词转换为数字的技术。在词嵌入中，你将单词映射到一组实数向量，这些向量将每个单词的意义分配到相应词向量的坐标上。具有相似意义的单词在这样的向量空间中应该彼此接近，从而使你能够通过单词的共现关系来确定单词的含义。'
- en: 'The following is a fragment of such an implementation:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此类实现的一个片段：
- en: the 0.0897 0.0160 -0.0571 0.0405 -0.0696  ...
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 0.0897 0.0160 -0.0571 0.0405 -0.0696 ...
- en: and -0.0314 0.0149 -0.0205 0.0557 0.0205  ...
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 和 -0.0314 0.0149 -0.0205 0.0557 0.0205 ...
- en: of -0.0063 -0.0253 -0.0338 0.0178 -0.0966 ...
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: of -0.0063 -0.0253 -0.0338 0.0178 -0.0966 ...
- en: to 0.0495 0.0411 0.0041 0.0309 -0.0044    ...
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: to 0.0495 0.0411 0.0041 0.0309 -0.0044 ...
- en: in -0.0234 -0.0268 -0.0838 0.0386 -0.0321 ...
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: in -0.0234 -0.0268 -0.0838 0.0386 -0.0321 ...
- en: This fragment maps the words “the,” “and,” “of,” “to,” and “in” to the coordinates
    that follow it. If you represented these coordinates graphically, the words that
    are closer in meaning would be closer in the graph as well. (But this doesn’t
    mean that you can expect the closer-in-meaning words to be grouped together in
    a textual representation like the one whose fragment is shown here. The textual
    representation of a word vector space usually starts with the most common words,
    such as “the,” “and,” and so on. This is the way word vector space generators
    lay out words.)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段将单词“the”、“and”、“of”、“to”和“in”映射到随后的坐标。如果你将这些坐标图形化表示，具有相似意义的单词在图中也会彼此靠近。（但这并不意味着你可以指望具有相似意义的单词在像这里展示的片段那样的文本表示中被分组在一起。词向量空间的文本表示通常以最常见的单词开始，例如“the”、“and”等。这是词向量空间生成器布置单词的方式。）
- en: '**NOTE**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*A graphical representation of a multidimensional vector space can be implemented
    in the form of a 2D or a 3D projection. To prepare such a projection, you can
    use the first two or three principal components (or coordinates) of a vector,
    respectively. We’ll return to this concept in [Chapter 5](../Text/ch05.xhtml#ch05).*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*多维向量空间的图形表示可以以2D或3D投影的形式实现。为了准备这样的投影，你可以分别使用向量的前两个或三个主成分（或坐标）。我们将在[第5章](../Text/ch05.xhtml#ch05)中回到这个概念。*'
- en: Once you have a matrix that maps words to numeric vectors, you can perform arithmetic
    on those vectors. For example, you can determine the *semantic similarity* (synonymy)
    of words, sentences, and even entire documents. You might use this information
    to programmatically determine what a text is about, for example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有一个将单词映射到数值向量的矩阵，你就可以对这些向量进行算术运算。例如，你可以确定单词、句子甚至整个文档的*语义相似度*（同义性）。你可能会利用这些信息通过编程方式确定文本的内容。
- en: Mathematically, determining the semantic similarity between two words is reduced
    to calculating the cosine similarity between the corresponding vectors, or to
    calculating the cosine of the angle between the vectors. Although a complete explanation
    of calculating semantic similarity is outside the scope of this book, [Chapter
    5](../Text/ch05.xhtml#ch05) will cover working with word vectors in more detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，确定两个单词的语义相似度可以简化为计算相应向量之间的余弦相似度，或者计算向量之间角度的余弦值。尽管计算语义相似度的完整解释超出了本书的范围，[第5章](../Text/ch05.xhtml#ch05)将更详细地讨论如何使用词向量。
- en: '***Using Machine Learning for Natural Language Processing***'
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用机器学习进行自然语言处理***'
- en: You can generate the numbers to put in the vectors using a machine learning
    algorithm. *Machine learning*, a subfield of artificial intelligence, creates
    computer systems that can automatically learn from data without being explicitly
    programmed. Machine learning algorithms can make predictions about new data, learn
    to recognize images and speech, classify photos and text documents, automate controls,
    and aid in game development.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用机器学习算法生成向量中的数值。*机器学习*是人工智能的一个子领域，它创建能够从数据中自动学习的计算机系统，而无需显式编程。机器学习算法可以对新数据进行预测，学习识别图像和语音，对照片和文本文件进行分类，自动化控制，并辅助游戏开发。
- en: Machine learning lets computers accomplish tasks that would be difficult, if
    not impossible, for them to do otherwise. If you wanted to, say, program a machine
    to play chess using a traditional programming approach in which you explicitly
    specify what the algorithm should do in every context, imagine how many if...else
    conditions you’d need to define. Even if you succeed, users of such an application
    will quickly discover weak points in your logic that they can take advantage of
    during the game until you make necessary corrections in the code.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习让计算机完成那些否则很难甚至不可能完成的任务。如果你想编写一个程序，让机器通过传统的编程方式下象棋，在这种方式中，你需要明确指定算法在每种情况下应该做什么，试想一下你需要定义多少个if...else条件。即使你成功了，使用这种应用的用户也会迅速发现你逻辑中的弱点，并在游戏中利用这些弱点，直到你对代码做出必要的修正。
- en: In contrast, applications built on machine learning algorithms don’t rely on
    predefined logic but use the capability to learn from past experience instead.
    Thus, a machine learning–based chess application looks for positions it remembers
    from the previous games and makes the move that leads to the best position. It
    stores this past experience in a statistical model, which is discussed in “[What
    Is a Statistical Model in NLP?](../Text/ch01.xhtml#lev5)” on [page 8](../Text/ch01.xhtml#page_8).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于机器学习算法构建的应用程序不依赖于预定义的逻辑，而是利用从过去经验中学习的能力。因此，基于机器学习的象棋应用会寻找它从之前的游戏中记住的位置，并做出导致最佳位置的移动。它将这些过去的经验存储在统计模型中，具体内容在[《什么是NLP中的统计模型？》](../Text/ch01.xhtml#lev5)一节中讨论，见[第8页](../Text/ch01.xhtml#page_8)。
- en: 'In spaCy, aside from generating word vectors, machine learning allows you to
    accomplish three tasks: *syntactic dependency parsing* (determining the relationships
    between words in a sentence), *part-of-speech tagging* (identifying nouns, verbs,
    and other parts of speech), and *named entity recognition* (sorting proper nouns
    into categories like people, organizations, and locations). We’ll discuss all
    of these at length in the following chapters.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，除了生成词向量，机器学习还可以帮助你完成三项任务：*句法依赖分析*（确定句子中词语之间的关系）、*词性标注*（识别名词、动词和其他词性）以及*命名实体识别*（将专有名词按人名、组织和地点等类别进行分类）。我们将在接下来的章节中详细讨论这些内容。
- en: 'The life cycle of a typical machine learning system has three steps: model
    training, testing, and making predictions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习系统的生命周期有三个步骤：模型训练、测试和预测。
- en: Model Training
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型训练
- en: In the first stage, you train a model by feeding your algorithm a large body
    of data. For these algorithms to give you reliable results, you must provide a
    sufficiently large volume of input—significantly more than the rongorongo tablets,
    for instance. When it comes to NLP, platforms like Wikipedia and Google News contain
    enough text to feed virtually any machine learning algorithm. But if you wanted
    to build a model specific to your particular use case, you might make it learn,
    for example, from customers using your site.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，你通过将大量数据输入到算法中来训练模型。为了让这些算法给出可靠的结果，你必须提供足够大的数据量——比如比“朗戈朗戈石板”要多得多。当涉及到自然语言处理（NLP）时，像维基百科和谷歌新闻这样的平台包含足够的文本，可以喂养几乎任何机器学习算法。但如果你想要构建一个特定于某个应用场景的模型，你可能会让它从例如你网站的客户行为中学习。
- en: '[Figure 1-1](../Text/ch01.xhtml#ch01fig01) provides a high-level depiction
    of the model training stage.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-1](../Text/ch01.xhtml#ch01fig01)提供了模型训练阶段的高层次示意图。'
- en: '![image](../Images/fig1-1.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-1.jpg)'
- en: '*Figure 1-1: Generating a statistical model with a machine learning algorithm
    using a large volume of text data as input*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-1：使用大量文本数据作为输入，通过机器学习算法生成统计模型*'
- en: Your model processes large volumes of text data to understand which words share
    characteristics; then it creates word vectors for those words that reflect those
    shared characteristics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型处理大量文本数据，以理解哪些词语共享特征；然后它为这些词语创建反映这些共享特征的词向量。
- en: As you’ll learn in “[What Is a Statistical Model in NLP?](../Text/ch01.xhtml#lev5)”
    on [page 8](../Text/ch01.xhtml#page_8), such a word vector space is not the only
    component of a statistical model built for NLP. The actual structure is typically
    more complicated, providing a way to extract linguistic features for each word
    depending on the context in which it appears.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在[《什么是NLP中的统计模型？》](../Text/ch01.xhtml#lev5)一节中学习到的那样，这种词向量空间并不是为NLP构建的统计模型的唯一组成部分。实际结构通常更复杂，提供了一种方法来提取每个词语的语言特征，具体取决于它所出现的上下文。
- en: In [Chapter 10](../Text/ch10.xhtml#ch10), you’ll learn how to train an already
    existing, pretrained model with new examples and a blank one from scratch.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](../Text/ch10.xhtml#ch10)中，你将学习如何使用新的示例训练一个已经存在的预训练模型，以及如何从头开始训练一个空白模型。
- en: Testing
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 测试
- en: Once you’ve trained the model, you can optionally test it to find out how well
    it will perform. To test the model, you feed it text it hasn’t seen before to
    check whether it can successfully identify the semantic similarities and other
    features learned during the training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好模型，你可以选择性地对其进行测试，以了解它的表现如何。测试模型时，你需要给它输入一些它从未见过的文本，看看它能否成功识别出训练中学到的语义相似性和其他特征。
- en: Making Predictions
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 进行预测
- en: If everything works as expected, you can use the model to make predictions in
    your NLP application. For example, you can use it to predict a dependency tree
    structure over the text you input, as depicted in [Figure 1-2](../Text/ch01.xhtml#ch01fig02).
    A *dependency tree structure* represents the relationships between the words in
    a sentence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，你可以在你的自然语言处理应用中使用该模型进行预测。例如，你可以用它预测你输入文本的依赖树结构，如[图1-2](../Text/ch01.xhtml#ch01fig02)所示。*依赖树结构*表示句子中单词之间的关系。
- en: '![image](../Images/fig1-2.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-2.jpg)'
- en: '*Figure 1-2: Predicting a dependency tree structure for an utterance using
    a statistical model*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-2：使用统计模型预测话语的依赖树结构*'
- en: Visually, we can represent a dependency tree using arcs of different lengths
    to connect syntactically related pairs of words. For example, the one shown here
    tells us that the verb “sent” agrees with the pronoun “she.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们可以通过不同长度的弧线来表示依赖树，连接语法上相关的单词对。例如，图中所示的依赖树告诉我们动词“sent”和代词“she”之间存在一致关系。
- en: '***Why Use Machine Learning for Natural Language Processing?***'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***为什么在自然语言处理过程中使用机器学习？***'
- en: Your algorithm’s predictions aren’t statements of fact; they’re typically calculated
    with a degree of certainty. To achieve a higher degree of accuracy, you’ll need
    to implement more complicated algorithms, which are less efficient and less practical
    to implement. Usually, people strive to achieve a reasonable balance between accuracy
    and performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你的算法预测并不是事实陈述；它们通常是通过一定程度的确定性来计算的。为了达到更高的准确度，你需要实现更复杂的算法，这些算法效率较低且实现起来不够实用。通常，人们会努力在准确性和性能之间找到一个合理的平衡点。
- en: Because machine learning models can’t predict perfectly, you might wonder whether
    machine learning is the best approach for building the models used in NLP applications.
    In other words, is there a more reliable approach based on strictly defined rules,
    similar to the one used by compilers and interpreters for processing programming
    languages? The quick answer is no. Here’s why.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因为机器学习模型无法完美预测，你可能会想知道机器学习是否是构建自然语言处理应用中使用的模型的最佳方法。换句话说，是否存在一种基于严格定义规则的更可靠方法，类似于编译器和解释器在处理编程语言时使用的方法？简短的答案是否定的。原因如下。
- en: To begin with, a programming language contains a relatively small number of
    words. For example, the Java programming language consists of 61 reserved words,
    each of which has a predefined meaning in the language.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，编程语言包含的单词数量相对较少。例如，Java编程语言包含61个保留字，每个保留字在语言中都有预定义的意义。
- en: By contrast, the *Oxford English Dictionary,* released in 1989, contains 171,476
    entries for words in current use. In 2010, a team of researchers at Harvard University
    and Google counted about 1,022,000 words in a body of digitized texts containing
    approximately 4 percent of all books ever published. The study estimated that
    the language would grow by several thousand words a year. Assigning each word
    to a corresponding number would take too long.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*《牛津英语词典》*在1989年发布，包含了当前使用的171,476个单词条目。在2010年，哈佛大学和谷歌的研究团队在一个包含约4%已出版书籍的数字化文本库中统计了大约1,022,000个单词。该研究估计，语言每年会增加几千个新词。将每个单词分配给一个相应的编号需要太长时间。
- en: But even if you tried to do it, you’d find it impossible, for several reasons,
    to determine the number of words used in a natural language. First of all, it’s
    unclear what really counts as an individual word. For example, should we count
    the verb “count” as one word, or two, or more? In one scenario, “count” might
    mean “to have value or importance.” In a different scenario, it might mean, “to
    say numbers one after another.” Of course, “count” can also be a noun.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使你尝试去做，你也会发现由于几个原因，无法确定自然语言中使用的单词数量。首先，什么算作一个独立的单词并不明确。例如，我们应该将动词“count”算作一个单词，还是两个，或者更多呢？在一种情况下，“count”可能表示“具有价值或重要性”。在另一种情况下，它可能表示“一个接一个地说数字”。当然，“count”也可以是一个名词。
- en: Should we count inflections—plural form of nouns, verb tenses, and so on—as
    separate entities, too? Should we count *loanwords* (words adopted from foreign
    languages), scientific terms, slang, and abbreviations? Evidently, the vocabulary
    of a natural language is defined loosely, because it’s hard to figure out which
    groups of words to include. In a programming language like Java, an attempt to
    include an unknown word in your code will force the compiler to interrupt processing
    with an error.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否也应该将屈折变化——名词的复数形式、动词时态等——视为独立的实体？我们是否应该将*外来词*（从外语中借用的词汇）、科学术语、俚语和缩写算作单词？显然，自然语言的词汇定义得比较松散，因为很难确定哪些词组应该包括在内。在像
    Java 这样的编程语言中，尝试在代码中包含一个未知词汇将迫使编译器中断处理并报错。
- en: 'A similar situation exists for formal rules. Like its vocabulary, many formal
    rules of a natural language are defined loosely. Some cause controversy, like
    *split infinitives*, a grammatical construction in which an adverb is placed between
    the infinitive verb and its preposition. Here is an example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正式规则也存在类似的情况。像词汇一样，许多自然语言的正式规则定义得较为松散。有些规则会引起争议，比如*拆分不定式*，这是一种语法结构，其中副词被放置在不定式动词和它的介词之间。以下是一个例子：
- en: spaCy allows you to programmatically extract the meaning of an utterance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 允许你以编程方式提取话语的含义。
- en: 'In this example, the adverb “programmatically” separates the preposition and
    infinitive “to extract.” Those who believe that split infinitives are incorrect
    could suggest rewriting the sentence as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，副词“programmatically”将介词和不定式“to extract”分开。那些认为拆分不定式不正确的人可能会建议将句子重写如下：
- en: spaCy allows you to extract the meaning of an utterance programmatically.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 允许你以编程方式提取话语的含义。
- en: But regardless of how you feel about split infinitives, your NLP application
    should understand both sentences equally well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但无论你如何看待拆分不定式，你的自然语言处理应用程序都应该能够同样理解这两个句子。
- en: 'In contrast, a computer program that processes code written in a programming
    language isn’t designed to handle this kind of problem. The reason is that the
    formal rules for a programming language are strictly defined, leaving no possibility
    for discrepancy. For example, consider the following statement, written in the
    SQL programming language, which you might use to insert data into a database table:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，处理编程语言中编写的代码的计算机程序并不设计来处理这类问题。原因在于，编程语言的正式规则是严格定义的，不允许有任何歧义。例如，考虑以下语句，它是用
    SQL 编程语言编写的，你可能会用它来向数据库表中插入数据：
- en: INSERT INTO table1 VALUES(1, 'Maya', 'Silver')
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: INSERT INTO table1 VALUES(1, 'Maya', 'Silver')
- en: The statement is fairly self-explanatory. Even if you don’t know SQL, you can
    easily guess that the statement is supposed to insert three values into table
    1.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语句相当直白。即使你不懂 SQL，你也可以很容易地猜到这个语句应该是将三个值插入到表格 1 中。
- en: 'Now, imagine that you change it as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你将它修改如下：
- en: INSERT VALUES(1, 'Maya', 'Silver') INTO table1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: INSERT VALUES(1, 'Maya', 'Silver') INTO table1
- en: From the standpoint of an English-speaking reader, the second statement should
    have the same meaning as the first one. After all, if you read it like an English
    sentence, it still makes sense. But if you try to execute it in a SQL tool, you’ll
    end up with the error missing INTO keyword. That’s because a SQL parser—like any
    other parser used in a programming language—relies on hardcoded rules, which means
    you must specify exactly what you want it to do in a way it expects. In this case,
    the SQL parser expects to see the keyword INTO right after the keyword INSERT
    without any other possible options.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个讲英语读者的角度来看，第二个语句应该与第一个语句具有相同的意义。毕竟，如果你像读英语句子那样阅读它，它依然有意义。但如果你试图在SQL工具中执行它，你将遇到错误，提示缺少
    INTO 关键字。这是因为SQL解析器——就像其他编程语言中使用的任何解析器一样——依赖于硬编码规则，这意味着你必须以它预期的方式精确地指定你希望它做什么。在这种情况下，SQL解析器期望在
    INSERT 关键字后面紧跟着 INTO 关键字，而没有其他可能的选项。
- en: Needless to say, such restrictions are impossible in a natural language. Taking
    all these differences into account, it’s fairly obvious that it would be inefficient
    or even impossible to define a set of formal rules that would specify a computational
    model for a natural language in the way we do for programming languages.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，这种限制在自然语言中是无法实现的。考虑到所有这些差异，很明显，定义一套正式规则来指定自然语言的计算模型，就像我们为编程语言所做的那样，既低效又不可能。
- en: Instead of a rule-based approach, we use an approach that is based on observations.
    Rather than encoding a language by assigning each word to a predetermined number,
    machine learning algorithms generate statistical models that detect patterns in
    large volumes of language data and then make predictions about the syntactic structure
    in new, previously unseen text data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用基于观察的方法，而非基于规则的方法。与其通过为每个单词分配一个预定的数字来编码语言，不如让机器学习算法生成统计模型，这些模型能够检测大量语言数据中的模式，并对新的、之前未见过的文本数据的句法结构做出预测。
- en: '[Figure 1-3](../Text/ch01.xhtml#ch01fig03) summarizes how language processing
    works for natural languages and programming languages, respectively.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-3](../Text/ch01.xhtml#ch01fig03)总结了自然语言处理和编程语言处理的工作方式。'
- en: A natural language processing system uses an underlying statistical model to
    make predictions about the meaning of input text and then generates an appropriate
    response. In contrast, a compiler processing programming code applies a set of
    strictly defined rules.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然语言处理系统使用底层的统计模型来预测输入文本的含义，并生成适当的回应。相比之下，编译器处理编程代码时会应用一套严格定义的规则。
- en: '![image](../Images/fig1-3.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-3.jpg)'
- en: '*Figure 1-3: On the left, a basic workflow for processing natural language;
    on the right, a basic workflow for processing a programming language*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-3：左侧是处理自然语言的基本工作流程；右侧是处理编程语言的基本工作流程*'
- en: '**What Is a Statistical Model in NLP?**'
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是自然语言处理中的统计模型？**'
- en: In NLP, a *statistical model* contains estimates for the probability distribution
    of linguistic units, such as words and phrases, allowing you to assign linguistic
    features to them. In probability theory and statistics, a *probability distribution*
    for a particular variable is a table of values that maps all of the possible outcomes
    of that variable to their probabilities of occurrence in an experiment. [Table
    1-1](../Text/ch01.xhtml#ch01tab01) illustrates what a probability distribution
    over part-of-speech tags for the word “count” might look like for a given sentence.
    (Remember that an individual word might act as more than one part of speech, depending
    on the context in which it appears.)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，*统计模型*包含了语言单位（如单词和短语）概率分布的估计，允许你为它们分配语言特征。在概率论和统计学中，某一特定变量的*概率分布*是一个值表，映射了该变量所有可能结果及其在实验中发生的概率。[表
    1-1](../Text/ch01.xhtml#ch01tab01)展示了“count”一词在给定句子中的词性标签的概率分布是怎样的。（记住，一个单词可能根据其出现的上下文，充当多个词性。）
- en: '**Table 1-1:** An Example of a Probability Distribution for a Linguistic Unit
    in a Context'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1-1：** 语境中语言单位的概率分布示例'
- en: '| **VERB** | **NOUN** |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **动词** | **名词** |'
- en: '| --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 78% | 22% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 78% | 22% |'
- en: Of course, you’ll get other figures for the word “count” used in another context.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您在其他上下文中使用“count”一词时，会得到不同的数字。
- en: Statistical language modeling is vital to many natural language processing tasks,
    such as natural language generating and natural language understanding. For this
    reason, a statistical model lies at the heart of virtually any NLP application.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 统计语言建模对于许多自然语言处理任务至关重要，例如自然语言生成和自然语言理解。因此，几乎所有的自然语言处理应用的核心都依赖于统计模型。
- en: '[Figure 1-4](../Text/ch01.xhtml#ch01fig04) provides a conceptual depiction
    of how an NLP application uses a statistical model.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-4](../Text/ch01.xhtml#ch01fig04)提供了一个自然语言处理应用如何使用统计模型的概念性图示。'
- en: '![image](../Images/fig1-4.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-4.jpg)'
- en: '*Figure 1-4: A high-level conceptual view of an NLP application’s architecture*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-4：自然语言处理应用架构的高层概念视图*'
- en: The application interacts with spaCy’s API, which abstracts the underlying statistical
    model. The statistical model contains information like word vectors and linguistic
    annotations. The linguistic annotations might include features such as part-of-speech
    tags and syntactic annotations. The statistical model also includes a set of machine
    learning algorithms that can extract the necessary pieces of information from
    the stored data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序与spaCy的API进行交互，API对底层的统计模型进行了抽象。统计模型包含诸如词向量和语言学注释等信息。语言学注释可能包括诸如词性标签和句法注释等特征。统计模型还包括一组机器学习算法，可以从存储的数据中提取必要的信息。
- en: In real systems, a model’s data is typically stored in a binary format. Binary
    data doesn’t look friendly to humans, but it’s a machine’s best friend because
    it’s easy to store and loads quickly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际系统中，模型的数据通常以二进制格式存储。二进制数据对人类来说不太友好，但对机器来说是最理想的，因为它易于存储并且加载速度快。
- en: '***Neural Network Models***'
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***神经网络模型***'
- en: The statistical models used in NLP tools like spaCy for syntactic dependency
    parsing, part-of-speech tagging, and named entity recognition are neural network
    models. A *neural network* is a set of prediction algorithms. It consists of a
    large number of simple processing elements, like neurons in a brain, that interact
    by sending and receiving signals to and from neighboring nodes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于自然语言处理工具如spaCy中的句法依赖分析、词性标注和命名实体识别的统计模型是神经网络模型。*神经网络*是一组预测算法。它由大量简单的处理单元组成，类似于大脑中的神经元，通过向邻近节点发送和接收信号来进行相互作用。
- en: Typically, nodes in a neural network are grouped into layers, including an input
    layer, an output layer, and one or more hidden layers in between. Every node in
    a layer (except the output layer) connects to every node in the successive layer
    through a connection. A connection has a weight value associated with it. During
    the training process, the algorithm adjusts the weights to minimize the error
    it makes in its predictions. This architecture enables a neural network to recognize
    patterns, even in complex data inputs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络中的节点被分组到多个层次中，包括输入层、输出层和一个或多个隐藏层。每个层中的节点（输出层除外）通过连接与后续层中的每个节点相连。连接与其相关联的权重值。在训练过程中，算法调整权重，以最小化其预测中的误差。这样的架构使得神经网络能够识别模式，即使在复杂的数据输入中。
- en: Conceptually, we can represent a neural network as shown in [Figure 1-5](../Text/ch01.xhtml#ch01fig05).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们可以如[图1-5](../Text/ch01.xhtml#ch01fig05)所示表示一个神经网络。
- en: '![image](../Images/fig1-5.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-5.jpg)'
- en: '*Figure 1-5: A conceptual depiction of the neural network layout and operations
    at one node*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-5：神经网络布局和一个节点操作的概念性图示*'
- en: When a signal comes in, it’s multiplied by a weight value, which is a real number.
    The input and weight values passed on to a neural network generally come from
    the word vectors generated during the network training.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当信号输入时，它会被一个权重值乘以，这个权重值是一个实数。传递给神经网络的输入和权重值通常来自于网络训练过程中生成的词向量。
- en: The neural network adds the results of the multiplications together for each
    node; then it passes the sum on to an activation function. The activation function
    generates a result that typically ranges from 0 to 1, thus producing a new signal
    that is passed on to each node in the successive layer, or, in the case of the
    output layer, an output parameter. Usually, the output layer has as many nodes
    as the number of possible distinct outputs for the given algorithm. For example,
    a neural network implemented for a part-of-speech tagger should have as many nodes
    in the output layer as the number of part-of-speech tags supported by the system,
    as illustrated in [Figure 1-6](../Text/ch01.xhtml#ch01fig06).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络将每个节点的乘法结果相加，然后将和传递给激活函数。激活函数生成一个通常在0到1之间的结果，从而产生一个新信号，并将其传递给后续层的每个节点，或者在输出层的情况下传递给输出参数。通常，输出层的节点数等于该算法可能的不同输出数量。例如，针对词性标注器实现的神经网络，其输出层的节点数应该与系统支持的词性标签数量相等，如[图
    1-6](../Text/ch01.xhtml#ch01fig06)所示。
- en: '![image](../Images/fig1-6.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-6.jpg)'
- en: '*Figure 1-6: A simplified depiction of the part-of-speech tagging process*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-6：词性标注过程的简化示意图*'
- en: The part-of-speech tagger then outputs a probability distribution over all possible
    parts of speech for a given word in a given context.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注器接着会输出一个概率分布，表示在特定上下文中给定单词可能的所有词性。
- en: '***Convolutional Neural Networks for NLP***'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***卷积神经网络在自然语言处理中的应用***'
- en: The architecture of a real neural network model can be quite complex; it’s formed
    by a number of different layers. Thus, the neural network model used in spaCy
    is a *convolutional neural network (CNN)* that includes a convolutional layer,
    which is shared between the part-of-speech tagger, dependency parser, and named
    entity recognizer. The convolutional layer applies *a set of detection filters
    to regions of input data to test for the presence of specific features.*
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的神经网络模型架构可能相当复杂，通常由多个不同的层组成。因此，spaCy中使用的神经网络模型是一个*卷积神经网络（CNN）*，它包含一个卷积层，且该卷积层在词性标注器、依存句法分析器和命名实体识别器之间共享。卷积层应用*一组检测滤波器到输入数据的区域，以检测特定特征的存在*。
- en: '*As an example, let’s look at how a CNN might work for the part-of-speech tagging
    task when performed on the sentence in the previous example:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*举个例子，让我们看看卷积神经网络（CNN）如何处理前面例句中的词性标注任务：'
- en: Can we count on them?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能依赖他们吗？
- en: Instead of analyzing each word on its own, the convolutional layer first breaks
    the sentence into chunks. You can consider a sentence in NLP as a matrix in which
    each row represents a word in the form of a vector. So if each word vector had
    300 dimensions and your sentence was five words long, you’d get a 5 × 300 matrix.
    The convolutional layer might use a detection filter size of three, applied to
    three consecutive words, thus having a tiling region size of 3 × 300\. This should
    provide sufficient context for making a decision on what part-of-speech tag each
    word is.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层并不是单独分析每个单词，而是首先将句子分解为若干个片段。你可以将自然语言处理中的句子看作一个矩阵，其中每一行代表一个单词，并以向量的形式表示。所以，如果每个单词向量有300个维度，且句子有五个单词，那么就会得到一个5
    × 300的矩阵。卷积层可能会使用一个大小为3的检测滤波器，应用于三个连续的单词，这样就有了一个3 × 300的区域大小。这应当为决定每个单词的词性标签提供足够的上下文信息。
- en: The operation of a part-of-speech tagging using the convolutional approach is
    depicted in [Figure 1-7](../Text/ch01.xhtml#ch01fig07).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积方法进行词性标注的操作如[图 1-7](../Text/ch01.xhtml#ch01fig07)所示。
- en: '![image](../Images/fig1-7.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-7.jpg)'
- en: '*Figure 1-7: A conceptual look at how the convolutional approach works for
    an NLP task*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-7：卷积方法在自然语言处理任务中的概念性展示*'
- en: In the preceding example, the most challenging task for the tagger is to determine
    what part-of-speech the word “count” is. The problem is that this word can be
    either a verb or a noun, depending on the context. But this task becomes a breeze
    when the tagger sees the chunk that includes the “we count on” word combination.
    In that context, it becomes clear that the word “count” can be only a verb.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，对于标注器来说，最具挑战性的任务是确定“count”这个单词属于什么词性。问题在于，这个单词可以是动词，也可以是名词，具体取决于上下文。但当标注器看到包含“we
    count on”词组的片段时，这个任务就变得非常简单。在那个上下文中，“count”显然只能是动词。
- en: A detailed look under the hood of the convolutional architecture is beyond the
    scope of this book. To learn more about the neural network model architecture
    behind statistical models used in spaCy, check out the “Neural Network Model Architecture”
    section in spaCy’s API documentation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对卷积架构的详细探讨超出了本书的范围。要了解更多有关spaCy中使用的统计模型背后的神经网络模型架构，请查看spaCy API文档中的“神经网络模型架构”部分。
- en: '**What Is Still on You**'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**仍然需要您处理的**'
- en: As you learned in the preceding section, spaCy uses neural models for syntactic
    dependency parsing, part-of-speech tagging, and named entity recognition. Because
    spaCy provides these functions for you, what’s left for you to do as the developer
    of an NLP application?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前一节中学到的，spaCy使用神经网络模型进行句法依存解析、词性标注和命名实体识别。由于spaCy为您提供了这些功能，作为NLP应用的开发者，剩下的工作是什么？
- en: 'One thing spaCy can’t do for you is recognize the user’s intent. For example,
    suppose you sell clothes and your online application that takes orders has received
    the following request from a user:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy无法为您做的一件事是识别用户的意图。例如，假设您在销售衣服，您的在线订购应用接收到用户的以下请求：
- en: I want to order a pair of jeans.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我想订购一条牛仔裤。
- en: The application should recognize that the user intends to place an order for
    a pair of jeans.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序应当识别出用户的意图是订购一条牛仔裤。
- en: If you use spaCy to perform syntactic dependency parsing for the utterance,
    you’ll get the result shown in [Figure 1-8](../Text/ch01.xhtml#ch01fig08).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用spaCy对话语进行句法依存解析，您将获得如[图1-8](../Text/ch01.xhtml#ch01fig08)所示的结果。
- en: '![image](../Images/fig1-8.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-8.jpg)'
- en: '*Figure 1-8: The dependency tree for the sample utterance*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-8：样本话语的依存树*'
- en: Notice that spaCy doesn’t mark anything as the user’s intent in the generated
    tree. In fact, it would be strange if it did so. The reason is that spaCy doesn’t
    know how you’ve implemented your application’s logic and what kind of intent you
    expect to see in particular. Which words to consider the key terms for the task
    of intent recognition is entirely up to you.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，spaCy在生成的树中并没有标记任何内容作为用户的意图。事实上，如果它这么做了，那会很奇怪。原因是spaCy并不知道您如何实现应用程序的逻辑，也不知道您特别期待看到什么样的意图。哪些单词应视为任务中意图识别的关键术语完全由您决定。
- en: 'To extract the meaning from an utterance or a discourse, you need to understand
    the following key aspects: keywords, context, and meaning transition.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要从话语或 discourse 中提取含义，您需要理解以下几个关键方面：关键词、上下文和含义转换。
- en: '***Keywords***'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***关键词***'
- en: You can use the results of the syntactic dependency parse to choose the most
    important words for meaning recognition. In the “I want to order a pair of jeans.”
    example, the keywords are probably “order” and “jeans.”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用句法依存解析的结果来选择最重要的单词用于意义识别。在“我想订购一条牛仔裤。”这个例子中，关键词可能是“订购”和“牛仔裤”。
- en: Normally, the transitive verb plus its direct object work well for composing
    the intent. But in this particular example, it’s a bit more complicated. You’ll
    need to navigate the dependency tree and extract “order” (the transitive verb)
    and “jeans” (the object of the preposition related to the direct object “pair”).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，及物动词加上其直接宾语可以很好地表达意图。但在这个具体的例子中，情况稍微复杂一些。您需要浏览依存树并提取“订购”（及物动词）和“牛仔裤”（与直接宾语“条”相关的介词宾语）。
- en: '***Context***'
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***上下文***'
- en: 'Context can matter when selecting keywords, because the same phrase might have
    different meanings when interpreted in different applications. Suppose you have
    the following utterance to treat:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文在选择关键词时很重要，因为相同的短语在不同的应用中可能有不同的含义。假设您有以下话语需要处理：
- en: I want the newspaper delivered to my door.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要报纸送到我家门口。
- en: Depending on the context, this statement might be either a request to subscribe
    to a newspaper or a request to deliver the issue to the door. In the first case,
    the keywords might be “want” and “newspaper.” In the latter case, the keywords
    might be “delivered” and “door.”
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文，这句话可能是请求订阅报纸，或者请求将报纸送到门口。在第一种情况下，关键词可能是“想”和“报纸”。在后一种情况下，关键词可能是“送达”和“门口”。
- en: '***Meaning Transition***'
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***含义转换***'
- en: 'Often, people use more than one sentence to express even a very straightforward
    intent. As an example, consider the following discourse:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们用不止一句话来表达即便是非常直接的意图。举个例子，考虑以下话语：
- en: I already have a relaxed pair of jeans. Now I want a skinny pair.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经有一条宽松的牛仔裤。现在我想要一条修身的牛仔裤。
- en: In this discourse, the words reflecting the intent expressed appear in two different
    sentences, as illustrated in [Figure 1-9](../Text/ch01.xhtml#ch01fig09).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段话中，反映表达意图的单词出现在两个不同的句子中，如[图 1-9](../Text/ch01.xhtml#ch01fig09)所示。
- en: '![image](../Images/fig1-9.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-9.jpg)'
- en: '*Figure 1-9: Recognizing the intent of the discourse*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-9：识别话语的意图*'
- en: 'As you might guess, the words “want” and “jeans” best describe the intent of
    this discourse. The following are the general steps to finding keywords that best
    describe the user’s intent in this particular example:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜测，单词“want”和“jeans”最好地描述了这段话的意图。以下是找到能够最好描述用户意图的关键词的一般步骤，针对这个例子：
- en: Within the discourse, find a transitive verb in the present tense.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这段话中，找出一个现在时的及物动词。
- en: Find the direct object of the transitive verb found in step 1.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出在步骤 1 中发现的及物动词的直接宾语。
- en: If the direct object found in the previous step is a pro-form, find its antecedent
    in a previous sentence.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在前一步中找到的直接宾语是代词，找出它在前一个句子中的先行词。
- en: With spaCy, you can easily implement these steps programmatically. We’ll describe
    this process in detail in [Chapter 8](../Text/ch08.xhtml#ch08).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy，你可以轻松地通过编程实现这些步骤。我们将在[第 8 章](../Text/ch08.xhtml#ch08)中详细描述这一过程。
- en: '**Summary**'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you learned the basics of natural language processing. You
    now know that, unlike humans, machines use vector–based representations of words,
    which allow you to perform math on natural language units, including words, sentences,
    and documents.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了自然语言处理的基础知识。你现在知道，与人类不同，机器使用基于向量的词表示，这使得你能够对自然语言单元（包括单词、句子和文档）进行数学运算。
- en: You learned that word vectors are implemented in statistical models based on
    the neural network architecture. Then you learned about the tasks that are still
    left up to you as an NLP application developer.*
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习了词向量是如何在基于神经网络架构的统计模型中实现的。然后，你了解了作为 NLP 应用开发者，仍然需要你完成的任务。*
