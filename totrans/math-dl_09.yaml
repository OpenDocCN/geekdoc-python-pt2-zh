- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9'
- en: DATA FLOW IN NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的数据流**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In this chapter, I’ll present how data flows through a trained neural network.
    In other words, we’ll look at how to go from an input vector or tensor to the
    output, and the form the data takes along the way. If you’re already familiar
    with how neural networks function, great, but if not, walking through how data
    flows from layer to layer will help you build an understanding of the processes
    involved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我将展示数据是如何在训练好的神经网络中流动的。换句话说，我们将查看如何从输入向量或张量转换到输出，以及数据在过程中所呈现的形式。如果你已经熟悉神经网络的运作原理，那就太好了；如果没有，跟随数据从一层流向另一层的过程，将帮助你建立对这些过程的理解。
- en: First, we’ll look at how we represent data in two different kinds of networks.
    Then, we’ll work through a traditional feedforward network to give ourselves a
    solid foundation. We’ll see just how compact inference with a neural network can
    be in terms of code. Finally, we’ll follow data through a convolutional neural
    network by introducing convolutional and pooling layers. The goal of this chapter
    isn’t to present how popular toolkits pass data around. The toolkits are highly
    optimized pieces of software, and such low-level knowledge isn’t helpful to us
    at this stage. Instead, the goal is to help you see how the data flows from input
    to output.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解如何在两种不同类型的网络中表示数据。接着，我们将通过一个传统的前馈网络来为自己打下坚实的基础。我们将看到在神经网络中进行推理时，代码是如何简洁的。最后，我们将通过引入卷积层和池化层，追踪数据在卷积神经网络中的流动。本章的目标不是展示流行工具包如何传递数据。这些工具包是高度优化的软件，其低层次的知识在此阶段对我们帮助不大。相反，目标是帮助你理解数据是如何从输入流向输出的。
- en: Representing Data
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据表示
- en: 'In the end, everything in deep learning is about data. We have data that we’re
    using to create a model, which we test with more data, ultimately letting us make
    predictions about even more data. We’ll start by looking at how we represent data
    in two types of neural networks: traditional neural networks and deep convolutional
    networks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，深度学习的一切都与数据有关。我们使用数据来创建模型，然后用更多的数据进行测试，最终让我们能够对更多的数据进行预测。我们将从了解如何在两种类型的神经网络中表示数据开始：传统神经网络和深度卷积网络。
- en: Traditional Neural Networks
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 传统神经网络
- en: For a *traditional neural network* or other classical machine learning models,
    the input is a vector of numbers, the feature vector. The training data is a collection
    of these vectors, each with an associated label. (We’ll restrict ourselves to
    basic supervised learning in this chapter.) A collection of feature vectors is
    conveniently implemented as a matrix, where each row is a feature vector and the
    number of rows matches the number of samples in the dataset. As we now know, a
    computer conveniently represents a matrix using a 2D array. Therefore, when working
    with traditional neural networks or other classical models (support vector machines,
    random forests, and so on), we’ll represent datasets as 2D arrays.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*传统神经网络*或其他经典机器学习模型，输入是一个数字向量，即特征向量。训练数据是一组这些特征向量，每个特征向量都带有一个关联的标签。（本章我们将限制在基本的监督学习上。）特征向量集合方便地实现为一个矩阵，每一行是一个特征向量，行数与数据集中的样本数相匹配。正如我们现在所知道的，计算机方便地使用二维数组表示矩阵。因此，在处理传统神经网络或其他经典模型（如支持向量机、随机森林等）时，我们将把数据集表示为二维数组。
- en: 'For example, the iris dataset, which we first encountered in [Chapter 6](ch06.xhtml#ch06),
    has four features in each feature vector. We represented it as a matrix:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第六章](ch06.xhtml#ch06)中我们首次接触的鸢尾花数据集，每个特征向量包含四个特征。我们将它表示为一个矩阵：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we’ve shown the first five samples as we did in [Chapter 6](ch06.xhtml#ch06).
    The samples above are all for class 0 (*I. setosa*). To pass this knowledge to
    the model, we need a matching vector of class labels; `X[i]` returns the feature
    vector for sample `i`, and `Y[i]` returns the class label. The class label is
    usually an integer and counts up from zero for each class in the dataset. Some
    toolkits prefer one-hot-encoded class labels, but we can easily create them from
    the more standard integer labels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们展示了前五个样本，就像在[第六章](ch06.xhtml#ch06)中做的那样。上面的样本全部属于类别 0（*I. setosa*）。为了将这些知识传递给模型，我们需要一个与之匹配的类别标签向量；`X[i]`返回样本`i`的特征向量，`Y[i]`返回类别标签。类别标签通常是一个整数，并且从零开始为数据集中每个类别编号。一些工具包更喜欢使用独热编码（one-hot
    encoding）的类别标签，但我们可以轻松地从更标准的整数标签中生成它们。
- en: Therefore, a traditional dataset uses matrices between layers to hold weights,
    with the input and output of each layer a vector. This is straightforward enough.
    What about a more modern, deep network?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，传统的数据集在层与层之间使用矩阵来保存权重，每一层的输入和输出是一个向量。这相对直接。那么，更现代的深度网络呢？
- en: Deep Convolutional Networks
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度卷积网络
- en: Deep networks might use feature vectors, especially if the model implements
    1D convolutions, but more often than not, the entire point of using a deep network
    is to allow *convolutional layers* to take advantage of spatial relationships
    in the data. Usually, this means the inputs are images, which we represent using
    2D arrays. But the input doesn’t always need to be an image. The model is blissfully
    unaware of *what* the input represents; only the model designer knows, and they
    decide the architecture based on that knowledge. For simplicity, we’ll assume
    the inputs are images, since we’re already aware of how computers work with images,
    at least at a high level.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络可能会使用特征向量，尤其是在模型实现一维卷积时，但更多情况下，使用深度网络的核心目的是让*卷积层*利用数据中的空间关系。通常，这意味着输入是图像，我们使用二维数组表示图像。但是，输入不一定非得是图像。模型并不关心输入代表的*是什么*；只有模型设计者知道，并根据这些知识决定架构。为了简单起见，我们假设输入是图像，因为我们已经了解计算机如何处理图像，至少从高层次来看是这样的。
- en: 'A black-and-white image, or one with shades of gray, known as a grayscale image,
    uses a single number to represent each pixel’s intensity. Therefore, a grayscale
    image consists of a single matrix represented in the computer as a 2D array. However,
    most of the images we see on our computers are color images, not grayscale. Most
    software represents a pixel’s color by three numbers: the amount of red, the amount
    of green, and the amount of blue. This is the origin of the *RGB* label given
    to color images on a computer. There are many other ways of representing colors,
    but RGB is by far the most common. The blending of these primary colors allows
    computers to display millions of colors. If each pixel needs three numbers, then
    a color image isn’t a single 2D array, but three 2D arrays, one for each color.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 黑白图像，或者带有灰度的图像，称为灰度图像，使用单个数字表示每个像素的强度。因此，灰度图像由一个矩阵组成，在计算机中表示为二维数组。然而，我们在计算机上看到的大多数图像都是彩色图像，而非灰度图像。大多数软件通过三个数字表示一个像素的颜色：红色的量、绿色的量和蓝色的量。这就是计算机上彩色图像被标记为*RGB*的原因。还有许多其他表示颜色的方法，但RGB是最常见的。通过这些基础色的混合，计算机能够显示数百万种颜色。如果每个像素需要三个数字，那么彩色图像就不再是一个二维数组，而是三个二维数组，每个数组代表一种颜色。
- en: 'For example, in [Chapter 4](ch04.xhtml#ch04), we loaded a color image from
    `sklearn`. Let’s look at it again to see how it’s arranged in memory:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第4章](ch04.xhtml#ch04)中，我们从`sklearn`加载了一张彩色图像。我们再来看一遍，看看它是如何在内存中排列的：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The image is returned as a NumPy array. Asking for the shape of the array returns
    a tuple: (427, 640, 3). The array has three dimensions. The first is the height
    of the image, 427 pixels. The second is the width of the image, 640 pixels. The
    third is the number of *bands* or *channels*, here three because it’s an RGB image.
    The first channel is the red component of the color of each pixel, the second
    the green, and the last the blue. We can look at each channel as a grayscale image
    if we want:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图像以NumPy数组的形式返回。请求数组的形状会返回一个元组：(427, 640, 3)。这个数组有三个维度。第一个是图像的高度，427个像素。第二个是图像的宽度，640个像素。第三个是*通道*的数量，这里是三，因为它是RGB图像。第一个通道是每个像素的红色分量，第二个是绿色，最后一个是蓝色。如果需要的话，我们可以将每个通道当作一张灰度图像来看：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'PIL refers to Pillow, Python’s library for working with images. If you don’t
    already have it installed, this will install it for you:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PIL指的是Pillow，这是Python用于处理图像的库。如果你还没有安装它，运行以下命令可以为你安装：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Each image looks similar, but if you place them side by side, you’ll notice
    differences. See [Figure 9-1](ch09.xhtml#ch09fig01). The net effect of each per-channel
    image creates the actual color displayed. Replace `china[:,:,0]` with just `china`
    to see the full color image.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像看起来相似，但如果将它们并排放置，你会注意到一些差异。见[图9-1](ch09.xhtml#ch09fig01)。每个通道图像的合成效果形成了显示的实际颜色。将`china[:,:,0]`替换为`china`，即可查看完整的彩色图像。
- en: '![image](Images/09fig01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig01.jpg)'
- en: '*Figure 9-1: The red (left), green (middle), and blue (right)* `*china*` *image
    channels*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-1：红色（左）、绿色（中）、蓝色（右）*`*china*`*图像通道*'
- en: 'Inputs to deep networks are often multidimensional. If the input’s a color
    image, we need to use a 3D tensor to contain the image. We’re not quite done,
    however. Each input sample to the model is a 3D tensor, but we seldom work with
    a single sample at a time. When training a deep network, we use *minibatches*,
    sets of samples processed together to get an average loss. This implies yet another
    dimension to the input tensor, one that lets us specify *which* member of the
    minibatch we want. Therefore, the input is a 4D tensor: *N* × *H* × *W* × *C*,
    with *N* being the number of samples in the minibatch, *H* the height of each
    image in the minibatch, *W* the width of each image, and *C* the number of channels.
    We’ll sometimes write this in tuple form as (*N*, *H*, *W*, *C*).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at some actual data meant for a deep network. The data is
    the CIFAR-10 dataset. It’s a widely used benchmark dataset and is available here:
    *[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)*.
    You don’t need to download the raw dataset, however. We’ve included NumPy versions
    with the code for this book. As mentioned above, we need two arrays: one for the
    images and the other for the associated labels. You’ll find them in the *cifar10_test_images.npy*
    and *cifar10_test_labels.npy* files, respectively. Let’s take a look:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that the `images` array has four dimensions. The first is the number
    of images in the array (*N* = 10,000). The second and third tell us that the images
    are 32×32 pixels. The last tells us that there are three channels, implying the
    dataset consists of color images. Note that, in general, the number of channels
    might refer to any collection of data grouped that way—it need not be an actual
    image. The `labels` vector has 10,000 elements as well. These are the class labels,
    of which there are 10 classes, a mix of animals and vehicles. For example,
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This indicates that image 123 is of class 2 (bird) and that the label is correct;
    the image displayed should be that of a bird. Recall that, in NumPy, asking for
    a single index returns the entire subarray, so `images[123]` is equivalent to
    `images[123,:,:,:]`. The `fromarray` method of the `Image` class converts a NumPy
    array to an image so `show` can display it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with minibatches means we pass a subset of the entire dataset through
    the model. If our model uses minibatches of 24, then the input to the deep network,
    if using CIFAR-10, is a (24, 32, 32, 3) array: 24 images, each of which has 32
    rows, 32 columns, and 3 channels. We’ll see below that the idea of channels is
    not restricted to the input to a deep network; it also applies to the shape of
    the data passed between layers.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: We’ll return to data for deep networks shortly. But for now, let’s switch gears
    to the more straightforward topic of dataflow in a traditional, feedforward neural
    network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Data Flow in Traditional Neural Networks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we indicated above, in a traditional neural network, the weights between
    layers are stored as matrices. If layer *i* has *n* nodes and layer *i* − 1 has
    *m* outputs, then the weight matrix between the two layers, ***W[i]***, is an
    *n* × *m* matrix. When this matrix is multiplied on the right by the *m* × 1 column
    vector of outputs from layer *i* − 1, the result is an *n* × 1 output representing
    the input to the *n* nodes for layer *i*. Specifically, we calculate
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，在传统的神经网络中，层与层之间的权重以矩阵的形式存储。如果第*i*层有*n*个节点，第*i*−1层有*m*个输出，那么这两层之间的权重矩阵***W[i]***就是一个*n*
    × *m*的矩阵。当这个矩阵与第*i*−1层的*m* × 1列向量相乘时，结果是一个*n* × 1的输出，表示输入到第*i*层的*n*个节点的值。具体来说，我们计算
- en: '***a[i]*** = σ(***W[i]a[i−1]*** + ***b[i]***)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '***a[i]*** = σ(***W[i]a[i−1]*** + ***b[i]***)'
- en: where ***a[i]***[−1], the *m* × 1 vector of outputs from layer *i* − 1, multiplies
    ***W[i]*** to produce an *n* × 1 column vector. We add ***b[i]***, the bias values
    for layer *i*, to this vector and apply the activation function, σ, to every element
    of the resulting vector, ***W[i]a[i]***[−1] + ***b[i]***, to produce ***a[i]***,
    the activations for layer *i*. We feed the activations to layer *i* + 1 as the
    output of layer *i*. By using matrices and vectors, the rules of matrix multiplication
    automatically calculate all the necessary products without explicit loops in the
    code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，***a[i]***[−1]是来自第*i*−1层的*m* × 1输出向量，它与***W[i]***相乘，产生一个*n* × 1列向量。我们将第*i*层的偏置值***b[i]***加到该向量中，并对结果向量***W[i]a[i]***[−1]
    + ***b[i]***的每个元素应用激活函数σ，从而得到***a[i]***，即第*i*层的激活值。我们将激活值作为第*i*层的输出传递给第*i*+1层。通过使用矩阵和向量，矩阵乘法规则自动计算所有必要的乘积，而无需在代码中显式使用循环。
- en: Let’s see an example with a simple neural network. We’ll generate a random dataset
    with two features and then split this dataset into train and test groups. We’ll
    use `sklearn` to train a simple feedforward neural network on the training set.
    The network has a single hidden layer with five nodes and uses a rectified linear
    activation function (ReLU). We’ll then test the trained network to see how well
    it learned and, most importantly, look at the actual weight matrices and bias
    vectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单神经网络的例子。我们将生成一个包含两个特征的随机数据集，然后将该数据集分成训练组和测试组。我们将使用`sklearn`在训练集上训练一个简单的前馈神经网络。该网络有一个隐藏层，包含五个节点，并使用修正线性激活函数（ReLU）。然后我们将测试训练好的网络，看看它学得如何，最重要的是，查看实际的权重矩阵和偏置向量。
- en: 'To build the dataset, we’ll select a set of points in 2D space that are clustered
    but slightly overlapping. We want the network to have to learn something that
    isn’t completely trivial. Here is the code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建数据集，我们将选择一组在二维空间中聚集但略有重叠的点。我们希望网络学习一些不完全简单的内容。以下是代码：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We need the `MLPClassifier` class from `sklearn`, so we load it first. We then
    define a 2D dataset, `x`, consisting of two clouds of 50 points each. The points
    are randomly distributed (`x0`, `y0` and `x1`, `y1`) but centered at (0.2, 0.8)
    and (0.8, 0.2), respectively ❶. Note, we set the NumPy random number seed to a
    fixed value, so each run produces the same set of numbers we’ll see below. Feel
    free to remove this line and experiment with how well the network trains for various
    generations of the dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从`sklearn`导入`MLPClassifier`类，因此首先加载它。然后我们定义一个二维数据集`x`，由两组各50个点组成。点是随机分布的（`x0`，`y0`和`x1`，`y1`），但分别集中在(0.2,
    0.8)和(0.8, 0.2)位置 ❶。请注意，我们将NumPy的随机数种子设置为固定值，因此每次运行都会生成相同的一组数字。如有需要，可以删除这一行并尝试在不同数据集生成的情况下，网络的训练效果。
- en: We know the first 50 points in `x` are from what we’ll call class 0, and the
    next 50 points are class 1, so we define a label vector, `y` ❷. Finally, we randomize
    the order of the points in `x` ❸, being careful to alter the labels in the same
    way, and we split them into a training set (`x_train`) and labels (`y_train`)
    and a test set (`x_test`) and labels (`y_test`). We keep 75 percent of the data
    for training and leave the remaining 25 percent for testing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道`x`中的前50个点来自我们所称之为类别0，接下来的50个点是类别1，因此我们定义一个标签向量`y` ❷。最后，我们随机化`x` ❸中的点的顺序，并小心地以相同的方式调整标签，然后将它们分成训练集（`x_train`）和标签（`y_train`），以及测试集（`x_test`）和标签（`y_test`）。我们保留75%的数据用于训练，剩下的25%用于测试。
- en: '[Figure 9-2](ch09.xhtml#ch09fig02) shows a plot of the full dataset, with each
    feature on one of the axes. The circles correspond to class 0 instances and the
    squares to class 1 instances. There is clear overlap between the two classes.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2](ch09.xhtml#ch09fig02)显示了完整数据集的图形，其中每个特征位于一个坐标轴上。圆圈表示类0实例，方块表示类1实例。两个类别之间有明显的重叠。'
- en: '![image](Images/09fig02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig02.jpg)'
- en: '*Figure 9-2: The dataset used to train the neural network, with the class 0
    instances shown as circles and the class 1 instances as squares*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-2：用于训练神经网络的数据集，类0实例以圆圈表示，类1实例以方块表示*'
- en: 'We’re now ready to train the model. The `sklearn` toolkit makes it easy for
    us, if we use the defaults:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练模型了。如果使用默认设置，`sklearn`工具包使这变得非常简单：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Training involves creating an instance of the model class ❶. Notice that by
    using the defaults, which include using a ReLU activation function, we only need
    to specify the number of nodes in the hidden layers. We want one hidden layer
    with five nodes, so we pass in the tuple `(5,)`. Training is a single call to
    the `fit` function passing in the training data, `x_train`, and the associated
    labels, `y_train`. When complete, we test the model by computing the accuracy
    (`score`) on the test set (`x_test`, `y_test`) and display the result.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括创建模型类的一个实例。注意，使用默认设置时（包括使用ReLU激活函数），我们只需指定隐藏层中节点的数量。我们希望有一个包含五个节点的隐藏层，因此传入元组`(5,)`。训练只需要调用一次`fit`函数，传入训练数据`x_train`和相应的标签`y_train`。完成后，我们通过计算测试集`(x_test,
    y_test)`上的准确率（`score`）来测试模型，并显示结果。
- en: Neural networks are initialized randomly, but because we fixed the NumPy random
    number seed when we generated the dataset, and because `sklearn` uses the NumPy
    random number generator as well, the outcome of training the network should be
    the same for each run of the code. The model has an accuracy of 92 percent on
    the test data ❷. This is convenient for us but concerning as well—so many toolkits
    use NumPy under the hood that interactions due to fixing the random number seed
    are probable, usually undesirable, and perhaps challenging to detect.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是随机初始化的，但由于我们在生成数据集时固定了NumPy随机数种子，并且由于`sklearn`也使用NumPy的随机数生成器，因此每次运行代码时，训练网络的结果应该是相同的。模型在测试数据上的准确率为92%。这对我们很方便，但也令人担忧——如此多的工具包在底层使用NumPy，因而固定随机数种子所导致的交互是很可能发生的，通常是不希望出现的，并且可能很难检测。
- en: We’re now finally ready to get the weight matrices and bias vectors from the
    trained network ❸. Because `sklearn` uses `np.dot` for matrix multiplication,
    we take the transpose of the weight matrices, `W0` and `W1`, to get them in a
    form that’s easier to follow mathematically. We’ll see precisely why this is necessary
    below. Likewise, `b0`, the bias vector for the hidden layer, is a 1D NumPy array,
    so we change it to a column vector. The output layer bias, `b1`, is a scalar,
    as there is only one output for this network, the value we pass to the sigmoid
    function to get the probability of class 1 membership.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在终于准备好从训练好的网络中获取权重矩阵和偏置向量。由于`sklearn`使用`np.dot`进行矩阵乘法，我们取权重矩阵`W0`和`W1`的转置，以便将它们转换为数学上更易于理解的形式。稍后我们将详细说明为什么这样做是必要的。同样，`b0`，隐藏层的偏置向量，是一个1D的NumPy数组，因此我们将其转换为列向量。输出层的偏置`b1`是一个标量，因为该网络只有一个输出，即我们传递给sigmoid函数的值，用于获得属于类1的概率。
- en: Let’s walk through the network for the first test sample. To save space, we’ll
    only show the first three digits of the numeric values, but our calculations will
    use full precision. The input to the network is
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跟随网络计算第一个测试样本。为了节省空间，我们只展示数值的前三位，但我们的计算将使用完整精度。网络的输入是
- en: '![Image](Images/228equ01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/228equ01.jpg)'
- en: We want the network to give us an output leading to the likelihood of this input
    belonging to class 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望网络给出一个输出，表示该输入属于类1的可能性。
- en: 'To get the output of the hidden layer, we multiply ***x*** by the weight matrix,
    ***W***[0], add the bias vector, ***b***[0], and pass that result through the
    ReLU:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得隐藏层的输出，我们将***x***与权重矩阵***W***[0]相乘，加入偏置向量***b***[0]，然后将结果通过ReLU：
- en: '![Image](Images/228equ02.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/228equ02.jpg)'
- en: 'The hidden layer to output transition uses the same form, with ***a***[0] in
    place of ***x***, but here, there is no ReLU applied:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层到输出层的过渡使用相同的形式，用***a***[0]代替***x***，但这里没有应用ReLU：
- en: '![Image](Images/229equ01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/229equ01.jpg)'
- en: 'To get the final output probability, we use ***a*****[1]**, a scalar value,
    as the argument to the *sigmoid function*, also called the *logistic function*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/229equ02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'This means the network has assigned a 35.5 percent likelihood of the input
    value being a member of class 1\. The usual threshold for class assignment for
    a binary model is 50 percent, so the network would assign ***x*** to class 0\.
    A peek at `y_test[0]` tells us the network is correct in this case: ***x*** is
    from class 0.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Data Flow in Convolutional Neural Networks
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We saw above how data flow through a traditional neural network was straightforward
    matrix-vector math. To track data flow through a *convolutional neural network
    (CNN)*, we need to learn first what the convolution operation is and how it works.
    Specifically, we’ll learn how to pass data through convolutional and pooling layers
    to a fully connected layer at the top of the model. This sequence accounts for
    many CNN architectures, at least at a conceptual level.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolution involves two functions and the *sliding* of one over the other.
    If the functions are *f*(*x*) and *g*(*x*), convolution is defined as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/09equ01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Fortunately for us, we’re working in a discrete domain and more often than not
    with 2D inputs, so the integral is not actually used, though * is still a useful
    notation for the operation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The net effect of [Equation 9.1](ch09.xhtml#ch09equ01) is to slide *g*(*x*)
    over *f*(*x*) for different shifts. Let’s clarify using a 1D, discrete example.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Convolution in One Dimension
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Figure 9-3](ch09.xhtml#ch09fig03) shows a plot on the bottom and two sets
    of numbers labeled *f* and *g* on the top.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-3: A 1D, discrete convolution*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the numbers shown at the top of [Figure 9-3](ch09.xhtml#ch09fig03).
    The first row lists the discrete values of *f*. Below that is *g*, a three-element
    linear ramp. Convolution aligns *g* with the left edge of *f* as shown. We multiply
    corresponding elements between the two arrays,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[2, 6, 15] × [−1, 0, 1] = [−2, 0, 15]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: and then sum the resulting values,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: −2 + 0 + 15 = 13
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: to produce the value that goes in the indicated element of the output, *f* *
    *g*. To complete the convolution, *g* slides one element to the right, and the
    process repeats. Note that in [Figure 9-3](ch09.xhtml#ch09fig03), we’re showing
    every other alignment of *f* and *g* for clarity, so it’ll appear as though *g*
    is sliding two elements to the right. In general, we refer to *g* as a *kernel*,
    the set of values that slide over the input, *f*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The plot on the bottom of [Figure 9-3](ch09.xhtml#ch09fig03) is *f*(*x*) = ⌊255
    exp(−0.5*x*²)⌋ for *x* in [−3, 3] at the points marked with circles. The floor
    operation makes the output an integer to simplify the discussion below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The square points in [Figure 9-3](ch09.xhtml#ch09fig03) are the output of the
    convolution of *f*(*x*) with *g*(*x*) = [−1, 0, 1].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The *f* and *f* * *g* points in [Figure 9-3](ch09.xhtml#ch09fig03) are generated
    via
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](ch09.xhtml#ch09fig03)中的*f*和*f* * *g*点是通过以下方式生成的：'
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code requires some explanation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要一些解释。
- en: First, we have `x`, a vector spanning [−3, 3] in 20 steps; this vector generates
    `f` (*f*(*x*) above). We want `f` to be of integer type, which is what `astype`
    does for us. Next, we define `g`, the small linear ramp. As we’ll see, the convolution
    operation slides `g` over the elements of `f` to produce the output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有`x`，一个在[−3, 3]范围内按20步生成的向量；这个向量生成了`f`（上面的*f*（*x*））。我们希望`f`是整数类型，这就是`astype`为我们做的事情。接下来，我们定义了`g`，这是一个小的线性斜坡。正如我们所看到的，卷积操作将`g`滑动到*f*的各个元素上以生成输出。
- en: The convolution operation comes next. As convolution is commonly used, NumPy
    supplies a 1D convolution function, `np.convolve`. The first argument is *f*,
    and the second is *g*. I’ll explain shortly why we added `[::-1]` to `g` to reverse
    it. I’ll also explain the meaning of `mode='same'`. The output of the convolution
    is stored in `fp`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是卷积操作。由于卷积是常用的操作，NumPy提供了一个一维卷积函数`np.convolve`。第一个参数是*f*，第二个是*g*。稍后我会解释为什么我们要在`g`上添加`[::-1]`来反转它。我还会解释`mode='same'`的含义。卷积的输出将存储在`fp`中。
- en: The first position shown in the top part of [Figure 9-3](ch09.xhtml#ch09fig03)
    fills in the 13 in the output. Where does the 6 to the left of the 13 come from?
    Convolution has issues at the edges of *f*, where the kernel does not entirely
    cover the input. For a three-element kernel, there will be one edge element on
    each end of *f*. Kernels typically have an odd number of values, so there is a
    clear middle element. If *g* had five elements, there would be two elements on
    each end of *f* that *g* wouldn’t cover.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](ch09.xhtml#ch09fig03)顶部显示的第一个位置填入了输出中的13。那么，13左边的6是从哪里来的呢？卷积在*f*的边缘存在问题，因为卷积核并没有完全覆盖输入数据。对于一个包含三个元素的卷积核，*f*的每一端都会有一个边缘元素。卷积核通常有奇数个值，因此会有一个明确的中间元素。如果*g*有五个元素，那么在*f*的两端会有两个元素是*g*无法覆盖的。'
- en: Convolution functions need to make a choice about these edge cases. One option
    would be to return only the valid portion of the convolution, to ignore the edge
    cases. If we had used this approach, called *valid convolution*, the output, `yp`,
    would start with element 13 and be two less in length than the input, `y`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积函数需要在这些边缘情况做出选择。一个选择是仅返回卷积的有效部分，忽略边缘情况。如果我们采用这种方法，称为*有效卷积*，那么输出`yp`将从元素13开始，长度比输入`y`少两个。
- en: Another approach is to fill in missing values in *f* with zero. This is known
    as *zero padding*, and we typically use it to make the output of a convolution
    operation the same size as the input.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用零填充*f*中的缺失值。这被称为*零填充*，我们通常使用它使卷积操作的输出与输入大小相同。
- en: 'Using `mode=''same''` with `np.convolve` selects zero padding. This explains
    the 6 to the left of the 13\. It’s what we get when adding a zero before the 2
    in *f* and applying the kernel:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mode='same'`与`np.convolve`一起时，选择了零填充。这解释了13左边的6。它是我们在*f*的2前面加上0并应用卷积核时得到的结果：
- en: '[0, 2, 6] × [−1, 0, 1] = [0, 0, 6], 0 + 0 + 6 = 6'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[0, 2, 6] × [−1, 0, 1] = [0, 0, 6]，0 + 0 + 6 = 6'
- en: If we wanted only the valid output values, we would have used `mode='valid'`
    instead.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想要有效的输出值，我们会使用`mode='valid'`。
- en: The call to `np.convolve` above didn’t use `g`. We passed `g[::-1]` instead,
    the reverse of `g`. We did this to make `np.convolve` act like the convolutions
    used in deep neural networks. From a mathematical and signal processing perspective,
    convolution uses the reverse of the kernel. The `np.convolve` function, therefore,
    reverses the kernel, meaning we need to reverse it beforehand to get the effect
    we want. To be technical, if we perform the operation we’ve called *convolution*
    without flipping the kernel, we’re actually performing *cross-correlation*. This
    issue seldom comes up in deep learning because we *learn* the kernel elements
    during training—we don’t assign them ahead of time. With that in mind, any flipping
    of the kernel by the toolkit process implementing the convolution operation won’t
    affect the outcome, because the learned kernel values were learned with that flip
    in place. We’ll assume going forward that there is no flip and, when necessary,
    we’ll flip the kernels we give to NumPy and SciPy functions. Additionally, we’ll
    continue to use the term *convolution* in this no-flip-of-the-kernel deep learning
    sense.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上述对`np.convolve`的调用并没有使用`g`，我们传入的是`g[::-1]`，即`g`的反向。我们这样做是为了让`np.convolve`的行为像深度神经网络中使用的卷积。从数学和信号处理的角度来看，卷积操作使用的是核的反向。因此，`np.convolve`函数会反转核，这意味着我们需要提前反转核，才能得到我们想要的效果。更技术一点地说，如果我们执行的操作被称为*卷积*，但没有翻转核，那么我们实际上在做*交叉相关*。在深度学习中，这个问题很少出现，因为我们在训练过程中*学习*核的元素，而不是提前指定它们。因此，工具包实现卷积操作时对核进行的任何翻转都不会影响结果，因为学习到的核值就是在翻转后的状态下学习得到的。我们假设接下来没有翻转，并且在必要时会翻转我们传递给NumPy和SciPy函数的核。另外，我们将继续使用*卷积*这一术语，指的是在深度学习中没有翻转核的情况。
- en: In general, convolution with discrete inputs involves placing the kernel over
    the input starting on the left, multiplying matching elements, summing, and putting
    the result in the output at the point where the center of the kernel matches.
    The kernel then slides one element to the right, and the process repeats. We can
    extend the discrete convolution operation to two dimensions. Most modern deep
    CNNs use 2D kernels, though it’s possible to use 1D and 3D kernels as well.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，离散卷积操作涉及将核放置在输入数据上，从左侧开始，进行元素匹配相乘、求和，并将结果放入输出中，位置是核的中心与输入位置重合的地方。然后，核向右滑动一个元素，过程重复进行。我们可以将离散卷积操作扩展到二维。大多数现代深度卷积神经网络（CNN）使用二维核，尽管也可以使用一维和三维核。
- en: Convolution in Two Dimensions
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二维卷积
- en: 'Convolution with a 2D kernel requires a 2D array. Images are 2D arrays of values,
    and convolution is a common image processing operation. For example, let’s load
    an image, the face of the raccoon we saw in [Chapter 3](ch03.xhtml#ch03), and
    alter it with a 2D convolution. Consider the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二维核进行卷积需要一个二维数组。图像是值的二维数组，卷积是常见的图像处理操作。例如，我们加载一张图像，之前在[第3章](ch03.xhtml#ch03)中看到的浣熊面部图像，并使用二维卷积对其进行处理。考虑以下内容：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we’re using the SciPy `convolve2d` function from the `signal` module.
    First, we load the raccoon image and subset it to a 512×512-pixel image of the
    raccoon’s face (`img`). Next, we define a 3 × 3 kernel, `k`. Lastly, we convolve
    the kernel, as it is, with the face image, storing the result in `c`. The `mode='same'`
    keyword zero pads the image to handle the edge cases.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是来自`signal`模块的SciPy `convolve2d`函数。首先，我们加载浣熊图像，并将其裁剪为一个512×512像素的浣熊面部图像（`img`）。接着，我们定义一个3
    × 3的核，`k`。最后，我们将这个核与面部图像进行卷积，并将结果存储在`c`中。`mode='same'`关键字对图像进行零填充，以处理边缘情况。
- en: The code above leads to
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会导致
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we’re showing the upper 8 × 8 corner of the image and the valid portion
    of the convolution. Recall, the valid portion is the part where the kernel completely
    covers the input array.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们展示的是图像的上8×8角以及卷积的有效部分。回顾一下，有效部分是指核完全覆盖输入数组的部分。
- en: For the kernel and the image, the first valid convolution output is −209\. Mathematically,
    the first step is element-wise multiplication with the kernel,
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于核和图像，第一个有效的卷积输出是−209。数学上，第一步是与核进行逐元素相乘，
- en: '![Image](Images/233equ01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/233equ01.jpg)'
- en: followed by a summation,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后进行求和，
- en: 264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209
- en: Notice how the kernel used wasn’t `k` as we defined it. Instead, `convolve2d`
    flipped the kernel top to bottom and then left to right before it was applied.
    The remainder of `c` flows from moving the kernel one position to the right and
    repeating the multiplication and addition. At the end of a row, the kernel moves
    down one position and back to the left, until the entire image has been processed.
    Deep learning toolkits refer to this motion as the *stride*, and it need not be
    one position or equal in the horizontal and vertical directions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-4](ch09.xhtml#ch09fig04) shows the effect of the convolution.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig04.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-4: The original raccoon face image (left) and the convolution result
    (right)*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: To make the image, `c` was shifted up, so the minimum value was zero, and then
    divided by the maximum to map to [0, 1]. Finally, the output was multiplied by
    255 and displayed as a grayscale image. The original face image is on the left.
    The convolved image is on the right. Convolution of the image with the kernel
    has altered the image, emphasizing some features while suppressing others.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolving kernels with images isn’t merely an exercise to help us understand
    the convolution operation. It’s of profound importance in the training of CNNs.
    Conceptually, a CNN consists of two main parts: a set of convolution and other
    layers taught to learn a new representation of the input, and a top-level classifier
    taught to use the new representation to classify the inputs. It’s the joint learning
    of the new representation and the classifier that makes CNNs so powerful. The
    key to learning a new representation of the input is the set of learned convolution
    kernels. How the kernels alter the input as data flows through the CNN creates
    the new representation. Training with gradient descent and backpropagation teaches
    the network which kernels to create.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: We’re now in a position to follow data through a CNN’s convolutional layers.
    Let’s take a look.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Above, we discussed how deep networks pass tensors from layer to layer and how
    the tensor usually has four dimensions, *N* × *H* × *W* × *C*. To follow data
    through a convolutional layer, we’ll ignore *N*, knowing that what we discuss
    is applied to each sample in the tensor. This leaves us with inputs to the convolutional
    layer that are *H* × *W* × *C*, a 3D tensor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The output of a convolutional layer is another 3D tensor. The height and width
    of the output depend on the convolution kernels’ particulars and how we decide
    to handle the edges. We’ll use valid convolution for the examples here, meaning
    we’ll discard parts of the input that the kernel doesn’t wholly cover. If the
    kernel is 3 × 3, the output will be two less in height and width, one less for
    each edge. A 5 × 5 kernel loses four in height and width, two less for each edge.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer uses sets of *filters* to accomplish its goal. A filter
    is a stack of kernels. We need one filter for each of the desired output channels.
    The number of kernels in the stack of each filter matches the number of channels
    in the input. So, if the input has *M* channels, and we want *N* output channels
    using *K* × *K* kernels, we need *N* filters, each of which is a stack of *M K*
    × *K* kernels.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we have a bias value for each of the *N* filters. We’ll see below
    how the bias is used, but we now know how many parameters we need to learn to
    implement a convolutional layer with *M* input channels, *K* × *K* kernels, and
    *N* outputs. It’s *K* × *K* × *M* × *N* for *N* filters with *K* × *K* × *M* parameters
    each, plus *N* bias terms—one per filter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make all of this concrete. We have a convolutional layer. The input to
    the layer is an (*H*,*W*,*C*) = (5,5,2) tensor, meaning a height and width of
    five and two channels. We’ll use a 3 × 3 kernel with valid convolution, so the
    output in height and width is 3 × 3 from the 5 × 5 input. We get to select the
    number of output channels. Let’s use three. Therefore, we need to use convolution
    and kernels to map a (5,5,2) input to a (3,3,3) output. From what we discussed
    above, we know we need three filters, and each filter has 3 × 3 × 2 parameters,
    plus a bias term.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Our input stack is
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/235equ01.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: We’ve split the third dimension to show the two input channels, each 5 × 5.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The three filters are
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/236equ01.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Again, we’ve separated the third dimension. Notice how each filter has two 3
    × 3 kernels, one for each channel of the 5 × 5 × 2 input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work through applying the first filter, *f*[0]. We need to convolve the
    first channel of the input with the first kernel of *f*[0]:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/236equ02.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'Then, we need to convolve the second input channel with the second kernel of
    *f*[0]:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/236equ03.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we add the two convolution outputs along with the single bias scalar:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/236equ04.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: We now have the first 3 × 3 output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Repeating the process above for *f*[1] and *f*[2] gives
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/236equ05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: We’ve completed the convolutional layer and generated the 3 × 3 × 3 output.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Many toolkits make it easy to add operations in the call that sets up the convolutional
    layer, but, conceptually, these are layers of their own that accept the 3 × 3
    × 3 output as an input. For example, if requested, Keras will apply a ReLU to
    the output. Applying a ReLU, a nonlinearity, to the output of the convolution
    would give us
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/237equ01.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Note that all elements less than zero are now zero. We use a nonlinearity between
    convolutional layers for the same reason we use a nonlinear activation function
    in a traditional neural network: to keep the convolutional layers from collapsing
    into a single layer. Notice how the operation to generate the filter outputs is
    purely linear; each output element is a linear combination of input values. Adding
    the ReLU breaks this linearity.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: One reason for the creation of convolutional layers was to reduce the number
    of learned parameters. For the example above, the input was 5 × 5 × 2 = 50 elements.
    The desired output was 3 × 3 × 3 = 27 elements. A fully connected layer between
    these would need to learn 50 × 27 = 1,350 weights, plus another 27 bias values.
    However, the convolutional layer learned three filters, each with 3 × 3 × 2 weights,
    as well as three bias values, for a total of 3(3 × 3 × 2) + 3 = 57 parameters.
    Adding the convolutional layer saved learning some 1,300 additional weights.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The output of a convolutional layer is often the input to a pooling layer. Let’s
    consider that type of layer next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional networks often use *pooling layers* after convolutional layers.
    Their use is a bit controversial, as they discard information, and the loss of
    information might make it harder for the network to learn spatial relationships.
    Pooling is generally performed in the spatial domain along the input tensor’s
    height and width while preserving the number of channels.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The pooling operation is straightforward: you move a window over the image,
    usually 2 × 2 with a stride of two, to group values. The specific pooling operation
    performed on each group is either max or average. The max-pooling operation preserves
    the maximum value in the window and discards the rest. Average pooling takes the
    mean of the values in the window.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: A 2 × 2 window with a stride of two results in a reduction of a factor of two
    in each spatial direction. Therefore, a (24,24,32) input tensor leads to a (12,12,32)
    output tensor. [Figure 9-5](ch09.xhtml#ch09fig05) illustrates the process for
    maximum pooling.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-5: Max pooling with a 2* × *2 window and a stride of two*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: One channel of the input, with a height and width of eight, is on the left.
    The 2 × 2 window slides over the input, jumping by two, so there is no overlap
    of windows. The output for each 2 × 2 region of the input is the maximum value.
    Average pooling would instead output the mean of the four numbers. As with normal
    convolution, at the end of the row, the window slides down two positions, and
    the process repeats to change the 8 × 8 input channel to a 4 × 4 output channel.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned above, pooling without overlap in the windows loses spatial information.
    This has caused some in the deep learning community, most notably Geoffrey Hinton,
    to lament its use, as dropping spatial information distorts the relationship between
    objects or parts of objects in the input. For example, applying a 2 × 2 max pooling
    window with a stride of one instead of two to the input matrix of [Figure 9-5](ch09.xhtml#ch09fig05)
    produces
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/238equ01.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: This is a 7 × 7 output, which only loses one row and column of the original
    8 × 8 input. In this case, the input matrix was randomly generated, so we should
    expect a max-pooling operation biased toward eights and nines—there is no structure
    to capture. This is not usually the case in an actual CNN, of course, as it’s
    the spatial structure inherent in the inputs we wish to utilize.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Pooling is commonly used in deep learning, especially for CNNs, so it’s essential
    to understand what a pooling operation is doing and be aware of its potential
    pitfalls. Let’s move on now to the output end of a CNN, typically the fully connected
    layers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layers
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A fully connected layer in a deep network is, in terms of weights and data,
    identical to a regular layer in a traditional neural network. Many deep networks
    concerned with classification pass the output of a set of convolution and pooling
    layers to the first fully connected layer via a layer that flattens the tensor,
    essentially unraveling it into a vector. Once the output is a vector, the fully
    connected layer uses a weight matrix in the same way a traditional neural network
    does to map a vector input to a vector output.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Data Flow Through a Convolutional Neural Network
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s put all the pieces together to see how data flows through a CNN from input
    to output. We’ll use a simple CNN trained on the MNIST dataset, a collection of
    28×28-pixel grayscale images of handwritten digits. The architecture is shown
    next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Input → Conv(32) → Conv(64) → Pool → Flatten → Dense(128) → Dense(10)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The input is a 28×28-pixel grayscale image (one channel). The convolutional
    layers (conv) use 3 × 3 kernels and valid convolution, so their output’s height
    and width are two less than their input. The first convolutional layer learns
    32 filters while the second learns 64\. We’re ignoring layers that do not affect
    the amount of data in the network, like the ReLU layers after the convolutional
    layers. The max-pooling layer is assumed to use a 2 × 2 window with a stride of
    two. The first fully connected layer (dense) has 128 nodes, followed by an output
    layer of 10 nodes, one for each digit, 0 to 9.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The tensors passed through this network for a single input sample are
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: (28,28,1) →(26,26,32) →(24,24,64) →(12,12,64) → 9216 → 128 → 10
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Input        Conv        Conv         Pool      Flatten  Dense   Dense
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The flatten layer unravels the (12,12,64) tensor to form a vector of 9,216 elements
    (12 × 12 × 64 = 9,216). We pass the 9,216 elements that the flatten layer outputs
    through the first dense layer to generate 128 output values, and the last step
    takes the 128-element vector and maps it to 10 output values.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Note, the values above refer to the *data* passed through the network for each
    input sample, one of the *N* samples in the minibatch. This is not the same as
    the number parameters (weights and biases) the network needed to learn during
    training.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The network shown above was trained on the MNIST digits using Keras. [Figure
    9-6](ch09.xhtml#ch09fig06) illustrates the action of the network for two inputs
    by showing, visually, the output of each layer. Specifically, it shows each layer’s
    output for two input images, depicting a 4 and a 6.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig06.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-6: A visual representation of the output of a CNN for two sample
    inputs*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Starting at the top, we see the two inputs. For the figure, intensities have
    been reversed, so darker represents higher numeric values. The input is a (28,28,1)
    tensor, the 1 indicating a single-channel grayscale image. Valid convolution with
    a 3 × 3 kernel returns a 26 × 26 output. The first convolutional layer learned
    32 filters, so the output is a (26,26,32) tensor. In the figure, we show the output
    of each filter as an image. Zero is scaled to midlevel gray (intensity 128), more
    positive values are darker, and more negative values are lighter. We see differences
    in how the inputs have been affected by the learned filters. The single input
    channel means each filter in this layer is a single 3 × 3 kernel. Transitions
    between light and dark indicate edges in particular orientations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: We pass the (26,26,32) tensor through a ReLU (not shown here) and then through
    the second convolutional layer. The output of this layer is a (24,24,64) tensor
    shown as an 8 × 8 grid of images in the figure. We can see many parts of the input
    digits highlighted.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The pooling layer preserves the number of channels but reduces the spatial dimension
    by two. In image form, the 8 × 8 grid of 24×24-pixel images is now an 8 × 8 grid
    of 12×12-pixel images. The flatten operation maps the (12,12,64) tensor to a 9,216-element
    vector.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The output of the first dense layer is a vector of 128 numbers. For [Figure
    9-6](ch09.xhtml#ch09fig06), we show this as a 128-element bar code. The values
    run from left to right. The height of each bar is unimportant and was selected
    only to make the bar code easy to see. The bar code generated from the input image
    is the final representation that the top layer of 10 nodes uses to create the
    output passed through the softmax function. The highest softmax output is used
    to select the class label, “4” or “6.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can think of all the CNN layers through the first dense layer
    as mapping inputs to a new representation, one that makes it easy for a simple
    classifier to handle. Indeed, if we pass 10 examples of “4” and “6” digits through
    this network and display the resulting 128-node feature vectors, we get [Figure
    9-7](ch09.xhtml#ch09fig07), where we can easily see the difference between the
    digit patterns.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig07.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-7: The first fully connected layer outputs for multiple “4” and “6”
    inputs*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the entire point of writing digits as we do is to make it easy for
    humans to see the differences between them. While we could teach ourselves to
    differentiate digits using the 128-element vector images, we naturally prefer
    to use the written digits because of habitual use and the fact we already employ
    highly sophisticated hierarchical feature detectors via our brain’s visual system.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The example of a CNN learning a new input representation that’s more conducive
    to interpretation by a machine is worth bearing in mind, since what a human might
    use in an image as a clue to its classification is not necessarily what a network
    learns to use. This might explain, in part, why certain preprocessing steps, like
    the changes made to training samples during data augmentation, are so effective
    in helping the network learn to generalize, when many of those alterations seem
    strange to us.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of this chapter was to demonstrate how neural networks manipulate
    data from input to output. Naturally, we couldn’t cover all network types, but,
    in general, the principles are the same: for traditional neural networks, data
    is passed from layer to layer as a vector, and for deep networks, it’s passed
    as a tensor, typically of four dimensions.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to present data to a network, either as a feature vector or a
    multidimensional input. We followed this by looking at how to pass data through
    a traditional neural network. We saw how the vectors used as input to, and output
    from, a layer made the implementation of a traditional neural network a straightforward
    exercise in matrix-vector multiplication and addition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Next, we saw how a deep convolutional network passes data from layer to layer.
    We learned first about the convolution operation and then about the specifics
    of how convolutional and pooling layers manipulate data as tensors—a 3D tensor
    for each sample in the input minibatch. At the top of a CNN meant for classification
    are fully connected layers, which we saw act precisely as they do in a traditional
    neural network.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: We ended the chapter by showing, visually, how input images moved through a
    CNN to produce an output representation, allowing the network to label the inputs
    correctly. We briefly discussed what this process might mean in terms of what
    a network picks up on during training and how that might differ from what a human
    naturally sees in an image.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: We are now in a position to discuss backpropagation, the first of the two critical
    algorithms that, together with gradient descent, make training deep neural networks
    possible.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
