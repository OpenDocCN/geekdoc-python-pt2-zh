<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch08"><span epub:type="pagebreak" id="page_193"/><strong><span class="big">8</span><br/>MATRIX CALCULUS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents"><a href="ch07.xhtml#ch07">Chapter 7</a> introduced us to differential calculus. In this chapter, we’ll discuss <em>matrix calculus</em>, which extends differentiation to functions involving vectors and matrices.</p>&#13;
<p class="indent">Deep learning works extensively with vectors and matrices, so it makes sense to develop a notation and approach to representing derivatives involving these objects. That’s what matrix calculus gives us. We saw a hint of this at the end of <a href="ch07.xhtml#ch07">Chapter 7</a>, when we introduced the gradient to represent the derivative of a scalar function of a vector—a function that accepts a vector argument and returns a scalar, <em>f</em>(<em><strong>x</strong></em>).</p>&#13;
<p class="indent">We’ll start with the table of matrix calculus derivatives and their definitions. Next, we’ll examine some identities involving matrix derivatives. Mathematicians love identities; however, to preserve our sanity, we’ll only consider a handful. Some special matrices come out of matrix calculus, namely the Jacobian and Hessian. You’ll run into both of these matrices during your sojourn through deep learning, so we’ll consider them next in the context of optimization. Recall that training a neural network is, fundamentally, an optimization problem, so understanding what these special matrices represent and how we use them is especially important. We’ll close the chapter with some examples of matrix derivatives.</p>&#13;
<h3 class="h3" id="ch08lev1_1"><span epub:type="pagebreak" id="page_194"/>The Formulas</h3>&#13;
<p class="noindent"><a href="ch08.xhtml#ch08tab01">Table 8-1</a> summarizes the matrix calculus derivatives we’ll explore in this chapter. These are the ones commonly used in practice.</p>&#13;
<p class="tabcap" id="ch08tab01"><strong>Table 8-1:</strong> Matrix Calculus Derivatives</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"/></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Scalar</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Vector</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Matrix</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Scalar</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em>f</em>/∂<em>x</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em><strong>f</strong></em>/∂<em>x</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em><strong>F</strong></em>/∂<em>x</em></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Vector</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em>f</em>/∂<em><strong>x</strong></em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em>f</em>/∂<em><strong>x</strong></em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">—</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Matrix</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em><strong>f</strong></em>/∂<em><strong>X</strong></em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">—</p></td>&#13;
<td style="vertical-align: top"><p class="tab">—</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The columns of <a href="ch08.xhtml#ch08tab01">Table 8-1</a> represent the function, meaning the return value. Notice we use three versions of the letter f: regular, bold, and capital. We use <em>f</em> if the return value is a scalar, <em><strong>f</strong></em> if a vector, and <em><strong>F</strong></em> if a matrix. The rows of <a href="ch08.xhtml#ch08tab01">Table 8-1</a> are the variables the derivatives are calculated with respect to. The same notation applies: <em>x</em> is a scalar, <em><strong>x</strong></em> is a vector, and <em><strong>X</strong></em> is a matrix.</p>&#13;
<p class="indent"><a href="ch08.xhtml#ch08tab01">Table 8-1</a> defines six derivatives, but there are nine cells in the table. While possible to define, the remaining derivatives are not standardized or used often enough to make covering them worthwhile. That’s good for us, as the six are enough of a challenge for our mathematical brains.</p>&#13;
<p class="indent">The first derivative in <a href="ch08.xhtml#ch08tab01">Table 8-1</a>, the one in the upper left, is the normal derivative of <a href="ch07.xhtml#ch07">Chapter 7</a>, a function producing a scalar with respect to a scalar. (Refer to <a href="ch07.xhtml#ch07">Chapter 7</a> for everything you need to know about standard differentiation.)</p>&#13;
<p class="indent">We’ll cover the remaining five derivatives in the sections below. We define each one in terms of scalar derivatives. We’ll first show the definition and then explain what the notation represents. The definition will help you build a model in your head of what the derivative is. I suspect that by the end of this section you’ll be predicting the definitions in advance.</p>&#13;
<p class="indent">Before we start, however, there is a complication we should discuss. Matrix calculus is notation heavy, but there’s no universal agreement on the notation. We’ve seen this before with the many ways to indicate differentiation. For matrix calculus, the two approaches are <em>numerator layout</em> or <em>denominator layout</em>. Specific disciplines seem to favor one over the other, though exceptions are almost the norm, as is mixing notations. For deep learning, a nonscientific perusal on my part seems to indicate a slight preference for numerator layout, so that’s what we’ll use here. Just be aware that there are two forms out there. One is typically the transpose of the other.</p>&#13;
<h4 class="h4" id="ch08lev2_1">A Vector Function by a Scalar Argument</h4>&#13;
<p class="noindent">A vector function accepting a scalar argument is our first derivative; see <a href="ch08.xhtml#ch08tab01">Table 8-1</a>, second column of the first row. We write such a function as <em><strong>f</strong></em>(<em>x</em>) <span epub:type="pagebreak" id="page_195"/>to indicate a scalar argument, <em>x</em>, and a vector output, <em><strong>f</strong></em>. Functions like <em><strong>f</strong></em> take a scalar and map it to a multidimensional vector:</p>&#13;
<p class="center"><em><strong>f</strong></em> : ℝ → ℝ<sup><em>m</em></sup></p>&#13;
<p class="noindent">Here, <em>m</em> is the number of elements in the output vector. Functions like <em><strong>f</strong></em> are known as <em>vector-valued functions</em> with scalar arguments.</p>&#13;
<p class="indent">A parametric curve in 3D space is an excellent example of such a function. Those functions are often written as</p>&#13;
<div class="imagec"><img src="Images/195equ01.jpg" alt="Image" width="264" height="22"/></div>&#13;
<p class="noindent">where <img src="Images/195equ02b.jpg" alt="Image" width="8" height="11"/>, <img src="Images/195equ02a.jpg" alt="Image" width="7" height="15"/>, and <img src="Images/195equ03.jpg" alt="Image" width="7" height="12"/> are unit vectors in the x, y, and z directions.</p>&#13;
<p class="indent"><a href="ch08.xhtml#ch08fig01">Figure 8-1</a> shows a plot of a 3D parametric curve,</p>&#13;
<div class="imagec" id="ch08equ01"><img src="Images/08equ01.jpg" alt="Image" width="481" height="22"/></div>&#13;
<p class="noindent">where, as <em>t</em> varies, the three axis values also vary to trace out the spiral. Here, each value of <em>t</em> specifies a single point in 3D space.</p>&#13;
<div class="image" id="ch08fig01"><img src="Images/08fig01.jpg" alt="image" width="532" height="475"/></div>&#13;
<p class="figcap"><em>Figure 8-1: A 3D parametric curve</em></p>&#13;
<p class="indent">In matrix calculus notation, we don’t write <em><strong>f</strong></em> as shown in <a href="ch08.xhtml#ch08equ01">Equation 8.1</a>. Instead, we write <em><strong>f</strong></em> as a column vector of the functions,</p>&#13;
<div class="imagec"><img src="Images/195equ04.jpg" alt="Image" width="142" height="70"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_196"/>and, in general,</p>&#13;
<div class="imagec"><img src="Images/196equ01.jpg" alt="Image" width="148" height="96"/></div>&#13;
<p class="noindent">for <em><strong>f</strong></em> with <em>n</em> elements.</p>&#13;
<p class="indent">The derivative of <em><strong>f</strong></em>(<em>x</em>) is known as the <em>tangent vector</em>. What does the derivative look like? Since <em><strong>f</strong></em> is a vector, we might expect the derivative of <em><strong>f</strong></em> to be the derivatives of the functions representing each element of <em><strong>f</strong></em>, and we’d be right:</p>&#13;
<div class="imagec"><img src="Images/196equ02.jpg" alt="Image" width="154" height="96"/></div>&#13;
<p class="indent">Let’s look at a simple example. First, we will define <em><strong>f</strong></em>(<em>x</em>), and then the derivative:</p>&#13;
<div class="imagec"><img src="Images/196equ03.jpg" alt="Image" width="344" height="50"/></div>&#13;
<p class="noindent">Here, each element of ∂<em><strong>f</strong></em>/∂<em>x</em> is the derivative of the corresponding function in <em><strong>f</strong></em>.</p>&#13;
<h4 class="h4" id="ch08lev2_2">A Scalar Function by a Vector Argument</h4>&#13;
<p class="noindent">In <a href="ch07.xhtml#ch07">Chapter 7</a>, we learned that a function accepting a vector input but returning a scalar is a scalar field:</p>&#13;
<p class="center"><em>f</em> : ℝ<sup><em>m</em></sup> → ℝ</p>&#13;
<p class="noindent">We also learned that the derivative of this function is the gradient. In matrix calculus notation, we write ∂<em>f</em>/∂<em><strong>x</strong></em> for <em>f</em>(<em><strong>x</strong></em>) as</p>&#13;
<div class="imagec"><img src="Images/196equ04.jpg" alt="Image" width="277" height="50"/></div>&#13;
<p class="noindent">where <em><strong>x</strong></em> = [<em>x</em><sub>0</sub> <em>x</em><sub>1</sub> ... <em>x</em><sub><em>m</em>–1</sub>]<sup>⊤</sup> is a vector of variables, and <em>f</em> is a function of those variables.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_197"/>Notice, because we decided to use the numerator layout approach, ∂<em>f</em>/∂<em><strong>x</strong></em> is written as a <em>row</em> vector. So, to be true to our notation, we have to write</p>&#13;
<div class="imagec"><img src="Images/197equ01.jpg" alt="Image" width="133" height="50"/></div>&#13;
<p class="noindent">turning the row vector into a column vector to match the gradient. Remember that ▽ is the symbol for gradient; we saw an example of the gradient in Chapter 7.</p>&#13;
<h4 class="h4" id="ch08lev2_3">A Vector Function by a Vector</h4>&#13;
<p class="noindent">If the derivative of a vector-valued function by a scalar produces a column vector, and the derivative of a scalar function by a vector results in a row vector, does the derivative of a vector-valued function by a vector produce a matrix? The answer is yes. In this case, we’re contemplating ∂<em><strong>f</strong></em>/∂<em><strong>x</strong></em> for <em><strong>f</strong></em>(<em><strong>x</strong></em>), a function that accepts a vector input and returns a vector.</p>&#13;
<p class="indent">The numerator layout convention gave us a column vector for the derivative of <em><strong>f</strong></em>(<em>x</em>), implying we need a row for each of the functions in <em><strong>f</strong></em>. Similarly, the derivative of <em>f</em>(<em><strong>x</strong></em>) produced a row vector. Therefore, merging the two gives us the derivative of <em><strong>f</strong></em>(<em><strong>x</strong></em>):</p>&#13;
<div class="imagec" id="ch08equ02"><img src="Images/08equ02.jpg" alt="Image" width="487" height="138"/></div>&#13;
<p class="noindent">This is for a function, <em><strong>f</strong></em>, returning an <em>n</em>-element vector and accepting an <em>m-</em>element vector, <em><strong>x</strong></em>, as its argument:</p>&#13;
<p class="center"><em>f</em> : ℝ<sup><em>m</em></sup> → ℝ<sup><em>n</em></sup></p>&#13;
<p class="indent">Each row of <em><strong>f</strong></em> is a scalar function of <em><strong>x</strong></em>, for example, <em>f</em><sub>0</sub>(<em><strong>x</strong></em>). Therefore, we can write <a href="ch08.xhtml#ch08equ02">Equation 8.2</a> as</p>&#13;
<div class="imagec" id="ch08equ03"><img src="Images/08equ03.jpg" alt="Image" width="430" height="99"/></div>&#13;
<p class="noindent">This gives us the matrix as a collection of gradients, one for each scalar function in <em><strong>f</strong></em>. We’ll return to this matrix later in the chapter.</p>&#13;
<h4 class="h4" id="ch08lev2_4"><span epub:type="pagebreak" id="page_198"/>A Matrix Function by a Scalar</h4>&#13;
<p class="noindent">If <em><strong>f</strong></em>(<em>x</em>) is a function accepting a scalar argument but returning a vector, then we’d be correct in assuming <em><strong>F</strong></em>(<em>x</em>) can be thought of as a function accepting a scalar argument but returning a matrix:</p>&#13;
<p class="center"><em><strong>F</strong></em> : ℝ → ℝ<sup><em>n</em>×<em>m</em></sup></p>&#13;
<p class="noindent">For example, assume <em><strong>F</strong></em> is an <em>n</em> × <em>m</em> matrix of scalar functions:</p>&#13;
<div class="imagec"><img src="Images/198equ01.jpg" alt="Image" width="383" height="100"/></div>&#13;
<p class="noindent">The derivative with respect to the argument, <em>x</em>, is straightforward:</p>&#13;
<div class="imagec"><img src="Images/198equ02.jpg" alt="Image" width="328" height="136"/></div>&#13;
<p class="indent">As we saw above, the derivative of a vector-valued function by a scalar is called the tangent vector. By analogy, then, the derivative of a matrix-valued function by a scalar is the <em>tangent matrix</em>.</p>&#13;
<h4 class="h4" id="ch08lev2_5">A Scalar Function by a Matrix</h4>&#13;
<p class="noindent">Now let’s consider <em>f</em>(<em><strong>X</strong></em>), a function accepting a matrix and returning a scalar:</p>&#13;
<p class="center"><em>f</em> : ℝ<sup><em>n</em>×<em>m</em></sup> → ℝ</p>&#13;
<p class="indent">We’d be correct in thinking that the derivative of <em>f</em> with respect to the matrix, <em><strong>X</strong></em>, is itself a matrix. However, to be true to our numerator layout convention, the resulting matrix is not arranged like <em><strong>X</strong></em>, but instead like <em><strong>X</strong></em><sup>⊤</sup>, the transpose of <em><strong>X</strong></em>.</p>&#13;
<p class="indent">Why use the transpose of <em><strong>X</strong></em> instead of <em><strong>X</strong></em> itself? To answer the question, we need to look back to how we defined ∂<em>f</em>/∂<em><strong>x</strong></em>. There, even though <em><strong>x</strong></em> is a column vector, according to standard convention, we said the derivative is a <em>row</em> vector. We used <em><strong>x</strong></em><sup>⊤</sup> as the ordering. Therefore, to be consistent, we need to arrange ∂<em>f</em>/∂<em><strong>X</strong></em> by the transpose of <em><strong>X</strong></em> and change the columns of <em><strong>X</strong></em> into rows in the derivative. As a result, we have the following definition.</p>&#13;
<span epub:type="pagebreak" id="page_199"/>&#13;
<div class="imagec" id="ch08equ04"><img src="Images/08equ04.jpg" alt="Image" width="518" height="142"/></div>&#13;
<p class="noindent">This is an <em>m</em> × <em>n</em> output matrix for the <em>n</em> × <em>m</em> input matrix, <em><strong>X</strong></em>.</p>&#13;
<p class="indent"><a href="ch08.xhtml#ch08equ04">Equation 8.4</a> defines the <em>gradient matrix</em>, which, for matrices, plays a role similar to that of the gradient, ▽<em>f</em>(<strong>x</strong>). <a href="ch08.xhtml#ch08equ04">Equation 8.4</a> also completes our collection of matrix calculus derivatives. Let’s move on to consider some matrix derivative identities.</p>&#13;
<h3 class="h3" id="ch08lev1_2">The Identities</h3>&#13;
<p class="noindent">Matrix calculus involves scalars, vectors, matrices, and functions thereof, which themselves return scalars, vectors, or matrices, implying many identities and relationships exist. However, here we’ll concentrate on basic identities showing the relationship between matrix calculus and the differential calculus of <a href="ch07.xhtml#ch07">Chapter 7</a>.</p>&#13;
<p class="indent">Each of the following subsections presents identities related to the specific type of derivative indicated. The identities cover fundamental relationships and, when applicable, the chain rule. In all cases, the results follow the numerator layout scheme we’ve used throughout the chapter.</p>&#13;
<h4 class="h4" id="ch08lev2_6">A Scalar Function by a Vector</h4>&#13;
<p class="noindent">We begin with identities related to a scalar function with a vector input. If not otherwise specified, <em>f</em> and <em>g</em> are functions of a vector, <em><strong>x</strong></em>, and return a scalar. A constant vector that doesn’t depend on <em><strong>x</strong></em> is given as <em><strong>a</strong></em>, and <em>a</em> denotes a scalar constant.</p>&#13;
<p class="indent">The basic rules are intuitive:</p>&#13;
<div class="imagec" id="ch08equ05"><img src="Images/08equ05.jpg" alt="Image" width="408" height="44"/></div>&#13;
<p class="noindent">and</p>&#13;
<div class="imagec" id="ch08equ06"><img src="Images/08equ06.jpg" alt="Image" width="440" height="45"/></div>&#13;
<p class="noindent">These show that multiplication by a scalar constant acts as it did in <a href="ch07.xhtml#ch07">Chapter 7</a>, as does the linearity of the partial derivative.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_200"/>The product rule also works as we expect:</p>&#13;
<div class="imagec" id="ch08equ07"><img src="Images/08equ07.jpg" alt="Image" width="435" height="44"/></div>&#13;
<p class="indent">Let’s pause here and remind ourselves of the inputs and outputs for the equations above. We know that the derivative of a scalar function by a vector argument is a row vector in our notation. So, <a href="ch08.xhtml#ch08equ05">Equation 8.5</a> returns a row vector multiplied by a scalar—each element of the derivative is multiplied by <em>a</em>.</p>&#13;
<p class="indent">As differentiation is a linear operator, it distributes over addition, so <a href="ch08.xhtml#ch08equ06">Equation 8.6</a> delivers two terms, each a row vector generated by the respective derivative.</p>&#13;
<p class="indent">For <a href="ch08.xhtml#ch08equ07">Equation 8.7</a>, the product rule, the result again includes two terms. In each case, the derivative returns a row vector, which is multiplied by a scalar function value, either <em>f</em>(<em><strong>x</strong></em>) or <em>g</em>(<em><strong>x</strong></em>). Therefore, the output of <a href="ch08.xhtml#ch08equ07">Equation 8.7</a> is also a row vector.</p>&#13;
<p class="indent">The scalar-by-vector chain rule becomes</p>&#13;
<div class="imagec" id="ch08equ08"><img src="Images/08equ08.jpg" alt="Image" width="415" height="50"/></div>&#13;
<p class="noindent">where <em>f</em>(<em>g</em>) returns a scalar and accepts a scalar argument, while <em>g</em>(<em><strong>x</strong></em>) returns a scalar and accepts a vector argument. The end result is a row vector. Let’s work through a complete example to demonstrate.</p>&#13;
<p class="indent">We have a vector, <em><strong>x</strong></em> = [<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>]<sup>⊤</sup>; a function of that vector written in component form, <em>g</em>(<em><strong>x</strong></em>) = <em>x</em><sub>0</sub> + <em>x</em><sub>1</sub><em>x</em><sub>2</sub>; and a function of <em>g</em>, <em>f</em>(<em>g</em>) = <em>g</em><sup>2</sup>. According to <a href="ch08.xhtml#ch08equ08">Equation 8.8</a>, the derivative of <em>f</em> with respect to <em><strong>x</strong></em> is</p>&#13;
<div class="imagec"><img src="Images/200equ01.jpg" alt="Image" width="463" height="397"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_201"/>To check our result, we can work through from <em>g</em>(<em><strong>x</strong></em>) = <em>x</em><sub>0</sub> + <em>x</em><sub>1</sub><em>x</em><sub>2</sub> and <em>f</em>(<em>g</em>) = <em>g</em><sup>2</sup> to find <em>f</em>(<em><strong>x</strong></em>) directly via substitution. Doing this gives us</p>&#13;
<div class="imagec"><img src="Images/201equ01.jpg" alt="Image" width="234" height="24"/></div>&#13;
<p class="noindent">from which we get</p>&#13;
<div class="imagec"><img src="Images/201equ02.jpg" alt="Image" width="445" height="114"/></div>&#13;
<p class="noindent">which matches the result we found using the chain rule. Of course, in this simple example, it was easier to work through the substitution before taking the derivative, but we proved our case all the same.</p>&#13;
<p class="indent">We’re not entirely through with scalar-by-vector identities, however. The dot product takes two vectors and produces a scalar, so it fits with the functional form we’re working with, even though the arguments to the dot product are vectors.</p>&#13;
<p class="indent">For example, consider this result:</p>&#13;
<div class="imagec" id="ch08equ09"><img src="Images/08equ09.jpg" alt="Image" width="460" height="44"/></div>&#13;
<p class="noindent">Here, we have the derivative of the dot product between <em><strong>x</strong></em> and a vector <em><strong>a</strong></em> that does not depend on <em><strong>x</strong></em>.</p>&#13;
<p class="indent">We can expand on <a href="ch08.xhtml#ch08equ09">Equation 8.9</a>, replacing <em><strong>x</strong></em> with a vector-valued function, <em><strong>f(x</strong></em>):</p>&#13;
<div class="imagec" id="ch08equ10"><img src="Images/08equ10.jpg" alt="Image" width="474" height="44"/></div>&#13;
<p class="indent">What’s the form of this result? Assume <em>f</em> accepts an <em>m</em>-element input and returns an <em>n</em>-element vector output. Likewise, assume <em><strong>a</strong></em> to be an <em>n</em>-element vector. From <a href="ch08.xhtml#ch08equ02">Equation 8.2</a>, we know the derivative ∂<em><strong>f</strong></em>/∂<em><strong>x</strong></em> to be an <em>n</em> × <em>m</em> matrix. Therefore, the final result is a (1 × <em>n</em>) × (<em>n</em> × <em>m</em>) → 1 × <em>m</em> row vector. Good! We know the derivative of a scalar function by a vector should be a row vector when using the numerator layout convention.</p>&#13;
<p class="indent">Finally, the derivative of the dot product of two vector-valued functions, <em><strong>f</strong></em> and <em><strong>g</strong></em>, is</p>&#13;
<div class="imagec" id="ch08equ11"><img src="Images/08equ11.jpg" alt="Image" width="507" height="44"/></div>&#13;
<p class="noindent">If <a href="ch08.xhtml#ch08equ10">Equation 8.10</a> is a row vector, then the sum of two terms like it is also a row vector.</p>&#13;
<h4 class="h4" id="ch08lev2_7"><span epub:type="pagebreak" id="page_202"/>A Vector Function by a Scalar</h4>&#13;
<p class="noindent">Vector-by-scalar differentiation, <a href="ch08.xhtml#ch08tab01">Table 8-1</a>, first row, second column, is less common in machine learning, so we’ll only examine a few identities. The first are multiplications by constants:</p>&#13;
<div class="imagec"><img src="Images/202equ01.jpg" alt="Image" width="132" height="127"/></div>&#13;
<p class="noindent">Note, we can multiply on the left by a matrix, as the derivative is a column vector.</p>&#13;
<p class="indent">The sum rule still applies,</p>&#13;
<div class="imagec"><img src="Images/202equ02.jpg" alt="Image" width="191" height="45"/></div>&#13;
<p class="noindent">as does the chain rule,</p>&#13;
<div class="imagec" id="ch08equ12"><img src="Images/08equ12.jpg" alt="Image" width="424" height="49"/></div>&#13;
<p class="indent"><a href="ch08.xhtml#ch08equ12">Equation 8.12</a> is correct, since the derivative of a vector by a scalar is a column vector, and the derivative of a vector by a vector is a matrix. Therefore, multiplying the matrix on the right by a column vector returns a column vector, as expected.</p>&#13;
<p class="indent">Two other derivatives involving dot products with respect to a scalar are worth knowing about. The first is similar to <a href="ch08.xhtml#ch08equ11">Equation 8.11</a> but with two vector-valued functions of a scalar:</p>&#13;
<div class="imagec"><img src="Images/202equ03.jpg" alt="Image" width="227" height="128"/></div>&#13;
<p class="indent">The second derivative concerns the composition of <em>f</em>(<em><strong>g</strong></em>) and <em><strong>g</strong></em>(<em>x</em>) with respect to <em>x</em>:</p>&#13;
<div class="imagec"><img src="Images/202equ04.jpg" alt="Image" width="252" height="49"/></div>&#13;
<p class="noindent">which is the dot product of a row vector and a column vector.</p>&#13;
<h4 class="h4" id="ch08lev2_8"><span epub:type="pagebreak" id="page_203"/>A Vector Function by a Vector</h4>&#13;
<p class="noindent">The derivatives of vector-valued functions with vector arguments are common in physics and engineering. In machine learning, they show up during backpropagation, for example, at the derivative of the loss function. Let’s begin with some straightforward identities:</p>&#13;
<div class="imagec"><img src="Images/203equ01.jpg" alt="Image" width="133" height="127"/></div>&#13;
<p class="noindent">and</p>&#13;
<div class="imagec"><img src="Images/203equ02.jpg" alt="Image" width="191" height="44"/></div>&#13;
<p class="noindent">where the result is the sum of two matrices.</p>&#13;
<p class="indent">The chain rule is next and works as it did above for scalar-by-vector and vector-by-scalar derivatives:</p>&#13;
<div class="imagec"><img src="Images/203equ03.jpg" alt="Image" width="157" height="49"/></div>&#13;
<p class="noindent">with the result being the product of two matrices.</p>&#13;
<h4 class="h4" id="ch08lev2_9">A Scalar Function by a Matrix</h4>&#13;
<p class="noindent">For functions of a matrix, <em><strong>X</strong></em>, returning a scalar, we have the usual form for the sum rule:</p>&#13;
<div class="imagec"><img src="Images/203equ04.jpg" alt="Image" width="203" height="44"/></div>&#13;
<p class="noindent">with the result being the sum of two matrices. Recall, if <em><strong>X</strong></em> is an <em>n</em> × <em>m</em> matrix, the derivative in numerator layout notation is an <em>m</em> × <em>n</em> matrix.</p>&#13;
<p class="indent">The product rule also works as expected:</p>&#13;
<div class="imagec"><img src="Images/203equ05.jpg" alt="Image" width="193" height="44"/></div>&#13;
<p class="indent">However, the chain rule is different. It depends on <em>f(g</em>), a scalar function accepting a scalar input, and <em>g</em>(<em><strong>X</strong></em>), a scalar function accepting a matrix input. With this restriction, the form of the chain rule looks familiar:</p>&#13;
<div class="imagec" id="ch08equ13"><img src="Images/08equ13.jpg" alt="Image" width="428" height="49"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_204"/>Let’s see <a href="ch08.xhtml#ch08equ13">Equation 8.13</a> in action. First, we need <em><strong>X</strong></em>, a 2 × 2 matrix:</p>&#13;
<div class="imagec"><img src="Images/204equ01.jpg" alt="Image" width="117" height="51"/></div>&#13;
<p class="indent">Next, we need <img src="Images/204equ02.jpg" alt="Image" width="101" height="26"/> and <em>g</em>(<em><strong>X</strong></em>) = <em>x</em><sub>0</sub><em>x</em><sub>3</sub> + <em>x</em><sub>1</sub><em>x</em><sub>2</sub>. Notice, while <em>g</em>(<em><strong>X</strong></em>) accepts a matrix input, the result is a scalar calculated from the values of the matrix.</p>&#13;
<p class="indent">To apply the chain rule, we need two derivatives,</p>&#13;
<div class="imagec"><img src="Images/204equ03.jpg" alt="Image" width="331" height="139"/></div>&#13;
<p class="noindent">where we are again using numerator layout for the result.</p>&#13;
<p class="indent">To find the overall result, we calculate</p>&#13;
<div class="imagec"><img src="Images/204equ04.jpg" alt="Image" width="362" height="317"/></div>&#13;
<p class="indent">To check, we combine the functions to write a single function, <img src="Images/204equ05.jpg" alt="Image" width="202" height="26"/>, and calculate the derivative using the standard chain rule for each element of the resulting matrix. This gives us</p>&#13;
<div class="imagec"><img src="Images/204equ06.jpg" alt="Image" width="554" height="51"/></div>&#13;
<p class="noindent">matching the previous result.</p>&#13;
<p class="indent">We have our definitions and identities. Let’s revisit the derivative of a vector-valued function with a vector argument, as the resulting matrix is special. We’ll encounter it frequently in deep learning.</p>&#13;
<h3 class="h3" id="ch08lev1_3"><span epub:type="pagebreak" id="page_205"/>Jacobians and Hessians</h3>&#13;
<p class="noindent"><a href="ch08.xhtml#ch08equ02">Equation 8.2</a> defined the derivative of a vector-valued function, <em><strong>f</strong></em>, with respect to a vector, <em><strong>x</strong></em>:</p>&#13;
<div class="imagec" id="ch08equ14"><img src="Images/08equ14.jpg" alt="Image" width="516" height="138"/></div>&#13;
<p class="indent">This derivative is known as the <em>Jacobian matrix</em>, <em><strong>J</strong></em>, or simply the <em>Jacobian</em>, and you’ll encounter it from time to time in the deep learning literature, especially during discussions of gradient descent and other optimization algorithms used in training models. The Jacobian sometimes has a subscript to indicate the variable it is with respect to; for example, <em><strong>J</strong></em><em><strong><sub>x</sub></strong></em> if with respect to <em><strong>x</strong></em>. When the context is clear, we’ll often neglect the subscript.</p>&#13;
<p class="indent">In this section, we’ll discuss the Jacobian and what it means. Then we’ll introduce another matrix, the <em>Hessian matrix</em> (or just the <em>Hessian</em>), which is based on the Jacobian, and learn how to use it in optimization problems.</p>&#13;
<p class="indent">The essence of this section is the following: the Jacobian is the generalization of the first derivative, and the Hessian is the generalization of the second derivative.</p>&#13;
<h4 class="h4" id="ch08lev2_10">Concerning Jacobians</h4>&#13;
<p class="noindent">We saw previously that we can think of <a href="ch08.xhtml#ch08equ14">Equation 8.14</a> as a stack of transposed gradient vectors (<a href="ch08.xhtml#ch08equ03">Equation 8.3</a>):</p>&#13;
<div class="imagec"><img src="Images/205equ01.jpg" alt="Image" width="159" height="98"/></div>&#13;
<p class="indent">Viewing the Jacobian as a stack of gradients gives us a clue as to what it represents. Recall, the gradient of a scalar field, a function accepting a vector argument and returning a scalar, points in the direction of the maximum change in the function. Similarly, the Jacobian gives us information about how the vector-valued function behaves in the vicinity of some point, <em><strong>x</strong></em><em><strong><sub>p</sub></strong></em>. The Jacobian is to vector-valued functions of vectors what the gradient is to scalar-valued functions of vectors; it tells us about how the function changes for a small change in the position of <em><strong>x</strong></em><em><strong><sub>p</sub></strong></em>.</p>&#13;
<p class="indent">One way to think of the Jacobian is as a generalization of the more specific cases we encountered in <a href="ch07.xhtml#ch07">Chapter 7</a>. <a href="ch08.xhtml#ch08tab02">Table 8-2</a> shows the relationship between the function and what its derivative measures.</p>&#13;
<p class="tabcap" id="ch08tab02"><span epub:type="pagebreak" id="page_206"/><strong>Table 8-2:</strong> The Relationship Between Jacobians, Gradients, and Slopes</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Function</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Derivative</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em><strong>f</strong></em>(<em><strong>x</strong></em>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em><strong>f</strong></em>/∂<em><strong>x</strong></em>, Jacobian matrix</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>f</em>(<em><strong>x</strong></em>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">∂<em>f</em>/∂<em><strong>x</strong></em>, gradient vector</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>f</em>(<em>x</em>)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><em>df</em>/<em>dx</em>, slope</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The Jacobian matrix is the most general of the three. If we limit the function to a scalar, then the Jacobian matrix becomes the gradient vector (row vector in numerator layout). If we limit the function and argument to scalars, the gradient becomes the slope. In a sense, they all indicate the same thing: how the function is changing around a point in space.</p>&#13;
<p class="indent">The Jacobian has many uses. I’ll present two examples. The first is from systems of differential equations. The second uses Newton’s method to find the roots of a vector-valued function. We’ll see Jacobians again when we discuss backpropagation, as that requires calculating derivatives of a vector-valued function with respect to a vector.</p>&#13;
<h5 class="h5" id="ch08lev3_1">Autonomous Differential Equations</h5>&#13;
<p class="noindent">A <em>differential equation</em> combines derivatives and function values in one equation. Differential equations show up everywhere in physics and engineering. Our example comes from the theory of <em>autonomous systems</em>, which are differential equations where the independent variable does not appear on the right-hand side. For instance, if the system consists of values of the function and first derivatives with respect to time, <em>t</em>, there is no <em>t</em> explicit in the equations governing the system.</p>&#13;
<p class="indent">The previous paragraph is just for background; you don’t need to memorize it. Working with systems of autonomous differential equations ultimately leads to the Jacobian, which is our goal. We can view the system as a vector-valued function, and we’ll use the Jacobian to characterize the critical points of that system (the points where the derivative is zero). We worked with critical points of functions in <a href="ch07.xhtml#ch07">Chapter 7</a>.</p>&#13;
<p class="indent">For example, let’s explore the following system of equations:</p>&#13;
<div class="imagec"><img src="Images/206equ01.jpg" alt="Image" width="163" height="125"/></div>&#13;
<p class="indent">This system includes two functions, <em>x</em>(<em>t</em>) and <em>y</em>(<em>t</em>), and they are coupled so that the rate of change of <em>x</em>(<em>t</em>) depends on the value of <em>x</em> and the value of <em>y</em>, and vice versa.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_207"/>We’ll view the system as a single, vector-valued function:</p>&#13;
<div class="imagec" id="ch08equ15"><img src="Images/08equ15.jpg" alt="Image" width="551" height="51"/></div>&#13;
<p class="noindent">where we replace <em>x</em> with <em>x</em><sub>0</sub> and <em>y</em> with <em>x</em><sub>1</sub>.</p>&#13;
<p class="indent">The system that <em><strong>f</strong></em> represents has critical points at locations where <em><strong>f</strong></em> = <strong>0</strong>, with <strong>0</strong> being the 2 × 1 dimensional zero vector. The critical points are</p>&#13;
<div class="imagec" id="ch08equ16"><img src="Images/08equ16.jpg" alt="Image" width="496" height="51"/></div>&#13;
<p class="noindent">where substitution into <em><strong>f</strong></em> shows that each point returns the zero vector. For the time being, assume we were given the critical points, and now we want to characterize them.</p>&#13;
<p class="indent">To characterize a critical point, we will need the Jacobian matrix that <em><strong>f</strong></em> generates:</p>&#13;
<div class="imagec" id="ch08equ17"><img src="Images/08equ17.jpg" alt="Image" width="544" height="71"/></div>&#13;
<p class="indent">Since the Jacobian describes how a function behaves in the vicinity of a point, we can use it to characterize the critical points. In <a href="ch07.xhtml#ch07">Chapter 7</a>, we used the derivative to tell us whether a point was a minimum or maximum of a function. For the Jacobian, we use the eigenvalues of <em><strong>J</strong></em> in much the same way to talk about the type and stability of critical points.</p>&#13;
<p class="indent">First, let’s find the Jacobian at each of the critical points:</p>&#13;
<div class="imagec"><img src="Images/207equ01.jpg" alt="Image" width="512" height="51"/></div>&#13;
<p class="indent">We can use NumPy to get the eigenvalues of the Jacobians:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import numpy as np</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.linalg.eig([[4,0],[0,2]])[0]</span><br/>&#13;
array([4., 2.])<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.linalg.eig([[2,0],[1,-2]])[0]</span><br/>&#13;
array([-2., 2.])<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.linalg.eig([[0,-4],[2,-4]])[0]</span><br/>&#13;
array([-2.+2.j, -2.-2.j])</pre>&#13;
<p class="noindent">We encountered <code>np.linalg.eig</code> in <a href="ch06.xhtml#ch06">Chapter 6</a>. The eigenvalues are the first values that <code>eig</code> returns, hence the <code>[0]</code> subscript to the function call.</p>&#13;
<p class="indent">For critical points of a system of autonomous differential equations, the eigenvalues indicate the points’ type and stability. If both eigenvalues are real and have the same sign, the critical point is a node. If the eigenvalues <span epub:type="pagebreak" id="page_208"/>are less than zero, the node is stable; otherwise, it is unstable. You can think of a stable node as a pit; if you’re near it, you’ll fall into it. An unstable node is like a hill; if you move away from the top, the critical point, you’ll fall off. The first critical point, <em><strong>c</strong></em><sub>0</sub>, has positive, real eigenvalues; therefore, it represents an unstable node.</p>&#13;
<p class="indent">If the eigenvalues of the Jacobian are real but of opposite signs, the critical point is a saddle point. We discussed saddle points in <a href="ch07.xhtml#ch07">Chapter 7</a>. A saddle point is ultimately unstable, but in two dimensions, there’s a direction where you can “fall into” the saddle and a direction where you can “fall off” the saddle. Some researchers believe most minima found when training deep neural networks are really saddle points of the loss function. We see that critical point <em><strong>c</strong></em><sub>1</sub> is a saddle point, since the eigenvalues are real with opposite signs.</p>&#13;
<p class="indent">Finally, the eigenvalues of <em><strong>c</strong></em><sub>2</sub> are complex. Complex eigenvalues indicate a spiral (also called a focus). If the real part of the eigenvalues is less than zero, the spiral is stable; otherwise, it is unstable. As the eigenvalues are complex conjugates of each other, the signs of the real parts must be the same; one can’t be positive while the other is negative. For <em><strong>c</strong></em><sub>2</sub>, the real parts are negative, so <em><strong>c</strong></em><sub>2</sub> indicates a stable spiral.</p>&#13;
<h5 class="h5" id="ch08lev3_2">Newton’s Method</h5>&#13;
<p class="noindent">I presented the critical points of <a href="ch08.xhtml#ch08equ15">Equation 8.15</a> by fiat. The system is easy enough that we can solve for the critical points algebraically, but that might not generally be the case. One classic method for finding the roots of a function (the places where it returns zero) is known as <em>Newton’s method</em>. This is an iterative method using the first derivative and an initial guess to zero in on the root. Let’s look at the method in one dimension and then extend it to two. We’ll see that moving to two or more dimensions requires the use of the Jacobian.</p>&#13;
<p class="indent">Let’s use Newton’s method to find the square root of 2. To do that, we need an equation such that <img src="Images/208equ01.jpg" alt="Image" width="99" height="23"/>. A moment’s thought gives us one: <em>f</em>(<em>x</em>) = 2 − <em>x</em><sup>2</sup>. Clearly, when <img src="Images/208equ02.jpg" alt="Image" width="144" height="23"/>.</p>&#13;
<p class="indent">The governing equation for Newton’s method in one dimension is</p>&#13;
<div class="imagec" id="ch08equ18"><img src="Images/08equ18.jpg" alt="Image" width="429" height="50"/></div>&#13;
<p class="noindent">where <em>x</em><sub>0</sub> is some initial guess at the solution.</p>&#13;
<p class="indent">We substitute <em>x</em><sub>0</sub> for <em>x<sub>n</sub></em> on the right-hand side of <a href="ch08.xhtml#ch08equ18">Equation 8.18</a> to find <em>x</em><sub>1</sub>. We then repeat using <em>x</em><sub>1</sub> on the right-hand side to get <em>x</em><sub>2</sub>, and so on until we see little change in <em>x<sub>n</sub></em>. At that point, if our initial guess is reasonable, we have the value we’re looking for. Newton’s method converges quickly, so for typical examples, we only need a handful of iterations. Of course, we have powerful computers at our fingertips, so we’ll use them instead of working by hand. The Python code we need is in <a href="ch08.xhtml#ch08ex01">Listing 8-1</a>.</p>&#13;
<pre><span epub:type="pagebreak" id="page_209"/>import numpy as np<br/>&#13;
def f(x):<br/>&#13;
    return 2.0 - x*x<br/>&#13;
def d(x):<br/>&#13;
    return -2.0*x<br/>&#13;
<br/>&#13;
x = 1.0<br/>&#13;
for i in range(5):<br/>&#13;
    x = x - f(x)/d(x)<br/>&#13;
    print("%2d: %0.16f" % (i+1,x))<br/>&#13;
print("NumPy says sqrt(2) = %0.16f for a deviation of %0.16f" %<br/>&#13;
     (np.sqrt(2), np.abs(np.sqrt(2)-x)))</pre>&#13;
<p class="ex-caption" id="ch08ex01"><em>Listing 8-1: Finding</em> <img src="Images/209equ01.jpg" alt="Image" width="22" height="18"/> <em>via Newton’s method</em></p>&#13;
<p class="indent"><a href="ch08.xhtml#ch08ex01">Listing 8-1</a> defines two functions. The first, <code>f(x)</code>, returns the function value for a given <code>x</code>. The second, <code>d(x)</code>, returns the derivative at <code>x</code>. If <em>f</em>(<em>x</em>) = 2 − <em>x</em><sup>2</sup>, then <em>f</em>′(<em>x</em>) = −2<em>x</em>.</p>&#13;
<p class="indent">Our initial guess is <em>x</em> = 1.0. We iterate <a href="ch08.xhtml#ch08equ18">Equation 8.18</a> five times, printing the current estimate of the square root of 2 each time. Finally, we use NumPy to calculate the <em>true</em> value and see how far we are from it.</p>&#13;
<p class="indent">Running <a href="ch08.xhtml#ch08ex01">Listing 8-1</a> produces</p>&#13;
<pre> 1: 1.5000000000000000<br/>&#13;
 2: 1.4166666666666667<br/>&#13;
 3: 1.4142156862745099<br/>&#13;
 4: 1.4142135623746899<br/>&#13;
 5: 1.4142135623730951<br/>&#13;
<br/>&#13;
NumPy says sqrt(2) = 1.4142135623730951 for a<br/>&#13;
deviation of 0.0000000000000000</pre>&#13;
<p class="noindent">which is impressive; we get <img src="Images/209equ02.jpg" alt="Image" width="34" height="21"/> to 16 decimals in only five iterations.</p>&#13;
<p class="indent">To extend Newton’s method to vector-valued functions of vectors, like <a href="ch08.xhtml#ch08equ15">Equation 8.15</a>, we replace the reciprocal of the derivative with the inverse of the Jacobian. Why the inverse? Recall, for a diagonal matrix, the inverse is the reciprocal of the diagonal elements. If we view the scalar derivative as a 1 × 1 matrix, then the reciprocal and inverse are the same. <a href="ch08.xhtml#ch08equ18">Equation 8.18</a> is already using the inverse of the Jacobian, albeit one for a 1 × 1 matrix. Therefore, we’ll iterate</p>&#13;
<div class="imagec" id="ch08equ19"><img src="Images/08equ19.jpg" alt="Image" width="458" height="44"/></div>&#13;
<p class="noindent">for a suitable initial value, <em><strong>x</strong></em><sub>0</sub>, and the inverse of the Jacobian evaluated at <em>x<sub>n</sub></em>. Let’s use Newton’s method to find the critical points of <a href="ch08.xhtml#ch08equ15">Equation 8.15</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_210"/>Before we can write some Python code, we need the inverse of the Jacobian, <a href="ch08.xhtml#ch08equ17">Equation 8.17</a>. The inverse of a 2 × 2 matrix,</p>&#13;
<div class="imagec"><img src="Images/210equ01.jpg" alt="Image" width="102" height="51"/></div>&#13;
<p class="noindent">is</p>&#13;
<div class="imagec"><img src="Images/210equ02.jpg" alt="Image" width="218" height="51"/></div>&#13;
<p class="noindent">assuming the determinant is not zero. The determinant of <em><strong>A</strong></em> is <em>ad</em> − <em>bc</em>. Therefore, the inverse of <a href="ch08.xhtml#ch08equ17">Equation 8.17</a> is</p>&#13;
<div class="imagec"><img src="Images/210equ03.jpg" alt="Image" width="564" height="51"/></div>&#13;
<p class="noindent">Now we can write our code. The result is <a href="ch08.xhtml#ch08ex02">Listing 8-2</a>.</p>&#13;
<pre>   import numpy as np<br/>&#13;
<br/>&#13;
   def f(x):<br/>&#13;
       x0,x1 = x[0,0],x[1,0]<br/>&#13;
       return np.array([[4*x0-2*x0*x1],[2*x1+x0*x1-2*x1**2]])<br/>&#13;
<br/>&#13;
   def JI(x):<br/>&#13;
       x0,x1 = x[0,0],x[1,0]<br/>&#13;
       d = (4-2*x1)*(2-x0-4*x1)+2*x0*x1<br/>&#13;
       return (1/d)*np.array([[2-x0-4*x1,2*x0],[-x1,4-2*x0]])<br/>&#13;
<br/>&#13;
   x0 = float(input("x0: "))<br/>&#13;
   x1 = float(input("x1: "))<br/>&#13;
<span class="ent">❶</span> x = np.array([[x0],[x1]])<br/>&#13;
<br/>&#13;
   N = 20<br/>&#13;
   for i in range(N):<br/>&#13;
    <span class="ent">❷</span> x = x - JI(x) @ f(x)<br/>&#13;
       if (i &gt; (N-10)):<br/>&#13;
           print("%4d: (%0.8f, %0.8f)" % (i, x[0,0],x[1,0]))</pre>&#13;
<p class="ex-caption" id="ch08ex02"><em>Listing 8-2: Newton’s method in 2D</em></p>&#13;
<p class="indent"><a href="ch08.xhtml#ch08ex02">Listing 8-2</a> echoes <a href="ch08.xhtml#ch08ex01">Listing 8-1</a> for the 1D case. We have <code>f(x)</code> to calculate the function value for a given input vector and <code>JI(x)</code> to give us the value of the inverse Jacobian at <em><strong>x</strong></em>. Notice that <code>f(x)</code> returns a column vector and <code>JI(x)</code> returns a 2 × 2 matrix.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_211"/>The code first asks the user for initial guesses, <code>x0</code> and <code>x1</code>. These are formed into the initial vector, <code>x</code>. Note that we explicitly form <code>x</code> as a column vector <span class="ent">❶</span>.</p>&#13;
<p class="indent">The implementation of <a href="ch08.xhtml#ch08equ19">Equation 8.19</a> comes next <span class="ent">❷</span>. The inverse Jacobian is a 2 × 2 matrix that we multiply on the right by the function value, a 2 × 1 column vector, using NumPy’s matrix multiplication operator, <code>@</code>. The result is a 2 × 1 column vector subtracted from the current value of <code>x</code>, itself a 2 × 1 column vector. If the loop is within 10 iterations of completion, the current value is printed at the console.</p>&#13;
<p class="indent">Does <a href="ch08.xhtml#ch08ex02">Listing 8-2</a> work? Let’s run it and see if we can find initial guesses leading to each of the critical points (<a href="ch08.xhtml#ch08equ16">Equation 8.16</a>). For an initial guess of <img src="Images/211equ01.jpg" alt="Image" width="91" height="51"/>, we get</p>&#13;
<pre>11: (0.00004807, -1.07511237)<br/>&#13;
12: (0.00001107, -0.61452262)<br/>&#13;
13: (0.00000188, -0.27403667)<br/>&#13;
14: (0.00000019, -0.07568702)<br/>&#13;
15: (0.00000001, -0.00755378)<br/>&#13;
16: (0.00000000, -0.00008442)<br/>&#13;
17: (0.00000000, -0.00000001)<br/>&#13;
18: (0.00000000, -0.00000000)<br/>&#13;
19: (0.00000000, -0.00000000)</pre>&#13;
<p class="noindent">which is the first critical point of <a href="ch08.xhtml#ch08equ15">Equation 8.15</a>. To find the two remaining critical points, we need to pick our initial guesses with some care. Some guesses explode, while many lead back to the zero vector. However, some trial and error gives us</p>&#13;
<div class="imagec"><img src="Images/211equ02.jpg" alt="Image" width="284" height="50"/></div>&#13;
<p class="noindent">showing that Newton’s method can find the critical points of <a href="ch08.xhtml#ch08equ15">Equation 8.15</a>.</p>&#13;
<p class="indent">We started this section with a system of differential equations that we interpreted as a vector-valued function. We then used the Jacobian to characterize the critical points of that system. Next, we used the Jacobian a second time to locate the system’s critical points via Newton’s method. We could do this because the Jacobian is the generalization of the gradient to vector-valued functions, and the gradient itself is a generalization of the first derivative of a scalar function. As mentioned above, we’ll see Jacobians again when we discuss backpropagation in <a href="ch10.xhtml#ch10">Chapter 10</a>.</p>&#13;
<h4 class="h4" id="ch08lev2_11">Concerning Hessians</h4>&#13;
<p class="noindent">If the Jacobian matrix is like the first derivative of a function of one variable, then the <em>Hessian matrix</em> is like the second derivative. In this case, we’re restricted to scalar fields, functions returning a scalar value for a vector input. <span epub:type="pagebreak" id="page_212"/>Let’s start with the definition and go from there. For the function <em>f</em>(<em><strong>x</strong></em>), the Hessian is defined as</p>&#13;
<div class="imagec" id="ch08equ20"><img src="Images/08equ20.jpg" alt="Image" width="553" height="209"/></div>&#13;
<p class="noindent">where <em><strong>x</strong></em> = [<em>x</em><sub>0</sub> <em>x</em><sub>1</sub> . . . <em>x</em><sub><em>n</em>–1</sub>]<sup>⊤</sup>.</p>&#13;
<p class="indent">Looking at <a href="ch08.xhtml#ch08equ20">Equation 8.20</a> tells us that the Hessian is a square matrix. Moreover, it’s a symmetric matrix implying <em><strong>H</strong></em> = <em><strong>H</strong></em><sup>⊤</sup>.</p>&#13;
<p class="indent">The Hessian is the Jacobian of the gradient of a scalar field:</p>&#13;
<p class="center"><em><strong>H</strong><sub>f</sub></em> = <em><strong>J</strong></em>(▽<em>f</em>)</p>&#13;
<p class="noindent">Let’s see this with an example. Consider this function:</p>&#13;
<div class="imagec"><img src="Images/212equ01.jpg" alt="Image" width="268" height="24"/></div>&#13;
<p class="noindent">If we use the definition of the Hessian in <a href="ch08.xhtml#ch08equ20">Equation 8.20</a> directly, we see that <img src="Images/212equ02.jpg" alt="Image" width="109" height="24"/> because ∂<em>f</em>/∂<em>x</em><sub>0</sub> = 4<em>x</em><sub>0</sub> + <em>x</em><sub>2</sub>. Similar calculations give us the rest of the Hessian matrix:</p>&#13;
<div class="imagec"><img src="Images/212equ03.jpg" alt="Image" width="104" height="69"/></div>&#13;
<p class="noindent">In this case, the Hessian is constant, not a function of <em><strong>x</strong></em>, because the highest power of a variable in <em>f</em>(<em><strong>x</strong></em>) is 2.</p>&#13;
<p class="indent">The gradient of <em>f</em>(<em><strong>x</strong></em>), using our column vector definition, is</p>&#13;
<div class="imagec"><img src="Images/212equ04.jpg" alt="Image" width="239" height="114"/></div>&#13;
<p class="noindent">with the Jacobian of the gradient giving the following, which is identical to the matrix we found by direct use of <a href="ch08.xhtml#ch08equ20">Equation 8.20</a>.</p>&#13;
<div class="imagec"><img src="Images/212equ05.jpg" alt="Image" width="374" height="114"/></div>&#13;
<h5 class="h5" id="ch08lev3_3"><span epub:type="pagebreak" id="page_213"/>Minima and Maxima</h5>&#13;
<p class="noindent">We saw in <a href="ch07.xhtml#ch07">Chapter 7</a> that we could use the second derivative to test whether critical points of a function were minima (<em>f</em>′′ &gt; 0) or maxima (<em>f</em>′′ &lt; 0). We’ll see in the next section how we can use critical points in optimization problems. For now, let’s use the Hessian to find critical points by considering its eigenvalues. We’ll continue with the example above. The Hessian matrix is 3 × 3, meaning there are three (or fewer) eigenvalues. Again, we’ll save time and use NumPy to tell us what they are:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">np.linalg.eig([[4,0,1],[0,-2,3],[1,3,0]])[0]</span><br/>&#13;
array([ 4.34211128, 1.86236874, -4.20448002])</pre>&#13;
<p class="indent">Two of the three eigenvalues are positive, and one is negative. If all three were positive, the critical point would be a minimum. Likewise, if all three were negative, the critical point would be a maximum. Notice that the minimum/maximum label is the opposite of the sign, just like the single-variable case. However, if at least one eigenvalue is positive and another negative, which is the case with our example, the critical point is a saddle point.</p>&#13;
<p class="indent">It seems natural to ask whether the Hessian of a vector-valued function, <em><strong>f</strong></em>(<em><strong>x</strong></em>), exists. After all, we can calculate the Jacobian of such a function; we did so above to show that the Hessian is the Jacobian of the gradient.</p>&#13;
<p class="indent">It is possible to extend the Hessian to a vector-valued function. However, the result is no longer a matrix, but an order-3 tensor. To see this is so, consider the definition of a vector-valued function:</p>&#13;
<div class="imagec"><img src="Images/213equ01.jpg" alt="Image" width="150" height="97"/></div>&#13;
<p class="indent">We can think of a vector-valued function, a vector field, as a vector of scalar functions of a vector. We could calculate the Hessian of each of the <em>m</em> functions in <em><strong>f</strong></em> to get a vector of matrices,</p>&#13;
<div class="imagec"><img src="Images/213equ02.jpg" alt="Image" width="117" height="102"/></div>&#13;
<p class="noindent">but a vector of matrices is a 3D object. Think of an RGB image: a 3D array made up of three 2D images, one each for the red, green, and blue channels. Therefore, while possible to define and calculate, the Hessian of a vector-valued function is beyond our current scope.</p>&#13;
<h5 class="h5" id="ch08lev3_4">Optimization</h5>&#13;
<p class="noindent">In deep learning, you’re most likely to see the Hessian in reference to optimization. Training a neural network is, to a first approximation, an <span epub:type="pagebreak" id="page_214"/>optimization problem—the goal is to find the weights and biases leading to a minimum in the loss function landscape.</p>&#13;
<p class="indent">In <a href="ch07.xhtml#ch07">Chapter 7</a>, we saw that the gradient provides information on how to move toward a minimum. An optimization algorithm, like gradient descent, the subject of <a href="ch11.xhtml#ch11">Chapter 11</a>, uses the gradient as a guide. As the gradient is a first derivative of the loss function, algorithms based solely on the gradient are known as <em>first-order optimization methods</em>.</p>&#13;
<p class="indent">The Hessian provides information beyond the gradient. As a second derivative, the Hessian contains information about how the loss landscape’s gradient is changing, that is, its curvature. An analogy from physics might help here. A particle’s motion in one dimension is described by some function of time, <em>x</em>(<em>t</em>). The first derivative, the velocity, is <em>dx</em>/<em>dt</em> = <em>v</em>(<em>t</em>). The velocity tells us how quickly the position is changing in time. However, the velocity might change with time, so its derivative, <em>dv</em>/<em>dt</em> = <em>a</em>(<em>t</em>), is the acceleration. And, if the velocity is the first derivative of the position, then the acceleration is the second, <em>d</em><sup>2</sup><em>x</em>/<em>dt</em><sup>2</sup> = <em>a</em>(<em>t</em>). Similarly, the second derivative of the loss function, the Hessian, provides information on how the gradient is changing. Optimization algorithms using the Hessian, or an approximation of it, are known as <em>second-order optimization methods</em>.</p>&#13;
<p class="indent">Let’s start with an example in one dimension. We have a function, <em>f</em>(<em>x</em>), and we’re currently at some <em>x</em><sub>0</sub>. We want to move from this position to a new position, <em>x</em><sub>1</sub>, closer to a minimum of <em>f</em>(<em>x</em>). A first-order algorithm will use the gradient, here the derivative, as a guide, since we know moving in the direction opposite to the derivative will move us toward a lower function value. Therefore, for some step size, call it η (eta), we can write</p>&#13;
<p class="center"><em>x</em><sub>1</sub> = <em>x</em><sub>0</sub> − η <em>f</em>′(<em>x</em><sub>0</sub>)</p>&#13;
<p class="noindent">This will move us from <em>x</em><sub>0</sub> toward <em>x</em><sub>1</sub>, which is closer to the minimum of <em>f</em>(<em>x</em>), assuming the minimum exists.</p>&#13;
<p class="indent">The equation above makes sense, so why think about a second-order method? The second-order method comes into play when we move from <em>f</em>(<em>x</em>) to <em>f</em>(<em>x</em>). Now we have a gradient, not just a derivative, and the landscape of <em>f</em>(<em><strong>x</strong></em>) around some point can be more complex. The general form of gradient descent is</p>&#13;
<p class="center"><em><strong>x</strong></em><sub>1</sub> = <em><strong>x</strong></em><sub>0</sub> − η▽<em>f</em>(<em><strong>x</strong></em><sub>0</sub>)</p>&#13;
<p class="noindent">but the information in the Hessian can be of assistance. To see how, we first need to introduce the idea of a <em>Taylor series expansion</em>, a way of approximating an arbitrary function as the sum of a series of terms. We use Taylor series frequently in physics and engineering to simplify complex functions in the vicinity of a specific point. We also often use them to calculate values of <em>transcendental functions</em> (functions that can’t be written as a finite set of basic algebra operations). For example, it’s likely that when you use <code>cos(x)</code> in a programming language, the result is generated by a Taylor series expansion <span epub:type="pagebreak" id="page_215"/>with a sufficient number of terms to get the cosine to 32- or 64-bit floating-point precision:</p>&#13;
<div class="imagec"><img src="Images/215equ01.jpg" alt="Image" width="440" height="55"/></div>&#13;
<p class="indent">In general, to approximate a function, <em>f</em>(<em>x</em>), in the vicinity of a point, <em>x</em> = <em>a</em>, the Taylor series expansion is</p>&#13;
<div class="imagec"><img src="Images/215equ02.jpg" alt="Image" width="223" height="56"/></div>&#13;
<p class="noindent">where <em>f</em><sup>(<em>k</em>)</sup>(<em>a</em>) is the <em>k</em>th derivative of <em>f</em>(<em>x</em>) evaluated at point <em>a</em>.</p>&#13;
<p class="indent">A linear approximation of <em>f</em>(<em>x</em>) around <em>x</em> = <em>a</em> is</p>&#13;
<p class="center"><em>f</em>(<em>x</em>) ≈ <em>f</em>(<em>a</em>) + <em>f</em>′(<em>a</em>)(<em>x</em> - <em>a</em>)</p>&#13;
<p class="noindent">while a quadratic approximation of <em>f</em>(<em>x</em>) becomes</p>&#13;
<div class="imagec" id="ch08equ21"><img src="Images/08equ21.jpg" alt="Image" width="542" height="43"/></div>&#13;
<p class="noindent">where we see the linear approximation using the first derivative and the quadratic using the first and second derivatives of <em>f</em>(<em>x</em>). A first-order optimization algorithm uses the linear approximation, while a second-order one uses the quadratic approximation.</p>&#13;
<p class="indent">Moving from a scalar function of a scalar, <em>f</em>(<em>x</em>), to a scalar function of a vector, <em>f</em>(<em><strong>x</strong></em>), changes the first derivative to a gradient and the second derivative to the Hessian matrix,</p>&#13;
<div class="imagec"><img src="Images/215equ03.jpg" alt="Image" width="470" height="43"/></div>&#13;
<p class="noindent">with <em><strong>H</strong></em><sub><em>f</em></sub>(<em><strong>a</strong></em>) the Hessian matrix for <em>f</em>(<em><strong>x</strong></em>) evaluated at the point <em><strong>a</strong></em>. The products’ order changes to make the dimensions work out properly, as we now have vectors and a matrix to deal with.</p>&#13;
<p class="indent">For example, if <em><strong>x</strong></em> has <em>n</em> elements, then <em>f</em>(<em><strong>a</strong></em>) is a scalar; the gradient at <em><strong>a</strong></em> is an <em>n</em>-element column vector multiplying (<em><strong>x</strong></em> − <em><strong>a</strong></em>)<sup>⊤</sup>, an <em>n</em>-element row vector, producing a scalar; and the last term is 1 × <em>n</em> times <em>n</em> × <em>n</em> times <em>n</em> × 1, leading to 1 × <em>n</em> times <em>n</em> × 1, which is also a scalar.</p>&#13;
<p class="indent">To use the Taylor series expansions for optimization, to find the minimum of <em>f</em>, we can use Newton’s method in much the same way used in <a href="ch08.xhtml#ch08equ18">Equation 8.18</a>. First, we rewrite <a href="ch08.xhtml#ch08equ21">Equation 8.21</a> to change our viewpoint to one of a displacement (Δ<em>x</em>) from a current position (<em>x</em>). <a href="ch08.xhtml#ch08equ21">Equation 8.21</a> then becomes</p>&#13;
<div class="imagec" id="ch08equ22"><img src="Images/08equ22.jpg" alt="Image" width="539" height="43"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_216"/><a href="ch08.xhtml#ch08equ22">Equation 8.22</a> is a parabola in Δ<em>x</em>, and we’re using it as a stand-in for the more complex shape of <em>f</em> in the region of <em>x</em> + Δ<em>x</em>. To find the minimum of <a href="ch08.xhtml#ch08equ22">Equation 8.22</a>, we take the derivative and set it to zero, then solve for Δ<em>x</em>. The derivative gives</p>&#13;
<div class="imagec"><img src="Images/216equ01.jpg" alt="Image" width="306" height="48"/></div>&#13;
<p class="noindent">which, if set to zero, leads to</p>&#13;
<div class="imagec" id="ch08equ23"><img src="Images/08equ23.jpg" alt="Image" width="407" height="50"/></div>&#13;
<p class="indent"><a href="ch08.xhtml#ch08equ23">Equation 8.23</a> tells us the offset from a current position, <em>x</em>, that would lead to the minimum of <em>f</em>(<em>x</em>) if <em>f</em>(<em>x</em>) were actually a parabola. In reality, <em>f</em>(<em>x</em>) isn’t a parabola, so the Δ<em>x</em> of <a href="ch08.xhtml#ch08equ23">Equation 8.23</a> isn’t the actual offset to the minimum of <em>f</em>(<em>x</em>). However, since the Taylor series expansion used the actual slope, <em>f</em>′(<em>x</em>), and curvature, <em>f</em>′′(<em>x</em>), of <em>f</em>(<em>x</em>) at <em>x</em>, the offset of <a href="ch08.xhtml#ch08equ23">Equation 8.23</a> is a better estimate of the actual minimum of <em>f</em>(<em>x</em>) than the linear approximation, assuming there is a minimum.</p>&#13;
<p class="indent">If we go from <em>x</em> to <em>x</em> + Δ<em>x</em>, there’s no reason why we can’t then use <a href="ch08.xhtml#ch08equ23">Equation 8.23</a> a second time, calling the new position <em>x</em>. Thinking like this leads to an equation we can iterate:</p>&#13;
<div class="imagec" id="ch08equ24"><img src="Images/08equ24.jpg" alt="Image" width="432" height="50"/></div>&#13;
<p class="noindent">for <em>x</em><sub>0</sub>, some initial starting point.</p>&#13;
<p class="indent">We can work out all of the above for scalar functions with vector arguments, <em>f</em>(<em><strong>x</strong></em>), which are the kind we most often encounter in deep learning via the loss function. <a href="ch08.xhtml#ch08equ24">Equation 8.24</a> becomes</p>&#13;
<div class="imagec"><img src="Images/216equ02.jpg" alt="Image" width="226" height="44"/></div>&#13;
<p class="noindent">where the reciprocal of the second derivative becomes the inverse of the Hessian matrix evaluated at <em><strong>x</strong><sub>n</sub></em>.</p>&#13;
<p class="indent">Excellent! We have an algorithm we can use to rapidly find the minimum of a function like <em>f</em>(<em><strong>x</strong></em>). We saw above that Newton’s method converges quickly, so using it to minimize a loss function also should converge quickly, faster than gradient descent, which only considers the first derivative.</p>&#13;
<p class="indent">If this is the case, why do we use gradient descent to train neural networks instead of Newton’s method?</p>&#13;
<p class="indent">There are several reasons. First, we haven’t discussed issues arising from the Hessian’s applicability, issues related to the Hessian being a positive definite matrix. A symmetric matrix is positive definite if all its eigenvalues are positive. Near saddle points, the Hessian might not be positive definite, which can cause the update rule to move away from the minimum. As you might expect with a simple algorithm like Newton’s method, some variations try to address issues like this, but even if problems with the eigenvalues <span epub:type="pagebreak" id="page_217"/>of the Hessian are addressed, the computational burden of using the Hessian for updating network parameters is what stops Newton’s algorithm in its tracks.</p>&#13;
<p class="indent">Every time the network’s weights and biases are updated, the Hessian changes, requiring it and its inverse to be calculated again. Think of the number of minibatches used during network training. For even one minibatch, there are <em>k</em> parameters in the network, where <em>k</em> is easily in the millions to even billions. The Hessian is a <em>k</em> × <em>k</em> symmetric and positive definite matrix. Inverting the Hessian typically uses Cholesky decomposition, which is more efficient than other methods but is still an 𝒪(<em>k</em><sup>3</sup>) algorithm. The <em>big-O</em> notation indicates that the algorithm’s resource use scales as the cube of the size of the matrix in time, memory, or both. This means doubling the number of parameters in the network increases the computational time to invert the Hessian by a factor of 2<sup>3</sup> = 8 while tripling the number of parameters requires some 3<sup>3</sup> = 27 times as much effort, and quadrupling some 4<sup>3</sup> = 64 times as much. And this is to say nothing about storing the <em>k</em><sup>2</sup> elements of the Hessian matrix, all floating-point values.</p>&#13;
<p class="indent">The computation necessary to use Newton’s method with even modest deep networks is staggering. Gradient-based, first-order optimization methods are about all we can use for training neural networks.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>This statement is perhaps a bit premature. Recent work in the area of</em> neuroevolution <em>has demonstrated that evolutionary algorithms can successfully train deep models. My experimentation with swarm optimization techniques and neural networks lends credence to this approach as well.</em></p>&#13;
</div>&#13;
<p class="indent">That first-order methods work as well as they do seems, for now, to be a very happy accident.</p>&#13;
<h3 class="h3" id="ch08lev1_4">Some Examples of Matrix Calculus Derivatives</h3>&#13;
<p class="noindent">We conclude the chapter with some examples similar to the kinds of derivatives we commonly find in deep learning.</p>&#13;
<h4 class="h4" id="ch08lev2_12">Derivative of Element-Wise Operations</h4>&#13;
<p class="noindent">Let’s begin with the derivative of element-wise operations, which includes things like adding two vectors together. Consider</p>&#13;
<div class="imagec"><img src="Images/217equ01.jpg" alt="Image" width="329" height="96"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_218"/>which is the straightforward addition of two vectors, element by element. What does ∂<em><strong>f</strong></em>/∂<em><strong>a</strong></em>, the Jacobian of <em><strong>f</strong></em>, look like? From the definition, we have</p>&#13;
<div class="imagec"><img src="Images/218equ01.jpg" alt="Image" width="279" height="138"/></div>&#13;
<p class="noindent">but <em>f</em><sub>0</sub> only depends on <em>a</em><sub>0</sub>, while <em>f</em><sub>1</sub> depends on <em>a</em><sub>1</sub>, and so on. Therefore, all derivatives ∂<em>f<sub>i</sub></em>/∂<em>a<sub>j</sub></em> for <em>i</em> ≠ <em>j</em> are zero. This removes all the off-diagonal elements of the matrix, leaving</p>&#13;
<div class="imagec"><img src="Images/218equ02.jpg" alt="Image" width="466" height="138"/></div>&#13;
<p class="noindent">since ∂<em>f</em><sub><em>i</em></sub>/∂<em>a</em><sub><em>i</em></sub> = 1 for all <em>i</em>. Similarly, ∂<em><strong>f</strong></em>/∂<em><strong>b</strong></em> = <em><strong>I</strong></em> as well. Also, if we change from addition to subtraction, ∂<em><strong>f</strong></em>/∂<em><strong>a</strong></em> = <em><strong>I</strong></em>, but ∂<em><strong>f</strong></em>/∂<em><strong>b</strong></em> = −<em><strong>I</strong></em>.</p>&#13;
<p class="indent">If the operator is element-wise multiplication of <em><strong>a</strong></em> and <em><strong>b</strong></em>, <em><strong>f</strong></em> = <em><strong>a</strong></em> ⊗ <em><strong>b</strong></em>, then we get the following, where the diag(<em><strong>x</strong></em>) notation means the <em>n</em> elements of vector <em><strong>x</strong></em> along the diagonal of an <em>n</em> × <em>n</em> matrix that is zero elsewhere.</p>&#13;
<div class="imagec"><img src="Images/218equ03.jpg" alt="Image" width="660" height="316"/></div>&#13;
<h4 class="h4" id="ch08lev2_13">Derivative of the Activation Function</h4>&#13;
<p class="noindent">Let’s find the derivative of the weights and bias value for a single node of a hidden layer in a feedforward network. Recall, the inputs to the node are the outputs of the previous layer, <em><strong>x</strong></em>, multiplied term by term by the weights, <em><strong>w</strong></em>, and summed along with the bias value, <em>b</em>, a scalar. The result, a scalar, is <span epub:type="pagebreak" id="page_219"/>passed to the activation function to produce the output value for the node. Here, we’re using the <em>rectified linear unit (ReLU)</em> which returns its argument if the argument is positive. If the argument is negative, ReLU returns zero. We can write this process as</p>&#13;
<div class="imagec" id="ch08equ25"><img src="Images/08equ25.jpg" alt="Image" width="437" height="22"/></div>&#13;
<p class="indent">In order to implement backpropagation, we need the derivatives of <a href="ch08.xhtml#ch08equ25">Equation 8.25</a> with respect to <em><strong>w</strong></em> and <em>b</em>. Let’s see how to find them.</p>&#13;
<p class="indent">We begin by considering the pieces of <a href="ch08.xhtml#ch08equ25">Equation 8.25</a>. For example, from <a href="ch08.xhtml#ch08equ09">Equation 8.9</a>, we know the derivative of the dot product with respect to <em><strong>w</strong></em> is</p>&#13;
<div class="imagec"><img src="Images/219equ01.jpg" alt="Image" width="118" height="45"/></div>&#13;
<p class="noindent">where we have taken advantage of the fact that the dot product is commutative, <em><strong>w</strong></em> • <em><strong>x</strong></em> = <em><strong>x</strong></em> • <em><strong>w</strong></em>. Also, since <em>b</em> does not depend on <em><strong>w</strong></em>, we have</p>&#13;
<div class="imagec" id="ch08equ26"><img src="Images/08equ26.jpg" alt="Image" width="543" height="45"/></div>&#13;
<p class="indent">What about the derivative of ReLU? By definition,</p>&#13;
<div class="imagec"><img src="Images/219equ02.jpg" alt="Image" width="319" height="63"/></div>&#13;
<p class="noindent">implying that</p>&#13;
<div class="imagec" id="ch08equ27"><img src="Images/08equ27.jpg" alt="Image" width="473" height="63"/></div>&#13;
<p class="noindent">since ∂<em>z</em>/∂<em>z</em> = 1.</p>&#13;
<p class="indent">To find the derivatives of <a href="ch08.xhtml#ch08equ25">Equation 8.25</a> with respect to <em><strong>w</strong></em> and <em>b</em>, we need the chain rule and the results above. Let’s start with <em><strong>w</strong></em>. The chain rule tells us how</p>&#13;
<div class="imagec"><img src="Images/219equ03.jpg" alt="Image" width="113" height="44"/></div>&#13;
<p class="noindent">with <em>z</em> = <em><strong>w</strong></em> • <em><strong>x</strong></em> + <em>b</em> and <em>y</em> = ReLU(<em>z</em>).</p>&#13;
<p class="indent">We know ∂<em>y</em>/∂<em>z</em>; it’s the two cases above for the ReLU, <a href="ch08.xhtml#ch08equ27">Equation 8.27</a>. So now we have</p>&#13;
<div class="imagec"><img src="Images/219equ04.jpg" alt="Image" width="186" height="63"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_220"/>and we know ∂<em>z</em>/∂<em><strong>w</strong></em> = <em><strong>x</strong></em><sup>⊤</sup>; it’s <a href="ch08.xhtml#ch08equ26">Equation 8.26</a>. Therefore, our final result is</p>&#13;
<div class="imagec"><img src="Images/220equ01.jpg" alt="Image" width="238" height="63"/></div>&#13;
<p class="noindent">where we’ve replaced <em>z</em> with <em><strong>w</strong></em> • <em><strong>x</strong></em> + <em>b</em>.</p>&#13;
<p class="indent">We follow much the same procedure to find ∂<em>y</em>/∂<em>b</em>, as</p>&#13;
<div class="imagec"><img src="Images/220equ02.jpg" alt="Image" width="102" height="44"/></div>&#13;
<p class="noindent">but ∂<em>y</em>/∂<em>z</em> is 0 or 1, depending on the <em>z</em>’s sign. Likewise, ∂<em>z</em>/∂<em>b</em> = 1, which leads to</p>&#13;
<div class="imagec"><img src="Images/220equ03.jpg" alt="Image" width="222" height="63"/></div>&#13;
<h3 class="h3" id="ch08lev1_5">Summary</h3>&#13;
<p class="noindent">In this dense chapter, we learned about matrix calculus, including working with derivatives of functions involving vectors and matrices. We worked through the definitions and discussed some identities. We then introduced the Jacobian and Hessian matrices as analogs for first and second derivatives and learned how to use them in optimization problems. Training a deep neural network is, fundamentally, an optimization problem, so the potential utility of the Jacobian and Hessian is clear, even if the latter can’t be easily used for large neural networks. We ended the chapter with some examples for derivatives of expressions found in deep learning.</p>&#13;
<p class="indent">This concludes the general mathematics portion of the book. We’ll now turn our attention to using what we’ve learned to understand the workings of deep neural networks. Let’s begin with a discussion of how data flows through a neural network model.</p>&#13;
</div></body></html>