- en: '**1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GETTING STARTED**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chapter introduces our operating environment and details how to set it
    up. It also includes a primer on some of the math we will encounter. We’ll end
    with a brief note about graphics processors, or GPUs, which you may have heard
    are essential for deep learning. For our purposes, they’re not, so don’t worry—this
    book won’t suddenly cost you a lot of money.
  prefs: []
  type: TYPE_NORMAL
- en: The Operating Environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll detail the environment we’ll assume throughout the remainder
    of the book. Our underlying assumption is that we’re using a 64-bit Linux system.
    The exact distribution is not critical, but to make things simpler in the presentation,
    we’ll also assume that we’re using Ubuntu 20.04\. Given the excellent support
    behind the Ubuntu distribution, we trust that any newer distributions will also
    work similarly. The Python language is our *lingua franca*, the common language
    of machine learning. Specifically, we’ll use Python 3.8.2; that’s the version
    used by Ubuntu 20.04.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a quick overview of the Python toolkits that we’ll be using.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*NumPy* is a Python library that adds array processing abilities to Python.
    While Python lists can be used like one-dimensional arrays, in practice they are
    too slow and inflexible. The NumPy library adds the array features missing from
    Python—features that are necessary for many scientific applications. NumPy is
    a base library required by all the other libraries we’ll use.'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the traditional machine learning models we’ll explore in this book are
    found in the superb `scikit-learn` library, or *sklearn*, as it’s usually called
    when loaded into Python. Note also that we’re writing *scikit-learn* without caps
    as this is how the authors consistently refer to it in their documentation. This
    library uses NumPy arrays. It implements a standardized interface to many different
    machine learning models as well as an entire host of other functionality that
    we won’t even have time to touch. I strongly encourage you to review the official
    sklearn documentation ([https://scikit-learn.org/stable/documentation.html](https://scikit-learn.org/stable/documentation.html))
    as you become more and more familiar with machine learning and the tools behind
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Keras with TensorFlow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning is hard enough to understand, let alone implement efficiently
    and correctly, so instead of attempting to write convolutional neural networks
    from scratch, we’ll use one of the popular toolkits already in active development.
    From its inception, the deep learning community has supported the development
    of toolkits to make deep networks easier to use and has made the toolkits open
    source with very generous licenses. At the time of this writing, there are many
    popular toolkits we could have used in Python. Among many others, these include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache MXnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these toolkits are waxing, and others appear to be waning. But the one
    that has probably the most active following at present is Keras with the TensorFlow
    backend, so that’s the one we’ll use here.
  prefs: []
  type: TYPE_NORMAL
- en: '*Keras* (*[https://keras.io/](https://keras.io/)*) is a Python deep learning
    toolkit that uses the TensorFlow toolkit (*[https://www.tensorflow.org/](https://www.tensorflow.org/)*)
    under the hood. *TensorFlow* is an open source Google product that implements
    the core functionality of deep neural networks for many different platforms. We
    selected Keras not only because it’s popular and in active development, but also
    because it’s straightforward to use. Our goal is to become familiar with deep
    learning to the point where we can implement models and use them with a minimum
    of programming overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Toolkits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can’t reasonably give an exhaustive guide for installing the toolkits on
    all systems and hardware. Instead, we’ll provide step-by-step instructions for
    the specific operating system we’ll use as the reference system. These steps,
    along with the minimum version numbers of the libraries, should be enough for
    most readers to get a working system in place.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, we’re assuming that we’re working in a Linux environment, specifically
    Ubuntu 20.04\. Ubuntu is a widely used Linux distribution, and it runs on almost
    any modern computer system. Other Linux distributions will work, as will macOS,
    but the instructions here are specific to Ubuntu. For the most part, the machine
    learning community has left the Windows operating system. Still, individuals have
    ported toolkits to Windows; therefore, an adventurous reader might give Windows
    a try.
  prefs: []
  type: TYPE_NORMAL
- en: 'A freshly installed Ubuntu 20.04 base desktop system gives us Python 3.8.2
    for free. To install the remaining packages, we need to go into a shell and execute
    the sequence of steps below in the order given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the installation is complete, we’ll have installed the following versions
    of the libraries and toolkits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `pillow` library is an image processing library, `h5py` is a library for
    working with HDF5 format data files, and `matplotlib` is for plotting. *HDF5*
    is a generic, hierarchical file format for storing scientific data. Keras uses
    it to store model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The following two sections are light introductions to some of the math that
    will creep into the book.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Linear Algebra
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re about to look at vectors and matrices. The math that deals with these
    concepts falls under the general heading of *linear algebra*, or matrix theory.
    As you might imagine, linear algebra is a complex field. All we need to know for
    this book is what a vector is, what a matrix is, and how we can multiply two vectors,
    or two matrices, or vectors and matrices together. We’ll see later on that this
    gives us a powerful way to implement specific models, particularly neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by looking at vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *vector* is a one-dimensional list of numbers. Mathematically, a vector might
    appear as
  prefs: []
  type: TYPE_NORMAL
- en: '*a* = [0, 1, 2, 3, 4]'
  prefs: []
  type: TYPE_NORMAL
- en: with the third element given as *a*[2] = 2\. Notice we’re following the programming
    convention of indexing from zero, so *a*[2] gives us the third element in the
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector above was written horizontally and therefore is known as a *row
    vector*. When used in mathematical expressions, however, vectors are usually assumed
    to be written vertically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/004equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When written vertically, a vector is known as a *column vector*. This vector
    has five elements and is denoted as a five-element column vector. In this book,
    we’ll typically use vectors to represent one *sample*: one set of features that
    we’ll input to a model.'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, vectors are used to represent points in space. If we’re talking
    about the two-dimensional (2D) Cartesian plane, we locate a point with a vector
    of two numbers, (*x*,*y*), where *x* is the distance along the x-axis and *y*
    is the distance along the y-axis. That vector represents a point in two dimensions,
    even though the vector itself has only one dimension. If we have three dimensions,
    we need a vector with three elements, (*x*,*y*,*z*).
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, since we often use vectors to represent the inputs to our
    models, we’ll be working with dozens to hundreds of dimensions. Of course, we
    can’t plot them as points in a space, but mathematically, that’s what they are.
    As we’ll see, some models, such as the *k*-Nearest Neighbors model, use the feature
    vectors as just that—points in a high-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *matrix* is a two-dimensional array of numbers where we index a particular
    entry by its row number and column number. For example, this is a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/005equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we want to refer to the 6, we write *a*[1,2] = 6\. Again, we’re indexing
    from zero. Because this matrix *a* has three rows and three columns, we call it
    a 3 × 3 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying Vectors and Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplest way to think of multiplying two vectors together is to multiply
    their corresponding elements. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1, 2, 3] × [4, 5, 6] = [4, 10, 18]'
  prefs: []
  type: TYPE_NORMAL
- en: This is the most common way to multiply an array when using a toolkit like NumPy,
    and we’ll make heavy use of this in the chapters that follow. However, in mathematics,
    this is seldom actually done.
  prefs: []
  type: TYPE_NORMAL
- en: When multiplying vectors together mathematically, we need to know if they are
    row or column vectors. We’ll work with two vectors, *A* = (*a*,*b*,*c*), and *B*
    = (*d*,*e*,*f* ), which, following mathematical convention, are assumed to be
    column vectors. Adding a superscript *T* turns a column vector into a row vector.
    The mathematically allowed ways to multiply *A* and *B* are
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/005equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is called the *outer product* and
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/005equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which is called the *inner product*, or *dot product*. Notice that the outer
    product becomes a matrix, and the inner product becomes a single number, a *scalar*.
  prefs: []
  type: TYPE_NORMAL
- en: When multiplying a matrix and a vector, the vector is typically on the right
    side of the matrix. The multiplication can proceed if the number of columns in
    the matrix matches the number of elements in the vector, again assumed to be a
    column vector. The result is also a vector with as many elements as there are
    rows in the matrix (read *ax* + *by* + *cz* as a single element).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/006equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we’ve multiplied a 2 × 3 matrix by a 3 × 1 column vector to get a 2 ×
    1 output vector. Notice that the number of columns of the matrix and the number
    of rows of the vector match. If they do not, then the multiplication is not defined.
    Also, notice that the values in the output vector are sums of products of the
    matrix and vector. This same rule applies when multiplying two matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/006equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here multiplying a 2 × 3 matrix by a 3 × 2 matrix has given us a 2 × 2 answer.
  prefs: []
  type: TYPE_NORMAL
- en: When we get to convolutional neural networks, we’ll work with arrays that have
    three and even four dimensions. Generically, these are referred to as *tensors*.
    If we imagine a stack of matrices, all the same size, we get a three-dimensional
    tensor, and we can use the first index to refer to any one of the matrices and
    the remaining two indices to refer to a particular element of that matrix. Similarly,
    if we have a stack of three-dimensional tensors, we have a four-dimensional tensor,
    and we can use the first index of that to refer to any one of the three-dimensional
    tensors.
  prefs: []
  type: TYPE_NORMAL
- en: The main points of this section are that vectors have one dimension, matrices
    have two dimensions, there are rules for multiplying these objects together, and
    our toolkits will work with four-dimensional tensors in the end. We’ll review
    some of these points as we encounter them later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and Probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The topics of statistics and probability are so broad that often it’s better
    to either say almost nothing or to write a book or two. Therefore, I’ll mention
    only key ideas that we’ll use throughout the book and leave the rest to you to
    pick up as you see fit. I’ll assume you know some basic things about probability
    from flipping coins and rolling dice.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive Statistics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When we do experiments, we need to report the results in some meaningful way.
    Typically, for us, we’ll report results as the mean (arithmetic average) plus
    or minus a quantity known as the *standard error of the mean (SE)*. Let’s define
    the standard error of the mean through an example.
  prefs: []
  type: TYPE_NORMAL
- en: If we have many measurements *x*, say the length of a part of a flower, then
    we can calculate the mean (![Image](Images/xbar.jpg)) by adding all the values
    together and dividing by the number of values we added. Then, once we have the
    mean, we can calculate the average spread of the individual values around the
    mean by subtracting each value from the mean, squaring the result, and adding
    all these squared values together before dividing by the number of values we added
    minus one. This number is the *variance*. If we take the square root of this value,
    we get the *standard deviation* (*σ*), which we’ll see again below. With the standard
    deviation, we can calculate the standard error of the mean as ![Image](Images/007equ01.jpg),
    where *n* is the number of values that we used to calculate the mean. The smaller
    the SE is, the more tightly the values are clustered around the mean. We can interpret
    this value as the uncertainty we have about the mean value. This means we expect
    the actual mean, which we don’t really know, to be between ![Image](Images/007equ02.jpg)
    and ![Image](Images/007equ03.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we’ll talk about the median instead of the mean. The *median* is
    the middle value, the value that half of our samples are below and half are above.
    To find the median for a set of values, we first sort the values numerically and
    then find the middle value. This is the exact middle value if we have an odd number
    of samples, or the mean of the two middle values if we have an even number of
    samples. The median is sometimes more useful than the mean if the samples do not
    have a good, even spread around the mean. The classic example is income. A few
    very rich people move the mean income up to the point where it does not have much
    meaning. Instead, the median, the value where half the people make less and half
    make more, is more representative.
  prefs: []
  type: TYPE_NORMAL
- en: 'In later chapters, we’ll talk about *descriptive statistics*. These are values
    derived from a dataset that can be used to understand the dataset. We just mentioned
    three of them: the mean, the median, and the standard deviation. We’ll see how
    to use these and how they can be plotted to help us understand a dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability Distributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this book, we’ll talk about something known as a *probability distribution*.
    You can think of it as an oracle of sorts—something that, when asked, will give
    us a number or set of numbers. For example, when we train a model, we use numbers,
    or sets of numbers, that we measure; we can think of those numbers as coming from
    a probability distribution. We’ll refer to that distribution as the *parent distribution*.
    Think of it as the thing that generates the data we’ll feed our model; another,
    more Platonic, way to think about it is as the ideal set of data that our data
    is approximating.
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability distributions come in many different forms; some even have names.
    The two that we’ll encounter are the two most common: uniform and normal distributions.
    You’ve already encountered a uniform distribution: it’s what we get if we roll
    a fair die. If the die has six sides, we know that the likelihood of getting any
    value, 1 through 6, is the same. If we roll the die 100 times and tally the numbers
    that come up, we know that the tally will be roughly equal for each number and
    that in the long run, we can easily convince ourselves that the number will even
    out.'
  prefs: []
  type: TYPE_NORMAL
- en: A *uniform distribution* is an oracle that is equally likely to give us any
    of its allowed responses. Mathematically, we’ll write uniform distributions as
    *U*(*a*,*b*) where *U* means uniform and *a* and *b* are the range of values it
    will use to bracket its response. Unless we specify the distribution gives only
    integers, any real number is allowed as the response. Notationally, we write *x*
    ~ *U*(0,1) to mean that *x* is a value returned by the oracle that gives real
    numbers in the range (0,1) with equal likelihood. Also, note that using “(" and
    “)" to bracket a range excludes the associated bound, while using “[" and “]"
    includes it. Thus *U*[0,1) returns values from 0 to 1, including 0 but excluding
    1.
  prefs: []
  type: TYPE_NORMAL
- en: A *normal distribution*, also called a *Gaussian* distribution, is visually
    a bell curve—a shape where one value is most likely, and then the likelihood of
    the other values decreases as one gets further from the most likely value. The
    most likely value is the mean, ![Image](Images/xbar.jpg), and the parameter that
    controls how quickly the likelihood drops to zero (without ever really reaching
    it) is the standard deviation, *σ* (sigma). For our purposes, if we want a sample
    from a normal distribution, we’ll write ![Image](Images/008equ01.jpg) to mean
    *x* is drawn from a normal distribution with a mean of ![Image](Images/xbar.jpg)
    and a standard deviation of *σ*.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Tests
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another topic that will pop up from time to time is the idea of a *statistical
    test*, a measurement used to decide if a particular hypothesis is likely true
    or not. Typically, the hypothesis relates to two sets of measurements, and the
    hypothesis is that the two sets of measurements came from the same parent distribution.
    If the statistic calculated by the test is outside of a certain range, we reject
    the hypothesis and claim we have evidence that the two sets of measurements are
    *not* from the same parent distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ll usually use the *t-test*, a common statistical test that assumes
    our data is normally distributed. Because we assumed that our data is normally
    distributed, which may or may not be true, the t-test is known as a *parametric
    test*.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we’ll use another test, the *Mann–Whitney U test*, which is like
    a t-test in that it helps us decide if two samples are from the same parent distribution,
    but it makes no assumption about how the data values in the sample are themselves
    distributed. Tests like these are known as *nonparametric tests*.
  prefs: []
  type: TYPE_NORMAL
- en: Whether the test is parametric or nonparametric, the value we ultimately get
    from the test is called a *p-value*. It represents the probability that we would
    see the test statistic value we calculated if the hypothesis that the samples
    come from the same parent distribution is true. If the *p*-value is low, we have
    evidence that the hypothesis is not true.
  prefs: []
  type: TYPE_NORMAL
- en: The usual *p*-value cutoff is 0.05, indicating a 1 in 20 chance that we’d measure
    the test statistic value (t-test or Mann–Whitney U) even if the samples came from
    the same parent distribution. However, in recent years, it has become clear that
    this threshold is too generous. When *p*-values are near 0.05, but not above,
    we begin to think there is some evidence against the hypothesis. If the *p*-value
    is, say, 0.001 or even less, then we have strong evidence that the samples are
    not from the same parent distribution. In this case, we say that the difference
    is *statistically significant*.
  prefs: []
  type: TYPE_NORMAL
- en: Graphics Processing Units
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the enabling technologies for modern deep learning was the development
    of powerful *graphics processing units (GPUs)*. These are co-computers implemented
    on graphics cards. Originally designed for video gaming, the highly parallel nature
    of GPUs has been adapted to the extreme computational demands of deep neural network
    models. Many of the advances of recent years would not have been possible without
    the supercomputer-like abilities GPUs provide to even basic desktop computers.
    NVIDIA is the leader in the creation of GPUs for deep learning, and via its Compute
    Unified Device Architecture (CUDA), NVIDIA has been foundational to the success
    of deep learning. It’s not an understatement to say that without GPUs, deep learning
    would not have happened, or at least not been so widely used.
  prefs: []
  type: TYPE_NORMAL
- en: That said, we’re not expecting GPUs to be present for the models we’ll work
    with in this book. We’ll use small enough datasets and models so that we can train
    in a reasonable amount of time using just a CPU. We’ve already enforced this decision
    in the packages we’ve installed, since the version of TensorFlow we installed
    is a CPU-only version.
  prefs: []
  type: TYPE_NORMAL
- en: If you do have a CUDA-capable GPU and you want to use it for the deep learning
    portion of this book, please do so, but don’t think that you need to purchase
    one to run the examples. If you’re using a GPU, be sure to have CUDA properly
    installed before installing the packages indicated previously and be sure to install
    a GPU-enabled version of TensorFlow. The sklearn toolkit is CPU only.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we summarized our operating environment. Next, we described
    the essential Python toolkits we’ll use throughout the book and gave detailed
    instructions for installing the toolkits assuming an Ubuntu 20.04 Linux distribution.
    As mentioned, the toolkits will work just as nicely on many other Linux distributions
    as well as macOS. We then briefly reviewed some of the math we’ll encounter later
    and ended with an explanation of why we do not need GPUs for our models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll review the fundamentals of Python.
  prefs: []
  type: TYPE_NORMAL
