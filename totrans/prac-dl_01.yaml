- en: '**1'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GETTING STARTED**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: This chapter introduces our operating environment and details how to set it
    up. It also includes a primer on some of the math we will encounter. We’ll end
    with a brief note about graphics processors, or GPUs, which you may have heard
    are essential for deep learning. For our purposes, they’re not, so don’t worry—this
    book won’t suddenly cost you a lot of money.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The Operating Environment
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll detail the environment we’ll assume throughout the remainder
    of the book. Our underlying assumption is that we’re using a 64-bit Linux system.
    The exact distribution is not critical, but to make things simpler in the presentation,
    we’ll also assume that we’re using Ubuntu 20.04\. Given the excellent support
    behind the Ubuntu distribution, we trust that any newer distributions will also
    work similarly. The Python language is our *lingua franca*, the common language
    of machine learning. Specifically, we’ll use Python 3.8.2; that’s the version
    used by Ubuntu 20.04.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a quick overview of the Python toolkits that we’ll be using.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*NumPy* is a Python library that adds array processing abilities to Python.
    While Python lists can be used like one-dimensional arrays, in practice they are
    too slow and inflexible. The NumPy library adds the array features missing from
    Python—features that are necessary for many scientific applications. NumPy is
    a base library required by all the other libraries we’ll use.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the traditional machine learning models we’ll explore in this book are
    found in the superb scikit-learn library, or *sklearn*, as it’s usually called
    when loaded into Python. Note also that we’re writing *scikit-learn* without caps
    as this is how the authors consistently refer to it in their documentation. This
    library uses NumPy arrays. It implements a standardized interface to many different
    machine learning models as well as an entire host of other functionality that
    we won’t even have time to touch. I strongly encourage you to review the official
    sklearn documentation ([https://scikit-learn.org/stable/documentation.html](https://scikit-learn.org/stable/documentation.html))
    as you become more and more familiar with machine learning and the tools behind
    it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Keras with TensorFlow
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning is hard enough to understand, let alone implement efficiently
    and correctly, so instead of attempting to write convolutional neural networks
    from scratch, we’ll use one of the popular toolkits already in active development.
    From its inception, the deep learning community has supported the development
    of toolkits to make deep networks easier to use and has made the toolkits open
    source with very generous licenses. At the time of this writing, there are many
    popular toolkits we could have used in Python. Among many others, these include
    the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache MXnet
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these toolkits are waxing, and others appear to be waning. But the one
    that has probably the most active following at present is Keras with the TensorFlow
    backend, so that’s the one we’ll use here.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*Keras* (*[https://keras.io/](https://keras.io/)*) is a Python deep learning
    toolkit that uses the TensorFlow toolkit (*[https://www.tensorflow.org/](https://www.tensorflow.org/)*)
    under the hood. *TensorFlow* is an open source Google product that implements
    the core functionality of deep neural networks for many different platforms. We
    selected Keras not only because it’s popular and in active development, but also
    because it’s straightforward to use. Our goal is to become familiar with deep
    learning to the point where we can implement models and use them with a minimum
    of programming overhead.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Toolkits
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can’t reasonably give an exhaustive guide for installing the toolkits on
    all systems and hardware. Instead, we’ll provide step-by-step instructions for
    the specific operating system we’ll use as the reference system. These steps,
    along with the minimum version numbers of the libraries, should be enough for
    most readers to get a working system in place.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Remember, we’re assuming that we’re working in a Linux environment, specifically
    Ubuntu 20.04\. Ubuntu is a widely used Linux distribution, and it runs on almost
    any modern computer system. Other Linux distributions will work, as will macOS,
    but the instructions here are specific to Ubuntu. For the most part, the machine
    learning community has left the Windows operating system. Still, individuals have
    ported toolkits to Windows; therefore, an adventurous reader might give Windows
    a try.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'A freshly installed Ubuntu 20.04 base desktop system gives us Python 3.8.2
    for free. To install the remaining packages, we need to go into a shell and execute
    the sequence of steps below in the order given:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: $ sudo apt - get update
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: $ sudo apt - get install python3 - pip
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: $ sudo apt - get install build - essential python3 - dev
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: $ sudo apt - get install python3 - setuptools python3 - numpy
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: $ sudo apt - get install python3 - scipy libatlas - base - dev
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: $ sudo apt - get install python3 - matplotlib
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: $ pip3 install scikit - learn
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: $ pip3 install tensorflow
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: $ pip3 install pillow
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: $ pip3 install h5py
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: $ pip3 install keras
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the installation is complete, we’ll have installed the following versions
    of the libraries and toolkits:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: NumPy 1.17.4
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: sklearn 0.23.2
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: keras 2.4.3
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: tensorflow 2.2.0
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: pillow 7.0.0
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: h5py 2.10.0
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib 3.1.2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The pillow library is an image processing library, h5py is a library for working
    with HDF5 format data files, and matplotlib is for plotting. *HDF5* is a generic,
    hierarchical file format for storing scientific data. Keras uses it to store model
    parameters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The following two sections are light introductions to some of the math that
    will creep into the book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Basic Linear Algebra
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re about to look at vectors and matrices. The math that deals with these
    concepts falls under the general heading of *linear algebra*, or matrix theory.
    As you might imagine, linear algebra is a complex field. All we need to know for
    this book is what a vector is, what a matrix is, and how we can multiply two vectors,
    or two matrices, or vectors and matrices together. We’ll see later on that this
    gives us a powerful way to implement specific models, particularly neural networks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by looking at vectors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *vector* is a one-dimensional list of numbers. Mathematically, a vector might
    appear as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '*a* = [0, 1, 2, 3, 4]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: with the third element given as *a*[2] = 2\. Notice we’re following the programming
    convention of indexing from zero, so *a*[2] gives us the third element in the
    vector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector above was written horizontally and therefore is known as a *row
    vector*. When used in mathematical expressions, however, vectors are usually assumed
    to be written vertically:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/004equ02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'When written vertically, a vector is known as a *column vector*. This vector
    has five elements and is denoted as a five-element column vector. In this book,
    we’ll typically use vectors to represent one *sample*: one set of features that
    we’ll input to a model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, vectors are used to represent points in space. If we’re talking
    about the two-dimensional (2D) Cartesian plane, we locate a point with a vector
    of two numbers, (*x*,*y*), where *x* is the distance along the x-axis and *y*
    is the distance along the y-axis. That vector represents a point in two dimensions,
    even though the vector itself has only one dimension. If we have three dimensions,
    we need a vector with three elements, (*x*,*y*,*z*).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, since we often use vectors to represent the inputs to our
    models, we’ll be working with dozens to hundreds of dimensions. Of course, we
    can’t plot them as points in a space, but mathematically, that’s what they are.
    As we’ll see, some models, such as the *k*-Nearest Neighbors model, use the feature
    vectors as just that—points in a high-dimensional space.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *matrix* is a two-dimensional array of numbers where we index a particular
    entry by its row number and column number. For example, this is a matrix:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/005equ01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: If we want to refer to the 6, we write *a*[1,2] = 6\. Again, we’re indexing
    from zero. Because this matrix *a* has three rows and three columns, we call it
    a 3 × 3 matrix.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying Vectors and Matrices
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplest way to think of multiplying two vectors together is to multiply
    their corresponding elements. For example:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[1, 2, 3] × [4, 5, 6] = [4, 10, 18]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: This is the most common way to multiply an array when using a toolkit like NumPy,
    and we’ll make heavy use of this in the chapters that follow. However, in mathematics,
    this is seldom actually done.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: When multiplying vectors together mathematically, we need to know if they are
    row or column vectors. We’ll work with two vectors, *A* = (*a*,*b*,*c*), and *B*
    = (*d*,*e*,*f* ), which, following mathematical convention, are assumed to be
    column vectors. Adding a superscript *T* turns a column vector into a row vector.
    The mathematically allowed ways to multiply *A* and *B* are
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/005equ02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: which is called the *outer product* and
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/005equ03.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: which is called the *inner product*, or *dot product*. Notice that the outer
    product becomes a matrix, and the inner product becomes a single number, a *scalar*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: When multiplying a matrix and a vector, the vector is typically on the right
    side of the matrix. The multiplication can proceed if the number of columns in
    the matrix matches the number of elements in the vector, again assumed to be a
    column vector. The result is also a vector with as many elements as there are
    rows in the matrix (read *ax* + *by* + *cz* as a single element).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/006equ01.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Here we’ve multiplied a 2 × 3 matrix by a 3 × 1 column vector to get a 2 ×
    1 output vector. Notice that the number of columns of the matrix and the number
    of rows of the vector match. If they do not, then the multiplication is not defined.
    Also, notice that the values in the output vector are sums of products of the
    matrix and vector. This same rule applies when multiplying two matrices:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/006equ02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Here multiplying a 2 × 3 matrix by a 3 × 2 matrix has given us a 2 × 2 answer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: When we get to convolutional neural networks, we’ll work with arrays that have
    three and even four dimensions. Generically, these are referred to as *tensors*.
    If we imagine a stack of matrices, all the same size, we get a three-dimensional
    tensor, and we can use the first index to refer to any one of the matrices and
    the remaining two indices to refer to a particular element of that matrix. Similarly,
    if we have a stack of three-dimensional tensors, we have a four-dimensional tensor,
    and we can use the first index of that to refer to any one of the three-dimensional
    tensors.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The main points of this section are that vectors have one dimension, matrices
    have two dimensions, there are rules for multiplying these objects together, and
    our toolkits will work with four-dimensional tensors in the end. We’ll review
    some of these points as we encounter them later in the book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and Probability
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The topics of statistics and probability are so broad that often it’s better
    to either say almost nothing or to write a book or two. Therefore, I’ll mention
    only key ideas that we’ll use throughout the book and leave the rest to you to
    pick up as you see fit. I’ll assume you know some basic things about probability
    from flipping coins and rolling dice.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive Statistics
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When we do experiments, we need to report the results in some meaningful way.
    Typically, for us, we’ll report results as the mean (arithmetic average) plus
    or minus a quantity known as the *standard error of the mean (SE)*. Let’s define
    the standard error of the mean through an example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: If we have many measurements *x*, say the length of a part of a flower, then
    we can calculate the mean (![Image](Images/xbar.jpg)) by adding all the values
    together and dividing by the number of values we added. Then, once we have the
    mean, we can calculate the average spread of the individual values around the
    mean by subtracting each value from the mean, squaring the result, and adding
    all these squared values together before dividing by the number of values we added
    minus one. This number is the *variance*. If we take the square root of this value,
    we get the *standard deviation* (*σ*), which we’ll see again below. With the standard
    deviation, we can calculate the standard error of the mean as ![Image](Images/007equ01.jpg),
    where *n* is the number of values that we used to calculate the mean. The smaller
    the SE is, the more tightly the values are clustered around the mean. We can interpret
    this value as the uncertainty we have about the mean value. This means we expect
    the actual mean, which we don’t really know, to be between ![Image](Images/007equ02.jpg)
    and ![Image](Images/007equ03.jpg).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we’ll talk about the median instead of the mean. The *median* is
    the middle value, the value that half of our samples are below and half are above.
    To find the median for a set of values, we first sort the values numerically and
    then find the middle value. This is the exact middle value if we have an odd number
    of samples, or the mean of the two middle values if we have an even number of
    samples. The median is sometimes more useful than the mean if the samples do not
    have a good, even spread around the mean. The classic example is income. A few
    very rich people move the mean income up to the point where it does not have much
    meaning. Instead, the median, the value where half the people make less and half
    make more, is more representative.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'In later chapters, we’ll talk about *descriptive statistics*. These are values
    derived from a dataset that can be used to understand the dataset. We just mentioned
    three of them: the mean, the median, and the standard deviation. We’ll see how
    to use these and how they can be plotted to help us understand a dataset.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Probability Distributions
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this book, we’ll talk about something known as a *probability distribution*.
    You can think of it as an oracle of sorts—something that, when asked, will give
    us a number or set of numbers. For example, when we train a model, we use numbers,
    or sets of numbers, that we measure; we can think of those numbers as coming from
    a probability distribution. We’ll refer to that distribution as the *parent distribution*.
    Think of it as the thing that generates the data we’ll feed our model; another,
    more Platonic, way to think about it is as the ideal set of data that our data
    is approximating.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将讨论一个被称为*概率分布*的东西。你可以把它看作是某种神谕——当被问及时，会给我们一个数字或一组数字。例如，当我们训练模型时，我们使用我们测量的数字或数字集合；我们可以把这些数字看作来自概率分布。我们将称之为*父分布*。可以将其视为生成我们将馈送给模型的数据的东西；另一种更柏拉图式的思考方式是将其视为我们的数据正在逼近的理想数据集。
- en: 'Probability distributions come in many different forms; some even have names.
    The two that we’ll encounter are the two most common: uniform and normal distributions.
    You’ve already encountered a uniform distribution: it’s what we get if we roll
    a fair die. If the die has six sides, we know that the likelihood of getting any
    value, 1 through 6, is the same. If we roll the die 100 times and tally the numbers
    that come up, we know that the tally will be roughly equal for each number and
    that in the long run, we can easily convince ourselves that the number will even
    out.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布有许多不同的形式；一些甚至有名字。我们将遇到的两种最常见的是均匀分布和正态分布。您已经遇到了均匀分布：这是我们掷一个公平的骰子所得到的。如果骰子有六个面，我们知道得到任何值，从1到6的可能性是相同的。如果我们掷100次骰子并记录出现的数字，我们知道每个数字的记录大致相等，并且在长期内，我们可以很容易地说服自己数字会平衡。
- en: A *uniform distribution* is an oracle that is equally likely to give us any
    of its allowed responses. Mathematically, we’ll write uniform distributions as
    *U*(*a*,*b*) where *U* means uniform and *a* and *b* are the range of values it
    will use to bracket its response. Unless we specify the distribution gives only
    integers, any real number is allowed as the response. Notationally, we write *x*
    ~ *U*(0,1) to mean that *x* is a value returned by the oracle that gives real
    numbers in the range (0,1) with equal likelihood. Also, note that using “(" and
    “)" to bracket a range excludes the associated bound, while using “[" and “]"
    includes it. Thus *U*[0,1) returns values from 0 to 1, including 0 but excluding
    1.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*均匀分布*是一个神谕，它同等可能地给出它允许的任何响应。在数学上，我们将均匀分布写为*U*(*a*,*b*)，其中*U*表示均匀分布，*a*和*b*是它用于括起其响应的值的范围。除非我们指定分布仅给出整数，否则任何实数都可以作为响应。在符号上，我们写*x*
    ~ *U*(0,1)来表示*x*是由神谕返回的值，该神谕在范围(0,1)内以相等的可能性给出。此外，请注意，使用“(”和“)”来括起范围会排除相关边界，而使用“[”和“]”会包括它。因此*U*[0,1)返回从0到1的值，包括0但不包括1。
- en: A *normal distribution*, also called a *Gaussian* distribution, is visually
    a bell curve—a shape where one value is most likely, and then the likelihood of
    the other values decreases as one gets further from the most likely value. The
    most likely value is the mean, ![Image](Images/xbar.jpg), and the parameter that
    controls how quickly the likelihood drops to zero (without ever really reaching
    it) is the standard deviation, *σ* (sigma). For our purposes, if we want a sample
    from a normal distribution, we’ll write ![Image](Images/008equ01.jpg) to mean
    *x* is drawn from a normal distribution with a mean of ![Image](Images/xbar.jpg)
    and a standard deviation of *σ*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*正态分布*，也称为*高斯*分布，在视觉上是一个钟形曲线——一个值最有可能，然后随着远离最可能值，其他值的可能性逐渐降低。最可能的值是平均值，![Image](Images/xbar.jpg)，控制可能性多快下降到零（但从未真正达到）的参数是标准差，*σ*（西格玛）。对于我们的目的，如果我们想要从正态分布中取样，我们将写![Image](Images/008equ01.jpg)表示*x*是从均值为![Image](Images/xbar.jpg)、标准差为*σ*的正态分布中抽取的样本。
- en: Statistical Tests
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统计检验
- en: Another topic that will pop up from time to time is the idea of a *statistical
    test*, a measurement used to decide if a particular hypothesis is likely true
    or not. Typically, the hypothesis relates to two sets of measurements, and the
    hypothesis is that the two sets of measurements came from the same parent distribution.
    If the statistic calculated by the test is outside of a certain range, we reject
    the hypothesis and claim we have evidence that the two sets of measurements are
    *not* from the same parent distribution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不时会出现的话题是*统计检验*的概念，这是一种用来判断某个假设是否可能成立的测量方法。通常，假设涉及到两组测量数据，假设是这两组测量来自同一个母体分布。如果检验计算出的统计量超出了某个范围，我们就会拒绝这个假设，并声称有证据表明这两组测量*不*来自同一个母体分布。
- en: Here, we’ll usually use the *t-test*, a common statistical test that assumes
    our data is normally distributed. Because we assumed that our data is normally
    distributed, which may or may not be true, the t-test is known as a *parametric
    test*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通常会使用* t 检验*，这是一种常见的统计检验，假设我们的数据是正态分布的。由于我们假设数据是正态分布的，这一假设可能对也可能不对，所以
    t 检验被称为*参数检验*。
- en: Sometimes, we’ll use another test, the *Mann–Whitney U test*, which is like
    a t-test in that it helps us decide if two samples are from the same parent distribution,
    but it makes no assumption about how the data values in the sample are themselves
    distributed. Tests like these are known as *nonparametric tests*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们会使用另一种检验，*曼–惠特尼 U 检验*，它与 t 检验类似，帮助我们判断两个样本是否来自同一个母体分布，但它不对样本中数据值的分布做任何假设。像这样的检验被称为*非参数检验*。
- en: Whether the test is parametric or nonparametric, the value we ultimately get
    from the test is called a *p-value*. It represents the probability that we would
    see the test statistic value we calculated if the hypothesis that the samples
    come from the same parent distribution is true. If the *p*-value is low, we have
    evidence that the hypothesis is not true.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 无论检验是参数检验还是非参数检验，我们最终得到的值被称为*p*-值。它表示在假设样本来自同一个母体分布的前提下，我们计算出的检验统计量值出现的概率。如果*p*-值较低，我们有证据表明假设不成立。
- en: The usual *p*-value cutoff is 0.05, indicating a 1 in 20 chance that we’d measure
    the test statistic value (t-test or Mann–Whitney U) even if the samples came from
    the same parent distribution. However, in recent years, it has become clear that
    this threshold is too generous. When *p*-values are near 0.05, but not above,
    we begin to think there is some evidence against the hypothesis. If the *p*-value
    is, say, 0.001 or even less, then we have strong evidence that the samples are
    not from the same parent distribution. In this case, we say that the difference
    is *statistically significant*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的*p*-值临界值是0.05，这意味着如果样本来自同一个母体分布，那么我们有1/20的机会测量到该检验统计量（t检验或曼–惠特尼U检验）。然而，近年来已经明确这个临界值过于宽松。当*p*-值接近0.05，但不超过时，我们开始怀疑有一些证据反对假设。如果*p*-值是0.001甚至更小，那么我们就有强有力的证据表明这些样本不来自同一个母体分布。在这种情况下，我们称差异是*统计显著*的。
- en: Graphics Processing Units
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图形处理单元
- en: One of the enabling technologies for modern deep learning was the development
    of powerful *graphics processing units (GPUs)*. These are co-computers implemented
    on graphics cards. Originally designed for video gaming, the highly parallel nature
    of GPUs has been adapted to the extreme computational demands of deep neural network
    models. Many of the advances of recent years would not have been possible without
    the supercomputer-like abilities GPUs provide to even basic desktop computers.
    NVIDIA is the leader in the creation of GPUs for deep learning, and via its Compute
    Unified Device Architecture (CUDA), NVIDIA has been foundational to the success
    of deep learning. It’s not an understatement to say that without GPUs, deep learning
    would not have happened, or at least not been so widely used.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习的一个重要技术是*图形处理单元（GPU）*的发展。这些是实现于显卡上的协同计算机。最初为视频游戏设计，GPU的高度并行特性已被应用于深度神经网络模型的极高计算需求。近年来的许多进展如果没有GPU提供的类似超级计算机的能力，即使是普通的桌面计算机，也不可能实现。NVIDIA是深度学习GPU创建的领导者，并且通过其计算统一设备架构（CUDA），NVIDIA为深度学习的成功奠定了基础。毫不夸张地说，如果没有GPU，深度学习就不可能发生，至少不会如此广泛应用。
- en: That said, we’re not expecting GPUs to be present for the models we’ll work
    with in this book. We’ll use small enough datasets and models so that we can train
    in a reasonable amount of time using just a CPU. We’ve already enforced this decision
    in the packages we’ve installed, since the version of TensorFlow we installed
    is a CPU-only version.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们并不指望在本书中使用的模型中配备 GPU。我们将使用足够小的数据集和模型，以便仅使用 CPU 就能在合理的时间内完成训练。我们已经在安装的包中强制执行了这一决定，因为我们安装的
    TensorFlow 版本是仅支持 CPU 的版本。
- en: If you do have a CUDA-capable GPU and you want to use it for the deep learning
    portion of this book, please do so, but don’t think that you need to purchase
    one to run the examples. If you’re using a GPU, be sure to have CUDA properly
    installed before installing the packages indicated previously and be sure to install
    a GPU-enabled version of TensorFlow. The sklearn toolkit is CPU only.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实拥有支持 CUDA 的 GPU，并希望将其用于本书的深度学习部分，可以使用它，但不要认为你需要购买一个 GPU 来运行示例。如果你使用 GPU，请确保在安装前正确安装
    CUDA，并确保安装支持 GPU 的 TensorFlow 版本。sklearn 工具包仅支持 CPU。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we summarized our operating environment. Next, we described
    the essential Python toolkits we’ll use throughout the book and gave detailed
    instructions for installing the toolkits assuming an Ubuntu 20.04 Linux distribution.
    As mentioned, the toolkits will work just as nicely on many other Linux distributions
    as well as macOS. We then briefly reviewed some of the math we’ll encounter later
    and ended with an explanation of why we do not need GPUs for our models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们总结了我们的操作环境。接下来，我们介绍了本书中将要使用的基本 Python 工具包，并假设使用 Ubuntu 20.04 Linux 发行版，详细说明了安装工具包的步骤。如前所述，这些工具包在许多其他
    Linux 发行版以及 macOS 上也能很好地工作。然后，我们简要回顾了后面会涉及到的一些数学内容，最后解释了为什么我们的模型不需要 GPU。
- en: In the next chapter, we’ll review the fundamentals of Python.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾 Python 的基础知识。
