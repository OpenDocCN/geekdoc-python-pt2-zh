- en: INTRODUCTION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Math is essential to the modern world. Deep learning is also rapidly becoming
    essential. From the promise of self-driving cars to medical systems detecting
    fractures better than all but the very best physicians, to say nothing of increasingly
    capable, and possibly worrisome, voice-controlled assistants, deep learning is
    everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: This book covers the essential math for making deep learning comprehensible.
    It’s true that you can learn the toolkits, set up the configuration files or Python
    code, format some data, and train a model, all without understanding *what* you’re
    doing, let alone the math behind it. And, because of the power of deep learning,
    you’ll often be successful. However, you won’t *understand*, and you shouldn’t
    be satisfied. To understand, you need some math. Not a lot of math, but some specific
    math. In particular, you’ll need working knowledge of topics in probability, statistics,
    linear algebra, and differential calculus. Fortunately, those are the very topics
    this book happens to address.
  prefs: []
  type: TYPE_NORMAL
- en: Who Is This Book For?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is not an introductory deep learning book. It will not teach you the basics
    of deep learning. Instead, it’s meant as an adjunct to such a book. (See my book
    *Practical Deep Learning: A Python-Based Introduction* [No Starch Press, 2021].)
    I expect you to be familiar with deep learning, at least conceptually, though
    I’ll explain things along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, I expect you to bring certain knowledge to the table. I expect
    you to know high school mathematics, in particular algebra. I also expect you
    to be familiar with programming using Python, R, or a similar language. We’ll
    be using Python 3.*x* and some of its popular toolkits, such as NumPy, SciPy,
    and scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve attempted to keep other expectations to a minimum. After all, the point
    of the book is to give *you* what you need to be successful in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: About This Book
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At its core, this is a math book. But instead of proofs and practice exercises,
    we’ll use code to illustrate the concepts. Deep learning is an applied discipline
    that you need to do to be able to understand. Therefore, we’ll use code to bridge
    the gap between pure mathematical knowledge and practice.
  prefs: []
  type: TYPE_NORMAL
- en: The chapters build one upon the other, with foundational chapters followed by
    more advanced math topics and, ultimately, deep learning algorithms that make
    use of everything covered in the earlier chapters. I recommend reading the book
    straight through and, if you wish, skipping topics you’re already familiar with
    as you encounter them.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 1: Setting the Stage](ch01.xhtml#ch01)** This chapter configures
    our working environment and the toolkits we’ll use, which are those used most
    often in deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 2: Probability](ch02.xhtml#ch02)** Probability affects almost all
    aspects of deep learning and is essential to understanding how neural networks
    learn. This chapter, the first of two on this subject, introduces fundamental
    topics in probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 3: More Probability](ch03.xhtml#ch03)** Probability is so important
    that one chapter isn’t enough. This chapter continues our exploration and includes
    key deep learning topics, like probability distributions and Bayes’ theorem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 4: Statistics](ch04.xhtml#ch04)** Statistics make sense of data
    and are crucial for evaluating models. Statistics go hand in hand with probability,
    so we need to understand statistics to understand deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 5: Linear Algebra](ch05.xhtml#ch05)** Linear algebra is the world
    of vectors and matrices. Deep learning is, at its core, linear algebra–focused.
    Implementing neural networks is an exercise in vector and matrix mathematics,
    so it is essential to understand what these concepts represent and how to work
    with them.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 6: More Linear Algebra](ch06.xhtml#ch06)** This chapter continues
    our exploration of linear algebra, focusing on important topics concerning matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 7: Differential Calculus](ch07.xhtml#ch07)** Perhaps the most fundamental
    concept behind the training of neural networks is the gradient. To understand
    the gradient, what it is and how to use it, we must know how to work with derivatives
    of functions. This chapter builds the foundation necessary to understand derivatives
    and gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 8: Matrix Calculus](ch08.xhtml#ch08)** Deep learning manipulates
    derivatives of vectors and matrices. Therefore, in this chapter we generalize
    the concept of a derivative to these objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 9: Data Flow in Neural Networks](ch09.xhtml#ch09)** To understand
    how neural networks manipulate vectors and matrices, we need to understand how
    data flows through the network. That’s the subject of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 10: Backpropagation](ch10.xhtml#ch10)** Successful training of neural
    networks usually involves two algorithms that go hand in hand: backpropagation
    and gradient descent. In this chapter, we work through backpropagation in detail
    to see how the math we learned earlier in the book applies to the training of
    actual neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Chapter 11: Gradient Descent](ch11.xhtml#ch11)** Gradient descent uses the
    gradients that the backpropagation algorithm provides to train a neural network.
    This chapter explores gradient descent, beginning with 1D examples and progressing
    through to fully connected neural networks. It also describes and compares common
    variants of gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Appendix: Going Further](app01.xhtml)** We must, of necessity, gloss over
    many topics in probability, statistics, linear algebra, and calculus. This appendix
    points you toward resources that will aid you in going further with the mathematics
    behind deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download all the code from the book here: *[https://github.com/rkneusel9/MathForDeepLearning/](https://github.com/rkneusel9/MathForDeepLearning/)*.
    And please look at *[https://nostarch.com/math-deep-learning/](https://nostarch.com/math-deep-learning/)*
    for future errata. Let’s get started.'
  prefs: []
  type: TYPE_NORMAL
