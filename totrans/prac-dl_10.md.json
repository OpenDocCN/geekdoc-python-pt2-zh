["```py\nMLPClassifier(hidden_layer_sizes=(100, ), activation='relu',\n\n  solver='adam', alpha=0.0001, batch_size='auto',\n\n  learning_rate='constant', learning_rate_init=0.001,\n\n  power_t=0.5, max_iter=200, shuffle=True,\n\n  random_state=None, tol=0.0001, verbose=False,\n\n  warm_start=False, momentum=0.9, nesterovs_momentum=True,\n\n  early_stopping=False, validation_fraction=0.1, beta_1=0.9,\n\n  beta_2=0.999, epsilon=1e-08)\n```", "```py\nConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached\n\nand the optimization hasn't converged yet.\n```", "```py\n$ python3 -W ignore mnist_nn_experiments.py\n```", "```py\nlayers:(3000,1500), score=0.8822+/-0.0007, loss=0.2107+/-0.0006\n\n(params=6871510, time=253.42s)\n```", "```py\nfor it in range(self.max_iter):\n\n  X, y = shuffle(X, y, random_state=self._random_state)\n\n  accumulated_loss = 0.0\n\n  for batch_slice in gen_batches(n_samples, batch_size):\n\n    activations[0] = X[batch_slice]\n\n    batch_loss, coef_grads, intercept_grads = self._backprop(\n\n      X[batch_slice], y[batch_slice], activations, deltas,\n\n      coef_grads, intercept_grads)\n\n    accumulated_loss += batch_loss * (batch_slice.stop -\n\n                                      batch_slice.start)\n\n    grads = coef_grads + intercept_grads\n\n    self._optimizer.update_params(grads)\n\n  self.n_iter_ += 1\n```", "```py\nMLPClassifier(solver=\"sgd\", verbose=False, tol=1e-8,\n\n     nesterovs_momentum=False, early_stopping=False,\n\n     learning_rate_init=0.001, momentum=0.9, max_iter=100,\n\n     hidden_layer_sizes=(1000,500), activation=\"relu\",\n\n     batch_size=bz)\n```", "```py\n[0.2, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]\n```", "```py\n[8, 15, 30, 150, 300, 1500, 3000, 15000]\n```", "```py\n[0.0, 0.1, 0.2, 0.3, 0.4]\n```", "```py\ndef epoch(x_train, y_train, x_test, y_test, clf):\n\n    clf.fit(x_train, y_train)\n\n    val_err = 1.0 - clf.score(x_test, y_test)\n\n    clf.warm_start = True\n\n    return val_err\n```", "```py\ndef run(x_train, y_train, x_test, y_test, clf, epochs):\n\n    val_err = []\n\n    clf.max_iter = 1\n\n    for i in range(epochs):\n\n        verr = epoch(x_train, y_train, x_test, y_test, clf)\n\n        val_err.append(verr)\n\n    return val_err\n```", "```py\n[0.0, 0.3, 0.5, 0.7, 0.9, 0.99]\n```", "```py\nclass Classifier(MLPClassifier):\n\n    def _init_coef(self, fan_in, fan_out):\n\n        if (self.init_scheme == 0):\n\n            return super(Classifier, self)._init_coef(fan_in, fan_out)\n\n        elif (self.init_scheme == 1):\n\n            weights = 0.01*(np.random.random((fan_in, fan_out))-0.5)\n\n            biases = np.zeros(fan_out)\n\n        elif (self.init_scheme == 2):\n\n            weights = 0.005*(np.random.normal(size=(fan_in, fan_out)))\n\n            biases = np.zeros(fan_out)\n\n        elif (self.init_scheme == 3):\n\n            weights = np.random.normal(size=(fan_in, fan_out))*  \\\n\n                        np.sqrt(2.0/fan_in)\n\n            biases = np.zeros(fan_out)\n\n        elif (self.init_scheme == 4):\n\n            weights = np.random.normal(size=(fan_in, fan_out))*  \\\n\n                        np.sqrt(1.0/fan_in)\n\n            biases = np.zeros(fan_out)\n```", "```py\ntest_err = np.zeros((trainings, init_types, epochs))\n```", "```py\nfor i in range(trainings):\n\n    for k in range(init_types):\n\n        nn = Classifier(solver=\"sgd\", verbose=False, tol=0,\n\n               nesterovs_momentum=False, early_stopping=False,\n\n               learning_rate_init=0.01, momentum=0.9,\n\n               hidden_layer_sizes=(100,50), activation=\"relu\", alpha=0.2,\n\n               learning_rate=\"constant\", batch_size=64, max_iter=1)\n\n        nn.init_scheme = k\n\n        test_err[i,k,:] = run(x_train, y_train, x_test, y_test, nn, epochs)\n\nnp.save(\"mnist_nn_experiments_init_results.npy\", test_err)\n```", "```py\nMLPClassifier(solver=\"sgd\", verbose=False, tol=0,\n\n  nesterovs_momentum=False, early_stopping=False,\n\n  learning_rate_init=0.01, momentum=0.9,\n\n  hidden_layer_sizes=(100,50), activation=\"relu\",\n\n  alpha=0.2, learning_rate=\"constant\", batch_size=64, max_iter=1)\n```"]