- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Premature Optimization Is the Root of All Evil
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, you’ll learn how premature optimization can hinder your productivity.
    *Premature optimization* is the act of spending valuable resources—time, effort,
    lines of code—on unnecessary code optimizations, especially before you have all
    the relevant information. It is one of the main problems with poorly written code.
    Premature optimization comes in many flavors; this chapter will introduce some
    of the most relevant ones. We’ll study practical examples showing where premature
    optimization occurs that will be relevant for your own code projects. We’ll close
    the chapter with actionable tips on performance tuning, ensuring that it is *not*
    premature.
  prefs: []
  type: TYPE_NORMAL
- en: Six Types of Premature Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There’s nothing wrong with optimized code per se, but it always comes with a
    cost, whether that’s additional programming time or extra lines of code. When
    you optimize code snippets, you’re generally trading complexity for performance.
    Sometimes you can obtain both low complexity and high performance, for example,
    by writing clean code, but you must spend programming time to accomplish this!
    If you do this too early in the process, you’ll often spend time optimizing code
    that may never be used in practice or that has little impact on the overall runtime
    of the program. You’ll also optimize without having enough information about when
    the code is called and possible input values. Wasting precious resources like
    programming time and code lines can reduce your productivity by orders of magnitude,
    so it’s important to know how to invest in them wisely.
  prefs: []
  type: TYPE_NORMAL
- en: 'But don’t take my word for it. Here’s what one of the most influential computer
    scientists of all time, Donald Knuth, says about premature optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmers waste enormous amounts of time thinking about, or worrying about,
    the speed of noncritical parts of their programs, and these attempts at efficiency
    actually have a strong negative impact when debugging and maintenance are considered.
    We should forget about small efficiencies, say about 97% of the time: premature
    optimization is the root of all evil.^([1](#c05-footnote-1))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Premature optimization can take many forms, so to explore the issue, we’ll look
    at six common cases I’ve encountered in which you too might be tempted to prematurely
    focus on small efficiencies, slowing your progress.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Code Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Be wary of spending time optimizing functions before you know how much those
    functions will be used. Say you encounter a function you just cannot stand to
    leave unoptimized. You reason to yourself that it’s bad programming style to use
    naive methods and that you should use more efficient data structures or algorithms
    to tackle the problem. You dive into research mode and spend hours researching
    and fine-tuning algorithms. But as it turns out, this function is executed only
    a few times in the final project: the optimization doesn’t result in meaningful
    performance improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Avoid adding features that aren’t strictly necessary and wasting time optimizing
    those features. Suppose you develop a smartphone app that translates text into
    Morse code, expressed by blinking lights. You’ve learned in Chapter 3 that implementing
    an MVP first, rather than creating a polished end product with many, possibly
    unnecessary, features, is the best way to go. In this case, the MVP would be a
    simple app with one function: translate text into Morse code by providing a text
    via a simple input form and hitting a button on which the app then translates
    this text to Morse code. However, you think the MVP rule doesn’t apply to your
    project and decide to add a few extra features: a text-to-audio converter and
    a receiver that translates light signals to text. After shipping your app, you
    learn that your users never use these features. Premature optimization has significantly
    slowed down your product development cycle and delayed your ability to incorporate
    user feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you prematurely optimize the planning phase, trying to find solutions to
    problems that haven’t yet occurred, you risk delaying your ability to receive
    valuable feedback. While you certainly shouldn’t avoid planning entirely, getting
    stuck in the planning phase can be just as costly. To ship something of value
    to the real world, you must accept imperfection. You need user feedback and sanity
    checks from testers to figure out where to focus. Planning can help you avoid
    certain pitfalls, but if you’re not taking action, you’ll never finish your project
    and will remain stuck in the ivory tower of theory.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prematurely optimizing the scalability of your application before you have a
    realistic idea of the audience can be a major distractor and can easily cost you
    tens of thousands of dollars’ worth of developer and server time. Expecting millions
    of users, you design a distributed architecture that dynamically adds virtual
    machines to handle peak load if necessary. However, creating distributed systems
    is a complex and error-prone task that may easily take you months to implement.
    Many projects fail anyway; if you do become as successful as your wildest dreams
    suggest, you’ll have plenty of opportunity to scale your system with the increase
    of demand. Worse, the distribution may *reduce* an application’s scalability,
    due to an increased communication and data consistency overhead. Scalable distributed
    systems come at a price—are you sure you need to pay it? Don’t try to scale to
    millions of users before you’ve served your first one.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Test Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Optimizing for tests too soon is also a major driver of wasted developer time.
    Test-driven development has many zealous followers who misinterpret the idea of
    *implementing tests before functionality* to always write tests first—even if
    the purpose of a code function is pure experimentation or the code function doesn’t
    lend itself well to testing in the first place. To write experimental code is
    to test concepts and ideas, and adding another layer of tests to experimental
    code can harm progress and does not adhere to the philosophy of rapid prototyping.
    In addition, suppose you believe in rigorous test-driven development and insist
    on 100 percent test coverage. Some functions—for instance, those that process
    free text from users—don’t work very well with unit tests because of their unpredictable
    human-based input. For those functions, only real human beings can test them in
    a meaningful way—in these cases, real-world users *are the only test that matters.
    Nevertheless, you prematurely optimize for a perfect coverage of unit tests. This
    approach has little value: it slows down the software development cycle while
    introducing unnecessary complexity.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*### Optimizing Object-Oriented World Building'
  prefs: []
  type: TYPE_NORMAL
- en: 'Object-oriented approaches can often introduce major unnecessary complexity
    and premature “conceptual” optimization. Suppose you want to model your application’s
    world using a complex hierarchy of classes. You write a small game about car racing.
    You create a class hierarchy where the `Porsche` class inherits from the `Car`
    class, which inherits from the `Vehicle` class. After all, every Porsche is a
    car, and every car is a vehicle. However, the multi-level class hierarchy leads
    to complexity in your codebase, and future programmers have trouble figuring out
    what your code does. In many cases, these types of stacked inheritance structures
    add unnecessary complexity. Avoid them by using the ideas of MVPs: start with
    the simplest model and extend it only if needed. Don’t optimize your code to model
    a world with more details than the application actually needs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premature Optimization: A Story'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a general sense of the problems premature optimization can
    cause, let’s write a small Python application and see in real time how premature
    optimization adds unnecessary complexity to the code of a small transaction-tracking
    application that doesn’t need to scale gracefully to thousands of users.
  prefs: []
  type: TYPE_NORMAL
- en: Alice, Bob, and Carl play poker each Friday night. After a few rounds, they
    decide that they need to develop a system to keep track of the money each player
    owes after a given game night. Alice is a passionate programmer and creates a
    small application that tracks the players’ balances, shown in [Listing 5-1](#listing5-1).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-1: A simple script to track transactions and balances'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script has two global variables, `transactions` and `balances`. The list
    `transactions` tracks the transactions between players as they occur during a
    game night. Each transaction is a tuple of the `sender` identifier, the `receiver`
    identifier, and the `amount` to be transferred from sender to receiver ❶. The
    dictionary `balances` tracks a player’s current balance: a dictionary that maps
    a user identifier to the number of credits of that user based on the transactions
    so far ❷.'
  prefs: []
  type: TYPE_NORMAL
- en: The function `transfer(sender, receiver, amount)` creates and stores a new transaction
    in the global list, creates new balances for the `sender` and `receiver` if they
    don’t already exist, and updates the balances according to the given `amount`.
    The function `get_balance(user)` returns the balance of the user given as an argument,
    and `max_transaction()` goes over all transactions and returns the one that has
    the maximum value in the third tuple element, the transaction amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially all balances are zero. The application transfers 2,000 units from
    Alice to Bob ❸, 4,000 units from Bob to Carl ❹, and 2,000 units from Alice to
    Carl ❺. At this point, Alice owes 4,000 (with a negative balance of −4,000), Bob
    owes 2,000, and Carl has 6,000 units. After printing the maximum transaction,
    Alice transfers 1,000 units to Bob ❻, and Carl transfers 8,000 units to Alice
    ❼. Now, the accounts have changed: Alice has 3,000, Bob −1,000, and Carl −2,000
    units. In particular, the application returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: But Alice isn’t happy with the application. She realizes that calling `max_transaction()`
    results in redundant calculations—because the function is called twice, the script
    goes over the list `transactions` twice to find the transaction with the maximum
    amount. But when calculating `max_transaction()` the second time, it partially
    performs the same calculations again by going through all the transactions to
    find the maximum—including those for which it already knows the maximum, that
    is, the first three transactions ❸–❺. Alice correctly sees some *optimization
    potential* by introducing a new variable, `max_transaction`, that keeps track
    of the maximum transaction seen so far whenever a new transaction is created.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5-2](#listing5-2) shows the three lines of code Alice added to implement
    this change.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-2: Applied optimization to reduce redundant computations'
  prefs: []
  type: TYPE_NORMAL
- en: The variable `max_transaction` maintains the maximum transaction amount among
    all transactions seen so far. Thus, there’s no need for the maximum to be recomputed
    after every game night. Initially, you set the maximum transaction value to negative
    infinity so that the first real transaction will definitely be larger. Each time
    a new transaction is added, the program compares that new transaction to the current
    maximum, and if it is larger, the current transaction becomes the current maximum.
    Without the optimization, if you called the function `max_transaction()` 1,000
    times on a list of 1,000 transactions, you’d have to perform 1,000,000 comparisons
    to find 1,000 maxima because you traversed the list of 1,000 elements 1,000 times
    (1,000 * 1,000 = 1,000,000). With the optimization, you’d need to retrieve the
    currently stored value in `max_transaction` only once for each function call.
    As the list has 1,000 elements, you need at most 1,000 operations to maintain
    the current maximum. This leads to a reduction of three orders of magnitude in
    the number of operations needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many coders cannot resist implementing such optimizations, but their complexity
    adds up. In Alice’s case, she’ll soon have to keep track of a handful of additional
    variables to track additional statistics her friends might be interested in: `min_transaction`,
    `avg_transaction`, `median_transaction`, and `alice_max_transaction` (to track
    her own maximum transaction value). Each injects a few more lines of code into
    the project, increasing the likelihood a bug will appear. If Alice forgets to
    update a variable at the proper location, for instance, she’ll have to spend precious
    time fixing it. Even worse, she may miss the bug entirely, resulting in a corrupted
    balance of Alice’s account and a few hundreds of dollars of damage. Her friends
    might even suspect that Alice wrote the code in her favor! This final point may
    sound a little tongue-in-cheek, but in real-world cases, the stakes are higher.
    Second-order consequences can be even more severe than the more predictable first-order
    consequences of complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: All these potential problems could have been averted had Alice refrained from
    applying a *potential optimization* without fully thinking through whether this
    optimization was premature. The goal of the app is to broker one evening’s transaction
    between three friends. Realistically, there will be at most a few hundred transactions
    and a dozen calls to `max_transaction` rather than the thousands for which the
    optimized code is designed. Alice’s computer could’ve executed the unoptimized
    code within a split second, and neither Bob nor Carl would even have realized
    that the code was unoptimized. Plus, the unoptimized code is simpler and easier
    to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: However, suppose word gets around and a casino—which relies on high performance,
    scalability, and long-term transaction histories—contacts Alice to implement the
    same system. In that case, she can still fix the bottleneck of recomputing the
    maximum instead of tracking it quickly. But now she’d be certain that the additional
    code complexity was indeed a good investment. By avoiding optimization until the
    application requires it, she’d have saved herself dozens of those unnecessary
    premature optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: Six Tips for Performance Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alice’s story not only gave us a detailed picture of premature optimization
    in practice but also hinted at the proper way to successfully optimize. It’s important
    to remember that Donald Knuth did not argue that optimization *itself* is the
    root of all evil. Instead, the real problem is *premature* optimization. Now that
    Knuth’s quote has become quite popular, many mistakenly take it to be an argument
    against all optimization. When it comes at the right time, however, optimization
    can be critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rapid technological improvements in recent decades are largely due to optimizations:
    circuit placement on chips, algorithms, and software usability have been optimized
    continuously over time. Moore’s law states that the improvements in computer chip
    technology that make computing incredibly cheap and efficient will continue exponentially
    for a long time yet. Improvements in chip technology have significant potential,
    and they cannot be considered premature. If they create value for many, optimizations
    are at the heart of progress.'
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, you should optimize only if you have clear evidence—such
    as measurements from performance optimization tools—that the code part or function
    to be optimized is indeed one of the bottlenecks and that users of the application
    will appreciate or even demand a better performance. Optimizing the speed of starting
    the Windows operating system is not premature because it’ll directly benefit millions
    of users, whereas optimizing the scalability of your web application with an upper
    limit of 1,000 users per month, who are only requesting a static website, is premature.
    The costs of *developing* an application are not as high as the costs of thousands
    of users *using* it. If you can spend one hour of your time to save the users
    a few seconds, it’s usually a win! Your users’ time is more valuable than your
    own. This is why we use computers in the first place—to invest a few resources
    upfront and gain many more resources afterward. Optimization is not always premature.
    Sometimes, you must optimize in order to create a valuable product in the first
    place—why bother shipping an unoptimized product that doesn’t generate any value?
    Having seen several reasons to avoid premature optimization, we will now look
    at six performance tips to help you choose how and when to optimize your code.
  prefs: []
  type: TYPE_NORMAL
- en: Measure First, Improve Second
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Measure your software’s performance so you know where it can and should be improved.
    What you don’t measure can’t be improved, since you have no way to track your
    progress.
  prefs: []
  type: TYPE_NORMAL
- en: Premature optimization is often an optimization applied before you’ve even measured,
    which is the direct basis of the idea that premature optimization is the root
    of all evil. You should always optimize only after you have begun to measure the
    performance of your non-optimized code, like memory footprint or speed. This is
    your benchmark. There’s no point in trying to improve runtime, for example, if
    you don’t know what your original runtime is. There’s no way to tell if your “optimization”
    actually increases total runtime or results in no measurable effect unless you
    begin with a clear benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: As a general strategy for measuring performance, start with writing the most
    straightforward, naive code possible that’s also easy to read. You may call this
    your *prototype*, the *naive approach*, or the *MVP*. Document your measurements
    in a spreadsheet. This is your first benchmark. Create an alternative code solution
    and measure its performance against the benchmark. Once you’ve rigorously proven
    that your optimization improves your code performance, the new optimized code
    becomes your new benchmark, which all subsequent improvements should be able to
    beat. If an optimization doesn’t measurably improve your code, throw it away.
  prefs: []
  type: TYPE_NORMAL
- en: This way, you can track your code’s improvement over time. You can also document,
    prove, and defend an optimization to your boss, your peer group, or even the scientific
    community.
  prefs: []
  type: TYPE_NORMAL
- en: Pareto Is King
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The 80/20 principle, or *Pareto principle*, discussed in Chapter 2, also applies
    to performance optimization. Some features will take up considerably more resources,
    such as time and memory footprint, than others, so focusing on improving bottlenecks
    like these will help you effectively optimize your code.
  prefs: []
  type: TYPE_NORMAL
- en: To exemplify the high degree of imbalance of different processes running in
    parallel on my operating system, take a look at my current central processing
    unit (CPU) usage in [Figure 5-1](#figure5-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screenshot of author’s CPU usage showing that of all programs in use, only
    seven use more than one percent of CPU.](image_fi/502185c05/f05001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1: Unequal distribution of the CPU demand of different applications
    running on a Windows PC'
  prefs: []
  type: TYPE_NORMAL
- en: If you plot this in Python, you see a Pareto-like distribution, as shown in
    [Figure 5-2](#figure5-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph mapping the CPU usage, which shows only a few programs use a significant
    percentage of CPU.](image_fi/502185c05/f05002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-2: CPU usage of different applications on a Windows PC'
  prefs: []
  type: TYPE_NORMAL
- en: A small percentage of application code requires a significant percentage of
    CPU usage. If I want to reduce the CPU usage on my computer, I just need to close
    Cortana and Search and—voilà—a significant portion of the CPU load disappears,
    shown in [Figure 5-3](#figure5-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Same graph as Figure 5-2 with the top two CPU users removed; now “Explorer”
    and “systems” are the top two, but the CPU usage pattern is the same as in Figure
    5-2.](image_fi/502185c05/f05003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3: The results after “optimizing” a Windows system by closing not-needed
    applications'
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing the two most expensive tasks reduces the CPU load considerably, but
    notice that the new plot looks similar to the first at a glance: two tasks, this
    time Explorer and System, are still much more expensive than the rest. This demonstrates
    an important rule of performance tuning: performance optimization is fractal.
    As soon as you’ve removed one bottleneck, you’ll find another bottleneck lurking
    around. Bottlenecks will always be in any system, but if you repeatedly remove
    them as they appear, you’ll get maximal “bang for your buck.” In a practical code
    project, you’ll see the same distribution of a relatively small number of functions
    taking the majority of the resources (for example, CPU cycles). Often you can
    focus your optimization effort on the bottleneck function that takes the most
    resources, such as by rewriting it with more sophisticated algorithms or thinking
    about ways to avoid the computation (for example, caching of intermediate results).
    Of course, the next bottleneck will appear right after you’ve resolved the current
    one; that’s why you need to measure your code and decide when it’s time to stop
    optimizing. For example, it doesn’t make a lot of sense to improve the response
    time of a web application from 2 ms to 1 ms when the user wouldn’t perceive the
    difference anyway. Due to the fractal nature of optimizations and the Pareto principle
    (see Chapter 2), obtaining these small gains often requires a lot of effort and
    developer time and may yield little gain in terms of usability or application
    utility.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic Optimization Wins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Say you’ve decided your code needs a particular optimization because user feedback
    and statistics indicate that your application is too slow. You’ve measured your
    current speed in seconds or bytes and know the target speed you’re aiming for,
    and you’ve found your bottleneck. Your next step is to figure out how to overcome
    that bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Many bottlenecks can be resolved by tuning your *[algorithms and data structures](https://www.cs.bham.ac.uk/~jxb/DSA/dsa.pdf)?*.
    For example, imagine you’re developing a financial application. You know your
    bottleneck is the function `calculate_ROI()`, which goes over all combinations
    of potential buying and selling points to calculate the maximum profit. As this
    function is the bottleneck of the entire application, you want to find a better
    algorithm for it. After a bit of research, you find out about the *maximum profit
    algorithm*, a simple, powerful replacement that will speed up your computation
    significantly. The same research can be done on data structures causing bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce bottlenecks and optimize performance, ask yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: Can you find better algorithms that are already proven—for example, in books,
    research papers, or even Wikipedia?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you tweak existing algorithms for your specific problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you improve the data structures? Some common easy solutions include using
    [sets](https://blog.finxter.com/sets-in-python/) instead of lists (checking membership,
    for example, is much faster for sets than lists) or [dictionaries](https://blog.finxter.com/python-dictionary/)
    [](https://blog.finxter.com/python-dictionary/) instead of collections of tuples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spending time researching these questions pays off both for your application
    and for you. You’ll become a better computer scientist in the process.
  prefs: []
  type: TYPE_NORMAL
- en: All Hail the Cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once you’ve made any necessary changes based on the previous tips, you can
    move on to this quick and dirty trick for removing unnecessary computations: store
    the result of a subset of computations you have already performed in a cache.
    This trick works surprisingly well for a variety of applications. Before performing
    any new computation, you first check the cache to see if you’ve already done that
    computation. This is similar to how you approach simple calculations in your head—at
    a certain point, you don’t *actually* calculate 6 * 5 in your head but simply
    rely on your memory to give you the result right away. Consequently, caching makes
    sense only if the same type of intermediate calculations appears multiple times
    throughout your application. Fortunately, this holds for most real-world applications—for
    example, thousands of users may watch the same YouTube video in a given day, so
    caching it close to the user (rather than thousands of miles away in a distant
    data center) saves scarce network bandwidth resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore a short code example where caching results in significant performance
    benefits: the [Fibonacci algorithm](https://blog.finxter.com/fibonacci-in-one-line-python/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the result of repeatedly adding the last and second-last element
    of the series up to the 100th sequence element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This algorithm is slow because the functions `fib(n-1)` and `fib(n-2)` calculate
    more or less the same things. For instance, both separately calculate the `(n−3)`th
    Fibonacci element instead of reusing each other’s result for this computation.
    The redundancy adds up—even for this simple function call, the computation takes
    much too long.
  prefs: []
  type: TYPE_NORMAL
- en: One way to improve performance here is to create a cache. *Caching* allows you
    to store the results of previous computations, so in this case, `fib2(n-3)` is
    calculated only once, and when you need it again, you can instantly pull the result
    from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, we can make a simple cache by creating a dictionary where you associate
    each function input (as an input string, for instance) with the function output.
    You can then ask the cache to give you the computations you’ve already performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the caching variant of Python Fibonacci:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You store the result of `fib(n-1) + fib(n-2)` in the cache ❶. If you already
    have the `n`th Fibonacci number result, you pull it from the cache rather than
    recalculating it again and again. On my machine, this increases the speed by almost
    2,000 times when calculating the first 40 Fibonacci numbers!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two basic strategies for effective caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '****Perform computations in advance (“offline”) and store their results in
    the cache.****'
  prefs: []
  type: TYPE_NORMAL
- en: This is a great strategy for web applications where you can fill up a large
    cache once, or once a day, and then serve the result of your pre-computations
    to the users. For them, your calculations seem blazingly fast. Mapping services
    heavily use this trick to speed up the shortest path computations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '****Perform computations as they appear (“online”) and store their results
    in the cache.****'
  prefs: []
  type: TYPE_NORMAL
- en: An example is an online Bitcoin address checker that sums all incoming transactions
    and deducts all outgoing transactions to compute the balance of a given Bitcoin
    address. Once done, it could cache the intermediate results for this address to
    avoid recomputing the same transactions once the same user checks again. This
    reactive form is the most basic form of caching, where you don’t need to decide
    which computations to perform in advance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In both cases, the more computations you store, the higher the likelihood of
    *cache hits* where the relevant computation can be returned immediately. However,
    as there’s usually a memory limit on the number of cache entries you can save,
    you’ll need a sensible *[cache replacement policy](https://en.wikipedia.org/wiki/Cache_replacement_policies)*:
    as the cache has a limited size, it may fill up quickly. At that point, the cache
    can store a new value only by replacing an old value. A common replacement policy
    is *first in, first out (FIFO)*, which would replace the oldest cache entry with
    the new one. The best strategy depends on the concrete application, but FIFO is
    a good first bet.'
  prefs: []
  type: TYPE_NORMAL
- en: Less Is More
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Is your problem too hard to be solved efficiently? Make it easier! It sounds
    obvious, but so many coders are perfectionists. They accept colossal complexity
    and computational overhead just to implement a small feature that may not even
    get recognized by users. Instead of optimizing, it’s often much better to reduce
    complexity and get rid of unnecessary features and computations. Consider the
    problems faced by search engine developers, for example: “What is the perfect
    match for a given search query?” Finding the optimal solution for such a problem
    is extremely hard and involves searching billions of websites. However, search
    engines like Google don’t solve the problem optimally; rather, they do the best
    they can in the time they have by using heuristics. Instead of checking billions
    of websites against a user search query, they focus on a couple of high-probability
    bets by using rough heuristics to estimate the quality of individual websites
    (such as the famous PageRank algorithm) and consult suboptimal websites if no
    other high-quality website answers the query. You too should use heuristics rather
    than optimal algorithms in most cases. Ask yourself the following questions: What
    is your current bottleneck calculating? Why does it exist? Is it worth the effort
    to solve the problem anyway? Can you remove the feature or offer a downsized version?
    If the feature is used by 1 percent of your users, but 100 percent perceive the
    increased latency, it may be time for some minimalism (removing the feature that
    is hardly used but provides a bad experience to those who use it).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify your code, think about whether it makes sense to do one of the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove your current bottleneck altogether by just skipping the feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplify the problem by replacing it with a simpler version of the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get rid of 1 expensive feature to add 10 cheap ones, in accordance with the
    80/20 policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Omit one important feature so that you can pursue an even more important one;
    think about opportunity costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know When to Stop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performance optimization can be one of the most time-consuming aspects of coding.
    There’s always room for improvement, but your effort needed to improve performance
    tends to increase once you have already exhausted the low-hanging fruit techniques.
    At some point, improving performance is just a waste of your time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ask yourself regularly: Is it worth the effort to keep optimizing? The answer
    can usually be found by studying the users of your application. What performance
    do they need? Do they even perceive the difference between the original and the
    optimized version of the application? Do some of them complain about bad performance?
    Answering these questions will give you a rough estimate of the maximum runtime
    of your application. Now, you can start optimizing bottlenecks until you reach
    this threshold. Then stop.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned why it’s important to avoid premature optimization.
    An optimization is premature if it takes more value than it adds. Depending on
    the project, value can often be measured in terms of developer time, usability
    metrics, expected revenue of an app or feature, or its utility for a subgroup
    of users. For instance, if an optimization can save time or money for thousands
    of users, it is likely not premature, even if you must spend significant developer
    resources to optimize the codebase. However, if the optimization cannot lead to
    perceptible differences in the quality of the lives of the users or programmers,
    it most likely is premature. Yes, there are many more advanced models on the software-engineering
    process, but common sense and a general awareness of the dangers of premature
    optimization go a long way without you needing to study fancy books or research
    papers on software development models. For instance, a useful rule of thumb is
    to write readable and clean code to start with and not care too much about performance,
    then optimize the parts with a high expected value based on experience, hard facts
    from performance-tracking tools, and real-world results from user studies.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll learn about the concept of flow—a programmer’s best
    friend.*
  prefs: []
  type: TYPE_NORMAL
