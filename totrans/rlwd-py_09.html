<html><head></head><body>
<h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_203"/><span class="big">9</span><br/>IDENTIFYING FRIEND OR FOE</h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>&#13;
<p class="noindent">Face detection is a machine learning technology that locates human faces in digital images. It’s the first step in the process of face <em>recognition</em>, a technique for identifying individual faces using code. Face detection and recognition methods have broad applications, such as tagging photographs on social media, autofocusing digital cameras, unlocking cell phones, finding missing children, tracking terrorists, facilitating secure payments, and more.</p>&#13;
<p class="indent">In this chapter, you’ll use machine learning algorithms in OpenCV to program a robot sentry gun. Because you’ll be distinguishing between humans and otherworldly mutants, you’ll only need to detect the <em>presence</em> of human faces rather than identify specific individuals. In <a href="ch10.xhtml">Chapter 10</a>, you’ll take the next step and identify people by their faces.</p>&#13;
<h3 class="h3" id="ch00lev1sec74"><strong>Detecting Faces in Photographs</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_204"/>Face detection is possible because human faces share similar patterns. Some common facial patterns are the eyes being darker than the cheeks and the bridge of the nose being brighter than the eyes, as seen in the left image of <a href="ch09.xhtml#ch09fig1">Figure 9-1</a>.</p>&#13;
<div class="image"><img src="../images/fig09_01.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig1"/>Figure 9-1: Example of some consistently bright and dark regions in a face</p>&#13;
<p class="indent">You can extract these patterns using templates like those in <a href="ch09.xhtml#ch09fig2">Figure 9-2</a>. These yield <em>Haar features</em>, a fancy name for the attributes of digital images used in object recognition. To calculate a Haar feature, place one of the templates on a grayscale image, add up the grayscale pixels that overlap with the white part, and subtract them from the sum of the pixels that overlap the black part. Thus, each feature consists of a single intensity value. We can use a range of template sizes to sample all possible locations on the image, making the system scale invariant.</p>&#13;
<div class="image"><img src="../images/fig09_02.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig2"/>Figure 9-2: Some example Haar feature templates</p>&#13;
<p class="indent">In the middle image in <a href="ch09.xhtml#ch09fig1">Figure 9-1</a>, an “edge feature” template extracts the relationship between the dark eyes and the bright cheeks. In the far-right image in <a href="ch09.xhtml#ch09fig1">Figure 9-1</a>, a “line feature” template extracts the relationship between the dark eyes and the bright nose.</p>&#13;
<p class="indent">By calculating Haar features on thousands of <em>known</em> face and nonface images, we can determine which combination of Haar features is most effective at identifying faces. This training process is slow, but it facilitates fast detection later. The resulting algorithm, known as a <em>face classifier</em>, takes the values of features in an image and predicts whether it contains a human face by outputting 1 or 0. OpenCV ships with a pretrained face detection classifier based on this technique.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_205"/>To apply the classifier, the algorithm uses a <em>sliding window</em> approach. A small rectangular area is incrementally moved across the image and evaluated using a <em>cascade classifier</em> consisting of multiple stages of filters. The filters at each stage are combinations of Haar features. If the window region fails to pass the threshold of a stage, it’s rejected, and the window slides to the next position. Quickly rejecting nonface regions, like the one shown in the right inset in <a href="ch09.xhtml#ch09fig3">Figure 9-3</a>, helps speed up the overall process.</p>&#13;
<div class="image"><img src="../images/fig09_03.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig3"/>Figure 9-3: Images are searched for faces using a rectangular sliding window.</p>&#13;
<p class="indent">If a region passes the threshold for a stage, the algorithm processes another set of Haar features and compares them to the threshold, and so on, until it either rejects or positively identifies a face. This causes the sliding window to speed up or slow down as it moves across an image. You can find a fantastic video example of this at <em><a href="https://vimeo.com/12774628/">https://vimeo.com/12774628/</a></em>.</p>&#13;
<p class="indent">For each face detected, the algorithm returns the coordinates of a rectangle around the face. You can use these rectangles as the basis for further analysis, such as identifying eyes.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec75"><strong>Project #13: Programming a Robot Sentry Gun</strong></h3>&#13;
<p class="noindent">Imagine that you’re a technician with the Coalition Marines, a branch of the Space Force. Your squad has been deployed to a secret research base operated by the Wykham-Yutasaki Corporation on planet LV-666. While studying a mysterious alien apparatus, the researchers have inadvertently opened a portal to a hellish alternate dimension. Anyone who gets near the portal, including dozens of civilians and several of your comrades, mutates into a murderous mindless monstrosity! You’ve even caught security footage of the result (<a href="ch09.xhtml#ch09fig4">Figure 9-4</a>).</p>&#13;
<div class="image"><img src="../images/fig09_04.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig4"/>Figure 9-4: Security camera footage of a mutated scientist (left) and marine (right)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_206"/>According to the remaining scientists, the mutation affects more than just organic matter. Any gear the victim is wearing, such as helmets and goggles, is also transmogrified and fused into the flesh. Eye tissue is especially vulnerable. All the mutants formed so far are eyeless and blind, though this doesn’t seem to affect their mobility. They’re still ferocious, deadly, and unstoppable without military-grade weapons.</p>&#13;
<p class="indent">That’s where you come in. It’s your job to set up an automatic firing station to guard Corridor 5, a key access point in the compromised facility. Without it, your small squad is in danger of being outflanked and overrun by hordes of rampaging mutants.</p>&#13;
<p class="indent">The firing station consists of a UAC 549-B automated sentry gun, called a <em>robot sentry</em> by the grunts (<a href="ch09.xhtml#ch09fig5">Figure 9-5</a>). It’s equipped with four M30 autocannons with 1,000 rounds of ammo and multiple sensors, including a motion detector, laser ranging unit, and optical camera. The gun also interrogates targets using an identification friend or foe (IFF) transponder. All Coalition Marines carry these transponders, allowing them to safely pass active sentry guns.</p>&#13;
<div class="image"><img src="../images/fig09_05.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig5"/>Figure 9-5: A UAC 549-B automated sentry gun</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_207"/>Unfortunately, the squad’s sentry gun was damaged during landing, so the transponders no longer function. Worse, the requisitions corporal forgot to download the software that visually interrogates targets. With the transponder sensor down, there’s no way to positively identify marines and civilians. You’ll need to get this fixed as quickly as possible, because your buddies are badly outnumbered and the mutants are on the move!</p>&#13;
<p class="indent">Fortunately, planet LV-666 has no indigenous life forms, so you need to distinguish between humans and mutants only. Since the mutants are basically faceless, a face detection algorithm is the logical solution.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that disables the sentry gun’s firing mechanism when it detects human faces in an image.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec45"><strong><em>The Strategy</em></strong></h4>&#13;
<p class="noindent">In situations like this, it’s best to keep things simple and leverage existing resources. This means relying on OpenCV’s face detection functionality rather than writing customized code to recognize the humans on the base. But you can’t be sure how well these canned procedures will work, so you’ll need to guide your human targets to make the job as easy as possible.</p>&#13;
<p class="indent">The sentry gun’s motion detector will handle the job of triggering the optical identification process. To permit humans to pass unharmed, you’ll need to warn them to stop and face the camera. They’ll need a few seconds to do this and a few seconds to proceed past the gun after they’re cleared.</p>&#13;
<p class="indent">You’ll also want to run some tests to ensure OpenCV’s training set is adequate and you’re not generating any false positives that would let a mutant sneak by. You don’t want to kill anyone with friendly fire, but you can’t be too cautious, either. If one mutant gets by, everyone could perish.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>In real life, the sentry guns would use a video feed. Since I don’t have my own film studio with special effects and makeup departments, you’ll work off still photos instead. You can think of these as individual video frames. Later in the chapter, you’ll get a chance to detect your own face using your computer’s video camera.</em></p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec46"><strong><em>The Code</em></strong></h4>&#13;
<p class="noindent">The <em>sentry.py</em> code will loop through a folder of images, identify human faces in the images, and show the image with the faces outlined. It will then either fire or disable the gun depending on the result. You’ll use the images in the <em>corridor_5</em> folder in the <em>Chapter_9</em>  folder, downloadable from <em><a href="https://nostarch.com/real-world-python/">https://nostarch.com/real-world-python/</a></em>. As always, don’t move or rename any files after downloading and launch <em>sentry.py</em> from the folder in which it’s stored.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_208"/>You’ll also need to install two modules, <span class="literal">playsound</span> and <span class="literal">pyttsx3</span>. The first is a cross-platform module for playing WAV and MP3 format audio files. You’ll use it to produce sound effects, such as machine gun fire and an “all clear” tone. The second is a cross-platform wrapper that supports the native text-to-speech libraries on Windows and Linux-based systems, including macOS. The sentry gun will use this to issue audio warnings and instructions. Unlike other text-to-speech libraries, <span class="literal">pyttsx3</span> reads text directly from the program, rather than first saving it to an audio file. It also works offline, making it reliable for voice-based projects.</p>&#13;
<p class="indent">You can install both modules with pip in a PowerShell or Terminal window.</p>&#13;
<pre>pip install playsound&#13;
pip install pyttsx3</pre>&#13;
<p class="indent">If you encounter an error installing <span class="literal">pyttsx3</span> on Windows, such as <span class="literal">No module</span> <span class="literal">named win32.com.client</span>, <span class="literal">No module named win32</span>, or <span class="literal">No module named win32api</span>, then install <span class="literal">pypiwin32</span>.</p>&#13;
<pre>pip install pypiwin32</pre>&#13;
<p class="indent">You may need to restart the Python shell and editor following this installation.</p>&#13;
<p class="indent">For more on <span class="literal">playsound</span>, see <em><a href="https://pypi.org/project/playsound/">https://pypi.org/project/playsound/</a></em>. The documentation for <span class="literal">pyttsx3</span> can be found at <em><a href="https://pyttsx3.readthedocs.io/en/latest/">https://pyttsx3.readthedocs.io/en/latest/</a></em> and <em><a href="https://pypi.org/project/pyttsx3/">https://pypi.org/project/pyttsx3/</a></em>.</p>&#13;
<p class="indent">If you don’t already have OpenCV installed, see “Installing the Python Libraries” on <a href="ch01.xhtml#page_6">page 6</a>.</p>&#13;
<h5 class="h5"><strong>Importing Modules, Setting Up Audio, and Referencing the Classifier Files and Corridor Images</strong></h5>&#13;
<p class="noindent"><a href="ch09.xhtml#ch09list1">Listing 9-1</a> imports modules, initializes and sets up the audio engine, assigns the classifier files to variables, and changes the directory to the folder containing the corridor images.</p>&#13;
<pre><span class="codeitalic1">sentry.py</span>, part 1&#13;
   import os&#13;
   import time&#13;
<span class="ent">➊</span> from datetime import datetime&#13;
   from playsound import playsound&#13;
   import pyttsx3&#13;
   import cv2 as cv&#13;
&#13;
<span class="ent">➋</span> engine = pyttsx3.init()&#13;
   engine.setProperty('rate', 145)  &#13;
   engine.setProperty('volume', 1.0) &#13;
&#13;
   root_dir = os.path.abspath('.')&#13;
   gunfire_path = os.path.join(root_dir, 'gunfire.wav')&#13;
   tone_path = os.path.join(root_dir, 'tone.wav')&#13;
&#13;
<span class="ent">➌</span> path= "C:/Python372/Lib/site-packages/cv2/data/"&#13;
   face_cascade = cv.CascadeClassifier(path + &#13;
                                       'haarcascade_frontalface_default.xml')&#13;
   <span epub:type="pagebreak" id="page_209"/>eye_cascade = cv.CascadeClassifier(path + 'haarcascade_eye.xml')&#13;
&#13;
<span class="ent">➍</span> os.chdir('corridor_5')&#13;
   contents = sorted(os.listdir())</pre>&#13;
<p class="listing"><a id="ch09list1"/>Listing 9-1: Importing modules, setting up the audio, and locating the classifier files and corridor images</p>&#13;
<p class="indent">Except for <span class="literal">datetime</span>, <span class="literal">playsound</span>, and <span class="literal">pytts3</span>, the imports should be familiar to you if you’ve worked through the earlier chapters <span class="ent">➊</span>. You’ll use <span class="literal">datetime</span> to record the exact time at which an intruder is detected in the corridor.</p>&#13;
<p class="indent">To use <span class="literal">pytts3</span>, initialize a <span class="literal">pyttsx3</span> object and assign it to a variable named, by convention, <span class="literal">engine</span> <span class="ent">➋</span>. According to the <span class="literal">pyttsx3</span> docs, an application uses the <span class="literal">engine</span> object to register and unregister event callbacks, produce and stop speech, get and set speech engine properties, and start and stop event loops.</p>&#13;
<p class="indent">In the next two lines, set the rate of speech and volume properties. The rate of speech value used here was obtained through trial and error. It should be fast but still clearly understandable. The volume should be set to the maximum value (<span class="literal">1.0</span>) so any humans stumbling into the corridor can easily hear the warning instructions.</p>&#13;
<p class="indent">The default voice on Windows is male, but other voices are available. For example, on a Windows 10 machine, you can switch to a female voice using the following voice ID:</p>&#13;
<pre>engine.setProperty('voice',&#13;
'HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Speech\Voices\Tokens\TTS_MS_EN-US_ZIRA_11.0')</pre>&#13;
<p class="indent">To see a list of voices available on your platform, refer to “Changing voices” at <em><a href="https://pyttsx3.readthedocs.io/en/latest/">https://pyttsx3.readthedocs.io/en/latest/</a></em>.</p>&#13;
<p class="indent">Next, set up the audio recording of gunfire, which you’ll play when a mutant is detected in the corridor. Specify the location of the audio file by generating a directory path string that will work on all platforms, which you do by combining the absolute path with the filename using the <span class="literal">os.path.join()</span> method. Use the same path for the <em>tone.wav</em>  file, which you’ll use as an “all clear” signal when the program identifies a human.</p>&#13;
<p class="indent">The pretrained Haar cascade classifiers should download as <em>.xml</em> files when you install OpenCV. Assign the path for the folder containing the classifiers to a variable <span class="ent">➌</span>. The path shown is for my Windows machine; your path may be different. On macOS, for example, you may find them under <em>opencv/data/haarcascades</em>. You can also find them online at <em><a href="https://github.com/opencv/opencv/tree/master/data/haarcascades/">https://github.com/opencv/opencv/tree/master/data/haarcascades/</a></em>.</p>&#13;
<p class="indent">Another option for finding the path to the cascade classifiers is to use the preinstalled <span class="literal">sysconfig</span> module, as in the following snippet:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import sysconfig</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">path = sysconfig.get_paths()['purelib'] + '/cv2/data'</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">path</span>&#13;
'C:\\Python372\\Lib\\site-packages/cv2/data'</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_210"/>This should work for Windows inside and outside of virtual environments. However, this will work on Ubuntu only within a virtual environment.</p>&#13;
<p class="indent">To load a classifier, use OpenCV’s <span class="literal">CascadeClassifier()</span> method. Use string concatenation to add the path variable to the filename string for the classifier and assign the result to a variable.</p>&#13;
<p class="indent">Note that I use only two classifiers, one for frontal faces and one for eyes, to keep things simple. Additional classifiers are available for profiles, smiles, eyeglasses, upper bodies, and so on.</p>&#13;
<p class="indent">Finish by pointing the program to the images taken in the corridor you are guarding. Change the directory to the proper folder <span class="ent">➍</span>; then list the folder contents and assign the results to a <span class="literal">contents</span> variable. Because you’re not providing a full path to the folder, you’ll need to launch your program from the folder containing it, which should be one level above the folder with the images.</p>&#13;
<h5 class="h5"><strong>Issuing a Warning, Loading Images, and Detecting Faces</strong></h5>&#13;
<p class="noindent"><a href="ch09.xhtml#ch09list2">Listing 9-2</a> starts a <span class="literal">for</span> loop to iterate through the folder containing the corridor images. In real life, the motion detectors on the sentry guns would launch your program as soon as something entered the corridor. Since we don’t have any motion detectors, we’ll assume that each loop represents the arrival of a new intruder.</p>&#13;
<p class="indent">The loop immediately arms the gun and prepares it to fire. It then verbally requests that the intruder stop and face the camera. This would occur at a set distance from the gun, as determined by the motion detector. As a result, you know the faces will all be roughly the same size, making it easy to test the program.</p>&#13;
<p class="indent">The intruder is given a few seconds to comply with the command. After that, the cascade classifier is called and used to search for faces.</p>&#13;
<pre><span class="codeitalic1">sentry.py</span>, part 2 &#13;
for image in contents:&#13;
 <span class="ent">➊</span> print(f"\nMotion detected...{datetime.now()}")&#13;
    discharge_weapon = True&#13;
 <span class="ent">➋</span> engine.say("You have entered an active fire zone. \&#13;
                Stop and face the gun immediately. \&#13;
                When you hear the tone, you have 5 seconds to pass.")&#13;
    engine.runAndWait()&#13;
    time.sleep(3)&#13;
    &#13;
 <span class="ent">➌</span> img_gray = cv.imread(image, cv.IMREAD_GRAYSCALE)&#13;
    height, width = img_gray.shape&#13;
    cv.imshow(f'Motion detected {image}', img_gray)&#13;
    cv.waitKey(2000)&#13;
    cv.destroyWindow(f'Motion detected {image}')&#13;
&#13;
 <span class="ent">➍</span> face_rect_list = []  &#13;
    face_rect_list.append(face_cascade.detectMultiScale(image=img_gray,&#13;
                                                        scaleFactor=1.1,&#13;
                                                        minNeighbors=5))</pre>&#13;
<p class="listing"><a id="ch09list2"/>Listing 9-2: Looping through images, issuing a verbal warning, and searching for faces</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_211"/>Start looping through the images in the folder. Each new image represents a new intruder in the corridor. Print a log of the event and the time at which it occurred <span class="ent">➊</span>. Note the <span class="literal">f</span> before the start of the string. This is the new <em>f-string</em> format introduced with Python 3.6 (<em><a href="https://www.python.org/dev/peps/pep-0498/">https://www.python.org/dev/peps/pep-0498/</a></em>). An f-string is a literal string that contains expressions, such as variables, strings, mathematical operations, and even function calls, inside curly braces. When the program prints the string, it replaces the expressions with their values. These are the fastest and most efficient string formats in Python, and we certainly want this program to be fast!</p>&#13;
<p class="indent">Assume every intruder is a mutant and prepare to discharge the weapon. Then, verbally warn the intruder to stop and be scanned.</p>&#13;
<p class="indent">Use the <span class="literal">pyttsx3</span> <span class="literal">engine</span> object’s <span class="literal">say()</span> method to speak <span class="ent">➋</span>. It takes a string as an argument. Follow this with the <span class="literal">runAndWait()</span> method. This halts program execution, flushes the <span class="literal">say()</span> queue, and plays the audio.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>For some macOS users, the program may exit with the second call to <span class="codeitalic">runAndWait()</span><span class="normal">.</span> If this occurs, download the <span class="normal">sentry_for_Mac_bug.py</span> code from the book’s website. This program uses the operating system’s text-to-speech functionality in place of <span class="codeitalic">pyttsx3</span>. You’ll still need to update the Haar cascade path variable in this program, as you did at <span class="ent">➌</span> in <a href="ch09.xhtml#ch09list1">Listing 9-1</a>.</em></p>&#13;
</div>&#13;
<p class="indent">Next, use the <span class="literal">time</span> module to pause the program for three seconds. This gives the intruder time to squarely face the gun’s camera.</p>&#13;
<p class="indent">At this point, you’d make a video capture, except we’re not using video. Instead, load the images in the <em>corridor_5</em> folder. Call the <span class="literal">cv.imread()</span> method with the <span class="literal">IMREAD_GRAYSCALE</span> flag <span class="ent">➌</span>.</p>&#13;
<p class="indent">Use the image’s <span class="literal">shape</span> attribute to get its height and width in pixels. This will come in handy later, when you post text on the images.</p>&#13;
<p class="indent">Face detection works only on grayscale images, but OpenCV will convert color images behind the scenes when applying the Haar cascades. I chose to use grayscale from the start as the results look creepier when the images display. If you want to see the images in color, just change the two previous lines as follows:</p>&#13;
<pre>    img_gray = cv.imread(image)&#13;
    height, width = img_gray.shape[:2]</pre>&#13;
<p class="indent">Next, show the image prior to face detection, keep it up for two seconds (input as milliseconds), and then destroy the window. This is for quality control to be sure all the images are being examined. You can comment out these steps later, after you’re satisfied everything is working as planned.</p>&#13;
<p class="indent">Create an empty list to hold any faces found in the current image <span class="ent">➍</span>. OpenCV treats images as <span class="literal">NumPy</span> arrays, so the items in this list are the corner- point coordinates (<em>x</em>, <em>y</em>, width, height) of a rectangle that frames the face, as shown in the following output snippet:</p>&#13;
<pre>[array([[383, 169,  54,  54]], dtype=int32)]</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_212"/>Now it’s time to detect faces using the Haar cascades. Do this for the <span class="literal">face_cascade</span> variable by calling the <span class="literal">detectMultiscale()</span> method. Pass the method the image and values for the scale factor and minimum number of neighbors. These can be used to tune the results in the event of false positives or failure to recognize faces.</p>&#13;
<p class="indent">For good results, the faces in an image should be the same size as the ones used to train the classifier. To ensure they are, the <span class="literal">scaleFactor</span> parameter rescales the original image to the correct size using a technique called a <em>scale pyramid</em> (<a href="ch09.xhtml#ch09fig6">Figure 9-6</a>).</p>&#13;
<div class="image"><img src="../images/fig09_06.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig6"/>Figure 9-6: Example “scale pyramid”</p>&#13;
<p class="indent">The scale pyramid resizes the image downward a set number of times. For example, a <span class="literal">scaleFactor</span> of 1.2 means the image will be scaled down in increments of 20 percent. The sliding window will repeat its movement across this smaller image and check again for Haar features. This shrinking and sliding will continue until the scaled image reaches the size of the images used for training. This is 20×20 pixels for the Haar cascade classifier (you can confirm this by opening one of the <em>.xml</em> files). Windows smaller than this can’t be detected, so the resizing ends at this point. Note that the scale pyramid will only <em>downscale</em> images, as upscaling can introduce artifacts in the resized image.</p>&#13;
<p class="indent">With each rescaling, the algorithm calculates lots of new Haar features, resulting in lots of false positives. To weed these out, use the <span class="literal">minNeighbors</span> parameter.</p>&#13;
<p class="indent">To see how this process works, look at <a href="ch09.xhtml#ch09fig7">Figure 9-7</a>. The rectangles in this figure represent faces detected by the <span class="literal">haarcascade_frontalface_alt2.xml</span> classifier, with the <span class="literal">scaleFactor</span> parameter set to 1.05 and <span class="literal">minNeighbors</span> set to 0. The rectangles have different sizes depending on which scaled image—determined by the <span class="literal">scaleFactor</span> parameter—was in use when the face was detected. Although there are many false positives, the rectangles tend to cluster around the true face.</p>&#13;
<div class="image"><img src="../images/fig09_07.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig7"/>Figure 9-7: Detected face rectangles with <span class="literal">minNeighbors=0</span></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_213"/>Increasing the value of the <span class="literal">minNeighbors</span> parameter will increase the quality of the detections but reduce their number. If you specify a value of 1, only rectangles with one or more closely neighboring rectangles are preserved, and all others are discarded (<a href="ch09.xhtml#ch09fig8">Figure 9-8</a>).</p>&#13;
<div class="image"><img src="../images/fig09_08.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig8"/>Figure 9-8: Detected face rectangles with <span class="literal">minNeighbors=1</span></p>&#13;
<p class="indent">Increasing the number of minimum neighbors to around five generally removes the false positives (<a href="ch09.xhtml#ch09fig9">Figure 9-9</a>). This may be good enough for most applications, but dealing with terrifying interdimensional monstrosities demands more rigor.</p>&#13;
<div class="image"><img src="../images/fig09_09.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig9"/>Figure 9-9: Detected face rectangles with <span class="literal">minNeighbors=5</span></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_214"/>To see why, check out <a href="ch09.xhtml#ch09fig10">Figure 9-10</a>. Despite using a <span class="literal">minNeighbor</span> value of 5, the toe region of the mutant is incorrectly identified as a face. With a little imagination, you can see two dark eyes and a bright nose at the top of the rectangle, and a dark, straight mouth at the base. This could allow the mutant to pass unharmed, earning you a dishonorable discharge at best and an excruciatingly painful death at worst.</p>&#13;
<div class="image"><img src="../images/fig09_10.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig10"/>Figure 9-10: A mutant’s right toe region incorrectly identified as a face</p>&#13;
<p class="indent">Fortunately, this problem can be easily remedied. The solution is to search for more than just faces.</p>&#13;
<h5 class="h5"><strong>Detecting Eyes and Disabling the Weapon</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_215"/>Still in the <span class="literal">for</span> loop through the corridor images, <a href="ch09.xhtml#ch09list3">Listing 9-3</a> uses OpenCV’s built-in eye cascade classifier to search for eyes in the list of detected face rectangles. Searching for eyes reduces false positives by adding a second verification step. And because mutants don’t have eyes, if at least one eye is found, you can assume a human is present and disable the sentry gun’s firing mechanism to let them pass.</p>&#13;
<pre><span class="codeitalic1">sentry.py</span>, part 3 &#13;
    print(f"Searching {image} for eyes.")&#13;
    for rect in face_rect_list:&#13;
        for (x, y, w, h) in rect:&#13;
         <span class="ent">➊</span> rect_4_eyes = img_gray[y:y+h, x:x+w]&#13;
            eyes = eye_cascade.detectMultiScale(image=rect_4_eyes, &#13;
                                                scaleFactor=1.05,&#13;
                                                minNeighbors=2)&#13;
         <span class="ent">➋</span> for (xe, ye, we, he) in eyes:&#13;
                print("Eyes detected.")&#13;
                center = (int(xe + 0.5 * we), int(ye + 0.5 * he))&#13;
                radius = int((we + he) / 3)&#13;
                cv.circle(rect_4_eyes, center, radius, 255, 2)&#13;
                cv.rectangle(img_gray, (x, y), (x+w, y+h), (255, 255, 255), 2)&#13;
             <span class="ent">➌</span> discharge_weapon = False&#13;
                break</pre>&#13;
<p class="listing"><a id="ch09list3"/>Listing 9-3: Detecting eyes in face rectangles and disabling the weapon</p>&#13;
<p class="indent">Print the name of the image being searched and start a loop through the rectangles in the <span class="literal">face_rect_list</span>. If a rectangle is present, start looping through the tuple of coordinates. Use these coordinates to make a subarray from the image, in which you’ll search for eyes <span class="ent">➊</span>.</p>&#13;
<p class="indent">Call the eye cascade classifier on the subarray. Because you’re now searching a much smaller area, you can reduce the <span class="literal">minNeighbors</span> argument.</p>&#13;
<p class="indent">Like the cascade classifiers for faces, the eye cascade returns coordinates for a rectangle. Start a loop through these coordinates, naming them with an <span class="literal">e</span> on the end, which stands for “eye,” to distinguish them from the face rectangle coordinates <span class="ent">➋</span>.</p>&#13;
<p class="indent">Next, draw a circle around the first eye you find. This is just for your own visual confirmation; as far as the algorithm’s concerned, the eye is already found. Calculate the center of the rectangle and then calculate a radius value that’s slightly larger than an eye. Use OpenCV’s <span class="literal">circle()</span> method to draw a white circle on the <span class="literal">rect_4_eyes</span> subarray.</p>&#13;
<p class="indent">Now, draw a rectangle around the face by calling OpenCV’s <span class="literal">rectangle()</span> method and passing it the <span class="literal">img_gray</span> array. Show the image for two seconds and then destroy the window. Because the <span class="literal">rect_4_eyes</span> subarray is part of <span class="literal">img_gray</span>, the circle will show up even though you didn’t explicitly pass the subarray to the <span class="literal">im_show()</span> method (<a href="ch09.xhtml#ch09fig11">Figure 9-11</a>).</p>&#13;
<div class="image"><img src="../images/fig09_11.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig11"/>Figure 9-11: Face rectangle and eye circle</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_216"/>With a human identified, disable the weapon <span class="ent">➌</span> and break out of the <span class="literal">for</span> loop. You need to identify only one eye to confirm that you have a face, so it’s time to move on to the next face rectangle.</p>&#13;
<h5 class="h5"><strong>Passing the Intruder or Discharging the Weapon</strong></h5>&#13;
<p class="noindent">Still in the <span class="literal">for</span> loop through the corridor images, <a href="ch09.xhtml#ch09list4">Listing 9-4</a> determines what happens if the weapon is disabled or if it’s allowed to fire. In the disabled case, it shows the image with the detected face and plays the “all clear” tone. Otherwise, it shows the image and plays the gunfire audio file.</p>&#13;
<pre><span class="codeitalic1">sentry.py</span>, part 4&#13;
    if discharge_weapon == False:&#13;
        playsound(tone_path, block=False)    &#13;
        cv.imshow('Detected Faces', img_gray)&#13;
        cv.waitKey(2000)&#13;
        cv.destroyWindow('Detected Faces')&#13;
        time.sleep(5)&#13;
&#13;
    else:&#13;
        print(f"No face in {image}. Discharging weapon!")&#13;
        cv.putText(img_gray, 'FIRE!', (int(width / 2) - 20, int(height / 2)),&#13;
                                       cv.FONT_HERSHEY_PLAIN, 3, 255, 3)&#13;
        playsound(gunfire_path, block=False)&#13;
        cv.imshow('Mutant', img_gray)&#13;
        cv.waitKey(2000)&#13;
        cv.destroyWindow('Mutant')&#13;
        time.sleep(3)&#13;
&#13;
engine.stop()</pre>&#13;
<p class="listing"><a id="ch09list4"/>Listing 9-4: Determining the course of action if the gun is disabled or enabled</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_217"/>Use a conditional to check whether the weapon is disabled. You set the <span class="literal">discharge_weapon</span> variable to <span class="literal">True</span> when you chose the current image from the <em>corridor_5</em> folder (see <a href="ch09.xhtml#ch09list2">Listing 9-2</a>). If the previous listing found an eye in a face rectangle, it changed the state to <span class="literal">False</span>.</p>&#13;
<p class="indent">If the weapon is disabled, show the positive detection image (such as in <a href="ch09.xhtml#ch09fig11">Figure 9-11</a>) and play the tone. First, call <span class="literal">playsound</span>, pass it the <span class="literal">tone_path</span> string, and set the <span class="literal">block</span> argument to <span class="literal">False</span>. By setting <span class="literal">block</span> to <span class="literal">False</span>, you allow <span class="literal">playsound</span> to run at the same time as OpenCV displays the image. If you set <span class="literal">block=True</span>, you won’t see the image until <em>after</em> the tone audio has completed. Show the image for two seconds and then destroy it and pause the program for five seconds using <span class="literal">time.sleep()</span>.</p>&#13;
<p class="indent">If <span class="literal">discharge_weapon</span> is still <span class="literal">True</span>, print a message to the shell that the gun is firing. Use OpenCV’s <span class="literal">putText()</span> method to announce this in the center of the image and then show the image (see <a href="ch09.xhtml#ch09fig12">Figure 9-12</a>).</p>&#13;
<div class="image"><img src="../images/fig09_12.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig12"/>Figure 9-12: Example mutant window</p>&#13;
<p class="indent">Now play the gunfire audio. Use <span class="literal">playsound</span>, passing it the <span class="literal">gunfire_path</span> string and setting the <span class="literal">block</span> argument to <span class="literal">False</span>. Note that you have the option of removing the <span class="literal">root_dir</span> and <span class="literal">gunfire_path</span> lines of code in <a href="ch09.xhtml#ch09list1">Listing 9-1</a> if you provide the full path when you call <span class="literal">playsound</span>. For example, I would use the following on my Windows machine:</p>&#13;
<pre>playsound('C:/Python372/book/mutants/gunfire.wav', block=False)</pre>&#13;
<p class="indent">Show the window for two seconds and then destroy it. Sleep the program for three seconds to pause between showing the mutant and displaying the next image in the <em>corridor_5</em> folder. When the loop completes, stop the <span class="literal">pyttsx3</span> engine.</p>&#13;
<h4 class="h4" id="ch00lev2sec47"><strong><em>Results</em></strong></h4>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_218"/>Your <em>sentry.py</em> program repaired the damage to the sentry gun and allowed it to function without the need for transponders. It’s biased to preserve human life, however, which could lead to disastrous consequences: if a mutant enters the corridor at around the same time as a human, the mutant could slip by the defenses (<a href="ch09.xhtml#ch09fig13">Figure 9-13</a>).</p>&#13;
<div class="image"><img src="../images/fig09_13.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig13"/>Figure 9-13: A worst-case scenario. Say “Cheese!”</p>&#13;
<p class="indent">Mutants might also trigger the firing mechanism with humans in the corridor, assuming the humans look away from the camera at the wrong moment (<a href="ch09.xhtml#ch09fig14">Figure 9-14</a>).</p>&#13;
<div class="image"><img src="../images/fig09_14.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig14"/>Figure 9-14: You had <span class="normal">one</span> job!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_219"/>I’ve seen enough sci-fi and horror movies to know that in a real scenario, I’d program the gun to shoot anything that moved. Fortunately, that’s a moral dilemma I’ll never have to <em>face</em>!</p>&#13;
<h3 class="h3" id="ch00lev1sec76"><strong>Detecting Faces from a Video Stream</strong></h3>&#13;
<p class="noindent">You can also detect faces in real time using video cameras. This is easy to do, so we won’t make it a dedicated project. Enter the code in <a href="ch09.xhtml#ch09list5">Listing 9-5</a> or use the digital version, <em>video_face_detect.py</em>, in the <em>Chapter_9</em>  folder downloadable from the book’s website. You’ll need to use your computer’s camera or an external camera that works through your computer.</p>&#13;
<pre><span class="codeitalic1">video_face_detect.py</span>&#13;
   import cv2 as cv&#13;
   &#13;
   path = "C:/Python372/Lib/site-packages/cv2/data/"&#13;
   face_cascade = cv.CascadeClassifier(path + 'haarcascade_frontalface_alt.xml')&#13;
&#13;
<span class="ent">➊</span> cap = cv.VideoCapture(0)&#13;
&#13;
   while True:&#13;
       _, frame = cap.read()&#13;
       face_rects = face_cascade.detectMultiScale(frame, scaleFactor=1.2,&#13;
                                                  minNeighbors=3)    &#13;
   &#13;
       for (x, y, w, h) in face_rects:&#13;
           cv.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)&#13;
           &#13;
   &#13;
       cv.imshow('frame', frame)&#13;
    <span class="ent">➋</span> if cv.waitKey(1) &amp; 0xFF == ord('q'):&#13;
          break&#13;
&#13;
 cap.release()&#13;
 cv.destroyAllWindows()</pre>&#13;
<p class="listing"><a id="ch09list5"/>Listing 9-5: Detecting faces in a video stream</p>&#13;
<p class="indent">After importing OpenCV, set up your path to the Haar cascade classifiers as you did at <span class="ent">➌</span> in <a href="ch09.xhtml#ch09list1">Listing 9-1</a>. I use the <em>haarcascade_frontalface_alt.xml</em> file here as it has higher precision (fewer false positives) than the <em>haarcascade_frontalface_default.xml</em> file you used in the previous project. Next, instantiate a <span class="literal">VideoCapture</span> class object, called <span class="literal">cap</span> for “capture.” Pass the constructor the index of the video device you want to use <span class="ent">➊</span>. If you have only one camera, such as your laptop’s built-in camera, then the index of this device should be <span class="literal">0</span>.</p>&#13;
<p class="indent">To keep the camera and face detection process running, use a <span class="literal">while</span> loop. Within the loop, you’ll capture each video frame and analyze it for faces, just as you did with the static images in the previous project. The face detection algorithm is fast enough to keep up with the continuous stream, despite all the work it must do!</p>&#13;
<p class="indent">To load the frames, call the <span class="literal">cap</span> object’s <span class="literal">read()</span> method. It returns a tuple consisting of a Boolean return code and a <span class="literal">NumPy</span> <span class="literal">ndarray</span> object representing the current frame. The return code is used to check whether you’ve run out of frames when reading from a file. Since we’re not reading from a file here, assign it to an underscore to indicate an insignificant variable.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_220"/>Next, reuse the code from the previous project that finds face rectangles and draws the rectangles on the frame. Display the frame with the OpenCV <span class="literal">imshow()</span> method. The program should draw a rectangle on this frame if it detects a face.</p>&#13;
<p class="indent">To end the loop, you’ll press the <small>Q</small> key, for quit <span class="ent">➋</span>. Start by calling OpenCV’s <span class="literal">waitKey()</span> method and passing it a short, one-millisecond time-span. This method will pause the program as it waits for a key press, but we don’t want to interrupt the video stream for too long.</p>&#13;
<p class="indent">Python’s built-in <span class="literal">ord()</span> function accepts a string as an argument and returns the Unicode code point representation of the passed argument, in this case a lowercase <em>q</em>. You can see a mapping of characters to numbers here: <em><a href="http://www.asciitable.com/">http://www.asciitable.com/</a></em>. To make this lookup compatible with all operating systems, you must include the bitwise <span class="literal">AND</span> operator, <span class="literal">&amp;</span>, with the hexadecimal number FF (0xFF), which has an integer value of 255. Using <span class="literal">&amp; 0xFF</span> ensures only the last 8 bits of the variable are read.</p>&#13;
<p class="indent">When the loop ends, call the <span class="literal">cap</span> object’s <span class="literal">release()</span> method. This frees up the camera for other applications. Complete the program by destroying the display window.</p>&#13;
<p class="indent">You can add more cascades to the face detection to increase its accuracy, as you did in the previous project. If this slows detection too much, try scaling down the video image. Right after the call to <span class="literal">cap.read()</span>, add the following snippet:</p>&#13;
<pre>    frame = cv.resize(frame, None, fx=0.5, fy=0.5,&#13;
                      interpolation=cv.INTER_AREA)</pre>&#13;
<p class="indent">The <span class="literal">fx</span> and <span class="literal">fy</span> arguments are scaling factors for the screen’s <em>x</em> and <em>y</em> dimensions. Using <span class="literal">0.5</span> will halve the default size of the window.</p>&#13;
<p class="indent">The program should have no trouble tracking your face unless you do something crazy, like tilt your head slightly to the side. That’s all it takes to break detection and make the rectangle disappear (<a href="ch09.xhtml#ch09fig15">Figure 9-15</a>).</p>&#13;
<div class="image"><img src="../images/fig09_15.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig15"/>Figure 9-15: Face detection using video frames</p>&#13;
<p class="indent">Haar cascade classifiers are designed to recognize upright faces, both frontal and profile views, and they do a great job. They can even handle eyeglasses and beards. But tilt your head, and they can quickly fail.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_221"/>An inefficient but simple way to manage tilted heads is to use a loop that rotates the images slightly before passing them on for face detection. The Haar cascade classifiers can handle a bit of tilt (<a href="ch09.xhtml#ch09fig16">Figure 9-16</a>), so you could rotate the image by 5 degrees or so with each pass and have a good chance of getting a positive result.</p>&#13;
<div class="image"><img src="../images/fig09_16.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig16"/>Figure 9-16: Rotating the image facilitates face detection.</p>&#13;
<p class="indent">The Haar feature approach to face detection is popular because it’s fast enough to run in real time with limited computational resources. As you probably suspect, however, more accurate, sophisticated, and resource-intensive techniques are available.</p>&#13;
<p class="indent">For example, OpenCV ships with an accurate and robust face detector based on the Caffe deep learning framework. To learn more about this detector, see the tutorial “Face Detection with OpenCV and Deep Learning” at <em><a href="https://www.pyimagesearch.com/">https://www.pyimagesearch.com/</a></em>.</p>&#13;
<p class="indent">Another option is to use OpenCV’s LBP cascade classifier for face detection. This technique divides a face into blocks and then extracts local binary pattern histograms (LBPHs) from them. Such histograms have proved effective at detecting <em>unconstrained</em> faces in images—that is, faces that aren’t well aligned and with similar poses. We’ll look at LBPH in the next chapter, where we’ll focus on <em>recognizing</em> faces rather than simply detecting them.</p>&#13;
<h3 class="h3" id="ch00lev1sec77"><strong>Summary</strong></h3>&#13;
<p class="noindent">In this chapter, you got to work with OpenCV’s Haar cascade classifier for detecting human faces; <span class="literal">playsound</span>, for playing audio files; and <span class="literal">pyttsx3</span>, for text-to-speech audio. Thanks to these useful libraries, you were able to quickly write a face detection program that also issued audio warnings and instructions.</p>&#13;
<h3 class="h3" id="ch00lev1sec78"><strong>Further Reading</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_222"/>“Rapid Object Detection Using a Boosted Cascade of Simple Features” (Conference on Computer Vision and Pattern Recognition, 2001), by Paul Viola and Michael Jones, is the first object detection framework to provide practical, real-time object detection rates. It forms the basis for the face detection process used in this chapter.</p>&#13;
<p class="indent">Adrian Rosebrock’s <em><a href="https://www.pyimagesearch.com/">https://www.pyimagesearch.com/</a></em> website is an excellent source for building image search engines and finding loads of interesting computer vision projects, such as programs that detect fire and smoke, find targets in drone video streams, distinguish living faces from printed faces, automatically recognize license plates, and do much, much more.</p>&#13;
<h3 class="h3" id="ch00lev1sec79"><strong>Practice Project: Blurring Faces</strong></h3>&#13;
<p class="noindent">Have you ever seen a documentary or news report where a person’s face has been blurred to preserve their anonymity, like in <a href="ch09.xhtml#ch09fig17">Figure 9-17</a>? Well, this cool effect is easy to do with OpenCV. You just need to extract the face rectangle from a frame, blur it, and then write it back over the frame image, along with an (optional) rectangle outlining the face.</p>&#13;
<div class="image"><img src="../images/fig09_17.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch09fig17"/>Figure 9-17: Example of face blurring with OpenCV</p>&#13;
<p class="indent">Blurring averages pixels within a local matrix called a <em>kernel</em>. Think of the kernel as a box you place on an image. All the pixels in this box are averaged to a single value. The larger the box, the more pixels are averaged, and thus the smoother the image appears. Thus, you can think of blurring as a low-pass filter that blocks high-frequency content, such as sharp edges.</p>&#13;
<p class="indent">Blurring is the only step in this process you haven’t done before. To blur an image, use the OpenCV <span class="literal">blur()</span> method and pass it an image and a tuple of the kernel size in pixels.</p>&#13;
<pre>blurred_image = cv.blur(image, (20, 20))</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_223"/>In this example, you replace the value of a given pixel in <span class="literal">image</span> with the average of all the pixels in a 20×20 square centered on that pixel. This operation repeats for every pixel in <span class="literal">image</span>.</p>&#13;
<p class="indent">You can find a solution, <em>practice_blur.py</em>, in the appendix and in the <em>Chapter_9</em> folder downloadable from the book’s website.</p>&#13;
<h3 class="h3" id="ch00lev1sec80"><strong>Challenge Project: Detecting Cat Faces</strong></h3>&#13;
<p class="noindent">It turns out there are three animal life forms on planet LV-666: humans, mutants, and cats. The base’s mascot, Mr. Kitty, has free rein of the place and is prone to wander through Corridor 5.</p>&#13;
<p class="indent">Edit and calibrate <em>sentry.py</em> so that Mr. Kitty can freely pass. This will be a challenge, as cats aren’t known for obeying verbal orders. To get him to at least look at the camera, you might add a “Here kitty, kitty” or “Puss, puss, puss” to the <span class="literal">pyttsx3</span> verbal commands. Or better, add the sound of a can of tuna being opened using <span class="literal">playsound</span>!</p>&#13;
<p class="indent">You can find Haar classifiers for cat faces in the same OpenCV folder as the classifiers you used in Project 13, and an empty corridor image, <em>empty_corridor.png</em>, in the book’s downloadable <em>Chapter_9</em>  folder. Select a few cat images from the internet, or your personal collection, and paste them in different places in the empty corridor. Use the humans in the other images to gauge the proper scale for the cat.</p>&#13;
</body></html>