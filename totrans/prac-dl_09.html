<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_189"/><strong><span class="big">9</span><br/>TRAINING A NEURAL NETWORK</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In this chapter, we’ll discuss how to train a neural network. We’ll look at the standard approaches and tricks being used in the field today. There will be some math, some hand-waving, and a whole host of new terms and concepts. But you don’t need to follow the math at a deep level: we’ll gloss over things as needed to get the main point across.</p>&#13;
<p class="indent">This chapter is perhaps the most challenging in the book, at least conceptually. It certainly is mathematically. While it’s crucially important to building intuition and understanding, sometimes we get impatient and like to dive into things first to test the waters. Thanks to preexisting libraries, we can do that here. If you want to play around with neural networks before learning how they work, jump to <a href="ch10.xhtml#ch10">Chapter 10</a> before coming back here to fill in the theory. But do come back.</p>&#13;
<p class="indent">It’s possible to learn to use powerful toolkits like sklearn and Keras without understanding how they work. That approach should not satisfy anyone, though the temptation is real. Understanding how these algorithms work is well worth your time.</p>&#13;
<h3 class="h3" id="lev1_53"><span epub:type="pagebreak" id="page_190"/>A High-Level Overview</h3>&#13;
<p class="noindent">Let’s begin this chapter with an overview of the concepts we’ll discuss. Read it, but don’t fret if the concepts are unclear. Instead, try to get a feel for the overall process.</p>&#13;
<p class="indent">The first step in training a neural network is selecting intelligent initial values for the weights and biases. We then use <em>gradient descent</em> to modify these weights and biases so that we reduce the error over the training set. We’ll use the average value of the loss function to measure the error, which tells us how wrong the network currently is. We know if the network is right or wrong because we have the expected output for each input sample in the training set (the class label).</p>&#13;
<p class="indent">Gradient descent is an algorithm that requires gradients. For now, think of gradients as measures of steepness. The larger the gradient, the steeper the function is at that point. To use gradient descent to search for the smallest value of the loss function, we need to be able to find gradients. For that, we’ll use <em>backpropagation</em>. This is the fundamental algorithm of neural networks, the one that allows them to learn successfully. It gives us the gradients we need by starting at the output of the network and moving back through the network toward the input. Along the way, it calculates the gradient value for each weight and bias.</p>&#13;
<p class="indent">With the gradient values, we can use the gradient descent algorithm to update the weights and biases so that the next time we pass the training samples through the network, the average of the loss function will be less than it was before. In other words, our network will be less wrong. This is the goal of training, and we hope it results in a network that has learned general features of the data.</p>&#13;
<p class="indent">Learning general features of the dataset requires <em>regularization</em>. There are many approaches to regularization, and we’ll discuss the main ones. Without regularization, the training process is in danger of overfitting, and we could end up with a network that doesn’t generalize. But with regularization, we can be successful and get a useful model.</p>&#13;
<p class="indent">So, the following sections introduce gradient descent, backpropagation, loss functions, weight initialization, and, finally, regularization. These are the main components of successful neural network training. We don’t need to understand these in all their gory mathematical details; instead, we need to understand them conceptually so we can build an intuitive approach to what it means to train a neural network. With this intuition, we’ll be able to make meaningful use of the parameters that sklearn and Keras give us for training.</p>&#13;
<h3 class="h3" id="lev1_54">Gradient Descent</h3>&#13;
<p class="noindent">The standard way to train a neural network is to use gradient descent.</p>&#13;
<p class="indent">Let’s parse the phrase <em>gradient descent</em>. We already know what the word <em>descent</em> means. It means to go down from somewhere higher up. What about <em>gradient</em>? The short answer is that a gradient indicates how quickly something changes with respect to how fast something else changes. Measuring <span epub:type="pagebreak" id="page_191"/>how much one thing changes as another changes is something we’re all familiar with. We all know about speed, which is how position changes as time changes. We even say it in words: <em>miles per hour</em> or <em>kilometers per hour</em>.</p>&#13;
<p class="indent">You’re probably already familiar with the gradient in another context. Consider the equation of a line</p>&#13;
<p class="center"><em>y</em> = <em>mx</em> + <em>b</em></p>&#13;
<p class="noindent">where <em>m</em> is the slope and <em>b</em> is the y-axis intercept. The slope is how quickly the line’s <em>y</em> position changes with each change in the <em>x</em> position. If we know two points that are on the line, (<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>) and (<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>), then we can calculate the slope as</p>&#13;
<div class="imagec"><img src="Images/191equ01.jpg" alt="image" width="98" height="43"/></div>&#13;
<p class="noindent">which, in words, we might say as “<em>y</em>’s per <em>x</em>.” It’s a measure of how steep or shallow the line is: its gradient. In mathematics, we often talk about a change in a variable, and the notation for that is to put a Δ (delta) in front. So, we might write the slope of a line as</p>&#13;
<div class="imagec"><img src="Images/191equ02.jpg" alt="image" width="66" height="44"/></div>&#13;
<p class="noindent">to drive home the point that the slope is the change in <em>y</em> for each change in <em>x</em>. Fortunately for us, it turns out that not only do lines have a slope at each point, but also most functions have a slope at each point. However, except for straight lines, this slope changes from point to point. A picture will help here. Consider <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>.</p>&#13;
<div class="image" id="ch9fig1"><img src="Images/09fig01.jpg" alt="image" width="650" height="456"/></div>&#13;
<p class="figcap"><em>Figure 9-1: A function with several tangent lines indicated</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_192"/>The graph in <a href="ch09.xhtml#ch9fig1">Figure 9-1</a> is of a polynomial. Notice the lines drawn on the figure that are just touching the function. These are <em>tangent</em> lines. And as lines, they have a slope we can see in the plot. Now imagine moving one of the lines over the function so that it continues to touch the function at only one point; imagine how the slope of the line changes as it moves.</p>&#13;
<p class="indent">It turns out that how the slope changes over the function is itself a function, and it’s called the <em>derivative</em>. Given a function and <em>x</em> value, the derivative tells us the slope of the function at that point, <em>x</em>. The fact that functions have derivatives is a fundamental insight of calculus, and of fundamental importance to us.</p>&#13;
<p class="indent">The notion of a derivative is essential because for single variable functions, the derivative at the point <em>x</em> is the gradient at <em>x</em>; it’s the direction in which the function is changing. If we want to find the minimum of the function, the <em>x</em> that gives us the smallest <em>y</em>, we want to move in the direction <em>opposite</em> to the gradient as that will move us in the direction of the minimum.</p>&#13;
<p class="indent">The derivative is written in many different ways, but the way that echoes the idea of the slope, how <em>y</em> changes for a change in <em>x</em>, is</p>&#13;
<div class="imagec"><img src="Images/192equ01.jpg" alt="image" width="20" height="43"/></div>&#13;
<p class="indent">We’ll return to this form next when discussing the backpropagation algorithm. That’s it for the gradient; now let’s take a closer look at descent.</p>&#13;
<h4 class="h4" id="lev2_81">Finding Minimums</h4>&#13;
<p class="noindent">Since we want a model that makes few mistakes, we need to find the set of parameters that lead to a small value for the loss function. In other words, we need to find a <em>minimum</em> of the loss function.</p>&#13;
<p class="indent">Look again at <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>. The minimum is on the right, where tangent line C is. We can see it’s the minimum, and notice that the gradient is 0 there. This tells us we’re at a minimum (or maximum). If we start at B, we see that the slope of the tangent line is negative (down and to the right). Therefore, we need to move to an <em>x</em> value in the positive direction because this is opposite to the sign of the gradient. Doing this will take us closer to the minimum at C. Similarly, if we start at D, the slope of the tangent line is positive (up and to the right) meaning we need to move in the negative <em>x</em> direction, again toward C, to move closer to the minimum. All of this hints at an algorithm for finding the minimum of a function: pick a starting point (an <em>x</em> value) and use the gradient to move to a lower point.</p>&#13;
<p class="indent">For simple functions of just <em>x</em>, like those of <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>, this approach will work nicely, assuming we start in a good place like B or D. When we move to more than one dimension, it turns out that this approach will still work nicely provided we start in a good place with our initial guess.</p>&#13;
<p class="indent">Working still with <a href="ch09.xhtml#ch9fig1">Figure 9-1</a> and assuming we’re starting at B, we see that the gradient tells us to move to the right, toward C. But how do we select the next <em>x</em> value to consider, to move us closer to C? This is the step size, <span epub:type="pagebreak" id="page_193"/>and it tells us how big a jump we make from one <em>x</em> position to the next. Step size is a parameter we have to choose, and in practice this value, called the <em>learning rate</em>, is often fluid and gets smaller and smaller as we move, under the assumption that as we move, we get closer and closer to the minimum value and therefore need smaller and smaller steps.</p>&#13;
<p class="indent">This is all well and good, even intuitive, but we have a small problem. What if instead of starting at B or D, we start at A? The gradient at A is pointing us to the left, not the right. In this case, our simple algorithm will fail—it will move us to the left, and we’ll never reach C. The figure shows only one minimum, at C, but we can easily imagine a second minimum, say to the left of A, that doesn’t go as low (doesn’t have as small a <em>y</em> value) as C. If we start at A, we’ll move toward this minimum, and not the one at C. Our algorithm will fall into a <em>local minimum</em>. Once in, our algorithm can’t get us out, and we won’t be able to find the global minimum at C. We’ll see that this is a genuine issue for neural networks, but one that for modern deep networks is, almost magically, not much of an issue after all.</p>&#13;
<p class="indent">So how does all of this help us train a neural network? The gradient tells us how a small change in <em>x</em> changes <em>y</em>. If <em>x</em> is one of the parameters of our network and <em>y</em> is the error given by the loss function, then the gradient tells us how much a change in that parameter affects the overall error of the network. Once we know that, we’re in a position to modify the parameter by an amount based on the gradient, and we know that this will move us toward a minimum error. When the error over the training set is at a minimum, we can claim that the network has been trained.</p>&#13;
<p class="indent">Let’s talk a bit more about the gradients and parameters. All of our discussion to this point, based on <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>, has been rather one-dimensional; our functions are functions of <em>x</em> only. We talked about changing one thing, the position along the x-axis, to see how it affects the <em>y</em> position. In reality, we’re not working with just one dimension. Every weight and bias in our network is a parameter, and the loss function value depends upon all of them. For the simple network in <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> alone, there are 20 parameters, meaning that the loss function is a 20-dimensional function. Regardless, our approach remains much the same: if we know the gradient for each parameter, we can still apply our algorithm in an attempt to locate a set of parameters minimizing the loss.</p>&#13;
<h4 class="h4" id="lev2_82">Updating the Weights</h4>&#13;
<p class="noindent">We’ll talk about how to get gradient values in a bit, but for the time being let’s assume we have them already. We’ll say that we have a set of numbers that tells us how, given the current configuration of the network, a change in any weight or bias value changes the loss. With that knowledge, we can apply gradient descent: we adjust the weight or bias by some fraction of that gradient value to move us, collectively, toward a minimum of the entire loss function.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_194"/>Mathematically, we update each weight and bias using a simple rule:</p>&#13;
<p class="center"><em>w</em> ← <em>w</em> –Δ<em>w</em></p>&#13;
<p class="noindent">Here <em>w</em> is one of the weights (or biases), <em>η</em> (eta) is the learning rate (the step size), and <em>Δw</em> is the gradient value.</p>&#13;
<p class="indent"><a href="ch09.xhtml#ch9lis1">Listing 9-1</a> gives an algorithm for training a neural network using gradient descent.</p>&#13;
<p class="programs" id="ch9lis1">1. Pick some intelligent starting values for the weights and biases.<br/>&#13;
2. Run the training set through the network using its current weights and<br/>&#13;
biases and calculate the average loss.<br/>&#13;
3. Use this loss to get the gradient for each weight and bias.<br/>&#13;
4. Update the weight or bias value by the step size times the gradient value.<br/>&#13;
5. Repeat from step 2 until the loss is low enough.</p>&#13;
<p class="figcap"><em>Listing 9-1: Gradient descent in five (deceptively) simple steps</em></p>&#13;
<p class="indent">The algorithm appears simple, but as they say, the devil is in the details. We have to make choices at every step, and every choice we make will prompt further questions. For example, step 1 says to “Pick some intelligent starting values.” What should they be? It turns out that successfully training a neural network depends critically on choosing good initial values. We already saw how this might be so in our preceding example using <a href="ch09.xhtml#ch9fig1">Figure 9-1</a> where if we start at A, we won’t find the minimum at C. Much research has been conducted over the years related to step 1.</p>&#13;
<p class="indent">Step 2 is straightforward; it’s the forward-pass through the network. We haven’t talked in detail about the loss function itself; for now, just think of it as a function measuring the effectiveness of the network on the training set.</p>&#13;
<p class="indent">Step 3 is a black box for the time being. We’ll explore how to do it shortly. For now, assume we can find the gradient values for each parameter.</p>&#13;
<p class="indent">Step 4 follows the form of the previous equation that moves the parameter from its current value to one that will reduce the overall loss. In practice, the simple form of this equation is not sufficient; there are other terms, like momentum, that preserve some fraction of the previous weight change for the next iteration (next pass of the training data through the network) so that parameters do not change too wildly. We’ll revisit momentum later. For now, let’s look at a variation of gradient descent, the one that is actually used to train deep networks.</p>&#13;
<h3 class="h3" id="lev1_55">Stochastic Gradient Descent</h3>&#13;
<p class="noindent">The previous steps describe gradient descent training of a neural network. As we might expect, in practice there are many different flavors of this basic idea. One that’s in widespread use and works well empirically is called <em>stochastic gradient descent (SGD)</em>. The word <em>stochastic</em> refers to a random process. We’ll see next why the word <em>stochastic</em> goes before <em>gradient descent</em> in this case.</p>&#13;
<h4 class="h4" id="lev2_83"><span epub:type="pagebreak" id="page_195"/>Batches and Minibatches</h4>&#13;
<p class="noindent">Step 2 of <a href="ch09.xhtml#ch9lis1">Listing 9-1</a> says to run the complete training set through the network using the current values of the weights and biases. This approach is called <em>batch training</em>, so named because we use all of the training data to estimate the gradients. Intuitively, this is a reasonable thing to do: we’ve carefully constructed the training set to be a fair representation of the unknown parent process that generates the data, and it’s this parent process we want the network to successfully model for us.</p>&#13;
<p class="indent">If our dataset is small, like the original iris dataset of <a href="ch05.xhtml#ch05">Chapter 5</a>, then batch training makes sense. But what if our training dataset isn’t small? What if it’s hundreds of thousands or even millions of samples? We’ll be facing longer and longer training times.</p>&#13;
<p class="indent">We’ve run into a problem. We want a large training set as that will (hopefully) better represent the unknown parent process that we want to model. But the larger the training set, the longer it takes to pass each sample through the network, get an average value for the loss, and update the weights and biases. We call passing the entire training set through the network an <em>epoch</em>, and we’ll need many dozens to hundreds of epochs to train the network. Doing a better job of representing the thing we want to model means longer and longer computation times because of all the samples that must be passed through the network.</p>&#13;
<p class="indent">This is where SGD comes into play. Instead of using all the training data on each pass, let’s alternatively select a small subset of the training data and use the average loss calculated from it to update the parameters. We’ll calculate an “incorrect” gradient value because we’re estimating the loss over the full training set using only a small sample, but we’ll save a lot of time.</p>&#13;
<p class="indent">Let’s see how this sampling plays out with a simple example. We’ll define a vector of 100 random bytes using NumPy:</p>&#13;
<p class="programs">&gt;&gt;&gt; d = np.random.normal(128,20,size=100).astype("uint8")<br/>&#13;
&gt;&gt;&gt; d<br/>&#13;
130, 141,  99, 106, 135, 119,  98, 147, 152, 163, 118, 149, 122,<br/>&#13;
133, 115, 128, 176, 132, 173, 145, 152,  79, 124, 133, 158, 111,<br/>&#13;
139, 140, 126, 117, 175, 123, 154, 115, 130, 108, 139, 129, 113,<br/>&#13;
129, 123, 135, 112, 146, 125, 134, 141, 136, 155, 152, 101, 149,<br/>&#13;
137, 119, 143, 136, 118, 161, 138, 112, 124,  86, 135, 161, 112,<br/>&#13;
117, 145, 140, 123, 110, 163, 122, 105, 135, 132, 145, 121,  92,<br/>&#13;
118, 125, 154, 148,  92, 142, 118, 128, 128, 129, 125, 121, 139,<br/>&#13;
152, 122, 128, 126, 126, 157, 124, 120, 152</p>&#13;
<p class="indent">Here the byte values are normally distributed around a mean of 128. The actual mean of the 100 values is 130.9. Selecting subsets of these values, 10 at a time, gives us an estimate of the actual mean value</p>&#13;
<p class="programs">&gt;&gt;&gt; i = np.argsort(np.random.random(100))<br/>&#13;
&gt;&gt;&gt; d[i[:10]].mean()<br/>&#13;
138.9</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_196"/>with repeated subsets leading to estimated means of 135.7, 131.7, 134.2, 128.1, and so forth.</p>&#13;
<p class="indent">None of the estimated means are the actual mean, but they are all close to it. If we can estimate the mean from a random subset of the full dataset, we can see by analogy that we should be able to estimate the gradients of the loss function with a subset of the full training set. Since the sample is randomly selected, the resulting gradient values are randomly varying estimates. This is why we add the word <em>stochastic</em> in front of <em>gradient descent</em>.</p>&#13;
<p class="indent">Because passing the full training set through the network on each weight and bias update step is known as <em>batch training</em>, passing a subset through is known as <em>minibatch training</em>. You will hear people use the term <em>minibatch</em> quite frequently. A minibatch is the subset of the training data used for each stochastic gradient descent step. Training is usually some number of epochs, where the relationship between epochs and minibatches is as follows:</p>&#13;
<div class="imagec"><img src="Images/196equ01.jpg" alt="image" width="497" height="51"/></div>&#13;
<p class="indent">In practice, we don’t really want to select the minibatches at random from the full training set. If we do that, we run the risk of not using all the samples: some might never be selected, and others might be selected too often. Typically, we randomize the order of the training samples and select fixed-size blocks of samples sequentially whenever a minibatch is required. When all available training samples are used, we can shuffle the full training set order and repeat the process. Some deep learning toolkits don’t even do this; they instead cycle through the same set of minibatches again.</p>&#13;
<h4 class="h4" id="lev2_84">Convex vs. Nonconvex Functions</h4>&#13;
<p class="noindent">SGD sounds like a concession to practicality. In theory, it seems that we’d never want to use it, and we might expect that our training results will suffer because of it. However, the opposite is generally true. In some sense, gradient descent training of a neural network shouldn’t work at all because we’re applying an algorithm meant for convex functions to one that is nonconvex. <a href="ch09.xhtml#ch9fig2">Figure 9-2</a> illustrates the difference between a convex function and a nonconvex function.</p>&#13;
<div class="image" id="ch9fig2"><img src="Images/09fig02.jpg" alt="image" width="685" height="257"/></div>&#13;
<p class="figcap"><em>Figure 9-2: A convex function of</em> x <em>(left). A nonconvex function of</em> x <em>(right)</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_197"/>A convex function is such that a line segment between any two points on the function does not cross the function at any other point. The black line on the left of <a href="ch09.xhtml#ch9fig2">Figure 9-2</a> is one example, and any such segment will not cross the function at any other point, indicating that this is a convex function. However, the same can’t be said of the curve on the right of <a href="ch09.xhtml#ch9fig2">Figure 9-2</a>. This is the curve from <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>. Here the black line does cross the function.</p>&#13;
<p class="indent">Gradient descent is designed to find the minimum when the function is convex, and because it relies only on the gradient, the first derivative, it’s sometimes known as a <em>first-order</em> optimization method. Gradient descent should not work, in general, with nonconvex functions because it runs the risk of getting trapped in a local minimum instead of finding the global minimum. Again, we saw this with the example in <a href="ch09.xhtml#ch9fig1">Figure 9-1</a>.</p>&#13;
<p class="indent">Here’s where stochastic gradient descent helps. In multiple dimensions, the gradient will point in a direction that isn’t necessarily toward the nearest minimum of the loss function. This means that our step will be in a slightly wrong direction, but that somewhat wrong direction might help us avoid getting trapped somewhere we don’t want to be.</p>&#13;
<p class="indent">The situation is more complicated, of course, and more mysterious. The machine learning community has been struggling with the contradiction between the obvious success of using first-order optimization on the nonconvex loss function and the fact that it shouldn’t work at all.</p>&#13;
<p class="indent">Two ideas are emerging. The first is what we just stated, that stochastic gradient descent helps by actually moving us in a slightly wrong direction. The second idea, which seems to be pretty much proven now, is that, for the loss functions used in deep learning, it turns out that there are many, many local minimums and that these are all basically the same, so that landing in almost any one of them will result in a network that performs well.</p>&#13;
<p class="indent">Some researchers argue that most gradient descent learning winds up on a <em>saddle point</em>; this is a place that looks like a minimum but isn’t. Imagine a saddle for a horse and place a marble in the middle. The marble will sit in place, but you could push the marble in a certain direction and have it roll off the saddle. The argument, not without some justification, is that most training ends on a saddle point, and better results are possible with a better algorithm. Again, however, even the saddle point, if it is one, is still for practical purposes a good place to be, so the model is successful regardless.</p>&#13;
<p class="indent">In practice, then, we should use stochastic gradient descent because it leads to better overall learning and reduces the training time by not requiring full batches. It does introduce a new hyperparameter, the minibatch size, that we must select at some point before training.</p>&#13;
<h4 class="h4" id="lev2_85">Ending Training</h4>&#13;
<p class="noindent">We haven’t yet discussed a critical question: when should we stop training? Remember that in <a href="ch05.xhtml#ch05">Chapter 5</a>, we went through some effort to create training sets, validation sets, and test sets. This is where we’ll use the validation sets. While training, we can use the accuracy, or some other metric, on the validation set to decide when to stop. If using SGD, we typically run the validation set through the network for each minibatch or set of minibatches to <span epub:type="pagebreak" id="page_198"/>compute the accuracy. By tracking the accuracy on the validation set, we can decide when to stop training.</p>&#13;
<p class="indent">If we train for a long time, eventually two things usually happen. The first is that the error on the training set goes toward zero; we get better and better on the training set. The second is that the error on the validation set goes down and then, eventually, starts to go back up.</p>&#13;
<p class="indent">These effects are due to overfitting. The training error goes down and down as the model learns more and more to represent the parent distribution that generated the dataset. But, eventually, it will stop learning general things about the training set. At this point, we’re overfitting, and we want to stop training because the model is no longer learning general features and is instead learning minutiae about the particular training set we’re using. We can watch for this by using the validation set while training. Since we don’t use the samples in the validation set to update the weights and biases of the network, it should give us a fair test of the current state of the network. When overfitting starts, the error on the validation set will begin to go up from a minimum value. What we can do then is to keep the weights and biases that produced the minimum value on the validation set and claim that those represent the best model.</p>&#13;
<p class="indent">We don’t want to use any data that has influenced training to measure the final effectiveness of our network. We use the validation set to decide when to stop training, so characteristics of the samples in the validation set have also influenced the final model; this means we can’t strongly rely on the validation set to give us an idea of how the model will behave on new data. It’s only the held-out test set, unused until we declare victory over training, that gives us some idea of how we might expect the model to perform on data in the wild. So, just as it is anathema to report training set accuracy as a measure of how good the model is, it’s also anathema to report the validation set accuracy.</p>&#13;
<h4 class="h4" id="lev2_86">Updating the Learning Rate</h4>&#13;
<p class="noindent">In our generic update equation for changing the weights and biases based on the gradient, we introduced a hyperparameter, <em>η</em> (eta), the learning rate or step size. It’s a scale factor indicating how much we should update the weight or bias based on the gradient value.</p>&#13;
<p class="indent">We previously stated that the learning rate doesn’t need to be fixed and that it could, and even should, get smaller and smaller as we train under the assumption that we need smaller and smaller steps to get to the actual minimum value of the loss function. We didn’t state how we should actually update the learning rate.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_199"/>There’s more than one way to update the step size, but some are more helpful than others. The <span class="literal">MLPClassifier</span> class of sklearn, which uses SGD solvers, has three options. The first is to never change the learning rate—just leave <em>η</em> at its initial value, <em>η</em><sub>0</sub>. The second is to scale <em>η</em> so that it decreases with epochs (minibatches) according to</p>&#13;
<div class="imagec"><img src="Images/199equ01.jpg" alt="image" width="56" height="39"/></div>&#13;
<p class="noindent">where <em>η</em><sub>0</sub> is set by the user, <em>t</em> is the iteration (epoch, minibatch), and <em>p</em> is an exponent on <em>t</em>, also picked by the user. The sklearn default <em>p</em> is 0.5—that is, scale by <span class="middle"><img src="Images/199equ02.jpg" alt="Image" width="22" height="22"/></span>, which seems a reasonable default.</p>&#13;
<p class="indent">The third option is to adapt the learning rate by watching the loss function value. As long as the loss is decreasing, leave the learning rate where it is. When the loss stops decreasing for a set number of minibatches, divide the learning rate by some value like 5, the sklearn default. If we never change the learning rate, and it’s too large, we might end up moving around the minimum without ever being able to reach it because we’re consistently stepping over it. It’s a good idea then to decrease the learning rate when using SGD. Later in the book, we’ll encounter other optimization approaches that automatically adjust the learning rate for us.</p>&#13;
<h4 class="h4" id="lev2_87">Momentum</h4>&#13;
<p class="noindent">There’s one last wrinkle in SGD we have to cover. As we saw previously, the weight update equation for both gradient descent and SGD is</p>&#13;
<p class="center"><em>w</em> ← <em>w</em> – <em>η</em>Δ<em>w</em></p>&#13;
<p class="noindent">We update the weight by the learning rate (<em>η</em>) times the gradient, which we are representing here as Δ<em>w</em>.</p>&#13;
<p class="indent">A common and powerful trick is to introduce a <em>momentum</em> term that adds back some fraction of the previous Δ<em>w</em>, the update of the prior minibatch. The momentum term prevents the <em>w</em> parameter from changing too quickly in response to a particular minibatch. Adding in this term gives us</p>&#13;
<p class="center"><em>w</em><sub><em>i</em>+1</sub> ← <em>w</em><sub><em>i</em></sub> – <em>η</em>Δ<em>w</em><sub><em>i</em></sub> + <em>μ</em>Δ<em>w</em><sub><em>i–1</em></sub></p>&#13;
<p class="noindent">We’ve added subscripts to indicate the next pass through the network (<em>i</em> + 1), the current pass (<em>i</em>), and the previous pass (<em>i –</em> 1). The previous pass Δ<em>w</em> is the one we need to use. A typical value for <em>μ</em> (mu), the momentum, is around 0.9. Virtually all toolkits implement momentum in some form, including sklearn.</p>&#13;
<h3 class="h3" id="lev1_56"><span epub:type="pagebreak" id="page_200"/>Backpropagation</h3>&#13;
<p class="noindent">We’ve been operating under the assumption that we know the gradient value for each parameter. Let’s discuss how the backpropagation algorithm gives us these magic numbers. The backpropagation algorithm is perhaps the single most important development in the history of neural networks as it enables the training of large networks with hundreds, thousands, millions, and even billions of parameters. This is especially true of the convolutional networks we’ll work with in <a href="ch12.xhtml#ch12">Chapter 12</a>.</p>&#13;
<p class="indent">The backpropagation algorithm itself was published by Rumelhart, Hinton, and Williams in 1986 in their paper “Learning Representations by Back-propagating Errors.” It’s a careful application of the chain rule for derivatives. The algorithm is called <em>backpropagation</em> because it works backward from the output layer of the network toward the input layer, propagating the error from the loss function down to each parameter of the network. Colloquially, the algorithm is known as <em>backprop</em>; we’ll use that term here, so we sound more like native machine learning experts.</p>&#13;
<p class="indent">Adding backprop into the training algorithm for gradient descent, and tailoring it to SGD, gives us the algorithm in <a href="ch09.xhtml#ch9lis2">Listing 9-2</a>.</p>&#13;
<p class="programs" id="ch9lis2">1. Pick some intelligent starting values for the weights and biases.<br/>&#13;
2. Run a minibatch through the network using its current weights and biases<br/>&#13;
and calculate the average loss.<br/>&#13;
3. Use this loss and backprop to get the gradient for each weight and bias.<br/>&#13;
4. Update the weight or bias value by the step size times the gradient value.<br/>&#13;
5. Repeat from step 2 until the loss is low enough.</p>&#13;
<p class="figcap"><em>Listing 9-2: Stochastic gradient descent with backprop</em></p>&#13;
<p class="indent">Step 2 of <a href="ch09.xhtml#ch9lis2">Listing 9-2</a> is referred to as the <em>forward pass</em>; step 3 is the <em>backward pass</em>. The forward pass is also how we’ll use the network after it’s finally trained. The backward pass is backprop calculating the gradients for us so that we can update the parameters in step 4.</p>&#13;
<p class="indent">We’ll describe backprop twice. First, we’ll do so with a simple example and work with the actual derivatives. Second, we’ll work with a more abstract notation to see how backprop applies to actual neural networks in a general sense. There is no way to sugarcoat this: this section involves derivatives, but we already have a good intuitive sense of what those are from our discussion of gradient descent, so we should be in good shape to proceed.</p>&#13;
<h4 class="h4" id="lev2_88">Backprop, Take 1</h4>&#13;
<p class="noindent">Suppose we have two functions, <em>z</em> = <em>f</em> (<em>y</em>) and <em>y</em> = <em>g</em>(<em>x</em>), meaning <em>z</em> = <em>f</em> (<em>g</em>(<em>x</em>)). We know that the derivative of the function <em>g</em> gives us <em>dy</em>/<em>dx</em>, which tells us how <em>y</em> changes when <em>x</em> changes. Similarly, we know that the derivative of the function <em>f</em> will give us <em>dz</em>/<em>dy</em>. The value of <em>z</em> depends upon the composition <span epub:type="pagebreak" id="page_201"/>of <em>f</em> and <em>g</em>, meaning the output of <em>g</em> is the input to <em>f</em>, so if we want to find an expression for <em>dz</em>/<em>dx</em>, how <em>z</em> changes with <em>x</em>, we need a way to link through the composed functions. This is what the chain rule for derivatives gives us:</p>&#13;
<div class="imagec"><img src="Images/201equ01.jpg" alt="image" width="90" height="49"/></div>&#13;
<p class="noindent">This notation is especially nice because we can imagine the <em>dy</em> “term” canceling just as it would if these were actual fractions.</p>&#13;
<p class="indent">How does this help us? In a neural network, the output of one layer is the input to the next, which is composition, so we can see intuitively that the chain rule might apply. Remember that we want the values that tell us how the loss function changes with respect to the weights and biases. Let’s call the loss function <em>L</em> and any given weight or bias <em>w</em>. We want to calculate <em>∂</em>ℒ/<em>∂w</em> for all the weights and biases.</p>&#13;
<p class="indent">Alarm bells should be going off in your head. The previous paragraph slipped in new notation. So far, we’ve been writing derivatives as <em>dy</em>/<em>dx</em>, but the derivative for the loss with respect to a weight was written as <em>∂</em>ℒ/<em>∂w</em>. What is this fancy <em>∂</em>?</p>&#13;
<p class="indent">When we had a function of one variable, just <em>x</em>, there was only one slope at a point to talk about. As soon as we have a function with more than one variable, the idea of the slope at a point becomes ambiguous. There are an infinite number of lines tangent to the function at any point. So we need the idea of the <em>partial derivative</em>, which is the slope of the line in the direction of the variable we’re considering when all other variables are treated as fixed. This tells us how the output will change as we change only the one variable. To note that we are using a partial derivative, we shift from <em>d</em> to <em>∂</em>, which is just a script <em>d</em>.</p>&#13;
<p class="indent">Let’s set up a straightforward network so that we can see how the chain rule leads directly to the expressions we want. We’re looking at the network in <a href="ch09.xhtml#ch9fig3">Figure 9-3</a>, which consists of an input, two hidden layers of a single node each, and an output layer.</p>&#13;
<div class="image" id="ch9fig3"><img src="Images/09fig03.jpg" alt="image" width="500" height="75"/></div>&#13;
<p class="figcap"><em>Figure 9-3: A simple network to illustrate the chain rule</em></p>&#13;
<p class="noindent">For simplicity, we’ll ignore any bias values. Additionally, let’s define the activation function to be the identity function, <em>h</em>(<em>x</em>) = <em>x</em>. This simplification removes the derivative of the activation function to make things more transparent.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_202"/>For this network, the forward pass computes</p>&#13;
<p class="center"><em>h</em><sub>1</sub> = <em>w</em><sub>1</sub><em>x</em></p>&#13;
<p class="center">  <em>h</em><sub>2</sub> = <em>w</em><sub>2</sub><em>h</em><sub>1</sub></p>&#13;
<p class="center">    <em>y</em> = <em>w</em><sub>3</sub><em>h</em><sub>2</sub></p>&#13;
<p class="noindent">which follows the form we’ve used previously, chaining things together by making the output of one layer the input to the next. This gives us the output of the network, <em>y</em>, for input <em>x</em>. If we’re looking to train the network, we’ll have a training set, a set of pairs, (<em>x</em><sub><em>i</em></sub>, <em><span class="ent">ŷ</span></em>), <em>i</em> = 0, 1, …, that are examples of what the output should be for a given input. Note that the forward pass moved from the input, <em>x</em>, to the output, <em>y</em>. We’ll next see why the backward pass moves from the output to the input.</p>&#13;
<p class="indent">Now let’s define the loss function, <em><span class="ent">ℒ</span></em>, to be the squared error between <em>y</em>, the network output for a given input <em>x</em>, and <em>ŷ</em>, the output we should get. Functionally, the loss looks like the following.</p>&#13;
<div class="imagec"><img src="Images/202equ02.jpg" alt="image" width="110" height="45"/></div>&#13;
<p class="noindent">For simplicity, we’re ignoring the fact that the loss is a mean over the training set or some minibatch drawn from it. The factor of <span class="middle"><img src="Images/1by2.jpg" alt="Image"/></span> is not strictly necessary but it’s commonly used to make the derivative a bit nicer. Since we’re looking to minimize the loss for a particular set of weights, it doesn’t matter that we’re always multiplying the loss by a constant factor of <span class="middle"><img src="Images/1by2.jpg" alt="Image"/></span>—the smallest loss will still be the smallest loss regardless of its actual numeric value.</p>&#13;
<p class="indent">To use gradient descent, we need to find how the loss changes with the weights. In this simple network, that means we need to find three gradient values, one each for <em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, and <em>w</em><sub>3</sub>. This is where the chain rule comes into play. We’ll write the equations first and then talk about them:</p>&#13;
<div class="imagec"><img src="Images/202equ03.jpg" alt="image" width="206" height="233"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_203"/>The order of these equations shows why this algorithm is called <em>backpropagation</em>. To get the partial derivative for the output layer parameter, we need only the output and the loss, <em>y</em> and <em><span class="ent">ℒ</span></em>. To get the partial derivative for the middle layer weight, we need the following two partial derivatives from the output layer:</p>&#13;
<div class="imagec"><img src="Images/203equ01.jpg" alt="image" width="34" height="141"/></div>&#13;
<p class="noindent">Finally, to get the partial derivative for the input layer weight, we need partial derivatives from the output and middle layer. In effect, we have moved backward through the network propagating values from later layers.</p>&#13;
<p class="indent">For each of these equations, the right-hand side matches the left-hand side if we imagine the “terms” canceling like fractions. Since we selected a particularly simple form for the network, we can calculate the actual gradients by hand. We need the following gradients, from the right-hand side of the preceding equations.</p>&#13;
<div class="imagec"><img src="Images/203equ02.jpg" alt="image" width="224" height="502"/></div>&#13;
<p class="noindent">The <em>∂ℒ</em>/<em>∂y</em> comes from the form we selected for the loss and the rules of differentiation from calculus.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_204"/>Putting these back into the equations for the gradients of the weights gives us</p>&#13;
<div class="imagec"><img src="Images/204equ01.jpg" alt="image" width="166" height="229"/></div>&#13;
<p class="indent">After a forward pass, we have numeric values for all the quantities on the right-hand side of these equations. Therefore, we know the numeric value of the gradients. The update rule from gradient descent then tells us to change the weights like the following.</p>&#13;
<div class="imagec"><img src="Images/204equ02.jpg" alt="image" width="332" height="229"/></div>&#13;
<p class="noindent">where <em>η</em> is the learning rate parameter defining how large a step to take when updating.</p>&#13;
<p class="indent">To recap, we need to use the chain rule, the heart of the backprop algorithm, to find the gradients we need to update the weights during training. For our simple network, we were able to work out the value of these gradients explicitly by moving backward through the network from the output toward the input. Of course, this is just a toy network. Let’s now take a second look at how to use backprop in a more general sense to calculate the necessary gradients for any network.</p>&#13;
<h4 class="h4" id="lev2_89">Backprop, Take 2</h4>&#13;
<p class="noindent">Let’s begin by revisiting the loss function and introducing some new notation. The loss function is a function of all the parameters in the network, meaning that every weight and bias value contributes to it. For example, the <span epub:type="pagebreak" id="page_205"/>loss for the network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a>, which has 20 weights and biases, could be written as</p>&#13;
<div class="imagec"><img src="Images/205equ01.jpg" alt="image" width="451" height="180"/></div>&#13;
<p class="noindent">Note we’ve introduced a new notation for the parameters:</p>&#13;
<div class="imagec"><img src="Images/205equ02.jpg" alt="image" width="29" height="35"/></div>&#13;
<p class="noindent">This represents the weight that links the <em>j</em>-th input, an output of the <em>i –</em> 1 layer, to the <em>k</em>-th node of layer <em>i</em>. We also have</p>&#13;
<div class="imagec"><img src="Images/205equ03.jpg" alt="image" width="25" height="31"/></div>&#13;
<p class="noindent">to represent the bias value for the <em>k</em>-th node of the <em>i</em>-th layer. Here layer 0 is the input layer itself. The parentheses on the exponent are a label, the layer number; they should not be interpreted as actual exponents. Therefore,</p>&#13;
<div class="imagec"><img src="Images/205equ04.jpg" alt="image" width="33" height="32"/></div>&#13;
<p class="noindent">is the weight from the third output of the first layer to the first node of the second layer. This is the highlighted weight in <a href="ch09.xhtml#ch9fig4">Figure 9-4</a>. Remember that we always number nodes top to bottom, starting with 0.</p>&#13;
<div class="image" id="ch9fig4"><img src="Images/09fig04.jpg" alt="image" width="588" height="302"/></div>&#13;
<p class="figcap"><em>Figure 9-4: The network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> with weight w<sup>(2)</sup><sub>20</sub> marked with a bold line</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_206"/>This notation is a bit daunting, but it will let us reference any weight or bias of the network precisely. The number we need to use backprop is the partial derivative of the loss with respect to each weight or bias. Therefore, what we want to find ultimately is written, in all its glorious mathematical notation, as</p>&#13;
<div class="imagec"><img src="Images/206equ01.jpg" alt="image" width="44" height="62"/></div>&#13;
<p class="noindent">This gives us the slope: the amount the loss will change for a change in the weight linking the <em>k</em>-th node of the <em>i</em>-th layer to the <em>j</em>-th output of the <em>i –</em> 1 layer. A similar equation gives us the partial derivatives of the biases.</p>&#13;
<p class="indent">We can simplify this cumbersome notation by dealing only with the layer number understanding that buried in the notation is a vector (biases, activations) or matrix (weights) so that we want to find</p>&#13;
<div class="imagec"><img src="Images/206equ02.jpg" alt="image" width="132" height="46"/></div>&#13;
<p class="noindent">These correspond to a matrix for all the weights linking layer <em>i –</em> 1 to <em>i</em>, and a vector for all the biases of layer <em>i</em>, respectively.</p>&#13;
<p class="indent">We’ll protect our notational sanity by looking at things in terms of vectors and matrices. Let’s start with the output layer and see what that buys us. We know that the activations of the output layer, layer <em>L</em>, are found via</p>&#13;
<p class="center"><em>a</em><sup>(<em>L</em>)</sup> = <em>h</em>(<em>W</em><sup>(<em>L</em>)</sup><em>a</em><sup>(<em>L</em>–1)</sup> + <em>b</em><sup>(<em>L</em>)</sup>)</p>&#13;
<p class="noindent">with <em>a</em> being the activations from layer <em>L –</em> 1, <em>b</em> the bias vector for layer <em>L</em>, and <em>W</em> the weight matrix between layers <em>L –</em> 1 and <em>L</em>. The activation function is <em>h</em>.</p>&#13;
<p class="indent">Additionally, we’ll define the argument to <em>h</em> to be <em>z</em><sup>(<em>L</em>)</sup></p>&#13;
<p class="center"><em>z</em><sup>(<em>L</em>)</sup> ≡ <em>W</em><sup>(<em>L</em>)</sup><em>a</em><sup>(<em>L</em>–1)</sup> + <em>b</em><sup>(<em>L</em>)</sup></p>&#13;
<p class="noindent">and call <em>∂L</em>/<em>∂z</em><sup>(<em>l</em>)</sup> the <em>error</em>, the contribution to the loss from the inputs to layer <em>l</em>. Next, we define</p>&#13;
<div class="imagec"><img src="Images/206equ03.jpg" alt="image" width="94" height="45"/></div>&#13;
<p class="noindent">so that we can work with <em>δ</em> (delta) from now on.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_207"/>For the output layer, we can write <em>δ</em> as</p>&#13;
<div class="imagec"><img src="Images/207equ01.jpg" alt="image" width="250" height="45"/></div>&#13;
<p class="noindent">The notation <em>h</em>′(<em>z</em><sup>(<em>L</em>)</sup>) is another way to write the derivative of <em>h</em> (with respect to <em>z</em>) evaluated at <em>z</em><sup>(<em>L</em>)</sup>. The ⋅ represents elementwise multiplication. This is the way NumPy works when multiplying two arrays of the same size so that if <em>C</em> = <em>A</em> ⋅ <em>B</em>, then <em>C</em><sub><em>ij</em></sub> = <em>A</em><sub><em>ij</em></sub><em>B</em><sub><em>ij</em></sub>. Technically, this product is called the <em>Hadamard product</em>, named for the French mathematician Jacques Hadamard.</p>&#13;
<p class="indent">The preceding means that to use backpropagation, we need a loss function that can be differentiated—a loss function for which a derivative exists at every point. This isn’t too much of a burden; the loss functions we’ll examine in the next section meet this criterion. We also need an activation function that can be differentiated so we can find <em>h</em>(<em>z</em>). Again, the activation functions we have considered so far are essentially all differentiable.</p>&#13;
<p class="note"><strong><span class="black">Note</span></strong> <em>I say “essentially” because the derivative of the ReLU is undefined at</em> x = 0<em>. The derivative from the left is 0 while the derivative from the right is 1. In practice, implementations choose a particular value to return should the argument to the derivative of the ReLU be exactly 0. For example, TensorFlow simply asks if the argument is less than or equal to 0 and, if it is, returns 0 as the derivative. Otherwise, it returns 1. This works because, numerically, there is so much rounding off happening to floating-point values during calculations that it’s unlikely the value passed to the derivative of the ReLU function was actually meant to be identically 0.</em></p>&#13;
<p class="indent">The equation for <em>δ</em> tells us the error due to the inputs to a particular layer. We’ll see next how to use this to get the error from each weight of a layer.</p>&#13;
<p class="indent">With <em>δ</em><sup>(<em>L</em>)</sup> in hand, we can propagate the error down to the next layer via</p>&#13;
<p class="center"><em>δ</em><sup>(<em>l</em>)</sup> = ((<em>W</em><sup>(<em>l</em>+1)</sup>)<sup><em>Tδ</em><sup><em>l</em>+1</sup></sup>) · <em>h</em>′(<em>z</em><sup>(<em>l</em>)</sup>)</p>&#13;
<p class="noindent">where, for the next-to-last layer, <em>l</em> + 1 = <em>L</em>. The <em>T</em> represents matrix transpose. This is a standard matrix operation that involves a reflection across the diagonal so that if</p>&#13;
<div class="imagec"><img src="Images/207equ02.jpg" alt="image" width="133" height="76"/></div>&#13;
<p class="noindent">then</p>&#13;
<div class="imagec"><img src="Images/207equ03.jpg" alt="image" width="145" height="76"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_208"/>We need the transpose of the weight matrix because we are going in the opposite direction from the forward pass. If there are three nodes in layer <em>l</em> and two in layer <em>l</em> + 1, then the weight matrix between them, <em>W</em>, is a 2 × 3 matrix, so <em>Wx</em> is a two-element vector. In backprop, we are going from layer <em>l</em> + 1 to layer <em>l</em>, so we transpose the weight matrix to map the two-element vector, here <em>δ</em>, to a three-element vector for layer <em>l</em>.</p>&#13;
<p class="indent">The <em>δ</em><sup>(<em>l</em>)</sup> equation is used for every layer moving backward through the network. The output layer values are given by <em>δ</em><sup>(<em>L</em>)</sup>, which starts the process.</p>&#13;
<p class="indent">Once we have the errors per layer, we can finally find the gradient values we need. For the biases, the values are the elements of <em>δ</em> for that layer</p>&#13;
<div class="imagec"><img src="Images/208equ01.jpg" alt="image" width="89" height="61"/></div>&#13;
<p class="noindent">for the <em>j</em>-th element of the bias for the <em>l</em>-th layer. For the weights, we need</p>&#13;
<div class="imagec"><img src="Images/208equ02.jpg" alt="image" width="138" height="61"/></div>&#13;
<p class="noindent">linking the <em>k</em>-th output of the previous layer to the <em>j</em>-th error for the current layer, <em>l</em>.</p>&#13;
<p class="indent">Using the preceding equations for each layer of the network gives us the set of weight and bias gradient values needed to continue applying gradient descent.</p>&#13;
<p class="indent">As I hope you can see from this rather dense section, we can use a convenient mathematical definition of the error to set up an iterative process that moves the error from the output of the network back through the layers of the network to the input layer. We cannot calculate the errors for a layer without already knowing the errors for the layer after it, so we end up propagating the error backward through the network, hence the name <em>backpropagation</em>.</p>&#13;
<h3 class="h3" id="lev1_57">Loss Functions</h3>&#13;
<p class="noindent">The <em>loss function</em> is used during training to measure how poorly the network is doing. The goal of training is to make this value as small as possible, while still generalizing to the true characteristics of the data. In theory, we can create any loss function we want if we feel it’s relevant to the problem at hand. If you read the deep learning literature, you’ll see papers do this all the time. Still, most research falls back on a few standard loss functions that, empirically, do a good job most of the time. We’ll discuss three of those here: absolute loss (sometimes called <em>L</em><sub>1</sub> loss), mean squared error (sometimes called <em>L</em><sub>2</sub> loss), and cross-entropy loss.</p>&#13;
<h4 class="h4" id="lev2_90"><span epub:type="pagebreak" id="page_209"/>Absolute and Mean Squared Error Loss</h4>&#13;
<p class="noindent">Let’s start with the absolute and mean squared error loss functions. We’ll discuss them together because they’re very similar mathematically.</p>&#13;
<p class="indent">We’ve seen mean squared error already in our discussion of backprop. Absolute loss is new. Mathematically, the two equations are</p>&#13;
<div class="imagec"><img src="Images/209equ01.jpg" alt="image" width="146" height="109"/></div>&#13;
<p class="noindent">where we’ve labeled them <em>abs</em> for <em>absolute value</em> and <em>MSE</em> for <em>mean squared error</em>, respectively. Note that we’ll always use <em>y</em> for the network output, the output from the forward pass with input <em>x</em>. We’ll always use <em>ŷ</em> for the known training class label, which is always an integer label starting with 0.</p>&#13;
<p class="indent">Even though we’re writing the loss functions in a simple form, we need to remember that when used, the value is really the mean of the loss over the training set or minibatch. This is also the origin of <em>mean</em> in <em>mean squared error</em>. Therefore, we really should be writing this:</p>&#13;
<div class="imagec"><img src="Images/209equ02.jpg" alt="image" width="374" height="64"/></div>&#13;
<p class="noindent">Here we’re finding the average of the squared error loss over the <em>N</em> values in the training set (or minibatch).</p>&#13;
<p class="indent">Both of these loss functions are reasonable if we consider what they are measuring. We want the network to output a value that matches the expected value, the sample label. The difference between these two is an indication of how wrong the network output is. For the absolute loss, we find the difference and drop the sign, which is what the absolute value does. For the MSE loss, we find the difference and then square it. This also makes the difference positive because multiplying a negative number by itself always results in a positive number. As mentioned in the “Backpropagation” section on <a href="#lev1_56">page 200</a>, the <span class="middle"><img src="Images/1by2.jpg" alt="Image"/></span> factor on the MSE loss simplifies the derivative of the loss function but does not change how it works.</p>&#13;
<p class="indent">The absolute loss and MSE are different, however. The MSE is more sensitive to outliers. This is because we’re squaring the difference, and a plot of <em>y</em> = <em>x</em><sup>2</sup> grows quickly as <em>x</em>, the difference, gets larger. For the absolute loss, this effect is minimized because there is no squaring; the difference is merely the difference.</p>&#13;
<p class="indent">In truth, neither of these loss functions are commonly used for neural networks when the goal of the network is <em>classification</em>, which is our implicit <span epub:type="pagebreak" id="page_210"/>assumption in this book. It’s more common to use the cross-entropy loss, presented next. We want the network output to lead to the correct class label for the input. However, it’s entirely possible to train a network to output a continuous real value instead. This is called <em>regression</em>, and both of these loss functions are quite useful in that context.</p>&#13;
<h4 class="h4" id="lev2_91">Cross-Entropy Loss</h4>&#13;
<p class="noindent">Even though we can use the absolute and MSE loss functions in training a neural network for classification, the most commonly used loss is the <em>cross-entropy loss</em> (closely related to the log-loss). This loss function assumes the output of the network is a softmax (vector) for the multiclass case or a sigmoid (logistic, scalar) for the binary case. Mathematically, it looks like this for <em>M</em> classes in the multiclass case:</p>&#13;
<div class="imagec"><img src="Images/210equ01.jpg" alt="image" width="410" height="130"/></div>&#13;
<p class="noindent">What is the cross-entropy doing that often makes it a better choice for training a neural network for classification? Let’s think about the multiclass case with softmax outputs. The definition of <em>softmax</em> means that the network outputs can be thought of as probability estimates of the likelihood that the input represents each of the possible classes. If we have three classes, we might get a softmax output that looks like this:</p>&#13;
<p class="center"><em>y</em> = (0.03, 0.87, 0.10)</p>&#13;
<p class="noindent">This output roughly means that the network thinks there is a 3 percent chance the input is of class 0, an 87 percent chance it is of class 1, and a 10 percent chance it is of class 2. This is the output vector, <em>y</em>. We compute the loss by supplying the actual label via a vector where 0 means <em>not this class</em> and 1 means <em>this class</em>. So, the <em>ŷ</em> vector associated with the input that led to this <em>y</em> would be</p>&#13;
<p class="center"><em>ŷ</em> = (0,1,0)</p>&#13;
<p class="noindent">for an overall loss value of</p>&#13;
<p class="center"><em>ℒ</em><sub>ent</sub> = –(0(log 0.03) + 1(log 0.87) + 0(log 0.10)) = 0.139262</p>&#13;
<p class="indent">The three predictions of the network can be thought of together as a probability distribution, just like the one we get when we sum together the likelihoods of different outcomes for throwing two dice. We also have a known probability distribution from the class label. For the preceding example, the actual class is class 1, so we made a probability distribution that <span epub:type="pagebreak" id="page_211"/>assigns no chance to classes 0 and 2, and 100 percent probability to class 1, the actual class. As the network trains, we expect the output distribution to be closer and closer to (0,1,0), the distribution for the label.</p>&#13;
<p class="indent">Minimizing the cross-entropy drives the network toward better and better predictions of the probability distribution for the different classes we want the network to learn about. Ideally, these output distributions will look like the training labels: 0 for all classes except the actual class, which has an output of 1.</p>&#13;
<p class="indent">For classification tasks, we usually use the cross-entropy loss. The sklearn <span class="literal">MLPClassifier</span> class uses cross-entropy. Keras supports cross-entropy loss as well, but provides many others, including absolute and mean squared error.</p>&#13;
<h3 class="h3" id="lev1_58">Weight Initialization</h3>&#13;
<p class="noindent">Before we can train a neural network, we need to initialize the weights and biases. Step 1 of <a href="ch09.xhtml#ch9lis1">Listing 9-1</a> on gradient descent says to “Pick some intelligent starting values for the weights and biases.”</p>&#13;
<p class="indent">The initialization techniques examined here all depend upon selecting random numbers in some range. More than that, the random numbers need to be either uniform over that range or normally distributed. <em>Uniformly distributed</em> means that all the values in the range are equally likely to be selected. This is what you get for each number, 1 through 6, if you roll a fair die many times over. Normally distributed values were introduced in <a href="ch04.xhtml#ch04">Chapter 4</a>. These are values with a particular mean, the most likely value returned, and a range around the mean over which the likelihood of a value being selected falls off gradually toward 0 according to a parameter known as the <em>standard deviation</em>. This is the classic bell curve shape. Either distribution can be used. The main point is that the initial weights are not all the same value (like 0) because if they are, all gradients will be the same during backprop, and each weight will change in the same way. The initial weights need to be different to break this symmetry and allow individual weights to adapt themselves to the training data.</p>&#13;
<p class="indent">In the early days of neural networks, people initialized the weights and biases by choosing values in [0,1) uniformly (<em>U</em>(0,1)) or by drawing them from the standard normal distribution, <em>N</em>(0,1), with a mean of 0 and a standard deviation of 1. These values were often multiplied by some small constant, like 0.01. In many cases, this approach works, at least for simple networks. However, as networks became more complex, this simple approach fell apart. Networks initialized in this way had trouble learning, and many failed to learn at all.</p>&#13;
<p class="indent">Let’s fast-forward several decades and a great deal of research later. Researchers realized that precisely how the weights of a particular layer should be initialized depended primarily on a few things: the type of activation function used and the number of weights coming into the layer (<em>f</em><sub><em>in</em></sub>) and, possibly, going out (<em>f</em><sub><em>out</em></sub>). These realizations led to the main initialization approaches in use today.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_212"/>The sklearn <span class="literal">MLPClassifier</span> class uses <em>Glorot initialization</em>. This is also sometimes called <em>Xavier initialization</em>, though some toolkits mean something different when they use that term.<sup><a id="Rch09fn1" href="ch09.xhtml#ch09fn1">1</a></sup> (Note <em>Xavier</em> and <em>Glorot</em> actually refer to the same person.) Let’s see how sklearn uses Glorot initialization. The key method in <span class="literal">MLPClassifier</span> for initializing the weights is <span class="literal">_init_coef</span>. This method uses a uniform distribution and sets the range for it so that the weights are in</p>&#13;
<div class="imagec"><img src="Images/212equ01.jpg" alt="image" width="237" height="63"/></div>&#13;
<p class="noindent">where the bracket notation indicates the smallest possible value selected (left) to the largest possible value (right). As the distribution is uniform, every value in that range is equally likely to be selected.</p>&#13;
<p class="indent">We did not yet specify what <em>A</em> is. This value depends upon the activation function used. According to the literature, if the activation function is a sigmoid (logistic), then <em>A</em> = 2 is suggested. Otherwise, <em>A</em> = 6 is recommended.</p>&#13;
<p class="indent">Now to confuse things. Some toolkits, like Caffe, use an alternate form of Xavier initialization by which they mean a multiplier on samples from a standard normal distribution. In that case, we initialize the weights with draws from</p>&#13;
<div class="imagec"><img src="Images/212equ02.jpg" alt="image" width="395" height="63"/></div>&#13;
<p class="indent">To add even more confusion, the introduction of the rectified linear unit (ReLU) resulted in a further recommended change. This is known now as <em>He initialization</em> and it replaces the 1 in Xavier initialization with a 2:</p>&#13;
<div class="imagec"><img src="Images/212equ03.jpg" alt="image" width="390" height="63"/></div>&#13;
<p class="noindent">For more on this, see “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification” by Kaiming He et al.</p>&#13;
<p class="indent">The key point with these initialization schemes is that the old-school “small random value” is replaced by a more principled set of values that take the network architecture into account via <em>f</em><sub><em>in</em></sub> and <em>f</em><sub><em>out</em></sub>.</p>&#13;
<p class="indent">The preceding discussion ignored bias values. This was intentional. While it might be okay to initialize the bias values instead of leaving them all 0, prevailing wisdom, which is fickle and fluid, currently says it’s best to initialize them all to 0. That said, sklearn <span class="literal">MLPClassifier</span> initializes the bias values in the same way as the weights.</p>&#13;
<h3 class="h3" id="lev1_59"><span epub:type="pagebreak" id="page_213"/>Overfitting and Regularization</h3>&#13;
<p class="noindent">The goal of training a model is for it to learn essential, general features of the parent distribution the dataset is sampled from. That way, when the model encounters new inputs, it’s prepared to interpret them correctly. As we’ve seen in this chapter, the primary method for training a neural network involves optimization—looking for the “best” set of parameters so that the network makes as few errors as possible on the training set.</p>&#13;
<p class="indent">However, it’s not enough to simply look for the best set of values that minimizes the training error. If we make no mistakes when classifying the training data, it’s often the case that we’ve overfitted and haven’t actually learned general features of the data. This is more likely the situation with traditional models, neural network or classical, and less so with deep models like the convolutional networks of <a href="ch12.xhtml#ch12">Chapter 12</a>.</p>&#13;
<h4 class="h4" id="lev2_92">Understanding Overfitting</h4>&#13;
<p class="noindent">We’ve mentioned overfitting from time to time before now but have not gained any good intuition as to what it is. One way to think of overfitting is to consider a separate problem, the problem of fitting a function to a set of points. This is known as <em>curve fitting</em>, and one approach to it is to optimize some measure of error over the points by finding parameters to the function that minimize the error. This should sound familiar. It’s exactly what we do when training a neural network.</p>&#13;
<p class="indent">As an example of curve fitting, consider the following points:</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><em>x</em></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><em>y</em></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab">50.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.61</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–17.8</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1.22</p></td>&#13;
<td style="vertical-align: top"><p class="tab">74.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1.83</p></td>&#13;
<td style="vertical-align: top"><p class="tab">29.9</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2.44</p></td>&#13;
<td style="vertical-align: top"><p class="tab">114.8</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3.06</p></td>&#13;
<td style="vertical-align: top"><p class="tab">55.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3.67</p></td>&#13;
<td style="vertical-align: top"><p class="tab">66.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">4.28</p></td>&#13;
<td style="vertical-align: top"><p class="tab">89.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">4.89</p></td>&#13;
<td style="vertical-align: top"><p class="tab">128.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">5.51</p></td>&#13;
<td style="vertical-align: top"><p class="tab">180.8</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">6.12</p></td>&#13;
<td style="vertical-align: top"><p class="tab">229.7</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">6.73</p></td>&#13;
<td style="vertical-align: top"><p class="tab">229.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">7.34</p></td>&#13;
<td style="vertical-align: top"><p class="tab">227.7</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">7.95</p></td>&#13;
<td style="vertical-align: top"><p class="tab">354.9</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">8.57</p></td>&#13;
<td style="vertical-align: top"><p class="tab">477.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">9.18</p></td>&#13;
<td style="vertical-align: top"><p class="tab">435.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">9.79</p></td>&#13;
<td style="vertical-align: top"><p class="tab">470.1</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">We want to find a function, <em>y</em> = <em>f</em> (<em>x</em>), that describes these points—a function that might have been the parent function these points were measured from, albeit noisily.</p>&#13;
<p class="indent">Typically, when curve fitting, we already know the form of the function; it’s the parameters we’re looking for. But what if we didn’t know the exact <span epub:type="pagebreak" id="page_214"/>form of the function, only that it was some kind of polynomial? In general, a polynomial looks like this for some maximum exponent, <em>n</em>:</p>&#13;
<p class="center"><em>y</em> = <em>a</em><sub>0</sub> + <em>a</em><sub>1</sub><em>x</em> + <em>a</em><sub>2</sub><em>x</em><sup>2</sup> + <em>a</em><sub>3</sub><em>x</em><sup>3</sup> + … + <em>a<sub>n</sub>x<sup>n</sup></em></p>&#13;
<p class="noindent">The goal of fitting a polynomial to a dataset is to find the parameters, <em>a</em><sub>0</sub>,<em>a</em><sub>1</sub>,<em>a</em><sub>2</sub>,…,<em>a</em><sub><em>n</em></sub>. The method for doing this usually minimizes the squared difference between <em>y</em>, a given output for a given <em>x</em> position, and <em>f</em> (<em>x</em>), the function output at the same <em>x</em> for the current set of parameters. This should sound very familiar, as we discussed using precisely this type of loss function for training a neural network.</p>&#13;
<p class="indent">How does this relate to overfitting? Let’s plot the previous dataset, along with the result of fitting two different functions to it. The first function is</p>&#13;
<p class="center"><em>y</em> = <em>a</em><sub>0</sub> + <em>a</em><sub>1</sub><em>x</em> + <em>a</em><sub>2</sub><em>x</em><sup>2</sup></p>&#13;
<p class="noindent">which is a quadratic function, the type of function you may have learned to hate as a beginning algebra student. The second function is</p>&#13;
<p class="center"><em>y</em> = <em>a</em><sub>0</sub> + <em>a</em><sub>1</sub><em>x</em> + <em>a</em><sub>2</sub><em>x</em><sup>2</sup> + <em>a</em><sub>3</sub><em>x</em><sup>3</sup> + … + <em>a</em><sub>14</sub><em>x</em><sup>14</sup> + <em>a</em><sub>15</sub><em>x</em><sup>15</sup></p>&#13;
<p class="noindent">which is a 15th-degree polynomial. The results are shown in <a href="ch09.xhtml#ch9fig5">Figure 9-5</a>.</p>&#13;
<div class="image" id="ch9fig5"><img src="Images/09fig05.jpg" alt="image" width="673" height="503"/></div>&#13;
<p class="figcap"><em>Figure 9-5: A dataset and two functions fit to it: a quadratic (dashed) and a 15th-degree polynomial (solid)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_215"/>Which function does a better job of capturing the general trend of the dataset? The quadratic clearly follows the general trend of the data, while the 15th-degree polynomial is all over the place. Look again at <a href="ch09.xhtml#ch9fig5">Figure 9-5</a>. If all we use to decide that we have fit the data well is the distance between the data points and the corresponding function value, we’d say that the 15th-degree polynomial is the better fit; it passes through nearly all the data points, after all. This is analogous to training a neural network and achieving perfection on the training set. The cost of that perfection might well be a poor ability to generalize to new inputs. The quadratic fit of <a href="ch09.xhtml#ch9fig5">Figure 9-5</a> did not hit the data points, but it did capture the general trend of the data, making it more useful should we want to make predictions about the <em>y</em> values we’d expect to get for a new <em>x</em> value.</p>&#13;
<p class="indent">When a human wants to fit a curve to something like our sample dataset, they usually look at the data and, noticing the general trend, select the function to fit. It might also be the case that the expected functional form is already known from theory. If we want to be analogous to neural networks, however, we’ll find ourselves in a situation where we don’t know the proper function to fit, and need to find a “best” one from the space of functions of <em>x</em> along with its parameters.</p>&#13;
<p class="indent">Hopefully, this example drives home the idea that training a neural network is not an optimization problem like other optimization problems—we need something to push the function the network is learning in a direction that captures the essence of the data without falling into the trap of paying too much attention to specific features of the training data. That something is regularization, and you need it, especially for large networks that have a huge capacity.</p>&#13;
<h4 class="h4" id="lev2_93">Understanding Regularization</h4>&#13;
<p class="noindent"><em>Regularization</em> is anything that pushes the network to learn the relevant features of the parent distribution and not the details of the training set. The best form of regularization is increasing the size and representative nature of the training set. The larger the dataset and the better it represents all the types of samples the network will encounter in the wild, the better it will learn. Of course, we’re typically forced to work with a finite training set. The machine learning community has spent, and is spending, untold time and energy learning how to get more from smaller datasets.</p>&#13;
<p class="indent">In <a href="ch05.xhtml#ch05">Chapter 5</a>, we encountered perhaps the second-best way to regularize a model, data augmentation. This is a proxy for having a larger dataset, where we use the data we do have to generate new training samples that are plausibly from the parent distribution. For example, we considered increasing a limited set of training images by simple rotations, flips, and shifts of the images already in the training set. Data augmentation is powerful, and you should use it when possible. It’s particularly easy to apply when working with images as inputs, though in <a href="ch05.xhtml#ch05">Chapter 5</a> we also saw a way to augment a dataset consisting of continuously valued vectors.</p>&#13;
<p class="indent">We now have two tricks in our regularization toolbox: more data and data augmentation. These are the best tricks to know, but there others that <span epub:type="pagebreak" id="page_216"/>you should use when available. Let’s look at two more: L2 regularization and dropout. The former is now standard and widely supported by the toolkits, including sklearn and Keras. The latter is powerful and was a game changer when it appeared in 2012.</p>&#13;
<h4 class="h4" id="lev2_94">L2 Regularization</h4>&#13;
<p class="noindent">A model with a few weights that have large values is somehow less simple than a model that has smaller weights. Therefore, keeping the weights small will hopefully allow the network to implement a simpler function better suited to the task we want it to learn.</p>&#13;
<p class="indent">We can encourage the weights to be small by using L2 regularization. <em>L2 regularization</em> adds a term to the loss function so that the loss becomes</p>&#13;
<div class="imagec"><img src="Images/216equ01.jpg" alt="image" width="226" height="55"/></div>&#13;
<p class="noindent">where the first term is whatever loss we’re already using, and the second term is the new L2 regularization term. Notice that the loss is a function of the input (<em>x</em>), the label (<em>y</em>), the weights (<em>w</em>), and the biases (<em>b</em>), where we mean all the weights and all the biases of the network. The regularization term is a sum over all the weights in the network and only the weights. The “L2” label is what causes us to square the weights.</p>&#13;
<p class="indent">Here <em>L2</em> refers to the type of norm or distance. You might be familiar with the equation for the distance between two points on the plane: <em>d</em><sup>2</sup> = (<em>x</em><sub>2</sub> <em>– x</em><sub>1</sub>)<sup>2</sup> + (<em>y</em><sub>2</sub> <em>– y</em><sub>1</sub>)<sup>2</sup>. This is the <em>Euclidean distance</em>, also known as the <em>L2 distance</em>, because the values are squared. This is why the regularization term is called <em>L2</em> and the weight values are squared. It’s also possible to use an L1 loss term, where instead of squaring the weights, one uses the absolute value. In practice, L2 regularization is more common and, at least empirically, seems to work better for neural network classifiers.</p>&#13;
<p class="indent">The <em>λ</em> (lambda) multiplier sets the importance of this term; the larger it is, the more it dominates the overall loss used to train the network. Typical values of <em>λ</em> are around 0.0005. We’ll see in a little bit why the multiplier is <em>λ</em>/2 and not just <em>λ</em>.</p>&#13;
<p class="indent">What is the L2 term doing? Recall that the loss is the thing we want to minimize while training. The new L2 term sums the squares of the weights of the network. If weights are large, the loss is large, and that’s something we don’t want while training. Smaller weights make the L2 term smaller, so gradient descent will favor small weights, whether they are positive or negative, since we square the weight value. If all the weights of the network are relatively small, and none strongly dominate, then the network will use all of the weights to represent the data, and this is a good thing when it comes to preventing overfitting.</p>&#13;
<p class="indent">L2 regularization is also known as <em>weight decay</em> because of what the L2 term does during backprop. Backprop gives us the partial derivative of the loss function with respect to <em>w</em><sub><em>i</em></sub>. Adding L2 regularization means that the partial derivative of the total loss now adds in the partial derivative of the <span epub:type="pagebreak" id="page_217"/>L2 term itself with respect to any particular weight, <em>w</em><sub><em>i</em></sub>. The derivative of <span class="middle"><img src="Images/217equ01.jpg" alt="Image" width="35" height="29"/></span> is <em>λw</em>; the <span class="middle"><img src="Images/1by2.jpg" alt="Image" width="10" height="29"/></span> cancels the factor of 2 that would otherwise be there. Also, since we want the partial derivative with respect to a specific weight, <em>w</em><sub><em>i</em></sub>, all the other parts of the L2 term go to 0. The net effect is that the update for weight <em>w</em><sub><em>i</em></sub> during gradient descent becomes</p>&#13;
<div class="imagec"><img src="Images/217equ02.jpg" alt="image" width="199" height="48"/></div>&#13;
<p class="noindent">where <em>η</em> (eta) is the learning rate, and we are ignoring any additional momentum term. The <em>ηλw</em><sub><em>i</em></sub> term is new. It is due to L2 regularization, and we can see that it’s pushing the weights toward 0 as training progresses because both <em>η</em> and <em>λ</em> are &lt; 1, so on each minibatch, we’re subtracting some small fraction of the weight value. The weight can still increase, but to do so, the gradient of the original loss must be large.</p>&#13;
<p class="indent">We previously stated that the form of the loss function is up to us, the developer of the network. A regularization term isn’t the only kind of term we can add to the loss function. As we did with the L2 term, we can create and add terms to change the behavior of the network during training and help it learn what we want it to learn. This is a powerful technique that can be used to customize various aspects of what a neural network learns.</p>&#13;
<h4 class="h4" id="lev2_95">Dropout</h4>&#13;
<p class="noindent">Dropout took the machine learning community by storm when it appeared in 2012, see “Imagenet Classification with Deep Convolutional Neural Networks” by Alex Krizhevsky et al. As of Fall 2020, this paper has been cited over 70,000 times, and as one well-known machine learning researcher told me privately at the time, “If we had had dropout in the 1980s, this would be a different world now.” So, what is dropout, and why was everyone so excited by it?</p>&#13;
<p class="indent">To answer that question, we need to review the concept of ensembles of models. We talked about them a bit in <a href="ch06.xhtml#ch06">Chapter 6</a>. An ensemble is a group of models, all slightly different and all trained on the same dataset or a slightly different version of the dataset. The idea is straightforward: since training most models involves randomness, training multiple similar models should result in a set that is mutually reinforcing—one where the set of outputs can be combined to produce a result that is better than any one model alone. Ensembles are useful, and we use them often, but they come at a price in terms of runtime. If it takes <em>x</em> milliseconds to run a sample through a neural network, and we have an ensemble of 20 networks, then our evaluation time (inference time) has jumped to 20<em>x</em> milliseconds, ignoring the possibility of parallel execution. In some situations, that is unacceptable (to say nothing of the storage and power requirements for 20 big networks versus 1). Since the net result of an ensemble of models is better overall performance, we can say that an ensemble is a kind of regularizer as well since it embodies the “wisdom of the crowd.”</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_218"/><em>Dropout</em> takes the ensemble idea to an extreme but does so only during training and without creating a second network so that in the end, we still have one model to deal with. Like many good ideas in statistics, this one requires randomness. Right now, when we train the network, we do a forward pass using the current weights and biases. What if, during that forward pass, we randomly assign a 0 or a 1 to each node of the network so that nodes with a 1 are used in the next layer while nodes with a 0 are dropped out? We’d effectively be running the training samples through a different neural network configuration each time. For example, see <a href="ch09.xhtml#ch9fig6">Figure 9-6</a>.</p>&#13;
<div class="image" id="ch9fig6"><img src="Images/09fig06.jpg" alt="image" width="677" height="402"/></div>&#13;
<p class="figcap"><em>Figure 9-6: Possible networks used when applying dropout during training</em></p>&#13;
<p class="indent">Here we show the network of <a href="ch08.xhtml#ch8fig1">Figure 8-1</a> but with a 0 or 1 for each of the hidden nodes. This 0 or 1 determines whether the output is used or not. The heavy lines in the network show the connections that are still valid. In other words, the heavy lines show the network that was actually used to create the output accumulated for backprop. If we do this for each training sample, we can readily see that we’ll be training a vast number of neural networks, each trained on a single sample. Moreover, since the weights and biases persist between forward passes, all the networks will share those weights in the hope that the process will reinforce good weight values that represent the essence of the dataset. As we’ve mentioned several times in this chapter, learning the essence of the data is the goal of training; we want to generalize well to new data from the same virtual parent distribution that generated the training set in the first place. Dropout is serious regularization.</p>&#13;
<p class="indent">I previously said that we “randomly assign a 0 or a 1” to the nodes. Do we assign them equally? The probability with which we drop nodes in a layer is something we get to specify. Let’s call it <em>p</em>. Typically, <em>p</em> = 0.5, meaning about 50 percent of the nodes in a layer will be dropped for each training sample. Setting <em>p</em> = 0.8 would drop 80 percent of the nodes, while <em>p</em> = 0.1 would drop only 10 percent. Sometimes a different probability is used for <span epub:type="pagebreak" id="page_219"/>different layers of the network, especially the first input layer, which should use a smaller probability than the hidden nodes. If we drop too many of the inputs, we’ll lose the source of the signal we’re trying to get the network to recognize. Dropout applied to the input layer can be thought of as a form of data augmentation.</p>&#13;
<p class="indent">Conceptually, dropout is training a large set of networks that share weights. The output of each of these networks can be combined with the others via a geometric mean, assuming we use a softmax output. The geometric mean of two numbers is the square root of their product. The geometric mean of <em>n</em> numbers is the <em>n</em>th root of their product. In the case of dropout, it turns out that this can be approximated by using the entire network with all the weights multiplied by the probability that they would be included. Given we said <em>p</em> is the probability that a node is dropped, the weights need to be multiplied by 1 <em>– p</em>, as that is the probability the node would not be dropped. So, if we fix <em>p</em> = 0.5 and use it for all the nodes, then the final network is the one where all the weights are divided by 2.</p>&#13;
<p class="indent">As of this writing, sklearn’s <span class="literal">MLPClassifier</span> class does not support dropout, but Keras most certainly does, so we’ll see dropout again in <a href="ch12.xhtml#ch12">Chapter 12</a>.</p>&#13;
<h3 class="h3" id="lev1_60">Summary</h3>&#13;
<p class="noindent">Because this is an important chapter, let’s review what we’ve learned in a little more depth. In this chapter, we described how to train a neural network using gradient descent and backpropagation. The overall sequence of steps is as follows:</p>&#13;
<ol>&#13;
<li class="noindent">Select the architecture of the model. This means the number of layers, their sizes, and the type of activation function.</li>&#13;
<li class="noindent">Initialize the weights and biases of the network using intelligently selected initial values.</li>&#13;
<li class="noindent">Run a minibatch of training samples through the network and compute the mean loss over the minibatch. We discussed common loss functions.</li>&#13;
<li class="noindent">Using backpropagation, calculate the contribution of each weight and bias to the overall loss for the minibatch.</li>&#13;
<li class="noindent">Using gradient descent, update the weight and bias values of the model based on the contributions found via backpropagation. We discussed stochastic gradient descent and its relationship to the concept of minibatches.</li>&#13;
<li class="noindent">Repeat from step 3 until the desired number of epochs or minibatches have been processed, or the loss has dropped below some threshold, or stopped changing much, or when the score on a validation set of samples has reached its minimum value.</li>&#13;
<li class="noindent">If the network isn’t learning well, apply regularization and train again. We looked at L2 regularization and dropout in this chapter. Data augmentation, or increasing the size or representativeness of the training set, can also be thought of as regularization.</li>&#13;
</ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_220"/>The goal of training a neural network is to learn the parameters of a model that generalizes well to unseen inputs. This is the goal of all supervised machine learning. For a neural network, we know it’s able to approximate any function, with enough capacity and enough training data. Naïvely, we may think that we are doing nothing more than ordinary optimization, but, in an important sense, we are not. Perfection on the training set is often not a good thing; it’s often a sign of overfitting. Instead, we want the model to learn a function that captures the essential nature of the function implied by the training set. We use the test data to give us confidence that we’ve learned a useful function.</p>&#13;
<p class="indent">In the next chapter, we’ll get real and explore traditional neural networks through a series of experiments using sklearn.</p>&#13;
<p class="fnote1"><a id="ch09fn1" href="ch09.xhtml#Rch09fn1">1.</a> For more on these, see Glorot, Xavier, and Yoshua Bengio. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.”</p>&#13;
</div></body></html>