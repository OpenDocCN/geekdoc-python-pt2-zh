<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch07"><span epub:type="pagebreak" id="page_129"/><strong><span class="big">7</span><br/>EXPERIMENTS WITH CLASSICAL MODELS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In <a href="ch06.xhtml#ch06">Chapter 6</a>, we introduced several classical machine learning models. Let’s now take the datasets we built in <a href="ch05.xhtml#ch05">Chapter 5</a> and use them with these models to see how well they perform. We’ll use sklearn to create the models and then we’ll compare them by looking at how well they do on the held-out test sets.</p>&#13;
<p class="indent">This will give us a good overview of how to work with sklearn and help us build intuition about how the different models perform relative to one another. We’ll use three datasets: the iris dataset, both original and augmented; the breast cancer dataset; and the vector form of the MNIST handwritten digits dataset.</p>&#13;
<h3 class="h3" id="lev1_44">Experiments with the Iris Dataset</h3>&#13;
<p class="noindent">We’ll start with the iris dataset. This data set has four continuous features—the measurements of the sepal length, sepal width, petal length, and petal width—and three classes—different iris species. There are 150 samples, 50 each from the three classes. In <a href="ch05.xhtml#ch05">Chapter 5</a>, we applied PCA augmentation <span epub:type="pagebreak" id="page_130"/>to the dataset, so we actually have two versions we can work with: the original 150 samples and the 1200 augmented training samples. Both can use the same test set.</p>&#13;
<p class="indent">We’ll use sklearn to implement versions of the Nearest Centroid, <em>k</em>-NN, Naïve Bayes, Decision Tree, Random Forest, and SVM models we outlined in <a href="ch06.xhtml#ch06">Chapter 6</a>. We’ll quickly see how powerful and elegant the sklearn toolkit is since our tests are virtually all identical across the models. The only thing that changes is the particular class we instantiate.</p>&#13;
<h4 class="h4" id="lev2_53">Testing the Classical Models</h4>&#13;
<p class="noindent">The code for our initial tests is in <a href="ch07.xhtml#ch7lis1">Listing 7-1</a>.</p>&#13;
<p class="programs" id="ch7lis1">   import numpy as np<br/>&#13;
   from sklearn.neighbors import NearestCentroid<br/>&#13;
   from sklearn.neighbors import KNeighborsClassifier<br/>&#13;
   from sklearn.naive_bayes import GaussianNB, MultinomialNB<br/>&#13;
   from sklearn.tree import DecisionTreeClassifier<br/>&#13;
   from sklearn.ensemble import RandomForestClassifier<br/>&#13;
   from sklearn.svm import SVC<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
       clf.fit(x_train, y_train)<br/>&#13;
       print("    predictions  :", clf.predict(x_test))<br/>&#13;
       print("    actual labels:", y_test)<br/>&#13;
       print("    score = %0.4f" % clf.score(x_test, y_test))<br/>&#13;
       print()<br/>&#13;
<br/>&#13;
   def main():<br/>&#13;
    <span class="ent">❷</span> x = np.load("../data/iris/iris_features.npy")<br/>&#13;
       y = np.load("../data/iris/iris_labels.npy")<br/>&#13;
       N = 120<br/>&#13;
       x_train = x[:N]; x_test = x[N:]<br/>&#13;
       y_train = y[:N]; y_test = y[N:]<br/>&#13;
    <span class="ent">❸</span> xa_train=np.load("../data/iris/iris_train_features_augmented.npy")<br/>&#13;
       ya_train=np.load("../data/iris/iris_train_labels_augmented.npy")<br/>&#13;
       xa_test =np.load("../data/iris/iris_test_features_augmented.npy")<br/>&#13;
       ya_test =np.load("../data/iris/iris_test_labels_augmented.npy")<br/>&#13;
<br/>&#13;
       print("Nearest Centroid:")<br/>&#13;
    <span class="ent">❹</span> run(x_train, y_train, x_test, y_test, NearestCentroid())<br/>&#13;
       print("k-NN classifier (k=3):")<br/>&#13;
       run(x_train, y_train, x_test, y_test,<br/>&#13;
           KNeighborsClassifier(n_neighbors=3))<br/>&#13;
       print("Naive Bayes classifier (Gaussian):")<br/>&#13;
    <span class="ent">❺</span> run(x_train, y_train, x_test, y_test, GaussianNB())<br/>&#13;
       print("Naive Bayes classifier (Multinomial):")<br/>&#13;
       run(x_train, y_train, x_test, y_test, MultinomialNB())<br/>&#13;
<span epub:type="pagebreak" id="page_131"/>    <span class="ent">❻</span> print("Decision Tree classifier:")<br/>&#13;
       run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())<br/>&#13;
       print("Random Forest classifier (estimators=5):")<br/>&#13;
       run(xa_train, ya_train, xa_test, ya_test,<br/>&#13;
           RandomForestClassifier(n_estimators=5))<br/>&#13;
<br/>&#13;
    <span class="ent">❼</span> print("SVM (linear, C=1.0):")<br/>&#13;
       run(xa_train, ya_train, xa_test, ya_test, SVC(kernel="linear", C=1.0))<br/>&#13;
       print("SVM (RBF, C=1.0, gamma=0.25):")<br/>&#13;
       run(xa_train, ya_train, xa_test, ya_test,<br/>&#13;
           SVC(kernel="rbf", C=1.0, gamma=0.25))<br/>&#13;
       print("SVM (RBF, C=1.0, gamma=0.001, augmented)")<br/>&#13;
       run(xa_train, ya_train, xa_test, ya_test,<br/>&#13;
           SVC(kernel="rbf", C=1.0, gamma=0.001))<br/>&#13;
    <span class="ent">❽</span> print("SVM (RBF, C=1.0, gamma=0.001, original)")<br/>&#13;
       run(x_train, y_train, x_test, y_test,<br/>&#13;
           SVC(kernel="rbf", C=1.0, gamma=0.001))</p>&#13;
<p class="figcap"><em>Listing 7-1: Classic models using the iris dataset. See</em> iris_experiments.py.</p>&#13;
<p class="indent">First, we import the necessary classes and modules. Notice that each of the classes represents a single type of model (classifier). For the Naïve Bayes classifier, we’re using two versions: the Gaussian version, <code>GaussianNB</code>, because the features are continuous values, and <code>MultinomialNB</code> for the discrete case to illustrate the effect of choosing a model that’s inappropriate for the dataset we’re working with. Because sklearn has a uniform interface for its classifiers, we can simplify things by using the same function to train and test any particular classifier. That function is <code>run</code> <span class="ent">❶</span>. We pass in the training features (<code>x_train</code>) and labels (<code>y_train</code>) along with the test features and labels (<code>x_test</code>, <code>y_test</code>). We also pass in the particular classifier object (<code>clf</code>).</p>&#13;
<p class="indent">The first thing we do inside <code>run</code> is fit the model to the data by calling <code>fit</code> with the training data samples and labels. This is the training step. After the model is trained, we can test how well it does by calling the <code>predict</code> method with the held-out test data. This method returns the predicted class label for each sample in the test data. We held back 30 samples from the original 150 so <code>predict</code> will return a vector of 30 class label assignments, which we print. Next, we print the actual test labels so we can compare them visually with the predictions. Finally, we use the <code>score</code> method to apply the classifier to the test data (<code>x_test</code>) using the known test labels (<code>y_test</code>) to calculate the overall accuracy.</p>&#13;
<p class="indent">The accuracy is returned as a fraction between 0 and 1. If every test sample were given the wrong label, the accuracy would be 0. Even random guessing will do better than that, so a return value of 0 is a sign that something is amiss. Since there are three classes in the iris dataset, we’d expect a classifier that guesses the class at random to be right about one-third of the time and return a value close to 0.3333. The actual score is calculated as</p>&#13;
<p class="center">score = <em>N<sub>c</sub></em>/(<em>N<sub>c</sub></em> + <em>N<sub>w</sub></em>)</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_132"/>where <em>N</em><sub><em>c</em></sub> is the number of test samples for which the predicted class is correct; that is, it matches the class label in <code>y_test</code>. <em>N</em><sub><em>w</em></sub> is the number of test samples where the predicted class does not match the actual class label.</p>&#13;
<p class="indent">Now that we have a way to train and test each classifier, all we need to do is load the datasets and run a series of experiments by creating different classifier objects and passing them to <code>run</code>. Back inside of <code>main</code>, we begin by loading the original iris dataset and separating it into train and test cases <span class="ent">❷</span>. We also load the augmented iris dataset that we created in <a href="ch05.xhtml#ch05">Chapter 5</a> <span class="ent">❸</span>. By design, the two test sets are identical, so regardless of which training set we use, the test set will be the same. This simplifies our comparisons.</p>&#13;
<p class="indent">We then define and execute the Nearest Centroid classifier <span class="ent">❹</span>. The output is shown here:</p>&#13;
<pre>Nearest Centroid:<br/>&#13;
 predictions  :[011202120211112202201101102211]<br/>&#13;
 actual labels:[011202120211112202201101102211]<br/>&#13;
 score = 1.0000</pre>&#13;
<p class="indent">We’ve removed spaces to make a visual comparison between the predicted and actual class labels easier. If there’s an error, the corresponding value, 0–2, will not match between the two lines. The score is also shown. In this case, it’s 1.0, which tells us that the classifier was perfect in its predictions on the held-out test set. This isn’t surprising; the iris dataset is a simple one. Because the iris dataset was randomized when created in <a href="ch05.xhtml#ch05">Chapter 5</a>, you might get a different overall score. However, unless your randomization was particularly unfortunate, you should have a high test score.</p>&#13;
<p class="indent">Based on what we learned in <a href="ch06.xhtml#ch06">Chapter 6</a>, we should expect that if the Nearest Centroid classifier is perfect on the test data, then all the other more sophisticated models will likewise be perfect. This is generally the case here, but as we’ll see, careless selection of model type or model hyperparameter values will result in inferior performance even from a more sophisticated model.</p>&#13;
<p class="indent">Look again at <a href="ch07.xhtml#ch7lis1">Listing 7-1</a>, where we train a Gaussian Naïve Bayes classifier by passing an instance of <code>GaussianNB</code> to <code>run</code> <span class="ent">❺</span>. This classifier is also perfect and returns a score of 1.0. This is the correct way to use continuous values with a Naïve Bayes classifier. What happens if we instead use the discrete case even though we have continuous features? This is the <code>MultinomialNB</code> classifier, which assumes the features are selected from a discrete set of possible values. For the iris dataset, we can get away with defining such a classifier because the feature values are non-negative. However, because the features are not discrete, this model is not perfect and returns the following:</p>&#13;
<pre>Naive Bayes classifier (Multinomial):<br/>&#13;
 predictions  :[011202220211122202202101102221]<br/>&#13;
 actual labels:[011202120211112202201101102211]<br/>&#13;
 score = 0.8667</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_133"/>Here we see that the classifier is only 86.7 percent accurate on our test samples. If we need discrete counts for the probabilities, why did this approach work at all in this case? The answer is evident in the sklearn source code for the <code>MultinomialNB</code> classifier. The method that counts feature frequencies per class uses <code>np.dot</code> so that even if the feature values are continuous, the output will be a valid number, though not an integer. Still, mistakes were made, so we shouldn’t be happy. We should instead be careful to select the proper classifier type for the actual data we’re working with.</p>&#13;
<p class="indent">The next model we train in <a href="ch07.xhtml#ch7lis1">Listing 7-1</a> is a Decision Tree <span class="ent">❻</span>. This classifier is perfect on this dataset, as is the Random Forest trained next. Note, the Random Forest is using five estimators, meaning five random trees are created and trained; voting between the individual outputs determines the final class label. Note also that the Random Forest is trained on the augmented iris dataset, <code>xa_train</code>, because of the limited number of training samples in the unaugmented dataset.</p>&#13;
<p class="indent">We then train several SVM classifiers <span class="ent">❼</span>, also on the augmented dataset. Recall that SVMs have two parameters we control: the margin constant, <code>C</code>, and <code>gamma</code> used by the Gaussian kernel.</p>&#13;
<p class="indent">The first is a linear SVM, meaning we need a value for the margin constant (<code>C</code>). We define <code>C</code> to be 1.0, the default value for sklearn. This classifier is perfect on the test data, as is the following classifier using the Gaussian kernel, for which we also set <em>γ</em> to 0.25. The <code>SVC</code> class defaults to <code>auto</code> for <code>gamma</code>, which sets <em>γ</em> to 1/<em>n</em>, where <em>n</em> is the number of features. For the iris dataset, <em>n</em> = 4 so <em>γ</em> = 0.25.</p>&#13;
<p class="indent">Next, we train a model with very small <em>γ</em>. The classifier is still perfect on the test data. Lastly, we train the same type of SVM, but instead of the augmented training data, we use the original training data <span class="ent">❽</span>. This classifier is not perfect:</p>&#13;
<pre>SVM (RBF, C=1.0, gamma=0.001, original)<br/>&#13;
 predictions  :[022202020222222202202202202220]<br/>&#13;
 actual labels:[011202120211112202201101102211]<br/>&#13;
 score = 0.5667</pre>&#13;
<p class="indent">In fact, it’s rather dismal. It never predicts class 1 and is right only 56.7 percent of the time. This shows that data augmentation is valuable as it turned a lousy classifier into a good one—at least, good as far as we can know from the small test set we are using!</p>&#13;
<h4 class="h4" id="lev2_54">Implementing a Nearest Centroid Classifier</h4>&#13;
<p class="noindent">What if we were stranded on a deserted island and didn’t have access to sklearn? Could we still quickly build a suitable classifier for the iris dataset? The answer is “yes,” as <a href="ch07.xhtml#ch7lis2">Listing 7-2</a> shows. This code implements a quick-and-dirty Nearest Centroid classifier for the iris dataset.</p>&#13;
<p class="programs" id="ch7lis2"><span epub:type="pagebreak" id="page_134"/>  import numpy as np<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> def centroids(x,y):<br/>&#13;
       c0 = x[np.where(y==0)].mean(axis=0)<br/>&#13;
       c1 = x[np.where(y==1)].mean(axis=0)<br/>&#13;
       c2 = x[np.where(y==2)].mean(axis=0)<br/>&#13;
       return [c0,c1,c2]<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> def predict(c0,c1,c2,x):<br/>&#13;
       p = np.zeros(x.shape[0], dtype="uint8")<br/>&#13;
       for i in range(x.shape[0]):<br/>&#13;
           d = [((c0-x[i])**2).sum(),<br/>&#13;
                ((c1-x[i])**2).sum(),<br/>&#13;
                ((c2-x[i])**2).sum()]<br/>&#13;
           p[i] = np.argmin(d)<br/>&#13;
       return p<br/>&#13;
<br/>&#13;
  def main():<br/>&#13;
    <span class="ent">❸</span> x = np.load("../data/iris/iris_features.npy")<br/>&#13;
       y = np.load("../data/iris/iris_labels.npy")<br/>&#13;
       N = 120<br/>&#13;
       x_train = x[:N]; x_test = x[N:]<br/>&#13;
       y_train = y[:N]; y_test = y[N:]<br/>&#13;
       c0, c1, c2 = centroids(x_train, y_train)<br/>&#13;
       p = predict(c0,c1,c2, x_test)<br/>&#13;
       nc = len(np.where(p == y_test)[0])<br/>&#13;
       nw = len(np.where(p != y_test)[0])<br/>&#13;
       acc = float(nc) / (float(nc)+float(nw))<br/>&#13;
       print("predicted:", p)<br/>&#13;
       print("actual   :", y_test)<br/>&#13;
       print("test accuracy = %0.4f" % acc)</p>&#13;
<p class="figcap"><em>Listing 7-2: A quick-and-dirty Nearest Centroid classifier for the iris dataset. See</em> iris_centroids.py.</p>&#13;
<p class="indent">We load the iris data and separate it into train and test sets as before <span class="ent">❸</span>. The <code>centroids</code> function returns the centroids of the three classes <span class="ent">❶</span>. We can easily calculate these by finding the per feature means of each training sample of the desired class. This is all it takes to train this model. If we compare the returned centroids with those in the preceding trained <code>NearestCentroid</code> classifier (see the <code>centroids_</code> member variable), we get precisely the same values.</p>&#13;
<p class="indent">Using the classifier is straightforward, as <code>predict</code> shows <span class="ent">❷</span>. First, we define the vector of predictions, one per test sample (<code>x</code>). The loop defines <code>d</code>, a vector of Euclidean distances from the current test sample, <code>x[i]</code>, to the three class centroids. The index of the smallest distance in <code>d</code> is the predicted class label (<code>p[i]</code>).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_135"/>Let’s unpack <code>d</code> a bit more. We set <code>d</code> to a list of three values, the distances from the centroids to the current test sample. The expression</p>&#13;
<pre>((c0-x[i])**2).sum()</pre>&#13;
<p class="noindent">is a bit dense. The phrase <code>c0-x[i]</code> returns a vector of four numbers—four because we have four features. These are the differences between the centroid of class 0 and the test sample feature value. This quantity is squared, which squares each of the four values. This squared vector is summed, element by element, to return the distance measure.</p>&#13;
<p class="indent">Strictly speaking, we’re missing a final step. The actual distance between <code>c0</code> and <code>x[i]</code> is the square root of this value. Since we’re simply looking for the smallest distance to each of the centroids, we don’t need to calculate the square root. The smallest value will still be the smallest value, whether we take the square root of all the values or not. Running this code produces the same output as we saw previously for the Nearest Centroid classifier, which is encouraging.</p>&#13;
<p class="indent">The iris dataset is extremely simple, so we shouldn’t be surprised by the excellent performance of our models even though we saw that careless selection of model type and hyperparameters will cause us trouble. Let’s now look at a larger dataset with more features, one that was not meant as a toy.</p>&#13;
<h3 class="h3" id="lev1_45">Experiments with the Breast Cancer Dataset</h3>&#13;
<p class="noindent">The two-class breast cancer dataset we developed in <a href="ch05.xhtml#ch05">Chapter 5</a> has 569 samples, each with 30 features, all measurements from a histology slide. There are 212 malignant cases (class 1) and 357 benign cases (class 0). Let’s train our classic models on this dataset and see what sort of results we get. As all the features are continuous, let’s use the normalized version of the dataset. Recall that a normalized dataset is one where, per feature in the feature vector, each value has the mean for that feature subtracted and then is divided by the standard deviation:</p>&#13;
<div class="imagec"><img src="Images/135equ01.jpg" alt="image" width="78" height="43"/></div>&#13;
<p class="indent">Normalization of the dataset maps all the features into the same overall range so that the value of one feature is similar to the value of another. This helps many model types and is a typical data preprocessing step, as we discussed in <a href="ch04.xhtml#ch04">Chapter 4</a>.</p>&#13;
<h4 class="h4" id="lev2_55">Two Initial Test Runs</h4>&#13;
<p class="noindent">First, we’ll do a quick run with a single test split, as we did in the previous section. The code is in <a href="ch07.xhtml#ch7lis3">Listing 7-3</a> and mimics the code we described previously, where we pass in the model instance, train it, and then score it using the testing data.</p>&#13;
<p class="programs" id="ch7lis3"><span epub:type="pagebreak" id="page_136"/>import numpy as np<br/>&#13;
from sklearn.neighbors import NearestCentroid<br/>&#13;
from sklearn.neighbors import KNeighborsClassifier<br/>&#13;
from sklearn.naive_bayes import GaussianNB, MultinomialNB<br/>&#13;
from sklearn.tree import DecisionTreeClassifier<br/>&#13;
from sklearn.ensemble import RandomForestClassifier<br/>&#13;
from sklearn.svm import SVC<br/>&#13;
<br/>&#13;
def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    clf.fit(x_train, y_train)<br/>&#13;
    print("    score = %0.4f" % clf.score(x_test, y_test))<br/>&#13;
    print()<br/>&#13;
<br/>&#13;
def main():<br/>&#13;
    x = np.load("../data/breast/bc_features_standard.npy")<br/>&#13;
    y = np.load("../data/breast/bc_labels.npy")<br/>&#13;
  <span class="ent">❶</span> N = 455<br/>&#13;
    x_train = x[:N];  x_test = x[N:]<br/>&#13;
    y_train = y[:N];  y_test = y[N:]<br/>&#13;
<br/>&#13;
    print("Nearest Centroid:")<br/>&#13;
    run(x_train, y_train, x_test, y_test, NearestCentroid())<br/>&#13;
    print("k-NN classifier (k=3):")<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        KNeighborsClassifier(n_neighbors=3))<br/>&#13;
    print("k-NN classifier (k=7):")<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        KNeighborsClassifier(n_neighbors=7))<br/>&#13;
    print("Naive Bayes classifier (Gaussian):")<br/>&#13;
    run(x_train, y_train, x_test, y_test, GaussianNB())<br/>&#13;
    print("Decision Tree classifier:")<br/>&#13;
    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())<br/>&#13;
    print("Random Forest classifier (estimators=5):")<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        RandomForestClassifier(n_estimators=5))<br/>&#13;
    print("Random Forest classifier (estimators=50):")<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        RandomForestClassifier(n_estimators=50))<br/>&#13;
    print("SVM (linear, C=1.0):")<br/>&#13;
    run(x_train, y_train, x_test, y_test, SVC(kernel="linear", C=1.0))<br/>&#13;
    print("SVM (RBF, C=1.0, gamma=0.03333):")<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        SVC(kernel="rbf", C=1.0, gamma=0.03333))</p>&#13;
<p class="figcap"><em>Listing 7-3: Initial models using the breast cancer dataset. See</em> bc_experiments.py.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_137"/>As before, we load the dataset and split it into training and testing data. We keep 455 of the 569 samples for training (80 percent), and the remaining 114 samples are the test set (74 benign, 40 malignant). The dataset is already randomized, so we skip that step here. We then train nine models: Nearest Centroid (1), <em>k</em>-NN (2), Naïve Bayes (1), Decision Tree (1), Random Forest (2), linear SVM (1), and an RBF SVM (1). For the Support Vector Machines, we use the default <em>C</em> value, and for <em>γ</em>, we use 1/30 = 0.033333 since we have 30 features. Running this code gives us the scores in <a href="ch07.xhtml#ch7tab1">Table 7-1</a>.</p>&#13;
<p class="tabcap" id="ch7tab1"><strong>Table 7-1:</strong> Breast Cancer Model Scores</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model type</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Score</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9649</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-NN classifier</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9912</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">7-NN classifier</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9737</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes (Gaussian)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9825</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9474</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9298</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9737</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Linear SVM (C = 1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9737</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">RBF SVM (C = 1, <em>γ</em> = 0.03333)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9825</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">Note the number in parentheses for the Random Forest classifiers is the number of estimators (number of trees in the forest).</p>&#13;
<p class="indent">A few things jump out at us. First, perhaps surprisingly, the simple Nearest Centroid classifier is right nearly 97 percent of the time. We also see that all the other classifiers are doing better than the Nearest Centroid, except for the Decision Tree and the Random Forest with five trees. Somewhat surprisingly, the Naïve Bayes classifier does very well, matching the RBF SVM. The <em>k</em> = 3 Nearest Neighbor classifier does best of all, 99 percent accurate, even though we have 30 features, meaning our 569 samples are points scattered in a 30-dimensional space. Recall, a weakness of <em>k</em>-NN is the curse of dimensionality: it requires more and more training samples as the number of features increases. The results with all the classifiers are good, so this is a hint to us that the separation between malignant and benign is, for this dataset, distinct. There isn’t much overlap between the two classes using these features.</p>&#13;
<p class="indent">So, are we done with this dataset? Hardly! In fact, we’ve just begun. What happens if we run the code a second time? Do we get the same scores? Would we expect not to? A second run gives us <a href="ch07.xhtml#ch7tab2">Table 7-2</a>.</p>&#13;
<p class="tabcap" id="ch7tab2"><span epub:type="pagebreak" id="page_138"/><strong>Table 7-2:</strong> Breast Cancer Scores, Second Run</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model type</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Score</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9649</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-NN classifier</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9912</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">7-NN classifier</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9737</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes (Gaussian)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9825</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0.9386</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0.9474</strong></p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0.9649</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Linear SVM (C = 1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9737</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">RBF SVM (C = 1, <em>γ</em> = 0.03333)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9825</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">We’ve highlighted the scores that changed. Why would anything change? A bit of reflection leads to an <em>aha!</em> moment: the Random Forest is just that, random, so naturally we’d expect different results run to run. What about the Decision Tree? In sklearn, the Decision Tree classifier will randomly select a feature and find the best split, so different runs will also lead to different trees. This is a variation on the basic decision tree algorithm we discussed in <a href="ch06.xhtml#ch06">Chapter 6</a>.</p>&#13;
<p class="indent">All the other algorithms are fixed: for a given training dataset, they can lead to only one model. As an aside, the SVM implementation in sklearn does use a random number generator, so at times different runs will give slightly different results, but, conceptually, we’d expect the same model for the same input data. The tree-based classifiers, however, do change between training runs. We’ll explore this variation more next. For now, we need to add some rigor to our quick analysis.</p>&#13;
<h4 class="h4" id="lev2_56">The Effect of Random Splits</h4>&#13;
<p class="noindent">Let’s change the split between training and testing data and see what happens to our results. We don’t need to list all the code again since the only change is to how <code>x_train</code> and <code>x_test</code> are defined. Before splitting, we randomize the order of the full dataset but do so by first fixing the pseudorandom number seed so that each run gives the same ordering to the dataset.</p>&#13;
<p class="indent">Looking again at <a href="ch07.xhtml#ch7lis3">Listing 7-3</a>, insert the following code before <span class="ent">❶</span> so that we generate a fixed permutation of the dataset (<code>idx</code>).</p>&#13;
<pre>np.random.seed(12345)<br/>&#13;
idx = np.argsort(np.random.random(y.shape[0]))<br/>&#13;
x = x[idx]<br/>&#13;
y = y[idx]</pre>&#13;
<p class="noindent">It’s fixed because we fixed the pseudorandom number generator seed value. We then reorder the samples (<code>x</code>) and labels (<code>y</code>) accordingly before splitting into train and test subsets as before. Running this code gives us the results in <a href="ch07.xhtml#ch7tab3">Table 7-3</a>.</p>&#13;
<p class="tabcap" id="ch7tab3"><span epub:type="pagebreak" id="page_139"/><strong>Table 7-3:</strong> Breast Cancer Scores After Randomizing the Dataset</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model type</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Score</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9474</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-NN classifier</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9912</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">7-NN classifier</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9912</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes (Gaussian)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9474</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9474</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9912</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Linear SVM (C = 1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9649</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">RBF SVM (C = 1, <em>γ</em> = 0.03333)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9737</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">Notice these are entirely different from our earlier results. The <em>k</em>-NN classifiers are both equally good, the SVM classifiers are worse, and the 50-tree Random Forest achieves perfection on the test set. So, what is happening? Why are we getting all these different results run to run?</p>&#13;
<p class="indent">We’re seeing the effect of the random sampling that builds the train and test splits. The first split just happened to use an ordering of samples that gave good results for one model type and less good results for other model types. The new split favors different model types. Which is correct? Both. Recall what the dataset represents: a sampling from some unknown parent distribution that generates the data that we actually have. If we think in those terms, we see that the dataset we have is an incomplete picture of the true parent distribution. It has biases, though we don’t know what they are necessarily, and is deficient in that there are parts of the parent distribution that the dataset does not represent well.</p>&#13;
<p class="indent">Further, when we split the data after randomizing the order, we might end up with a “bad” mix in the train or test portion—a mix of the data that does a poor job of representing the true distribution. If so, we might train a model to recognize a slightly different distribution that does not match the true distribution well, or the test set might be a bad mix and not be a fair representation of what the model has learned. This effect is even more pronounced when the proportion of the classes is such that one or more are rare and possibly not present in the train or test split. This is precisely the issue that caused us to introduce the idea of <em>k</em>-fold cross-validation in <a href="ch04.xhtml#ch04">Chapter 4</a>. With <em>k</em>-fold validation, we’ll be sure to use every sample as both train and test at some point and buy ourselves some protection against a bad split by averaging across all the folds.</p>&#13;
<p class="indent">However, before we apply <em>k</em>-fold validation to the breast cancer dataset, we should notice one essential thing. We modified the code of <a href="ch07.xhtml#ch7lis3">Listing 7-3</a> to fix the pseudorandom number seed so that we could reorder the dataset in exactly the same way each time we run. We then ran the code and saw the results. If we rerun the code, we’ll get <em>exactly</em> the same output, even for the tree-based classifiers. This is not what we saw earlier. The tree classifiers are <em>stochastic</em>—they will generate a unique tree or forest each time—so <span epub:type="pagebreak" id="page_140"/>we should expect the results to vary somewhat from run to run. But now they don’t vary; we get the same output each time. By setting the NumPy pseudorandom number seed explicitly, we fixed not only the ordering of the dataset, but also the ordering of the <em>pseudorandom sequence</em> sklearn will use to generate the tree models. This is because sklearn is also using the NumPy pseudorandom number generator. This is a subtle effect with potentially serious consequences and in a larger project might be very difficult to pick up as a bug. The solution is to set the seed to a random value after we’re done reordering the dataset. We can do this by adding one line after <code>y = y[idx]</code></p>&#13;
<pre>np.random.seed()</pre>&#13;
<p class="noindent">so that the pseudorandom number generator is reset by using the system state, typically read from <em>/dev/urandom</em>. Now when we run again, we’ll get different results for the tree models, as before.</p>&#13;
<h4 class="h4" id="lev2_57">Adding k-fold Validation</h4>&#13;
<p class="noindent">To implement <em>k</em>-fold validation, we first need to pick a value for <em>k</em>. Our dataset has 569 samples. We want to split it so that there are a decent number of samples per fold because we want to make the test set a reasonable representation of the data. This argues toward making <em>k</em> small. However, we also want to average out the effect of a bad split, so we might want <em>k</em> to be larger. As with most things in life, a balance must be sought. If we set <em>k</em> = 5, we’ll get 113 samples per split (ignoring the final four samples, which should have no meaningful impact). This leaves 80 percent for training and 20 percent for test for each combination of folds, a reasonable thing to do. So, we’ll use <em>k</em> = 5, but we’ll write our code so that we can vary <em>k</em> if we want.</p>&#13;
<p class="indent">We already have an approach for training multiple models on a train/ test split. All we need to add is code to generate each of the <em>k</em> folds and then train the models on them. The code is in <a href="ch07.xhtml#ch7lis4">Listing 7-4</a> and <a href="ch07.xhtml#ch7lis5">Listing 7-5</a>, which show the helper functions and <code>main</code> function, respectively. Let’s start with <a href="ch07.xhtml#ch7lis4">Listing 7-4</a>.</p>&#13;
<p class="programs" id="ch7lis4">import numpy as np<br/>&#13;
from sklearn.neighbors import NearestCentroid<br/>&#13;
from sklearn.neighbors import KNeighborsClassifier<br/>&#13;
from sklearn.naive_bayes import GaussianNB, MultinomialNB<br/>&#13;
from sklearn.tree import DecisionTreeClassifier<br/>&#13;
from sklearn.ensemble import RandomForestClassifier<br/>&#13;
from sklearn.svm import SVC<br/>&#13;
import sys<br/>&#13;
<br/>&#13;
def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    clf.fit(x_train, y_train)<br/>&#13;
    return clf.score(x_test, y_test)<br/>&#13;
<br/>&#13;
def split(x,y,k,m):<br/>&#13;
<span epub:type="pagebreak" id="page_141"/>  <span class="ent">❶</span> ns = int(y.shape[0]/m)<br/>&#13;
    s = []<br/>&#13;
    for i in range(m):<br/>&#13;
      <span class="ent">❷</span> s.append([x[(ns*i):(ns*i+ns)],<br/>&#13;
                   y[(ns*i):(ns*i+ns)]])<br/>&#13;
    x_test, y_test = s[k]<br/>&#13;
    x_train = []<br/>&#13;
    y_train = []<br/>&#13;
    for i in range(m):<br/>&#13;
        if (i==k):<br/>&#13;
            continue<br/>&#13;
        else:<br/>&#13;
            a,b = s[i]<br/>&#13;
            x_train.append(a)<br/>&#13;
            y_train.append(b)<br/>&#13;
  <span class="ent">❸</span> x_train = np.array(x_train).reshape(((m-1)*ns,30))<br/>&#13;
    y_train = np.array(y_train).reshape((m-1)*ns)<br/>&#13;
    return [x_train, y_train, x_test, y_test]<br/>&#13;
<br/>&#13;
def pp(z,k,s):<br/>&#13;
    m = z.shape[1]<br/>&#13;
    print("%-19s: %0.4f +/- %0.4f | " % (s, z[k].mean(),<br/>&#13;
          z[k].std()/np.sqrt(m)), end='')<br/>&#13;
    for i in range(m):<br/>&#13;
        print("%0.4f " % z[k,i], end='')<br/>&#13;
    print()</p>&#13;
<p class="figcap"><em>Listing 7-4: Using k-fold validation to evaluate the breast cancer dataset. Helper functions. See</em> bc_kfold.py.</p>&#13;
<p class="indent"><a href="ch07.xhtml#ch7lis4">Listing 7-4</a> begins by including all the modules we used before and then defines three functions: <code>run</code>, <code>split</code>, and <code>pp</code>. The <code>run</code> function looks familiar. It takes a train set, test set, and model instance, trains the model, and then scores the model against the test set. The <code>pp</code> function is a pretty-print function to show the per split scores along with the average score across all the splits. The average is shown as the mean ± the standard error of the mean. Recall that an sklearn score is the overall accuracy of the model on the test set, or the fraction of times that the model predicted the actual class of the test sample. Perfection is a score of 1.0, and complete failure is 0.0. Complete failure is rare because even random guessing will get it right some fraction of the time.</p>&#13;
<p class="indent">The only interesting function in <a href="ch07.xhtml#ch7lis4">Listing 7-4</a> is <code>split</code>. Its arguments are the full dataset, <code>x</code>, the corresponding labels, <code>y</code>, the current fold number, <code>k</code>, and the total number of folds, <code>m</code>. We’ll divide the full dataset into <em>m</em> distinct sets, the folds, and use the <em>k</em>-th fold as test while merging the remaining <em>m –</em> 1 folds into a new training set. First, we set the number of samples per fold <span class="ent">❶</span>. The loop then creates a list of folds, <code>s</code>. Each element of this list contains the feature vectors and labels of the fold <span class="ent">❷</span>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_142"/>The test set is simple, it’s the <em>k</em>-th fold, so we set those values next (<code>x_test</code>, <code>y_test</code>). The loop then takes the remaining <em>m –</em> 1 folds and merges them into a new training set, <code>x_train</code>, with labels, <code>y_train</code>.</p>&#13;
<p class="indent">The two lines after the loop are a bit mysterious <span class="ent">❸</span>. When the loop ends, <code>x_train</code> is a <em>list</em>, each element of which is a list representing the feature vectors of the fold we want in the training set. So we first make a NumPy array of this list and then reshape it so that <code>x_train</code> has 30 columns, the number of features per vector, and <em>n</em><sub><em>s</em></sub>(<em>m –</em> 1) rows, where <em>n</em><sub><em>s</em></sub> is the number of samples per fold. Thus <code>x_train</code> becomes <code>x</code> minus the samples we put into the test fold, those of the <em>k</em>-th fold. We also build <code>y_train</code> so that the correct label goes with each the feature vector in <code>x_train</code>.</p>&#13;
<p class="indent"><a href="ch07.xhtml#ch7lis5">Listing 7-5</a> shows us how to use the helper functions.</p>&#13;
<p class="programs" id="ch7lis5">def main():<br/>&#13;
     x = np.load("../data/breast/bc_features_standard.npy")<br/>&#13;
     y = np.load("../data/breast/bc_labels.npy")<br/>&#13;
     idx = np.argsort(np.random.random(y.shape[0]))<br/>&#13;
     x = x[idx]<br/>&#13;
     y = y[idx]<br/>&#13;
  <span class="ent">❶</span> m = int(sys.argv[1])<br/>&#13;
     z = np.zeros((8,m))<br/>&#13;
<br/>&#13;
     for k in range(m):<br/>&#13;
         x_train, y_train, x_test, y_test = split(x,y,k,m)<br/>&#13;
         z[0,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      NearestCentroid())<br/>&#13;
         z[1,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      KNeighborsClassifier(n_neighbors=3))<br/>&#13;
         z[2,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      KNeighborsClassifier(n_neighbors=7))<br/>&#13;
         z[3,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      GaussianNB())<br/>&#13;
         z[4,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      DecisionTreeClassifier())<br/>&#13;
         z[5,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      RandomForestClassifier(n_estimators=5))<br/>&#13;
         z[6,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      RandomForestClassifier(n_estimators=50))<br/>&#13;
         z[7,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                      SVC(kernel="linear", C=1.0))<br/>&#13;
<br/>&#13;
     pp(z,0,"Nearest"); pp(z,1,"3-NN")<br/>&#13;
     pp(z,2,"7-NN");    pp(z,3,"Naive Bayes")<br/>&#13;
     pp(z,4,"Decision Tree");    pp(z,5,"Random Forest (5)")<br/>&#13;
     pp(z,6,"Random Forest (50)");    pp(z,7,"SVM (linear)")</p>&#13;
<p class="figcap"><em>Listing 7-5: Using k-fold validation to evaluate the breast cancer dataset. Main code. See</em> bc_kfold.py.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_143"/>The first thing we do in <code>main</code> is load the full dataset and randomize the ordering. The number of folds, <code>m</code>, is read from the command line <span class="ent">❶</span> and used to create the output array, <code>z</code>. This array holds the per fold scores for each of the eight models we’ll train, so it has shape 8 × <em>m</em>. Recall, when running a Python script from the command line, any arguments passed after the script name are available in <code>sys.argv</code>, a list of strings. This is why the argument is passed to <code>int</code> to convert it to an integer <span class="ent">❶</span>.</p>&#13;
<p class="indent">Next, we loop over the <em>m</em> folds, where <em>k</em> is the fold that we’ll be using for test data. We create the split and then use the split to train the eight model types we trained previously. Each call to <code>run</code> trains a model of the type passed in and returns the score found by running that model against the <em>k</em>-th fold as test data. We store these results in <code>z</code>. Finally, we use <code>pp</code> to display the per model type and per fold scores along with the average score over all the folds.</p>&#13;
<p class="indent">A sample run of this code, for <em>k</em> = 5 and showing only the mean score across folds, gives the results in <a href="ch07.xhtml#ch7tab4">Table 7-4</a>.</p>&#13;
<p class="tabcap" id="ch7tab4"><strong>Table 7-4:</strong> Breast Cancer Scores as Mean Over Five Folds</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Model</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Mean</strong> ± <strong>SE</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9310 ± 0.0116</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-NN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9735 ± 0.0035</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">7-NN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9717 ± 0.0039</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9363 ± 0.0140</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9027 ± 0.0079</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9540 ± 0.0107</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9540 ± 0.0077</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">SVM (linear)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9699 ± 0.0096</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Here we’re showing the average performance of each model over all folds. One way to understand the results is that this is the sort of performance we should expect, per model type, if we were to train the model using <em>all</em> of the data in the dataset and test it against new samples from the same parent distribution. Indeed, in practice, we would do just this, as we can assume that the reason behind making the model in the first place is to use it for some purpose going forward.</p>&#13;
<p class="indent">Run the code a second time with <em>k</em> = 5. A new set of outputs appears. This is because we’re randomizing the order of the dataset on every run (<a href="ch07.xhtml#ch7lis5">Listing 7-5</a>). This makes a new set of splits and implies that each model will be trained on a different subset mix of the full dataset on each run. So, we should expect different results. Let’s run the code 1,000 times with <em>k</em> = 5. Note, training this many models takes about 20 minutes on a very standard desktop computer. For each run we’ll get an average score over the five folds. We then compute the mean of these averages, which is known as the <em>grand mean</em>. <a href="ch07.xhtml#ch7tab5">Table 7-5</a> shows the results.</p>&#13;
<p class="tabcap" id="ch7tab5"><span epub:type="pagebreak" id="page_144"/><strong>Table 7-5:</strong> Breast Cancer Scores as Grand Mean Over 1,000 Runs with Five Folds</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Grand mean</strong> ± <strong>SE</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.929905 ± 0.000056</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-NN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.966334 ± 0.000113</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">7-NN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.965496 ± 0.000110</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.932973 ± 0.000095</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.925706 ± 0.000276</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.948378 ± 0.000213</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.958845 ± 0.000135</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">SVM (linear)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.971871 ± 0.000136</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">We can take these grand means as an indication of how well we’d expect each model to do against a new set of unknown feature vectors. The small standard errors of the mean are an indication of how well the mean value is known, not how well a model of that type trained on a dataset will necessarily perform. We use the grand mean to help us order the models so we can select one over another.</p>&#13;
<p class="indent">Ranking the models from highest score to lowest gives the following:</p>&#13;
<ol>&#13;
<li class="noindent">SVM (linear)</li>&#13;
<li class="noindent"><em>k</em>-NN (<em>k</em> = 3)</li>&#13;
<li class="noindent"><em>k</em>-NN (<em>k</em> = 7)</li>&#13;
<li class="noindent">Random Forest (50)</li>&#13;
<li class="noindent">Random Forest (5)</li>&#13;
<li class="noindent">Naïve Bayes (Gaussian)</li>&#13;
<li class="noindent">Nearest Centroid</li>&#13;
<li class="noindent">Decision Tree</li>&#13;
</ol>&#13;
<p class="noindent">This is interesting given that we might expect the SVM to be best, but would likely assume the Random Forests to do better than <em>k</em>-NN. The Decision Tree was not as good as we thought, and was less accurate than the Nearest Centroid classifier.</p>&#13;
<p class="indent">Some comments are in order here. First, note that these results are derived from the training of 8,000 different models on 1,000 different orderings of the dataset. When we study neural networks, we’ll see much longer training times. Experimenting with classical machine learning models is generally easy to do since each change to a parameter doesn’t require a lengthy training session.</p>&#13;
<p class="indent">Second, we didn’t try to optimize any of the model hyperparameters. Some of these hyperparameters are indirect, like assuming that the features are normally distributed so that the Gaussian Naïve Bayes classifier is a reasonable choice, while others are numerical, like the number of neighbors <span epub:type="pagebreak" id="page_145"/>in <em>k</em>-NN or the number of trees in a Random Forest. If we want to thoroughly develop a good classifier for this dataset using a classic model, we’ll have to explore some of these hyperparameters. Ideally, we’d repeat the experiments many, many times for each new hyperparameter setting to arrive at a tight mean value for the score, as we have previously with the grand means over 1,000 runs. We’ll play a bit more with hyperparameters in the next section, where we see how we can search for good ones that work well with our dataset.</p>&#13;
<h4 class="h4" id="lev2_58">Searching for Hyperparameters</h4>&#13;
<p class="noindent">Let’s explore the effect of some of the hyperparameters on various model types. Specifically, let’s see if we can optimize our choice of <em>k</em> for <em>k</em>-NN, forest size for Random Forest, and the <em>C</em> margin size of the linear SVM.</p>&#13;
<h5 class="h5">Fine-Tuning Our k-NN Classifier</h5>&#13;
<p class="noindent">Because the number of neighbors in a <em>k</em>-NN classifier is an integer, typically odd, it’s straightforward to repeat our five-fold cross validation experiment while varying <em>k</em> for <em>k</em> ∈ <em>{</em>1,3,5,7,9,11,13,15<em>}</em>. To do this, we need only change the main loop in <a href="ch07.xhtml#ch7lis5">Listing 7-5</a> so that each call to <code>run</code> uses <code>KNeighborsClassifier</code> with a different number of neighbors, as follows.</p>&#13;
<pre><span class="codestrong1">for</span> k <span class="codestrong1">in range</span>(m):<br/>&#13;
    x_train, y_train, x_test, y_test = split(x,y,k,m)<br/>&#13;
    z[0,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=1))<br/>&#13;
    z[1,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=3))<br/>&#13;
    z[2,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=5))<br/>&#13;
    z[3,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=7))<br/>&#13;
    z[4,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=9))<br/>&#13;
    z[5,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=11))<br/>&#13;
    z[6,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=13))<br/>&#13;
    z[7,k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                 KNeighborsClassifier(n_neighbors=15))</pre>&#13;
<p class="indent">The grand mean of the scores for 1,000 repetitions of the five-fold cross-validation code using a different random ordering of the full dataset each time gives the results in <a href="ch07.xhtml#ch7tab6">Table 7-6</a>.</p>&#13;
<p class="tabcap" id="ch7tab6"><span epub:type="pagebreak" id="page_146"/><strong>Table 7-6:</strong> Breast Cancer Scores as Grand Mean for Different k Values and Five-Fold Validation</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong><em>k</em></strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Grand mean</strong> ± <strong>SE</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.951301 ± 0.000153</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.966282 ± 0.000112</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.965998 ± 0.000097</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.96520 ± 0.000108</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0.967011</strong> ± <strong>0.000100</strong></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.965069 ± 0.000107</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">13</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.962400 ± 0.000106</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">15</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.959976 ± 0.000101</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">We’ve highlighted the <em>k</em> = 9 because it returned the highest score. This indicates that we might want to use <em>k</em> = 9 for this dataset.</p>&#13;
<h5 class="h5">Fine-Tuning Our Random Forest</h5>&#13;
<p class="noindent">Let’s look at the Random Forest model. The sklearn <code>RandomForestClassifier</code> class has quite a few hyperparameters that we could manipulate. To avoid being excessively pedantic, we’ll seek only an optimal number of trees in the forest. This is the <code>n_estimators</code> parameter. As we did for <em>k</em> in <em>k</em>-NN, we’ll search over a range of forest sizes and select the one that gives the best grand mean score for 1,000 runs at five folds each per run.</p>&#13;
<p class="indent">This is a one-dimensional grid-like search. We varied <em>k</em> by one, but for the number of trees in the forest, we need to cover a larger scale. We don’t expect there to be a meaningful difference between 10 trees in the forest or 11, especially considering that each Random Forest training session will lead to a different set of trees even if the number of trees is fixed. We saw this effect several times in the previous section. Instead, let’s vary the number of trees by selecting from <em>n</em><sub><em>t</em></sub> ∈ <em>{</em>5,20,50,100,200,500,1000,5000<em>}</em> where <em>n</em><sub><em>t</em></sub> is the number of trees in the forest (number of estimators). Running this search gives us the grand means in <a href="ch07.xhtml#ch7tab7">Table 7-7</a>.</p>&#13;
<p class="tabcap" id="ch7tab7"><strong>Table 7-7:</strong> Breast Cancer Scores as Grand Mean for Different Random Forest Sizes and Five-Fold Validation</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong><em>n<sub>t</sub></em></strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Grand mean</strong> ± <strong>SE</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.948327 ± 0.000206</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">20</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.956808 ±0.000166</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">50</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.959048 ± 0.000139</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.959740 ± 0.000130</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">200</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.959913 ± 0.000122</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">500</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.960049 ± 0.000117</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">750</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.960147 ± 0.000118</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.960181 ± 0.000116</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_147"/>The first thing to notice is that the differences are very small, though if you run the Mann–Whitney U test, you’ll see that the difference between <em>n</em><sub><em>t</em></sub> = 5 (worst) and <em>n</em><sub><em>t</em></sub> = 1000 (best) is statistically significant. However, the difference between <em>n</em><sub><em>t</em></sub> = 200 and <em>n</em><sub><em>t</em></sub> = 1000 is not significant. Here we need to make a judgment call. Setting <em>n</em><sub><em>t</em></sub> = 1000 did give the best result but it’s indistinguishable, for practical purposes, from <em>n</em><sub><em>t</em></sub> = 500 or even <em>n</em><sub><em>t</em></sub> = 100. Since runtime for a Random Forest scales linearly in the number of trees, using <em>n</em><sub><em>t</em></sub> = 100 results in a classifier that is on average 10× faster than using <em>n</em><sub><em>t</em></sub> = 1000. So, depending upon the task, we might select <em>n</em><sub><em>t</em></sub> = 100 over <em>n</em><sub><em>t</em></sub> = 1000 for that reason.</p>&#13;
<h5 class="h5">Fine-Tuning Our SVMs</h5>&#13;
<p class="noindent">Let’s turn our attention to the linear SVM. For the linear kernel, we’ll adjust <em>C</em>. Note, sklearn has other parameters, as it did for the Random Forest, but we’ll leave them at their default settings.</p>&#13;
<p class="indent">What range of <em>C</em> should we search over? The answer is problem dependent but the sklearn default value of <em>C</em> = 1 is a good starting point. We’ll select <em>C</em> values around 1 but over several orders of magnitude. Specifically, we’ll select from <em>C</em> ∈ <em>{</em>0.001,0.01,0.1,1.0,2.0,10.0,50.0,100.0<em>}</em>. Running one thousand five-fold validations, each for a different random ordering of the full dataset, gives grand means as shown in <a href="ch07.xhtml#ch7tab8">Table 7-8</a>.</p>&#13;
<p class="tabcap" id="ch7tab8"><strong>Table 7-8:</strong> Breast Cancer Scores as Grand Mean for Different SVM C Values and Five-Fold Validation</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>C</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Grand mean</strong> ± <strong>SE</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.001</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.938500 ± 0.000066</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.01</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.967151 ± 0.000089</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.975943 ± 0.000101</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.971890 ± 0.000141</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.969994 ± 0.000144</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">10.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.966239 ± 0.000154</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">50.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.959637 ± 0.000186</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">100.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.957006 ± 0.000189</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><em>C</em> = 0.1 gives the best accuracy. While, statistically, the difference between <em>C</em> = 0.1 and <em>C</em> = 1 is meaningful, in practice the difference is only about 0.4 percent, so the default value of <em>C</em> = 1 would likewise be a reasonable choice. Further refinement of <em>C</em> is possible because we see that <em>C</em> = 0.01 and <em>C</em> = 2 give the same accuracy, while <em>C</em> = 0.1 is higher than either, implying that if the <em>C</em> curve is smooth, there’s a maximum accuracy for some <em>C</em> in [0.01,2.0].</p>&#13;
<p class="indent">Finding the right <em>C</em> for our dataset is a crucial part of successfully using a linear SVM. Our preceding rough run used a one-dimensional grid search. We do expect, since <em>C</em> is continuous, that a plot of the accuracy as a function of <em>C</em> will also be smooth. If that’s the case, one can imagine searching for the right <em>C</em>, not with a grid search but with an optimization algorithm. <span epub:type="pagebreak" id="page_148"/>In practice, however, the randomness of the ordering of the dataset and its effect on the output of <em>k</em>-fold cross-validation results will probably make any <em>C</em> found by an optimization algorithm too specific to the problem at hand. Grid search over a larger scale, with possibly one level of refinement, is sufficient in most cases. The take-home message is: do spend some time looking for the proper <em>C</em> value to maximize the effectiveness of the linear SVM.</p>&#13;
<p class="indent">Observant readers will have noticed that the preceding analysis has ignored the RBF kernel SVM. Let’s revisit it now and see how to do a simple two-dimensional grid search over <em>C</em> and <em>γ</em>, where <em>γ</em> is the parameter associated with the RBF (Gaussian) kernel. sklearn has the <code>GridSearchCV</code> class to perform sophisticated grid searching. We’re not using it here to be pedagogical and show how to do simple grid searches directly. It’s especially important for this kernel to select good values for both of these parameters.</p>&#13;
<p class="indent">For the search, we’ll use the same range of <em>C</em> values as we used for the linear case. For <em>γ</em> we’ll use powers of two, 2<sup><em>p</em></sup>, times the sklearn default value, 1/30 = 0.03333 for <em>p</em> ∈ [<em>–</em>4,3]. The search will, for the current <em>C</em> value, do five-fold validation over the dataset for each <em>γ</em> value before moving to the next <em>C</em> value so that all pairs of (<em>C</em>,<em>γ</em>) are considered. The pair that results in the largest score (accuracy) will be output. The code is in <a href="ch07.xhtml#ch7lis6">Listing 7-6</a>.</p>&#13;
<p class="programs" id="ch7lis6">import numpy as np<br/>&#13;
from sklearn.svm import SVC<br/>&#13;
<br/>&#13;
def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    clf.fit(x_train, y_train)<br/>&#13;
    return clf.score(x_test, y_test)<br/>&#13;
<br/>&#13;
def split(x,y,k,m):<br/>&#13;
    ns = int(y.shape[0]/m)<br/>&#13;
    s = []<br/>&#13;
    for i in range(m):<br/>&#13;
        s.append([x[(ns*i):(ns*i+ns)], y[(ns*i):(ns*i+ns)]])<br/>&#13;
    x_test, y_test = s[k]<br/>&#13;
    x_train = []<br/>&#13;
    y_train = []<br/>&#13;
    for i in range(m):<br/>&#13;
        if (i==k):<br/>&#13;
            continue<br/>&#13;
        else:<br/>&#13;
            a,b = s[i]<br/>&#13;
            x_train.append(a)<br/>&#13;
            y_train.append(b)<br/>&#13;
    x_train = np.array(x_train).reshape(((m-1)*ns,30))<br/>&#13;
    y_train = np.array(y_train).reshape((m-1)*ns)<br/>&#13;
    return [x_train, y_train, x_test, y_test]<br/>&#13;
<br/>&#13;
def main():<br/>&#13;
    m = 5<br/>&#13;
<span epub:type="pagebreak" id="page_149"/>    x = np.load("../data/breast/bc_features_standard.npy")<br/>&#13;
    y = np.load("../data/breast/bc_labels.npy")<br/>&#13;
    idx = np.argsort(np.random.random(y.shape[0]))<br/>&#13;
    x = x[idx]<br/>&#13;
    y = y[idx]<br/>&#13;
<br/>&#13;
  <span class="ent">❶</span> Cs = np.array([0.01,0.1,1.0,2.0,10.0,50.0,100.0])<br/>&#13;
    gs = (1./30)*2.0**np.array([-4,-3,-2,-1,0,1,2,3])<br/>&#13;
    zmax = 0.0<br/>&#13;
  <span class="ent">❷</span> for C in Cs:<br/>&#13;
        for g in gs:<br/>&#13;
            z = np.zeros(m)<br/>&#13;
            for k in range(m):<br/>&#13;
                x_train, y_train, x_test, y_test = split(x,y,k,m)<br/>&#13;
                z[k] = run(x_train, y_train, x_test, y_test,<br/>&#13;
                       SVC(C=C,gamma=g,kernel="rbf"))<br/>&#13;
          <span class="ent">❸</span> if (z.mean() &gt; zmax):<br/>&#13;
                zmax = z.mean()<br/>&#13;
                bestC = C<br/>&#13;
                bestg = g<br/>&#13;
    print("best C     = %0.5f" % bestC)<br/>&#13;
    print("     gamma = %0.5f" % bestg)<br/>&#13;
    print("   accuracy= %0.5f" % zmax)</p>&#13;
<p class="figcap"><em>Listing 7-6: A two-dimensional grid search for C and  for an RBF kernel SVM. Breast cancer dataset. See</em> bc_rbf_svm_search.py.</p>&#13;
<p class="indent">The two helper functions, <code>run</code> and <code>split</code>, are exactly the same as we used before (see <a href="ch07.xhtml#ch7lis4">Listing 7-4</a>); all the action is in <code>main</code>. We fix the number of folds at five and then load and randomize the full dataset.</p>&#13;
<p class="indent">We then define the specific <em>C</em> and <em>γ</em> values to search over <span class="ent">❶</span>. Note how <code>gs</code> is defined. The first part is 1/30, the reciprocal of the number of features. This is the default value for <em>γ</em> used by sklearn. We then multiply this factor by an array, (2<sup><em>–</em>4</sup>,2<sup><em>–</em>3</sup>,2<sup><em>–</em>1</sup>,2<sup>0</sup>,2<sup>1</sup>,2<sup>2</sup>,2<sup>3</sup>), to get the final <em>γ</em> values we’ll search over. Notice that one of the <em>γ</em> values is exactly the default sklearn uses since 2<sup>0</sup> = 1.</p>&#13;
<p class="indent">The double loop <span class="ent">❷</span> iterates over all possible pairs of <em>C</em> and <em>γ</em>. For each one, we do five-fold validation to get a set of five scores in <code>z</code>. We then ask if the mean of this set is greater than the current maximum (<em>z</em><sub>max</sub>) and if so, update the maximum and keep the <em>C</em> and <em>γ</em> values as our current bests <span class="ent">❸</span>. When the loops over <em>C</em> and <em>γ</em> exit, we have our best values in <code>bestC</code> and <code>bestg</code>.</p>&#13;
<p class="indent">If we run this code repeatedly, we’ll get different outputs each time. This is because we’re randomizing the order of the full dataset, which will alter the subsets in the folds, leading to a different mean score over the folds. For example, 10 runs produced the output in <a href="ch07.xhtml#ch7tab9">Table 7-9</a>.</p>&#13;
<p class="tabcap" id="ch7tab9"><span epub:type="pagebreak" id="page_150"/><strong>Table 7-9:</strong> Breast Cancer Scores for an RBF SVM with Different C and <em>γ</em> Values Averaged Over 10 Runs</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:45%"/>&#13;
<col style="width:45%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong><em>C</em></strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong><em>γ</em></strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong><em>accuracy</em></strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03333</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.97345</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03333</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.98053</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00417</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.97876</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00417</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.97699</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00417</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.98053</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.01667</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.98053</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.01667</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.97876</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.01667</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.98053</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03333</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.97522</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00417</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.97876</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">These results hint that (<em>C</em>,<em>γ</em>) = (10,0.00417) is a good combination. If we use these values to generate a grand mean over 1,000 runs of five-fold validation as before, we get an overall accuracy of 0.976991, or 97.70 percent, which is the highest grand mean accuracy of any model type we trained on the breast cancer histology dataset.</p>&#13;
<p class="indent">The breast cancer dataset is not a large dataset. We were able to use <em>k</em>-fold validation to find a good model that worked well with it. Now, let’s move from a pure vector-only dataset to one that is actually image-based and much larger, the MNIST dataset.</p>&#13;
<h3 class="h3" id="lev1_46">Experiments with the MNIST Dataset</h3>&#13;
<p class="noindent">The last dataset we’ll work with in this chapter is the vector version of the MNIST handwritten digit dataset (see <a href="ch05.xhtml#ch05">Chapter 5</a>). Recall, this dataset consists of 28×28 pixel grayscale images of handwritten digits, [0,9], one digit centered per image. This dataset is by far the most common workhorse dataset in machine learning, especially in deep learning, and we’ll use it throughout the remainder of the book.</p>&#13;
<h4 class="h4" id="lev2_59">Testing the Classical Models</h4>&#13;
<p class="noindent">MNIST contains 60,000 training images, roughly evenly split among the digits, and 10,000 test images. Since we have a lot of training data, at least for classic models like those we’re concerned with here, we won’t make use of <em>k</em>-fold validation, though we certainly could. We’ll train on the training data and test on the testing data and trust that the two come from a common parent distribution (they do).</p>&#13;
<p class="indent">Since our classic models expect vector inputs, we’ll use the vector form of the MNIST dataset we created in <a href="ch05.xhtml#ch05">Chapter 5</a>. The images are unraveled so that the first 28 elements of the vector are row 0, the next 28 are row 1, and so on for an input vector of 28 × 28 = 784 elements. The images are stored <span epub:type="pagebreak" id="page_151"/>as 8-bit grayscale, so the data values run from 0 to 255. We’ll consider three versions of the dataset. The first is the raw byte version. The second is a version where we scale the data to [0,1) by dividing by 256, the number of possible grayscale values. The third is a normalized version where, per “feature” (really, pixel), we subtract the mean of that feature across the dataset and then divide by the standard deviation. This will let us explore how the range of the feature values affects things, if at all.</p>&#13;
<p class="indent"><a href="ch07.xhtml#ch7fig1">Figure 7-1</a> shows examples of the original images and the resulting normalized vectors raveled back into images and scaled [0,255]. Normalizing affects the appearance but does not destroy spatial relationships among the parts of the digit images. Just scaling the data to [0,1) will result in images that look the same as those on the top of <a href="ch07.xhtml#ch7fig1">Figure 7-1</a>.</p>&#13;
<div class="image" id="ch7fig1"><img src="Images/07fig01.jpg" alt="image" width="532" height="108"/></div>&#13;
<p class="figcap"><em>Figure 7-1: Original MNIST digits (top) and normalized versions used by the models (bottom)</em></p>&#13;
<p class="indent">The code we’ll use is very similar to what we used previously, but for reasons that will be explained next, we will replace the <code>SVC</code> class with a new SVM class, <code>LinearSVC</code>. First, take a look at the helper functions in <a href="ch07.xhtml#ch7lis7">Listing 7-7</a>.</p>&#13;
<p class="programs" id="ch7lis7">import time<br/>&#13;
import numpy as np<br/>&#13;
from sklearn.neighbors import NearestCentroid<br/>&#13;
from sklearn.neighbors import KNeighborsClassifier<br/>&#13;
from sklearn.naive_bayes import GaussianNB, MultinomialNB<br/>&#13;
from sklearn.tree import DecisionTreeClassifier<br/>&#13;
from sklearn.ensemble import RandomForestClassifier<br/>&#13;
from sklearn.svm import LinearSVC<br/>&#13;
from sklearn import decomposition<br/>&#13;
<br/>&#13;
def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    s = time.time()<br/>&#13;
    clf.fit(x_train, y_train)<br/>&#13;
    e_train = time.time() - s<br/>&#13;
    s = time.time()<br/>&#13;
    score = clf.score(x_test, y_test)<br/>&#13;
    e_test = time.time() - s<br/>&#13;
    print("score = %0.4f (time, train=%8.3f, test=%8.3f)"<br/>&#13;
          % (score, e_train, e_test))<br/>&#13;
<br/>&#13;
def train(x_train, y_train, x_test, y_test):<br/>&#13;
    print("    Nearest Centroid          : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, NearestCentroid())<br/>&#13;
    print("    k-NN classifier (k=3)     : ", end='')<br/>&#13;
<span epub:type="pagebreak" id="page_152"/>    run(x_train, y_train, x_test, y_test,<br/>&#13;
        KNeighborsClassifier(n_neighbors=3))<br/>&#13;
    print("    k-NN classifier (k=7)     : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        KNeighborsClassifier(n_neighbors=7))<br/>&#13;
    print("    Naive Bayes (Gaussian)    : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, GaussianNB())<br/>&#13;
    print("    Decision Tree             : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, DecisionTreeClassifier())<br/>&#13;
    print("    Random Forest (trees=  5) : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        RandomForestClassifier(n_estimators=5))<br/>&#13;
    print("    Random Forest (trees= 50) : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        RandomForestClassifier(n_estimators=50))<br/>&#13;
    print("    Random Forest (trees=500) : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        RandomForestClassifier(n_estimators=500))<br/>&#13;
    print("    Random Forest (trees=1000): ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test,<br/>&#13;
        RandomForestClassifier(n_estimators=1000))<br/>&#13;
    print("    LinearSVM (C=0.01)        : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))<br/>&#13;
    print("    LinearSVM (C=0.1)         : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))<br/>&#13;
    print("    LinearSVM (C=1.0)         : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))<br/>&#13;
    print("    LinearSVM (C=10.0)        : ", end='')<br/>&#13;
    run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))</p>&#13;
<p class="figcap"><em>Listing 7-7: Training differently scaled versions of the MNIST dataset using classic models. Helper functions. See</em> mnist_experiments.py.</p>&#13;
<p class="indent">The <code>run</code> function of <a href="ch07.xhtml#ch7lis7">Listing 7-7</a> is also similar to those used previously, except we’ve added code to time how long training and testing takes. These times are reported along with the score. We added this code for MNIST because, unlike the tiny iris and breast cancer datasets, MNIST has a larger number of training samples so that runtime differences among the model types will start to show themselves. The <code>train</code> function is new, but all it does is wrap calls to <code>run</code> for the different model types.</p>&#13;
<p class="indent">Now take a look at <a href="ch07.xhtml#ch7lis8">Listing 7-8</a>, which contains the <code>main</code> function.</p>&#13;
<p class="programs" id="ch7lis8">def main():<br/>&#13;
    x_train = np.load("mnist_train_vectors.npy").astype("float64")<br/>&#13;
    y_train = np.load("mnist_train_labels.npy")<br/>&#13;
    x_test = np.load("mnist_test_vectors.npy").astype("float64")<br/>&#13;
    y_test = np.load("mnist_test_labels.npy")<br/>&#13;
<br/>&#13;
    print("Models trained on raw [0,255] images:")<br/>&#13;
<span epub:type="pagebreak" id="page_153"/>    train(x_train, y_train, x_test, y_test)<br/>&#13;
    print("Models trained on raw [0,1) images:")<br/>&#13;
    train(x_train/256.0, y_train, x_test/256.0, y_test)<br/>&#13;
<br/>&#13;
  <span class="ent">❶</span> m = x_train.mean(axis=0)<br/>&#13;
    s = x_train.std(axis=0) + 1e-8<br/>&#13;
    x_ntrain = (x_train - m) / s<br/>&#13;
    x_ntest  = (x_test - m) / s<br/>&#13;
<br/>&#13;
    print("Models trained on normalized images:")<br/>&#13;
    train(x_ntrain, y_train, x_ntest, y_test)<br/>&#13;
<br/>&#13;
  <span class="ent">❷</span> pca = decomposition.PCA(n_components=15)<br/>&#13;
    pca.fit(x_ntrain)<br/>&#13;
    x_ptrain = pca.transform(x_ntrain)<br/>&#13;
    x_ptest = pca.transform(x_ntest)<br/>&#13;
<br/>&#13;
    print("Models trained on first 15 PCA components of normalized images:")<br/>&#13;
    train(x_ptrain, y_train, x_ptest, y_test)</p>&#13;
<p class="figcap"><em>Listing 7-8: Training differently scaled versions of the MNIST dataset using classic models. Main function. See</em> mnist_experiments.py.</p>&#13;
<p class="indent">The <code>main</code> function of <a href="ch07.xhtml#ch7lis8">Listing 7-8</a> loads the data and then trains the models using the raw byte values. It then repeats the training using a scaled [0,1) version of the data and a scaled version of the testing data. These are the first two versions of the dataset we’ll use.</p>&#13;
<p class="indent">Normalizing the data requires knowledge of the per feature means and standard deviations <span class="ent">❶</span>. Note, we add a small value to the standard deviations to make up for pixels that have a standard deviation of zero. We can’t divide by zero, after all. We need to normalize the test data, but which means and which standard deviations should we use? Generally, we have more training data than testing data, so using the means and standard deviations from the training data makes sense; they are a better representation of the true means and standard deviations of the parent distribution that generated the data in the first place. However, at times, there may be slight differences between the training and testing data distributions, in which case it might make sense to consider the testing means and standard deviations. In this case, because the MNIST training and test datasets were created together, there’s no difference, so the training values are what we’ll use. Note that the same per feature means and standard deviations will need to be used for all new, unknown samples, too.</p>&#13;
<p class="indent">Next, we apply PCA to the dataset just as we did for the iris data in <a href="ch05.xhtml#ch05">Chapter 5</a> <span class="ent">❷</span>. Here we’re keeping the first 15 components. These account for just over 33 percent of the variance in the data and reduce the feature vector from 784 features (the pixels) to 15 features (the principal components). Then we train the models using these features.</p>&#13;
<p class="indent">Running this code produces a wealth of output that we can learn from. Let’s first consider the scores per model type and data source. These are in <span epub:type="pagebreak" id="page_154"/><a href="ch07.xhtml#ch7tab10">Table 7-10</a>; values in parentheses are the number of trees in the Random Forest.</p>&#13;
<p class="tabcap" id="ch7tab10"><strong>Table 7-10:</strong> MNIST Model Scores for Different Preprocessing Steps</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:40%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top" class="borderr"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Raw [0,255]</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Scaled [0,1)</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Normalized</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>PCA</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8203</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8203</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8092</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7523</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><em>k</em>-NN (<em>k</em>  = 3)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9705</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9705</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9452</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9355</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><em>k</em>-NN (<em>k</em>  = 7)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9694</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9694</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9433</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9370</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Naïve Bayes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5558</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5558</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5239</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7996</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8773</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8784</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8787</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8403</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9244</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9244</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9220</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8845</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9660</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9661</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9676</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9215</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Random Forest (500)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9708</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9709</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9725</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9262</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">Random Forest (1000)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9715</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9716</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9719</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9264</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">LinearSVM (C  = 0.01)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8494</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9171</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9158</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8291</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">LinearSVM (C  = 0.1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8592</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9181</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9163</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8306</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">LinearSVM (C  = 1.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8639</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9182</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9079</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8322</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab">LinearSVM (C  = 10.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8798</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9019</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8787</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7603</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Look at the Nearest Centroid scores. These make sense as we move from left to right across the different versions of the dataset. For the raw data, the center location of each of the 10 classes leads to a simple classifier with an accuracy of 82 percent—not too bad considering random guessing would have an accuracy closer to 10 percent (1/10 for 10 classes). Scaling the data by a constant won’t change the relative relationship between the per class centroids so we’d expect the same performance in column 2 of <a href="ch07.xhtml#ch7tab10">Table 7-10</a> as in column 1.</p>&#13;
<p class="indent">Normalizing, however, does more than divide the data by a constant. We saw the effect clearly in <a href="ch07.xhtml#ch7fig1">Figure 7-1</a>. This alteration, at least for the MNIST dataset, changes the centroids’ relationships to each other and results in a decrease in accuracy to 80.9 percent.</p>&#13;
<p class="indent">Finally, using PCA to reduce the number of features from 784 to 15 has a severe negative impact, resulting in an accuracy of only 75.2 percent. Note the word <em>only</em>. In the past, before the advent of deep learning, an accuracy of 75 percent on a problem with 10 classes would generally have been considered to be pretty good. Of course, it really isn’t. Who would get in a self-driving car that has an accident one time out of every four trips? We want to do better.</p>&#13;
<p class="indent">Let’s consider the <em>k</em>-NN classifiers next. We see similar performance for both <em>k</em> = 3 and <em>k</em> = 7 and the same sort of trend as we saw with the Nearest Centroid classifier. This is to be expected given how similar the two types of models actually are. The difference in accuracy between the two (centroid and <em>k</em>-NN) is dramatic, however. An accuracy of 97 percent is generally regarded as good. But still, who would opt for elective surgery with a 3 percent failure rate?</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_155"/>Things get interesting when we look at the Naïve Bayes classifier. Here all the versions of the dataset perform poorly, though still five times better than guessing. We see a large jump in accuracy with the PCA processed dataset, from 56 percent to 80 percent. This is the only model type to improve after using PCA. Why might this be? Remember, we’re using Gaussian Naïve Bayes, which means our independence assumption is coupled with an assumption that the continuous feature values are, per feature, really drawn from a normal distribution whose parameters, the mean and standard deviation, we can estimate from the feature values themselves.</p>&#13;
<p class="indent">Now recall what PCA does, geometrically. It’s the equivalent of rotating the feature vectors onto a new set of coordinates aligned with the largest orthogonal directions derivable from the dataset. The word <em>orthogonal</em> implies that no part of a direction overlaps with any other part of any other direction. Think of the x-, y-, and z-axes of a three-dimensional plot. No part of the <em>x</em> is along the <em>y</em> or <em>z</em>, and so forth. This is what PCA does. Therefore, PCA makes the first assumption of Naïve Bayes more likely to be true, that the new features are indeed independent of each other. Add in the Gaussian assumption as to the distribution of the per pixel values, and we have an explanation for what we see in <a href="ch07.xhtml#ch7tab10">Table 7-10</a>.</p>&#13;
<p class="indent">The tree-based classifiers, Decision Tree and Random Forest, perform much the same until we get to the PCA version of the dataset. Indeed, there is no difference between the raw data and the data scaled by 256. Again, this is to be expected as all scaling by a constant does is scale the decision thresholds for each of the nodes in the body of the tree or trees. As before, working with reduced dimensionality vectors via PCA results in a loss of accuracy because potentially important information has been discarded.</p>&#13;
<p class="indent">For any data source, we see scores that make sense relative to each other. As before, the single Decision Tree performs worst, which it should except for simple cases since it’s competing against a collection of trees via the Random Forests. For the Random Forests, we see that the score improves as the number of trees in the forest increases—again expected. However, the improvement comes with diminishing returns. There’s a significant improvement when going from 5 trees to 50 trees, but a minimal improvement in going from 500 trees to 1,000 trees.</p>&#13;
<p class="indent">Before we look at the SVM results, let’s understand why we made the switch from the <code>SVC</code> class to <code>LinearSVC</code>. As the name suggests, <code>LinearSVC</code> implements only a linear kernel. The <code>SVC</code> class is more generic and can implement other kernels, so why switch?</p>&#13;
<p class="indent">The reason has to do with runtime. In computer science, there are specific definitions of complexity and an entire branch devoted to the analysis of algorithms and how they perform as their inputs scale larger and larger. All we’ll concern ourselves with here is <em>big-O</em> notation. This is a way of characterizing how the runtime of an algorithm changes as the input (or the number of inputs) gets larger and larger.</p>&#13;
<p class="indent">For example, a classic bubble sort algorithm works just fine on a few dozen numbers to be sorted. But, as the input gets larger (more numbers to be sorted), the runtime increases not linearly but quadratically, meaning <span epub:type="pagebreak" id="page_156"/>the time to sort the numbers, <em>t</em>, is proportional to the <em>square</em> of the number of numbers to be sorted, <em>t</em> ∝ <em>n</em><sup>2</sup>, which is written as <em>O</em>(<em>n</em><sup>2</sup>). So, the bubble sort is an order <em>n</em><sup>2</sup> algorithm. In general, we want algorithms that are better than <em>n</em><sup>2</sup>, more like <em>n</em>, written as <em>O</em>(<em>n</em>), or even independent of <em>n</em>, written as <em>O</em>(1). It turns out that the kernel algorithm for training an SVM is <em>worse</em> than <em>O</em>(<em>n</em><sup>2</sup>) so that when the number of training samples increases, the runtime explodes. This is one reason for the switch from the <code>SVC</code> class to <code>LinearSVC</code>, which doesn’t use kernels.</p>&#13;
<p class="indent">The second reason for the switch has to do with the fact that Support Vector Machines are designed for binary classification—only two classes. The MNIST dataset has 10 classes, so something different has to be done. There are multiple approaches. According to the sklearn documentation, the <code>SVC</code> class uses a <em>one-versus-one</em> approach that trains pairs of classifiers, one class versus another: class 0 versus class 1, class 1 versus class 2, class 0 versus class 2, and so on. This means it ends up training not one but <em>m</em>(<em>m –</em> 1)/2 classifiers for <em>m</em> = 10 classes, or 10(10 <em>–</em> 1)/2 = 45 separate classifiers. This isn’t efficient in this case. The <code>LinearSVC</code> classifier uses a <em>one-versus-rest</em> approach. This means it trains an SVM to classify “0” versus “1–9”, then “1” versus “0, 2–9”, and so on, for a total of only 10 classifiers, one for each digit.</p>&#13;
<p class="indent">It’s with the SVM classifiers that we see a definite benefit to scaling the data versus the raw byte inputs. We also see that the optimal <em>C</em> value is likely between <em>C</em> = 0.1 and <em>C</em> = 1.0. Note that simple [0,1) scaling leads to SVM models that outperform (for this one dataset!) the models trained on the normalized data. The effect is small but consistent for different <em>C</em> values. And, as we saw before, dropping the dimensionality from 784 features to only 15 features via PCA leads to a rather large loss of accuracy. PCA seems not to have helped in this case. We’ll come back to it in a bit and see if we can understand why.</p>&#13;
<h4 class="h4" id="lev2_60">Analyzing Runtimes</h4>&#13;
<p class="noindent">Let’s now look at the runtime performance of the algorithms. <a href="ch07.xhtml#ch7tab11">Table 7-11</a> shows the train and test times, in seconds, for each model type and dataset version.</p>&#13;
<p class="indent">Look at the test times. This is how long each model takes to classify all 10,000 digit images in the test set. The first thing that jumps out at us is that <em>k</em>-NN is slow. Classifying the test set takes over 10 minutes when full feature vectors are used! It’s only when we drop down to the first 15 PCA components that we see reasonable <em>k</em>-NN runtimes. This is a good example of the price we pay for a seemingly simple idea. Recall, the <em>k</em>-NN classifier finds the <em>k</em> closest training samples to the unknown sample we wish to classify. Here <em>closest</em> means in a Euclidean sense, like the distance between two points on a graph, except in this case we don’t have two or three dimensions but 784.</p>&#13;
<p class="tabcap" id="ch7tab11"><span epub:type="pagebreak" id="page_157"/><strong>Table 7-11:</strong> Training and Testing Times (Seconds) for Each Model Type</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top" colspan="2"><p class="tab-c"><strong>Raw [0,255]</strong></p></th>&#13;
<th style="vertical-align: top" colspan="2"><p class="tab-c"><strong>Scaled [0,1)</strong></p></th>&#13;
<th style="vertical-align: top" colspan="2"><p class="tab-c"><strong>Normalized</strong></p></th>&#13;
<th style="vertical-align: top" colspan="2"><p class="tab-c"><strong>PCA</strong></p></th>&#13;
</tr>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tabz">train</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">test</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">train</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">test</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">train</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">test</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">train</p></th>&#13;
<th style="vertical-align: top"><p class="tabz">test</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.23</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.24</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.24</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.01</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>K</em>-NN (<em>K</em> = 3)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">33.24</p></td>&#13;
<td style="vertical-align: top"><p class="tab">747.34</p></td>&#13;
<td style="vertical-align: top"><p class="tab">33.63</p></td>&#13;
<td style="vertical-align: top"><p class="tab">747.22</p></td>&#13;
<td style="vertical-align: top"><p class="tab">33.66</p></td>&#13;
<td style="vertical-align: top"><p class="tab">699.58</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.09</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.64</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>K</em>-NN (<em>K</em> = 7)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">33.45</p></td>&#13;
<td style="vertical-align: top"><p class="tab">746.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab">33.69</p></td>&#13;
<td style="vertical-align: top"><p class="tab">746.65</p></td>&#13;
<td style="vertical-align: top"><p class="tab">33.68</p></td>&#13;
<td style="vertical-align: top"><p class="tab">709.62</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.09</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.65</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.80</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.88</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.85</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.90</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.83</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.94</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.01</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">25.42</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03</p></td>&#13;
<td style="vertical-align: top"><p class="tab">25.41</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">25.42</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.65</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.06</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.70</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.06</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.61</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.06</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.20</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.03</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">25.56</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.46</p></td>&#13;
<td style="vertical-align: top"><p class="tab">25.14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.46</p></td>&#13;
<td style="vertical-align: top"><p class="tab">25.27</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.46</p></td>&#13;
<td style="vertical-align: top"><p class="tab">12.06</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.25</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (500)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">252.65</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.41</p></td>&#13;
<td style="vertical-align: top"><p class="tab">249.69</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.47</p></td>&#13;
<td style="vertical-align: top"><p class="tab">249.19</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.45</p></td>&#13;
<td style="vertical-align: top"><p class="tab">121.10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.51</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (1000)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">507.52</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.86</p></td>&#13;
<td style="vertical-align: top"><p class="tab">499.23</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.71</p></td>&#13;
<td style="vertical-align: top"><p class="tab">499.10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.91</p></td>&#13;
<td style="vertical-align: top"><p class="tab">242.44</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.00</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 0.01)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">169.45</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.93</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">232.93</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">16.91</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 0.1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">170.58</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">36.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">320.17</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">37.46</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 1.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">170.74</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">96.34</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">488.06</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">66.49</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 10.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">170.46</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">154.34</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">541.69</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.02</p></td>&#13;
<td style="vertical-align: top"><p class="tab">86.87</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.00</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Therefore, for each of the test samples, we need to find the <em>k</em> = 3 or <em>k</em> = 7 closest points in the training data. The naïve way to do this is to calculate the distance between the unknown sample and each of the 60,000 training samples, sort them, look at the <em>k</em> smallest distances, and vote to decide the output class label. This is a lot of work because we have 60,000 training samples and 10,000 test samples for a total of 600,000,000 distance calculations. It isn’t as bad as all that because sklearn automatically selects the algorithm used to find the nearest neighbors, and decades of research has uncovered “better than brute force” approaches. Curious readers will want to investigate the terms <em>K-D-tree</em> and <em>Ball tree</em> (sometimes called <em>Metric tree</em>). See “An Empirical Comparison of Exact Nearest Neighbor Algorithms” by Kibriya and Frank (2007). Still, because of the extreme difference in runtimes between the other model types and <em>k</em>-NN, it’s necessary to remember just how slow <em>k</em>-NN can be if the dataset is large.</p>&#13;
<p class="indent">The next slowest test times are for the Random Forest classifiers. We understand why the forest with 500 trees takes 10 times longer to run than the forest with 50 trees; we have 10 times as many trees to evaluate. Training times also scale linearly. Reducing the size of the feature vectors with PCA <span epub:type="pagebreak" id="page_158"/>improves things but not by a factor of 50 (784 features divided by 15 PCA features ≈ 50), so the performance difference is not primarily influenced by the size of the feature vector.</p>&#13;
<p class="indent">The linear SVMs are the next slowest to train after the Random Forests, but their execution time is extremely low. Long training times and short classification (inference) times are a hallmark of many model types. The simplest models are quick to train and quick to use, like Nearest Centroid or Naïve Bayes, but in general, “slow to train, quick to use” is a safe assumption. It’s especially true of neural networks.</p>&#13;
<p class="indent">Using PCA hurt the performance of the models except for the Naïve Bayes classifier. Let’s do an experiment to see the effect of PCA as the number of PCA components changes.</p>&#13;
<h4 class="h4" id="lev2_61">Experimenting with PCA Components</h4>&#13;
<p class="noindent">For <a href="ch07.xhtml#ch7tab10">Tables 7-10</a> and <a href="ch07.xhtml#ch7tab11">7-11</a>, we selected 15 PCA components that represented about 33 percent of the variance in the dataset. This value was selected at random. You could imagine training models using some other number of principal components.</p>&#13;
<p class="indent">Let’s examine the effect of the number of PCA components used on the accuracy of the resulting model. We’ll vary the number of components from 10 to 780, which is basically all the features in the image. For each number of components, we’ll train a Naïve Bayes classifier, a Random Forest of 50 trees, and a linear SVM with <em>C</em> = 1.0. The code to do this is in <a href="ch07.xhtml#ch7lis9">Listing 7-9</a>.</p>&#13;
<p class="programs" id="ch7lis9">def main():<br/>&#13;
    x_train = np.load("../data/mnist/mnist_train_vectors.npy")<br/>&#13;
                       .astype("float64")<br/>&#13;
    y_train = np.load("../data/mnist/mnist_train_labels.npy")<br/>&#13;
    x_test = np.load("../data/mnist/mnist_test_vectors.npy").astype("float64")<br/>&#13;
    y_test = np.load("../data/mnist/mnist_test_labels.npy")<br/>&#13;
    m = x_train.mean(axis=0)<br/>&#13;
    s = x_train.std(axis=0) + 1e-8<br/>&#13;
    x_ntrain = (x_train - m) / s<br/>&#13;
    x_ntest  = (x_test - m) / s<br/>&#13;
<br/>&#13;
    n = 78<br/>&#13;
    pcomp = np.linspace(10,780,n, dtype="int16")<br/>&#13;
    nb=np.zeros((n,4))<br/>&#13;
    rf=np.zeros((n,4))<br/>&#13;
    sv=np.zeros((n,4))<br/>&#13;
    tv=np.zeros((n,2))<br/>&#13;
<br/>&#13;
    for i,p in enumerate(pcomp):<br/>&#13;
      <span class="ent">❶</span> pca = decomposition.PCA(n_components=p)<br/>&#13;
         pca.fit(x_ntrain)<br/>&#13;
         (*\newpage*)<br/>&#13;
<span epub:type="pagebreak" id="page_159"/>         xtrain = pca.transform(x_ntrain)<br/>&#13;
         xtest = pca.transform(x_ntest)<br/>&#13;
         tv[i,:] = [p, pca.explained_variance_ratio_.sum()]<br/>&#13;
      <span class="ent">❷</span> sc,etrn,etst =run(xtrain, y_train, xtest, y_test, GaussianNB())<br/>&#13;
         nb[i,:] = [p,sc,etrn,etst]<br/>&#13;
         sc,etrn,etst =run(xtrain, y_train, xtest, y_test,<br/>&#13;
                       RandomForestClassifier(n_estimators=50))<br/>&#13;
         rf[i,:] = [p,sc,etrn,etst]<br/>&#13;
         sc,etrn,etst =run(xtrain, y_train, xtest, y_test, LinearSVC(C=1.0))<br/>&#13;
         sv[i,:] = [p,sc,etrn,etst]<br/>&#13;
<br/>&#13;
    np.save("mnist_pca_tv.npy", tv)<br/>&#13;
    np.save("mnist_pca_nb.npy", nb)<br/>&#13;
    np.save("mnist_pca_rf.npy", rf)<br/>&#13;
    np.save("mnist_pca_sv.npy", sv)</p>&#13;
<p class="figcap"><em>Listing 7-9: Model accuracy as a function of the number of PCA components used. See</em> mnist_pca.py.</p>&#13;
<p class="indent">First, we load the MNIST dataset and compute the normalized version. This is the version that we’ll use with PCA. Next, we set up storage for the results. The variable <code>pcomp</code> stores the specific number of PCA components that will be used from 10 to 780 in steps of 10. Then we start a loop over the number PCA components. We find the requested number of components (<code>p</code>) and map the dataset to the actual dataset trained and tested (<code>xtrain</code>, <code>xtest</code>) <span class="ent">❶</span>.</p>&#13;
<p class="indent">We also store the actual amount of variance in the dataset explained by the current number of principal components (<code>tv</code>). We’ll plot this value later to see how quickly the number of components covers the majority of the variance in the dataset.</p>&#13;
<p class="indent">Next, we train and test a Gaussian Naïve Bayes classifier using the current number of features <span class="ent">❷</span>. The <code>run</code> function called here is virtually identical to that used in <a href="ch07.xhtml#ch7lis7">Listing 7-7</a> except that it returns the score, the training time, and the testing time. These are captured and put into the appropriate output array (<code>nb</code>). Then we do the same for the Random Forest and linear SVM.</p>&#13;
<p class="indent">When the loop completes, we have all the data we need and we store the NumPy arrays on disk for plotting. Running this code takes some time, but the output, when plotted, leads to <a href="ch07.xhtml#ch7fig2">Figure 7-2</a>.</p>&#13;
<p class="indent">The solid curve shows the fraction of the total variance in the dataset explained by the current number of PCA components (x-axis). This curve will reach a maximum of 1.0 when all the features in the dataset are used. It’s helpful in this case because it shows how quickly adding new components explains major orientations of the data. For MNIST, we see that about 90 percent of the variance is explained by using less than half the possible number of PCA components.</p>&#13;
<div class="image" id="ch7fig2"><span epub:type="pagebreak" id="page_160"/><img src="Images/07fig02.jpg" alt="image" width="678" height="507"/></div>&#13;
<p class="figcap"><em>Figure 7-2: Results of the PCA search</em></p>&#13;
<p class="indent">The remaining three curves plot the accuracy of the resulting models on the test data. The best-performing model, in this case, is the Random Forest with 50 trees (triangles). This is followed by the linear SVM (squares) and then Naïve Bayes (circles). These curves show how the number of PCA components tracks with accuracy, and while the Random Forest and SVM change only slowly as PCA changes, we see that the Naïve Bayes classifier rapidly loses accuracy as the number of PCA components increases. Even the Random Forest and SVM decrease as the number of PCA components increases, which we might expect because the curse of dimensionality will eventually creep in. It seems likely that the dramatically different behavior of the Naïve Bayes classifier is due to violations of the independence assumption as the number of components used increases.</p>&#13;
<p class="indent">The maximum accuracy and the number of PCA components where it occurs are shown in <a href="ch07.xhtml#ch7tab12">Table 7-12</a>.</p>&#13;
<p class="tabcap" id="ch7tab12"><strong>Table 7-12:</strong> Maximum Accuracy on MNIST by Model and Number of Components</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:40%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<thead class="borderb">&#13;
<tr>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Accuracy</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Components</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Variance</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.81390</p></td>&#13;
<td style="vertical-align: top"><p class="tab">20</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3806</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.94270</p></td>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7033</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Linear SVM (C = 1.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.91670</p></td>&#13;
<td style="vertical-align: top"><p class="tab">370</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9618</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><a href="ch07.xhtml#ch7tab12">Table 7-12</a> tracks with the plot in <a href="ch07.xhtml#ch7fig2">Figure 7-2</a>. Interestingly, the SVM does not reach a maximum until nearly all the features in the original dataset are <span epub:type="pagebreak" id="page_161"/>used. Also, the best accuracy found for the Random Forest and SVM is not as good as seen previously for other versions of the dataset that did not use PCA. So, for these models, PCA is not a benefit; it is, however, for the Naïve Bayes classifier.</p>&#13;
<h4 class="h4" id="lev2_62">Scrambling Our Dataset</h4>&#13;
<p class="noindent">Before we leave this section, let’s look at one more experiment that we’ll come back to in <a href="ch09.xhtml#ch09">Chapter 9</a> and again in <a href="ch12.xhtml#ch12">Chapter 12</a>. In <a href="ch05.xhtml#ch05">Chapter 5</a>, we made a version of the MNIST dataset that scrambled the order of the pixels in the digit images. The scrambling wasn’t random: the same pixel in each input image was moved to the same position in the output image, resulting in images that, at least to us, no longer look like the original digit, as <a href="ch07.xhtml#ch7fig3">Figure 7-3</a> shows. How might this scrambling affect the accuracy of the models we’ve been using in this chapter?</p>&#13;
<div class="image" id="ch7fig3"><img src="Images/07fig03.jpg" alt="image" width="532" height="107"/></div>&#13;
<p class="figcap"><em>Figure 7-3: Original MNIST digits (top) and scrambled versions of the same digit (bottom).</em></p>&#13;
<p class="indent">Let’s repeat the experiment code of <a href="ch07.xhtml#ch7lis8">Listing 7-8</a>, this time running only the scaled [0,1) version of the scrambled MNIST images. Since the only difference to the original code is the source filenames and the fact that we call <code>run</code> only once, we’ll forgo a new listing.</p>&#13;
<p class="indent">Placing the accuracy results side by side gives us <a href="ch07.xhtml#ch7tab13">Table 7-13</a>.</p>&#13;
<p class="tabcap" id="ch7tab13"><strong>Table 7-13:</strong> MNIST Scores by Model Type for Unscrambled and Scrambled Digits</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:40%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead class="borderb">&#13;
<tr>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Unscrambled [0,1)</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Scrambled [0,1)</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8203</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8203</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>k</em>-NN (<em>k</em> = 3)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9705</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9705</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>k</em>-NN (<em>k</em> = 7)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9694</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9694</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Naïve Bayes</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5558</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5558</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Decision Tree</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8784</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8772</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9244</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9214</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9661</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9651</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (500)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9709</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9721</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (1000)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9716</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9711</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 0.01)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9171</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9171</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 0.1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9181</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9181</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 1.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9182</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9185</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">LinearSVM (C = 10.0)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9019</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8885</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_162"/>Here we see virtually no difference between the scrambled and unscrambled results. In fact, for several models, the results are identical. For stochastic models, like the Random Forests, the results are still very similar. Is this surprising? Perhaps at first, but if we think about it for a bit, we realize that it really shouldn’t be.</p>&#13;
<p class="indent">All of the classic models are holistic: they operate on the entire feature vector as a single entity. While we can’t see the digits anymore because our vision does not operate holistically, the <em>information</em> present in the image is still there, so the models are just as happy with the scrambled as unscrambled inputs. When we get to <a href="ch12.xhtml#ch12">Chapter 12</a>, we’ll encounter a different result of this experiment.</p>&#13;
<h3 class="h3" id="lev1_47">Classical Model Summary</h3>&#13;
<p class="noindent">What follows is a summary of the pros and cons related to each of the classical model types we have explored in this chapter. This can be used as a quick list for future reference. It will also take some of the observations we made via our experiments and make them more concrete.</p>&#13;
<h4 class="h4" id="lev2_63">Nearest Centroid</h4>&#13;
<p class="noindent">This is the simplest of all the models and can serve as a baseline. It’s seldom adequate unless the task at hand is particularly easy. The single centroid for each class is needlessly restrictive. You could use a more generalized approach that first finds an appropriate number of centroids for each class and then groups them together to build the classifier. In the extreme, this approaches <em>k</em>-NN but is still simpler in that the number of centroids is likely far less than the number of training samples. We’ll leave the implementation of this variation as an exercise for the motivated reader.</p>&#13;
<h5 class="h5">Pros</h5>&#13;
<p class="noindent">As we saw in this chapter, the implementation of a Nearest Centroid classifier takes only a handful of code. Additionally, Nearest Centroid is not restricted to binary models and readily supports multiclass models, like the irises. Training is very fast and since only one centroid is stored per class, the memory overhead is likewise very small. When used to label an unknown sample, run time is also very small because the distance from the sample to each class centroid is all that needs to be computed.</p>&#13;
<h5 class="h5">Cons</h5>&#13;
<p class="noindent">Nearest Centroid makes a simplistic assumption about the distribution of the classes in the feature space—one that’s seldom met in practice. As a consequence of this assumption, the Nearest Centroid classifier is only highly accurate when the classes form a single tight group in the feature space and the groups are distant from each other like isolated islands.</p>&#13;
<h4 class="h4" id="lev2_64"><span epub:type="pagebreak" id="page_163"/><em>k</em>-Nearest Neighbors</h4>&#13;
<p class="noindent">This is the simplest model to train since there’s no training: we store the training set and use it to classify new instances by finding the <em>k</em> nearest training set vectors and voting.</p>&#13;
<h5 class="h5">Pros</h5>&#13;
<p class="noindent">As just mentioned, no training required makes <em>k</em>-NN particularly attractive. It also can perform quite well, especially if the number of training samples is large relative to the dimensionality of the feature space (that is, the number of features in the feature vector). Multiclass support is implicit and doesn’t require a special approach.</p>&#13;
<h5 class="h5">Cons</h5>&#13;
<p class="noindent">The simplicity of “training” comes at a cost: classification is slow because of the need to look at every training example to find the nearest neighbors to the unknown feature vector. Decades of research, still underway, have sped up the search to improve the naïve implementation of looking at every training sample every time, but, as we saw in this chapter, classification is still slow, especially when compared to the speed of other model types (for example, SVM).</p>&#13;
<h4 class="h4" id="lev2_65">Naïve Bayes</h4>&#13;
<p class="noindent">This model is conceptually simple and efficient, and surprisingly valid even when the core assumption of feature independence isn’t met.</p>&#13;
<h5 class="h5">Pros</h5>&#13;
<p class="noindent">Naïve Bayes is fast to train and fast to classify with, both positives. It also supports multiclass models instead of just binary, and other than continuous features. As long as the probability of a particular feature value can be computed, we can apply Naïve Bayes.</p>&#13;
<h5 class="h5">Cons</h5>&#13;
<p class="noindent">The feature independence assumption central to Naïve Bayes is seldom true in practice. The more correlated the features (the more a change in, say, feature <em>x</em><sub>2</sub> implies that <em>x</em><sub>3</sub> will change), the poorer the performance of the model (in all likelihood).</p>&#13;
<p class="indent">While Naïve Bayes works directly with discrete valued features, using continuous features often involves a second level of assumption, as when we assumed that the continuous breast cancer dataset features were well represented as samples from a Gaussian distribution. This second assumption, which is also likely seldom true in practice, means that we need to estimate the parameters of the distribution from the dataset instead of using histograms to stand in for the actual feature probabilities.</p>&#13;
<h4 class="h4" id="lev2_66"><span epub:type="pagebreak" id="page_164"/>Decision Trees</h4>&#13;
<p class="noindent">This model is useful when it’s important to be able to understand, in human terms, why a particular class was selected.</p>&#13;
<h5 class="h5">Pros</h5>&#13;
<p class="noindent">Decision Trees are reasonably fast to train. They’re also fast to use for classifying. Multiclass models are not a problem and are not restricted to using just continuous features. A Decision Tree can justify its answer by showing the particular steps used to reach a decision: the series of questions asked from the root to the leaf.</p>&#13;
<h5 class="h5">Cons</h5>&#13;
<p class="noindent">Decision Trees are prone to overfitting—to learning elements of the training data that are not generally true of the parent distribution. Also, interpretability degrades as the tree increases in size. Tree depth needs to be balanced with the quality of the decisions (labels) as the leaves of the tree. This directly affects the error rate.</p>&#13;
<h4 class="h4" id="lev2_67">Random Forests</h4>&#13;
<p class="noindent">This is a more powerful form of Decision Tree that uses randomness to reduce the overfitting problem. Random Forests are one of the best performing of the classic model types and apply to a wide range of problem domains.</p>&#13;
<h5 class="h5">Pros</h5>&#13;
<p class="noindent">Like Decision Trees, Random Forests support multiclass models and other than continuous features. They are reasonably fast to train and to use for inference. Random Forests are also robust to differences in scale between features in the feature vector. In general, the accuracy improves, with diminishing returns, as the size of the forest grows.</p>&#13;
<h5 class="h5">Cons</h5>&#13;
<p class="noindent">The easy interpretability of a Decision Tree disappears with a Random Forest. While each tree in the forest can justify its decision, the combined effect of the forest as a whole can be difficult to understand.</p>&#13;
<p class="indent">The inference runtime of a forest scales linearly with the number of trees. However, this can be mitigated by parallelization since each tree in the forest is making a calculation that does not depend on any other tree until combining the output of all trees to make an overall decision.</p>&#13;
<p class="indent">As stochastic models, the overall performance of a forest varies from training session to training session for the same dataset. In general, this isn’t an issue, but a pathological forest could exist—if possible, train the forest several times to get a sense of the actual performance.</p>&#13;
<h4 class="h4" id="lev2_68"><span epub:type="pagebreak" id="page_165"/>Support Vector Machines</h4>&#13;
<p class="noindent">Before the “rebirth” of neural networks, Support Vector Machines were generally considered to provide the pinnacle of model performance when they were applicable and well-tuned.</p>&#13;
<h5 class="h5">Pros</h5>&#13;
<p class="noindent">SVMs can give show excellent performance when properly tuned. Inference is very fast once trained.</p>&#13;
<h5 class="h5">Cons</h5>&#13;
<p class="noindent">Multiclass models are not directly supported. Extensions for multiclass problems require training multiple models whether using one-versus-one or one-versus-rest approaches. Additionally, SVMs expect only continuous features and feature scaling matters; normalization or other scaling is often necessary to get good performance.</p>&#13;
<p class="indent">Large datasets are difficult to train when using other than linear kernels, and SVMs often require careful tuning of margin and kernel parameters (<em>C</em>, <em>γ</em>), though this can be mitigated somewhat by search algorithms that seek the best hyperparameter values.</p>&#13;
<h3 class="h3" id="lev1_48">When to Use Classical Models</h3>&#13;
<p class="noindent">The classical models may be <em>classic</em>, but they are still appropriate under the right conditions. In this section, we’ll discuss when you should consider a classical model instead of a more modern approach.</p>&#13;
<h4 class="h4" id="lev2_69">Handling Small Datasets</h4>&#13;
<p class="noindent">One of the best reasons for working with a classic model is when the dataset is small. If you have only a few tens or hundreds of examples, then a classic model might be a good fit, whereas a deep learning model might not have enough training data to condition itself to the problem. Of course, there are exceptions. A deep neural network can, via transfer learning, sometimes learn from relatively few examples. Other approaches, like zero-shot or few-shot learning, may also allow a deep network to learn from a small dataset. However, these techniques are far beyond the scope of what we want to address in this book. For us, the rule of thumb is: when the dataset is small, consider using a classic model.</p>&#13;
<h4 class="h4" id="lev2_70">Dealing with Reduced Computational Requirements</h4>&#13;
<p class="noindent">Another reason to consider a classic model is when computational requirements must be kept to a minimum. Deep neural networks are notoriously demanding of computational resources. The thousands, millions, and even billions of connections in a deep network all require extensive calculation. <span epub:type="pagebreak" id="page_166"/>Implementing such a model on a small handheld device, or on an embedded microcontroller, will not work, or at least not work in any reasonable timeframe.</p>&#13;
<p class="indent">In such cases, you might consider a classic model that doesn’t require a lot of overhead. Simple models like Nearest Centroid or Naïve Bayes are good candidates. So are Decision Trees and Support Vector Machines, once trained. From the previous experiments, <em>k</em>-NN is probably not a good candidate unless the feature space or training set is small. This leads to our next rule of thumb: when computation must be kept to a minimum, consider using a classic model.</p>&#13;
<h4 class="h4" id="lev2_71">Having Explainable Models</h4>&#13;
<p class="noindent">Some classic models can explain themselves by revealing exactly <em>how</em> they arrived at their answer for a given unknown input. This includes Decision Trees, by design, but also <em>k</em>-NN (by showing the labels of the <em>k</em> voters), Nearest Centroid (by virtue of the selected centroid), and even Naïve Bayes (by the selected posterior probability). By way of contrast, deep neural networks are black boxes—they do not explain themselves—and it’s an active area of research to learn how to get a deep network to give some reason for its decision. This research has not been entirely unsuccessful, to be sure, but it’s still far from looking like the decision path in a tree classifier. Therefore, we can give another rule of thumb: when it’s essential to know how the classifier makes its decision, consider using a classic model.</p>&#13;
<h4 class="h4" id="lev2_72">Working with Vector Inputs</h4>&#13;
<p class="noindent">Our final rule of thumb, acknowledging, of course, that there are indeed others we could give, has to do with the form of the inputs to the model. Modern deep learning systems often work with inputs that are not an amalgamation of separate features put into a single vector but instead are multidimensional inputs, such as images, where the “features” (pixels) are not different from each other but of the same kind and often highly correlated (the red pixel of the apple likely has a red pixel next to it, for example). A color image is a three-dimensional beast: there are three color images, one for the red channel, one for the blue channel, and one for the green channel. If the inputs are images from other sources, like satellites, there might be four to eight or more channels per image. A convolutional neural network is designed precisely for inputs such as these and will look for spatial patterns characteristic of the classes the network is trying to learn about. See <a href="ch12.xhtml#ch12">Chapter 12</a> for more details.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_167"/>But if the input to the model is a vector, especially a vector where the particular features are not related to each other (the key assumption of the Naïve Bayes classifier), then a classic model might be appropriate, since there’s no need to look for structure among the features beyond the global interpretation that the classic models perform by considering the input as a single monolithic entity. Therefore, we might give the rule as: when the input is a feature vector without spatial structure (unlike an image), especially if the features are not related to each other, consider using a classic model.</p>&#13;
<p class="indent">It’s important to remember that these are rule-of-thumb suggestions, and that they aren’t always applicable to a particular problem. Also, it’s possible to use deep networks even if these rules seem to apply; it’s just that they may not give the best performance, or might be overkill, like using a shotgun to kill a fly. The main point of this book is to build intuition so that when a situation arises, we’ll know how to use the techniques we are exploring to maximum advantage. Pasteur said, “In the fields of observation, chance favors only the prepared mind” (lecture at the University of Lille, December 1854), and we wholeheartedly agree.</p>&#13;
<h3 class="h3" id="lev1_49">Summary</h3>&#13;
<p class="noindent">In this chapter, we worked with six common classical machine learning models: Nearest Centroid, <em>k</em>-Nearest Neighbors, Naïve Bayes, Decision Trees, Random Forests, and Support Vector Machines. We applied them to three datasets that were developed in <a href="ch05.xhtml#ch05">Chapter 5</a>: irises, breast cancer, and MNIST digits. We used the results of the experiments with these datasets to gain insight into the strengths and weaknesses of each model type along with the effect of different data preprocessing steps. We ended the chapter with a discussion of the classic models and when it might be appropriate to use them.</p>&#13;
<p class="indent">In the next chapter, we’ll move on from the classic models and begin our exploration of neural networks, the backbone of modern deep learning.<span epub:type="pagebreak" id="page_168"/></p>&#13;
</div></body></html>