- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: MACHINE LEARNING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习**
- en: '![Image](Images/comm-1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/comm-1.jpg)'
- en: Machine learning is found in almost every area of computer science. Over the
    past few years, I’ve attended computer science conferences in fields as diverse
    as distributed systems, databases, and stream processing, and no matter where
    I go, machine learning is already there. At some conferences, more than half of
    the presented research ideas have relied on machine learning methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习几乎在计算机科学的每个领域都有应用。在过去几年中，我参加了多个计算机科学领域的会议，涵盖了分布式系统、数据库和流处理等不同领域，无论走到哪里，机器学习都已经成为其中的一部分。在一些会议中，展示的研究创意中，有超过一半的依赖了机器学习方法。
- en: As a computer scientist, you must know the fundamental machine learning ideas
    and algorithms to round out your overall skill set. This chapter provides an introduction
    to the most important machine learning algorithms and methods, and gives you 10
    practical one-liners to apply these algorithms in your own projects.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名计算机科学家，你必须了解基本的机器学习理念和算法，以完善你的整体技能。本章介绍了最重要的机器学习算法和方法，并提供了10个实用的单行代码，帮助你在自己的项目中应用这些算法。
- en: '**The Basics of Supervised Machine Learning**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**监督学习的基础**'
- en: The main aim of machine learning is to make accurate predictions using existing
    data. Let’s say you want to write an algorithm that predicts the value of a specific
    stock over the next two days. To achieve this goal, you’ll need to train a machine
    learning model. But what exactly is a *model*?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的主要目的是利用现有数据做出准确的预测。假设你想编写一个算法，预测未来两天某只股票的价值。为此，你需要训练一个机器学习模型。那么，到底什么是一个*模型*呢？
- en: 'From the perspective of a machine learning user, the machine learning model
    looks like a black box ([Figure 4-1](#ch04fig01)): you put data in and get predictions
    out.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习用户的角度来看，机器学习模型像一个黑箱（[图4-1](#ch04fig01)）：你输入数据，输出预测结果。
- en: '![images](Images/fig4-1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-1.jpg)'
- en: '*Figure 4-1: A machine learning model, shown as a black box*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-1：机器学习模型，表示为黑箱*'
- en: In this model, you call the input data *features* and denote them using the
    variable *x*, which can be a numerical value or a multidimensional vector of numerical
    values. Then the box does its magic and processes your input data. After a bit
    of time, you get prediction *y* back, which is the model’s predicted output, given
    the input features. For regression problems, the prediction consists of one or
    multiple numerical values—just like the input features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，你将输入数据称为*特征*，并用变量*x*表示，它可以是一个数值或一个多维数值向量。然后，模型会进行处理并完成它的“魔法”，在经过一段时间后，你会得到预测结果*y*，这是模型基于输入特征所预测的输出。对于回归问题，预测结果由一个或多个数值组成——就像输入特征一样。
- en: 'Supervised machine learning is divided into two separate phases: the training
    phase and the inference phase.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习分为两个独立的阶段：训练阶段和推理阶段。
- en: '***Training Phase***'
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***训练阶段***'
- en: During the *training phase*, you tell your model your desired output *y’* for
    a given input *x*. When the model outputs the prediction *y*, you compare it to
    *y’*, and if they are not the same, you update the model to generate an output
    that is closer to *y’*, as shown in [Figure 4-2](#ch04fig02). Let’s look at an
    example from image recognition. Say you train a model to predict fruit names (outputs)
    when given images (inputs). For example, your specific training input is an image
    of a banana, but your model wrongly predicts *apple*. Because your desired output
    is different from the model prediction, you change the model so that next time
    the model will correctly predict *banana*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在*训练阶段*，你将给模型提供一个特定输入*x*和你期望的输出*y’*。当模型输出预测结果*y*时，你会将其与*y’*进行比较，如果它们不相同，你就更新模型，以便生成一个更接近*y’*的输出，如[图4-2](#ch04fig02)所示。让我们看一个图像识别的例子。假设你训练一个模型，当给定图像（输入）时，预测水果名称（输出）。例如，你的训练输入是一个香蕉的图像，但模型错误地预测为*苹果*。由于你的期望输出与模型预测不同，你将调整模型，使得下次模型能正确预测*香蕉*。
- en: '![images](Images/fig4-2.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-2.jpg)'
- en: '*Figure 4-2: The training phase of a machine learning model*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-2：机器学习模型的训练阶段*'
- en: 'As you keep telling the model your desired outputs for many different inputs
    and adjusting the model, you train the model by using your *training data*. Over
    time, the model will learn which output you’d like to get for certain inputs.
    That’s why data is so important in the 21st century: your model will be only as
    good as its training data. Without good training data, the model is guaranteed
    to fail. Roughly speaking, the training data supervises the machine learning process.
    That’s why we denote it *supervised learning*.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不断向模型提供不同输入的期望输出并调整模型时，你就在使用*训练数据*训练模型。随着时间的推移，模型将学习你希望在某些输入下得到的输出。这就是为什么数据在21世纪如此重要：你的模型将与其训练数据一样好。没有好的训练数据，模型注定会失败。粗略地说，训练数据监督着机器学习过程。这就是我们将其称为*有监督学习*的原因。
- en: '***Inference Phase***'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***推理阶段***'
- en: 'During the *inference phase*, you use the trained model to predict output values
    for new input features *x*. Note that the model has the power to predict outputs
    for inputs that have never been observed in the training data. For example, the
    fruit prediction model from the *training phase* can now identify the name of
    the fruits (learned in the training data) in images it has never seen before.
    In other words, suitable machine learning models possess the ability to *generalize*:
    they use their experience from the training data to predict outcomes for new inputs.
    Roughly speaking, models that generalize well produce accurate predictions for
    new input data. Generalized prediction for unseen input data is one of the strengths
    of machine learning and is a prime reason for its popularity across a wide range
    of applications.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在*推理阶段*，你使用训练好的模型来预测新输入特征*x*的输出值。请注意，模型具有预测在训练数据中从未观察过的输入的输出的能力。例如，*训练阶段*中的水果预测模型现在可以识别它从未见过的图像中（在训练数据中学到的）水果名称。换句话说，合适的机器学习模型具备*概括能力*：它们利用训练数据中的经验来预测新输入的结果。粗略地说，能够良好概括的模型会为新输入数据产生准确的预测。对于未见过的输入数据的泛化预测是机器学习的一个强项，也是它在广泛应用中受欢迎的主要原因。
- en: '**Linear Regression**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**线性回归**'
- en: '*Linear regression* is the one machine learning algorithm you’ll find most
    often in beginner-level machine learning tutorials. It’s commonly used in *regression
    problems*, for which the model predicts missing data values by using existing
    ones. A considerable advantage of linear regression, both for teachers and users,
    is its simplicity. But that doesn’t mean it can’t solve real problems! Linear
    regression has lots of practical use cases in diverse areas such as market research,
    astronomy, and biology. In this section, you’ll learn everything you need to know
    to get started with linear regression.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归*是你在初学者级别的机器学习教程中最常见的机器学习算法。它通常用于*回归问题*，其中模型通过使用现有的数据预测缺失的数据值。线性回归的一个显著优势是其简洁性，这对教师和用户都非常有利。但这并不意味着它不能解决实际问题！线性回归在市场研究、天文学和生物学等各个领域都有很多实际应用。在这一节中，你将学到开始使用线性回归所需的所有知识。'
- en: '***The Basics***'
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: How can you use linear regression to predict stock prices on a given day? Before
    I answer this question, let’s start with some definitions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用线性回归预测某一天的股价？在我回答这个问题之前，让我们先从一些定义开始。
- en: 'Every machine learning model consists of model parameters. *Model parameters*
    are internal configuration variables that are estimated from the data. These model
    parameters determine how exactly the model calculates the prediction, given the
    input features. For linear regression, the model parameters are called *coefficients*.
    You may remember the formula for two-dimensional lines from school: *f(x)* = *ax*
    + *c*.  The two variables *a* and *c* are the coefficients in the linear equation
    *ax* + *c*. You can describe how each input *x* is transformed into an output
    *f(x)* so that all outputs together describe a line in the two-dimensional space.
    By changing the coefficients, you can describe any line in the two-dimensional
    space.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习模型都由模型参数组成。*模型参数*是从数据中估算出来的内部配置变量。这些模型参数决定了给定输入特征时，模型如何准确地计算预测值。对于线性回归，模型参数被称为*系数*。你可能还记得学校时学过的二维直线公式：*f(x)*
    = *ax* + *c*。这两个变量*a*和*c*就是线性方程*ax* + *c*中的系数。你可以描述每个输入*x*是如何转化为输出*f(x)*的，使得所有输出一起描述二维空间中的一条直线。通过改变系数，你可以描述二维空间中的任何一条直线。
- en: 'Given the input features *x*[1], *x*[2], . . ., *x**[k]*,  the linear regression
    model combines the input features with the coefficients *a*[1], *a*[2], . . .,
    *a**[k]* to calculate the predicted output *y* by using this formula:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入特征*x*[1]、*x*[2]、...、*x**[k]*，线性回归模型将输入特征与系数*a*[1]、*a*[2]、...、*a**[k]*结合，通过使用该公式计算预测输出*y*：
- en: '*y* = *f*(*x*) = *a*[0] + *a*[1] × *x*[1] + *a*[2] × *x*[2] + ... + *a*[*k*]
    × *x*[*k*]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*) = *a*[0] + *a*[1] × *x*[1] + *a*[2] × *x*[2] + ... + *a*[*k*]
    × *x*[*k*]'
- en: 'In our stock price example, you have a single input feature, *x*, the day.
    You input the day *x* with the hope of getting a stock price, the output *y*.
    This simplifies the linear regression model to the formula of a two-dimensional
    line:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的股价示例中，你有一个输入特征*x*，即日期。你输入日期*x*，期望得到股价，即输出*y*。这将线性回归模型简化为二维直线公式：
- en: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x*'
- en: Let’s have a look at three lines for which you change only the two model parameters
    *a*[0] and *a*[1] in [Figure 4-3](#ch04fig03). The first axis describes the input
    *x*. The second axis describes the output *y*. The line represents the (linear)
    relationship between input and output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在[图4-3](#ch04fig03)中，只改变两个模型参数*a*[0]和*a*[1]的三条直线。第一轴描述输入*x*，第二轴描述输出*y*。直线表示输入与输出之间的（线性）关系。
- en: '![images](Images/fig4-3.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-3.jpg)'
- en: '*Figure 4-3: Three linear regression models (lines) described by different
    model parameters (coefficients). Every line represents a unique relationship between
    the input and the output variables.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-3：由不同模型参数（系数）描述的三条线性回归模型（直线）。每条线代表输入与输出变量之间的独特关系。*'
- en: 'In our stock price example, let’s say our training data is the indices of three
    days, `[0, 1, 2]`, matched with the stock prices `[155, 156, 157]`. To put it
    differently:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的股价示例中，假设我们的训练数据是三天的索引`[0, 1, 2]`，与股价`[155, 156, 157]`相对应。换句话说：
- en: Input `x=0` should cause output `y=155`
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`x=0`应导致输出`y=155`
- en: Input `x=1` should cause output `y=156`
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`x=1`应导致输出`y=156`
- en: Input `x=2` should cause output `y=157`
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`x=2`应导致输出`y=157`
- en: Now, which line best fits our training data? I plotted the training data in
    [Figure 4-4](#ch04fig04).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，哪条直线最适合我们的训练数据？我在[图4-4](#ch04fig04)中绘制了训练数据。
- en: '![images](Images/fig4-4.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-4.jpg)'
- en: '*Figure 4-4: Our training data, with its index in the array as the* x *coordinate,
    and its price as the* y *coordinate*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-4：我们的训练数据，其在数组中的索引作为*x*坐标，价格作为*y*坐标*'
- en: To find the line that best describes the data and, thus, to create a linear
    regression model, we need to determine the coefficients. This is where machine
    learning comes in. There are two principal ways of determining model parameters
    for linear regression. First, you can analytically calculate the line of best
    fit that goes between these points (the standard method for linear regression).
    Second, you can try different models, testing each against the labeled sample
    data, and ultimately deciding on the best one. In any case, you determine “best”
    through a process called *error minimization*, in which the model minimizes the
    squared difference (or selects the coefficients that lead to a minimal squared
    difference) of the predicted model values and the ideal output, selecting the
    model with the lowest error.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到最能描述数据的直线，从而创建线性回归模型，我们需要确定系数。这就是机器学习的作用所在。确定线性回归模型参数的主要方法有两种。首先，你可以通过解析方法计算出最佳拟合线，即通过这些数据点的直线（这是线性回归的标准方法）。其次，你可以尝试不同的模型，逐个对比标记样本数据，最终选择最合适的模型。无论如何，你通过一种叫做*误差最小化*的过程来确定“最佳”，在这个过程中，模型最小化预测模型值和理想输出之间的平方差（或选择能导致最小平方差的系数），从而选择误差最小的模型。
- en: 'For our data, you end up with coefficients of *a*[0] = 155.0 and *a*[1] = 1.0\.
    Then you put them into our formula for linear regression:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，你得到的系数是*a*[0] = 155.0 和 *a*[1] = 1.0。然后你将它们代入我们的线性回归公式：
- en: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x* = 155.0 + 1.0 × *x*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x* = 155.0 + 1.0 × *x*'
- en: and plot both the line and the training data in the same space, as shown in
    [Figure 4-5](#ch04fig05).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在同一坐标系中绘制直线和训练数据，如[图4-5](#ch04fig05)所示。
- en: '![images](Images/fig4-5.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-5.jpg)'
- en: '*Figure 4-5: A prediction line made using our linear regression model*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-5：使用我们的线性回归模型制作的预测线*'
- en: A perfect fit! The squared distance between the line (model prediction) and
    the training data is zero—so you have found the model that minimizes the error.
    Using this model, you can now predict the stock price for any value of *x*. For
    example, say you want to predict the stock price on day *x* = 4\. To accomplish
    this, you simply use the model to calculate *f(x)* = 155.0 + 1.0 × 4 = 159.0\.
    The predicted stock price on day 4 is $159\. Of course, whether this prediction
    accurately reflects the real world is another story.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的拟合！线（模型预测）和训练数据之间的平方距离为零——因此你已经找到了最小化误差的模型。使用这个模型，你现在可以预测任何 *x* 值下的股票价格。例如，假设你想预测第
    *x* 天 = 4 的股票价格。为了做到这一点，你只需使用模型计算 *f(x)* = 155.0 + 1.0 × 4 = 159.0。第4天的预测股票价格是159美元。当然，这个预测是否准确反映了现实世界是另一个问题。
- en: That’s the high-level overview of what happens. Let’s take a closer look at
    how to do this in code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是发生的高层次概览。让我们更仔细地看看如何用代码实现这一点。
- en: '***The Code***'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: '[Listing 4-1](#list4-1) shows how to build a simple linear regression model
    in a single line of code (you may need to install the scikit-learn library first
    by running `pip install sklearn` in your shell).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 4-1](#list4-1)展示了如何用一行代码构建一个简单的线性回归模型（你可能需要先通过在命令行中运行`pip install sklearn`来安装scikit-learn库）。'
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 4-1: A simple linear regression model*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-1：一个简单的线性回归模型*'
- en: Can you already guess the output of this code snippet?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜出这个代码片段的输出吗？
- en: '***How It Works***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***工作原理***'
- en: 'This one-liner uses two Python libraries: NumPy and scikit-learn. The former
    is the de facto standard library for numerical computations (like matrix operations).
    The latter is the most comprehensive library for machine learning and has implementations
    of hundreds of machine learning algorithms and techniques.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个一行代码使用了两个 Python 库：NumPy 和 scikit-learn。前者是数值计算（如矩阵运算）的事实标准库，后者是最全面的机器学习库，包含了数百种机器学习算法和技术的实现。
- en: 'You may ask: “Why are you using libraries in a Python one-liner? Isn’t this
    cheating?” It’s a good question, and the answer is yes. Any Python program—with
    or without libraries—uses high-level functionality built on low-level operations.
    There’s not much point in reinventing the wheel when you can reuse existing code
    bases (that is, stand on the shoulders of giants). Aspiring coders often feel
    the urge to implement everything on their own, but this reduces their coding productivity.
    In this book, we’re going to use, not reject, the wide spectrum of powerful functionality
    implemented by some of the world’s best Python coders and pioneers. Each of these
    libraries took skilled coders years to develop, optimize, and tweak.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问：“为什么在 Python 的一行代码中使用库？这不是作弊吗？”这是个好问题，答案是肯定的。任何 Python 程序——无论是否使用库——都使用了基于低级操作构建的高级功能。在可以重用现有代码库的情况下，重新发明轮子是没有太大意义的（也就是站在巨人的肩膀上）。有抱负的编码者往往有一种冲动，想要自己实现所有功能，但这会降低他们的编码生产力。在这本书中，我们将使用，而不是拒绝，世界上一些最优秀的
    Python 编程者和先驱者实现的强大功能。这些库的每一个都经过了熟练的编码者多年的开发、优化和调整。
- en: Let’s go through [Listing 4-1](#list4-1) step by step. First, we create a simple
    data set of three values and store its length in a separate variable `n` to make
    the code more concise. Our data is three Apple stock prices for three consecutive
    days. The variable `apple` holds this data set as a one-dimensional NumPy array.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步分析[清单 4-1](#list4-1)。首先，我们创建一个包含三个值的简单数据集，并将其长度存储在一个单独的变量 `n` 中，以便代码更加简洁。我们的数据是三天内的三次苹果股票价格。变量
    `apple` 将这个数据集作为一维的 NumPy 数组保存。
- en: 'Second, we build the model by calling `LinearRegression()`. But what are the
    model parameters? To find them, we call the `fit()` function to train the model.
    The `fit()` function takes two arguments: the input features of the training data
    and the ideal outputs for these inputs. Our ideal outputs are the real stock prices
    of the Apple stock. But for the input features, `fit()` requires an array with
    the following format:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们通过调用 `LinearRegression()` 来构建模型。那么模型的参数是什么呢？为了找到它们，我们调用 `fit()` 函数来训练模型。`fit()`
    函数接受两个参数：训练数据的输入特征和这些输入的理想输出。我们的理想输出是苹果股票的实际股票价格。但是对于输入特征，`fit()` 需要一个以下格式的数组：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'where each training data value is a sequence of feature values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个训练数据值都是一组特征值的序列：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In our case, the input consists of only a single feature *x* (the current day).
    Moreover, the prediction also consists of only a single value *y* (the current
    stock price). To bring the input array into the correct shape, you need to reshape
    it to this strange-looking matrix form:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，输入仅包含一个特征 *x*（当前日期）。此外，预测也仅包含一个值 *y*（当前股票价格）。为了将输入数组调整为正确的形状，你需要将其重塑为这种看起来奇怪的矩阵形式：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A matrix with only one column is called a *column vector*. You use `np.arange()`
    to create the sequence of increasing *x* values; then you use `reshape((n, 1))`
    to convert the one-dimensional NumPy array into a two-dimensional array with one
    column and `n` rows (see [Chapter 3](ch03.xhtml#ch03)). Note that scikit-learn
    allows the output to be a one-dimensional array (otherwise, you would have to
    reshape the `apple` data array as well).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一列的矩阵称为 *列向量*。你可以使用 `np.arange()` 创建一个递增的 *x* 值序列；然后使用 `reshape((n, 1))` 将一维的
    NumPy 数组转换为一个具有一列和 `n` 行的二维数组（见 [第 3 章](ch03.xhtml#ch03)）。请注意，scikit-learn 允许输出为一维数组（否则，你还需要重塑
    `apple` 数据数组）。
- en: 'Once it has the training data and the ideal outputs, `fit()` then does error
    minimization: it finds the model parameters (that means *line*) so that the difference
    between the predicted model values and the desired outputs is minimal.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得了训练数据和理想的输出，`fit()` 就会进行误差最小化：它会找到模型参数（这意味着*直线*），使得预测模型值与期望输出之间的差异最小。
- en: 'When `fit()` is satisfied with its model, it’ll return a model that you can
    use to predict two new stock values by using the `predict()` function. The `predict()`
    function has the same input requirements as `fit()`, so to satisfy them, you’ll
    pass a one-column matrix with our two new values that you want predictions for:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `fit()` 对它的模型感到满意时，它会返回一个模型，你可以使用 `predict()` 函数预测两个新的股票值。`predict()` 函数与
    `fit()` 函数有相同的输入要求，所以为了满足这些要求，你需要传入一个包含我们想要预测的两个新值的一列矩阵：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because our error minimization was zero, you should get perfectly linear outputs
    of 158 and 159\. This fits well along the line of fit plotted in [Figure 4-5](#ch04fig05).
    But it’s often not possible to find such a perfectly fitting single straight-line
    linear model. For example, if our stock prices are `[157, 156, 159]`, and you
    run the same function and plot it, you should get the line in [Figure 4-6](#ch04fig06).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的误差最小化为零，你应该得到完美的线性输出 158 和 159。这个结果很好地符合在 [图 4-5](#ch04fig05) 中绘制的拟合直线。但通常情况下，很难找到这样一个完美拟合的单一直线模型。例如，如果我们的股票价格是
    `[157, 156, 159]`，然后你运行相同的函数并绘制它，你应该得到在 [图 4-6](#ch04fig06) 中的直线。
- en: In this case, the `fit()` function finds the line that minimizes the squared
    error between the training data and the predictions as described previously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`fit()` 函数会找到一条最小化训练数据和预测之间平方误差的直线，正如之前所描述的那样。
- en: Let’s wrap this up. Linear regression is a machine learning technique whereby
    your model learns coefficients as model parameters. The resulting linear model
    (for example, a line in the two-dimensional space) directly provides you with
    predictions on new input data. This problem of predicting numerical values when
    given numerical input values belongs to the class of regression problems. In the
    next section, you’ll learn about another important area in machine learning called
    classification.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下。线性回归是一种机器学习技术，在这种技术中，模型通过学习系数作为模型参数来进行训练。得到的线性模型（例如二维空间中的一条直线）直接为你提供新的输入数据的预测值。当给定数值输入时，预测数值的问题属于回归问题的范畴。在下一节，你将学习机器学习中的另一个重要领域——分类。
- en: '![images](Images/fig4-6.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-6.jpg)'
- en: '*Figure 4-6: A linear regression model with an imperfect fit*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-6：一个拟合不完美的线性回归模型*'
- en: '**Logistic Regression in One Line**'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**一行代码实现逻辑回归**'
- en: Logistic regression is commonly used for *classification problems*, in which
    you predict whether a sample belongs to a specific category (or class). This contrasts
    with regression problems, where you’re given a sample and predict a numerical
    value that falls into a continuous range. An example classification problem is
    to divide Twitter users into the male and female, given different input features
    such as their *posting frequency* or the *number of tweet replies*. The logistic
    regression model belongs to one of the most fundamental machine learning models.
    Many concepts introduced in this section will be the basis of more advanced machine
    learning techniques.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归通常用于*分类问题*，在这些问题中，你预测一个样本是否属于某个特定的类别（或类）。这与回归问题不同，回归问题中给定一个样本并预测一个数值，该数值属于一个连续的范围。例如，一个分类问题是根据不同的输入特征（如*发帖频率*或*推文回复数*）将Twitter用户分为男性和女性。逻辑回归模型属于最基础的机器学习模型之一。本节中介绍的许多概念将构成更高级机器学习技术的基础。
- en: '***The Basics***'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: 'To introduce logistic regression, let’s briefly review how linear regression
    works: given the training data, you compute a line that fits this training data
    and predicts the outcome for input *x*. In general, linear regression is great
    for predicting a *continuous* output, whose value can take an infinite number
    of values. The stock price predicted earlier, for example, could conceivably have
    been any number of positive values.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍逻辑回归，我们先简要回顾一下线性回归是如何工作的：给定训练数据，你计算出一条拟合这些数据的直线，并预测输入 *x* 的结果。一般来说，线性回归非常适合预测*连续*输出，其值可以取无限多个数值。例如，之前预测的股价，理论上可以是任何正数值。
- en: But what if the output is not continuous, but *categorical*, belonging to a
    limited number of groups or categories? For example, let’s say you want to predict
    the likelihood of lung cancer, given the number of cigarettes a patient smokes.
    Each patient can either have lung cancer or not. In contrast to the stock price,
    here you have only these two possible outcomes. Predicting the likelihood of categorical
    outcomes is the primary motivation for logistic regression.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果输出不是连续的，而是*分类的*，属于有限数量的组或类别呢？例如，假设你想预测一个患者吸烟量与得肺癌的可能性。每个患者要么得肺癌，要么不。这与股价预测不同，这里只有两种可能的结果。预测分类结果的可能性是逻辑回归的主要动机。
- en: '**The Sigmoid Function**'
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**Sigmoid 函数**'
- en: Whereas linear regression fits a line to the training data, logistic regression
    fits an S-shaped curve, called *the sigmoid function*. The S-shaped curve helps
    you make binary decisions (for example, yes/no). For most input values, the sigmoid
    function will return a value that is either very close to 0 (one category) or
    very close to 1 (the other category). It’s relatively unlikely that your given
    input value generates an ambiguous output. Note that it is possible to generate
    0.5 probabilities for a given input value—but the shape of the curve is designed
    in a way to minimize those in practical settings (for most possible values on
    the horizontal axis, the probability value is either very close to 0 or very close
    to 1). [Figure 4-7](#ch04fig07) shows a logistic regression curve for the lung
    cancer scenario.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 而线性回归拟合训练数据中的一条直线，逻辑回归则拟合一条S形曲线，这条曲线称为*Sigmoid函数*。S形曲线帮助你做出二元决策（例如， 是/否）。对于大多数输入值，Sigmoid
    函数的输出值要么非常接近 0（一个类别），要么非常接近 1（另一个类别）。在实际应用中，很少会有输入值生成模糊的输出。请注意，虽然对于某些输入值，可能会生成
    0.5 的概率值，但曲线的形状设计上是为了最小化这种情况（对于横轴上的大多数可能值，概率值要么非常接近 0，要么非常接近 1）。[图 4-7](#ch04fig07)展示了肺癌情景下的逻辑回归曲线。
- en: '![images](Images/fig4-7.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-7.jpg)'
- en: '*Figure 4-7: A logistic regression curve that predicts cancer based on cigarette
    use*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-7：基于吸烟量预测癌症的逻辑回归曲线*'
- en: '**NOTE**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*You can apply logistic regression for* multinomial classification *to classify
    the data into more than two classes. To accomplish this, you’ll use the generalization
    of the sigmoid function, called the* softmax function, *which returns a tuple
    of probabilities, one for each class. The sigmoid function transforms the input
    feature(s) into only a single probability value. However, for clarity and readability,
    I’ll focus on* binomial classification *and the sigmoid function in this section.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以应用逻辑回归进行* 多项分类 *，将数据分类为两个以上的类别。为此，你将使用 sigmoid 函数的推广版本，称为* softmax 函数，*
    它返回一个包含每个类别概率的元组。Sigmoid 函数将输入特征转换为单一的概率值。然而，为了清晰和可读性，我将在本节中重点讨论* 二项分类 *和 sigmoid
    函数。*'
- en: 'The sigmoid function in [Figure 4-7](#ch04fig07) approximates the probability
    that a patient has lung cancer, given the number of cigarettes they smoke. This
    probability helps you make a robust decision on the subject when the only information
    you have is the number of cigarettes the patient smokes: does the patient have
    lung cancer?'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#ch04fig07)中的 sigmoid 函数近似了一个病人是否患有肺癌的概率，给定他吸烟的数量。当你唯一掌握的信息是病人的吸烟量时，这个概率帮助你做出更有依据的决策：病人是否患有肺癌？'
- en: Have a look at the predictions in [Figure 4-8](#ch04fig08), which shows two
    new patients (in light gray at the bottom of the graph). You know nothing about
    them but the number of cigarettes they smoke. You’ve trained our logistic regression
    model (the sigmoid function) that returns a probability value for any new input
    value *x*. If the probability given by the sigmoid function is higher than 50
    percent, the model predicts *lung cancer positive*; otherwise, it predicts *lung
    cancer negative*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下[图 4-8](#ch04fig08)中的预测，它展示了两个新病人（图表底部的浅灰色部分）。你对他们一无所知，只知道他们的吸烟数量。你已经训练了我们的逻辑回归模型（sigmoid
    函数），它会根据任何新的输入值 *x* 返回一个概率值。如果 sigmoid 函数给出的概率超过 50%，模型预测为*肺癌阳性*；否则，它预测为*肺癌阴性*。
- en: '![images](Images/fig4-8.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-8.jpg)'
- en: '*Figure 4-8: Using logistic regression to estimate probabilities of a result*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-8：使用逻辑回归估算结果的概率*'
- en: '**Finding the Maximum Likelihood Model**'
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**寻找最大似然模型**'
- en: The main question for logistic regression is how to select the correct sigmoid
    function that best fits the training data. The answer is in each model’s *likelihood:*
    the probability that the model would generate the observed training data. You
    want to select the model with the maximum likelihood. Your sense is that this
    model best approximates the real-world process that generated the training data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的主要问题是如何选择最佳拟合训练数据的正确 sigmoid 函数。答案在于每个模型的*似然性*：即模型生成观察到的训练数据的概率。你想选择具有最大似然性的模型。你的直觉是，该模型最好地近似了生成训练数据的真实世界过程。
- en: To calculate the likelihood of a given model for a given set of training data,
    you calculate the likelihood for each single training data point, and then multiply
    those with each other to get the likelihood of the whole set of training data.
    How to calculate the likelihood of a single training data point? Simply apply
    this model’s sigmoid function to the training data point; it’ll give you the data
    point’s probability under this model. To select the maximum likelihood model for
    all data points, you repeat this same likelihood computation for different sigmoid
    functions (shifting the sigmoid function a little bit), as in [Figure 4-9](#ch04fig09).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算给定模型在一组训练数据上的似然性，你需要计算每一个单独训练数据点的似然性，然后将这些似然性相乘，得到整个训练数据集的似然性。如何计算单个训练数据点的似然性？只需将该模型的
    sigmoid 函数应用于该训练数据点；它会给出该数据点在此模型下的概率。为了选择对所有数据点具有最大似然的模型，你需要对不同的 sigmoid 函数重复相同的似然性计算（稍微调整一下
    sigmoid 函数），如[图 4-9](#ch04fig09)所示。
- en: In the previous paragraph, I described how to determine the maximum likelihood
    sigmoid function (model). This sigmoid function fits the data best—so you can
    use it to predict new data points.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一段中，我描述了如何确定最大似然的 sigmoid 函数（模型）。这个 sigmoid 函数最能拟合数据——因此，你可以用它来预测新的数据点。
- en: Now that we’ve covered the theory, let’s look at how you’d implement logistic
    regression as a Python one-liner.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了理论，让我们看看如何将逻辑回归实现为一行 Python 代码。
- en: '![images](Images/fig4-9.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-9.jpg)'
- en: '*Figure 4-9: Testing several sigmoid functions to determine maximum likelihood*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-9：测试多个 sigmoid 函数以确定最大似然性*'
- en: '***The Code***'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: You’ve seen an example of using logistic regression for a health application
    (correlating cigarette consumption with cancer probability). This “virtual doc”
    application would be a great idea for a smartphone app, wouldn’t it? Let’s program
    your first virtual doc using logistic regression, as shown in [Listing 4-2](#list4-2)—in
    a single line of Python code!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到过一个健康应用程序的逻辑回归示例（将吸烟量与癌症概率相关联）。这个“虚拟医生”应用程序将是一个非常棒的智能手机应用程序创意，不是吗？让我们使用逻辑回归编写你的第一个虚拟医生，代码如[清单
    4-2](#list4-2)所示——仅需一行 Python 代码！
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Listing 4-2: A logistic regression model*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-2：一个逻辑回归模型*'
- en: 'Take a guess: what’s the output of this code snippet?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜看：这段代码的输出结果是什么？
- en: '***How It Works***'
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***工作原理***'
- en: The training data `X` consists of four patient records (the rows) with two columns.
    The first column holds the number of cigarettes the patients smoke (*input feature*),
    and the second column holds the *class labels*, which say whether they ultimately
    suffered from lung cancer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据`X`包含四条患者记录（行），每条记录有两列。第一列是患者吸烟的数量（*输入特征*），第二列是*类别标签*，表示他们是否最终患上了肺癌。
- en: You create the model by calling the `LogisticRegression()` constructor. You
    call the `fit()` function on this model; `fit()` takes two arguments, which are
    the input (cigarette consumption) and the output class labels (cancer). The `fit()`
    function expects a two-dimensional input array format with one row per training
    data sample and one column per feature of this training data sample. In this case,
    you have only a single feature value so you transform the one-dimensional input
    into a two-dimensional NumPy array by using the `reshape()` operation. The first
    argument to `reshape()` specifies the number of rows, and the second specifies
    the number of columns. You care about only the number of columns, which here is
    `1`. You’ll pass `-1` as the number of desired rows, which is a special signal
    to NumPy to determine the number of rows automatically.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过调用`LogisticRegression()`构造函数来创建模型。你在该模型上调用`fit()`函数；`fit()`函数接受两个参数，分别是输入（吸烟量）和输出的类别标签（癌症）。`fit()`函数期望输入是一个二维数组格式，其中每一行代表一个训练数据样本，每一列代表该样本的一个特征。在此案例中，你只有一个特征值，因此你通过使用`reshape()`操作将一维输入转换为二维
    NumPy 数组。`reshape()`的第一个参数指定行数，第二个参数指定列数。你只关心列数，这里是`1`。你将传递`-1`作为期望的行数，这意味着 NumPy
    会自动决定行数。
- en: 'The input training data will look as follows after reshaping (in essence, you
    simply remove the class labels and keep the two-dimensional array shape intact):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 经过调整形状后的输入训练数据如下所示（本质上，你只是去除了类别标签，保持了二维数组的形状）：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, you predict whether a patient has lung cancer, given the number of cigarettes
    they smoke: your input will be 2, 12, 13, 40, 90 cigarettes. That gives an output
    as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要根据患者吸烟的数量预测他们是否患有肺癌：输入的数据为2、12、13、40、90支香烟。输出结果如下：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The model predicts that the first two patients are lung cancer negative, while
    the latter three are lung cancer positive.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测前两位患者为肺癌阴性，而后面三位患者为肺癌阳性。
- en: 'Let’s look in detail at the probabilities the sigmoid function came up with
    that lead to this prediction! Simply run the following code snippet after [Listing
    4-2](#list4-2):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看 sigmoid 函数计算出的概率，看看是如何得出这个预测的！只需在[清单 4-2](#list4-2)之后运行以下代码片段：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `predict_proba()` function takes as input the number of cigarettes and
    returns an array containing the probability of lung cancer negative (at index
    0) and the probability of lung cancer positive (index 1). When you run this code,
    you should get the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict_proba()`函数接受香烟数量作为输入，并返回一个数组，其中包含肺癌阴性的概率（索引0）和肺癌阳性的概率（索引1）。当你运行此代码时，你应该得到如下输出：'
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If the probability of lung cancer being negative is higher than the probability
    of lung cancer being positive, the predicted outcome will be *lung cancer negative*.
    This happens the last time for `x=12`. If the patient has smoked more than 12
    cigarettes, the algorithm will classify them as *lung cancer positive*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果肺癌为负的概率大于肺癌为正的概率，那么预测结果将是*肺癌阴性*。这发生在`x=12`时。如果患者吸烟超过12支香烟，算法将把他们分类为*肺癌阳性*。
- en: 'In summary, you’ve learned how to classify problems easily with logistic regression
    using the scikit-learn library. The idea of logistic regression is to fit an S-shaped
    curve (the sigmoid function) to the data. This function assigns a numerical value
    between 0 and 1 to every new data point and each possible class. The numerical
    value models the probability of this data point belonging to the given class.
    However, in practice, you often have training data but no class label assigned
    to the training data. For example, you have customer data (say, their age and
    their income) but you don’t know any class label for each data point. To still
    extract useful insights from this kind of data, you will learn about another category
    of machine learning next: unsupervised learning. Specifically, you’ll learn about
    how to find similar clusters of data points, an important subset of unsupervised
    learning.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你已经学会了如何使用 scikit-learn 库轻松地用逻辑回归进行问题分类。逻辑回归的思想是将一个 S 形曲线（即 sigmoid 函数）拟合到数据上。这个函数为每个新数据点和每个可能的类别分配一个介于
    0 和 1 之间的数值。这个数值表示该数据点属于给定类别的概率。然而，在实际应用中，你通常会有训练数据，但没有为训练数据分配类别标签。例如，你有客户数据（比如他们的年龄和收入），但你不知道每个数据点的类别标签。为了从这种数据中提取有用的见解，接下来你将学习另一类机器学习：无监督学习。具体来说，你将学习如何找到相似的数据点聚类，这是无监督学习的一个重要子集。
- en: '**K-Means Clustering in One Line**'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**K-均值聚类的一行代码**'
- en: If there’s one clustering algorithm you need to know—whether you’re a computer
    scientist, data scientist, or machine learning expert—it’s the *K-Means algorithm*.
    In this section, you’ll learn the general idea and when and how to use it in a
    single line of Python code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个聚类算法是你必须了解的，无论你是计算机科学家、数据科学家，还是机器学习专家，那就是*K-均值算法*。在本节中，你将学习其基本概念，并且通过一行
    Python 代码来了解何时以及如何使用它。
- en: '***The Basics***'
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: The previous sections covered supervised learning, in which the training data
    is *labeled*. In other words, you know the output value of every input value in
    the training data. But in practice, this isn’t always the case. Often, you’ll
    find yourself confronted with *unlabeled* data—especially in many data analytics
    applications—where it’s not clear what “the optimal output” means. In these situations,
    a prediction is impossible (because there is no output to start with), but you
    can still distill useful knowledge from these unlabeled data sets (for example,
    you can find clusters of similar unlabeled data). Models that use unlabeled data
    fall under the category of *unsupervised learning*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节讲解了有监督学习，在这种学习中，训练数据是*标注过的*。换句话说，你知道训练数据中每个输入值的输出值。但在实际应用中，情况并非总是如此。你常常会遇到*未标注*的数据，尤其是在许多数据分析应用中，这些数据无法明确说明“最优输出”是什么意思。在这种情况下，预测是不可能的（因为没有输出值可供参考），但你仍然可以从这些未标注的数据集中提取有用的知识（例如，找到相似的未标注数据聚类）。使用未标注数据的模型属于*无监督学习*的范畴。
- en: As an example, suppose you’re working in a startup that serves different target
    markets with various income levels and ages. Your boss tells you to find a certain
    number of target personas that best fit your target markets. You can use clustering
    methods to identify the *average customer personas* that your company serves.
    [Figure 4-10](#ch04fig10) shows an example.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你在一家初创公司工作，该公司为不同目标市场提供服务，目标市场的收入水平和年龄各不相同。你的老板告诉你，找出最符合目标市场的若干目标人物。你可以使用聚类方法来识别公司服务的*平均客户画像*。[图
    4-10](#ch04fig10)展示了一个示例。
- en: '![images](Images/fig4-10.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-10.jpg)'
- en: '*Figure 4-10: Observed customer data in the two-dimensional space*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-10：二维空间中观察到的客户数据*'
- en: Here, you can easily identify three types of personas with different types of
    incomes and ages. But how to find those algorithmically? This is the domain of
    clustering algorithms such as the widely popular K-Means algorithm. Given the
    data sets and an integer *k*, the K-Means algorithm finds *k* clusters of data
    such that the difference between the center of a cluster (called the *centroid*)
    and the data in the cluster is minimal. In other words, you can find the different
    personas by running the K-Means algorithm on your data sets, as shown in [Figure
    4-11](#ch04fig11).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以轻松地识别出三种不同类型的画像，它们的收入和年龄各不相同。但如何通过算法来找到这些呢？这就是聚类算法的应用领域，比如广泛使用的 K-均值算法。给定数据集和一个整数*k*，K-均值算法会找到*k*个数据聚类，使得每个聚类的中心（称为*质心*）与该聚类中的数据点之间的差异最小。换句话说，你可以通过对数据集运行
    K-均值算法来找到不同的目标画像，如[图 4-11](#ch04fig11)所示。
- en: '![images](Images/fig4-11.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-11.jpg)'
- en: '*Figure 4-11: Customer data with customer personas (cluster centroids) in the
    two-dimensional space*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-11：具有客户画像（聚类中心）的客户数据在二维空间中的分布*'
- en: 'The cluster centers (black dots) match the clustered customer data. Every cluster
    center can be viewed as one customer persona. Thus, you have three idealized personas:
    a 20-year-old earning $2000, a 25-year-old earning $3000, and a 40-year-old earning
    $4000\. And the great thing is that the K-Means algorithm finds those cluster
    centers even in high-dimensional spaces (where it would be hard for humans to
    find the personas visually).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类中心（黑点）与聚类后的客户数据相匹配。每个聚类中心可以视为一个客户画像。因此，你有三个理想化的画像：一个 20 岁的员工赚 $2000，一个 25
    岁的员工赚 $3000，和一个 40 岁的员工赚 $4000。最棒的是，K-Means 算法甚至可以在高维空间中找到这些聚类中心（在这些空间中，人类很难通过视觉找到这些画像）。
- en: The K-Means algorithm requires “the number of cluster centers *k*” as an input.
    In this case, you look at the data and “magically” define *k* = 3\. More advanced
    algorithms can find the number of cluster centers automatically (for an example,
    look at the 2004 paper “Learning the k in K-Means” by Greg Hamerly and Charles
    Elkan).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 算法需要“聚类中心的数量 *k*”作为输入。在这种情况下，你观察数据并“神奇地”定义 *k* = 3。更高级的算法可以自动找到聚类中心的数量（例如，可以参考
    Greg Hamerly 和 Charles Elkan 在 2004 年的论文《Learning the k in K-Means》）。
- en: 'So how does the K-Means algorithm work? In a nutshell, it performs the following
    procedure:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 K-Means 算法是如何工作的呢？简而言之，它执行以下步骤：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in multiple loop iterations: you first assign the data to the
    *k* cluster centers, and then you recompute each cluster center as the centroid
    of the data assigned to it.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致多个循环迭代：首先，你将数据分配到 *k* 个聚类中心，然后重新计算每个聚类中心，作为分配给它的数据的质心。
- en: Let’s implement it!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现它吧！
- en: 'Consider the following problem: given two-dimensional salary data (*hours worked*,
    *salary earned*), find two clusters of employees in the given data set that work
    a similar number of hours and earn a similar salary.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下问题：给定二维薪资数据（*工作小时数*，*薪资收入*），在给定数据集中找到两个工作时长相似且薪资相似的员工聚类。
- en: '***The Code***'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: How can you do all of this in a single line of code? Fortunately, the scikit-learn
    library in Python already has an efficient implementation of the K-Means algorithm.
    [Listing 4-3](#list4-3) shows the one-liner code snippet that runs K-Means clustering
    for you.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在一行代码中完成这一切？幸运的是，Python 中的 scikit-learn 库已经高效地实现了 K-Means 算法。[清单 4-3](#list4-3)
    显示了运行 K-Means 聚类的一行代码。
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 4-3: K-Means clustering in one line*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-3：一行代码实现 K-Means 聚类*'
- en: What’s the output of this code snippet? Try to guess a solution even if you
    don’t understand every syntactical detail. This will open your knowledge gap and
    prepare your brain to absorb the algorithm much better.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出是什么？即使你不理解每个语法细节，也试着猜出一个解决方案。这将帮助你发现知识空白，并为大脑更好地吸收算法做准备。
- en: '***How It Works***'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***原理***'
- en: In the first lines, you import the `KMeans` module from the `sklearn.cluster`
    package. This module takes care of the clustering itself. You also need to import
    the NumPy library because the `KMeans` module works on NumPy arrays.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几行中，你从 `sklearn.cluster` 包中导入了 `KMeans` 模块。这个模块负责执行聚类操作。你还需要导入 NumPy 库，因为
    `KMeans` 模块是基于 NumPy 数组工作的。
- en: Our data is two-dimensional. It correlates the number of working hours with
    the salary of some workers. [Figure 4-12](#ch04fig12) shows the six data points
    in this employee data set.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据是二维的。它将工作小时数与一些工人的薪资进行了关联。[图 4-12](#ch04fig12) 显示了这个员工数据集中的六个数据点。
- en: '![images](Images/fig4-12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-12.jpg)'
- en: '*Figure 4-12: Employee salary data*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-12：员工薪资数据*'
- en: 'The goal is to find the two cluster centers that best fit this data:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到两个最适合这些数据的聚类中心：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the one-liner, you create a new `KMeans` object that handles the algorithm
    for you. When you create the `KMeans` object, you define the number of cluster
    centers by using the `n_clusters` function argument. Then you simply call the
    instance method `fit(X)` to run the K-Means algorithm on the input data `X`. The
    `KMeans` object now holds all the results. All that’s left is to retrieve the
    results from its attributes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这行代码中，你创建了一个新的 `KMeans` 对象，负责为你处理算法。当你创建 `KMeans` 对象时，通过使用 `n_clusters` 函数参数来定义聚类中心的数量。然后你只需调用实例方法
    `fit(X)` 来对输入数据 `X` 运行 K-Means 算法。此时，`KMeans` 对象将保存所有结果。剩下的就是从其属性中获取结果：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that in the `sklearn` package, the convention is to use a trailing underscore
    for some attribute names (for example, `cluster_centers_`) to indicate that these
    attributes were created dynamically within the training phase (the `fit()` function).
    Before the training phase, these attributes do not exist yet. This is not general
    Python convention (trailing underscores are usually used only to avoid naming
    conflicts with Python keywords—`variable list_` instead of `list`). However, if
    you get used to it, you appreciate the consistent use of attributes in the `sklearn`
    package. So, what are the cluster centers and what is the output of this code
    snippet? Take a look at [Figure 4-13](#ch04fig13).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 `sklearn` 包中，约定使用尾部下划线来表示一些属性名称（例如，`cluster_centers_`），以表明这些属性是在训练阶段动态生成的（即
    `fit()` 函数）。在训练阶段之前，这些属性并不存在。这并不是 Python 的通用约定（尾部下划线通常仅用于避免与 Python 关键字的命名冲突——例如，使用
    `variable list_` 而不是 `list`）。然而，如果你习惯了这种方式，你会更欣赏 `sklearn` 包中属性的一致性。那么，聚类中心是什么？这段代码片段的输出是什么？请看
    [图 4-13](#ch04fig13)。
- en: '![images](Images/fig4-13.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-13.jpg)'
- en: '*Figure 4-13: Employee salary data with cluster centers in the two-dimensional
    space*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-13：员工薪资数据和二维空间中的聚类中心*'
- en: 'You can see that the two cluster centers are (20, 2000) and (50, 7000). This
    is also the result of the Python one-liner. These clusters correspond to two idealized
    employee personas: the first works for 20 hours a week and earns $2000 per month,
    while the second works for 50 hours a week and earns $7000 per month. Those two
    types of personas fit the data reasonably well. Thus, the result of the one-liner
    code snippet is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，两个聚类中心分别是 (20, 2000) 和 (50, 7000)。这也是 Python 一行代码的结果。这些聚类对应着两种理想化的员工角色：第一个每周工作
    20 小时，月薪 2000 美元；而第二个每周工作 50 小时，月薪 7000 美元。这两种角色合理地符合了数据。因此，这段一行代码的结果如下：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To summarize, this section introduced you to an important subtopic of unsupervised
    learning: clustering. The K-Means algorithm is a simple, efficient, and popular
    way of extracting *k* clusters from multidimensional data. Behind the scenes,
    the algorithm iteratively recomputes cluster centers and reassigns each data value
    to its closest cluster center until it finds the optimal clusters. But clusters
    are not always ideal for finding similar data items. Many data sets do not show
    a clustered behavior, but you’ll still want to leverage the distance information
    for machine learning and prediction. Let’s stay in the multidimensional space
    and explore another way to use the distance of (Euclidean) data values: the K-Nearest
    Neighbors algorithm.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，本节介绍了无监督学习中的一个重要子主题：聚类。K-Means 算法是一种简单、高效且广泛使用的方法，可以从多维数据中提取 *k* 个聚类。在背后，算法会反复计算聚类中心，并将每个数据值分配给其最近的聚类中心，直到找到最优的聚类。但聚类并不总是适合找到相似的数据项。许多数据集并不表现出聚类行为，但你仍然希望利用距离信息进行机器学习和预测。让我们继续留在多维空间，探索另一种利用（欧几里得）数据值距离的方法：K-最近邻算法。
- en: '**K-Nearest Neighbors in One Line**'
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**K-最近邻算法一行代码实现**'
- en: The popular *K-Nearest Neighbors* *(KNN)* algorithm is used for regression and
    classification in many applications such as recommender systems, image classification,
    and financial data forecasting. It’s the basis of many advanced machine learning
    techniques (for example, in information retrieval). There is no doubt that understanding
    KNN is an important building block of your proficient computer science education.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的 *K-最近邻算法*（*KNN*）广泛应用于回归和分类任务，涉及推荐系统、图像分类和金融数据预测等多个领域。它是许多高级机器学习技术的基础（例如，信息检索中的应用）。毫无疑问，理解
    KNN 是你掌握计算机科学教育中重要组成部分的基石。
- en: '***The Basics***'
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: The KNN algorithm is a robust, straightforward, and popular machine learning
    method. It’s simple to implement but still a competitive and fast machine learning
    technique. All other machine learning models we’ve discussed so far use the training
    data to compute a *representation* of the original data. You can use this representation
    to predict, classify, or cluster new data. For example, the linear and logistic
    regression algorithms define learning parameters, while the clustering algorithm
    calculates cluster centers based on the training data. However, the KNN algorithm
    is different. In contrast to the other approaches, it does not compute a new model
    (or representation) but uses the *whole data set* as a model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 算法是一种稳健、直接且流行的机器学习方法。它易于实现，但仍然是一种具有竞争力且快速的机器学习技术。我们迄今讨论的所有其他机器学习模型都使用训练数据来计算原始数据的*表示*。你可以使用这种表示来预测、分类或聚类新数据。例如，线性回归和逻辑回归算法定义学习参数，而聚类算法则根据训练数据计算聚类中心。然而，KNN
    算法不同。与其他方法相比，它不会计算新的模型（或表示），而是将*整个数据集*作为模型。
- en: 'Yes, you read that right. The machine learning model is nothing more than a
    set of observations. Every single instance of your training data is one part of
    your model. This has advantages and disadvantages. A disadvantage is that the
    model can quickly blow up as the training data grows—which may require sampling
    or filtering as a preprocessing step. A great advantage, however, is the simplicity
    of the training phase (just add the new data values to the model). Additionally,
    you can use the KNN algorithm for prediction or classification. You execute the
    following strategy, given your input vector *x*:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你没看错。机器学习模型不过是一组观察结果。你的训练数据中的每一个实例都是你模型的一部分。这有优点也有缺点。一个缺点是，随着训练数据的增长，模型可能迅速膨胀——这可能需要在预处理步骤中进行采样或过滤。然而，一个很大的优点是训练阶段的简单性（只需将新数据值添加到模型中）。此外，你还可以使用
    KNN 算法进行预测或分类。给定输入向量*x*，你执行以下策略：
- en: Find the *k* nearest neighbors of *x* (according to a predefined distance metric).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到*x*的*k*个最近邻（根据预定义的距离度量）。
- en: Aggregate the *k* nearest neighbors into a single prediction or classification
    value. You can use any aggregator function such as average, mean, max, or min.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*k*个最近邻聚合成一个单一的预测或分类值。你可以使用任何聚合函数，比如平均值、均值、最大值或最小值。
- en: Let’s walk through an example. Your company sells homes for clients. It has
    acquired a large database of customers and house prices (see [Figure 4-14](#ch04fig14)).
    One day, your client asks how much they must expect to pay for a house of 52 square
    meters. You query your KNN model, and it immediately gives you the response $33,167\.
    And indeed, your client finds a home for $33,489 the same week. How did the KNN
    system come to this surprisingly accurate prediction?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明。你的公司为客户销售房屋，已收集了大量客户和房价数据库（参见[图 4-14](#ch04fig14)）。有一天，客户问你 52
    平方米的房子大概多少钱。你查询 KNN 模型，它立即给出答案 $33,167。事实上，客户在同一周就找到了一套价格为 $33,489 的房子。KNN 系统是如何做出这个惊人准确的预测的？
- en: First, the KNN system simply calculates the *k = 3* nearest neighbors to the
    query *D = 52 square meters* using Euclidean distance. The three nearest neighbors
    are A, B, and C with prices $34,000, $33,500, and $32,000, respectively. Then,
    it aggregates the three nearest neighbors by calculating the simple average of
    their values. Because *k = 3* in this example, you denote the model as *3NN*.
    Of course, you can vary the similarity functions, the parameter *k*, and the aggregation
    method to come up with more sophisticated prediction models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，KNN 系统简单地使用欧几里得距离计算查询 *D = 52 平方米* 的*k = 3* 个最近邻。三个最近邻分别是 A、B 和 C，价格分别为 $34,000、$33,500
    和 $32,000。然后，它通过计算这些值的简单平均值来聚合这三个最近邻。由于本例中 *k = 3*，你将模型表示为*3NN*。当然，你可以改变相似度函数、参数*k*以及聚合方法，以便得出更复杂的预测模型。
- en: '![images](Images/fig4-14.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-14.jpg)'
- en: '*Figure 4-14: Calculating the price of house D based on the three nearest neighbors
    A, B, and C*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-14：根据三个最近邻 A、B 和 C 计算房子 D 的价格*'
- en: Another advantage of KNN is that it can be easily adapted as new observations
    are made. This is not generally true for machine learning models. An obvious weakness
    in this regard is that as the computational complexity of finding the *k* nearest
    neighbors becomes harder and harder, the more points you add. To accommodate for
    that, you can continuously remove stale values from the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 的另一个优点是，当有新观察数据时，它可以轻松适应。这对于大多数机器学习模型来说并不常见。在这方面的明显弱点是，随着你增加更多的数据点，寻找 *k*
    个最近邻的计算复杂度会越来越高。为了适应这一点，你可以不断从模型中移除过时的值。
- en: 'As I mentioned, you can also use KNN for classification problems. Instead of
    averaging over the *k* nearest neighbors, you can use a voting mechanism: each
    nearest neighbor votes for its class, and the class with the most votes wins.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，你也可以使用 KNN 解决分类问题。你可以使用投票机制：每个最近邻对它的类别进行投票，得票最多的类别获胜，而不是对 *k* 个最近邻的平均值进行计算。
- en: '***The Code***'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: Let’s dive into how to use KNN in Python—in a single line of code (see [Listing
    4-4](#list4-4)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解如何在 Python 中使用 KNN——用一行代码来实现（见[清单 4-4](#list4-4)）。
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Listing 4-4: Running the KNN algorithm in one line of Python*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-4：在一行 Python 代码中运行 KNN 算法*'
- en: 'Take a guess: what’s the output of this code snippet?'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜看：这段代码的输出是什么？
- en: '***How It Works***'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***工作原理***'
- en: To help you see the result, let’s plot the housing data from this code in [Figure
    4-15](#ch04fig15).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你查看结果，让我们在[图 4-15](#ch04fig15)中绘制来自这段代码的住房数据。
- en: '![images](Images/fig4-15.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-15.jpg)'
- en: '*Figure 4-15: Housing data in the two-dimensional space*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-15：二维空间中的住房数据*'
- en: Can you see the general trend? With the growing size of your house, you can
    expect a linear growth of its market price. Double the square meters, and the
    price will double too.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看到一般趋势吗？随着房屋面积的增加，你可以预期它的市场价格会呈线性增长。将面积翻倍，价格也会翻倍。
- en: In the code (see [Listing 4-4](#list4-4)), the client requests your price prediction
    for a house of 30 square meters. What does KNN with *k = 3* (in short, 3NN) predict?
    Take a look at [Figure 4-16](#ch04fig16).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中（见[清单 4-4](#list4-4)），客户端请求你预测一栋 30 平方米房屋的价格。使用 *k = 3* 的 KNN（简称 3NN）预测结果如何？看看[图
    4-16](#ch04fig16)。
- en: Beautiful, isn’t it? The KNN algorithm finds the three closest houses with respect
    to house size and averages the predicted house price as the average of the *k=3*
    nearest neighbors. Thus, the result is $32,500.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 漂亮吧？KNN 算法找到三个与房屋大小最接近的房子，并将预测的房价作为 *k=3* 个最近邻的平均值。因此，结果是 $32,500。
- en: 'If you are confused by the data conversions in the one-liner, let me quickly
    explain what is happening here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这行代码中的数据转换感到困惑，让我快速解释一下这里发生了什么：
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![images](Images/fig4-16.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-16.jpg)'
- en: '*Figure 4-16: Housing data in the two-dimensional space with predicted house
    price for a new data point (house size equals 30 square meters) using KNN*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-16：二维空间中的住房数据，使用 KNN 预测新数据点（房屋面积为 30 平方米）的房价*'
- en: First, you create a new machine learning model called `KNeighborsRegressor`.
    If you wanted to use KNN for classification, you’d use `KNeighborsClassifier`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你创建一个新的机器学习模型，叫做 `KNeighborsRegressor`。如果你想用 KNN 解决分类问题，则使用 `KNeighborsClassifier`。
- en: 'Second, you train the model by using the `fit()` function with two parameters.
    The first parameter defines the input (the house size), and the second parameter
    defines the output (the house price). The shape of both parameters must be an
    array-like data structure. For example, to use `30` as an input, you’d have to
    pass it as `[30]`. The reason is that, in general, the input can be multidimensional
    rather than one-dimensional. Therefore, you reshape the input:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，你通过使用 `fit()` 函数并传入两个参数来训练模型。第一个参数定义输入（房屋大小），第二个参数定义输出（房屋价格）。这两个参数的形状必须是类似数组的数据结构。例如，要使用
    `30` 作为输入，你必须将其传递为 `[30]`。原因是，通常情况下，输入可以是多维的，而不仅仅是一维的。因此，你需要重塑输入：
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that if you were to use this 1D NumPy array as an input to the `fit()`
    function, the function wouldn’t work because it expects an array of (array-like)
    observations, and not an array of integers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你将这个 1D 的 NumPy 数组作为 `fit()` 函数的输入，函数将无法工作，因为它期望的是一个类似数组的观察数据结构，而不是一个整数数组。
- en: 'In summary, this one-liner taught you how to create your first KNN regressor
    in a single line of code. If you have a lot of changing data and model updates,
    KNN is your best friend! Let’s move on to a wildly popular machine learning model
    these days: neural networks.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这行代码教你如何在一行代码中创建第一个KNN回归器。如果你有大量变化的数据和模型更新，KNN是你最好的朋友！接下来，我们将讨论目前非常流行的一个机器学习模型：神经网络。
- en: '**Neural Network Analysis in One Line**'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**一行代码中的神经网络分析**'
- en: Neural networks have gained massive popularity in recent years. This is in part
    because the algorithms and learning techniques in the field have improved, but
    also because of the improved hardware and the rise of general-purpose GPU (GPGPU)
    technology. In this section, you’ll learn about the *multilayer perceptron* *(MLP)*
    which is one of the most popular neural network representations. After reading
    this, you’ll be able to write your own neural network in a single line of Python
    code!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在近年来获得了广泛的流行。这部分归功于该领域的算法和学习技术的改进，但也得益于硬件的提升和通用GPU（GPGPU）技术的兴起。在这一部分，你将学习到*多层感知器*（*MLP*），它是最流行的神经网络表示之一。阅读完这部分后，你将能够用一行Python代码编写自己的神经网络！
- en: '***The Basics***'
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: For this one-liner, I have prepared a special data set with fellow Python colleagues
    on my email list. My goal was to create a relatable real-world data set, so I
    asked my email subscribers to participate in a data-generation experiment for
    this chapter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一行代码，我准备了一个与我的Python同事在电子邮件列表上共同参与的数据集。我的目标是创建一个有相关性的真实世界数据集，所以我请我的电子邮件订阅者参与了这章的数据生成实验。
- en: '**The Data**'
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**数据**'
- en: If you’re reading this book, you’re interested in learning Python. To create
    an interesting data set, I asked my email subscribers six anonymized questions
    about their Python expertise and income. The responses to these questions will
    serve as training data for the simple neural network example (as a Python one-liner).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在阅读这本书，你一定有学习Python的兴趣。为了创建一个有趣的数据集，我请我的电子邮件订阅者就他们的Python专业知识和收入回答了六个匿名问题。这些问题的回答将作为简单神经网络示例的训练数据（作为Python一行代码）。
- en: 'The training data is based on the answers to the following six questions:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据基于以下六个问题的答案：
- en: How many hours have you looked at Python code in the last seven days?
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去七天里，你一共看了多少小时的Python代码？
- en: How many years ago did you start to learn about computer science?
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你从几年前开始学习计算机科学的？
- en: How many coding books are on your shelf?
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你书架上有多少本编程书？
- en: What percentage of your Python time do you spend working on real-world projects?
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在Python时间中有多少百分比是用于处理真实世界的项目？
- en: How much do you earn per month (round to $1000) from selling your technical
    skills (in the widest sense)?
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你每月通过销售技术技能（广义上的技术）赚取多少收入（四舍五入到1000美元）？
- en: What’s your approximate Finxter rating, rounded to 100 points?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你大约的Finxter评分是多少，四舍五入到100分？
- en: The first five questions will be your input, and the sixth question will be
    the output for the neural network analysis. In this one-liner section, you’re
    examining neural network regression. In other words, you predict a numerical value
    (your Python skills) based on numerical input features. We’re not going to explore
    neural network classification in this book, which is another great strength of
    neural networks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 前五个问题将作为你的输入，第六个问题将作为神经网络分析的输出。在这一行代码部分，你将分析神经网络回归。换句话说，你根据数字输入特征预测一个数值（你的Python技能）。本书中我们不会探讨神经网络分类，这是神经网络的另一个重要优势。
- en: The sixth question approximates the skill level of a Python coder. Finxter ([*https://finxter.com/*](https://finxter.com/))
    is our puzzle-based learning application that assigns a rating value to any Python
    coder based on their performance in solving Python puzzles. In this way, it helps
    you quantify your skill level in Python.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 第六个问题大致反映了Python编程者的技能水平。Finxter（[*https://finxter.com/*](https://finxter.com/)）是我们的基于谜题的学习应用，它根据Python编程者在解决Python谜题中的表现，分配一个评分值给每个Python编码者。通过这种方式，它帮助你量化自己的Python技能水平。
- en: Let’s start with visualizing how each question influences the output (the skill
    rating of a Python developer), as shown in [Figure 4-17](#ch04fig17).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从可视化每个问题如何影响输出（即Python开发者的技能评分）开始，如[图 4-17](#ch04fig17)所示。
- en: '![images](Images/fig4-17.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-17.jpg)'
- en: '*Figure 4-17: Relationship between questionnaire answers and the Python skill
    rating at Finxter*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-17：问卷答案与Python技能评分之间的关系*'
- en: Note that these plots show only how each separate feature (question) impacts
    the final Finxter rating, but they tell you nothing about the impact of a combination
    of two or more features. Note also that some Pythonistas didn’t answer all six
    questions; in those cases, I used the dummy value `-1`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些图表仅展示了每个单独特征（问题）如何影响最终的 Finxter 评分，但它们并没有告诉你两个或更多特征组合的影响。此外，还要注意，一些 Python
    爱好者并没有回答所有六个问题；在这些情况下，我使用了虚拟值 `-1`。
- en: '**What Is an Artificial Neural Network?**'
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**什么是人工神经网络？**'
- en: The idea of creating a theoretical model of the human brain (the biological
    neural network) has been studied extensively in recent decades. But the foundations
    of artificial neural networks were proposed as early as the 1940s and ’50s! Since
    then, the concept of artificial neural networks has been refined and continually
    improved.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 创建人脑（生物神经网络）理论模型的想法在近几十年得到了广泛研究。但人工神经网络的基础理论早在 1940 年代和 1950 年代就已经提出！从那时起，人工神经网络的概念不断被改进和完善。
- en: The basic idea is to break the big task of learning and inference into multiple
    micro-tasks. These micro-tasks are not independent but interdependent. The brain
    consists of billions of neurons that are connected with trillions of synapses.
    In the simplified model, learning is merely adjusting the *strength* of synapses
    (also called *weights* or *parameters* in artificial neural networks). So how
    do you “create” a new synapse in the model? Simple—you increase its weight from
    zero to a nonzero value.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路是将学习和推理的大任务分解为多个微任务。这些微任务不是独立的，而是相互依赖的。大脑由数十亿个神经元组成，这些神经元通过数万亿个突触连接。在简化模型中，学习仅仅是调整突触的*强度*（在人工神经网络中也称为*权重*或*参数*）。那么，如何在模型中“创建”一个新的突触呢？很简单——你将其权重从零增加到非零值。
- en: '[Figure 4-18](#ch04fig18) shows a basic neural network with three layers (input,
    hidden, output). Each layer consists of multiple neurons that are connected from
    the input layer via the hidden layer to the output layer.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-18](#ch04fig18)展示了一个基本的神经网络，包含三个层次（输入层、隐藏层、输出层）。每一层由多个神经元组成，神经元从输入层通过隐藏层连接到输出层。'
- en: '![images](Images/fig4-18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-18.jpg)'
- en: '*Figure 4-18: A simple neural network analysis for animal classification*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-18：一种用于动物分类的简单神经网络分析*'
- en: In this example, the neural network is trained to detect animals in images.
    In practice, you would use one input neuron per pixel of the image as an input
    layer. This can result in millions of input neurons that are connected with millions
    of hidden neurons. Often, each output neuron is responsible for one bit of the
    overall output. For example, to detect two different animals (for example, cats
    and dogs), you’ll use only a single neuron in the output layer that can model
    two different states (`0=cat`, `1=dog`).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，神经网络被训练用来检测图像中的动物。实际上，你会为图像的每个像素使用一个输入神经元作为输入层。这可能导致数百万个输入神经元与数百万个隐藏神经元相连接。通常，每个输出神经元负责整个输出的一位。例如，要检测两种不同的动物（例如，猫和狗），你只会在输出层使用一个神经元来表示两种不同的状态（`0=猫`，`1=狗`）。
- en: The idea is that each neuron can be activated, or “fired”, when a certain input
    impulse arrives at the neuron. Each neuron decides independently, based on the
    strength of the input impulse, whether to fire or not. This way, you simulate
    the human brain, in which neurons activate each other via impulses. The activation
    of the input neurons propagates through the network until the output neurons are
    reached. Some output neurons will be activated, and others won’t. The specific
    pattern of firing output neurons forms your final output (or prediction) of the
    artificial neural network. In your model, a firing output neuron could encode
    a 1, and a nonfiring output neuron could encode a 0\. This way, you can train
    your neural network to predict anything that can be encoded as a series of 0s
    and 1s (which is everything a computer can represent).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思想是，每个神经元在接收到某个输入脉冲时可以被激活，或者“发射”。每个神经元根据输入脉冲的强度独立决定是否发射。通过这种方式，你模拟了人脑中神经元通过脉冲相互激活的过程。输入神经元的激活通过网络传播，直到到达输出神经元。一些输出神经元会被激活，而其他则不会。输出神经元的发射模式形成了你人工神经网络的最终输出（或预测）。在你的模型中，激活的输出神经元可以表示
    1，而未激活的输出神经元可以表示 0。通过这种方式，你可以训练你的神经网络来预测任何可以用一系列 0 和 1 表示的事物（即计算机能表示的任何事物）。
- en: Let’s have a detailed look at how neurons work mathematically, in [Figure 4-19](#ch04fig19).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解神经元如何在数学上工作，参见[图 4-19](#ch04fig19)。
- en: '![images](Images/fig4-19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-19.jpg)'
- en: '*Figure 4-19: Mathematical model of a single neuron: the output is a function
    of the three inputs.*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-19：单个神经元的数学模型：输出是三个输入的函数。*'
- en: Each neuron is connected to other neurons, but not all connections are equal.
    Instead, each connection has an associated weight. Formally, a firing neuron propagates
    an impulse of 1 to the outgoing neighbors, while a nonfiring neuron propagates
    an impulse of 0\. You can think of the weight as indicating how much of the impulse
    of the firing input neuron is forwarded to the neuron via the connection. Mathematically,
    you multiply the impulse by the weight of the connection to calculate the input
    for the next neuron. In our example, the neuron simply sums over all inputs to
    calculate its own output. This is the *activation function* that describes how
    exactly the inputs of a neuron generate an output. In our example, a neuron fires
    with higher likelihood if its relevant input neurons fire too. This is how the
    impulses propagate through the neural network.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元都与其他神经元相连，但并非所有连接都相等。相反，每个连接都有一个相关的权重。严格来说，一个激活的神经元会向外部邻居传播一个值为1的脉冲，而一个非激活的神经元则传播一个值为0的脉冲。你可以将权重视为表示激活输入神经元的脉冲通过连接传递给目标神经元的程度。数学上，你将脉冲乘以连接的权重来计算下一个神经元的输入。在我们的示例中，神经元简单地对所有输入进行求和来计算其自身的输出。这就是*激活函数*，它描述了神经元的输入如何生成输出。在我们的示例中，如果相关的输入神经元也被激活，那么神经元激活的可能性就会更高。这就是脉冲如何在神经网络中传播的方式。
- en: What does the learning algorithm do? It uses the training data to select the
    weights *w* of the neural network. Given a training input value *x*, different
    weights *w* lead to different outputs. Hence, the learning algorithm gradually
    changes the weights *w*—in many iterations—until the output layer produces similar
    results as the training data. In other words, the training algorithm gradually
    reduces the error of correctly predicting the training data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法是做什么的？它使用训练数据来选择神经网络的权重 *w*。给定一个训练输入值 *x*，不同的权重 *w* 会导致不同的输出。因此，学习算法会逐步调整权重
    *w*——在多次迭代中——直到输出层产生与训练数据相似的结果。换句话说，训练算法逐渐减少正确预测训练数据的误差。
- en: There are many network structures, training algorithms, and activation functions.
    This chapter shows you a hands-on approach of using the neural network now, within
    a single line of code. You can then learn the finer details as you need to improve
    upon this (for example, you could start by reading the “Neural Network” entry
    on Wikipedia, [*https://en.wikipedia.org/wiki/Neural_network*](https://en.wikipedia.org/wiki/Neural_network)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构、训练算法和激活函数有很多种。本章将向你展示如何在一行代码中使用神经网络的实用方法。然后，你可以根据需要深入学习更详细的内容（例如，你可以从阅读维基百科上的“神经网络”条目开始，[*https://en.wikipedia.org/wiki/Neural_network*](https://en.wikipedia.org/wiki/Neural_network)）。
- en: '***The Code***'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: 'The goal is to create a neural network that predicts the Python skill level
    (Finxter rating) by using the five input features (answers to the questions):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建一个神经网络，通过使用五个输入特征（对问题的回答）来预测 Python 技能水平（Finxter 评分）：
- en: '**`WEEK`** How many hours have you been exposed to Python code in the last
    seven days?'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**`周`** 在过去七天里，你接触 Python 代码的小时数是多少？'
- en: '**`YEARS`** How many years ago did you start to learn about computer science?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**`年`** 你是什么时候开始学习计算机科学的？'
- en: '**`BOOKS`** How many coding books are on your shelf?'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**`书籍`** 你的书架上有多少本编程书？'
- en: '**`PROJECTS`** What percentage of your Python time do you spend implementing
    real-world projects?'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**`项目`** 你花费多少百分比的 Python 时间来实现现实世界的项目？'
- en: '**`EARN`** How much do you earn per month (round to $1000) from selling your
    technical skills (in the widest sense)?'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**`收入`** 你每月通过销售你的技术技能（广义上讲）赚多少钱（以 $1000 为单位）？'
- en: Again, let’s stand on the shoulders of giants and use the scikit-learn (`sklearn`)
    library for neural network regression, as in [Listing 4-5](#list4-5).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 再次让我们站在巨人的肩膀上，使用 scikit-learn (`sklearn`) 库进行神经网络回归，就像在[清单 4-5](#list4-5)中一样。
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 4-5: Neural network analysis in a single line of code*'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-5：用一行代码进行神经网络分析*'
- en: It’s impossible for a human to correctly figure out the output—but would you
    like to try?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是不可能正确地计算出输出的——但你想试试吗？
- en: '***How It Works***'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***工作原理***'
- en: In the first few lines, you create the data set. The machine learning algorithms
    in the scikit-learn library use a similar input format. Each row is a single observation
    with multiple features. The more rows, the more training data exists; the more
    columns, the more features of each observation. In this case, you have five features
    for the input and one feature for the output value of each training data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几行，你创建了数据集。scikit-learn库中的机器学习算法使用类似的输入格式。每一行是一个单独的观测值，包含多个特征。行数越多，训练数据就越多；列数越多，每个观测值的特征就越多。在这种情况下，你的输入有五个特征，每个训练数据的输出只有一个特征。
- en: The one-liner creates a neural network by using the constructor of the `MLPRegressor`
    class. I passed `max_iter=10000` as an argument because the training doesn’t converge
    when using the default number of iterations (`max_iter=200`).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个一行代码通过使用`MLPRegressor`类的构造函数创建了一个神经网络。我传递了`max_iter=10000`作为参数，因为在使用默认的迭代次数（`max_iter=200`）时，训练无法收敛。
- en: After that, you call the `fit()` function, which determines the parameters of
    the neural network. After calling `fit()`, the neural network has been successfully
    initialized. The `fit()` function takes a multidimensional input array (one observation
    per row, one feature per column) and a one-dimensional output array (size = number
    of observations).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你调用`fit()`函数，它决定了神经网络的参数。调用`fit()`后，神经网络已经成功初始化。`fit()`函数接收一个多维输入数组（每行一个观测值，每列一个特征）和一个一维输出数组（大小
    = 观测值的数量）。
- en: 'The only thing left is calling the predict function on some input values:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是在一些输入值上调用预测函数：
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the actual output may vary slightly because of the nondeterministic
    nature of the function and the different convergence behavior.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于该函数的非确定性特性以及不同的收敛行为，实际输出可能会略有不同。
- en: 'In plain English: if . . .'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的英语来说：如果 . . .
- en: . . . you have trained 0 hours in the last week,
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . . 你在过去一周没有进行任何训练，
- en: . . . you started your computer science studies 0 years ago,
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . . 你从零年前开始学习计算机科学，
- en: . . . you have 0 coding books in your shelf,
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . . 你书架上没有任何Python编程书籍，
- en: . . . you spend 0 percent of your time implementing real Python projects, and
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . . 你把0%的时间用来实现真正的Python项目，并且
- en: . . . you earn $0 selling your coding skills,
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . . 你通过出售编程技能赚取了$0，
- en: the neural network estimates that your skill level is *very* low (a Finxter
    rating of 94 means you have difficulty understanding the Python program `print("hello,
    world")`).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络估计你的技能水平是*非常*低（Finxter评分94意味着你在理解Python程序`print("hello, world")`时有困难）。
- en: 'So let’s change this: what happens if you invest 20 hours a week learning and
    revisit the neural network after one week:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们来改变这一点：如果你每周投入20小时进行学习，并在一周后重新审视神经网络，会发生什么呢：
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Not bad—your skills improve quite significantly! But you’re still not happy
    with this rating number, are you? (An above-average Python coder has at least
    a 1500–1700 rating on Finxter.)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 不错——你的技能进步了不少！但你对这个评分还是不满意吧？（一个高于平均水平的Python程序员在Finxter上的评分至少是1500–1700。）
- en: 'No problem. Buy 10 Python books (only nine left after this one). Let’s see
    what happens to your rating:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 没问题。买10本Python书（这本买完后就剩下9本）。让我们看看你的评分会发生什么变化：
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, you make significant progress and double your rating number! But buying
    Python books alone will not help you much. You need to study them! Let’s do this
    for a year:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你取得了显著进展，并且将你的评分翻倍！但仅仅买Python书籍并不会帮到你太多。你需要学习它们！我们来做一年：
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Not much happens. This is where I don’t trust the neural network too much.
    In my opinion, you should have reached a much better performance of at least 1500\.
    But this also shows that the neural network can be only as good as its training
    data. You have very limited data, and the neural network can’t really overcome
    this limitation: there’s just too little knowledge in a handful of data points.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 并没有发生太多事情。这是我对神经网络不太信任的地方。在我看来，你本应该达到更好的表现，至少1500的评分。但这也说明了，神经网络的表现仅能依赖于其训练数据。你有的数据非常有限，神经网络无法突破这个限制：数据点太少，知识量太少。
- en: 'But you don’t give up, right? Next, you spend 50 percent of your Python time
    selling your skills as a Python freelancer:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 但你不会放弃，对吧？接下来，你将50%的Python时间用来作为Python自由职业者出售你的技能：
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Boom! Suddenly the neural network considers you to be an expert Python coder.
    A wise prediction of the neural network, indeed! Learn Python for at least a year
    and do practical projects, and you’ll become a great coder.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！突然间，神经网络认为你是一个专家级的 Python 编程者。的确，神经网络做出了明智的预测！学习 Python 至少一年并做一些实际项目，你就会成为一个优秀的程序员。
- en: To sum up, you’ve learned about the basics of neural networks and how to use
    them in a single line of Python code. Interestingly, the questionnaire data indicates
    that starting out with practical projects—maybe even doing freelance projects
    from the beginning—matters a lot to your learning success. The neural network
    certainly knows that. If you want to learn my exact strategy of becoming a freelancer,
    join the free webinar about state-of-the-art Python freelancing at [*https://blog.finxter.com/webinar-freelancer/*](https://blog.finxter.com/webinar-freelancer/).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，你已经了解了神经网络的基础知识，以及如何通过一行 Python 代码使用它们。有趣的是，问卷调查数据显示，开始时做一些实际项目——甚至从一开始就做自由职业项目——对你的学习成功非常重要。神经网络显然知道这一点。如果你想了解我成为自由职业者的确切策略，可以参加[*https://blog.finxter.com/webinar-freelancer/*](https://blog.finxter.com/webinar-freelancer/)的免费网络研讨会。
- en: 'In the next section, you’ll dive deeper into another powerful model representation:
    decision trees. While neural networks can be quite expensive to train (they often
    need multiple machines and many hours, and sometimes even weeks, to train), decision
    trees are lightweight. Nevertheless, they are a fast, effective way to extract
    patterns from your training data.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将更深入地了解另一种强大的模型表示方法：决策树。虽然神经网络的训练可能非常昂贵（它们通常需要多台机器和很多小时，有时甚至几周的时间来训练），但决策树则相对轻量。然而，它们是从训练数据中提取模式的快速而有效的方式。
- en: '**Decision-Tree Learning in One Line**'
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**一行代码实现决策树学习**'
- en: '*Decision trees* are powerful and intuitive tools in your machine learning
    toolbelt. A big advantage of decision trees is that, unlike many other machine
    learning techniques, they’re human-readable. You can easily train a decision tree
    and show it to your supervisors, who do not need to know anything about machine
    learning in order to understand what your model does. This is especially great
    for data scientists who often must defend and present their results to management.
    In this section, I’ll show you how to use decision trees in a single line of Python
    code.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是机器学习工具箱中强大而直观的工具。决策树的一个大优点是，与许多其他机器学习技术不同，它们是人类可读的。你可以轻松地训练一个决策树并展示给你的主管，他们无需了解机器学习的任何内容就能理解你的模型是如何工作的。这对于经常需要为自己的结果向管理层辩护和展示的资料科学家来说尤为重要。在本节中，我将向你展示如何通过一行
    Python 代码使用决策树。'
- en: '***The Basics***'
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: Unlike many machine learning algorithms, the ideas behind decision trees might
    be familiar from your own experience. They represent a structured way of making
    decisions. Each decision opens new branches. By answering a bunch of questions,
    you’ll finally land on the recommended outcome. [Figure 4-20](#ch04fig20) shows
    an example.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多机器学习算法不同，决策树背后的思想可能源于你自己的经验。它们代表了一种有结构的决策方式。每一个决策都会打开新的分支。通过回答一系列问题，最终你会得出推荐的结果。[图
    4-20](#ch04fig20)展示了一个例子。
- en: '![images](Images/fig4-20.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-20.jpg)'
- en: '*Figure 4-20: A simplified decision tree for recommending a study subject*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-20：一个简化的决策树，用于推荐学习科目*'
- en: Decision trees are used for classification problems such as “which subject should
    I study, given my interests?” You start at the top. Now, you repeatedly answer
    questions and select the choices that describe your features best. Finally, you
    reach a *leaf node* of the tree, a node with no *children*. This is the recommended
    class based on your feature selection.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树用于分类问题，比如“根据我的兴趣，我应该学习哪个科目？”你从顶部开始，然后反复回答问题并选择最能描述你特征的选项。最终，你会到达树的*叶节点*，一个没有*子节点*的节点。这就是基于你选择的特征推荐的类别。
- en: Decision-tree learning has many nuances. In the preceding example, the first
    question carries more weight than the last question. If you like math, the decision
    tree will never recommend art or linguistics. This is useful because some features
    may be much more important for the classification decision than others. For example,
    a classification system that predicts your current health may use your sex (feature)
    to practically rule out many diseases (classes).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习有许多微妙之处。在前面的示例中，第一个问题比最后一个问题更重要。如果您喜欢数学，决策树将永远不会推荐艺术或语言学。这很有用，因为某些特征对分类决策可能比其他特征重要得多。例如，一个预测您当前健康状况的分类系统可能使用您的性别（特征）来实际上排除许多疾病（类别）。
- en: 'Hence, the order of the decision nodes lends itself to performance optimizations:
    place the features at the top that have a high impact on the final classification.
    In decision-tree learning, you’ll then aggregate the questions with little impact
    on the final classification, as shown in [Figure 4-21](#ch04fig21).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策节点的顺序有助于性能优化：将对最终分类影响较大的特征置于顶部。在决策树学习中，您将聚合对最终分类影响较小的问题，如[图4-21](#ch04fig21)所示。
- en: '![images](Images/fig4-21.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/fig4-21.jpg)'
- en: '*Figure 4-21: Pruning improves efficiency of decision-tree learning.*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-21：修剪提升了决策树学习的效率。*'
- en: Suppose the full decision tree looks like the tree on the left. For any combination
    of features, there’s a separate classification outcome (the tree leaves). However,
    some features may not give you any additional information with respect to the
    classification problem (for example, the first Language decision node in the example).
    Decision-tree learning would effectively get rid of these nodes for efficiency
    reasons, a process called *pruning*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 假设完整的决策树看起来像左侧的树。对于任何特征组合，都有单独的分类结果（树叶）。然而，某些特征可能不会为分类问题提供任何额外信息（例如，示例中的第一个语言决策节点）。出于效率原因，决策树学习会有效地去除这些节点，这个过程称为*修剪*。
- en: '***The Code***'
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: You can create your own decision tree in a single line of Python code. [Listing
    4-6](#list4-6) shows you how.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用一行Python代码创建自己的决策树。[清单4-6](#list4-6)展示了具体操作方法。
- en: '[PRE24]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*Listing 4-6: Decision-tree classification in a single line of code*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单4-6：单行代码中的决策树分类*'
- en: Guess the output of this code snippet!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 猜测这段代码的输出！
- en: '***How It Works***'
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***工作原理***'
- en: The data in this code describes three students with their estimated skill levels
    (a score from 1–10) in the three areas of math, language, and creativity. You
    also know the study subjects of these students. For example, the first student
    is highly skilled in math and studies computer science. The second student is
    skilled in language much more than in the other two skills and studies linguistics.
    The third student is skilled in creativity and studies art.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码中的数据描述了三名学生在数学、语言和创造力三个领域的估计技能水平（从1到10的评分）。您还知道这些学生的研究课题。例如，第一个学生在数学方面技能高超，学习计算机科学。第二个学生在语言方面的技能远远超过其他两个技能，学习语言学。第三个学生在创造力方面技能高超，学习艺术。
- en: 'The one-liner creates a new decision-tree object and trains the model by using
    the `fit()` function on the labeled training data (the last column is the label).
    Internally, it creates three nodes, one for each feature: math, language, and
    creativity. When predicting the class of `student_0` (math = 8, language = 6,
    creativity = 5), the decision tree returns `computer science`. It has learned
    that this feature pattern (high, medium, medium) is an indicator of the first
    class. On the other hand, when asked for (3, 7, 9), the decision tree predicts
    `art` because it has learned that the score (low, medium, high) hints to the third
    class.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一行代码创建一个新的决策树对象，并使用`fit()`函数在标记的训练数据上训练模型（最后一列是标签）。在内部，它创建三个节点，分别为数学、语言和创造力特征。当预测`student_0`的类别（数学
    = 8，语言 = 6，创造力 = 5）时，决策树返回`计算机科学`。它学习到这种特征模式（高、中、中）是第一类的指标。另一方面，当要求（3, 7, 9）时，决策树预测`艺术`，因为它学习到分数（低、中、高）暗示第三类。
- en: Note that the algorithm is nondeterministic. In other words, when executing
    the same code twice, different results may arise. This is common for machine learning
    algorithms that work with random generators. In this case, the order of the features
    is randomly organized, so the final decision tree may have a different order of
    the features.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该算法是非确定性的。换句话说，当两次执行相同的代码时，可能会得到不同的结果。这在使用随机生成器的机器学习算法中是常见的情况。在这种情况下，特征的顺序是随机组织的，因此最终的决策树可能会有不同的特征顺序。
- en: 'To summarize, decision trees are an intuitive way of creating human-readable
    machine learning models. Every branch represents a choice based on a single feature
    of a new sample. The leaves of the tree represent the final prediction (classification
    or regression). Next, we’ll leave concrete machine learning algorithms for a moment
    and explore a critical concept in machine learning: variance.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，决策树是创建可供人类阅读的机器学习模型的一种直观方式。每个分支表示基于新样本的某个特征作出的选择。树的叶子表示最终的预测（分类或回归）。接下来，我们将暂时离开具体的机器学习算法，探讨机器学习中的一个关键概念：方差。
- en: '**Get Row with Minimal Variance in One Line**'
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**在一行中获取最小方差的行**'
- en: 'You may have read about the Vs in Big Data: volume, velocity, variety, veracity,
    and value. *Variance* is yet another important V: it measures the expected (squared)
    deviation of the data from its mean. In practice, variance is an important measure
    with relevant application domains in financial services, weather forecasting,
    and image processing.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过大数据中的Vs：体量（volume）、速度（velocity）、多样性（variety）、真实性（veracity）和价值（value）。*方差*是另一个重要的V：它衡量数据从其均值的期望（平方）偏差。实际上，方差是一个重要的度量，在金融服务、天气预测和图像处理等领域有着相关的应用。
- en: '***The Basics***'
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: Variance measures how much data spreads around its average in the one-dimensional
    or multidimensional space. You’ll see a graphical example in a moment. In fact,
    variance is one of the most important properties in machine learning. It captures
    the patterns of the data in a generalized manner—and machine learning is all about
    pattern recognition.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 方差衡量数据在一维或多维空间中围绕其平均值的分布情况。稍后你会看到一个图形示例。事实上，方差是机器学习中最重要的特性之一。它以概括的方式捕捉数据的模式——而机器学习的核心就是模式识别。
- en: 'Many machine learning algorithms rely on variance in one form or another. For
    instance, the *bias-variance trade-off* is a well-known problem in machine learning:
    sophisticated machine learning models risk overfitting the data (high variance)
    but represent the training data very accurately (low bias). On the other hand,
    simple models often generalize well (low variance) but do not represent the data
    accurately (high bias).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法依赖于方差的某种形式。例如，*偏差-方差权衡*是机器学习中的一个著名问题：复杂的机器学习模型可能会导致数据过拟合（高方差），但能够非常准确地表示训练数据（低偏差）。另一方面，简单模型通常能很好地进行泛化（低方差），但无法准确表示数据（高偏差）。
- en: 'So what exactly is variance? It’s a simple statistical property that captures
    how much the data set spreads from its mean. [Figure 4-22](#ch04fig22) shows an
    example plotting two data sets: one with low variance, and one with high variance.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，方差究竟是什么呢？它是一个简单的统计特性，衡量数据集从其均值的扩展程度。[图 4-22](#ch04fig22)展示了一个例子，绘制了两个数据集：一个具有低方差，另一个具有高方差。
- en: '![images](Images/fig4-22.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-22.jpg)'
- en: '*Figure 4-22: Variance comparison of two company stock prices*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-22：两家公司股票价格的方差比较*'
- en: This example shows the stock prices of two companies. The stock price of the
    tech startup fluctuates heavily around its average. The stock price of the food
    company is quite stable and fluctuates only in minor ways around the average.
    In other words, the tech startup has high variance, and the food company has low
    variance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了两家公司的股票价格。科技初创公司的股票价格围绕其平均值大幅波动。食品公司的股票价格则非常稳定，围绕平均值的波动很小。换句话说，科技初创公司具有较高的方差，而食品公司具有较低的方差。
- en: 'In mathematical terms, you can calculate the variance *var(X)* of a set of
    numerical values *X* by using the following formula:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学术语中，你可以通过以下公式计算一组数值*X*的方差*var(X)*：
- en: '![images](Images/pg114_equ-01.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/pg114_equ-01.jpg)'
- en: The value ![images](Images/eqn4-5.jpg) is the average value of the data in *X*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 该值![images](Images/eqn4-5.jpg)是*X*数据的平均值。
- en: '***The Code***'
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: As they get older, many investors want to reduce the overall risk of their investment
    portfolio. According to the dominant investment philosophy, you should consider
    stocks with lower variance as less-risky investment vehicles. Roughly speaking,
    you can lose less money investing in a stable, predictable, and large company
    than in a small tech startup.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 随着年龄的增长，许多投资者希望降低其投资组合的整体风险。根据主流的投资哲学，应该将方差较小的股票视为风险较低的投资工具。粗略来说，投资于稳定、可预测的大公司要比投资小型科技创业公司损失更少。
- en: The goal of the one-liner in [Listing 4-7](#list4-7) is to identify the stock
    in your portfolio with minimal variance. By investing more money into this stock,
    you can expect a lower overall variance of your portfolio.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 4-7](#list4-7) 中的单行代码的目标是找出投资组合中方差最小的股票。通过向该股票投资更多资金，你可以期望你的投资组合的整体方差会更低。'
- en: '[PRE25]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Listing 4-7: Calculating minimum variance in a single line of code*'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 4-7：在一行代码中计算最小方差*'
- en: What’s the output of this code snippet?
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出是什么？
- en: '***How It Works***'
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***它是如何工作的***'
- en: As usual, you first define the data you want to run the one-liner on (see the
    top of [Listing 4-7](#list4-7)). The NumPy array `X` contains five rows (one row
    per stock in your portfolio) with four values per row (stock prices).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，你首先定义你想要在其上运行单行代码的数据（见 [Listing 4-7](#list4-7) 顶部）。NumPy 数组 `X` 包含五行（每行代表投资组合中的一只股票），每行有四个值（股票价格）。
- en: The goal is to find the ID and variance of the stock with minimal variance.
    Hence, the outermost function of the one-liner is the `min()` function. You execute
    the `min()` function on a sequence of tuples `(a,b)`, where the first tuple value
    `a` is the row index (stock index), and the second tuple value `b` is the variance
    of the row.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到方差最小的股票的 ID 和方差。因此，单行代码的最外层函数是 `min()` 函数。你在元组序列 `(a,b)` 上执行 `min()` 函数，其中第一个元组值
    `a` 是行索引（股票索引），第二个元组值 `b` 是该行的方差。
- en: 'You may ask: what’s the minimal value of a sequence of tuples? Of course, you
    need to properly define this operation before using it. To this end, you use the
    `key` argument of the `min()` function. The `key` argument takes a function that
    returns a comparable object value, given a sequence value. Again, our sequence
    values are tuples, and you need to find the tuple with minimal variance (the second
    tuple value). Because variance is the second value, you’ll return `x[1]` as the
    basis for comparison. In other words, the tuple with the minimal second tuple
    value wins.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问：一序列元组的最小值是什么？当然，在使用之前，你需要正确定义此操作。为此，你使用 `min()` 函数的 `key` 参数。`key` 参数接受一个函数，该函数根据序列值返回一个可比较的对象值。再次强调，我们的序列值是元组，你需要找到方差最小的元组（即第二个元组值）。由于方差是第二个值，因此你将返回
    `x[1]` 作为比较的基础。换句话说，方差最小的第二个元组值所在的元组获胜。
- en: Let’s look at how to create the sequence of tuple values. You use list comprehension
    to create a tuple for any row index (stock). The first tuple element is simply
    the index of row *i*. The second tuple element is the variance of this row. You
    use the NumPy `var()` function in combination with slicing to calculate the row
    variance.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何创建元组值的序列。你可以使用列表推导式为每个行索引（股票）创建一个元组。第一个元组元素只是行索引 *i*。第二个元组元素是这一行的方差。你使用
    NumPy 的 `var()` 函数结合切片来计算行的方差。
- en: 'The result of the one-liner is, therefore, as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单行代码的结果如下：
- en: '[PRE26]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'I’d like to add that there’s an alternative way of solving this problem. If
    this wasn’t a book about Python one-liners, I would prefer the following solution
    instead of the one-liner:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我想补充一下，这个问题还有一种替代的解决方法。如果这不是一本关于 Python 单行代码的书，我更倾向于以下的解决方案，而不是单行代码：
- en: '[PRE27]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the first line, you calculate the variance of the NumPy array `X` along the
    columns (`axis=1`). In the second line, you create the tuple. The first tuple
    value is the index of the minimum in the variance array. The second tuple value
    is the minimum in the variance array. Note that multiple rows may have the same
    (minimal) variance.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，你计算 NumPy 数组 `X` 沿列（`axis=1`）的方差。在第二行，你创建元组。第一个元组值是方差数组中的最小值索引。第二个元组值是方差数组中的最小值。注意，可能有多个行具有相同的（最小）方差。
- en: This solution is more readable. So clearly, there is a trade-off between conciseness
    and readability. Just because you can cram everything into a single line of code
    doesn’t mean you should. All things being equal, it’s much better to write concise
    *and* readable code, instead of blowing up your code with unnecessary definitions,
    comments, or intermediate steps.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案更具可读性。所以很明显，简洁性和可读性之间存在权衡。仅仅因为你可以把所有内容挤进一行代码并不意味着你应该这么做。在所有条件相同的情况下，编写简洁*且*可读的代码远比将代码搞得冗长、包含不必要的定义、注释或中间步骤要好。
- en: After learning the basics of variance in this section, you’re now ready to absorb
    how to calculate basic statistics.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节学习了方差的基础知识后，你现在准备好吸收如何计算基础统计数据。
- en: '**Basic Statistics in One Line**'
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**一行代码实现基本统计**'
- en: As a data scientist and machine learning engineer, you need to know basic statistics.
    Some machine learning algorithms are entirely based on statistics (for example,
    Bayesian networks).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家和机器学习工程师，你需要掌握基础统计学。一些机器学习算法完全基于统计学（例如，贝叶斯网络）。
- en: For example, extracting basic statistics from matrices (such as average, variance,
    and standard deviation) is a critical component for analyzing a wide range of
    data sets such as financial data, health data, or social media data. With the
    rise of machine learning and data science, knowing about how to use NumPy—which
    is at the heart of Python data science, statistics, and linear algebra—will become
    more and more valuable to the marketplace.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从矩阵中提取基本统计数据（如平均值、方差和标准差）是分析各种数据集（如金融数据、健康数据或社交媒体数据）的关键组成部分。随着机器学习和数据科学的兴起，了解如何使用
    NumPy —— 它是 Python 数据科学、统计学和线性代数的核心 —— 将变得越来越有价值。
- en: In this one-liner, you’ll learn how to calculate basic statistics with NumPy.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这行代码中，你将学习如何使用 NumPy 计算基本统计数据。
- en: '***The Basics***'
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: This section explains how to calculate the average, the standard deviation,
    and the variance along an axis. These three calculations are very similar; if
    you understand one, you’ll understand all of them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何沿着轴计算平均值、标准差和方差。这三个计算非常相似；如果你理解了其中一个，你就能理解所有的。
- en: 'Here’s what you want to achieve: given a NumPy array of stock data with rows
    indicating the different companies and columns indicating their daily stock prices,
    the goal is to find the average and standard deviation of each company’s stock
    price (see [Figure 4-23](#ch04fig23)).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要实现的目标是：给定一个包含股票数据的 NumPy 数组，行表示不同的公司，列表示它们的每日股票价格，目标是找到每个公司股票价格的平均值和标准差（见[图
    4-23](#ch04fig23)）。
- en: '![images](Images/fig4-23.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-23.jpg)'
- en: '*Figure 4-23: Average and variance along axis 1*'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-23：沿轴 1 计算平均值和方差*'
- en: This example shows a two-dimensional NumPy array, but in practice, the array
    can have much higher dimensionality.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了一个二维的 NumPy 数组，但在实际中，数组可能具有更高的维度。
- en: '**Simple Average, Variance, Standard Deviation**'
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**简单平均值、方差、标准差**'
- en: 'Before examining how to accomplish this in NumPy, let’s slowly build the background
    you need to know. Say you want to calculate the simple average, the variance,
    or the standard deviation over all values in a NumPy array. You’ve already seen
    examples of the average and the variance function in this chapter. The standard
    deviation is simply the square root of the variance. You can achieve this easily
    with the following functions:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究如何在 NumPy 中完成这些操作之前，让我们逐步建立你需要知道的背景知识。假设你想计算 NumPy 数组中所有值的简单平均值、方差或标准差。你已经在本章中看过了平均值和方差函数的例子。标准差就是方差的平方根。你可以通过以下函数轻松实现这一点：
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You may have noted that you apply those functions on the two-dimensional NumPy
    array `X`. But NumPy simply flattens the array and calculates the functions on
    the flattened array. For example, the simple average of the flattened NumPy array
    `X` is calculated as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到你在二维 NumPy 数组 `X` 上应用了这些函数。但 NumPy 会简单地将数组展平，并在展平后的数组上计算这些函数。例如，展平后的
    NumPy 数组 `X` 的简单平均值计算如下：
- en: (1 + 3 + 5 + 1 + 1 + 1 + 0 + 2 + 4) / 9 = 18 / 9 = 2.0
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: (1 + 3 + 5 + 1 + 1 + 1 + 0 + 2 + 4) / 9 = 18 / 9 = 2.0
- en: '**Calculating Average, Variance, Standard Deviation Along an Axis**'
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**沿轴计算平均值、方差、标准差**'
- en: However, sometimes you want to calculate these functions along an axis. You
    can do this by specifying the keyword `axis` as an argument to the average, variance,
    and standard deviation functions (see [Chapter 3](ch03.xhtml#ch03) for a detailed
    introduction to the `axis` argument).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时你可能希望沿某个轴计算这些函数。你可以通过将`axis`关键字作为参数传递给平均值、方差和标准差函数来实现这一点（参见[第 3 章](ch03.xhtml#ch03)了解`axis`参数的详细介绍）。
- en: '***The Code***'
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: '[Listing 4-8](#list4-8) shows you exactly how to calculate the average, variance,
    and standard deviation along an axis. Our goal is to calculate the averages, variances,
    and standard deviations of all stocks in a two-dimensional matrix with rows representing
    stocks and columns representing daily prices.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4-8](#list4-8)展示了如何沿着某一轴计算平均值、方差和标准差。我们的目标是计算一个二维矩阵中所有股票的平均值、方差和标准差，其中行表示股票，列表示每日价格。'
- en: '[PRE29]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*Listing 4-8: Calculating basic statistics along an axis*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-8：沿某轴计算基本统计数据*'
- en: Guess the output of the puzzle!
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜这个难题的输出是什么！
- en: '***How It Works***'
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***工作原理***'
- en: The one-liner uses the `axis` keyword to specify the axis along which to calculate
    the average, variance, and standard deviation. For example, if you perform these
    three functions along `axis=1`, each row is aggregated into a single value. Hence,
    the resulting NumPy array has a reduced dimensionality of one.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个一行代码使用了`axis`关键字来指定沿哪个轴计算平均值、方差和标准差。例如，如果你沿着`axis=1`执行这三个函数，则每一行会被聚合为一个单一的值。因此，结果的
    NumPy 数组会减少一个维度。
- en: 'The result of the puzzle is the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这个难题的结果如下：
- en: '[PRE30]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Before moving on to the next one-liner, I want to show you how to use the same
    idea for an even higher-dimensional NumPy array.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一个一行代码之前，我想展示如何用相同的思路处理一个更高维度的 NumPy 数组。
- en: 'When averaging along an axis for high-dimensional NumPy arrays, you’ll always
    aggregate the axis defined in the `axis` argument. Here’s an example:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在对高维度 NumPy 数组进行轴向平均时，你总是会在`axis`参数中定义的轴上进行汇总。这里是一个示例：
- en: '[PRE31]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There are three examples of computing the average, variance, and standard deviation
    along axis 2 (see [Chapter 3](ch03.xhtml#ch03); the innermost axis). In other
    words, all values of axis 2 will be combined into a single value that results
    in axis 2 being dropped from the resulting array. Dive into the three examples
    and figure out how exactly axis 2 is collapsed into a single average, variance,
    or standard deviation value.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个例子展示了如何沿着轴 2 计算平均值、方差和标准差（参见[第 3 章](ch03.xhtml#ch03)；最内层轴）。换句话说，轴 2 上的所有值将合并为一个单一值，结果是轴
    2 从结果数组中消失。深入这三个例子，弄清楚轴 2 是如何被压缩成一个平均值、方差或标准差的。
- en: To summarize, a wide range of data sets (including financial data, health data,
    and social media data) requires you to be able to extract basic insights from
    your data sets. This section gives you a deeper understanding of how to use the
    powerful NumPy toolset to extract basic statistics quickly and efficiently from
    multidimensional arrays. This is needed as a basic preprocessing step for many
    machine learning algorithms.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，广泛的数据集（包括金融数据、健康数据和社交媒体数据）要求你能够从数据集中提取基本的见解。本节内容让你更深入地理解如何利用强大的 NumPy 工具集，从多维数组中快速高效地提取基本统计数据。这是许多机器学习算法所需的基本预处理步骤。
- en: '**Classification with Support-Vector Machines in One Line**'
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用支持向量机进行分类，仅一行代码**'
- en: '*Support-vector machines* (*SVMs*) have gained massive popularity in recent
    years because they have robust classification performance, even in high-dimensional
    spaces. Surprisingly, SVMs work even if there are more dimensions (features) than
    data items. This is unusual for classification algorithms because of the *curse
    of dimensionality*: with increasing dimensionality, the data becomes extremely
    sparse, which makes it hard for algorithms to find patterns in the data set. Understanding
    the basic ideas of SVMs is a fundamental step to becoming a sophisticated machine
    learning engineer.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机*（*SVMs*）在近年来广受欢迎，因为它们在高维空间中仍然具有强大的分类性能。令人惊讶的是，SVM 即使在特征数（维度）大于数据项的情况下也能工作。这对分类算法来说是非常不同寻常的，因为存在*维度灾难*：随着维度的增加，数据变得非常稀疏，这使得算法很难从数据集中发现模式。理解
    SVM 的基本概念是成为一名成熟的机器学习工程师的基础步骤。'
- en: '***The Basics***'
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: How do classification algorithms work? They use the training data to find a
    decision boundary that divides data in the one class from data in the other class
    (in “Logistic Regression in One Line” on [page 89](#page_89), the decision boundary
    would be whether the probability of the sigmoid function is below or above the
    0.5 threshold).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法是如何工作的？它们使用训练数据找到一个决策边界，将一个类别的数据与另一个类别的数据分开（在[第89页](#page_89)的“Logistic回归一行代码”中，决策边界是sigmoid函数的概率是否高于或低于0.5的阈值）。
- en: '**A High-Level Look at Classification**'
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**分类的高级视角**'
- en: '[Figure 4-24](#ch04fig24) shows an example of a general classifier.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-24](#ch04fig24)展示了一个通用分类器的例子。'
- en: '![images](Images/fig4-24.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-24.jpg)'
- en: '*Figure 4-24: Diverse skill sets of computer scientists and artists*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-24：计算机科学家和艺术家的多样化技能组合*'
- en: 'Suppose you want to build a recommendation system for aspiring university students.
    The figure visualizes the training data consisting of users classified according
    to their skills in two areas: logic and creativity. Some people have high logic
    skills and relatively low creativity; others have high creativity and relatively
    low logic skills. The first group is labeled as *computer scientists*, and the
    second group is labeled as *artists*.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想为有志的大学生建立一个推荐系统。图示展示了训练数据，数据由根据逻辑和创造力两个领域的技能对用户进行分类。一些人具有较高的逻辑能力，而创造力相对较低；另一些人则具有较高的创造力，而逻辑能力相对较低。第一组被标记为*计算机科学家*，第二组被标记为*艺术家*。
- en: To classify new users, the machine learning model must find a decision boundary
    that separates the computer scientists from the artists. Roughly speaking, you’ll
    classify a user by where they fall with respect to the decision boundary. In the
    example, you’ll classify users who fall into the left area as computer scientists,
    and users who fall into the right area as artists.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分类新用户，机器学习模型必须找到一个决策边界，将计算机科学家和艺术家区分开。大致来说，你将根据用户在决策边界的哪一侧进行分类。在这个例子中，你会将落在左侧区域的用户分类为计算机科学家，将落在右侧区域的用户分类为艺术家。
- en: In the two-dimensional space, the decision boundary is either a line or a (higher-order)
    curve. The former is called a *linear classifier*, and the latter is called a
    *nonlinear classifier*. In this section, we’ll explore only linear classifiers.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，决策边界要么是直线，要么是（更高阶的）曲线。前者称为*线性分类器*，后者称为*非线性分类器*。在这一节中，我们只讨论线性分类器。
- en: '[Figure 4-24](#ch04fig24) shows three decision boundaries that are all valid
    separators of the data. In our example, it’s impossible to quantify which of the
    given decision boundaries is better; they all lead to perfect accuracy when classifying
    the training data.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-24](#ch04fig24)展示了三个有效的数据分隔决策边界。在我们的例子中，无法量化哪个决策边界更好；它们在分类训练数据时都能达到完美的准确度。'
- en: '**But What Is the Best Decision Boundary?**'
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**但最佳决策边界是什么？**'
- en: Support-vector machines provide a unique and beautiful answer to this question.
    Arguably, the best decision boundary provides a maximal margin of safety. In other
    words, SVMs maximize the distance between the closest data points and the decision
    boundary. The goal is to minimize the error of new points that are close to the
    decision boundary.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机为这个问题提供了一个独特而美妙的答案。可以说，最佳的决策边界提供了最大的安全边距。换句话说，SVM最大化最接近数据点与决策边界之间的距离。目标是最小化新数据点接近决策边界时的误差。
- en: '[Figure 4-25](#ch04fig25) shows an example.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-25](#ch04fig25)展示了一个例子。'
- en: '![images](Images/fig4-25.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-25.jpg)'
- en: '*Figure 4-25: Support-vector machines maximize the error of margin.*'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-25：支持向量机最大化边距误差。*'
- en: The SVM classifier finds the respective support vectors so that the zone between
    the support vectors is as thick as possible. Here, the support vectors are the
    data points that lie on the two dotted lines parallel to the decision boundary.
    These lines are denoted as *margins*. The decision boundary is the line in the
    middle with maximal distance to the margins. Because the zone between the margins
    and the decision boundary is maximized, the *margin of error* is expected to be
    maximal when classifying new data points. This idea shows high classification
    accuracy for many practical problems.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 分类器找到相应的支持向量，使得支持向量之间的区域尽可能厚实。在这里，支持向量是位于与决策边界平行的两条虚线上的数据点。这些线被称为*边际*。决策边界是位于两条边际之间的线，且与边际的距离最大。由于最大化了边际和决策边界之间的区域，因此在分类新数据点时，*误差边际*预计将是最大的。这一思想对于许多实际问题展示了高分类准确性。
- en: '***The Code***'
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: Is it possible to create your own SVM in a single line of Python code? Take
    a look at [Listing 4-9](#list4-9).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可以用一行 Python 代码创建你自己的 SVM？查看[列表 4-9](#list4-9)。
- en: '[PRE32]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*Listing 4-9: SVM classification in a single line of code*'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-9：单行代码实现 SVM 分类*'
- en: Guess the output of this code.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜这段代码的输出是什么。
- en: '***How It Works***'
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***它是如何工作的***'
- en: The code breaks down how you can use support-vector machines in Python in the
    most basic form. The NumPy array holds the labeled training data with one row
    per user and one column per feature (skill level in math, language, and creativity).
    The last column is the label (the class).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码展示了如何在 Python 中以最基本的形式使用支持向量机。NumPy 数组保存了带标签的训练数据，每行代表一个用户，每列代表一个特征（数学、语言和创造力的技能水平）。最后一列是标签（类别）。
- en: Because you have three-dimensional data, the support-vector machine separates
    the data by using two-dimensional planes (the linear separator) rather than one-dimensional
    lines. As you can see, it’s also possible to separate three classes rather than
    only two as shown in the preceding examples.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你有三维数据，支持向量机通过使用二维平面（线性分隔符）来分离数据，而不是使用一维的直线。如你所见，支持向量机不仅能分离两个类别，也能分离三个类别，如前面的例子所示。
- en: 'The one-liner itself is straightforward: you first create the model by using
    the constructor of the `svm.SVC` class (*SVC* stands for *support-vector classification*).
    Then, you call the `fit()` function to perform the training based on your labeled
    training data.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码本身很简洁：你首先通过使用 `svm.SVC` 类的构造函数来创建模型（*SVC* 代表 *支持向量分类*）。然后，调用 `fit()` 函数基于已标记的训练数据进行训练。
- en: In the results part of the code snippet, you call the `predict()` function on
    new observations. Because `student_0` has skills indicated as math=3, language=3,
    and creativity=6, the support-vector machine predicts that the label *art* fits
    this student’s skills. Similarly, `student_1` has skills indicated as math=8,
    language=1, and creativity=1\. Thus, the support-vector machine predicts that
    the label *computer science* fits this student’s skills.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码片段的结果部分，你对新观察结果调用 `predict()` 函数。因为 `student_0` 的技能被标示为数学=3，语言=3，创造力=6，支持向量机预测该学生的标签是*艺术*。类似地，`student_1`
    的技能被标示为数学=8，语言=1，创造力=1，因此支持向量机预测该学生的标签是*计算机科学*。
- en: 'Here’s the final output of the one-liner:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该单行代码的最终输出：
- en: '[PRE33]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In summary, SVMs perform well even in high-dimensional spaces when there are
    more features than training data vectors. The idea of maximizing the *margin of
    safety* is intuitive and leads to robust performance when classifying *boundary
    cases*—that is, vectors that fall within the margin of safety. In the final section
    of this chapter, we’ll zoom one step back and have a look at a meta-algorithm
    for classification: ensemble learning with random forests.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，SVM 即使在高维空间中也表现良好，尤其当特征数多于训练数据向量时。最大化*安全边际*的思想直观且有助于在分类*边界案例*时实现稳健的性能——即，位于安全边际内的向量。在本章的最后一部分，我们将后退一步，看看一个用于分类的元算法：随机森林的集成学习。
- en: '**Classification with Random Forests in One Line**'
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用随机森林进行分类的单行代码**'
- en: 'Let’s move on to an exciting machine learning technique: *ensemble learning*.
    Here’s my quick-and-dirty tip if your prediction accuracy is lacking but you need
    to meet the deadline at all costs: try this meta-learning approach that combines
    the predictions (or classifications) of multiple machine learning algorithms.
    In many cases, it will give you better last-minute results.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论一种令人兴奋的机器学习技术：*集成学习*。如果你的预测准确率不足，但你需要在最后时刻完成工作，下面是我的快速建议：尝试这种元学习方法，结合多个机器学习算法的预测（或分类）。在许多情况下，它将为你提供更好的最后时刻结果。
- en: '***The Basics***'
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: In the previous sections, you’ve studied multiple machine learning algorithms
    that you can use to get quick results. However, different algorithms have different
    strengths. For example, neural network classifiers can generate excellent results
    for complex problems. However, they are also prone to overfitting the data because
    of their powerful capacity to memorize fine-grained patterns of the data. Ensemble
    learning for classification problems partially overcomes the problem that you
    often don’t know in advance which machine learning technique works best.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你已经学习了多个机器学习算法，可以用来快速获取结果。然而，不同的算法有不同的优势。例如，神经网络分类器可以为复杂问题生成出色的结果。然而，由于它们强大的记忆能力，能够记住数据中的细粒度模式，因此也容易发生过拟合。集成学习在分类问题中部分解决了你常常无法预先知道哪种机器学习技术最有效的问题。
- en: How does this work? You create a meta-classifier consisting of multiple types
    or instances of basic machine learning algorithms. In other words, you train multiple
    models. To classify a single observation, you ask all models to classify the input
    independently. Next, you return the class that was returned most often, given
    your input, as a *meta-prediction*. This is the final output of your ensemble
    learning algorithm.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的？你创建了一个由多种类型或实例的基本机器学习算法组成的元分类器。换句话说，你训练了多个模型。为了对单个观察值进行分类，你让所有模型独立地对输入进行分类。接下来，你返回根据输入返回次数最多的类别，作为*元预测*。这就是集成学习算法的最终输出。
- en: '*Random forests* are a special type of ensemble learning algorithms. They focus
    on decision-tree learning. A forest consists of many trees. Similarly, a random
    forest consists of many decision trees. Each decision tree is built by injecting
    randomness in the tree-generation procedure during the training phase (for example,
    which tree node to select first). This leads to various decision trees—exactly
    what you want.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林*是一种特殊类型的集成学习算法。它们专注于决策树学习。一个森林由许多树组成。同样，随机森林由许多决策树组成。每棵决策树通过在训练阶段树生成过程中注入随机性来构建（例如，选择哪个树节点作为第一个）。这导致了不同的决策树——这正是你所需要的。'
- en: '[Figure 4-26](#ch04fig26) shows how the prediction works for a trained random
    forest using the following scenario. Alice has high math and language skills.
    The *ensemble* consists of three decision trees (building a random forest). To
    classify Alice, each decision tree is queried about Alice’s classification. Two
    of the decision trees classify Alice as a computer scientist. Because this is
    the class with the most votes, it’s returned as the final output for the classification.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-26](#ch04fig26) 显示了如何使用以下场景来进行训练后的随机森林预测。Alice 具有较高的数学和语言技能。*集成学习*由三棵决策树组成（构建随机森林）。为了对
    Alice 进行分类，每棵决策树都会询问 Alice 的分类结果。两棵决策树将 Alice 分类为计算机科学家。由于这是获得最多票数的类别，它将作为分类的最终输出返回。'
- en: '![images](Images/fig4-26.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/fig4-26.jpg)'
- en: '*Figure 4-26: Random forest classifier aggregating the output of three decision
    trees*'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-26：随机森林分类器聚合三棵决策树的输出*'
- en: '***The Code***'
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***代码***'
- en: Let’s stick to this example of classifying the study field based on a student’s
    skill level in three areas (math, language, creativity). You may think that implementing
    an ensemble learning method is complicated in Python. But it’s not, thanks to
    the comprehensive scikit-learn library (see [Listing 4-10](#list4-10)).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们坚持这个例子，根据学生在三个领域（数学、语言、创造力）的技能水平来分类学习领域。你可能认为在 Python 中实现集成学习方法很复杂。但实际上并不复杂，得益于功能强大的
    scikit-learn 库（参见 [列表 4-10](#list4-10)）。
- en: '[PRE34]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*Listing 4-10: Ensemble learning with random forest classifiers*'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-10：使用随机森林分类器的集成学习*'
- en: 'Take a guess: what’s the output of this code snippet?'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜看：这段代码的输出是什么？
- en: '***How It Works***'
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***原理***'
- en: After initializing the labeled training data in [Listing 4-10](#list4-10), the
    code creates a random forest by using the constructor on the class `RandomForestClassifier`
    with one parameter `n_estimators` that defines the number of trees in the forest.
    Next, you populate the model that results from the previous initialization (an
    empty forest) by calling the function `fit()`. To this end, the input training
    data consists of all but the last column of array `X`, while the labels of the
    training data are defined in the last column. As in the previous examples, you
    use slicing to extract the respective columns from the data array `X`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 4-10](#list4-10)中初始化标注的训练数据后，代码使用`RandomForestClassifier`类的构造函数创建一个随机森林，构造函数接受一个参数`n_estimators`，定义森林中的树木数量。接下来，通过调用`fit()`函数来填充先前初始化（一个空森林）的模型。为此，输入的训练数据由数组`X`的所有列（除了最后一列）组成，而训练数据的标签定义在最后一列。如同前面的示例，你使用切片来从数据数组`X`中提取相应的列。
- en: The classification part is slightly different in this code snippet. I wanted
    to show you how to classify multiple observations instead of only one. You can
    achieve this here by creating a multidimensional array with one row per observation.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的分类部分略有不同。我想向你展示如何对多个观察值进行分类，而不仅仅是一个。你可以通过创建一个多维数组，每行代表一个观察值，来实现这一点。
- en: 'Here’s the output of the code snippet:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码片段的输出：
- en: '[PRE35]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note that the result is still nondeterministic (the result may be different
    for different executions of the code) because the random forest algorithm relies
    on the random number generator that returns different numbers at different points
    in time. You can make this call deterministic by using the integer argument `random_state`.
    For example, you can set `random_state=1` when calling the random forest constructor:
    `RandomForestClassifier(n_estimators=10, random_state=1)`. In this case, each
    time you create a new random forest classifier, the same output results because
    the same random numbers are created: they are all based on the seed integer 1.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，结果仍然是非确定性的（不同执行代码时结果可能不同），因为随机森林算法依赖于随机数生成器，该生成器在不同的时间点返回不同的数字。你可以通过使用整数参数`random_state`使此调用变为确定性。例如，你可以在调用随机森林构造函数时设置`random_state=1`：`RandomForestClassifier(n_estimators=10,
    random_state=1)`。在这种情况下，每次创建一个新的随机森林分类器时，都会得到相同的输出，因为生成的随机数相同：它们都是基于种子整数1。
- en: 'In summary, this section introduced a meta-approach for classification: using
    the output of various decision trees to reduce the variance of the classification
    error. This is one version of ensemble learning, which combines multiple basic
    models into a single meta-model that’s able to leverage their individual strengths.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本节介绍了一种分类的元方法：使用各种决策树的输出以减少分类误差的方差。这是集成学习的一种形式，它将多个基本模型组合成一个单一的元模型，能够利用它们各自的优点。
- en: '**NOTE**'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Two different decision trees can lead to a high variance of the error: one
    generates good results, while the other one doesn’t. By using random forests,
    you mitigate this effect.*'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '*两棵不同的决策树可能导致高方差的错误：一棵产生良好的结果，而另一棵则没有。通过使用随机森林，你可以减轻这一效果。*'
- en: 'Variations of this idea are common in machine learning—and if you need to quickly
    improve your prediction accuracy, simply run multiple machine learning models
    and evaluate their output to find the best one (a quick-and-dirty secret of machine
    learning practitioners). In a way, ensemble learning techniques automatically
    perform the task that’s often done by experts in practical machine learning pipelines:
    selecting, comparing, and combining the output of different machine learning models.
    The big strength of ensemble learning is that this can be done individually for
    each data value at runtime.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这种思想的变种在机器学习中很常见——如果你需要快速提高预测准确性，只需运行多个机器学习模型，并评估它们的输出以找到最佳模型（这是机器学习从业者的一个快速而粗糙的秘密）。某种程度上，集成学习技术自动执行了在实际机器学习流程中通常由专家完成的任务：选择、比较并结合不同机器学习模型的输出。集成学习的最大优势是，这一过程可以在运行时对每个数据值单独进行。
- en: '**Summary**'
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: This chapter covered 10 basic machine learning algorithms that are fundamental
    to your success in the field. You’ve learned about regression algorithms to predict
    values such as linear regression, KNNs, and neural networks. You’ve learned about
    classification algorithms such as logistic regression, decision-tree learning,
    SVMs, and random forests. Furthermore, you’ve learned how to calculate basic statistics
    of multidimensional data arrays, and to use the K-Means algorithm for unsupervised
    learning. These algorithms and methods are among the most important algorithms
    in the field of machine learning, and there are a lot more to study if you want
    to start working as a machine learning engineer. That learning will pay off—machine
    learning engineers usually earn six figures in the United States (a simple web
    search should confirm this)! For students who want to dive deeper into machine
    learning, I recommend the excellent (and free) Coursera course from Andrew Ng.
    You can find the course material online by asking your favorite search engine.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了 10 个基础的机器学习算法，这些算法对于你在该领域的成功至关重要。你已经学习了回归算法，如线性回归、KNN 和神经网络，用于预测数值。你也学习了分类算法，如逻辑回归、决策树学习、SVM
    和随机森林。此外，你还学会了如何计算多维数据数组的基本统计量，并使用 K-Means 算法进行无监督学习。这些算法和方法是机器学习领域中最重要的算法之一，如果你想成为一名机器学习工程师，还有很多内容需要学习。这些学习将会带来回报——机器学习工程师在美国通常能赚到六位数的薪水（一个简单的网络搜索应该能验证这一点）！对于那些希望深入了解机器学习的学生，我推荐
    Andrew Ng 的优秀（且免费的）Coursera 课程。你可以通过你最喜欢的搜索引擎找到该课程的在线材料。
- en: 'In the next chapter, you’ll study one of the most important (and most undervalued)
    skills of highly efficient programmers: regular expressions. While this chapter
    was a bit more on the conceptual side (you learned the general ideas, but the
    scikit-learn library did the heavy lifting), the next chapter will be highly technical.
    So, roll up your sleeves and read on!'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习高效程序员最重要（也是最被低估）的一项技能：正则表达式。虽然这一章更侧重于概念性内容（你了解了基本的概念，但实际的重担由 scikit-learn
    库承担），但下一章将会是高度技术性的内容。所以，挽起袖子，继续阅读吧！
