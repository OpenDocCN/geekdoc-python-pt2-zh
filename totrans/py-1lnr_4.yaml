- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MACHINE LEARNING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/comm-1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Machine learning is found in almost every area of computer science. Over the
    past few years, I’ve attended computer science conferences in fields as diverse
    as distributed systems, databases, and stream processing, and no matter where
    I go, machine learning is already there. At some conferences, more than half of
    the presented research ideas have relied on machine learning methods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: As a computer scientist, you must know the fundamental machine learning ideas
    and algorithms to round out your overall skill set. This chapter provides an introduction
    to the most important machine learning algorithms and methods, and gives you 10
    practical one-liners to apply these algorithms in your own projects.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '**The Basics of Supervised Machine Learning**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main aim of machine learning is to make accurate predictions using existing
    data. Let’s say you want to write an algorithm that predicts the value of a specific
    stock over the next two days. To achieve this goal, you’ll need to train a machine
    learning model. But what exactly is a *model*?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of a machine learning user, the machine learning model
    looks like a black box ([Figure 4-1](#ch04fig01)): you put data in and get predictions
    out.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-1: A machine learning model, shown as a black box*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In this model, you call the input data *features* and denote them using the
    variable *x*, which can be a numerical value or a multidimensional vector of numerical
    values. Then the box does its magic and processes your input data. After a bit
    of time, you get prediction *y* back, which is the model’s predicted output, given
    the input features. For regression problems, the prediction consists of one or
    multiple numerical values—just like the input features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised machine learning is divided into two separate phases: the training
    phase and the inference phase.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '***Training Phase***'
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: During the *training phase*, you tell your model your desired output *y’* for
    a given input *x*. When the model outputs the prediction *y*, you compare it to
    *y’*, and if they are not the same, you update the model to generate an output
    that is closer to *y’*, as shown in [Figure 4-2](#ch04fig02). Let’s look at an
    example from image recognition. Say you train a model to predict fruit names (outputs)
    when given images (inputs). For example, your specific training input is an image
    of a banana, but your model wrongly predicts *apple*. Because your desired output
    is different from the model prediction, you change the model so that next time
    the model will correctly predict *banana*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-2.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-2: The training phase of a machine learning model*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'As you keep telling the model your desired outputs for many different inputs
    and adjusting the model, you train the model by using your *training data*. Over
    time, the model will learn which output you’d like to get for certain inputs.
    That’s why data is so important in the 21st century: your model will be only as
    good as its training data. Without good training data, the model is guaranteed
    to fail. Roughly speaking, the training data supervises the machine learning process.
    That’s why we denote it *supervised learning*.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不断向模型提供不同输入的期望输出并调整模型时，你就在使用*训练数据*训练模型。随着时间的推移，模型将学习你希望在某些输入下得到的输出。这就是为什么数据在21世纪如此重要：你的模型将与其训练数据一样好。没有好的训练数据，模型注定会失败。粗略地说，训练数据监督着机器学习过程。这就是我们将其称为*有监督学习*的原因。
- en: '***Inference Phase***'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***推理阶段***'
- en: 'During the *inference phase*, you use the trained model to predict output values
    for new input features *x*. Note that the model has the power to predict outputs
    for inputs that have never been observed in the training data. For example, the
    fruit prediction model from the *training phase* can now identify the name of
    the fruits (learned in the training data) in images it has never seen before.
    In other words, suitable machine learning models possess the ability to *generalize*:
    they use their experience from the training data to predict outcomes for new inputs.
    Roughly speaking, models that generalize well produce accurate predictions for
    new input data. Generalized prediction for unseen input data is one of the strengths
    of machine learning and is a prime reason for its popularity across a wide range
    of applications.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在*推理阶段*，你使用训练好的模型来预测新输入特征*x*的输出值。请注意，模型具有预测在训练数据中从未观察过的输入的输出的能力。例如，*训练阶段*中的水果预测模型现在可以识别它从未见过的图像中（在训练数据中学到的）水果名称。换句话说，合适的机器学习模型具备*概括能力*：它们利用训练数据中的经验来预测新输入的结果。粗略地说，能够良好概括的模型会为新输入数据产生准确的预测。对于未见过的输入数据的泛化预测是机器学习的一个强项，也是它在广泛应用中受欢迎的主要原因。
- en: '**Linear Regression**'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**线性回归**'
- en: '*Linear regression* is the one machine learning algorithm you’ll find most
    often in beginner-level machine learning tutorials. It’s commonly used in *regression
    problems*, for which the model predicts missing data values by using existing
    ones. A considerable advantage of linear regression, both for teachers and users,
    is its simplicity. But that doesn’t mean it can’t solve real problems! Linear
    regression has lots of practical use cases in diverse areas such as market research,
    astronomy, and biology. In this section, you’ll learn everything you need to know
    to get started with linear regression.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归*是你在初学者级别的机器学习教程中最常见的机器学习算法。它通常用于*回归问题*，其中模型通过使用现有的数据预测缺失的数据值。线性回归的一个显著优势是其简洁性，这对教师和用户都非常有利。但这并不意味着它不能解决实际问题！线性回归在市场研究、天文学和生物学等各个领域都有很多实际应用。在这一节中，你将学到开始使用线性回归所需的所有知识。'
- en: '***The Basics***'
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***基础知识***'
- en: How can you use linear regression to predict stock prices on a given day? Before
    I answer this question, let’s start with some definitions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用线性回归预测某一天的股价？在我回答这个问题之前，让我们先从一些定义开始。
- en: 'Every machine learning model consists of model parameters. *Model parameters*
    are internal configuration variables that are estimated from the data. These model
    parameters determine how exactly the model calculates the prediction, given the
    input features. For linear regression, the model parameters are called *coefficients*.
    You may remember the formula for two-dimensional lines from school: *f(x)* = *ax*
    + *c*.  The two variables *a* and *c* are the coefficients in the linear equation
    *ax* + *c*. You can describe how each input *x* is transformed into an output
    *f(x)* so that all outputs together describe a line in the two-dimensional space.
    By changing the coefficients, you can describe any line in the two-dimensional
    space.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习模型都由模型参数组成。*模型参数*是从数据中估算出来的内部配置变量。这些模型参数决定了给定输入特征时，模型如何准确地计算预测值。对于线性回归，模型参数被称为*系数*。你可能还记得学校时学过的二维直线公式：*f(x)*
    = *ax* + *c*。这两个变量*a*和*c*就是线性方程*ax* + *c*中的系数。你可以描述每个输入*x*是如何转化为输出*f(x)*的，使得所有输出一起描述二维空间中的一条直线。通过改变系数，你可以描述二维空间中的任何一条直线。
- en: 'Given the input features *x*[1], *x*[2], . . ., *x**[k]*,  the linear regression
    model combines the input features with the coefficients *a*[1], *a*[2], . . .,
    *a**[k]* to calculate the predicted output *y* by using this formula:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入特征*x*[1]、*x*[2]、...、*x**[k]*，线性回归模型将输入特征与系数*a*[1]、*a*[2]、...、*a**[k]*结合，通过使用该公式计算预测输出*y*：
- en: '*y* = *f*(*x*) = *a*[0] + *a*[1] × *x*[1] + *a*[2] × *x*[2] + ... + *a*[*k*]
    × *x*[*k*]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*) = *a*[0] + *a*[1] × *x*[1] + *a*[2] × *x*[2] + ... + *a*[*k*]
    × *x*[*k*]'
- en: 'In our stock price example, you have a single input feature, *x*, the day.
    You input the day *x* with the hope of getting a stock price, the output *y*.
    This simplifies the linear regression model to the formula of a two-dimensional
    line:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的股价示例中，你有一个输入特征*x*，即日期。你输入日期*x*，期望得到股价，即输出*y*。这将线性回归模型简化为二维直线公式：
- en: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x*'
- en: Let’s have a look at three lines for which you change only the two model parameters
    *a*[0] and *a*[1] in [Figure 4-3](#ch04fig03). The first axis describes the input
    *x*. The second axis describes the output *y*. The line represents the (linear)
    relationship between input and output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在[图4-3](#ch04fig03)中，只改变两个模型参数*a*[0]和*a*[1]的三条直线。第一轴描述输入*x*，第二轴描述输出*y*。直线表示输入与输出之间的（线性）关系。
- en: '![images](Images/fig4-3.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-3.jpg)'
- en: '*Figure 4-3: Three linear regression models (lines) described by different
    model parameters (coefficients). Every line represents a unique relationship between
    the input and the output variables.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-3：由不同模型参数（系数）描述的三条线性回归模型（直线）。每条线代表输入与输出变量之间的独特关系。*'
- en: 'In our stock price example, let’s say our training data is the indices of three
    days, `[0, 1, 2]`, matched with the stock prices `[155, 156, 157]`. To put it
    differently:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的股价示例中，假设我们的训练数据是三天的索引`[0, 1, 2]`，与股价`[155, 156, 157]`相对应。换句话说：
- en: Input `x=0` should cause output `y=155`
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`x=0`应导致输出`y=155`
- en: Input `x=1` should cause output `y=156`
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`x=1`应导致输出`y=156`
- en: Input `x=2` should cause output `y=157`
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入`x=2`应导致输出`y=157`
- en: Now, which line best fits our training data? I plotted the training data in
    [Figure 4-4](#ch04fig04).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，哪条直线最适合我们的训练数据？我在[图4-4](#ch04fig04)中绘制了训练数据。
- en: '![images](Images/fig4-4.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-4.jpg)'
- en: '*Figure 4-4: Our training data, with its index in the array as the* x *coordinate,
    and its price as the* y *coordinate*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-4：我们的训练数据，其在数组中的索引作为*x*坐标，价格作为*y*坐标*'
- en: To find the line that best describes the data and, thus, to create a linear
    regression model, we need to determine the coefficients. This is where machine
    learning comes in. There are two principal ways of determining model parameters
    for linear regression. First, you can analytically calculate the line of best
    fit that goes between these points (the standard method for linear regression).
    Second, you can try different models, testing each against the labeled sample
    data, and ultimately deciding on the best one. In any case, you determine “best”
    through a process called *error minimization*, in which the model minimizes the
    squared difference (or selects the coefficients that lead to a minimal squared
    difference) of the predicted model values and the ideal output, selecting the
    model with the lowest error.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到最能描述数据的直线，从而创建线性回归模型，我们需要确定系数。这就是机器学习的作用所在。确定线性回归模型参数的主要方法有两种。首先，你可以通过解析方法计算出最佳拟合线，即通过这些数据点的直线（这是线性回归的标准方法）。其次，你可以尝试不同的模型，逐个对比标记样本数据，最终选择最合适的模型。无论如何，你通过一种叫做*误差最小化*的过程来确定“最佳”，在这个过程中，模型最小化预测模型值和理想输出之间的平方差（或选择能导致最小平方差的系数），从而选择误差最小的模型。
- en: 'For our data, you end up with coefficients of *a*[0] = 155.0 and *a*[1] = 1.0\.
    Then you put them into our formula for linear regression:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，你得到的系数是*a*[0] = 155.0 和 *a*[1] = 1.0。然后你将它们代入我们的线性回归公式：
- en: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x* = 155.0 + 1.0 × *x*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *f*(*x*) = *a*[0] + *a*[1]*x* = 155.0 + 1.0 × *x*'
- en: and plot both the line and the training data in the same space, as shown in
    [Figure 4-5](#ch04fig05).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在同一坐标系中绘制直线和训练数据，如[图4-5](#ch04fig05)所示。
- en: '![images](Images/fig4-5.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![images](Images/fig4-5.jpg)'
- en: '*Figure 4-5: A prediction line made using our linear regression model*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-5：使用我们的线性回归模型制作的预测线*'
- en: A perfect fit! The squared distance between the line (model prediction) and
    the training data is zero—so you have found the model that minimizes the error.
    Using this model, you can now predict the stock price for any value of *x*. For
    example, say you want to predict the stock price on day *x* = 4\. To accomplish
    this, you simply use the model to calculate *f(x)* = 155.0 + 1.0 × 4 = 159.0\.
    The predicted stock price on day 4 is $159\. Of course, whether this prediction
    accurately reflects the real world is another story.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: That’s the high-level overview of what happens. Let’s take a closer look at
    how to do this in code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 4-1](#list4-1) shows how to build a simple linear regression model
    in a single line of code (you may need to install the scikit-learn library first
    by running `pip install sklearn` in your shell).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Listing 4-1: A simple linear regression model*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Can you already guess the output of this code snippet?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This one-liner uses two Python libraries: NumPy and scikit-learn. The former
    is the de facto standard library for numerical computations (like matrix operations).
    The latter is the most comprehensive library for machine learning and has implementations
    of hundreds of machine learning algorithms and techniques.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'You may ask: “Why are you using libraries in a Python one-liner? Isn’t this
    cheating?” It’s a good question, and the answer is yes. Any Python program—with
    or without libraries—uses high-level functionality built on low-level operations.
    There’s not much point in reinventing the wheel when you can reuse existing code
    bases (that is, stand on the shoulders of giants). Aspiring coders often feel
    the urge to implement everything on their own, but this reduces their coding productivity.
    In this book, we’re going to use, not reject, the wide spectrum of powerful functionality
    implemented by some of the world’s best Python coders and pioneers. Each of these
    libraries took skilled coders years to develop, optimize, and tweak.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through [Listing 4-1](#list4-1) step by step. First, we create a simple
    data set of three values and store its length in a separate variable `n` to make
    the code more concise. Our data is three Apple stock prices for three consecutive
    days. The variable `apple` holds this data set as a one-dimensional NumPy array.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we build the model by calling `LinearRegression()`. But what are the
    model parameters? To find them, we call the `fit()` function to train the model.
    The `fit()` function takes two arguments: the input features of the training data
    and the ideal outputs for these inputs. Our ideal outputs are the real stock prices
    of the Apple stock. But for the input features, `fit()` requires an array with
    the following format:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'where each training data value is a sequence of feature values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In our case, the input consists of only a single feature *x* (the current day).
    Moreover, the prediction also consists of only a single value *y* (the current
    stock price). To bring the input array into the correct shape, you need to reshape
    it to this strange-looking matrix form:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A matrix with only one column is called a *column vector*. You use `np.arange()`
    to create the sequence of increasing *x* values; then you use `reshape((n, 1))`
    to convert the one-dimensional NumPy array into a two-dimensional array with one
    column and `n` rows (see [Chapter 3](ch03.xhtml#ch03)). Note that scikit-learn
    allows the output to be a one-dimensional array (otherwise, you would have to
    reshape the `apple` data array as well).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it has the training data and the ideal outputs, `fit()` then does error
    minimization: it finds the model parameters (that means *line*) so that the difference
    between the predicted model values and the desired outputs is minimal.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'When `fit()` is satisfied with its model, it’ll return a model that you can
    use to predict two new stock values by using the `predict()` function. The `predict()`
    function has the same input requirements as `fit()`, so to satisfy them, you’ll
    pass a one-column matrix with our two new values that you want predictions for:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Because our error minimization was zero, you should get perfectly linear outputs
    of 158 and 159\. This fits well along the line of fit plotted in [Figure 4-5](#ch04fig05).
    But it’s often not possible to find such a perfectly fitting single straight-line
    linear model. For example, if our stock prices are `[157, 156, 159]`, and you
    run the same function and plot it, you should get the line in [Figure 4-6](#ch04fig06).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the `fit()` function finds the line that minimizes the squared
    error between the training data and the predictions as described previously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Let’s wrap this up. Linear regression is a machine learning technique whereby
    your model learns coefficients as model parameters. The resulting linear model
    (for example, a line in the two-dimensional space) directly provides you with
    predictions on new input data. This problem of predicting numerical values when
    given numerical input values belongs to the class of regression problems. In the
    next section, you’ll learn about another important area in machine learning called
    classification.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-6.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-6: A linear regression model with an imperfect fit*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic Regression in One Line**'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic regression is commonly used for *classification problems*, in which
    you predict whether a sample belongs to a specific category (or class). This contrasts
    with regression problems, where you’re given a sample and predict a numerical
    value that falls into a continuous range. An example classification problem is
    to divide Twitter users into the male and female, given different input features
    such as their *posting frequency* or the *number of tweet replies*. The logistic
    regression model belongs to one of the most fundamental machine learning models.
    Many concepts introduced in this section will be the basis of more advanced machine
    learning techniques.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To introduce logistic regression, let’s briefly review how linear regression
    works: given the training data, you compute a line that fits this training data
    and predicts the outcome for input *x*. In general, linear regression is great
    for predicting a *continuous* output, whose value can take an infinite number
    of values. The stock price predicted earlier, for example, could conceivably have
    been any number of positive values.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: But what if the output is not continuous, but *categorical*, belonging to a
    limited number of groups or categories? For example, let’s say you want to predict
    the likelihood of lung cancer, given the number of cigarettes a patient smokes.
    Each patient can either have lung cancer or not. In contrast to the stock price,
    here you have only these two possible outcomes. Predicting the likelihood of categorical
    outcomes is the primary motivation for logistic regression.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '**The Sigmoid Function**'
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Whereas linear regression fits a line to the training data, logistic regression
    fits an S-shaped curve, called *the sigmoid function*. The S-shaped curve helps
    you make binary decisions (for example, yes/no). For most input values, the sigmoid
    function will return a value that is either very close to 0 (one category) or
    very close to 1 (the other category). It’s relatively unlikely that your given
    input value generates an ambiguous output. Note that it is possible to generate
    0.5 probabilities for a given input value—but the shape of the curve is designed
    in a way to minimize those in practical settings (for most possible values on
    the horizontal axis, the probability value is either very close to 0 or very close
    to 1). [Figure 4-7](#ch04fig07) shows a logistic regression curve for the lung
    cancer scenario.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-7.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-7: A logistic regression curve that predicts cancer based on cigarette
    use*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '*You can apply logistic regression for* multinomial classification *to classify
    the data into more than two classes. To accomplish this, you’ll use the generalization
    of the sigmoid function, called the* softmax function, *which returns a tuple
    of probabilities, one for each class. The sigmoid function transforms the input
    feature(s) into only a single probability value. However, for clarity and readability,
    I’ll focus on* binomial classification *and the sigmoid function in this section.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid function in [Figure 4-7](#ch04fig07) approximates the probability
    that a patient has lung cancer, given the number of cigarettes they smoke. This
    probability helps you make a robust decision on the subject when the only information
    you have is the number of cigarettes the patient smokes: does the patient have
    lung cancer?'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Have a look at the predictions in [Figure 4-8](#ch04fig08), which shows two
    new patients (in light gray at the bottom of the graph). You know nothing about
    them but the number of cigarettes they smoke. You’ve trained our logistic regression
    model (the sigmoid function) that returns a probability value for any new input
    value *x*. If the probability given by the sigmoid function is higher than 50
    percent, the model predicts *lung cancer positive*; otherwise, it predicts *lung
    cancer negative*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-8.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-8: Using logistic regression to estimate probabilities of a result*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the Maximum Likelihood Model**'
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The main question for logistic regression is how to select the correct sigmoid
    function that best fits the training data. The answer is in each model’s *likelihood:*
    the probability that the model would generate the observed training data. You
    want to select the model with the maximum likelihood. Your sense is that this
    model best approximates the real-world process that generated the training data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the likelihood of a given model for a given set of training data,
    you calculate the likelihood for each single training data point, and then multiply
    those with each other to get the likelihood of the whole set of training data.
    How to calculate the likelihood of a single training data point? Simply apply
    this model’s sigmoid function to the training data point; it’ll give you the data
    point’s probability under this model. To select the maximum likelihood model for
    all data points, you repeat this same likelihood computation for different sigmoid
    functions (shifting the sigmoid function a little bit), as in [Figure 4-9](#ch04fig09).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: In the previous paragraph, I described how to determine the maximum likelihood
    sigmoid function (model). This sigmoid function fits the data best—so you can
    use it to predict new data points.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the theory, let’s look at how you’d implement logistic
    regression as a Python one-liner.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-9.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-9: Testing several sigmoid functions to determine maximum likelihood*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You’ve seen an example of using logistic regression for a health application
    (correlating cigarette consumption with cancer probability). This “virtual doc”
    application would be a great idea for a smartphone app, wouldn’t it? Let’s program
    your first virtual doc using logistic regression, as shown in [Listing 4-2](#list4-2)—in
    a single line of Python code!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Listing 4-2: A logistic regression model*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a guess: what’s the output of this code snippet?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training data `X` consists of four patient records (the rows) with two columns.
    The first column holds the number of cigarettes the patients smoke (*input feature*),
    and the second column holds the *class labels*, which say whether they ultimately
    suffered from lung cancer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: You create the model by calling the `LogisticRegression()` constructor. You
    call the `fit()` function on this model; `fit()` takes two arguments, which are
    the input (cigarette consumption) and the output class labels (cancer). The `fit()`
    function expects a two-dimensional input array format with one row per training
    data sample and one column per feature of this training data sample. In this case,
    you have only a single feature value so you transform the one-dimensional input
    into a two-dimensional NumPy array by using the `reshape()` operation. The first
    argument to `reshape()` specifies the number of rows, and the second specifies
    the number of columns. You care about only the number of columns, which here is
    `1`. You’ll pass `-1` as the number of desired rows, which is a special signal
    to NumPy to determine the number of rows automatically.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'The input training data will look as follows after reshaping (in essence, you
    simply remove the class labels and keep the two-dimensional array shape intact):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, you predict whether a patient has lung cancer, given the number of cigarettes
    they smoke: your input will be 2, 12, 13, 40, 90 cigarettes. That gives an output
    as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The model predicts that the first two patients are lung cancer negative, while
    the latter three are lung cancer positive.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look in detail at the probabilities the sigmoid function came up with
    that lead to this prediction! Simply run the following code snippet after [Listing
    4-2](#list4-2):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `predict_proba()` function takes as input the number of cigarettes and
    returns an array containing the probability of lung cancer negative (at index
    0) and the probability of lung cancer positive (index 1). When you run this code,
    you should get the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If the probability of lung cancer being negative is higher than the probability
    of lung cancer being positive, the predicted outcome will be *lung cancer negative*.
    This happens the last time for `x=12`. If the patient has smoked more than 12
    cigarettes, the algorithm will classify them as *lung cancer positive*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, you’ve learned how to classify problems easily with logistic regression
    using the scikit-learn library. The idea of logistic regression is to fit an S-shaped
    curve (the sigmoid function) to the data. This function assigns a numerical value
    between 0 and 1 to every new data point and each possible class. The numerical
    value models the probability of this data point belonging to the given class.
    However, in practice, you often have training data but no class label assigned
    to the training data. For example, you have customer data (say, their age and
    their income) but you don’t know any class label for each data point. To still
    extract useful insights from this kind of data, you will learn about another category
    of machine learning next: unsupervised learning. Specifically, you’ll learn about
    how to find similar clusters of data points, an important subset of unsupervised
    learning.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '**K-Means Clustering in One Line**'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If there’s one clustering algorithm you need to know—whether you’re a computer
    scientist, data scientist, or machine learning expert—it’s the *K-Means algorithm*.
    In this section, you’ll learn the general idea and when and how to use it in a
    single line of Python code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The previous sections covered supervised learning, in which the training data
    is *labeled*. In other words, you know the output value of every input value in
    the training data. But in practice, this isn’t always the case. Often, you’ll
    find yourself confronted with *unlabeled* data—especially in many data analytics
    applications—where it’s not clear what “the optimal output” means. In these situations,
    a prediction is impossible (because there is no output to start with), but you
    can still distill useful knowledge from these unlabeled data sets (for example,
    you can find clusters of similar unlabeled data). Models that use unlabeled data
    fall under the category of *unsupervised learning*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: As an example, suppose you’re working in a startup that serves different target
    markets with various income levels and ages. Your boss tells you to find a certain
    number of target personas that best fit your target markets. You can use clustering
    methods to identify the *average customer personas* that your company serves.
    [Figure 4-10](#ch04fig10) shows an example.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-10.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-10: Observed customer data in the two-dimensional space*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can easily identify three types of personas with different types of
    incomes and ages. But how to find those algorithmically? This is the domain of
    clustering algorithms such as the widely popular K-Means algorithm. Given the
    data sets and an integer *k*, the K-Means algorithm finds *k* clusters of data
    such that the difference between the center of a cluster (called the *centroid*)
    and the data in the cluster is minimal. In other words, you can find the different
    personas by running the K-Means algorithm on your data sets, as shown in [Figure
    4-11](#ch04fig11).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-11.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-11: Customer data with customer personas (cluster centroids) in the
    two-dimensional space*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The cluster centers (black dots) match the clustered customer data. Every cluster
    center can be viewed as one customer persona. Thus, you have three idealized personas:
    a 20-year-old earning $2000, a 25-year-old earning $3000, and a 40-year-old earning
    $4000\. And the great thing is that the K-Means algorithm finds those cluster
    centers even in high-dimensional spaces (where it would be hard for humans to
    find the personas visually).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The K-Means algorithm requires “the number of cluster centers *k*” as an input.
    In this case, you look at the data and “magically” define *k* = 3\. More advanced
    algorithms can find the number of cluster centers automatically (for an example,
    look at the 2004 paper “Learning the k in K-Means” by Greg Hamerly and Charles
    Elkan).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'So how does the K-Means algorithm work? In a nutshell, it performs the following
    procedure:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in multiple loop iterations: you first assign the data to the
    *k* cluster centers, and then you recompute each cluster center as the centroid
    of the data assigned to it.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement it!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following problem: given two-dimensional salary data (*hours worked*,
    *salary earned*), find two clusters of employees in the given data set that work
    a similar number of hours and earn a similar salary.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How can you do all of this in a single line of code? Fortunately, the scikit-learn
    library in Python already has an efficient implementation of the K-Means algorithm.
    [Listing 4-3](#list4-3) shows the one-liner code snippet that runs K-Means clustering
    for you.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 4-3: K-Means clustering in one line*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: What’s the output of this code snippet? Try to guess a solution even if you
    don’t understand every syntactical detail. This will open your knowledge gap and
    prepare your brain to absorb the algorithm much better.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the first lines, you import the `KMeans` module from the `sklearn.cluster`
    package. This module takes care of the clustering itself. You also need to import
    the NumPy library because the `KMeans` module works on NumPy arrays.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Our data is two-dimensional. It correlates the number of working hours with
    the salary of some workers. [Figure 4-12](#ch04fig12) shows the six data points
    in this employee data set.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-12.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-12: Employee salary data*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to find the two cluster centers that best fit this data:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the one-liner, you create a new `KMeans` object that handles the algorithm
    for you. When you create the `KMeans` object, you define the number of cluster
    centers by using the `n_clusters` function argument. Then you simply call the
    instance method `fit(X)` to run the K-Means algorithm on the input data `X`. The
    `KMeans` object now holds all the results. All that’s left is to retrieve the
    results from its attributes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that in the `sklearn` package, the convention is to use a trailing underscore
    for some attribute names (for example, `cluster_centers_`) to indicate that these
    attributes were created dynamically within the training phase (the `fit()` function).
    Before the training phase, these attributes do not exist yet. This is not general
    Python convention (trailing underscores are usually used only to avoid naming
    conflicts with Python keywords—`variable list_` instead of `list`). However, if
    you get used to it, you appreciate the consistent use of attributes in the `sklearn`
    package. So, what are the cluster centers and what is the output of this code
    snippet? Take a look at [Figure 4-13](#ch04fig13).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-13.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-13: Employee salary data with cluster centers in the two-dimensional
    space*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the two cluster centers are (20, 2000) and (50, 7000). This
    is also the result of the Python one-liner. These clusters correspond to two idealized
    employee personas: the first works for 20 hours a week and earns $2000 per month,
    while the second works for 50 hours a week and earns $7000 per month. Those two
    types of personas fit the data reasonably well. Thus, the result of the one-liner
    code snippet is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To summarize, this section introduced you to an important subtopic of unsupervised
    learning: clustering. The K-Means algorithm is a simple, efficient, and popular
    way of extracting *k* clusters from multidimensional data. Behind the scenes,
    the algorithm iteratively recomputes cluster centers and reassigns each data value
    to its closest cluster center until it finds the optimal clusters. But clusters
    are not always ideal for finding similar data items. Many data sets do not show
    a clustered behavior, but you’ll still want to leverage the distance information
    for machine learning and prediction. Let’s stay in the multidimensional space
    and explore another way to use the distance of (Euclidean) data values: the K-Nearest
    Neighbors algorithm.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbors in One Line**'
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The popular *K-Nearest Neighbors* *(KNN)* algorithm is used for regression and
    classification in many applications such as recommender systems, image classification,
    and financial data forecasting. It’s the basis of many advanced machine learning
    techniques (for example, in information retrieval). There is no doubt that understanding
    KNN is an important building block of your proficient computer science education.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The KNN algorithm is a robust, straightforward, and popular machine learning
    method. It’s simple to implement but still a competitive and fast machine learning
    technique. All other machine learning models we’ve discussed so far use the training
    data to compute a *representation* of the original data. You can use this representation
    to predict, classify, or cluster new data. For example, the linear and logistic
    regression algorithms define learning parameters, while the clustering algorithm
    calculates cluster centers based on the training data. However, the KNN algorithm
    is different. In contrast to the other approaches, it does not compute a new model
    (or representation) but uses the *whole data set* as a model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, you read that right. The machine learning model is nothing more than a
    set of observations. Every single instance of your training data is one part of
    your model. This has advantages and disadvantages. A disadvantage is that the
    model can quickly blow up as the training data grows—which may require sampling
    or filtering as a preprocessing step. A great advantage, however, is the simplicity
    of the training phase (just add the new data values to the model). Additionally,
    you can use the KNN algorithm for prediction or classification. You execute the
    following strategy, given your input vector *x*:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Find the *k* nearest neighbors of *x* (according to a predefined distance metric).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the *k* nearest neighbors into a single prediction or classification
    value. You can use any aggregator function such as average, mean, max, or min.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s walk through an example. Your company sells homes for clients. It has
    acquired a large database of customers and house prices (see [Figure 4-14](#ch04fig14)).
    One day, your client asks how much they must expect to pay for a house of 52 square
    meters. You query your KNN model, and it immediately gives you the response $33,167\.
    And indeed, your client finds a home for $33,489 the same week. How did the KNN
    system come to this surprisingly accurate prediction?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: First, the KNN system simply calculates the *k = 3* nearest neighbors to the
    query *D = 52 square meters* using Euclidean distance. The three nearest neighbors
    are A, B, and C with prices $34,000, $33,500, and $32,000, respectively. Then,
    it aggregates the three nearest neighbors by calculating the simple average of
    their values. Because *k = 3* in this example, you denote the model as *3NN*.
    Of course, you can vary the similarity functions, the parameter *k*, and the aggregation
    method to come up with more sophisticated prediction models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-14.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-14: Calculating the price of house D based on the three nearest neighbors
    A, B, and C*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of KNN is that it can be easily adapted as new observations
    are made. This is not generally true for machine learning models. An obvious weakness
    in this regard is that as the computational complexity of finding the *k* nearest
    neighbors becomes harder and harder, the more points you add. To accommodate for
    that, you can continuously remove stale values from the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'As I mentioned, you can also use KNN for classification problems. Instead of
    averaging over the *k* nearest neighbors, you can use a voting mechanism: each
    nearest neighbor votes for its class, and the class with the most votes wins.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s dive into how to use KNN in Python—in a single line of code (see [Listing
    4-4](#list4-4)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Listing 4-4: Running the KNN algorithm in one line of Python*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a guess: what’s the output of this code snippet?'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To help you see the result, let’s plot the housing data from this code in [Figure
    4-15](#ch04fig15).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-15.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-15: Housing data in the two-dimensional space*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Can you see the general trend? With the growing size of your house, you can
    expect a linear growth of its market price. Double the square meters, and the
    price will double too.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In the code (see [Listing 4-4](#list4-4)), the client requests your price prediction
    for a house of 30 square meters. What does KNN with *k = 3* (in short, 3NN) predict?
    Take a look at [Figure 4-16](#ch04fig16).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful, isn’t it? The KNN algorithm finds the three closest houses with respect
    to house size and averages the predicted house price as the average of the *k=3*
    nearest neighbors. Thus, the result is $32,500.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are confused by the data conversions in the one-liner, let me quickly
    explain what is happening here:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![images](Images/fig4-16.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-16: Housing data in the two-dimensional space with predicted house
    price for a new data point (house size equals 30 square meters) using KNN*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: First, you create a new machine learning model called `KNeighborsRegressor`.
    If you wanted to use KNN for classification, you’d use `KNeighborsClassifier`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, you train the model by using the `fit()` function with two parameters.
    The first parameter defines the input (the house size), and the second parameter
    defines the output (the house price). The shape of both parameters must be an
    array-like data structure. For example, to use `30` as an input, you’d have to
    pass it as `[30]`. The reason is that, in general, the input can be multidimensional
    rather than one-dimensional. Therefore, you reshape the input:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that if you were to use this 1D NumPy array as an input to the `fit()`
    function, the function wouldn’t work because it expects an array of (array-like)
    observations, and not an array of integers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, this one-liner taught you how to create your first KNN regressor
    in a single line of code. If you have a lot of changing data and model updates,
    KNN is your best friend! Let’s move on to a wildly popular machine learning model
    these days: neural networks.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '**Neural Network Analysis in One Line**'
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural networks have gained massive popularity in recent years. This is in part
    because the algorithms and learning techniques in the field have improved, but
    also because of the improved hardware and the rise of general-purpose GPU (GPGPU)
    technology. In this section, you’ll learn about the *multilayer perceptron* *(MLP)*
    which is one of the most popular neural network representations. After reading
    this, you’ll be able to write your own neural network in a single line of Python
    code!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For this one-liner, I have prepared a special data set with fellow Python colleagues
    on my email list. My goal was to create a relatable real-world data set, so I
    asked my email subscribers to participate in a data-generation experiment for
    this chapter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**The Data**'
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you’re reading this book, you’re interested in learning Python. To create
    an interesting data set, I asked my email subscribers six anonymized questions
    about their Python expertise and income. The responses to these questions will
    serve as training data for the simple neural network example (as a Python one-liner).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'The training data is based on the answers to the following six questions:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: How many hours have you looked at Python code in the last seven days?
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many years ago did you start to learn about computer science?
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many coding books are on your shelf?
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What percentage of your Python time do you spend working on real-world projects?
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much do you earn per month (round to $1000) from selling your technical
    skills (in the widest sense)?
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s your approximate Finxter rating, rounded to 100 points?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first five questions will be your input, and the sixth question will be
    the output for the neural network analysis. In this one-liner section, you’re
    examining neural network regression. In other words, you predict a numerical value
    (your Python skills) based on numerical input features. We’re not going to explore
    neural network classification in this book, which is another great strength of
    neural networks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The sixth question approximates the skill level of a Python coder. Finxter ([*https://finxter.com/*](https://finxter.com/))
    is our puzzle-based learning application that assigns a rating value to any Python
    coder based on their performance in solving Python puzzles. In this way, it helps
    you quantify your skill level in Python.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with visualizing how each question influences the output (the skill
    rating of a Python developer), as shown in [Figure 4-17](#ch04fig17).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-17.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-17: Relationship between questionnaire answers and the Python skill
    rating at Finxter*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Note that these plots show only how each separate feature (question) impacts
    the final Finxter rating, but they tell you nothing about the impact of a combination
    of two or more features. Note also that some Pythonistas didn’t answer all six
    questions; in those cases, I used the dummy value `-1`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '**What Is an Artificial Neural Network?**'
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The idea of creating a theoretical model of the human brain (the biological
    neural network) has been studied extensively in recent decades. But the foundations
    of artificial neural networks were proposed as early as the 1940s and ’50s! Since
    then, the concept of artificial neural networks has been refined and continually
    improved.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is to break the big task of learning and inference into multiple
    micro-tasks. These micro-tasks are not independent but interdependent. The brain
    consists of billions of neurons that are connected with trillions of synapses.
    In the simplified model, learning is merely adjusting the *strength* of synapses
    (also called *weights* or *parameters* in artificial neural networks). So how
    do you “create” a new synapse in the model? Simple—you increase its weight from
    zero to a nonzero value.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-18](#ch04fig18) shows a basic neural network with three layers (input,
    hidden, output). Each layer consists of multiple neurons that are connected from
    the input layer via the hidden layer to the output layer.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-18: A simple neural network analysis for animal classification*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the neural network is trained to detect animals in images.
    In practice, you would use one input neuron per pixel of the image as an input
    layer. This can result in millions of input neurons that are connected with millions
    of hidden neurons. Often, each output neuron is responsible for one bit of the
    overall output. For example, to detect two different animals (for example, cats
    and dogs), you’ll use only a single neuron in the output layer that can model
    two different states (`0=cat`, `1=dog`).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that each neuron can be activated, or “fired”, when a certain input
    impulse arrives at the neuron. Each neuron decides independently, based on the
    strength of the input impulse, whether to fire or not. This way, you simulate
    the human brain, in which neurons activate each other via impulses. The activation
    of the input neurons propagates through the network until the output neurons are
    reached. Some output neurons will be activated, and others won’t. The specific
    pattern of firing output neurons forms your final output (or prediction) of the
    artificial neural network. In your model, a firing output neuron could encode
    a 1, and a nonfiring output neuron could encode a 0\. This way, you can train
    your neural network to predict anything that can be encoded as a series of 0s
    and 1s (which is everything a computer can represent).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a detailed look at how neurons work mathematically, in [Figure 4-19](#ch04fig19).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-19: Mathematical model of a single neuron: the output is a function
    of the three inputs.*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Each neuron is connected to other neurons, but not all connections are equal.
    Instead, each connection has an associated weight. Formally, a firing neuron propagates
    an impulse of 1 to the outgoing neighbors, while a nonfiring neuron propagates
    an impulse of 0\. You can think of the weight as indicating how much of the impulse
    of the firing input neuron is forwarded to the neuron via the connection. Mathematically,
    you multiply the impulse by the weight of the connection to calculate the input
    for the next neuron. In our example, the neuron simply sums over all inputs to
    calculate its own output. This is the *activation function* that describes how
    exactly the inputs of a neuron generate an output. In our example, a neuron fires
    with higher likelihood if its relevant input neurons fire too. This is how the
    impulses propagate through the neural network.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: What does the learning algorithm do? It uses the training data to select the
    weights *w* of the neural network. Given a training input value *x*, different
    weights *w* lead to different outputs. Hence, the learning algorithm gradually
    changes the weights *w*—in many iterations—until the output layer produces similar
    results as the training data. In other words, the training algorithm gradually
    reduces the error of correctly predicting the training data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: There are many network structures, training algorithms, and activation functions.
    This chapter shows you a hands-on approach of using the neural network now, within
    a single line of code. You can then learn the finer details as you need to improve
    upon this (for example, you could start by reading the “Neural Network” entry
    on Wikipedia, [*https://en.wikipedia.org/wiki/Neural_network*](https://en.wikipedia.org/wiki/Neural_network)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal is to create a neural network that predicts the Python skill level
    (Finxter rating) by using the five input features (answers to the questions):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '**`WEEK`** How many hours have you been exposed to Python code in the last
    seven days?'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '**`YEARS`** How many years ago did you start to learn about computer science?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '**`BOOKS`** How many coding books are on your shelf?'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '**`PROJECTS`** What percentage of your Python time do you spend implementing
    real-world projects?'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '**`EARN`** How much do you earn per month (round to $1000) from selling your
    technical skills (in the widest sense)?'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Again, let’s stand on the shoulders of giants and use the scikit-learn (`sklearn`)
    library for neural network regression, as in [Listing 4-5](#list4-5).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 4-5: Neural network analysis in a single line of code*'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: It’s impossible for a human to correctly figure out the output—but would you
    like to try?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the first few lines, you create the data set. The machine learning algorithms
    in the scikit-learn library use a similar input format. Each row is a single observation
    with multiple features. The more rows, the more training data exists; the more
    columns, the more features of each observation. In this case, you have five features
    for the input and one feature for the output value of each training data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The one-liner creates a neural network by using the constructor of the `MLPRegressor`
    class. I passed `max_iter=10000` as an argument because the training doesn’t converge
    when using the default number of iterations (`max_iter=200`).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: After that, you call the `fit()` function, which determines the parameters of
    the neural network. After calling `fit()`, the neural network has been successfully
    initialized. The `fit()` function takes a multidimensional input array (one observation
    per row, one feature per column) and a one-dimensional output array (size = number
    of observations).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The only thing left is calling the predict function on some input values:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the actual output may vary slightly because of the nondeterministic
    nature of the function and the different convergence behavior.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'In plain English: if . . .'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: . . . you have trained 0 hours in the last week,
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . . you started your computer science studies 0 years ago,
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . . you have 0 coding books in your shelf,
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . . you spend 0 percent of your time implementing real Python projects, and
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . . . you earn $0 selling your coding skills,
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the neural network estimates that your skill level is *very* low (a Finxter
    rating of 94 means you have difficulty understanding the Python program `print("hello,
    world")`).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s change this: what happens if you invest 20 hours a week learning and
    revisit the neural network after one week:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Not bad—your skills improve quite significantly! But you’re still not happy
    with this rating number, are you? (An above-average Python coder has at least
    a 1500–1700 rating on Finxter.)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'No problem. Buy 10 Python books (only nine left after this one). Let’s see
    what happens to your rating:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, you make significant progress and double your rating number! But buying
    Python books alone will not help you much. You need to study them! Let’s do this
    for a year:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Not much happens. This is where I don’t trust the neural network too much.
    In my opinion, you should have reached a much better performance of at least 1500\.
    But this also shows that the neural network can be only as good as its training
    data. You have very limited data, and the neural network can’t really overcome
    this limitation: there’s just too little knowledge in a handful of data points.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'But you don’t give up, right? Next, you spend 50 percent of your Python time
    selling your skills as a Python freelancer:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Boom! Suddenly the neural network considers you to be an expert Python coder.
    A wise prediction of the neural network, indeed! Learn Python for at least a year
    and do practical projects, and you’ll become a great coder.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, you’ve learned about the basics of neural networks and how to use
    them in a single line of Python code. Interestingly, the questionnaire data indicates
    that starting out with practical projects—maybe even doing freelance projects
    from the beginning—matters a lot to your learning success. The neural network
    certainly knows that. If you want to learn my exact strategy of becoming a freelancer,
    join the free webinar about state-of-the-art Python freelancing at [*https://blog.finxter.com/webinar-freelancer/*](https://blog.finxter.com/webinar-freelancer/).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, you’ll dive deeper into another powerful model representation:
    decision trees. While neural networks can be quite expensive to train (they often
    need multiple machines and many hours, and sometimes even weeks, to train), decision
    trees are lightweight. Nevertheless, they are a fast, effective way to extract
    patterns from your training data.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision-Tree Learning in One Line**'
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Decision trees* are powerful and intuitive tools in your machine learning
    toolbelt. A big advantage of decision trees is that, unlike many other machine
    learning techniques, they’re human-readable. You can easily train a decision tree
    and show it to your supervisors, who do not need to know anything about machine
    learning in order to understand what your model does. This is especially great
    for data scientists who often must defend and present their results to management.
    In this section, I’ll show you how to use decision trees in a single line of Python
    code.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike many machine learning algorithms, the ideas behind decision trees might
    be familiar from your own experience. They represent a structured way of making
    decisions. Each decision opens new branches. By answering a bunch of questions,
    you’ll finally land on the recommended outcome. [Figure 4-20](#ch04fig20) shows
    an example.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-20.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-20: A simplified decision tree for recommending a study subject*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are used for classification problems such as “which subject should
    I study, given my interests?” You start at the top. Now, you repeatedly answer
    questions and select the choices that describe your features best. Finally, you
    reach a *leaf node* of the tree, a node with no *children*. This is the recommended
    class based on your feature selection.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Decision-tree learning has many nuances. In the preceding example, the first
    question carries more weight than the last question. If you like math, the decision
    tree will never recommend art or linguistics. This is useful because some features
    may be much more important for the classification decision than others. For example,
    a classification system that predicts your current health may use your sex (feature)
    to practically rule out many diseases (classes).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the order of the decision nodes lends itself to performance optimizations:
    place the features at the top that have a high impact on the final classification.
    In decision-tree learning, you’ll then aggregate the questions with little impact
    on the final classification, as shown in [Figure 4-21](#ch04fig21).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-21.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-21: Pruning improves efficiency of decision-tree learning.*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the full decision tree looks like the tree on the left. For any combination
    of features, there’s a separate classification outcome (the tree leaves). However,
    some features may not give you any additional information with respect to the
    classification problem (for example, the first Language decision node in the example).
    Decision-tree learning would effectively get rid of these nodes for efficiency
    reasons, a process called *pruning*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create your own decision tree in a single line of Python code. [Listing
    4-6](#list4-6) shows you how.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '*Listing 4-6: Decision-tree classification in a single line of code*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Guess the output of this code snippet!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data in this code describes three students with their estimated skill levels
    (a score from 1–10) in the three areas of math, language, and creativity. You
    also know the study subjects of these students. For example, the first student
    is highly skilled in math and studies computer science. The second student is
    skilled in language much more than in the other two skills and studies linguistics.
    The third student is skilled in creativity and studies art.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'The one-liner creates a new decision-tree object and trains the model by using
    the `fit()` function on the labeled training data (the last column is the label).
    Internally, it creates three nodes, one for each feature: math, language, and
    creativity. When predicting the class of `student_0` (math = 8, language = 6,
    creativity = 5), the decision tree returns `computer science`. It has learned
    that this feature pattern (high, medium, medium) is an indicator of the first
    class. On the other hand, when asked for (3, 7, 9), the decision tree predicts
    `art` because it has learned that the score (low, medium, high) hints to the third
    class.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Note that the algorithm is nondeterministic. In other words, when executing
    the same code twice, different results may arise. This is common for machine learning
    algorithms that work with random generators. In this case, the order of the features
    is randomly organized, so the final decision tree may have a different order of
    the features.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, decision trees are an intuitive way of creating human-readable
    machine learning models. Every branch represents a choice based on a single feature
    of a new sample. The leaves of the tree represent the final prediction (classification
    or regression). Next, we’ll leave concrete machine learning algorithms for a moment
    and explore a critical concept in machine learning: variance.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**Get Row with Minimal Variance in One Line**'
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have read about the Vs in Big Data: volume, velocity, variety, veracity,
    and value. *Variance* is yet another important V: it measures the expected (squared)
    deviation of the data from its mean. In practice, variance is an important measure
    with relevant application domains in financial services, weather forecasting,
    and image processing.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Variance measures how much data spreads around its average in the one-dimensional
    or multidimensional space. You’ll see a graphical example in a moment. In fact,
    variance is one of the most important properties in machine learning. It captures
    the patterns of the data in a generalized manner—and machine learning is all about
    pattern recognition.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Many machine learning algorithms rely on variance in one form or another. For
    instance, the *bias-variance trade-off* is a well-known problem in machine learning:
    sophisticated machine learning models risk overfitting the data (high variance)
    but represent the training data very accurately (low bias). On the other hand,
    simple models often generalize well (low variance) but do not represent the data
    accurately (high bias).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'So what exactly is variance? It’s a simple statistical property that captures
    how much the data set spreads from its mean. [Figure 4-22](#ch04fig22) shows an
    example plotting two data sets: one with low variance, and one with high variance.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-22.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-22: Variance comparison of two company stock prices*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: This example shows the stock prices of two companies. The stock price of the
    tech startup fluctuates heavily around its average. The stock price of the food
    company is quite stable and fluctuates only in minor ways around the average.
    In other words, the tech startup has high variance, and the food company has low
    variance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, you can calculate the variance *var(X)* of a set of
    numerical values *X* by using the following formula:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/pg114_equ-01.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: The value ![images](Images/eqn4-5.jpg) is the average value of the data in *X*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As they get older, many investors want to reduce the overall risk of their investment
    portfolio. According to the dominant investment philosophy, you should consider
    stocks with lower variance as less-risky investment vehicles. Roughly speaking,
    you can lose less money investing in a stable, predictable, and large company
    than in a small tech startup.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the one-liner in [Listing 4-7](#list4-7) is to identify the stock
    in your portfolio with minimal variance. By investing more money into this stock,
    you can expect a lower overall variance of your portfolio.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Listing 4-7: Calculating minimum variance in a single line of code*'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: What’s the output of this code snippet?
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As usual, you first define the data you want to run the one-liner on (see the
    top of [Listing 4-7](#list4-7)). The NumPy array `X` contains five rows (one row
    per stock in your portfolio) with four values per row (stock prices).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to find the ID and variance of the stock with minimal variance.
    Hence, the outermost function of the one-liner is the `min()` function. You execute
    the `min()` function on a sequence of tuples `(a,b)`, where the first tuple value
    `a` is the row index (stock index), and the second tuple value `b` is the variance
    of the row.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'You may ask: what’s the minimal value of a sequence of tuples? Of course, you
    need to properly define this operation before using it. To this end, you use the
    `key` argument of the `min()` function. The `key` argument takes a function that
    returns a comparable object value, given a sequence value. Again, our sequence
    values are tuples, and you need to find the tuple with minimal variance (the second
    tuple value). Because variance is the second value, you’ll return `x[1]` as the
    basis for comparison. In other words, the tuple with the minimal second tuple
    value wins.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how to create the sequence of tuple values. You use list comprehension
    to create a tuple for any row index (stock). The first tuple element is simply
    the index of row *i*. The second tuple element is the variance of this row. You
    use the NumPy `var()` function in combination with slicing to calculate the row
    variance.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the one-liner is, therefore, as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'I’d like to add that there’s an alternative way of solving this problem. If
    this wasn’t a book about Python one-liners, I would prefer the following solution
    instead of the one-liner:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the first line, you calculate the variance of the NumPy array `X` along the
    columns (`axis=1`). In the second line, you create the tuple. The first tuple
    value is the index of the minimum in the variance array. The second tuple value
    is the minimum in the variance array. Note that multiple rows may have the same
    (minimal) variance.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: This solution is more readable. So clearly, there is a trade-off between conciseness
    and readability. Just because you can cram everything into a single line of code
    doesn’t mean you should. All things being equal, it’s much better to write concise
    *and* readable code, instead of blowing up your code with unnecessary definitions,
    comments, or intermediate steps.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: After learning the basics of variance in this section, you’re now ready to absorb
    how to calculate basic statistics.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic Statistics in One Line**'
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a data scientist and machine learning engineer, you need to know basic statistics.
    Some machine learning algorithms are entirely based on statistics (for example,
    Bayesian networks).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: For example, extracting basic statistics from matrices (such as average, variance,
    and standard deviation) is a critical component for analyzing a wide range of
    data sets such as financial data, health data, or social media data. With the
    rise of machine learning and data science, knowing about how to use NumPy—which
    is at the heart of Python data science, statistics, and linear algebra—will become
    more and more valuable to the marketplace.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In this one-liner, you’ll learn how to calculate basic statistics with NumPy.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section explains how to calculate the average, the standard deviation,
    and the variance along an axis. These three calculations are very similar; if
    you understand one, you’ll understand all of them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what you want to achieve: given a NumPy array of stock data with rows
    indicating the different companies and columns indicating their daily stock prices,
    the goal is to find the average and standard deviation of each company’s stock
    price (see [Figure 4-23](#ch04fig23)).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-23.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-23: Average and variance along axis 1*'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: This example shows a two-dimensional NumPy array, but in practice, the array
    can have much higher dimensionality.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple Average, Variance, Standard Deviation**'
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Before examining how to accomplish this in NumPy, let’s slowly build the background
    you need to know. Say you want to calculate the simple average, the variance,
    or the standard deviation over all values in a NumPy array. You’ve already seen
    examples of the average and the variance function in this chapter. The standard
    deviation is simply the square root of the variance. You can achieve this easily
    with the following functions:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You may have noted that you apply those functions on the two-dimensional NumPy
    array `X`. But NumPy simply flattens the array and calculates the functions on
    the flattened array. For example, the simple average of the flattened NumPy array
    `X` is calculated as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: (1 + 3 + 5 + 1 + 1 + 1 + 0 + 2 + 4) / 9 = 18 / 9 = 2.0
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating Average, Variance, Standard Deviation Along an Axis**'
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: However, sometimes you want to calculate these functions along an axis. You
    can do this by specifying the keyword `axis` as an argument to the average, variance,
    and standard deviation functions (see [Chapter 3](ch03.xhtml#ch03) for a detailed
    introduction to the `axis` argument).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 4-8](#list4-8) shows you exactly how to calculate the average, variance,
    and standard deviation along an axis. Our goal is to calculate the averages, variances,
    and standard deviations of all stocks in a two-dimensional matrix with rows representing
    stocks and columns representing daily prices.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '*Listing 4-8: Calculating basic statistics along an axis*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Guess the output of the puzzle!
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The one-liner uses the `axis` keyword to specify the axis along which to calculate
    the average, variance, and standard deviation. For example, if you perform these
    three functions along `axis=1`, each row is aggregated into a single value. Hence,
    the resulting NumPy array has a reduced dimensionality of one.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the puzzle is the following:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Before moving on to the next one-liner, I want to show you how to use the same
    idea for an even higher-dimensional NumPy array.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'When averaging along an axis for high-dimensional NumPy arrays, you’ll always
    aggregate the axis defined in the `axis` argument. Here’s an example:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: There are three examples of computing the average, variance, and standard deviation
    along axis 2 (see [Chapter 3](ch03.xhtml#ch03); the innermost axis). In other
    words, all values of axis 2 will be combined into a single value that results
    in axis 2 being dropped from the resulting array. Dive into the three examples
    and figure out how exactly axis 2 is collapsed into a single average, variance,
    or standard deviation value.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, a wide range of data sets (including financial data, health data,
    and social media data) requires you to be able to extract basic insights from
    your data sets. This section gives you a deeper understanding of how to use the
    powerful NumPy toolset to extract basic statistics quickly and efficiently from
    multidimensional arrays. This is needed as a basic preprocessing step for many
    machine learning algorithms.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification with Support-Vector Machines in One Line**'
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Support-vector machines* (*SVMs*) have gained massive popularity in recent
    years because they have robust classification performance, even in high-dimensional
    spaces. Surprisingly, SVMs work even if there are more dimensions (features) than
    data items. This is unusual for classification algorithms because of the *curse
    of dimensionality*: with increasing dimensionality, the data becomes extremely
    sparse, which makes it hard for algorithms to find patterns in the data set. Understanding
    the basic ideas of SVMs is a fundamental step to becoming a sophisticated machine
    learning engineer.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How do classification algorithms work? They use the training data to find a
    decision boundary that divides data in the one class from data in the other class
    (in “Logistic Regression in One Line” on [page 89](#page_89), the decision boundary
    would be whether the probability of the sigmoid function is below or above the
    0.5 threshold).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '**A High-Level Look at Classification**'
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Figure 4-24](#ch04fig24) shows an example of a general classifier.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-24.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-24: Diverse skill sets of computer scientists and artists*'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you want to build a recommendation system for aspiring university students.
    The figure visualizes the training data consisting of users classified according
    to their skills in two areas: logic and creativity. Some people have high logic
    skills and relatively low creativity; others have high creativity and relatively
    low logic skills. The first group is labeled as *computer scientists*, and the
    second group is labeled as *artists*.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: To classify new users, the machine learning model must find a decision boundary
    that separates the computer scientists from the artists. Roughly speaking, you’ll
    classify a user by where they fall with respect to the decision boundary. In the
    example, you’ll classify users who fall into the left area as computer scientists,
    and users who fall into the right area as artists.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: In the two-dimensional space, the decision boundary is either a line or a (higher-order)
    curve. The former is called a *linear classifier*, and the latter is called a
    *nonlinear classifier*. In this section, we’ll explore only linear classifiers.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-24](#ch04fig24) shows three decision boundaries that are all valid
    separators of the data. In our example, it’s impossible to quantify which of the
    given decision boundaries is better; they all lead to perfect accuracy when classifying
    the training data.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '**But What Is the Best Decision Boundary?**'
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Support-vector machines provide a unique and beautiful answer to this question.
    Arguably, the best decision boundary provides a maximal margin of safety. In other
    words, SVMs maximize the distance between the closest data points and the decision
    boundary. The goal is to minimize the error of new points that are close to the
    decision boundary.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-25](#ch04fig25) shows an example.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-25.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-25: Support-vector machines maximize the error of margin.*'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: The SVM classifier finds the respective support vectors so that the zone between
    the support vectors is as thick as possible. Here, the support vectors are the
    data points that lie on the two dotted lines parallel to the decision boundary.
    These lines are denoted as *margins*. The decision boundary is the line in the
    middle with maximal distance to the margins. Because the zone between the margins
    and the decision boundary is maximized, the *margin of error* is expected to be
    maximal when classifying new data points. This idea shows high classification
    accuracy for many practical problems.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Is it possible to create your own SVM in a single line of Python code? Take
    a look at [Listing 4-9](#list4-9).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*Listing 4-9: SVM classification in a single line of code*'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Guess the output of this code.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code breaks down how you can use support-vector machines in Python in the
    most basic form. The NumPy array holds the labeled training data with one row
    per user and one column per feature (skill level in math, language, and creativity).
    The last column is the label (the class).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Because you have three-dimensional data, the support-vector machine separates
    the data by using two-dimensional planes (the linear separator) rather than one-dimensional
    lines. As you can see, it’s also possible to separate three classes rather than
    only two as shown in the preceding examples.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'The one-liner itself is straightforward: you first create the model by using
    the constructor of the `svm.SVC` class (*SVC* stands for *support-vector classification*).
    Then, you call the `fit()` function to perform the training based on your labeled
    training data.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: In the results part of the code snippet, you call the `predict()` function on
    new observations. Because `student_0` has skills indicated as math=3, language=3,
    and creativity=6, the support-vector machine predicts that the label *art* fits
    this student’s skills. Similarly, `student_1` has skills indicated as math=8,
    language=1, and creativity=1\. Thus, the support-vector machine predicts that
    the label *computer science* fits this student’s skills.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the final output of the one-liner:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In summary, SVMs perform well even in high-dimensional spaces when there are
    more features than training data vectors. The idea of maximizing the *margin of
    safety* is intuitive and leads to robust performance when classifying *boundary
    cases*—that is, vectors that fall within the margin of safety. In the final section
    of this chapter, we’ll zoom one step back and have a look at a meta-algorithm
    for classification: ensemble learning with random forests.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification with Random Forests in One Line**'
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s move on to an exciting machine learning technique: *ensemble learning*.
    Here’s my quick-and-dirty tip if your prediction accuracy is lacking but you need
    to meet the deadline at all costs: try this meta-learning approach that combines
    the predictions (or classifications) of multiple machine learning algorithms.
    In many cases, it will give you better last-minute results.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '***The Basics***'
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous sections, you’ve studied multiple machine learning algorithms
    that you can use to get quick results. However, different algorithms have different
    strengths. For example, neural network classifiers can generate excellent results
    for complex problems. However, they are also prone to overfitting the data because
    of their powerful capacity to memorize fine-grained patterns of the data. Ensemble
    learning for classification problems partially overcomes the problem that you
    often don’t know in advance which machine learning technique works best.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: How does this work? You create a meta-classifier consisting of multiple types
    or instances of basic machine learning algorithms. In other words, you train multiple
    models. To classify a single observation, you ask all models to classify the input
    independently. Next, you return the class that was returned most often, given
    your input, as a *meta-prediction*. This is the final output of your ensemble
    learning algorithm.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '*Random forests* are a special type of ensemble learning algorithms. They focus
    on decision-tree learning. A forest consists of many trees. Similarly, a random
    forest consists of many decision trees. Each decision tree is built by injecting
    randomness in the tree-generation procedure during the training phase (for example,
    which tree node to select first). This leads to various decision trees—exactly
    what you want.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-26](#ch04fig26) shows how the prediction works for a trained random
    forest using the following scenario. Alice has high math and language skills.
    The *ensemble* consists of three decision trees (building a random forest). To
    classify Alice, each decision tree is queried about Alice’s classification. Two
    of the decision trees classify Alice as a computer scientist. Because this is
    the class with the most votes, it’s returned as the final output for the classification.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![images](Images/fig4-26.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-26: Random forest classifier aggregating the output of three decision
    trees*'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '***The Code***'
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s stick to this example of classifying the study field based on a student’s
    skill level in three areas (math, language, creativity). You may think that implementing
    an ensemble learning method is complicated in Python. But it’s not, thanks to
    the comprehensive scikit-learn library (see [Listing 4-10](#list4-10)).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*Listing 4-10: Ensemble learning with random forest classifiers*'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a guess: what’s the output of this code snippet?'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After initializing the labeled training data in [Listing 4-10](#list4-10), the
    code creates a random forest by using the constructor on the class `RandomForestClassifier`
    with one parameter `n_estimators` that defines the number of trees in the forest.
    Next, you populate the model that results from the previous initialization (an
    empty forest) by calling the function `fit()`. To this end, the input training
    data consists of all but the last column of array `X`, while the labels of the
    training data are defined in the last column. As in the previous examples, you
    use slicing to extract the respective columns from the data array `X`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: The classification part is slightly different in this code snippet. I wanted
    to show you how to classify multiple observations instead of only one. You can
    achieve this here by creating a multidimensional array with one row per observation.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the output of the code snippet:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Note that the result is still nondeterministic (the result may be different
    for different executions of the code) because the random forest algorithm relies
    on the random number generator that returns different numbers at different points
    in time. You can make this call deterministic by using the integer argument `random_state`.
    For example, you can set `random_state=1` when calling the random forest constructor:
    `RandomForestClassifier(n_estimators=10, random_state=1)`. In this case, each
    time you create a new random forest classifier, the same output results because
    the same random numbers are created: they are all based on the seed integer 1.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, this section introduced a meta-approach for classification: using
    the output of various decision trees to reduce the variance of the classification
    error. This is one version of ensemble learning, which combines multiple basic
    models into a single meta-model that’s able to leverage their individual strengths.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '*Two different decision trees can lead to a high variance of the error: one
    generates good results, while the other one doesn’t. By using random forests,
    you mitigate this effect.*'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'Variations of this idea are common in machine learning—and if you need to quickly
    improve your prediction accuracy, simply run multiple machine learning models
    and evaluate their output to find the best one (a quick-and-dirty secret of machine
    learning practitioners). In a way, ensemble learning techniques automatically
    perform the task that’s often done by experts in practical machine learning pipelines:
    selecting, comparing, and combining the output of different machine learning models.
    The big strength of ensemble learning is that this can be done individually for
    each data value at runtime.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter covered 10 basic machine learning algorithms that are fundamental
    to your success in the field. You’ve learned about regression algorithms to predict
    values such as linear regression, KNNs, and neural networks. You’ve learned about
    classification algorithms such as logistic regression, decision-tree learning,
    SVMs, and random forests. Furthermore, you’ve learned how to calculate basic statistics
    of multidimensional data arrays, and to use the K-Means algorithm for unsupervised
    learning. These algorithms and methods are among the most important algorithms
    in the field of machine learning, and there are a lot more to study if you want
    to start working as a machine learning engineer. That learning will pay off—machine
    learning engineers usually earn six figures in the United States (a simple web
    search should confirm this)! For students who want to dive deeper into machine
    learning, I recommend the excellent (and free) Coursera course from Andrew Ng.
    You can find the course material online by asking your favorite search engine.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, you’ll study one of the most important (and most undervalued)
    skills of highly efficient programmers: regular expressions. While this chapter
    was a bit more on the conceptual side (you learned the general ideas, but the
    scikit-learn library did the heavy lifting), the next chapter will be highly technical.
    So, roll up your sleeves and read on!'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
