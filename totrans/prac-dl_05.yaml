- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5'
- en: BUILDING DATASETS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 构建数据集**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: The previous chapter had a lot of detailed advice. Now let’s put it all into
    practice to build the datasets we’ll use throughout the remainder of the book.
    Some of these datasets are well suited to traditional models, because they consist
    of feature vectors. Others are better suited to deep learning models that work
    with multidimensional inputs—in particular, images, or things that can be visualized
    as images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章提供了很多详细的建议。现在让我们将这些建议付诸实践，构建在本书剩余部分将使用的数据集。这些数据集有些非常适合传统模型，因为它们由特征向量组成。而另一些则更适合使用多维输入的深度学习模型——特别是图像，或是可以视作图像的东西。
- en: We’ll work through acquiring the raw data and preprocessing the data to make
    it suitable for our tools. We won’t make actual training/validation/test splits
    until we use these datasets for specific models. It is worth noting here that
    preprocessing the data to make it suitable for a model is often one of the most
    labor-intensive of machine learning tasks. All the same, if it is not done, or
    not done well, your model may end up being far less useful than you want it to
    be.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过获取原始数据并对其进行预处理，使其适合我们的工具进行处理。在我们将这些数据集应用于具体模型之前，不会进行实际的训练/验证/测试划分。值得注意的是，预处理数据以使其适合模型，通常是机器学习中最为繁重的任务之一。尽管如此，如果这一步没有做好，或者做得不充分，你的模型可能会远不如预期有用。
- en: Irises
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 鸢尾花
- en: 'Perhaps the most classic of all machine learning datasets is the iris flower
    dataset, developed in 1936 by R. A. Fisher in his paper, “The Use of Multiple
    Measurements in Taxonomic Problems.” It’s a small dataset of three classes with
    50 samples in each class. There are four features: sepal width, sepal length,
    petal width, and petal length, all in centimeters. The three classes are *I. setosa*,
    *I. versicolour*, and *I. virginica*. This dataset is built into sklearn, but
    we’ll instead download it from the University of California, Irvine, Machine Learning
    Repository to practice working with externally sourced data and introduce a rich
    collection of datasets suitable for many traditional machine learning models.
    The main repository is located at *[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)*,
    but you can download the irises dataset directly from *[https://archive.ics.uci.edu/ml/datasets/iris/](https://archive.ics.uci.edu/ml/datasets/iris/)*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 也许所有机器学习数据集中最经典的是鸢尾花数据集，它由R. A. Fisher于1936年在论文《多重测量在分类问题中的应用》中提出。这是一个包含三个类别的小型数据集，每个类别有50个样本。数据集有四个特征：花萼宽度、花萼长度、花瓣宽度和花瓣长度，单位为厘米。这三个类别是*I.
    setosa*、*I. versicolor*和*I. virginica*。该数据集已内置于sklearn中，但我们将从加利福尼亚大学欧文分校机器学习库下载它，以便练习使用外部数据源，并引入一个适合许多传统机器学习模型的数据集合集。主库位于*[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)*，但你可以直接从*[https://archive.ics.uci.edu/ml/datasets/iris/](https://archive.ics.uci.edu/ml/datasets/iris/)*下载鸢尾花数据集。
- en: 'At the time of this writing, this dataset has been downloaded nearly 1.8 million
    times. You can download it by selecting the **Data Folder** link near the top
    of the page, then right-clicking and saving the *iris.data* file, ideally to a
    new directory called *iris*. Let’s take a look at the start of this file:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，该数据集已被下载近180万次。你可以通过点击页面顶部附近的**数据文件夹**链接，然后右键点击并保存*iris.data*文件，最好保存到一个名为*iris*的新目录中。我们来看一下这个文件的开头：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Because the class names at the end of each line are all the same, we should
    immediately suspect that the samples are sorted by class. Looking at the rest
    of the file confirms this. So, as emphasized in [Chapter 4](ch04.xhtml#ch04),
    we must be sure to randomize the data before training a model. Also, we need to
    replace the class names with integer labels. We can load the dataset into Python
    with the script in [Listing 5-1](ch05.xhtml#ch5lis1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每行末尾的类别名称都相同，我们应该立即怀疑样本是按类别排序的。查看文件的其余部分证实了这一点。因此，正如[第4章](ch04.xhtml#ch04)中强调的那样，我们必须在训练模型之前确保对数据进行随机化。此外，我们还需要将类别名称替换为整数标签。我们可以通过[清单5-1](ch05.xhtml#ch5lis1)中的脚本将数据集加载到Python中。
- en: import numpy as np
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: '❶ with open("iris.data") as f:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ with open("iris.data") as f:'
- en: lines = [i[:-1] for i in f.readlines()]
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: lines = [i[:-1] for i in f.readlines()]
- en: ❷ n = ["Iris-setosa","Iris-versicolor","Iris-virginica"]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ n = ["Iris-setosa", "Iris-versicolor", "Iris-virginica"]
- en: x = [n.index(i.split(",")[-1]) for i in lines if i != ""]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: x = [n.index(i.split(",")[-1]) for i in lines if i != ""]
- en: x = np.array(x, dtype="uint8")
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.array(x, dtype="uint8")
- en: ❸ y = [[float(j) for j in i.split(",")[:-1]] for i in lines if i != ""]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ y = [[float(j) for j in i.split(",")[:-1]] for i in lines if i != ""]
- en: y = np.array(y)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.array(y)
- en: ❹ i = np.argsort(np.random.random(x.shape[0]))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ i = np.argsort(np.random.random(x.shape[0]))
- en: x = x[i]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: ❺ np.save("iris_features.npy", y)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ np.save("iris_features.npy", y)
- en: np.save("iris_labels.npy", x)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_labels.npy", x)
- en: '*Listing 5-1: Loading the raw iris dataset and mapping to our standard format*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 5-1：加载原始的鸢尾花数据集并映射到我们的标准格式*'
- en: First, we load the text file containing the data. The list comprehension removes
    the extraneous newline character ❶. Next, we create the vector of labels by converting
    the text label into an integer, 0–2\. The last element in the list, created by
    splitting a line along commas, is the text label. We want NumPy arrays, so we
    turn the list into one. The `uint8` is unnecessary, but since the labels are never
    negative and they’re never larger than 2, we save a bit of space by making the
    data type an unsigned 8-bit integer ❷.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载包含数据的文本文件。列表推导式去除了多余的换行符❶。接下来，我们通过将文本标签转换为整数（0–2）来创建标签向量。通过按逗号拆分每行，列表中的最后一个元素是文本标签。我们需要的是
    NumPy 数组，因此将列表转换为一个。`uint8` 类型是多余的，但由于标签从不为负且不超过2，我们通过将数据类型设为无符号8位整数来节省一些空间❷。
- en: Creating the feature vectors as a 150-row by 4-column matrix comes next via
    a double list comprehension. The outer comprehension (`i`) moves over lines from
    the file, and the inner one (`j`) takes the list of measurements for each sample
    and turns them into floating-point numbers. We then convert the list of lists
    into a 2D NumPy array ❸. We finish by randomizing the dataset as we did previously
    ❹, and, finally, we write the NumPy arrays to disk so we can use them later ❺.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来通过双重列表推导式创建150行4列的特征向量。外部推导式（`i`）遍历文件中的每一行，内部推导式（`j`）将每个样本的测量值转换为浮动小数。然后，我们将这个列表列表转换为2D
    NumPy数组❸。最后，我们像之前一样随机化数据集❹，并最终将 NumPy 数组写入磁盘，以便稍后使用❺。
- en: '[Figure 5-1](ch05.xhtml#ch5fig1) shows a box plot of the features. This is
    a well-behaved dataset, but the second feature does have some possible outliers.
    Because the features all have similar scales, we’ll use the features as they are.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](ch05.xhtml#ch5fig1) 显示了特征的箱线图。这个数据集表现良好，但第二个特征确实可能存在一些离群值。由于所有特征的尺度相似，我们将保持特征原样使用。'
- en: '![image](Images/05fig01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig01.jpg)'
- en: '*Figure 5-1: Box plot of the four iris dataset features*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-1：四个鸢尾花数据集特征的箱线图*'
- en: Breast Cancer
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乳腺癌
- en: Our second dataset, the Wisconsin Diagnostic Breast Cancer dataset, is also
    in sklearn, and you can also download it from the UCI Machine Learning Repository.
    We’ll follow the preceding procedure and download the dataset to see how to process
    it. This seems unnecessary, true, but just as it’s crucial to build a good dataset
    to have any hope of training a good model, it’s equally important to learn how
    to work with data sources that are not in the format we want. Should you one day
    decide to make machine learning and data science a career, you’ll be faced with
    this issue on a near-daily basis.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个数据集，威斯康星乳腺癌诊断数据集，也包含在 sklearn 中，你也可以从 UCI 机器学习库下载。我们将遵循之前的步骤，下载数据集并查看如何处理它。虽然看起来这似乎没有必要，但正如构建一个良好的数据集对于训练一个好模型至关重要一样，学习如何处理格式不符合我们需求的数据源同样重要。如果有一天你决定将机器学习和数据科学作为职业，你将几乎每天都会遇到这个问题。
- en: Download the dataset by going to *[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/)*.
    Then, click the **Data Folder** link, and save the *wdbc.data* file.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据集，访问 *[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/)*。然后，点击
    **Data Folder** 链接，保存 *wdbc.data* 文件。
- en: 'This dataset contains cell measurements taken from slides of fine-needle biopsies
    of breast masses. There are 30 continuous features and two classes: malignant
    (cancer, 212 samples) and benign (no cancer, 357 samples). This is also a popular
    dataset, with over 670,000 downloads. The first line of the file is shown here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含从乳腺肿块的细针穿刺活检切片中获取的细胞测量数据。数据集包含30个连续特征和两个类别：恶性（癌症，212个样本）和良性（无癌症，357个样本）。这个数据集也非常流行，下载量超过67万次。文件的第一行如下所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first element in that line is a patient ID number that we don’t need to
    worry about. The second element is the label—*M* for malignant, and *B* for benign.
    The rest of the numbers in the line are 30 measurements related to cell size.
    The features themselves are of different scales, so besides creating the raw dataset,
    we’ll also create a standardized version. As this is the entirety of the dataset
    and we’ll have to hold some of it back for testing, we don’t need to record the
    per feature means and standard deviations in this case. If we were able to acquire
    more data generated in the same way, perhaps from an old file that was forgotten
    about, we would need to keep these values so that we could standardize the new
    inputs. The script to build this dataset, and to generate a summary box plot,
    is in [Listing 5-2](ch05.xhtml#ch5lis2).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那一行的第一个元素是病人的ID号，我们不需要担心。第二个元素是标签——*M*代表恶性，*B*代表良性。该行的其余数字是与细胞大小相关的30个测量值。特征本身的量纲不同，因此除了创建原始数据集外，我们还将创建一个标准化版本。由于这就是数据集的全部内容，并且我们将需要保留一部分数据用于测试，所以在这种情况下我们不需要记录每个特征的均值和标准差。如果我们能够获得更多类似的生成数据，可能来自一个被遗忘的旧文件，我们就需要保留这些值，以便能够标准化新的输入。构建此数据集并生成汇总箱形图的脚本在[清单
    5-2](ch05.xhtml#ch5lis2)中。
- en: import numpy as np
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pyplot as plt
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: '❶ with open("wdbc.data") as f:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ with open("wdbc.data") as f:'
- en: lines = [i[:-1] for i in f.readlines() if i != ""]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: lines = [i[:-1] for i in f.readlines() if i != ""]
- en: ❷ n = ["B","M"]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ n = ["B", "M"]
- en: x = np.array([n.index(i.split(",")[1]) for i in lines],dtype="uint8")
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.array([n.index(i.split(",")[1]) for i in lines], dtype="uint8")
- en: y = np.array([[float(j) for j in i.split(",")[2:]] for i in lines])
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.array([[float(j) for j in i.split(",")[2:]] for i in lines])
- en: i = np.argsort(np.random.random(x.shape[0]))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.argsort(np.random.random(x.shape[0]))
- en: x = x[i]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: z = (y - y.mean(axis=0)) / y.std(axis=0)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: z = (y - y.mean(axis=0)) / y.std(axis=0)
- en: ❸ np.save("bc_features.npy", y)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ np.save("bc_features.npy", y)
- en: np.save("bc_features_standard.npy", z)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("bc_features_standard.npy", z)
- en: np.save("bc_labels.npy", x)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("bc_labels.npy", x)
- en: plt.boxplot(z)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: plt.boxplot(z)
- en: plt.show()
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: '*Listing 5-2: Loading the raw breast cancer dataset*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-2：加载原始乳腺癌数据集*'
- en: The first thing we do is read in the raw text data ❶. We then extract each label
    and map it to 0 for benign and 1 for malignant. Note here that we used 1 for the
    natural target case, so that a model outputting a probability value is indicating
    likelihood of finding cancer ❷. We extract the 30 features per sample as floats
    using a nested list comprehension to first pull out the text of the features (`i`)
    and then map them to floats (`j`). This produces a nested list, which NumPy conveniently
    converts into a matrix of 569 rows and 30 columns.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先做的是读取原始文本数据 ❶。然后，我们提取每个标签，并将其映射为0表示良性，1表示恶性。请注意，这里我们使用1表示自然目标情况，这样输出概率值的模型就可以指示是否发现癌症的可能性
    ❷。我们使用嵌套的列表推导式提取每个样本的30个特征值，并将它们转换为浮动类型（`i`是特征的文本值，`j`是转换后的浮动值）。这将生成一个嵌套列表，NumPy将其方便地转换成一个包含569行和30列的矩阵。
- en: Next, we randomize the dataset and calculate the standardized version by subtracting,
    per feature, the mean value of that feature and dividing by the standard deviation.
    We’ll work with this version and examine it in the box plot of [Figure 5-2](ch05.xhtml#ch5fig2)
    ❸, which shows all 30 features after standardization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对数据集进行随机化，并通过减去每个特征的均值并除以标准差来计算标准化版本。我们将使用这个版本，并在[图 5-2](ch05.xhtml#ch5fig2)
    ❸的箱形图中对其进行分析，该图展示了所有30个特征标准化后的结果。
- en: '![image](Images/05fig02.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig02.jpg)'
- en: '*Figure 5-2: Box plot of the 30 breast cancer dataset features*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-2：30个乳腺癌数据集特征的箱形图*'
- en: 'We don’t need to know in this case what the features represent. We’ll work
    with the dataset under the assumption that the selected features are sufficient
    to the task of determining malignancy. Our models will indicate to us whether
    or not this is the case. The features are now all of the same scale as we can
    see by the location of the boxes on the y-axis: they’re all covering basically
    the same range. One characteristic of the data is immediately evident—namely,
    that there are many apparent outliers, as called out by the interquartile range
    (see [Figure 4-6](ch04.xhtml#ch4fig6)). These aren’t necessarily bad values, but
    they are an indicator that the data isn’t normally distributed—it doesn’t, per
    feature, follow a bell-curve-type distribution.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不需要知道特征代表什么。我们将假设所选的特征对于确定恶性程度的任务是足够的。我们的模型将告诉我们这种假设是否成立。现在，这些特征的尺度都是相同的，从我们在
    y 轴上看到的框的位置可以看出：它们基本覆盖了相同的范围。数据的一个显著特征立刻显现出来——即，存在许多明显的异常值，正如四分位数范围所指出的（见[图 4-6](ch04.xhtml#ch4fig6)）。这些值不一定是错误的，但它们表明数据并不是正态分布的——每个特征都没有呈现钟形曲线型分布。
- en: MNIST Digits
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST 手写数字
- en: Our next dataset isn’t typically composed of feature vectors, but is instead
    made up of thousands of small images of handwritten digits. This dataset is the
    workhorse of modern machine learning, and one of the first datasets deep learning
    researchers go to when looking to test new ideas. It’s overused, but that’s because
    it’s so well understood and simple to work with.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个数据集通常不是由特征向量组成的，而是由成千上万张手写数字的小图像构成。这个数据集是现代机器学习中的工作马，也是深度学习研究者在测试新想法时首选的第一个数据集。它虽然过于使用，但也正因为它如此容易理解且便于使用。
- en: The dataset has a long history, but the version we’ll use, the most common version,
    is known simply as the *MNIST dataset*. The canonical source for the dataset,
    [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/), includes
    some background material. To save time, we’ll use Keras to download and format
    the dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有着悠久的历史，但我们将使用的版本——最常见的版本——被简单地称为*MNIST 数据集*。该数据集的标准来源，[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)，包含了一些背景材料。为了节省时间，我们将使用
    Keras 来下载并格式化数据集。
- en: Keras will return the dataset as 3D NumPy arrays. The first dimension is the
    number of images—60,000 for training and 10,000 for test. The second and third
    dimensions are the pixels of the images. The images are 28×28 pixels in size.
    Each pixel is an unsigned 8-bit integer, [0,255].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 会将数据集以 3D NumPy 数组的形式返回。第一维是图像的数量——60,000 张用于训练，10,000 张用于测试。第二维和第三维是图像的像素。图像的大小为
    28×28 像素。每个像素是一个无符号 8 位整数，[0,255]。
- en: Because we want to work with models that expect vectors as inputs, and because
    we want to use this dataset to illustrate certain properties of models later in
    the book, we’ll create additional datasets from this initial one. To do so, first
    we’ll unravel the images to form feature vectors so that we can use this dataset
    with traditional models that expect vector inputs. Second, we’ll use images, but
    we’ll permute the order of the images in the dataset. We’ll permute the order
    of the pixels of each image in the same way, so while the pixels will no longer
    be in the order that produces the digit image, the reordering will be deterministic,
    and applied consistently across all images. Third, we’ll create an unraveled feature
    vector version of these permuted images. We’ll use these additional datasets to
    explore differences between traditional neural networks and convolutional neural
    network models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想要使用期望输入为向量的模型，并且希望使用这个数据集来说明书中后续章节中模型的一些特性，我们将从这个初始数据集中创建额外的数据集。为此，首先我们将展开图像以形成特征向量，这样我们就可以将这个数据集与传统模型一起使用，这些模型期望输入为向量。其次，我们将使用图像，但我们会打乱数据集中图像的顺序。我们将以相同的方式打乱每张图像的像素顺序，因此，尽管这些像素不再按照生成数字图像的顺序排列，但这种重排是确定性的，并且会在所有图像中一致地应用。第三，我们将创建这些打乱图像的展开特征向量版本。我们将使用这些额外的数据集来探索传统神经网络与卷积神经网络模型之间的差异。
- en: Use [Listing 5-3](ch05.xhtml#ch5lis3) to build the dataset files.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[清单 5-3](ch05.xhtml#ch5lis3)来构建数据集文件。
- en: import numpy as np
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import keras
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.datasets import mnist
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.datasets import mnist
- en: ❶ (xtrn, ytrn), (xtst, ytst) = mnist.load_data()
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (xtrn, ytrn), (xtst, ytst) = mnist.load_data()
- en: idx = np.argsort(np.random.random(ytrn.shape[0]))
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytrn.shape[0]))
- en: xtrn = xtrn[idx]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn = xtrn[idx]
- en: ytrn = ytrn[idx]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = ytrn[idx]
- en: idx = np.argsort(np.random.random(ytst.shape[0]))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytst.shape[0]))
- en: xtst = xtst[idx]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: xtst = xtst[idx]
- en: ytst = ytst[idx]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = ytst[idx]
- en: np.save("mnist_train_images.npy", xtrn)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_images.npy", xtrn)
- en: np.save("mnist_train_labels.npy", ytrn)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_labels.npy", ytrn)
- en: np.save("mnist_test_images.npy", xtst)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_images.npy", xtst)
- en: np.save("mnist_test_labels.npy", ytst)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_labels.npy", ytst)
- en: ❷ xtrnv = xtrn.reshape((60000,28*28))
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ xtrnv = xtrn.reshape((60000,28*28))
- en: xtstv = xtst.reshape((10000,28*28))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: xtstv = xtst.reshape((10000,28*28))
- en: np.save("mnist_train_vectors.npy", xtrnv)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_vectors.npy", xtrnv)
- en: np.save("mnist_test_vectors.npy", xtstv)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_vectors.npy", xtstv)
- en: ❸ idx = np.argsort(np.random.random(28*28))
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ idx = np.argsort(np.random.random(28*28))
- en: 'for i in range(60000):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(60000):'
- en: xtrnv[i,:] = xtrnv[i,idx]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: xtrnv[i,:] = xtrnv[i,idx]
- en: 'for i in range(10000):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10000):'
- en: xtstv[i,:] = xtstv[i,idx]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: xtstv[i,:] = xtstv[i,idx]
- en: np.save("mnist_train_scrambled_vectors.npy", xtrnv)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_scrambled_vectors.npy", xtrnv)
- en: np.save("mnist_test_scrambled_vectors.npy", xtstv)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_scrambled_vectors.npy", xtstv)
- en: ❹ t = np.zeros((60000,28,28))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ t = np.zeros((60000,28,28))
- en: 'for i in range(60000):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(60000):'
- en: t[i,:,:] = xtrnv[i,:].reshape((28,28))
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: t[i,:,:] = xtrnv[i,:].reshape((28,28))
- en: np.save("mnist_train_scrambled_images.npy", t)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_train_scrambled_images.npy", t)
- en: t = np.zeros((10000,28,28))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.zeros((10000,28,28))
- en: 'for i in range(10000):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10000):'
- en: t[i,:,:] = xtstv[i,:].reshape((28,28))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: t[i,:,:] = xtstv[i,:].reshape((28,28))
- en: np.save("mnist_test_scrambled_images.npy", t)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("mnist_test_scrambled_images.npy", t)
- en: '*Listing 5-3: Loading and building the various MNIST datasets*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-3：加载和构建各种 MNIST 数据集*'
- en: We start by telling Keras to load the MNIST dataset ❶. When run for the first
    time, Keras will show a message about downloading the dataset. After that, it
    won’t show the message again.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先告诉 Keras 加载 MNIST 数据集❶。当第一次运行时，Keras 会显示一条关于下载数据集的消息。之后，它不会再显示此消息。
- en: The dataset itself is stored in four NumPy arrays. The first, `xtrn`, has a
    shape of (60000, 28, 28) for the 60,000 training images, each 28×28 pixels. The
    associated labels are in `ytrn` as integers, [0,9]. The 10,000 test images are
    in `xtst` with labels in `ytst`. We also randomize the order of the samples and
    write the arrays to disk for future use.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集本身存储在四个 NumPy 数组中。第一个 `xtrn` 具有形状 (60000, 28, 28)，用于存储60,000张训练图像，每张28×28像素。相应的标签存储在
    `ytrn` 中，标签为整数 [0,9]。10,000张测试图像存储在 `xtst` 中，标签存储在 `ytst`。我们还对样本的顺序进行了随机化，并将这些数组写入磁盘以便日后使用。
- en: Next, we unravel the training and test images and turn them into vectors of
    784 elements ❷. Unraveling takes the first row of pixels followed by the second
    row and so on until all rows are laid end to end. We get 784 elements because
    28 × 28 = 784.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练和测试图像展开，并将其转换为包含784个元素的向量❷。展开操作将第一行像素放在前面，然后是第二行，以此类推，直到所有行按顺序排列在一起。由于28
    × 28 = 784，因此我们得到784个元素。
- en: Following this, we generate a permutation of the 784 elements in the unraveled
    vectors (`idx`) ❸.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成784个元素的无序向量排列（`idx`）❸。
- en: We use the permuted vectors to form new, scrambled, digit images and store them
    on disk ❹. The scrambled images are made from the scrambled vectors by undoing
    the unravel operation. In NumPy, this is just a call to the `reshape` method of
    the vector arrays. Note that at no time do we alter the relative ordering of the
    images, so we need to store only one file each for the train and test labels.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用排列后的向量来形成新的、打乱的数字图像并将其存储到磁盘上❹。打乱的图像通过对打乱后的向量进行反向展开操作生成。在 NumPy 中，这只是对向量数组调用
    `reshape` 方法的过程。请注意，我们在任何时候都没有改变图像的相对顺序，因此我们只需要为训练集和测试集标签各存储一个文件。
- en: '[Figure 5-3](ch05.xhtml#ch5fig3) shows representative digits from the MNIST
    dataset.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-3](ch05.xhtml#ch5fig3) 显示了 MNIST 数据集中的代表性数字。'
- en: '![image](Images/05fig03.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig03.jpg)'
- en: '*Figure 5-3: Representative MNIST digit images*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-3：MNIST 代表性数字图像*'
- en: We don’t need to standardize the images, as we know they’re all on the same
    scale already, since they’re pixels. We’ll sometimes scale them as we use them,
    but for now we can leave them on disk as byte grayscale images. The dataset is
    reasonably balanced; [Table 5-1](ch05.xhtml#ch5tab1) shows the training distribution.
    Therefore, we don’t need to worry about imbalanced data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要对图像进行标准化，因为我们知道它们已经处于相同的尺度，因为它们是像素值。我们在使用时会对它们进行缩放，但现在可以将它们保留为字节灰度图像存储在磁盘上。数据集相对平衡；[表
    5-1](ch05.xhtml#ch5tab1) 显示了训练集的分布。因此，我们不需要担心数据不平衡问题。
- en: '**Table 5-1:** Digit Frequencies for the MNIST Training Set'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5-1：** MNIST 训练集的数字频率'
- en: '| **Digit** | **Count** |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **数字** | **计数** |'
- en: '| --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | 5,923 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 5,923 |'
- en: '| 1 | 6,742 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6,742 |'
- en: '| 2 | 5,958 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5,958 |'
- en: '| 3 | 6,131 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6,131 |'
- en: '| 4 | 5,842 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5,842 |'
- en: '| 5 | 5,421 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5,421 |'
- en: '| 6 | 5,918 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5,918 |'
- en: '| 7 | 6,265 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 6,265 |'
- en: '| 8 | 5,851 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 5,851 |'
- en: '| 9 | 5,949 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 5,949 |'
- en: CIFAR-10
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: '*CIFAR-10* is another standard deep learning dataset that’s small enough for
    us to use without requiring a lot of training time or a GPU. As with MNIST, we
    can extract the dataset with Keras, which will download it the first time it’s
    requested. The source page for CIFAR-10 is at *https://www.cs.toronto.edu/\%7Ekriz/cifar.html*.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*CIFAR-10* 是另一个标准的深度学习数据集，足够小，使用时无需大量训练时间或GPU。与MNIST一样，我们可以使用Keras提取数据集，第一次请求时它将自动下载。CIFAR-10的源页面地址是
    *https://www.cs.toronto.edu/\%7Ekriz/cifar.html*。'
- en: It’s worth perusing the page to learn more about where the dataset came from.
    It consists of 60,000 32×32 pixel RGB images from 10 classes, with 6,000 samples
    in each class. The training set contains 50,000 images, and the test set contains
    10,000 images. The 10 classes are shown here in [Table 5-2](ch05.xhtml#ch5tab2).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 值得浏览一下页面，了解数据集的来源。它包含来自10个类别的60,000张32×32像素的RGB图像，每个类别有6,000个样本。训练集包含50,000张图像，测试集包含10,000张图像。10个类别在[表
    5-2](ch05.xhtml#ch5tab2)中展示。
- en: '**Table 5-2:** CIFAR-10 Class Labels and Names'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5-2：** CIFAR-10 类别标签和名称'
- en: '| Label | Class |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 类别 |'
- en: '| --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | airplane |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 飞机 |'
- en: '| 1 | automobile |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 汽车 |'
- en: '| 2 | bird |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 鸟 |'
- en: '| 3 | cat |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 猫 |'
- en: '| 4 | deer |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 鹿 |'
- en: '| 5 | dog |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 狗 |'
- en: '| 6 | frog |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 青蛙 |'
- en: '| 7 | horse |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 马 |'
- en: '| 8 | ship |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 船 |'
- en: '| 9 | truck |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 卡车 |'
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows, row by row, a collection of representative
    images from each class. Let’s extract the dataset, store it for future use, and
    create vector representations, much as we did for MNIST.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](ch05.xhtml#ch5fig4)按行展示了每个类别的代表性图像集合。我们将提取数据集，存储以供未来使用，并创建向量表示，就像我们对MNIST所做的那样。'
- en: '![image](Images/05fig04.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig04.jpg)'
- en: '*Figure 5-4: Representative CIFAR-10 images*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-4：代表性的 CIFAR-10 图像*'
- en: The script to do all of this is in [Listing 5-4](ch05.xhtml#ch5lis4).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这一切的脚本在[清单 5-4](ch05.xhtml#ch5lis4)中。
- en: import numpy as np
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import keras
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.datasets import cifar10
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.datasets import cifar10
- en: ❶ (xtrn, ytrn), (xtst, ytst) = cifar10.load_data()
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (xtrn, ytrn), (xtst, ytst) = cifar10.load_data()
- en: idx = np.argsort(np.random.random(ytrn.shape[0]))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytrn.shape[0]))
- en: xtrn = xtrn[idx]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn = xtrn[idx]
- en: ytrn = ytrn[idx]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = ytrn[idx]
- en: idx = np.argsort(np.random.random(ytst.shape[0]))
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(ytst.shape[0]))
- en: xtst = xtst[idx]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: xtst = xtst[idx]
- en: ytst = ytst[idx]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = ytst[idx]
- en: np.save("cifar10_train_images.npy", xtrn)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_train_images.npy", xtrn)
- en: np.save("cifar10_train_labels.npy", ytrn)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_train_labels.npy", ytrn)
- en: np.save("cifar10_test_images.npy", xtst)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_test_images.npy", xtst)
- en: np.save("cifar10_test_labels.npy", ytst)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_test_labels.npy", ytst)
- en: ❷ xtrnv = xtrn.reshape((50000,32*32*3))
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ xtrnv = xtrn.reshape((50000,32*32*3))
- en: xtstv = xtst.reshape((10000,32*32*3))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: xtstv = xtst.reshape((10000,32*32*3))
- en: np.save("cifar10_train_vectors.npy", xtrnv)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_train_vectors.npy", xtrnv)
- en: np.save("cifar10_test_vectors.npy", xtstv)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("cifar10_test_vectors.npy", xtstv)
- en: '*Listing 5-4: Loading and building the various CIFAR-10 datasets*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-4：加载并构建各种 CIFAR-10 数据集*'
- en: 'We first load CIFAR-10 from Keras ❶. As with MNIST, the dataset will download
    automatically the first time that the code is run. And, as with MNIST, we randomize
    the train and test splits. The training data is in `xtrn` as a (50,000; 32; 32;
    3) array. The last dimension is for the three color components for each pixel:
    red, green, and blue. The test data is similar, and is in `xtst` as a (10,000;
    32; 32; 3) array. Finally, we write the randomized train and test images to disk.
    Next, we unravel the images to produce 32 × 32 × 3 = 3072 element feature vectors
    representing the images ❷ and write them to disk.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从Keras加载CIFAR-10 ❶。与MNIST一样，数据集将在第一次运行代码时自动下载。而且，与MNIST一样，我们会随机化训练和测试数据的分配。训练数据在`xtrn`中，形状为(50,000;
    32; 32; 3)的数组。最后一个维度代表每个像素的三个颜色分量：红色、绿色和蓝色。测试数据类似，存储在`xtst`中，形状为(10,000; 32; 32;
    3)的数组。最后，我们将随机化的训练和测试图像写入磁盘。接下来，我们将图像展开，生成32 × 32 × 3 = 3072个元素的特征向量表示这些图像 ❷，并将其写入磁盘。
- en: Data Augmentation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: As we saw in [Chapter 4](ch04.xhtml#ch04), the dataset is everything, so it
    needs to be as complete as possible. You’ll normally achieve this by carefully
    selecting samples that fit within the range of inputs the model will encounter
    when you use it. Thinking back to our earlier analogy, we need the model to *interpolate*
    and not *extrapolate*. But sometimes, even though we have a wide range of possible
    samples, we don’t have a lot of actual samples. This is where data augmentation
    can help.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.xhtml#ch04)中看到的，数据集是至关重要的，因此它需要尽可能完整。通常，你可以通过仔细选择符合模型在使用时可能遇到的输入范围的样本来实现这一点。回想我们之前的类比，我们希望模型能够*插值*而不是*外推*。但有时候，即使我们有广泛的可能样本，我们实际上也没有很多样本。这时，数据增强就可以发挥作用。
- en: '*Data augmentation* uses the data in the existing dataset to generate new possible
    samples to add to the set. These samples are always based, in some way, on the
    existing data. Data augmentation is a powerful technique and is particularly helpful
    when our actual dataset is small. In a practical sense, data augmentation should
    probably be used whenever it’s feasible.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据增强*利用现有数据集中的数据生成新的可能样本，加入到数据集中。这些样本总是以某种方式基于现有数据。数据增强是一项强大的技术，尤其在我们的实际数据集较小时，它特别有用。从实际角度来看，数据增强应该在可行的情况下尽可能使用。'
- en: Data augmentation takes the data we already have and modifies it to create new
    samples that could have plausibly come from the same parent distribution as our
    actual data. That means that if we were patient enough to keep collecting real
    data, we could measure those new samples. Sometimes data augmentation can go beyond
    what we would actually measure, yet still help the model learn to generalize to
    the actual data. For example, a model using images as input might benefit from
    unrealistic colors or backgrounds when the actual inputs to the model would never
    use those colors or backgrounds.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强通过修改我们已有的数据来创建新的样本，这些新样本可能来自与我们实际数据相同的母体分布。这意味着，如果我们有足够的耐心继续收集真实数据，我们可以测量这些新的样本。有时，数据增强可以超越我们实际测量的内容，但仍然能帮助模型学习如何推广到实际数据。例如，使用图像作为输入的模型可能从不现实的颜色或背景中受益，尽管实际输入模型时不会使用这些颜色或背景。
- en: While data augmentation works in many situations and is a mainstay of deep learning,
    you won’t always be able to use it because not all data can be realistically enhanced.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据增强在许多情况下都有效，并且是深度学习中的主流技术，但并不是所有数据都能通过增强得到有效提升，因此你并不总是能够使用它。
- en: 'In this section, we’ll take a look at why we’d want to consider using data
    augmentation and how we might go about doing it. We’ll then augment two of the
    datasets we developed previously, so when we build models, we can see how augmentation
    affects the models’ learning. As far as augmentation is concerned, here’s a rule
    of thumb: in general, you should perform data augmentation whenever possible,
    especially if the dataset is small.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨为什么我们需要考虑使用数据增强以及如何进行增强。然后，我们将增强之前开发的两个数据集，这样在构建模型时，我们可以看到增强如何影响模型的学习。关于数据增强，这里有一个经验法则：一般来说，只要可能，尤其是当数据集较小时，你应该进行数据增强。
- en: Why Should You Augment Training Data?
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么你应该增强训练数据？
- en: In [Chapter 4](ch04.xhtml#ch04), we encountered the curse of dimensionality.
    We saw that the solution to it, for many models, is to fill in the space of possible
    inputs with more and more training data. Data augmentation is one way we can fill
    in this space. We’ll need to do this in the future; in [Chapter 6](ch06.xhtml#ch06),
    for example, we’ll meet the *k*-Nearest Neighbor classifier, perhaps the simplest
    of all classifiers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#ch04)中，我们遇到了维度灾难。我们看到，对于许多模型来说，解决方案是通过更多的训练数据来填充可能的输入空间。数据增强是我们填充这个空间的一种方式。未来我们将需要这么做；例如，在[第6章](ch06.xhtml#ch06)中，我们将会接触到*k*-最近邻分类器，可能是所有分类器中最简单的一种。
- en: This classifier depends, critically, on having enough training data to adequately
    fill in the input feature space. If there are three features, then the space is
    three-dimensional and the training data will fit into some cube in that space.
    The more training data we have, the more samples we’ll have in the cube, and the
    better the classifier will do. That’s because the classifier measures the distance
    between points in the training data and that of a new, unknown feature vector
    and votes on what label to assign. The denser the space is with training points,
    the more often the voting process will succeed. Loosely speaking, data augmentation
    fills in this space. For most datasets, acquiring more data, more samples of the
    parent distribution, will not fill in every part of the feature space but will
    create a more and more complete picture of what the parent distribution looks
    like in the feature space.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器关键依赖于有足够的训练数据来充分填充输入特征空间。如果有三个特征，那么空间是三维的，训练数据将适合在这个空间中的某个立方体内。我们拥有的训练数据越多，立方体中的样本就越多，分类器的表现也就越好。这是因为分类器测量的是训练数据中各点与新输入特征向量之间的距离，并根据距离投票决定要分配哪个标签。训练数据点在空间中的分布越密集，投票过程成功的几率就越大。宽泛来说，数据增强填补了这个空间。对于大多数数据集来说，获取更多的数据，即获取更多来自父分布的样本，并不能填补特征空间的每一部分，但它会创造出一幅越来越完整的父分布在特征空间中的图像。
- en: When we work with modern deep learning models ([Chapter 12](ch12.xhtml#ch12)),
    we’ll see that data augmentation has additional benefits. During training, a neural
    network becomes conditioned to learn features of the training data. If the features
    the network learns to pay attention to are actually useful for distinguishing
    the classes, all is well. But, as we saw with the wolf and husky example of [Chapter
    4](ch04.xhtml#ch04), sometimes the network learns the wrong thing, which can’t
    be used to generalize to new inputs—like the fact that the wolf class images had
    snow in the background and the husky images did not.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用现代深度学习模型时（[第12章](ch12.xhtml#ch12)），我们将看到数据增强有额外的好处。在训练过程中，神经网络会逐渐学会训练数据中的特征。如果网络学会关注的特征确实有助于区分类别，那么一切都很好。但正如我们在[第4章](ch04.xhtml#ch04)中看到的狼与哈士奇的例子，有时网络会学到错误的东西，这些错误的特征无法用于泛化到新的输入——就像狼类图像背景有雪，而哈士奇图像没有雪一样。
- en: Taking steps to avoid this tendency is known as *regularization*. Regularization
    helps the network learn important features of the training data, ones that generalize
    as we want them to. Data augmentation is—short of acquiring more actual data—perhaps
    the simplest way to regularize the network as it learns. It conditions the learning
    process to not pay attention to quirks of the particular samples selected for
    the training set but to instead focus on more general features of the data. At
    least, that is the hope.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种倾向的措施被称为*正则化*。正则化帮助网络学习训练数据中重要的特征，这些特征能够按我们希望的方式进行泛化。数据增强是——除非获取更多实际数据——也许是正则化网络学习过程的最简单方式。它使学习过程不关注训练集中特定样本的特征，而是聚焦于数据的更一般性特征。至少，这是我们的期望。
- en: An additional benefit of data augmentation is that it lessens the likelihood
    of overfitting when training. We’ll discuss overfitting more in [Chapter 9](ch09.xhtml#ch09),
    but succinctly, it’s what happens when the model learns the training data nearly
    perfectly without learning to generalize to new inputs. Using a small dataset
    can lead to overfitting if the model is able to basically memorize the training
    data. Data augmentation increases the dataset size, reducing the probability of
    overfitting and possibly allowing use of a model with a larger capacity. (Capacity
    is a nebulous concept. Think “bigger,” in that the model can learn more of what
    is important in the training data, while still generalizing to new data.)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强的另一个好处是，它可以减少训练时过拟合的可能性。我们将在[第9章](ch09.xhtml#ch09)中更详细地讨论过拟合，但简而言之，过拟合是指模型几乎完美地学习了训练数据，但没有学会如何泛化到新的输入数据。如果使用小数据集，模型可能会将训练数据“记住”，从而导致过拟合。数据增强通过增加数据集的大小，降低了过拟合的概率，并可能允许使用更大容量的模型。（容量是一个模糊的概念。可以理解为“更大”，即模型可以学习到训练数据中更多重要的内容，同时还能泛化到新的数据上。）
- en: 'One extremely important point needs to be made about data augmentation as it
    relates to the training/validation/test split of the dataset: you *must* be sure
    that every augmented sample belongs to the same set. For example, if we augment
    sample *X*[12345], and this sample has been assigned to the training set, then
    we must ensure that *all* of the augmented samples based on *X*[12345] are also
    members of the training set. This is so important that it’s worth reiterating:
    *be sure to never mix an augmented sample based on an original sample between
    the training, validation, and test sets.*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据增强与训练/验证/测试集划分相关的一个极其重要的点是：你*必须*确保每个增强样本属于相同的集。例如，如果我们增强了样本*X*[12345]，并且这个样本被分配到了训练集，那么我们必须确保基于*X*[12345]增强的*所有*样本也属于训练集。这一点非常重要，值得再次强调：*确保永远不要将基于原始样本的增强样本混入训练、验证和测试集中*。
- en: If we don’t follow this rule, our beliefs about the quality of the model will
    be unfounded, or at least partially unwarranted, because there will be samples
    in the validation and test sets that are, essentially, also in the training set,
    since they’re based on the training data. This warning may seem unnecessary, but
    it’s really easy to make this mistake, especially if working with others or with
    a database of some kind.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不遵循这个规则，我们对模型质量的信念将是没有依据的，或者至少是部分不成立的，因为验证集和测试集中的样本本质上也在训练集中，因为它们基于训练数据。这条警告看起来可能不必要，但实际上很容易犯这个错误，特别是在与他人合作或使用某种数据库时。
- en: The correct way to augment data is *after* the training, validation, and test
    splits have been made. Then, augment at least the training data and label all
    the new samples as training data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的数据增强方法是在训练、验证和测试集划分完成后进行的。然后，至少增强训练数据，并将所有新样本标记为训练数据。
- en: What about augmenting the validation and test splits? It isn’t wrong to do so,
    and might make sense if you don’t have a lot of either. I haven’t run across any
    studies that tried to be rigorous about the effects of augmenting the validation
    and test data, but, conceptually, it shouldn’t hurt, and might even help.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，增强验证集和测试集怎么样？这样做并不错误，如果你没有很多验证集或测试集数据，这样做可能是有意义的。我还没看到有研究严格探讨增强验证集和测试数据的效果，但从概念上讲，这不会有什么坏处，甚至可能有所帮助。
- en: Ways to Augment Training Data
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强训练数据的方法
- en: To augment a dataset, we need to generate new samples from it that are plausible,
    meaning they could really occur in the dataset. For images, this is straightforward;
    you can often rotate the image, or flip it horizontally or vertically. Other times,
    you can manipulate the pixels themselves to change the contrast or alter the colors.
    Some have even gone so far as to simply swap entire color bands—swapping the red
    channel with the blue channel, for example.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强数据集，我们需要从中生成新的、合理的样本，即这些样本在数据集中是可能出现的。对于图像，这很简单；你可以经常旋转图像，或水平、垂直翻转图像。其他时候，你可以直接操作像素，改变对比度或调整颜色。有些方法甚至会直接交换整个颜色通道——例如交换红色通道和蓝色通道。
- en: Of course, the manipulations must make sense. A subtle rotation might mimic
    a change in the camera’s orientation, and a left-to-right flip might mimic the
    experience of looking in a mirror. But a top-to-bottom flip probably wouldn’t
    be as realistic. True, a monkey might hang upside-down in a picture, but flipping
    the picture would flip the tree and the ground, as well. On the other hand, you
    might be able to do a top-to-bottom flip in an aerial image, which shows objects
    in any orientation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，操作必须合理。细微的旋转可能模仿相机方向的变化，左右翻转可能模拟照镜子的体验。但上下翻转可能就不那么真实了。的确，猴子可能会在画面中倒挂，但翻转画面也会翻转树和地面。另一方面，你可能能够在航空影像中进行上下翻转，因为航空影像展示了任意方向的物体。
- en: Okay, so images are generally straightforward to augment, and it’s easy to understand
    whether the augmentation makes sense. Augmentation of a feature vector is more
    subtle. It’s not always clear how to do it, or if it’s even possible. What can
    we do in that case?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，图像通常很容易增强，而且很容易理解增强是否合理。特征向量的增强则更加微妙。它并不总是那么明显，如何操作，甚至是否可能进行增强。那我们该怎么做呢？
- en: Again, the guiding principle is that the augmentation makes sense. If we encoded
    color as a one-hot vector of, say, red, green, or blue, and an instance of a class
    can be red or green or blue, then one way to augment is to shift the color between
    red, green, and blue. If a sample can represent male or female, then we could
    also change those values to get a new sample of the same class but with a different
    gender.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，指导原则是增强操作必须合理。如果我们将颜色编码为一个独热向量，例如红色、绿色或蓝色，并且一个类别的实例可以是红色、绿色或蓝色，那么一种增强方法就是在红色、绿色和蓝色之间切换颜色。如果一个样本可以表示男性或女性，那么我们也可以改变这些值，从而获得一个性别不同但类别相同的新样本。
- en: These are unusual things to do, however. Typically, you try to augment continuous
    values, creating a new feature vector that still represents the original class.
    We’ll examine one way to do this next by augmenting the iris dataset. After that,
    we’ll augment the CIFAR-10 dataset to see how to work with images.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些做法比较不常见。通常，你会尝试增强连续值，创建一个仍能代表原始类别的新特征向量。接下来，我们将通过增强鸢尾花数据集来研究这种方法。之后，我们将增强
    CIFAR-10 数据集，看看如何处理图像。
- en: Augmenting the Iris Dataset
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强鸢尾花数据集
- en: The iris dataset has 150 samples from three classes, each with four continuous
    features. We’ll augment it by using *principal component analysis (PCA)*. This
    is an old technique, in use for over a century. It was common in machine learning
    before the advent of deep learning to combat the curse of dimensionality, because
    it can reduce the number of features in a dataset. It also has a variety of uses
    outside of machine learning.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集包含来自三个类别的150个样本，每个样本有四个连续特征。我们将通过使用*主成分分析（PCA）*来增强该数据集。这是一种使用了超过一个世纪的老技术。在深度学习出现之前，它在机器学习中很常见，用于应对维度灾难，因为它可以减少数据集中的特征数量。它在机器学习之外也有多种用途。
- en: Imagine that we have a dataset with only two features—for example, the first
    two features of the iris dataset. A scatter plot of these features will show us
    where the samples fall in 2D space. [Figure 5-5](ch05.xhtml#ch5fig5) shows a plot
    of the first two features of the iris dataset for classes 1 and 2\. The plot has
    shifted the origin to (0,0) by subtracting the mean value of each feature. This
    does not change the variance or scatter of the data, only its origin.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个只有两个特征的数据集——例如，鸢尾花数据集的前两个特征。这些特征的散点图将显示样本在二维空间中的分布。[图 5-5](ch05.xhtml#ch5fig5)展示了鸢尾花数据集类别
    1 和类别 2 的前两个特征的图像。该图通过减去每个特征的均值，将原点移动到(0,0)位置。这样做不会改变数据的方差或分散程度，只是改变了其原点。
- en: 'The plot in [Figure 5-5](ch05.xhtml#ch5fig5) also shows two arrows. These are
    the two principal components of the data. Since the data is 2D, we have two components.
    If we had 100 features, then we would have up to 100 principal components. This
    is what PCA does: it tells you the directions of the variance of the data. These
    directions are the *principal components*.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-5](ch05.xhtml#ch5fig5)中的图还显示了两条箭头。这是数据的两个主成分。由于数据是二维的，所以我们有两个成分。如果我们有100个特征，那么就会有最多100个主成分。这就是PCA的作用：它告诉你数据方差的方向。这些方向就是*主成分*。'
- en: '![image](Images/05fig05.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig05.jpg)'
- en: '*Figure 5-5: The first two iris features for classes 1 and 2, with their principal
    components*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-5：类别 1 和类别 2 的前两个鸢尾花特征及其主成分*'
- en: 'The principal components also tell you how much of the variance of the data
    is explained by each of these directions. In the plot, the length of the arrow
    corresponds to the fraction of the total variance explained by each component.
    As you can see, the largest component is along the diagonal that matches the greatest
    scatter of the points. Traditional machine learning uses PCA to reduce the number
    of features while still, hopefully, representing the dataset well. This is how
    PCA can help fight the curse of dimensionality: find the principal components
    and then throw the less influential ones away. However, for data augmentation,
    we want to keep all the components.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分还告诉你每个方向解释的数据方差的多少。在图中，箭头的长度对应于每个成分解释的总方差的比例。如你所见，最大的成分位于与数据点最大散布匹配的对角线方向。传统的机器学习方法使用PCA来减少特征数量，同时仍然尽量保留数据集的表示。这就是PCA如何帮助应对维度灾难的方法：找到主成分，然后舍弃那些影响较小的成分。然而，对于数据增强，我们希望保留所有成分。
- en: The code that produced [Figure 5-5](ch05.xhtml#ch5fig5) is in [Listing 5-5](ch05.xhtml#ch5lis5).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 生成[图 5-5](ch05.xhtml#ch5fig5)的代码位于[清单 5-5](ch05.xhtml#ch5lis5)中。
- en: import numpy as np
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pylab as plt
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pylab as plt
- en: from sklearn import decomposition
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn import decomposition
- en: ❶ x = np.load("../data/iris/iris_features.npy")[:,:2]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x = np.load("../data/iris/iris_features.npy")[:,:2]
- en: y = np.load("../data/iris/iris_labels.npy")
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/iris/iris_labels.npy")
- en: idx = np.where(y != 0)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.where(y != 0)
- en: x = x[idx]
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: x[:,0] -= x[:,0].mean()
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,0] -= x[:,0].mean()
- en: x[:,1] -= x[:,1].mean()
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,1] -= x[:,1].mean()
- en: ❷ pca = decomposition.PCA(n_components=2)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ pca = decomposition.PCA(n_components=2)
- en: pca.fit(x)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: pca.fit(x)
- en: v = pca.explained_variance_ratio_
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: v = pca.explained_variance_ratio_
- en: ❸ plt.scatter(x[:,0],x[:,1],marker='o',color='b')
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ plt.scatter(x[:,0],x[:,1],marker='o',color='b')
- en: ax = plt.axes()
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ax = plt.axes()
- en: x0 = v[0]*pca.components_[0,0]
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: x0 = v[0]*pca.components_[0,0]
- en: y0 = v[0]*pca.components_[0,1]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = v[0]*pca.components_[0,1]
- en: ax.arrow(0, 0, x0, y0, head_width=0.05, head_length=0.1, fc='r', ec='r')
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ax.arrow(0, 0, x0, y0, head_width=0.05, head_length=0.1, fc='r', ec='r')
- en: x1 = v[1]*pca.components_[1,0]
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = v[1]*pca.components_[1,0]
- en: y1 = v[1]*pca.components_[1,1]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = v[1]*pca.components_[1,1]
- en: ax.arrow(0, 0, x1, y1, head_width=0.05, head_length=0.1, fc='r', ec='r')
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ax.arrow(0, 0, x1, y1, head_width=0.05, head_length=0.1, fc='r', ec='r')
- en: plt.xlabel("$x_0$", fontsize=16)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel("$x_0$", fontsize=16)
- en: plt.ylabel("$x_1$", fontsize=16)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel("$x_1$", fontsize=16)
- en: plt.show()
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: '*Listing 5-5: Iris PCA plot*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-5: 鸢尾花PCA图*'
- en: Much of the preceding code is to make the plot ❸. The imports are standard except
    for a new one from sklearn, the `decomposition` module. We load the iris dataset
    we previously saved, keeping only the first two features in `x` and the labels
    in `y`. We then keep only class 1 and class 2 features by excluding class 0\.
    Next, we subtract the per feature means to center the data about the point (0,0)
    ❶.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码大部分是为了绘制图形 ❸。导入的模块是标准的，除了从sklearn导入的`decomposition`模块。我们加载之前保存的鸢尾花数据集，只保留`x`中的前两个特征，`y`中的标签。然后通过排除类0的特征，只保留类1和类2的特征。接着，我们通过减去每个特征的均值来将数据集中在点(0,0)周围
    ❶。
- en: 'Then we create the PCA object and fit the iris data to it ❷. There are two
    features, so the number of components in this case is also two. The PCA Python
    class mimics the standard approach of sklearn: it defines the model, then fits
    data to it. Once this is done, we have the principal components stored in `pca`
    and accessible via the `components_` member variable. We set `v` to a vector representing
    the fraction of the variance in the data explained by each of the principal component
    directions. Since there are two components, this vector also has two components.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建PCA对象并将鸢尾花数据拟合到它上面 ❷。有两个特征，所以在这种情况下，成分数也是两个。PCA Python类模仿了sklearn的标准方法：首先定义模型，然后拟合数据。一旦完成，我们就可以通过`components_`成员变量访问存储在`pca`中的主成分。我们将`v`设置为一个向量，表示数据中每个主成分方向解释的方差的比例。由于有两个成分，这个向量也有两个成分。
- en: The components are always listed in decreasing order, so that the first component
    is the direction describing the majority of the variance, the second component
    is the next most important, and so on. In this case, the first component describes
    some 84 percent of the variance and the second describes the remaining 16 percent.
    We’ll use this ordering when we generate new augmented samples. Here we use the
    fraction to scale the length of the arrows in the plot showing the principal component
    directions and relative importance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 成分总是按降序排列，因此第一个成分是描述大部分方差的方向，第二个成分是下一个最重要的方向，以此类推。在这种情况下，第一个成分描述了大约84%的方差，第二个成分描述了剩下的16%。我们在生成新的增强样本时会使用这种排序。这里我们使用方差比例来缩放图中显示的箭头的长度，以表示主成分方向及其相对重要性。
- en: How is [Figure 5-5](ch05.xhtml#ch5fig5) useful for data augmentation? Once you
    know the principal components, you can use PCA to create derived variables, which
    means you rotate the data to align it with the principal components. The `transform`
    method of the PCA class does this by mapping an input—in our case, the original
    data—to a new representation where the variance is aligned with the principal
    components. This mapping is exact, and you can reverse it by using the `inverse_transform`
    method.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用[图 5-5](ch05.xhtml#ch5fig5)进行数据增强？一旦你知道了主成分，就可以使用PCA来创建派生变量，这意味着你将数据旋转，使其与主成分对齐。PCA类的`transform`方法通过将输入（在我们这里是原始数据）映射到一个新的表示来实现这一点，在这个新的表示中，方差与主成分对齐。这个映射是精确的，你可以通过使用`inverse_transform`方法将其逆向操作。
- en: Doing this alone doesn’t generate new samples for us. If we take the original
    data, `x`, transform it to the new representation, and then inverse transform
    it, we’ll end up where we started, with `x`. But, if we transform `x` and then,
    before calling the inverse transform, *modify* some of the principal components,
    we’ll return a new set of samples that are not `x` but are based on `x`. This
    is precisely what we want for data augmentation. Next, we’ll see which components
    to modify, and how.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 单独这样做并不会为我们生成新的样本。如果我们将原始数据`x`转换为新的表示形式，然后再进行反向转换，我们将会回到原始的`x`。但是，如果我们先转换`x`，然后在调用反向转换之前，*修改*一些主成分，我们将得到一组新的样本，这些样本不等于`x`，但基于`x`。这正是我们想要的数据增强方法。接下来，我们将看到哪些主成分需要修改，以及如何修改。
- en: The components are ordered in `pca` by their importance. We want to keep the
    most important components as they are, because we want the inverse transform to
    produce data that looks much like the original data. We don’t want to transform
    things too much, or the new samples won’t be plausible instances of the class
    we claim they represent. We’ll arbitrarily say that we want to keep the components
    that, cumulatively, represent some 90 percent to 95 percent of the variance in
    the data. These we won’t modify at all. The remaining components will be modified
    by adding normally distributed noise. Recall that *normally distributed* means
    it follows the bell curve so that most of the time the value will be near the
    middle, which we’ll set to 0, meaning no change to the component, and increasingly
    rarely to larger values. We’ll add the noise to the existing component and call
    the inverse transform to produce new samples that are very similar but not identical
    to the originals.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主成分在`pca`中按其重要性排序。我们希望保持最重要的主成分不变，因为我们希望反向转换产生的数据看起来尽可能接近原始数据。我们不希望对数据进行过度转换，否则新生成的样本就不再是我们所声明的类的合理实例。我们可以随便说，我们希望保留那些累计表示数据方差的90%到95%的主成分。这些主成分我们将完全不做修改。剩余的主成分将通过添加正态分布的噪声进行修改。请记住，*正态分布*意味着它遵循钟形曲线，大多数情况下其值会接近中间值，我们将中间值设为0，表示该主成分不变，且随着值的增大，变化的频率越来越低。我们将噪声添加到现有主成分中，并调用反向转换来生成新的样本，这些样本与原始样本非常相似，但不完全相同。
- en: The previous paragraph is pretty dense. The code will make things easier to
    understand. Our approach to generating augmented data is shown in [Listing 5-6](ch05.xhtml#ch5lis6).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段内容比较密集，代码会让理解变得更容易。我们生成增强数据的方法在[Listing 5-6](ch05.xhtml#ch5lis6)中展示。
- en: import numpy as np
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn import decomposition
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn import decomposition
- en: '❶ def generateData(pca, x, start):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def generateData(pca, x, start):'
- en: original = pca.components_.copy()
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: original = pca.components_.copy()
- en: ncomp = pca.components_.shape[0]
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ncomp = pca.components_.shape[0]
- en: a = pca.transform(x)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: a = pca.transform(x)
- en: 'for i in range(start, ncomp):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(start, ncomp):'
- en: pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)
- en: b = pca.inverse_transform(a)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: b = pca.inverse_transform(a)
- en: pca.components_ = original.copy()
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: pca.components_ = original.copy()
- en: return b
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: return b
- en: 'def main():'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../../../data/iris/iris_features.npy")
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../../../data/iris/iris_features.npy")
- en: y = np.load("../../../data/iris/iris_labels.npy")
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../../../data/iris/iris_labels.npy")
- en: N = 120
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: N = 120
- en: x_train = x[:N]
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:N]
- en: y_train = y[:N]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:N]
- en: x_test = x[N:]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x[N:]
- en: y_test = y[N:]
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = y[N:]
- en: pca = decomposition.PCA(n_components=4)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: pca = decomposition.PCA(n_components=4)
- en: pca.fit(x)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: pca.fit(x)
- en: print(pca.explained_variance_ratio_)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: print(pca.explained_variance_ratio_)
- en: start = 2
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: start = 2
- en: ❸ nsets = 10
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ nsets = 10
- en: nsamp = x_train.shape[0]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: nsamp = x_train.shape[0]
- en: newx = np.zeros((nsets*nsamp, x_train.shape[1]))
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: newx = np.zeros((nsets*nsamp, x_train.shape[1]))
- en: newy = np.zeros(nsets*nsamp, dtype="uint8")
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: newy = np.zeros(nsets*nsamp, dtype="uint8")
- en: '❹ for i in range(nsets):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ for i in range(nsets):'
- en: 'if (i == 0):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (i == 0):'
- en: newx[0:nsamp,:] = x_train
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: newx[0:nsamp,:] = x_train
- en: newy[0:nsamp] = y_train
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: newy[0:nsamp] = y_train
- en: 'else:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: newx[(i*nsamp):(i*nsamp+nsamp),:] =
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: newx[(i*nsamp):(i*nsamp+nsamp),:] =
- en: generateData(pca, x_train, start)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: generateData(pca, x_train, start)
- en: newy[(i*nsamp):(i*nsamp+nsamp)] = y_train
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: newy[(i*nsamp):(i*nsamp+nsamp)] = y_train
- en: ❺ idx = np.argsort(np.random.random(nsets*nsamp))
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ idx = np.argsort(np.random.random(nsets*nsamp))
- en: newx = newx[idx]
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: newx = newx[idx]
- en: newy = newy[idx]
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: newy = newy[idx]
- en: np.save("iris_train_features_augmented.npy", newx)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_train_features_augmented.npy", newx)
- en: np.save("iris_train_labels_augmented.npy", newy)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_train_labels_augmented.npy", newy)
- en: np.save("iris_test_features_augmented.npy", x_test)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_test_features_augmented.npy", x_test)
- en: np.save("iris_test_labels_augmented.npy", y_test)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris_test_labels_augmented.npy", y_test)
- en: main()
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: main()
- en: '*Listing 5-6: Augmenting the iris data with PCA. See* iris_data_augmentation.py.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 5-6: 使用PCA增强鸢尾花数据。见* iris_data_augmentation.py。'
- en: 'The `main` function ❷ loads the existing iris data, `x`, and the corresponding
    labels, `y`, and then calls PCA, this time using all four features of the dataset.
    This gives us the four principal components telling us how much of the variance
    is explained by each component:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数❷加载现有的鸢尾花数据，`x`，以及相应的标签，`y`，然后调用PCA，这次使用数据集中的所有四个特征。这将给我们四个主成分，告诉我们每个成分解释的方差是多少：'
- en: '[PRE2]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first two principal components describe over 97 percent of the variance.
    Therefore, we’ll leave the first two components alone, indices 0 and 1, and start
    with index 2 when we want to generate new samples.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个主成分描述了超过97%的方差。因此，我们将保留前两个成分不变，即索引0和1，在我们想生成新样本时，从索引2开始。
- en: We next declare the number of sets we’ll define ❸. A *set* here means a new
    collection of samples. Since the samples are based on the original data, `x`,
    with 150 samples, each new set will contain 150 samples as well. In fact, they’ll
    be in the same order as the original samples, so that the class label that should
    go with each of these new samples is in the same order as the class labels in
    `y`. We don’t want to lose our original data, either, so `nsets=10` puts the original
    data and nine new sets of samples based on that original data—for a total of 1,500
    samples—in the new dataset. We grab the number of samples in `x`, 150, and define
    the arrays to hold our new features (`newx`) and associated labels (`newy`).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明我们将定义的集合数量❸。这里的*集合*指的是一组新的样本。由于这些样本基于原始数据`x`，并且每组有150个样本，因此每个新集合也将包含150个样本。事实上，它们的顺序与原始样本相同，因此每个新样本对应的类别标签将与
    `y` 中的类别标签顺序一致。我们也不想丢失原始数据，因此 `nsets=10` 将原始数据和基于原始数据生成的九组新样本放在一起——总共1,500个样本——进入新的数据集中。我们抓取
    `x` 中样本的数量150，并定义数组来保存我们的新特征（`newx`）和关联的标签（`newy`）。
- en: Next, we loop to generate the new samples, one set of 150 at a time ❹. The first
    pass simply copies the original data into the output arrays. The remaining passes
    are similar, updating the source and destination indices of the output arrays
    appropriately, but instead of assigning `x`, we assign the output of `generateData`.
    When the loop is done, we scramble the order of the entire dataset and write it
    to disk ❺.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们循环生成新样本，每次生成一组150个样本❹。第一次循环只是将原始数据复制到输出数组中。其余的循环类似，适当地更新输出数组的源和目标索引，但不是赋值
    `x`，而是赋值 `generateData` 的输出。当循环完成时，我们将整个数据集的顺序打乱，并将其写入磁盘❺。
- en: All of the magic is in `generateData` ❶. We pass in the PCA object (`pca`),
    the original data (`x`), and the starting principal component index (`start`).
    We set the last argument to 2 to leave the two most important components alone.
    We keep a copy of the actual components so we can reset the `pca` object before
    we return. Then we define `ncomp`, the number of principal components, for convenience
    and call the forward transformation mapping the original data along the principal
    components.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的魔法都在 `generateData` ❶ 中。我们传入PCA对象（`pca`）、原始数据（`x`）和起始主成分索引（`start`）。我们将最后一个参数设为2，以保留最重要的两个成分不变。我们保留实际成分的副本，以便在返回之前重置`pca`对象。接着，我们定义
    `ncomp`，主成分的数量，方便起见，并调用前向变换，将原始数据映射到主成分上。
- en: The loop updates the two least important components by adding a random value
    drawn from a normal curve with mean value 0 and a standard deviation of 0.1\.
    Why 0.1? No special reason; if the standard deviation is small, then the new samples
    will be near the old samples, while if it’s larger, they’ll be farther away and
    possibly not representative of the class anymore. Next, we call the inverse transformation
    using the modified principal components and restore the actual components. Finally,
    we return the new set of samples.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 循环通过添加一个从正态曲线中抽取的随机值来更新两个最不重要的成分，该值的均值为0，标准差为0.1。为什么是0.1？没有特别的原因；如果标准差较小，那么新样本将接近旧样本，而如果标准差较大，它们将更远，甚至可能不再代表该类别。接着，我们使用修改后的主成分调用逆变换，并恢复实际的成分。最后，我们返回新的样本集合。
- en: Let’s look at the new dataset, shown in [Figure 5-6](ch05.xhtml#ch5fig6). The
    big gray dots are from our original dataset, and the smaller black dots are the
    augmented samples. As we can readily see, they all fall near an existing sample,
    which is what we would expect from modifying only the weakest of the principal
    components. Since we copied the original data into the augmented dataset, each
    big dot has a small dot at the center.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看新的数据集，如[图5-6](ch05.xhtml#ch5fig6)所示。大灰点来自我们原始的数据集，而较小的黑点则是增强后的样本。我们可以清楚地看到，所有的点都靠近现有的样本，这正是我们从仅修改最弱主成分时所期望的结果。由于我们将原始数据复制到增强数据集中，所以每个大点的中心都有一个小点。
- en: '![image](Images/05fig06.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig06.jpg)'
- en: '*Figure 5-6: The first two features of the original iris dataset (large dots)
    and the augmented features generated by [Listing 5-6](ch05.xhtml#ch5lis6) (small
    points)*'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5-6：原始虹膜数据集的前两个特征（大点）与通过[清单5-6](ch05.xhtml#ch5lis6)生成的增强特征（小点）*'
- en: This approach is appropriate for continuous features only, as was previously
    stated, and you should be careful to modify only the weakest of the principal
    components, and only by a small amount. Experimentation is important here. As
    an exercise, try applying the same technique to augment the breast cancer dataset,
    which also consists of continuous features.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法仅适用于连续特征，正如之前所述，你应该小心只修改最弱的主成分，而且修改幅度要小。实验在这里非常重要。作为练习，尝试应用相同的技术来增强乳腺癌数据集，它也由连续特征组成。
- en: Augmenting the CIFAR-10 Dataset
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强 CIFAR-10 数据集
- en: Augmenting the iris dataset involved a lot of discussion and some less than
    obvious math. Fortunately for us, augmenting images is generally a lot simpler,
    but still just as effective when training modern models. When we build convolutional
    neural network models ([Chapter 12](ch12.xhtml#ch12)), we’ll see how to do augmentation
    on the fly when training, a particularly helpful approach, but for now we’ll do
    the augmentation first and build a new dataset with additional versions of the
    existing images.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 增强虹膜数据集涉及了很多讨论以及一些不太显而易见的数学问题。幸运的是，增强图像通常要简单得多，但在训练现代模型时同样有效。当我们构建卷积神经网络模型时（[第12章](ch12.xhtml#ch12)），我们将看到如何在训练过程中动态进行数据增强，这是一种特别有帮助的方法，但现在我们首先进行增强，创建一个包含现有图像的额外版本的新数据集。
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows representative images from each class
    in the CIFAR-10 dataset. These are color images stored as RGB data for the red,
    green, and blue channels. They were taken from ground level, so top and bottom
    flips do not make sense here, while left and right flips do. Translations—shifting
    the image in the *x* or *y* direction, or both—are one common technique. Small
    rotations are another common technique.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-4](ch05.xhtml#ch5fig4)展示了CIFAR-10数据集中每个类别的代表性图像。这些是存储为RGB数据的彩色图像，分别表示红色、绿色和蓝色通道。它们是在地面水平拍摄的，因此上下翻转在这里没有意义，而左右翻转则有意义。平移—将图像在
    *x* 或 *y* 方向（或两者）上平移—是一种常见技术。小角度旋转是另一种常见技术。'
- en: 'However, each of these raises an issue: what to do with pixels that have no
    data after the shift or rotate? If I shift an image 3 pixels to the left, I need
    to fill in the three columns on the right with something. Or, if I rotate to the
    right, there will be pixels at the upper right and lower left that need to be
    filled in. There are several ways to handle this. One is to simply leave the pixels
    black, or all 0 values, and let the model learn that there is no helpful information
    there. Another is to replace the pixels with the mean value of the image, which
    also provides no information and will, we hope, be ignored by the model. However,
    the most popular solution is to crop the image.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每种方法都会引发一个问题：如何处理在平移或旋转后没有数据的像素？例如，如果我将图像平移3个像素到左边，我需要用某些内容填充右边的三个列。或者，如果我向右旋转图像，那么右上角和左下角会有一些需要填充的像素。处理这些问题有几种方法。一种方法是简单地将这些像素保持为黑色，或者全是0值，让模型学习到这些地方没有有用信息。另一种方法是用图像的均值替换这些像素，这样也不会提供任何信息，并且我们希望模型能忽略这些区域。然而，最常见的解决方案是裁剪图像。
- en: The image is 32×32 pixels. Pulling a random patch from the image of, say, 28×28
    pixels is the equivalent of shifting the image by a random *x* or *y* position
    of up to 4 pixels without needing to worry about filling in anything. If we rotate
    the image first, which will require interpolation of the pixels, and then crop
    to remove the edge regions, we’ll again have no empty pixels to worry about. Keras
    has tools for doing this via an image generator object used during training. When
    we’re using Keras to build models, we’ll make use of it, but for now, we’ll do
    all of the work ourselves in order to understand the process.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像是32×32像素。我们从图像中随机抽取一个28×28像素的补丁，相当于将图像在随机的*x*或*y*位置上平移最多4个像素，而无需担心填补空白像素。如果我们先旋转图像，旋转过程中会进行像素插值，然后再裁剪去除边缘区域，这时我们仍然不需要担心空白像素。Keras提供了通过图像生成器对象来执行这些操作的工具，这个生成器在训练过程中使用。我们在使用Keras构建模型时会利用这个工具，但为了理解整个过程，暂时我们自己手动完成所有这些操作。
- en: We need to mention one point here. So far, we’ve talked about building a dataset
    for training a model. What should we do when we want to use the model? Do we hand
    the model random croppings of the test inputs as well? No. Instead, we hand the
    model a cropping centered on the image. So, for CIFAR-10, we would take each 32
    × 32 test input and crop it to 28 × 28 by dropping the outer 6 pixels, then present
    that to the model. We do this because the center crop still represents the actual
    test image and not some augmented version of it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里提到一点。到目前为止，我们讨论的是如何为训练模型构建数据集。那么，当我们想要使用模型时该怎么办呢？我们是否也会给模型提供测试输入的随机裁剪？不，正确的做法是给模型提供一个以图像中心为基准的裁剪。所以，对于CIFAR-10，我们会将每个32
    × 32的测试输入裁剪成28 × 28，去掉外侧6个像素，然后将其输入模型。我们这么做是因为中心裁剪仍然代表了实际的测试图像，而不是某种增强版本的图像。
- en: '[Figure 5-7](ch05.xhtml#ch5fig7) illustrates what we mean by rotations, flips,
    random croppings for training, and center cropping for testing. In (a) we rotate
    the image and take a center crop. The output image is in the white square. In
    (b) we flip left to right and crop randomly. In (c), we take two random crops
    without flipping, and in (d) we take a center crop for testing, without any rotation
    or flip. Some people augment test images, but we won’t do so here.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-7](ch05.xhtml#ch5fig7)说明了我们所说的旋转、翻转、训练时的随机裁剪和测试时的中心裁剪的含义。在(a)中，我们旋转图像并进行中心裁剪，输出图像位于白色方框内。在(b)中，我们进行左右翻转并随机裁剪。在(c)中，我们做两个随机裁剪，但不进行翻转；在(d)中，我们对测试图像进行中心裁剪，且不进行旋转或翻转。有些人会对测试图像进行增强，但我们在这里不这么做。'
- en: '![image](Images/05fig07.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/05fig07.jpg)'
- en: '*Figure 5-7: Rotate, then center crop (a). Flip left to right, then crop randomly
    (b). Two random crops during training (c). Center crop for testing, with no rotation
    or flip (d).*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5-7：旋转，然后中心裁剪（a）。左右翻转，然后随机裁剪（b）。训练时的两个随机裁剪（c）。测试时的中心裁剪，不旋转或翻转（d）。*'
- en: '[Listing 5-7](ch05.xhtml#ch5lis7) shows how to augment the CIFAR-10 training
    set with random crops, rotations, and flips.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表5-7](ch05.xhtml#ch5lis7)展示了如何通过随机裁剪、旋转和翻转来增强CIFAR-10训练集。'
- en: import numpy as np
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from PIL import Image
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: '❶ def augment(im, dim):'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ def augment(im, dim):'
- en: img = Image.fromarray(im)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: img = Image.fromarray(im)
- en: 'if (np.random.random() < 0.5):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (np.random.random() < 0.5):'
- en: img = img.transpose(Image.FLIP_LEFT_RIGHT)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.transpose(Image.FLIP_LEFT_RIGHT)
- en: 'if (np.random.random() < 0.3333):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (np.random.random() < 0.3333):'
- en: z = (32-dim)/2
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: z = (32-dim)/2
- en: r = 10*np.random.random()-5
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: r = 10*np.random.random()-5
- en: img = img.rotate(r, resample=Image.BILINEAR)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.rotate(r, resample=Image.BILINEAR)
- en: img = img.crop((z,z,32-z,32-z))
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.crop((z,z,32-z,32-z))
- en: 'else:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: x = int((32-dim-1)*np.random.random())
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: x = int((32-dim-1)*np.random.random())
- en: y = int((32-dim-1)*np.random.random())
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: y = int((32-dim-1)*np.random.random())
- en: img = img.crop((x,y,x+dim,y+dim))
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: img = img.crop((x,y,x+dim,y+dim))
- en: return np.array(img)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: return np.array(img)
- en: 'def main():'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: ❷ x = np.load("../data/cifar10/cifar10_train_images.npy")
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = np.load("../data/cifar10/cifar10_train_images.npy")
- en: y = np.load("../data/cifar10/cifar10_train_labels.npy")
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.load("../data/cifar10/cifar10_train_labels.npy")
- en: factor = 10
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: factor = 10
- en: dim = 28
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: dim = 28
- en: z = (32-dim)/2
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: z = (32-dim)/2
- en: newx = np.zeros((x.shape[0]*factor, dim,dim,3), dtype="uint8")
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: newx = np.zeros((x.shape[0]*factor, dim,dim,3), dtype="uint8")
- en: newy = np.zeros(y.shape[0]*factor, dtype="uint8")
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: newy = np.zeros(y.shape[0]*factor, dtype="uint8")
- en: k=0
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: k=0
- en: '❸ for i in range(x.shape[0]):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ for i in range(x.shape[0]):'
- en: im = Image.fromarray(x[i,:])
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: im = Image.fromarray(x[i,:])
- en: im = im.crop((z,z,32-z,32-z))
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: im = im.crop((z,z,32-z,32-z))
- en: newx[k,...] = np.array(im)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: newx[k,...] = np.array(im)
- en: newy[k] = y[i]
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: newy[k] = y[i]
- en: k += 1
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: k += 1
- en: 'for j in range(factor-1):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 'for j in range(factor-1):'
- en: newx[k,...] = augment(x[i,:], dim)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: newx[k,...] = augment(x[i,:], dim)
- en: newy[k] = y[i]
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: newy[k] = y[i]
- en: k += 1
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: k += 1
- en: idx = np.argsort(np.random.random(newx.shape[0]))
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(newx.shape[0]))
- en: newx = newx[idx]
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: newx = newx[idx]
- en: newy = newy[idx]
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: newy = newy[idx]
- en: np.save("../data/cifar10/cifar10_aug_train_images.npy", newx)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("../data/cifar10/cifar10_aug_train_images.npy", newx)
- en: np.save("../data/cifar10/cifar10_aug_train_labels.npy", newy)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("../data/cifar10/cifar10_aug_train_labels.npy", newy)
- en: ❹ x = np.load("../data/cifar10/cifar10_test_images.npy")
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ x = np.load("../data/cifar10/cifar10_test_images.npy")
- en: newx = np.zeros((x.shape[0], dim,dim,3), dtype="uint8")
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: newx = np.zeros((x.shape[0], dim, dim, 3), dtype="uint8")
- en: 'for i in range(x.shape[0]):'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 i 在 range(x.shape[0]) 中：
- en: im = Image.fromarray(x[i,:])
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: im = Image.fromarray(x[i,:])
- en: im = im.crop((z,z,32-z,32-z))
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: im = im.crop((z, z, 32-z, 32-z))
- en: newx[i,...] = np.array(im)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: newx[i,...] = np.array(im)
- en: np.save("../data/cifar10/cifar10_aug_test_images.npy", newx)
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("../data/cifar10/cifar10_aug_test_images.npy", newx)
- en: '*Listing 5-7: Augmenting the CIFAR-10 dataset. See* cifar10_augment.py.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5-7：增强 CIFAR-10 数据集。见* cifar10_augment.py。'
- en: The `main` function loads the existing dataset and defines our augmentation
    factor, crop size, and a constant for defining a center crop ❷.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数加载现有数据集并定义我们的增强因子、裁剪大小和一个用于定义中心裁剪的常量❷。'
- en: 'The new image will be put in `newx`, which has the following dimensions: (500,000;28;28;3);
    there are 50,000 training images, each with 32×32 pixels and three color bands.
    We set the augmentation factor to 10\. Similarly, there will be 500,000 labels.
    The counter, `k`, will index into this new dataset. For every image in the old
    dataset, we’ll create nine completely new versions and center crop the original
    ❶ ❸.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 新图像将放入`newx`中，其维度如下：(500,000;28;28;3)；包含50,000张训练图像，每张图像为32×32像素，具有三个颜色通道。我们将增强因子设置为10。类似地，也会有500,000个标签。计数器`k`将用于索引此新数据集。对于旧数据集中的每张图像，我们将创建九个完全不同的新版本，并对原始图像进行中心裁剪❶❸。
- en: As the dataset consists of images, it’s easiest to work with the data in image
    form, so we make the current sample an actual `PIL` image in order to easily crop
    it. This is the center crop of the original image. We store it in the new output
    array.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集包含图像，最方便的方式是以图像形式处理数据，因此我们将当前样本转换为实际的 `PIL` 图像，以便轻松裁剪。这是原始图像的中心裁剪。我们将其存储到新的输出数组中。
- en: 'There are two Python idioms here that we’ll see more than once. The first is
    to turn a NumPy array representing an image into a `PIL` image:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个 Python 的习惯用法，我们将看到不止一次。第一个是将表示图像的 NumPy 数组转换为 `PIL` 图像：
- en: '[PRE3]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The second is to go the other way and turn a `PIL` image into a NumPy array:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是反过来，将 `PIL` 图像转换为 NumPy 数组：
- en: '[PRE4]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We must be sure that the NumPy array is a valid image data type like unsigned
    byte (`uint8`). Use the `astype` NumPy array method to cast between types, remembering
    that you bear all responsibility for understanding what that casting entails.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保 NumPy 数组是有效的图像数据类型，如无符号字节（`uint8`）。使用 `astype` NumPy 数组方法进行类型转换，记住你需要完全理解这种类型转换的含义。
- en: Referring back to [Listing 5-7](ch05.xhtml#ch5lis7), we are creating the nine
    versions of the current image. For each of these, we simply copy the label and
    assign the output array an augmented version. We’ll describe the `augment` function
    shortly. Once the new dataset has been constructed, we scramble the order and
    write the augmented training dataset to disk ❸.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [清单 5-7](ch05.xhtml#ch5lis7)，我们正在创建当前图像的九个版本。对于每一个版本，我们简单地复制标签并为输出数组分配增强版。稍后我们将描述
    `augment` 函数。一旦新数据集构建完成，我们将打乱顺序并将增强后的训练数据集写入磁盘❸。
- en: We’re not quite done, however. We created an augmented training set that cropped
    the original 32 × 32 images to 28 × 28\. We must, therefore, at least crop the
    original test set ❹. As we stated previously, we use a center crop and no augmentation
    of the test data. Therefore, we simply load the test dataset, define the new output
    test dataset, and run a loop that crops the 32 × 32 images to 28 × 28\. When done,
    we write the cropped test data to disk. Note that we did not modify the *order*
    of the images in the test set; we simply cropped them, so we do not need to write
    a new file for the test labels.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完全完成。我们创建了一个增强的训练集，将原始的 32 × 32 图像裁剪为 28 × 28。因此，我们必须至少裁剪原始的测试集❹。正如我们之前所说，我们使用中心裁剪，并且不对测试数据进行增强。因此，我们只需加载测试数据集，定义新的输出测试数据集，然后运行一个循环，将
    32 × 32 的图像裁剪为 28 × 28。完成后，我们将裁剪后的测试数据写入磁盘。请注意，我们并没有修改测试集中的*顺序*；我们只是裁剪了它们，因此不需要为测试标签写入新文件。
- en: The `augment` function ❶ is where all the action is. We immediately change the
    input NumPy array into an actual `PIL` image object. We next decide, with a 50-50
    chance, whether or not we will flip the image left to right. Note that we do not
    crop the image just yet.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`augment`函数 ❶是所有操作的核心。我们立即将输入的NumPy数组转换为实际的`PIL`图像对象。接着，我们以50-50的概率决定是否将图像左右翻转。请注意，我们暂时不对图像进行裁剪。'
- en: Next, we ask whether we should rotate the image or not. We select rotation with
    a probability of 33 percent (1 in 3 chance). Why 33 percent? No particular reason,
    but it seems that we might want to crop randomly more often than we rotate. We
    could even drop this probability down to 20 percent (1 in 5 chance). If we do
    rotate, we select the rotation angle, [*–*5,5] and then call the `rotate` method
    using bilinear interpolation to make the rotated image look a bit nicer than simply
    using the nearest neighbor, which is the `PIL` default. Next, we center crop the
    rotated image. This way, we will not get any black pixels on the edges where the
    rotation had no image information to work with.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要决定是否对图像进行旋转。我们以33%的概率（1/3的机会）选择旋转。为什么是33%？没有特别的原因，但似乎我们更希望随机裁剪的频率高于旋转。我们甚至可以将这个概率降低到20%（1/5的机会）。如果我们进行旋转，我们选择旋转角度，[*–*5,5]，然后使用双线性插值调用`rotate`方法，以使旋转后的图像看起来比仅使用最近邻（即`PIL`默认方法）更美观。接下来，我们对旋转后的图像进行中心裁剪。这样，我们就不会在旋转后的图像边缘出现黑色像素，因为这些区域在旋转时没有图像信息。
- en: If we do not rotate, we are free to select a random crop. We choose the upper-left
    corner of this random crop, ensuring that the cropped square will not exceed the
    dimensions of the original image. Finally, we convert the data back to a NumPy
    array and return.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不进行旋转，我们可以自由选择一个随机裁剪区域。我们选择这个随机裁剪区域的左上角，确保裁剪后的正方形不会超过原始图像的尺寸。最后，我们将数据转换回NumPy数组并返回。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we built four datasets that we’ll use as examples throughout
    the rest of the book. The first two, irises and breast cancer histology, are based
    on feature vectors. The last two, MNIST and CIFAR-10, are represented as images.
    We then learned about two data augmentation methods: augmenting a feature vector
    of continuous values using PCA and, more critical for deep learning, augmenting
    images by basic transformations.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了四个数据集，将在本书的其余部分作为示例使用。前两个数据集，鸢尾花数据集和乳腺癌组织学数据集，基于特征向量。最后两个数据集，MNIST和CIFAR-10，以图像形式表示。然后，我们学习了两种数据增强方法：使用PCA增强连续值的特征向量，以及对于深度学习更为关键的，使用基本变换增强图像。
- en: In the next chapter, we’ll transition to our discussion of classical machine
    learning models. In the chapter after that, we’ll use these datasets with those
    models.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将转向讨论经典的机器学习模型。在之后的章节中，我们将使用这些数据集与这些模型进行结合。
