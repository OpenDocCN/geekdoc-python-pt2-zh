- en: '**5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BUILDING DATASETS**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous chapter had a lot of detailed advice. Now let’s put it all into
    practice to build the datasets we’ll use throughout the remainder of the book.
    Some of these datasets are well suited to traditional models, because they consist
    of feature vectors. Others are better suited to deep learning models that work
    with multidimensional inputs—in particular, images, or things that can be visualized
    as images.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll work through acquiring the raw data and preprocessing the data to make
    it suitable for our tools. We won’t make actual training/validation/test splits
    until we use these datasets for specific models. It is worth noting here that
    preprocessing the data to make it suitable for a model is often one of the most
    labor-intensive of machine learning tasks. All the same, if it is not done, or
    not done well, your model may end up being far less useful than you want it to
    be.
  prefs: []
  type: TYPE_NORMAL
- en: Irises
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perhaps the most classic of all machine learning datasets is the iris flower
    dataset, developed in 1936 by R. A. Fisher in his paper, “The Use of Multiple
    Measurements in Taxonomic Problems.” It’s a small dataset of three classes with
    50 samples in each class. There are four features: sepal width, sepal length,
    petal width, and petal length, all in centimeters. The three classes are *I. setosa*,
    *I. versicolour*, and *I. virginica*. This dataset is built into sklearn, but
    we’ll instead download it from the University of California, Irvine, Machine Learning
    Repository to practice working with externally sourced data and introduce a rich
    collection of datasets suitable for many traditional machine learning models.
    The main repository is located at *[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)*,
    but you can download the irises dataset directly from *[https://archive.ics.uci.edu/ml/datasets/iris/](https://archive.ics.uci.edu/ml/datasets/iris/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of this writing, this dataset has been downloaded nearly 1.8 million
    times. You can download it by selecting the **Data Folder** link near the top
    of the page, then right-clicking and saving the *iris.data* file, ideally to a
    new directory called *iris*. Let’s take a look at the start of this file:'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1,3.5,1.4,0.2,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: 4.9,3.0,1.4,0.2,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: 4.7,3.2,1.3,0.2,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: 4.6,3.1,1.5,0.2,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: 5.0,3.6,1.4,0.2,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: 5.4,3.9,1.7,0.4,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: 4.6,3.4,1.4,0.3,Iris-setosa
  prefs: []
  type: TYPE_NORMAL
- en: Because the class names at the end of each line are all the same, we should
    immediately suspect that the samples are sorted by class. Looking at the rest
    of the file confirms this. So, as emphasized in [Chapter 4](ch04.xhtml#ch04),
    we must be sure to randomize the data before training a model. Also, we need to
    replace the class names with integer labels. We can load the dataset into Python
    with the script in [Listing 5-1](ch05.xhtml#ch5lis1).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: '❶ with open("iris.data") as f:'
  prefs: []
  type: TYPE_NORMAL
- en: lines = [i[:-1] for i in f.readlines()]
  prefs: []
  type: TYPE_NORMAL
- en: ❷ n = ["Iris-setosa","Iris-versicolor","Iris-virginica"]
  prefs: []
  type: TYPE_NORMAL
- en: x = [n.index(i.split(",")[-1]) for i in lines if i != ""]
  prefs: []
  type: TYPE_NORMAL
- en: x = np.array(x, dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: ❸ y = [[float(j) for j in i.split(",")[:-1]] for i in lines if i != ""]
  prefs: []
  type: TYPE_NORMAL
- en: y = np.array(y)
  prefs: []
  type: TYPE_NORMAL
- en: ❹ i = np.argsort(np.random.random(x.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: x = x[i]
  prefs: []
  type: TYPE_NORMAL
- en: y = y[i]
  prefs: []
  type: TYPE_NORMAL
- en: ❺ np.save("iris_features.npy", y)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris_labels.npy", x)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-1: Loading the raw iris dataset and mapping to our standard format*'
  prefs: []
  type: TYPE_NORMAL
- en: First, we load the text file containing the data. The list comprehension removes
    the extraneous newline character ❶. Next, we create the vector of labels by converting
    the text label into an integer, 0–2\. The last element in the list, created by
    splitting a line along commas, is the text label. We want NumPy arrays, so we
    turn the list into one. The uint8 is unnecessary, but since the labels are never
    negative and they’re never larger than 2, we save a bit of space by making the
    data type an unsigned 8-bit integer ❷.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the feature vectors as a 150-row by 4-column matrix comes next via
    a double list comprehension. The outer comprehension (i) moves over lines from
    the file, and the inner one (j) takes the list of measurements for each sample
    and turns them into floating-point numbers. We then convert the list of lists
    into a 2D NumPy array ❸. We finish by randomizing the dataset as we did previously
    ❹, and, finally, we write the NumPy arrays to disk so we can use them later ❺.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-1](ch05.xhtml#ch5fig1) shows a box plot of the features. This is
    a well-behaved dataset, but the second feature does have some possible outliers.
    Because the features all have similar scales, we’ll use the features as they are.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: Box plot of the four iris dataset features*'
  prefs: []
  type: TYPE_NORMAL
- en: Breast Cancer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our second dataset, the Wisconsin Diagnostic Breast Cancer dataset, is also
    in sklearn, and you can also download it from the UCI Machine Learning Repository.
    We’ll follow the preceding procedure and download the dataset to see how to process
    it. This seems unnecessary, true, but just as it’s crucial to build a good dataset
    to have any hope of training a good model, it’s equally important to learn how
    to work with data sources that are not in the format we want. Should you one day
    decide to make machine learning and data science a career, you’ll be faced with
    this issue on a near-daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset by going to *[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/)*.
    Then, click the **Data Folder** link, and save the *wdbc.data* file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset contains cell measurements taken from slides of fine-needle biopsies
    of breast masses. There are 30 continuous features and two classes: malignant
    (cancer, 212 samples) and benign (no cancer, 357 samples). This is also a popular
    dataset, with over 670,000 downloads. The first line of the file is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 842302,M,17.99,10.38,122.8,1001,0.1184, ...
  prefs: []
  type: TYPE_NORMAL
- en: The first element in that line is a patient ID number that we don’t need to
    worry about. The second element is the label—*M* for malignant, and *B* for benign.
    The rest of the numbers in the line are 30 measurements related to cell size.
    The features themselves are of different scales, so besides creating the raw dataset,
    we’ll also create a standardized version. As this is the entirety of the dataset
    and we’ll have to hold some of it back for testing, we don’t need to record the
    per feature means and standard deviations in this case. If we were able to acquire
    more data generated in the same way, perhaps from an old file that was forgotten
    about, we would need to keep these values so that we could standardize the new
    inputs. The script to build this dataset, and to generate a summary box plot,
    is in [Listing 5-2](ch05.xhtml#ch5lis2).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: '❶ with open("wdbc.data") as f:'
  prefs: []
  type: TYPE_NORMAL
- en: lines = [i[:-1] for i in f.readlines() if i != ""]
  prefs: []
  type: TYPE_NORMAL
- en: ❷ n = ["B","M"]
  prefs: []
  type: TYPE_NORMAL
- en: x = np.array([n.index(i.split(",")[1]) for i in lines],dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: y = np.array([[float(j) for j in i.split(",")[2:]] for i in lines])
  prefs: []
  type: TYPE_NORMAL
- en: i = np.argsort(np.random.random(x.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: x = x[i]
  prefs: []
  type: TYPE_NORMAL
- en: y = y[i]
  prefs: []
  type: TYPE_NORMAL
- en: z = (y - y.mean(axis=0)) / y.std(axis=0)
  prefs: []
  type: TYPE_NORMAL
- en: ❸ np.save("bc_features.npy", y)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("bc_features_standard.npy", z)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("bc_labels.npy", x)
  prefs: []
  type: TYPE_NORMAL
- en: plt.boxplot(z)
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-2: Loading the raw breast cancer dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we do is read in the raw text data ❶. We then extract each label
    and map it to 0 for benign and 1 for malignant. Note here that we used 1 for the
    natural target case, so that a model outputting a probability value is indicating
    likelihood of finding cancer ❷. We extract the 30 features per sample as floats
    using a nested list comprehension to first pull out the text of the features (i)
    and then map them to floats (j). This produces a nested list, which NumPy conveniently
    converts into a matrix of 569 rows and 30 columns.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomize the dataset and calculate the standardized version by subtracting,
    per feature, the mean value of that feature and dividing by the standard deviation.
    We’ll work with this version and examine it in the box plot of [Figure 5-2](ch05.xhtml#ch5fig2)
    ❸, which shows all 30 features after standardization.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: Box plot of the 30 breast cancer dataset features*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t need to know in this case what the features represent. We’ll work
    with the dataset under the assumption that the selected features are sufficient
    to the task of determining malignancy. Our models will indicate to us whether
    or not this is the case. The features are now all of the same scale as we can
    see by the location of the boxes on the y-axis: they’re all covering basically
    the same range. One characteristic of the data is immediately evident—namely,
    that there are many apparent outliers, as called out by the interquartile range
    (see [Figure 4-6](ch04.xhtml#ch4fig6)). These aren’t necessarily bad values, but
    they are an indicator that the data isn’t normally distributed—it doesn’t, per
    feature, follow a bell-curve-type distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Digits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our next dataset isn’t typically composed of feature vectors, but is instead
    made up of thousands of small images of handwritten digits. This dataset is the
    workhorse of modern machine learning, and one of the first datasets deep learning
    researchers go to when looking to test new ideas. It’s overused, but that’s because
    it’s so well understood and simple to work with.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has a long history, but the version we’ll use, the most common version,
    is known simply as the *MNIST dataset*. The canonical source for the dataset,
    [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/), includes
    some background material. To save time, we’ll use Keras to download and format
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Keras will return the dataset as 3D NumPy arrays. The first dimension is the
    number of images—60,000 for training and 10,000 for test. The second and third
    dimensions are the pixels of the images. The images are 28×28 pixels in size.
    Each pixel is an unsigned 8-bit integer, [0,255].
  prefs: []
  type: TYPE_NORMAL
- en: Because we want to work with models that expect vectors as inputs, and because
    we want to use this dataset to illustrate certain properties of models later in
    the book, we’ll create additional datasets from this initial one. To do so, first
    we’ll unravel the images to form feature vectors so that we can use this dataset
    with traditional models that expect vector inputs. Second, we’ll use images, but
    we’ll permute the order of the images in the dataset. We’ll permute the order
    of the pixels of each image in the same way, so while the pixels will no longer
    be in the order that produces the digit image, the reordering will be deterministic,
    and applied consistently across all images. Third, we’ll create an unraveled feature
    vector version of these permuted images. We’ll use these additional datasets to
    explore differences between traditional neural networks and convolutional neural
    network models.
  prefs: []
  type: TYPE_NORMAL
- en: Use [Listing 5-3](ch05.xhtml#ch5lis3) to build the dataset files.
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import keras
  prefs: []
  type: TYPE_NORMAL
- en: from keras.datasets import mnist
  prefs: []
  type: TYPE_NORMAL
- en: ❶ (xtrn, ytrn), (xtst, ytst) = mnist.load_data()
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(ytrn.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: xtrn = xtrn[idx]
  prefs: []
  type: TYPE_NORMAL
- en: ytrn = ytrn[idx]
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(ytst.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: xtst = xtst[idx]
  prefs: []
  type: TYPE_NORMAL
- en: ytst = ytst[idx]
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_train_images.npy", xtrn)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_train_labels.npy", ytrn)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_test_images.npy", xtst)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_test_labels.npy", ytst)
  prefs: []
  type: TYPE_NORMAL
- en: ❷ xtrnv = xtrn.reshape((60000,28*28))
  prefs: []
  type: TYPE_NORMAL
- en: xtstv = xtst.reshape((10000,28*28))
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_train_vectors.npy", xtrnv)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_test_vectors.npy", xtstv)
  prefs: []
  type: TYPE_NORMAL
- en: ❸ idx = np.argsort(np.random.random(28*28))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(60000):'
  prefs: []
  type: TYPE_NORMAL
- en: xtrnv[i,:] = xtrnv[i,idx]
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(10000):'
  prefs: []
  type: TYPE_NORMAL
- en: xtstv[i,:] = xtstv[i,idx]
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_train_scrambled_vectors.npy", xtrnv)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_test_scrambled_vectors.npy", xtstv)
  prefs: []
  type: TYPE_NORMAL
- en: ❹ t = np.zeros((60000,28,28))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(60000):'
  prefs: []
  type: TYPE_NORMAL
- en: t[i,:,:] = xtrnv[i,:].reshape((28,28))
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_train_scrambled_images.npy", t)
  prefs: []
  type: TYPE_NORMAL
- en: t = np.zeros((10000,28,28))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(10000):'
  prefs: []
  type: TYPE_NORMAL
- en: t[i,:,:] = xtstv[i,:].reshape((28,28))
  prefs: []
  type: TYPE_NORMAL
- en: np.save("mnist_test_scrambled_images.npy", t)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-3: Loading and building the various MNIST datasets*'
  prefs: []
  type: TYPE_NORMAL
- en: We start by telling Keras to load the MNIST dataset ❶. When run for the first
    time, Keras will show a message about downloading the dataset. After that, it
    won’t show the message again.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset itself is stored in four NumPy arrays. The first, xtrn, has a shape
    of (60000, 28, 28) for the 60,000 training images, each 28×28 pixels. The associated
    labels are in ytrn as integers, [0,9]. The 10,000 test images are in xtst with
    labels in ytst. We also randomize the order of the samples and write the arrays
    to disk for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we unravel the training and test images and turn them into vectors of
    784 elements ❷. Unraveling takes the first row of pixels followed by the second
    row and so on until all rows are laid end to end. We get 784 elements because
    28 × 28 = 784.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we generate a permutation of the 784 elements in the unraveled
    vectors (idx) ❸.
  prefs: []
  type: TYPE_NORMAL
- en: We use the permuted vectors to form new, scrambled, digit images and store them
    on disk ❹. The scrambled images are made from the scrambled vectors by undoing
    the unravel operation. In NumPy, this is just a call to the reshape method of
    the vector arrays. Note that at no time do we alter the relative ordering of the
    images, so we need to store only one file each for the train and test labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-3](ch05.xhtml#ch5fig3) shows representative digits from the MNIST
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-3: Representative MNIST digit images*'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need to standardize the images, as we know they’re all on the same
    scale already, since they’re pixels. We’ll sometimes scale them as we use them,
    but for now we can leave them on disk as byte grayscale images. The dataset is
    reasonably balanced; [Table 5-1](ch05.xhtml#ch5tab1) shows the training distribution.
    Therefore, we don’t need to worry about imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Digit Frequencies for the MNIST Training Set'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Digit** | **Count** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5,923 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 6,742 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 5,958 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6,131 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 5,842 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 5,421 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 5,918 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 6,265 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 5,851 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5,949 |'
  prefs: []
  type: TYPE_TB
- en: CIFAR-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*CIFAR-10* is another standard deep learning dataset that’s small enough for
    us to use without requiring a lot of training time or a GPU. As with MNIST, we
    can extract the dataset with Keras, which will download it the first time it’s
    requested. The source page for CIFAR-10 is at *https://www.cs.toronto.edu/\%7Ekriz/cifar.html*.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth perusing the page to learn more about where the dataset came from.
    It consists of 60,000 32×32 pixel RGB images from 10 classes, with 6,000 samples
    in each class. The training set contains 50,000 images, and the test set contains
    10,000 images. The 10 classes are shown here in [Table 5-2](ch05.xhtml#ch5tab2).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-2:** CIFAR-10 Class Labels and Names'
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Class |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | airplane |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | automobile |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | bird |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | cat |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | deer |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | dog |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | frog |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | horse |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | ship |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | truck |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows, row by row, a collection of representative
    images from each class. Let’s extract the dataset, store it for future use, and
    create vector representations, much as we did for MNIST.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-4: Representative CIFAR-10 images*'
  prefs: []
  type: TYPE_NORMAL
- en: The script to do all of this is in [Listing 5-4](ch05.xhtml#ch5lis4).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import keras
  prefs: []
  type: TYPE_NORMAL
- en: from keras.datasets import cifar10
  prefs: []
  type: TYPE_NORMAL
- en: ❶ (xtrn, ytrn), (xtst, ytst) = cifar10.load_data()
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(ytrn.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: xtrn = xtrn[idx]
  prefs: []
  type: TYPE_NORMAL
- en: ytrn = ytrn[idx]
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(ytst.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: xtst = xtst[idx]
  prefs: []
  type: TYPE_NORMAL
- en: ytst = ytst[idx]
  prefs: []
  type: TYPE_NORMAL
- en: np.save("cifar10_train_images.npy", xtrn)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("cifar10_train_labels.npy", ytrn)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("cifar10_test_images.npy", xtst)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("cifar10_test_labels.npy", ytst)
  prefs: []
  type: TYPE_NORMAL
- en: ❷ xtrnv = xtrn.reshape((50000,32*32*3))
  prefs: []
  type: TYPE_NORMAL
- en: xtstv = xtst.reshape((10000,32*32*3))
  prefs: []
  type: TYPE_NORMAL
- en: np.save("cifar10_train_vectors.npy", xtrnv)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("cifar10_test_vectors.npy", xtstv)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-4: Loading and building the various CIFAR-10 datasets*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load CIFAR-10 from Keras ❶. As with MNIST, the dataset will download
    automatically the first time that the code is run. And, as with MNIST, we randomize
    the train and test splits. The training data is in xtrn as a (50,000; 32; 32;
    3) array. The last dimension is for the three color components for each pixel:
    red, green, and blue. The test data is similar, and is in xtst as a (10,000; 32;
    32; 3) array. Finally, we write the randomized train and test images to disk.
    Next, we unravel the images to produce 32 × 32 × 3 = 3072 element feature vectors
    representing the images ❷ and write them to disk.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in [Chapter 4](ch04.xhtml#ch04), the dataset is everything, so it
    needs to be as complete as possible. You’ll normally achieve this by carefully
    selecting samples that fit within the range of inputs the model will encounter
    when you use it. Thinking back to our earlier analogy, we need the model to *interpolate*
    and not *extrapolate*. But sometimes, even though we have a wide range of possible
    samples, we don’t have a lot of actual samples. This is where data augmentation
    can help.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data augmentation* uses the data in the existing dataset to generate new possible
    samples to add to the set. These samples are always based, in some way, on the
    existing data. Data augmentation is a powerful technique and is particularly helpful
    when our actual dataset is small. In a practical sense, data augmentation should
    probably be used whenever it’s feasible.'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation takes the data we already have and modifies it to create new
    samples that could have plausibly come from the same parent distribution as our
    actual data. That means that if we were patient enough to keep collecting real
    data, we could measure those new samples. Sometimes data augmentation can go beyond
    what we would actually measure, yet still help the model learn to generalize to
    the actual data. For example, a model using images as input might benefit from
    unrealistic colors or backgrounds when the actual inputs to the model would never
    use those colors or backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: While data augmentation works in many situations and is a mainstay of deep learning,
    you won’t always be able to use it because not all data can be realistically enhanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll take a look at why we’d want to consider using data
    augmentation and how we might go about doing it. We’ll then augment two of the
    datasets we developed previously, so when we build models, we can see how augmentation
    affects the models’ learning. As far as augmentation is concerned, here’s a rule
    of thumb: in general, you should perform data augmentation whenever possible,
    especially if the dataset is small.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Should You Augment Training Data?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.xhtml#ch04), we encountered the curse of dimensionality.
    We saw that the solution to it, for many models, is to fill in the space of possible
    inputs with more and more training data. Data augmentation is one way we can fill
    in this space. We’ll need to do this in the future; in [Chapter 6](ch06.xhtml#ch06),
    for example, we’ll meet the *k*-Nearest Neighbor classifier, perhaps the simplest
    of all classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: This classifier depends, critically, on having enough training data to adequately
    fill in the input feature space. If there are three features, then the space is
    three-dimensional and the training data will fit into some cube in that space.
    The more training data we have, the more samples we’ll have in the cube, and the
    better the classifier will do. That’s because the classifier measures the distance
    between points in the training data and that of a new, unknown feature vector
    and votes on what label to assign. The denser the space is with training points,
    the more often the voting process will succeed. Loosely speaking, data augmentation
    fills in this space. For most datasets, acquiring more data, more samples of the
    parent distribution, will not fill in every part of the feature space but will
    create a more and more complete picture of what the parent distribution looks
    like in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: When we work with modern deep learning models ([Chapter 12](ch12.xhtml#ch12)),
    we’ll see that data augmentation has additional benefits. During training, a neural
    network becomes conditioned to learn features of the training data. If the features
    the network learns to pay attention to are actually useful for distinguishing
    the classes, all is well. But, as we saw with the wolf and husky example of [Chapter
    4](ch04.xhtml#ch04), sometimes the network learns the wrong thing, which can’t
    be used to generalize to new inputs—like the fact that the wolf class images had
    snow in the background and the husky images did not.
  prefs: []
  type: TYPE_NORMAL
- en: Taking steps to avoid this tendency is known as *regularization*. Regularization
    helps the network learn important features of the training data, ones that generalize
    as we want them to. Data augmentation is—short of acquiring more actual data—perhaps
    the simplest way to regularize the network as it learns. It conditions the learning
    process to not pay attention to quirks of the particular samples selected for
    the training set but to instead focus on more general features of the data. At
    least, that is the hope.
  prefs: []
  type: TYPE_NORMAL
- en: An additional benefit of data augmentation is that it lessens the likelihood
    of overfitting when training. We’ll discuss overfitting more in [Chapter 9](ch09.xhtml#ch09),
    but succinctly, it’s what happens when the model learns the training data nearly
    perfectly without learning to generalize to new inputs. Using a small dataset
    can lead to overfitting if the model is able to basically memorize the training
    data. Data augmentation increases the dataset size, reducing the probability of
    overfitting and possibly allowing use of a model with a larger capacity. (Capacity
    is a nebulous concept. Think “bigger,” in that the model can learn more of what
    is important in the training data, while still generalizing to new data.)
  prefs: []
  type: TYPE_NORMAL
- en: 'One extremely important point needs to be made about data augmentation as it
    relates to the training/validation/test split of the dataset: you *must* be sure
    that every augmented sample belongs to the same set. For example, if we augment
    sample *X*[12345], and this sample has been assigned to the training set, then
    we must ensure that *all* of the augmented samples based on *X*[12345] are also
    members of the training set. This is so important that it’s worth reiterating:
    *be sure to never mix an augmented sample based on an original sample between
    the training, validation, and test sets.*'
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t follow this rule, our beliefs about the quality of the model will
    be unfounded, or at least partially unwarranted, because there will be samples
    in the validation and test sets that are, essentially, also in the training set,
    since they’re based on the training data. This warning may seem unnecessary, but
    it’s really easy to make this mistake, especially if working with others or with
    a database of some kind.
  prefs: []
  type: TYPE_NORMAL
- en: The correct way to augment data is *after* the training, validation, and test
    splits have been made. Then, augment at least the training data and label all
    the new samples as training data.
  prefs: []
  type: TYPE_NORMAL
- en: What about augmenting the validation and test splits? It isn’t wrong to do so,
    and might make sense if you don’t have a lot of either. I haven’t run across any
    studies that tried to be rigorous about the effects of augmenting the validation
    and test data, but, conceptually, it shouldn’t hurt, and might even help.
  prefs: []
  type: TYPE_NORMAL
- en: Ways to Augment Training Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To augment a dataset, we need to generate new samples from it that are plausible,
    meaning they could really occur in the dataset. For images, this is straightforward;
    you can often rotate the image, or flip it horizontally or vertically. Other times,
    you can manipulate the pixels themselves to change the contrast or alter the colors.
    Some have even gone so far as to simply swap entire color bands—swapping the red
    channel with the blue channel, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the manipulations must make sense. A subtle rotation might mimic
    a change in the camera’s orientation, and a left-to-right flip might mimic the
    experience of looking in a mirror. But a top-to-bottom flip probably wouldn’t
    be as realistic. True, a monkey might hang upside-down in a picture, but flipping
    the picture would flip the tree and the ground, as well. On the other hand, you
    might be able to do a top-to-bottom flip in an aerial image, which shows objects
    in any orientation.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so images are generally straightforward to augment, and it’s easy to understand
    whether the augmentation makes sense. Augmentation of a feature vector is more
    subtle. It’s not always clear how to do it, or if it’s even possible. What can
    we do in that case?
  prefs: []
  type: TYPE_NORMAL
- en: Again, the guiding principle is that the augmentation makes sense. If we encoded
    color as a one-hot vector of, say, red, green, or blue, and an instance of a class
    can be red or green or blue, then one way to augment is to shift the color between
    red, green, and blue. If a sample can represent male or female, then we could
    also change those values to get a new sample of the same class but with a different
    gender.
  prefs: []
  type: TYPE_NORMAL
- en: These are unusual things to do, however. Typically, you try to augment continuous
    values, creating a new feature vector that still represents the original class.
    We’ll examine one way to do this next by augmenting the iris dataset. After that,
    we’ll augment the CIFAR-10 dataset to see how to work with images.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting the Iris Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The iris dataset has 150 samples from three classes, each with four continuous
    features. We’ll augment it by using *principal component analysis (PCA)*. This
    is an old technique, in use for over a century. It was common in machine learning
    before the advent of deep learning to combat the curse of dimensionality, because
    it can reduce the number of features in a dataset. It also has a variety of uses
    outside of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we have a dataset with only two features—for example, the first
    two features of the iris dataset. A scatter plot of these features will show us
    where the samples fall in 2D space. [Figure 5-5](ch05.xhtml#ch5fig5) shows a plot
    of the first two features of the iris dataset for classes 1 and 2\. The plot has
    shifted the origin to (0,0) by subtracting the mean value of each feature. This
    does not change the variance or scatter of the data, only its origin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot in [Figure 5-5](ch05.xhtml#ch5fig5) also shows two arrows. These are
    the two principal components of the data. Since the data is 2D, we have two components.
    If we had 100 features, then we would have up to 100 principal components. This
    is what PCA does: it tells you the directions of the variance of the data. These
    directions are the *principal components*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-5: The first two iris features for classes 1 and 2, with their principal
    components*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The principal components also tell you how much of the variance of the data
    is explained by each of these directions. In the plot, the length of the arrow
    corresponds to the fraction of the total variance explained by each component.
    As you can see, the largest component is along the diagonal that matches the greatest
    scatter of the points. Traditional machine learning uses PCA to reduce the number
    of features while still, hopefully, representing the dataset well. This is how
    PCA can help fight the curse of dimensionality: find the principal components
    and then throw the less influential ones away. However, for data augmentation,
    we want to keep all the components.'
  prefs: []
  type: TYPE_NORMAL
- en: The code that produced [Figure 5-5](ch05.xhtml#ch5fig5) is in [Listing 5-5](ch05.xhtml#ch5lis5).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pylab as plt
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn import decomposition
  prefs: []
  type: TYPE_NORMAL
- en: ❶ x = np.load("../data/iris/iris_features.npy")[:,:2]
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/iris/iris_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.where(y != 0)
  prefs: []
  type: TYPE_NORMAL
- en: x = x[idx]
  prefs: []
  type: TYPE_NORMAL
- en: x[:,0] -= x[:,0].mean()
  prefs: []
  type: TYPE_NORMAL
- en: x[:,1] -= x[:,1].mean()
  prefs: []
  type: TYPE_NORMAL
- en: ❷ pca = decomposition.PCA(n_components=2)
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(x)
  prefs: []
  type: TYPE_NORMAL
- en: v = pca.explained_variance_ratio_
  prefs: []
  type: TYPE_NORMAL
- en: ❸ plt.scatter(x[:,0],x[:,1],marker='o',color='b')
  prefs: []
  type: TYPE_NORMAL
- en: ax = plt.axes()
  prefs: []
  type: TYPE_NORMAL
- en: x0 = v[0]*pca.components_[0,0]
  prefs: []
  type: TYPE_NORMAL
- en: y0 = v[0]*pca.components_[0,1]
  prefs: []
  type: TYPE_NORMAL
- en: ax.arrow(0, 0, x0, y0, head_width=0.05, head_length=0.1, fc='r', ec='r')
  prefs: []
  type: TYPE_NORMAL
- en: x1 = v[1]*pca.components_[1,0]
  prefs: []
  type: TYPE_NORMAL
- en: y1 = v[1]*pca.components_[1,1]
  prefs: []
  type: TYPE_NORMAL
- en: ax.arrow(0, 0, x1, y1, head_width=0.05, head_length=0.1, fc='r', ec='r')
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel("$x_0$", fontsize=16)
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("$x_1$", fontsize=16)
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-5: Iris PCA plot*'
  prefs: []
  type: TYPE_NORMAL
- en: Much of the preceding code is to make the plot ❸. The imports are standard except
    for a new one from sklearn, the decomposition module. We load the iris dataset
    we previously saved, keeping only the first two features in x and the labels in
    y. We then keep only class 1 and class 2 features by excluding class 0\. Next,
    we subtract the per feature means to center the data about the point (0,0) ❶.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we create the PCA object and fit the iris data to it ❷. There are two
    features, so the number of components in this case is also two. The PCA Python
    class mimics the standard approach of sklearn: it defines the model, then fits
    data to it. Once this is done, we have the principal components stored in pca
    and accessible via the components_ member variable. We set v to a vector representing
    the fraction of the variance in the data explained by each of the principal component
    directions. Since there are two components, this vector also has two components.'
  prefs: []
  type: TYPE_NORMAL
- en: The components are always listed in decreasing order, so that the first component
    is the direction describing the majority of the variance, the second component
    is the next most important, and so on. In this case, the first component describes
    some 84 percent of the variance and the second describes the remaining 16 percent.
    We’ll use this ordering when we generate new augmented samples. Here we use the
    fraction to scale the length of the arrows in the plot showing the principal component
    directions and relative importance.
  prefs: []
  type: TYPE_NORMAL
- en: How is [Figure 5-5](ch05.xhtml#ch5fig5) useful for data augmentation? Once you
    know the principal components, you can use PCA to create derived variables, which
    means you rotate the data to align it with the principal components. The transform
    method of the PCA class does this by mapping an input—in our case, the original
    data—to a new representation where the variance is aligned with the principal
    components. This mapping is exact, and you can reverse it by using the inverse_transform
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this alone doesn’t generate new samples for us. If we take the original
    data, x, transform it to the new representation, and then inverse transform it,
    we’ll end up where we started, with x. But, if we transform x and then, before
    calling the inverse transform, *modify* some of the principal components, we’ll
    return a new set of samples that are not x but are based on x. This is precisely
    what we want for data augmentation. Next, we’ll see which components to modify,
    and how.
  prefs: []
  type: TYPE_NORMAL
- en: The components are ordered in pca by their importance. We want to keep the most
    important components as they are, because we want the inverse transform to produce
    data that looks much like the original data. We don’t want to transform things
    too much, or the new samples won’t be plausible instances of the class we claim
    they represent. We’ll arbitrarily say that we want to keep the components that,
    cumulatively, represent some 90 percent to 95 percent of the variance in the data.
    These we won’t modify at all. The remaining components will be modified by adding
    normally distributed noise. Recall that *normally distributed* means it follows
    the bell curve so that most of the time the value will be near the middle, which
    we’ll set to 0, meaning no change to the component, and increasingly rarely to
    larger values. We’ll add the noise to the existing component and call the inverse
    transform to produce new samples that are very similar but not identical to the
    originals.
  prefs: []
  type: TYPE_NORMAL
- en: The previous paragraph is pretty dense. The code will make things easier to
    understand. Our approach to generating augmented data is shown in [Listing 5-6](ch05.xhtml#ch5lis6).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: from sklearn import decomposition
  prefs: []
  type: TYPE_NORMAL
- en: '❶ def generateData(pca, x, start):'
  prefs: []
  type: TYPE_NORMAL
- en: original = pca.components_.copy()
  prefs: []
  type: TYPE_NORMAL
- en: ncomp = pca.components_.shape[0]
  prefs: []
  type: TYPE_NORMAL
- en: a = pca.transform(x)
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(start, ncomp):'
  prefs: []
  type: TYPE_NORMAL
- en: pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)
  prefs: []
  type: TYPE_NORMAL
- en: b = pca.inverse_transform(a)
  prefs: []
  type: TYPE_NORMAL
- en: pca.components_ = original.copy()
  prefs: []
  type: TYPE_NORMAL
- en: return b
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  prefs: []
  type: TYPE_NORMAL
- en: ❷ x = np.load("../../../data/iris/iris_features.npy")
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../../../data/iris/iris_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: N = 120
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x[:N]
  prefs: []
  type: TYPE_NORMAL
- en: y_train = y[:N]
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x[N:]
  prefs: []
  type: TYPE_NORMAL
- en: y_test = y[N:]
  prefs: []
  type: TYPE_NORMAL
- en: pca = decomposition.PCA(n_components=4)
  prefs: []
  type: TYPE_NORMAL
- en: pca.fit(x)
  prefs: []
  type: TYPE_NORMAL
- en: print(pca.explained_variance_ratio_)
  prefs: []
  type: TYPE_NORMAL
- en: start = 2
  prefs: []
  type: TYPE_NORMAL
- en: ❸ nsets = 10
  prefs: []
  type: TYPE_NORMAL
- en: nsamp = x_train.shape[0]
  prefs: []
  type: TYPE_NORMAL
- en: newx = np.zeros((nsets*nsamp, x_train.shape[1]))
  prefs: []
  type: TYPE_NORMAL
- en: newy = np.zeros(nsets*nsamp, dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: '❹ for i in range(nsets):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if (i == 0):'
  prefs: []
  type: TYPE_NORMAL
- en: newx[0:nsamp,:] = x_train
  prefs: []
  type: TYPE_NORMAL
- en: newy[0:nsamp] = y_train
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: newx[(i*nsamp):(i*nsamp+nsamp),:] =
  prefs: []
  type: TYPE_NORMAL
- en: generateData(pca, x_train, start)
  prefs: []
  type: TYPE_NORMAL
- en: newy[(i*nsamp):(i*nsamp+nsamp)] = y_train
  prefs: []
  type: TYPE_NORMAL
- en: ❺ idx = np.argsort(np.random.random(nsets*nsamp))
  prefs: []
  type: TYPE_NORMAL
- en: newx = newx[idx]
  prefs: []
  type: TYPE_NORMAL
- en: newy = newy[idx]
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris_train_features_augmented.npy", newx)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris_train_labels_augmented.npy", newy)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris_test_features_augmented.npy", x_test)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("iris_test_labels_augmented.npy", y_test)
  prefs: []
  type: TYPE_NORMAL
- en: main()
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-6: Augmenting the iris data with PCA. See* iris_data_augmentation.py.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main function ❷ loads the existing iris data, x, and the corresponding
    labels, y, and then calls PCA, this time using all four features of the dataset.
    This gives us the four principal components telling us how much of the variance
    is explained by each component:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.92461621 0.05301557 0.01718514 0.00518309
  prefs: []
  type: TYPE_NORMAL
- en: The first two principal components describe over 97 percent of the variance.
    Therefore, we’ll leave the first two components alone, indices 0 and 1, and start
    with index 2 when we want to generate new samples.
  prefs: []
  type: TYPE_NORMAL
- en: We next declare the number of sets we’ll define ❸. A *set* here means a new
    collection of samples. Since the samples are based on the original data, x, with
    150 samples, each new set will contain 150 samples as well. In fact, they’ll be
    in the same order as the original samples, so that the class label that should
    go with each of these new samples is in the same order as the class labels in
    y. We don’t want to lose our original data, either, so nsets=10 puts the original
    data and nine new sets of samples based on that original data—for a total of 1,500
    samples—in the new dataset. We grab the number of samples in x, 150, and define
    the arrays to hold our new features (newx) and associated labels (newy).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we loop to generate the new samples, one set of 150 at a time ❹. The first
    pass simply copies the original data into the output arrays. The remaining passes
    are similar, updating the source and destination indices of the output arrays
    appropriately, but instead of assigning x, we assign the output of generateData.
    When the loop is done, we scramble the order of the entire dataset and write it
    to disk ❺.
  prefs: []
  type: TYPE_NORMAL
- en: All of the magic is in generateData ❶. We pass in the PCA object (pca), the
    original data (x), and the starting principal component index (start). We set
    the last argument to 2 to leave the two most important components alone. We keep
    a copy of the actual components so we can reset the pca object before we return.
    Then we define ncomp, the number of principal components, for convenience and
    call the forward transformation mapping the original data along the principal
    components.
  prefs: []
  type: TYPE_NORMAL
- en: The loop updates the two least important components by adding a random value
    drawn from a normal curve with mean value 0 and a standard deviation of 0.1\.
    Why 0.1? No special reason; if the standard deviation is small, then the new samples
    will be near the old samples, while if it’s larger, they’ll be farther away and
    possibly not representative of the class anymore. Next, we call the inverse transformation
    using the modified principal components and restore the actual components. Finally,
    we return the new set of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the new dataset, shown in [Figure 5-6](ch05.xhtml#ch5fig6). The
    big gray dots are from our original dataset, and the smaller black dots are the
    augmented samples. As we can readily see, they all fall near an existing sample,
    which is what we would expect from modifying only the weakest of the principal
    components. Since we copied the original data into the augmented dataset, each
    big dot has a small dot at the center.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-6: The first two features of the original iris dataset (large dots)
    and the augmented features generated by [Listing 5-6](ch05.xhtml#ch5lis6) (small
    points)*'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is appropriate for continuous features only, as was previously
    stated, and you should be careful to modify only the weakest of the principal
    components, and only by a small amount. Experimentation is important here. As
    an exercise, try applying the same technique to augment the breast cancer dataset,
    which also consists of continuous features.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting the CIFAR-10 Dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Augmenting the iris dataset involved a lot of discussion and some less than
    obvious math. Fortunately for us, augmenting images is generally a lot simpler,
    but still just as effective when training modern models. When we build convolutional
    neural network models ([Chapter 12](ch12.xhtml#ch12)), we’ll see how to do augmentation
    on the fly when training, a particularly helpful approach, but for now we’ll do
    the augmentation first and build a new dataset with additional versions of the
    existing images.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-4](ch05.xhtml#ch5fig4) shows representative images from each class
    in the CIFAR-10 dataset. These are color images stored as RGB data for the red,
    green, and blue channels. They were taken from ground level, so top and bottom
    flips do not make sense here, while left and right flips do. Translations—shifting
    the image in the *x* or *y* direction, or both—are one common technique. Small
    rotations are another common technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, each of these raises an issue: what to do with pixels that have no
    data after the shift or rotate? If I shift an image 3 pixels to the left, I need
    to fill in the three columns on the right with something. Or, if I rotate to the
    right, there will be pixels at the upper right and lower left that need to be
    filled in. There are several ways to handle this. One is to simply leave the pixels
    black, or all 0 values, and let the model learn that there is no helpful information
    there. Another is to replace the pixels with the mean value of the image, which
    also provides no information and will, we hope, be ignored by the model. However,
    the most popular solution is to crop the image.'
  prefs: []
  type: TYPE_NORMAL
- en: The image is 32×32 pixels. Pulling a random patch from the image of, say, 28×28
    pixels is the equivalent of shifting the image by a random *x* or *y* position
    of up to 4 pixels without needing to worry about filling in anything. If we rotate
    the image first, which will require interpolation of the pixels, and then crop
    to remove the edge regions, we’ll again have no empty pixels to worry about. Keras
    has tools for doing this via an image generator object used during training. When
    we’re using Keras to build models, we’ll make use of it, but for now, we’ll do
    all of the work ourselves in order to understand the process.
  prefs: []
  type: TYPE_NORMAL
- en: We need to mention one point here. So far, we’ve talked about building a dataset
    for training a model. What should we do when we want to use the model? Do we hand
    the model random croppings of the test inputs as well? No. Instead, we hand the
    model a cropping centered on the image. So, for CIFAR-10, we would take each 32
    × 32 test input and crop it to 28 × 28 by dropping the outer 6 pixels, then present
    that to the model. We do this because the center crop still represents the actual
    test image and not some augmented version of it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-7](ch05.xhtml#ch5fig7) illustrates what we mean by rotations, flips,
    random croppings for training, and center cropping for testing. In (a) we rotate
    the image and take a center crop. The output image is in the white square. In
    (b) we flip left to right and crop randomly. In (c), we take two random crops
    without flipping, and in (d) we take a center crop for testing, without any rotation
    or flip. Some people augment test images, but we won’t do so here.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-7: Rotate, then center crop (a). Flip left to right, then crop randomly
    (b). Two random crops during training (c). Center crop for testing, with no rotation
    or flip (d).*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5-7](ch05.xhtml#ch5lis7) shows how to augment the CIFAR-10 training
    set with random crops, rotations, and flips.'
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: '❶ def augment(im, dim):'
  prefs: []
  type: TYPE_NORMAL
- en: img = Image.fromarray(im)
  prefs: []
  type: TYPE_NORMAL
- en: 'if (np.random.random() < 0.5):'
  prefs: []
  type: TYPE_NORMAL
- en: img = img.transpose(Image.FLIP_LEFT_RIGHT)
  prefs: []
  type: TYPE_NORMAL
- en: 'if (np.random.random() < 0.3333):'
  prefs: []
  type: TYPE_NORMAL
- en: z = (32-dim)/2
  prefs: []
  type: TYPE_NORMAL
- en: r = 10*np.random.random()-5
  prefs: []
  type: TYPE_NORMAL
- en: img = img.rotate(r, resample=Image.BILINEAR)
  prefs: []
  type: TYPE_NORMAL
- en: img = img.crop((z,z,32-z,32-z))
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: x = int((32-dim-1)*np.random.random())
  prefs: []
  type: TYPE_NORMAL
- en: y = int((32-dim-1)*np.random.random())
  prefs: []
  type: TYPE_NORMAL
- en: img = img.crop((x,y,x+dim,y+dim))
  prefs: []
  type: TYPE_NORMAL
- en: return np.array(img)
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  prefs: []
  type: TYPE_NORMAL
- en: ❷ x = np.load("../data/cifar10/cifar10_train_images.npy")
  prefs: []
  type: TYPE_NORMAL
- en: y = np.load("../data/cifar10/cifar10_train_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: factor = 10
  prefs: []
  type: TYPE_NORMAL
- en: dim = 28
  prefs: []
  type: TYPE_NORMAL
- en: z = (32-dim)/2
  prefs: []
  type: TYPE_NORMAL
- en: newx = np.zeros((x.shape[0]*factor, dim,dim,3), dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: newy = np.zeros(y.shape[0]*factor, dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: k=0
  prefs: []
  type: TYPE_NORMAL
- en: '❸ for i in range(x.shape[0]):'
  prefs: []
  type: TYPE_NORMAL
- en: im = Image.fromarray(x[i,:])
  prefs: []
  type: TYPE_NORMAL
- en: im = im.crop((z,z,32-z,32-z))
  prefs: []
  type: TYPE_NORMAL
- en: newx[k,...] = np.array(im)
  prefs: []
  type: TYPE_NORMAL
- en: newy[k] = y[i]
  prefs: []
  type: TYPE_NORMAL
- en: k += 1
  prefs: []
  type: TYPE_NORMAL
- en: 'for j in range(factor-1):'
  prefs: []
  type: TYPE_NORMAL
- en: newx[k,...] = augment(x[i,:], dim)
  prefs: []
  type: TYPE_NORMAL
- en: newy[k] = y[i]
  prefs: []
  type: TYPE_NORMAL
- en: k += 1
  prefs: []
  type: TYPE_NORMAL
- en: idx = np.argsort(np.random.random(newx.shape[0]))
  prefs: []
  type: TYPE_NORMAL
- en: newx = newx[idx]
  prefs: []
  type: TYPE_NORMAL
- en: newy = newy[idx]
  prefs: []
  type: TYPE_NORMAL
- en: np.save("../data/cifar10/cifar10_aug_train_images.npy", newx)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("../data/cifar10/cifar10_aug_train_labels.npy", newy)
  prefs: []
  type: TYPE_NORMAL
- en: ❹ x = np.load("../data/cifar10/cifar10_test_images.npy")
  prefs: []
  type: TYPE_NORMAL
- en: newx = np.zeros((x.shape[0], dim,dim,3), dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(x.shape[0]):'
  prefs: []
  type: TYPE_NORMAL
- en: im = Image.fromarray(x[i,:])
  prefs: []
  type: TYPE_NORMAL
- en: im = im.crop((z,z,32-z,32-z))
  prefs: []
  type: TYPE_NORMAL
- en: newx[i,...] = np.array(im)
  prefs: []
  type: TYPE_NORMAL
- en: np.save("../data/cifar10/cifar10_aug_test_images.npy", newx)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-7: Augmenting the CIFAR-10 dataset. See* cifar10_augment.py.'
  prefs: []
  type: TYPE_NORMAL
- en: The main function loads the existing dataset and defines our augmentation factor,
    crop size, and a constant for defining a center crop ❷.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new image will be put in newx, which has the following dimensions: (500,000;28;28;3);
    there are 50,000 training images, each with 32×32 pixels and three color bands.
    We set the augmentation factor to 10\. Similarly, there will be 500,000 labels.
    The counter, k, will index into this new dataset. For every image in the old dataset,
    we’ll create nine completely new versions and center crop the original ❶ ❸.'
  prefs: []
  type: TYPE_NORMAL
- en: As the dataset consists of images, it’s easiest to work with the data in image
    form, so we make the current sample an actual PIL image in order to easily crop
    it. This is the center crop of the original image. We store it in the new output
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two Python idioms here that we’ll see more than once. The first is
    to turn a NumPy array representing an image into a PIL image:'
  prefs: []
  type: TYPE_NORMAL
- en: im = Image.fromarray(arr)
  prefs: []
  type: TYPE_NORMAL
- en: 'The second is to go the other way and turn a PIL image into a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: arr = np.array(im)
  prefs: []
  type: TYPE_NORMAL
- en: We must be sure that the NumPy array is a valid image data type like unsigned
    byte (uint8). Use the astype NumPy array method to cast between types, remembering
    that you bear all responsibility for understanding what that casting entails.
  prefs: []
  type: TYPE_NORMAL
- en: Referring back to [Listing 5-7](ch05.xhtml#ch5lis7), we are creating the nine
    versions of the current image. For each of these, we simply copy the label and
    assign the output array an augmented version. We’ll describe the augment function
    shortly. Once the new dataset has been constructed, we scramble the order and
    write the augmented training dataset to disk ❸.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not quite done, however. We created an augmented training set that cropped
    the original 32 × 32 images to 28 × 28\. We must, therefore, at least crop the
    original test set ❹. As we stated previously, we use a center crop and no augmentation
    of the test data. Therefore, we simply load the test dataset, define the new output
    test dataset, and run a loop that crops the 32 × 32 images to 28 × 28\. When done,
    we write the cropped test data to disk. Note that we did not modify the *order*
    of the images in the test set; we simply cropped them, so we do not need to write
    a new file for the test labels.
  prefs: []
  type: TYPE_NORMAL
- en: The augment function ❶ is where all the action is. We immediately change the
    input NumPy array into an actual PIL image object. We next decide, with a 50-50
    chance, whether or not we will flip the image left to right. Note that we do not
    crop the image just yet.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we ask whether we should rotate the image or not. We select rotation with
    a probability of 33 percent (1 in 3 chance). Why 33 percent? No particular reason,
    but it seems that we might want to crop randomly more often than we rotate. We
    could even drop this probability down to 20 percent (1 in 5 chance). If we do
    rotate, we select the rotation angle, [*–*5,5] and then call the rotate method
    using bilinear interpolation to make the rotated image look a bit nicer than simply
    using the nearest neighbor, which is the PIL default. Next, we center crop the
    rotated image. This way, we will not get any black pixels on the edges where the
    rotation had no image information to work with.
  prefs: []
  type: TYPE_NORMAL
- en: If we do not rotate, we are free to select a random crop. We choose the upper-left
    corner of this random crop, ensuring that the cropped square will not exceed the
    dimensions of the original image. Finally, we convert the data back to a NumPy
    array and return.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we built four datasets that we’ll use as examples throughout
    the rest of the book. The first two, irises and breast cancer histology, are based
    on feature vectors. The last two, MNIST and CIFAR-10, are represented as images.
    We then learned about two data augmentation methods: augmenting a feature vector
    of continuous values using PCA and, more critical for deep learning, augmenting
    images by basic transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll transition to our discussion of classical machine
    learning models. In the chapter after that, we’ll use these datasets with those
    models.
  prefs: []
  type: TYPE_NORMAL
