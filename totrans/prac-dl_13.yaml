- en: '**13'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EXPERIMENTS WITH KERAS AND MNIST**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the last chapter, we covered the essential components and functionality of
    a CNN. In this chapter, we’ll work with our test model from [Chapter 12](ch12.xhtml#ch12).
    We’ll first learn how to implement and train it in Keras. After that, we’ll conduct
    a set of experiments that will build our intuition for how different architectures
    and learning parameter choices affect the model.
  prefs: []
  type: TYPE_NORMAL
- en: From there, we’ll move beyond classification of simple input images and expand
    the network by converting it into a fully convolutional model capable of processing
    arbitrary inputs and locating digits wherever they occur in the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'After fully convolutional networks, we’ll wander a little deeper into the pool
    of deep learning and fulfill a promise made in [Chapter 7](ch07.xhtml#ch07): we’ll
    explore how well CNNs perform on the scrambled MNIST digit experiment. We saw
    in [Chapter 10](ch10.xhtml#ch10) that scrambling the pixels of the digits made
    it virtually impossible for us to see what the digit was but had little to no
    effect on how well a traditional neural network was able to interpret the digits.
    Is the same true with a CNN? We’ll find out.'
  prefs: []
  type: TYPE_NORMAL
- en: Building CNNs in Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model from [Figure 12-3](ch12.xhtml#ch12fig3) is straightforward to implement
    in Python using the `keras` library. We’ll list the code first, explain it, and
    then run it to see what sort of output it produces. The code naturally falls into
    three sections. The first loads the MNIST data and configures it for Keras; the
    second builds the model; and the third trains the model and applies it to the
    test data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the MNIST Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 13-1](ch13.xhtml#ch13lis1) has the first part of our code.'
  prefs: []
  type: TYPE_NORMAL
- en: import keras
  prefs: []
  type: TYPE_NORMAL
- en: from keras.datasets import mnist
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import Sequential
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Dense, Dropout, Flatten
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Conv2D, MaxPooling2D
  prefs: []
  type: TYPE_NORMAL
- en: from keras import backend as K
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 128
  prefs: []
  type: TYPE_NORMAL
- en: num_classes = 10
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 12
  prefs: []
  type: TYPE_NORMAL
- en: img_rows, img_cols = 28, 28
  prefs: []
  type: TYPE_NORMAL
- en: ❶ (x_train, y_train), (x_test, y_test) = mnist.load_data()
  prefs: []
  type: TYPE_NORMAL
- en: '❷ if K.image_data_format() == ''channels_first'':'
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = (1, img_rows, img_cols)
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = (img_rows, img_cols, 1)
  prefs: []
  type: TYPE_NORMAL
- en: ❸ x_train = x_train.astype('float32')
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.astype('float32')
  prefs: []
  type: TYPE_NORMAL
- en: x_train /= 255
  prefs: []
  type: TYPE_NORMAL
- en: x_test /= 255
  prefs: []
  type: TYPE_NORMAL
- en: ❹ y_train = keras.utils.to_categorical(y_train, num_classes)
  prefs: []
  type: TYPE_NORMAL
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-1: Loading and data preprocessing*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras is a rather large toolkit consisting of many modules. We import the library
    first and then specific functions from it. The `mnist` module gives us access
    to the MNIST data from within Keras; the `Sequential` model type is for implementing
    a CNN. Our CNN will need some specific layers, the ones we saw used in [Figure
    12-3](ch12.xhtml#ch12fig3): Dense, Dropout, Flatten, Conv2D, and MaxPool2D, all
    of which we import. Keras supports a plethora of other layers; I encourage you
    to spend some quality time with their documentation pages: *[https://keras.io/](https://keras.io/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set the learning parameters, including the number of epochs, classes,
    and minibatch size. There are 10 classes, and the images are 28×28 pixel grayscale.
    Like sklearn, in Keras, you specify the number of epochs (full passes through
    the training set), not the number of minibatches that should be processed. Keras
    automatically processes the entire training set per epoch in sets of the minibatch
    size—here 128 samples at a time. Recall that MNIST’s training set consists of
    60,000 samples, so there are at least 60,000/128 = 468 minibatches per epoch using
    integer division. There will be 469 if Keras uses the remainder, the samples that
    do not build a complete minibatch. Remember that each minibatch process results
    in a gradient descent step: an update of the parameters of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: After loading the MNIST train and test data ❶ come a few lines of code that
    may seem somewhat mysterious at first ❷. Keras is a higher-level toolkit that
    uses potentially different lower-level backends. In our case, the backend is TensorFlow,
    which we installed in [Chapter 1](ch01.xhtml#ch01). Different backends expect
    the model input in different forms. The `image_data_format` function returns a
    string indicating where the underlying toolkit expects to see the number of channels
    or filters for convolutional layers. The TensorFlow backend returns `channels_last`,
    meaning it expects an image to be represented as a 3D array of H × W × C, where
    H is the image height, W is the image width, and C is the number of channels.
    For a grayscale image like MNIST, the number of channels is 1\. The code in ❷
    reformats the input images to match what Keras is expecting to see.
  prefs: []
  type: TYPE_NORMAL
- en: The next block of code converts the byte image values to floating-point numbers
    in the range [0,1] ❸. This is the only scaling done to the input data, and this
    type of scaling is typical of CNNs that work with images.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `to_categorical` function is used to map the class labels in `y_test`
    to one-hot vector representations ❹, which is how Keras wants to see the labels.
    As we’ll see, the model has 10 outputs, so the mapping is to a vector of 10 elements;
    each element is 0 except for the element whose index corresponds to the label
    in `y_test`. That element is set to 1\. For example, `y_test[333]` is of class
    6 (a “6” digit). After the call to `to_categorical`, `y_test[333]` becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: where all entries are 0 except index 6, which is 1.
  prefs: []
  type: TYPE_NORMAL
- en: Building Our Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the dataset preprocessed, we can build our model. The code shown in [Listing
    13-2](ch13.xhtml#ch13lis2) builds the exact model we defined with pictures in
    [Figure 12-3](ch12.xhtml#ch12fig3).
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu',
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=input_shape))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  prefs: []
  type: TYPE_NORMAL
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  prefs: []
  type: TYPE_NORMAL
- en: optimizer=keras.optimizers.Adadelta(),
  prefs: []
  type: TYPE_NORMAL
- en: metrics=['accuracy'])
  prefs: []
  type: TYPE_NORMAL
- en: print("Model parameters = %d" % model.count_params())
  prefs: []
  type: TYPE_NORMAL
- en: print(model.summary())
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-2: Building the MNIST model*'
  prefs: []
  type: TYPE_NORMAL
- en: Keras defines the model as an instance of the `Sequential` class. The model
    is built by adding layers to that instance, hence all the calls to the `add` method.
    The argument to `add` is the new layer. The layers are added from the input side
    to the output side, so the first layer we need to add is the 2D convolutional
    layer that uses a 3 × 3 kernel on the input image. Note, we are not specifying
    the number of images nor the minibatch size; Keras will handle that for us when
    the model is put together and trained. Right now, we are defining the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Using the architecture defined in [Figure 12-3](ch12.xhtml#ch12fig3), the first
    layer is a `Conv2D` layer. The first argument is the number of filters; here,
    32\. The kernel size is given as a tuple, `(3,3)`. Kernels don’t need to be square,
    hence the kernel width and height. It’s possible that the spatial relationship
    of the parts of your input might be better detected with a non-square kernel.
    If so, Keras lets you use one. That said, almost all kernels in practical use
    are square. After the kernel, we define an activation function to apply to the
    output of the convolutional layer, here a ReLU. The shape of the input to this
    layer is explicitly defined via `input_shape`, and we saw that earlier for our
    MNIST model using a TensorFlow backend, the shape is a tuple, `(28,28,1)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we add the second convolutional layer. This one has 64 filters, also
    using a 3 × 3 kernel and a ReLU activation on the output. Note, we do not need
    to specify the shape here: Keras knows the input shape because it knows the shape
    of the previous convolutional layer’s output.'
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling comes next. We explicitly state that the pooling size is 2 × 2,
    with an implied stride of 2\. If we wanted to use average pooling here, we would
    replace `MaxPooling2D` with `AveragePooling2D`.
  prefs: []
  type: TYPE_NORMAL
- en: After pooling comes our first dropout layer, which uses a 25 percent probability
    of dropping an output, here the output of the max-pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed earlier how Keras separates the operations of a fully connected
    layer into Flatten and Dense layers. This allows more fine-grained control of
    the architecture. We add a `Flatten` layer to map the pooling output to a vector
    and then pass this vector to a `Dense` layer to implement the classic fully connected
    layer. The dense layer has 128 nodes and uses a ReLU for the activation function.
    If we want dropout on the output of the dense layer, we need to add it explicitly,
    so we add one with a probability of 50 percent.
  prefs: []
  type: TYPE_NORMAL
- en: The final dense layer has 10 nodes, one for each possible class label. The activation
    is set to `softmax` to get a softmax output on the inputs to this layer. Since
    this is the last layer we define, the output of this layer, the softmax probabilities
    for membership in each of the 10 classes, is the output of the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: To configure the model for training, we need to call the `compile` method. This
    sets the loss function used during training (`loss`) and the specific optimization
    algorithm to use (`optimizer`). The `metrics` keyword is used to define which
    metrics to report during training. For our example, we are using the categorical
    cross-entropy loss, which is the multiclass version of the binary cross-entropy
    loss. We described this loss function in [Chapter 9](ch09.xhtml#ch09); it is the
    go-to loss function for many CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: We will need to discuss the optimizer keyword more thoroughly. In [Chapter 9](ch09.xhtml#ch09),
    we presented gradient descent and the more common version, stochastic gradient
    descent. As you might expect, the machine learning community has not been content
    to simply use this algorithm as is; much research has been done to see if it can
    be improved upon for training neural networks. This has led to the development
    of multiple variations on gradient descent, many of which Keras supports.
  prefs: []
  type: TYPE_NORMAL
- en: If we want, we can use classic stochastic gradient descent here. The example,
    however, is using a variant called *Adadelta*. This is itself a variant of the
    Adagrad algorithm that seeks to change the learning rate (step size) intelligently
    during training. For practical purposes, we should consider Adadelta an improved
    version of stochastic gradient descent. Keras also supports other optimization
    approaches that we do not intend to cover here, but you can read about in the
    Keras documentation, particularly Adam and RMSprop.
  prefs: []
  type: TYPE_NORMAL
- en: After the call to `compile`, our model is defined. The convenience methods `count_params`
    and `summary` produce output characterizing the model itself. When we run the
    code, we’ll see the sort of output they produce.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Evaluating the Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, with both data and model defined, we can train and then evaluate the
    model on the test data. The code for this is in [Listing 13-3](ch13.xhtml#ch13lis3).
  prefs: []
  type: TYPE_NORMAL
- en: history = model.fit(x_train, y_train,
  prefs: []
  type: TYPE_NORMAL
- en: batch_size=batch_size,
  prefs: []
  type: TYPE_NORMAL
- en: epochs=epochs,
  prefs: []
  type: TYPE_NORMAL
- en: verbose=1,
  prefs: []
  type: TYPE_NORMAL
- en: validation_data=(x_test, y_test))
  prefs: []
  type: TYPE_NORMAL
- en: score = model.evaluate(x_test, y_test, verbose=0)
  prefs: []
  type: TYPE_NORMAL
- en: print('Test loss:', score[0])
  prefs: []
  type: TYPE_NORMAL
- en: print('Test accuracy:', score[1])
  prefs: []
  type: TYPE_NORMAL
- en: model.save("mnist_cnn_model.h5")
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-3: Training and testing the MNIST model*'
  prefs: []
  type: TYPE_NORMAL
- en: The `fit` method trains the network using the supplied training samples (`x_train`)
    and a one-hot vector version of the associated class label (`y_test`). We also
    pass in the number of epochs and the minibatch size. Setting `verbose` to 1 will
    produce the output shown in [Listing 13-4](ch13.xhtml#ch13lis4). Lastly, we have
    `validation` `_data`. For this example, we’re being a bit sloppy and passing in
    all the test data instead of holding some back for final testing. (This is just
    a simple example, after all.) Normally, we’d hold some test data back to use after
    the final model has been trained. This ensures that results on this held-out test
    data represent what we might encounter when using the model in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the `fit` method returns something. This is a `History` object,
    and its `history` property holds a per epoch summary of the training and validation
    loss and accuracy values. We can use these to make summary plots if we wish.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we can get a score, similar to the `score` of sklearn,
    by calling the `evaluate` method and passing in the test data. The method returns
    a list with the loss and accuracy of the model on the supplied data, which we
    simply print.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `save` method to write the model itself to disk for future use.
    Notice the file extension. Keras dumps the model in an HDF5 file. *HDF5* is a
    generic hierarchical data format widely used in scientific circles. In this case,
    the file contains all the weights and biases of the model and the layer structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code produces the output shown in [Listing 13-4](ch13.xhtml#ch13lis4):'
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow backend.
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters = 1199882
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer (type)                 Output Shape              Param #'
  prefs: []
  type: TYPE_NORMAL
- en: ==============================================================
  prefs: []
  type: TYPE_NORMAL
- en: conv2d_1 (Conv2D)            (None, 26, 26, 32)        320
  prefs: []
  type: TYPE_NORMAL
- en: conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496
  prefs: []
  type: TYPE_NORMAL
- en: max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0
  prefs: []
  type: TYPE_NORMAL
- en: dropout_1 (Dropout)          (None, 12, 12, 64)        0
  prefs: []
  type: TYPE_NORMAL
- en: flatten_1 (Flatten)          (None, 9216)              0
  prefs: []
  type: TYPE_NORMAL
- en: dense_1 (Dense)              (None, 128)               1179776
  prefs: []
  type: TYPE_NORMAL
- en: dropout_2 (Dropout)          (None, 128)               0
  prefs: []
  type: TYPE_NORMAL
- en: dense_2 (Dense)              (None, 10)                1290
  prefs: []
  type: TYPE_NORMAL
- en: ==============================================================
  prefs: []
  type: TYPE_NORMAL
- en: 'Total params: 1,199,882'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trainable params: 1,199,882'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-trainable params: 0'
  prefs: []
  type: TYPE_NORMAL
- en: Train on 60000 samples, validate on 10000 samples
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  1/12-loss:0.2800 acc:0.9147 val_loss:0.0624 val_acc:0.9794
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  2/12-loss:0.1003 acc:0.9695 val_loss:0.0422 val_acc:0.9854
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  3/12-loss:0.0697 acc:0.9789 val_loss:0.0356 val_acc:0.9880
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  4/12-loss:0.0573 acc:0.9827 val_loss:0.0282 val_acc:0.9910
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  5/12-loss:0.0478 acc:0.9854 val_loss:0.0311 val_acc:0.9901
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  6/12-loss:0.0419 acc:0.9871 val_loss:0.0279 val_acc:0.9908
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  7/12-loss:0.0397 acc:0.9883 val_loss:0.0250 val_acc:0.9914
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  8/12-loss:0.0344 acc:0.9891 val_loss:0.0288 val_acc:0.9910
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  9/12-loss:0.0329 acc:0.9895 val_loss:0.0273 val_acc:0.9916
  prefs: []
  type: TYPE_NORMAL
- en: Epoch 10/12-loss:0.0305 acc:0.9909 val_loss:0.0296 val_acc:0.9904
  prefs: []
  type: TYPE_NORMAL
- en: Epoch 11/12-loss:0.0291 acc:0.9911 val_loss:0.0275 val_acc:0.9920
  prefs: []
  type: TYPE_NORMAL
- en: Epoch 12/12-loss:0.0274 acc:0.9916 val_loss:0.0245 val_acc:0.9916
  prefs: []
  type: TYPE_NORMAL
- en: 'Test loss: 0.02452171179684301'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test accuracy: 0.9916'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-4: MNIST training output*'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve excluded some informational and warning messages from the lower-level
    TensorFlow toolkit and condensed the output to make it easier to follow in the
    text.
  prefs: []
  type: TYPE_NORMAL
- en: At the start of the run, Keras informs us that TensorFlow is our backend. It
    also shows us the shape of the training data, the now familiar 60,000 samples
    with a shape of 28 × 28 × 1 (×1 since the images are grayscale). We have the usual
    10,000 test samples as well.
  prefs: []
  type: TYPE_NORMAL
- en: Next comes a report on the model. This report shows the layer type, shape of
    the output from the layer, and the number of parameters in the layer. For example,
    the first convolutional layer uses 32 filters and 3 × 3 kernels, so the output
    with a 28 × 28 input will be 26 × 26 × 32\. The `None` listed for each layer is
    in the place where the number of elements in the minibatch normally is. The printout
    is showing only the relationship between the layers; because nothing in the architecture
    changes the number of elements in the minibatch, there’s no need to explicitly
    mention the minibatch elements (hence the `None`). The parameters are 3 × 3 ×
    32 for the filters plus an additional 32 bias terms for the 320 parameters listed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in [Chapter 12](ch12.xhtml#ch12), the lion’s share of the parameters
    in the model are between the Flatten layer and the Dense layer. The layer named
    `dense_2` is the softmax layer mapping the 128 elements of the Dense layer to
    the 10 elements of the softmax: 128 × 10 + 10 = 1290, where the additional 10
    are the bias terms. Note that the Dropout and Pooling layers have no parameters
    because there is nothing to learn in those layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the report on the model’s structure, we have the verbose output of the
    training call to `fit`. We asked for 12 epochs—12 full passes through the training
    data—using a minibatch of 128 samples. The output lists the stats for each pass.
    We see that the loss goes down as we train, which is expected if the model is
    learning, and that the accuracy (`acc`) on the training data goes up. The validation
    data is used during training to test the model, but this data is not used to update
    the model’s weights and biases. The loss on the validation data is also going
    down with each epoch, but more slowly. What we don’t want to see here is the validation
    loss going up, though it will jump around somewhat, especially if the validation
    set is not very big. We see an opposite effect with the validation accuracy (`val_acc`).
    It is going up for each epoch of training. If the model were to start overfitting,
    we’d see this accuracy go down after some point. This is the value of validation
    data: to tell us when to stop training.'
  prefs: []
  type: TYPE_NORMAL
- en: The final two lines of output are the loss and accuracy of the model on the
    test samples passed to the `evaluate` method. Since the validation and test sets
    are the same in this example, these lines match the output for epoch 12\. The
    final accuracy of this model is 99.16 percent—certainly a very good accuracy to
    see.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the Error
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use the saved history to plot the loss or error (1 – accuracy) as a function
    of the training epoch. The plots are similar in shape, so we show only the error
    plot in [Figure 13-1](ch13.xhtml#ch13fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-1: The MNIST training and validation errors as a function of epoch*'
  prefs: []
  type: TYPE_NORMAL
- en: The error on the training data falls off quickly and, as we’ve seen before,
    tends toward 0 as training continues. In this case, the validation error falls
    slightly and then levels off at a value similar to the training error. At this
    point in the book, you might have alarm bells going off in your head when you
    look at [Figure 13-1](ch13.xhtml#ch13fig1). The initial training error is *greater
    than* the initial validation error!
  prefs: []
  type: TYPE_NORMAL
- en: The full cause of this is hard to pin down, but one component is using dropout
    in the network. Dropout is applicable only during training, and because of the
    dropping of nodes in layers, dropout is in effect training many models at once,
    which, initially, causes a large error before the model “settles down” and the
    error drops per epoch. We can see that this might be the case here because if
    we simply comment out the `Dropout` layers in [Listing 13-4](ch13.xhtml#ch13lis4)
    and retrain, we get a new error plot, [Figure 13-2](ch13.xhtml#ch13fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-2: The MNIST training and validation errors as a function of epoch
    when no Dropout layers are present*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 13-2](ch13.xhtml#ch13fig2), we see that the validation error quickly
    becomes greater than the training error, as we would expect. Additionally, we
    see that the final validation error is much greater than the final validation
    error for [Figure 13-1](ch13.xhtml#ch13fig1), about 10 percent versus 1 percent.
    This is also something we expect if dropout is actually a sensible thing to use,
    which it is. Note also that by the 12th epoch, the training set error is roughly
    the same regardless of the presence of Dropout layers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, some of what we see in [Figures 13-1](ch13.xhtml#ch13fig1) and [13-2](ch13.xhtml#ch13fig2)
    is due to the way Keras reports training and validation accuracies. The reported
    training accuracy (and loss) at the end of an epoch is the average over the epoch,
    but, of course, this is changing as the model learns and tends to increase. However,
    the validation accuracy reported is for the model as it is at the end of the epoch,
    so at times is it possible for the training accuracy to be reported as less than
    the validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how to build a simple CNN and run it on a dataset, we are
    in a position to start experimenting with CNNs. Of course, there are an infinite
    number of experiments we could perform—just look at the rate at which new papers
    on deep learning appear on sites like [arxiv.org](http://arxiv.org) or the explosion
    of attendance at machine learning conferences—so we need to restrict ourselves
    to some basic explorations. Hopefully, these will motivate you to explore more
    on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We did a bit of experimentation already when we removed the Dropout layer.
    All our experiments follow the same general pattern: make a slightly different
    version of the model, train it, and evaluate it against the test set. We will
    try three different types of experiments. The first type modifies the architecture
    of the model; removing Dropout layers falls into this category. The second type
    explores the interplay between training set size, minibatch size, and epochs.
    The last type alters the optimizer used during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In all three cases, to avoid excessive code listings, we’ll simply comment
    on the variation to the code in the previous section with the understanding that
    the remaining code is the same from experiment to experiment. We’ll number the
    experiments, and you can match the results with the number to find the actual
    Python source code for the experiment. The code is available from the website
    associated with this book: *[https://nostarch.com/practical-deep-learning-python/](https://nostarch.com/practical-deep-learning-python/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we used the entire training set of 60,000 samples for
    training and the whole test set of 10,000 samples for both validation and as the
    final test set. Here, we’ll restrict ourselves to using the first 1,000 or 1,024
    training samples as the entire training set. Additionally, we’ll use the first
    1,000 samples of the test set as the validation set and reserve the last 9,000
    samples for the final test set we’ll use when training is complete. We’ll report
    the accuracy of these 9,000 images that were unseen during training. The results
    will include the baseline model accuracy and number of parameters, for comparison
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that unless stated otherwise, the accuracies we present represent
    a single training session for each experiment. You should get slightly different
    results if you run these experiments yourself, but those slight differences shouldn’t
    outweigh the larger differences in accuracy that will result from changing the
    model and/or training process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the models in this section are multiclass, so we could examine the
    confusion matrices to see how the models are making their mistakes. However, it
    would be exceedingly tedious to do this for each experiment. Instead, we will
    use the overall accuracy as our metric, trusting that it is a sufficient measure
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture Experiments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Architecture modifications imply removing or adding new layers or altering the
    parameters of a layer. We’ve made a number of architecture modifications and compiled
    the resulting accuracies in [Table 13-1](ch13.xhtml#ch13tab1).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 13-1:** Results from Modifying the Model Architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Exp.** | **Modification** | **Test accuracy** | **Parameters** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Baseline | 92.70% | 1,199,882 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Add Conv3, 3 × 3 × 64 before Pooling | 94.30% | 2,076,554 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Duplicate Conv2, Pooling layer | 94.11% | 261,962 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Conv1, 3 × 3 × 32 to 5 × 5 × 32 | 93.56% | 1,011,978 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Dense layer to 1,024 nodes | 92.76% | 9,467,274 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Conv1, Conv2, halve number of filters | 92.38% | 596,042 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Second Dense layer with 128 nodes | 91.90% | 1,216,394 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Dense layer to 32 nodes | 91.43% | 314,090 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Remove Pooling layer | 90.68% | 4,738,826 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | No ReLU after conv layers | 90.48% | 1,199,882 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Remove Conv2 | 89.39% | 693,962 |'
  prefs: []
  type: TYPE_TB
- en: In [Table 13-1](ch13.xhtml#ch13tab1), the baseline results and model size are
    given first, followed by the various experiments from most accurate to least accurate.
    Let’s look at the table and interpret the results.
  prefs: []
  type: TYPE_NORMAL
- en: First, we see that adding a third convolutional layer after the second convolutional
    layer (Experiment 1) improves the performance of the model but also adds 876,672
    parameters. Increasing the depth of the network seems to improve the performance
    of the model but at the expense of increasing the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: However, in Experiment 2 we also increase the depth of the network by duplicating
    the second convolutional layer and the following pooling layer, but because of
    the second pooling layer, the total number of parameters in the network goes down
    by 937,920\. This is a substantial saving for virtually the same performance.
    This indicates that depth is good, but so is the judicious use of pooling layers
    to keep the number of parameters small. For this dataset, Experiment 2 is a solid
    architecture to use.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we see that an adjustment to the kernel size of the first convolutional
    layer, Experiment 3, leads to an improvement relative to the baseline. There are
    more parameters in the first convolutional layer (832 versus 320), but because
    of the edge effects when using an exact convolution, by the time we get to the
    output of the Flatten layer, there are now only 7744 values versus 9216 for the
    baseline model. This means that the large matrix between the Flatten and Dense
    layers goes from 1,179,776 down to 991,360 parameters with a net result that the
    model overall has 187,904 fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is good: better performance and fewer parameters to learn. Is there a
    downside to the change of Experiment 3? Not really. Instead, one might argue that
    adjusting the kernel size for the first convolutional layer has made the model
    more appropriate for the spatial information in the digit images, thereby making
    the new representation learned by the convolutional and pooling layers that much
    better at separating the classes. In general, there seems to be a best kernel
    size for the first convolutional layer, the layer that deals with the input to
    the model. That kernel size is related to the spatial structure of the inputs:
    some sizes will be better at detecting input features that are better for separating
    the classes. This general rule does not appear to hold for higher convolutional
    layers, and there the prevailing wisdom is to use 3 × 3 kernels for most convolutional
    layers except the first.'
  prefs: []
  type: TYPE_NORMAL
- en: Can we combine Experiment 3 and Experiment 2? Certainly. We simply make the
    first convolutional layer of Experiment 2 use a 5 × 5 kernel instead of a 3 ×
    3 kernel. If we do this, we get a model with an overall accuracy of 94.23 percent
    that needs only 188,746 parameters. With this trivial change, we’ve achieved the
    performance of Experiment 10 by using only 9 percent of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be tempted to simply increase the size of the Dense layer, the layer
    that can be thought of as using the new feature representation discovered by the
    convolutional and pooling layers below it. However, doing so (Experiment 4) results
    in no real improvement in overall accuracy, but with a substantial increase in
    the number of parameters. We know the cause: the 9,216 × 128 weight matrix between
    the Flatten and Dense layers is now a 9,216 × 1,024 matrix. Clearly, for CNNs,
    we want to create the best feature representation so that a simpler top layer
    can be used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With Experiment 5, we see that we can make the model significantly smaller,
    a reduction of 603,840 parameters, while still achieving the same overall accuracy
    by simply halving the number of filters learned in each of the convolutional layers:
    32 → 16 for Conv1 and 64 → 32 for Conv2\. Again, this is a good optimization provided
    the slight (perhaps in this case meaningless) difference in accuracy is acceptable.
    If we look again at [Figure 12-8](ch12.xhtml#ch12fig8), we can see that, especially
    for the second convolutional layer with 64 filters, the responses are very similar
    for many filters. This implies that there are redundant filters that are not adding
    much to the new feature representation presented to the Dense layers. Despite
    halving the number of filters learned, there are still filters that learn to capture
    the important aspects of the input data used to separate the classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment 7 plays with the Dense layer nodes, and Experiment 6 adds a second
    Dense layer. Neither offers a real benefit. For Experiment 7, the change in the
    number of model parameters is significant due to the 9,216 × 128 matrix weight
    becoming a 9,216 × 32 matrix. However, 32 nodes does not seem to be the ideal
    number to make use of the new feature representation. The second Dense layer of
    Experiment 6 isn’t too awful in terms of increasing the number of parameters to
    learn, but it isn’t buying us much, either. If we use a larger training set, we
    might get some improvement, but we’ll leave that as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we read about the criticisms levied against pooling
    layers. What if we remove the pooling layer entirely (Experiment 8)? First, we
    see that accuracy drops relative to the baseline model. Worse, we see that the
    size of the network has increased dramatically, from 1,199,882 parameters to 4,738,826,
    by a factor of nearly four. This is due to the increase in the number of elements
    in the output of the Flatten layer, which has gone from 9,216 to 36,864, resulting
    in a weight matrix of 36,864 × 128 + 128 = 4,718,720 elements. This example demonstrates
    why we use pooling layers even with the price they bring in terms of the loss
    of information about the relative position of object parts.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the convolutional layers in the baseline model uses ReLU on its outputs.
    Removing these ReLU operations, Experiment 9, leads to a 2 percent reduction in
    accuracy on the test set. Clearly, the ReLU is helping somewhat. What might it
    be doing? The ReLU leaves positive values unchanged and sets negative values to
    0\. When used with the output of a convolutional layer, the ReLU is keeping more
    strongly activated responses to the filters, positive responses, while suppressing
    negative responses. This seems to be helping the entire process of learning a
    new representation of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Experiment 10 removed Conv2 entirely. This has the greatest effect
    on the overall accuracy because the features passed to the Dense layer are then
    based solely on the output of the first convolutional layer filters. There was
    no opportunity for the model to learn from these outputs and develop filter responses
    based on the larger effective receptive field seen by the second convolutional
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, given what we saw in the results of Experiment 3, which increased
    the kernel size used by the first convolutional layer, we might wonder whether
    this change from 3 × 3 to 5 × 5 kernels might somewhat compensate for the loss
    of the second convolutional layer. Fortunately, this is very easy to test. We
    simply change the 3 × 3 kernel parameter of Conv1 to a 5 × 5 and train again.
    Doing this validates our intuition: the resulting overall accuracy increases to
    92.39 percent, virtually the same as the baseline model. Also, this 5 × 5 model
    has only 592,074 parameters, making the change inexpensive in terms of the number
    of model parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: From all of these results, do we have a winner, an architecture that is lean
    but highly effective? We do—it is Experiment 2 with a 5 × 5 kernel for the first
    convolutional layer. In Keras, to build this architecture, we need the code in
    [Listing 13-5](ch13.xhtml#ch13lis5).
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(5, 5),
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu',
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=input_shape))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-5: Building the architecture for Experiment 2*'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve simply duplicated the `Conv2D`, `MaxPooling2D`, and `Dropout` layers and
    used `(5,5)` for the kernel size of the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: If we train this model using all 60,000 samples of the MNIST training set, we
    get a final held-out test set accuracy of 99.51 percent for an error of 0.49 percent.
    This is using all 10,000 samples. According to [benchmarks.ai](http://benchmarks.ai),
    a website that tracks current bests on different machine learning datasets, the
    state-of-the-art MNIST error is 0.21 percent, so we are not state of the art,
    but we are better than the 99.16 percent accuracy we saw in [Listing 13-4](ch13.xhtml#ch13lis4)
    for the default architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Training Set Size, Minibatches, and Epochs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These experiments examine the interplay between training set size, minibatch
    size, and number of epochs. Our model will be the default model we were using
    previously, Experiment 0, but this time we’ll use 1,024 samples for the training
    set. We’ll be using powers of two as the minibatch sizes; this is a convenient
    size, as all of our minibatch sizes divide it evenly.
  prefs: []
  type: TYPE_NORMAL
- en: Recall, Keras, like sklearn, runs through a given number of epochs, or full
    passes through the training set. Additionally, the minibatch size (`batch` `_size`)
    specifies the number of samples used in each iteration, after which the average
    error (the cross-entropy loss) is used to update the parameters. Therefore, each
    processed minibatch leads to a single gradient descent step, and the training
    set size divided by the minibatch size is the number of gradient descent steps
    taken per epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our experiments, we’ll use the following minibatch sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For a training set of 1,024 samples (approximately 100 for each digit), the
    number of gradient descent steps per epoch is the reverse of this list: 1,024
    for a batch size of 1, down to 1 for a batch size of 1,024.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate two plots. The first will plot the final test set accuracy for
    the two cases: a fixed number of gradient descent steps regardless of minibatch
    size, and a fixed number of epochs irrespective of minibatch size. The second
    plot will show us the clock time to train the model for each case. The code leading
    to the plots is in Experiments 26 through 31 (fixed number of gradient descent
    steps) and Experiments 32 through 42 (fixed number of epochs).'
  prefs: []
  type: TYPE_NORMAL
- en: The test set accuracy as a function of minibatch size is given as the mean of
    five runs in [Figure 13-3](ch13.xhtml#ch13fig3), and the standard error of the
    mean given by the error bars. Let’s look at the fixed number of gradient descent
    steps regardless of minibatch size (triangles). In some toolkits, this is referred
    to as using a fixed number of iterations, where *iteration* means a single update
    leading to a gradient descent step.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-3: MNIST test set accuracy as a function of minibatch size and fixed
    gradient descent steps or fixed epochs*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of gradient descent steps was fixed at 1,024\. This means that we
    need to change the number of epochs, depending on how many minibatches are in
    the training set. For the case of a single sample used per update (`batch_size=1`),
    we get the required 1,024 steps in a single epoch, so we set the number of epochs
    to 1\. For a minibatch size of two, we get 512 steps per epoch, so we set the
    number of epochs in the code to 2 to get 1,024 steps overall. The pattern continues:
    to get the 1,024 gradient descent steps for each minibatch size, we need to set
    the number of epochs to the minibatch size.'
  prefs: []
  type: TYPE_NORMAL
- en: We see that except for the smallest minibatch sizes, when we fix the number
    of gradient descent steps, we get very consistent overall accuracies. After minibatch
    sizes of about 16, things do not change too much. The general rule for CNNs is
    to use smaller batch sizes. This seems to help with model generalization for datasets
    that are not as simple as MNIST, and, as we’ll soon see, decreases training time
    significantly.
  prefs: []
  type: TYPE_NORMAL
- en: The second curve of [Figure 13-3](ch13.xhtml#ch13fig3) shows the accuracy for
    a fixed number of epochs (circles). If we change the minibatch size but do not
    increase the number of training epochs to make the number of gradient descent
    steps remain constant, we’ll be using larger and better estimates of the gradient,
    but we’ll also be taking fewer steps. We see that doing this quickly results in
    a substantial decrease in accuracy. We should not be surprised. With a minibatch
    size of one, training for 12 epochs gives us a model that took 12 × 1,024 = 12,288
    gradient descent steps. Granted, the gradient estimate was particularly noisy
    in this case since we use only one training sample to estimate it, but with lots
    of steps, we arrived at a well-performing model all the same. By the time we get
    to a minibatch of 1,024 samples, the size of our training set, we see that we
    took only 12 gradient descent steps before calling it a day. No wonder the results
    are so poor.
  prefs: []
  type: TYPE_NORMAL
- en: The second plot of this section is [Figure 13-4](ch13.xhtml#ch13fig4).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-4: Model training time as a function of minibatch size and fixed
    gradient descent steps or fixed epochs*'
  prefs: []
  type: TYPE_NORMAL
- en: As before, let’s start with the fixed number of gradient descent steps (triangles).
    We see immediately that the training time is linearly proportional to the minibatch
    size. This is reasonable since we just saw that we need to increase the number
    of epochs to keep the number of gradient descent steps constant while increasing
    the minibatch size. Therefore, the amount of data passed through the network is
    increasing proportionally, so the time required for the forward and backward training
    passes will increase proportionally as well. From this, we see that large minibatch
    sizes cost clock time.
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed number of epochs, we see a different story. For tiny minibatch sizes,
    training time goes up due to the number of forward and backward passes. For the
    case of a minibatch of one, we need 12,288 such passes, as we just saw. However,
    by the time we get to minibatches of even 32 samples at a time, we have only 1024/32
    = 32 passes per epoch for a total of 384 for the entire training session. This
    is far fewer than for the smallest minibatch size, so we might expect the training
    time for minibatches this size or larger to be roughly constant in time, as we
    see in [Figure 13-4](ch13.xhtml#ch13fig4).
  prefs: []
  type: TYPE_NORMAL
- en: 'What can we glean from these two plots? The following: to balance run time
    and accuracy, we want to use minibatch sizes that are large enough to give us
    a reasonable estimate of the gradient but small enough that, for a fixed number
    of model updates (gradient descent steps), we can train quickly. This argues for
    minibatch sizes in the 16 to 128 range, in general. Indeed, a review of the deep
    learning literature sees minibatches in this range almost exclusively for most
    applications. For this example, minibatch sizes above 16, based on [Figure 13-3](ch13.xhtml#ch13fig3)
    (triangles), result in models that are all basically the same in terms of accuracy,
    but the training time for a model using a minibatch size of 16, according to [Figure
    13-4](ch13.xhtml#ch13fig4) (triangles), compared to one of size 1,024, is a few
    seconds versus approximately 30 minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So far, all of our experiments have used the same gradient descent algorithm,
    or optimizer: Adadelta. Let’s take a look at how our MNIST model does if we change
    the optimizer but leave everything else fixed. Our model is Experiment 0, the
    model we’ve been using all along in this chapter. We’ll continue to use 1,000
    test samples for validation and 9,000 to determine our final accuracy. However,
    instead of using the first 1,000 or 1,024 training samples, we’ll increase the
    number to the first 16,384 samples. We fix the minibatch size at 128 and the number
    of epochs to 12, as before. We’ll report the results as mean and standard error
    over five runs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras currently supports the following optimizers: stochastic gradient descent
    (SGD), RMSprop, Adagrad, Adadelta, and Adam. We’ll train using each of these in
    turn. For Adagrad, Adadelta, and Adam, we leave the parameters at their default
    settings, as recommended by the Keras documentation. For RMSprop, the only parameter
    the Keras documentation recommends adjusting is the learning rate (`lr`), which
    we set to 0.01, a typical value. For SGD, we set the learning rate to 0.01 as
    well and set standard momentum to 0.9, also a very typical value. The code is
    found in Experiments 43 through 47.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-5](ch13.xhtml#ch13fig5) shows the test set accuracy (top) and training
    time (bottom) for each of the optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-5: Test set accuracy (top) and training time (bottom) by optimization
    algorithm*'
  prefs: []
  type: TYPE_NORMAL
- en: First, notice that there is not too much difference between the results produced
    by each optimizer. This is good news. However, by looking at the error bars, it
    seems clear that Adadelta, Adagrad, and Adam all perform slightly better than
    SGD or RMSprop. This is borne out in the deep learning literature as well, though
    each dataset should be looked at independently.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of training time, the different optimizers are also roughly equivalent,
    though SGD is fastest and consistently so. This performance difference might be
    important for a very large dataset. Adam is also consistently faster than Adadelta
    for basically the same performance. Again, these results are evident in the literature
    where both SGD and Adam are widely used.
  prefs: []
  type: TYPE_NORMAL
- en: This section has been thorough in terms of minute details associated with changing
    model architectures and training parameters. Hopefully, it has helped you develop
    intuition about how to configure a CNN and how to train it. Rules of thumb are
    hard to present in this area, but I have given some general guidance. Exceptions
    to these rules abound, however, and you have to simply try things, observe the
    results, and adapt when presented with a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now from classifying simple inputs to answering the question of
    how to make a model that can locate targets in arbitrary images.
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced fully convolutional networks in [Chapter 12](ch12.xhtml#ch12).
    Let’s take our basic MNIST CNN model and convert it to a fully convolutional version
    and see how we can use it to locate digits in larger images. Our basic approach
    is to train the model using fully connected layers, as before, and then to create
    a fully convolutional version and update the weights with those from the fully
    connected model. We can then apply the new model to arbitrary size inputs to locate
    digits (hopefully).
  prefs: []
  type: TYPE_NORMAL
- en: Building and Training the Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, we train our base model on the full MNIST dataset. The only change we’ll
    make is to train for 24 epochs instead of just 12\. The output of this process
    is an HDF5 file that contains the trained weights and biases. All we then need
    to do is create the fully convolutional version by changing the fully connected
    layer and copy the weights and biases from the old model to the new. The code
    for this is straightforward, as shown in [Listing 13-6](ch13.xhtml#ch13lis6).
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import Sequential, load_model
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Dense, Dropout, Flatten
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Conv2D, MaxPooling2D
  prefs: []
  type: TYPE_NORMAL
- en: ❶ weights = load_model('mnist_cnn_base_model.h5').get_weights()
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu',
  prefs: []
  type: TYPE_NORMAL
- en: ❷ input_shape=(None,None,1)))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  prefs: []
  type: TYPE_NORMAL
- en: ❸ model.add(Conv2D(128, (12,12), activation='relu'))
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  prefs: []
  type: TYPE_NORMAL
- en: ❹ model.add(Conv2D(10, (1,1), activation='softmax'))
  prefs: []
  type: TYPE_NORMAL
- en: ❺ model.layers[0].set_weights([weights[0], weights[1]])
  prefs: []
  type: TYPE_NORMAL
- en: model.layers[1].set_weights([weights[2], weights[3]])
  prefs: []
  type: TYPE_NORMAL
- en: model.layers[4].set_weights([weights[4].reshape([12,12,64,128]), weights[5]])
  prefs: []
  type: TYPE_NORMAL
- en: model.layers[6].set_weights([weights[6].reshape([1,1,128,10]), weights[7]])
  prefs: []
  type: TYPE_NORMAL
- en: model.save('mnist_cnn_fcn_model.h5')
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-6: Creating the trained fully connected model*'
  prefs: []
  type: TYPE_NORMAL
- en: After importing the necessary Keras modules, we load the trained weights from
    the fully connected model ❶. Then, we construct the fully convolutional version
    much as we did for the fully connected version. However, there are some key differences.
    The first has to do with the input convolutional layer ❷. In the fully connected
    model, we specified the input image size here, 28×28 pixels with one channel (grayscale).
    For the fully convolutional case, we do not know the size of the input, so we
    use `None` instead for the width and height. We do know that the input will be
    a single channel image, so we leave the 1.
  prefs: []
  type: TYPE_NORMAL
- en: Since the Dense layers are the reason we have to use fixed size inputs, we replace
    them with equivalent convolutional layers ❸. We replace
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `(12,12)` is the output size of the max-pooling layer above, and the 128
    is the number of filters to learn that stand in for the 128 nodes we had before.
    Again, the critical point here is that the output of this convolutional layer
    is 1 × 1 × 128 because convolving a 12 × 12 input with a 12 × 12 kernel produces
    a single output value. The difference is that the convolutional layer is not tied
    to any fixed input size as the combination of Flatten and Dense was.
  prefs: []
  type: TYPE_NORMAL
- en: The final softmax layer also needs to be made fully convolutional ❹. There are
    ten outputs, one per digit, and the activation remains the same. The kernel size,
    however, is 1 × 1\. The input to this layer is 1 × 1 × 128, so the kernel size
    to cover it is 1 × 1\. Again, if we were to work through the math, we’d see that
    a 1 × 1 × 128 input to a 1 × 1 × 10 convolutional layer matches that of a fully
    connected layer of 128 nodes mapped to 10 nodes in the next layer. The difference
    here is that if the input is larger than 1 × 1, we can still convolve over it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve constructed the fully convolutional version of our model, we
    need to copy the weights from the trained fully connected model to it ❺. We loaded
    the trained weights into `weights`. This is a list of NumPy arrays for the weights
    and biases, layer by layer. So, `weights[0]` refers to the weights of the first
    `Conv2D` layer, and `weights[1]` are the biases. Similarly, `weights[2]` and `weights[3]`
    are the weights and bias values for the second convolutional layer. We set them
    in the new fully convolutional model by updating the proper layers via the `set_weights`
    method. Layers 0 and 1 are the two convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 4 is the new `Conv2D` layer that replaced the Flatten and Dense layers
    of the original model. Here when we set the weights, we need to reshape them to
    match the form of a convolutional layer: 12 × 12 × 64 × 128\. This is for a 12
    × 12 kernel mapped over 64 inputs leading to 128 outputs. The 64 is the number
    of 12 × 12 outputs from the pooling layer above.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we set the output layer weights. Again, we need to reshape them to
    1 × 1 × 128 × 10 for the 1 × 1 × 128 input and 10 outputs. The biases for the
    two new `Conv2D` layers are in `weights[5]` and `weights[7]`, so we add them as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: The fully convolutional model is now defined and completely populated with the
    weights and biases from the fully connected model. [Figure 13-6](ch13.xhtml#ch13fig6)
    shows the mapping between models, with the original architecture on the left and
    the fully convolutional architecture on the right. The boxes represent layers,
    with the top set of numbers being the input and the bottom the output. For the
    fully convolutional model, input height and width are arbitrary and marked with
    “`--`”.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-6: Mapping a fully connected model (left) to a fully convolutional
    model (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: All that is left to do is write the new fully convolutional model to disk, and
    it’s ready to use. Let’s see how.
  prefs: []
  type: TYPE_NORMAL
- en: Making the Test Images
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To test the fully convolutional model, we first need images with digits. Unlike
    our training images, which were small and had a single digit in the center, we
    want larger test images that contain many digits in arbitrary locations. The MNIST
    dataset consists of shades of gray on a black background; therefore, our test
    images should have a black background also. This will make the test images come
    from the same “domain” as the training images, which, as we’ve emphasized before,
    is critical. Making models adapt to different data domains is an active research
    area. Search for *domain adaptation*.
  prefs: []
  type: TYPE_NORMAL
- en: Making the test images by using Python and the digits from the MNIST test set
    is straightforward. We didn’t use the test set images for training, so using them
    to make our larger test images isn’t cheating. The code is shown in [Listing 13-7](ch13.xhtml#ch13lis7).
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: import random
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf images; mkdir images")
  prefs: []
  type: TYPE_NORMAL
- en: 'if (len(sys.argv) > 1):'
  prefs: []
  type: TYPE_NORMAL
- en: N = int(sys.argv[1])
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: N = 10
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("data/mnist/mnist_test_images.npy")
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(N):'
  prefs: []
  type: TYPE_NORMAL
- en: ❶ r,c = random.randint(6,12), random.randint(6,12)
  prefs: []
  type: TYPE_NORMAL
- en: g = np.zeros(r*c)
  prefs: []
  type: TYPE_NORMAL
- en: '❷ for j in range(r*c):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if (random.random() < 0.15):'
  prefs: []
  type: TYPE_NORMAL
- en: g[j] = 1
  prefs: []
  type: TYPE_NORMAL
- en: g = g.reshape((r,c))
  prefs: []
  type: TYPE_NORMAL
- en: g[:,0] = g[0,:] = g[:,-1] = g[-1,:] = 0
  prefs: []
  type: TYPE_NORMAL
- en: ❸ img = np.zeros((28*r,28*c), dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: 'for x in range(r):'
  prefs: []
  type: TYPE_NORMAL
- en: 'for y in range(c):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if (g[x,y] == 1):'
  prefs: []
  type: TYPE_NORMAL
- en: ❹ n = random.randint(0, x_test.shape[0])
  prefs: []
  type: TYPE_NORMAL
- en: im = x_test[n]
  prefs: []
  type: TYPE_NORMAL
- en: img[28*x:(28*x+28), 28*y:(28*y+28)] = im
  prefs: []
  type: TYPE_NORMAL
- en: Image.fromarray(img).save("images/image_%04d.png" % i)
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-7: Building large MNIST test set images*'
  prefs: []
  type: TYPE_NORMAL
- en: We’re making use of the MNIST test set file we created in [Chapter 5](ch05.xhtml#ch05).
    We could just as quickly have loaded the test images through Keras, as we did
    earlier for our basic CNN experiments. The code itself creates an output directory,
    *images*, and gets the number of images to build from the command line, if given
    (`N`).
  prefs: []
  type: TYPE_NORMAL
- en: The images are of random sizes ❶. Here, `r` and `c` are the number of rows and
    columns in the large image in terms of the number of 28 × 28 MNIST digits. To
    decide where to place our digits so they don’t overlap, we create a grid, `g`,
    with either a 0 or a 1 in each possible digit position (`r*c` of them) ❷. There
    is a 15 percent chance that any grid position will contain a 1\. We then reshape
    the grid into an actual 2D array and set the border positions of the grid to 0
    to ensure that no digits appear on the edge of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The actual output image is then defined ❸ as the number of rows and columns
    are multiplied by 28, the width and height of the MNIST digit. We loop over each
    digit position (`x` and `y`), and if the grid value at that row and column is
    1, then we select a digit at random and copy it to the current row and column
    digit position in the output image (`img`) ❹. When every grid position has been
    examined, the image is written to disk so that we can use it with our fully convolutional
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s test the model—first on single MNIST digits and then on the randomly generated
    large digit images. The fully convolutional model should work as well with single
    MNIST digits as the fully connected model did. The code to test this assertion
    is in [Listing 13-8](ch13.xhtml#ch13lis8).
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import load_model
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("data/mnist/mnist_test_images.npy")/255.0
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("data/mnist/mnist_test_labels.npy")
  prefs: []
  type: TYPE_NORMAL
- en: model = load_model("mnist_cnn_fcn_model.h5")
  prefs: []
  type: TYPE_NORMAL
- en: N = y_test.shape[0]
  prefs: []
  type: TYPE_NORMAL
- en: nc = nw = 0.0
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(N):'
  prefs: []
  type: TYPE_NORMAL
- en: ❶ p = model.predict(x_test[i][np.newaxis,:,:,np.newaxis])
  prefs: []
  type: TYPE_NORMAL
- en: c = np.argmax(p)
  prefs: []
  type: TYPE_NORMAL
- en: 'if (c == y_test[i]):'
  prefs: []
  type: TYPE_NORMAL
- en: nc += 1
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: nw += 1
  prefs: []
  type: TYPE_NORMAL
- en: print("Single MNIST digits, n=%d, accuracy = %0.2f%%" % (N, 100*nc/N))
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-8: Verifying that the fully convolutional model works with single
    MNIST digits*'
  prefs: []
  type: TYPE_NORMAL
- en: We load the MNIST test images and labels, along with the fully convolutional
    model, and then loop over each test image and ask the model to make a prediction
    ❶. Note, the image is 2D, but we must pass a 4D array to the `predict` method,
    hence using `np.newaxis` to create the missing axes. The prediction for the digit
    is stored in `p` as a vector of per class probabilities. The label associated
    with the largest of these probabilities is the label assigned to the input digit
    by the model `c`. If `c` matches the actual test label, we increment the number
    of correct predictions (`nc`); otherwise, we increment the number of wrong predictions
    (`nw`). Once all 10,000 test images have been processed, we can output the overall
    accuracy, which is 99.25 percent for my training of the fully convolutional model.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, the fully convolutional model is highly accurate, but so what? We passed
    single-digit images to it as inputs and got a single output value. We had this
    capability before with the fully connected model. To expose the utility of the
    fully convolutional model, let’s now pass the large MNIST digit images as input.
    In code, we do this as shown in [Listing 13-9](ch13.xhtml#ch13lis9).
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import load_model
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: model = load_model("mnist_cnn_fcn_model.h5")
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf results; mkdir results")
  prefs: []
  type: TYPE_NORMAL
- en: n = len(os.listdir("images"))
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(n):'
  prefs: []
  type: TYPE_NORMAL
- en: f = "images/image_%04d.png" % i
  prefs: []
  type: TYPE_NORMAL
- en: ❶ im = np.array(Image.open(f))/255.0
  prefs: []
  type: TYPE_NORMAL
- en: p = model.predict(im[np.newaxis,:,:,np.newaxis])
  prefs: []
  type: TYPE_NORMAL
- en: np.save("results/results_%04d.npy" % i, p[0,:,:,:])
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-9: Running the fully convolutional model over large test images*'
  prefs: []
  type: TYPE_NORMAL
- en: We import the necessary modules and then load the fully convolutional model.
    We then create a new output directory, *results*, and find the number of large
    digit images (`n`). Next, we loop over each of the large digit images.
  prefs: []
  type: TYPE_NORMAL
- en: After loading the image from disk, being careful to make a NumPy array from
    it and scaling it by 255 since the training data was also scaled by 255 ❶, we
    make a prediction and store the model output in `p`. Notice, we make a 4D input
    to `predict`, just as we did for the single digits earlier, but this time, `im`
    is larger than 28 × 28 and contains multiple digits. Because the model is fully
    convolutional, this isn’t an issue; we will not get an error. Instead, `p` is
    a 4D array with the first dimension of one, the number of input images, and a
    final dimension of ten, the number of digits. The middle two dimensions of `p`
    are a function of the size of the input passed to the `predict` method. Since
    the input was larger than 28×28 pixels, the entire model convolved over the input
    image as though the model was a convolutional layer with a kernel of 28 × 28\.
    Specifically, the output of this convolution has height and width of
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/334equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *H*,*W* are the height and width of the input image and *h*,*w* are the
    height and width of the output array from `predict`. The 28 in the formula is
    the size of the inputs we initially trained on, 28 × 28 digit images. Where did
    the mysterious 2 come from in the denominator? This is the stride of the 28 ×
    28 kernel over the input image. It is 2 because that is the factor the input image
    is changed by when it gets down to the fully convolutional output layers. The
    input was 28 × 28, but, after the two convolutional layers and the pooling layer,
    the input is mapped to 12 × 12 and ⌊28/12⌋ = 2.
  prefs: []
  type: TYPE_NORMAL
- en: We stated that the array in `p` is 4D; now we know we get a specific-size output
    based on convolving a 28 × 28 region over the input image using a stride of 2\.
    What do we get at each of the *h*,*w* output array positions? The last element
    of the 4D output has size 10; these are the per class predictions at the specific
    *h*,*w* output position that corresponds to the 28 × 28 kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this abstract description more concrete. The upper-left corner of
    [Figure 13-7](ch13.xhtml#ch13fig7) shows one of the large input images where we’ve
    inverted it to make it black on a white background and added a border so you can
    see the full size of the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-7: Per digit heatmap output of the fully convolutional model for
    the input image on the upper left. The model was trained on the standard MNIST
    dataset.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image in the top left of [Figure 13-7](ch13.xhtml#ch13fig7) is 336 pixels
    wide and 308 pixels tall, meaning the output from passing this image to the model
    will be an array that is 1 × 141 × 155 × 10, exactly what we expect from the equations
    on page 744 for the output array dimensions. The output array represents the model’s
    predictions at each of the 28 × 28 regions of the input image when using a stride
    of 2\. There is one prediction for each digit. For example, if `p` is the 4D output
    of the `predict` method for the image on the left of [Figure 13-7](ch13.xhtml#ch13fig7),
    then `p[0,77,88,:]` will return a 10-element vector representing the per class
    probabilities of each digit class for the 28 × 28 input region of the image that
    maps to 77 × 88\. In this case, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that there is no strong likelihood, according to the model, that
    any particular digit is present at this location. We know this because all the
    output probabilities are much lower than even the minimum cutoff threshold of
    0.5\. The output of `predict` can be thought of as a probability map, typically
    called a *heatmap*, giving us the probability that there is a digit in that location.
    The model output can be thought of as 10 heatmaps, one for each digit.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining images of [Figure 13-7](ch13.xhtml#ch13fig7) show the heatmaps
    for each of the 10 digits, again inverted so that higher probabilities are darker.
    The heatmaps were thresholded at 0.98, meaning any probability value less than
    0.98 was set to 0\. This removes the weak outputs like the ones we just saw. We
    are interested only in the model’s strongest responses per digit. To make the
    heat maps, we double the size of the output from the model and set the output
    image locations with an offset to account for the position of the convolutional
    output. This is akin to what we saw in [Figure 12-1](ch12.xhtml#ch12fig1), where
    the convolution operation returns an output that is smaller than the input when
    no zero padding is used. Specifically, the code that produces the digit heatmaps
    is in [Listing 13-10](ch13.xhtml#ch13lis10).
  prefs: []
  type: TYPE_NORMAL
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  prefs: []
  type: TYPE_NORMAL
- en: ❶ threshold = float(sys.argv[1])
  prefs: []
  type: TYPE_NORMAL
- en: iname = sys.argv[2]
  prefs: []
  type: TYPE_NORMAL
- en: rname = sys.argv[3]
  prefs: []
  type: TYPE_NORMAL
- en: outdir= sys.argv[4]
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf %s; mkdir %s" % (outdir, outdir))
  prefs: []
  type: TYPE_NORMAL
- en: ❷ img = Image.open(iname)
  prefs: []
  type: TYPE_NORMAL
- en: c,r = img.size
  prefs: []
  type: TYPE_NORMAL
- en: hmap = np.zeros((r,c,10))
  prefs: []
  type: TYPE_NORMAL
- en: res = np.load(rname)
  prefs: []
  type: TYPE_NORMAL
- en: x,y,_ = res.shape
  prefs: []
  type: TYPE_NORMAL
- en: xoff = (r - 2*x) // 2
  prefs: []
  type: TYPE_NORMAL
- en: yoff = (c - 2*y) // 2
  prefs: []
  type: TYPE_NORMAL
- en: '❸ for j in range(10):'
  prefs: []
  type: TYPE_NORMAL
- en: h = np.array(Image.fromarray(res[:,:,j]).resize((2*y,2*x)))
  prefs: []
  type: TYPE_NORMAL
- en: hmap[xoff:(xoff+x*2), yoff:(yoff+y*2),j] = h
  prefs: []
  type: TYPE_NORMAL
- en: np.save("%s/graymaps.npy" % outdir, hmap)
  prefs: []
  type: TYPE_NORMAL
- en: ❹ hmap[np.where(hmap < threshold)] = 0.0
  prefs: []
  type: TYPE_NORMAL
- en: 'for j in range(10):'
  prefs: []
  type: TYPE_NORMAL
- en: img = np.zeros((r,c), dtype="uint8")
  prefs: []
  type: TYPE_NORMAL
- en: 'for x in range(r):'
  prefs: []
  type: TYPE_NORMAL
- en: 'for y in range(c):'
  prefs: []
  type: TYPE_NORMAL
- en: ❺ img[x,y] = int(255.0*hmap[x,y,j])
  prefs: []
  type: TYPE_NORMAL
- en: img = 255-img
  prefs: []
  type: TYPE_NORMAL
- en: Image.fromarray(img).save("%s/graymap_digit_%d.png" % (outdir, j))
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-10: Building heatmap images*'
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re calling the output images *graymaps* because they’re grayscale images
    representing the response of the model to different locations in the input image.
    We first pass in the threshold value, the source image name, the response of the
    model to that source image, and an output directory where the graymaps will be
    written ❶. This directory is overwritten each time. Next, the source image is
    loaded to get its dimensions ❷. These are used to create the output heatmaps (`hmap`).
    We also load the associated model responses (`res`) and calculate the offsets.
    Note that `hmap` is the same size as the image. We then fill in each digit graymap
    of `hmap` with the resized model response ❸ and store the full set of graymaps
    in the output directory.
  prefs: []
  type: TYPE_NORMAL
- en: To make the output grayscale images like those shown in [Figure 13-7](ch13.xhtml#ch13fig7),
    we first threshold the heatmaps, setting any value less than the supplied cutoff
    to 0 ❹. Then, for each digit, we create an output image and simply scale the remaining
    heatmap values by 255 since they are probabilities in the range [0,1) ❺. Then,
    before writing the image to disk, we invert by subtracting from 255\. This makes
    stronger activations dark and weaker activations lighter. Because of the strong
    threshold applied (0.98), our output graymaps are effectively binary; this is
    what we want to indicate where the model is most certain of a digit being located.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look back at [Figure 13-7](ch13.xhtml#ch13fig7) and see if we can interpret
    these responses. The source image has one 0 on the lower right. If we look at
    the graymap for digit 0, we see a single dark blob in that location. This means
    the model has indicated a strong response that there is a 0 digit at that location.
    So far, so good. However, we also see another strong response from the model near
    the 4 on the left side of the input image. The model has made a mistake. The input
    has two 4s in it. If we look at the graymaps for digit 4, we see two dark blobs
    corresponding to these digits, but we also see many other small areas of strong
    activation near other digits that are not 4s. The model we trained was over 99
    percent accurate on single MNIST test digits, so why are the responses of the
    fully convolutional model so noisy? Just look at all the small strong responses
    for 2s when the input does not contain any 2s. Sometimes, the model is doing well,
    as for 8s where the graymap shows strong responses for all the 8s in the input,
    but then it does poorly for other digits like 7s. And, there are no 5s at all,
    but the model is returning many hits.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an opportunity for us to expand our thinking and intuition. We trained
    the model on the standard MNIST digit dataset. All of the digits in this dataset
    are well centered in the images. However, when the model is convolved over the
    large input image, there will be many times when the input to the model is not
    a well-centered digit but only part of a digit. The model has never seen partial
    digits, and since it must give an answer, it sometimes offers answers that are
    meaningless—the part of a digit it sees may be part of a 6, for example, but the
    model “thinks” it is a 5.
  prefs: []
  type: TYPE_NORMAL
- en: One possible solution is to teach the model about partial MNIST digits. We can
    do this by augmenting the standard MNIST dataset with shifted versions of its
    digits. Imagine a 4 shifted to the lower right so that only part of it is visible.
    It will still be labeled a 4, so the model will have an opportunity to learn what
    a shifted 4 digit looks like. The code to make this shifted dataset is in the
    file *make_shifted_mnist_dataset.py*, but we’ll show only the function that makes
    a shifted copy of an input MNIST digit here. This function is called four times
    for each training and test image (to create a shifted test dataset). We keep the
    original centered digit and four randomly shifted copies of it to make a dataset
    that is five times as large as the original. The random shift function is.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: with `im` being the input image supplied as a NumPy array. Based on the input
    size, we pick random x and y shifts that can be positive or negative and up to
    one-quarter of the image size. Varying this limit to, say, one-third or one-half
    would be worth the experiment. A new image is created (`img`), which is twice
    the size of the original. The original is then put into the larger image at an
    offset based on the shift positions, and the center portion of the larger image,
    matching the input image dimensions, is returned as the offset version of the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: To use the augmented dataset, we need first to retrain the fully connected MNIST
    model, then rebuild the fully convolutional model using the new weights and biases,
    and, finally, run the large test images through the model as before. Doing all
    of this leads to new graymaps ([Figure 13-8](ch13.xhtml#ch13fig8)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-8: Per digit heatmap output of the fully convolutional model for
    the upper-left input image. The model was trained on the MNIST dataset augmented
    by shifting the digits.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see a vast improvement, so we have impressive evidence that our intuition
    was correct: the initial model was unable to deal effectively with partial digits,
    but when we trained it with partial digits included, the resulting responses were
    robust over the actual digits and very weak to nonexistent for other digits. We
    really should not be surprised by these results. Our first model did not represent
    the space of inputs that the model would see when used in the wild. It knew nothing
    about partial MNIST digits. The second model was trained on a dataset that is
    a better representation of the space of possible inputs, so it performs significantly
    better.'
  prefs: []
  type: TYPE_NORMAL
- en: Using fully convolutional networks in this way has been superseded in recent
    years by other, more advanced techniques that we do not have space nor computing
    power to work with in this book. Many models that localize objects in images output
    not a heatmap, but a bounding box covering the image. For example, the YOLO model
    (*[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)*) is
    capable of real-time object detection in images, and it uses bounding boxes around
    the objects with their label. In [Chapter 12](ch12.xhtml#ch12), we mentioned semantic
    segmentation and U-Nets as current state-of-the-art models that assign a class
    label to each pixel of the input. Both of these approaches are useful and are,
    in a sense, extensions of the fully convolutional model approach we just demonstrated
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Scrambled MNIST Digits
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.xhtml#ch10), we showed that scrambling the order of the
    pixels in an MNIST digit ([Figure 7-3](ch07.xhtml#ch7fig3)), provided the remapping
    of pixels is deterministically applied to each image, poses no problem for a traditional
    neural network. It is still able to train well and assign class labels just as
    effectively as with unscrambled digits. See [Figure 10-9](ch10.xhtml#ch10fig9).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if this still holds with CNNs. We made the scrambled MNIST digit dataset
    in [Chapter 5](ch05.xhtml#ch05). All we need to do here is substitute it for the
    standard MNIST dataset in our baseline CNN model, the one we started the chapter
    with. If we train this model with the scrambled data, and do so repeatedly to
    get some error bars, we get [Figure 13-9](ch13.xhtml#ch13fig9).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-9: Test set error per epoch for a model trained on unscrambled and
    scrambled MNIST digits. Mean and SE over six training sessions.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we see that unlike the traditional neural network, the CNN does have some
    trouble: the test error for the scrambled digits is higher than for the unscrambled
    digits. Why? Recall, the CNN uses convolution and learns kernels that help create
    a new representation of the input, one that a simple model, the top layers, can
    readily use to distinguish between classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution generates responses that are spatially dependent. In the case of
    the scrambled digits, that spatial dependence is mostly eliminated; it is only
    by considering the digit image as a whole, like a traditional neural network,
    that a class determination can be made. This means there is little for the lower
    layers of the CNN to learn. Of course, the CNN is still learning and does a better
    job with the scrambled digits than the traditional model in the end, about 2 percent
    error versus 4.4 percent, but the distinction between scrambled and unscrambled
    is more significant.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we built our intuition around CNNs by working with the MNIST
    dataset. We explored the effect of basic architecture changes; learned about the
    interplay among training set size, minibatch size, and number of training epochs;
    and explored the effect of the optimization algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to convert a model using fully connected layers into a fully convolutional
    model. We then learned how to apply that model to search for digits in arbitrarily
    sized input images. We also learned that we needed to increase the expressiveness
    of our dataset to do a better job of representing the distribution of inputs the
    model sees when used.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw via an experiment with the scrambled MNIST digits that the strength
    of CNNs—their ability to learn spatial relationships within data—can sometimes
    be of little help when the spatial relationships are weak or nonexistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll continue our exploration of basic CNNs with a new
    dataset, one of actual images: CIFAR-10.'
  prefs: []
  type: TYPE_NORMAL
