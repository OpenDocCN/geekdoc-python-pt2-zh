- en: '**13'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EXPERIMENTS WITH KERAS AND MNIST**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: In the last chapter, we covered the essential components and functionality of
    a CNN. In this chapter, we’ll work with our test model from [Chapter 12](ch12.xhtml#ch12).
    We’ll first learn how to implement and train it in Keras. After that, we’ll conduct
    a set of experiments that will build our intuition for how different architectures
    and learning parameter choices affect the model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: From there, we’ll move beyond classification of simple input images and expand
    the network by converting it into a fully convolutional model capable of processing
    arbitrary inputs and locating digits wherever they occur in the input.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'After fully convolutional networks, we’ll wander a little deeper into the pool
    of deep learning and fulfill a promise made in [Chapter 7](ch07.xhtml#ch07): we’ll
    explore how well CNNs perform on the scrambled MNIST digit experiment. We saw
    in [Chapter 10](ch10.xhtml#ch10) that scrambling the pixels of the digits made
    it virtually impossible for us to see what the digit was but had little to no
    effect on how well a traditional neural network was able to interpret the digits.
    Is the same true with a CNN? We’ll find out.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Building CNNs in Keras
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model from [Figure 12-3](ch12.xhtml#ch12fig3) is straightforward to implement
    in Python using the keras library. We’ll list the code first, explain it, and
    then run it to see what sort of output it produces. The code naturally falls into
    three sections. The first loads the MNIST data and configures it for Keras; the
    second builds the model; and the third trains the model and applies it to the
    test data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Loading the MNIST Data
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 13-1](ch13.xhtml#ch13lis1) has the first part of our code.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: import keras
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: from keras.datasets import mnist
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import Sequential
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: from keras import backend as K
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 128
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: num_classes = 10
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 12
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: img_rows, img_cols = 28, 28
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: ❶ (x_train, y_train), (x_test, y_test) = mnist.load_data()
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '❷ if K.image_data_format() == ''channels_first'':'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = (1, img_rows, img_cols)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = (img_rows, img_cols, 1)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: ❸ x_train = x_train.astype('float32')
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.astype('float32')
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: x_train /= 255
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: x_test /= 255
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: ❹ y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-1: Loading and data preprocessing*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras is a rather large toolkit consisting of many modules. We import the library
    first and then specific functions from it. The mnist module gives us access to
    the MNIST data from within Keras; the Sequential model type is for implementing
    a CNN. Our CNN will need some specific layers, the ones we saw used in [Figure
    12-3](ch12.xhtml#ch12fig3): Dense, Dropout, Flatten, Conv2D, and MaxPool2D, all
    of which we import. Keras supports a plethora of other layers; I encourage you
    to spend some quality time with their documentation pages: *[https://keras.io/](https://keras.io/)*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we set the learning parameters, including the number of epochs, classes,
    and minibatch size. There are 10 classes, and the images are 28×28 pixel grayscale.
    Like sklearn, in Keras, you specify the number of epochs (full passes through
    the training set), not the number of minibatches that should be processed. Keras
    automatically processes the entire training set per epoch in sets of the minibatch
    size—here 128 samples at a time. Recall that MNIST’s training set consists of
    60,000 samples, so there are at least 60,000/128 = 468 minibatches per epoch using
    integer division. There will be 469 if Keras uses the remainder, the samples that
    do not build a complete minibatch. Remember that each minibatch process results
    in a gradient descent step: an update of the parameters of the network.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: After loading the MNIST train and test data ❶ come a few lines of code that
    may seem somewhat mysterious at first ❷. Keras is a higher-level toolkit that
    uses potentially different lower-level backends. In our case, the backend is TensorFlow,
    which we installed in [Chapter 1](ch01.xhtml#ch01). Different backends expect
    the model input in different forms. The image_data_format function returns a string
    indicating where the underlying toolkit expects to see the number of channels
    or filters for convolutional layers. The TensorFlow backend returns channels_last,
    meaning it expects an image to be represented as a 3D array of H × W × C, where
    H is the image height, W is the image width, and C is the number of channels.
    For a grayscale image like MNIST, the number of channels is 1\. The code in ❷
    reformats the input images to match what Keras is expecting to see.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The next block of code converts the byte image values to floating-point numbers
    in the range [0,1] ❸. This is the only scaling done to the input data, and this
    type of scaling is typical of CNNs that work with images.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the to_categorical function is used to map the class labels in y_test
    to one-hot vector representations ❹, which is how Keras wants to see the labels.
    As we’ll see, the model has 10 outputs, so the mapping is to a vector of 10 elements;
    each element is 0 except for the element whose index corresponds to the label
    in y_test. That element is set to 1\. For example, y_test[333] is of class 6 (a
    “6” digit). After the call to to_categorical, y_test[333] becomes
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: array([0.,0.,0.,0.,0.,0.,1.,0.,0.,0.], dtype=float32)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: where all entries are 0 except index 6, which is 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Building Our Model
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the dataset preprocessed, we can build our model. The code shown in [Listing
    13-2](ch13.xhtml#ch13lis2) builds the exact model we defined with pictures in
    [Figure 12-3](ch12.xhtml#ch12fig3).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu',
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=input_shape))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: optimizer=keras.optimizers.Adadelta(),
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: metrics=['accuracy'])
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: print("Model parameters = %d" % model.count_params())
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: print(model.summary())
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-2: Building the MNIST model*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Keras defines the model as an instance of the Sequential class. The model is
    built by adding layers to that instance, hence all the calls to the add method.
    The argument to add is the new layer. The layers are added from the input side
    to the output side, so the first layer we need to add is the 2D convolutional
    layer that uses a 3 × 3 kernel on the input image. Note, we are not specifying
    the number of images nor the minibatch size; Keras will handle that for us when
    the model is put together and trained. Right now, we are defining the architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Using the architecture defined in [Figure 12-3](ch12.xhtml#ch12fig3), the first
    layer is a Conv2D layer. The first argument is the number of filters; here, 32\.
    The kernel size is given as a tuple, (3,3). Kernels don’t need to be square, hence
    the kernel width and height. It’s possible that the spatial relationship of the
    parts of your input might be better detected with a non-square kernel. If so,
    Keras lets you use one. That said, almost all kernels in practical use are square.
    After the kernel, we define an activation function to apply to the output of the
    convolutional layer, here a ReLU. The shape of the input to this layer is explicitly
    defined via input_shape, and we saw that earlier for our MNIST model using a TensorFlow
    backend, the shape is a tuple, (28,28,1).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we add the second convolutional layer. This one has 64 filters, also
    using a 3 × 3 kernel and a ReLU activation on the output. Note, we do not need
    to specify the shape here: Keras knows the input shape because it knows the shape
    of the previous convolutional layer’s output.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling comes next. We explicitly state that the pooling size is 2 × 2,
    with an implied stride of 2\. If we wanted to use average pooling here, we would
    replace MaxPooling2D with AveragePooling2D.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: After pooling comes our first dropout layer, which uses a 25 percent probability
    of dropping an output, here the output of the max-pooling layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: We discussed earlier how Keras separates the operations of a fully connected
    layer into Flatten and Dense layers. This allows more fine-grained control of
    the architecture. We add a Flatten layer to map the pooling output to a vector
    and then pass this vector to a Dense layer to implement the classic fully connected
    layer. The dense layer has 128 nodes and uses a ReLU for the activation function.
    If we want dropout on the output of the dense layer, we need to add it explicitly,
    so we add one with a probability of 50 percent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了 Keras 如何将全连接层的操作分解为 Flatten 层和 Dense 层。这使得对架构有了更细粒度的控制。我们添加了一个 Flatten
    层，将池化输出映射到一个向量，然后将这个向量传递给 Dense 层，以实现经典的全连接层。该 Dense 层有 128 个节点，并使用 ReLU 作为激活函数。如果我们希望在
    Dense 层的输出上使用 dropout，我们需要显式地添加它，因此我们添加了一个具有 50% 概率的 dropout 层。
- en: The final dense layer has 10 nodes, one for each possible class label. The activation
    is set to softmax to get a softmax output on the inputs to this layer. Since this
    is the last layer we define, the output of this layer, the softmax probabilities
    for membership in each of the 10 classes, is the output of the entire model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层 Dense 层有 10 个节点，每个节点对应一个可能的类别标签。激活函数设置为 softmax，以便在该层的输入上得到 softmax 输出。由于这是我们定义的最后一层，这一层的输出，即每个类别的
    softmax 概率，是整个模型的输出。
- en: To configure the model for training, we need to call the compile method. This
    sets the loss function used during training (loss) and the specific optimization
    algorithm to use (optimizer). The metrics keyword is used to define which metrics
    to report during training. For our example, we are using the categorical cross-entropy
    loss, which is the multiclass version of the binary cross-entropy loss. We described
    this loss function in [Chapter 9](ch09.xhtml#ch09); it is the go-to loss function
    for many CNNs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置模型进行训练，我们需要调用 compile 方法。这会设置在训练过程中使用的损失函数（loss）和使用的具体优化算法（optimizer）。metrics
    关键词用于定义在训练过程中报告的指标。对于我们的示例，我们使用的是分类交叉熵损失，这是二元交叉熵损失的多类版本。我们在[第 9 章](ch09.xhtml#ch09)中描述了这个损失函数；它是许多
    CNN 使用的标准损失函数。
- en: We will need to discuss the optimizer keyword more thoroughly. In [Chapter 9](ch09.xhtml#ch09),
    we presented gradient descent and the more common version, stochastic gradient
    descent. As you might expect, the machine learning community has not been content
    to simply use this algorithm as is; much research has been done to see if it can
    be improved upon for training neural networks. This has led to the development
    of multiple variations on gradient descent, many of which Keras supports.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更详细地讨论 optimizer 关键词。在[第 9 章](ch09.xhtml#ch09)中，我们介绍了梯度下降法和更常见的版本——随机梯度下降法。正如你所预期的，机器学习社区并不满足于直接使用这个算法；大量的研究已经进行，以便看看是否可以改进它来训练神经网络。这导致了梯度下降法的多种变体的出现，其中许多是
    Keras 支持的。
- en: If we want, we can use classic stochastic gradient descent here. The example,
    however, is using a variant called *Adadelta*. This is itself a variant of the
    Adagrad algorithm that seeks to change the learning rate (step size) intelligently
    during training. For practical purposes, we should consider Adadelta an improved
    version of stochastic gradient descent. Keras also supports other optimization
    approaches that we do not intend to cover here, but you can read about in the
    Keras documentation, particularly Adam and RMSprop.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，可以在这里使用经典的随机梯度下降法。然而，本示例使用的是一种名为 *Adadelta* 的变体。它本身是 Adagrad 算法的变体，旨在在训练过程中智能地调整学习率（步长）。从实用的角度来看，我们可以将
    Adadelta 视为随机梯度下降法的改进版本。Keras 还支持其他优化方法，我们不打算在这里讨论，但你可以在 Keras 文档中阅读到，特别是 Adam
    和 RMSprop。
- en: After the call to compile, our model is defined. The convenience methods count_params
    and summary produce output characterizing the model itself. When we run the code,
    we’ll see the sort of output they produce.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 compile 后，我们的模型就定义好了。方便的方法 count_params 和 summary 会输出模型本身的特征。运行代码时，我们将看到它们生成的输出。
- en: Training and Evaluating the Model
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练与评估模型
- en: Finally, with both data and model defined, we can train and then evaluate the
    model on the test data. The code for this is in [Listing 13-3](ch13.xhtml#ch13lis3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在定义了数据和模型后，我们可以训练模型并在测试数据上进行评估。相关的代码见[代码清单 13-3](ch13.xhtml#ch13lis3)。
- en: history = model.fit(x_train, y_train,
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: history = model.fit(x_train, y_train,
- en: batch_size=batch_size,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size,
- en: epochs=epochs,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=epochs,
- en: verbose=1,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=1,
- en: validation_data=(x_test, y_test))
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=(x_test, y_test))
- en: score = model.evaluate(x_test, y_test, verbose=0)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test, y_test, verbose=0)
- en: print('Test loss:', score[0])
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: print('Test loss:', score[0])
- en: print('Test accuracy:', score[1])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: model.save("mnist_cnn_model.h5")
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-3: Training and testing the MNIST model*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The fit method trains the network using the supplied training samples (x_train)
    and a one-hot vector version of the associated class label (y_test). We also pass
    in the number of epochs and the minibatch size. Setting verbose to 1 will produce
    the output shown in [Listing 13-4](ch13.xhtml#ch13lis4). Lastly, we have validation
    _data. For this example, we’re being a bit sloppy and passing in all the test
    data instead of holding some back for final testing. (This is just a simple example,
    after all.) Normally, we’d hold some test data back to use after the final model
    has been trained. This ensures that results on this held-out test data represent
    what we might encounter when using the model in the wild.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the fit method returns something. This is a History object, and
    its history property holds a per epoch summary of the training and validation
    loss and accuracy values. We can use these to make summary plots if we wish.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, we can get a score, similar to the score of sklearn,
    by calling the evaluate method and passing in the test data. The method returns
    a list with the loss and accuracy of the model on the supplied data, which we
    simply print.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: We can use the save method to write the model itself to disk for future use.
    Notice the file extension. Keras dumps the model in an HDF5 file. *HDF5* is a
    generic hierarchical data format widely used in scientific circles. In this case,
    the file contains all the weights and biases of the model and the layer structure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code produces the output shown in [Listing 13-4](ch13.xhtml#ch13lis4):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow backend.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters = 1199882
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer (type)                 Output Shape              Param #'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: ==============================================================
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: conv2d_1 (Conv2D)            (None, 26, 26, 32)        320
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: dropout_1 (Dropout)          (None, 12, 12, 64)        0
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: flatten_1 (Flatten)          (None, 9216)              0
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: dense_1 (Dense)              (None, 128)               1179776
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: dropout_2 (Dropout)          (None, 128)               0
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: dense_2 (Dense)              (None, 10)                1290
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: ==============================================================
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Total params: 1,199,882'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Trainable params: 1,199,882'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-trainable params: 0'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Train on 60000 samples, validate on 10000 samples
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  1/12-loss:0.2800 acc:0.9147 val_loss:0.0624 val_acc:0.9794
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  2/12-loss:0.1003 acc:0.9695 val_loss:0.0422 val_acc:0.9854
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  3/12-loss:0.0697 acc:0.9789 val_loss:0.0356 val_acc:0.9880
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  4/12-loss:0.0573 acc:0.9827 val_loss:0.0282 val_acc:0.9910
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  5/12-loss:0.0478 acc:0.9854 val_loss:0.0311 val_acc:0.9901
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  6/12-loss:0.0419 acc:0.9871 val_loss:0.0279 val_acc:0.9908
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Epoch  7/12-loss:0.0397 acc:0.9883 val_loss:0.0250 val_acc:0.9914
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 7/12-loss:0.0397 acc:0.9883 val_loss:0.0250 val_acc:0.9914
- en: Epoch  8/12-loss:0.0344 acc:0.9891 val_loss:0.0288 val_acc:0.9910
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 8/12-loss:0.0344 acc:0.9891 val_loss:0.0288 val_acc:0.9910
- en: Epoch  9/12-loss:0.0329 acc:0.9895 val_loss:0.0273 val_acc:0.9916
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 9/12-loss:0.0329 acc:0.9895 val_loss:0.0273 val_acc:0.9916
- en: Epoch 10/12-loss:0.0305 acc:0.9909 val_loss:0.0296 val_acc:0.9904
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 10/12-loss:0.0305 acc:0.9909 val_loss:0.0296 val_acc:0.9904
- en: Epoch 11/12-loss:0.0291 acc:0.9911 val_loss:0.0275 val_acc:0.9920
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 11/12-loss:0.0291 acc:0.9911 val_loss:0.0275 val_acc:0.9920
- en: Epoch 12/12-loss:0.0274 acc:0.9916 val_loss:0.0245 val_acc:0.9916
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 12/12-loss:0.0274 acc:0.9916 val_loss:0.0245 val_acc:0.9916
- en: 'Test loss: 0.02452171179684301'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '测试损失: 0.02452171179684301'
- en: 'Test accuracy: 0.9916'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '测试准确率: 0.9916'
- en: '*Listing 13-4: MNIST training output*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13-4: MNIST 训练输出*'
- en: We’ve excluded some informational and warning messages from the lower-level
    TensorFlow toolkit and condensed the output to make it easier to follow in the
    text.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们排除了来自较低级别的 TensorFlow 工具包的一些信息性和警告消息，并简化了输出内容，使其更易于在文本中跟随。
- en: At the start of the run, Keras informs us that TensorFlow is our backend. It
    also shows us the shape of the training data, the now familiar 60,000 samples
    with a shape of 28 × 28 × 1 (×1 since the images are grayscale). We have the usual
    10,000 test samples as well.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行开始时，Keras 会通知我们 TensorFlow 是我们的后端。它还显示了训练数据的形状，即现在熟悉的 60,000 个样本，形状为 28 ×
    28 × 1（×1 是因为图像是灰度图）。我们也有常规的 10,000 个测试样本。
- en: Next comes a report on the model. This report shows the layer type, shape of
    the output from the layer, and the number of parameters in the layer. For example,
    the first convolutional layer uses 32 filters and 3 × 3 kernels, so the output
    with a 28 × 28 input will be 26 × 26 × 32\. The None listed for each layer is
    in the place where the number of elements in the minibatch normally is. The printout
    is showing only the relationship between the layers; because nothing in the architecture
    changes the number of elements in the minibatch, there’s no need to explicitly
    mention the minibatch elements (hence the None). The parameters are 3 × 3 × 32
    for the filters plus an additional 32 bias terms for the 320 parameters listed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是关于模型的报告。该报告显示了层的类型、层输出的形状以及层中参数的数量。例如，第一个卷积层使用了 32 个滤波器和 3 × 3 的卷积核，因此，输入为
    28 × 28 时，输出将是 26 × 26 × 32。每一层中列出的 None 表示通常会出现的 minibatch 元素数量的位置。输出显示的只是各层之间的关系；由于架构中没有任何内容会改变
    minibatch 中元素的数量，所以不需要显式地提及 minibatch 元素（因此显示为 None）。参数为 3 × 3 × 32 的滤波器，另外还有
    32 个偏置项，总共列出了 320 个参数。
- en: 'As mentioned in [Chapter 12](ch12.xhtml#ch12), the lion’s share of the parameters
    in the model are between the Flatten layer and the Dense layer. The layer named
    dense_2 is the softmax layer mapping the 128 elements of the Dense layer to the
    10 elements of the softmax: 128 × 10 + 10 = 1290, where the additional 10 are
    the bias terms. Note that the Dropout and Pooling layers have no parameters because
    there is nothing to learn in those layers.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第 12 章](ch12.xhtml#ch12)所述，模型中大部分的参数位于 Flatten 层和 Dense 层之间。名为 dense_2 的层是
    softmax 层，它将 Dense 层的 128 个元素映射到 softmax 的 10 个元素：128 × 10 + 10 = 1290，其中额外的 10
    个是偏置项。请注意，Dropout 和 Pooling 层没有参数，因为这些层中没有需要学习的内容。
- en: 'After the report on the model’s structure, we have the verbose output of the
    training call to fit. We asked for 12 epochs—12 full passes through the training
    data—using a minibatch of 128 samples. The output lists the stats for each pass.
    We see that the loss goes down as we train, which is expected if the model is
    learning, and that the accuracy (acc) on the training data goes up. The validation
    data is used during training to test the model, but this data is not used to update
    the model’s weights and biases. The loss on the validation data is also going
    down with each epoch, but more slowly. What we don’t want to see here is the validation
    loss going up, though it will jump around somewhat, especially if the validation
    set is not very big. We see an opposite effect with the validation accuracy (val_acc).
    It is going up for each epoch of training. If the model were to start overfitting,
    we’d see this accuracy go down after some point. This is the value of validation
    data: to tell us when to stop training.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在报告模型结构之后，我们会看到训练调用的详细输出，以进行拟合。我们请求了12个epoch——即12次完整的训练数据迭代——使用一个128样本的小批量。输出列出了每次迭代的统计数据。我们看到，随着训练的进行，损失逐渐降低，这是预期中的情况，表示模型正在学习，同时训练数据上的准确度（acc）也在上升。验证数据在训练过程中用于测试模型，但这些数据不会用于更新模型的权重和偏置。验证数据的损失每个epoch也在下降，但下降的速度较慢。我们不希望看到的是验证损失上升，尽管它可能会有一些波动，尤其是在验证集不够大的时候。我们在验证准确度（val_acc）上看到了相反的效果。它在每个训练epoch中都在上升。如果模型开始过拟合，我们会看到这种准确度在某个时刻开始下降。这就是验证数据的价值：告诉我们何时停止训练。
- en: The final two lines of output are the loss and accuracy of the model on the
    test samples passed to the evaluate method. Since the validation and test sets
    are the same in this example, these lines match the output for epoch 12\. The
    final accuracy of this model is 99.16 percent—certainly a very good accuracy to
    see.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后两行是模型在通过evaluate方法传入的测试样本上的损失和准确度。由于在这个例子中验证集和测试集是相同的，这两行与epoch 12的输出匹配。该模型的最终准确率为99.16%——这无疑是一个非常好的准确率。
- en: Plotting the Error
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绘制误差图
- en: We can use the saved history to plot the loss or error (1 – accuracy) as a function
    of the training epoch. The plots are similar in shape, so we show only the error
    plot in [Figure 13-1](ch13.xhtml#ch13fig1).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用保存的历史数据来绘制损失或误差（1 - 准确度）与训练epoch的关系。图形形状相似，因此我们仅展示[图13-1](ch13.xhtml#ch13fig1)中的误差图。
- en: '![image](Images/13fig01.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig01.jpg)'
- en: '*Figure 13-1: The MNIST training and validation errors as a function of epoch*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-1：MNIST训练和验证错误与epoch的关系*'
- en: The error on the training data falls off quickly and, as we’ve seen before,
    tends toward 0 as training continues. In this case, the validation error falls
    slightly and then levels off at a value similar to the training error. At this
    point in the book, you might have alarm bells going off in your head when you
    look at [Figure 13-1](ch13.xhtml#ch13fig1). The initial training error is *greater
    than* the initial validation error!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据上的误差迅速下降，正如我们之前所看到的，随着训练的进行，误差趋向于0。在这种情况下，验证误差稍微下降，然后在一个与训练误差相似的值上趋于平稳。在本书的这一部分，看到[图13-1](ch13.xhtml#ch13fig1)时，可能会让你感到警觉：最初的训练误差*大于*最初的验证误差！
- en: The full cause of this is hard to pin down, but one component is using dropout
    in the network. Dropout is applicable only during training, and because of the
    dropping of nodes in layers, dropout is in effect training many models at once,
    which, initially, causes a large error before the model “settles down” and the
    error drops per epoch. We can see that this might be the case here because if
    we simply comment out the Dropout layers in [Listing 13-4](ch13.xhtml#ch13lis4)
    and retrain, we get a new error plot, [Figure 13-2](ch13.xhtml#ch13fig2).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的完全原因很难确定，但其中一个因素是网络中使用了dropout。Dropout只在训练期间适用，由于在层中丢弃节点，dropout实际上是在同时训练多个模型，这最初会导致较大的误差，直到模型“稳定”下来，误差才会逐渐减少。我们可以看到，这可能正是这里的情况，因为如果我们仅仅注释掉[Listing
    13-4](ch13.xhtml#ch13lis4)中的Dropout层并重新训练，我们会得到一张新的误差图，[图13-2](ch13.xhtml#ch13fig2)。
- en: '![image](Images/13fig02.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig02.jpg)'
- en: '*Figure 13-2: The MNIST training and validation errors as a function of epoch
    when no Dropout layers are present*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-2：当没有Dropout层时，MNIST训练和验证错误与epoch的关系*'
- en: In [Figure 13-2](ch13.xhtml#ch13fig2), we see that the validation error quickly
    becomes greater than the training error, as we would expect. Additionally, we
    see that the final validation error is much greater than the final validation
    error for [Figure 13-1](ch13.xhtml#ch13fig1), about 10 percent versus 1 percent.
    This is also something we expect if dropout is actually a sensible thing to use,
    which it is. Note also that by the 12th epoch, the training set error is roughly
    the same regardless of the presence of Dropout layers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 13-2](ch13.xhtml#ch13fig2)中，我们看到验证误差迅速大于训练误差，这是我们所预期的。此外，我们还看到，最终的验证误差比[图
    13-1](ch13.xhtml#ch13fig1)中的最终验证误差大得多，大约是 10% 对比 1%。如果 Dropout 确实是一个合理的使用方法，我们也会预期这种结果。还要注意的是，在第
    12 轮迭代后，无论是否存在 Dropout 层，训练集的误差大致相同。
- en: Finally, some of what we see in [Figures 13-1](ch13.xhtml#ch13fig1) and [13-2](ch13.xhtml#ch13fig2)
    is due to the way Keras reports training and validation accuracies. The reported
    training accuracy (and loss) at the end of an epoch is the average over the epoch,
    but, of course, this is changing as the model learns and tends to increase. However,
    the validation accuracy reported is for the model as it is at the end of the epoch,
    so at times is it possible for the training accuracy to be reported as less than
    the validation accuracy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在[图 13-1](ch13.xhtml#ch13fig1)和[图 13-2](ch13.xhtml#ch13fig2)中看到的部分内容，实际上是由于
    Keras 报告训练和验证准确度的方式所致。在每个迭代结束时报告的训练准确度（和损失）是该迭代的平均值，但显然，随着模型的学习，这个值会不断变化并趋于上升。然而，报告的验证准确度是针对迭代结束时模型的状态，因此，有时训练准确度可能会低于验证准确度。
- en: Now that we’ve seen how to build a simple CNN and run it on a dataset, we are
    in a position to start experimenting with CNNs. Of course, there are an infinite
    number of experiments we could perform—just look at the rate at which new papers
    on deep learning appear on sites like [arxiv.org](http://arxiv.org) or the explosion
    of attendance at machine learning conferences—so we need to restrict ourselves
    to some basic explorations. Hopefully, these will motivate you to explore more
    on your own.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何构建一个简单的 CNN 并在数据集上运行它，我们可以开始对 CNN 进行实验了。当然，我们可以进行无数的实验——看看新论文在 [arxiv.org](http://arxiv.org)
    上的发布频率，或者机器学习会议的参会人数激增——因此，我们需要将自己限制在一些基本的探索中。希望这些能够激励你自己进一步探索。
- en: Basic Experiments
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本实验
- en: 'We did a bit of experimentation already when we removed the Dropout layer.
    All our experiments follow the same general pattern: make a slightly different
    version of the model, train it, and evaluate it against the test set. We will
    try three different types of experiments. The first type modifies the architecture
    of the model; removing Dropout layers falls into this category. The second type
    explores the interplay between training set size, minibatch size, and epochs.
    The last type alters the optimizer used during training.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经进行了一些实验，当我们移除 Dropout 层时。我们所有的实验都遵循相同的一般模式：创建一个略有不同的模型版本，训练它，并用测试集进行评估。我们将尝试三种不同类型的实验。第一种类型修改了模型的架构；移除
    Dropout 层属于这一类别。第二种类型探索了训练集大小、最小批量大小和迭代次数之间的相互作用。最后一种类型则改变了训练过程中使用的优化器。
- en: 'In all three cases, to avoid excessive code listings, we’ll simply comment
    on the variation to the code in the previous section with the understanding that
    the remaining code is the same from experiment to experiment. We’ll number the
    experiments, and you can match the results with the number to find the actual
    Python source code for the experiment. The code is available from the website
    associated with this book: *[https://nostarch.com/practical-deep-learning-python/](https://nostarch.com/practical-deep-learning-python/)*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三种情况下，为了避免过多列出代码，我们将仅对上一节中代码的变化进行评论，理解剩余的代码在每个实验中是相同的。我们将为每个实验编号，你可以通过编号匹配结果找到实际的
    Python 源代码。代码可以在与本书相关的网站上找到： *[https://nostarch.com/practical-deep-learning-python/](https://nostarch.com/practical-deep-learning-python/)*。
- en: In the previous section, we used the entire training set of 60,000 samples for
    training and the whole test set of 10,000 samples for both validation and as the
    final test set. Here, we’ll restrict ourselves to using the first 1,000 or 1,024
    training samples as the entire training set. Additionally, we’ll use the first
    1,000 samples of the test set as the validation set and reserve the last 9,000
    samples for the final test set we’ll use when training is complete. We’ll report
    the accuracy of these 9,000 images that were unseen during training. The results
    will include the baseline model accuracy and number of parameters, for comparison
    purposes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了整个60,000样本的训练集，以及10,000样本的测试集进行验证和最终测试。在这里，我们将限制使用前1,000或1,024个训练样本作为整个训练集。此外，我们将使用测试集的前1,000个样本作为验证集，并将剩余的9,000个样本保留作为训练完成后使用的最终测试集。我们将报告这些在训练过程中未曾见过的9,000张图像的准确率。结果将包括基准模型的准确率和参数数量，供比较使用。
- en: Bear in mind that unless stated otherwise, the accuracies we present represent
    a single training session for each experiment. You should get slightly different
    results if you run these experiments yourself, but those slight differences shouldn’t
    outweigh the larger differences in accuracy that will result from changing the
    model and/or training process.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，除非另有说明，否则我们呈现的准确率代表每个实验的单次训练过程。如果你自己运行这些实验，结果可能会略有不同，但这些小的差异不应该超过因模型和/或训练过程的变化而导致的准确率的更大差异。
- en: Finally, the models in this section are multiclass, so we could examine the
    confusion matrices to see how the models are making their mistakes. However, it
    would be exceedingly tedious to do this for each experiment. Instead, we will
    use the overall accuracy as our metric, trusting that it is a sufficient measure
    in this case.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本节中的模型为多分类模型，因此我们可以通过检查混淆矩阵来查看模型的错误。然而，对每个实验进行此操作将是极其繁琐的。相反，我们将使用总体准确率作为衡量标准，相信在这种情况下它足够准确。
- en: Architecture Experiments
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 架构实验
- en: Architecture modifications imply removing or adding new layers or altering the
    parameters of a layer. We’ve made a number of architecture modifications and compiled
    the resulting accuracies in [Table 13-1](ch13.xhtml#ch13tab1).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 架构修改意味着移除或添加新层，或更改层的参数。我们做了一些架构修改，并在[表 13-1](ch13.xhtml#ch13tab1)中汇总了相应的准确率。
- en: '**Table 13-1:** Results from Modifying the Model Architecture'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 13-1：** 修改模型架构的结果'
- en: '| **Exp.** | **Modification** | **Test accuracy** | **Parameters** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **实验** | **修改** | **测试准确率** | **参数数量** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | Baseline | 92.70% | 1,199,882 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 基准 | 92.70% | 1,199,882 |'
- en: '| 1 | Add Conv3, 3 × 3 × 64 before Pooling | 94.30% | 2,076,554 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 在池化前添加Conv3，3 × 3 × 64 | 94.30% | 2,076,554 |'
- en: '| 2 | Duplicate Conv2, Pooling layer | 94.11% | 261,962 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 重复Conv2、池化层 | 94.11% | 261,962 |'
- en: '| 3 | Conv1, 3 × 3 × 32 to 5 × 5 × 32 | 93.56% | 1,011,978 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 将Conv1，3 × 3 × 32更改为5 × 5 × 32 | 93.56% | 1,011,978 |'
- en: '| 4 | Dense layer to 1,024 nodes | 92.76% | 9,467,274 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 将全连接层节点数设置为1,024 | 92.76% | 9,467,274 |'
- en: '| 5 | Conv1, Conv2, halve number of filters | 92.38% | 596,042 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Conv1、Conv2，滤波器数量减半 | 92.38% | 596,042 |'
- en: '| 6 | Second Dense layer with 128 nodes | 91.90% | 1,216,394 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 第二个全连接层，节点数为128 | 91.90% | 1,216,394 |'
- en: '| 7 | Dense layer to 32 nodes | 91.43% | 314,090 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 将全连接层节点数设置为32 | 91.43% | 314,090 |'
- en: '| 8 | Remove Pooling layer | 90.68% | 4,738,826 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 移除池化层 | 90.68% | 4,738,826 |'
- en: '| 9 | No ReLU after conv layers | 90.48% | 1,199,882 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 卷积层后不使用ReLU | 90.48% | 1,199,882 |'
- en: '| 10 | Remove Conv2 | 89.39% | 693,962 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 移除Conv2 | 89.39% | 693,962 |'
- en: In [Table 13-1](ch13.xhtml#ch13tab1), the baseline results and model size are
    given first, followed by the various experiments from most accurate to least accurate.
    Let’s look at the table and interpret the results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 13-1](ch13.xhtml#ch13tab1)中，首先给出了基准结果和模型大小，随后列出了从最准确到最不准确的各种实验。让我们来看看表格并解读结果。
- en: First, we see that adding a third convolutional layer after the second convolutional
    layer (Experiment 1) improves the performance of the model but also adds 876,672
    parameters. Increasing the depth of the network seems to improve the performance
    of the model but at the expense of increasing the number of parameters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们看到在第二个卷积层后添加第三个卷积层（实验1）提高了模型的性能，但也增加了876,672个参数。增加网络深度似乎能够提高模型的性能，但代价是增加了参数数量。
- en: However, in Experiment 2 we also increase the depth of the network by duplicating
    the second convolutional layer and the following pooling layer, but because of
    the second pooling layer, the total number of parameters in the network goes down
    by 937,920\. This is a substantial saving for virtually the same performance.
    This indicates that depth is good, but so is the judicious use of pooling layers
    to keep the number of parameters small. For this dataset, Experiment 2 is a solid
    architecture to use.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Next, we see that an adjustment to the kernel size of the first convolutional
    layer, Experiment 3, leads to an improvement relative to the baseline. There are
    more parameters in the first convolutional layer (832 versus 320), but because
    of the edge effects when using an exact convolution, by the time we get to the
    output of the Flatten layer, there are now only 7744 values versus 9216 for the
    baseline model. This means that the large matrix between the Flatten and Dense
    layers goes from 1,179,776 down to 991,360 parameters with a net result that the
    model overall has 187,904 fewer parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'This is good: better performance and fewer parameters to learn. Is there a
    downside to the change of Experiment 3? Not really. Instead, one might argue that
    adjusting the kernel size for the first convolutional layer has made the model
    more appropriate for the spatial information in the digit images, thereby making
    the new representation learned by the convolutional and pooling layers that much
    better at separating the classes. In general, there seems to be a best kernel
    size for the first convolutional layer, the layer that deals with the input to
    the model. That kernel size is related to the spatial structure of the inputs:
    some sizes will be better at detecting input features that are better for separating
    the classes. This general rule does not appear to hold for higher convolutional
    layers, and there the prevailing wisdom is to use 3 × 3 kernels for most convolutional
    layers except the first.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Can we combine Experiment 3 and Experiment 2? Certainly. We simply make the
    first convolutional layer of Experiment 2 use a 5 × 5 kernel instead of a 3 ×
    3 kernel. If we do this, we get a model with an overall accuracy of 94.23 percent
    that needs only 188,746 parameters. With this trivial change, we’ve achieved the
    performance of Experiment 10 by using only 9 percent of the parameters.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be tempted to simply increase the size of the Dense layer, the layer
    that can be thought of as using the new feature representation discovered by the
    convolutional and pooling layers below it. However, doing so (Experiment 4) results
    in no real improvement in overall accuracy, but with a substantial increase in
    the number of parameters. We know the cause: the 9,216 × 128 weight matrix between
    the Flatten and Dense layers is now a 9,216 × 1,024 matrix. Clearly, for CNNs,
    we want to create the best feature representation so that a simpler top layer
    can be used.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'With Experiment 5, we see that we can make the model significantly smaller,
    a reduction of 603,840 parameters, while still achieving the same overall accuracy
    by simply halving the number of filters learned in each of the convolutional layers:
    32 → 16 for Conv1 and 64 → 32 for Conv2\. Again, this is a good optimization provided
    the slight (perhaps in this case meaningless) difference in accuracy is acceptable.
    If we look again at [Figure 12-8](ch12.xhtml#ch12fig8), we can see that, especially
    for the second convolutional layer with 64 filters, the responses are very similar
    for many filters. This implies that there are redundant filters that are not adding
    much to the new feature representation presented to the Dense layers. Despite
    halving the number of filters learned, there are still filters that learn to capture
    the important aspects of the input data used to separate the classes.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Experiment 7 plays with the Dense layer nodes, and Experiment 6 adds a second
    Dense layer. Neither offers a real benefit. For Experiment 7, the change in the
    number of model parameters is significant due to the 9,216 × 128 matrix weight
    becoming a 9,216 × 32 matrix. However, 32 nodes does not seem to be the ideal
    number to make use of the new feature representation. The second Dense layer of
    Experiment 6 isn’t too awful in terms of increasing the number of parameters to
    learn, but it isn’t buying us much, either. If we use a larger training set, we
    might get some improvement, but we’ll leave that as an exercise for the reader.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we read about the criticisms levied against pooling
    layers. What if we remove the pooling layer entirely (Experiment 8)? First, we
    see that accuracy drops relative to the baseline model. Worse, we see that the
    size of the network has increased dramatically, from 1,199,882 parameters to 4,738,826,
    by a factor of nearly four. This is due to the increase in the number of elements
    in the output of the Flatten layer, which has gone from 9,216 to 36,864, resulting
    in a weight matrix of 36,864 × 128 + 128 = 4,718,720 elements. This example demonstrates
    why we use pooling layers even with the price they bring in terms of the loss
    of information about the relative position of object parts.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Each of the convolutional layers in the baseline model uses ReLU on its outputs.
    Removing these ReLU operations, Experiment 9, leads to a 2 percent reduction in
    accuracy on the test set. Clearly, the ReLU is helping somewhat. What might it
    be doing? The ReLU leaves positive values unchanged and sets negative values to
    0\. When used with the output of a convolutional layer, the ReLU is keeping more
    strongly activated responses to the filters, positive responses, while suppressing
    negative responses. This seems to be helping the entire process of learning a
    new representation of the input.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Experiment 10 removed Conv2 entirely. This has the greatest effect
    on the overall accuracy because the features passed to the Dense layer are then
    based solely on the output of the first convolutional layer filters. There was
    no opportunity for the model to learn from these outputs and develop filter responses
    based on the larger effective receptive field seen by the second convolutional
    layer.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'However, given what we saw in the results of Experiment 3, which increased
    the kernel size used by the first convolutional layer, we might wonder whether
    this change from 3 × 3 to 5 × 5 kernels might somewhat compensate for the loss
    of the second convolutional layer. Fortunately, this is very easy to test. We
    simply change the 3 × 3 kernel parameter of Conv1 to a 5 × 5 and train again.
    Doing this validates our intuition: the resulting overall accuracy increases to
    92.39 percent, virtually the same as the baseline model. Also, this 5 × 5 model
    has only 592,074 parameters, making the change inexpensive in terms of the number
    of model parameters.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: From all of these results, do we have a winner, an architecture that is lean
    but highly effective? We do—it is Experiment 2 with a 5 × 5 kernel for the first
    convolutional layer. In Keras, to build this architecture, we need the code in
    [Listing 13-5](ch13.xhtml#ch13lis5).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(5, 5),
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu',
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=input_shape))
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-5: Building the architecture for Experiment 2*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: We’ve simply duplicated the Conv2D, MaxPooling2D, and Dropout layers and used
    (5,5) for the kernel size of the first layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: If we train this model using all 60,000 samples of the MNIST training set, we
    get a final held-out test set accuracy of 99.51 percent for an error of 0.49 percent.
    This is using all 10,000 samples. According to [benchmarks.ai](http://benchmarks.ai),
    a website that tracks current bests on different machine learning datasets, the
    state-of-the-art MNIST error is 0.21 percent, so we are not state of the art,
    but we are better than the 99.16 percent accuracy we saw in [Listing 13-4](ch13.xhtml#ch13lis4)
    for the default architecture.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Training Set Size, Minibatches, and Epochs
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These experiments examine the interplay between training set size, minibatch
    size, and number of epochs. Our model will be the default model we were using
    previously, Experiment 0, but this time we’ll use 1,024 samples for the training
    set. We’ll be using powers of two as the minibatch sizes; this is a convenient
    size, as all of our minibatch sizes divide it evenly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Recall, Keras, like sklearn, runs through a given number of epochs, or full
    passes through the training set. Additionally, the minibatch size (batch _size)
    specifies the number of samples used in each iteration, after which the average
    error (the cross-entropy loss) is used to update the parameters. Therefore, each
    processed minibatch leads to a single gradient descent step, and the training
    set size divided by the minibatch size is the number of gradient descent steps
    taken per epoch.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'For our experiments, we’ll use the following minibatch sizes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'For a training set of 1,024 samples (approximately 100 for each digit), the
    number of gradient descent steps per epoch is the reverse of this list: 1,024
    for a batch size of 1, down to 1 for a batch size of 1,024.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate two plots. The first will plot the final test set accuracy for
    the two cases: a fixed number of gradient descent steps regardless of minibatch
    size, and a fixed number of epochs irrespective of minibatch size. The second
    plot will show us the clock time to train the model for each case. The code leading
    to the plots is in Experiments 26 through 31 (fixed number of gradient descent
    steps) and Experiments 32 through 42 (fixed number of epochs).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The test set accuracy as a function of minibatch size is given as the mean of
    five runs in [Figure 13-3](ch13.xhtml#ch13fig3), and the standard error of the
    mean given by the error bars. Let’s look at the fixed number of gradient descent
    steps regardless of minibatch size (triangles). In some toolkits, this is referred
    to as using a fixed number of iterations, where *iteration* means a single update
    leading to a gradient descent step.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig03.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-3: MNIST test set accuracy as a function of minibatch size and fixed
    gradient descent steps or fixed epochs*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of gradient descent steps was fixed at 1,024\. This means that we
    need to change the number of epochs, depending on how many minibatches are in
    the training set. For the case of a single sample used per update (batch_size=1),
    we get the required 1,024 steps in a single epoch, so we set the number of epochs
    to 1\. For a minibatch size of two, we get 512 steps per epoch, so we set the
    number of epochs in the code to 2 to get 1,024 steps overall. The pattern continues:
    to get the 1,024 gradient descent steps for each minibatch size, we need to set
    the number of epochs to the minibatch size.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: We see that except for the smallest minibatch sizes, when we fix the number
    of gradient descent steps, we get very consistent overall accuracies. After minibatch
    sizes of about 16, things do not change too much. The general rule for CNNs is
    to use smaller batch sizes. This seems to help with model generalization for datasets
    that are not as simple as MNIST, and, as we’ll soon see, decreases training time
    significantly.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The second curve of [Figure 13-3](ch13.xhtml#ch13fig3) shows the accuracy for
    a fixed number of epochs (circles). If we change the minibatch size but do not
    increase the number of training epochs to make the number of gradient descent
    steps remain constant, we’ll be using larger and better estimates of the gradient,
    but we’ll also be taking fewer steps. We see that doing this quickly results in
    a substantial decrease in accuracy. We should not be surprised. With a minibatch
    size of one, training for 12 epochs gives us a model that took 12 × 1,024 = 12,288
    gradient descent steps. Granted, the gradient estimate was particularly noisy
    in this case since we use only one training sample to estimate it, but with lots
    of steps, we arrived at a well-performing model all the same. By the time we get
    to a minibatch of 1,024 samples, the size of our training set, we see that we
    took only 12 gradient descent steps before calling it a day. No wonder the results
    are so poor.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The second plot of this section is [Figure 13-4](ch13.xhtml#ch13fig4).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig04.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-4: Model training time as a function of minibatch size and fixed
    gradient descent steps or fixed epochs*'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: As before, let’s start with the fixed number of gradient descent steps (triangles).
    We see immediately that the training time is linearly proportional to the minibatch
    size. This is reasonable since we just saw that we need to increase the number
    of epochs to keep the number of gradient descent steps constant while increasing
    the minibatch size. Therefore, the amount of data passed through the network is
    increasing proportionally, so the time required for the forward and backward training
    passes will increase proportionally as well. From this, we see that large minibatch
    sizes cost clock time.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed number of epochs, we see a different story. For tiny minibatch sizes,
    training time goes up due to the number of forward and backward passes. For the
    case of a minibatch of one, we need 12,288 such passes, as we just saw. However,
    by the time we get to minibatches of even 32 samples at a time, we have only 1024/32
    = 32 passes per epoch for a total of 384 for the entire training session. This
    is far fewer than for the smallest minibatch size, so we might expect the training
    time for minibatches this size or larger to be roughly constant in time, as we
    see in [Figure 13-4](ch13.xhtml#ch13fig4).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'What can we glean from these two plots? The following: to balance run time
    and accuracy, we want to use minibatch sizes that are large enough to give us
    a reasonable estimate of the gradient but small enough that, for a fixed number
    of model updates (gradient descent steps), we can train quickly. This argues for
    minibatch sizes in the 16 to 128 range, in general. Indeed, a review of the deep
    learning literature sees minibatches in this range almost exclusively for most
    applications. For this example, minibatch sizes above 16, based on [Figure 13-3](ch13.xhtml#ch13fig3)
    (triangles), result in models that are all basically the same in terms of accuracy,
    but the training time for a model using a minibatch size of 16, according to [Figure
    13-4](ch13.xhtml#ch13fig4) (triangles), compared to one of size 1,024, is a few
    seconds versus approximately 30 minutes.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So far, all of our experiments have used the same gradient descent algorithm,
    or optimizer: Adadelta. Let’s take a look at how our MNIST model does if we change
    the optimizer but leave everything else fixed. Our model is Experiment 0, the
    model we’ve been using all along in this chapter. We’ll continue to use 1,000
    test samples for validation and 9,000 to determine our final accuracy. However,
    instead of using the first 1,000 or 1,024 training samples, we’ll increase the
    number to the first 16,384 samples. We fix the minibatch size at 128 and the number
    of epochs to 12, as before. We’ll report the results as mean and standard error
    over five runs.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras currently supports the following optimizers: stochastic gradient descent
    (SGD), RMSprop, Adagrad, Adadelta, and Adam. We’ll train using each of these in
    turn. For Adagrad, Adadelta, and Adam, we leave the parameters at their default
    settings, as recommended by the Keras documentation. For RMSprop, the only parameter
    the Keras documentation recommends adjusting is the learning rate (lr), which
    we set to 0.01, a typical value. For SGD, we set the learning rate to 0.01 as
    well and set standard momentum to 0.9, also a very typical value. The code is
    found in Experiments 43 through 47.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-5](ch13.xhtml#ch13fig5) shows the test set accuracy (top) and training
    time (bottom) for each of the optimizers.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig05.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-5: Test set accuracy (top) and training time (bottom) by optimization
    algorithm*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: First, notice that there is not too much difference between the results produced
    by each optimizer. This is good news. However, by looking at the error bars, it
    seems clear that Adadelta, Adagrad, and Adam all perform slightly better than
    SGD or RMSprop. This is borne out in the deep learning literature as well, though
    each dataset should be looked at independently.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In terms of training time, the different optimizers are also roughly equivalent,
    though SGD is fastest and consistently so. This performance difference might be
    important for a very large dataset. Adam is also consistently faster than Adadelta
    for basically the same performance. Again, these results are evident in the literature
    where both SGD and Adam are widely used.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: This section has been thorough in terms of minute details associated with changing
    model architectures and training parameters. Hopefully, it has helped you develop
    intuition about how to configure a CNN and how to train it. Rules of thumb are
    hard to present in this area, but I have given some general guidance. Exceptions
    to these rules abound, however, and you have to simply try things, observe the
    results, and adapt when presented with a new dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now from classifying simple inputs to answering the question of
    how to make a model that can locate targets in arbitrary images.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Networks
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced fully convolutional networks in [Chapter 12](ch12.xhtml#ch12).
    Let’s take our basic MNIST CNN model and convert it to a fully convolutional version
    and see how we can use it to locate digits in larger images. Our basic approach
    is to train the model using fully connected layers, as before, and then to create
    a fully convolutional version and update the weights with those from the fully
    connected model. We can then apply the new model to arbitrary size inputs to locate
    digits (hopefully).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Building and Training the Model
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, we train our base model on the full MNIST dataset. The only change we’ll
    make is to train for 24 epochs instead of just 12\. The output of this process
    is an HDF5 file that contains the trained weights and biases. All we then need
    to do is create the fully convolutional version by changing the fully connected
    layer and copy the weights and biases from the old model to the new. The code
    for this is straightforward, as shown in [Listing 13-6](ch13.xhtml#ch13lis6).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import Sequential, load_model
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: ❶ weights = load_model('mnist_cnn_base_model.h5').get_weights()
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu',
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: ❷ input_shape=(None,None,1)))
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: ❸ model.add(Conv2D(128, (12,12), activation='relu'))
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: ❹ model.add(Conv2D(10, (1,1), activation='softmax'))
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: ❺ model.layers[0].set_weights([weights[0], weights[1]])
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: model.layers[1].set_weights([weights[2], weights[3]])
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: model.layers[4].set_weights([weights[4].reshape([12,12,64,128]), weights[5]])
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: model.layers[6].set_weights([weights[6].reshape([1,1,128,10]), weights[7]])
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: model.save('mnist_cnn_fcn_model.h5')
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-6: Creating the trained fully connected model*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: After importing the necessary Keras modules, we load the trained weights from
    the fully connected model ❶. Then, we construct the fully convolutional version
    much as we did for the fully connected version. However, there are some key differences.
    The first has to do with the input convolutional layer ❷. In the fully connected
    model, we specified the input image size here, 28×28 pixels with one channel (grayscale).
    For the fully convolutional case, we do not know the size of the input, so we
    use None instead for the width and height. We do know that the input will be a
    single channel image, so we leave the 1.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Since the Dense layers are the reason we have to use fixed size inputs, we replace
    them with equivalent convolutional layers ❸. We replace
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: with
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(128, (12,12), activation='relu'))
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The (12,12) is the output size of the max-pooling layer above, and the 128 is
    the number of filters to learn that stand in for the 128 nodes we had before.
    Again, the critical point here is that the output of this convolutional layer
    is 1 × 1 × 128 because convolving a 12 × 12 input with a 12 × 12 kernel produces
    a single output value. The difference is that the convolutional layer is not tied
    to any fixed input size as the combination of Flatten and Dense was.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: The final softmax layer also needs to be made fully convolutional ❹. There are
    ten outputs, one per digit, and the activation remains the same. The kernel size,
    however, is 1 × 1\. The input to this layer is 1 × 1 × 128, so the kernel size
    to cover it is 1 × 1\. Again, if we were to work through the math, we’d see that
    a 1 × 1 × 128 input to a 1 × 1 × 10 convolutional layer matches that of a fully
    connected layer of 128 nodes mapped to 10 nodes in the next layer. The difference
    here is that if the input is larger than 1 × 1, we can still convolve over it.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve constructed the fully convolutional version of our model, we
    need to copy the weights from the trained fully connected model to it ❺. We loaded
    the trained weights into weights. This is a list of NumPy arrays for the weights
    and biases, layer by layer. So, weights[0] refers to the weights of the first
    Conv2D layer, and weights[1] are the biases. Similarly, weights[2] and weights[3]
    are the weights and bias values for the second convolutional layer. We set them
    in the new fully convolutional model by updating the proper layers via the set_weights
    method. Layers 0 and 1 are the two convolutional layers.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 4 is the new Conv2D layer that replaced the Flatten and Dense layers
    of the original model. Here when we set the weights, we need to reshape them to
    match the form of a convolutional layer: 12 × 12 × 64 × 128\. This is for a 12
    × 12 kernel mapped over 64 inputs leading to 128 outputs. The 64 is the number
    of 12 × 12 outputs from the pooling layer above.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we set the output layer weights. Again, we need to reshape them to
    1 × 1 × 128 × 10 for the 1 × 1 × 128 input and 10 outputs. The biases for the
    two new Conv2D layers are in weights[5] and weights[7], so we add them as well.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The fully convolutional model is now defined and completely populated with the
    weights and biases from the fully connected model. [Figure 13-6](ch13.xhtml#ch13fig6)
    shows the mapping between models, with the original architecture on the left and
    the fully convolutional architecture on the right. The boxes represent layers,
    with the top set of numbers being the input and the bottom the output. For the
    fully convolutional model, input height and width are arbitrary and marked with
    “--”.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig06.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-6: Mapping a fully connected model (left) to a fully convolutional
    model (right)*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: All that is left to do is write the new fully convolutional model to disk, and
    it’s ready to use. Let’s see how.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Making the Test Images
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To test the fully convolutional model, we first need images with digits. Unlike
    our training images, which were small and had a single digit in the center, we
    want larger test images that contain many digits in arbitrary locations. The MNIST
    dataset consists of shades of gray on a black background; therefore, our test
    images should have a black background also. This will make the test images come
    from the same “domain” as the training images, which, as we’ve emphasized before,
    is critical. Making models adapt to different data domains is an active research
    area. Search for *domain adaptation*.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Making the test images by using Python and the digits from the MNIST test set
    is straightforward. We didn’t use the test set images for training, so using them
    to make our larger test images isn’t cheating. The code is shown in [Listing 13-7](ch13.xhtml#ch13lis7).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: import os
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: import random
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf images; mkdir images")
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'if (len(sys.argv) > 1):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: N = int(sys.argv[1])
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: N = 10
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("data/mnist/mnist_test_images.npy")
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(N):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: ❶ r,c = random.randint(6,12), random.randint(6,12)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: g = np.zeros(r*c)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '❷ for j in range(r*c):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'if (random.random() < 0.15):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: g[j] = 1
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: g = g.reshape((r,c))
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: g[:,0] = g[0,:] = g[:,-1] = g[-1,:] = 0
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: ❸ img = np.zeros((28*r,28*c), dtype="uint8")
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'for x in range(r):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'for y in range(c):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'if (g[x,y] == 1):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: ❹ n = random.randint(0, x_test.shape[0])
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: im = x_test[n]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: img[28*x:(28*x+28), 28*y:(28*y+28)] = im
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Image.fromarray(img).save("images/image_%04d.png" % i)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-7: Building large MNIST test set images*'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: We’re making use of the MNIST test set file we created in [Chapter 5](ch05.xhtml#ch05).
    We could just as quickly have loaded the test images through Keras, as we did
    earlier for our basic CNN experiments. The code itself creates an output directory,
    *images*, and gets the number of images to build from the command line, if given
    (N).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The images are of random sizes ❶. Here, r and c are the number of rows and columns
    in the large image in terms of the number of 28 × 28 MNIST digits. To decide where
    to place our digits so they don’t overlap, we create a grid, g, with either a
    0 or a 1 in each possible digit position (r*c of them) ❷. There is a 15 percent
    chance that any grid position will contain a 1\. We then reshape the grid into
    an actual 2D array and set the border positions of the grid to 0 to ensure that
    no digits appear on the edge of the image.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: The actual output image is then defined ❸ as the number of rows and columns
    are multiplied by 28, the width and height of the MNIST digit. We loop over each
    digit position (x and y), and if the grid value at that row and column is 1, then
    we select a digit at random and copy it to the current row and column digit position
    in the output image (img) ❹. When every grid position has been examined, the image
    is written to disk so that we can use it with our fully convolutional network.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Model
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s test the model—first on single MNIST digits and then on the randomly generated
    large digit images. The fully convolutional model should work as well with single
    MNIST digits as the fully connected model did. The code to test this assertion
    is in [Listing 13-8](ch13.xhtml#ch13lis8).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import load_model
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("data/mnist/mnist_test_images.npy")/255.0
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("data/mnist/mnist_test_labels.npy")
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: model = load_model("mnist_cnn_fcn_model.h5")
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: N = y_test.shape[0]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: nc = nw = 0.0
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(N):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: ❶ p = model.predict(x_test[i][np.newaxis,:,:,np.newaxis])
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: c = np.argmax(p)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'if (c == y_test[i]):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: nc += 1
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: nw += 1
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: print("Single MNIST digits, n=%d, accuracy = %0.2f%%" % (N, 100*nc/N))
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-8: Verifying that the fully convolutional model works with single
    MNIST digits*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: We load the MNIST test images and labels, along with the fully convolutional
    model, and then loop over each test image and ask the model to make a prediction
    ❶. Note, the image is 2D, but we must pass a 4D array to the predict method, hence
    using np.newaxis to create the missing axes. The prediction for the digit is stored
    in p as a vector of per class probabilities. The label associated with the largest
    of these probabilities is the label assigned to the input digit by the model c.
    If c matches the actual test label, we increment the number of correct predictions
    (nc); otherwise, we increment the number of wrong predictions (nw). Once all 10,000
    test images have been processed, we can output the overall accuracy, which is
    99.25 percent for my training of the fully convolutional model.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Okay, the fully convolutional model is highly accurate, but so what? We passed
    single-digit images to it as inputs and got a single output value. We had this
    capability before with the fully connected model. To expose the utility of the
    fully convolutional model, let’s now pass the large MNIST digit images as input.
    In code, we do this as shown in [Listing 13-9](ch13.xhtml#ch13lis9).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: import os
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import load_model
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: model = load_model("mnist_cnn_fcn_model.h5")
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf results; mkdir results")
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: n = len(os.listdir("images"))
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(n):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: f = "images/image_%04d.png" % i
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: ❶ im = np.array(Image.open(f))/255.0
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: p = model.predict(im[np.newaxis,:,:,np.newaxis])
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: np.save("results/results_%04d.npy" % i, p[0,:,:,:])
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-9: Running the fully convolutional model over large test images*'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: We import the necessary modules and then load the fully convolutional model.
    We then create a new output directory, *results*, and find the number of large
    digit images (n). Next, we loop over each of the large digit images.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: After loading the image from disk, being careful to make a NumPy array from
    it and scaling it by 255 since the training data was also scaled by 255 ❶, we
    make a prediction and store the model output in p. Notice, we make a 4D input
    to predict, just as we did for the single digits earlier, but this time, im is
    larger than 28 × 28 and contains multiple digits. Because the model is fully convolutional,
    this isn’t an issue; we will not get an error. Instead, p is a 4D array with the
    first dimension of one, the number of input images, and a final dimension of ten,
    the number of digits. The middle two dimensions of p are a function of the size
    of the input passed to the predict method. Since the input was larger than 28×28
    pixels, the entire model convolved over the input image as though the model was
    a convolutional layer with a kernel of 28 × 28\. Specifically, the output of this
    convolution has height and width of
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/334equ01.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: where *H*,*W* are the height and width of the input image and *h*,*w* are the
    height and width of the output array from predict. The 28 in the formula is the
    size of the inputs we initially trained on, 28 × 28 digit images. Where did the
    mysterious 2 come from in the denominator? This is the stride of the 28 × 28 kernel
    over the input image. It is 2 because that is the factor the input image is changed
    by when it gets down to the fully convolutional output layers. The input was 28
    × 28, but, after the two convolutional layers and the pooling layer, the input
    is mapped to 12 × 12 and ⌊28/12⌋ = 2.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: We stated that the array in p is 4D; now we know we get a specific-size output
    based on convolving a 28 × 28 region over the input image using a stride of 2\.
    What do we get at each of the *h*,*w* output array positions? The last element
    of the 4D output has size 10; these are the per class predictions at the specific
    *h*,*w* output position that corresponds to the 28 × 28 kernel.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this abstract description more concrete. The upper-left corner of
    [Figure 13-7](ch13.xhtml#ch13fig7) shows one of the large input images where we’ve
    inverted it to make it black on a white background and added a border so you can
    see the full size of the image.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig07.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-7: Per digit heatmap output of the fully convolutional model for
    the input image on the upper left. The model was trained on the standard MNIST
    dataset.*'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'The image in the top left of [Figure 13-7](ch13.xhtml#ch13fig7) is 336 pixels
    wide and 308 pixels tall, meaning the output from passing this image to the model
    will be an array that is 1 × 141 × 155 × 10, exactly what we expect from the equations
    on page 744 for the output array dimensions. The output array represents the model’s
    predictions at each of the 28 × 28 regions of the input image when using a stride
    of 2\. There is one prediction for each digit. For example, if p is the 4D output
    of the predict method for the image on the left of [Figure 13-7](ch13.xhtml#ch13fig7),
    then p[0,77,88,:] will return a 10-element vector representing the per class probabilities
    of each digit class for the 28 × 28 input region of the image that maps to 77
    × 88\. In this case, we get the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: array([0.10930195, 0.12363277, 0.131005  , 0.10506018, 0.05257199,
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 0.07958104, 0.0947836 , 0.11399861, 0.08733559, 0.10272926],
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: dtype=float32)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that there is no strong likelihood, according to the model, that
    any particular digit is present at this location. We know this because all the
    output probabilities are much lower than even the minimum cutoff threshold of
    0.5\. The output of predict can be thought of as a probability map, typically
    called a *heatmap*, giving us the probability that there is a digit in that location.
    The model output can be thought of as 10 heatmaps, one for each digit.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: The remaining images of [Figure 13-7](ch13.xhtml#ch13fig7) show the heatmaps
    for each of the 10 digits, again inverted so that higher probabilities are darker.
    The heatmaps were thresholded at 0.98, meaning any probability value less than
    0.98 was set to 0\. This removes the weak outputs like the ones we just saw. We
    are interested only in the model’s strongest responses per digit. To make the
    heat maps, we double the size of the output from the model and set the output
    image locations with an offset to account for the position of the convolutional
    output. This is akin to what we saw in [Figure 12-1](ch12.xhtml#ch12fig1), where
    the convolution operation returns an output that is smaller than the input when
    no zero padding is used. Specifically, the code that produces the digit heatmaps
    is in [Listing 13-10](ch13.xhtml#ch13lis10).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: import os
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: ❶ threshold = float(sys.argv[1])
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: iname = sys.argv[2]
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: rname = sys.argv[3]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: outdir= sys.argv[4]
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf %s; mkdir %s" % (outdir, outdir))
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: ❷ img = Image.open(iname)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: c,r = img.size
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: hmap = np.zeros((r,c,10))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: res = np.load(rname)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: x,y,_ = res.shape
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: xoff = (r - 2*x) // 2
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: yoff = (c - 2*y) // 2
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '❸ for j in range(10):'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: h = np.array(Image.fromarray(res[:,:,j]).resize((2*y,2*x)))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: hmap[xoff:(xoff+x*2), yoff:(yoff+y*2),j] = h
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: np.save("%s/graymaps.npy" % outdir, hmap)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: ❹ hmap[np.where(hmap < threshold)] = 0.0
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'for j in range(10):'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: img = np.zeros((r,c), dtype="uint8")
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'for x in range(r):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'for y in range(c):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: ❺ img[x,y] = int(255.0*hmap[x,y,j])
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: img = 255-img
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Image.fromarray(img).save("%s/graymap_digit_%d.png" % (outdir, j))
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13-10: Building heatmap images*'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re calling the output images *graymaps* because they’re grayscale images
    representing the response of the model to different locations in the input image.
    We first pass in the threshold value, the source image name, the response of the
    model to that source image, and an output directory where the graymaps will be
    written ❶. This directory is overwritten each time. Next, the source image is
    loaded to get its dimensions ❷. These are used to create the output heatmaps (hmap).
    We also load the associated model responses (res) and calculate the offsets. Note
    that hmap is the same size as the image. We then fill in each digit graymap of
    hmap with the resized model response ❸ and store the full set of graymaps in the
    output directory.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: To make the output grayscale images like those shown in [Figure 13-7](ch13.xhtml#ch13fig7),
    we first threshold the heatmaps, setting any value less than the supplied cutoff
    to 0 ❹. Then, for each digit, we create an output image and simply scale the remaining
    heatmap values by 255 since they are probabilities in the range [0,1) ❺. Then,
    before writing the image to disk, we invert by subtracting from 255\. This makes
    stronger activations dark and weaker activations lighter. Because of the strong
    threshold applied (0.98), our output graymaps are effectively binary; this is
    what we want to indicate where the model is most certain of a digit being located.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look back at [Figure 13-7](ch13.xhtml#ch13fig7) and see if we can interpret
    these responses. The source image has one 0 on the lower right. If we look at
    the graymap for digit 0, we see a single dark blob in that location. This means
    the model has indicated a strong response that there is a 0 digit at that location.
    So far, so good. However, we also see another strong response from the model near
    the 4 on the left side of the input image. The model has made a mistake. The input
    has two 4s in it. If we look at the graymaps for digit 4, we see two dark blobs
    corresponding to these digits, but we also see many other small areas of strong
    activation near other digits that are not 4s. The model we trained was over 99
    percent accurate on single MNIST test digits, so why are the responses of the
    fully convolutional model so noisy? Just look at all the small strong responses
    for 2s when the input does not contain any 2s. Sometimes, the model is doing well,
    as for 8s where the graymap shows strong responses for all the 8s in the input,
    but then it does poorly for other digits like 7s. And, there are no 5s at all,
    but the model is returning many hits.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Here is an opportunity for us to expand our thinking and intuition. We trained
    the model on the standard MNIST digit dataset. All of the digits in this dataset
    are well centered in the images. However, when the model is convolved over the
    large input image, there will be many times when the input to the model is not
    a well-centered digit but only part of a digit. The model has never seen partial
    digits, and since it must give an answer, it sometimes offers answers that are
    meaningless—the part of a digit it sees may be part of a 6, for example, but the
    model “thinks” it is a 5.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: One possible solution is to teach the model about partial MNIST digits. We can
    do this by augmenting the standard MNIST dataset with shifted versions of its
    digits. Imagine a 4 shifted to the lower right so that only part of it is visible.
    It will still be labeled a 4, so the model will have an opportunity to learn what
    a shifted 4 digit looks like. The code to make this shifted dataset is in the
    file *make_shifted_mnist_dataset.py*, but we’ll show only the function that makes
    a shifted copy of an input MNIST digit here. This function is called four times
    for each training and test image (to create a shifted test dataset). We keep the
    original centered digit and four randomly shifted copies of it to make a dataset
    that is five times as large as the original. The random shift function is.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'def shifted(im):'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: r,c = im.shape
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: x = random.randint(-r//4, r//4)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: y = random.randint(-c//4, c//4)
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: img = np.zeros((2*r,2*c), dtype="uint8")
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: xoff = r//2 + x
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: yoff = c//2 + y
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: img[xoff:(xoff+r), yoff:(yoff+c)] = im
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: img = img[r//2:(r//2+r),c//2:(c//2+c)]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: return img
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: with im being the input image supplied as a NumPy array. Based on the input
    size, we pick random x and y shifts that can be positive or negative and up to
    one-quarter of the image size. Varying this limit to, say, one-third or one-half
    would be worth the experiment. A new image is created (img), which is twice the
    size of the original. The original is then put into the larger image at an offset
    based on the shift positions, and the center portion of the larger image, matching
    the input image dimensions, is returned as the offset version of the input.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: To use the augmented dataset, we need first to retrain the fully connected MNIST
    model, then rebuild the fully convolutional model using the new weights and biases,
    and, finally, run the large test images through the model as before. Doing all
    of this leads to new graymaps ([Figure 13-8](ch13.xhtml#ch13fig8)).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig08.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-8: Per digit heatmap output of the fully convolutional model for
    the upper-left input image. The model was trained on the MNIST dataset augmented
    by shifting the digits.*'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'We see a vast improvement, so we have impressive evidence that our intuition
    was correct: the initial model was unable to deal effectively with partial digits,
    but when we trained it with partial digits included, the resulting responses were
    robust over the actual digits and very weak to nonexistent for other digits. We
    really should not be surprised by these results. Our first model did not represent
    the space of inputs that the model would see when used in the wild. It knew nothing
    about partial MNIST digits. The second model was trained on a dataset that is
    a better representation of the space of possible inputs, so it performs significantly
    better.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Using fully convolutional networks in this way has been superseded in recent
    years by other, more advanced techniques that we do not have space nor computing
    power to work with in this book. Many models that localize objects in images output
    not a heatmap, but a bounding box covering the image. For example, the YOLO model
    (*[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)*) is
    capable of real-time object detection in images, and it uses bounding boxes around
    the objects with their label. In [Chapter 12](ch12.xhtml#ch12), we mentioned semantic
    segmentation and U-Nets as current state-of-the-art models that assign a class
    label to each pixel of the input. Both of these approaches are useful and are,
    in a sense, extensions of the fully convolutional model approach we just demonstrated
    here.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Scrambled MNIST Digits
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.xhtml#ch10), we showed that scrambling the order of the
    pixels in an MNIST digit ([Figure 7-3](ch07.xhtml#ch7fig3)), provided the remapping
    of pixels is deterministically applied to each image, poses no problem for a traditional
    neural network. It is still able to train well and assign class labels just as
    effectively as with unscrambled digits. See [Figure 10-9](ch10.xhtml#ch10fig9).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if this still holds with CNNs. We made the scrambled MNIST digit dataset
    in [Chapter 5](ch05.xhtml#ch05). All we need to do here is substitute it for the
    standard MNIST dataset in our baseline CNN model, the one we started the chapter
    with. If we train this model with the scrambled data, and do so repeatedly to
    get some error bars, we get [Figure 13-9](ch13.xhtml#ch13fig9).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/13fig09.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13-9: Test set error per epoch for a model trained on unscrambled and
    scrambled MNIST digits. Mean and SE over six training sessions.*'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we see that unlike the traditional neural network, the CNN does have some
    trouble: the test error for the scrambled digits is higher than for the unscrambled
    digits. Why? Recall, the CNN uses convolution and learns kernels that help create
    a new representation of the input, one that a simple model, the top layers, can
    readily use to distinguish between classes.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Convolution generates responses that are spatially dependent. In the case of
    the scrambled digits, that spatial dependence is mostly eliminated; it is only
    by considering the digit image as a whole, like a traditional neural network,
    that a class determination can be made. This means there is little for the lower
    layers of the CNN to learn. Of course, the CNN is still learning and does a better
    job with the scrambled digits than the traditional model in the end, about 2 percent
    error versus 4.4 percent, but the distinction between scrambled and unscrambled
    is more significant.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we built our intuition around CNNs by working with the MNIST
    dataset. We explored the effect of basic architecture changes; learned about the
    interplay among training set size, minibatch size, and number of training epochs;
    and explored the effect of the optimization algorithm.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to convert a model using fully connected layers into a fully convolutional
    model. We then learned how to apply that model to search for digits in arbitrarily
    sized input images. We also learned that we needed to increase the expressiveness
    of our dataset to do a better job of representing the distribution of inputs the
    model sees when used.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw via an experiment with the scrambled MNIST digits that the strength
    of CNNs—their ability to learn spatial relationships within data—can sometimes
    be of little help when the spatial relationships are weak or nonexistent.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll continue our exploration of basic CNNs with a new
    dataset, one of actual images: CIFAR-10.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
