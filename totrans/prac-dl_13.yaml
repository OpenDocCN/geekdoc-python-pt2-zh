- en: '**13'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**13'
- en: EXPERIMENTS WITH KERAS AND MNIST**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras和MNIST的实验**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In the last chapter, we covered the essential components and functionality of
    a CNN. In this chapter, we’ll work with our test model from [Chapter 12](ch12.xhtml#ch12).
    We’ll first learn how to implement and train it in Keras. After that, we’ll conduct
    a set of experiments that will build our intuition for how different architectures
    and learning parameter choices affect the model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了卷积神经网络（CNN）的基本组件和功能。在本章中，我们将使用[第12章](ch12.xhtml#ch12)中的测试模型。我们将首先学习如何在Keras中实现并训练该模型。之后，我们将进行一系列实验，以帮助我们直观理解不同架构和学习参数选择如何影响模型。
- en: From there, we’ll move beyond classification of simple input images and expand
    the network by converting it into a fully convolutional model capable of processing
    arbitrary inputs and locating digits wherever they occur in the input.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们将超越简单输入图像的分类，将网络扩展为一个全卷积模型，能够处理任意输入，并在输入中的任何位置定位数字。
- en: 'After fully convolutional networks, we’ll wander a little deeper into the pool
    of deep learning and fulfill a promise made in [Chapter 7](ch07.xhtml#ch07): we’ll
    explore how well CNNs perform on the scrambled MNIST digit experiment. We saw
    in [Chapter 10](ch10.xhtml#ch10) that scrambling the pixels of the digits made
    it virtually impossible for us to see what the digit was but had little to no
    effect on how well a traditional neural network was able to interpret the digits.
    Is the same true with a CNN? We’ll find out.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在全卷积网络之后，我们将稍微深入一点深度学习的领域，并实现[第7章](ch07.xhtml#ch07)中做出的承诺：我们将探讨CNN在打乱的MNIST数字实验中的表现。我们在[第10章](ch10.xhtml#ch10)中看到，将数字的像素打乱几乎让我们无法辨认出数字，但对传统神经网络解析数字的能力几乎没有影响。那么，CNN也是如此吗？我们将揭晓答案。
- en: Building CNNs in Keras
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Keras中构建CNN
- en: The model from [Figure 12-3](ch12.xhtml#ch12fig3) is straightforward to implement
    in Python using the keras library. We’ll list the code first, explain it, and
    then run it to see what sort of output it produces. The code naturally falls into
    three sections. The first loads the MNIST data and configures it for Keras; the
    second builds the model; and the third trains the model and applies it to the
    test data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[图12-3](ch12.xhtml#ch12fig3)的模型可以通过Python中的Keras库轻松实现。我们首先列出代码，解释它，然后运行它，看看它产生了什么样的输出。代码自然分为三个部分：第一部分加载MNIST数据并为Keras配置它；第二部分构建模型；第三部分训练模型并将其应用于测试数据。
- en: Loading the MNIST Data
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加载MNIST数据
- en: '[Listing 13-1](ch13.xhtml#ch13lis1) has the first part of our code.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 13-1](ch13.xhtml#ch13lis1)是我们代码的第一部分。'
- en: import keras
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.datasets import mnist
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.datasets import mnist
- en: from keras.models import Sequential
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Conv2D, MaxPooling2D
- en: from keras import backend as K
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: from keras import backend as K
- en: batch_size = 128
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 128
- en: num_classes = 10
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: num_classes = 10
- en: epochs = 12
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 12
- en: img_rows, img_cols = 28, 28
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: img_rows, img_cols = 28, 28
- en: ❶ (x_train, y_train), (x_test, y_test) = mnist.load_data()
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (x_train, y_train), (x_test, y_test) = mnist.load_data()
- en: '❷ if K.image_data_format() == ''channels_first'':'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ if K.image_data_format() == ''channels_first'':'
- en: x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
- en: x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
- en: input_shape = (1, img_rows, img_cols)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape = (1, img_rows, img_cols)
- en: 'else:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
- en: x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
- en: input_shape = (img_rows, img_cols, 1)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape = (img_rows, img_cols, 1)
- en: ❸ x_train = x_train.astype('float32')
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x_train = x_train.astype('float32')
- en: x_test = x_test.astype('float32')
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x_test.astype('float32')
- en: x_train /= 255
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: x_train /= 255
- en: x_test /= 255
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: x_test /= 255
- en: ❹ y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ y_train = keras.utils.to_categorical(y_train, num_classes)
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = keras.utils.to_categorical(y_test, num_classes)
- en: '*Listing 13-1: Loading and data preprocessing*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单13-1：加载和数据预处理*'
- en: 'Keras is a rather large toolkit consisting of many modules. We import the library
    first and then specific functions from it. The mnist module gives us access to
    the MNIST data from within Keras; the Sequential model type is for implementing
    a CNN. Our CNN will need some specific layers, the ones we saw used in [Figure
    12-3](ch12.xhtml#ch12fig3): Dense, Dropout, Flatten, Conv2D, and MaxPool2D, all
    of which we import. Keras supports a plethora of other layers; I encourage you
    to spend some quality time with their documentation pages: *[https://keras.io/](https://keras.io/)*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个相当大的工具包，包含许多模块。我们首先导入库，然后从中导入特定的函数。mnist 模块让我们可以从 Keras 内部访问 MNIST
    数据；Sequential 模型类型用于实现 CNN。我们的 CNN 需要一些特定的层，这些层在[图 12-3](ch12.xhtml#ch12fig3)中有所展示：Dense、Dropout、Flatten、Conv2D
    和 MaxPool2D，我们都导入了。Keras 支持大量其他层；我鼓励你花些时间浏览它们的文档页面：*[https://keras.io/](https://keras.io/)*。
- en: 'Next, we set the learning parameters, including the number of epochs, classes,
    and minibatch size. There are 10 classes, and the images are 28×28 pixel grayscale.
    Like sklearn, in Keras, you specify the number of epochs (full passes through
    the training set), not the number of minibatches that should be processed. Keras
    automatically processes the entire training set per epoch in sets of the minibatch
    size—here 128 samples at a time. Recall that MNIST’s training set consists of
    60,000 samples, so there are at least 60,000/128 = 468 minibatches per epoch using
    integer division. There will be 469 if Keras uses the remainder, the samples that
    do not build a complete minibatch. Remember that each minibatch process results
    in a gradient descent step: an update of the parameters of the network.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置学习参数，包括 epochs 数量、类别数和小批量大小。有 10 个类别，图像是 28×28 像素的灰度图像。像 sklearn 一样，在
    Keras 中，你指定的是 epochs 的数量（即完整通过训练集的次数），而不是应该处理的小批量数量。Keras 会自动按小批量大小处理整个训练集——这里是每次处理
    128 个样本。回想一下，MNIST 的训练集包含 60,000 个样本，因此每个 epoch 至少有 60,000/128 = 468 个小批量（使用整数除法）。如果
    Keras 使用余数，即那些不能组成完整小批量的样本，那么就会有 469 个小批量。记住，每处理一个小批量就会进行一次梯度下降更新：即更新网络的参数。
- en: After loading the MNIST train and test data ❶ come a few lines of code that
    may seem somewhat mysterious at first ❷. Keras is a higher-level toolkit that
    uses potentially different lower-level backends. In our case, the backend is TensorFlow,
    which we installed in [Chapter 1](ch01.xhtml#ch01). Different backends expect
    the model input in different forms. The image_data_format function returns a string
    indicating where the underlying toolkit expects to see the number of channels
    or filters for convolutional layers. The TensorFlow backend returns channels_last,
    meaning it expects an image to be represented as a 3D array of H × W × C, where
    H is the image height, W is the image width, and C is the number of channels.
    For a grayscale image like MNIST, the number of channels is 1\. The code in ❷
    reformats the input images to match what Keras is expecting to see.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载 MNIST 的训练集和测试集数据❶之后，接下来的几行代码看起来可能会有些神秘❷。Keras 是一个高阶工具包，它使用潜在不同的低阶后端。在我们的例子中，后端是
    TensorFlow，它在[第 1 章](ch01.xhtml#ch01)中已经安装。不同的后端期望模型输入的格式各异。image_data_format
    函数返回一个字符串，指示底层工具包期望在哪里看到卷积层的通道数或滤波器数。TensorFlow 后端返回 channels_last，意味着它期望图像以 H
    × W × C 的 3D 数组表示，其中 H 是图像高度，W 是图像宽度，C 是通道数。对于像 MNIST 这样的灰度图像，通道数是 1。❷ 中的代码重新格式化了输入图像，以匹配
    Keras 期望的格式。
- en: The next block of code converts the byte image values to floating-point numbers
    in the range [0,1] ❸. This is the only scaling done to the input data, and this
    type of scaling is typical of CNNs that work with images.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码块将字节图像值转换为范围在 [0,1] 之间的浮点数❸。这是对输入数据进行的唯一缩放操作，这种类型的缩放通常用于处理图像的 CNN。
- en: Finally, the to_categorical function is used to map the class labels in y_test
    to one-hot vector representations ❹, which is how Keras wants to see the labels.
    As we’ll see, the model has 10 outputs, so the mapping is to a vector of 10 elements;
    each element is 0 except for the element whose index corresponds to the label
    in y_test. That element is set to 1\. For example, y_test[333] is of class 6 (a
    “6” digit). After the call to to_categorical, y_test[333] becomes
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，to_categorical 函数用于将 y_test 中的类标签映射到一热向量表示❹，这正是 Keras 所期望的标签格式。正如我们将看到的，模型有
    10 个输出，因此映射到一个包含 10 个元素的向量；除了与 y_test 中标签对应索引的元素外，其他元素均为 0。该元素被设置为 1。例如，y_test[333]
    属于类 6（一个“6”数字）。在调用 to_categorical 后，y_test[333] 变成了
- en: array([0.,0.,0.,0.,0.,0.,1.,0.,0.,0.], dtype=float32)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: array([0.,0.,0.,0.,0.,0.,1.,0.,0.,0.], dtype=float32)
- en: where all entries are 0 except index 6, which is 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中所有条目的值为0，除了索引6的值为1。
- en: Building Our Model
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建我们的模型
- en: With the dataset preprocessed, we can build our model. The code shown in [Listing
    13-2](ch13.xhtml#ch13lis2) builds the exact model we defined with pictures in
    [Figure 12-3](ch12.xhtml#ch12fig3).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集预处理完成后，我们可以开始构建我们的模型。[列表13-2](ch13.xhtml#ch13lis2)中的代码构建了我们在[图12-3](ch12.xhtml#ch12fig3)中定义的完全相同的模型。
- en: model = Sequential()
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(32, kernel_size=(3, 3),
- en: activation='relu',
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: activation='relu',
- en: input_shape=input_shape))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape=input_shape))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Dropout(0.25))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Flatten())
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(128, activation='relu'))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(128, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(loss=keras.losses.categorical_crossentropy,
- en: optimizer=keras.optimizers.Adadelta(),
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=keras.optimizers.Adadelta(),
- en: metrics=['accuracy'])
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'])
- en: print("Model parameters = %d" % model.count_params())
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: print("模型参数 = %d" % model.count_params())
- en: print(model.summary())
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: print(model.summary())
- en: '*Listing 13-2: Building the MNIST model*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表13-2：构建MNIST模型*'
- en: Keras defines the model as an instance of the Sequential class. The model is
    built by adding layers to that instance, hence all the calls to the add method.
    The argument to add is the new layer. The layers are added from the input side
    to the output side, so the first layer we need to add is the 2D convolutional
    layer that uses a 3 × 3 kernel on the input image. Note, we are not specifying
    the number of images nor the minibatch size; Keras will handle that for us when
    the model is put together and trained. Right now, we are defining the architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Keras将模型定义为Sequential类的一个实例。通过向该实例添加层来构建模型，因此所有对add方法的调用都用于添加新层。添加的参数就是新层。层是从输入端到输出端依次添加的，所以我们首先需要添加的是一个2D卷积层，该层使用3
    × 3的卷积核处理输入图像。注意，我们没有指定图像的数量或小批量大小；当模型组建并训练时，Keras会处理这些问题。目前，我们定义的是架构。
- en: Using the architecture defined in [Figure 12-3](ch12.xhtml#ch12fig3), the first
    layer is a Conv2D layer. The first argument is the number of filters; here, 32\.
    The kernel size is given as a tuple, (3,3). Kernels don’t need to be square, hence
    the kernel width and height. It’s possible that the spatial relationship of the
    parts of your input might be better detected with a non-square kernel. If so,
    Keras lets you use one. That said, almost all kernels in practical use are square.
    After the kernel, we define an activation function to apply to the output of the
    convolutional layer, here a ReLU. The shape of the input to this layer is explicitly
    defined via input_shape, and we saw that earlier for our MNIST model using a TensorFlow
    backend, the shape is a tuple, (28,28,1).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[图12-3](ch12.xhtml#ch12fig3)中定义的架构，第一层是一个Conv2D层。第一个参数是过滤器的数量，这里为32。卷积核大小是一个元组(3,
    3)。卷积核不一定是方形的，因此需要指定宽度和高度。实际上，输入的各部分之间的空间关系可能通过非方形卷积核更好地被检测到。如果是这种情况，Keras允许使用非方形卷积核。尽管如此，实际应用中几乎所有的卷积核都是方形的。接着，定义一个激活函数来应用于卷积层的输出，这里是ReLU。输入到这一层的形状通过input_shape显式定义，之前我们已经看到过，对于使用TensorFlow后端的MNIST模型，形状是一个元组(28,
    28, 1)。
- en: 'Next, we add the second convolutional layer. This one has 64 filters, also
    using a 3 × 3 kernel and a ReLU activation on the output. Note, we do not need
    to specify the shape here: Keras knows the input shape because it knows the shape
    of the previous convolutional layer’s output.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加第二个卷积层。这个层有64个过滤器，仍然使用3 × 3的卷积核，并在输出上应用ReLU激活函数。注意，我们不需要在这里指定形状：Keras知道输入形状，因为它知道前一个卷积层输出的形状。
- en: Max pooling comes next. We explicitly state that the pooling size is 2 × 2,
    with an implied stride of 2\. If we wanted to use average pooling here, we would
    replace MaxPooling2D with AveragePooling2D.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是最大池化层。我们明确指定池化的大小是2 × 2，步幅隐含为2。如果我们想在这里使用平均池化，我们只需将MaxPooling2D替换为AveragePooling2D。
- en: After pooling comes our first dropout layer, which uses a 25 percent probability
    of dropping an output, here the output of the max-pooling layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化层之后是我们的第一个丢弃层，它使用25%的概率丢弃一个输出，这里是来自最大池化层的输出。
- en: We discussed earlier how Keras separates the operations of a fully connected
    layer into Flatten and Dense layers. This allows more fine-grained control of
    the architecture. We add a Flatten layer to map the pooling output to a vector
    and then pass this vector to a Dense layer to implement the classic fully connected
    layer. The dense layer has 128 nodes and uses a ReLU for the activation function.
    If we want dropout on the output of the dense layer, we need to add it explicitly,
    so we add one with a probability of 50 percent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了 Keras 如何将全连接层的操作分解为 Flatten 层和 Dense 层。这使得对架构有了更细粒度的控制。我们添加了一个 Flatten
    层，将池化输出映射到一个向量，然后将这个向量传递给 Dense 层，以实现经典的全连接层。该 Dense 层有 128 个节点，并使用 ReLU 作为激活函数。如果我们希望在
    Dense 层的输出上使用 dropout，我们需要显式地添加它，因此我们添加了一个具有 50% 概率的 dropout 层。
- en: The final dense layer has 10 nodes, one for each possible class label. The activation
    is set to softmax to get a softmax output on the inputs to this layer. Since this
    is the last layer we define, the output of this layer, the softmax probabilities
    for membership in each of the 10 classes, is the output of the entire model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层 Dense 层有 10 个节点，每个节点对应一个可能的类别标签。激活函数设置为 softmax，以便在该层的输入上得到 softmax 输出。由于这是我们定义的最后一层，这一层的输出，即每个类别的
    softmax 概率，是整个模型的输出。
- en: To configure the model for training, we need to call the compile method. This
    sets the loss function used during training (loss) and the specific optimization
    algorithm to use (optimizer). The metrics keyword is used to define which metrics
    to report during training. For our example, we are using the categorical cross-entropy
    loss, which is the multiclass version of the binary cross-entropy loss. We described
    this loss function in [Chapter 9](ch09.xhtml#ch09); it is the go-to loss function
    for many CNNs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置模型进行训练，我们需要调用 compile 方法。这会设置在训练过程中使用的损失函数（loss）和使用的具体优化算法（optimizer）。metrics
    关键词用于定义在训练过程中报告的指标。对于我们的示例，我们使用的是分类交叉熵损失，这是二元交叉熵损失的多类版本。我们在[第 9 章](ch09.xhtml#ch09)中描述了这个损失函数；它是许多
    CNN 使用的标准损失函数。
- en: We will need to discuss the optimizer keyword more thoroughly. In [Chapter 9](ch09.xhtml#ch09),
    we presented gradient descent and the more common version, stochastic gradient
    descent. As you might expect, the machine learning community has not been content
    to simply use this algorithm as is; much research has been done to see if it can
    be improved upon for training neural networks. This has led to the development
    of multiple variations on gradient descent, many of which Keras supports.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更详细地讨论 optimizer 关键词。在[第 9 章](ch09.xhtml#ch09)中，我们介绍了梯度下降法和更常见的版本——随机梯度下降法。正如你所预期的，机器学习社区并不满足于直接使用这个算法；大量的研究已经进行，以便看看是否可以改进它来训练神经网络。这导致了梯度下降法的多种变体的出现，其中许多是
    Keras 支持的。
- en: If we want, we can use classic stochastic gradient descent here. The example,
    however, is using a variant called *Adadelta*. This is itself a variant of the
    Adagrad algorithm that seeks to change the learning rate (step size) intelligently
    during training. For practical purposes, we should consider Adadelta an improved
    version of stochastic gradient descent. Keras also supports other optimization
    approaches that we do not intend to cover here, but you can read about in the
    Keras documentation, particularly Adam and RMSprop.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，可以在这里使用经典的随机梯度下降法。然而，本示例使用的是一种名为 *Adadelta* 的变体。它本身是 Adagrad 算法的变体，旨在在训练过程中智能地调整学习率（步长）。从实用的角度来看，我们可以将
    Adadelta 视为随机梯度下降法的改进版本。Keras 还支持其他优化方法，我们不打算在这里讨论，但你可以在 Keras 文档中阅读到，特别是 Adam
    和 RMSprop。
- en: After the call to compile, our model is defined. The convenience methods count_params
    and summary produce output characterizing the model itself. When we run the code,
    we’ll see the sort of output they produce.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 compile 后，我们的模型就定义好了。方便的方法 count_params 和 summary 会输出模型本身的特征。运行代码时，我们将看到它们生成的输出。
- en: Training and Evaluating the Model
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练与评估模型
- en: Finally, with both data and model defined, we can train and then evaluate the
    model on the test data. The code for this is in [Listing 13-3](ch13.xhtml#ch13lis3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在定义了数据和模型后，我们可以训练模型并在测试数据上进行评估。相关的代码见[代码清单 13-3](ch13.xhtml#ch13lis3)。
- en: history = model.fit(x_train, y_train,
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: history = model.fit(x_train, y_train,
- en: batch_size=batch_size,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size,
- en: epochs=epochs,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=epochs,
- en: verbose=1,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=1,
- en: validation_data=(x_test, y_test))
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=(x_test, y_test))
- en: score = model.evaluate(x_test, y_test, verbose=0)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test, y_test, verbose=0)
- en: print('Test loss:', score[0])
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: print('Test loss:', score[0])
- en: print('Test accuracy:', score[1])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: print('测试准确率:', score[1])
- en: model.save("mnist_cnn_model.h5")
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: model.save("mnist_cnn_model.h5")
- en: '*Listing 13-3: Training and testing the MNIST model*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13-3：训练和测试 MNIST 模型*'
- en: The fit method trains the network using the supplied training samples (x_train)
    and a one-hot vector version of the associated class label (y_test). We also pass
    in the number of epochs and the minibatch size. Setting verbose to 1 will produce
    the output shown in [Listing 13-4](ch13.xhtml#ch13lis4). Lastly, we have validation
    _data. For this example, we’re being a bit sloppy and passing in all the test
    data instead of holding some back for final testing. (This is just a simple example,
    after all.) Normally, we’d hold some test data back to use after the final model
    has been trained. This ensures that results on this held-out test data represent
    what we might encounter when using the model in the wild.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: fit 方法使用提供的训练样本（x_train）和相应的类标签的一种独热编码版本（y_test）来训练网络。我们还会传入训练轮次的数量和小批量大小。设置
    verbose 为 1 将产生在[列表 13-4](ch13.xhtml#ch13lis4)中显示的输出。最后，我们有验证 _data。在这个例子中，我们有点马虎，直接传入了所有的测试数据，而没有留出一些用于最终测试。（毕竟这只是一个简单的示例。）通常情况下，我们会保留一些测试数据，用于最终模型训练完成后进行测试。这可以确保在这个留出的测试数据上的结果能够代表我们在实际应用中遇到的情况。
- en: Notice that the fit method returns something. This is a History object, and
    its history property holds a per epoch summary of the training and validation
    loss and accuracy values. We can use these to make summary plots if we wish.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 fit 方法会返回某个结果。这是一个 History 对象，它的 history 属性保存了每个训练轮次的训练和验证损失以及准确率的摘要。我们可以使用这些信息绘制摘要图。
- en: Once the model is trained, we can get a score, similar to the score of sklearn,
    by calling the evaluate method and passing in the test data. The method returns
    a list with the loss and accuracy of the model on the supplied data, which we
    simply print.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以通过调用 evaluate 方法并传入测试数据来获得类似于 sklearn 中的得分。该方法返回一个列表，其中包含模型在提供的数据上的损失和准确率，我们只需要打印它。
- en: We can use the save method to write the model itself to disk for future use.
    Notice the file extension. Keras dumps the model in an HDF5 file. *HDF5* is a
    generic hierarchical data format widely used in scientific circles. In this case,
    the file contains all the weights and biases of the model and the layer structure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 save 方法将模型本身写入磁盘以备将来使用。注意文件扩展名。Keras 会将模型保存为一个 HDF5 文件。*HDF5* 是一种通用的层次化数据格式，广泛应用于科学领域。在这种情况下，文件包含了模型的所有权重、偏差以及层结构。
- en: 'Running this code produces the output shown in [Listing 13-4](ch13.xhtml#ch13lis4):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将生成在[列表 13-4](ch13.xhtml#ch13lis4)中显示的输出：
- en: Using TensorFlow backend.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 后端。
- en: Model parameters = 1199882
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数 = 1199882
- en: 'Layer (type)                 Output Shape              Param #'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 层（类型）                 输出形状             参数数量
- en: ==============================================================
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ==============================================================
- en: conv2d_1 (Conv2D)            (None, 26, 26, 32)        320
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: conv2d_1 (Conv2D)            (None, 26, 26, 32)        320
- en: conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496
- en: max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: max_pooling2d_1 (MaxPooling2) (None, 12, 12, 64)        0
- en: dropout_1 (Dropout)          (None, 12, 12, 64)        0
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: dropout_1 (Dropout)          (None, 12, 12, 64)        0
- en: flatten_1 (Flatten)          (None, 9216)              0
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: flatten_1 (Flatten)          (None, 9216)              0
- en: dense_1 (Dense)              (None, 128)               1179776
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: dense_1 (Dense)              (None, 128)               1179776
- en: dropout_2 (Dropout)          (None, 128)               0
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: dropout_2 (Dropout)          (None, 128)               0
- en: dense_2 (Dense)              (None, 10)                1290
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: dense_2 (Dense)              (None, 10)                1290
- en: ==============================================================
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ==============================================================
- en: 'Total params: 1,199,882'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '总参数: 1,199,882'
- en: 'Trainable params: 1,199,882'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '可训练参数: 1,199,882'
- en: 'Non-trainable params: 0'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '不可训练参数: 0'
- en: Train on 60000 samples, validate on 10000 samples
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据样本 60000 个，验证数据样本 10000 个
- en: Epoch  1/12-loss:0.2800 acc:0.9147 val_loss:0.0624 val_acc:0.9794
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '训练轮次 1/12 - 损失: 0.2800 准确率: 0.9147 验证损失: 0.0624 验证准确率: 0.9794'
- en: Epoch  2/12-loss:0.1003 acc:0.9695 val_loss:0.0422 val_acc:0.9854
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '训练轮次 2/12 - 损失: 0.1003 准确率: 0.9695 验证损失: 0.0422 验证准确率: 0.9854'
- en: Epoch  3/12-loss:0.0697 acc:0.9789 val_loss:0.0356 val_acc:0.9880
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '训练轮次 3/12 - 损失: 0.0697 准确率: 0.9789 验证损失: 0.0356 验证准确率: 0.9880'
- en: Epoch  4/12-loss:0.0573 acc:0.9827 val_loss:0.0282 val_acc:0.9910
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '训练轮次 4/12 - 损失: 0.0573 准确率: 0.9827 验证损失: 0.0282 验证准确率: 0.9910'
- en: Epoch  5/12-loss:0.0478 acc:0.9854 val_loss:0.0311 val_acc:0.9901
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '训练轮次 5/12 - 损失: 0.0478 准确率: 0.9854 验证损失: 0.0311 验证准确率: 0.9901'
- en: Epoch  6/12-loss:0.0419 acc:0.9871 val_loss:0.0279 val_acc:0.9908
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '训练轮次 6/12 - 损失: 0.0419 准确率: 0.9871 验证损失: 0.0279 验证准确率: 0.9908'
- en: Epoch  7/12-loss:0.0397 acc:0.9883 val_loss:0.0250 val_acc:0.9914
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 7/12-loss:0.0397 acc:0.9883 val_loss:0.0250 val_acc:0.9914
- en: Epoch  8/12-loss:0.0344 acc:0.9891 val_loss:0.0288 val_acc:0.9910
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 8/12-loss:0.0344 acc:0.9891 val_loss:0.0288 val_acc:0.9910
- en: Epoch  9/12-loss:0.0329 acc:0.9895 val_loss:0.0273 val_acc:0.9916
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 9/12-loss:0.0329 acc:0.9895 val_loss:0.0273 val_acc:0.9916
- en: Epoch 10/12-loss:0.0305 acc:0.9909 val_loss:0.0296 val_acc:0.9904
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 10/12-loss:0.0305 acc:0.9909 val_loss:0.0296 val_acc:0.9904
- en: Epoch 11/12-loss:0.0291 acc:0.9911 val_loss:0.0275 val_acc:0.9920
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 11/12-loss:0.0291 acc:0.9911 val_loss:0.0275 val_acc:0.9920
- en: Epoch 12/12-loss:0.0274 acc:0.9916 val_loss:0.0245 val_acc:0.9916
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 12/12-loss:0.0274 acc:0.9916 val_loss:0.0245 val_acc:0.9916
- en: 'Test loss: 0.02452171179684301'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '测试损失: 0.02452171179684301'
- en: 'Test accuracy: 0.9916'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '测试准确率: 0.9916'
- en: '*Listing 13-4: MNIST training output*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13-4: MNIST 训练输出*'
- en: We’ve excluded some informational and warning messages from the lower-level
    TensorFlow toolkit and condensed the output to make it easier to follow in the
    text.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们排除了来自较低级别的 TensorFlow 工具包的一些信息性和警告消息，并简化了输出内容，使其更易于在文本中跟随。
- en: At the start of the run, Keras informs us that TensorFlow is our backend. It
    also shows us the shape of the training data, the now familiar 60,000 samples
    with a shape of 28 × 28 × 1 (×1 since the images are grayscale). We have the usual
    10,000 test samples as well.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行开始时，Keras 会通知我们 TensorFlow 是我们的后端。它还显示了训练数据的形状，即现在熟悉的 60,000 个样本，形状为 28 ×
    28 × 1（×1 是因为图像是灰度图）。我们也有常规的 10,000 个测试样本。
- en: Next comes a report on the model. This report shows the layer type, shape of
    the output from the layer, and the number of parameters in the layer. For example,
    the first convolutional layer uses 32 filters and 3 × 3 kernels, so the output
    with a 28 × 28 input will be 26 × 26 × 32\. The None listed for each layer is
    in the place where the number of elements in the minibatch normally is. The printout
    is showing only the relationship between the layers; because nothing in the architecture
    changes the number of elements in the minibatch, there’s no need to explicitly
    mention the minibatch elements (hence the None). The parameters are 3 × 3 × 32
    for the filters plus an additional 32 bias terms for the 320 parameters listed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是关于模型的报告。该报告显示了层的类型、层输出的形状以及层中参数的数量。例如，第一个卷积层使用了 32 个滤波器和 3 × 3 的卷积核，因此，输入为
    28 × 28 时，输出将是 26 × 26 × 32。每一层中列出的 None 表示通常会出现的 minibatch 元素数量的位置。输出显示的只是各层之间的关系；由于架构中没有任何内容会改变
    minibatch 中元素的数量，所以不需要显式地提及 minibatch 元素（因此显示为 None）。参数为 3 × 3 × 32 的滤波器，另外还有
    32 个偏置项，总共列出了 320 个参数。
- en: 'As mentioned in [Chapter 12](ch12.xhtml#ch12), the lion’s share of the parameters
    in the model are between the Flatten layer and the Dense layer. The layer named
    dense_2 is the softmax layer mapping the 128 elements of the Dense layer to the
    10 elements of the softmax: 128 × 10 + 10 = 1290, where the additional 10 are
    the bias terms. Note that the Dropout and Pooling layers have no parameters because
    there is nothing to learn in those layers.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第 12 章](ch12.xhtml#ch12)所述，模型中大部分的参数位于 Flatten 层和 Dense 层之间。名为 dense_2 的层是
    softmax 层，它将 Dense 层的 128 个元素映射到 softmax 的 10 个元素：128 × 10 + 10 = 1290，其中额外的 10
    个是偏置项。请注意，Dropout 和 Pooling 层没有参数，因为这些层中没有需要学习的内容。
- en: 'After the report on the model’s structure, we have the verbose output of the
    training call to fit. We asked for 12 epochs—12 full passes through the training
    data—using a minibatch of 128 samples. The output lists the stats for each pass.
    We see that the loss goes down as we train, which is expected if the model is
    learning, and that the accuracy (acc) on the training data goes up. The validation
    data is used during training to test the model, but this data is not used to update
    the model’s weights and biases. The loss on the validation data is also going
    down with each epoch, but more slowly. What we don’t want to see here is the validation
    loss going up, though it will jump around somewhat, especially if the validation
    set is not very big. We see an opposite effect with the validation accuracy (val_acc).
    It is going up for each epoch of training. If the model were to start overfitting,
    we’d see this accuracy go down after some point. This is the value of validation
    data: to tell us when to stop training.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在报告模型结构之后，我们会看到训练调用的详细输出，以进行拟合。我们请求了12个epoch——即12次完整的训练数据迭代——使用一个128样本的小批量。输出列出了每次迭代的统计数据。我们看到，随着训练的进行，损失逐渐降低，这是预期中的情况，表示模型正在学习，同时训练数据上的准确度（acc）也在上升。验证数据在训练过程中用于测试模型，但这些数据不会用于更新模型的权重和偏置。验证数据的损失每个epoch也在下降，但下降的速度较慢。我们不希望看到的是验证损失上升，尽管它可能会有一些波动，尤其是在验证集不够大的时候。我们在验证准确度（val_acc）上看到了相反的效果。它在每个训练epoch中都在上升。如果模型开始过拟合，我们会看到这种准确度在某个时刻开始下降。这就是验证数据的价值：告诉我们何时停止训练。
- en: The final two lines of output are the loss and accuracy of the model on the
    test samples passed to the evaluate method. Since the validation and test sets
    are the same in this example, these lines match the output for epoch 12\. The
    final accuracy of this model is 99.16 percent—certainly a very good accuracy to
    see.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后两行是模型在通过evaluate方法传入的测试样本上的损失和准确度。由于在这个例子中验证集和测试集是相同的，这两行与epoch 12的输出匹配。该模型的最终准确率为99.16%——这无疑是一个非常好的准确率。
- en: Plotting the Error
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绘制误差图
- en: We can use the saved history to plot the loss or error (1 – accuracy) as a function
    of the training epoch. The plots are similar in shape, so we show only the error
    plot in [Figure 13-1](ch13.xhtml#ch13fig1).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用保存的历史数据来绘制损失或误差（1 - 准确度）与训练epoch的关系。图形形状相似，因此我们仅展示[图13-1](ch13.xhtml#ch13fig1)中的误差图。
- en: '![image](Images/13fig01.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig01.jpg)'
- en: '*Figure 13-1: The MNIST training and validation errors as a function of epoch*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-1：MNIST训练和验证错误与epoch的关系*'
- en: The error on the training data falls off quickly and, as we’ve seen before,
    tends toward 0 as training continues. In this case, the validation error falls
    slightly and then levels off at a value similar to the training error. At this
    point in the book, you might have alarm bells going off in your head when you
    look at [Figure 13-1](ch13.xhtml#ch13fig1). The initial training error is *greater
    than* the initial validation error!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据上的误差迅速下降，正如我们之前所看到的，随着训练的进行，误差趋向于0。在这种情况下，验证误差稍微下降，然后在一个与训练误差相似的值上趋于平稳。在本书的这一部分，看到[图13-1](ch13.xhtml#ch13fig1)时，可能会让你感到警觉：最初的训练误差*大于*最初的验证误差！
- en: The full cause of this is hard to pin down, but one component is using dropout
    in the network. Dropout is applicable only during training, and because of the
    dropping of nodes in layers, dropout is in effect training many models at once,
    which, initially, causes a large error before the model “settles down” and the
    error drops per epoch. We can see that this might be the case here because if
    we simply comment out the Dropout layers in [Listing 13-4](ch13.xhtml#ch13lis4)
    and retrain, we get a new error plot, [Figure 13-2](ch13.xhtml#ch13fig2).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这背后的完全原因很难确定，但其中一个因素是网络中使用了dropout。Dropout只在训练期间适用，由于在层中丢弃节点，dropout实际上是在同时训练多个模型，这最初会导致较大的误差，直到模型“稳定”下来，误差才会逐渐减少。我们可以看到，这可能正是这里的情况，因为如果我们仅仅注释掉[Listing
    13-4](ch13.xhtml#ch13lis4)中的Dropout层并重新训练，我们会得到一张新的误差图，[图13-2](ch13.xhtml#ch13fig2)。
- en: '![image](Images/13fig02.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig02.jpg)'
- en: '*Figure 13-2: The MNIST training and validation errors as a function of epoch
    when no Dropout layers are present*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-2：当没有Dropout层时，MNIST训练和验证错误与epoch的关系*'
- en: In [Figure 13-2](ch13.xhtml#ch13fig2), we see that the validation error quickly
    becomes greater than the training error, as we would expect. Additionally, we
    see that the final validation error is much greater than the final validation
    error for [Figure 13-1](ch13.xhtml#ch13fig1), about 10 percent versus 1 percent.
    This is also something we expect if dropout is actually a sensible thing to use,
    which it is. Note also that by the 12th epoch, the training set error is roughly
    the same regardless of the presence of Dropout layers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 13-2](ch13.xhtml#ch13fig2)中，我们看到验证误差迅速大于训练误差，这是我们所预期的。此外，我们还看到，最终的验证误差比[图
    13-1](ch13.xhtml#ch13fig1)中的最终验证误差大得多，大约是 10% 对比 1%。如果 Dropout 确实是一个合理的使用方法，我们也会预期这种结果。还要注意的是，在第
    12 轮迭代后，无论是否存在 Dropout 层，训练集的误差大致相同。
- en: Finally, some of what we see in [Figures 13-1](ch13.xhtml#ch13fig1) and [13-2](ch13.xhtml#ch13fig2)
    is due to the way Keras reports training and validation accuracies. The reported
    training accuracy (and loss) at the end of an epoch is the average over the epoch,
    but, of course, this is changing as the model learns and tends to increase. However,
    the validation accuracy reported is for the model as it is at the end of the epoch,
    so at times is it possible for the training accuracy to be reported as less than
    the validation accuracy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在[图 13-1](ch13.xhtml#ch13fig1)和[图 13-2](ch13.xhtml#ch13fig2)中看到的部分内容，实际上是由于
    Keras 报告训练和验证准确度的方式所致。在每个迭代结束时报告的训练准确度（和损失）是该迭代的平均值，但显然，随着模型的学习，这个值会不断变化并趋于上升。然而，报告的验证准确度是针对迭代结束时模型的状态，因此，有时训练准确度可能会低于验证准确度。
- en: Now that we’ve seen how to build a simple CNN and run it on a dataset, we are
    in a position to start experimenting with CNNs. Of course, there are an infinite
    number of experiments we could perform—just look at the rate at which new papers
    on deep learning appear on sites like [arxiv.org](http://arxiv.org) or the explosion
    of attendance at machine learning conferences—so we need to restrict ourselves
    to some basic explorations. Hopefully, these will motivate you to explore more
    on your own.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何构建一个简单的 CNN 并在数据集上运行它，我们可以开始对 CNN 进行实验了。当然，我们可以进行无数的实验——看看新论文在 [arxiv.org](http://arxiv.org)
    上的发布频率，或者机器学习会议的参会人数激增——因此，我们需要将自己限制在一些基本的探索中。希望这些能够激励你自己进一步探索。
- en: Basic Experiments
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本实验
- en: 'We did a bit of experimentation already when we removed the Dropout layer.
    All our experiments follow the same general pattern: make a slightly different
    version of the model, train it, and evaluate it against the test set. We will
    try three different types of experiments. The first type modifies the architecture
    of the model; removing Dropout layers falls into this category. The second type
    explores the interplay between training set size, minibatch size, and epochs.
    The last type alters the optimizer used during training.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经进行了一些实验，当我们移除 Dropout 层时。我们所有的实验都遵循相同的一般模式：创建一个略有不同的模型版本，训练它，并用测试集进行评估。我们将尝试三种不同类型的实验。第一种类型修改了模型的架构；移除
    Dropout 层属于这一类别。第二种类型探索了训练集大小、最小批量大小和迭代次数之间的相互作用。最后一种类型则改变了训练过程中使用的优化器。
- en: 'In all three cases, to avoid excessive code listings, we’ll simply comment
    on the variation to the code in the previous section with the understanding that
    the remaining code is the same from experiment to experiment. We’ll number the
    experiments, and you can match the results with the number to find the actual
    Python source code for the experiment. The code is available from the website
    associated with this book: *[https://nostarch.com/practical-deep-learning-python/](https://nostarch.com/practical-deep-learning-python/)*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这三种情况下，为了避免过多列出代码，我们将仅对上一节中代码的变化进行评论，理解剩余的代码在每个实验中是相同的。我们将为每个实验编号，你可以通过编号匹配结果找到实际的
    Python 源代码。代码可以在与本书相关的网站上找到： *[https://nostarch.com/practical-deep-learning-python/](https://nostarch.com/practical-deep-learning-python/)*。
- en: In the previous section, we used the entire training set of 60,000 samples for
    training and the whole test set of 10,000 samples for both validation and as the
    final test set. Here, we’ll restrict ourselves to using the first 1,000 or 1,024
    training samples as the entire training set. Additionally, we’ll use the first
    1,000 samples of the test set as the validation set and reserve the last 9,000
    samples for the final test set we’ll use when training is complete. We’ll report
    the accuracy of these 9,000 images that were unseen during training. The results
    will include the baseline model accuracy and number of parameters, for comparison
    purposes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了整个60,000样本的训练集，以及10,000样本的测试集进行验证和最终测试。在这里，我们将限制使用前1,000或1,024个训练样本作为整个训练集。此外，我们将使用测试集的前1,000个样本作为验证集，并将剩余的9,000个样本保留作为训练完成后使用的最终测试集。我们将报告这些在训练过程中未曾见过的9,000张图像的准确率。结果将包括基准模型的准确率和参数数量，供比较使用。
- en: Bear in mind that unless stated otherwise, the accuracies we present represent
    a single training session for each experiment. You should get slightly different
    results if you run these experiments yourself, but those slight differences shouldn’t
    outweigh the larger differences in accuracy that will result from changing the
    model and/or training process.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，除非另有说明，否则我们呈现的准确率代表每个实验的单次训练过程。如果你自己运行这些实验，结果可能会略有不同，但这些小的差异不应该超过因模型和/或训练过程的变化而导致的准确率的更大差异。
- en: Finally, the models in this section are multiclass, so we could examine the
    confusion matrices to see how the models are making their mistakes. However, it
    would be exceedingly tedious to do this for each experiment. Instead, we will
    use the overall accuracy as our metric, trusting that it is a sufficient measure
    in this case.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本节中的模型为多分类模型，因此我们可以通过检查混淆矩阵来查看模型的错误。然而，对每个实验进行此操作将是极其繁琐的。相反，我们将使用总体准确率作为衡量标准，相信在这种情况下它足够准确。
- en: Architecture Experiments
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 架构实验
- en: Architecture modifications imply removing or adding new layers or altering the
    parameters of a layer. We’ve made a number of architecture modifications and compiled
    the resulting accuracies in [Table 13-1](ch13.xhtml#ch13tab1).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 架构修改意味着移除或添加新层，或更改层的参数。我们做了一些架构修改，并在[表 13-1](ch13.xhtml#ch13tab1)中汇总了相应的准确率。
- en: '**Table 13-1:** Results from Modifying the Model Architecture'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 13-1：** 修改模型架构的结果'
- en: '| **Exp.** | **Modification** | **Test accuracy** | **Parameters** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **实验** | **修改** | **测试准确率** | **参数数量** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | Baseline | 92.70% | 1,199,882 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 基准 | 92.70% | 1,199,882 |'
- en: '| 1 | Add Conv3, 3 × 3 × 64 before Pooling | 94.30% | 2,076,554 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 在池化前添加Conv3，3 × 3 × 64 | 94.30% | 2,076,554 |'
- en: '| 2 | Duplicate Conv2, Pooling layer | 94.11% | 261,962 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 重复Conv2、池化层 | 94.11% | 261,962 |'
- en: '| 3 | Conv1, 3 × 3 × 32 to 5 × 5 × 32 | 93.56% | 1,011,978 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 将Conv1，3 × 3 × 32更改为5 × 5 × 32 | 93.56% | 1,011,978 |'
- en: '| 4 | Dense layer to 1,024 nodes | 92.76% | 9,467,274 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 将全连接层节点数设置为1,024 | 92.76% | 9,467,274 |'
- en: '| 5 | Conv1, Conv2, halve number of filters | 92.38% | 596,042 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Conv1、Conv2，滤波器数量减半 | 92.38% | 596,042 |'
- en: '| 6 | Second Dense layer with 128 nodes | 91.90% | 1,216,394 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 第二个全连接层，节点数为128 | 91.90% | 1,216,394 |'
- en: '| 7 | Dense layer to 32 nodes | 91.43% | 314,090 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 将全连接层节点数设置为32 | 91.43% | 314,090 |'
- en: '| 8 | Remove Pooling layer | 90.68% | 4,738,826 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 移除池化层 | 90.68% | 4,738,826 |'
- en: '| 9 | No ReLU after conv layers | 90.48% | 1,199,882 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 卷积层后不使用ReLU | 90.48% | 1,199,882 |'
- en: '| 10 | Remove Conv2 | 89.39% | 693,962 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 移除Conv2 | 89.39% | 693,962 |'
- en: In [Table 13-1](ch13.xhtml#ch13tab1), the baseline results and model size are
    given first, followed by the various experiments from most accurate to least accurate.
    Let’s look at the table and interpret the results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表 13-1](ch13.xhtml#ch13tab1)中，首先给出了基准结果和模型大小，随后列出了从最准确到最不准确的各种实验。让我们来看看表格并解读结果。
- en: First, we see that adding a third convolutional layer after the second convolutional
    layer (Experiment 1) improves the performance of the model but also adds 876,672
    parameters. Increasing the depth of the network seems to improve the performance
    of the model but at the expense of increasing the number of parameters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们看到在第二个卷积层后添加第三个卷积层（实验1）提高了模型的性能，但也增加了876,672个参数。增加网络深度似乎能够提高模型的性能，但代价是增加了参数数量。
- en: However, in Experiment 2 we also increase the depth of the network by duplicating
    the second convolutional layer and the following pooling layer, but because of
    the second pooling layer, the total number of parameters in the network goes down
    by 937,920\. This is a substantial saving for virtually the same performance.
    This indicates that depth is good, but so is the judicious use of pooling layers
    to keep the number of parameters small. For this dataset, Experiment 2 is a solid
    architecture to use.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实验2中，我们还通过复制第二个卷积层和随后的池化层来增加网络的深度，但由于第二个池化层的存在，网络中的总参数数减少了937,920个。这是一个相当大的节省，并且几乎没有影响性能。这表明，深度是有好处的，但合理使用池化层来保持参数数量小也是有益的。对于这个数据集，实验2是一个稳健的架构选择。
- en: Next, we see that an adjustment to the kernel size of the first convolutional
    layer, Experiment 3, leads to an improvement relative to the baseline. There are
    more parameters in the first convolutional layer (832 versus 320), but because
    of the edge effects when using an exact convolution, by the time we get to the
    output of the Flatten layer, there are now only 7744 values versus 9216 for the
    baseline model. This means that the large matrix between the Flatten and Dense
    layers goes from 1,179,776 down to 991,360 parameters with a net result that the
    model overall has 187,904 fewer parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看到对第一个卷积层核大小的调整（实验3）相对于基准模型有所改进。第一个卷积层的参数更多（832比320），但是由于使用精确卷积时的边缘效应，到达Flatten层输出时，参数值已经从基准模型的9,216个减少到7,744个。这意味着，Flatten和Dense层之间的大矩阵从1,179,776个参数减少到991,360个参数，最终结果是，整个模型减少了187,904个参数。
- en: 'This is good: better performance and fewer parameters to learn. Is there a
    downside to the change of Experiment 3? Not really. Instead, one might argue that
    adjusting the kernel size for the first convolutional layer has made the model
    more appropriate for the spatial information in the digit images, thereby making
    the new representation learned by the convolutional and pooling layers that much
    better at separating the classes. In general, there seems to be a best kernel
    size for the first convolutional layer, the layer that deals with the input to
    the model. That kernel size is related to the spatial structure of the inputs:
    some sizes will be better at detecting input features that are better for separating
    the classes. This general rule does not appear to hold for higher convolutional
    layers, and there the prevailing wisdom is to use 3 × 3 kernels for most convolutional
    layers except the first.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这样很好：更好的性能和更少的参数需要学习。实验3的变化有不好的地方吗？其实没有。相反，有人可能会认为，调整第一个卷积层的核大小使得模型更适应数字图像中的空间信息，从而使卷积和池化层学习到的新表示在分类上表现得更加优秀。一般来说，似乎对于第一个卷积层（即处理模型输入的层）来说，存在一个最佳的核大小。这个核大小与输入的空间结构有关：某些大小在检测有助于区分类别的输入特征时会表现得更好。这个普遍规律似乎不适用于更高的卷积层，在这些层中，普遍的做法是对于大多数卷积层（除了第一个层）使用3
    × 3的核。
- en: Can we combine Experiment 3 and Experiment 2? Certainly. We simply make the
    first convolutional layer of Experiment 2 use a 5 × 5 kernel instead of a 3 ×
    3 kernel. If we do this, we get a model with an overall accuracy of 94.23 percent
    that needs only 188,746 parameters. With this trivial change, we’ve achieved the
    performance of Experiment 10 by using only 9 percent of the parameters.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将实验3和实验2结合起来吗？当然可以。我们只需将实验2中的第一个卷积层使用5 × 5的核，而不是3 × 3的核。如果这样做，我们可以得到一个总准确率为94.23%的模型，且只需要188,746个参数。通过这个微小的改变，我们仅使用了9%的参数就达到了实验10的性能。
- en: 'You might be tempted to simply increase the size of the Dense layer, the layer
    that can be thought of as using the new feature representation discovered by the
    convolutional and pooling layers below it. However, doing so (Experiment 4) results
    in no real improvement in overall accuracy, but with a substantial increase in
    the number of parameters. We know the cause: the 9,216 × 128 weight matrix between
    the Flatten and Dense layers is now a 9,216 × 1,024 matrix. Clearly, for CNNs,
    we want to create the best feature representation so that a simpler top layer
    can be used.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想简单地增加Dense层的大小，这一层可以看作是利用卷积和池化层下方发现的新特征表示。然而，这样做（实验4）并没有带来总体准确率的实质性提升，反而大幅增加了参数数量。我们知道原因：Flatten和Dense层之间的9,216
    × 128权重矩阵现在变成了9,216 × 1,024的矩阵。显然，对于卷积神经网络（CNNs）来说，我们希望创建最佳的特征表示，以便使用更简单的顶层结构。
- en: 'With Experiment 5, we see that we can make the model significantly smaller,
    a reduction of 603,840 parameters, while still achieving the same overall accuracy
    by simply halving the number of filters learned in each of the convolutional layers:
    32 → 16 for Conv1 and 64 → 32 for Conv2\. Again, this is a good optimization provided
    the slight (perhaps in this case meaningless) difference in accuracy is acceptable.
    If we look again at [Figure 12-8](ch12.xhtml#ch12fig8), we can see that, especially
    for the second convolutional layer with 64 filters, the responses are very similar
    for many filters. This implies that there are redundant filters that are not adding
    much to the new feature representation presented to the Dense layers. Despite
    halving the number of filters learned, there are still filters that learn to capture
    the important aspects of the input data used to separate the classes.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验5中，我们看到通过简单地将每个卷积层中学习到的滤波器数量减半，我们可以显著减小模型的规模，减少603,840个参数，同时仍然保持相同的总体准确度：Conv1从32减到16，Conv2从64减到32。再次说明，这是一个不错的优化，前提是可以接受准确度上的轻微（也许在这种情况下毫无意义）差异。如果我们再次查看[图12-8](ch12.xhtml#ch12fig8)，可以看到，尤其是对于第二个卷积层的64个滤波器，许多滤波器的响应非常相似。这意味着存在冗余的滤波器，它们并未为呈现给Dense层的新特征表示做出太多贡献。尽管我们将滤波器数量减半，但仍然有一些滤波器能学习到输入数据中重要的方面，用于区分不同类别。
- en: Experiment 7 plays with the Dense layer nodes, and Experiment 6 adds a second
    Dense layer. Neither offers a real benefit. For Experiment 7, the change in the
    number of model parameters is significant due to the 9,216 × 128 matrix weight
    becoming a 9,216 × 32 matrix. However, 32 nodes does not seem to be the ideal
    number to make use of the new feature representation. The second Dense layer of
    Experiment 6 isn’t too awful in terms of increasing the number of parameters to
    learn, but it isn’t buying us much, either. If we use a larger training set, we
    might get some improvement, but we’ll leave that as an exercise for the reader.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 实验7对Dense层的节点进行了调整，实验6则添加了第二个Dense层。两者都没有带来实质性的好处。在实验7中，由于9,216 × 128的矩阵权重变为9,216
    × 32矩阵，模型参数的变化较大。然而，32个节点似乎并不是理想的数量，无法充分利用新的特征表示。实验6中的第二个Dense层在增加学习参数的数量方面并不算糟糕，但它也没有带来太多好处。如果我们使用更大的训练集，可能会有所改善，但我们将这部分留给读者作为练习。
- en: In the previous chapter, we read about the criticisms levied against pooling
    layers. What if we remove the pooling layer entirely (Experiment 8)? First, we
    see that accuracy drops relative to the baseline model. Worse, we see that the
    size of the network has increased dramatically, from 1,199,882 parameters to 4,738,826,
    by a factor of nearly four. This is due to the increase in the number of elements
    in the output of the Flatten layer, which has gone from 9,216 to 36,864, resulting
    in a weight matrix of 36,864 × 128 + 128 = 4,718,720 elements. This example demonstrates
    why we use pooling layers even with the price they bring in terms of the loss
    of information about the relative position of object parts.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了对池化层的批评。如果完全去掉池化层会怎样（实验8）？首先，我们看到相较于基线模型，准确率有所下降。更糟糕的是，我们看到网络的规模大幅增加，从1,199,882个参数增至4,738,826个，几乎是原来的四倍。这是由于Flatten层输出的元素数量增加，已从9,216增加到36,864，导致了一个36,864
    × 128 + 128 = 4,718,720的权重矩阵。这个例子说明了即使池化层带来了关于物体部件相对位置的信息丧失，仍然需要使用池化层的原因。
- en: Each of the convolutional layers in the baseline model uses ReLU on its outputs.
    Removing these ReLU operations, Experiment 9, leads to a 2 percent reduction in
    accuracy on the test set. Clearly, the ReLU is helping somewhat. What might it
    be doing? The ReLU leaves positive values unchanged and sets negative values to
    0\. When used with the output of a convolutional layer, the ReLU is keeping more
    strongly activated responses to the filters, positive responses, while suppressing
    negative responses. This seems to be helping the entire process of learning a
    new representation of the input.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型中的每个卷积层都会对其输出使用ReLU激活函数。去除这些ReLU操作后，实验9在测试集上的准确率减少了2个百分点。显然，ReLU在一定程度上起到了帮助作用。它可能在做什么呢？ReLU保持正值不变，将负值设为0。当与卷积层的输出一起使用时，ReLU有助于保持对滤波器强烈激活的响应（即正响应），同时抑制负响应。这似乎有助于整个学习输入新表示的过程。
- en: Finally, Experiment 10 removed Conv2 entirely. This has the greatest effect
    on the overall accuracy because the features passed to the Dense layer are then
    based solely on the output of the first convolutional layer filters. There was
    no opportunity for the model to learn from these outputs and develop filter responses
    based on the larger effective receptive field seen by the second convolutional
    layer.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，实验10完全去除了Conv2层。这个变化对整体准确率有最大影响，因为传递到Dense层的特征仅基于第一层卷积层的输出。没有机会让模型从这些输出中学习，并基于第二层卷积层看到的更大有效感受野开发滤波器响应。
- en: 'However, given what we saw in the results of Experiment 3, which increased
    the kernel size used by the first convolutional layer, we might wonder whether
    this change from 3 × 3 to 5 × 5 kernels might somewhat compensate for the loss
    of the second convolutional layer. Fortunately, this is very easy to test. We
    simply change the 3 × 3 kernel parameter of Conv1 to a 5 × 5 and train again.
    Doing this validates our intuition: the resulting overall accuracy increases to
    92.39 percent, virtually the same as the baseline model. Also, this 5 × 5 model
    has only 592,074 parameters, making the change inexpensive in terms of the number
    of model parameters.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到我们在实验3中的结果，该实验增加了第一层卷积层使用的核大小，我们可能会想，这个从3×3到5×5的核变化是否在某种程度上能够弥补第二层卷积层的缺失。幸运的是，这很容易验证。我们只需将Conv1的3×3核参数改为5×5，并重新训练。这样做验证了我们的直觉：最终的整体准确率提高到92.39％，几乎与基线模型相同。而且，这个5×5模型只有592,074个参数，相比模型参数的数量，这个变化并不昂贵。
- en: From all of these results, do we have a winner, an architecture that is lean
    but highly effective? We do—it is Experiment 2 with a 5 × 5 kernel for the first
    convolutional layer. In Keras, to build this architecture, we need the code in
    [Listing 13-5](ch13.xhtml#ch13lis5).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从所有这些结果来看，我们是否找到了一个赢家，一个简洁但高效的架构？我们找到了——那就是实验2，它将第一层卷积层的核大小设置为5×5。在Keras中，要构建这个架构，我们需要[Listing
    13-5](ch13.xhtml#ch13lis5)中的代码。
- en: model = Sequential()
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv2D(32, kernel_size=(5, 5),
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(32, kernel_size=(5, 5),
- en: activation='relu',
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: activation='relu',
- en: input_shape=input_shape))
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape=input_shape))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Dropout(0.25))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Dropout(0.25))
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Flatten())
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(128, activation='relu'))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(128, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: '*Listing 13-5: Building the architecture for Experiment 2*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13-5：为实验2构建架构*'
- en: We’ve simply duplicated the Conv2D, MaxPooling2D, and Dropout layers and used
    (5,5) for the kernel size of the first layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅复制了Conv2D、MaxPooling2D和Dropout层，并且将第一层的核大小设置为(5,5)。
- en: If we train this model using all 60,000 samples of the MNIST training set, we
    get a final held-out test set accuracy of 99.51 percent for an error of 0.49 percent.
    This is using all 10,000 samples. According to [benchmarks.ai](http://benchmarks.ai),
    a website that tracks current bests on different machine learning datasets, the
    state-of-the-art MNIST error is 0.21 percent, so we are not state of the art,
    but we are better than the 99.16 percent accuracy we saw in [Listing 13-4](ch13.xhtml#ch13lis4)
    for the default architecture.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用MNIST训练集中的所有60,000个样本训练该模型，我们将得到一个最终的保留测试集准确率为99.51％，误差为0.49％。这是使用所有10,000个样本得到的结果。根据[benchmarks.ai](http://benchmarks.ai)，这是一个追踪当前不同机器学习数据集最佳表现的网站，当前最先进的MNIST误差为0.21％，因此我们不是最先进的，但我们比在[Listing
    13-4](ch13.xhtml#ch13lis4)中看到的默认架构的99.16％准确率要好。
- en: Training Set Size, Minibatches, and Epochs
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练集大小、迷你批次和周期
- en: These experiments examine the interplay between training set size, minibatch
    size, and number of epochs. Our model will be the default model we were using
    previously, Experiment 0, but this time we’ll use 1,024 samples for the training
    set. We’ll be using powers of two as the minibatch sizes; this is a convenient
    size, as all of our minibatch sizes divide it evenly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验考察了训练集大小、迷你批次大小和周期数之间的相互作用。我们的模型将是我们之前使用的默认模型，即实验0，但这次我们将使用1,024个样本作为训练集。我们将使用2的幂作为迷你批次的大小；这是一个方便的大小，因为所有的迷你批次大小都能均匀地划分它。
- en: Recall, Keras, like sklearn, runs through a given number of epochs, or full
    passes through the training set. Additionally, the minibatch size (batch _size)
    specifies the number of samples used in each iteration, after which the average
    error (the cross-entropy loss) is used to update the parameters. Therefore, each
    processed minibatch leads to a single gradient descent step, and the training
    set size divided by the minibatch size is the number of gradient descent steps
    taken per epoch.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，Keras和sklearn一样，都会运行给定的周期数，即完全通过训练集的次数。此外，迷你批量大小（batch_size）指定了每次迭代中使用的样本数量，之后使用平均误差（交叉熵损失）来更新参数。因此，每处理一个迷你批量，就会进行一次梯度下降步骤，训练集大小除以迷你批量大小就是每个周期内进行的梯度下降步骤数。
- en: 'For our experiments, we’ll use the following minibatch sizes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将使用以下迷你批量大小：
- en: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024
- en: 'For a training set of 1,024 samples (approximately 100 for each digit), the
    number of gradient descent steps per epoch is the reverse of this list: 1,024
    for a batch size of 1, down to 1 for a batch size of 1,024.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个包含1,024个样本的训练集（每个数字大约100个样本），每个周期的梯度下降步数是该列表的逆序：当批量大小为1时为1,024，批量大小为1,024时为1。
- en: 'Let’s generate two plots. The first will plot the final test set accuracy for
    the two cases: a fixed number of gradient descent steps regardless of minibatch
    size, and a fixed number of epochs irrespective of minibatch size. The second
    plot will show us the clock time to train the model for each case. The code leading
    to the plots is in Experiments 26 through 31 (fixed number of gradient descent
    steps) and Experiments 32 through 42 (fixed number of epochs).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成两个图。第一个图将绘制两种情况的最终测试集准确率：无论迷你批量大小如何，固定梯度下降步数，以及无论迷你批量大小如何，固定周期数。第二个图将显示每种情况下训练模型的时钟时间。导致这些图的代码位于实验26至31（固定梯度下降步数）和实验32至42（固定周期数）。
- en: The test set accuracy as a function of minibatch size is given as the mean of
    five runs in [Figure 13-3](ch13.xhtml#ch13fig3), and the standard error of the
    mean given by the error bars. Let’s look at the fixed number of gradient descent
    steps regardless of minibatch size (triangles). In some toolkits, this is referred
    to as using a fixed number of iterations, where *iteration* means a single update
    leading to a gradient descent step.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集准确率作为迷你批量大小的函数，以五次运行的平均值呈现，见[图13-3](ch13.xhtml#ch13fig3)，误差条表示均值的标准误差。让我们看看无论迷你批量大小如何固定梯度下降步数（用三角形表示）。在某些工具包中，这被称为使用固定的迭代次数，其中*迭代*指的是一次更新，导致一次梯度下降步。
- en: '![image](Images/13fig03.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig03.jpg)'
- en: '*Figure 13-3: MNIST test set accuracy as a function of minibatch size and fixed
    gradient descent steps or fixed epochs*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-3：MNIST测试集准确率作为迷你批量大小与固定梯度下降步数或固定周期数的函数*'
- en: 'The number of gradient descent steps was fixed at 1,024\. This means that we
    need to change the number of epochs, depending on how many minibatches are in
    the training set. For the case of a single sample used per update (batch_size=1),
    we get the required 1,024 steps in a single epoch, so we set the number of epochs
    to 1\. For a minibatch size of two, we get 512 steps per epoch, so we set the
    number of epochs in the code to 2 to get 1,024 steps overall. The pattern continues:
    to get the 1,024 gradient descent steps for each minibatch size, we need to set
    the number of epochs to the minibatch size.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步数固定为1,024。意味着我们需要根据训练集中每个迷你批量的数量来调整周期数。对于每次更新使用单个样本的情况（batch_size=1），我们在一个周期内获得所需的1,024步，因此我们将周期数设置为1。对于迷你批量大小为2的情况，我们每个周期获得512步，因此我们将代码中的周期数设置为2，以获得1,024步。这个模式继续下去：为了每个迷你批量大小获得1,024个梯度下降步数，我们需要将周期数设置为迷你批量大小。
- en: We see that except for the smallest minibatch sizes, when we fix the number
    of gradient descent steps, we get very consistent overall accuracies. After minibatch
    sizes of about 16, things do not change too much. The general rule for CNNs is
    to use smaller batch sizes. This seems to help with model generalization for datasets
    that are not as simple as MNIST, and, as we’ll soon see, decreases training time
    significantly.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，除了最小的迷你批量大小外，当我们固定梯度下降的步数时，我们得到的整体准确率非常一致。大约16的迷你批量大小后，变化不大。对于卷积神经网络（CNNs），一般的规则是使用较小的批量大小。这似乎有助于提高模型在像MNIST这样不是那么简单的数据集上的泛化能力，正如我们很快会看到的，它显著减少了训练时间。
- en: The second curve of [Figure 13-3](ch13.xhtml#ch13fig3) shows the accuracy for
    a fixed number of epochs (circles). If we change the minibatch size but do not
    increase the number of training epochs to make the number of gradient descent
    steps remain constant, we’ll be using larger and better estimates of the gradient,
    but we’ll also be taking fewer steps. We see that doing this quickly results in
    a substantial decrease in accuracy. We should not be surprised. With a minibatch
    size of one, training for 12 epochs gives us a model that took 12 × 1,024 = 12,288
    gradient descent steps. Granted, the gradient estimate was particularly noisy
    in this case since we use only one training sample to estimate it, but with lots
    of steps, we arrived at a well-performing model all the same. By the time we get
    to a minibatch of 1,024 samples, the size of our training set, we see that we
    took only 12 gradient descent steps before calling it a day. No wonder the results
    are so poor.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-3](ch13.xhtml#ch13fig3)中的第二条曲线展示了在固定轮次下的准确率（圆圈）。如果我们改变小批量的大小，但不增加训练轮次以保持梯度下降步骤数不变，我们将使用更大且更好的梯度估计，但同时我们也会采取更少的步骤。我们看到，这样做很快会导致准确率显著下降。我们不应该感到惊讶。使用小批量大小为1时，训练12个轮次给我们带来了一个模型，它需要12
    × 1,024 = 12,288个梯度下降步骤。当然，由于我们仅使用一个训练样本来估计梯度，估计结果在这种情况下尤其嘈杂，但由于步骤较多，我们依然得到了一个表现良好的模型。当我们增加到1,024个样本的小批量时，即我们整个训练集的大小，我们看到在结束训练前，我们只进行了12个梯度下降步骤。难怪结果如此差劲。'
- en: The second plot of this section is [Figure 13-4](ch13.xhtml#ch13fig4).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的第二张图是[图13-4](ch13.xhtml#ch13fig4)。
- en: '![image](Images/13fig04.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig04.jpg)'
- en: '*Figure 13-4: Model training time as a function of minibatch size and fixed
    gradient descent steps or fixed epochs*'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-4：模型训练时间与小批量大小和固定梯度下降步骤或固定训练轮次的关系*'
- en: As before, let’s start with the fixed number of gradient descent steps (triangles).
    We see immediately that the training time is linearly proportional to the minibatch
    size. This is reasonable since we just saw that we need to increase the number
    of epochs to keep the number of gradient descent steps constant while increasing
    the minibatch size. Therefore, the amount of data passed through the network is
    increasing proportionally, so the time required for the forward and backward training
    passes will increase proportionally as well. From this, we see that large minibatch
    sizes cost clock time.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，让我们从固定的梯度下降步骤数（三角形）开始。我们可以立刻看到，训练时间与小批量大小成线性关系。这是合理的，因为我们刚刚看到，为了保持梯度下降步骤数不变而增加小批量大小时，必须增加训练轮次。因此，传递给网络的数据量也在成比例增加，前向和反向训练传播所需的时间也会成比例增加。从这一点来看，大的小批量大小会增加训练时间。
- en: For a fixed number of epochs, we see a different story. For tiny minibatch sizes,
    training time goes up due to the number of forward and backward passes. For the
    case of a minibatch of one, we need 12,288 such passes, as we just saw. However,
    by the time we get to minibatches of even 32 samples at a time, we have only 1024/32
    = 32 passes per epoch for a total of 384 for the entire training session. This
    is far fewer than for the smallest minibatch size, so we might expect the training
    time for minibatches this size or larger to be roughly constant in time, as we
    see in [Figure 13-4](ch13.xhtml#ch13fig4).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于固定的训练轮次，我们看到一个不同的情况。对于极小的小批量大小，由于前向和反向传播的次数增加，训练时间也会增加。在小批量为1的情况下，我们需要12,288次这样的传播，就如我们刚才看到的那样。然而，当我们使用每次32个样本的小批量时，我们每个轮次只需要进行1024/32
    = 32次传播，总共进行384次传播。这比最小的小批量次数少得多，因此我们可以预期，对于这种大小或更大的小批量，训练时间大致会保持不变，正如我们在[图13-4](ch13.xhtml#ch13fig4)中看到的那样。
- en: 'What can we glean from these two plots? The following: to balance run time
    and accuracy, we want to use minibatch sizes that are large enough to give us
    a reasonable estimate of the gradient but small enough that, for a fixed number
    of model updates (gradient descent steps), we can train quickly. This argues for
    minibatch sizes in the 16 to 128 range, in general. Indeed, a review of the deep
    learning literature sees minibatches in this range almost exclusively for most
    applications. For this example, minibatch sizes above 16, based on [Figure 13-3](ch13.xhtml#ch13fig3)
    (triangles), result in models that are all basically the same in terms of accuracy,
    but the training time for a model using a minibatch size of 16, according to [Figure
    13-4](ch13.xhtml#ch13fig4) (triangles), compared to one of size 1,024, is a few
    seconds versus approximately 30 minutes.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两张图中我们可以得出什么结论？以下几点：为了平衡运行时间和准确性，我们希望使用足够大的小批量大小来合理估计梯度，但又要小到足以在固定数量的模型更新（梯度下降步骤）下快速训练。这就意味着通常小批量大小应在16到128之间。实际上，深度学习文献中几乎所有应用都采用这个范围的小批量大小。以这个例子为例，基于[图13-3](ch13.xhtml#ch13fig3)（三角形标记），16以上的小批量大小，训练结果基本相同，但根据[图13-4](ch13.xhtml#ch13fig4)（三角形标记），使用16的小批量大小的模型训练时间是几秒钟，而使用1,024的小批量大小则约为30分钟。
- en: Optimizers
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化器
- en: 'So far, all of our experiments have used the same gradient descent algorithm,
    or optimizer: Adadelta. Let’s take a look at how our MNIST model does if we change
    the optimizer but leave everything else fixed. Our model is Experiment 0, the
    model we’ve been using all along in this chapter. We’ll continue to use 1,000
    test samples for validation and 9,000 to determine our final accuracy. However,
    instead of using the first 1,000 or 1,024 training samples, we’ll increase the
    number to the first 16,384 samples. We fix the minibatch size at 128 and the number
    of epochs to 12, as before. We’ll report the results as mean and standard error
    over five runs.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的所有实验都使用了相同的梯度下降算法，或者说优化器：Adadelta。让我们来看一下如果我们更换优化器而其他条件保持不变，MNIST模型的表现如何。我们的模型是实验
    0，也就是本章中一直使用的模型。我们将继续使用1,000个测试样本进行验证，并用9,000个样本来确定最终准确性。然而，代替使用前1,000个或1,024个训练样本，我们将训练样本数增加到前16,384个样本。我们将小批量大小固定为128，训练周期数设为12，与之前相同。我们将报告结果为五次实验的均值和标准误差。
- en: 'Keras currently supports the following optimizers: stochastic gradient descent
    (SGD), RMSprop, Adagrad, Adadelta, and Adam. We’ll train using each of these in
    turn. For Adagrad, Adadelta, and Adam, we leave the parameters at their default
    settings, as recommended by the Keras documentation. For RMSprop, the only parameter
    the Keras documentation recommends adjusting is the learning rate (lr), which
    we set to 0.01, a typical value. For SGD, we set the learning rate to 0.01 as
    well and set standard momentum to 0.9, also a very typical value. The code is
    found in Experiments 43 through 47.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Keras目前支持以下优化器：随机梯度下降（SGD）、RMSprop、Adagrad、Adadelta和Adam。我们将依次使用这些优化器进行训练。对于Adagrad、Adadelta和Adam，我们保持参数为Keras文档推荐的默认设置。对于RMSprop，Keras文档推荐调整的唯一参数是学习率（lr），我们将其设置为0.01，这是一个典型的值。对于SGD，我们同样将学习率设置为0.01，并将标准动量设置为0.9，这也是一个非常典型的值。代码可以在实验43到47中找到。
- en: '[Figure 13-5](ch13.xhtml#ch13fig5) shows the test set accuracy (top) and training
    time (bottom) for each of the optimizers.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-5](ch13.xhtml#ch13fig5)显示了每个优化器的测试集准确性（上）和训练时间（下）。'
- en: '![image](Images/13fig05.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig05.jpg)'
- en: '*Figure 13-5: Test set accuracy (top) and training time (bottom) by optimization
    algorithm*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-5：按优化算法分的测试集准确性（上）和训练时间（下）*'
- en: First, notice that there is not too much difference between the results produced
    by each optimizer. This is good news. However, by looking at the error bars, it
    seems clear that Adadelta, Adagrad, and Adam all perform slightly better than
    SGD or RMSprop. This is borne out in the deep learning literature as well, though
    each dataset should be looked at independently.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意到每个优化器的结果差异并不大。这是一个好消息。然而，通过观察误差条，似乎很明显Adadelta、Adagrad和Adam的表现略优于SGD或RMSprop。这在深度学习文献中也得到了证实，尽管每个数据集应该单独分析。
- en: In terms of training time, the different optimizers are also roughly equivalent,
    though SGD is fastest and consistently so. This performance difference might be
    important for a very large dataset. Adam is also consistently faster than Adadelta
    for basically the same performance. Again, these results are evident in the literature
    where both SGD and Adam are widely used.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时间方面，不同的优化器大致相同，尽管SGD速度最快并且始终如此。这个性能差异对于非常大的数据集可能很重要。Adam也始终比Adadelta更快，而且性能几乎相同。再说一次，这些结果在文献中是显而易见的，SGD和Adam广泛使用。
- en: This section has been thorough in terms of minute details associated with changing
    model architectures and training parameters. Hopefully, it has helped you develop
    intuition about how to configure a CNN and how to train it. Rules of thumb are
    hard to present in this area, but I have given some general guidance. Exceptions
    to these rules abound, however, and you have to simply try things, observe the
    results, and adapt when presented with a new dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了与更改模型架构和训练参数相关的细节。希望它能够帮助你对如何配置CNN以及如何训练它产生直观理解。在这个领域，很难提出固定的经验法则，但我已经给出了一些一般性的指导。然而，这些规则也有很多例外，你需要通过尝试不同的方法、观察结果并根据新的数据集进行调整。
- en: Let’s move on now from classifying simple inputs to answering the question of
    how to make a model that can locate targets in arbitrary images.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从对简单输入进行分类转到如何制作一个能够在任意图像中定位目标的模型。
- en: Fully Convolutional Networks
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全卷积网络
- en: We introduced fully convolutional networks in [Chapter 12](ch12.xhtml#ch12).
    Let’s take our basic MNIST CNN model and convert it to a fully convolutional version
    and see how we can use it to locate digits in larger images. Our basic approach
    is to train the model using fully connected layers, as before, and then to create
    a fully convolutional version and update the weights with those from the fully
    connected model. We can then apply the new model to arbitrary size inputs to locate
    digits (hopefully).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第12章](ch12.xhtml#ch12)中介绍了完全卷积网络。让我们将基础的MNIST CNN模型转换为完全卷积版本，看看如何使用它来定位更大图像中的数字。我们基本的方法是首先使用完全连接层训练模型，然后创建一个完全卷积版本，并用完全连接模型的权重更新它。之后，我们可以将新模型应用于任意大小的输入来定位数字（希望如此）。
- en: Building and Training the Model
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建与训练模型
- en: First, we train our base model on the full MNIST dataset. The only change we’ll
    make is to train for 24 epochs instead of just 12\. The output of this process
    is an HDF5 file that contains the trained weights and biases. All we then need
    to do is create the fully convolutional version by changing the fully connected
    layer and copy the weights and biases from the old model to the new. The code
    for this is straightforward, as shown in [Listing 13-6](ch13.xhtml#ch13lis6).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在完整的MNIST数据集上训练我们的基础模型。我们唯一的变化是将训练周期数从12增加到24。此过程的输出是一个包含训练好的权重和偏置的HDF5文件。接下来，我们只需要通过修改完全连接层来创建完全卷积版本，并将旧模型的权重和偏置复制到新模型中。这个过程的代码很简单，如[Listing
    13-6](ch13.xhtml#ch13lis6)所示。
- en: from keras.models import Sequential, load_model
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential, load_model
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Conv2D, MaxPooling2D
- en: ❶ weights = load_model('mnist_cnn_base_model.h5').get_weights()
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ weights = load_model('mnist_cnn_base_model.h5').get_weights()
- en: model = Sequential()
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(32, kernel_size=(3, 3),
- en: activation='relu',
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: activation='relu',
- en: ❷ input_shape=(None,None,1)))
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ input_shape=(None,None,1)))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Dropout(0.25))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: ❸ model.add(Conv2D(128, (12,12), activation='relu'))
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ model.add(Conv2D(128, (12,12), activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: ❹ model.add(Conv2D(10, (1,1), activation='softmax'))
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ model.add(Conv2D(10, (1,1), activation='softmax'))
- en: ❺ model.layers[0].set_weights([weights[0], weights[1]])
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ model.layers[0].set_weights([weights[0], weights[1]])
- en: model.layers[1].set_weights([weights[2], weights[3]])
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: model.layers[1].set_weights([weights[2], weights[3]])
- en: model.layers[4].set_weights([weights[4].reshape([12,12,64,128]), weights[5]])
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: model.layers[4].set_weights([weights[4].reshape([12,12,64,128]), weights[5]])
- en: model.layers[6].set_weights([weights[6].reshape([1,1,128,10]), weights[7]])
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: model.layers[6].set_weights([weights[6].reshape([1,1,128,10]), weights[7]])
- en: model.save('mnist_cnn_fcn_model.h5')
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: model.save('mnist_cnn_fcn_model.h5')
- en: '*Listing 13-6: Creating the trained fully connected model*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13-6: 创建训练好的完全连接模型*'
- en: After importing the necessary Keras modules, we load the trained weights from
    the fully connected model ❶. Then, we construct the fully convolutional version
    much as we did for the fully connected version. However, there are some key differences.
    The first has to do with the input convolutional layer ❷. In the fully connected
    model, we specified the input image size here, 28×28 pixels with one channel (grayscale).
    For the fully convolutional case, we do not know the size of the input, so we
    use None instead for the width and height. We do know that the input will be a
    single channel image, so we leave the 1.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的Keras模块后，我们从全连接模型中加载训练好的权重❶。然后，我们构建完全卷积的版本，就像我们为全连接版本所做的那样。然而，这里有一些关键的不同之处。第一个不同之处在于输入卷积层❷。在全连接模型中，我们在这里指定了输入图像的大小，28×28像素，单通道（灰度）。对于完全卷积的情况，我们并不知道输入的大小，所以我们使用`None`来表示宽度和高度。我们知道输入将是单通道图像，所以我们保留1。
- en: Since the Dense layers are the reason we have to use fixed size inputs, we replace
    them with equivalent convolutional layers ❸. We replace
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Dense层是我们需要使用固定大小输入的原因，因此我们将它们替换为等效的卷积层❸。我们将
- en: model.add(Flatten())
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(128, activation='relu'))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(128, activation='relu'))
- en: with
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: with
- en: model.add(Conv2D(128, (12,12), activation='relu'))
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(128, (12,12), activation='relu'))
- en: The (12,12) is the output size of the max-pooling layer above, and the 128 is
    the number of filters to learn that stand in for the 128 nodes we had before.
    Again, the critical point here is that the output of this convolutional layer
    is 1 × 1 × 128 because convolving a 12 × 12 input with a 12 × 12 kernel produces
    a single output value. The difference is that the convolutional layer is not tied
    to any fixed input size as the combination of Flatten and Dense was.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: (12, 12)是上面最大池化层的输出大小，而128是学习的过滤器数量，代表之前的128个节点。再次强调，这里关键的一点是，这个卷积层的输出是1 × 1
    × 128，因为12 × 12的输入与12 × 12的卷积核进行卷积，输出一个值。不同之处在于，卷积层不再像Flatten和Dense组合那样依赖于固定的输入大小。
- en: The final softmax layer also needs to be made fully convolutional ❹. There are
    ten outputs, one per digit, and the activation remains the same. The kernel size,
    however, is 1 × 1\. The input to this layer is 1 × 1 × 128, so the kernel size
    to cover it is 1 × 1\. Again, if we were to work through the math, we’d see that
    a 1 × 1 × 128 input to a 1 × 1 × 10 convolutional layer matches that of a fully
    connected layer of 128 nodes mapped to 10 nodes in the next layer. The difference
    here is that if the input is larger than 1 × 1, we can still convolve over it.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的softmax层也需要变成完全卷积的❹。这里有十个输出，每个输出对应一个数字，激活函数保持不变。然而，卷积核的大小是1 × 1。此层的输入是1 ×
    1 × 128，因此覆盖它的卷积核大小也是1 × 1。再一次，如果我们通过数学运算，能发现1 × 1 × 128的输入与1 × 1 × 10的卷积层输出是匹配的，这就像一个128节点的全连接层映射到下一个层的10个节点。区别在于，如果输入大于1
    × 1，我们仍然可以对其进行卷积操作。
- en: Now that we’ve constructed the fully convolutional version of our model, we
    need to copy the weights from the trained fully connected model to it ❺. We loaded
    the trained weights into weights. This is a list of NumPy arrays for the weights
    and biases, layer by layer. So, weights[0] refers to the weights of the first
    Conv2D layer, and weights[1] are the biases. Similarly, weights[2] and weights[3]
    are the weights and bias values for the second convolutional layer. We set them
    in the new fully convolutional model by updating the proper layers via the set_weights
    method. Layers 0 and 1 are the two convolutional layers.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了模型的完全卷积版本，接下来需要将训练好的全连接模型的权重复制到这个模型中❺。我们将训练好的权重加载到`weights`中。这个`weights`是一个包含每一层权重和偏置的NumPy数组列表。所以，weights[0]表示第一个Conv2D层的权重，而weights[1]是偏置。同样，weights[2]和weights[3]分别表示第二个卷积层的权重和偏置。我们通过`set_weights`方法更新新模型的相应层来设置这些权重。层0和层1是两个卷积层。
- en: 'Layer 4 is the new Conv2D layer that replaced the Flatten and Dense layers
    of the original model. Here when we set the weights, we need to reshape them to
    match the form of a convolutional layer: 12 × 12 × 64 × 128\. This is for a 12
    × 12 kernel mapped over 64 inputs leading to 128 outputs. The 64 is the number
    of 12 × 12 outputs from the pooling layer above.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 层4是新的Conv2D层，取代了原始模型中的Flatten和Dense层。在设置权重时，我们需要将它们重塑成卷积层的形式：12 × 12 × 64 ×
    128。这是针对12 × 12的卷积核在64个输入上进行卷积，输出128个结果。64是来自上面池化层的12 × 12输出的数量。
- en: Finally, we set the output layer weights. Again, we need to reshape them to
    1 × 1 × 128 × 10 for the 1 × 1 × 128 input and 10 outputs. The biases for the
    two new Conv2D layers are in weights[5] and weights[7], so we add them as well.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设置输出层的权重。同样，我们需要将它们调整为 1 × 1 × 128 × 10，以适应 1 × 1 × 128 的输入和 10 个输出。两个新的
    Conv2D 层的偏置在 weights[5] 和 weights[7] 中，因此我们也需要将它们加进去。
- en: The fully convolutional model is now defined and completely populated with the
    weights and biases from the fully connected model. [Figure 13-6](ch13.xhtml#ch13fig6)
    shows the mapping between models, with the original architecture on the left and
    the fully convolutional architecture on the right. The boxes represent layers,
    with the top set of numbers being the input and the bottom the output. For the
    fully convolutional model, input height and width are arbitrary and marked with
    “--”.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，全卷积模型已经定义并完全填充了来自全连接模型的权重和偏置。[图 13-6](ch13.xhtml#ch13fig6) 显示了模型之间的映射，左边是原始架构，右边是全卷积架构。方框表示层，顶部的数字是输入，底部是输出。对于全卷积模型，输入的高度和宽度是任意的，并标记为“--”。
- en: '![image](Images/13fig06.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig06.jpg)'
- en: '*Figure 13-6: Mapping a fully connected model (left) to a fully convolutional
    model (right)*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13-6: 将全连接模型（左）映射到全卷积模型（右）*'
- en: All that is left to do is write the new fully convolutional model to disk, and
    it’s ready to use. Let’s see how.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是将新的全卷积模型写入磁盘，它就可以准备使用了。我们来看一下怎么做。
- en: Making the Test Images
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 制作测试图像
- en: To test the fully convolutional model, we first need images with digits. Unlike
    our training images, which were small and had a single digit in the center, we
    want larger test images that contain many digits in arbitrary locations. The MNIST
    dataset consists of shades of gray on a black background; therefore, our test
    images should have a black background also. This will make the test images come
    from the same “domain” as the training images, which, as we’ve emphasized before,
    is critical. Making models adapt to different data domains is an active research
    area. Search for *domain adaptation*.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试全卷积模型，我们首先需要带有数字的图像。与我们用于训练的图像不同，训练图像很小且只有一个数字在中央，而我们现在需要的是较大的测试图像，图像中包含许多数字，位置是任意的。MNIST
    数据集由黑色背景上的灰度阴影组成；因此，我们的测试图像也应该有黑色背景。这将使得测试图像与训练图像来自相同的“领域”，正如我们之前强调过的，这一点至关重要。让模型适应不同数据领域是一个活跃的研究领域。请搜索
    *领域适应*。
- en: Making the test images by using Python and the digits from the MNIST test set
    is straightforward. We didn’t use the test set images for training, so using them
    to make our larger test images isn’t cheating. The code is shown in [Listing 13-7](ch13.xhtml#ch13lis7).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 和 MNIST 测试集中的数字制作测试图像是直接的。我们没有使用测试集图像进行训练，因此使用它们来制作更大的测试图像并不算作弊。代码见[清单
    13-7](ch13.xhtml#ch13lis7)。
- en: import os
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import sys
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: import numpy as np
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import random
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: from PIL import Image
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: os.system("rm -rf images; mkdir images")
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf images; mkdir images")
- en: 'if (len(sys.argv) > 1):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (len(sys.argv) > 1):'
- en: N = int(sys.argv[1])
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: N = int(sys.argv[1])
- en: 'else:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: N = 10
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: N = 10
- en: x_test = np.load("data/mnist/mnist_test_images.npy")
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("data/mnist/mnist_test_images.npy")
- en: 'for i in range(N):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(N):'
- en: ❶ r,c = random.randint(6,12), random.randint(6,12)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ r,c = random.randint(6,12), random.randint(6,12)
- en: g = np.zeros(r*c)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: g = np.zeros(r*c)
- en: '❷ for j in range(r*c):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ for j in range(r*c):'
- en: 'if (random.random() < 0.15):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (random.random() < 0.15):'
- en: g[j] = 1
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: g[j] = 1
- en: g = g.reshape((r,c))
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: g = g.reshape((r,c))
- en: g[:,0] = g[0,:] = g[:,-1] = g[-1,:] = 0
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: g[:,0] = g[0,:] = g[:,-1] = g[-1,:] = 0
- en: ❸ img = np.zeros((28*r,28*c), dtype="uint8")
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ img = np.zeros((28*r,28*c), dtype="uint8")
- en: 'for x in range(r):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 'for x in range(r):'
- en: 'for y in range(c):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'for y in range(c):'
- en: 'if (g[x,y] == 1):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (g[x,y] == 1):'
- en: ❹ n = random.randint(0, x_test.shape[0])
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ n = random.randint(0, x_test.shape[0])
- en: im = x_test[n]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: im = x_test[n]
- en: img[28*x:(28*x+28), 28*y:(28*y+28)] = im
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: img[28*x:(28*x+28), 28*y:(28*y+28)] = im
- en: Image.fromarray(img).save("images/image_%04d.png" % i)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Image.fromarray(img).save("images/image_%04d.png" % i)
- en: '*Listing 13-7: Building large MNIST test set images*'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 13-7: 构建大型 MNIST 测试集图像*'
- en: We’re making use of the MNIST test set file we created in [Chapter 5](ch05.xhtml#ch05).
    We could just as quickly have loaded the test images through Keras, as we did
    earlier for our basic CNN experiments. The code itself creates an output directory,
    *images*, and gets the number of images to build from the command line, if given
    (N).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是在[第 5 章](ch05.xhtml#ch05)中创建的 MNIST 测试集文件。我们也可以像之前在基本 CNN 实验中一样，通过 Keras
    加载测试图像。代码本身创建了一个输出目录，*images*，并从命令行获取要构建的图像数量（如果给定的话，N）。
- en: The images are of random sizes ❶. Here, r and c are the number of rows and columns
    in the large image in terms of the number of 28 × 28 MNIST digits. To decide where
    to place our digits so they don’t overlap, we create a grid, g, with either a
    0 or a 1 in each possible digit position (r*c of them) ❷. There is a 15 percent
    chance that any grid position will contain a 1\. We then reshape the grid into
    an actual 2D array and set the border positions of the grid to 0 to ensure that
    no digits appear on the edge of the image.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像的尺寸是随机的❶。这里，r 和 c 是大图像中行和列的数量，单位是 28 × 28 的 MNIST 数字。为了决定我们放置数字的位置，以免它们重叠，我们创建了一个网格
    g，在每个可能的数字位置（r*c 个位置）上放置 0 或 1❷。任意网格位置有 15% 的概率会包含 1。接着，我们将网格重塑为一个实际的 2D 数组，并将网格的边界位置设为
    0，以确保没有数字出现在图像的边缘。
- en: The actual output image is then defined ❸ as the number of rows and columns
    are multiplied by 28, the width and height of the MNIST digit. We loop over each
    digit position (x and y), and if the grid value at that row and column is 1, then
    we select a digit at random and copy it to the current row and column digit position
    in the output image (img) ❹. When every grid position has been examined, the image
    is written to disk so that we can use it with our fully convolutional network.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 实际输出图像的定义是：当行和列的数量与 28（MNIST 数字的宽度和高度）相乘时，我们遍历每个数字位置（x 和 y），如果该行列的网格值为 1，则我们随机选择一个数字并将其复制到输出图像（img）的当前行列位置❹。当每个网格位置都被检查完毕后，图像会被写入磁盘，以便我们能将其用于完全卷积网络。
- en: Testing the Model
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试模型
- en: Let’s test the model—first on single MNIST digits and then on the randomly generated
    large digit images. The fully convolutional model should work as well with single
    MNIST digits as the fully connected model did. The code to test this assertion
    is in [Listing 13-8](ch13.xhtml#ch13lis8).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试这个模型——首先是单个的 MNIST 数字，然后是随机生成的大数字图像。这个完全卷积模型应该和完全连接模型一样，在单个 MNIST 数字上表现得很好。用来验证这一说法的代码在[清单
    13-8](ch13.xhtml#ch13lis8)中。
- en: import numpy as np
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from keras.models import load_model
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import load_model
- en: x_test = np.load("data/mnist/mnist_test_images.npy")/255.0
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("data/mnist/mnist_test_images.npy")/255.0
- en: y_test = np.load("data/mnist/mnist_test_labels.npy")
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("data/mnist/mnist_test_labels.npy")
- en: model = load_model("mnist_cnn_fcn_model.h5")
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: model = load_model("mnist_cnn_fcn_model.h5")
- en: N = y_test.shape[0]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: N = y_test.shape[0]
- en: nc = nw = 0.0
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: nc = nw = 0.0
- en: 'for i in range(N):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(N):'
- en: ❶ p = model.predict(x_test[i][np.newaxis,:,:,np.newaxis])
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ p = model.predict(x_test[i][np.newaxis,:,:,np.newaxis])
- en: c = np.argmax(p)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: c = np.argmax(p)
- en: 'if (c == y_test[i]):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (c == y_test[i]):'
- en: nc += 1
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: nc += 1
- en: 'else:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: nw += 1
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: nw += 1
- en: print("Single MNIST digits, n=%d, accuracy = %0.2f%%" % (N, 100*nc/N))
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: print("单个 MNIST 数字，n=%d，准确率 = %0.2f%%" % (N, 100*nc/N))
- en: '*Listing 13-8: Verifying that the fully convolutional model works with single
    MNIST digits*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 13-8：验证完全卷积模型是否能处理单个 MNIST 数字*'
- en: We load the MNIST test images and labels, along with the fully convolutional
    model, and then loop over each test image and ask the model to make a prediction
    ❶. Note, the image is 2D, but we must pass a 4D array to the predict method, hence
    using np.newaxis to create the missing axes. The prediction for the digit is stored
    in p as a vector of per class probabilities. The label associated with the largest
    of these probabilities is the label assigned to the input digit by the model c.
    If c matches the actual test label, we increment the number of correct predictions
    (nc); otherwise, we increment the number of wrong predictions (nw). Once all 10,000
    test images have been processed, we can output the overall accuracy, which is
    99.25 percent for my training of the fully convolutional model.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载 MNIST 测试图像和标签，以及完全卷积模型，然后遍历每个测试图像并请求模型进行预测❶。请注意，图像是 2D 的，但我们必须传递一个 4D 数组给预测方法，因此使用
    np.newaxis 来创建缺失的轴。数字的预测结果存储在 p 中，作为每个类别的概率向量。与这些概率最大值对应的标签是模型分配给输入数字的标签 c。如果
    c 与实际的测试标签匹配，我们就增加正确预测的数量 (nc)；否则，增加错误预测的数量 (nw)。一旦所有 10,000 张测试图像处理完毕，我们就可以输出整体的准确率，对于我训练的完全卷积模型，准确率为
    99.25%。
- en: Okay, the fully convolutional model is highly accurate, but so what? We passed
    single-digit images to it as inputs and got a single output value. We had this
    capability before with the fully connected model. To expose the utility of the
    fully convolutional model, let’s now pass the large MNIST digit images as input.
    In code, we do this as shown in [Listing 13-9](ch13.xhtml#ch13lis9).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，完全卷积模型非常准确，但这又有什么意义呢？我们曾经将单数字图像作为输入，并获得一个输出值。在完全连接模型中，我们已经具备了这种能力。为了展示完全卷积模型的实用性，现在让我们将大尺寸的MNIST数字图像作为输入。在代码中，我们通过[清单
    13-9](ch13.xhtml#ch13lis9)来实现。
- en: import os
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import numpy as np
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from keras.models import load_model
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import load_model
- en: from PIL import Image
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: model = load_model("mnist_cnn_fcn_model.h5")
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: model = load_model("mnist_cnn_fcn_model.h5")
- en: os.system("rm -rf results; mkdir results")
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf results; mkdir results")
- en: n = len(os.listdir("images"))
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: n = len(os.listdir("images"))
- en: 'for i in range(n):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(n):'
- en: f = "images/image_%04d.png" % i
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: f = "images/image_%04d.png" % i
- en: ❶ im = np.array(Image.open(f))/255.0
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ im = np.array(Image.open(f))/255.0
- en: p = model.predict(im[np.newaxis,:,:,np.newaxis])
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: p = model.predict(im[np.newaxis,:,:,np.newaxis])
- en: np.save("results/results_%04d.npy" % i, p[0,:,:,:])
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("results/results_%04d.npy" % i, p[0,:,:,:])
- en: '*Listing 13-9: Running the fully convolutional model over large test images*'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 13-9：在大尺寸测试图像上运行完全卷积模型*'
- en: We import the necessary modules and then load the fully convolutional model.
    We then create a new output directory, *results*, and find the number of large
    digit images (n). Next, we loop over each of the large digit images.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入必要的模块，然后加载完全卷积模型。接着，我们创建一个新的输出目录，*results*，并找到大尺寸数字图像的数量（n）。然后，我们遍历每一张大尺寸数字图像。
- en: After loading the image from disk, being careful to make a NumPy array from
    it and scaling it by 255 since the training data was also scaled by 255 ❶, we
    make a prediction and store the model output in p. Notice, we make a 4D input
    to predict, just as we did for the single digits earlier, but this time, im is
    larger than 28 × 28 and contains multiple digits. Because the model is fully convolutional,
    this isn’t an issue; we will not get an error. Instead, p is a 4D array with the
    first dimension of one, the number of input images, and a final dimension of ten,
    the number of digits. The middle two dimensions of p are a function of the size
    of the input passed to the predict method. Since the input was larger than 28×28
    pixels, the entire model convolved over the input image as though the model was
    a convolutional layer with a kernel of 28 × 28\. Specifically, the output of this
    convolution has height and width of
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在从磁盘加载图像后，我们小心地将其转换为NumPy数组并通过255进行缩放，因为训练数据也经过了相同的缩放❶，然后进行预测并将模型输出存储在p中。请注意，我们对predict方法使用了4D输入，就像之前处理单数字图像时一样，但这次im的大小超过了28
    × 28并包含了多个数字。由于模型是完全卷积的，这不会出现问题；我们不会得到错误。相反，p是一个4D数组，其中第一个维度为1，表示输入图像的数量，最后一个维度为10，表示数字的数量。p的中间两个维度是传递给predict方法的输入大小的函数。由于输入大于28×28像素，整个模型像一个28
    × 28的卷积核一样卷积输入图像。具体来说，这次卷积的输出具有以下高度和宽度：
- en: '![image](Images/334equ01.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/334equ01.jpg)'
- en: where *H*,*W* are the height and width of the input image and *h*,*w* are the
    height and width of the output array from predict. The 28 in the formula is the
    size of the inputs we initially trained on, 28 × 28 digit images. Where did the
    mysterious 2 come from in the denominator? This is the stride of the 28 × 28 kernel
    over the input image. It is 2 because that is the factor the input image is changed
    by when it gets down to the fully convolutional output layers. The input was 28
    × 28, but, after the two convolutional layers and the pooling layer, the input
    is mapped to 12 × 12 and ⌊28/12⌋ = 2.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*H*、*W*是输入图像的高度和宽度，*h*、*w*是从预测得到的输出数组的高度和宽度。公式中的28是我们最初训练时输入的大小，即28 × 28的数字图像。那么，分母中的神秘2从哪里来呢？这是28
    × 28卷积核在输入图像上滑动的步幅（stride）。它是2，因为这是输入图像在经过两层卷积层和池化层后变换到完全卷积输出层时的变化因子。输入是28 × 28，但在经过两层卷积层和池化层后，输入图像映射为12
    × 12，并且⌊28/12⌋ = 2。
- en: We stated that the array in p is 4D; now we know we get a specific-size output
    based on convolving a 28 × 28 region over the input image using a stride of 2\.
    What do we get at each of the *h*,*w* output array positions? The last element
    of the 4D output has size 10; these are the per class predictions at the specific
    *h*,*w* output position that corresponds to the 28 × 28 kernel.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到p中的数组是4D的；现在我们知道，基于步幅为2的情况下，将28 × 28区域卷积到输入图像上，我们将得到一个特定大小的输出。那么在每个*h*，*w*输出数组位置上我们会得到什么？4D输出的最后一个元素大小为10；这些是对应28
    × 28核的特定*h*，*w*输出位置的每类预测。
- en: Let’s make this abstract description more concrete. The upper-left corner of
    [Figure 13-7](ch13.xhtml#ch13fig7) shows one of the large input images where we’ve
    inverted it to make it black on a white background and added a border so you can
    see the full size of the image.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个抽象描述变得更加具体。[图13-7](ch13.xhtml#ch13fig7)的左上角展示了其中一张大输入图像，我们已将其反转，使其呈现为黑色背景上的白色图像，并添加了边框，以便能够看到图像的完整大小。
- en: '![image](Images/13fig07.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig07.jpg)'
- en: '*Figure 13-7: Per digit heatmap output of the fully convolutional model for
    the input image on the upper left. The model was trained on the standard MNIST
    dataset.*'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13-7：全卷积模型对左上方输入图像的每个数字的热力图输出。该模型是在标准MNIST数据集上训练的。*'
- en: 'The image in the top left of [Figure 13-7](ch13.xhtml#ch13fig7) is 336 pixels
    wide and 308 pixels tall, meaning the output from passing this image to the model
    will be an array that is 1 × 141 × 155 × 10, exactly what we expect from the equations
    on page 744 for the output array dimensions. The output array represents the model’s
    predictions at each of the 28 × 28 regions of the input image when using a stride
    of 2\. There is one prediction for each digit. For example, if p is the 4D output
    of the predict method for the image on the left of [Figure 13-7](ch13.xhtml#ch13fig7),
    then p[0,77,88,:] will return a 10-element vector representing the per class probabilities
    of each digit class for the 28 × 28 input region of the image that maps to 77
    × 88\. In this case, we get the following:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-7](ch13.xhtml#ch13fig7)左上方的图像宽度为336像素，高度为308像素，这意味着将该图像输入模型后，输出将是一个1 ×
    141 × 155 × 10的数组，这正是我们从第744页的输出数组维度方程中预期的。该输出数组表示模型在使用步幅为2的情况下，对输入图像28 × 28区域的每个位置的预测。每个数字都有一个预测。例如，如果p是[图13-7](ch13.xhtml#ch13fig7)左侧图像的预测方法的4D输出，那么p[0,77,88,:]将返回一个包含10个元素的向量，表示图像28
    × 28输入区域（映射到77 × 88）的每个数字类别的类概率。在这种情况下，我们得到以下结果：'
- en: array([0.10930195, 0.12363277, 0.131005  , 0.10506018, 0.05257199,
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: array([0.10930195, 0.12363277, 0.131005  , 0.10506018, 0.05257199,
- en: 0.07958104, 0.0947836 , 0.11399861, 0.08733559, 0.10272926],
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 0.07958104, 0.0947836 , 0.11399861, 0.08733559, 0.10272926],
- en: dtype=float32)
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: dtype=float32)
- en: This tells us that there is no strong likelihood, according to the model, that
    any particular digit is present at this location. We know this because all the
    output probabilities are much lower than even the minimum cutoff threshold of
    0.5\. The output of predict can be thought of as a probability map, typically
    called a *heatmap*, giving us the probability that there is a digit in that location.
    The model output can be thought of as 10 heatmaps, one for each digit.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，根据模型的判断，在该位置没有强烈的可能性显示任何特定的数字。我们知道这一点，因为所有的输出概率都远低于0.5的最小阈值。predict的输出可以视为一个概率图，通常称为*热力图*，它给出了该位置是否有数字的概率。模型输出可以看作是10个热力图，每个数字一个热力图。
- en: The remaining images of [Figure 13-7](ch13.xhtml#ch13fig7) show the heatmaps
    for each of the 10 digits, again inverted so that higher probabilities are darker.
    The heatmaps were thresholded at 0.98, meaning any probability value less than
    0.98 was set to 0\. This removes the weak outputs like the ones we just saw. We
    are interested only in the model’s strongest responses per digit. To make the
    heat maps, we double the size of the output from the model and set the output
    image locations with an offset to account for the position of the convolutional
    output. This is akin to what we saw in [Figure 12-1](ch12.xhtml#ch12fig1), where
    the convolution operation returns an output that is smaller than the input when
    no zero padding is used. Specifically, the code that produces the digit heatmaps
    is in [Listing 13-10](ch13.xhtml#ch13lis10).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13-7](ch13.xhtml#ch13fig7)中的剩余图像显示了每个数字的热力图，再次进行了反转，使得较高的概率值显示为较暗。热力图在0.98的阈值下被处理，这意味着任何低于0.98的概率值都被设置为0。这去除了像我们刚才看到的那些较弱的输出。我们只关心模型在每个数字上的最强响应。为了制作热力图，我们将模型输出的大小加倍，并设置输出图像的位置，利用偏移量来考虑卷积输出的位置。这类似于我们在[图12-1](ch12.xhtml#ch12fig1)中看到的情况，其中卷积操作返回的输出比输入要小，当没有使用零填充时。具体来说，生成数字热力图的代码位于[列表13-10](ch13.xhtml#ch13lis10)中。'
- en: import os
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import sys
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: import numpy as np
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from PIL import Image
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: ❶ threshold = float(sys.argv[1])
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ threshold = float(sys.argv[1])
- en: iname = sys.argv[2]
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: iname = sys.argv[2]
- en: rname = sys.argv[3]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: rname = sys.argv[3]
- en: outdir= sys.argv[4]
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: outdir= sys.argv[4]
- en: os.system("rm -rf %s; mkdir %s" % (outdir, outdir))
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf %s; mkdir %s" % (outdir, outdir))
- en: ❷ img = Image.open(iname)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ img = Image.open(iname)
- en: c,r = img.size
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: c,r = img.size
- en: hmap = np.zeros((r,c,10))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: hmap = np.zeros((r,c,10))
- en: res = np.load(rname)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: res = np.load(rname)
- en: x,y,_ = res.shape
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: x,y,_ = res.shape
- en: xoff = (r - 2*x) // 2
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: xoff = (r - 2*x) // 2
- en: yoff = (c - 2*y) // 2
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: yoff = (c - 2*y) // 2
- en: '❸ for j in range(10):'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ for j in range(10):'
- en: h = np.array(Image.fromarray(res[:,:,j]).resize((2*y,2*x)))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: h = np.array(Image.fromarray(res[:,:,j]).resize((2*y,2*x)))
- en: hmap[xoff:(xoff+x*2), yoff:(yoff+y*2),j] = h
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: hmap[xoff:(xoff+x*2), yoff:(yoff+y*2),j] = h
- en: np.save("%s/graymaps.npy" % outdir, hmap)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("%s/graymaps.npy" % outdir, hmap)
- en: ❹ hmap[np.where(hmap < threshold)] = 0.0
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ hmap[np.where(hmap < threshold)] = 0.0
- en: 'for j in range(10):'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 'for j in range(10):'
- en: img = np.zeros((r,c), dtype="uint8")
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: img = np.zeros((r,c), dtype="uint8")
- en: 'for x in range(r):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 'for x in range(r):'
- en: 'for y in range(c):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 'for y in range(c):'
- en: ❺ img[x,y] = int(255.0*hmap[x,y,j])
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ img[x,y] = int(255.0*hmap[x,y,j])
- en: img = 255-img
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: img = 255-img
- en: Image.fromarray(img).save("%s/graymap_digit_%d.png" % (outdir, j))
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Image.fromarray(img).save("%s/graymap_digit_%d.png" % (outdir, j))
- en: '*Listing 13-10: Building heatmap images*'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表13-10：构建热力图图像*'
- en: Here we’re calling the output images *graymaps* because they’re grayscale images
    representing the response of the model to different locations in the input image.
    We first pass in the threshold value, the source image name, the response of the
    model to that source image, and an output directory where the graymaps will be
    written ❶. This directory is overwritten each time. Next, the source image is
    loaded to get its dimensions ❷. These are used to create the output heatmaps (hmap).
    We also load the associated model responses (res) and calculate the offsets. Note
    that hmap is the same size as the image. We then fill in each digit graymap of
    hmap with the resized model response ❸ and store the full set of graymaps in the
    output directory.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将输出图像称为*graymaps*，因为它们是灰度图像，表示模型对输入图像不同位置的响应。我们首先传入阈值、源图像名称、模型对该源图像的响应以及灰度图像将被写入的输出目录❶。该目录每次都会被覆盖。接下来，加载源图像以获取其尺寸❷。这些尺寸用于创建输出热力图（hmap）。我们还加载相关的模型响应（res）并计算偏移量。请注意，hmap与图像大小相同。然后，我们将每个数字的灰度图（hmap）填充为调整大小后的模型响应❸，并将完整的灰度图集存储在输出目录中。
- en: To make the output grayscale images like those shown in [Figure 13-7](ch13.xhtml#ch13fig7),
    we first threshold the heatmaps, setting any value less than the supplied cutoff
    to 0 ❹. Then, for each digit, we create an output image and simply scale the remaining
    heatmap values by 255 since they are probabilities in the range [0,1) ❺. Then,
    before writing the image to disk, we invert by subtracting from 255\. This makes
    stronger activations dark and weaker activations lighter. Because of the strong
    threshold applied (0.98), our output graymaps are effectively binary; this is
    what we want to indicate where the model is most certain of a digit being located.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成像[图13-7](ch13.xhtml#ch13fig7)中所示的灰度输出图像，我们首先对热力图进行阈值处理，将任何小于指定截止值的数值设置为0
    ❹。然后，对于每个数字，我们创建一个输出图像，并简单地通过255缩放剩余的热力图值，因为它们是[0,1)范围内的概率值 ❺。接下来，在将图像写入磁盘之前，我们通过从255中减去这些值来反转颜色。这使得较强的激活显示为较暗的颜色，而较弱的激活则为较亮的颜色。由于施加了强阈值（0.98），我们的输出灰度图实际上是二进制的；这正是我们想要的，用以表示模型最确定数字所在的位置。
- en: Let’s look back at [Figure 13-7](ch13.xhtml#ch13fig7) and see if we can interpret
    these responses. The source image has one 0 on the lower right. If we look at
    the graymap for digit 0, we see a single dark blob in that location. This means
    the model has indicated a strong response that there is a 0 digit at that location.
    So far, so good. However, we also see another strong response from the model near
    the 4 on the left side of the input image. The model has made a mistake. The input
    has two 4s in it. If we look at the graymaps for digit 4, we see two dark blobs
    corresponding to these digits, but we also see many other small areas of strong
    activation near other digits that are not 4s. The model we trained was over 99
    percent accurate on single MNIST test digits, so why are the responses of the
    fully convolutional model so noisy? Just look at all the small strong responses
    for 2s when the input does not contain any 2s. Sometimes, the model is doing well,
    as for 8s where the graymap shows strong responses for all the 8s in the input,
    but then it does poorly for other digits like 7s. And, there are no 5s at all,
    but the model is returning many hits.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回头看看[图13-7](ch13.xhtml#ch13fig7)，看看我们能否解读这些反应。源图像的右下角有一个0。如果我们查看数字0的灰度图，我们会看到这个位置有一个深色的斑点。这意味着模型已经强烈地表示在这个位置有一个0数字。到目前为止，一切顺利。然而，我们也看到模型在输入图像左侧靠近4的地方有另一个强烈反应。模型犯了一个错误。输入图像中有两个4。如果我们查看数字4的灰度图，我们会看到对应这两个数字的两个深色斑点，但我们也看到许多其他小的强激活区域，出现在其他不是4的数字附近。我们训练的模型在单个MNIST测试数字上的准确率超过了99%，那么为什么完全卷积模型的反应如此嘈杂呢？只要看看所有关于数字2的小的强反应，当输入中并没有包含任何2时。有时候，模型表现得很好，比如在数字8上，灰度图显示了输入中所有8的强烈反应，但对于其他数字，如7，它表现得很差。而且，完全没有5，但模型却返回了许多匹配的结果。
- en: Here is an opportunity for us to expand our thinking and intuition. We trained
    the model on the standard MNIST digit dataset. All of the digits in this dataset
    are well centered in the images. However, when the model is convolved over the
    large input image, there will be many times when the input to the model is not
    a well-centered digit but only part of a digit. The model has never seen partial
    digits, and since it must give an answer, it sometimes offers answers that are
    meaningless—the part of a digit it sees may be part of a 6, for example, but the
    model “thinks” it is a 5.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个让我们扩展思维和直觉的机会。我们在标准的MNIST数字数据集上训练了模型。这个数据集中的所有数字都在图像中很好地居中。然而，当模型对大输入图像进行卷积时，很多时候输入给模型的并不是一个居中的完整数字，而只是一个数字的一部分。模型从未见过部分数字，并且因为它必须给出答案，所以它有时会给出无意义的答案——它看到的数字部分可能是6的一部分，例如，但模型却“认为”它是5。
- en: One possible solution is to teach the model about partial MNIST digits. We can
    do this by augmenting the standard MNIST dataset with shifted versions of its
    digits. Imagine a 4 shifted to the lower right so that only part of it is visible.
    It will still be labeled a 4, so the model will have an opportunity to learn what
    a shifted 4 digit looks like. The code to make this shifted dataset is in the
    file *make_shifted_mnist_dataset.py*, but we’ll show only the function that makes
    a shifted copy of an input MNIST digit here. This function is called four times
    for each training and test image (to create a shifted test dataset). We keep the
    original centered digit and four randomly shifted copies of it to make a dataset
    that is five times as large as the original. The random shift function is.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解决方案是教模型识别部分的 MNIST 数字。我们可以通过将标准 MNIST 数据集与其数字的平移版本进行增强来实现这一点。假设数字 4 被平移到右下方，只露出部分。它仍然会被标记为
    4，这样模型就有机会学习到平移后的 4 的样子。创建这个平移数据集的代码在 *make_shifted_mnist_dataset.py* 文件中，但我们这里只展示创建平移版
    MNIST 数字副本的函数。该函数会被每个训练和测试图像调用四次（以创建一个平移后的测试数据集）。我们保留原始居中的数字和它的四个随机平移副本，从而生成一个比原始数据集大五倍的数据集。随机平移的函数是。
- en: 'def shifted(im):'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 'def shifted(im):'
- en: r,c = im.shape
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: r,c = im.shape
- en: x = random.randint(-r//4, r//4)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: x = random.randint(-r//4, r//4)
- en: y = random.randint(-c//4, c//4)
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: y = random.randint(-c//4, c//4)
- en: img = np.zeros((2*r,2*c), dtype="uint8")
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: img = np.zeros((2*r,2*c), dtype="uint8")
- en: xoff = r//2 + x
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: xoff = r//2 + x
- en: yoff = c//2 + y
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: yoff = c//2 + y
- en: img[xoff:(xoff+r), yoff:(yoff+c)] = im
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: img[xoff:(xoff+r), yoff:(yoff+c)] = im
- en: img = img[r//2:(r//2+r),c//2:(c//2+c)]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: img = img[r//2:(r//2+r),c//2:(c//2+c)]
- en: return img
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: return img
- en: with im being the input image supplied as a NumPy array. Based on the input
    size, we pick random x and y shifts that can be positive or negative and up to
    one-quarter of the image size. Varying this limit to, say, one-third or one-half
    would be worth the experiment. A new image is created (img), which is twice the
    size of the original. The original is then put into the larger image at an offset
    based on the shift positions, and the center portion of the larger image, matching
    the input image dimensions, is returned as the offset version of the input.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 im 是作为 NumPy 数组提供的输入图像。根据输入大小，我们选择随机的 x 和 y 平移，平移量可以是正的或负的，最大为图像大小的四分之一。将这一限制调整为三分之一或二分之一也是值得实验的。创建一个新图像（img），其大小是原图的两倍。然后根据平移位置将原图放入大图中，并返回大图中与输入图像尺寸匹配的中心部分，作为输入图像的平移版本。
- en: To use the augmented dataset, we need first to retrain the fully connected MNIST
    model, then rebuild the fully convolutional model using the new weights and biases,
    and, finally, run the large test images through the model as before. Doing all
    of this leads to new graymaps ([Figure 13-8](ch13.xhtml#ch13fig8)).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用增强的数据集，我们首先需要重新训练全连接的 MNIST 模型，然后使用新的权重和偏置重建全卷积模型，最后像以前一样将大尺寸测试图像输入模型。完成这些操作后，会得到新的灰度图（[图
    13-8](ch13.xhtml#ch13fig8)）。
- en: '![image](Images/13fig08.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig08.jpg)'
- en: '*Figure 13-8: Per digit heatmap output of the fully convolutional model for
    the upper-left input image. The model was trained on the MNIST dataset augmented
    by shifting the digits.*'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13-8：全卷积模型对左上角输入图像的每个数字热图输出。该模型在通过平移数字增强的 MNIST 数据集上训练。*'
- en: 'We see a vast improvement, so we have impressive evidence that our intuition
    was correct: the initial model was unable to deal effectively with partial digits,
    but when we trained it with partial digits included, the resulting responses were
    robust over the actual digits and very weak to nonexistent for other digits. We
    really should not be surprised by these results. Our first model did not represent
    the space of inputs that the model would see when used in the wild. It knew nothing
    about partial MNIST digits. The second model was trained on a dataset that is
    a better representation of the space of possible inputs, so it performs significantly
    better.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到显著的改进，因此有了令人印象深刻的证据，证明我们的直觉是正确的：最初的模型无法有效处理部分数字，但当我们用包含部分数字的数据进行训练时，最终的响应对实际数字非常稳健，对其他数字则几乎没有反应。我们不应该对这些结果感到惊讶。我们的第一个模型没有代表模型在实际应用中会遇到的输入空间。它对部分
    MNIST 数字一无所知。第二个模型是在一个更能代表可能输入空间的数据集上进行训练的，因此表现明显更好。
- en: Using fully convolutional networks in this way has been superseded in recent
    years by other, more advanced techniques that we do not have space nor computing
    power to work with in this book. Many models that localize objects in images output
    not a heatmap, but a bounding box covering the image. For example, the YOLO model
    (*[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)*) is
    capable of real-time object detection in images, and it uses bounding boxes around
    the objects with their label. In [Chapter 12](ch12.xhtml#ch12), we mentioned semantic
    segmentation and U-Nets as current state-of-the-art models that assign a class
    label to each pixel of the input. Both of these approaches are useful and are,
    in a sense, extensions of the fully convolutional model approach we just demonstrated
    here.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，使用全卷积网络的方法已被其他更先进的技术取代，而这些技术在本书中由于篇幅和计算能力的限制无法讨论。许多定位图像中物体的模型输出的不是热图，而是覆盖图像的边界框。例如，YOLO
    模型（*[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)*)
    能够实时进行物体检测，并且使用包围物体的边界框及其标签。在[第 12 章](ch12.xhtml#ch12)中，我们提到了语义分割和 U-Net，这些是当前最先进的模型，它们为输入的每个像素分配一个类别标签。这两种方法都非常有用，并且在某种意义上是我们在这里展示的全卷积模型方法的扩展。
- en: Scrambled MNIST Digits
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打乱的 MNIST 数字
- en: In [Chapter 10](ch10.xhtml#ch10), we showed that scrambling the order of the
    pixels in an MNIST digit ([Figure 7-3](ch07.xhtml#ch7fig3)), provided the remapping
    of pixels is deterministically applied to each image, poses no problem for a traditional
    neural network. It is still able to train well and assign class labels just as
    effectively as with unscrambled digits. See [Figure 10-9](ch10.xhtml#ch10fig9).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 10 章](ch10.xhtml#ch10)中，我们展示了打乱 MNIST 数字中像素顺序（[图 7-3](ch07.xhtml#ch7fig3)）时，只要像素的重映射在每张图像上确定性地应用，传统神经网络并不会受到影响。它仍然能够很好地训练并像处理未打乱的数字一样有效地分配类别标签。见[图
    10-9](ch10.xhtml#ch10fig9)。
- en: Let’s see if this still holds with CNNs. We made the scrambled MNIST digit dataset
    in [Chapter 5](ch05.xhtml#ch05). All we need to do here is substitute it for the
    standard MNIST dataset in our baseline CNN model, the one we started the chapter
    with. If we train this model with the scrambled data, and do so repeatedly to
    get some error bars, we get [Figure 13-9](ch13.xhtml#ch13fig9).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 CNN 是否仍然适用。我们在[第 5 章](ch05.xhtml#ch05)中制作了打乱的 MNIST 数字数据集。我们要做的就是将其替换为我们基准
    CNN 模型中的标准 MNIST 数据集，就是我们在本章开始时使用的那个。如果我们用打乱的数据训练这个模型，并重复训练以获得一些误差条形图，我们会得到[图
    13-9](ch13.xhtml#ch13fig9)。
- en: '![image](Images/13fig09.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/13fig09.jpg)'
- en: '*Figure 13-9: Test set error per epoch for a model trained on unscrambled and
    scrambled MNIST digits. Mean and SE over six training sessions.*'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13-9：每个时期的测试集误差，模型分别在未打乱和打乱的 MNIST 数字上训练。六次训练会话的均值和标准误差。*'
- en: 'Here we see that unlike the traditional neural network, the CNN does have some
    trouble: the test error for the scrambled digits is higher than for the unscrambled
    digits. Why? Recall, the CNN uses convolution and learns kernels that help create
    a new representation of the input, one that a simple model, the top layers, can
    readily use to distinguish between classes.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们看到，与传统的神经网络不同，CNN 确实存在一些问题：打乱顺序的数字测试误差高于未打乱顺序的数字。为什么？回想一下，CNN 使用卷积并学习帮助创建输入新表示的卷积核，这些表示能让简单的模型（即顶部层）更容易区分不同的类别。
- en: Convolution generates responses that are spatially dependent. In the case of
    the scrambled digits, that spatial dependence is mostly eliminated; it is only
    by considering the digit image as a whole, like a traditional neural network,
    that a class determination can be made. This means there is little for the lower
    layers of the CNN to learn. Of course, the CNN is still learning and does a better
    job with the scrambled digits than the traditional model in the end, about 2 percent
    error versus 4.4 percent, but the distinction between scrambled and unscrambled
    is more significant.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积生成的响应是空间依赖的。对于打乱顺序的数字，这种空间依赖性大部分被消除；只有将数字图像作为整体来考虑，像传统神经网络那样，才能做出类别判定。这意味着
    CNN 的低层几乎没有学习的东西。当然，CNN 仍然在学习，并且最终在处理打乱的数字时，比传统模型做得更好，误差大约是 2% 对比 4.4%，但是打乱与未打乱的区别更加显著。
- en: Summary
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we built our intuition around CNNs by working with the MNIST
    dataset. We explored the effect of basic architecture changes; learned about the
    interplay among training set size, minibatch size, and number of training epochs;
    and explored the effect of the optimization algorithm.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们通过使用MNIST数据集构建了对CNN的直觉。我们探讨了基本架构变化的影响；了解了训练集大小、小批量大小和训练周期数之间的相互关系；并探索了优化算法的效果。
- en: We saw how to convert a model using fully connected layers into a fully convolutional
    model. We then learned how to apply that model to search for digits in arbitrarily
    sized input images. We also learned that we needed to increase the expressiveness
    of our dataset to do a better job of representing the distribution of inputs the
    model sees when used.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何将一个使用全连接层的模型转换为一个全卷积模型。接着，我们学习了如何将该模型应用于在任意大小的输入图像中搜索数字。我们还了解到，为了更好地表示模型在使用时所看到的输入分布，我们需要增加数据集的表现力。
- en: Finally, we saw via an experiment with the scrambled MNIST digits that the strength
    of CNNs—their ability to learn spatial relationships within data—can sometimes
    be of little help when the spatial relationships are weak or nonexistent.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过一个关于打乱MNIST数字的实验，我们看到了CNN的优势——它们能够学习数据中的空间关系——有时在空间关系较弱或不存在时，仍然可能无法发挥多大作用。
- en: 'In the next chapter, we’ll continue our exploration of basic CNNs with a new
    dataset, one of actual images: CIFAR-10.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续探索基本的卷积神经网络（CNN），使用一个新的数据集，它由实际图像组成：CIFAR-10。
