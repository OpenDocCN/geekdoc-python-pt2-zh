- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Natural Language Processing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: Finding ways to mathematically analyze textual data is the main goal of the
    field known as *natural language processing (NLP)*. In this chapter, we’ll go
    over some important ideas from the world of NLP and talk about how to use NLP
    tools in data science projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 找到数学化分析文本数据的方法是*自然语言处理（NLP）*领域的主要目标。在本章中，我们将回顾一些NLP世界中的重要思想，并讨论如何在数据科学项目中使用NLP工具。
- en: We’ll start the chapter by introducing a business scenario and thinking through
    how NLP can help with it. We’ll use the word2vec model, which can convert individual
    words to numbers in a way that enables all kinds of powerful analyses. We’ll walk
    through the Python code for this conversion and then explore some applications
    of it. Next, we’ll discuss the Universal Sentence Encoder (USE), a tool that can
    convert entire sentences to numeric vectors. We’ll go over the Python code for
    setting up and using the USE. Along the way, we’ll find ways to use ideas from
    previous chapters. Let’s begin!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍一个商业场景开始，并思考NLP如何能够帮助解决这个问题。我们将使用word2vec模型，它可以将单个单词转换为数字，从而使得进行各种强大的分析成为可能。我们将展示这个转换的Python代码，然后探索它的一些应用。接下来，我们将讨论通用句子编码器（USE），这是一种可以将整个句子转换为数字向量的工具。我们将回顾设置和使用USE的Python代码。在这个过程中，我们将找到使用前几章中的思想的方法。让我们开始吧！
- en: Using NLP to Detect Plagiarism
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NLP检测抄袭
- en: Suppose that you’re the president of a literary agency. Your agency receives
    hundreds of emails every day, each containing book chapters from aspiring authors.
    Chapters can be quite long, consisting of thousands or tens of thousands of words
    each, and your agency needs to carefully sift through these long chapters, trying
    to find a small number to accept. The longer it takes agents to filter through
    these submitted emails, the less time they’ll have to spend on their other important
    tasks, like selling books to publishers. It’s difficult, but possible, to automate
    some of the filtering that literary agencies have to do. For example, you could
    write a Python script that could automatically detect plagiarism.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一个文学代理机构的负责人。你的机构每天收到数百封电子邮件，每封邮件都包含来自有抱负的作者的书籍章节。这些章节可能相当长，每篇包含数千到数万字，而你的机构需要仔细筛选这些长篇章，试图找到一些值得接受的章节。代理人筛选这些提交的邮件所花费的时间越长，他们就越没有时间去处理其他重要任务，比如向出版社推销书籍。虽然困难，但文学代理机构确实有可能自动化其中的一些筛选工作。例如，你可以编写一个Python脚本，自动检测抄袭。
- en: Literary agencies are not the only businesses that could be interested in plagiarism
    detection. Suppose that you’re the president of a large university. Every year,
    your students submit thousands of long papers, which you want to make sure are
    not plagiarized. Plagiarism is not only a moral and educational matter but also
    a business concern. If your university gains a reputation for allowing plagiarism,
    graduates will face worse job prospects, alumni donations will go down, fewer
    students will want to enroll, and the revenue and profits your university makes
    will surely take a nosedive. Your university’s professors and graders are overworked
    already, so you want to save them time and find an automated approach to plagiarism
    detection.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文学代理机构并不是唯一可能对抄袭检测感兴趣的企业。假设你是一个大型大学的校长。每年，你的学生会提交成千上万篇长论文，你希望确保这些论文没有抄袭。抄袭不仅是一个道德和教育问题，也是一个商业问题。如果你的大学因容忍抄袭而声名狼藉，毕业生将面临更差的就业前景，校友捐赠将下降，越来越少的学生愿意报名，大学的收入和利润无疑会急剧下降。你的大学教授和评分人员已经非常辛苦，因此你希望节省他们的时间，寻找一种自动化的抄袭检测方法。
- en: 'A simple plagiarism detector might look for exact matches of text. For example,
    one of your university’s students may have submitted the following sentences in
    one of their papers:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的抄袭检测器可能会寻找完全匹配的文本。例如，你所在大学的某个学生可能在他们的论文中提交了以下几句话：
- en: People’s whole lives do pass in front of their eyes before they die. The process
    is called “Living.”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人们的一生确实会在死前在眼前一闪而过。这个过程被称为“生活”。
- en: Maybe you read this paper and the idea sounds familiar to you, so you ask your
    librarians to search for this text in their book databases. They find an exact
    match of every character of this sentence in Terry Pratchett’s classic *The Last
    Continent*, indicating plagiarism; the student is punished accordingly.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你读了这篇论文，感觉这个想法很熟悉，于是你请图书管理员在他们的图书数据库中搜索这段文字。他们找到了 Terry Pratchett 的经典作品 *The
    Last Continent* 中每个字符的精确匹配，表明了抄袭行为；学生因此受到惩罚。
- en: 'Other students may be more wily. Instead of directly copying text from published
    books, they learn to paraphrase so they can copy ideas with minor, insignificant
    changes to phrasing. For example, one student may wish to plagiarize the following
    text (also by Pratchett):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其他学生可能更狡猾。他们不是直接从出版书籍中抄袭文本，而是学会了改写，这样他们可以通过对措辞做一些微小、不重要的改变来复制想法。例如，有一个学生可能想要抄袭以下文字（同样来自
    Pratchett）：
- en: The trouble with having an open mind, of course, is that people will insist
    on coming along and trying to put things in it.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 拥有开放心态的麻烦，当然在于，人们会坚持过来并试图把东西放进你的脑袋里。
- en: 'The student rephrases the sentence slightly, changing it to the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这位学生稍微改写了句子，变成了如下所示：
- en: The problem with having an open mind is that people will insist on approaching
    and trying to insert things into your mind.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 拥有开放心态的问题在于，人们会坚持不懈地试图将东西插入你的脑海。
- en: If your librarians do a search for exact matches of this student’s sentence,
    they won’t find any, since the student rephrased the sentence slightly. To catch
    clever plagiarists like this one, you will need to rely on NLP tools that can
    detect not only exact text matches but also “loose” or “fuzzy” matches based on
    the meanings of similar words and sentences. For example, we’ll need a method
    that can identify that *trouble* and *problem* are similar words used roughly
    as synonyms in the student’s paraphrase. By identifying synonyms and near-synonyms,
    we’ll be able determine which non-identical sentences are similar enough to each
    other to constitute evidence for plagiarism. We’ll use an NLP model called word2vec
    to accomplish this.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的图书管理员搜索这个学生句子的精确匹配，他们是找不到任何结果的，因为学生稍微改写了句子。为了抓住像这样的聪明抄袭者，你需要依赖能检测到不仅仅是精确文本匹配，还能基于相似词语和句子的意义检测到“松散”或“模糊”匹配的
    NLP 工具。例如，我们需要一种方法来识别 *trouble* 和 *problem* 是在学生改写中作为近义词使用的相似词。通过识别同义词和近义词，我们就能确定哪些不完全相同的句子彼此足够相似，可以作为抄袭证据。我们将使用一个名为
    word2vec 的 NLP 模型来完成这一任务。
- en: Understanding the word2vec NLP Model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 word2vec 自然语言处理模型
- en: 'We need a method that can take any two words and quantify exactly how similar
    they are. Let’s think about what it means for two words to be similar. Consider
    the words *sword* and *knife*. The alphabetic letters in these words are totally
    different, with no overlaps, but the words refer to things that are similar to
    each other: both are words for sharp, metallic objects used to cut things. These
    words are not exact synonyms, but their meanings are fairly similar. We humans
    have a lifetime of experience that has given us an intuitive sense for how similar
    these words are, but our computer programs can’t rely on intuition, so we have
    to find a way to quantify the similarity of these words based on data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方法，能够精确量化任意两个词语之间的相似性。让我们思考一下，两个词语相似意味着什么。考虑 *sword* 和 *knife* 这两个词。它们的字母完全不同，没有重叠，但这两个词所指的事物是相似的：它们都是用来切东西的锋利金属物体。这两个词不是完全的同义词，但它们的意义非常相似。我们人类拥有一生的经验，赋予我们直观的感觉来判断这两个词的相似性，但我们的计算机程序不能依赖直觉，所以我们必须找到一种方法，基于数据来量化这些词语的相似性。
- en: We’ll use data that comes from a large collection of natural language text,
    also called a *corpus*. A corpus may be a collection of books, newspaper articles,
    research papers, theatrical plays, or blog posts or a mix of these. The important
    point is that it consists of *natural language*—phrases and sentences that were
    put together by humans and reflect the way humans speak and write. Once we have
    our natural language corpus, we can look at how to use it to quantify the meanings
    of words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自大量自然语言文本的数据，这些文本也被称为 *语料库*。语料库可以是书籍、报纸文章、研究论文、戏剧或博客文章的集合，或者这些的混合。重要的一点是，它由
    *自然语言* 组成——由人类组合的短语和句子，反映了人类的说话和写作方式。一旦我们拥有了自然语言语料库，我们就可以研究如何利用它来量化词语的意义。
- en: Quantifying Similarities Between Words
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化词语之间的相似性
- en: 'Let’s start by looking at some natural language sentences and thinking about
    the words in them. Imagine two possible sentences that might contain the words
    *sword* and *knife*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从看一些自然语言句子开始，思考其中的单词。想象一下可能包含*剑*和*刀*的两个句子：
- en: Westley attacked me with a sword and cut my skin.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 韦斯利用剑攻击我，割破了我的皮肤。
- en: Westley attacked me with a knife and cut my skin.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 韦斯利用刀攻击我，割破了我的皮肤。
- en: 'You can see that these sentences are identical, except for the detail of whether
    the attacker used a sword or a knife. With one word substituted for the other,
    they still have rather similar meanings. This is one indication that the words
    *sword* and *knife* are similar: they can be substituted for each other in many
    sentences without drastically changing the sentence’s meaning or implications.
    Of course, it’s possible that something other than a sword or knife could be used
    in an attack, so a sentence like the following could also be in the corpus:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这些句子几乎是相同的，除了细节上攻击者使用了剑还是刀。将一个词替换为另一个词后，它们的意思依然相似。这是一个表明*剑*和*刀*相似的迹象：它们可以在许多句子中互相替换，而不会大幅改变句子的意思或含义。当然，也有可能使用其他物品进行攻击，因此像下面这样的句子也可能出现在语料库中：
- en: Westley attacked me with a herring and cut my skin.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 韦斯利用鲱鱼攻击我，割破了我的皮肤。
- en: Though a sentence about a skin-puncturing attack with a herring is technically
    possible, it’s less likely to appear in any natural language corpus than a sentence
    about a sword or knife attack. Someone who doesn’t know any English, or a Python
    script, could find evidence for this by looking at our corpus and noticing that
    the word *attack* frequently appears near the word *sword*, but doesn’t frequently
    appear near the word *herring*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于用鲱鱼进行皮肤刺穿攻击的句子从技术上讲是可能的，但它在任何自然语言语料库中出现的可能性远低于关于剑或刀攻击的句子。一个不懂英语的人，或者一个Python脚本，能够通过查看我们的语料库并注意到*攻击*这个词经常出现在*剑*这个词附近，而不常出现在*鲱鱼*这个词附近，来找到这一证据。
- en: Noticing which words tend to appear near which other words will be very useful
    to us, because we can use a word’s neighbors to better understand the word itself.
    Take a look at [Table 10-1](#table10-1), which shows words that often appear near
    the words *sword*, *knife*, and *herring.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意哪些词汇经常出现在其他词汇附近将对我们非常有用，因为我们可以利用一个词的邻居更好地理解这个词。看看[表 10-1](#table10-1)，它展示了经常出现在*剑*、*刀*和*鲱鱼*附近的词汇。
- en: 'Table 10-1: Words That Tend to Appear Near Each Other in Natural Language'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1：自然语言中经常出现在彼此附近的词汇
- en: '| **Word** | **Words that often appear nearby in a natural language corpus**
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **词汇** | **自然语言语料库中常常出现在附近的词汇** |'
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| sword | cut, attack, sheath, fight, sharp, steel |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 剑 | 切割，攻击，鞘，战斗，锋利，钢铁 |'
- en: '| knife | cut, attack, pie, fight, sharp, steel |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 刀 | 切割，攻击，馅饼，战斗，锋利，钢铁 |'
- en: '| herring | pickled, ocean, fillet, pie, silver, cut |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 鲱鱼 | 腌制的，海洋，鱼片，馅饼，银色，切割 |'
- en: Swords and knives both tend to be *sharp*, made of *steel*, used to *attack*,
    and used to *cut* things and to *fight*, so we see in [Table 10-1](#table10-1)
    that all of these words often appear near both *sword* and *knife* in a natural
    language corpus. However, we can also see differences between the lists of nearby
    words. For example, *sword* appears often near *sheath*, but *knife* doesn’t often
    appear near *sheath*. Also, *knife* often appears near *pie*, but *sword* usually
    doesn’t. For its part, *herring* appears near *pie* sometimes (since people sometimes
    eat herring pie) and also appears near *cut* sometimes (since people sometimes
    cut herring when preparing meals). But the other words that tend to appear near
    *herring* have no overlap with the words that tend to appear near *sword* and
    *knife*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 剑和刀都通常是*锋利的*，由*钢铁*制成，用来*攻击*，用来*切割*东西和*战斗*，因此我们可以在[表 10-1](#table10-1)中看到，这些词汇经常出现在*剑*和*刀*附近。然而，我们也可以看到这些邻近词汇列表之间的不同。例如，*剑*经常出现在*鞘*附近，而*刀*则不常出现在*鞘*附近。而且，*刀*经常出现在*馅饼*附近，但*剑*通常不会。至于*鲱鱼*，它有时出现在*馅饼*附近（因为人们有时吃鲱鱼馅饼），也有时出现在*切割*附近（因为人们有时在准备食物时切割鲱鱼）。但是，*鲱鱼*附近的其他词汇与*剑*和*刀*附近的词汇没有重叠。
- en: '[Table 10-1](#table10-1) is useful because we can use it to understand and
    express the similarity of two words, using data rather than gut reactions. We
    can say that *sword* and *knife* are similar, not just because we have a gut feeling
    that they mean similar things, but because they tend to appear near the same neighbors
    in natural language texts. By contrast, *sword* and *herring* are quite different,
    because little overlap exists between their common neighbors in natural language
    texts. [Table 10-1](#table10-1) gives us a data-centric way to determine whether
    words are similar, rather than a way based on vague intuition, and importantly,
    [Table 10-1](#table10-1) can be created and interpreted even by someone who doesn’t
    know a single word of English, since even a non-English speaker can look at a
    text and find which words tend to be neighbors. The table can also be created
    by a Python script that reads any corpus and finds common neighbors.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#table10-1) 很有用，因为我们可以通过它来理解和表达两个单词的相似性，使用数据而不是直觉反应。我们可以说 *剑* 和 *刀*
    是相似的，不仅仅是因为我们凭直觉觉得它们的意思相近，而是因为它们通常会出现在相同的邻近词附近。相比之下，*剑* 和 *鲱鱼* 的相似性就比较小，因为它们在自然语言文本中的常见邻近词几乎没有重叠。[表
    10-1](#table10-1) 给我们提供了一种基于数据的方法来判断单词是否相似，而不是基于模糊的直觉，而且，重要的是，即使是一个不会英语的人也能创建和解读[表
    10-1](#table10-1)，因为即使是不懂英语的人也可以查看文本并找出哪些单词常常是邻近词。这个表也可以通过一个读取任何语料库并找到共同邻近词的 Python
    脚本来生成。'
- en: Our goal is to convert words to numbers, so our next step is to create a version
    of [Table 10-1](#table10-1) that has numeric measurements of how likely words
    are to be each other’s neighbors, as in [Table 10-2](#table10-2).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将单词转化为数字，因此我们的下一步是创建一个[表 10-1](#table10-1)的版本，该版本包含单词作为彼此邻近词的概率数值，如[表
    10-2](#table10-2)所示。
- en: 'Table 10-2: Probabilities of Words Appearing Near Each Other in a Natural Language
    Corpus'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-2：单词在自然语言语料库中共同出现的概率
- en: '| **Word** | **Neighbor word** | **Probability that the neighbor appears near
    the word in a natural language corpus** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **单词** | **邻近词** | **邻近词在自然语言语料库中与该单词共同出现的概率** |'
- en: '| --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| sword | cut | 61% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 剑 | 切割 | 61% |'
- en: '| knife | cut | 69% |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 刀 | 切割 | 69% |'
- en: '| herring | cut | 12% |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 鲱鱼 | 切割 | 12% |'
- en: '| sword | pie | 1% |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 剑 | 派 | 1% |'
- en: '| knife | pie | 49% |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 刀 | 派 | 49% |'
- en: '| herring | pie | 16% |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 鲱鱼 | 派 | 16% |'
- en: '| sword | sheath | 56% |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 剑 | 鞘 | 56% |'
- en: '| knife | sheath | 16% |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 刀 | 鞘 | 16% |'
- en: '| herring | sheath | 2% |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 鲱鱼 | 鞘 | 2% |'
- en: '[Table 10-2](#table10-2) gives us much of the same information as [Table 10-1](#table10-1);
    it shows which words are likely to appear near other words in a natural language
    corpus. But [Table 10-2](#table10-2) is more precise; it gives us numeric measurements
    of the likelihood of words appearing together, instead of just a list of neighbor
    words. Again, you can see that the percentages in [Table 10-2](#table10-2) seem
    plausible: *sword* and *knife* frequently have *cut* as a neighbor, *knife* and
    *herring* are more likely than *sword* to have *pie* as a neighbor, and *herring*
    doesn’t often have *sheath* as a neighbor. Again, [Table 10-2](#table10-2) could
    be created by someone who doesn’t speak English, and it could also be created
    by a Python script that had a collection of books or English language texts to
    analyze. Similarly, even someone who doesn’t know a word of English, or a Python
    script, could look at [Table 10-2](#table10-2) and have a good idea about the
    similarities and differences between various words.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-2](#table10-2)提供的信息与[表 10-1](#table10-1)大致相同；它展示了哪些单词有可能出现在自然语言语料库中的其他单词附近。但是[表
    10-2](#table10-2)更为精确；它给出了单词共同出现的概率数值，而不仅仅是邻近词的列表。再次可以看到，[表 10-2](#table10-2)中的百分比似乎是合理的：*剑*
    和 *刀* 常常以 *切割* 作为邻近词，*刀* 和 *鲱鱼* 比 *剑* 更有可能以 *派* 作为邻近词，而 *鲱鱼* 很少以 *鞘* 作为邻近词。同样，[表
    10-2](#table10-2)可以由一个不会英语的人创建，也可以由一个有着书籍或英文文本的 Python 脚本生成。同样，甚至是不懂英语的人，或者一个
    Python 脚本，也能查看[表 10-2](#table10-2)并清楚了解不同单词之间的相似性与差异性。'
- en: Creating a System of Equations
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建方程组
- en: We’re almost ready to represent words purely as numbers. The next step is to
    create something even more numeric than [Table 10-2](#table10-2). Instead of representing
    these percentage likelihoods in a table, let’s try to represent them in a system
    of equations. We will need only a few equations to succinctly represent all the
    information in [Table 10-2](#table10-2).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好将单词完全表示为数字了。下一步是创建一个比[表10-2](#table10-2)更具数字化的东西。我们不再使用表格表示这些百分比的可能性，而是尝试将它们表示为一个方程组。我们只需要几个方程就能简洁地表示[表10-2](#table10-2)中的所有信息。
- en: 'Let’s start with a fact from arithmetic. This arithmetic fact may seem useless,
    but you’ll see later why it’s useful:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个算术事实开始。这个算术事实可能看起来毫无用处，但你稍后会看到它为什么有用：
- en: 61 = 5 · 10 – 5 · 1 + 3 · 5 + 1 · 1
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 61 = 5 · 10 – 5 · 1 + 3 · 5 + 1 · 1
- en: 'You can see that this is an equation for the number 61—that’s exactly the probability
    that the word *cut* appears near the word *sword* according to [Table 10-2](#table10-2).
    We can also rewrite the right side of the equation by using different notation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这是数字61的一个方程——这正是根据[表10-2](#table10-2)，单词*cut*出现在单词*sword*附近的概率。我们还可以通过使用不同的符号来重写方程的右边：
- en: 61 = (5, –5, 3, 1) · (10, 1, 5, 1)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 61 = (5, –5, 3, 1) · (10, 1, 5, 1)
- en: 'Here, the dot is meant to represent the dot product, which we introduced in
    Chapter 9. When calculating the dot product, we multiply the first elements of
    both vectors together, multiply the second elements of both vectors together,
    and so on, summing up the results. We can write out this dot product as a more
    standard equation using only multiplication and addition as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，点（·）代表的是点积，这是我们在第9章引入的概念。在计算点积时，我们将两个向量的第一个元素相乘，将第二个元素相乘，依此类推，并将结果相加。我们可以使用乘法和加法来写出这个点积的更标准的方程式，形式如下：
- en: 61 = 5 · 10 + (–5) · 1 + 3 · 5 + 1 · 1
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 61 = 5 · 10 + (–5) · 1 + 3 · 5 + 1 · 1
- en: 'You can see that this is just the same equation we started with. The 5 and
    10 are multiplied together, since they’re the first element of the first and second
    vectors, respectively. The numbers –5 and 1 are also multiplied together, because
    they’re the second elements of the first and second vectors, respectively. When
    we take a dot product, we multiply all of these corresponding elements together
    and sum up the results. Let’s write one more fact of arithmetic in this same dot
    product style:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这与我们开始时的方程完全相同。5和10相乘，因为它们分别是第一个和第二个向量的第一个元素。数字–5和1也相乘，因为它们分别是第一个和第二个向量的第二个元素。当我们进行点积时，我们将所有这些对应的元素相乘并加总结果。让我们用这种点积风格再写一个算术事实：
- en: 12 = (5, –5, 3, 1) · (2, 2, 2, 6)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 12 = (5, –5, 3, 1) · (2, 2, 2, 6)
- en: 'This is just another fact of arithmetic, using dot product notation. But notice,
    this is an equation for 12—exactly the probability that the word *cut* appears
    near the word *herring* according to [Table 10-2](#table10-2). We can also notice
    that the first vector in the equation, (5,–5, 3, 1), is exactly the same as the
    first vector in the previous equation. Now that we have both of these arithmetic
    facts, we can rewrite them yet again as a simple system of equations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是另一个算术事实，使用点积表示法。但请注意，这是一个关于12的方程——正是根据[表10-2](#table10-2)，单词*cut*出现在单词*herring*附近的概率。我们还可以注意到，方程中的第一个向量(5,
    –5, 3, 1)与前一个方程中的第一个向量完全相同。现在，我们有了这两个算术事实，可以再次将它们重写为一个简单的方程组：
- en: sword = (10, 1, 5, 1)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: sword = (10, 1, 5, 1)
- en: herring = (2, 2 ,2, 6)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: herring = (2, 2 ,2, 6)
- en: Probability that *cut* appears near a word = (5, –5, 3, 1) · the word’s vector
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*cut*出现在某个单词附近的概率 = (5, –5, 3, 1) · 该单词的向量'
- en: 'Here, we’ve taken a leap: instead of just writing down arithmetic facts, we’re
    claiming that we have numeric vectors that represent the words *sword* and *herring*,
    and we’re claiming that we can use these vectors to calculate the probability
    that the word *cut* is near any word. This may seem like a bold leap, but soon
    you’re going to see why it’s justified. For now, we can keep going and write more
    arithmetic facts as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们迈出了重要的一步：我们不仅仅写下算术事实，而是声称我们拥有代表单词*sword*和*herring*的数字向量，并且我们声称可以使用这些向量来计算单词*cut*与任何单词靠近的概率。也许这看起来是一个大胆的假设，但很快你会看到它为何是合理的。现在，我们可以继续，并写下更多的算术事实，如下所示：
- en: 60 = (5, –5, 3, 1) · (10, 1, 5, 9)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 60 = (5, –5, 3, 1) · (10, 1, 5, 9)
- en: 1 = (1, –10, –1, 6) · (10, 1, 5, 1)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 1 = (1, –10, –1, 6) · (10, 1, 5, 1)
- en: 49 = (1, –10, –1, 6) · (2, 2, 2, 6)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 49 = (1, –10, –1, 6) · (2, 2, 2, 6)
- en: 16 = (1, –10, –1, 6) · (10, 1, 5, 9)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 16 = (1, –10, –1, 6) · (10, 1, 5, 9)
- en: 56 = (1, 6, 9, –5) · (10, 1, 5, 1)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 56 = (1, 6, 9, –5) · (10, 1, 5, 1)
- en: 16 = (1, 6, 9, –5) · (2, 2, 2, 6)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 16 = (1, 6, 9, –5) · (2, 2, 2, 6)
- en: 2 = (1, 6, 9, –5) · (10, 1, 5, 9)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2 = (1, 6, 9, –5) · (10, 1, 5, 9)
- en: You can look at these as just arbitrary arithmetic facts. But we can also connect
    them to [Table 10-2](#table10-2). In fact, we can rewrite all of our arithmetic
    facts so far as the system shown in [Equation 10-1](#equation10-1).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把这些看作是任意的算术事实。但我们也可以将它们与[表10-2](#table10-2)连接起来。事实上，我们可以将目前所有的算术事实重写成[方程10-1](#equation10-1)所示的系统。
- en: sword = (10, 1, 5, 1)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 剑 = (10, 1, 5, 1)
- en: knife = (10, 1 , 5, 9)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 刀 = (10, 1, 5, 9)
- en: herring = (2, 2, 2, 6)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 鲱鱼 = (2, 2, 2, 6)
- en: Probability that *cut* appears near a word = (5, –5, 3, 1) · the word’s vector
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*切割*出现在某个单词附近的概率 = (5, –5, 3, 1) · 该单词的向量'
- en: Probability that *pie* appears near a word = (1, –10, –1, 6) · the word’s vector
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*派*出现在某个单词附近的概率 = (1, –10, –1, 6) · 该单词的向量'
- en: Probability that *sheath* appears near a word = (1, 6, 9, –5) · the word’s vector
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*鞘*出现在某个单词附近的概率 = (1, 6, 9, –5) · 该单词的向量'
- en: 'Equation 10-1: A system of equations containing vector representations of words'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 方程10-1：包含单词向量表示的方程组
- en: 'Mathematically, you can verify that all the equations in [Equation 10-1](#equation10-1)
    are correct: by plugging the word vectors into the equations, we’re able to calculate
    all the probabilities in [Table 10-2](#table10-2). You may wonder why we created
    this system of equations. It seems to be doing nothing more than repeating the
    probabilities that we already have in [Table 10-2](#table10-2), but in a more
    complicated way with more vectors. The important leap we’ve taken here is that
    by creating these vectors and this system of equations instead of using [Table
    10-2](#table10-2), we’ve found numeric representations of each of our words. The
    vector (10, 1, 5, 1) in some sense “captures the meaning” of *sword*, and the
    same goes for (10, 1, 5, 9) and *knife* and (2, 2, 2, 6) and *herring*.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，你可以验证[方程10-1](#equation10-1)中的所有方程都是正确的：通过将单词向量代入方程，我们可以计算出[表10-2](#table10-2)中的所有概率。你可能会想知道我们为什么创建了这个方程组。它似乎不过是在以更复杂的方式用更多的向量重复我们在[表10-2](#table10-2)中已经拥有的概率而已。我们在这里迈出的重要一步是，通过创建这些向量和这个方程组，而不是直接使用[表10-2](#table10-2)，我们找到了每个单词的数值表示。向量
    (10, 1, 5, 1) 在某种意义上“捕捉了*剑*的含义”，同样，(10, 1, 5, 9) 捕捉了*刀*的含义，(2, 2, 2, 6) 捕捉了*鲱鱼*的含义。
- en: 'Even though we have vectors for each of our words, you may not feel convinced
    that these vectors really represent the meanings of English words. To help convince
    you, let’s do some simple calculations with our vectors and see what we can learn.
    First, let’s define these vectors in a Python session:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经有了每个单词的向量，但你可能并不完全相信这些向量真的能代表英语单词的含义。为了帮助你更有信心，我们可以用这些向量做一些简单的计算，看看能学到什么。首先，让我们在
    Python 会话中定义这些向量：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we define each of our word vectors as a Python list, a standard way to
    work with vectors in Python. We’re interested in knowing how similar our words
    are to each other, so let’s define a function that can calculate the distance
    between any two vectors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将每个单词向量定义为 Python 列表，这是在 Python 中处理向量的标准方式。我们关注的是了解我们的单词之间有多相似，因此让我们定义一个函数来计算任意两个向量之间的距离：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This function is called `euclidean()`, because it’s technically calculating
    a Euclidean distance between any two vectors. In two dimensions, a *Euclidean
    distance* is the length of the hypotenuse of a right triangle, which we can calculate
    with the Pythagorean theorem. More informally, we often refer to the Euclidean
    distance as just *distance*. In more than two dimensions, we use the same Pythagorean
    theorem formula to calculate a Euclidean distance, and the only difference is
    that it’s harder to draw. Calculating Euclidean distances between vectors is a
    reasonable way to calculate the similarity of two vectors: the closer the Euclidean
    distance between the vectors, the more similar they are. Let’s calculate the Euclidean
    distances between our word vectors:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数叫做`euclidean()`，因为它本质上是在计算任意两个向量之间的欧几里得距离。在二维空间中，*欧几里得距离*是直角三角形的斜边长度，我们可以用毕达哥拉斯定理来计算。更非正式地，我们通常将欧几里得距离称作*距离*。在超过二维的空间中，我们使用相同的毕达哥拉斯定理公式来计算欧几里得距离，唯一的区别是它更难画出来。计算向量之间的欧几里得距离是一种合理的方式来计算两个向量的相似度：欧几里得距离越小，向量之间的相似度越高。让我们计算一下这些单词向量之间的欧几里得距离：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see that *sword* and *knife* have a distance of 8 from each other.
    By contrast, *sword* and *herring* have a distance of 9.9 from each other. These
    distance measurements reflect our understandings of these words: *sword* and *knife*
    are similar to each other, so their vectors are close to each other, while *sword*
    and *herring* are less similar to each other, so their vectors are further apart.
    This is evidence that our method for converting words to numeric vectors has worked:
    it’s enabling us to quantify word similarities successfully.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到 *sword* 和 *knife* 之间的距离是 8。而 *sword* 和 *herring* 之间的距离则是 9.9。这些距离测量反映了我们对这些单词的理解：*sword*
    和 *knife* 彼此相似，因此它们的向量很接近，而 *sword* 和 *herring* 彼此不太相似，所以它们的向量相距较远。这证明了我们将单词转换为数值向量的方法是有效的：它让我们成功地量化了单词之间的相似度。
- en: If we want to detect plagiarism, we’ll have to find numeric vectors that represent
    more than just these three words. We’ll want to find vectors for every word in
    the English language, or at least the majority of the words that tend to appear
    in student papers. We can imagine what some of these vectors might be. For example,
    the word *haddock* refers to a type of fish, not too different from a herring.
    So, we’ll expect to find that *haddock* has similar neighbors to *herring* and
    similar probabilities in [Table 10-2](#table10-2) (a similar probability of having
    *cut* or *pie* or anything else as a neighbor).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要检测抄袭，我们需要找到能够代表的不仅仅是这三个词的数值向量。我们需要为英语语言中的每个单词，或者至少为那些经常出现在学生论文中的大多数单词找到向量。我们可以想象这些向量可能是什么。例如，*haddock*
    是一种鱼类，跟鲱鱼差不多。因此，我们会预期 *haddock* 的邻近词与 *herring* 相似，并且在 [表 10-2](#table10-2) 中的概率也会相似（具有类似的邻居概率，像是
    *cut* 或 *pie* 或其他任何词）。
- en: Anytime two words have similar probabilities in [Table 10-2](#table10-2), we
    expect that they’ll have similar vectors, since we’ll be multiplying those vectors
    according to the system of equations in [Equation 10-1](#equation10-1) to get
    those probabilities. For example, we might find that the numeric vector for *haddock*
    is something like (2.1, 1.9, 2.3, 6.5). This vector will be close in Euclidean
    distance to the vector for *herring* (2, 2, 2, 6), and if we multiply the *haddock*
    vector by the other vectors in [Equation 10-1](#equation10-1), we’ll find that
    *haddock* should have probabilities of being near each neighbor word that are
    similar to the probabilities for *herring* in [Table 10-2](#table10-2). We’ll
    similarly need to find vectors for thousands of other words in English, and we’ll
    expect that words that have similar meanings should have similar vectors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每当两个单词在 [表 10-2](#table10-2) 中有相似的概率时，我们预期它们会有相似的向量，因为我们会根据 [方程 10-1](#equation10-1)
    中的方程系统，将这些向量相乘，以得到这些概率。例如，我们可能会发现 *haddock* 的数值向量类似于 (2.1, 1.9, 2.3, 6.5)。这个向量在欧几里得距离上将接近
    *herring* 的向量 (2, 2, 2, 6)，如果我们将 *haddock* 向量与 [方程 10-1](#equation10-1) 中的其他向量相乘，我们会发现
    *haddock* 与每个邻近词的概率应该与 *herring* 在 [表 10-2](#table10-2) 中的概率相似。同样，我们还需要为英语中成千上万的其他单词找到向量，我们预期具有相似含义的单词应该有相似的向量。
- en: 'It’s easy to say that we need vectors for every English word, but then the
    question becomes: How should we determine each of these vectors? To understand
    how we can determine the vectors for every word, consider a diagram of the system
    of equations in [Figure 10-1](#figure10-1).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 说我们需要为每个英语单词找到向量很容易，但接下来问题就来了：我们应该如何确定这些向量呢？要理解如何为每个单词确定向量，看看 [图 10-1](#figure10-1)
    中的方程系统示意图。
- en: '![](image_fi/502888c10/f10001.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c10/f10001.png)'
- en: 'Figure 10-1: A visual representation of the probabilities of words appearing
    near each other'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-1：单词彼此接近的概率的可视化表示
- en: 'This diagram looks complex, but it’s meant to illustrate nothing more and nothing
    less than our system of equations. In [Equation 10-1](#equation10-1), we represent
    every word as a vector with four elements, like this: (*a*, *b*, *c*, *d*). The
    *a*, *b*, *c*, and *d* on the left of [Figure 10-1](#figure10-1) represent these
    elements. Each arrow extending from those elements represents a multiplication.
    For example, the arrow marked with a 5 that extends from the circle labeled *a*
    to the oval labeled *probability of cut appearing nearby* means that we should
    multiply every *a* value by 5, and add it to the estimated probability of *cut*
    appearing near a word. If we consider all the multiplications indicated by all
    of these arrows, you can see from [Figure 10-1](#figure10-1) that the probability
    of *cut* appearing near a word is 5 · *a* – 5 · *b* + 3 · *c* + 1 · *d*, just
    as described in [Equation 10-1](#equation10-1). [Figure 10-1](#figure10-1) is
    just another way to represent [Equation 10-1](#equation10-1). If we can find the
    right *a*, *b*, *c*, and *d* values for every word in English, we’ll have the
    word vectors we need to check for plagiarism.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图看起来很复杂，但它的目的是为了说明我们的方程组。 在[方程式 10-1](#equation10-1)中，我们将每个单词表示为一个四个元素的向量，像这样：（*a*，*b*，*c*，*d*）。在[图
    10-1](#figure10-1)左侧的 *a*、*b*、*c* 和 *d* 表示这些元素。从这些元素延伸出的每个箭头表示乘法。例如，从标记为 *a* 的圆圈到标记为
    *切割概率附近出现* 的椭圆的箭头上标有 5，表示我们应该将每个 *a* 的值乘以 5，并加到估算的 *cut* 在一个单词附近出现的概率中。如果我们考虑所有这些箭头所表示的乘法，你可以从[图
    10-1](#figure10-1)中看到，*cut* 在一个单词附近出现的概率是 5 · *a* – 5 · *b* + 3 · *c* + 1 · *d*，这正如[方程式
    10-1](#equation10-1)中描述的那样。[图 10-1](#figure10-1) 只是另一种表示[方程式 10-1](#equation10-1)的方式。如果我们能够找到每个英语单词的正确
    *a*、*b*、*c* 和 *d* 值，我们就能得到用于检查抄袭所需的单词向量。
- en: The reason we drew [Figure 10-1](#figure10-1) is to point out that it has exactly
    the form of a neural network, the type of supervised learning model we already
    discussed in Chapter 6. Since it constitutes a neural network, we can use advanced
    software (including several free Python packages) to train the neural network
    and find out exactly what *a*, *b*, *c*, and *d* should be for every word in English.
    The whole point of creating Tables 10-1 and 10-2, and the system of equations
    in [Equation 10-1](#equation10-1), is to create a neural network like the hypothetical
    one shown in [Figure 10-1](#figure10-1). As long as we have data like that in
    [Table 10-2](#table10-2), we can use this neural network software to train the
    neural network shown in [Figure 10-1](#figure10-1) and find all the word vectors
    we need.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们画[图 10-1](#figure10-1)的原因是要指出，它恰好呈现出一个神经网络的形式，这是我们在第六章已经讨论过的有监督学习模型的类型。由于它构成了一个神经网络，我们可以使用先进的软件（包括几个免费的
    Python 包）来训练这个神经网络，找出每个英语单词的 *a*、*b*、*c* 和 *d* 应该是多少。创建[表格 10-1](#table10-1)和[表格
    10-2](#table10-2)，以及[方程式 10-1](#equation10-1)中的方程组的全部目的，是为了创建一个像[图 10-1](#figure10-1)所示的假设性神经网络。只要我们拥有像[表格
    10-2](#table10-2)中那样的数据，我们就可以使用这个神经网络软件来训练[图 10-1](#figure10-1)中所示的神经网络，并找到我们需要的所有单词向量。
- en: 'The most important output of this neural network training will be values of
    *a*, *b*, *c*, and *d* for every word in our data. In other words, the output
    of the neural network training will be (*a*, *b*, *c*, *d*) vectors for every
    word. This process is called the *word2vec* model: create a table of probabilities
    like [Table 10-2](#table10-2) for every word in a corpus, use that table to set
    up a neural network like the one shown in [Figure 10-1](#figure10-1), and then
    train that neural network to find numeric vectors that represent every word.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络训练的最重要输出将是我们数据中每个单词的 *a*、*b*、*c* 和 *d* 值。换句话说，神经网络训练的输出将是每个单词的 (*a*，*b*，*c*，*d*)
    向量。这个过程被称为 *word2vec* 模型：为语料库中的每个单词创建像[表格 10-2](#table10-2)那样的概率表，使用该表建立一个像[图
    10-1](#figure10-1)所示的神经网络，然后训练该神经网络来找到表示每个单词的数字向量。
- en: The word2vec model is popular because it can create numeric vectors for any
    words, and those vectors can be used for many useful applications. One reason
    that word2vec is popular is that we can train a word2vec model by using only raw
    text as input; we don’t need to annotate or label any words before training the
    model and getting our word vectors. So, even someone who doesn’t speak English
    can create word vectors and reason about them by using the word2vec approach.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型之所以流行，是因为它可以为任何单词创建数字向量，这些向量可以用于许多有用的应用程序。word2vec流行的一个原因是，我们可以仅使用原始文本作为输入来训练word2vec模型；在训练模型并获取词向量之前，我们不需要对任何单词进行注释或标记。因此，即使是一个不懂英语的人，也可以通过使用word2vec方法创建词向量并对其进行推理。
- en: 'If this sounds complex, don’t worry. Next, we’ll go through code for working
    with these kinds of vectors, and you’ll see that although the ideas and theory
    of word2vec are complex, the code and applications can be straightforward and
    simple. For now, try to feel comfortable with the basic overall idea of what we’ve
    discussed so far: if we create data about which words appear near each other in
    natural language, we can use that data to create vectors that allow us to quantify
    the similarity of any pair of words. Let’s continue with some code, where we can
    see how to use these numeric vectors for detecting plagiarism.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来很复杂，别担心。接下来，我们将通过代码来处理这些类型的向量，你会看到，尽管word2vec的理论和概念很复杂，但代码和应用可以是直接且简单的。目前，尽量让自己对我们到目前为止讨论的基本概念感到舒适：如果我们创建了关于哪些单词在自然语言中彼此接近的数据，我们就可以利用这些数据创建向量，从而量化任何一对单词的相似性。接下来，让我们继续用一些代码，看看如何使用这些数字向量来检测抄袭。
- en: Analyzing Numeric Vectors in word2vec
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析word2vec中的数字向量
- en: 'Not only has someone already created the word2vec model, but they’ve also already
    done all its hard work for us: written the code, calculated the vectors, and published
    all of it online for us to download anytime for free. In Python, we can use the
    Gensim package to access word vectors for many English words:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅有人已经创建了word2vec模型，而且他们还为我们做了所有的繁重工作：编写代码、计算向量，并将所有这些内容发布到网上，供我们随时免费下载。在Python中，我们可以使用Gensim包来访问许多英语单词的词向量：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Gensim package has a `downloader` that allows us to access many NLP models
    and tools just by using the `load()` method. You can load one such collection
    of vectors in one line as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim包有一个`downloader`，允许我们仅通过使用`load()`方法来访问许多NLP模型和工具。你可以用以下一行代码加载一个词向量集合：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code loads a collection of word vectors that was created from a corpus
    of news texts containing about 100 billion words. Essentially, someone got the
    information that would be in a table like our [Table 10-2](#table10-2), but with
    thousands more words in it. They obtained these words and probabilities from real
    news sources written by humans. Then they used that information to create a neural
    network like the one in [Figure 10-1](#figure10-1)—again, with thousands more
    words in it. They trained this neural network and found vectors for every word
    in their corpus.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码加载了一个由约1000亿个单词组成的新闻语料库创建的词向量集合。本质上，某人获取了类似于我们[表10-2](#table10-2)中的信息，但其中包含更多的单词。他们从由人类编写的真实新闻来源中获取了这些单词和概率。然后，他们利用这些信息创建了一个类似于[图10-1](#figure10-1)中的神经网络——同样，其中包含更多的单词。他们训练了这个神经网络，并为语料库中的每个单词找到了对应的向量。
- en: 'We’ve just downloaded the vectors they calculated. One reason we expect these
    vectors to be useful is that the text corpus used to create these vectors was
    large and diverse, and large, diverse text data sources tend to lead to higher
    accuracy in NLP models. We can look at the vector corresponding to any word as
    follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚下载了他们计算的向量。我们预期这些向量会有用的一个原因是，用于创建这些向量的文本语料库既庞大又多样化，而庞大且多样化的文本数据源往往能提高NLP模型的准确性。我们可以按如下方式查看对应于任何单词的向量：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we print out the vector for the word *sword*. The output you’ll see is
    a vector with 512 numeric elements. Word vectors like this one, which represents
    the word *sword* according to the model we downloaded, are also called *embeddings*,
    because we’ve successfully *embedded* a word in a *vector space*. In short, we’ve
    converted a word to a vector of numbers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打印出单词*sword*的向量。你将看到的输出是一个包含512个数字元素的向量。像这样的词向量，表示我们下载的模型中的单词*sword*，也被称为*嵌入向量*，因为我们已经成功地将一个单词*嵌入*到*向量空间*中。简而言之，我们已经将一个单词转换成了一个数字向量。
- en: 'You’ll notice that this 512-element vector for *sword* is not the same as the
    vector we used for *sword* previously in the chapter, which was (10, 1, 5, 1).
    This vector is different from our vector for a few reasons. First, this vector
    uses a different corpus than the one we used, so it will have different probabilities
    listed in its version of [Table 10-2](#table10-2). Second, the creators of this
    model decided to find vectors with 512 elements instead of 4 elements as we did,
    so they get more vector elements. Third, their vectors were adjusted to have values
    close to 0, while ours were not. Every corpus and every neural network will lead
    to slightly different results, but if we’re using a good corpus and a good neural
    network, we expect the same qualitative result: vectors that represent words and
    enable us to detect plagiarism (and do many other things).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，*sword*的这个512维向量与我们在本章之前使用的*sword*的向量（10, 1, 5, 1）不同。这个向量与我们之前的向量不同有几个原因。首先，这个向量使用了与我们不同的语料库，因此在它的[表10-2](#table10-2)版本中会列出不同的概率。其次，这个模型的创建者决定使用512维的向量，而不是我们使用的4维向量，因此它们得到了更多的向量元素。第三，他们的向量调整为接近0的值，而我们的向量没有。每个语料库和每个神经网络都会导致略有不同的结果，但如果我们使用的是一个好的语料库和一个好的神经网络，我们期望得到相同的定性结果：这些向量代表了单词，并使我们能够检测抄袭（以及执行许多其他任务）。
- en: 'After downloading these vectors, we can work with them just like any other
    Python object. For example, we can calculate distances between our word vectors,
    as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下载这些向量后，我们可以像处理任何其他Python对象一样使用它们。例如，我们可以计算单词向量之间的距离，方法如下：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we’re doing the same Euclidean distance calculations that we did before,
    but instead of calculating distances with the vectors in [Equation 10-1](#equation10-1),
    we’re doing calculations with the vectors we downloaded. When you run these comparisons,
    you’ll see the following outputs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们进行了与之前相同的欧几里得距离计算，但这次我们使用的是我们下载的向量，而不是[方程10-1](#equation10-1)中的向量。当你运行这些比较时，你会看到以下输出：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see that these distances make sense: the vector for *sword* is similar
    to the vector for *knife* (they have distance about 3.28 from each other), but
    it’s different from the vector for *herring* (they have distance about 4.94 from
    each other, much larger than the difference between *sword* and *knife*). You
    can try the same calculation for any other pairs words that are in the corpus,
    like *car* and *van* as well. You can compare differences between pairs of words
    to find out which pairs have the most and the least similar meanings.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这些距离是合理的：*sword*的向量与*knife*的向量相似（它们之间的距离约为3.28），但与*herring*的向量不同（它们之间的距离约为4.94，比*sword*和*knife*之间的差异要大得多）。你也可以对语料库中的其他词对（如*car*和*van*）进行相同的计算。你可以比较词对之间的差异，找出哪些词对的意思最相似或最不相似。
- en: 'Euclidean distance is not the only distance metric people use to compare word
    vectors. Using cosine similarity measurements is also common, just as we did in
    Chapter 9. Remember that we used this code to calculate cosine similarities:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离并不是唯一用于比较单词向量的距离度量。使用余弦相似度度量也是常见的，就像我们在第9章中做的那样。记住，我们用这段代码来计算余弦相似度：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here we define a function called `cosine_similarity()` to check the cosine
    of the angle between any two vectors. We can check some cosine similarities between
    vectors as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们定义了一个名为`cosine_similarity()`的函数，用来检查任意两个向量之间的角度余弦。我们可以如下检查一些向量之间的余弦相似度：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When you run this snippet, you’ll see the following results:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这段代码时，你会看到以下结果：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can see that these metrics are doing exactly what we want: they give us
    lower values for words that we perceive as different and higher values for words
    that we perceive as similar. Even though your laptop doesn’t “speak English,”
    just by analyzing a corpus of natural language text, it’s able to quantify exactly
    how similar and exactly how different distinct words are.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这些度量正是我们想要的：它们对我们认为不同的词给出了较低的值，而对我们认为相似的词给出了较高的值。尽管你的笔记本电脑不会“说英语”，但通过分析自然语言文本语料库，它能够量化出不同词语之间的相似度和差异度。
- en: Manipulating Vectors with Mathematical Calculations
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用数学计算操作向量
- en: 'One famous illustration of the power of word2vec comes from an analysis of
    the words *king* and *queen*. To see this illustration, let’s start by getting
    the vectors associated with some English words:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec的一个著名例子来自对*king*和*queen*这两个词的分析。为了看到这个例子，我们首先获取一些英语单词的向量：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we define some vectors associated with several words. As humans, we know
    that a king is a male head of a monarchy and a queen is a female head of a monarchy.
    We might even express the relationship between the words *king* and *queen* as
    follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义与几个单词相关的向量。作为人类，我们知道国王是君主制国家的男性领导人，而女王是女性领导人。我们甚至可能将*king*和*queen*之间的关系表示如下：
- en: king – man + woman = queen
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: king – man + woman = queen
- en: 'Starting with the idea of a *king*, and taking away from that the idea of a
    *man*, then adding to that the idea of a *woman*, we end up with the idea of a
    *queen*. If we’re thinking about just the world of words, this equation might
    seem ridiculous, since it’s typically not possible to add and subtract words or
    ideas in this way. However, remember that we have vector versions of each of these
    words, so we can add and subtract vectors from each other and see what we get.
    Let’s try to add and subtract the vectors corresponding to each of these words
    in Python:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从*king*的概念开始，去掉*man*的概念，再加上*woman*的概念，最终得到*queen*的概念。如果我们仅从单词的角度来看，这个方程可能显得荒谬，因为通常情况下无法以这种方式加减单词或概念。然而，请记住，我们拥有这些单词的向量版本，因此我们可以相互加减向量，并看看结果如何。让我们尝试在Python中对这些单词对应的向量进行加减运算：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, we take our `king` vector, subtract our `man` vector, add our `woman`
    vector, and define the result as a new variable called `newvector`. If our additions
    and subtractions do what we want them to, our `newvector` should capture a specific
    meaning: the idea of a king without any attributes of a *man*, but with the added
    attributes of a *woman*. In other words, even though the `newvector` is a sum
    of three vectors, none of which are the vector for *queen*, we expect their sum
    to be close to, or equal to, the vector for *queen*. Let’s check the difference
    between our `newvector` and our `queen` vector to see whether this is the case:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们取`king`向量，减去`man`向量，再加上`woman`向量，并将结果定义为一个新变量`newvector`。如果我们的加法和减法按预期进行，`newvector`应该捕捉到一个特定的含义：去除*man*属性后，加入*woman*属性的国王。换句话说，尽管`newvector`是三个向量的和，它们其中没有一个是*queen*的向量，但我们期望它们的和接近或等于*queen*的向量。让我们检查一下`newvector`与`queen`向量之间的差异，看是否符合预期：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can see that our `newvector` is similar to our `queen` vector: their cosine
    similarity is 0.76, and their Euclidean distance is 2.5\. We can compare this
    to the differences between other familiar pairs of words:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的`newvector`与我们的`queen`向量相似：它们的余弦相似度为0.76，欧几里得距离为2.5。我们可以将其与其他熟悉的单词对的差异进行比较：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should see that *king – man + woman* is more similar to *queen* than *fish*
    is to *herring*. Our mathematical calculations with the words’ vectors lead to
    exactly the results we expect based on what we know about the words’ linguistic
    meanings. This demonstrates the usefulness of these vectors: we can not only compare
    them to find similarities between pairs of words but also manipulate them through
    addition and subtraction to add and subtract concepts. The ability to add and
    subtract these vectors, and get results that make sense, is more evidence that
    these vectors are reliably capturing the meanings of their associated words.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到*king – man + woman*比*fish*与*herring*的相似度更接近*queen*。我们对单词向量的数学计算得到了完全符合我们对这些单词语言意义的预期结果。这证明了这些向量的有用性：我们不仅可以通过比较它们找到单词对之间的相似性，还可以通过加法和减法操作来添加和减去概念。能够对这些向量进行加减运算，并得到合乎逻辑的结果，进一步证明了这些向量可靠地捕捉了它们所关联单词的含义。
- en: Detecting Plagiarism with word2vec
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用word2vec检测抄袭
- en: 'Let’s go back to the plagiarism scenario from earlier in the chapter. Remember
    that we introduced the following two sentences as an example of plagiarism:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章早些时候的抄袭情境。记得我们介绍了以下两个句子作为抄袭的例子：
- en: The trouble with having an open mind, of course, is that people will insist
    on coming along and trying to put things in it. [original sentence]
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有开放心态的问题，当然，是人们会坚持走近并试图将东西放进去。[原句]
- en: The problem with having an open mind is that people will insist on approaching
    and trying to insert things into your mind. [plagiarized sentence]
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有开放心态的问题是，人们会坚持走近并试图将东西塞进你的脑袋。[抄袭句子]
- en: 'Since these two sentences differ in a few places, a naive plagiarism checker
    that looks for exact matches won’t detect plagiarism here. Instead of checking
    for exact matches for every character of both sentences, we want to check for
    close matches between the meanings of each word. For words that are identical,
    we’ll find that they have 0 distance between them:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这两句话在某些地方有所不同，所以一个只寻找精确匹配的简单抄袭检测工具在这里是无法检测到抄袭的。我们并不希望检查每个字符的精确匹配，而是要检查每个单词的意思是否接近。对于那些完全相同的单词，我们会发现它们之间的距离为0：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results of this are not surprising. We find that a word’s vector has perfect
    (1.0) cosine similarity with itself and a word’s vector has 0 Euclidean distance
    from itself. Every word is equal to itself. But remember, wily plagiarists are
    paraphrasing, not using the exact same words as published texts. If we compare
    words that are paraphrased, we expect that they’ll be similar to the words of
    the original, but not exactly the same. We can measure the similarity of potentially
    paraphrased words as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果并不令人惊讶。我们发现，一个单词的向量与其自身的余弦相似度为1.0，而一个单词的向量与其自身的欧几里得距离为0。每个单词都与自己相等。但请记住，狡猾的抄袭者是在进行释义，而不是使用与已发布文本完全相同的词语。如果我们比较那些经过释义的单词，我们可以预期它们会与原始单词相似，但不会完全相同。我们可以通过以下方式衡量这些潜在释义单词的相似性：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You’ll see the following results from this code snippet:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到这段代码的以下结果：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This snippet compares the words of the plagiarized text and the words of the
    original text. The results show close matches in almost every case: either a relatively
    small Euclidean distance or a relatively high cosine similarity. If the individual
    words of a student’s sentence are all close matches to the individual words of
    a published sentence, that’s good evidence of plagiarism, even if there are few
    or no exact matches. Importantly, we can check for these close matches automatically,
    not based on slow, costly human judgment, but based on data and quick Python scripts.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码比较了抄袭文本和原文中的单词。结果显示，几乎每种情况下都有接近匹配：要么是相对较小的欧几里得距离，要么是相对较高的余弦相似度。如果学生句子的单个单词与已发表句子的单个单词都能接近匹配，那么即便没有精确匹配，这也是抄袭的有力证据。重要的是，我们可以通过自动化检查这些接近匹配，而不依赖于缓慢且成本高昂的人类判断，而是依赖于数据和快速的
    Python 脚本。
- en: Checking for close matches for all the individual words in sentences is a reasonable
    way to start detecting plagiarism. However, it’s not perfect. The main problem
    is that so far, we’ve learned to evaluate only individual words instead of full
    sentences. We can think of sentences as collections or sequences of individual
    words. But in many cases, it will be better to have a technique that can evaluate
    meanings and similarities for entire sentences simultaneously, treating sentences
    as individual units instead of only as collections of words. For that, we’ll turn
    to a powerful new approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 检查句子中每个单词的接近匹配是检测抄袭的合理起点。然而，这并不完美。主要问题在于，迄今为止，我们只学会了评估单个单词，而不是完整句子。我们可以将句子看作是单个单词的集合或序列。但是，在许多情况下，最好采用一种能够同时评估整个句子的意义和相似性的技术，将句子视为独立的单元，而不仅仅是单词的集合。为此，我们将转向一种强大的新方法。
- en: Using Skip-Thoughts
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用跳跃思维
- en: The *skip-thoughts* model is an NLP model that uses data and neural networks
    to convert entire sentences to numeric vectors. It’s quite similar to word2vec,
    but instead of converting individual words one at a time to vectors, we convert
    entire sentences to vectors as units.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*跳跃思维*模型是一个自然语言处理（NLP）模型，利用数据和神经网络将整个句子转换为数值向量。它与 word2vec 非常相似，但不同之处在于，它不是一次将单个单词转换为向量，而是将整个句子作为单元转换为向量。'
- en: 'The theory behind skip-thoughts is similar to the theory behind word2vec: you
    take a natural language corpus, find which sentences tend to appear near each
    other, and train a neural network that can predict which sentences are expected
    to appear before or after any other sentence. For word2vec, we saw an illustration
    of a neural network model in [Figure 10-1](#figure10-1). For skip-thoughts, a
    similar illustration is shown in [Figure 10-2](#figure10-2).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃思维的理论与 word2vec 的理论相似：你获取一个自然语言语料库，找出哪些句子往往彼此接近，然后训练一个神经网络，能够预测哪些句子可能出现在其他句子的前面或后面。对于
    word2vec，我们在[图 10-1](#figure10-1)中看到过神经网络模型的示意图。对于跳跃思维，[图 10-2](#figure10-2)展示了类似的示意图。
- en: '![](image_fi/502888c10/f10002.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c10/f10002.png)'
- en: 'Figure 10-2: We use the skip-thoughts model to predict which sentences appear
    near each other.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-2：我们使用跳跃思维模型来预测哪些句子会彼此接近。
- en: '[Figure 10-2](#figure10-2) is based on the model from the 2015 paper titled
    “Skip-Thought Vectors” by Ryan Kiros and colleagues ([https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)).
    You can see that the sentence on the left of the figure is taken as the input.
    This sentence is a sequence of individual words, but all the words are considered
    together as a single unit. The skip-thoughts model attempts to find a vector representation
    of this sentence that, when used as an input in a neural network, can predict
    sentences that are most likely to come before and after it (including the sentences
    on the right of [Figure 10-2](#figure10-2)). Just as word2vec is based on which
    individual words are expected to be near other words, skip-thoughts is based on
    predicting the full sentences that are near other sentences. You don’t need to
    worry about the theory too much; just try to remember that skip-thoughts is a
    way to encode natural language sentences as vectors by calculating probabilities
    of other sentences appearing nearby.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-2](#figure10-2)基于Ryan Kiros及其同事在2015年的论文“Skip-Thought Vectors”中的模型（[https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)）。你可以看到图左侧的句子被作为输入。这个句子是一个单词序列，但所有单词都被视为一个单元一起考虑。skip-thoughts模型试图找到这个句子的向量表示，当用作神经网络输入时，可以预测最可能出现在它之前和之后的句子（包括[图10-2](#figure10-2)右侧的句子）。就像word2vec基于预期哪些单词接近其他单词一样，skip-thoughts基于预测哪些句子接近其他句子。你不需要太担心理论，只需记住skip-thoughts是一种通过计算附近句子出现概率来对自然语言句子进行向量编码的方法。'
- en: Just as with word2vec, writing code for any of this ourselves is not necessary.
    Instead, we’ll turn to the *Universal Sentence Encoder (USE)*. This tool converts
    sentences into vectors, using the idea of skip-thoughts to find the vectors (plus
    other advanced technical methods). We’re going to use the vector outputs of the
    USE for plagiarism detection, but USE vectors can also be used for chatbot implementations,
    image tagging, and much more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用word2vec一样，我们不需要自己编写任何代码。相反，我们将转向*Universal Sentence Encoder (USE)*。这个工具将句子转换为向量，使用skip-thoughts的思想来找到这些向量（以及其他高级技术方法）。我们将使用USE的向量输出进行抄袭检测，但USE向量也可以用于聊天机器人实现、图像标记等多种用途。
- en: 'The code for the USE is not hard to work with, because someone else has written
    it for us. We can start by defining sentences that we want to analyze:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因为有其他人已经为我们编写了USE的代码，所以使用USE的代码并不难。我们可以从定义要分析的句子开始：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here we have a list of sentences, and we want to convert each into a numeric
    vector. We can import code someone else has written to do this conversion:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有一个句子列表，我们想将每个句子转换为数值向量。我们可以导入其他人编写的代码来进行这种转换：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `tensorfow_hub` module allows us to load the USE from an online repository.
    The USE is a big model, so don’t panic if loading it into your Python session
    takes a few minutes or more. When we load it, we save it as a variable called
    `embed`. Now that we have the USE in our Python session, we can create word embeddings
    (vectors) with one simple line of code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorfow_hub`模块允许我们从在线库中加载USE。USE是一个大模型，所以如果将其加载到Python会话中需要几分钟甚至更长时间也不必惊慌。加载后，我们将其保存为名为`embed`的变量。现在USE已经在我们的Python会话中，我们可以用一行简单的代码创建单词嵌入（向量）：'
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our `embeddings` variable contains vectors that represent each of the sentences
    in our `Sentences` list. You can look at the vector for the first sentence by
    checking the first element of the `embeddings` variable as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`embeddings`变量包含代表`Sentences`列表中每个句子的向量。你可以通过检查`embeddings`变量的第一个元素来查看第一句的向量：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When you run this snippet, you’ll see a numeric vector with 512 elements, the
    first 16 of which are shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码时，你会看到一个包含512个元素的数值向量，其中前16个元素如下所示：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is the vector representation of the first sentence in your list—not the
    individual words in the sentence, but the sentence itself. Just as we did with
    our word2vec vectors, we can calculate the distance between any two of our sentence
    vectors. In this case, the distances between vectors will represent the degree
    of difference between the overall meanings of two sentences. Let’s check the distance
    between our first two sentences as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你列表中第一句的向量表示，不是句子中的单个单词，而是整个句子本身。就像我们用word2vec向量做的那样，我们可以计算任意两个句子向量之间的距离。在这种情况下，向量之间的距离将代表两个句子整体意义的差异程度。让我们检查一下第一和第二句之间的距离：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can see that the cosine similarity is about 0.85—indicating that the first
    two sentences in our `Sentences` list are quite similar to each other. This is
    evidence that the student’s sentence (the second one in our `Sentences` list)
    is plagiarized from Pratchett’s sentence (the first one in our `Sentences` list).
    By contrast, we can check distances between other vectors and see that they’re
    not quite so similar. For example, if you run `print(cosine_similarity(embeddings[0],embeddings[2]))`,
    you can see that the cosine similarity of these two sentences is about 0.02, indicating
    that these sentences are almost as different as it’s possible for two sentences
    to be. This is evidence that Pratchett didn’t plagiarize *Hamlet*. If you run
    `print(cosine_similarity(embeddings[0],embeddings[3]))`, you can see that the
    cosine similarity of these two sentences is about –0.07, another low similarity
    score, indicating that Pratchett also didn’t plagiarize *Moby Dick*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，余弦相似度大约为0.85——这表明我们`Sentences`列表中的前两个句子非常相似。这是证据，表明学生的句子（我们`Sentences`列表中的第二个句子）是从普拉特切特的句子（我们`Sentences`列表中的第一个句子）抄袭的。相反，我们可以检查其他向量之间的距离，发现它们并不像这两个句子那样相似。例如，如果你运行`print(cosine_similarity(embeddings[0],embeddings[2]))`，你会看到这两个句子的余弦相似度约为0.02，表明这两个句子几乎是完全不同的。这是证据，表明普拉特切特没有抄袭*哈姆雷特*。如果你运行`print(cosine_similarity(embeddings[0],embeddings[3]))`，你会看到这两个句子的余弦相似度约为-0.07，另一个低相似度分数，表明普拉特切特也没有抄袭*白鲸*。
- en: You can see that checking the distances between the meanings of any two sentences
    is straightforward. Your plagiarism detector can simply check for the cosine similarity
    (or Euclidean distance) between a student’s work and previously published sentences,
    and if the similarities appear to be large (or the Euclidean distances appear
    to be too small), you can take it as evidence that the student is guilty of plagiarism.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，检查任意两句子之间意义的距离是非常直接的。你的抄袭检测器可以简单地检查学生的作品和以前发布的句子之间的余弦相似度（或欧几里得距离），如果相似度很大（或欧几里得距离过小），你可以认为这是学生抄袭的证据。
- en: Topic Modeling
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题建模
- en: To finish the chapter, let’s introduce one final business scenario and talk
    about how we can combine NLP tools with tools from previous chapters to deal with
    it. In this scenario, imagine that you run a forum website. Your site has become
    so successful that you can no longer read all the threads and conversations yourself,
    but you still want to understand the topics people are writing about on your site
    so you can understand who your users are and what they care about. You want to
    find a reliable, automated way to analyze all the text on your site and discover
    the main topics being discussed. This goal, called *topic modeling*, is common
    in NLP.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这一章，让我们介绍一个最后的商业场景，讨论如何将NLP工具与前几章的工具结合使用来处理这个问题。在这个场景中，假设你经营一个论坛网站。你的网站已经取得了巨大成功，以至于你再也无法亲自阅读所有的帖子和对话，但你仍然希望了解人们在你的网站上写的主题，以便了解你的用户是谁，他们关心什么。你希望找到一种可靠的自动化方式，分析你网站上的所有文本，并发现讨论的主要话题。这个目标，称为*主题建模*，在NLP中很常见。
- en: 'To start, let’s take a collection of sentences that might have appeared on
    your site. A successful forum website could receive thousands of comments every
    second, but we’ll start by looking at a small sample of the full data, just eight
    sentences:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一组可能出现在你网站上的句子。一个成功的论坛网站可能每秒钟接收到成千上万的评论，但我们从一个小样本开始，只有八个句子：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Just as you did before, you can calculate the vectors, or embeddings, for all
    these sentences:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你之前做的那样，你可以计算所有这些句子的向量或嵌入：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we can create a matrix of all our sentence embeddings:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个包含所有句子嵌入的矩阵：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This snippet starts by creating a list called `arrays` and adding all the sentence
    vectors to the list. Next, it creates a matrix called `sentencematrix`. This matrix
    is just your sentence vectors stacked on top of each other, with one row for each
    sentence vector. Finally, we convert this matrix into a pandas dataframe, so it’s
    easier to work with. The final result, called `pandasmatrix`, has eight rows;
    each row is a sentence vector for one of our eight sentences.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先创建一个名为`arrays`的列表，并将所有句子向量添加到该列表中。接下来，它创建一个名为`sentencematrix`的矩阵。这个矩阵只是将你的句子向量堆叠在一起，每个句子向量占一行。最后，我们将这个矩阵转换成一个pandas数据框，以便更方便地操作。最终结果，名为`pandasmatrix`，包含八行；每一行都是我们八个句子的句子向量。
- en: Now we have a matrix that contains our sentence vectors. But getting our vectors
    isn’t enough; we need to decide what to do with them. Remember that our goal is
    topic modeling. We want to understand the topics that people are writing about
    and which sentences relate to which topics. We have several ways to accomplish
    this. One natural way to accomplish topic modeling is by using clustering, something
    we already discussed in Chapter 7.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含句子向量的矩阵。但仅仅获得我们的向量还不够；我们需要决定如何使用它们。记住，我们的目标是主题建模。我们希望理解人们在写什么主题，以及哪些句子与哪些主题相关。我们有几种方法可以实现这一目标。实现主题建模的一种自然方法是使用聚类，这是我们在第
    7 章中已经讨论过的内容。
- en: 'Our clustering approach is simple: we’ll take our matrix of sentence vectors
    as our input data. We’ll apply clustering to determine the natural groups that
    exist in the data. We’ll interpret the groups (clusters) that we find as the distinct
    topics being discussed on your forum website. We can do this clustering as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聚类方法很简单：我们将句子向量的矩阵作为输入数据。我们将应用聚类来确定数据中存在的自然分组。我们将解释我们找到的这些组（簇）为你论坛网站上正在讨论的不同主题。我们可以按以下方式进行聚类：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can see that the clustering requires only a few lines of code. We import
    the `KMeans` code from the sklearn module. We create a variable called `m` and
    then use its `fit()` method to find two clusters for our matrix (the `pandasmatrix`).
    The `fit()` method uses Euclidean distance measurements with our sentence vectors
    to find two clusters of documents. The two clusters it finds are what we will
    take to be the two main topics of our collection of sentences. After we find these
    clusters, we add two new columns to our `pandasmatrix`: first, we add the labels
    that are the result of our clustering (in the `topic` variable), and second, we
    add the actual sentences that we’re trying to cluster. Let’s look at the results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，聚类只需要几行代码。我们从 sklearn 模块导入了 `KMeans` 代码。我们创建了一个叫做 `m` 的变量，然后使用它的 `fit()`
    方法为我们的矩阵（`pandasmatrix`）找到两个簇。`fit()` 方法利用欧几里得距离度量和我们的句子向量来找到两个文档簇。它找到的这两个簇就是我们将视为我们的句子集合的两个主要主题。在找到这些簇之后，我们向
    `pandasmatrix` 添加了两个新列：首先，我们添加了作为聚类结果的标签（在 `topic` 变量中），其次，我们添加了我们正在尝试聚类的实际句子。让我们看看结果：
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This snippet prints out two sets of sentences: first, sentences that are labeled
    as belonging to cluster 0 (sentences on rows with the `topic` variable equal to
    0) and second, sentences that are labeled as belonging to cluster 1 (sentences
    on rows with the `topic` variable equal to 1). You should see the following results:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段输出两组句子：首先是标记为属于簇 0 的句子（`topic` 变量等于 0 的行中的句子），其次是标记为属于簇 1 的句子（`topic`
    变量等于 1 的行中的句子）。你应该看到以下结果：
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can see that this method has identified two clusters in our data, and they’ve
    been labeled as cluster 0 and cluster 1\. When you look at the sentences that
    have been classified as cluster 0, you can see that many seem to be discussions
    about food and restaurants. When you look at cluster 1, you can see that it seems
    to consist of critiques of books. At least according to this sample of eight sentences,
    these are the main topics being discussed on your forum, and they’ve been identified
    and organized automatically.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这种方法已经在我们的数据中识别出了两个簇，并将其标记为簇 0 和簇 1。当你查看被分类为簇 0 的句子时，你会发现许多似乎是在讨论食物和餐馆。而当你查看簇
    1 时，你会看到它似乎由书籍评论组成。至少根据这八个句子的样本，这些是你论坛上讨论的主要话题，并且它们已经被自动识别和组织。
- en: We’ve accomplished topic modeling, using a numeric method (clustering) on non-numeric
    data (natural language text). You can see that USE, and word embeddings in general,
    can be useful in many applications.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了主题建模，使用了一个数值方法（聚类）来处理非数值数据（自然语言文本）。你可以看到，USE 和词嵌入在许多应用中是非常有用的。
- en: Other Applications of NLP
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理的其他应用
- en: One useful application of NLP is in the world of recommendation systems. Imagine
    that you run a movie website and want to recommend movies to your users. In Chapter
    9, we discussed how to use interaction matrices to make recommendations based
    on comparisons of transaction histories. However, you could also make a recommendation
    system based on comparisons of content. For example, you could take plot summaries
    of individual movies and use the USE to get sentence embeddings for each plot
    summary. Then you could calculate distances between plot summaries to determine
    the similarity of various movies and, anytime a user watches a move, recommend
    that that user also watch movies with the most similar plots. This is a *content-based
    recommendation system*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的一个有用应用是在推荐系统中。假设你经营一个电影网站，并希望向用户推荐电影。在第9章中，我们讨论了如何使用互动矩阵基于交易历史的比较来进行推荐。然而，你也可以基于内容的比较来构建推荐系统。例如，你可以获取每部电影的情节概要，并使用USE为每个情节概要获取句子嵌入。然后，你可以计算情节概要之间的距离，确定各种电影的相似度，并在用户观看电影后，向该用户推荐情节最相似的电影。这就是*基于内容的推荐系统*。
- en: Another interesting application of NLP is *sentiment analysis*. We encountered
    sentiment analysis a little already in Chapter 6. Certain tools can determine
    whether any given sentence is positive, negative, or neutral in its tone or the
    sentiment it expresses. Some of these tools rely on word embeddings like those
    we’ve covered in this chapter, and others don’t. Sentiment analysis could be useful
    for a business that receives thousands of emails and messages every day. By running
    automatic sentiment analysis on all of its incoming emails, a business could determine
    which customers were the most happy or unhappy, and potentially prioritize responses
    based on customer sentiment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的NLP应用是*情感分析*。我们在第6章中稍微接触到过情感分析。某些工具能够判断给定句子的语气或表达的情感是积极、消极还是中立。有些工具依赖于我们在本章中介绍的词嵌入技术，而其他工具则不依赖。情感分析对于每天收到成千上万封电子邮件和消息的企业非常有用。通过对所有收到的电子邮件进行自动情感分析，企业可以判断哪些客户最为满意或不满，并可能根据客户的情感优先回应。
- en: Many businesses today deploy *chatbots* on their websites—computer programs
    that can understand text inputs and answer questions. Chatbots have varying levels
    of sophistication, but many rely on some kind of word embeddings like the word2vec
    and skip-thought methods described in this chapter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 今天许多企业在其网站上部署*聊天机器人*—一种能够理解文本输入并回答问题的计算机程序。聊天机器人的复杂性不同，但许多都依赖于一些类似于本章所述的word2vec和skip-thought方法的词嵌入技术。
- en: NLP has many other possible applications in business. Today, law firms are trying
    to use NLP to automatically analyze documents and even automatically generate
    or at least organize contracts. News websites have tried to use NLP to automatically
    generate certain kinds of formulaic articles, like recaps of sports games. There’s
    no end to the possibilities of NLP as the field itself continues to develop. If
    you know some powerful methods like word2vec and skip-thoughts as a starting point,
    there’s no limit to the useful applications you can create.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: NLP在商业中有许多其他潜在的应用。今天，律师事务所正在尝试使用NLP自动分析文档，甚至自动生成或至少整理合同。新闻网站也尝试使用NLP自动生成某些类型的格式化文章，例如体育赛事的回顾。随着NLP领域的不断发展，应用的可能性是无穷无尽的。如果你掌握了像word2vec和skip-thoughts这样强大的方法作为起点，创建有用的应用是没有限制的。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we went over natural language processing. All of the applications
    of NLP that we discussed relied on embeddings: numeric vectors that accurately
    represent words and sentences. If you can represent a word numerically, you can
    do math with it, including calculating similarities (which we did for plagiarism
    detection) and clustering (which we did for topic modeling). NLP tools might be
    difficult to use and master, but they can be astonishing in their capabilities.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了自然语言处理。我们讨论的所有NLP应用都依赖于嵌入技术：能够准确表示单词和句子的数值向量。如果你能够将一个词用数字表示，就可以对它进行数学运算，包括计算相似度（我们在抄袭检测中做过）和聚类（我们在主题建模中做过）。NLP工具可能很难使用和掌握，但它们的能力可以让人惊叹。
- en: In the next chapter, we’ll shift gears and wrap up the book by going over some
    simple ideas about working with other programming languages that are important
    for data science.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将转变方向，通过讨论一些与数据科学相关的其他编程语言的简单概念来结束本书。
