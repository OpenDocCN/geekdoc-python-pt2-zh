- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural Language Processing
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Finding ways to mathematically analyze textual data is the main goal of the
    field known as *natural language processing (NLP)*. In this chapter, we’ll go
    over some important ideas from the world of NLP and talk about how to use NLP
    tools in data science projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start the chapter by introducing a business scenario and thinking through
    how NLP can help with it. We’ll use the word2vec model, which can convert individual
    words to numbers in a way that enables all kinds of powerful analyses. We’ll walk
    through the Python code for this conversion and then explore some applications
    of it. Next, we’ll discuss the Universal Sentence Encoder (USE), a tool that can
    convert entire sentences to numeric vectors. We’ll go over the Python code for
    setting up and using the USE. Along the way, we’ll find ways to use ideas from
    previous chapters. Let’s begin!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Using NLP to Detect Plagiarism
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that you’re the president of a literary agency. Your agency receives
    hundreds of emails every day, each containing book chapters from aspiring authors.
    Chapters can be quite long, consisting of thousands or tens of thousands of words
    each, and your agency needs to carefully sift through these long chapters, trying
    to find a small number to accept. The longer it takes agents to filter through
    these submitted emails, the less time they’ll have to spend on their other important
    tasks, like selling books to publishers. It’s difficult, but possible, to automate
    some of the filtering that literary agencies have to do. For example, you could
    write a Python script that could automatically detect plagiarism.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Literary agencies are not the only businesses that could be interested in plagiarism
    detection. Suppose that you’re the president of a large university. Every year,
    your students submit thousands of long papers, which you want to make sure are
    not plagiarized. Plagiarism is not only a moral and educational matter but also
    a business concern. If your university gains a reputation for allowing plagiarism,
    graduates will face worse job prospects, alumni donations will go down, fewer
    students will want to enroll, and the revenue and profits your university makes
    will surely take a nosedive. Your university’s professors and graders are overworked
    already, so you want to save them time and find an automated approach to plagiarism
    detection.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple plagiarism detector might look for exact matches of text. For example,
    one of your university’s students may have submitted the following sentences in
    one of their papers:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: People’s whole lives do pass in front of their eyes before they die. The process
    is called “Living.”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Maybe you read this paper and the idea sounds familiar to you, so you ask your
    librarians to search for this text in their book databases. They find an exact
    match of every character of this sentence in Terry Pratchett’s classic *The Last
    Continent*, indicating plagiarism; the student is punished accordingly.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Other students may be more wily. Instead of directly copying text from published
    books, they learn to paraphrase so they can copy ideas with minor, insignificant
    changes to phrasing. For example, one student may wish to plagiarize the following
    text (also by Pratchett):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The trouble with having an open mind, of course, is that people will insist
    on coming along and trying to put things in it.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The student rephrases the sentence slightly, changing it to the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The problem with having an open mind is that people will insist on approaching
    and trying to insert things into your mind.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If your librarians do a search for exact matches of this student’s sentence,
    they won’t find any, since the student rephrased the sentence slightly. To catch
    clever plagiarists like this one, you will need to rely on NLP tools that can
    detect not only exact text matches but also “loose” or “fuzzy” matches based on
    the meanings of similar words and sentences. For example, we’ll need a method
    that can identify that *trouble* and *problem* are similar words used roughly
    as synonyms in the student’s paraphrase. By identifying synonyms and near-synonyms,
    we’ll be able determine which non-identical sentences are similar enough to each
    other to constitute evidence for plagiarism. We’ll use an NLP model called word2vec
    to accomplish this.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the word2vec NLP Model
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need a method that can take any two words and quantify exactly how similar
    they are. Let’s think about what it means for two words to be similar. Consider
    the words *sword* and *knife*. The alphabetic letters in these words are totally
    different, with no overlaps, but the words refer to things that are similar to
    each other: both are words for sharp, metallic objects used to cut things. These
    words are not exact synonyms, but their meanings are fairly similar. We humans
    have a lifetime of experience that has given us an intuitive sense for how similar
    these words are, but our computer programs can’t rely on intuition, so we have
    to find a way to quantify the similarity of these words based on data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use data that comes from a large collection of natural language text,
    also called a *corpus*. A corpus may be a collection of books, newspaper articles,
    research papers, theatrical plays, or blog posts or a mix of these. The important
    point is that it consists of *natural language*—phrases and sentences that were
    put together by humans and reflect the way humans speak and write. Once we have
    our natural language corpus, we can look at how to use it to quantify the meanings
    of words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying Similarities Between Words
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by looking at some natural language sentences and thinking about
    the words in them. Imagine two possible sentences that might contain the words
    *sword* and *knife*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Westley attacked me with a sword and cut my skin.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Westley attacked me with a knife and cut my skin.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see that these sentences are identical, except for the detail of whether
    the attacker used a sword or a knife. With one word substituted for the other,
    they still have rather similar meanings. This is one indication that the words
    *sword* and *knife* are similar: they can be substituted for each other in many
    sentences without drastically changing the sentence’s meaning or implications.
    Of course, it’s possible that something other than a sword or knife could be used
    in an attack, so a sentence like the following could also be in the corpus:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Westley attacked me with a herring and cut my skin.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Though a sentence about a skin-puncturing attack with a herring is technically
    possible, it’s less likely to appear in any natural language corpus than a sentence
    about a sword or knife attack. Someone who doesn’t know any English, or a Python
    script, could find evidence for this by looking at our corpus and noticing that
    the word *attack* frequently appears near the word *sword*, but doesn’t frequently
    appear near the word *herring*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Noticing which words tend to appear near which other words will be very useful
    to us, because we can use a word’s neighbors to better understand the word itself.
    Take a look at [Table 10-1](#table10-1), which shows words that often appear near
    the words *sword*, *knife*, and *herring.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10-1: Words That Tend to Appear Near Each Other in Natural Language'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word** | **Words that often appear nearby in a natural language corpus**
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| sword | cut, attack, sheath, fight, sharp, steel |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| knife | cut, attack, pie, fight, sharp, steel |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| herring | pickled, ocean, fillet, pie, silver, cut |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: Swords and knives both tend to be *sharp*, made of *steel*, used to *attack*,
    and used to *cut* things and to *fight*, so we see in [Table 10-1](#table10-1)
    that all of these words often appear near both *sword* and *knife* in a natural
    language corpus. However, we can also see differences between the lists of nearby
    words. For example, *sword* appears often near *sheath*, but *knife* doesn’t often
    appear near *sheath*. Also, *knife* often appears near *pie*, but *sword* usually
    doesn’t. For its part, *herring* appears near *pie* sometimes (since people sometimes
    eat herring pie) and also appears near *cut* sometimes (since people sometimes
    cut herring when preparing meals). But the other words that tend to appear near
    *herring* have no overlap with the words that tend to appear near *sword* and
    *knife*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-1](#table10-1) is useful because we can use it to understand and
    express the similarity of two words, using data rather than gut reactions. We
    can say that *sword* and *knife* are similar, not just because we have a gut feeling
    that they mean similar things, but because they tend to appear near the same neighbors
    in natural language texts. By contrast, *sword* and *herring* are quite different,
    because little overlap exists between their common neighbors in natural language
    texts. [Table 10-1](#table10-1) gives us a data-centric way to determine whether
    words are similar, rather than a way based on vague intuition, and importantly,
    [Table 10-1](#table10-1) can be created and interpreted even by someone who doesn’t
    know a single word of English, since even a non-English speaker can look at a
    text and find which words tend to be neighbors. The table can also be created
    by a Python script that reads any corpus and finds common neighbors.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to convert words to numbers, so our next step is to create a version
    of [Table 10-1](#table10-1) that has numeric measurements of how likely words
    are to be each other’s neighbors, as in [Table 10-2](#table10-2).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10-2: Probabilities of Words Appearing Near Each Other in a Natural Language
    Corpus'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word** | **Neighbor word** | **Probability that the neighbor appears near
    the word in a natural language corpus** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| sword | cut | 61% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| knife | cut | 69% |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| herring | cut | 12% |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| sword | pie | 1% |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| knife | pie | 49% |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| herring | pie | 16% |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| sword | sheath | 56% |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| knife | sheath | 16% |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| herring | sheath | 2% |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '[Table 10-2](#table10-2) gives us much of the same information as [Table 10-1](#table10-1);
    it shows which words are likely to appear near other words in a natural language
    corpus. But [Table 10-2](#table10-2) is more precise; it gives us numeric measurements
    of the likelihood of words appearing together, instead of just a list of neighbor
    words. Again, you can see that the percentages in [Table 10-2](#table10-2) seem
    plausible: *sword* and *knife* frequently have *cut* as a neighbor, *knife* and
    *herring* are more likely than *sword* to have *pie* as a neighbor, and *herring*
    doesn’t often have *sheath* as a neighbor. Again, [Table 10-2](#table10-2) could
    be created by someone who doesn’t speak English, and it could also be created
    by a Python script that had a collection of books or English language texts to
    analyze. Similarly, even someone who doesn’t know a word of English, or a Python
    script, could look at [Table 10-2](#table10-2) and have a good idea about the
    similarities and differences between various words.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Creating a System of Equations
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re almost ready to represent words purely as numbers. The next step is to
    create something even more numeric than [Table 10-2](#table10-2). Instead of representing
    these percentage likelihoods in a table, let’s try to represent them in a system
    of equations. We will need only a few equations to succinctly represent all the
    information in [Table 10-2](#table10-2).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好将单词完全表示为数字了。下一步是创建一个比[表10-2](#table10-2)更具数字化的东西。我们不再使用表格表示这些百分比的可能性，而是尝试将它们表示为一个方程组。我们只需要几个方程就能简洁地表示[表10-2](#table10-2)中的所有信息。
- en: 'Let’s start with a fact from arithmetic. This arithmetic fact may seem useless,
    but you’ll see later why it’s useful:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个算术事实开始。这个算术事实可能看起来毫无用处，但你稍后会看到它为什么有用：
- en: 61 = 5 · 10 – 5 · 1 + 3 · 5 + 1 · 1
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 61 = 5 · 10 – 5 · 1 + 3 · 5 + 1 · 1
- en: 'You can see that this is an equation for the number 61—that’s exactly the probability
    that the word *cut* appears near the word *sword* according to [Table 10-2](#table10-2).
    We can also rewrite the right side of the equation by using different notation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这是数字61的一个方程——这正是根据[表10-2](#table10-2)，单词*cut*出现在单词*sword*附近的概率。我们还可以通过使用不同的符号来重写方程的右边：
- en: 61 = (5, –5, 3, 1) · (10, 1, 5, 1)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 61 = (5, –5, 3, 1) · (10, 1, 5, 1)
- en: 'Here, the dot is meant to represent the dot product, which we introduced in
    Chapter 9. When calculating the dot product, we multiply the first elements of
    both vectors together, multiply the second elements of both vectors together,
    and so on, summing up the results. We can write out this dot product as a more
    standard equation using only multiplication and addition as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，点（·）代表的是点积，这是我们在第9章引入的概念。在计算点积时，我们将两个向量的第一个元素相乘，将第二个元素相乘，依此类推，并将结果相加。我们可以使用乘法和加法来写出这个点积的更标准的方程式，形式如下：
- en: 61 = 5 · 10 + (–5) · 1 + 3 · 5 + 1 · 1
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 61 = 5 · 10 + (–5) · 1 + 3 · 5 + 1 · 1
- en: 'You can see that this is just the same equation we started with. The 5 and
    10 are multiplied together, since they’re the first element of the first and second
    vectors, respectively. The numbers –5 and 1 are also multiplied together, because
    they’re the second elements of the first and second vectors, respectively. When
    we take a dot product, we multiply all of these corresponding elements together
    and sum up the results. Let’s write one more fact of arithmetic in this same dot
    product style:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这与我们开始时的方程完全相同。5和10相乘，因为它们分别是第一个和第二个向量的第一个元素。数字–5和1也相乘，因为它们分别是第一个和第二个向量的第二个元素。当我们进行点积时，我们将所有这些对应的元素相乘并加总结果。让我们用这种点积风格再写一个算术事实：
- en: 12 = (5, –5, 3, 1) · (2, 2, 2, 6)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 12 = (5, –5, 3, 1) · (2, 2, 2, 6)
- en: 'This is just another fact of arithmetic, using dot product notation. But notice,
    this is an equation for 12—exactly the probability that the word *cut* appears
    near the word *herring* according to [Table 10-2](#table10-2). We can also notice
    that the first vector in the equation, (5,–5, 3, 1), is exactly the same as the
    first vector in the previous equation. Now that we have both of these arithmetic
    facts, we can rewrite them yet again as a simple system of equations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是另一个算术事实，使用点积表示法。但请注意，这是一个关于12的方程——正是根据[表10-2](#table10-2)，单词*cut*出现在单词*herring*附近的概率。我们还可以注意到，方程中的第一个向量(5,
    –5, 3, 1)与前一个方程中的第一个向量完全相同。现在，我们有了这两个算术事实，可以再次将它们重写为一个简单的方程组：
- en: sword = (10, 1, 5, 1)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: sword = (10, 1, 5, 1)
- en: herring = (2, 2 ,2, 6)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: herring = (2, 2 ,2, 6)
- en: Probability that *cut* appears near a word = (5, –5, 3, 1) · the word’s vector
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*cut*出现在某个单词附近的概率 = (5, –5, 3, 1) · 该单词的向量'
- en: 'Here, we’ve taken a leap: instead of just writing down arithmetic facts, we’re
    claiming that we have numeric vectors that represent the words *sword* and *herring*,
    and we’re claiming that we can use these vectors to calculate the probability
    that the word *cut* is near any word. This may seem like a bold leap, but soon
    you’re going to see why it’s justified. For now, we can keep going and write more
    arithmetic facts as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们迈出了重要的一步：我们不仅仅写下算术事实，而是声称我们拥有代表单词*sword*和*herring*的数字向量，并且我们声称可以使用这些向量来计算单词*cut*与任何单词靠近的概率。也许这看起来是一个大胆的假设，但很快你会看到它为何是合理的。现在，我们可以继续，并写下更多的算术事实，如下所示：
- en: 60 = (5, –5, 3, 1) · (10, 1, 5, 9)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 60 = (5, –5, 3, 1) · (10, 1, 5, 9)
- en: 1 = (1, –10, –1, 6) · (10, 1, 5, 1)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 1 = (1, –10, –1, 6) · (10, 1, 5, 1)
- en: 49 = (1, –10, –1, 6) · (2, 2, 2, 6)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 49 = (1, –10, –1, 6) · (2, 2, 2, 6)
- en: 16 = (1, –10, –1, 6) · (10, 1, 5, 9)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 16 = (1, –10, –1, 6) · (10, 1, 5, 9)
- en: 56 = (1, 6, 9, –5) · (10, 1, 5, 1)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 56 = (1, 6, 9, –5) · (10, 1, 5, 1)
- en: 16 = (1, 6, 9, –5) · (2, 2, 2, 6)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 16 = (1, 6, 9, –5) · (2, 2, 2, 6)
- en: 2 = (1, 6, 9, –5) · (10, 1, 5, 9)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2 = (1, 6, 9, –5) · (10, 1, 5, 9)
- en: You can look at these as just arbitrary arithmetic facts. But we can also connect
    them to [Table 10-2](#table10-2). In fact, we can rewrite all of our arithmetic
    facts so far as the system shown in [Equation 10-1](#equation10-1).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把这些看作是任意的算术事实。但我们也可以将它们与[表10-2](#table10-2)连接起来。事实上，我们可以将目前所有的算术事实重写成[方程10-1](#equation10-1)所示的系统。
- en: sword = (10, 1, 5, 1)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 剑 = (10, 1, 5, 1)
- en: knife = (10, 1 , 5, 9)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 刀 = (10, 1, 5, 9)
- en: herring = (2, 2, 2, 6)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 鲱鱼 = (2, 2, 2, 6)
- en: Probability that *cut* appears near a word = (5, –5, 3, 1) · the word’s vector
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*切割*出现在某个单词附近的概率 = (5, –5, 3, 1) · 该单词的向量'
- en: Probability that *pie* appears near a word = (1, –10, –1, 6) · the word’s vector
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*派*出现在某个单词附近的概率 = (1, –10, –1, 6) · 该单词的向量'
- en: Probability that *sheath* appears near a word = (1, 6, 9, –5) · the word’s vector
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*鞘*出现在某个单词附近的概率 = (1, 6, 9, –5) · 该单词的向量'
- en: 'Equation 10-1: A system of equations containing vector representations of words'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 方程10-1：包含单词向量表示的方程组
- en: 'Mathematically, you can verify that all the equations in [Equation 10-1](#equation10-1)
    are correct: by plugging the word vectors into the equations, we’re able to calculate
    all the probabilities in [Table 10-2](#table10-2). You may wonder why we created
    this system of equations. It seems to be doing nothing more than repeating the
    probabilities that we already have in [Table 10-2](#table10-2), but in a more
    complicated way with more vectors. The important leap we’ve taken here is that
    by creating these vectors and this system of equations instead of using [Table
    10-2](#table10-2), we’ve found numeric representations of each of our words. The
    vector (10, 1, 5, 1) in some sense “captures the meaning” of *sword*, and the
    same goes for (10, 1, 5, 9) and *knife* and (2, 2, 2, 6) and *herring*.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，你可以验证[方程10-1](#equation10-1)中的所有方程都是正确的：通过将单词向量代入方程，我们可以计算出[表10-2](#table10-2)中的所有概率。你可能会想知道我们为什么创建了这个方程组。它似乎不过是在以更复杂的方式用更多的向量重复我们在[表10-2](#table10-2)中已经拥有的概率而已。我们在这里迈出的重要一步是，通过创建这些向量和这个方程组，而不是直接使用[表10-2](#table10-2)，我们找到了每个单词的数值表示。向量
    (10, 1, 5, 1) 在某种意义上“捕捉了*剑*的含义”，同样，(10, 1, 5, 9) 捕捉了*刀*的含义，(2, 2, 2, 6) 捕捉了*鲱鱼*的含义。
- en: 'Even though we have vectors for each of our words, you may not feel convinced
    that these vectors really represent the meanings of English words. To help convince
    you, let’s do some simple calculations with our vectors and see what we can learn.
    First, let’s define these vectors in a Python session:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经有了每个单词的向量，但你可能并不完全相信这些向量真的能代表英语单词的含义。为了帮助你更有信心，我们可以用这些向量做一些简单的计算，看看能学到什么。首先，让我们在
    Python 会话中定义这些向量：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we define each of our word vectors as a Python list, a standard way to
    work with vectors in Python. We’re interested in knowing how similar our words
    are to each other, so let’s define a function that can calculate the distance
    between any two vectors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将每个单词向量定义为 Python 列表，这是在 Python 中处理向量的标准方式。我们关注的是了解我们的单词之间有多相似，因此让我们定义一个函数来计算任意两个向量之间的距离：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This function is called `euclidean()`, because it’s technically calculating
    a Euclidean distance between any two vectors. In two dimensions, a *Euclidean
    distance* is the length of the hypotenuse of a right triangle, which we can calculate
    with the Pythagorean theorem. More informally, we often refer to the Euclidean
    distance as just *distance*. In more than two dimensions, we use the same Pythagorean
    theorem formula to calculate a Euclidean distance, and the only difference is
    that it’s harder to draw. Calculating Euclidean distances between vectors is a
    reasonable way to calculate the similarity of two vectors: the closer the Euclidean
    distance between the vectors, the more similar they are. Let’s calculate the Euclidean
    distances between our word vectors:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数叫做`euclidean()`，因为它本质上是在计算任意两个向量之间的欧几里得距离。在二维空间中，*欧几里得距离*是直角三角形的斜边长度，我们可以用毕达哥拉斯定理来计算。更非正式地，我们通常将欧几里得距离称作*距离*。在超过二维的空间中，我们使用相同的毕达哥拉斯定理公式来计算欧几里得距离，唯一的区别是它更难画出来。计算向量之间的欧几里得距离是一种合理的方式来计算两个向量的相似度：欧几里得距离越小，向量之间的相似度越高。让我们计算一下这些单词向量之间的欧几里得距离：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see that *sword* and *knife* have a distance of 8 from each other.
    By contrast, *sword* and *herring* have a distance of 9.9 from each other. These
    distance measurements reflect our understandings of these words: *sword* and *knife*
    are similar to each other, so their vectors are close to each other, while *sword*
    and *herring* are less similar to each other, so their vectors are further apart.
    This is evidence that our method for converting words to numeric vectors has worked:
    it’s enabling us to quantify word similarities successfully.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: If we want to detect plagiarism, we’ll have to find numeric vectors that represent
    more than just these three words. We’ll want to find vectors for every word in
    the English language, or at least the majority of the words that tend to appear
    in student papers. We can imagine what some of these vectors might be. For example,
    the word *haddock* refers to a type of fish, not too different from a herring.
    So, we’ll expect to find that *haddock* has similar neighbors to *herring* and
    similar probabilities in [Table 10-2](#table10-2) (a similar probability of having
    *cut* or *pie* or anything else as a neighbor).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Anytime two words have similar probabilities in [Table 10-2](#table10-2), we
    expect that they’ll have similar vectors, since we’ll be multiplying those vectors
    according to the system of equations in [Equation 10-1](#equation10-1) to get
    those probabilities. For example, we might find that the numeric vector for *haddock*
    is something like (2.1, 1.9, 2.3, 6.5). This vector will be close in Euclidean
    distance to the vector for *herring* (2, 2, 2, 6), and if we multiply the *haddock*
    vector by the other vectors in [Equation 10-1](#equation10-1), we’ll find that
    *haddock* should have probabilities of being near each neighbor word that are
    similar to the probabilities for *herring* in [Table 10-2](#table10-2). We’ll
    similarly need to find vectors for thousands of other words in English, and we’ll
    expect that words that have similar meanings should have similar vectors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s easy to say that we need vectors for every English word, but then the
    question becomes: How should we determine each of these vectors? To understand
    how we can determine the vectors for every word, consider a diagram of the system
    of equations in [Figure 10-1](#figure10-1).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c10/f10001.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-1: A visual representation of the probabilities of words appearing
    near each other'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram looks complex, but it’s meant to illustrate nothing more and nothing
    less than our system of equations. In [Equation 10-1](#equation10-1), we represent
    every word as a vector with four elements, like this: (*a*, *b*, *c*, *d*). The
    *a*, *b*, *c*, and *d* on the left of [Figure 10-1](#figure10-1) represent these
    elements. Each arrow extending from those elements represents a multiplication.
    For example, the arrow marked with a 5 that extends from the circle labeled *a*
    to the oval labeled *probability of cut appearing nearby* means that we should
    multiply every *a* value by 5, and add it to the estimated probability of *cut*
    appearing near a word. If we consider all the multiplications indicated by all
    of these arrows, you can see from [Figure 10-1](#figure10-1) that the probability
    of *cut* appearing near a word is 5 · *a* – 5 · *b* + 3 · *c* + 1 · *d*, just
    as described in [Equation 10-1](#equation10-1). [Figure 10-1](#figure10-1) is
    just another way to represent [Equation 10-1](#equation10-1). If we can find the
    right *a*, *b*, *c*, and *d* values for every word in English, we’ll have the
    word vectors we need to check for plagiarism.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The reason we drew [Figure 10-1](#figure10-1) is to point out that it has exactly
    the form of a neural network, the type of supervised learning model we already
    discussed in Chapter 6. Since it constitutes a neural network, we can use advanced
    software (including several free Python packages) to train the neural network
    and find out exactly what *a*, *b*, *c*, and *d* should be for every word in English.
    The whole point of creating Tables 10-1 and 10-2, and the system of equations
    in [Equation 10-1](#equation10-1), is to create a neural network like the hypothetical
    one shown in [Figure 10-1](#figure10-1). As long as we have data like that in
    [Table 10-2](#table10-2), we can use this neural network software to train the
    neural network shown in [Figure 10-1](#figure10-1) and find all the word vectors
    we need.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important output of this neural network training will be values of
    *a*, *b*, *c*, and *d* for every word in our data. In other words, the output
    of the neural network training will be (*a*, *b*, *c*, *d*) vectors for every
    word. This process is called the *word2vec* model: create a table of probabilities
    like [Table 10-2](#table10-2) for every word in a corpus, use that table to set
    up a neural network like the one shown in [Figure 10-1](#figure10-1), and then
    train that neural network to find numeric vectors that represent every word.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec model is popular because it can create numeric vectors for any
    words, and those vectors can be used for many useful applications. One reason
    that word2vec is popular is that we can train a word2vec model by using only raw
    text as input; we don’t need to annotate or label any words before training the
    model and getting our word vectors. So, even someone who doesn’t speak English
    can create word vectors and reason about them by using the word2vec approach.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'If this sounds complex, don’t worry. Next, we’ll go through code for working
    with these kinds of vectors, and you’ll see that although the ideas and theory
    of word2vec are complex, the code and applications can be straightforward and
    simple. For now, try to feel comfortable with the basic overall idea of what we’ve
    discussed so far: if we create data about which words appear near each other in
    natural language, we can use that data to create vectors that allow us to quantify
    the similarity of any pair of words. Let’s continue with some code, where we can
    see how to use these numeric vectors for detecting plagiarism.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Numeric Vectors in word2vec
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not only has someone already created the word2vec model, but they’ve also already
    done all its hard work for us: written the code, calculated the vectors, and published
    all of it online for us to download anytime for free. In Python, we can use the
    Gensim package to access word vectors for many English words:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Gensim package has a `downloader` that allows us to access many NLP models
    and tools just by using the `load()` method. You can load one such collection
    of vectors in one line as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code loads a collection of word vectors that was created from a corpus
    of news texts containing about 100 billion words. Essentially, someone got the
    information that would be in a table like our [Table 10-2](#table10-2), but with
    thousands more words in it. They obtained these words and probabilities from real
    news sources written by humans. Then they used that information to create a neural
    network like the one in [Figure 10-1](#figure10-1)—again, with thousands more
    words in it. They trained this neural network and found vectors for every word
    in their corpus.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve just downloaded the vectors they calculated. One reason we expect these
    vectors to be useful is that the text corpus used to create these vectors was
    large and diverse, and large, diverse text data sources tend to lead to higher
    accuracy in NLP models. We can look at the vector corresponding to any word as
    follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we print out the vector for the word *sword*. The output you’ll see is
    a vector with 512 numeric elements. Word vectors like this one, which represents
    the word *sword* according to the model we downloaded, are also called *embeddings*,
    because we’ve successfully *embedded* a word in a *vector space*. In short, we’ve
    converted a word to a vector of numbers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll notice that this 512-element vector for *sword* is not the same as the
    vector we used for *sword* previously in the chapter, which was (10, 1, 5, 1).
    This vector is different from our vector for a few reasons. First, this vector
    uses a different corpus than the one we used, so it will have different probabilities
    listed in its version of [Table 10-2](#table10-2). Second, the creators of this
    model decided to find vectors with 512 elements instead of 4 elements as we did,
    so they get more vector elements. Third, their vectors were adjusted to have values
    close to 0, while ours were not. Every corpus and every neural network will lead
    to slightly different results, but if we’re using a good corpus and a good neural
    network, we expect the same qualitative result: vectors that represent words and
    enable us to detect plagiarism (and do many other things).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading these vectors, we can work with them just like any other
    Python object. For example, we can calculate distances between our word vectors,
    as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we’re doing the same Euclidean distance calculations that we did before,
    but instead of calculating distances with the vectors in [Equation 10-1](#equation10-1),
    we’re doing calculations with the vectors we downloaded. When you run these comparisons,
    you’ll see the following outputs:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see that these distances make sense: the vector for *sword* is similar
    to the vector for *knife* (they have distance about 3.28 from each other), but
    it’s different from the vector for *herring* (they have distance about 4.94 from
    each other, much larger than the difference between *sword* and *knife*). You
    can try the same calculation for any other pairs words that are in the corpus,
    like *car* and *van* as well. You can compare differences between pairs of words
    to find out which pairs have the most and the least similar meanings.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Euclidean distance is not the only distance metric people use to compare word
    vectors. Using cosine similarity measurements is also common, just as we did in
    Chapter 9. Remember that we used this code to calculate cosine similarities:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here we define a function called `cosine_similarity()` to check the cosine
    of the angle between any two vectors. We can check some cosine similarities between
    vectors as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When you run this snippet, you’ll see the following results:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can see that these metrics are doing exactly what we want: they give us
    lower values for words that we perceive as different and higher values for words
    that we perceive as similar. Even though your laptop doesn’t “speak English,”
    just by analyzing a corpus of natural language text, it’s able to quantify exactly
    how similar and exactly how different distinct words are.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating Vectors with Mathematical Calculations
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One famous illustration of the power of word2vec comes from an analysis of
    the words *king* and *queen*. To see this illustration, let’s start by getting
    the vectors associated with some English words:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we define some vectors associated with several words. As humans, we know
    that a king is a male head of a monarchy and a queen is a female head of a monarchy.
    We might even express the relationship between the words *king* and *queen* as
    follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: king – man + woman = queen
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the idea of a *king*, and taking away from that the idea of a
    *man*, then adding to that the idea of a *woman*, we end up with the idea of a
    *queen*. If we’re thinking about just the world of words, this equation might
    seem ridiculous, since it’s typically not possible to add and subtract words or
    ideas in this way. However, remember that we have vector versions of each of these
    words, so we can add and subtract vectors from each other and see what we get.
    Let’s try to add and subtract the vectors corresponding to each of these words
    in Python:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, we take our `king` vector, subtract our `man` vector, add our `woman`
    vector, and define the result as a new variable called `newvector`. If our additions
    and subtractions do what we want them to, our `newvector` should capture a specific
    meaning: the idea of a king without any attributes of a *man*, but with the added
    attributes of a *woman*. In other words, even though the `newvector` is a sum
    of three vectors, none of which are the vector for *queen*, we expect their sum
    to be close to, or equal to, the vector for *queen*. Let’s check the difference
    between our `newvector` and our `queen` vector to see whether this is the case:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can see that our `newvector` is similar to our `queen` vector: their cosine
    similarity is 0.76, and their Euclidean distance is 2.5\. We can compare this
    to the differences between other familiar pairs of words:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should see that *king – man + woman* is more similar to *queen* than *fish*
    is to *herring*. Our mathematical calculations with the words’ vectors lead to
    exactly the results we expect based on what we know about the words’ linguistic
    meanings. This demonstrates the usefulness of these vectors: we can not only compare
    them to find similarities between pairs of words but also manipulate them through
    addition and subtraction to add and subtract concepts. The ability to add and
    subtract these vectors, and get results that make sense, is more evidence that
    these vectors are reliably capturing the meanings of their associated words.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Plagiarism with word2vec
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s go back to the plagiarism scenario from earlier in the chapter. Remember
    that we introduced the following two sentences as an example of plagiarism:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The trouble with having an open mind, of course, is that people will insist
    on coming along and trying to put things in it. [original sentence]
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The problem with having an open mind is that people will insist on approaching
    and trying to insert things into your mind. [plagiarized sentence]
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since these two sentences differ in a few places, a naive plagiarism checker
    that looks for exact matches won’t detect plagiarism here. Instead of checking
    for exact matches for every character of both sentences, we want to check for
    close matches between the meanings of each word. For words that are identical,
    we’ll find that they have 0 distance between them:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results of this are not surprising. We find that a word’s vector has perfect
    (1.0) cosine similarity with itself and a word’s vector has 0 Euclidean distance
    from itself. Every word is equal to itself. But remember, wily plagiarists are
    paraphrasing, not using the exact same words as published texts. If we compare
    words that are paraphrased, we expect that they’ll be similar to the words of
    the original, but not exactly the same. We can measure the similarity of potentially
    paraphrased words as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You’ll see the following results from this code snippet:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This snippet compares the words of the plagiarized text and the words of the
    original text. The results show close matches in almost every case: either a relatively
    small Euclidean distance or a relatively high cosine similarity. If the individual
    words of a student’s sentence are all close matches to the individual words of
    a published sentence, that’s good evidence of plagiarism, even if there are few
    or no exact matches. Importantly, we can check for these close matches automatically,
    not based on slow, costly human judgment, but based on data and quick Python scripts.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Checking for close matches for all the individual words in sentences is a reasonable
    way to start detecting plagiarism. However, it’s not perfect. The main problem
    is that so far, we’ve learned to evaluate only individual words instead of full
    sentences. We can think of sentences as collections or sequences of individual
    words. But in many cases, it will be better to have a technique that can evaluate
    meanings and similarities for entire sentences simultaneously, treating sentences
    as individual units instead of only as collections of words. For that, we’ll turn
    to a powerful new approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Using Skip-Thoughts
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *skip-thoughts* model is an NLP model that uses data and neural networks
    to convert entire sentences to numeric vectors. It’s quite similar to word2vec,
    but instead of converting individual words one at a time to vectors, we convert
    entire sentences to vectors as units.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The theory behind skip-thoughts is similar to the theory behind word2vec: you
    take a natural language corpus, find which sentences tend to appear near each
    other, and train a neural network that can predict which sentences are expected
    to appear before or after any other sentence. For word2vec, we saw an illustration
    of a neural network model in [Figure 10-1](#figure10-1). For skip-thoughts, a
    similar illustration is shown in [Figure 10-2](#figure10-2).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c10/f10002.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-2: We use the skip-thoughts model to predict which sentences appear
    near each other.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-2](#figure10-2) is based on the model from the 2015 paper titled
    “Skip-Thought Vectors” by Ryan Kiros and colleagues ([https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)).
    You can see that the sentence on the left of the figure is taken as the input.
    This sentence is a sequence of individual words, but all the words are considered
    together as a single unit. The skip-thoughts model attempts to find a vector representation
    of this sentence that, when used as an input in a neural network, can predict
    sentences that are most likely to come before and after it (including the sentences
    on the right of [Figure 10-2](#figure10-2)). Just as word2vec is based on which
    individual words are expected to be near other words, skip-thoughts is based on
    predicting the full sentences that are near other sentences. You don’t need to
    worry about the theory too much; just try to remember that skip-thoughts is a
    way to encode natural language sentences as vectors by calculating probabilities
    of other sentences appearing nearby.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Just as with word2vec, writing code for any of this ourselves is not necessary.
    Instead, we’ll turn to the *Universal Sentence Encoder (USE)*. This tool converts
    sentences into vectors, using the idea of skip-thoughts to find the vectors (plus
    other advanced technical methods). We’re going to use the vector outputs of the
    USE for plagiarism detection, but USE vectors can also be used for chatbot implementations,
    image tagging, and much more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the USE is not hard to work with, because someone else has written
    it for us. We can start by defining sentences that we want to analyze:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here we have a list of sentences, and we want to convert each into a numeric
    vector. We can import code someone else has written to do this conversion:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `tensorfow_hub` module allows us to load the USE from an online repository.
    The USE is a big model, so don’t panic if loading it into your Python session
    takes a few minutes or more. When we load it, we save it as a variable called
    `embed`. Now that we have the USE in our Python session, we can create word embeddings
    (vectors) with one simple line of code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our `embeddings` variable contains vectors that represent each of the sentences
    in our `Sentences` list. You can look at the vector for the first sentence by
    checking the first element of the `embeddings` variable as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When you run this snippet, you’ll see a numeric vector with 512 elements, the
    first 16 of which are shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is the vector representation of the first sentence in your list—not the
    individual words in the sentence, but the sentence itself. Just as we did with
    our word2vec vectors, we can calculate the distance between any two of our sentence
    vectors. In this case, the distances between vectors will represent the degree
    of difference between the overall meanings of two sentences. Let’s check the distance
    between our first two sentences as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can see that the cosine similarity is about 0.85—indicating that the first
    two sentences in our `Sentences` list are quite similar to each other. This is
    evidence that the student’s sentence (the second one in our `Sentences` list)
    is plagiarized from Pratchett’s sentence (the first one in our `Sentences` list).
    By contrast, we can check distances between other vectors and see that they’re
    not quite so similar. For example, if you run `print(cosine_similarity(embeddings[0],embeddings[2]))`,
    you can see that the cosine similarity of these two sentences is about 0.02, indicating
    that these sentences are almost as different as it’s possible for two sentences
    to be. This is evidence that Pratchett didn’t plagiarize *Hamlet*. If you run
    `print(cosine_similarity(embeddings[0],embeddings[3]))`, you can see that the
    cosine similarity of these two sentences is about –0.07, another low similarity
    score, indicating that Pratchett also didn’t plagiarize *Moby Dick*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: You can see that checking the distances between the meanings of any two sentences
    is straightforward. Your plagiarism detector can simply check for the cosine similarity
    (or Euclidean distance) between a student’s work and previously published sentences,
    and if the similarities appear to be large (or the Euclidean distances appear
    to be too small), you can take it as evidence that the student is guilty of plagiarism.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modeling
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To finish the chapter, let’s introduce one final business scenario and talk
    about how we can combine NLP tools with tools from previous chapters to deal with
    it. In this scenario, imagine that you run a forum website. Your site has become
    so successful that you can no longer read all the threads and conversations yourself,
    but you still want to understand the topics people are writing about on your site
    so you can understand who your users are and what they care about. You want to
    find a reliable, automated way to analyze all the text on your site and discover
    the main topics being discussed. This goal, called *topic modeling*, is common
    in NLP.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, let’s take a collection of sentences that might have appeared on
    your site. A successful forum website could receive thousands of comments every
    second, but we’ll start by looking at a small sample of the full data, just eight
    sentences:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Just as you did before, you can calculate the vectors, or embeddings, for all
    these sentences:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we can create a matrix of all our sentence embeddings:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This snippet starts by creating a list called `arrays` and adding all the sentence
    vectors to the list. Next, it creates a matrix called `sentencematrix`. This matrix
    is just your sentence vectors stacked on top of each other, with one row for each
    sentence vector. Finally, we convert this matrix into a pandas dataframe, so it’s
    easier to work with. The final result, called `pandasmatrix`, has eight rows;
    each row is a sentence vector for one of our eight sentences.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a matrix that contains our sentence vectors. But getting our vectors
    isn’t enough; we need to decide what to do with them. Remember that our goal is
    topic modeling. We want to understand the topics that people are writing about
    and which sentences relate to which topics. We have several ways to accomplish
    this. One natural way to accomplish topic modeling is by using clustering, something
    we already discussed in Chapter 7.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Our clustering approach is simple: we’ll take our matrix of sentence vectors
    as our input data. We’ll apply clustering to determine the natural groups that
    exist in the data. We’ll interpret the groups (clusters) that we find as the distinct
    topics being discussed on your forum website. We can do this clustering as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can see that the clustering requires only a few lines of code. We import
    the `KMeans` code from the sklearn module. We create a variable called `m` and
    then use its `fit()` method to find two clusters for our matrix (the `pandasmatrix`).
    The `fit()` method uses Euclidean distance measurements with our sentence vectors
    to find two clusters of documents. The two clusters it finds are what we will
    take to be the two main topics of our collection of sentences. After we find these
    clusters, we add two new columns to our `pandasmatrix`: first, we add the labels
    that are the result of our clustering (in the `topic` variable), and second, we
    add the actual sentences that we’re trying to cluster. Let’s look at the results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This snippet prints out two sets of sentences: first, sentences that are labeled
    as belonging to cluster 0 (sentences on rows with the `topic` variable equal to
    0) and second, sentences that are labeled as belonging to cluster 1 (sentences
    on rows with the `topic` variable equal to 1). You should see the following results:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can see that this method has identified two clusters in our data, and they’ve
    been labeled as cluster 0 and cluster 1\. When you look at the sentences that
    have been classified as cluster 0, you can see that many seem to be discussions
    about food and restaurants. When you look at cluster 1, you can see that it seems
    to consist of critiques of books. At least according to this sample of eight sentences,
    these are the main topics being discussed on your forum, and they’ve been identified
    and organized automatically.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: We’ve accomplished topic modeling, using a numeric method (clustering) on non-numeric
    data (natural language text). You can see that USE, and word embeddings in general,
    can be useful in many applications.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Other Applications of NLP
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One useful application of NLP is in the world of recommendation systems. Imagine
    that you run a movie website and want to recommend movies to your users. In Chapter
    9, we discussed how to use interaction matrices to make recommendations based
    on comparisons of transaction histories. However, you could also make a recommendation
    system based on comparisons of content. For example, you could take plot summaries
    of individual movies and use the USE to get sentence embeddings for each plot
    summary. Then you could calculate distances between plot summaries to determine
    the similarity of various movies and, anytime a user watches a move, recommend
    that that user also watch movies with the most similar plots. This is a *content-based
    recommendation system*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting application of NLP is *sentiment analysis*. We encountered
    sentiment analysis a little already in Chapter 6. Certain tools can determine
    whether any given sentence is positive, negative, or neutral in its tone or the
    sentiment it expresses. Some of these tools rely on word embeddings like those
    we’ve covered in this chapter, and others don’t. Sentiment analysis could be useful
    for a business that receives thousands of emails and messages every day. By running
    automatic sentiment analysis on all of its incoming emails, a business could determine
    which customers were the most happy or unhappy, and potentially prioritize responses
    based on customer sentiment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Many businesses today deploy *chatbots* on their websites—computer programs
    that can understand text inputs and answer questions. Chatbots have varying levels
    of sophistication, but many rely on some kind of word embeddings like the word2vec
    and skip-thought methods described in this chapter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: NLP has many other possible applications in business. Today, law firms are trying
    to use NLP to automatically analyze documents and even automatically generate
    or at least organize contracts. News websites have tried to use NLP to automatically
    generate certain kinds of formulaic articles, like recaps of sports games. There’s
    no end to the possibilities of NLP as the field itself continues to develop. If
    you know some powerful methods like word2vec and skip-thoughts as a starting point,
    there’s no limit to the useful applications you can create.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we went over natural language processing. All of the applications
    of NLP that we discussed relied on embeddings: numeric vectors that accurately
    represent words and sentences. If you can represent a word numerically, you can
    do math with it, including calculating similarities (which we did for plagiarism
    detection) and clustering (which we did for topic modeling). NLP tools might be
    difficult to use and master, but they can be astonishing in their capabilities.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll shift gears and wrap up the book by going over some
    simple ideas about working with other programming languages that are important
    for data science.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
