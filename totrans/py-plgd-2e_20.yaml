- en: '[15](nsp-venkitachalam503045-0008.xhtml#rch15)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[15](nsp-venkitachalam503045-0008.xhtml#rch15)'
- en: Audio ML on Pi
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树莓派上的音频机器学习
- en: '![](images/nsp-venkitachalam503045-circle-image.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-circle-image.jpg)'
- en: In the past decade, *machine learning (ML)* has taken the world by storm. It’s
    everywhere from facial recognition to predictive text to self-driving cars, and
    we keep hearing about novel applications of ML seemingly every day. In this chapter,
    you’ll use Python and TensorFlow to develop an ML-based speech recognition system
    that will run on an inexpensive Raspberry Pi computer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，*机器学习（ML）*已经风靡全球。从面部识别到预测文本再到自动驾驶汽车，机器学习无处不在，而且我们似乎每天都在听到有关机器学习新应用的消息。在本章中，你将使用
    Python 和 TensorFlow 开发一个基于机器学习的语音识别系统，该系统将运行在廉价的树莓派计算机上。
- en: Speech recognition systems are already deployed in a huge number of devices
    and appliances in the form of voice assistants such as Alexa, Google, and Siri.
    These systems can perform tasks ranging from setting reminders to switching on
    your home lights from your office. But all of these platforms require your device
    to be connected to the internet and for you to sign up for their services. This
    brings up issues of privacy, security, and power consumption. Does your light
    bulb *really* need to be connected to the internet to respond to a voice command?
    The answer is *no*. With this project, you’ll get a sense of how to design a speech
    recognition system that works on a low-power device, without the device needing
    to be connected to the internet.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别系统已经在大量设备和家电中得到应用，形式为语音助手，如 Alexa、Google 和 Siri。这些系统可以执行从设置提醒到在办公室控制家里灯光等任务。但所有这些平台都需要你的设备连接到互联网，并且你需要注册他们的服务。这就引出了隐私、安全性和电力消耗的问题。你的灯泡*真的*需要连接互联网才能响应语音命令吗？答案是*不*。通过这个项目，你将了解如何设计一个在低功耗设备上运行的语音识别系统，而不需要设备连接到互联网。
- en: 'Some of the concepts you’ll learn about through this project are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个项目，你将学习到一些概念，包括：
- en: • Using a machine learning workflow to solve a problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 使用机器学习工作流来解决问题
- en: • Creating an ML model with TensorFlow and Google Colab
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 使用 TensorFlow 和 Google Colab 创建机器学习模型
- en: • Streamlining an ML model for use on a Raspberry Pi
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 精简一个机器学习模型以便在树莓派上使用
- en: • Processing audio and generating spectrograms with the short-time Fourier transform
    (STFT)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 处理音频并使用短时傅里叶变换（STFT）生成谱图
- en: • Leveraging multiprocessing to run tasks in parallel
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 利用多进程并行运行任务
- en: '[A Machine Learning Overview](nsp-venkitachalam503045-0008.xhtml#rah1701)'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[机器学习概述](nsp-venkitachalam503045-0008.xhtml#rah1701)'
- en: It’s impossible to do justice to a topic as vast as machine learning in a single
    section of a single book chapter. Instead, our approach will be to treat ML as
    just another tool for solving a problem—in this case, how to distinguish between
    different spoken words. In truth, ML frameworks like TensorFlow have become so
    mature and easy to use these days that it’s possible to effectively apply ML to
    a problem without being an expert in the subject. So in this section, we’ll only
    briefly touch upon the ML terminology relevant to the project.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一本书的单一章节中很难公正地讲解像机器学习这么广泛的话题。因此，我们的做法是将机器学习视为解决问题的另一个工具——在本例中，就是区分不同的语音单词。事实上，像
    TensorFlow 这样的机器学习框架已经变得如此成熟和易于使用，以至于即使你不是这个领域的专家，也能够有效地将机器学习应用于实际问题。所以，在本节中，我们将简要介绍与本项目相关的机器学习术语。
- en: ML is a small part of the larger computer science discipline of *artificial
    intelligence (AI)*, although when AI is mentioned in the popular press, ML is
    usually what they mean. ML itself is made of various subdisciplines that involve
    different approaches and algorithms. In this project, you’ll use a subset of ML
    called *deep learning*, which harnesses *deep neural networks (**DNNs)* to identify
    features and patterns in large sets of data. DNNs have their origin in *artificial
    neural networks (ANNs)*, which are loosely based on neurons in our brains. ANNs
    consists of a bunch of *nodes* with multiple inputs. Each node also has a *weight*
    associated with it. The output of an ANN is typically a nonlinear function of
    the inputs and weights. This output can be connected to the input of another ANN.
    When you have more than one layer of ANNs, the network becomes a deep neural network.
    Typically, the more layers the network has—that is, the deeper it goes—the more
    accurate the learning model becomes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是更大计算机科学领域——*人工智能（AI）*的一部分，尽管在大众媒体提到人工智能时，通常指的就是机器学习。机器学习本身由不同的方法和算法组成的多个子学科构成。在这个项目中，你将使用机器学习的一个子集——*深度学习*，它利用*深度神经网络（**DNNs**）*来识别大量数据中的特征和模式。深度神经网络源自于*人工神经网络（ANNs）*，后者大致模仿我们大脑中的神经元。人工神经网络由多个*节点*构成，每个节点有多个输入。每个节点还有一个与之相关的*权重*。人工神经网络的输出通常是输入和权重的非线性函数。这个输出可以连接到另一个人工神经网络的输入。当你有多个层的人工神经网络时，网络就变成了深度神经网络。通常，网络的层数越多——即网络越深——学习模型的准确性也就越高。
- en: For this project, you’ll be using a *supervised learning* process, which can
    be divided into two phases. First is the *training phase*, where you show the
    model several inputs and their expected outputs. For example, if you were trying
    to build a human presence detection system to recognize whether or not there’s
    a person in a video frame, you would use the training phase to show examples of
    both cases (human versus no human), with each example labeled correctly. Next
    is the *inference phase*, where you show new inputs and the model makes predictions
    about them based on what it learned during training. Continuing the example, you’d
    show your human presence detection system new video frames, and the model would
    predict whether or not there’s a human in each frame. (There are also *unsupervised
    learning* processes, in which the ML system attempts to find patterns by itself,
    based on unlabeled data.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用*监督学习*过程，该过程可以分为两个阶段。第一个阶段是*训练阶段*，你需要向模型展示若干输入及其期望的输出。例如，如果你正在构建一个人类存在检测系统来识别视频帧中是否有人，你将在训练阶段展示这两种情况的示例（有人的情况与没有人的情况），每个示例都要正确标注。接下来是*推理阶段*，在这个阶段，你会展示新的输入，模型根据在训练过程中学到的内容对其进行预测。继续上述示例，你将向人类存在检测系统展示新的视频帧，模型会预测每一帧中是否有一个人。（还有*无监督学习*过程，在该过程中，机器学习系统通过未标记的数据尝试自行发现模式。）
- en: An ML model has many numerical *parameters* that help it process data. During
    training, these parameters are adjusted automatically to minimize errors between
    the expected values and the values the model predicts. Usually a class of algorithms
    called *gradient descent* is used to minimize the error. In addition to the parameters
    of an ML model, which are adjusted during training, there are also *hyperparameters*,
    variables that are adjusted for the model as a whole, such as which neural network
    architecture to use or the size of your training batch. [Figure 15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1)
    shows the neural network architecture I’ve chosen for this project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习模型有许多数值*参数*，这些参数帮助模型处理数据。在训练过程中，这些参数会自动调整，以最小化期望值和模型预测值之间的误差。通常，使用一种叫做*梯度下降*的算法来最小化误差。除了机器学习模型在训练过程中调整的参数外，还有一些*超参数*，这些变量会调整整个模型的设置，比如使用哪种神经网络架构或训练批次的大小。[图
    15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1)展示了我为这个项目选择的神经网络架构。
- en: '![](images/nsp-venkitachalam503045-f15001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15001.jpg)'
- en: 'Figure 15-1: The neural network architecture for the speech recognition project'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-1：语音识别项目的神经网络架构
- en: Each layer in the network architecture represents some form of processing on
    the data that helps improve the model’s accuracy. The design of the network isn’t
    trivial, but just defining each layer won’t tell us much about how it works. A
    broader question to consider is *why* I’ve chosen this particular network. The
    answer is that one needs to determine the best network architecture for the project
    at hand via experimentation. It’s common to try different neural network architectures
    and see which one produces the most accurate results after training. There are
    also architectures published by ML researchers that are known to perform well,
    and that’s a good place to start for practical applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构中的每一层代表对数据的一种处理形式，旨在帮助提高模型的准确性。网络设计并非简单任务，仅仅定义每一层并不能告诉我们它是如何工作的。一个更广泛的问题是，*为什么*我选择了这个特定的网络。答案是，通过实验需要确定适合当前项目的最佳网络架构。通常的做法是尝试不同的神经网络架构，并观察哪一种在训练后能产生最准确的结果。也有一些由机器学习研究者发布的架构，已知它们表现良好，这为实际应用提供了一个很好的起点。
- en: 'NOTE For more information on machine learning, I highly recommend the book
    *Deep Learning: A Visual Approach* by Andrew Glassner (No Starch Press, 2021).
    The book will gives you a good intuition about the subject without getting too
    much into the math or code. For a comprehensive, hands-on approach, I also recommend
    the online ML courses on Coursera taught by Andrew Ng.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注：有关机器学习的更多信息，我强烈推荐Andrew Glassner（《深度学习：一种视觉方法》，No Starch Press，2021年）所著的书籍。该书将帮助你对该主题有更好的直观理解，而不会过多涉及数学或代码。对于全面且实践性强的学习方式，我还推荐Andrew
    Ng教授在Coursera上的在线机器学习课程。
- en: '[How It Works](nsp-venkitachalam503045-0008.xhtml#rah1702)'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[工作原理](nsp-venkitachalam503045-0008.xhtml#rah1702)'
- en: In this project, you’ll use Google’s TensorFlow machine learning framework to
    train a neural network using a collection of audio files containing speech commands.
    Then you’ll load an optimized version of the trained model onto a Raspberry Pi
    equipped with a microphone so the Pi can recognize the commands when you speak
    them. [Figure 15-2](nsp-venkitachalam503045-0030.xhtml#fig15-2) shows a block
    diagram for the project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用谷歌的TensorFlow机器学习框架，使用包含语音命令的音频文件集训练神经网络。然后，你将把训练好的模型的优化版本加载到配有麦克风的树莓派上，以便树莓派在你发出命令时能够识别它们。[图
    15-2](nsp-venkitachalam503045-0030.xhtml#fig15-2)展示了该项目的框图。
- en: '![](images/nsp-venkitachalam503045-f15002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15002.jpg)'
- en: 'Figure 15-2: A block diagram of the speech recognition project'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-2：语音识别项目的框图
- en: For the training portion of the project, you’ll work in Google Colab (short
    for Colaboratory), a free cloud-based service that lets you write and run Python
    programs in your web browser. There are two advantages to using Colab. First,
    you don’t need to install TensorFlow locally on your computer, nor deal with incompatibility
    issues related to various versions of TensorFlow. Second, Colab runs on machines
    that are likely much more powerful than yours, so the training process will go
    more quickly. For training data, you’ll use the Mini Speech Commands dataset from
    Google, a subset of a larger Speech Commands dataset published in 2018\. It consists
    of thousands of sample recordings of the words *yes*, *no*, *up*, *down*, *left*,
    *right*, *stop*, and *go*, all standardized as 16-bit WAV files with a 16,000
    Hz sampling rate. You’ll generate a *spectrogram* of each recording, an image
    that shows how the frequency content of the audio changes over time, and use those
    spectrograms to train a deep neural network (DNN) via TensorFlow.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目的训练部分，你将在Google Colab（即Colaboratory）中进行操作，这是一个免费的云端服务，允许你在网页浏览器中编写和运行Python程序。使用Colab有两个优点。首先，你不需要在本地计算机上安装TensorFlow，也不用处理与不同版本TensorFlow相关的不兼容问题。其次，Colab运行的机器通常比你的计算机更强大，因此训练过程将更快。作为训练数据，你将使用来自Google的Mini
    Speech Commands数据集，这是2018年发布的更大Speech Commands数据集的一个子集。它包含成千上万条样本录音，内容包括*yes*（是）、*no*（否）、*up*（向上）、*down*（向下）、*left*（向左）、*right*（向右）、*stop*（停止）和*go*（开始），所有录音均标准化为16位WAV文件，采样率为16,000
    Hz。你将生成每个录音的*声谱图*，这是一种图像，显示音频的频率内容如何随时间变化，并利用这些声谱图通过TensorFlow训练一个深度神经网络（DNN）。
- en: NOTE The training portion of this project takes inspiration from Google’s official
    TensorFlow example called “Simple Audio Recognition.” You’ll use the same neural
    network architecture as that example. However, the rest of the project deviates
    significantly from Google’s example, since our goal is to recognize live audio
    on a Raspberry Pi, whereas the latter runs inference on existing WAV files.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本项目的训练部分灵感来自谷歌官方的TensorFlow示例“简单音频识别”。你将使用与该示例相同的神经网络架构。然而，本项目的其余部分与谷歌的示例有很大不同，因为我们的目标是在树莓派上识别实时音频，而后者是在现有的WAV文件上进行推理。
- en: Once the training is complete, you’ll convert the trained model to a simplified
    format called TensorFlow Lite, which is designed to run on less capable hardware
    such as embedded systems, and load that streamlined model onto the Raspberry Pi.
    There you’ll run Python code to continuously monitor the audio input from a USB
    microphone, take spectrograms of the audio, and perform inference on that data
    to identify the spoken commands from the training set. You’ll print out the commands
    that the model identifies to the serial monitor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，你将把训练好的模型转换为一个简化格式，称为TensorFlow Lite，该格式旨在运行在硬件性能较低的设备上，如嵌入式系统，并将该精简模型加载到树莓派上。在树莓派上，你将运行Python代码，持续监控USB麦克风的音频输入，获取音频的频谱图，并对这些数据进行推理，识别出训练集中的语音命令。你将把模型识别出的命令打印到串口监视器上。
- en: '[Spectrograms](nsp-venkitachalam503045-0008.xhtml#rbh1701)'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[频谱图](nsp-venkitachalam503045-0008.xhtml#rbh1701)'
- en: A key step in this project is generating spectrograms of the audio data—both
    the preexisting data used to train the model and the real-time data encountered
    during inference. In [Chapter 4](nsp-venkitachalam503045-0016.xhtml#ch04), you
    saw how a spectral plot reveals the frequencies present in an audio sample at
    a particular moment in time. Then, in [Chapter 13](nsp-venkitachalam503045-0028.xhtml#ch13),
    you learned how spectral plots are calculated with a mathematical tool called
    a *discrete Fourier transform (DFT)*. A spectrogram is essentially just a series
    of spectral plots, generated through a sequence of Fourier transformers, which
    together reveal how the frequency content of some audio data evolves over time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目中的一个关键步骤是生成音频数据的频谱图——包括用于训练模型的预先存在的数据和在推理过程中遇到的实时数据。在[第4章](nsp-venkitachalam503045-0016.xhtml#ch04)中，你已经看到如何通过频谱图揭示音频样本在特定时刻的频率。接着，在[第13章](nsp-venkitachalam503045-0028.xhtml#ch13)中，你学习了如何使用一种数学工具，*离散傅里叶变换（DFT）*，来计算频谱图。频谱图本质上就是一系列通过傅里叶变换器生成的频谱图，这些频谱图共同展示了某些音频数据的频率内容如何随时间变化。
- en: You need a spectrogram, rather than a single spectral plot, of each audio sample
    because the sound of human speech is incredibly complex. Even in the case of a
    single word, the frequencies present in the sound change significantly—and in
    distinctive ways—as the word is spoken. For this project, you’ll be working with
    one-second-long audio clips, each consisting of 16,000 samples. If you computed
    a single DFT of the entire clip in one go, you wouldn’t get an accurate picture
    of how the frequencies change over the course of the clip, and thus you wouldn’t
    be able to reliably identify the word being spoken. Instead, you’ll divide the
    clip into a bunch of overlapping intervals and compute the DFT for each of these
    intervals, giving you the series of spectral plots needed for a spectrogram. [Figure
    15-3](nsp-venkitachalam503045-0030.xhtml#fig15-3) illustrates this type of computation,
    called a *short-time Fourier transform (**STFT)*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要每个音频样本的频谱图，而不是单一的频谱图，因为人类语音的声音非常复杂。即使是一个单词，其声音中的频率也会发生显著变化，而且这种变化具有独特的方式。当这个词被发音时，频率会随着时间的推移而变化。对于本项目，你将处理每个持续一秒钟的音频片段，每个片段由16,000个样本组成。如果你一次性计算整个片段的单一离散傅里叶变换（DFT），你无法准确显示频率如何随时间变化，因此也无法可靠地识别正在说的词。相反，你将把音频片段分成一段段重叠的时间窗口，并计算每个时间窗口的DFT，从而得到频谱图所需的一系列频谱图。[图15-3](nsp-venkitachalam503045-0030.xhtml#fig15-3)展示了这种计算方法，称为*短时傅里叶变换（**STFT**）*。
- en: '![](images/nsp-venkitachalam503045-f15003.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15003.jpg)'
- en: 'Figure 15-3: Computing the spectrogram of a signal'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-3：计算信号的频谱图
- en: The STFT gives you *M* DFTs of the audio, taken at even time intervals. Time
    is shown along the x-axis of the spectrogram. Each DFT gives you *N* frequency
    bins and the intensity of the sound within each of those bins. The frequency bins
    are mapped to the y-axis of the spectrogram. Thus, the spectrogram takes the form
    of an *M*×*N* image. Each column of pixels in the image represents one of the
    DFTs, with color used to convey the intensity of the signal in a given frequency
    band.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: STFT为你提供了音频的*M*个离散傅里叶变换（DFT），它们在均匀的时间间隔内取样。时间沿着频谱图的x轴显示。每个DFT给出了*N*个频率桶，以及每个桶内的声音强度。频率桶被映射到频谱图的y轴。因此，频谱图呈现出一个*M*×*N*的图像。图像中的每一列像素代表一个DFT，颜色则用来表示给定频率带中的信号强度。
- en: You might be wondering why we need to use Fourier transforms at all for this
    project. Why not use the waveforms of the audio clips directly, instead of extracting
    the frequency information from those waveforms? For an answer, consider [Figure
    15-4](nsp-venkitachalam503045-0030.xhtml#fig15-4).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么这个项目需要使用傅里叶变换呢？为什么不直接使用音频片段的波形，而是从这些波形中提取频率信息呢？要回答这个问题，可以参考[图 15-4](nsp-venkitachalam503045-0030.xhtml#fig15-4)。
- en: '![](images/nsp-venkitachalam503045-f15004_annotated.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15004_annotated.jpg)'
- en: 'Figure 15-4: The waveform and spectrogram of speech samples'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-4：语音样本的波形和频谱图
- en: The top half of the figure shows the waveform of a recording made by speaking
    the sequence “Left, right, left, right.” The bottom half of the figure shows a
    spectrogram of that recording. Looking just at the waveform, you can see that
    the two *left*s look vaguely similar, as do the two *right*s, but it’s hard to
    pick out strong identifying characteristics of each word’s waveform. By contrast,
    the spectrogram reveals more visual features associated with each word, like the
    bright C-shaped curve (shown by the arrows) in each instance of *right*. We can
    see these distinctive features more clearly with our own eyes, and a neural network
    can “see” them as well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图的上半部分显示了一个录音的波形，该录音为“左、右、左、右”序列。图的下半部分显示了该录音的频谱图。仅从波形来看，可以看到两个*左*听起来大致相似，两个*右*也类似，但很难从每个词的波形中提取出明显的识别特征。相反，频谱图揭示了与每个词相关的更多视觉特征，比如每个*右*实例中的明亮C形曲线（由箭头所示）。我们可以用自己的眼睛更清楚地看到这些独特的特征，神经网络同样也能“看到”它们。
- en: In the end, since a spectrogram is essentially an image, taking spectrograms
    of the data turns a speech recognition problem into an image classification problem,
    allowing us to leverage the rich set of ML techniques that already exist for classifying
    images. (Of course, a waveform can be treated as an image too, but as you’ve seen,
    a spectrogram is better at capturing the “signature” of the audio data and hence
    more suited for ML applications.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，由于频谱图本质上是图像，获取数据的频谱图将语音识别问题转化为图像分类问题，使我们能够利用已有的丰富的机器学习技术来进行图像分类。（当然，波形也可以作为图像处理，但正如你所看到的，频谱图在捕捉音频数据的“特征”方面更为出色，因此更适合用于机器学习应用。）
- en: '[Inference on the Raspberry Pi](nsp-venkitachalam503045-0008.xhtml#rbh1702)'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[树莓派上的推理](nsp-venkitachalam503045-0008.xhtml#rbh1702)'
- en: 'The code on the Raspberry Pi must accomplish several tasks: it needs to read
    the audio input from the attached microphone, compute the spectrogram of that
    audio, and do inference using the trained TensorFlow Lite model. Here’s one possible
    sequence of operations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 树莓派上的代码必须完成几个任务：它需要读取来自附加麦克风的音频输入，计算该音频的频谱图，并使用训练好的TensorFlow Lite模型进行推理。以下是一个可能的操作顺序：
- en: 1\. Read microphone data for one second.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1. 读取麦克风数据，持续一秒钟。
- en: 2\. Process data.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 处理数据。
- en: 3\. Do inference.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 进行推理。
- en: 4\. Repeat.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 重复。
- en: 'There’s a big problem with this approach, however. While you’re busy with steps
    2 and 3, more speech data could be coming in, which you’ll end up missing. The
    solution is to use Python multiprocessing to perform different tasks concurrently.
    Your main process will just collect the audio data and put it in a queue. In a
    separate, simultaneous process, you’ll take this data out of the queue and run
    inference on it. Here’s what the new scheme looks like:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有一个大问题。你在忙于执行步骤 2 和 3 时，可能会错过更多的语音数据。解决方法是使用 Python 的多进程功能并行执行不同任务。你的主进程只负责收集音频数据并将其放入队列。在另一个独立且同时进行的进程中，你会从队列中取出这些数据并对其进行推理。以下是新的方案：
- en: Main Process
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程
- en: 1\. Read microphone data for one second.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1. 读取麦克风数据，持续一秒钟。
- en: 2\. Put data into the queue.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 将数据放入队列。
- en: Inference Process
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 推理进程
- en: 1\. Check if there’s any data in the queue.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 检查队列中是否有数据。
- en: 2\. Run inference on the data.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 对数据进行推理。
- en: 'Now the main process won’t miss any audio input, since putting data into the
    queue is a very quick operation. But there’s another problem. You’re collecting
    one-second audio samples continuously from the microphone and processing them,
    but you can’t assume that all spoken commands will fit cleanly into those one-second
    intervals. A command could come at an edge and be broken up across two consecutive
    samples, in which case it probably won’t be identified during inference. A better
    approach is to create overlapping samples, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在主进程不会错过任何音频输入，因为将数据放入队列是一个非常快速的操作。但另一个问题也随之而来。你正在从麦克风连续收集一秒钟的音频样本并进行处理，但你不能假设所有的语音命令都能完美地适应这些一秒钟的时间间隔。命令可能出现在边缘，并被分割到两个连续的样本中，在这种情况下，它可能在推理过程中无法识别。一个更好的方法是创建重叠样本，如下所示：
- en: Main Process
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程
- en: 1\. For the very first frame, collect a two-second sample.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 对于第一个帧，收集一个两秒钟的样本。
- en: 2\. Put the two-second sample into the queue.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 将两秒钟的样本放入队列。
- en: 3\. Collect another one-second sample.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 收集另一个一秒钟的样本。
- en: 4\. Create a two-second sample by moving the latter half of the sample from
    step 2 to the front and replacing the second half with the sample from step 3.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4\. 通过将步骤2中的样本后半部分移到前面，并用步骤3中的样本替换后半部分来创建一个两秒钟的样本。
- en: 5\. Return to step 2.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5\. 返回步骤2。
- en: Inference Process
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程
- en: 1\. Check if there’s any data in the queue.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 检查队列中是否有数据。
- en: 2\. Do inference on a one-second portion of the two-second data based on peak
    amplitude.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 基于峰值幅度对两秒数据中的一秒部分进行推理。
- en: 3\. Return to step 1.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 返回步骤1。
- en: In this new scheme, each sample placed into the queue is two seconds long, but
    there’s a one-second overlap between consecutive samples, as illustrated in [Figure
    15-5](nsp-venkitachalam503045-0030.xhtml#fig15-5). This way, even if a word is
    partially cut off in one sample, you’ll get the full word in the next sample.
    You’ll still run inference on only one-second clips, which you’ll center on the
    point in the two-second sample that has the highest amplitude value. This is the
    part of the sample most likely to contain a spoken word. You need the clips to
    be one second long for consistency with the training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新方案中，每个放入队列的样本都是两秒钟长的，但连续样本之间有一秒钟的重叠，如[图15-5](nsp-venkitachalam503045-0030.xhtml#fig15-5)所示。这样，即使一个词在一个样本中被部分截断，你也可以在下一个样本中获得完整的词。你仍然只会对一秒钟的片段进行推理，并且会以两秒钟样本中幅度最大的位置为中心来处理这些片段。这是最有可能包含语音单词的部分。你需要将片段长度保持为一秒钟，以保持与训练数据的一致性。
- en: '![](images/nsp-venkitachalam503045-f15005.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15005.jpg)'
- en: 'Figure 15-5: The two-frame overlapping scheme'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-5：两帧重叠方案
- en: Through this combination of multiprocessing and overlapping samples, you’ll
    design a speech recognition system that minimizes missing inputs and improves
    inference results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种多进程和重叠样本的结合，你将设计一个语音识别系统，最大限度地减少丢失的输入并提高推理结果。
- en: '[Requirements](nsp-venkitachalam503045-0008.xhtml#rah1703)'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[要求](nsp-venkitachalam503045-0008.xhtml#rah1703)'
- en: 'For this project, you’ll need to sign up with Google Colab to train your ML
    model. On the Raspberry Pi, you’ll need the following Python modules:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，你需要在Google Colab上注册，以便训练你的机器学习模型。在树莓派上，你需要以下Python模块：
- en: • `tflite_runtime` for running the TensorFlow inference
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `tflite_runtime`用于运行TensorFlow推理
- en: • `scipy` for computing the STFT of audio waveforms
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `scipy`用于计算音频波形的STFT
- en: • `numpy` arrays for handling audio data
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `numpy`数组用于处理音频数据
- en: • `pyaudio` for streaming audio data from the microphone input
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `pyaudio`用于从麦克风输入流式传输音频数据
- en: The installation for these modules is covered in [Appendix B](nsp-venkitachalam503045-0032.xhtml#appb).
    You’ll also use Python’s built-in `multiprocessing` module for running ML inference
    in a separate thread from the audio processing thread.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块的安装请参见[附录B](nsp-venkitachalam503045-0032.xhtml#appb)。你还将使用Python内置的`multiprocessing`模块来在与音频处理线程分开的线程中运行机器学习推理。
- en: 'In the hardware department, you’ll need the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件方面，你将需要以下设备：
- en: • One Raspberry Pi 3B+ or newer
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一台树莓派3B+或更新版本
- en: • One 5 V power supply for the Raspberry Pi
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一个5V电源供应给树莓派
- en: • One 16GB SD card
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一张16GB的SD卡
- en: • One single-channel USB microphone compatible with the Pi
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一只与树莓派兼容的单通道USB麦克风
- en: Various types of USB microphones are compatible with the Raspberry Pi. [Figure
    15-6](nsp-venkitachalam503045-0030.xhtml#fig15-6) shows an example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 各种类型的 USB 麦克风都可以与 Raspberry Pi 兼容。[图 15-6](nsp-venkitachalam503045-0030.xhtml#fig15-6)展示了一个例子。
- en: '![](images/nsp-venkitachalam503045-f15006.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15006.jpg)'
- en: 'Figure 15-6: A USB microphone for the Raspberry Pi'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-6：Raspberry Pi 的 USB 麦克风
- en: 'To check if your Pi can recognize your USB microphone, SSH into your Pi and
    run the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查你的 Pi 是否能识别 USB 麦克风，可以通过 SSH 连接到 Pi 并运行以下命令：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now plug your microphone into a USB port on the Pi. You should see something
    similar to the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将你的麦克风插入 Pi 的 USB 端口。你应该会看到类似以下的输出：
- en: '[26965.023138] usb 1-1.3: New USB device found, idVendor=cafe, idProduct=4010,
    bcdDevice= 1.00'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023138] usb 1-1.3: 新的 USB 设备已找到，idVendor=cafe，idProduct=4010，bcdDevice=
    1.00'
- en: '[26965.023163] usb 1-1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023163] usb 1-1.3: 新的 USB 设备字符串：Mfr=1，Product=2，SerialNumber=3'
- en: '[26965.023179] usb 1-1.3: Product: Mico'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023179] usb 1-1.3: 产品：Mico'
- en: '[26965.023194] usb 1-1.3: Manufacturer: Electronut Labs'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023194] usb 1-1.3: 厂商：Electronut Labs'
- en: '[26965.023209] usb 1-1.3: SerialNumber: 123456'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023209] usb 1-1.3: 序列号：123456'
- en: The information should match the specs of your microphone, indicating it’s been
    correctly identified.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 信息应该与麦克风的规格匹配，表明它已被正确识别。
- en: '[The Code](nsp-venkitachalam503045-0008.xhtml#rah1704)'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[代码](nsp-venkitachalam503045-0008.xhtml#rah1704)'
- en: 'The code for this project exists in two parts: the training portion, which
    you’ll run in Google Colab, and the inference portion, which you’ll run on your
    Raspberry Pi. We’ll examine these parts one at a time.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的代码分为两个部分：训练部分，你将在 Google Colab 中运行；推理部分，你将在 Raspberry Pi 上运行。我们将逐一检查这两个部分。
- en: '[Training the Model in Google Colab](nsp-venkitachalam503045-0008.xhtml#rbh1703)'
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[在 Google Colab 中训练模型](nsp-venkitachalam503045-0008.xhtml#rbh1703)'
- en: In this section, we’ll look at the Google Colab code needed to train the speech
    recognition model. I recommend working with Colab in the Chrome web browser. You’ll
    begin by getting set up and downloading the training dataset. Then you’ll run
    some code to get to know the data. You’ll clean up the data to prepare it for
    training and explore how to generate spectrograms from the data. Finally, you’ll
    put what you’ve learned to work by creating and training the model. The end result
    will be a *.tflite* file, a streamlined TensorFlow Lite version of the trained
    model that you can load onto your Raspberry Pi. You can also download this file
    from the book’s GitHub repository at [https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看在 Google Colab 中训练语音识别模型所需的代码。我建议在 Chrome 浏览器中使用 Colab。你将从设置环境并下载训练数据集开始。然后，你将运行一些代码来了解数据。你会清理数据以准备训练，并探索如何从数据生成频谱图。最后，你将通过创建和训练模型将所学知识付诸实践。最终结果将是一个
    *.tflite* 文件，这是训练模型的精简版 TensorFlow Lite，可以加载到你的 Raspberry Pi 上。你还可以从本书的 GitHub
    仓库 [https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml)
    下载该文件。
- en: A Google Colab notebook consists of a series of cells where you enter one or
    more lines of Python code. Once you’ve entered your desired code into a cell,
    you run it by clicking the Play icon in the top-left corner of the cell. Any output
    associated with that cell’s code will then appear beneath the cell. Throughout
    this section, each code listing will represent a complete Google Colab cell. The
    cell’s output, if any, will be shown in gray at the end of the listing, beneath
    a dashed line.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 笔记本由一系列单元格组成，你可以在其中输入一行或多行 Python 代码。输入所需的代码后，可以通过点击单元格左上角的播放图标来运行它。与该单元格代码相关的任何输出将显示在单元格下方。在本节中，每个代码示例代表一个完整的
    Google Colab 单元格。如果有输出，它将以灰色显示在示例的末尾，位于虚线下方。
- en: Setting Up
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'You begin your Colab notebook with some initial setup. First you import the
    required Python modules:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你开始你的 Colab 笔记本时进行一些初始设置。首先导入所需的 Python 模块：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the next cell, you do some initialization:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个单元格中，你进行一些初始化操作：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here you initialize the random functions you’ll be using to shuffle the input
    filenames.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你初始化了将用于打乱输入文件名的随机函数。
- en: 'Next, download the training data:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下载训练数据：
- en: data_dir = 'data/mini_speech_commands'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: data_dir = 'data/mini_speech_commands'
- en: data_path = pathlib.Path(data_dir)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: data_path = pathlib.Path(data_dir)
- en: filename = 'mini_speech_commands.zip'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: filename = 'mini_speech_commands.zip'
- en: url = "http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip"
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: url = "http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip"
- en: 'if not data_path.exists():'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not data_path.exists():'
- en: tf.keras.utils.get_file(filename, origin=url, extract=True, cache_dir='.',
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: tf.keras.utils.get_file(filename, origin=url, extract=True, cache_dir='.',
- en: cache_subdir='data')
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: cache_subdir='data')
- en: This cell downloads the Mini Speech Commands dataset from Google and extracts
    the data into a directory called *data*. Since you’re using Colab, the data will
    be downloaded to the filesystem on the cloud, not to your local machine, and when
    your session ends, these files will be deleted. While the session is still active,
    however, you don’t want to have to keep downloading the data every time you run
    the cell. The `tf.keras.utils.``get_file()` function caches the data so you won’t
    need to keep downloading it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单元格从Google下载Mini Speech Commands数据集，并将数据提取到一个名为*data*的目录中。由于你使用的是Colab，数据将被下载到云端的文件系统中，而不是本地计算机，当你的会话结束时，这些文件将被删除。然而，在会话仍然有效的情况下，你不希望每次运行单元格时都要重新下载数据。`tf.keras.utils.get_file()`函数会缓存数据，这样你就不需要一直下载它了。
- en: Getting to Know the Data
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 了解数据
- en: 'Before you start training your model, it would be useful to take a look at
    what you just downloaded to get to know your data. You can use Python’s `glob`
    module, which helps you find files and directories through pattern matching:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，查看一下你刚刚下载的数据，熟悉数据内容会很有帮助。你可以使用Python的`glob`模块，它通过模式匹配帮助你查找文件和目录：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ wav_file_names = glob.glob(data_dir + '/*/*')
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ wav_file_names = glob.glob(data_dir + '/*/*')
- en: ❷ np.random.shuffle(wav_file_names)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ np.random.shuffle(wav_file_names)
- en: print(len(wav_file_names))
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: print(len(wav_file_names))
- en: 'for file_name in wav_file_names[:5]:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'for file_name in wav_file_names[:5]:'
- en: print(file_name)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: print(file_name)
- en: '8000'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '8000'
- en: data/mini_speech_commands/down/27c30960_nohash_0.wav
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/down/27c30960_nohash_0.wav
- en: data/mini_speech_commands/go/19785c4e_nohash_0.wav
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/go/19785c4e_nohash_0.wav
- en: data/mini_speech_commands/yes/d9b50b8b_nohash_0.wav
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/yes/d9b50b8b_nohash_0.wav
- en: data/mini_speech_commands/no/f953e1af_nohash_3.wav
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/no/f953e1af_nohash_3.wav
- en: data/mini_speech_commands/stop/f632210f_nohash_0.wav
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/stop/f632210f_nohash_0.wav
- en: You use `glob` again, this time showing it the pattern `'/*/*'` to list all
    the files in the subdirectories ❶. Then you randomly shuffle the returned list
    of filenames to reduce any bias in the training data ❷. You print the total number
    of files found, as well as the first five filenames. The output indicates that
    there are 8,000 WAV files in the dataset, and it gives you some idea of how the
    files are named—for example, *f632210f_nohash_0.wav*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你再次使用`glob`，这次使用模式`'/*/*'`来列出子目录中的所有文件 ❶。然后你随机打乱返回的文件名列表，以减少训练数据中的任何偏差 ❷。你打印出找到的文件总数，以及前五个文件名。输出结果表明数据集中有8000个WAV文件，并且能让你大致了解文件的命名方式——例如，*f632210f_nohash_0.wav*。
- en: 'Next, take a look at some individual WAV files from the dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，查看数据集中的一些单独WAV文件：
- en: filepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav' ❶
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav' ❶
- en: rate, data = wavfile.read(filepath) ❷
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: rate, data = wavfile.read(filepath) ❷
- en: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
- en: filepath = 'data/mini_speech_commands/no/f953e1af_nohash_3.wav'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = 'data/mini_speech_commands/no/f953e1af_nohash_3.wav'
- en: rate, data = wavfile.read(filepath)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: rate, data = wavfile.read(filepath)
- en: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
- en: rate = 16000, data.shape = (13654,), data.dtype = int16
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: rate = 16000, data.shape = (13654,), data.dtype = int16
- en: rate = 16000, data.shape = (16000,), data.dtype = int16
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: rate = 16000, data.shape = (16000,), data.dtype = int16
- en: 'You set the name of a WAV file you want to look at ❶ and use the `wavefile`
    module from `scipy` to read data from the file ❷. Then you print the sampling
    rate, shape (number of samples), and type of the data. You do the same for a second
    WAV file. The output shows that the sampling rates of both the WAV files are 16,000,
    as expected, and that each sample is a 16-bit integer for both—also expected.
    However, the shape indicates the first file has only 13,654 samples, and this
    is a problem. To train the neural network, each WAV file needs to have the same
    length; in this case, you’d like each recording to be one second, or 16,000 samples,
    long. Unfortunately, not all the files in the dataset fit that standard. We’ll
    look at a solution to this problem shortly, but first, try plotting the data from
    one of these WAV files:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你设置一个WAV文件的名称，想要查看该文件❶，然后使用`wavefile`模块从`scipy`读取文件中的数据❷。接着，你打印采样率、形状（样本数）和数据类型。你对第二个WAV文件做相同的操作。输出结果显示，两个WAV文件的采样率都为16,000，符合预期，并且每个样本都是16位整数，这也是预期的。然而，形状表明第一个文件只有13,654个样本，这是一个问题。为了训练神经网络，每个WAV文件需要有相同的长度；在这种情况下，你希望每个录音文件的长度为一秒钟，即16,000个样本。不幸的是，数据集中的并非所有文件都符合这个标准。我们稍后会看看这个问题的解决方案，但首先，试着绘制其中一个WAV文件的数据：
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](images/nsp-venkitachalam503045-g15001.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-g15001.jpg)'
- en: You use `matplotlib` to create a plot of the audio waveform ❶. The WAV files
    in this dataset contain 16-bit signed data, which can range from −32,768 to +32,767\.
    The y-axis of the plot shows you that the data in this file ranges from only around
    −10,000 to +7,500\. The plot’s x-axis also underscores that the data is short
    of the necessary 16,000 samples—the axis runs only to 14,000.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用`matplotlib`创建音频波形图❶。该数据集中的WAV文件包含16位有符号数据，范围从−32,768到+32,767。图表的y轴显示，这个文件中的数据仅在大约−10,000到+7,500之间。图表的x轴也强调了数据不足16,000个样本——x轴只显示到14,000。
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Cleaning the Data
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据清理
- en: You’ve seen that the dataset needs to be standardized so that each clip is one
    second long. This type of preparatory work is called *data cleaning*, and you
    can do it by padding the audio data with zeros until it reaches a length of 16,000
    samples. You should clean the data further by *normalizing* it—mapping the value
    of each sample from range [−32,768, +32,767] to range [−1, 1]. This type of normalization
    is crucial for machine learning, as keeping the input data small and uniform will
    help the training. (For the mathematically curious, large numbers in the inputs
    will cause problems in the convergence of gradient descent algorithms used to
    train the data.)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到，数据集需要标准化，以便每个片段的长度为一秒钟。这类预处理工作称为*数据清理*，你可以通过用零填充音频数据，直到其长度达到16,000个样本来完成这项工作。你还应该进一步清理数据，进行*归一化*——将每个样本的值从范围[−32,768,
    +32,767]映射到[−1, 1]的范围。这种类型的归一化对机器学习至关重要，因为保持输入数据小且一致有助于训练。（对于那些对数学感兴趣的人来说，输入数据中较大的数值会导致梯度下降算法在训练数据时收敛出现问题。）
- en: As an example of data cleaning, here you apply both padding and normalization
    to the WAV file you viewed in the previous listing. Then you plot the results
    to confirm that the cleaning has worked.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据清理的一个例子，你将在上一段中查看的WAV文件上应用填充和归一化操作。然后，你绘制结果以确认清理工作已经完成。
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](images/nsp-venkitachalam503045-g15002.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-g15002.jpg)'
- en: You create a 16-bit `numpy` array of length 16,000 filled with zeros ❶. Then
    you use the array slice operator, `[:]`, to copy the contents of the too-short
    WAV file into the beginning of the array ❷. Here `data.shape[0]` gives you the
    number of samples in the original WAV file, since `data.shape` is a tuple in the
    form `(13654,)`. You now have one second of WAV data, consisting of the original
    audio data followed by a padding of zeros as needed. You next create a normalized
    version of the data by dividing the values in the array by 32,768, the maximum
    value a 16-bit integer could have ❸. Then you plot the data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建一个长度为16,000的16位`numpy`数组，填充为零❶。然后你使用数组切片操作符`[:]`，将过短的WAV文件的内容复制到数组的开头❷。这里，`data.shape[0]`给出了原始WAV文件中的样本数，因为`data.shape`是一个形如`(13654,)`的元组。现在你得到了一个一秒钟的WAV数据，包含了原始的音频数据，后面跟着必要的零填充。接下来，你通过将数组中的值除以32,768（16位整数的最大值）来创建数据的归一化版本❸。然后，你绘制数据。
- en: The x-axis of the output shows that the data has been padded to extend to 16,000
    samples, with the values from around 14,000 to 16,000 all being zero. Also, the
    y-axis shows that the values have all been normalized to fall nicely within the
    range of (−1, 1).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的 x 轴显示数据已经被填充，扩展到了 16,000 个样本，其中从 14,000 到 16,000 的值都为零。同时，y 轴显示所有的值都已被归一化，恰好落在（−1,
    1）的范围内。
- en: Looking at Spectrograms
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查看频谱图
- en: 'As we’ve discussed, you won’t be training your model on the raw data from the
    WAV files. Instead, you’ll generate spectrograms of the files and use them to
    train the model. Here’s an example of how to generate a spectrogram:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，你不会在 WAV 文件的原始数据上训练模型。相反，你将生成文件的频谱图，并用它们来训练模型。下面是生成频谱图的一个示例：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training the Model
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练模型
- en: You’re now ready to turn your attention to training the ML model, and that largely
    means leaving behind Python libraries like `numpy` and `scipy` in favor of TensorFlow
    methods and data structures like `tf.Tensor` and `tf.data.Dataset`. You’ve been
    using `numpy` and `scipy` so far because they’ve provided a convenient way to
    explore the speech commands dataset, and in fact you could continue using them,
    but then you’d miss out on the optimization opportunities provided by TensorFlow,
    which is designed for large-scale ML systems. You’ll find that TensorFlow has
    near-identical functions for most of the computations you’ve done until now, so
    the transition will be smooth. For our purposes, when I refer to a *tensor* in
    the upcoming discussion, understand that it’s similar to talking about a `numpy`
    array.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好将注意力转向训练机器学习模型，这主要意味着要抛弃像 `numpy` 和 `scipy` 这样的 Python 库，转而使用像 `tf.Tensor`
    和 `tf.data.Dataset` 这样的 TensorFlow 方法和数据结构。到目前为止，你一直在使用 `numpy` 和 `scipy`，因为它们提供了一个方便的方式来探索语音命令数据集，实际上你仍然可以继续使用它们，但那样你就错过了
    TensorFlow 提供的优化机会，TensorFlow 是为大规模机器学习系统设计的。你会发现，TensorFlow 为大多数你到目前为止做的计算提供了几乎相同的函数，因此过渡会非常平滑。就我们的讨论而言，当我提到
    *张量* 时，你可以理解为它类似于 `numpy` 数组。
- en: 'To train the ML model, you need to be able to extract the spectrogram and label
    ID (the spoken command) from the filepath of an input audio file. For that, first
    create a function that computes an STFT:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练机器学习模型，你需要能够从输入音频文件的文件路径中提取频谱图和标签 ID（即说出的命令）。为此，首先创建一个计算 STFT 的函数：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The function takes in `x`, the data extracted from a WAV file, and computes
    its STFT using `scipy`, as before ❶. Then you convert the returned `numpy` array
    to a `tf.Tensor` object and return the result ❷. There is, in fact, a TensorFlow
    method called `tf.signal.stft()` that’s similar to the `scipy.signal.stft()` method,
    so why not use it? The answer is that the TensorFlow method won’t be available
    on the Raspberry Pi, where you’ll be using the slimmed-down TensorFlow Lite interpreter.
    Any preprocessing you do during the training phase should be identical to any
    preprocessing you do during inference, so you need to ensure that you use the
    same functions in Colab as you’ll use on the Pi.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接收 `x`，即从 WAV 文件中提取的数据，并像之前一样使用 `scipy` 计算其 STFT ❶。然后你将返回的 `numpy` 数组转换为
    `tf.Tensor` 对象并返回结果 ❷。事实上，有一个类似于 `scipy.signal.stft()` 方法的 TensorFlow 方法叫做 `tf.signal.stft()`，那么为什么不使用它呢？答案是，TensorFlow
    方法在树莓派上不可用，在那里你将使用精简版的 TensorFlow Lite 解释器。在训练阶段进行的任何预处理，应该与推理阶段的预处理相同，因此你需要确保在
    Colab 中使用的函数与在树莓派上使用的函数相同。
- en: Now you can make use of your `stft()` function in a helper function that extracts
    the spectrogram and label ID from a filepath.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在一个辅助函数中使用 `stft()` 函数，从文件路径中提取频谱图和标签 ID。
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You start by reading the file using `tf.io.read_file()` and decoding the WAV
    format using the `tf.audio.decode_wav()` function. (The latter is comparable to
    the `scipy.io.wavfile.read()` function you used previously.) You then use `tf.squeeze()`
    to change the shape of the `data` tensor from (*N*, 1) to (*N*, ), which is required
    for functions coming ahead. Next, you create a tensor for zero-padding the data
    ❶. Tensors are immutable objects, however, so you can’t copy the WAV data directly
    into a tensor full of zeros, as you did earlier with `numpy` arrays. Instead,
    you create a tensor with the exact number of zeros you need to pad the data, and
    then you concatenate it with the data tensor ❷.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: You next use `tf.py_function()` to call the `stft()` function you defined earlier
    ❸. In this call, you also need to specify the input and the data type of the output.
    This is a common method for calling a non-TensorFlow function from TensorFlow.
    You then do some reshaping of the tensor returned by `stft()`. First you use `set_shape()`
    to reshape it to (129, 124), which is necessary because you’re going from a TensorFlow
    function to a Python function and back. Then you run `tf.expand_dims(spec, -1)`
    to add a third dimension, going from (129, 124) to (129, 124, 1). The extra dimension
    is needed for the neural network model you’ll be building. Finally, you extract
    the label (for example, `'no'`) associated with the filepath ❹ and convert the
    label string to the integer `label_id` ❺, which is the index of the string in
    your `commands` list.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to get the input files ready for training. Recall that you had
    8,000 audio files in the subdirectories and that you randomly shuffled their filepath
    strings and put them into a list called `wav_file_names`. Now you’ll partition
    the data into three: 80 percent, or 6,400 files, for training; 10 percent, or
    800 files, for validation; and the other 10 percent for testing. Such partitioning
    is a common practice in machine learning. Once the model is trained using training
    data, you can use the validation data to tweak the model’s accuracy by changing
    the hyperparameters. The testing data is used only for checking the final accuracy
    of the (tweaked) model.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you load the filepath strings into TensorFlow `Dataset` objects. These
    objects are critical to working with TensorFlow; they hold your input data and
    allow for data transformations, and all this can happen at a large scale:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Take a look at what you just created:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'for val in train_ds.take(5):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: print(val)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(b'data/mini_speech_commands/stop/b4aa9fef_nohash_2.wav', shape=(),
    dtype=string)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(b'data/mini_speech_commands/stop/962f27eb_nohash_0.wav', shape=(),
    dtype=string)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: --snip--
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: tf.Tensor(b'data/mini_speech_commands/left/cf87b736_nohash_1.wav', shape=(),
    dtype=string)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `Dataset` object contains a bunch of tensors of type `string`, each holding
    a filepath. What you really need, however, are the `(spec, label_id)` pairs corresponding
    to those filepaths. You create those here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You use `map()` to apply your `get_spec_label_pair()` function to each `Dataset`
    object. This technique of mapping a function to a list of things is common in
    computing. Essentially, you’re going through each filepath in the `Dataset` object,
    calling `get_spec_label_pair()` on it, and storing the resulting `(spec, label_id)`
    pair in a new `Dataset` object.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you further prepare the dataset for training by splitting it up into smaller
    batches:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here you set the training and validation datasets to have a batch size of 64\.
    This is a common technique for speeding up the training process. If you tried
    to work with all 6,400 training samples and 800 validation samples at once, it
    would require a huge amount of memory and would slow down the training.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you’re finally ready to create your neural network model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: model.compile(
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: optimizer=tf.keras.optimizers.Adam(),
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: loss=`tf.keras.losses`.SparseCategoricalCrossentropy(from_logits=True),
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: metrics=['accuracy'],
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: test_audio = []
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: test_labels = []
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '❶ for audio, label in test_ds:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: test_audio.append(audio.numpy())
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: test_labels.append(label.numpy())
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: ❷ test_audio = np.array(test_audio)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: test_labels = np.array(test_labels)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ❸ y_pred = np.argmax(model.predict(test_audio), axis=1)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: y_true = test_labels
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: ❹ test_acc = sum(y_pred == y_true) / len(y_true)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f''Test set accuracy: {test_acc:.0%}'')'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 25/25 [==============================] - 1s 35ms/step
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Test set accuracy: 84%'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: You first fill in two lists, `test_audio` and `test_labels`, by iterating through
    the test dataset `test_ds` ❶. Then you create `numpy` arrays from these lists
    ❷ and run inference on the data ❸. You compute the test accuracy by summing up
    the number of times the predictions matched the true value and dividing them by
    the total number of items ❹. The output shows an accuracy of 84 percent. Not perfect,
    but good enough for this project.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Exporting the Model to the Pi
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Congratulations! You have a fully trained machine learning model. Now you need
    to get it from Colab onto your Raspberry Pi. The first step is to save it:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This saves the model to a file on the cloud called *audioml.sav*. Next, convert
    that file to the TensorFlow Lite format so you can use it on your Pi:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You create a `TFLiteConverter` object, passing in the saved model filename ❶.
    Then you do the conversion ❷ and write the simplified TensorFlow model to a file
    called *audioml.tflite* ❸. Now you need to download this *.tflite* file from Colab
    onto your computer. Running the following snippet will give you a browser prompt
    to save the *.tflite* file:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once you have the file, you can transfer it to your Raspberry Pi using SSH as
    we’ve discussed in other chapters.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[Using the Model on the Raspberry Pi](nsp-venkitachalam503045-0008.xhtml#rbh1704)'
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we’ll turn our attention to the Raspberry Pi portion of the code. This code
    uses parallel processing to take in audio data from the microphone, prepare that
    data for your trained ML model, and show the data to the model to perform inference.
    As usual, you can write the code on your local machine and then transfer it to
    your Pi via SSH. To view the complete code, see [“The Complete Code”](nsp-venkitachalam503045-0030.xhtml#ah1708)
    on [page 389](nsp-venkitachalam503045-0030.xhtml#p389). You can also download
    the code from [https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Start by importing the required modules:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, you initialize some parameters that are defined as global variables:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`VERBOSE_DEBUG` is a flag you’ll use in many places in the code. For now, you
    set it to `False`, but if set to `True` (via a command line option), it will print
    out a lot of debugging information.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: NOTE I’ve omitted the `print()` statements for debugging from the code listings
    that follow. You can find them in the full code listing and on the book’s GitHub
    repository.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The next global variables are for working with the audio input. `CHUNK` sets
    the number of data samples read at a time using `PyAudio`, and `FORMAT` specifies
    that the audio data will consist of 16-bit integers. You set `CHANNELS` to `1`,
    since you’ll be using a single-channel microphone, and `SAMPLE_RATE` to `16000`
    for consistency with the ML training data. `RECORD_SECONDS` indicates that you’ll
    be grouping the audio into one-second increments (which you’ll stitch together
    into overlapping two-second clips, as discussed earlier). You calculate the number
    of chunks in each one-second recording as `NCHUNKS`. You’ll use `ND` and `NDH`
    to implement the overlapping technique—more on that later.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you initialize the device index number of the microphone to `-1` ❶.
    You’ll need to update this value at the command line once you know your microphone’s
    index. Here’s a function to help you figure that out. You’ll be able to call this
    function as a command line option.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You initialize `PyAudio` ❶ and get a count of the audio devices it detects ❷.
    Then you iterate through the devices ❸. For each one, you retrieve information
    about the device using `get_device_info_by_index()` and print out devices with
    one or more input channels—that is, microphones. You finish by cleaning up `PyAudio`
    ❹.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what a typical output of the function looks like:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This indicates there’s an input device called Mico with a default sample rate
    of 16,000 and an index of `1`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Taking In Audio Data
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the main tasks for the Pi is to continuously take in the audio input
    from the microphone and break it up into clips that you can run inference on.
    You create a `get_live_input()` function for this purpose. It takes in the `interpreter`
    object needed to work with the TensorFlow Lite model. Here’s the start of the
    function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we discussed in [“How It Works,”](nsp-venkitachalam503045-0030.xhtml#ah1702)
    you’ll need to use separate processes for reading the audio data and doing the
    inference to avoid missing any input. You create a `multiprocessing.``Queue` object
    that the processes will use to communicate with each other ❶. Then you create
    the inference process using `multiprocessing.``Process()` ❷. You specify the name
    of the handler function for the process as `inference_process`, which takes the
    `dataq` and `interpreter` objects as arguments (we’ll view this function later).
    You next start the process so the inference will run parallel to the data capture.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: You continue the `get_live_input()` function by initializing `PyAudio:`
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You create a `PyAudio` object `p` ❶ and open an audio input stream ❷, using
    some of your global variables as parameters. Then you discard the first one second
    of data ❸. This is to disregard any spurious data that comes in when the microphone
    is enabled for the first time.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you’re ready to start reading the data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You initialize `count` to `0` ❶. You’ll use this variable to keep track of the
    number of one-second frames of audio data read in. Then you initialize a 16-bit
    array `inference_data` with zeros ❷. It has `ND` elements, which corresponds to
    two seconds of audio. You next enter a `while` loop to process the audio data
    continuously ❸. In it, you use a `for` loop ❹ to read in one second of audio data,
    one chunk at a time, appending those chunks to the list `chunks`. Once you have
    a full second of data, you convert it into a `numpy` array ❺.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Next, still within the `while` loop started in the previous listing, you implement
    the technique we discussed in [“How It Works”](nsp-venkitachalam503045-0030.xhtml#ah1702)
    to create overlapping two-second audio clips. You get help from your `NDH` global
    variable.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The very first time a one-second frame is read in, it’s stored in the first
    half of `inference_data` ❶. The next frame that comes in is stored in the second
    half of `inference_data` ❷. Now you have a full two seconds of audio data, so
    you put `inference_data` into the queue for the inference process to pick it up
    ❸. For every subsequent frame, the second half of the data is moved to the first
    half of `inference_data` ❹, the new data is set to the second half ❺, and `inference_data`
    is added to the queue ❻. This creates the desired one-second overlap between each
    consecutive two-second audio clip.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'The `while` loop occurs inside a `try` block. To exit the loop, you just need
    to press CTRL-C and trigger the following `except` block:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This `except` block performs some basic cleanup by stopping and closing the
    stream and by terminating `PyAudio`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Audio Data
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, you’ll create a few functions to prepare the audio data for inference.
    First is `process_audio_data()`, which takes in a raw two-second clip of audio
    data pulled from the queue and extracts the most interesting one second of audio
    from it, based on peak amplitude. We’ll look at this function across several listings:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You want to skip doing any inference on the microphone audio input if nobody
    is talking. There will always be some noise in the environment, however, so you
    can’t simply look for the signal to be 0\. Instead, you’ll skip inference if the
    peak-to-peak amplitude (the difference between the highest value and the lowest
    value) of the audio is below a certain threshold. For this, you first divide the
    audio by `32768` to normalize it to a range of (−1, 1), and you pass the result
    to `np.ptp()` to get the peak-to-peak amplitude ❶. The normalization makes it
    easier to express the threshold as a fraction. You return an empty list (which
    will bypass the inference process) if the peak-to-peak amplitude is below `0.3`
    ❷. You may need to adjust this threshold value depending on the noise level of
    your environment.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The `process_audio_data()` function continues with another technique for normalizing
    any audio data that won’t be skipped:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: When you normalized the data before skipping quiet audio samples, you divided
    the audio by 32,768, the maximum possible value of a 16-bit signed integer. In
    most cases, however, the peak amplitude of the audio data will be well below this
    value. Now you want to normalize the audio such that its maximum amplitude, whatever
    that may be, is scaled to 1\. To do this, you first determine the peak amplitude
    in the audio signal and then divide the signal by that amplitude value ❶. Then
    you compute the new peak-to-peak value of the normalized audio ❷ and use this
    value to scale and center the data ❸. Specifically, the expression `(waveform
    – np.min(waveform))/PTP` will scale the waveform values to the range (0, 1). Multiplying
    this by 2 and subtracting 1 will put the values in the range (−1, 1), which is
    what you need.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the function extracts one second of audio from the data:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You want to make sure you’re getting the most interesting one second of the
    data, so you find the array index where the audio amplitude is at the maximum
    ❶. Then you try to grab 8,000 values before ❷ and after ❸ this index to get a
    full second of data, using `max()` and `min()` to ensure that the start and end
    indices don’t fall out of range of the original clip. You use slicing to extract
    the relevant audio data ❹. Because of the `max()` and `min()` operations, you
    may end up with less than 16,000 samples, but the neural network strictly requires
    each input to be 16,000 samples long. To address this problem, you pad the data
    with zeros, using the same `numpy` techniques you saw during training. Then you
    return the result.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-7](nsp-venkitachalam503045-0030.xhtml#fig15-7) summarizes the `process_audio_data()`
    function by showing an example waveform at the various stages of processing.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/nsp-venkitachalam503045-f15007.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-7: The audio preparation process at various stages'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The top waveform in [Figure 15-7](nsp-venkitachalam503045-0030.xhtml#fig15-7)
    shows the unprocessed audio. The second waveform shows the audio with the values
    normalized to range (−1, 1). The third waveform shows the audio after a shift
    and scale—notice on the y-axis how the waveform now fills the entire (−1, 1) range.
    The fourth waveform consists of 16,000 samples extracted from the third one, centered
    on the peak amplitude.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need a `get_spectrogram()` function for computing the spectrogram
    of the audio data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You call your `process_audio_data()` function to prepare the audio ❶. If the
    function returns an empty list (because the audio is too quiet), `get_spectrogram()`
    returns an empty list as well ❷. Next, you compute the spectrogram with `signal`.`stft()`
    from `scipy`, exactly as you did when training the model ❸. You then calculate
    the absolute value of the STFT ❹ to convert from complex numbers—again, as you
    did during training—and return the result.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Running Inference
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The heart of this project is using your trained model to run inference on the
    incoming audio data and identify any spoken commands. Recall that this occurs
    in a separate process from the code for taking in audio data from the microphone.
    Here’s the handler function that coordinates this process:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The inference process runs continuously inside a `while`. Within this loop,
    you check if there’s any data in the queue ❶, and if so, you retrieve it ❷. Then
    you run inference on it with the `run_inference()` function, which we’ll look
    at next, but only if the `success` flag is `False` ❸. This flag keeps you from
    responding to the same speech command twice. Recall that because of the overlap
    technique, the second half of one audio clip will be repeated as the first half
    of the next clip. This lets you catch any audio commands that might be split across
    two frames, but it means that once you have a successful inference, you should
    skip the next element in the queue because it will have a portion of the audio
    from the previous element. When you do a skip like this, you reset `success` to
    `False` ❹ to start running inference again on the next piece of data that comes
    in.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the `run_inference()` function, where the inference is actually
    carried out:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The function takes in the raw audio data (`waveform`) for interacting with
    your TensorFlow Lite model (`interpreter`). You call `get_spectrogram()` to process
    the audio and generate the spectrogram ❶, and if the audio was too quiet, you
    return `False`. Then you get the input ❷ and output ❸ details from the TensorFlow
    Lite interpreter. These tell you what the model is expecting as input and what
    you can expect from it as output. This is what `input_details` looks like:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[{''name'': ''serving_default_input_5:0'', ''index'': 0, ''shape'': array([1,
    129, 124,   1]),'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '''shape_signature'': array([ -1, 129, 124,   1]), ''dtype'': <class ''numpy.float32''>,'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '''quantization'': (0.0, 0), ''quantization_parameters'': {''scales'': array([],
    dtype=float32),'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '''zero_points'': array([], dtype=int32), ''quantized_dimension'': 0}, ''sparsity_parameters'':
    {}}]'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that `input_details` is an array with a dictionary inside it. The `''shape''`
    entry is especially of interest: `array([1, 129, 124, 1])`. You’ve already ensured
    that your spectrogram, which will be the input to the interpreter, is shaped to
    this value. The `''index''` entry is just the index of the tensor in the tensor
    list inside the interpreter, and `''dtype''` is the expected data type of the
    input, which in this case is `float32`, a signed 32-bit float. You’ll need to
    reference both `''index''` and `''dtype''` later in the `run_inference()` function.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s `output_details`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[{''name'': ''StatefulPartitionedCall:0'', ''index'': 17, ''shape'': array([1,
    8]), ''shape_signature'':'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'array([-1,  8]), ''dtype'': <class ''numpy.float32''>, ''quantization'': (0.0,
    0),'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '''quantization_parameters'': {''scales'': array([], dtype=float32), ''zero_points'':'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'array([], dtype=int32), ''quantized_dimension'': 0}, ''sparsity_parameters'':
    {}}]'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Notice the `'shape'` entry in this dictionary. It shows that the output will
    be an array of shape (1, 8). The shape corresponds to the label IDs of the eight
    speech commands.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'You continue the `run_inference()` function by actually running inference on
    the input data:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'First you convert the spectrogram data to 32-bit floating point values ❶. Recall
    that your audio data started as 16-bit integers. The scaling and other processing
    operations converted the data to 64-bit floats, but as you saw in `input_details`,
    the TensorFlow Lite model requires 32-bit floats, which is the reason for the
    conversion. You next set the input value to the appropriate tensor inside the
    interpreter ❷. Here the `[0]` accesses the first (and only) element in `input_details`,
    which as you saw is a dictionary, and `[''index'']` retrieves the value under
    that key in the dictionary to specify which tensor you’re setting. You run inference
    on the input using the `invoke()` method ❸. Then you retrieve the output tensor
    using similar indexing to the input ❹ and get the output itself by extracting
    the first element from the `output_data` array ❺. (Since you provided only one
    input, you expect only one output.) Here’s an example of what `yvals` looks like:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[  6.640185  -26.032831  -26.028618  8.746256  62.545185  -0.5698182  -15.045679  -29.140179 ]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'These eight numbers correspond to the eight commands you trained the model
    with. The values indicate the likelihood of the input data being each word. In
    this particular array, the value at index `4` is by far the largest, so that’s
    what the neural network is predicting as the most probable answer. Here’s how
    you interpret the result:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You define a `commands` list in the same order as you used during training.
    It’s important to keep the order consistent across training and inference, or
    you’ll end up misinterpreting the results! Then you use `np.``argmax()` to get
    the index of the highest value in the output data and use that index to pick up
    the corresponding string from `commands`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Writing the main() Function
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let’s look at the `main()` function, which brings everything together:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You start by setting `VERBOSE_DEBUG` as a global, since you’ll be setting it
    in this function and don’t want it to be treated as a local variable. Then you
    create a familiar `argparse.ArgumentParser` object and add a mutually exclusive
    group to the parser ❶, since some of your command line options won’t be compatible
    with each other. Those are the `--list` option ❷, which will list all the `PyAudio`
    devices so you can get your microphone’s index number; the `--input` option ❸,
    which lets you specify a WAV file to use as input instead of live data from the
    microphone (useful for testing); and the `--index` option ❹, which starts capturing
    audio and running inference using the microphone with the specified index. You
    also add the non–mutually exclusive `--verbose` option ❺ to print out detailed
    debug information as the program is run.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you create the TensorFlow Lite interpreter so you can use the ML model:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here you create an `Interpreter` object, passing it the *audioml.tflite* file
    with the model you created during training. Then you call `allocate_tensors()`
    to prepare the necessary tensors for running the inference.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main()` function finishes with branches for the different command line
    arguments:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If the `--input` command line option is used, you get the name of the WAV file
    ❶ and read its contents ❷. The resulting data is passed along for inference ❸.
    If the `--list` option is used, you call your `list_devices()` function ❹. If
    the `--index` option is used, you parse the device index ❺ and start processing
    live audio by calling the `get_live_input()` function ❻.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[Running the Speech Recognition System](nsp-venkitachalam503045-0008.xhtml#rah1705)'
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run the project, gather your Python code and the *audioml.tflite* file into
    a folder on your Pi. For testing, you can also download the *right.wav* file from
    the book’s GitHub repository and add that to the folder. You can work with your
    Pi via SSH, as explained in [Appendix B](nsp-venkitachalam503045-0032.xhtml#appb).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'First, try using the `--input` command line option to run inference on a WAV
    file:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here’s the output:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Notice that the program has correctly identified the *right* command recorded
    on the WAV file ❶.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Now plug your microphone into the Pi and use the `--list` option to determine
    its index number, as shown here:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Your output should be similar to the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In this example, the microphone has index `1`. Use that number to run the `--index`
    command to do some live speech detection! Here’s an example run:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: After starting the program and getting the “Listening . . .” prompt, I spoke
    the words *left* and *right*. The output at ❶ and ❷ indicates that the program
    was able to identify the commands correctly.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Try running the program with the `--verbose` option to see more information
    about how it’s working. Also, try speaking different commands in quick succession
    to verify whether the multiprocessing and overlapping techniques are working.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[Summary](nsp-venkitachalam503045-0008.xhtml#rah1706)'
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter introduced you to the world of machine learning. You learned how
    to train a deep neural network to recognize speech commands using the TensorFlow
    framework, and you converted the resulting model to a TensorFlow Lite format for
    use on a resource-constrained Raspberry Pi. You also learned about spectrograms
    and the importance of processing input data before ML training. You practiced
    using Python multiprocessing, reading USB microphone input on a Raspberry Pi using
    `PyAudio`, and running a TensorFlow Lite interpreter for ML inference.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[Experiments!](nsp-venkitachalam503045-0008.xhtml#rah1707)'
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1\. Now that you know how to process speech commands on a Raspberry Pi, you
    can build an assistive device that responds to those commands by doing more than
    printing out the identified words. For example, you could use the commands *left*,
    *right*, *up*, *down*, *stop*, and *go* to control a camera (or laser!) mounted
    on a pan/tilt mount. Hint: you''ll need to retrain the ML model with just these
    six commands. You’ll also need to get a two-axis pan/tilt bracket with two servo
    motors attached. The servos will be connected to the Raspberry Pi and controlled
    based on the inference results.'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Read about the *mel spectrogram*, a variant of the spectrogram you used
    for this project that’s better suited for human speech data.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Try modifying the neural network by adding or removing some layers. For
    example, remove the second Conv2D layer. See how the changes affect the training
    accuracy of the model and the inference accuracy on the Pi.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. This project used an ad hoc neural network, but there are also pretrained
    neural networks available that you could leverage. For example, read up on MobileNet
    V2\. What changes are needed to adapt your project to use this network instead?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[The Complete Code](nsp-venkitachalam503045-0008.xhtml#rah1708)'
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here’s a complete listing of the code that goes on the Raspberry Pi, including
    the `print()` statements for verbose debugging. The Google Colab notebook code
    can be found at [https://github.com/mkvenkit/pp2e/blob/main/audioml/audioml.ipynb](https://github.com/mkvenkit/pp2e/blob/main/audioml/audioml.ipynb).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
