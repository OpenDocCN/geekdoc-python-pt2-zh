<html><head></head><body>
<h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_95"/><span class="big">5</span><br/>FINDING PLUTO</h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>&#13;
<p class="noindent">According to Woody Allen, 80 percent of success is just showing up. This certainly describes the success of Clyde Tombaugh, an untrained Kansas farm boy growing up in the 1920s. With a passion for astronomy but no money for college, he took a stab in the dark and mailed his best astronomical sketches to Lowell Observatory. To his great surprise, he was hired as an assistant. A year later, he had discovered Pluto and gained eternal glory!</p>&#13;
<p class="indent">Percival Lowell, the famous astronomer and founder of Lowell Observatory, had postulated the presence of Pluto based on perturbations in the orbit of Neptune. His calculations were wrong, but by pure coincidence, he correctly predicted Pluto’s orbital path. Between 1906 and his death in 1916, he had photographed Pluto twice. Both times, his team failed to notice it. Tombaugh, on the other hand, photographed <em>and</em> recognized Pluto in January 1930, after only a year of searching (<a href="ch05.xhtml#ch05fig1">Figure 5-1</a>).</p>&#13;
<div class="image"><img src="../images/fig05_01.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig1"/>Figure 5-1: Discovery plates for Pluto, indicated by the arrow</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_96"/>What Tombaugh accomplished was extraordinary. Without computers, the methodology he followed was impractical, tedious, and demanding. He had to photograph and re-photograph small parts of the sky night after night, usually in a freezing cold dome shaken by icy winds. He then developed and sifted through all the negatives, searching for the faintest signs of movement within crowded star fields.</p>&#13;
<p class="indent">Although he lacked a computer, he did have a state-of-the-art device, known as a <em>blink comparator</em>, that let him rapidly switch between negatives from successive nights. As viewed through the blink comparator, the stars remained stationary, but Pluto, a moving object, flashed on and off like a beacon.</p>&#13;
<p class="indent">In this chapter, you’ll first write a Python program that replicates an early 20th-century blink comparator. Then you’ll move into the 21st century and write a program that automates the detection of moving objects using modern computer vision techniques.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>In 2006, the International Astronomical Union reclassified Pluto as a dwarf planet. This was based on the discovery of other Pluto-sized bodies in the Kuiper Belt, including one—Eris—that is volumetrically smaller but 27 percent more massive than Pluto.</em></p>&#13;
</div>&#13;
<h3 class="h3ab" id="ch00lev1sec34"><strong>Project #7: Replicating a Blink Comparator</strong></h3>&#13;
<p class="noindent">Pluto may have been photographed with a telescope, but it was found with a microscope. The blink comparator (<a href="ch05.xhtml#ch05fig2">Figure 5-2</a>), also called the <em>blink microscope</em>, lets the user mount two photographic plates and rapidly switch from looking at one to the other. During this “blinking,” any object that changes position between photographs will appear to jump back and forth.</p>&#13;
<div class="image"><img src="../images/fig05_02.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig2"/>Figure 5-2: A blink comparator</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_97"/>For this technique to work, the photos need to be taken with the same exposure and under similar viewing conditions. Most importantly, the stars in the two images must line up perfectly. In Tombaugh’s day, technicians achieved this through painstaking manual labor; they carefully guided the telescope during the hour-long exposures, developed the photographic plates, and then shifted them in the blink comparator to fine-tune the alignment. Because of this exacting work, it would sometimes take Tombaugh a week to examine a single pair of plates.</p>&#13;
<p class="indent">In this project, you’ll digitally duplicate the process of aligning the plates and blinking them on and off. You’ll work with bright and dim objects, see the impact of different exposures between photos, and compare the use of positive images to the negative ones that Tombaugh used.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that aligns two nearly identical images and displays each one in rapid succession in the same window.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec24"><strong><em>The Strategy</em></strong></h4>&#13;
<p class="noindent">The photos for this project are already taken, so all you need to do is align them and flash them on and off. Aligning images is often referred to as image <em>registration</em>. This involves making a combination of vertical, horizontal, or rotational transformations to one of the images. If you’ve ever taken a panorama with a digital camera, you’ve seen registration at work.</p>&#13;
<p class="indent">Image registration follows these steps:</p>&#13;
<ol>&#13;
<li class="noindent">Locate distinctive features in each image.</li>&#13;
<li class="noindent">Numerically describe each feature.</li>&#13;
<li class="noindent"><span epub:type="pagebreak" id="page_98"/>Use the numerical descriptors to match identical features in each image.</li>&#13;
<li class="noindent">Warp one image so that matched features share the same pixel locations in both images.</li>&#13;
</ol>&#13;
<p class="noindent">For this to work well, the images should be the same size and cover close to the same area.</p>&#13;
<p class="indent">Fortunately, the OpenCV Python package ships with algorithms that perform these steps. If you skipped <a href="ch01.xhtml">Chapter 1</a>, you can read about OpenCV on <a href="ch01.xhtml#page_6">page 6</a>.</p>&#13;
<p class="indent">Once the images are registered, you’ll need to display them in the same window so that they overlay exactly and then loop through the display a set number of times. Again, you can easily accomplish this with the help of OpenCV.</p>&#13;
<h4 class="h4" id="ch00lev2sec25"><strong><em>The Data</em></strong></h4>&#13;
<p class="noindent">The images you’ll need are in the <em>Chapter_5</em> folder in the book’s supporting files, downloadable from <em><a href="https://nostarch.com/real-world-python/">https://nostarch.com/real-world-python/</a></em>. The folder structure should look like <a href="ch05.xhtml#ch05fig3">Figure 5-3</a>. After downloading the folders, don’t change this organizational structure or the folder contents and names.</p>&#13;
<div class="image"><img src="../images/fig05_03.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig3"/>Figure 5-3: The folder structure for Project 7</p>&#13;
<p class="indent">The <em>night_1</em> and <em>night_2</em> folders contain the input images you’ll use to get started. In theory, these would be images of the same region of space taken on different nights. The ones used here are the same star field image to which I’ve added an artificial <em>transient</em>. A transient, short for <em>transient astronomical event</em>, is a celestial object whose motion is detectable over relatively short time frames. Comets, asteroids, and planets can all be considered transients, as their movement is easily detected against the more static background of the galaxy.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch05table1">Table 5-1</a> briefly describes the contents of the <em>night_1</em> folder. This folder contains files with <em>left</em> in their filenames, which means they should go on the left side of a blink comparator. The images in the <em>night_2</em> folder contain <em>right</em> in the filenames and should go on the other side.<span epub:type="pagebreak" id="page_99"/></p>&#13;
<p class="tabcap"><a id="ch05table1"/><strong>Table 5-1:</strong> Files in the <em>night_1</em> folder</p>&#13;
<table class="topbot-d">&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Filename</p></th>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Description</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba"><em>1_bright_transient_left.png</em></p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Contains a large, bright transient</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba"><em>2_dim_transient_left.png</em></p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Contains a dim transient a single pixel in diameter</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba"><em>3_diff_exposures_left.png</em></p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Contains a dim transient with an overexposed background</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba"><em>4_single_transient_left.png</em></p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Contains a bright transient in left image only</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba"><em>5_no_transient_left.png</em></p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Star field with no transient</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-h1"><p class="taba"><em>6_bright_transient_neg_left.png</em></p></td>&#13;
<td style="vertical-align: top;" class="table-h1"><p class="taba">A negative of the first file to show the type of image Tombaugh used</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><a href="ch05.xhtml#ch05fig4">Figure 5-4</a> is an example of one of the images. The arrow points to the transient (but isn’t part of the image file).</p>&#13;
<div class="image"><img src="../images/fig05_04.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig4"/>Figure 5-4: <span class="normal">1_bright_transient_left.png</span> with an arrow indicating the transient</p>&#13;
<p class="indent">To duplicate the difficulty in perfectly aligning a telescope from night to night, I’ve slightly shifted the images in the <em>night_2</em> folder with respect to those in <em>night_1</em>. You’ll need to loop through the contents of the two folders, registering and comparing each pair of photos. For this reason, the number of files in each folder should be the same, and the naming convention should ensure that the photos are properly paired.</p>&#13;
<h4 class="h4" id="ch00lev2sec26"><strong><em>The Blink Comparator Code</em></strong></h4>&#13;
<p class="noindent">The following <em>blink_comparator.py</em> code will digitally duplicate a blink comparator. Find this program in the <em>Chapter_5</em> folder from the website. You’ll also need the folders described in the previous section. Keep the code in the folder above the <em>night_1</em> and <em>night_2</em> folders.</p>&#13;
<h5 class="h5"><strong>Importing Modules and Assigning a Constant</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_100"/><a href="ch05.xhtml#ch05list1">Listing 5-1</a> imports the modules you’ll need to run the program and assigns a constant for the minimum number of keypoint matches to accept. Also called interest points, <em>keypoints</em> are interesting features in an image that you can use to characterize the image. They’re usually associated with sharp changes in intensity, such as corners or, in this case, stars.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 1&#13;
import os&#13;
from pathlib import Path&#13;
import numpy as np&#13;
import cv2 as cv&#13;
&#13;
MIN_NUM_KEYPOINT_MATCHES = 50</pre>&#13;
<p class="listing"><a id="ch05list1"/>Listing 5-1: Importing modules and assigning a constant for keypoint matches</p>&#13;
<p class="indent">Start by importing the operating system module, which you’ll use to list the contents of folders. Then import <span class="literal">pathlib</span>, a handy module that simplifies working with files and folders. Finish by importing <span class="literal">NumPy</span> and <span class="literal">cv</span> (OpenCV) for working with images. If you skipped <a href="ch01.xhtml">Chapter 1</a>, you can find installation instructions for <span class="literal">NumPy</span> on <a href="ch01.xhtml#page_8">page 8</a>.</p>&#13;
<p class="indent">Assign a constant variable for the minimum number of keypoint matches to accept. For efficiency, you ideally want the smallest value that will yield an acceptable registration result. In this project, the algorithm runs so quickly that you can increase this value without a significant cost.</p>&#13;
<h5 class="h5"><strong>Defining the main() Function</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list2">Listing 5-2</a> defines the first part of the <span class="literal">main()</span> function, used to run the program. These initial steps create lists and directory paths used to access the various image files.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 2 &#13;
def main():&#13;
    """Loop through 2 folders with paired images, register &amp; blink images."""&#13;
    night1_files = sorted(os.listdir('night_1'))&#13;
    night2_files = sorted(os.listdir('night_2'))             &#13;
    path1 = Path.cwd() / 'night_1'&#13;
    path2 = Path.cwd() / 'night_2'&#13;
    path3 = Path.cwd() / 'night_1_registered'</pre>&#13;
<p class="listing"><a id="ch05list2"/>Listing 5-2: Defining the first part of <span class="codeitalic">main()</span>, used to manipulate files and folders</p>&#13;
<p class="indent">Start by defining <span class="literal">main()</span> and then use the <span class="literal">os</span> module’s <span class="literal">listdir()</span> method to create a list of the filenames in the <em>night_1</em> and <em>night_2</em> folders. For the <em>night_1</em> folder, <span class="literal">listdir()</span> returns the following:</p>&#13;
<pre>['1_bright_transient_left.png', '2_dim_transient_left.png', '3_diff_exposures_&#13;
left.png', '4_no_transient_left.png', '5_bright_transient_neg_left.png']</pre>&#13;
<p class="indent">Note that <span class="literal">os.listdir()</span> does not impose an order on the files when they’re returned. The underlying operating system determines the order, <span epub:type="pagebreak" id="page_101"/>meaning macOS will return a different list than Windows! To ensure that the lists are consistent and the files are paired correctly, wrap <span class="literal">os.listdir()</span> with the built-in <span class="literal">sorted()</span> function. This function will return the files in numerical order, based on the first character in the filename.</p>&#13;
<p class="indent">Next, assign path names to variables using the <span class="literal">pathlib</span> <span class="literal">Path</span> class. The first two variables will point to the two input folders, and the third will point to an output folder to hold the registered images.</p>&#13;
<p class="indent">The <span class="literal">pathlib</span> module, introduced in Python 3.4, is an alternative to <span class="literal">os.path</span> for handling file paths. The <span class="literal">os</span> module treats paths as strings, which can be cumbersome and requires you to use functionality from across the Standard Library. Instead, the <span class="literal">pathlib</span> module treats paths as objects and gathers the necessary functionality in one place. The official documentation for <span class="literal">pathlib</span> is at <em><a href="https://docs.python.org/3/library/pathlib.html">https://docs.python.org/3/library/pathlib.html</a></em>.</p>&#13;
<p class="indent">For the first part of the directory path, use the <span class="literal">cwd()</span> class method to get the current working directory. If you have at least one <span class="literal">Path</span> object, you can use a mix of objects and strings in the path designation. You can join the string, representing the folder name, with the <span class="literal">/</span> symbol. This is similar to using <span class="literal">os.path.join()</span>, if you’re familiar with the <span class="literal">os</span> module.</p>&#13;
<p class="indent">Note that you will need to execute the program from within the project directory. If you call it from elsewhere in the filesystem, it will fail.</p>&#13;
<h5 class="h5"><strong>Looping in main()</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list3">Listing 5-3</a>, still in the <span class="literal">main()</span> function, runs the program with a big <span class="literal">for</span> loop. This loop will take a file from each of the two “night” folders, load them as grayscale images, find matching keypoints in each image, use the keypoints to warp (or <em>register</em>) the first image to match the second, save the registered image, and then compare (or <em>blink</em>) the registered first image with the original second image. I’ve also included a few optional quality control steps that you can comment out once you’re satisfied with the results.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 3&#13;
for i, _ in enumerate(night1_files):    &#13;
    img1 = cv.imread(str(path1 / night1_files[i]), cv.IMREAD_GRAYSCALE)&#13;
    img2 = cv.imread(str(path2 / night2_files[i]), cv.IMREAD_GRAYSCALE)&#13;
    print("Comparing {} to {}.\n".format(night1_files[i], night2_files[i]))&#13;
 <span class="ent">➊</span> kp1, kp2, best_matches = find_best_matches(img1, img2)&#13;
    img_match = cv.drawMatches(img1, kp1, img2, kp2, &#13;
                               best_matches, outImg=None)&#13;
    height, width = img1.shape&#13;
    cv.line(img_match, (width, 0), (width, height), (255, 255, 255), 1)&#13;
 <span class="ent">➋</span> QC_best_matches(img_match)  # Comment out to ignore.&#13;
    img1_registered = register_image(img1, img2, kp1, kp2, best_matches)&#13;
&#13;
 <span class="ent">➌</span> blink(img1, img1_registered, 'Check Registration', num_loops=5)  &#13;
    out_filename = '{}_registered.png'.format(night1_files[i][:-4])&#13;
    cv.imwrite(str(path3 / out_filename), img1_registered) # Will overwrite!&#13;
    cv.destroyAllWindows()&#13;
    blink(img1_registered, img2, 'Blink Comparator', num_loops=15)</pre>&#13;
<p class="listing"><a id="ch05list3"/>Listing 5-3: Running the program loop in <span class="codeitalic">main()</span></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_102"/>Begin the loop by enumerating the <span class="literal">night1_files</span> list. The <span class="literal">enumerate()</span> built-in function adds a counter to each item in the list and returns this counter along with the item. Since you only need the counter, use a single underscore (<span class="literal">_</span>) for the list item. By convention, the single underscore indicates a temporary or insignificant variable. It also keeps code-checking programs, such as Pylint, happy. Were you to use a variable name here, such as <span class="literal">infile</span>, Pylint would complain about an <em>unused variable</em>.</p>&#13;
<pre>W: 17,11: Unused variable 'infile' (unused-variable)</pre>&#13;
<p class="indent">Next, load the image, along with its pair from the <span class="literal">night2_files</span> list, using OpenCV. Note that you have to convert the path to a string for the <span class="literal">imread()</span> method. You’ll also want to convert the image to grayscale. This way, you’ll need to work with only a single channel, which represents intensity. To keep track of what’s going on during the loop, print a message to the shell indicating which files are being compared.</p>&#13;
<p class="indent">Now, find the keypoints and their best matches <span class="ent">➊</span>. The <span class="literal">find_best_matches()</span> function, which you’ll define later, will return these values as three variables: <span class="literal">kp1</span>, which represents the keypoints for the first loaded image; <span class="literal">kp2</span>, which represents the keypoints for the second; and <span class="literal">best_matches</span>, which represents a list of the matching keypoints.</p>&#13;
<p class="indent">So you can visually check the matches, draw them on <span class="literal">img1</span> and <span class="literal">img2</span> using OpenCV’s <span class="literal">drawMatches()</span> method. As arguments, this method takes each image with its keypoints, the list of best matching keypoints, and an output image. In this case, the output image argument is set to <span class="literal">None</span>, as you’re just going to look at the output, not save it to a file.</p>&#13;
<p class="indent">To distinguish between the two images, draw a vertical white line down the right side of <span class="literal">img1</span>. First get the height and width of the image using <span class="literal">shape</span>. Next, call OpenCV’s <span class="literal">line()</span> method and pass it the image on which you want to draw, the start and end coordinates, the line color, and the thickness. Note that this is a color image, so to represent white, you need the full BGR tuple <span class="literal">(255, 255, 255)</span> rather than the single intensity value (255) used in grayscale images.</p>&#13;
<p class="indent">Now, call the quality control function—which you’ll define later—to  display the matches <span class="ent">➋</span>. <a href="ch05.xhtml#ch05fig5">Figure 5-5</a> shows an example output. You may want to comment out this line after you confirm the program is behaving correctly.</p>&#13;
<div class="image"><img src="../images/fig05_05.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig5"/>Figure 5-5: Example output of the <span class="literal">QC_best_matches()</span> function</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_103"/>With the best keypoint matches found and checked, it’s time to register the first image to the second. Do this with a function you’ll write later. Pass the function the two images, the keypoints, and the list of best matches.</p>&#13;
<p class="indent">The blink comparator, named <span class="literal">blink()</span>, is another function that you’ll write later. Call it here to see the effect of the registration process on the first image. Pass it the original and registered images, a name for the display window, and the number of blinks you want to perform <span class="ent">➌</span>. The function will flash between the two images. The amount of “wiggle” you see will depend on the amount of warping needed to match <span class="literal">img2</span>. This is another line you may want to comment out after you’ve confirmed that the program runs as intended.</p>&#13;
<p class="indent">Next, save the registered image into a folder named <em>night_1_registered</em>, which the <span class="literal">path3</span> variable points to. Start by assigning a filename variable that references the original filename, with <em>_registered.png</em> appended to the end. So you don’t repeat the file extension in the name, use index slicing (<span class="literal">[:-4]</span>) to remove it before adding the new ending. Finish by using <span class="literal">imwrite()</span> to save the file. Note that this will overwrite existing files with the same name without warning.</p>&#13;
<p class="indent">You’ll want an uncluttered view when you start looking for transients, so call the method to destroy all the current OpenCV windows. Then call the <span class="literal">blink()</span> function again, passing it the registered image, the second image, a window name, and the number of times to loop through the images. The first images are shown side by side in <a href="ch05.xhtml#ch05fig6">Figure 5-6</a>. Can you find the transient?</p>&#13;
<div class="image"><img src="../images/fig05_06.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig6"/>Figure 5-6: Blink Comparator windows for first image in <span class="normal">night_1_registered</span> and <span class="normal">night_2</span> folders</p>&#13;
<h5 class="h5"><strong>Finding the Best Keypoint Matches</strong></h5>&#13;
<p class="noindent">Now it’s time to define the functions used in <span class="literal">main()</span>. <a href="ch05.xhtml#ch05list4">Listing 5-4</a> defines the function that finds the best keypoint matches between each pair of images extracted from the <em>night_1</em> and <em>night_2</em> folders. It should locate, describe, and <span epub:type="pagebreak" id="page_104"/>match keypoints, generate a list of the matches, and then truncate that list by the constant for the minimum number of acceptable keypoints. The function returns the list of keypoints for each image and the list of best matches.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 4 &#13;
def find_best_matches(img1, img2):&#13;
    """Return list of keypoints and list of best matches for two images."""&#13;
    orb = cv.ORB_create(nfeatures=100)  #  Initiate ORB object.&#13;
 <span class="ent">➊</span> kp1, desc1 = orb.detectAndCompute(img1, mask=None)&#13;
    kp2, desc2 = orb.detectAndCompute(img2, mask=None)    &#13;
    bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)&#13;
 <span class="ent">➋</span> matches = bf.match(desc1, desc2)&#13;
    matches = sorted(matches, key=lambda x: x.distance)&#13;
    best_matches = matches[:MIN_NUM_KEYPOINT_MATCHES]&#13;
              &#13;
    return kp1, kp2, best_matches</pre>&#13;
<p class="listing"><a id="ch05list4"/>Listing 5-4: Defining the function to find the best keypoint matches</p>&#13;
<p class="indent">Start by defining the function, which takes two images as arguments. The <span class="literal">main()</span> function will pick these images from the input folders with each run of the <span class="literal">for</span> loop.</p>&#13;
<p class="indent">Next, create an <span class="literal">orb</span> object using OpenCV’s <span class="literal">ORB_create()</span> method. ORB is an acronym of nested acronyms: <em>O</em>riented FAST and <em>R</em>otated <em>B</em>RIEF.</p>&#13;
<p class="indent">FAST, short for <em>F</em>eatures from <em>A</em>ccelerated <em>S</em>egment <em>T</em>est, is a fast, efficient, and free algorithm for <em>detecting</em> keypoints. To <em>describe</em> the keypoints so that you can compare them across different images, you need BRIEF. Short for <em>B</em>inary <em>R</em>obust <em>I</em>ndependent <em>E</em>lementary <em>F</em>eatures, BRIEF is also fast, compact, and open source.</p>&#13;
<p class="indent">ORB combines FAST and BRIEF into a matching algorithm that works by first detecting distinctive regions in an image, where pixel values change sharply, and then recording the position of these distinctive regions as <em>keypoints</em>. Next, ORB describes the feature found at the keypoint using numerical arrays, or <em>descriptors</em>, by defining a small area, called a <em>patch</em>, around a keypoint. Within the image patch, the algorithm uses a pattern template to take regular samples of intensity. It then compares preselected pairs of samples and converts them into binary strings called <em>feature vectors</em> (<a href="ch05.xhtml#ch05fig7">Figure 5-7</a>).</p>&#13;
<p class="indent">A <em>vector</em> is a series of numbers. A <em>matrix</em> is a rectangular array of numbers in rows and columns that’s treated as a single entity and manipulated according to rules. A <em>feature vector</em> is a matrix with one row and multiple columns. To build one, the algorithm converts the sample pairs into a binary series by concatenating a 1 to the end of the vector if the first sample has the largest intensity and a 0 if the reverse is true.<span epub:type="pagebreak" id="page_105"/></p>&#13;
<div class="image"><img src="../images/fig05_07.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig7"/>Figure 5-7: Cartoon example of how ORB generates keypoint descriptors</p>&#13;
<p class="indent">Some example feature vectors are shown next. I’ve shortened the list of vectors, because ORB usually compares and records 512 pairs of samples!</p>&#13;
<pre>V<sub>1</sub> = [010010110100101101100<span class="codeitalic1">--snip--</span>]&#13;
V<sub>2</sub> = [100111100110010101101<span class="codeitalic1">--snip--</span>]&#13;
V<sub>3</sub> = [001101100011011101001<span class="codeitalic1">--snip--</span>]&#13;
<span class="codeitalic1">--snip--</span></pre>&#13;
<p class="indent">These descriptors act as digital fingerprints for features. OpenCV uses additional code to compensate for rotation and scale changes. This allows it to match similar features even if the feature sizes and orientations are different (see <a href="ch05.xhtml#ch05fig8">Figure 5-8</a>).</p>&#13;
<div class="image"><img src="../images/fig05_08.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig8"/>Figure 5-8: OpenCV can match keypoints despite differences in scale and orientation.<span epub:type="pagebreak" id="page_106"/></p>&#13;
<p class="indent">When you create the ORB object, you can specify the number of keypoints to examine. The method defaults to 500, but 100 will be more than enough for the image registration needed in this project.</p>&#13;
<p class="indent">Next, using the <span class="literal">orb.detectAndCompute()</span> method <span class="ent">➊</span>, find the keypoints and  their descriptors. Pass it <span class="literal">img1</span> and then repeat the code for <span class="literal">img2</span>.</p>&#13;
<p class="indent">With the keypoints located and described, the next step is to find the keypoints common to both images. Start this process by creating a <span class="literal">BFMatcher</span> object that includes a distance measurement. The brute-force matcher takes the descriptor of one feature in the first image and compares it to all the features in the second image using the Hamming distance. It returns the closest feature.</p>&#13;
<p class="indent">For two strings of equal length, the <em>Hamming distance</em> is the number of positions, or indexes, at which the corresponding values are different. For the following feature vectors, the positions that don’t match are shown in bold, and the Hamming distance is 3:</p>&#13;
<pre>1<span class="codestrong1">0</span>0<span class="codestrong1">10</span>11001010&#13;
1<span class="codestrong1">1</span>0<span class="codestrong1">01</span>11001010</pre>&#13;
<p class="indent">The <span class="literal">bf</span> variable will be a <span class="literal">BFMatcher</span> object. Call the <span class="literal">match()</span> method and pass it the descriptors for the two images <span class="ent">➋</span>. Assign the returned list of <span class="literal">DMatch</span> objects to a variable named <span class="literal">matches</span>.</p>&#13;
<p class="indent">The best matches will have the lowest Hamming distance, so sort the objects in ascending order to move these to the start of the list. Note that you use a lambda function along with the object’s <span class="literal">distance</span> attribute. A <em>lambda function</em> is a small, one-off, unnamed function defined on the fly. Words and characters that directly follow <span class="literal">lambda</span> are parameters. Expressions come after the colon, and returns are automatic.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_107"/>Since you only need the minimum number of keypoint matches defined at the start of the program, create a new list by slicing the <span class="literal">matches</span> list. The best matches are at the start, so slice from the start of <span class="literal">matches</span> up to the value specified in <span class="literal">MIN_NUM_KEYPOINT_MATCHES</span>.</p>&#13;
<p class="indent">At this point, you’re still dealing with arcane objects, as shown here:</p>&#13;
<pre>best matches =  [&lt;DMatch 0000028BEBAFBFB0&gt;, &lt;DMatch 0000028BEBB21090&gt;,<span class="codeitalic1"> --snip--</span></pre>&#13;
<p class="indent">Fortunately, OpenCV knows how to handle these. Complete the function by returning the two sets of keypoints and the list of best matching objects.</p>&#13;
<h5 class="h5"><strong>Checking the Best Matches</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list5">Listing 5-5</a> defines a short function to let you visually check the keypoint matches. You saw the results of this function in <a href="ch05.xhtml#ch05fig5">Figure 5-5</a>. By encapsulating these tasks in a function, you can reduce the clutter in <span class="literal">main()</span> and allow the user to turn off the functionality by commenting out a single line.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 5&#13;
def QC_best_matches(img_match):&#13;
    """Draw best keypoint matches connected by colored lines."""    &#13;
    cv.imshow('Best {} Matches'.format(MIN_NUM_KEYPOINT_MATCHES), img_match)&#13;
    cv.waitKey(2500)  # Keeps window active 2.5 seconds.</pre>&#13;
<p class="listing"><a id="ch05list5"/>Listing 5-5: Defining a function to check the best keypoint matches</p>&#13;
<p class="indent">Define the function with one parameter: the matched image. This image was generated by the <span class="literal">main()</span> function in <a href="ch05.xhtml#ch05list3">Listing 5-3</a>. It consists of the left and right images with the keypoints drawn as colored circles and with colored lines connecting corresponding keypoints.</p>&#13;
<p class="indent">Next, call OpenCV’s <span class="literal">imshow()</span> method to display the window. You can use the <span class="literal">format()</span> method when naming the window. Pass it the constant for the number of minimum keypoint matches.</p>&#13;
<p class="indent">Complete the function by giving the user 2.5 seconds to view the window. Note that the <span class="literal">waitKey()</span> method doesn’t destroy the window; it just suspends the program for the allocated amount of time. After the wait period, new windows will appear as the program resumes.</p>&#13;
<h5 class="h5"><strong>Registering Images</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list6">Listing 5-6</a> defines the function to register the first image to the second image.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 6&#13;
def register_image(img1, img2, kp1, kp2, best_matches):&#13;
    """Return first image registered to second image."""&#13;
    if len(best_matches) &gt;= MIN_NUM_KEYPOINT_MATCHES:&#13;
        src_pts = np.zeros((len(best_matches), 2), dtype=np.float32)&#13;
        dst_pts = np.zeros((len(best_matches), 2), dtype=np.float32)&#13;
&#13;
     <span class="ent">➊</span> for i, match in enumerate(best_matches):&#13;
            src_pts[i, :] = kp1[match.queryIdx].pt&#13;
            dst_pts[i, :] = kp2[match.trainIdx].pt            &#13;
        h_array, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC)&#13;
&#13;
     <span epub:type="pagebreak" id="page_108"/><span class="ent">➋</span> height, width = img2.shape  # Get dimensions of image 2.&#13;
        img1_warped = cv.warpPerspective(img1, h_array, (width, height))&#13;
&#13;
        return img1_warped&#13;
&#13;
    else:&#13;
        print("WARNING: Number of keypoint matches &lt; {}\n".format&#13;
              (MIN_NUM_KEYPOINT_MATCHES))&#13;
        return img1</pre>&#13;
<p class="listing"><a id="ch05list6"/>Listing 5-6: Defining a function to register one image to another</p>&#13;
<p class="indent">Define a function that takes the two input images, their keypoint lists, and the list of <span class="literal">DMatch</span> objects returned from the <span class="literal">find_best_matches()</span> function as arguments. Next, load the location of the best matches into <span class="literal">NumPy</span> arrays. Start with a conditional to check that the list of best matches equals or exceeds the <span class="literal">MIN_NUM_KEYPOINT_MATCHES</span> constant. If it does, then initialize two <span class="literal">NumPy</span> arrays with as many rows as there are best matches.</p>&#13;
<p class="indent">The <span class="literal">np.zeros()</span> <span class="literal">NumPy</span> method returns a new array of a given shape and data type, filled with zeros. For example, the following snippet produces a zero-filled array three rows tall and two columns wide:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import numpy as np</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">ndarray = np.zeros((3, 2), dtype=np.float32)</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">ndarray</span>&#13;
array([[0., 0.],&#13;
       [0., 0.],&#13;
       [0., 0.]], dtype=float32)</pre>&#13;
<p class="indent">In the actual code, the arrays will be at least 50×2, since you stipulated a minimum of 50 matches.</p>&#13;
<p class="indent">Now, enumerate the <span class="literal">matches</span> list and start populating the arrays with actual data <span class="ent">➊</span>. For the source points, use the <span class="literal">queryIdx.pt</span> attribute to get the index of the descriptor in the list of descriptors for <span class="literal">kp1</span>. Repeat this for the next set of points, but use the <span class="literal">trainIdx.pt</span> attribute. The query/train terminology is a bit confusing but basically refers to the first and second images, respectively.</p>&#13;
<p class="indent">The next step is to apply <em>homography</em>. Homography is a transformation, using a 3×3 matrix, that maps points in one image to corresponding points in another image. Two images can be related by a homography if both are viewing the same plane from a different angle or if both images are taken from the same camera rotated around its optical axis with no shift. To run correctly, homography needs at least four corresponding points in two images.</p>&#13;
<p class="indent">Homography assumes that the matching points really are corresponding points. But if you look carefully at <a href="ch05.xhtml#ch05fig5">Figures 5-5</a> and <a href="ch05.xhtml#ch05fig8">5-8</a>, you’ll see that the feature matching isn’t perfect. In <a href="ch05.xhtml#ch05fig8">Figure 5-8</a>, around 30 percent of the matches are incorrect!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_109"/>Fortunately, OpenCV includes a <span class="literal">findHomography()</span> method with an outlier detector called <em>random sample consensus</em> (RANSAC). RANSAC takes random samples of the matching points, finds a mathematical model that explains their distribution, and favors the model that predicts the most points. It then discards outliers. For example, consider the points in the “Raw data” box in <a href="ch05.xhtml#ch05fig9">Figure 5-9</a>.</p>&#13;
<div class="image"><img src="../images/fig05_09.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig9"/>Figure 5-9: Example line fit using RANSAC to ignore outliers</p>&#13;
<p class="indent">As you can see, you want to fit a line through the true data points (called the <em>inliers</em>) and ignore the smaller number of spurious points (the <em>outliers</em>). Using RANSAC, you randomly sample a subset of the raw data points, fit a line to these, and then repeat this process a set number of times. Each line-fit equation would then be applied to all the points. The line that passes through the most points is used for the final line fit. In <a href="ch05.xhtml#ch05fig9">Figure 5-9</a>, this would be the line in the rightmost box.</p>&#13;
<p class="indent">To run <span class="literal">findHomography()</span>, pass it the source and destination points and call the RANSAC method. This returns a <span class="literal">NumPy</span> array and a mask. The mask specifies the inlier and outlier points or the good matches and bad matches, respectively. You can use it to do tasks like draw only the good matches.</p>&#13;
<p class="indent">The final step is to warp the first image so that it perfectly aligns with the second. You’ll need the dimensions of the second image, so use <span class="literal">shape()</span> to get the height and width of <span class="literal">img2</span> <span class="ent">➋</span>. Pass this information, along with <span class="literal">img1</span> and the homography <span class="literal">h_array</span>, to the <span class="literal">warpPerspective()</span> method. Return the registered image, which will be a <span class="literal">NumPy</span> array.</p>&#13;
<p class="indent">If the number of keypoint matches is less than the minimum number you stipulated at the start of the program, the image <em>may not</em> be properly aligned. So, print a warning and return the original, nonregistered image. This will allow the <span class="literal">main()</span> function to continue looping through the folder images uninterrupted. If the registration is poor, the user will be aware something is wrong as the problem pair of images won’t be properly aligned in the blink comparator window. An error message will also appear in the shell.<span epub:type="pagebreak" id="page_110"/></p>&#13;
<pre>Comparing 2_dim_transient_left.png to 2_dim_transient_right.png.&#13;
WARNING: Number of keypoint matches &lt; 50</pre>&#13;
<h5 class="h5"><strong>Building the Blink Comparator</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list7">Listing 5-7</a> defines a function to run the blink comparator and then calls <span class="literal">main()</span> if the program is run in stand-alone mode. The <span class="literal">blink()</span> function loops through a specified range, showing first the registered image and then the second image, both in the same window. It shows each image for only one-third of a second, Clyde Tombaugh’s preferred frequency when using a blink comparator.</p>&#13;
<pre><span class="codeitalic1">blink_comparator.py</span>, part 7 &#13;
def blink(image_1, image_2, window_name, num_loops):&#13;
    """Replicate blink comparator with two images."""&#13;
    for _ in range(num_loops):&#13;
        cv.imshow(window_name, image_1)&#13;
        cv.waitKey(330)&#13;
        cv.imshow(window_name, image_2)&#13;
        cv.waitKey(330)&#13;
        &#13;
if __name__ == '__main__':&#13;
    main()</pre>&#13;
<p class="listing"><a id="ch05list7"/>Listing 5-7: Defining a function to blink images on and off</p>&#13;
<p class="indent">Define the <span class="literal">blink()</span> function with four parameters: two image files, a window name, and the number of blinks to perform. Start a <span class="literal">for</span> loop with a range set to the number of blinks. Since you don’t need access to the running index, use a single underscore (<span class="literal">_</span>) to indicate the use of an insignificant variable. As mentioned previously in this chapter, this will prevent code-checking programs from raising an “unused variable” warning.</p>&#13;
<p class="indent">Now call OpenCV’s <span class="literal">imshow()</span> method and pass it the window name and the first image. This will be the <em>registered</em> first image. Then pause the program for 330 milliseconds, the amount of time recommended by Clyde Tombaugh himself.</p>&#13;
<p class="indent">Repeat the previous two lines of code for the second image. Because the two images are aligned, the only thing that will change in the window are transients. If only one image contains a transient, it will appear to blink on and off. If both images capture the transient, it will appear to dance back and forth.</p>&#13;
<p class="indent">End the program with the standard code that lets it run in stand-alone mode or be imported as a module.</p>&#13;
<h4 class="h4" id="ch00lev2sec27"><strong><em>Using the Blink Comparator</em></strong></h4>&#13;
<p class="noindent">Before you run <em>blink_comparator.py</em>, dim your room lights to simulate looking through the device’s eyepieces. Then launch the program. You should first see two obvious bright dots flashing near the center of the image. In the next pair of images, the same dots will become very small—only a pixel across—but you should still be able to detect them.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_111"/>The third loop will show the same small transient, only this time the second image will be brighter overall than the first. You should still be able to find the transient, but it will be much more difficult. This is why Tombaugh had to carefully take and develop the images to a consistent exposure.</p>&#13;
<p class="indent">The fourth loop contains a single transient, shown in the left image. It should blink on and off rather than dance back and forth as in the previous images.</p>&#13;
<p class="indent">The fifth image pair represents control images with no transients. This is what the astronomer would see almost all the time: disappointing static star fields.</p>&#13;
<p class="indent">The final loop uses negative versions of the first image pair. The bright transient appears as flashing black dots. This is the type of image Clyde Tombaugh used, as it saved time. Since a black dot is as easy to spot as a white one, he felt no need to print positive images for each negative.</p>&#13;
<p class="indent">If you look along the left side of the registered negative image, you’ll see a black stripe that represents the amount of translation needed to align the images (<a href="ch05.xhtml#ch05fig10">Figure 5-10</a>). You won’t notice this on the positive images because it blends in with the black background.</p>&#13;
<div class="image"><img src="../images/fig05_10.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig10"/>Figure 5-10: The negative image, <span class="normal">6_bright_transient_neg_left_registered.png</span></p>&#13;
<p class="indent">In all the loops, you may notice a dim star blinking in the upper-left corner of each image pair. This is not a transient but a false positive caused by an <em>edge artifact</em>. An edge artifact is a change to an image caused by image misalignment. An experienced astronomer would ignore this dim star because: it occurs very close to the edge of the image, and the possible transient doesn’t move between images but just dims.</p>&#13;
<p class="indent">You can see the cause of this false positive in <a href="ch05.xhtml#ch05fig11">Figure 5-11</a>. Because only part of a star is captured in the first frame, its brightness is reduced relative to the same star in the second image.</p>&#13;
<div class="image"><img src="../images/fig05_11.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig11"/>Figure 5-11: Registering a truncated star in Image 1 results in a noticeably dimmer star than in Image 2</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_112"/>Humans can handle edge effects intuitively, but computers require explicit rules. In the next project, you’ll address this issue by excluding the edges of images when searching for transients.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec35"><strong>Project #8: Detecting Astronomical Transients with Image Differencing</strong></h3>&#13;
<p class="noindent">Blink comparators, once considered as important as telescopes, now sit idly gathering dust in museums. Astronomers no longer need them, as modern image-differencing techniques are much better at detecting moving objects than human eyes. Today, every part of Clyde Tombaugh’s work would be done by computers.</p>&#13;
<p class="indent">In this project, let’s pretend you’re a summer intern at an observatory. Your job is to produce a digital workflow for an ancient astronomer still clinging to his rusty blink comparator.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that takes two registered images and highlights any differences between them.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec28"><strong><em>The Strategy</em></strong></h4>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_113"/>Instead of an algorithm that blinks the images, you now want one that automatically finds the transients. This process will still require registered images, but for convenience, just use the ones already produced in Project 7.</p>&#13;
<p class="indent">Detecting differences between images is a common enough practice that OpenCV ships with an absolute difference method, <span class="literal">absdiff()</span>, dedicated to this purpose. It takes the per-element difference between two arrays.  But just detecting the differences isn’t enough. Your program will need to recognize that a difference exists and show the user only the images containing transients. After all, astronomers have more important things to do, like demoting planets!</p>&#13;
<p class="indent">Because the objects you’re looking for rest on a black background and matching bright objects are removed, any bright object remaining after differencing is worth noting. And since the odds of having more than one transient in a star field are astronomically low, flagging one or two differences should be enough to get an astronomer’s attention.</p>&#13;
<h4 class="h4" id="ch00lev2sec29"><strong><em>The Transient Detector Code</em></strong></h4>&#13;
<p class="noindent">The following <em>transient_detector.py</em> code will automate the process of detecting transients in astronomical images. Find it in the <em>Chapter_5</em> folder from the website. To avoid duplicating code, the program uses the images already registered by <em>blink_comparator.py</em>, so you’ll need the <em>night_1_registered_transients</em> and <em>night_2</em> folders in the directory for this project (see <a href="ch05.xhtml#ch05fig3">Figure 5-3</a>). As in the previous project, keep the Python code in the folder <em>above</em> these two folders.</p>&#13;
<h5 class="h5"><strong>Importing Modules and Assigning a Constant</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list8">Listing 5-8</a> imports the modules needed to run the program and assigns a pad constant to manage edge artifacts (see <a href="ch05.xhtml#ch05fig11">Figure 5-11</a>). The pad represents a small distance, measured perpendicular to the image’s edges, that you want to exclude from the analysis. Any objects detected between the edge of the image and the pad will be ignored.</p>&#13;
<pre><span class="codeitalic1">transient_detector.py</span>, part 1&#13;
import os&#13;
from pathlib import Path&#13;
import cv2 as cv&#13;
&#13;
PAD = 5  # Ignore pixels this distance from edge</pre>&#13;
<p class="listing"><a id="ch05list8"/>Listing 5-8: Importing modules and assigning a constant to manage edge effects</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_114"/>You’ll need all the modules used in the previous project except for <span class="literal">NumPy</span>, so import them here. Set the pad distance to 5 pixels. This value may change slightly with different datasets. Later, you’ll draw a rectangle around the edge space within the image so you can see how much area this parameter is excluding.</p>&#13;
<h5 class="h5"><strong>Detecting and Circling Transients</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list9">Listing 5-9</a> defines a function you’ll use to find and circle up to two transients in each image pair. It will ignore transients in the padded area.</p>&#13;
<pre><span class="codeitalic1">transient_detector.py</span>, part 2 &#13;
def find_transient(image, diff_image, pad):&#13;
    """Find and circle transients moving against a star field. """&#13;
    transient = False&#13;
    height, width = diff_image.shape&#13;
    cv.rectangle(image, (PAD, PAD), (width - PAD, height - PAD), 255, 1)&#13;
    minVal, maxVal, minLoc, maxLoc = cv.minMaxLoc(diff_image)&#13;
 <span class="ent">➊</span> if pad &lt; maxLoc[0] &lt; width - pad and pad &lt; maxLoc[1] &lt; height - pad:&#13;
        cv.circle(image, maxLoc, 10, 255, 0)&#13;
        transient = True&#13;
    return transient, maxLoc</pre>&#13;
<p class="listing"><a id="ch05list9"/>Listing 5-9: Defining a function to detect and circle transients</p>&#13;
<p class="indent">The <span class="literal">find_transient()</span> function has three parameters: the input image, an image representing the difference between the first and second input images (representing the <em>difference map</em>), and the <span class="literal">PAD</span> constant. The function will find the location of the brightest pixel in the difference map, draw a circle around it, and return the location along with a Boolean indicating that an object was found.</p>&#13;
<p class="indent">Begin the function by setting a variable, named <span class="literal">transient</span>, to <span class="literal">False</span>. You’ll use this variable to indicate whether a transient has been discovered. As transients are rare in real life, its base state should be <span class="literal">False</span>.</p>&#13;
<p class="indent">To apply the <span class="literal">PAD</span> constant and exclude the area near the edge of the image, you’ll need the limits of the image. Get these with the <span class="literal">shape</span> attribute, which returns a tuple of the image’s height and width.</p>&#13;
<p class="indent">Use the <span class="literal">height</span> and <span class="literal">width</span> variables and the <span class="literal">PAD</span> constant to draw a white rectangle on the <span class="literal">image</span> variable using OpenCV’s <span class="literal">rectangle()</span> method. Later, this will show the user which parts of the image were ignored.</p>&#13;
<p class="indent">The <span class="literal">diff_image</span> variable is a <span class="literal">NumPy</span> array representing pixels. The background is black, and any “stars” that changed position (or appeared out of nowhere) between the two input images will be gray or white (see <a href="ch05.xhtml#ch05fig12">Figure 5-12</a>).</p>&#13;
<div class="image"><img src="../images/fig05_12.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig12"/>Figure 5-12: Difference image derived from the “bright transient”  input images</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_115"/>To locate the brightest transient present, use OpenCV’s <span class="literal">minMaxLoc()</span> method, which returns the minimum and maximum pixel values in the image, along with their location tuple. Note that I’m naming the variables to be consistent with OpenCV’s mixed-case naming scheme (evident in names such as <span class="literal">maxLoc</span>). If you want to use something more acceptable to Python’s PEP8 style guide (<em><a href="https://www.python.org/dev/peps/pep-0008/">https://www.python.org/dev/peps/pep-0008/</a></em>), feel free to use names like <span class="literal">max_loc</span> in place of <span class="literal">maxLoc</span>.</p>&#13;
<p class="indent">You may have found a maximum value near the edge of the image, so run a conditional to exclude this case by ignoring values found in the area delimited by the <span class="literal">PAD</span> constant <span class="ent">➊</span>. If the location passes, circle it on the <span class="literal">image</span> variable. Use a white circle with a radius of 10 pixels and a line width of 0.</p>&#13;
<p class="indent">If you’ve drawn a circle, then you’ve found a transient, so set the <span class="literal">transient</span> variable to <span class="literal">True</span>. This will trigger additional activity later in the program.</p>&#13;
<p class="indent">End the function by returning the <span class="literal">transient</span> and <span class="literal">maxLoc</span> variables.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The <span class="codeitalic">minMaxLoc()</span> method is susceptible to noise, such as false positives, as it works on individual pixels. Normally, you would first run a preprocessing step, like blurring, to remove spurious pixels. This can cause you to miss dim astronomical objects, however, which can be indistinguishable from noise in a single image.</em></p>&#13;
</div>&#13;
<h5 class="h5"><strong>Preparing Files and Folders</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list10">Listing 5-10</a> defines the <span class="literal">main()</span> function, creates lists of the filenames in the input folders, and assigns the folder paths to variables.<span epub:type="pagebreak" id="page_116"/></p>&#13;
<pre><span class="codeitalic1">transient_detector.py</span>, part 3&#13;
def main():&#13;
    night1_files = sorted(os.listdir('night_1_registered_transients'))&#13;
    night2_files = sorted(os.listdir('night_2'))             &#13;
    path1 = Path.cwd() / 'night_1_registered_transients'&#13;
    path2 = Path.cwd() / 'night_2'&#13;
    path3 = Path.cwd() / 'night_1_2_transients'</pre>&#13;
<p class="listing"><a id="ch05list10"/>Listing 5-10: Defining <span class="literal">main()</span>, listing the folder contents, and assigning path variables</p>&#13;
<p class="indent">Define the <span class="literal">main()</span> function. Then, just as you did in <a href="ch05.xhtml#ch05list2">Listing 5-2</a> on <a href="ch05.xhtml#page_100">page 100</a>, list the contents of the folders containing the input images and assign their paths to variables. You’ll use an existing folder to hold images containing identified transients.</p>&#13;
<h5 class="h5"><strong>Looping Through Images and Calculating Absolute Difference</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list11">Listing 5-11</a> starts the <span class="literal">for</span> loop through the image pairs. The function reads corresponding image pairs as grayscale arrays, calculates the difference between the images, and shows the result in a window. It then calls the  <span class="literal">find_transient()</span> function on the difference image.</p>&#13;
<pre><span class="codeitalic1">transient_detector.py</span>, part 4 &#13;
for i, _ in enumerate(night1_files[:-1]):  # Leave off negative image   &#13;
    img1 = cv.imread(str(path1 / night1_files[i]), cv.IMREAD_GRAYSCALE)&#13;
    img2 = cv.imread(str(path2 / night2_files[i]), cv.IMREAD_GRAYSCALE)&#13;
&#13;
    diff_imgs1_2 = cv.absdiff(img1, img2)&#13;
    cv.imshow('Difference', diff_imgs1_2)&#13;
    cv.waitKey(2000)&#13;
&#13;
    temp = diff_imgs1_2.copy()&#13;
    transient1, transient_loc1 = find_transient(img1, temp, PAD)&#13;
    cv.circle(temp, transient_loc1, 10, 0, -1)&#13;
&#13;
    transient2, _ = find_transient(img1, temp, PAD)</pre>&#13;
<p class="listing"><a id="ch05list11"/>Listing 5-11: Looping through the images and finding the transients</p>&#13;
<p class="indent">Start a <span class="literal">for</span> loop that iterates through the images in the <em>night1_files</em> list. The program is designed to work on <em>positive</em> images, so use image slicing (<span class="literal">[:-1]</span>) to exclude the negative image. Use <span class="literal">enumerate()</span> to get a counter; name it <span class="literal">i</span>, rather than <span class="literal">_</span>, since you’ll use it as an index later.</p>&#13;
<p class="indent">To find the differences between images, just call the <span class="literal">cv.absdiff()</span> method and pass it the variables for the two images. Show the results for two seconds before continuing the program.</p>&#13;
<p class="indent">Since you’re going to blank out the brightest transient, first make a  copy of <span class="literal">diff_imgs1_2</span>. Name this copy <span class="literal">temp</span>, for temporary. Now, call the  <span class="literal">find_transient()</span> function you wrote earlier. Pass it the first input image,  the difference image, and the <span class="literal">PAD</span> constant. Use the results to update the <span class="literal">transient</span> variable and to create a new variable, <span class="literal">transient_loc1</span>, that records the location of the brightest pixel in the difference image.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_117"/>The transient may or may not have been captured in both images taken on successive nights. To see if it was, obliterate the bright spot you just found by covering it with a black circle. Do this on the <span class="literal">temp</span> image by using black as the color and a line width of –1, which tells OpenCV to fill the circle. Continue to use a radius of 10, though you can reduce this if you’re concerned the two transients will be very close together.</p>&#13;
<p class="indent">Call the <span class="literal">find_transient()</span> function again but use a single underscore for the location variable, as you won’t be using it again. It’s unlikely there’ll be more than two transients present, and finding even one will be enough to open the images up to further scrutiny, so don’t bother looking for more.</p>&#13;
<h5 class="h5"><strong>Revealing the Transient and Saving the Image</strong></h5>&#13;
<p class="noindent"><a href="ch05.xhtml#ch05list12">Listing 5-12</a>, still in the <span class="literal">for</span> loop of the <span class="literal">main()</span> function, displays the first input image with any transients circled, posts the names of the image files involved, and saves the image with a new filename. You’ll also print a log of the results for each image pair in the interpreter window.</p>&#13;
<pre><span class="codeitalic1">transient_detector.py</span>, part 5&#13;
        if transient1 or transient2:&#13;
            print('\nTRANSIENT DETECTED between {} and {}\n'&#13;
                  .format(night1_files[i], night2_files[i]))&#13;
        <span class="ent">➊</span> font = cv.FONT_HERSHEY_COMPLEX_SMALL&#13;
           cv.putText(img1, night1_files[i], (10, 25),&#13;
                      font, 1, (255, 255, 255), 1, cv.LINE_AA)&#13;
           cv.putText(img1, night2_files[i], (10, 55),&#13;
                      font, 1, (255, 255, 255), 1, cv.LINE_AA)&#13;
&#13;
           blended = cv.addWeighted(img1, 1, diff_imgs1_2, 1, 0)&#13;
           cv.imshow('Surveyed', blended)&#13;
           cv.waitKey(2500)  &#13;
&#13;
         <span class="ent">➋</span> out_filename = '{}_DECTECTED.png'.format(night1_files[i][:-4])&#13;
            cv.imwrite(str(path3 / out_filename), blended)  # Will overwrite!&#13;
&#13;
       else:&#13;
           print('\nNo transient detected between {} and {}\n'&#13;
                  .format(night1_files[i], night2_files[i]))&#13;
&#13;
if __name__ == '__main__':&#13;
    main()</pre>&#13;
<p class="listing"><a id="ch05list12"/>Listing 5-12: Showing the circled transients, logging the results, and saving the results</p>&#13;
<p class="indent">Start a conditional that checks whether a transient was found. If this evaluates to <span class="literal">True</span>, print a message in the shell. For the four images evaluated by the <span class="literal">for</span> loop, you should get this result:</p>&#13;
<pre>TRANSIENT DETECTED between 1_bright_transient_left_registered.png and 1_bright_transient_right.png&#13;
&#13;
TRANSIENT DETECTED between 2_dim_transient_left_registered.png and 2_dim_transient_right.png&#13;
&#13;
TRANSIENT DETECTED between 3_diff_exposures_left_registered.png and 3_diff_exposures_right.png&#13;
&#13;
TRANSIENT DETECTED between 4_single_transient_left_registered.png and 4_single_transient_right.png&#13;
&#13;
No transient detected between 5_no_transient_left_registered.png and 5_no_transient_right.png</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_118"/>Posting a negative outcome shows that the program is working as expected and leaves no doubt that the images were compared.</p>&#13;
<p class="indent">Next, post the names of the two images with a positive response on the <span class="literal">img1</span> array. Start by assigning a font variable for OpenCV <span class="ent">➊</span>. For a listing of available fonts, search for <em>HersheyFonts</em> at <em><a href="https://docs.opencv.org/4.3.0/">https://docs.opencv.org/4.3.0/</a></em>.</p>&#13;
<p class="indent">Now call OpenCV’s <span class="literal">putText()</span> method and pass it the first input image, the filename of the image, a position, the <span class="literal">font</span> variable, a size, a color (white), a thickness, and a line type. The <span class="literal">LINE_AA</span> attribute creates an anti-aliased line. Repeat this code for the second image.</p>&#13;
<p class="indent">If you found two transients, you can show them both on the same image using OpenCV’s <span class="literal">addWeighted()</span> method. This method calculates the weighted sum of two arrays. The arguments are the first image and a weight, the second image and a weight, and a scalar that’s added to each sum. Use the first input image and the difference image, set the weights to <span class="literal">1</span> so that each image is used fully, and set the scalar to <span class="literal">0</span>. Assign the result to a variable named <span class="literal">blended</span>.</p>&#13;
<p class="indent">Show the blended image in a window named Surveyed. <a href="ch05.xhtml#ch05fig13">Figure 5-13</a> shows an example outcome for the “bright” transient.</p>&#13;
<div class="image"><img src="../images/fig05_13.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig13"/>Figure 5-13: Example output window of <span class="normal">transient_detector.py</span> with the pad  rectangle indicated by the arrow</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_119"/>Note the white rectangle near the edges of the image. This represents the <span class="literal">PAD</span> distance. Any transients outside this rectangle were ignored by the program.</p>&#13;
<p class="indent">Save the blended image using the filename of the current input image plus “DETECTED” <span class="ent">➋</span>. The dim transient in <a href="ch05.xhtml#ch05fig13">Figure 5-13</a> would be saved as <em>1_bright_transient_left_registered_DECTECTED.png</em>. Write it to the <em>night_1_2_transients</em> folder, using the <span class="literal">path3</span> variable.</p>&#13;
<p class="indent">If no transients were found, document the result in the shell window. Then end the program with the code to run it as a module or in stand-alone mode.</p>&#13;
<h4 class="h4" id="ch00lev2sec30"><strong><em>Using the Transient Detector</em></strong></h4>&#13;
<p class="noindent">Imagine how happy Clyde Tombaugh would’ve been with your transient detector. It’s truly set-it-and-forget-it. Even the changing brightness between the third pair of images, so problematic with the blink comparator, is no challenge for this program.</p>&#13;
<h3 class="h3" id="ch00lev1sec36"><strong>Summary</strong></h3>&#13;
<p class="noindent">In this chapter, you replicated an old-time blink comparator device and then updated the process using modern computer vision techniques. Along the way, you used the <span class="literal">pathLib</span> module to simplify working with directory paths, and you used a single underscore for insignificant, unused variable names. You also used OpenCV to find, describe, and match interesting features in images, align the features with homography, blend the images together, and write the result to a file.</p>&#13;
<h3 class="h3" id="ch00lev1sec37"><strong>Further Reading</strong></h3>&#13;
<p class="noindent"><em>Out of the Darkness: The Planet Pluto</em> (Stackpole Books, 2017), by Clyde Tombaugh and Patrick Moore, is the standard reference on the discovery of Pluto, told in the discoverer’s own words.</p>&#13;
<p class="indent"><em>Chasing New Horizons: Inside the Epic First Mission to Pluto</em> (Picador, 2018), by Alan Stern and David Grinspoon, records the monumental effort to finally send a spacecraft—which, incidentally, contained Clyde Tombaugh’s ashes—to Pluto.</p>&#13;
<h3 class="h3" id="ch00lev1sec38"><strong>Practice Project: Plotting the Orbital Path</strong></h3>&#13;
<p class="noindent">Edit the <em>transient_detector.py</em> program so that if the transient is present in both input image pairs, OpenCV draws a line connecting the two transients. This will reveal the transient’s orbital path against the background stars.</p>&#13;
<p class="indent">This kind of information was key to the discovery of Pluto. Clyde Tombaugh used the distance Pluto traveled in the two discovery plates, along with the time between exposures, to verify that the planet was near Lowell’s predicted path and not just some asteroid orbiting closer to Earth.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_120"/>You can find a solution, <em>practice_orbital_path.py</em>, in the appendix and in the <em>Chapter_5</em> folder.</p>&#13;
<h3 class="h3" id="ch00lev1sec39"><strong>Practice Project: What’s the Difference?</strong></h3>&#13;
<p class="noindent">The feature matching you did in this chapter has broad-reaching applications beyond astronomy. For example, marine biologists use similar techniques  to identify whale sharks by their spots. This improves the accuracy of the scientists’ population counts.</p>&#13;
<p class="indent">In <a href="ch05.xhtml#ch05fig14">Figure 5-14</a>, something has changed between the left and right photos. Can you spot it? Even better, can you write Python programs that align and compare the two images and circle the change?</p>&#13;
<div class="image"><img src="../images/fig05_14.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch05fig14"/>Figure 5-14: Spot the difference between the left and right images.</p>&#13;
<p class="indent">The starting images can be found in the <em>montages</em> folder in the <em>Chapter_5</em> folder, downloadable from the book’s website. These are color images that you’ll need to convert to grayscale and align prior to object detection. You can find solutions, <em>practice_montage_aligner.py</em> and <em>practice_montage_difference_finder.py</em>, in the appendix and in the <em>montages</em> folder.</p>&#13;
<h3 class="h3" id="ch00lev1sec40"><strong>Challenge Project: Counting Stars</strong></h3>&#13;
<p class="noindent">According to <em>Sky and Telescope</em> magazine, there are 9,096 stars visible to the naked eye from both hemispheres (<em><a href="https://www.skyandtelescope.com/astronomy-resources/how-many-stars-night-sky-09172014/">https://www.skyandtelescope.com/astronomy-resources/how-many-stars-night-sky-09172014/</a></em>). That’s a lot on its own, but if you look through a telescope, the number increases exponentially.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_121"/>To estimate large numbers of stars, astronomers survey small regions of the sky, use a computer program to count the stars, and then extrapolate the results to larger areas. For this challenge project, pretend you’re an assistant at Lowell Observatory and you’re on a survey team. Write a Python program that counts the number of stars in the image <em>5_no_transient_left.png</em>, used in Projects 7 and 8.</p>&#13;
<p class="indent">For hints, search online for <em>how to count dots in an image with Python and OpenCV</em>. For a solution using Python and SciPy, see <em><a href="http://prancer.physics.louisville.edu/astrowiki/index.php/Image_processing_with_Python_and_SciPy">http://prancer.physics.louisville.edu/astrowiki/index.php/Image_processing_with_Python_and_SciPy</a></em>. You may find your results improve if you divide the image into smaller parts.</p>&#13;
</body></html>