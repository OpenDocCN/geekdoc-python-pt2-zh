- en: '**2'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2'
- en: THE TEXT-PROCESSING PIPELINE**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理管道**
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: Now that you understand the structure of an NLP application, it’s time to see
    these underlying concepts in action. In this chapter, you’ll install spaCy and
    set up your working environment. Then you’ll learn about the *text-processing
    pipeline*, a series of basic NLP operations you’ll use to determine the meaning
    and intent of a discourse. These operations include tokenization, lemmatization,
    part-of-speech tagging, syntactic dependency parsing, and named entity recognition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了 NLP 应用程序的结构，是时候在实际操作中看到这些基础概念的应用了。在本章中，你将安装 spaCy 并设置工作环境。接着，你将学习 *文本处理管道*，这是你用来确定话语的含义和意图的一系列基本
    NLP 操作。这些操作包括分词、词形还原、词性标注、句法依赖解析和命名实体识别。
- en: '**Setting Up Your Working Environment**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**设置工作环境**'
- en: 'Before you start using spaCy, you need to set up a working environment by installing
    the following software components on your machine:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 spaCy 之前，你需要通过在机器上安装以下软件组件来设置工作环境：
- en: Python 2.7 or later, or 3.4 or later
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 2.7 或更高版本，或 3.4 或更高版本
- en: The spaCy library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy 库
- en: A statistical model for spaCy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy 的统计模型
- en: 'You’ll need Python 2.7 or later, or 3.4 or later to use spaCy v2.0.*x*. Download
    it at *[https://www.python.org/downloads/](https://www.python.org/downloads/)*
    and follow the instructions to set up a Python environment. Next, install spaCy
    in your Python environment using `pip` by running the following command:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要 Python 2.7 或更高版本，或 3.4 或更高版本才能使用 spaCy v2.0.*x*。可以在 *[https://www.python.org/downloads/](https://www.python.org/downloads/)*
    下载并按照说明设置 Python 环境。接下来，在你的 Python 环境中使用 `pip` 安装 spaCy，运行以下命令：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you have more than one Python installation on your system, select the `pip`
    executable associated with the Python installation you want to use. For instance,
    if you want to use spaCy with Python 3.5, you’d run the following command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统上有多个 Python 安装，请选择与你想要使用的 Python 安装相关的 `pip` 可执行文件。例如，如果你想在 Python 3.5
    中使用 spaCy，应该运行以下命令：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you already have spaCy installed on your system, you might want to upgrade
    it to a new release. The examples in this book assume you use spaCy v2.0.*x* or
    later. You can verify which version of spaCy you have installed with the following
    command:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统中已经安装了 spaCy，你可能想将其升级到新版本。本书中的示例假设你使用的是 spaCy v2.0.*x* 或更高版本。你可以使用以下命令验证安装的
    spaCy 版本：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once again, you might need to replace the `python` command with the command
    for the python executable used in your particular environment, say, `python3.5`.
    From now on, we’ll use `python` and `pip` regardless of the executables your system
    uses.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要将 `python` 命令替换为适用于你特定环境中的 Python 可执行文件的命令，例如 `python3.5`。从现在开始，我们将使用 `python`
    和 `pip`，无论你的系统使用的是哪个可执行文件。
- en: 'If you decide to upgrade your installed spaCy package to the latest version,
    you can do this using the following `pip` command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定将已安装的 spaCy 包升级到最新版本，可以使用以下 `pip` 命令进行升级：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Installing Statistical Models for spaCy**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**安装 spaCy 的统计模型**'
- en: The spaCy installation doesn’t include statistical models that you’ll need when
    you start using the library. The statistical models contain knowledge collected
    about the particular language from a set of sources. You must separately download
    and install each model you want to use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 安装包并不包括你开始使用库时所需的统计模型。这些统计模型包含从一组来源收集的关于特定语言的知识。你必须单独下载并安装每个你想要使用的模型。
- en: 'Several pretrained statistical models are available for different languages.
    For English, for example, the following models are available for download from
    spaCy’s website: `en_core_web_sm`, `en_core_web_md`, `en_core_web_lg`, and `en_vectors_web_lg`.
    The models use the following naming convention: *lang_type_genre_size*. *Lang*
    specifies the language. *Type* indicates the model’s capabilities (for example,
    core is a general-purpose model that includes vocabulary, syntax, entities, and
    vectors). *Genre* indicates the type of text the model has been trained on: web
    (such as Wikipedia or similar media resources) or news (news articles). *Size*
    indicates how large the model is: `lg` is large, `md` is medium, and `sm` is small.
    The larger the model is, the more disk space it requires. For example, the `en_vectors_web_lg-2.1.0`
    model takes 631MB, whereas `en_core_web_sm-2.1.0` takes only 10MB.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个预训练的统计模型可供不同语言使用。例如，针对英语，可以从spaCy的网站下载以下模型：`en_core_web_sm`、`en_core_web_md`、`en_core_web_lg`
    和 `en_vectors_web_lg`。这些模型遵循以下命名规则：*lang_type_genre_size*。*Lang*指定语言。*Type*表示模型的功能（例如，core是一个通用模型，包括词汇、语法、实体和向量）。*Genre*表示模型训练的文本类型：web（如Wikipedia或类似的媒体资源）或news（新闻文章）。*Size*表示模型的大小：`lg`是大型，`md`是中型，`sm`是小型。模型越大，所需的磁盘空间也越大。例如，`en_vectors_web_lg-2.1.0`模型需要631MB，而`en_core_web_sm-2.1.0`只需要10MB。
- en: 'To follow along with the examples provided in this book, `en_core_web_sm` (the
    most lightweight model) will work fine. spaCy will choose it by default when you
    use spaCy’s download command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本书中提供的示例，`en_core_web_sm`（最轻量的模型）就能很好地工作。当你使用spaCy的下载命令时，spaCy会默认选择这个模型：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `en` shortcut link in the command instructs spaCy to download and install
    the best-matching default model for the English language. The best-matching model,
    in this context, means the one that is generated for the specified language (English
    in this example), a general purpose model, and the most lightweight.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 命令中的`en`快捷链接指示spaCy下载并安装适用于英语的最佳默认模型。在这个上下文中，最佳匹配的模型是指为指定语言（例如英语）生成的通用模型，且是最轻量的。
- en: 'To download a specific model, you must specify its name, like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载特定的模型，您必须指定其名称，如下所示：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once installed, you can load the model using this same shortcut you specified
    during the installation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你可以使用安装时指定的相同快捷链接加载模型：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Basic NLP Operations with spaCy**'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用spaCy的基本NLP操作**'
- en: Let’s begin by performing a chain of basic NLP operations that we call a processing
    pipeline. spaCy does all these operations for you behind the scenes, allowing
    you to concentrate on your application’s specific logic. [Figure 2-1](../Text/ch02.xhtml#ch02fig01)
    provides a simplified depiction of this process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始执行一系列基本的NLP操作，我们称之为处理管道。spaCy会在后台为你执行所有这些操作，让你能够专注于应用程序的特定逻辑。[图2-1](../Text/ch02.xhtml#ch02fig01)提供了该过程的简化示意图。
- en: '![image](../Images/fig2-1.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-1.jpg)'
- en: '*Figure 2-1: A high-level view of the processing pipeline*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2-1：处理管道的高层视图*'
- en: The processing pipeline typically includes tokenization, lemmatization, part-of-speech
    tagging, syntactic dependency parsing, and named entity recognition. We’ll introduce
    each of these tasks in this section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 处理管道通常包括分词、词形还原、词性标注、句法依存分析和命名实体识别。我们将在本节中介绍这些任务。
- en: '***Tokenization***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***分词***'
- en: The very first action any NLP application typically performs on a text is parsing
    that text into *tokens*, which can be words, numbers, or punctuation marks. Tokenization
    is the first operation because all the other operations require you to have tokens
    already in place.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 任何自然语言处理（NLP）应用程序通常对文本执行的第一个操作是将文本解析为*词元*，词元可以是单词、数字或标点符号。分词是第一个操作，因为所有其他操作都需要你首先获取词元。
- en: 'The following code shows the tokenization process:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了分词过程：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We start by importing the spaCy library ➊ to gain access to its functionality.
    Then, we load a model package using the `en` shortcut link ➋ to create an instance
    of spaCy’s Language class. A Language object contains the language’s vocabulary
    and other data from the statistical model. We call the Language object `nlp`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过导入spaCy库 ➊ 来访问它的功能。然后，我们使用`en`快捷链接 ➋ 加载一个模型包，以创建一个spaCy语言类的实例。语言对象包含该语言的词汇和来自统计模型的其他数据。我们将这个语言对象命名为`nlp`。
- en: Next, we apply the object just created ➌ to a sample sentence, creating a *Doc
    object* instance. A Doc object is a container for a sequence of Token objects.
    spaCy generates it implicitly based on the text you provide it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将刚刚创建的对象 ③ 应用到示例句子中，创建一个*Doc对象*实例。Doc对象是一个用于存储一系列Token对象的容器。spaCy会根据你提供的文本隐式地生成它。
- en: At this point, with just three lines of code, spaCy has generated the grammatical
    structure for the sample sentence. How you’ll use it is entirely up to you. In
    this very simple example, you just print out the *text content* of each token
    from the sample sentence ➍.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，仅用三行代码，spaCy已经生成了示例句子的语法结构。你如何使用它完全取决于你自己。在这个非常简单的示例中，你只需要打印出每个标记的*文本内容*
    ④。
- en: 'The script outputs the sample sentence’s tokens as a list:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将示例句子的标记输出为一个列表：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The *text content*—the group of characters that compose the token, such as the
    letters “a” and “m” in the token “am”—is just one of many properties of a Token
    object. You can also extract various linguistic features assigned to a token,
    as you’ll see in the following examples.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本内容*——组成标记的字符组，例如标记“am”中的字母“a”和“m”——是Token对象的许多属性之一。你还可以提取分配给标记的各种语言特征，正如你将在以下示例中看到的那样。'
- en: '***Lemmatization***'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词形还原***'
- en: 'A *lemma* is the base form of a token. You can think of it as the form in which
    the token would appear if it were listed in a dictionary. For example, the lemma
    for the token “flying” is “fly.” *Lemmatization* is the process of reducing word
    forms to their lemma. The following script provides a simple example of how to
    do lemmatization with spaCy:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*词形*是一个标记的基本形式。你可以把它看作是标记如果列在字典中会出现的形式。例如，“flying”这个标记的词形是“fly”。*词形还原*是将单词形式还原为其词形的过程。以下脚本提供了一个使用spaCy进行词形还原的简单示例：'
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first three lines in the script are the same as those in the previous script.
    Recall that they import the spaCy library, load an English model using the `en`
    shortcut and create a text-processing pipeline, and apply the pipeline to a sample
    sentence—creating a Doc object through which you can access the grammatical structure
    of the sentence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本中的前三行与之前的脚本相同。回顾一下，这些行导入了spaCy库，使用`en`快捷方式加载了一个英文模型，创建了一个文本处理管道，并将管道应用于示例句子——通过创建一个Doc对象，你可以访问句子的语法结构。
- en: '**NOTE**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*In grammar, sentence structure is the arrangement of individual words, as
    well as phrases and clauses in a sentence. The grammatical meaning of a sentence
    depends on this structural organization.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*在语法中，句子结构是指单个单词、短语和从句在句子中的排列方式。句子的语法意义取决于这种结构组织。*'
- en: 'Once you have a Doc object containing the tokens from your example sentence,
    you iterate over those tokens in a loop, and then print out a token’s text content
    ➊ along with its corresponding lemma ➋. This script produces the following output
    (I’ve tabulated it to make it more readable):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了包含示例句子标记的Doc对象，你就可以在循环中遍历这些标记，并打印出标记的文本内容 ➊ 以及其对应的词形 ➋。该脚本将输出如下结果（为了便于阅读，我已将其整理成表格）：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The column on the left contains the tokens, and the column on the right contains
    their lemmas.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧列包含标记，右侧列包含它们的词形。
- en: '***Applying Lemmatization for Meaning Recognition***'
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***应用词形还原进行语义识别***'
- en: 'Lemmatization is an important step in the task of meaning recognition. To see
    how, let’s return to the sample sentence from the previous section:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是语义识别任务中的一个重要步骤。为了理解这一点，让我们回到上一节中的示例句子：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Suppose this sentence was submitted to an NLP application interacting with an
    online system that provides an API for booking tickets for trips. The application
    processes a customer’s request, extracting necessary information from it and then
    passing on that information to the underlying API. This design might look like
    the one depicted in [Figure 2-2](../Text/ch02.xhtml#ch02fig02).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个句子被提交到一个与在线系统交互的NLP应用程序，该系统提供了一个API用于预定旅行票务。该应用程序处理客户的请求，从中提取必要的信息，然后将信息传递给底层API。这个设计可能看起来像[图2-2](../Text/ch02.xhtml#ch02fig02)所示。
- en: '![image](../Images/fig2-2.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-2.jpg)'
- en: '*Figure 2-2: Using lemmatization in the process of extracting necessary information
    from a customer’s request*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2-2：在提取客户请求中必要信息的过程中使用词形还原*'
- en: 'The NLP application tries to get the following information from a customer’s
    request: a form of travel (plane, rail, bus, and so on) and a destination. The
    application needs to first determine whether the customer wants an air ticket,
    a railway ticket, or a bus ticket. To determine this, the application searches
    for a word that matches one of the keywords in the predefined list. An easy way
    to simplify the search for these keywords is to first convert all the words in
    a sentence being processed to their lemmas. In that case, the predefined list
    of keywords will be much shorter and clearer. For example, you won’t need to include
    all the word forms of the word fly (such as “fly,” “flying,” “flew,” and “flown”)
    to serve as an indicator that the customer wants an air ticket, reducing all possible
    variants to the base form of the word—that is, “fly.”'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理应用尝试从客户请求中获取以下信息：一种旅行方式（飞机、火车、公交等）和目的地。应用程序首先需要确定客户是想要机票、火车票还是公交票。为此，应用程序会搜索与预定义关键词列表中的某个关键词匹配的单词。简化这些关键词搜索的一个简单方法是，首先将句子中的所有单词转换为它们的词元。这样，预定义的关键词列表就会简短而清晰。例如，你不需要包括“fly”这个词的所有词形（如“fly”，“flying”，“flew”和“flown”），就能作为客户想要机票的指示，从而将所有可能的变体简化为该词的基础形式——也就是“fly”。
- en: Lemmatization also comes in handy when the application tries to determine a
    destination from a submitted request. There are a lot of nicknames for the globe’s
    cities. But the system that books the tickets requires official names. Of course,
    the default Tokenizer that performs lemmatization won’t know the difference between
    nicknames and official names for cities, countries, and so on. To solve this problem,
    you can add special case rules to an existing Tokenizer instance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 词元化在应用程序尝试从提交的请求中确定目的地时也非常有用。全球城市有很多昵称。但系统需要的是官方名称，预设的执行词元化的分词器无法区分城市、国家等的昵称和官方名称。为了解决这个问题，你可以向现有的分词器实例添加特例规则。
- en: The following script illustrates how you might implement lemmatization for the
    destination cities example. It prints out the lemmas of the words composing the
    sentence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本演示了如何实现目的地城市的词元化。它会打印出组成句子的单词的词元。
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You define a *special case* for the word `Frisco` ➊ by replacing its default
    lemma with `San Francisco`. Then you add this special case to the Tokenizer instance
    ➋. Once added, the Tokenizer instance will use this special case each time it’s
    asked for the lemma of `Frisco`. To make sure that everything works as expected,
    you print out the lemmas of the words in the sentence ➌.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你为单词`Frisco`定义了一个*特例* ➊，通过将其默认词元替换为`San Francisco`。然后你将这个特例添加到分词器实例中 ➋。一旦添加，分词器实例将在每次请求`Frisco`的词元时使用这个特例。为了确保一切按预期工作，你可以打印出句子中单词的词元
    ➌。
- en: 'The script generates the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本生成以下输出：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output lists the lemmas for all words occurring in the sentence with the
    exception of `Frisco`, for which it lists `San Francisco`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出列出了句子中所有单词的词元（lemmas），但不包括`Frisco`，对于`Frisco`，它列出的是`San Francisco`。
- en: '***Part-of-Speech Tagging***'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词性标注***'
- en: A *part-of-speech tag* tells you the part-of-speech (noun, verb, and so on)
    of a given word in a given sentence. (Recall from [Chapter 1](../Text/ch01.xhtml#ch01)
    that a word can act as more than one part of speech depending on the context in
    which it appears.)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*词性标注*告诉你在特定句子中给定单词的词性（名词、动词等）。（回想一下[第1章](../Text/ch01.xhtml#ch01)，一个单词根据出现的上下文可以作为多个词性。）'
- en: 'In spaCy, part-of-speech tags can include detailed information about a token.
    In the case of verbs, they might tell you the following features: tense (past,
    present, or future), aspect (simple, progressive, or perfect), person (1st, 2nd,
    or 3rd), and number (singular or plural).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，词性标注可以包含关于一个标记（token）的详细信息。对于动词，可能会告诉你以下特征：时态（过去、现在或将来）、体（简单、进行时或完成时）、人称（第一、第二或第三）和数（单数或复数）。
- en: 'Extracting these verb part-of-speech tags can help identify a user’s intent
    when tokenization and lemmatization alone aren’t sufficient. For instance, the
    lemmatization script for the ticket booking application in the preceding section
    won’t decide how the NLP application chooses words in a sentence to compose a
    request to the underlying API. In a real situation, doing so might be quite complicated.
    For example, a customer’s request might consist of more than one sentence:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 提取这些动词词性标签可以帮助在仅进行标记化和词形还原不足以判断时识别用户意图。例如，前一节中的机票预订应用程序的词形还原脚本并不能决定 NLP 应用程序如何选择句子中的词汇来构建请求底层
    API 的请求。在实际情况中，做出这样的决定可能相当复杂。例如，客户的请求可能包含多于一个句子：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For these sentences, the results of lemmatization would be as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些句子，词形还原的结果如下：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Performing lemmatization alone isn’t enough here; the application might consider
    the lemmas “fly” and “LA” from the first sentence as the keywords, indicating
    that the customer intends to fly to LA when in fact the customer intends to fly
    to San Francisco. Part of the problem is that lemmatization changes verbs to their
    infinitive forms, making it hard to know the role they play in a sentence.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 仅执行词形还原并不足够；应用程序可能会将第一个句子中的“fly”和“LA”作为关键字，从而表示客户打算飞往洛杉矶，而实际上客户打算飞往旧金山。问题的部分原因是词形还原会将动词转换为其原形形式，这使得很难知道它们在句子中扮演的角色。
- en: This is where part-of-speech tags come into play. In English, the core parts
    of speech include noun, pronoun, determiner, adjective, verb, adverb, preposition,
    conjunction, and interjection. (See the linguistic primer in the appendix for
    more information about these parts of speech.) In spaCy, these same categories—plus
    some additional ones for symbols, punctuation marks, and others—are called *coarse-grained
    parts of speech* and are available as a fixed set of tags through the `Token.pos`
    (int) and `Token.pos_` (unicode) attributes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是词性标签发挥作用的地方。在英语中，核心词性包括名词、代词、限定词、形容词、动词、副词、介词、连词和感叹词。（有关这些词性的更多信息，请参阅附录中的语言学入门。）在
    spaCy 中，这些相同的类别——以及一些额外的符号、标点符号等类别——被称为*粗粒度词性*，并通过 `Token.pos`（整数）和 `Token.pos_`（unicode）属性以固定标签集的形式提供。
- en: Also, spaCy offers *fine-grained parts of speech* tags that provide more detailed
    information about a token, covering morphological features, such as verb tenses
    and types of pronouns. Naturally, the list of fine-grained parts of speech contains
    many more tags than the coarse-grained list. The fine-grained part-of-speech tags
    are available as the `Token.tag` (int) and `Token.tag_` (unicode) attributes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，spaCy 提供了*细粒度词性*标签，它们提供了关于标记的更详细的信息，涵盖了形态学特征，例如动词时态和代词类型。显然，细粒度词性标签的列表包含了比粗粒度列表更多的标签。细粒度词性标签可以通过
    `Token.tag`（整数）和 `Token.tag_`（unicode）属性获得。
- en: '[Table 2-1](../Text/ch02.xhtml#ch02tab01) lists some of the common part-of-speech
    tags used in spaCy for English models.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2-1](../Text/ch02.xhtml#ch02tab01) 列出了 spaCy 中用于英文模型的一些常见词性标签。'
- en: '**Table 2-1:** Some Common spaCy Part-of-Speech Tags'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2-1：** 一些常见的 spaCy 词性标签'
- en: '| **TAG (fine-grained part of speech)** | **POS (coarse-grained part of speech)**
    | **Morphology** | **Description** |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **标签（细粒度词性）** | **词性（粗粒度词性）** | **形态学** | **描述** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `NN` | NOUN | Number=sing | Noun, singular |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `NN` | 名词 | 数量=单数 | 名词，单数 |'
- en: '| `NNS` | NOUN | Number=plur | Noun, plural |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `NNS` | 名词 | 数量=复数 | 名词，复数 |'
- en: '| `PRP` | PRON | PronType=prs | Pronoun, personal |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `PRP` | 代词 | 代词类型=人称 | 代词，人称代词 |'
- en: '| `PRP$` | PRON | PronType=prs Poss=yes | Pronoun, possessive |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `PRP$` | 代词 | 代词类型=人称 有主格 | 代词，所有格 |'
- en: '| `VB` | VERB | VerbForm=inf | Verb, base form |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `VB` | 动词 | 动词形式=原形 | 动词，基础形式 |'
- en: '| `VBD` | VERB | VerbForm=fin Tense=past | Verb, past tense |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `VBD` | 动词 | 动词形式=陈述 时态=过去 | 动词，过去时 |'
- en: '| `VBG` | VERB | VerbForm=part Tense=pres'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '| `VBG` | 动词 | 动词形式=进行时 时态=现在 |'
- en: Aspect=prog | Verb, gerund, or present participle |
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 方面=进行时 | 动词，动名词或现在分词 |
- en: '| `JJ` | ADJ | Degree=pos | Adjective |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `JJ` | 形容词 | 程度=原级 | 形容词 |'
- en: '**NOTE**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*You can find the entire list of the fine-grained part-of-speech tags used
    in spaCy in the “[Part-of-Speech Tagging](../Text/ch02.xhtml#lev19)” section in
    the Annotation Specifications manual at* *[https://spacy.io/api/annotation#pos-tagging](https://spacy.io/api/annotation#pos-tagging)*.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以在“[词性标注](../Text/ch02.xhtml#lev19)”部分的注释规范手册中找到 spaCy 中使用的所有细粒度词性标签，手册链接为*
    *[https://spacy.io/api/annotation#pos-tagging](https://spacy.io/api/annotation#pos-tagging)*。'
- en: Tense and aspect are perhaps the most interesting properties of verbs for NLP
    applications. Together, they indicate a verb’s reference to a position in time.
    For example, we use the *present tense progressive aspect* form of a verb to describe
    what is happening right now or what will happen in the near future. To form the
    present tense progressive aspect verb, you add the present tense form of the verb
    “to be” before an -ing verb. For example, in the sentence “I am looking into it,”
    you add “am”—the form of the verb “to be” in the first person, present tense—before
    the -ing verb “looking.” In this example, “am” indicates the present tense and
    “looking” points to the progressive aspect.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 时态和体是自然语言处理应用中最有趣的动词特性之一。它们一起表示动词与时间的关系。例如，我们使用动词的*现在时进行体*形式来描述当前正在发生的事情或即将发生的事情。要形成现在时进行体动词，你需要在动词“to
    be”的现在时形式前加上-ing动词。例如，在句子“I am looking into it.”中，你在动词“looking”前加上“am”——这是动词“to
    be”在第一人称单数的现在时形式。在这个例子中，“am”表示现在时，“looking”指向进行时体。
- en: '***Using Part-of-Speech Tags to Find Relevant Verbs***'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用词性标签来寻找相关动词***'
- en: The ticket booking application could use the fine-grained part-of speech tags
    available in spaCy to filter the verbs in the discourse, choosing only those that
    could be key to determining the customer’s intent.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 票务预订应用可以使用spaCy提供的细粒度词性标签来过滤对话中的动词，只选择那些可能有助于确定顾客意图的动词。
- en: 'Before moving onto the code for this process, let’s try to figure out what
    kind of utterances a customer might use to express their intention to book a plane
    ticket to, say, LA. We could start by looking at some sentences that contain the
    following combination of lemmas: “fly”, “to”, and “LA”. Here are some simple options:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入这个过程的代码之前，让我们尝试弄清楚顾客可能会用什么样的表达来表示他们想预订飞往LA的机票。我们可以通过查找包含以下词根组合的句子来开始：“fly”、“to”和“LA”。这里有一些简单的选项：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that although all of these sentences would include the “fly to LA” combination
    if reduced to lemmas, only some of them imply the customer’s intent to book a
    plane ticket to LA. The first two definitely aren’t suitable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然这些句子如果简化为词根形式都会包含“fly to LA”组合，但只有其中一部分句子暗示顾客有预订飞往LA的机票的意图。前两句显然不合适。
- en: 'A quick analysis reveals that the past and past perfect forms of the verb “fly”—the
    tenses used in the first two sentences—don’t imply the intent we’re looking for.
    Only the infinitive and present progressive forms are suitable. The following
    script illustrates how to find those forms in the sample discourse:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 快速分析表明，动词“fly”的过去式和过去完成式——这两个时态出现在前两句——并不暗示我们要寻找的意图。只有不定式和现在进行时形式才是合适的。以下脚本演示了如何在示例对话中找到这些形式：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `tag_` property ➊ of a Token object contains the fine-grained part-of-speech
    attribute assigned to that object. You use a loop performed over the tokens composing
    the discourse to check whether the fine-grained part-of-speech tag assigned to
    a token is `VB` (a verb in the base, or infinitive, form) ➌ or `VBG` (a verb in
    the present progressive form) ➋.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`tag_`属性 ➊ 是Token对象的一个属性，包含分配给该对象的细粒度词性标签。你可以对构成对话的tokens执行循环，检查是否分配给某个token的细粒度词性标签是`VB`（动词的不定式形式）
    ➌ 或 `VBG`（动词的现在进行时形式） ➋。'
- en: 'In the sample discourse, only the verb “flying” in the second sentence meets
    the specified condition. So you should see the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例对话中，只有第二句中的动词“flying”符合指定条件。所以你应该看到以下输出：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Of course, fine-grained part-of-speech tags aren’t only assigned to verbs;
    they’re also assigned to the other parts of speech in a sentence. For example,
    spaCy would recognize LA and Frisco as proper nouns—nouns that are the names of
    individuals, places, objects, or organizations—and tag them with `PROPN`. If you
    wanted, you could add the following line of code to the previous script:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，细粒度的词性标签不仅仅是分配给动词的；它们也分配给句子中的其他词性。例如，spaCy会将“LA”和“Frisco”识别为专有名词——表示人名、地名、物体或组织名称的名词——并用`PROPN`标签标记它们。如果你想，你可以将以下代码行添加到之前的脚本中：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Adding that code should output the following list:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 添加该代码应该输出以下列表：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The proper nouns from both sentences of the sample discourse are in the list.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 示例对话中的专有名词出现在列表中。
- en: '***Context Is Important***'
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***上下文很重要***'
- en: 'Fine-grained part-of-speech tags might not always be enough to determine an
    utterance’s meaning. For this, you might still need to rely on context. As an
    example, consider the following utterance: “I am flying to LA.” The part-of-speech
    tagger will assign the `VBG` tag to the verb “flying” in this example, because
    it’s in the present progressive form. But because we use this verb form to describe
    either what is happening right now or what will happen in the near future, the
    utterance might mean either “I’m already in the sky, flying to LA.” or “I’m going
    to fly to LA.” When submitted to the ticket booking NLP application, the application
    should interpret only one of these sentences as “I need an air ticket to LA.”
    Similarly, consider the following discourse: “I am flying to LA. In the evening,
    I have to be back in Frisco.” This most likely implies that the speaker wants
    an air ticket from LA to Frisco for an evening flight. You’ll find more examples
    about recognizing meaning based on context in “[Using Context to Improve the Ticket-Booking
    Chatbot](../Text/ch06.xhtml#lev85)” on [page 91](../Text/ch06.xhtml#page_91).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度的词性标签可能并不总是足以确定话语的含义。为此，你可能仍然需要依赖上下文。举个例子，考虑以下话语：“I am flying to LA.” 在这个例子中，词性标注器会将动词“flying”标注为
    `VBG`，因为它是现在进行时。但由于我们使用这种动词形式来描述正在发生的事情或即将发生的事情，这句话的含义可能是“我已经在天上，飞往洛杉矶。”或者“我要飞往洛杉矶。”当提交给机票预订
    NLP 应用程序时，该应用程序应该仅将其中一句解读为“我需要一张去洛杉矶的机票。”同样，考虑以下话语：“I am flying to LA. In the
    evening, I have to be back in Frisco.” 这很可能意味着说话者想要一张从洛杉矶飞往弗里斯科的晚班机票。你可以在[《使用上下文改善机票预订聊天机器人》](../Text/ch06.xhtml#lev85)的[第91页](../Text/ch06.xhtml#page_91)中找到更多关于基于上下文识别含义的例子。
- en: '***Syntactic Relations***'
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***句法关系***'
- en: 'Now let’s combine the proper nouns with the verb that the part-of-speech tagger
    selected earlier. Recall that the list of verbs you could potentially use to identify
    the intent of the discourse contains only the verb “flying” in the second sentence.
    How can you get the verb/proper noun pair that best describes the intent behind
    the discourse? A human would obviously compose the verb/proper noun pairs from
    words found in the same sentence. Because the verb “flown” in the first sentence
    doesn’t meet the condition specified (remember that only infinitive and present
    progressive forms meet the condition), you’d be able to compose such a pair for
    the second sentence only: “flying, Frisco.”'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将适当的专有名词与之前词性标注器选出的动词结合起来。回想一下，你可以用来识别话语意图的动词列表中，在第二个句子中只有“flying”这个动词。那么如何获得最佳描述话语背后意图的动词/专有名词组合呢？人类显然会将来自同一句话的动词和专有名词组合在一起。因为第一句中的动词“flown”不符合指定的条件（记住，只有不定式和现在进行时形式符合条件），所以你只能为第二句组合这样的词对：“flying,
    Frisco”。
- en: To handle these situations programmatically, spaCy features a syntactic dependency
    parser that discovers syntactic relations between individual tokens in a sentence
    and connects syntactically related pairs of words with a single arc.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以编程方式处理这些情况，spaCy 提供了一个句法依赖解析器，它能够发现句子中各个词汇之间的句法关系，并通过单一的弧线将句法相关的词对连接起来。
- en: Like lemmas and part-of-speech tags discussed in the previous sections, *syntactic
    dependency labels* are linguistic features that spaCy assigns to the Token objects
    that make up a text contained in a Doc object. For example, the dependency label
    `dobj` stands for “direct object.” We could illustrate the syntactic relation
    it represents as an arrow arc, as shown in [Figure 2-3](../Text/ch02.xhtml#ch02fig03).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面章节中讨论的词元和词性标签一样，*句法依赖标签*是 spaCy 分配给构成 Doc 对象中文本的 Token 对象的语言学特征。例如，依赖标签
    `dobj` 代表“直接宾语”。我们可以通过[图 2-3](../Text/ch02.xhtml#ch02fig03)中所示的箭头弧线来说明它所表示的句法关系。
- en: '**HEAD AND CHILD**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**主词与子词**'
- en: 'A syntactic dependency label describes the type of syntactic relation between
    two words in a sentence. In such a pair, one word is the syntactic governor (also
    called the head or parent) and the other is the dependent (also called the child).
    spaCy assigns a syntactic dependency label to the pair’s dependent. For example,
    in the pair “need, ticket,” extracted from the sentence “I need a plane ticket,”
    the word “ticket” is the child and word “need” is the head, because “need” is
    the verb in what’s called a verb phrase. In this same sentence, “a plane ticket”
    is a noun phrase: the noun “ticket” is the head, and “a” and “plane” are its children.
    To learn more, consult “[Dependency Grammars vs. Phrase Structure Grammars](../Text/app01.xhtml#lev173)”
    on [page 185](../Text/app01.xhtml#page_185).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 句法依存关系标签描述了句子中两个单词之间的句法关系类型。在这样的对中，一个单词是句法上的支配词（也叫做主词或父项），另一个是依存词（也叫做子项）。spaCy
    将句法依存关系标签分配给对中的依存词。例如，在“need, ticket”这一对中，提取自句子“I need a plane ticket”，单词“ticket”是子项，而“need”是主词，因为“need”是所谓的动词短语中的动词。在同一个句子中，“a
    plane ticket”是名词短语：名词“ticket”是主词，而“a”和“plane”是它的子项。要了解更多，请查阅[《依存语法与短语结构语法》](../Text/app01.xhtml#lev173)，见[第185页](../Text/app01.xhtml#page_185)。
- en: Each word in a sentence has exactly one head. Consequently, a word can be a
    child only to one head. The opposite is not always the case. The same word can
    act as a head in none, one, or several pairs. The latter means that the head has
    several children. This explains why a dependency label is always assigned to the
    child.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 每个句子中的每个单词都有一个确切的主词。因此，一个单词只能作为一个主词的子项。相反的情况并不总是成立。同一个单词可以作为多个对中的主词。后者意味着该主词有多个子项。这也解释了为什么依存关系标签总是赋给子项。
- en: '![image](../Images/fig2-3.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-3.jpg)'
- en: '*Figure 2-3: A graphical representation of a syntactic dependency arc*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2-3：句法依存弧的图示表示*'
- en: The `dobj` label is assigned to the word “ticket” because it’s the child of
    the relation. A dependency label is always assigned to the child. In your script,
    you can determine the head of a relation using the `Token.head` attribute.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`dobj`标签分配给单词“ticket”，因为它是该关系的子项。依存关系标签总是赋给子项。在你的脚本中，你可以使用`Token.head`属性来确定关系的主词。'
- en: You might also want to look at the other head/child relations in the sentence,
    like the ones shown in [Figure 2-4](../Text/ch02.xhtml#ch02fig04).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想查看句子中其他的主词/子项关系，像[图2-4](../Text/ch02.xhtml#ch02fig04)中展示的那些关系。
- en: '![image](../Images/fig2-4.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-4.jpg)'
- en: '*Figure 2-4: Head/child relations in an entire sentence*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2-4：整个句子中的主词/子项关系*'
- en: As you can see, the same word in a sentence can participate in several syntactic
    relations. [Table 2-2](../Text/ch02.xhtml#ch02tab02) lists some of the most commonly
    used English dependency labels.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，句子中的同一个单词可以参与多种句法关系。[表2-2](../Text/ch02.xhtml#ch02tab02)列出了最常用的一些英语依存关系标签。
- en: '**Table 2-2:** Some Common Dependency Labels'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2-2：** 一些常见的依存关系标签'
- en: '| **Dependency label** | **Description** |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **依存关系标签** | **描述** |'
- en: '| --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `acomp` | Adjectival complement |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `acomp` | 形容词补语 |'
- en: '| `amod` | Adjectival modifier |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `amod` | 形容词修饰语 |'
- en: '| `aux` | Auxiliary |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `aux` | 助动词 |'
- en: '| `compound` | Compound |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `compound` | 复合词 |'
- en: '| `dative` | Dative |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `dative` | 与格 |'
- en: '| `det` | Determiner |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `det` | 限定词 |'
- en: '| `dobj` | Direct object |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| `dobj` | 直接宾语 |'
- en: '| `nsubj` | Nominal subject |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `nsubj` | 名词主语 |'
- en: '| `pobj` | Object of preposition |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| `pobj` | 介词宾语 |'
- en: '| `ROOT` | Root |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| `ROOT` | 根 |'
- en: The `ROOT` label marks the token whose head is itself. Typically, spaCy assigns
    it to the main verb of the sentence (the verb that is at the heart of the predicate).
    Every complete sentence should have a verb with the `ROOT` tag and a subject with
    the `nsubj` tag. The other elements are optional.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`ROOT`标签标记的是其主词是自身的词。通常，spaCy 会将其分配给句子的主动词（谓语的核心动词）。每个完整的句子应该有一个带有`ROOT`标签的动词和一个带有`nsubj`标签的主语。其他元素是可选的。'
- en: '**NOTE**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Most of the examples in this book will assume that the submitted text is a
    complete sentence and use the `ROOT` tag to locate the sentence’s main verb. Keep
    in mind that this won’t work for every possible input.*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*本书中的大多数示例假定提交的文本是完整的句子，并使用`ROOT`标签来定位句子的主动词。请记住，这并不适用于所有可能的输入。*'
- en: 'The following script illustrates how to access the syntactic dependency labels
    of the tokens in the discourse from the example in “[Part-of-Speech Tagging](../Text/ch02.xhtml#lev19)”
    on [page 21](../Text/ch02.xhtml#page_21):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本演示了如何访问示例中在 “[词性标注](../Text/ch02.xhtml#lev19)” 中的语篇中标记的句法依赖标签，参考 [第21页](../Text/ch02.xhtml#page_21)：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The script outputs the coarse-grained part-of-speech tags ➊ (see [Table 2-1](../Text/ch02.xhtml#ch02tab01))
    and dependency labels assigned to the tokens ➋ composing the sample discourse:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本输出了粗粒度的词性标签 ➊（见 [表 2-1](../Text/ch02.xhtml#ch02tab01)）和分配给构成示例语篇的标记 ➋ 的依赖标签：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'But what it doesn’t show you is how words are related to each other in a sentence
    by means of the commonly called *dependency arcs* explained at the beginning of
    this section. To look at the dependency arcs in the sample discourse, replace
    the loop in the preceding script with the following one:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但它没有展示的是单词是如何通过通常所说的 *依赖弧* 在句子中相互关联的，这一点在本节开始时已作解释。要查看示例语篇中的依赖弧，请将前面脚本中的循环替换为以下内容：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The head property of a token object ➊ refers to the syntactic head of this
    token. When you print this line, you’ll see how words in the discourse sentences
    are connected to each other by syntactic dependencies. If they were presented
    graphically, you would see an arc for each line in the following output, except
    for the `ROOT` relation. The reason is that the word to which this label is assigned
    is the only word in a sentence that doesn’t have a head:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标记对象的头部属性 ➊ 指的是该标记的句法主语。当你打印这一行时，你会看到语篇中的单词是如何通过句法依赖关系互相连接的。如果它们以图形化方式呈现，你会看到每一行都对应一个弧线，除了
    `ROOT` 关系。原因是分配给该标签的单词是句子中唯一没有主语的单词：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Looking at the earlier list of syntactic dependencies, let’s try to figure
    out what labels point to the tokens that could potentially best describe the customer’s
    intent: in other words, you need to find a pair that would alone appropriately
    describe the customer’s intent.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 查看之前的句法依赖列表，让我们尝试找出哪些标签指向的标记可以最好地描述客户的意图：换句话说，你需要找到一对能够单独恰当地描述客户意图的标记。
- en: You might be interested in the tokens marked with the `ROOT` and `pobj` dependency
    labels, because in this example they’re key in intent recognition. As stated earlier,
    the `ROOT` label marks the main verb of the sentence, and `pobj`, in this example,
    marks the entity that—in conjunction with the verb—summarizes the meaning of the
    entire utterance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对标记为 `ROOT` 和 `pobj` 依赖标签的标记感兴趣，因为在这个例子中它们是意图识别的关键。如前所述，`ROOT` 标签标记了句子的主要动词，而
    `pobj` 在本例中标记了与动词共同总结整个话语意义的实体。
- en: 'The following script locates words that are assigned to those two dependency
    labels:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本定位分配给这两个依赖标签的单词：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this script, you *shred the discourse* ➊ to separate the sentences with the
    `doc.sents` property, which iterates over the sentences in the document. Shredding
    a text into separate sentences can be useful when you need to find, for example,
    certain parts of speech in each sentence of the discourse. (We’ll discuss `doc.sents`
    in the next chapter, where you’ll see an example of how to refer to the tokens
    in a document with sentence-level indices.) This allows you to create a list of
    potential keywords for each sentence based on specific dependency labels assigned
    to the tokens ➋. The filter conditions used in this example are chosen based on
    the examination of the syntactically related pairs generated by the previous script.
    In particular, you pick up the tokens with `ROOT` and `pobj` dependency labels
    ➌, because these tokens form the pairs you’re interested in.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，你 *分割语篇* ➊，通过 `doc.sents` 属性将句子分开，该属性会遍历文档中的句子。当你需要在语篇的每个句子中找到某些词性时，将文本分割成单独的句子是非常有用的。（我们将在下一章讨论
    `doc.sents`，在那里你会看到如何使用句子级别的索引引用文档中的标记。）这使你能够基于分配给标记的特定依赖标签，创建每个句子的潜在关键词列表 ➋。此示例中使用的过滤条件是根据前一个脚本生成的句法相关对的检查来选择的。特别是，你挑选出带有
    `ROOT` 和 `pobj` 依赖标签的标记 ➌，因为这些标记组成了你感兴趣的对。
- en: 'The script’s output should look as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的输出应该如下所示：
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In both sentence pairs, the output nouns are the ones labeled as `pobj`. You
    could use this in your ticket booking application to choose the noun that best
    belongs with the verb. In this case, that would be “flying,” which goes with “Frisco.”
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个句子对中，输出的名词是被标记为 `pobj` 的词汇。你可以在你的机票预订应用程序中使用它，选择与动词最匹配的名词。在这种情况下，就是“flying”（飞行），它与“Frisco”（旧金山）匹配。
- en: This is a simplified example of information extraction using dependency labels.
    In the following chapters, you’ll be given more sophisticated examples of how
    to iterate over the dependency tree of a sentence or even an entire discourse,
    extracting necessary pieces of information.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简化的使用依赖标签进行信息提取的示例。在接下来的章节中，你将看到如何在句子的依存树甚至整篇语篇中迭代提取所需信息的更复杂示例。
- en: '***Try This***'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: Now that you know how to take advantage of lemmatization, part-of-speech tags,
    and syntactic dependency labels, you can put them all together to do something
    useful. Try combining the examples from the preceding sections into a single script
    that correctly identifies a speaker’s intent to fly to San Francisco.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何利用词形还原、词性标注和句法依赖标签，你可以将它们结合起来做一些有用的事情。尝试将前面各节的示例结合成一个脚本，正确识别出说话者飞往旧金山的意图。
- en: 'Your script should generate the following output:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你的脚本应该生成以下输出：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: To achieve this, start with the latest script from this section and enhance
    the conditional clause in the loop, adding the conditions to account for fine-grained
    part-of-speech tags, as discussed in “[Part-of-Speech Tagging](../Text/ch02.xhtml#lev19)”
    on [page 21](../Text/ch02.xhtml#page_21). Then add the lemmatization functionality
    to your script, as discussed in “[Lemmatization](../Text/ch02.xhtml#lev17)” on
    [page 18](../Text/ch02.xhtml#page_18).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，从本节的最新脚本开始，并增强循环中的条件语句，添加条件以考虑细粒度的词性标签，如在“[词性标注](../Text/ch02.xhtml#lev19)”中第
    [21 页](../Text/ch02.xhtml#page_21)讨论的内容。然后将词形还原功能添加到脚本中，如在“[词形还原](../Text/ch02.xhtml#lev17)”中第
    [18 页](../Text/ch02.xhtml#page_18)讨论的内容。
- en: '***Named Entity Recognition***'
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***命名实体识别***'
- en: 'A *named entity* is a real object that you can refer to by a proper name. It
    can be a person, organization, location, or other entity. Named entities are important
    in NLP because they reveal the place or organization the user is talking about.
    The following script finds named entities in the sample discourse used in the
    previous examples:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*命名实体*是你可以通过专有名词引用的真实对象。它可以是人、组织、地点或其他实体。命名实体在自然语言处理中的重要性在于，它们揭示了用户所谈论的地点或组织。以下脚本查找了前面示例中使用的语料中的命名实体：'
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If the `ent_type` attribute of a token is not set to 0 ➊, then the token is
    a named entity. If so, you print the `ent_type_` attribute of a token ➋, which
    contains the type of named entity in unicode. As a result, the script should output
    the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词汇的 `ent_type` 属性未设置为 0 ➊，那么该词汇就是命名实体。如果是这样，你就打印该词汇的 `ent_type_` 属性 ➋，它包含了该命名实体的类型，且以
    Unicode 编码形式表示。最终，脚本应输出如下结果：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Both `LA` and `Frisco` are marked as `GPE`, the acronym for “geopolitical entity”
    and includes countries, cities, states, and other place names.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`LA` 和 `Frisco` 都被标记为 `GPE`，即“地理政治实体”的缩写，包括国家、城市、州和其他地名。'
- en: '**Summary**'
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you set up a working environment for using spaCy. Then you
    learned simple scripts that illustrate how to use spaCy’s features to perform
    the basic NLP operations for extracting important information. These operations
    included tokenization, lemmatization, and identifying syntactic relations between
    individual tokens in a sentence. The examples provided in this chapter are simplified
    and don’t reflect real-world scenarios. To write a more sophisticated script using
    spaCy, you’ll need to implement an algorithm to derive the necessary tokens from
    a dependency tree, using the linguistic features assigned to tokens. We’ll return
    to extracting and using linguistic features in [Chapter 4](../Text/ch04.xhtml#ch04),
    and we’ll cover dependency trees in detail in [Chapter 6](../Text/ch06.xhtml#ch06).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你为使用 spaCy 设置了工作环境。然后，你学习了简单的脚本，演示了如何使用 spaCy 的功能来执行基本的自然语言处理操作，以提取重要信息。这些操作包括分词、词形还原和识别句子中各个词汇之间的句法关系。本章提供的示例是简化的，并未反映现实世界的场景。要编写一个更复杂的脚本来使用
    spaCy，你需要实现一个算法，从依存树中推导出必要的词汇，使用分配给词汇的语言学特征。我们将在[第 4 章](../Text/ch04.xhtml#ch04)回顾如何提取和使用语言学特征，并将在[第
    6 章](../Text/ch06.xhtml#ch06)详细介绍依存树。
- en: In the next chapter, you’ll look at the key objects of spaCy’s API, including
    containers and processing pipeline components. Also, you’ll learn to use spaCy’s
    C-level data structures and interfaces to create Python modules capable of processing
    large amounts of text.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将了解spaCy API的关键对象，包括容器和处理管道组件。此外，您还将学习使用spaCy的C级数据结构和接口来创建能够处理大量文本的Python模块。
