- en: '**2'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2'
- en: THE TEXT-PROCESSING PIPELINE**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 文本处理管道**
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/comm1.jpg)'
- en: Now that you understand the structure of an NLP application, it’s time to see
    these underlying concepts in action. In this chapter, you’ll install spaCy and
    set up your working environment. Then you’ll learn about the *text-processing
    pipeline*, a series of basic NLP operations you’ll use to determine the meaning
    and intent of a discourse. These operations include tokenization, lemmatization,
    part-of-speech tagging, syntactic dependency parsing, and named entity recognition.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了NLP应用的结构，是时候看到这些基础概念的实际应用了。在这一章中，你将安装spaCy并设置工作环境。接着，你将学习*文本处理管道*，这是你用来确定语篇意义和意图的一系列基本NLP操作。这些操作包括分词、词形还原、词性标注、句法依存分析和命名实体识别。
- en: '**Setting Up Your Working Environment**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**设置工作环境**'
- en: 'Before you start using spaCy, you need to set up a working environment by installing
    the following software components on your machine:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用spaCy之前，你需要通过在机器上安装以下软件组件来设置工作环境：
- en: Python 2.7 or later, or 3.4 or later
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 2.7或更高版本，或者3.4或更高版本
- en: The spaCy library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy库
- en: A statistical model for spaCy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy的统计模型
- en: 'You’ll need Python 2.7 or later, or 3.4 or later to use spaCy v2.0.*x*. Download
    it at *[https://www.python.org/downloads/](https://www.python.org/downloads/)*
    and follow the instructions to set up a Python environment. Next, install spaCy
    in your Python environment using pip by running the following command:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装Python 2.7或更高版本，或者3.4或更高版本，才能使用spaCy v2.0.*x*。可以在*[https://www.python.org/downloads/](https://www.python.org/downloads/)*下载，并按照指示设置Python环境。接下来，使用pip在你的Python环境中安装spaCy，通过运行以下命令：
- en: $ pip install spacy
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: $ pip install spacy
- en: 'If you have more than one Python installation on your system, select the pip
    executable associated with the Python installation you want to use. For instance,
    if you want to use spaCy with Python 3.5, you’d run the following command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统上有多个Python安装，选择与要使用的Python安装相关联的pip可执行文件。例如，如果你想使用Python 3.5与spaCy，运行以下命令：
- en: $ pip3.5 install spacy
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: $ pip3.5 install spacy
- en: 'If you already have spaCy installed on your system, you might want to upgrade
    it to a new release. The examples in this book assume you use spaCy v2.0.*x* or
    later. You can verify which version of spaCy you have installed with the following
    command:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统已经安装了spaCy，你可能希望将其升级到新版本。本书中的示例假设你使用的是spaCy v2.0.*x*或更高版本。你可以通过以下命令验证已安装的spaCy版本：
- en: $ python -m spacy info
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: $ python -m spacy info
- en: Once again, you might need to replace the python command with the command for
    the python executable used in your particular environment, say, python3.5. From
    now on, we’ll use python and pip regardless of the executables your system uses.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，你可能需要将python命令替换为在你的特定环境中使用的python可执行文件命令，比如python3.5。从现在开始，我们将使用python和pip，无论你的系统使用的可执行文件是什么。
- en: 'If you decide to upgrade your installed spaCy package to the latest version,
    you can do this using the following pip command:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定将已安装的spaCy包升级到最新版本，可以使用以下pip命令：
- en: $ pip install -U spacy
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: $ pip install -U spacy
- en: '**Installing Statistical Models for spaCy**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**安装spaCy的统计模型**'
- en: The spaCy installation doesn’t include statistical models that you’ll need when
    you start using the library. The statistical models contain knowledge collected
    about the particular language from a set of sources. You must separately download
    and install each model you want to use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的安装不包括你开始使用库时所需的统计模型。统计模型包含了从一组来源收集的关于特定语言的知识。你必须单独下载并安装每个想要使用的模型。
- en: 'Several pretrained statistical models are available for different languages.
    For English, for example, the following models are available for download from
    spaCy’s website: en_core_web_sm, en_core_web_md, en_core_web_lg, and en_vectors_web_lg.
    The models use the following naming convention: *lang_type_genre_size*. *Lang*
    specifies the language. *Type* indicates the model’s capabilities (for example,
    core is a general-purpose model that includes vocabulary, syntax, entities, and
    vectors). *Genre* indicates the type of text the model has been trained on: web
    (such as Wikipedia or similar media resources) or news (news articles). *Size*
    indicates how large the model is: lg is large, md is medium, and sm is small.
    The larger the model is, the more disk space it requires. For example, the en_vectors_web_lg-2.1.0
    model takes 631MB, whereas en_core_web_sm-2.1.0 takes only 10MB.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种预训练的统计模型可用于不同语言。例如，英语的以下模型可以从 spaCy 的网站上下载：en_core_web_sm、en_core_web_md、en_core_web_lg
    和 en_vectors_web_lg。模型使用以下命名约定：*lang_type_genre_size*。*Lang* 指定语言。*Type* 表示模型的功能（例如，core
    是一个通用模型，包括词汇、语法、实体和向量）。*Genre* 表示模型训练的文本类型：web（例如 Wikipedia 或类似的媒体资源）或 news（新闻文章）。*Size*
    表示模型的大小：lg 为大，md 为中，sm 为小。模型越大，所需的磁盘空间越大。例如，en_vectors_web_lg-2.1.0 模型需要 631MB，而
    en_core_web_sm-2.1.0 只需要 10MB。
- en: 'To follow along with the examples provided in this book, en_core_web_sm (the
    most lightweight model) will work fine. spaCy will choose it by default when you
    use spaCy’s download command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本书中的示例，en_core_web_sm（最轻量级的模型）就足够使用。使用 spaCy 的下载命令时，spaCy 会默认选择该模型：
- en: $ python -m spacy download en
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $ python -m spacy download en
- en: The en shortcut link in the command instructs spaCy to download and install
    the best-matching default model for the English language. The best-matching model,
    in this context, means the one that is generated for the specified language (English
    in this example), a general purpose model, and the most lightweight.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 命令中的 en 快捷方式指示 spaCy 下载并安装适合英语语言的最佳默认模型。在这个上下文中，最佳匹配的模型意味着为指定语言（本例为英语）生成的模型、通用模型以及最轻量级的模型。
- en: 'To download a specific model, you must specify its name, like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载特定的模型，您必须指定其名称，像这样：
- en: $ python -m spacy download en_core_web_md
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: $ python -m spacy download en_core_web_md
- en: 'Once installed, you can load the model using this same shortcut you specified
    during the installation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以使用在安装过程中指定的相同快捷方式加载该模型：
- en: nlp = spacy.load('en')
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '**Basic NLP Operations with spaCy**'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用 spaCy 的基本 NLP 操作**'
- en: Let’s begin by performing a chain of basic NLP operations that we call a processing
    pipeline. spaCy does all these operations for you behind the scenes, allowing
    you to concentrate on your application’s specific logic. [Figure 2-1](../Text/ch02.xhtml#ch02fig01)
    provides a simplified depiction of this process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始执行一系列基本的 NLP 操作，这些操作被称为处理流程。spaCy 会在后台为您执行所有这些操作，让您可以专注于应用程序的具体逻辑。[图 2-1](../Text/ch02.xhtml#ch02fig01)
    提供了这个过程的简化示意图。
- en: '![image](../Images/fig2-1.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-1.jpg)'
- en: '*Figure 2-1: A high-level view of the processing pipeline*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-1：处理流程的高级视图*'
- en: The processing pipeline typically includes tokenization, lemmatization, part-of-speech
    tagging, syntactic dependency parsing, and named entity recognition. We’ll introduce
    each of these tasks in this section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 处理流程通常包括分词、词形还原、词性标注、句法依赖分析和命名实体识别。在本节中，我们将介绍这些任务。
- en: '***Tokenization***'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***分词***'
- en: The very first action any NLP application typically performs on a text is parsing
    that text into *tokens*, which can be words, numbers, or punctuation marks. Tokenization
    is the first operation because all the other operations require you to have tokens
    already in place.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 NLP 应用程序通常在处理文本时执行的第一个操作是将文本解析为*词元*，它可以是单词、数字或标点符号。分词是第一个操作，因为所有其他操作都需要已经处理好的词元。
- en: 'The following code shows the tokenization process:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了分词过程：
- en: ➊ import spacy
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ import spacy
- en: ➋ nlp = spacy.load('en')
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ nlp = spacy.load('en')
- en: ➌ doc = nlp(u'I am flying to Frisco')
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ doc = nlp(u'I am flying to Frisco')
- en: ➍ print([w.text for w in doc])
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ print([w.text for w in doc])
- en: We start by importing the spaCy library ➊ to gain access to its functionality.
    Then, we load a model package using the en shortcut link ➋ to create an instance
    of spaCy’s Language class. A Language object contains the language’s vocabulary
    and other data from the statistical model. We call the Language object nlp.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 spaCy 库 ➊，以便访问其功能。然后，我们使用 en 快捷方式链接加载模型包，以创建 spaCy 的 Language 类的实例。Language
    对象包含语言的词汇表和来自统计模型的其他数据。我们称语言对象为 nlp。
- en: Next, we apply the object just created ➌ to a sample sentence, creating a *Doc
    object* instance. A Doc object is a container for a sequence of Token objects.
    spaCy generates it implicitly based on the text you provide it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将刚刚创建的对象应用到一个样本句子中，创建一个 *Doc 对象* 实例。Doc 对象是 Token 对象序列的容器。spaCy 根据您提供的文本隐式生成它。
- en: At this point, with just three lines of code, spaCy has generated the grammatical
    structure for the sample sentence. How you’ll use it is entirely up to you. In
    this very simple example, you just print out the *text content* of each token
    from the sample sentence ➍.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点为止，仅用三行代码，spaCy 就已经生成了样本句子的语法结构。如何使用它完全取决于您。在这个非常简单的例子中，您只是打印出样本句子的每个令牌的
    *文本内容* ➍。
- en: 'The script outputs the sample sentence’s tokens as a list:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本将样本句子的令牌输出为一个列表：
- en: '[''I'', ''am'', ''flying'', ''to'', ''Frisco'']'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[''I'', ''am'', ''flying'', ''to'', ''Frisco'']'
- en: The *text content*—the group of characters that compose the token, such as the
    letters “a” and “m” in the token “am”—is just one of many properties of a Token
    object. You can also extract various linguistic features assigned to a token,
    as you’ll see in the following examples.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本内容* —— 组成令牌的字符组，例如令牌“am”中的字母“a”和“m”，仅仅是 Token 对象的众多属性之一。您还可以提取分配给令牌的各种语言特征，正如您将在以下示例中看到的那样。'
- en: '***Lemmatization***'
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词形归并***'
- en: 'A *lemma* is the base form of a token. You can think of it as the form in which
    the token would appear if it were listed in a dictionary. For example, the lemma
    for the token “flying” is “fly.” *Lemmatization* is the process of reducing word
    forms to their lemma. The following script provides a simple example of how to
    do lemmatization with spaCy:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*词形* 是令牌的基本形式。您可以将其视为令牌在字典中列出时的形式。例如，令牌“flying”的词形是“fly”。*词形归并* 是将词形减少为它们的词形的过程。以下脚本提供了使用
    spaCy 进行词形归并的简单示例：'
- en: import spacy
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc = nlp(u'this product integrates both libraries for downloading and
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'这个产品集成了用于下载和库
- en: applying patches')
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 应用补丁')
- en: 'for token in doc:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in doc:'
- en: print(➊token.text, ➋token.lemma_)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: print(➊token.text, ➋token.lemma_)
- en: The first three lines in the script are the same as those in the previous script.
    Recall that they import the spaCy library, load an English model using the en
    shortcut and create a text-processing pipeline, and apply the pipeline to a sample
    sentence—creating a Doc object through which you can access the grammatical structure
    of the sentence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的前三行与之前的脚本相同。回想一下，它们导入 spaCy 库，使用 en 快捷方式加载英语模型，并创建一个文本处理管道，并将管道应用于一个样本句子
    —— 通过它您可以访问句子的语法结构。
- en: '**NOTE**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*In grammar, sentence structure is the arrangement of individual words, as
    well as phrases and clauses in a sentence. The grammatical meaning of a sentence
    depends on this structural organization.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*在语法中，句子结构是句子中个别词语、短语和从句的排列。句子的语法含义取决于这种结构组织。*'
- en: 'Once you have a Doc object containing the tokens from your example sentence,
    you iterate over those tokens in a loop, and then print out a token’s text content
    ➊ along with its corresponding lemma ➋. This script produces the following output
    (I’ve tabulated it to make it more readable):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有一个包含样例句子中令牌的 Doc 对象，您可以在循环中迭代这些令牌，然后打印出令牌的文本内容 ➊ 以及其对应的词形 ➋。此脚本生成以下输出（我已制表以使其更易读）：
- en: this        this
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: this        这
- en: product     product
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: product     产品
- en: integrates  integrate
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: integrates  集成
- en: both        both
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: both        两者
- en: libraries   library
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: libraries   库
- en: for         for
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: for         为了
- en: downloading download
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: downloading 下载
- en: and         and
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: and         和
- en: applying    apply
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: applying    应用
- en: patches     patch
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: patches     补丁
- en: The column on the left contains the tokens, and the column on the right contains
    their lemmas.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧列出了令牌，右侧列出了它们的词形。
- en: '***Applying Lemmatization for Meaning Recognition***'
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***应用词形归并进行意义识别***'
- en: 'Lemmatization is an important step in the task of meaning recognition. To see
    how, let’s return to the sample sentence from the previous section:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: I am flying to Frisco.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Suppose this sentence was submitted to an NLP application interacting with an
    online system that provides an API for booking tickets for trips. The application
    processes a customer’s request, extracting necessary information from it and then
    passing on that information to the underlying API. This design might look like
    the one depicted in [Figure 2-2](../Text/ch02.xhtml#ch02fig02).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig2-2.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2-2: Using lemmatization in the process of extracting necessary information
    from a customer’s request*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The NLP application tries to get the following information from a customer’s
    request: a form of travel (plane, rail, bus, and so on) and a destination. The
    application needs to first determine whether the customer wants an air ticket,
    a railway ticket, or a bus ticket. To determine this, the application searches
    for a word that matches one of the keywords in the predefined list. An easy way
    to simplify the search for these keywords is to first convert all the words in
    a sentence being processed to their lemmas. In that case, the predefined list
    of keywords will be much shorter and clearer. For example, you won’t need to include
    all the word forms of the word fly (such as “fly,” “flying,” “flew,” and “flown”)
    to serve as an indicator that the customer wants an air ticket, reducing all possible
    variants to the base form of the word—that is, “fly.”'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization also comes in handy when the application tries to determine a
    destination from a submitted request. There are a lot of nicknames for the globe’s
    cities. But the system that books the tickets requires official names. Of course,
    the default Tokenizer that performs lemmatization won’t know the difference between
    nicknames and official names for cities, countries, and so on. To solve this problem,
    you can add special case rules to an existing Tokenizer instance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The following script illustrates how you might implement lemmatization for the
    destination cities example. It prints out the lemmas of the words composing the
    sentence.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: from spacy.symbols import ORTH, LEMMA
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u'I am flying to Frisco')
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: print([w.text for w in doc])
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '➊ special_case = [{ORTH: u''Frisco'', LEMMA: u''San Francisco''}]'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: ➋ nlp.tokenizer.add_special_case(u'Frisco', special_case)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: ➌ print([w.lemma_ for w in nlp(u'I am flying to Frisco')])
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: You define a *special case* for the word Frisco ➊ by replacing its default lemma
    with San Francisco. Then you add this special case to the Tokenizer instance ➋.
    Once added, the Tokenizer instance will use this special case each time it’s asked
    for the lemma of Frisco. To make sure that everything works as expected, you print
    out the lemmas of the words in the sentence ➌.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'The script generates the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[''I'', ''am'', ''flying'', ''to'', ''Frisco'']'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[''-PRON-'', ''be'', ''fly'', ''to'', ''San Francisco'']'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[''-PRON-'', ''be'', ''fly'', ''to'', ''San Francisco'']'
- en: The output lists the lemmas for all words occurring in the sentence with the
    exception of Frisco, for which it lists San Francisco.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出列出了句子中所有单词的词元，除了“Frisco”，对于它会列出“San Francisco”。
- en: '***Part-of-Speech Tagging***'
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词性标注***'
- en: A *part-of-speech tag* tells you the part-of-speech (noun, verb, and so on)
    of a given word in a given sentence. (Recall from [Chapter 1](../Text/ch01.xhtml#ch01)
    that a word can act as more than one part of speech depending on the context in
    which it appears.)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*词性标签*会告诉你给定句子中某个单词的词性（名词、动词等）。（请回顾[第1章](../Text/ch01.xhtml#ch01)，一个单词根据其出现的上下文可能充当多个词性。）'
- en: 'In spaCy, part-of-speech tags can include detailed information about a token.
    In the case of verbs, they might tell you the following features: tense (past,
    present, or future), aspect (simple, progressive, or perfect), person (1st, 2nd,
    or 3rd), and number (singular or plural).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，词性标签可以包含关于标记的详细信息。对于动词，它们可能会告诉你以下特征：时态（过去式、现在式或将来式）、体（简单、进行或完成）、人称（第一、第二或第三人称）以及数（单数或复数）。
- en: 'Extracting these verb part-of-speech tags can help identify a user’s intent
    when tokenization and lemmatization alone aren’t sufficient. For instance, the
    lemmatization script for the ticket booking application in the preceding section
    won’t decide how the NLP application chooses words in a sentence to compose a
    request to the underlying API. In a real situation, doing so might be quite complicated.
    For example, a customer’s request might consist of more than one sentence:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 提取这些动词的词性标签有助于在仅有分词和词元化时无法充分识别用户意图的情况下。比如，前一节中提到的票务预订应用的词元化脚本无法决定NLP应用如何选择句子中的词来向底层API发出请求。在实际情况中，这可能会相当复杂。例如，客户的请求可能包含多个句子：
- en: I have flown to LA. Now I am flying to Frisco.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经飞到洛杉矶。现在我正飞往Frisco。
- en: 'For these sentences, the results of lemmatization would be as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些句子，词元化的结果如下：
- en: '[''-PRON-'', ''have'', ''fly'', ''to'', ''LA'', ''.'', ''now'', ''-PRON-'',
    ''be'', ''fly'', ''to'', ''San Francisco'', ''.'']'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[''-PRON-'', ''have'', ''fly'', ''to'', ''LA'', ''.'', ''now'', ''-PRON-'',
    ''be'', ''fly'', ''to'', ''San Francisco'', ''.'']'
- en: Performing lemmatization alone isn’t enough here; the application might consider
    the lemmas “fly” and “LA” from the first sentence as the keywords, indicating
    that the customer intends to fly to LA when in fact the customer intends to fly
    to San Francisco. Part of the problem is that lemmatization changes verbs to their
    infinitive forms, making it hard to know the role they play in a sentence.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 仅执行词元化在这里是不够的；应用程序可能会将第一句中的词元“fly”和“LA”作为关键字，表示客户打算飞往洛杉矶，实际上客户打算飞往旧金山。问题的一部分是，词元化将动词转换为不定式形式，难以了解它们在句子中的角色。
- en: This is where part-of-speech tags come into play. In English, the core parts
    of speech include noun, pronoun, determiner, adjective, verb, adverb, preposition,
    conjunction, and interjection. (See the linguistic primer in the appendix for
    more information about these parts of speech.) In spaCy, these same categories—plus
    some additional ones for symbols, punctuation marks, and others—are called *coarse-grained
    parts of speech* and are available as a fixed set of tags through the Token.pos
    (int) and Token.pos_ (unicode) attributes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是词性标签发挥作用的地方。在英语中，核心词性包括名词、代词、限定词、形容词、动词、副词、介词、连词和感叹词。（有关这些词性的更多信息，请参阅附录中的语言学入门。）在spaCy中，这些相同的类别—以及一些用于符号、标点符号和其他的附加类别—被称为*粗粒度词性*，并通过Token.pos（int）和Token.pos_（unicode）属性作为固定标签集提供。
- en: Also, spaCy offers *fine-grained parts of speech* tags that provide more detailed
    information about a token, covering morphological features, such as verb tenses
    and types of pronouns. Naturally, the list of fine-grained parts of speech contains
    many more tags than the coarse-grained list. The fine-grained part-of-speech tags
    are available as the Token.tag (int) and Token.tag_ (unicode) attributes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，spaCy还提供了*精细粒度词性*标签，它提供了关于标记的更多详细信息，涵盖了形态学特征，例如动词时态和代词类型。显然，精细粒度词性标签的数量要比粗粒度词性标签更多。精细粒度词性标签可以通过Token.tag（int）和Token.tag_（unicode）属性获得。
- en: '[Table 2-1](../Text/ch02.xhtml#ch02tab01) lists some of the common part-of-speech
    tags used in spaCy for English models.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-1](../Text/ch02.xhtml#ch02tab01)列出了spaCy中用于英语模型的一些常见词性标签。'
- en: '**Table 2-1:** Some Common spaCy Part-of-Speech Tags'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2-1：** 一些常见的spaCy词性标签'
- en: '| **TAG (fine-grained part of speech)** | **POS (coarse-grained part of speech)**
    | **Morphology** | **Description** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **标记（精细化词性）** | **词性（粗粒度词性）** | **形态学** | **描述** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| NN | NOUN | Number=sing | Noun, singular |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| NN | 名词 | 数量=单数 | 单数名词 |'
- en: '| NNS | NOUN | Number=plur | Noun, plural |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| NNS | 名词 | 数量=复数 | 复数名词 |'
- en: '| PRP | PRON | PronType=prs | Pronoun, personal |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| PRP | 代词 | 代词类型=人称 | 个人代词 |'
- en: '| PRP$ | PRON | PronType=prs Poss=yes | Pronoun, possessive |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| PRP$ | 代词 | 代词类型=人称 所有格=有 | 所有格代词 |'
- en: '| VB | VERB | VerbForm=inf | Verb, base form |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| VB | 动词 | 动词形式=原形 | 动词，基本形式 |'
- en: '| VBD | VERB | VerbForm=fin Tense=past | Verb, past tense |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| VBD | 动词 | 动词形式=完成时 时态=过去时 | 动词，过去时 |'
- en: '| VBG | VERB | VerbForm=part Tense=pres'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '| VBG | 动词 | 动词形式=动名词 时态=现在时'
- en: Aspect=prog | Verb, gerund, or present participle |
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 时态=进行时 | 动词，动名词或现在分词 |
- en: '| JJ | ADJ | Degree=pos | Adjective |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| JJ | 形容词 | 程度=普通 | 形容词 |'
- en: '**NOTE**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*You can find the entire list of the fine-grained part-of-speech tags used
    in spaCy in the “[Part-of-Speech Tagging](../Text/ch02.xhtml#lev19)” section in
    the Annotation Specifications manual at* *[https://spacy.io/api/annotation#pos-tagging](https://spacy.io/api/annotation#pos-tagging)*.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以在[注释规范手册](../Text/ch02.xhtml#lev19)的“[词性标注](../Text/ch02.xhtml#lev19)”章节中找到spaCy使用的精细化词性标记的完整列表，链接如下*
    *[https://spacy.io/api/annotation#pos-tagging](https://spacy.io/api/annotation#pos-tagging)*。'
- en: Tense and aspect are perhaps the most interesting properties of verbs for NLP
    applications. Together, they indicate a verb’s reference to a position in time.
    For example, we use the *present tense progressive aspect* form of a verb to describe
    what is happening right now or what will happen in the near future. To form the
    present tense progressive aspect verb, you add the present tense form of the verb
    “to be” before an -ing verb. For example, in the sentence “I am looking into it,”
    you add “am”—the form of the verb “to be” in the first person, present tense—before
    the -ing verb “looking.” In this example, “am” indicates the present tense and
    “looking” points to the progressive aspect.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 时态和体貌可能是动词对于自然语言处理应用来说最有趣的属性。它们一起表示动词在时间中的参考位置。例如，我们使用*现在时进行时*形式的动词来描述当前正在发生的事情或即将发生的事情。要构成现在时进行时的动词，您需要在动词“to
    be”的现在时形式之前加上一个-ing动词。例如，在句子“I am looking into it”中，我们在动词“looking”之前加上了“am”——动词“to
    be”在第一人称现在时的形式。在这个例子中，“am”表示现在时，“looking”则指示进行时。
- en: '***Using Part-of-Speech Tags to Find Relevant Verbs***'
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用词性标注来找到相关的动词***'
- en: The ticket booking application could use the fine-grained part-of speech tags
    available in spaCy to filter the verbs in the discourse, choosing only those that
    could be key to determining the customer’s intent.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 票务预订应用可以使用spaCy中可用的精细化词性标记来过滤话语中的动词，选择那些可能有助于确定客户意图的动词。
- en: 'Before moving onto the code for this process, let’s try to figure out what
    kind of utterances a customer might use to express their intention to book a plane
    ticket to, say, LA. We could start by looking at some sentences that contain the
    following combination of lemmas: “fly”, “to”, and “LA”. Here are some simple options:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入此过程的代码之前，让我们先尝试弄清楚客户可能用什么样的表达方式来表达他们预定机票的意图，比如说，飞往洛杉矶。我们可以从以下包含词元组合“fly”、“to”和“LA”的句子开始。以下是一些简单的例子：
- en: I flew to LA.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我飞到了洛杉矶。
- en: I have flown to LA.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经飞到了洛杉矶。
- en: I need to fly to LA.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要飞往洛杉矶。
- en: I am flying to LA.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在飞往洛杉矶。
- en: I will fly to LA.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我将飞往洛杉矶。
- en: Notice that although all of these sentences would include the “fly to LA” combination
    if reduced to lemmas, only some of them imply the customer’s intent to book a
    plane ticket to LA. The first two definitely aren’t suitable.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管所有这些句子在简化为词元时都会包含“fly to LA”这个组合，但只有部分句子能够暗示客户有预定飞往洛杉矶的意图。前两句显然不合适。
- en: 'A quick analysis reveals that the past and past perfect forms of the verb “fly”—the
    tenses used in the first two sentences—don’t imply the intent we’re looking for.
    Only the infinitive and present progressive forms are suitable. The following
    script illustrates how to find those forms in the sample discourse:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 快速分析表明，“fly”这个动词的过去时和过去完成时形式——第一和第二个句子中使用的时态——并不暗示我们所寻找的意图。只有不定式和现在进行时形式是合适的。以下脚本展示了如何在样本话语中找到这些形式：
- en: import spacy
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'我已经飞到了洛杉矶。现在我正在飞往旧金山。')
- en: print([w.text for w in doc if ➊w.tag_== ➋'VBG' or w.tag_== ➌'VB'])
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: print([w.text for w in doc if ➊w.tag_== ➋'VBG' or w.tag_== ➌'VB'])
- en: The tag_ property ➊ of a Token object contains the fine-grained part-of-speech
    attribute assigned to that object. You use a loop performed over the tokens composing
    the discourse to check whether the fine-grained part-of-speech tag assigned to
    a token is VB (a verb in the base, or infinitive, form) ➌ or VBG (a verb in the
    present progressive form) ➋.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Token 对象的 tag_ 属性 ➊ 包含分配给该对象的细粒度词性属性。你可以通过对构成语篇的词元执行循环来检查分配给某个词元的细粒度词性标签是否为
    VB（动词的基本形式或不定式形式） ➌ 或 VBG（动词的现在进行时形式） ➋。
- en: 'In the sample discourse, only the verb “flying” in the second sentence meets
    the specified condition. So you should see the following output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例语篇中，只有第二句中的动词“flying”符合指定的条件。因此，你应该看到以下输出：
- en: '[''flying'']'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[''flying'']'
- en: 'Of course, fine-grained part-of-speech tags aren’t only assigned to verbs;
    they’re also assigned to the other parts of speech in a sentence. For example,
    spaCy would recognize LA and Frisco as proper nouns—nouns that are the names of
    individuals, places, objects, or organizations—and tag them with PROPN. If you
    wanted, you could add the following line of code to the previous script:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，细粒度的词性标签不仅会分配给动词；它们还会分配给句子中的其他词性。例如，spaCy 会将 LA 和 Frisco 识别为专有名词——指代个人、地点、物体或组织的名词——并标记为
    PROPN。如果需要，你可以在之前的脚本中添加以下代码行：
- en: print([w.text for w in doc if w.pos_ == 'PROPN'])
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: print([w.text for w in doc if w.pos_ == 'PROPN'])
- en: 'Adding that code should output the following list:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 添加代码应该输出以下列表：
- en: '[''LA'', ''Frisco'']'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[''LA'', ''Frisco'']'
- en: The proper nouns from both sentences of the sample discourse are in the list.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 来自两个句子的专有名词都出现在列表中。
- en: '***Context Is Important***'
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***上下文很重要***'
- en: 'Fine-grained part-of-speech tags might not always be enough to determine an
    utterance’s meaning. For this, you might still need to rely on context. As an
    example, consider the following utterance: “I am flying to LA.” The part-of-speech
    tagger will assign the VBG tag to the verb “flying” in this example, because it’s
    in the present progressive form. But because we use this verb form to describe
    either what is happening right now or what will happen in the near future, the
    utterance might mean either “I’m already in the sky, flying to LA.” or “I’m going
    to fly to LA.” When submitted to the ticket booking NLP application, the application
    should interpret only one of these sentences as “I need an air ticket to LA.”
    Similarly, consider the following discourse: “I am flying to LA. In the evening,
    I have to be back in Frisco.” This most likely implies that the speaker wants
    an air ticket from LA to Frisco for an evening flight. You’ll find more examples
    about recognizing meaning based on context in “[Using Context to Improve the Ticket-Booking
    Chatbot](../Text/ch06.xhtml#lev85)” on [page 91](../Text/ch06.xhtml#page_91).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度词性标签可能并不总是足够用来确定一个话语的意义。为此，你可能仍然需要依赖上下文。例如，考虑以下话语：“I am flying to LA。” 在这个例子中，词性标注器会为动词“flying”分配
    VBG 标签，因为它是现在进行时形式。但由于我们使用这个动词形式来描述当前发生的事情或即将发生的事情，因此该话语的意思可能是“我已经在空中，飞往 LA。”或“我要飞往
    LA。”当提交到机票预订 NLP 应用程序时，应用程序应该只将这句话解释为“我需要一张飞往 LA 的机票。”类似地，考虑以下语篇：“I am flying
    to LA. In the evening, I have to be back in Frisco。”这很可能意味着说话者想要一张从 LA 飞往 Frisco
    的晚间航班机票。你可以在 “[使用上下文改进机票预订聊天机器人](../Text/ch06.xhtml#lev85)” 第 91 页 ([page 91](../Text/ch06.xhtml#page_91))
    中找到更多关于基于上下文识别意义的例子。
- en: '***Syntactic Relations***'
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***句法关系***'
- en: 'Now let’s combine the proper nouns with the verb that the part-of-speech tagger
    selected earlier. Recall that the list of verbs you could potentially use to identify
    the intent of the discourse contains only the verb “flying” in the second sentence.
    How can you get the verb/proper noun pair that best describes the intent behind
    the discourse? A human would obviously compose the verb/proper noun pairs from
    words found in the same sentence. Because the verb “flown” in the first sentence
    doesn’t meet the condition specified (remember that only infinitive and present
    progressive forms meet the condition), you’d be able to compose such a pair for
    the second sentence only: “flying, Frisco.”'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将专有名词与词性标注器之前选择的动词结合起来。回想一下，你用来识别语篇意图的动词列表中，只有第二个句子中的动词“flying”符合条件。如何获取最能描述语篇背后意图的动词/专有名词对呢？人类显然会从同一句话中的词语构成动词/专有名词对。由于第一句中的动词“flown”不符合指定条件（记住，只有不定式和现在进行时形式符合条件），你只能为第二个句子构建这样的对：
    “flying, Frisco。”
- en: To handle these situations programmatically, spaCy features a syntactic dependency
    parser that discovers syntactic relations between individual tokens in a sentence
    and connects syntactically related pairs of words with a single arc.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以编程方式处理这些情况，spaCy提供了一个句法依赖解析器，它可以发现句子中各个词之间的句法关系，并通过单一的弧连接句法相关的词对。
- en: Like lemmas and part-of-speech tags discussed in the previous sections, *syntactic
    dependency labels* are linguistic features that spaCy assigns to the Token objects
    that make up a text contained in a Doc object. For example, the dependency label
    dobj stands for “direct object.” We could illustrate the syntactic relation it
    represents as an arrow arc, as shown in [Figure 2-3](../Text/ch02.xhtml#ch02fig03).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面章节中讨论的词根和词性标签类似，*句法依赖标签*是spaCy赋予构成Doc对象中文本的Token对象的语言特征。例如，依赖标签dobj表示“直接宾语”。我们可以用箭头弧来表示它所代表的句法关系，如[图
    2-3](../Text/ch02.xhtml#ch02fig03)所示。
- en: '**HEAD AND CHILD**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**头词与子词**'
- en: 'A syntactic dependency label describes the type of syntactic relation between
    two words in a sentence. In such a pair, one word is the syntactic governor (also
    called the head or parent) and the other is the dependent (also called the child).
    spaCy assigns a syntactic dependency label to the pair’s dependent. For example,
    in the pair “need, ticket,” extracted from the sentence “I need a plane ticket,”
    the word “ticket” is the child and word “need” is the head, because “need” is
    the verb in what’s called a verb phrase. In this same sentence, “a plane ticket”
    is a noun phrase: the noun “ticket” is the head, and “a” and “plane” are its children.
    To learn more, consult “[Dependency Grammars vs. Phrase Structure Grammars](../Text/app01.xhtml#lev173)”
    on [page 185](../Text/app01.xhtml#page_185).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 句法依赖标签描述了句子中两个词之间的句法关系类型。在这样的词对中，一个词是句法主导词（也称为头或父词），另一个词是依赖词（也称为子词）。spaCy会将句法依赖标签赋给依赖词。例如，在“need,
    ticket”这一对词中，摘自句子“I need a plane ticket”，其中“ticket”是子词，“need”是头词，因为“need”是动词短语中的动词。在同一句子中，“a
    plane ticket”是名词短语：“ticket”是头词，而“a”和“plane”是它的子词。想了解更多信息，请参考[《依赖语法与短语结构语法》](../Text/app01.xhtml#lev173)，见[第185页](../Text/app01.xhtml#page_185)。
- en: Each word in a sentence has exactly one head. Consequently, a word can be a
    child only to one head. The opposite is not always the case. The same word can
    act as a head in none, one, or several pairs. The latter means that the head has
    several children. This explains why a dependency label is always assigned to the
    child.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个句子中的词只有一个头词。因此，一个词只能是一个头词的子词。而反过来并不总是成立。相同的词可以在多个词对中充当头词。这意味着头词有多个子词。这也解释了为什么依赖标签总是赋给子词。
- en: '![image](../Images/fig2-3.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-3.jpg)'
- en: '*Figure 2-3: A graphical representation of a syntactic dependency arc*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-3：句法依赖弧的图形表示*'
- en: The dobj label is assigned to the word “ticket” because it’s the child of the
    relation. A dependency label is always assigned to the child. In your script,
    you can determine the head of a relation using the Token.head attribute.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: dobj标签被赋给“ticket”这个词，因为它是该关系的子词。依赖标签总是赋给子词。在你的脚本中，你可以使用Token.head属性来确定关系的头词。
- en: You might also want to look at the other head/child relations in the sentence,
    like the ones shown in [Figure 2-4](../Text/ch02.xhtml#ch02fig04).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还想查看句子中其他的头/子关系，例如[图 2-4](../Text/ch02.xhtml#ch02fig04)所示的那些。
- en: '![image](../Images/fig2-4.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig2-4.jpg)'
- en: '*Figure 2-4: Head/child relations in an entire sentence*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2-4：整个句子中的头/子关系*'
- en: As you can see, the same word in a sentence can participate in several syntactic
    relations. [Table 2-2](../Text/ch02.xhtml#ch02tab02) lists some of the most commonly
    used English dependency labels.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，句子中的相同词可以参与多个句法关系。[表 2-2](../Text/ch02.xhtml#ch02tab02)列出了一些常用的英语依赖标签。
- en: '**Table 2-2:** Some Common Dependency Labels'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2-2：一些常见的依赖标签**'
- en: '| **Dependency label** | **Description** |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **依赖标签** | **描述** |'
- en: '| --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| acomp | Adjectival complement |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| acomp | 形容词补语 |'
- en: '| amod | Adjectival modifier |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| amod | 形容词修饰语 |'
- en: '| aux | Auxiliary |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| aux | 助动词 |'
- en: '| compound | Compound |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| compound | 复合词 |'
- en: '| dative | Dative |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| dative | 与格 |'
- en: '| det | Determiner |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| det | 限定词 |'
- en: '| dobj | Direct object |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| dobj | 直接宾语 |'
- en: '| nsubj | Nominal subject |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| nsubj | 名词主语 |'
- en: '| pobj | Object of preposition |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| pobj | 介词宾语 |'
- en: '| ROOT | Root |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ROOT | 根词 |'
- en: The ROOT label marks the token whose head is itself. Typically, spaCy assigns
    it to the main verb of the sentence (the verb that is at the heart of the predicate).
    Every complete sentence should have a verb with the ROOT tag and a subject with
    the nsubj tag. The other elements are optional.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '*Most of the examples in this book will assume that the submitted text is a
    complete sentence and use the ROOT tag to locate the sentence’s main verb. Keep
    in mind that this won’t work for every possible input.*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script illustrates how to access the syntactic dependency labels
    of the tokens in the discourse from the example in “[Part-of-Speech Tagging](../Text/ch02.xhtml#lev19)”
    on [page 21](../Text/ch02.xhtml#page_21):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in doc:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: print(token.text, ➊token.pos_, ➋token.dep_)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'The script outputs the coarse-grained part-of-speech tags ➊ (see [Table 2-1](../Text/ch02.xhtml#ch02tab01))
    and dependency labels assigned to the tokens ➋ composing the sample discourse:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: I      PRON   nsubj
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: have   VERB   aux
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: flown  VERB   ROOT
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: to     ADP    prep
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: LA     PROPN  pobj
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: .      PUNCT  punct
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Now    ADV    advmod
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: I      PRON   nsubj
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: am     VERB   aux
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: flying VERB   ROOT
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: to     ADP    prep
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Frisco PROPN  pobj
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: .      PUNCT  punct
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'But what it doesn’t show you is how words are related to each other in a sentence
    by means of the commonly called *dependency arcs* explained at the beginning of
    this section. To look at the dependency arcs in the sample discourse, replace
    the loop in the preceding script with the following one:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'for token in doc:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: print(➊token.head.text, token.dep_, token.text)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The head property of a token object ➊ refers to the syntactic head of this
    token. When you print this line, you’ll see how words in the discourse sentences
    are connected to each other by syntactic dependencies. If they were presented
    graphically, you would see an arc for each line in the following output, except
    for the ROOT relation. The reason is that the word to which this label is assigned
    is the only word in a sentence that doesn’t have a head:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: flown  nsubj  I
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: flown  aux    have
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: flown  ROOT   flown
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: flown  prep   to
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: to     pobj   LA
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: flown  punct  .
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: flying advmod Now
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: flying nsubj  I
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: flying aux    am
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: flying ROOT   flying
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: flying prep   to
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: to     pobj   Frisco
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: flying punct  .
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the earlier list of syntactic dependencies, let’s try to figure
    out what labels point to the tokens that could potentially best describe the customer’s
    intent: in other words, you need to find a pair that would alone appropriately
    describe the customer’s intent.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: You might be interested in the tokens marked with the ROOT and pobj dependency
    labels, because in this example they’re key in intent recognition. As stated earlier,
    the ROOT label marks the main verb of the sentence, and pobj, in this example,
    marks the entity that—in conjunction with the verb—summarizes the meaning of the
    entire utterance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The following script locates words that are assigned to those two dependency
    labels:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '➊ for sent in doc.sents:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: ➋ print([w.text for w in sent ➌if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In this script, you *shred the discourse* ➊ to separate the sentences with the
    doc.sents property, which iterates over the sentences in the document. Shredding
    a text into separate sentences can be useful when you need to find, for example,
    certain parts of speech in each sentence of the discourse. (We’ll discuss doc.sents
    in the next chapter, where you’ll see an example of how to refer to the tokens
    in a document with sentence-level indices.) This allows you to create a list of
    potential keywords for each sentence based on specific dependency labels assigned
    to the tokens ➋. The filter conditions used in this example are chosen based on
    the examination of the syntactically related pairs generated by the previous script.
    In particular, you pick up the tokens with ROOT and pobj dependency labels ➌,
    because these tokens form the pairs you’re interested in.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'The script’s output should look as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[''flown'', ''LA'']'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[''flying'', ''Frisco'']'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: In both sentence pairs, the output nouns are the ones labeled as pobj. You could
    use this in your ticket booking application to choose the noun that best belongs
    with the verb. In this case, that would be “flying,” which goes with “Frisco.”
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: This is a simplified example of information extraction using dependency labels.
    In the following chapters, you’ll be given more sophisticated examples of how
    to iterate over the dependency tree of a sentence or even an entire discourse,
    extracting necessary pieces of information.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '***Try This***'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that you know how to take advantage of lemmatization, part-of-speech tags,
    and syntactic dependency labels, you can put them all together to do something
    useful. Try combining the examples from the preceding sections into a single script
    that correctly identifies a speaker’s intent to fly to San Francisco.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Your script should generate the following output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[''fly'', ''San Francisco'']'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, start with the latest script from this section and enhance
    the conditional clause in the loop, adding the conditions to account for fine-grained
    part-of-speech tags, as discussed in “[Part-of-Speech Tagging](../Text/ch02.xhtml#lev19)”
    on [page 21](../Text/ch02.xhtml#page_21). Then add the lemmatization functionality
    to your script, as discussed in “[Lemmatization](../Text/ch02.xhtml#lev17)” on
    [page 18](../Text/ch02.xhtml#page_18).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '***Named Entity Recognition***'
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *named entity* is a real object that you can refer to by a proper name. It
    can be a person, organization, location, or other entity. Named entities are important
    in NLP because they reveal the place or organization the user is talking about.
    The following script finds named entities in the sample discourse used in the
    previous examples:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: import spacy
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: nlp = spacy.load('en')
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'我已经飞到洛杉矶。现在我正飞往旧金山。')
- en: 'for token in doc:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in doc:'
- en: '➊ if token.ent_type != 0:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ if token.ent_type != 0:'
- en: print(token.text, ➋token.ent_type_)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: print(token.text, ➋token.ent_type_)
- en: 'If the ent_type attribute of a token is not set to 0 ➊, then the token is a
    named entity. If so, you print the ent_type_ attribute of a token ➋, which contains
    the type of named entity in unicode. As a result, the script should output the
    following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词语的ent_type属性没有被设置为0 ➊，那么该词语就是一个命名实体。如果是这样，你将打印该词语的ent_type_属性 ➋，它包含该命名实体的类型（以unicode表示）。因此，脚本应输出以下内容：
- en: LA     GPE
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶     GPE
- en: Frisco GPE
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山 GPE
- en: Both LA and Frisco are marked as GPE, the acronym for “geopolitical entity”
    and includes countries, cities, states, and other place names.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶和旧金山都被标记为GPE，这是“地理政治实体”的缩写，包括国家、城市、州和其他地名。
- en: '**Summary**'
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you set up a working environment for using spaCy. Then you
    learned simple scripts that illustrate how to use spaCy’s features to perform
    the basic NLP operations for extracting important information. These operations
    included tokenization, lemmatization, and identifying syntactic relations between
    individual tokens in a sentence. The examples provided in this chapter are simplified
    and don’t reflect real-world scenarios. To write a more sophisticated script using
    spaCy, you’ll need to implement an algorithm to derive the necessary tokens from
    a dependency tree, using the linguistic features assigned to tokens. We’ll return
    to extracting and using linguistic features in [Chapter 4](../Text/ch04.xhtml#ch04),
    and we’ll cover dependency trees in detail in [Chapter 6](../Text/ch06.xhtml#ch06).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你为使用spaCy设置了工作环境。然后，你学习了简单的脚本，展示了如何使用spaCy的功能来执行提取重要信息的基本NLP操作。这些操作包括分词、词形还原以及识别句子中各个词语之间的句法关系。本章提供的示例被简化了，并不反映现实世界的场景。为了编写更复杂的脚本使用spaCy，你需要实现一个算法，从依赖树中派生出所需的词语，利用分配给词语的语言特征。我们将在[第4章](../Text/ch04.xhtml#ch04)回到提取和使用语言特征的话题，并将在[第6章](../Text/ch06.xhtml#ch06)详细讨论依赖树。
- en: In the next chapter, you’ll look at the key objects of spaCy’s API, including
    containers and processing pipeline components. Also, you’ll learn to use spaCy’s
    C-level data structures and interfaces to create Python modules capable of processing
    large amounts of text.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解spaCy API的关键对象，包括容器和处理管道组件。此外，你还将学习如何使用spaCy的C级数据结构和接口创建能够处理大量文本的Python模块。
