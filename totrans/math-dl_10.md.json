["```py\n❶ epochs = 1000\n\n   eta = 0.1\n\n❷ xtrn, ytrn, xtst, ytst = BuildDataset()\n\n❸ net = {}\n\n   net[\"b2\"] = 0.0\n\n   net[\"b1\"] = 0.0\n\n   net[\"b0\"] = 0.0\n\n   net[\"w5\"] = 0.0001*(np.random.random() - 0.5)\n\n   net[\"w4\"] = 0.0001*(np.random.random() - 0.5)\n\n   net[\"w3\"] = 0.0001*(np.random.random() - 0.5)\n\n   net[\"w2\"] = 0.0001*(np.random.random() - 0.5)\n\n   net[\"w1\"] = 0.0001*(np.random.random() - 0.5)\n\n   net[\"w0\"] = 0.0001*(np.random.random() - 0.5)\n\n❹ tn0,fp0,fn0,tp0,pred0 = Evaluate(net, xtst, ytst)\n\n❺ net = GradientDescent(net, xtrn, ytrn, epochs, eta)\n\n❻ tn,fp,fn,tp,pred = Evaluate(net, xtst, ytst)\n\n   print(\"Training for %d epochs, learning rate %0.5f\" % (epochs, eta))\n\n print()\n\n   print(\"Before training:\")\n\n   print(\"   TN:%3d FP:%3d\" % (tn0, fp0))\n\n   print(\"   FN:%3d TP:%3d\" % (fn0, tp0))\n\n   print()\n\n   print(\"After training:\")\n\n   print(\"   TN:%3d FP:%3d\" % (tn, fp))\n\n   print(\"   FN:%3d TP:%3d\" % (fn, tp))\n```", "```py\n\ndef Evaluate(net, x, y):\n\n    out = Forward(net, x)\n\n    tn = fp = fn = tp = 0\n\n    pred = []\n\n    for i in range(len(y)):\n\n     ❶ c = 0 if (out[i] < 0.5) else 1\n\n        pred.append(c)\n\n        if (c == 0) and (y[i] == 0):\n\n            tn += 1\n\n        elif (c == 0) and (y[i] == 1):\n\n            fn += 1\n\n        elif (c == 1) and (y[i] == 0):\n\n            fp += 1\n\n        else:\n\n            tp += 1\n\n    return tn,fp,fn,tp,pred\n\n def Forward(net, x):\n\n    out = np.zeros(x.shape[0])\n\n    for k in range(x.shape[0]):\n\n     ❷ z0 = net[\"w0\"]*x[k,0] + net[\"w2\"]*x[k,1] + net[\"b0\"]\n\n        a0 = sigmoid(z0)\n\n        z1 = net[\"w1\"]*x[k,0] + net[\"w3\"]*x[k,1] + net[\"b1\"]\n\n        a1 = sigmoid(z1)\n\n        out[k] = net[\"w4\"]*a0 + net[\"w5\"]*a1 + net[\"b2\"]\n\n    return out\n```", "```py\n\ndef GradientDescent(net, x, y, epochs, eta):\n\n ❶ for e in range(epochs):\n\n        dw0 = dw1 = dw2 = dw3 = dw4 = dw5 = db0 = db1 = db2 = 0.0\n\n     ❷ for k in range(len(y)):\n\n         ❸ z0 = net[\"w0\"]*x[k,0] + net[\"w2\"]*x[k,1] + net[\"b0\"]\n\n            a0 = sigmoid(z0)\n\n            z1 = net[\"w1\"]*x[k,0] + net[\"w3\"]*x[k,1] + net[\"b1\"]\n\n            a1 = sigmoid(z1)\n\n            a2 = net[\"w4\"]*a0 + net[\"w5\"]*a1 + net[\"b2\"]\n\n         ❹ db2 += a2 - y[k]\n\n            dw4 += (a2 - y[k]) * a0\n\n dw5 += (a2 - y[k]) * a1\n\n            db1 += (a2 - y[k]) * net[\"w5\"] * a1 * (1 - a1)\n\n            dw1 += (a2 - y[k]) * net[\"w5\"] * a1 * (1 - a1) * x[k,0]\n\n            dw3 += (a2 - y[k]) * net[\"w5\"] * a1 * (1 - a1) * x[k,1]\n\n            db0 += (a2 - y[k]) * net[\"w4\"] * a0 * (1 - a0)\n\n            dw0 += (a2 - y[k]) * net[\"w4\"] * a0 * (1 - a0) * x[k,0]\n\n            dw2 += (a2 - y[k]) * net[\"w4\"] * a0 * (1 - a0) * x[k,1]\n\n        m = len(y)\n\n     ❺ net[\"b2\"] = net[\"b2\"] - eta * db2 / m\n\n        net[\"w4\"] = net[\"w4\"] - eta * dw4 / m\n\n        net[\"w5\"] = net[\"w5\"] - eta * dw5 / m\n\n        net[\"b1\"] = net[\"b1\"] - eta * db1 / m\n\n        net[\"w1\"] = net[\"w1\"] - eta * dw1 / m\n\n        net[\"w3\"] = net[\"w3\"] - eta * dw3 / m\n\n        net[\"b0\"] = net[\"b0\"] - eta * db0 / m\n\n        net[\"w0\"] = net[\"w0\"] - eta * dw0 / m\n\n        net[\"w2\"] = net[\"w2\"] - eta * dw2 / m\n\n     return net\n```", "```py\nnet[\"b2\"] = net[\"b2\"] - eta * db2 / m\n```", "```py\npython3 nn_by_hand.py\n```", "```py\n\nclass ActivationLayer:\n\n    def forward(self, input_data):\n\n        self.input = input_data\n\n        return sigmoid(input_data)\n\n    def backward(self, output_error):\n\n        return sigmoid_prime(self.input) * output_error\n\n    def step(self, eta):\n\n        return\n```", "```py\n\ndef sigmoid(x):\n\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef sigmoid_prime(x):\n\n    return sigmoid(x)*(1.0 - sigmoid(x))\n```", "```py\n\nclass FullyConnectedLayer:\n\n    def __init__(self, input_size, output_size):\n\n     ❶ self.delta_w = np.zeros((input_size, output_size))\n\n        self.delta_b = np.zeros((1,output_size))\n\n        self.passes = 0\n\n     ❷ self.weights = np.random.rand(input_size, output_size) - 0.5\n\n        self.bias = np.random.rand(1, output_size) - 0.5\n\n    def forward(self, input_data):\n\n        self.input = input_data\n\n     ❸ return np.dot(self.input, self.weights) + self.bias\n\n    def backward(self, output_error):\n\n        input_error = np.dot(output_error, self.weights.T)\n\n        weights_error = np.dot(self.input.T, output_error)\n\n        self.delta_w += np.dot(self.input.T, output_error)\n\n        self.delta_b += output_error\n\n        self.passes += 1\n\n        return input_error\n\n    def step(self, eta):\n\n     ❹ self.weights -= eta * self.delta_w / self.passes\n\n        self.bias -= eta * self.delta_b / self.passes\n\n     ❺ self.delta_w = np.zeros(self.weights.shape)\n\n        self.delta_b = np.zeros(self.bias.shape)\n\n        self.passes = 0\n```", "```py\n\nclass Network:\n\n    def __init__(self, verbose=True):\n\n        self.verbose = verbose\n\n      ❶ self.layers = []\n\n    def add(self, layer):\n\n      ❷ self.layers.append(layer)\n\n    def predict(self, input_data):\n\n        result = []\n\n        for i in range(input_data.shape[0]):\n\n            output = input_data[i]\n\n            for layer in self.layers:\n\n                output = layer.forward(output)\n\n            result.append(output)\n\n      ❸ return result\n\n     def fit(self, x_train, y_train, minibatches, learning_rate, batch_size=64):\n\n      ❹ for i in range(minibatches):\n\n err = 0\n\n        idx = np.argsort(np.random.random(x_train.shape[0]))[:batch_size]\n\n        x_batch = x_train[idx]\n\n        y_batch = y_train[idx]\n\n      ❺ for j in range(batch_size):\n\n            output = x_batch[j]\n\n            for layer in self.layers:\n\n                output = layer.forward(output)\n\n          ❻ err += mse(y_batch[j], output)\n\n          ❼ error = mse_prime(y_batch[j], output)\n\n             for layer in reversed(self.layers):\n\n                 error = layer.backward(error)\n\n      ❽ for layer in self.layers:\n\n             layer.step(learning_rate)\n\n         if (self.verbose) and ((i%10) == 0):\n\n              err /= batch_size\n\n              print('minibatch %5d/%d error=%0.9f' % (i, minibatches, err))\n```", "```py\n   import numpy as np\n\n   from NN import *\n\n ❶ x_train = np.load(\"dataset/train_images_small.npy\")\n\n   x_test = np.load(\"dataset/test_images_small.npy\")\n\n   y_train = np.load(\"dataset/train_labels_vector.npy\")\n\n   y_test = np.load(\"dataset/test_labels.npy\")\n\n❷ x_train = x_train.reshape(x_train.shape[0], 1, 14*14)\n\n   x_train /= 255\n\n   x_test = x_test.reshape(x_test.shape[0], 1, 14*14)\n\n   x_test /= 255\n\n❸ net = Network()\n\n   net.add(FullyConnectedLayer(14*14, 100))\n\n   net.add(ActivationLayer())\n\n   net.add(FullyConnectedLayer(100, 50))\n\n   net.add(ActivationLayer())\n\n   net.add(FullyConnectedLayer(50, 10))\n\n   net.add(ActivationLayer())\n\n❹ net.fit(x_train, y_train, minibatches=40000, learning_rate=1.0)\n\n❺ out = net.predict(x_test)\n\n   cm = np.zeros((10,10), dtype=\"uint32\")\n\n   for i in range(len(y_test)):\n\n       cm[y_test[i],np.argmax(out[i])] += 1\n\n   print()\n\n   print(np.array2string(cm))\n\n   print()\n\n   print(\"accuracy = %0.7f\" % (np.diag(cm).sum() / cm.sum(),))\n```"]