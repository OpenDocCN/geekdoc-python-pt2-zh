- en: '**6**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**UNIT TESTING**'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Many find unit testing to be arduous and time-consuming, and some people and
    projects have no testing policy. This chapter assumes that you see the wisdom
    of unit testing! Writing code that is not tested is fundamentally useless, as
    there’s no way to conclusively prove that it works. If you need convincing, I
    suggest you start by reading about the benefits of test-driven development.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter you’ll learn about the Python tools you can use to construct
    a comprehensive suite of tests that will make testing simpler and more automated.
    We’ll talk about how you can use tools to make your software rock solid and regression-free.
    We’ll cover creating reusable test objects, running tests in parallel, revealing
    untested code, and using virtual environments to make sure your tests are clean,
    as well as some other good-practice methods and ideas.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Basics of Testing**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Writing and running unit tests is uncomplicated in Python. The process is not
    intrusive or disruptive, and unit testing will greatly help you and other developers
    in maintaining your software. Here I’ll discuss some of the absolute basics of
    testing that will make things easier for you.
  prefs: []
  type: TYPE_NORMAL
- en: '***Some Simple Tests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, you should store tests inside a `tests` submodule of the application
    or library they apply to. Doing so will allow you to ship the tests as part of
    your module so that they can be run or reused by anyone—even after your software
    is installed—without necessarily using the source package. Making the tests a
    submodule of your main module also prevents them from being installed by mistake
    in a top-level `tests` module.
  prefs: []
  type: TYPE_NORMAL
- en: Using a hierarchy in your test tree that mimics the hierarchy of your module
    tree will make the tests more manageable. This means that the tests covering the
    code of *mylib/foobar.py* should be stored inside *mylib/tests/test_foobar.py*.
    Consistent nomenclature makes things simpler when you’re looking for the tests
    related to a particular file. [Listing 6-1](ch06.xhtml#ch6list1) shows the simplest
    unit test you can write.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-1: A really simple test in test_true.py*'
  prefs: []
  type: TYPE_NORMAL
- en: This will simply assert that the behavior of the program is what you expect.
    To run this test, you need to load the *test_true.py* file and run the `test_true()`
    function defined within.
  prefs: []
  type: TYPE_NORMAL
- en: However, writing and running an individual test for each of your test files
    and functions would be a pain. For small projects with simple usage, the `pytest`
    package comes to the rescue—once installed via `pip`, pytest provides the `pytest`
    command, which loads every file whose name starts with *test_* and then executes
    all functions within that start with `test_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With just the *test_true.py* file in our source tree, running `pytest` gives
    us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `-v` option tells `pytest` to be verbose and print the name of each test
    run on a separate line. If a test fails, the output changes to indicate the failure,
    accompanied by the whole traceback.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s add a failing test this time, as shown in [Listing 6-2](ch06.xhtml#ch6list2).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-2: A failing test in test_true.py*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the test file again, here’s what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A test fails as soon as an `AssertionError` exception is raised; our `assert`
    test will raise an `AssertionError` when its argument is evaluated to something
    false (`False`, `None`, 0, etc.). If any other exception is raised, the test also
    errors out.
  prefs: []
  type: TYPE_NORMAL
- en: Simple, isn’t it? While simplistic, a lot of small projects use this approach
    and it works very well. Those projects require no tools or libraries other than
    pytest and thus can rely on simple `assert` tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you start to write more sophisticated tests, pytest will help you understand
    what’s wrong in your failing tests. Imagine the following test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When `pytest` is run, it gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that `a` and `b` are different and that this test does not pass.
    It also tells us exactly how they are different, making it easy to fix the test
    or code.
  prefs: []
  type: TYPE_NORMAL
- en: '***Skipping Tests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a test cannot be run, you will probably want to skip that test—for example,
    you may wish to run a test conditionally based on the presence or absence of a
    particular library. To that end, you can use the `pytest.skip()` function, which
    will mark the test as skipped and move on to the next one. The `pytest.mark.skip`
    decorator skips the decorated test function unconditionally, so you’ll use it
    when a test always needs to be skipped. [Listing 6-3](ch06.xhtml#ch6list3) shows
    how to skip a test using these methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-3: Skipping tests*'
  prefs: []
  type: TYPE_NORMAL
- en: 'When executed, this test file will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output of the test run in [Listing 6-3](ch06.xhtml#ch6list3) indicates that,
    in this case, all the tests have been skipped. This information allows you to
    ensure you didn’t accidentally skip a test you expected to run.
  prefs: []
  type: TYPE_NORMAL
- en: '***Running Particular Tests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When using `pytest`, you often want to run only a particular subset of your
    tests. You can select which tests you want to run by passing their directory or
    files as an argument to the `pytest` command line. For example, calling `pytest
    test_one.py` will only run the *test_one.py* test. Pytest also accepts a directory
    as argument, and in that case, it will recursively scan the directory and run
    any file that matches the *test_*.py* pattern.
  prefs: []
  type: TYPE_NORMAL
- en: You can also add a filter with the `-k` argument on the command line in order
    to execute only the test matching a name, as shown in [Listing 6-4](ch06.xhtml#ch6list4).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-4: Filtering tests run by name*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Names are not always the best way to filter which tests will run. Commonly,
    a developer would group tests by functionalities or types instead. Pytest provides
    a dynamic marking system that allows you to mark tests with a keyword that can
    be used as a filter. To mark tests in this way, use the `-m` option. If we set
    up a couple of tests like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'we can use the `-m` argument with `pytest` to run only one of those tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-m` marker accepts more complex queries, so we can also run all tests
    that are *not* marked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here pytest executed every test that was not marked as `dicttest`—in this case,
    the `test_something_else` test, which failed. The remaining marked test, `test_something`,
    was not executed and so is listed as `deselected`.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest accepts complex expressions composed of the `or`, `and`, and `not` keywords,
    allowing you to do more advanced filtering.
  prefs: []
  type: TYPE_NORMAL
- en: '***Running Tests in Parallel***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Test suites can take a long time to run. It’s not uncommon for a full suite
    of unit tests to take tens of minutes to run in large software projects. By default,
    pytest runs all tests serially, in an undefined order. Since most computers have
    several CPUs, you can usually speed things up if you split the list of tests and
    run them on multiple CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: To handle this approach, pytest provides the plugin `pytest-xdist`, which you
    can install with `pip`. This plugin extends the pytest command line with the `--numprocesses`
    argument (shortened as `-n`), which accepts as its argument the number of CPUs
    to use. Running `pytest -n 4` would run your test suite using four parallel processes,
    balancing the load across the available CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Because the number of CPUs can change from one computer to another, the plugin
    also accepts the `auto` keyword as a value. In this case, it will probe the machine
    to retrieve the number of CPUs available and start this number of processes.
  prefs: []
  type: TYPE_NORMAL
- en: '***Creating Objects Used in Tests with Fixtures***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In unit testing, you’ll often need to execute a set of common instructions before
    and after running a test, and those instructions will use certain components.
    For example, you might need an object that represents the configuration state
    of your application, and you’ll likely want that object to be initialized before
    each test, then reset to its default values when the test is achieved. Similarly,
    if your test relies on the temporary creation of a file, the file must be created
    before the test starts and deleted once the test is done. These components, known
    as *fixtures*, are set up before a test and cleaned up after the test has finished.
  prefs: []
  type: TYPE_NORMAL
- en: With pytest, fixtures are defined as simple functions. The fixture function
    should return the desired object(s) so that a test using that fixture can use
    that object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The database fixture is automatically used by any test that has `database` in
    its argument list. The `test_insert()` function will receive the result of the
    `database()` function as its first argument and use that result as it wants. When
    we use a fixture this way, we don’t need to repeat the database initialization
    code several times.
  prefs: []
  type: TYPE_NORMAL
- en: Another common feature of code testing is tearing down after a test has used
    a fixture. For example, you may need to close a database connection. Implementing
    the fixture as a generator allows us to add teardown functionality, as shown in
    [Listing 6-5](ch06.xhtml#ch6list5).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-5: Teardown functionality*'
  prefs: []
  type: TYPE_NORMAL
- en: Because we used the `yield` keyword and made `database` a generator, the code
    after the `yield` statement runs when the test is done. That code will close the
    database connection at the end of the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, closing a database connection for each test might impose an unnecessary
    runtime cost, as tests may be able to reuse that same connection. In that case,
    you can pass the `scope` argument to the fixture decorator, specifying the scope
    of the fixture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By specifying the `scope="module"` parameter, you initialize the fixture once
    for the whole module, and the same database connection will be passed to all test
    functions requesting a database connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can run some common code before and after your tests by marking
    fixtures as *automatically used* with the `autouse` keyword, rather than specifying
    them as an argument for each of the test functions. Specifying the `autouse=True`
    keyword argument to the `pytest.fixture()` function will make sure the fixture
    is called before running any test in the module or class it is defined in, as
    in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Such automatically enabled features are handy, but make sure not to abuse fixtures:
    they are run before each and every test covered by their scope, so they can slow
    down a test run significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Running Test Scenarios***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When unit testing, you may want to run the same error-handling test with several
    different objects that trigger that error, or you may want to run an entire test
    suite against different drivers.
  prefs: []
  type: TYPE_NORMAL
- en: We relied heavily on this latter approach when developing *Gnocchi*, a time
    series database. Gnocchi provides an abstract class that we call the *storage
    API*. Any Python class can implement this abstract base and register itself to
    become a driver. The software loads the configured storage driver when required
    and uses the implemented storage API to store or retrieve data. In this case,
    we need a class of unit tests that runs against each driver—thus running against
    each implementation of this storage API—to be sure all drivers conform to what
    the callers expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to achieve this is by using *parameterized fixtures*, which will
    run all the tests that use them several times, once for each of the defined parameters.
    [Listing 6-6](ch06.xhtml#ch6list6) shows an example of using parameterized fixtures
    to run a single test twice with different parameters: once for `mysql` and once
    for `postgresql`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-6: Running a test using parameterized fixtures*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Listing 6-6](ch06.xhtml#ch6list6), the `driver` fixture is parameterized
    with two different values, each the name of a database driver that is supported
    by the application. When `test_insert` is run, it is actually run twice: once
    with a MySQL database connection and once with a PostgreSQL database connection.
    This allows us to easily reuse the same test with different scenarios, without
    adding many lines of code.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Controlled Tests Using Mocking***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mock objects are simulated objects that mimic the behavior of real application
    objects, but in particular and controlled ways. These are especially useful in
    creating environments that describe precisely the conditions for which you would
    like to test code. You can replace all objects but one with mock objects to isolate
    the behavior of your focus object and create an enviroment for testing your code.
  prefs: []
  type: TYPE_NORMAL
- en: One use case is in writing an HTTP client, since it is likely impossible (or
    at least extremely complicated) to spawn the HTTP server and test it through all
    scenarios to return every possible value. HTTP clients are especially difficult
    to test for all failure scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard library for creating mock objects in Python is `mock`. Starting
    with Python 3.3, `mock` has been merged into the Python Standard Library as `unittest.mock`.
    You can, therefore, use a snippet like the following to maintain backward compatibility
    between Python 3.3 and earlier versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `mock` library is pretty simple to use. Any attribute accessed on a `mock.Mock`
    object is dynamically created at runtime. Any value can be set to such an attribute.
    [Listing 6-7](ch06.xhtml#ch6list7) shows `mock` being used to create a fake object
    with a fake attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-7: Accessing the mock.Mock attribute*'
  prefs: []
  type: TYPE_NORMAL
- en: You can also dynamically create a method on a malleable object, as in [Listing
    6-8](ch06.xhtml#ch6list8) where we create a fake method that always returns 42
    and accepts anything as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-8: Creating methods on a mock.Mock object*'
  prefs: []
  type: TYPE_NORMAL
- en: In just a few lines, your `mock.Mock` object now has a `some_method()` method
    that returns 42\. It accepts any kind of argument, and there is no check on what
    the values are—yet.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamically created methods can also have (intentional) side effects. Rather
    than being boilerplate methods that just return a value, they can be defined to
    execute useful code.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 6-9](ch06.xhtml#ch6list9) creates a fake method that has the side
    effect of printing the "`hello world`" string.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-9: Creating methods on a mock.Mock object with side effects*'
  prefs: []
  type: TYPE_NORMAL
- en: We assign an entire function to the `some_method` attribute ➊. This technique
    allows us to implement more complex scenarios in a test because we can plug any
    code needed for testing into a mock object. We then just need to pass this mock
    object to whichever function expects it.
  prefs: []
  type: TYPE_NORMAL
- en: The `call_count` attribute ➋ is a simple way of checking the number of times
    a method has been called.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mock` library uses the action/assertion pattern: this means that once
    your test has run, it’s up to you to check that the actions you are mocking were
    correctly executed. [Listing 6-10](ch06.xhtml#ch6list10) applies the `assert()`
    method to our mock objects to perform these checks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-10: Checking method calls*'
  prefs: []
  type: TYPE_NORMAL
- en: We create a method with the arguments `foo` and `bar` to stand in as our tests
    by calling the method ➊. The usual way to check calls to a mock object is to use
    the `assert_called()` methods, such as `assert_called_once_with()` ➋. To these
    methods, you need to pass the values that you expect callers to use when calling
    your mock method. If the values passed are not the ones being used, then `mock`
    raises an `AssertionError`. If you don’t know what arguments may be passed, you
    can use `mock.ANY` as a value ➌; that will match any argument passed to your mock
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Th `mock` library can also be used to patch some function, method, or object
    from an external module. In [Listing 6-11](ch06.xhtml#ch6list11), we replace the
    `os.unlink()` function with a fake function we provide.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-11: Using mock.patch*'
  prefs: []
  type: TYPE_NORMAL
- en: When used as a context manager, `mock.patch()` replaces the target function
    with the function we provide so the code executed inside the context uses that
    patched method. With the `mock.patch()` method, it’s possible to change any part
    of an external piece of code, making it behave in a way that lets you test all
    conditions in your application, as shown in [Listing 6-12](ch06.xhtml#ch6list12).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-12: Using mock.patch() to test a set of behaviors*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 6-12](ch06.xhtml#ch6list12) implements a test suite that searches
    for all instances of the string “Python is a programming language” on the *[http://python.org/](http://python.org/)*
    web page ➊. There is no way to test negative scenarios (where this sentence is
    not on the web page) without modifying the page itself—something we’re not able
    to do, obviously. In this case, we’re using `mock` to cheat and change the behavior
    of the request so it returns a mocked reply with a fake page that doesn’t contain
    that string. This allows us to test the negative scenario in which *[http://python.org/](http://python.org/)*
    does not contain this sentence, making sure the program handles that case correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: This example uses the decorator version of `mock.patch()` ➋. Using the decorator
    does not change the mocking behavior, but it is simpler when you need to use mocking
    within the context of an entire test function.
  prefs: []
  type: TYPE_NORMAL
- en: Using mocking, we can simulate any problem, such as a web server returning a
    404 error, an I/O error, or a network latency issue. We can make sure code returns
    the correct values or raises the correct exception in every case, ensuring our
    code always behaves as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '***Revealing Untested Code with coverage***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A great complement to unit testing, the `coverage` tool identifies whether any
    of your code has been missed during testing. It uses code analysis tools and tracing
    hooks to determine which lines of your code have been executed; when used during
    a unit test run, it can show you which parts of your codebase have been crossed
    over and which parts have not. Writing tests is useful, but having a way to know
    what part of your code you may have missed during the testing process is the cherry
    on the cake.
  prefs: []
  type: TYPE_NORMAL
- en: Install the `coverage` Python module on your system via `pip` to have access
    to the `coverage` program command from your shell.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The command may also be named python-coverage, if you install coverage through
    your operating system installation software. This is the case on Debian, for example.*'
  prefs: []
  type: TYPE_NORMAL
- en: Using `coverage` in stand-alone mode is straightforward. It can show you parts
    of your programs that are never run and which code might be “dead code,” that
    is, code that could be removed without modifying the normal workflow of the program.
    All the test tools we’ve talked about so far in this chapter are integrated with
    `coverage`.
  prefs: []
  type: TYPE_NORMAL
- en: When using `pytest`, just install the `pytest-cov` plugin via `pip install pytest-pycov`
    and add a few option switches to generate a detailed code coverage output, as
    shown in [Listing 6-13](ch06.xhtml#ch6list13).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-13: Using coverage with pytest*'
  prefs: []
  type: TYPE_NORMAL
- en: The `--cov` option enables the coverage report at the end of the test run. You
    need to pass the package name as an argument for the plugin to filter the coverage
    report properly. The output includes the lines of code that were not run and therefore
    have no tests. All you need to do now is spawn your favorite text editor and start
    writing tests for that code.
  prefs: []
  type: TYPE_NORMAL
- en: However, `coverage` goes one better, allowing you to generate clear HTML reports.
    Simply add the `--cov-report=html` flag, and the *htmlcov* directory from which
    you ran the command will be populated with HTML pages. Each page will show you
    which parts of your source code were or were not run.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to be *that* person, you can use the option `--cover-fail-under=COVER_MIN_PERCENTAGE`,
    which will make the test suite fail if a minimum percentage of the code is not
    executed when the test suite is run. While having a good coverage percentage is
    a decent goal, and while the tool is useful to gain insight into the state of
    your test coverage, defining an arbitrary percentage value does not provide much
    insight. [Figure 6-1](ch06.xhtml#ch6fig1) shows an example of a coverage report
    with the percentage at the top.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a code coverage score of 100 percent is a respectable goal, but
    it does not necessarily mean the code is entirely tested and you can rest. It
    only proves that your whole code path has been run; there is no indication that
    every possible condition has been tested.
  prefs: []
  type: TYPE_NORMAL
- en: You should use coverage information to consolidate your test suite and add tests
    for any code that is currently not being run. This facilitates later project maintenance
    and increases your code’s overall quality.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f06-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6-1: Coverage of ceilometer.publisher*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual Environments**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Earlier we mentioned the danger that your tests may not capture the absence
    of dependencies. Any application of significant size inevitably depends on external
    libraries to provide features the application needs, but there are many ways external
    libraries might cause issues on your operating system. Here are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: Your system does not have the library you need packaged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your system does not have the right *version* of the library you need packaged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need two different versions of the same library for two different applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These problems can happen when you first deploy your application or later on,
    while it’s running. Upgrading a Python library installed via your system manager
    might break your application in a snap without warning, for reasons as simple
    as an API change in the library being used by the application.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is for each application to use a library directory that contains
    all the application’s dependencies. This directory is then used to load the needed
    Python modules rather than the system-installed ones.
  prefs: []
  type: TYPE_NORMAL
- en: Such a directory is known as a *virtual environment*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Setting Up a Virtual Environment***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The tool `virtualenv` handles virtual environments automatically for you. Until
    Python 3.2, you’ll find it in the `virtualenv` package that you can install using
    `pip install virtualenv`. If you use Python 3.3 or later, it’s available directly
    via Python under the `venv` name.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the module, load it as the main program with a destination directory
    as its argument, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Once run, `venv` creates a *lib/pythonX.Y* directory and uses it to install
    `pip` into the virtual environment, which will be useful to install further Python
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then activate the virtual environment by “sourcing” the `activate`
    command. Use the following on Posix systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'On Windows systems, use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Once you do that, your shell prompt should appear prefixed by the name of your
    virtual environment. Executing `python` will call the version of Python that has
    been copied into the virtual environment. You can check that it’s working by reading
    the `sys.path` variable and checking that it has your virtual environment directory
    as its first component.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can stop and leave the virtual environment at any time by calling the `deactivate`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s it. Also note that you are not forced to run `activate` if you want
    to use the Python installed in your virtual environment just once. Calling the
    `python` binary will also work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, while we’re in our activated virtual environment, we do not have access
    to any of the modules installed and available on the main system. That is the
    point of using a virtual environment, but it does mean we probably need to install
    the packages we need. To do that, use the standard `pip` command to install each
    package, and the packages will install in the right place, without changing anything
    about your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Voilà! We can install all the libraries we need and then run our application
    from this virtual environment, without breaking our system. It’s easy to see how
    we can script this to automate the installation of a virtual environment based
    on a list of dependencies, as in [Listing 6-14](ch06.xhtml#ch6list14).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-14: Automatic virtual environment creation*'
  prefs: []
  type: TYPE_NORMAL
- en: It can still be useful to have access to your system-installed packages, so
    `virtualenv` allows you to enable them when creating your virtual environment
    by passing the `--system-site-packages` flag to the `virtualenv` command.
  prefs: []
  type: TYPE_NORMAL
- en: Inside `myvenv`, you will find a *pyvenv.cfg*, the configuration file for this
    environment. It doesn’t have a lot of configuration options by default. You should
    recognize `include-system-site-package`, whose purpose is the same as the `--system-site-packages`
    of `virtualenv` that we described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: As you might guess, virtual environments are incredibly useful for automated
    runs of unit test suites. Their use is so widespread that a particular tool has
    been built to address it.
  prefs: []
  type: TYPE_NORMAL
- en: '***Using virtualenv with tox***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the central uses of virtual environments is to provide a clean environment
    for running unit tests. It would be detrimental if you were under the impression
    that your tests were working, when they were not, for example, respecting the
    dependency list.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to ensure you’re accounting for all the dependencies would be to write
    a script to deploy a virtual environment, install `setuptools`, and then install
    all of the dependencies required for both your application/library runtime and
    unit tests. Luckily, this is such a popular use case that an application dedicated
    to this task has already been built: `tox`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `tox` management tool aims to automate and standardize how tests are run
    in Python. To that end, it provides everything needed to run an entire test suite
    in a clean virtual environment, while also installing your application to check
    that the installation works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using `tox`, you need to provide a configuration file named *tox.ini*
    that should be placed in the root directory of your project, beside your *setup.py*
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then run `tox` successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In this instance, `tox` creates a virtual environment in *.tox/python* using
    the default Python version. It uses *setup.py* to create a distribution of your
    package, which it then installs inside this virtual environment. No commands are
    run, because we did not specify any in the configuration file. This alone is not
    particularly useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change this default behavior by adding a command to run inside our test
    environment. Edit *tox.ini* to include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now `tox` runs the command `pytest`. However, since we do not have `pytest`
    installed in the virtual environment, this command will likely fail. We need to
    list `pytest` as a dependency to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: When run now, `tox` re-creates the environment, installs the new dependency,
    and runs the command `pytest`, which executes all of the unit tests. To add more
    dependencies, you can either list them in the `deps` configuration option, as
    is done here, or use the `-rfile` syntax to read from a file.
  prefs: []
  type: TYPE_NORMAL
- en: '***Re-creating an Environment***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes you’ll need to re-create an environment to, for example, ensure things
    work as expected when a new developer clones the source code repository and runs
    `tox` for the first time. For this, `tox` accepts a `--recreate` option that will
    rebuild the virtual environment from scratch based on parameters you lay out.
  prefs: []
  type: TYPE_NORMAL
- en: 'You define the parameters for all virtual environments managed by `tox` in
    the `[testenv]` section of *tox.ini*. And, as mentioned, `tox` can manage multiple
    Python virtual environments—indeed, it is possible to run our tests under a Python
    version other than the default one by passing the `-e` flag to `tox`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `tox` simulates any environment that matches an existing Python
    version: `py24`, `py25`, `py26`, `py27`, `py30`, `py31`, `py32`, `py33`, `py34`,
    `py35`, `py36`, `py37`, `jython`, and `pypy`! Furthermore, you can define your
    own environments. You just need to add another section named `[testenv:_envname_]`.
    If you want to run a particular command for just one of the environments, you
    can do so easily by listing the following in the *tox.ini* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'By using `pytest --cov=myproject` under the `py36-coverage` section as shown
    here, you override the commands for the `py36-coverage` environment, meaning when
    you run `tox -e py36-coverage`, `pytest` is installed as part of the dependencies,
    but the command `pytest` is actually run instead with the coverage option. For
    that to work, the `pytest-cov` extension must be installed: to this end, we replace
    the `deps` value with the `deps` from `testenv` and add the `pytest-cov` dependency.
    Variable interpolation is also supported by `tox`, so you can refer to any other
    field from the *tox.ini* file and use it as a variable, the syntax being `{[env_name`]variable_name}.
    This allows us to avoid repeating the same things over and over again.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Using Different Python Versions***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can also create a new environment with an unsupported version of Python
    right away with the following in *tox.ini*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: When we run this, it will now (attempt to) use Python 2.1 to run the test suite—although
    since it is very unlikely you have this ancient Python version installed on your
    system, I doubt this would work for you!
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s likely that you’ll want to support multiple Python versions, in which
    case it would be useful to have `tox` run all the tests for all the Python versions
    you want to support by default. You can do this by specifying the environment
    list you want to use when `tox` is run without arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: When `tox` is launched without any further arguments, all four environments
    listed are created, populated with the dependencies and the application, and then
    run with the command `pytest`.
  prefs: []
  type: TYPE_NORMAL
- en: '***Integrating Other Tests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can also use `tox` to integrate tests like `flake8`, as discussed in [Chapter
    1](ch01.xhtml#ch01). The following *tox.ini* file provides a PEP 8 environment
    that will install `flake8` and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the `pep8` environment is run using the default version of Python,
    which is probably fine, though you can still specify the `basepython` option if
    you want to change that.
  prefs: []
  type: TYPE_NORMAL
- en: When running `tox`, you’ll notice that all the environments are built and run
    sequentially. This can make the process very long, but since virtual environments
    are isolated, nothing prevents you from running `tox` commands in parallel. This
    is exactly what the `detox` package does, by providing a `detox` command that
    runs all of the default environments from *envlist* in parallel. You should `pip
    install` it!
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing Policy**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Embedding testing code in your project is an excellent idea, but how that code
    is run is also extremely important. Too many projects have test code lying around
    that fails to run for some reason or other. This topic is not strictly limited
    to Python, but I consider it important enough to emphasize here: you should have
    a zero-tolerance policy regarding untested code. No code should be merged without
    a proper set of unit tests to cover it.'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum you should aim for is that each of the commits you push passes all
    the tests. Automating this process is even better. For example, OpenStack relies
    on a specific workflow based on *Gerrit* (a web-based code review service) and
    *Zuul* (a continuous integration and delivery service). Each commit pushed goes
    through the code review system provided by Gerrit, and Zuul is in charge of running
    a set of testing jobs. Zuul runs the unit tests and various higher-level functional
    tests for each project. This code review, which is executed by a couple of developers,
    makes sure all code committed has associated unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using the popular GitHub hosting service, *Travis CI* is a tool that
    allows you to run tests after each push or merge or against pull requests that
    are submitted. While it is unfortunate that this testing is done post-push, it’s
    still a fantastic way to track regressions. Travis supports all significant Python
    versions out of the box, and it can be customized significantly. Once you’ve activated
    Travis on your project via the web interface at *[https://www.travis-ci.org/](https://www.travis-ci.org/)*,
    just add a *.travis.yml* file that will determine how the tests are run. [Listing
    6-15](ch06.xhtml#ch6list15) shows an example of a .*travis.yml* file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-15: A .travis.yml example file*'
  prefs: []
  type: TYPE_NORMAL
- en: With this file in place in your code repository and Travis enabled, the latter
    will spawn a set of jobs to test your code with the associated unit tests. It’s
    easy to see how you can customize this by simply adding dependencies and tests.
    Travis is a paid service, but the good news is that for open source projects,
    it’s entirely free!
  prefs: []
  type: TYPE_NORMAL
- en: The `tox-travis` package (*[https://pypi.python.org/pypi/tox-travis/](https://pypi.python.org/pypi/tox-travis/)*)
    is also worth looking into, as it will polish the integration between `tox` and
    Travis by running the correct `tox` target depending on the Travis environment
    being used. [Listing 6-16](ch06.xhtml#ch6list16) shows an example of a *.travis.yml*
    file that will install `tox-travis` before running `tox`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 6-16: A .travis.yml example file with tox-travis*'
  prefs: []
  type: TYPE_NORMAL
- en: Using `tox-travis`, you can simply call `tox` as the script on Travis, and it
    will call `tox` with the environment you specify here in the *.travis.yml* file,
    building the necessary virtual environment, installing the dependency, and running
    the commands you specified in *tox.ini*. This makes it easy to use the same workflow
    both on your local development machine and on the Travis continuous integration
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: These days, wherever your code is hosted, it is always possible to apply some
    automatic testing of your software and to make sure your project is moving forward,
    not being held back by the addition of bugs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Robert Collins on Testing**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Robert Collins is, among other things, the original author of the *Bazaar* distributed
    version control system. Today, he is a Distinguished Technologist at HP Cloud
    Services, where he works on OpenStack. Robert is also the author of many of the
    Python tools described in this book, such as fixtures, `testscenarios`, `testrepository`,
    and even `python-subunit`—you may have used one of his programs without knowing
    it!
  prefs: []
  type: TYPE_NORMAL
- en: '**What kind of testing policy would you advise using? Is it ever acceptable
    not to test code?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I think testing is an engineering trade-off: you must consider the likelihood
    of a failure slipping through to production undetected, the cost and size of an
    undetected failure, and cohesion of the team doing the work. Take OpenStack, which
    has 1,600 contributors: it’s difficult to work with a nuanced policy with so many
    people with their own opinions. Generally speaking, a project needs some automated
    testing to check that the code will do what it is intended to do, and that what
    it is intended to do is what is needed. Often that requires functional tests that
    might be in different codebases. Unit tests are excellent for speed and pinning
    down corner cases. I think it is okay to vary the balance between styles of testing,
    as long as there is testing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where the cost of testing is very high and the returns are very low, I think
    it’s fine to make an informed decision not to test, but that situation is relatively
    rare: most things can be tested reasonably cheaply, and the benefit of catching
    errors early is usually quite high.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the best strategies when writing Python code to make testing manageable
    and improve the quality of the code?**'
  prefs: []
  type: TYPE_NORMAL
- en: Separate out concerns and don’t do multiple things in one place; this makes
    reuse natural, and that makes it easier to put test doubles in place. Take a purely
    functional approach when possible; for example, in a single method either calculate
    something or change some state, but avoid doing both. That way you can test all
    of the calculating behaviors without dealing with state changes, such as writing
    to a database or talking to an HTTP server. The benefit works the other way around
    too—you can replace the calculation logic for tests to provoke corner case behavior
    and use mocks and test doubles to check that the expected state propagation happens
    as desired. The most heinous things to test are deeply layered stacks with complex
    cross-layer behavioral dependencies. There you want to evolve the code so that
    the contract between layers is simple, predictable, and—most usefully for testing—replaceable.
  prefs: []
  type: TYPE_NORMAL
- en: '**What’s the best way to organize unit tests in source code?**'
  prefs: []
  type: TYPE_NORMAL
- en: Have a clear hierarchy, like *$ROOT/$PACKAGE/tests*. I tend to do just one hierarchy
    for a whole source tree, for example *$ROOT/$PACKAGE/$SUBPACKAGE/tests*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within tests, I often mirror the structure of the rest of the source tree:
    *$ROOT/$PACKAGE/foo.py* would be tested in *$ROOT/$PACKAGE/tests/test_foo.py*.'
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the tree should not import from the tests tree, except perhaps in
    the case of a `test_suite`/`load_tests` function in the top level `__init__`.
    This permits you to easily detach the tests for small-footprint installations.
  prefs: []
  type: TYPE_NORMAL
- en: '**What do you see as the future of unit-testing libraries and frameworks in
    Python?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The significant challenges I see are these:'
  prefs: []
  type: TYPE_NORMAL
- en: The continued expansion of parallel capabilities in new machines, like phones
    with four CPUs. Existing unit test internal APIs are not optimized for parallel
    workloads. My work on the StreamResult Java class is aimed directly at resolving
    this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More complex scheduling support—a less ugly solution for the problems that class
    and module-scoped setup aim at.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finding some way to consolidate the vast variety of frameworks we have today:
    for integration testing, it would be great to be able to get a consolidated view
    across multiple projects that have different test runners in use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
