<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="app01"><span epub:type="pagebreak" id="page_305"/><strong>GOING FURTHER</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">The goal of this book was to discuss the core mathematics behind deep learning, the sort of math needed to follow what deep learning is and how it operates. We’ve done just that in the previous 11 chapters.</p>&#13;
<p class="indent">In this appendix, my goal is to point you toward more. Out of necessity, we only waded in the tide pools, which are fascinating enough, but in the depths, you’ll find still more beauty and elegance. What follows are pointers to help you get more out of the topics we covered.</p>&#13;
<h3 class="h3" id="app01lev1_1">Probability and Statistics</h3>&#13;
<p class="noindent">There are hundreds, if not thousands, of books on probability and statistics. The list here is, naturally, incomplete and not comprehensive, but it should help you expand your knowledge of these areas.</p>&#13;
<p class="block"><em><strong>Probability and Statistics</strong></em> <strong>by Michael Evans and Jeffrey Rosenthal</strong> A comprehensive textbook approach that’s available for free here: <em><a href="http://www.utstat.toronto.edu/mikevans/jeffrosenthal/book.pdf">http://www.utstat.toronto.edu/mikevans/jeffrosenthal/book.pdf</a></em>. Evans and Rosenthal’s book targets readers with exactly the sort of background that this book covers.</p>&#13;
<p class="block"><span epub:type="pagebreak" id="page_306"/><em><strong>Bayesian Statistics the Fun Way</strong></em> <strong>by Will Kurt</strong> We discussed Bayes’ theorem in <a href="ch03.xhtml#ch03">Chapter 3</a>. This book presents Bayesian statistics in an approachable way. Bayesian statistics is strongly related to machine learning, and you’ll eventually encounter it as you progress in your studies.</p>&#13;
<p class="block"><em><strong>Introduction to Probability</strong></em> <strong>by Joseph Blitzstein and Jessica Hwang</strong> Another well-liked introduction to probability, which includes Monte Carlo modeling.</p>&#13;
<p class="block"><em><strong>Python for Probability, Statistics, and Machine Learning</strong></em> <strong>by José Unpingco</strong> This book provides an alternative view to the approach I took in this book. It covers slightly different topics, but still using Python and NumPy. The machine learning portion covers what I call “classic machine learning” with some mention of deep learning.</p>&#13;
<p class="block"><em><strong>Practical Statistics for Medical Research</strong></em> <strong>by Douglas Altman</strong> A classic text, but still highly readable and relevant, though it comes from the era before much was done on personal computers. The focus is biostatistics, but the basics are the basics regardless of the application area.</p>&#13;
<h3 class="h3" id="app01lev1_2">Linear Algebra</h3>&#13;
<p class="noindent">Of all the subjects we covered, we were most unfair to linear algebra. The references here will help you appreciate the full elegance of the subject.</p>&#13;
<p class="block"><em><strong>Introduction to Linear Algebra</strong></em> <strong>by Gilbert Strang</strong> A popular introductory book. It covers in greater detail many of the topics I touched on in <a href="ch05.xhtml#ch05">Chapters 5</a> and <a href="ch06.xhtml#ch06">6</a>.</p>&#13;
<p class="block"><em><strong>Linear Algebra</strong></em> <strong>by David Cherney et al.</strong> Similar to the above, but available for free: <em><a href="https://www.math.ucdavis.edu/~linear/linear-guest.pdf">https://www.math.ucdavis.edu/~linear/linear-guest.pdf</a></em>.</p>&#13;
<p class="block"><em><strong>Linear Algebra</strong></em> <strong>by Jim Hefferon</strong> Also available for free, and at the same level as the others: <em><a href="http://joshua.smcvt.edu/linearalgebra/book.pdf">http://joshua.smcvt.edu/linearalgebra/book.pdf</a></em>.</p>&#13;
<h3 class="h3" id="app01lev1_3">Calculus</h3>&#13;
<p class="noindent">We discussed calculus in <a href="ch07.xhtml#ch07">Chapters 7</a> and <a href="ch08.xhtml#ch08">8</a>, but we limited ourselves to differentiation only. Calculus is, of course, much more than differentiation. The other primary part of calculus is integration. We ignored integration because deep learning seldom uses it. Convolution is integration when working with continuous variables, but most integrals become summations in the digital world. The references listed here will fill in the gaps in our cursory treatment.</p>&#13;
<p class="block"><em><strong>Essential Calculus Skills Practice Workbook</strong></em> <strong>by Chris McMullen</strong> This popular textbook/workbook covers differentiation and beginning integration. It includes solutions to problems. View this book as a review of <a href="ch07.xhtml#ch07">Chapter 7</a> and an introduction to integration.</p>&#13;
<p class="block"><span epub:type="pagebreak" id="page_307"/><em><strong>Calculus</strong></em> <strong>by James Stewart</strong> If McMullen’s book is a gentle introduction, this book is a comprehensive treatment of the topic. The book covers differentiation and integration, including multivariate calculus, that is, partial derivatives and vector calculus (see <a href="ch08.xhtml#ch08">Chapter 8</a>), and differential equations. It includes applications.</p>&#13;
<p class="block"><em><strong>Matrix Differential Calculus with Applications in Statistics and Econometrics</strong></em> <strong>by Jan Magnus and Heinz Neudecker</strong> Considered by some to be the standard matrix calculus reference, providing an in-depth and thorough treatment of the topic.</p>&#13;
<p class="block"><em><strong>The Matrix Cookbook</strong></em> <strong>by Kaare Brandt Petersen and Michael Syskind Pedersen</strong> A popular reference for matrix calculus that goes beyond what we covered in <a href="ch08.xhtml#ch08">Chapter 8</a>. You can find it here: <em><a href="http://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf">http://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf</a></em>.</p>&#13;
<h3 class="h3" id="app01lev1_4">Deep Learning</h3>&#13;
<p class="noindent">Deep learning is evolving rapidly. While some of the early, impressive “Wow, I didn’t know that was even possible” results are becoming less common, the field is quietly maturing and embedding itself in almost every area of science and technology. Our world will never be the same because of what deep learning has made possible.</p>&#13;
<p class="block"><em><strong>Deep Learning</strong></em> <strong>by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</strong> One of the first deep learning–specific textbooks and widely regarded as one of the best. It covers all the essentials but goes rather quickly at times.</p>&#13;
<p class="block"><em><strong>A Matrix Algebra Approach to Artificial Intelligence</strong></em> <strong>by Xian-Da Zhang</strong> A new book covering both matrices and machine learning, including deep learning. View it as a more mathematical treatment of machine learning.</p>&#13;
<p class="block"><strong>Deep Learning Specialization on Coursera</strong> Not a text, but a series of online courses with top-tier instructors. You can find the courses here: <em><a href="https://www.coursera.org/specializations/deep-learning/">https://www.coursera.org/specializations/deep-learning/</a></em>.</p>&#13;
<p class="block"><strong>Geoffrey Hinton’s Coursera lectures</strong> In 2012, Hinton gave a lecture series on Coursera, and the lectures are well worth listening to even now. RMSprop was discussed in this series. The lectures are quite accessible and not overly math-heavy. You can find them here: <em><a href="https://www.cs.toronto.edu/~hinton/coursera_lectures.html">https://www.cs.toronto.edu/~hinton/coursera_lectures.html</a></em>.</p>&#13;
<p class="block"><em><strong>Deep Learning: A Visual Approach</strong></em> <strong>by Andrew Glassner</strong> This book covers the breadth of deep learning, from supervised learning to reinforcement learning—all without mathematics. Use it as a rapid, high-level introduction to many topics in the field.</p>&#13;
<p class="block"><strong>Reddit</strong> To keep your finger on the pulse of the deep learning community, follow the conversation on Reddit: <em><a href="https://www.reddit.com/r/MachineLearning/">https://www.reddit.com/r/MachineLearning/</a></em>.</p>&#13;
<p class="block"><span epub:type="pagebreak" id="page_308"/><strong>Arxiv</strong> Found at <em><a href="https://arxiv.org/">https://arxiv.org/</a></em>, Arxiv is a preprint repository. The latest deep learning papers will show up here. Note, as the field progresses so rapidly, publication in peer-reviewed journals is not the norm. Rather, posting papers on arxiv.org, especially those presented at conferences, is the way to see the latest research. Arxiv is separated into categories. Those I’ve provided here are the ones I tend to follow, though there are others:</p>&#13;
<ul>&#13;
<li><p>Computer Vision and Pattern Recognition: <em><a href="https://arxiv.org/list/cs.CV/recent/">https://arxiv.org/list/cs.CV/recent/</a></em></p></li>&#13;
<li><p>Artificial Intelligence: <em><a href="https://arxiv.org/list/cs.AI/recent/">https://arxiv.org/list/cs.AI/recent/</a></em></p></li>&#13;
<li><p>Neural and Evolutionary Computing: <em><a href="https://arxiv.org/list/cs.NE/recent/">https://arxiv.org/list/cs.NE/recent/</a></em></p></li>&#13;
<li><p>Machine Learning: <em><a href="https://arxiv.org/list/stat.ML/recent/">https://arxiv.org/list/stat.ML/recent/</a></em></p></li>&#13;
</ul>&#13;
</div></body></html>