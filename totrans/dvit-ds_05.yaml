- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Binary Classification
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'Many difficult questions can be phrased simply as yes/no questions: To buy
    the stock or not? To take the job or not? To hire the applicant or not? This chapter
    is about *binary classification*, the technical term for answering yes/no questions,
    or deciding between true and false, 1 and 0.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多难以回答的问题可以简单地表述为是/否问题：买这只股票还是不买？接受这份工作还是不接受？雇佣这个申请人还是不雇佣？本章讲述的是*二元分类*，这是回答是/否问题或在真与假、1与0之间做出决策的技术术语。
- en: We’ll start by introducing a common business scenario that depends on binary
    classification. We’ll continue by discussing linear probability models, a simple
    but powerful binary classification approach based on linear regression. We’ll
    also cover logistic regression, a more advanced classification method that improves
    on some of the shortcomings of linear probability models. We’ll conclude by going
    over some of the many applications of binary classification methods, including
    risk analysis and forecasting.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍一个依赖于二元分类的常见商业场景开始。接着，我们将讨论线性概率模型，这是一种基于线性回归的简单但强大的二元分类方法。我们还会介绍逻辑回归，这是一种更先进的分类方法，能够改进线性概率模型的一些不足之处。最后，我们将讨论二元分类方法的众多应用，包括风险分析和预测。
- en: Minimizing Customer Attrition
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化客户流失
- en: 'Imagine you’re running a big tech company with about 10,000 large clients.
    Each client has a long-term contract with you, promising that they’ll pay you
    regularly to use your company’s software. However, all your clients are free to
    exit their contracts at any time and stop paying you if they decide they don’t
    want to use your software anymore. You want to have as many clients as you can,
    so you do your best to do two things: one, grow the company by signing new contracts
    with new clients and, two, prevent attrition by ensuring that your existing clients
    don’t exit their contracts.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你经营着一家大型科技公司，拥有大约10,000个大客户。每个客户与你签订了长期合同，承诺定期支付费用以使用贵公司的软件。然而，所有客户都可以随时退出合同，如果他们决定不再使用你的软件，就可以停止支付。你希望尽可能拥有更多的客户，因此你尽力做到两件事：一是通过与新客户签订新合同来推动公司发展，二是通过确保现有客户不退出合同来防止客户流失。
- en: 'In this chapter, we’ll focus on your second goal: preventing client attrition.
    This is an extremely common concern for businesses in every industry, and one
    that every company struggles with. It’s especially important since acquiring new
    customers is well known to be much more costly than retaining existing ones.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点讨论你的第二个目标：防止客户流失。这是各行业企业都非常关心的一个问题，也是每家公司都在努力解决的问题。特别重要的是，因为获取新客户的成本显然远高于留住现有客户的成本。
- en: To prevent attrition, you have a team of client managers who stay in touch with
    clients, make sure they’re happy, resolve any problems that come up, and in general
    ensure that they’re satisfied enough to continue renewing their contracts indefinitely.
    Your client management team is small, however—only a few people who together have
    to try to keep 10,000 clients happy. It’s impossible for them to be in constant
    contact with all 10,000 clients, and inevitably some clients will have concerns
    and problems that your client management team isn’t able to find out about or
    resolve.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止客户流失，你有一支客户经理团队与客户保持联系，确保他们满意，解决任何出现的问题，并确保他们足够满意以便继续无限期续签合同。然而，你的客户管理团队规模较小——只有几个人，他们必须共同努力保持10,000个客户的满意。对他们来说，无法与所有10,000个客户保持持续联系，难免会有一些客户的问题和担忧是你的客户管理团队无法发现或解决的。
- en: As the leader of your company, you have to decide how to direct the efforts
    of the client managers to minimize attrition. Every hour they spend working with
    a client who’s at high risk of attrition is probably worthwhile, but their time
    is wasted when they spend too much time on a client who is not at risk of attrition.
    The best use of the client managers’ time will be to focus on the clients who
    have the highest likelihood of canceling their contracts. All the managers need
    is a list of all the high-risk clients to contact, and then they can use their
    time with maximum efficiency to minimize attrition.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为公司领导者，你需要决定如何引导客户经理的努力，以最小化流失。他们每花一小时与高流失风险的客户合作，可能是值得的，但如果他们花太多时间在那些没有流失风险的客户身上，那就是浪费时间。客户经理们最有效的时间使用方式是集中精力处理那些最有可能取消合同的客户。所有经理需要的，只有一个高风险客户的名单，这样他们就可以以最高效率利用时间来减少流失。
- en: Getting an accurate list of high-attrition-risk clients is not an easy task,
    since you can’t read the minds of all your clients and immediately know which
    ones are in danger of canceling their contracts and which ones are happy as clams.
    Many companies rely on intuition or guessing to decide which clients have the
    highest attrition risk. But intuition and guessing rarely lead to the most accurate
    possible results. We’ll get better accuracy, and therefore better cost savings,
    by using data science tools to decide whether each client is high risk or low
    risk.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 获取准确的高流失风险客户列表并非易事，因为你不能读懂所有客户的心思，立即知道哪些客户面临取消合同的风险，哪些客户则一切如意。许多公司依赖直觉或猜测来决定哪些客户的流失风险最高。但直觉和猜测很少能得出最准确的结果。通过使用数据科学工具来决定每个客户是高风险还是低风险，我们可以获得更好的准确度，从而节省更多成本。
- en: 'Deciding whether a client is high risk or low risk for attrition is a binary
    classification problem; it consists of answering a yes-or-no question: Is this
    client at high risk for attrition? What started as a daunting business problem
    (how to increase revenue growth with limited resources) has been reduced to a
    much simpler data analysis problem (how to perform a binary classification of
    attrition risk). We’ll approach this problem by reading in historical data related
    to past attrition, analyzing that data to find useful patterns in it, and applying
    our knowledge of those patterns to more recent data to perform our binary classification
    and make useful business recommendations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决定一个客户是高风险还是低风险流失是一个二分类问题；它由一个是或否的问题组成：这个客户面临高流失风险吗？最初作为一个令人头疼的商业问题（如何在有限的资源下增加收入增长）已经被简化为一个更简单的数据分析问题（如何进行流失风险的二分类）。我们将通过读取与过去流失相关的历史数据，分析这些数据以发现其中有用的模式，然后将我们对这些模式的理解应用于更近期的数据，以执行我们的二分类并提出有价值的商业建议。
- en: Using Linear Probability Models to Find High-Risk Customers
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性概率模型来寻找高风险客户
- en: 'We can choose from several data analysis methods to do binary classification.
    But before we explore these methods, we should read some data into Python. We’ll
    use fabricated data about hypothetical clients of our imaginary firm. You can
    load it into your Python session directly from its online home by using the following
    snippet:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从几种数据分析方法中选择来进行二分类。但在探索这些方法之前，我们需要先将一些数据读取到Python中。我们将使用关于我们虚构公司假设客户的虚拟数据。你可以通过使用以下代码段直接从它的在线地址加载到Python会话中：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this snippet, we import pandas and read the file for our data. This time,
    we read the file directly from a website where it’s being stored. The file is
    in *.csv* format, which you’ve already encountered in previous chapters. You can
    print the top five rows of our data as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码段中，我们导入了pandas并读取了数据文件。这次我们直接从存储文件的一个网站读取该文件。该文件是*.csv*格式，你在之前的章节中已经遇到过。你可以按照以下方式打印数据的前五行：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see the following output:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到以下输出：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The last line of the output tells us that the dataset has five columns. Suppose
    that the first four columns of the data were generated about six months ago. The
    first column is a four-character code for every client. The second column is `lastmonth_activity`,
    a measurement of the number of times someone at that client company accessed our
    software in the last month before this data was generated (between 6 and 7 months
    ago). The third column is `lastyear_activity`, the same measurement for the entire
    year before the data was generated (between 6 and 18 months ago). The `lastyear_activity`
    column is not visible in the preceding snippet, where we can see only ellipses
    between the second and fourth columns. The reason for this is that the pandas
    package has default display settings that ensure its output will be small enough
    to fit easily onscreen. If you’d like to change the maximum number of columns
    that pandas prints out, you can run the following line in Python:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后一行告诉我们数据集有五列。假设数据的前四列是在大约六个月前生成的。第一列是每个客户的四字符代码。第二列是`lastmonth_activity`，即该客户公司在生成此数据之前的一个月内访问我们软件的次数（大约在6到7个月前）。第三列是`lastyear_activity`，即数据生成之前整整一年的相同测量（大约在6到18个月前）。`lastyear_activity`列在前面的代码段中不可见，我们只能看到第二列和第四列之间的省略号。原因是pandas包有默认的显示设置，确保其输出足够小，能轻松适应屏幕。如果你想更改pandas打印的最大列数，可以在Python中运行以下代码：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we use the pandas option `display.max_columns` to change the maximum number
    of columns pandas will display to `6`. This change ensures that if we ever print
    the `attrition_past` dataset again, we’ll see all five of its columns, and when
    we add one more column to the dataset, we’ll then be able to see all six of its
    columns. If you want to display all columns of every dataset, no matter how many,
    you can change the `6` to `None`, which will mean that there’s no maximum limit
    on the number of columns for pandas to display.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 pandas 选项 `display.max_columns` 将 pandas 显示的最大列数更改为 `6`。这个更改确保了如果我们再次打印
    `attrition_past` 数据集时，我们能够看到它的所有五列，当我们向数据集中添加一列时，我们就能看到它的所有六列。如果你希望显示所有数据集的所有列，不论多少列，你可以将
    `6` 改为 `None`，这意味着 pandas 将不再对列数设置最大限制。
- en: 'Besides columns recording activity levels, we also have a record of the number
    of employees each company had six months ago in the `number_of_employees` column.
    Finally, suppose that the final column, `exited`, was generated today. This column
    records whether a given corporation exited its contract at any time in the six-month
    period between when the first four columns were generated and today. This column
    is recorded in a binary format: 1 for a client that exited in the last six months
    and 0 for a client that didn’t exit. The `exited` column is our binary measurement
    of attrition, and it’s the column that interests us most because it’s what we’re
    going to learn to predict.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了记录活动水平的列，我们还有记录每个公司六个月前员工数量的 `number_of_employees` 列。最后，假设最终列 `exited` 是今天生成的。这个列记录了某个公司是否在从前四列生成至今天的六个月期间退出了合同。这个列以二进制格式记录：1
    表示在过去六个月内退出合同的客户，0 表示没有退出的客户。`exited` 列是我们衡量流失的二进制指标，它是我们最感兴趣的列，因为我们将学习如何预测它。
- en: Having four columns that are six months old and one column that’s new may seem
    like a bug or an unnecessary complication. However, this temporal difference between
    our columns enables us to find patterns relating the past and the future. We’ll
    find patterns in the data that show how activity levels and employee numbers at
    one particular time can be used to predict attrition levels later. Eventually,
    the patterns we’ll find in this data will enable us to use a client’s activity
    as measured today to predict their attrition likelihood during the next six months.
    If we can predict a client’s attrition risk in the next six months, we can take
    action during those six months to change their minds and keep them around. Convincing
    clients to stay will be the client manager’s role—the contribution of data science
    will be to make the attrition prediction itself.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有四个六个月前的列和一个新列，可能看起来像是一个错误或不必要的复杂化。然而，我们列之间的时间差异使我们能够找到过去和未来之间的模式。我们将会在数据中找到一些模式，显示出某一时刻的活动水平和员工数量如何能够预测未来的流失水平。最终，我们在这些数据中找到的模式将使我们能够利用今天测量的客户活动来预测他们在接下来六个月内的流失可能性。如果我们能预测客户在未来六个月的流失风险，就可以在这六个月内采取行动，改变他们的想法，并留住他们。说服客户留下将是客户经理的角色——数据科学的贡献将是进行流失预测本身。
- en: Plotting Attrition Risk
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘制流失风险
- en: 'Before we jump into finding all these patterns, let’s check how often attrition
    occurs in our data:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入寻找这些模式之前，让我们检查一下流失在数据中发生的频率：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result we get is about 0.58, meaning that about 58 percent of the clients
    in the data exited their contracts in the last six months. This shows us that
    attrition is a big problem for the business.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果是大约 0.58，意味着数据中大约 58% 的客户在过去六个月内退出了他们的合同。这表明流失是业务的一大问题。
- en: 'Next, we should make plots of our data. Doing this early and often is a good
    idea in any data analysis scenario. We’re interested in how each of our variables
    will relate to the binary `exited` variable, so we can start with a plot of the
    relationship of `lastmonth_activity` and `exited`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该绘制数据图表。在任何数据分析场景中，提前并经常绘制图表都是一个好主意。我们对每个变量与二元变量 `exited` 之间的关系感兴趣，所以我们可以从绘制
    `lastmonth_activity` 和 `exited` 的关系开始：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see the result in [Figure 5-1](#figure5-1).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图 5-1](#figure5-1)中看到结果。
- en: '![](image_fi/502888c05/f05001.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c05/f05001.png)'
- en: 'Figure 5-1: Historical attrition of clients of a hypothetical company'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-1：假设公司客户的历史流失情况
- en: On the x-axis, we see last month’s activity, although since the data was recorded
    six months ago, it’s really activity from six to seven months ago. The y-axis
    shows attrition from our `exited` variable, and that’s why all values are 0 (did
    not exit) or 1 (exited) in the most recent six months. Eyeballing this figure
    can give us a basic idea of the relationship between past activity and future
    attrition. In particular, the clients with the most activity (> 600) did not exit
    their contracts in the six months after their high activity was recorded. High
    activity seems to be a predictor of client loyalty, and if it is, low activity
    will be a predictor of client attrition.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在x轴上，我们看到的是上个月的活动，尽管由于数据是六个月前记录的，因此实际上显示的是六到七个月前的活动。y轴展示了我们`exited`变量的流失情况，这就是为什么最近六个月所有值都为0（未流失）或1（已流失）。从直观上看，这个图可以给我们一个关于过去活动与未来流失之间关系的基本印象。特别是，活动最多的客户（>
    600）在记录了高活动后的六个月内没有退出他们的合同。高活动似乎是客户忠诚度的预测因子，如果是这样，低活动则会是客户流失的预测因子。
- en: Confirming Relationships with Linear Regression
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用线性回归确认关系
- en: 'We’ll want to confirm our initial visual impression by performing a more rigorous
    quantitative test. In particular, we can use linear regression. Remember that
    in Chapter 2, we had a cloud of points and used linear regression to find a line
    that was the best fit to the cloud. Here, our points don’t look very cloud-like
    because of the limited range of the *y* variable: our “cloud” is two scattered
    lines at *y* = 0 and *y* = 1\. However, linear regression is a mathematical method
    from linear algebra, and it doesn’t care how cloud-like our plot looks. We can
    perform linear regression on our attrition data with code that’s almost identical
    to the code we used before:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过进行更严格的定量测试来确认我们最初的直观印象。特别是，我们可以使用线性回归。记住，在第2章中，我们有一组点，并使用线性回归找到了一条最佳拟合的线。在这里，由于*y*变量的范围有限，我们的点看起来不太像云状：我们的“云”是位于*y*
    = 0和*y* = 1的两条散点线。然而，线性回归是线性代数中的一种数学方法，它并不关心我们的图形看起来是否像云状。我们可以使用与之前几乎相同的代码在我们的流失数据上执行线性回归：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this snippet, we create a variable called `regressor`, which we then fit
    to our data. After fitting our regressor, we can plot our regression line going
    through our “cloud” of data, just as we did in Chapter 2:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们创建了一个名为`regressor`的变量，然后将其拟合到我们的数据上。在拟合完回归模型后，我们可以像在第2章中一样，将回归线绘制在我们的“数据云”上：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 5-2](#figure5-2) shows the result of this code.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](#figure5-2)展示了这段代码的结果。'
- en: '![](image_fi/502888c05/f05002.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c05/f05002.png)'
- en: 'Figure 5-2: A linear regression predicting a 0–1 attrition outcome'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-2：预测0–1流失结果的线性回归
- en: You can compare this plot with Figure 2-2 in Chapter 2. Just as we did in Figure
    2-2, we have a collection of points, and we’ve added a regression line that we
    know is the line of best fit to those points. Remember that we interpret the value
    of a regression line as an expected value. In Figure 2-2, we saw that our regression
    line went approximately through the point *x* = 109, *y* = 17,000, and we interpreted
    that to mean that in month 109, we expect about 17,000 car sales.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个图与第2章中的图 2-2进行比较。就像我们在图 2-2中所做的那样，我们有一组数据点，并且我们添加了一条回归线，我们知道这条回归线是这些数据点的最佳拟合线。记住，我们将回归线的值解释为期望值。在图
    2-2中，我们看到回归线大约通过了点*x* = 109，*y* = 17,000，我们将其解释为在第109个月，我们预计汽车销量大约为17,000辆。
- en: In [Figure 5-2](#figure5-2), the way to interpret our expected values may not
    seem immediately obvious. For example, at *x* = 400, the *y* value of the regression
    line is about 0.4\. This means that our expected value of `exited` is 0.4, but
    that’s not a cogent statement because `exited` can be only 0 or 1 (either you
    exit or you don’t, with no middle ground). So, what could it mean to expect 0.4
    “exiteds,” or 0.4 units of exiting at that activity level?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 5-2](#figure5-2)中，解释我们的期望值的方法可能看起来不是立即显而易见的。例如，在*x* = 400时，回归线的*y*值约为0.4。这意味着我们期望`exited`的值为0.4，但这是一个不太合适的说法，因为`exited`只能是0或1（要么退出，要么不退出，没有中间状态）。那么，在这个活动水平下，期望有0.4个“退出”，或者0.4个流失单位，这又意味着什么呢？
- en: 'The way we interpret an expected value of *0.4 units of exiting* is as a probability:
    we conclude that clients with an activity level of about 400 in the most recent
    month have about a 40 percent probability of exiting their contracts. Since our
    exited data consists of six months of exits after the activity levels were recorded,
    we interpret the value of the regression line as a 40 percent probability of attrition
    over the next six months after the activity level was recorded. Another way we
    can phrase our estimated 40 percent attrition probability is to say that we estimate
    a 40 percent attrition risk for clients with an activity level of 400.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释*0.4 单位流失*的预期值呢？我们将其解释为概率：我们得出结论，最近一个月活动水平约为 400 的客户有约 40% 的概率会退出合同。由于我们的流失数据是活动水平记录后六个月的流失情况，我们将回归线的值解释为活动水平记录后的接下来的六个月内，流失概率为
    40%。我们也可以这样表述我们估计的 40% 流失概率：我们估计活动水平为 400 的客户的流失风险为 40%。
- en: 'The regression in [Figure 5-2](#figure5-2) is a standard linear regression,
    exactly like the linear regression model we created in Chapter 2 and plotted in
    Figure 2-2\. However, when we perform a standard linear regression on binary data
    (data consisting of only two values like 0 and 1), we have a special name for
    it: we call it a *linear probability model (LPM)*. These models are simple and
    easy to implement, but they can be useful whenever we want to know a predicted
    probability of something that’s hard to predict.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](#figure5-2)中的回归是一个标准的线性回归，正如我们在第 2 章中创建的线性回归模型并在图 2-2 中绘制的那样。然而，当我们对二进制数据（仅由
    0 和 1 两个值组成的数据）进行标准线性回归时，我们有一个特殊的名称：我们称之为*线性概率模型（LPM）*。这些模型简单且易于实现，但当我们想要预测一些难以预测的事情时，它们非常有用。'
- en: 'After performing our regression and interpreting its values, the last important
    step is to make a business decision based on everything we’ve learned. [Figure
    5-2](#figure5-2) shows a simple relationship between activity and exit probability:
    lower activity is associated with higher exit probability, and higher activity
    is associated with lower exit probability. What we call *exit probability*, we
    can also call *attrition risk*, so we can also say that last month’s activity
    is negatively correlated with the next six months’ attrition risk. This negative
    correlation makes sense from a business point of view: if a client uses your product
    very actively, we expect them to be unlikely to exit their contract, and if a
    client is very inactive, we expect them to be more likely to exit.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行回归并解释其值后，最后一个重要步骤是根据我们学到的所有知识做出商业决策。[图 5-2](#figure5-2)展示了活动与退出概率之间的简单关系：较低的活动与较高的退出概率相关联，而较高的活动与较低的退出概率相关联。我们所称的*退出概率*，也可以称作*流失风险*，因此我们也可以说上个月的活动与接下来六个月的流失风险呈负相关。从商业角度来看，这个负相关是有意义的：如果一个客户非常活跃，我们预计他们不太可能退出合同，而如果客户非常不活跃，我们预计他们更可能退出。
- en: 'Knowing that a general negative correlation exists between activity and attrition
    risk is helpful. But we can be even more specific in our reasoning and decision-making
    if we calculate the exact predicted attrition risk for each client. This will
    enable us to make individualized decisions for each client based on their predicted
    risk. The following code calculates the attrition risk for each client (the predicted
    value from our regression) and stores its value in a new column called `predicted`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 知道活动与流失风险之间普遍存在负相关性是有帮助的。但如果我们计算出每个客户的确切预测流失风险，我们的推理和决策会更加具体。这将使我们能够根据每个客户的预测风险做出个性化决策。以下代码计算每个客户的流失风险（我们回归模型的预测值），并将其存储在一个名为`predicted`的新列中：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you run `print(attrition_past.head())`, you can see that our attrition dataset
    now has six columns. Its new sixth column is the predicted attrition probability
    for each client based on our regression. Of course, this is not very useful to
    us; we don’t need predicted attrition probabilities, since this is a record of
    past attrition and we already know with certainty whether each of these clients
    exited.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行`print(attrition_past.head())`，你可以看到我们的流失数据集现在有六列。它的新第六列是根据我们的回归模型预测的每个客户的流失概率。当然，这对我们并没有太大用处；我们不需要预测的流失概率，因为这是过去的流失记录，我们已经确定每个客户是否退出。
- en: 'Altogether, attrition prediction has two steps. First, we learn the relationships
    between features and target variables by using data from the past. Second, we
    use the relationships we learned from past data to make predictions for the future.
    So far, we’ve done only the first step: we’ve fit a regression that captures the
    relationship between customer attributes and attrition risk. Next, we need to
    make predictions for the future.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，员工流失预测有两个步骤。首先，我们通过使用过去的数据学习特征与目标变量之间的关系。其次，我们利用从过去数据中学到的关系来做未来的预测。到目前为止，我们只做了第一步：我们拟合了一个回归模型，捕捉了客户属性与流失风险之间的关系。接下来，我们需要做出未来的预测。
- en: Predicting the Future
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测未来
- en: 'Let’s download and open more fabricated data. This time, suppose that all the
    data was generated today, so its `lastmonthactivity` column refers to the previous
    month, and its `lastyearactivity` column refers to the 12-month period ending
    today. We can read in our data as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载并打开更多的虚拟数据。这次，假设所有数据都是今天生成的，因此它的`lastmonthactivity`列指的是上个月，而`lastyearactivity`列指的是截至今天的12个月期间。我们可以按照以下方式读取我们的数据：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `attrition_past` dataset that we worked with before used old data (more
    than six months old) to predict attrition that happened in the recent past (any
    time in the last six months). By contrast, with this dataset, we’ll use new data
    (generated today) to predict attrition that we expect to happen in the near future
    (in the next six months). That’s why we’re calling it `attrition_future`. If you
    run `print(attrition_future.head())`, you can see the first five rows of this
    data:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用的`attrition_past`数据集使用了旧数据（超过六个月的旧数据）来预测最近发生的员工流失（过去六个月内的任何时间）。相比之下，使用这个数据集时，我们将使用新的数据（今天生成的）来预测我们预期将在不久的将来（接下来的六个月内）发生的员工流失。这就是我们将其称为`attrition_future`的原因。如果你运行`print(attrition_future.head())`，你可以看到数据的前五行：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see that this dataset’s first four columns have the same names and interpretations
    as the first four columns of `attrition_past`. However, this dataset doesn’t have
    a fifth, `exited` column. The dataset lacks this column because the `exited` column
    is supposed to record whether a client exited their contract in the six-month
    period after the other columns were generated. But that six-month period hasn’t
    happened yet; it’s the six months that start today. We need to use what we’ve
    learned from the attrition dataset to predict the probabilities of attrition for
    this new set of clients. When we do, we’ll be making a prediction about the future
    rather than the past.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这个数据集的前四列与`attrition_past`的前四列具有相同的名称和含义。然而，这个数据集没有第五列`exited`。数据集缺少这列是因为`exited`列应该记录客户在其他列生成后的六个月期间是否退出了他们的合同。但是那个六个月的时间段还没有发生；它是从今天开始的六个月。我们需要利用从流失数据集中学到的知识来预测这个新客户集的流失概率。当我们这样做时，我们就是在做未来的预测，而不是过去的预测。
- en: All of the four-character corporation codes in the first column of `attrition_future`
    are new—they didn’t appear in the original attrition dataset. We can’t use anything
    from the original attrition dataset to learn directly about this new dataset.
    However, we can use the regressor we fit to make attrition probability predictions
    for this new dataset. In other words, we don’t use the actual data from `attrition_past`
    to learn about `attrition_future`, but we do use the patterns we found in `attrition_past`,
    which we encoded in a linear regression, to make predictions about `attrition_future`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`attrition_future`第一列中的所有四字符公司代码都是新的——它们在原始的流失数据集中没有出现。我们不能直接利用原始流失数据集中的任何数据来学习这个新的数据集。但是，我们可以利用我们拟合的回归模型来为这个新数据集做流失概率的预测。换句话说，我们不会直接用`attrition_past`中的实际数据来学习`attrition_future`，但我们会利用在`attrition_past`中发现的模式，这些模式我们通过线性回归编码出来，用来预测`attrition_future`。'
- en: 'We can predict attrition probabilities for the `attrition_future` dataset in
    exactly the same way we predicted attrition probabilities for the `attrition_past`
    dataset, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以与预测`attrition_past`数据集流失概率相同的方式预测`attrition_future`数据集的流失概率，如下所示：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This snippet adds a new column called `predicted` to the `attrition_future`
    dataset. We can run `print(attrition_future.head())` to see the top five rows
    after the change:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码为`attrition_future`数据集添加了一个名为`predicted`的新列。我们可以运行`print(attrition_future.head())`来查看更改后的前五行：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see that the pattern of low predicted exit probability for high-activity
    clients matches the pattern we observed for the `attrition_past` dataset. This
    is because our predicted probabilities were generated using the same regressor
    that was trained on the `attrition_past` dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，高活动客户的低预测流失概率模式与我们在`attrition_past`数据集中观察到的模式相匹配。这是因为我们的预测概率是使用与`attrition_past`数据集训练的相同回归模型生成的。
- en: Making Business Recommendations
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 制定业务建议
- en: 'After calculating these predicted probabilities, we want to translate them
    to business recommendations for our client management team. The simplest way to
    direct the team members’ efforts would be to provide them with a list of high-risk
    clients to focus their efforts on. We can specify a number of clients *n* that
    we think they have the time and bandwidth to focus on, and create a list of the
    top *n* highest-risk clients. We can do this for *n* = 5 as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算这些预测概率后，我们希望将它们转化为针对客户管理团队的业务建议。最简单的方式是为团队成员提供一个高风险客户名单，帮助他们集中精力。我们可以指定一个客户数量
    *n*，以此为依据选择我们认为他们有时间和精力关注的客户，并创建前 *n* 个高风险客户的名单。对于 *n* = 5，我们可以按如下方式操作：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When we run this line, we get the following output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这行代码时，得到如下输出：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can see that our top five highest-risk clients have predicted probabilities
    over 0.7 (70 percent), quite a high attrition probability.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们的前五大高风险客户的预测概率超过了0.7（70％），这是相当高的流失概率。
- en: 'Now, suppose your client managers are unsure of the number of clients they
    can focus on. Instead of asking for the top *n* clients for some *n*, they may
    simply want a ranked list of every client from highest to lowest attrition probability.
    The client managers can start at the beginning of the list and work their way
    through it as far as they can get. You can print this list easily as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你的客户经理们不确定他们可以关注多少客户。与其要求提供前* n *个客户，他们可能只想要一个从最高到最低流失概率排序的客户名单。客户经理可以从名单的开头开始，并尽可能深入地处理下去。你可以轻松地打印出这个名单，方法如下：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is a list of all corporations in the `attrition_future` dataset,
    ranked from highest to lowest attrition probability:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个按流失概率从高到低排名的`attrition_future`数据集中的所有公司名单：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first three corporations in this list—`whsh`, `pian`, and `mike`—are estimated
    to have the highest attrition risk (highest probability of exiting their contracts).
    In this case, the data shows a three-way tie for highest risk, since all three
    of these corporations have the same predicted high risk, and all the other corporations
    have lower predicted attrition risk.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该名单中的前三家公司——`whsh`、`pian` 和 `mike`——预计具有最高的流失风险（即退出合同的概率最大）。在这种情况下，数据表明这三家公司存在三方平局的最高风险，因为这三家公司都具有相同的高风险预测，而其他公司则具有较低的流失风险预测。
- en: 'Finally, you may decide that you’re interested in any clients whose predicted
    probabilities are higher than a certain threshold *x*. We can do this as follows
    for *x* = 0.7:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能决定只关注那些预测概率高于某个阈值 *x* 的客户。我们可以按如下方式进行操作，*x* = 0.7：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You’ll see a full list of all corporations that are predicted to have a greater
    than 70 percent attrition risk over the next six months. This could be a useful
    priority list for your client managers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到所有公司完整的名单，所有这些公司预计在未来六个月内流失风险超过70％。这可能是一个有用的优先级名单，供你的客户经理使用。
- en: Measuring Prediction Accuracy
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 衡量预测准确性
- en: In the previous section, we went through all the steps necessary to send a list
    of at-risk corporations to our client managers. Having reported our attrition
    risk predictions, we may feel that our task is complete and we can move on to
    the next one. But we’re not finished yet. As soon as we deliver our predictions
    to client managers, they’ll likely immediately ask us how accurate we expect our
    predictions to be. They’ll want to know how much they can trust our predictions
    before they put in great effort acting on them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分，我们已经完成了所有必要的步骤，将高风险公司名单发送给客户经理。在报告了我们的流失风险预测后，我们可能会觉得任务已经完成，可以继续进行下一个任务。但实际上，我们还没有完成。一旦我们将预测结果交给客户经理，他们很可能会立刻问我们，预测结果的准确性有多高。他们希望了解在采取行动之前，他们可以多大程度上信任我们的预测。
- en: 'In Chapter 2, we went over two common ways to measure the accuracy of linear
    regressions: root mean squared error (RMSE) and mean absolute error (MAE). Our
    LPM is technically a linear regression, so it’s possible to use these metrics
    again. However, for classification problems, the common convention is to use a
    different set of metrics that express classification accuracy in a more easily
    interpretable way. The first thing we’ll need to do is create lists of our predictions
    and actual values, respectively:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们讨论了衡量线性回归准确性的两种常见方法：均方根误差（RMSE）和平均绝对误差（MAE）。我们的LPM本质上是一个线性回归，因此可以再次使用这些指标。然而，对于分类问题，常见的惯例是使用一组不同的指标，这些指标以更易于解释的方式表达分类准确性。我们首先需要做的是分别创建预测值和实际值的列表：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this snippet, we calculate the median value of our `predicted` column. Then
    we create `prediction`, which will be 0 when our LPM predicts below-median probability,
    and 1 when our LPM predicts above-median probability. We’re doing this because
    when we measure accuracy for classification tasks, we’ll use metrics that count
    exact matches like `predicted` = 1, `actual` = 1 and `predicted` = 0, `actual`
    = 0\. Typical classification accuracy metrics don’t give “partial credit” for
    predicting 0.99 probability when the actual value is 1, so we convert our probabilities
    to 1s and 0s so we can get “full credit” where possible. We also convert our list
    of actual values (from the `exited` column) to a Python list.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们计算了`predicted`列的中位数值。然后我们创建了`prediction`，当LPM预测低于中位数概率时，`prediction`为0，当LPM预测高于中位数概率时，`prediction`为1。我们这样做是因为，在衡量分类任务的准确性时，我们将使用像`predicted`
    = 1，`actual` = 1 和 `predicted` = 0，`actual` = 0这样的精确匹配的指标。典型的分类准确性指标不会对预测0.99概率而实际值为1的情况给予“部分积分”，因此我们将概率转换为1和0，这样我们就能在可能的情况下获得“满分”。我们还将实际值列表（来自`exited`列）转换为Python列表。
- en: 'Now that our data is in the right format, we can create a *confusion matrix*,
    a standard way to measure accuracy in classification models:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据格式正确，我们可以创建一个*混淆矩阵*，这是衡量分类模型准确性的标准方法：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The confusion matrix that is output shows the number of true positives, true
    negatives, false positives, and false negatives we get when making predictions
    on our dataset. Our confusion matrix looks like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的混淆矩阵显示了我们在对数据集进行预测时，得到的真正正例、真正负例、假阳性和假阴性的数量。我们的混淆矩阵如下所示：
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Every confusion matrix has the following structure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个混淆矩阵都有以下结构：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, when we look at our confusion matrix, we find that our model made seven
    true-positive classifications: for seven corporations, our model predicted above-median
    exit probability (high attrition risk), and those seven corporations did exit.
    Our false positives are six cases in which we predicted above-median exit probability
    but the corporation didn’t exit. Our false negatives are four cases in which we
    predicted below-median exit probability but the corporation did exit. Finally,
    our true negatives are nine cases in which we predicted below-median exit probability
    for clients that didn’t exit.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们查看我们的混淆矩阵时，我们发现我们的模型做出了七个真正的正例分类：对于七家公司，我们的模型预测了高于中位数的退出概率（高流失风险），而这七家公司确实退出了。我们的假阳性是六个案例，其中我们预测了高于中位数的退出概率，但公司没有退出。我们的假阴性是四个案例，其中我们预测了低于中位数的退出概率，但公司却退出了。最后，我们的真正负例是九个案例，其中我们预测了低于中位数的退出概率，而这些客户并未退出。
- en: We’re always happy about true positives and true negatives, and we always want
    both (the values on the main diagonal of the confusion matrix) to be high. We’re
    never happy about false positives or false negatives, and we always want both
    (the values off the main diagonal) to be as low as possible.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是对真正的正例和真正的负例感到满意，并且我们总是希望这两者（混淆矩阵的主对角线上的值）尽可能高。我们从不对假阳性或假阴性感到满意，并且我们总是希望这两者（主对角线外的值）尽可能低。
- en: The confusion matrix contains all possible information about the classifications
    we’ve made and their correctness. However, data scientists can never get enough
    new ways to slice and dice and re-represent data. We can calculate a huge number
    of derived metrics from our little confusion matrix.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵包含了我们所做分类及其正确性的所有可能信息。然而，数据科学家永远不会满足于寻找新的方法来切分、分析和重新表示数据。我们可以从我们的小混淆矩阵中计算出大量的派生指标。
- en: Two of the most popular metrics we can derive are precision and recall. *Precision*
    is defined as *true positives* / (*true positives* + *false positives*). *Recall*
    is also called *sensitivity* and is defined as *true positives* / (*true positives*
    + *false negatives*). Precision is answering the question, Out of everything we
    thought was positive, how many times was it actually positive? (In our case, *positive*
    refers to attrition—out of all the times we thought a client was at high risk
    of leaving, how many times did they actually leave?) Recall is answering the slightly
    different question, Out of all the actually positive cases, how many did we think
    were positive? (In other words, out of all the clients who actually exited their
    contracts, how many did we predict were at high attrition risk?) If false positives
    are high, precision will be low. If false negatives are high, recall will be low.
    Ideally, both will be as high as possible.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以导出的两个最常见的指标是精确度和召回率。*精确度*被定义为*真正例* / (*真正例* + *假正例*)。*召回率*也叫*敏感性*，定义为*真正例*
    / (*真正例* + *假负例*)。精确度回答的问题是：我们认为是正例的所有情况中，实际上有多少次是真正的正例？（在我们的案例中，*正例*指的是流失——在我们认为客户有很高流失风险的所有情况中，实际上有多少次他们真的流失了？）召回率回答的是稍微不同的问题：在所有实际的正例中，我们认为有多少是正例？（换句话说，在所有实际流失的客户中，我们预测有多少客户是高流失风险？）如果假正例很多，精确度会低。如果假负例很多，召回率会低。理想情况下，两者应该尽可能高。
- en: 'We can calculate both precision and recall as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式计算精确度和召回率：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You’ll see that our precision is about 0.54, and our recall is about 0.64\.
    These are not extremely encouraging values. Precision and recall are always between
    0 and 1, and they’re supposed to be as close to 1 as possible. Our results are
    higher than 0, which is good news, but we have plenty of room for improvement.
    Let’s do our best to get better precision and recall by making some improvements
    in the next sections.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们的精确度大约为0.54，召回率大约为0.64。这些数值并不是非常令人鼓舞。精确度和召回率的值始终介于0和1之间，它们应该尽可能接近1。我们的结果高于0，这是好消息，但仍有很大的改进空间。让我们通过在接下来的部分中进行一些改进，尽力提高精确度和召回率。
- en: Using Multivariate LPMs
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多变量LPMs
- en: 'So far, all of our results have been simple: the clients with the lowest activity
    levels are also the clients with the highest predicted attrition probabilities.
    These models are so simple that they may hardly seem worthwhile. You may think
    that the relationship between low activity and attrition risk is both intuitive
    and visually evident in [Figure 5-2](#figure5-2), so fitting a regression to confirm
    it is superfluous. This is reasonable, although it’s wise to seek rigorous confirmation
    from a regression even in cases that seem intuitively obvious.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的结果都很简单：活动水平最低的客户也是预测流失概率最高的客户。这些模型如此简单，可能几乎看起来毫无价值。你可能会认为，低活动与流失风险之间的关系既直观又在[图
    5-2](#figure5-2)中有清晰的视觉表现，因此拟合回归模型来确认这一点显得多余。这个观点是合理的，尽管即使在那些看似直观的情况下，寻求回归模型的严格验证仍然是明智的做法。
- en: 'Regressions begin to become more useful when we have no clear intuitive relationship
    and no simple plots that can show them instantly. For example, we can use three
    predictors to predict attrition risk: last month’s activity, last year’s activity,
    and a client’s number of employees. If we wanted to plot the relationship of all
    three variables with attrition simultaneously, we would need to create a four-dimensional
    plot, which would be hard to read and think about. If we didn’t want to create
    a four-dimensional plot, we could create separate plots for the relationship between
    each individual variable and attrition. But each of these plots would show only
    one variable’s relationship with attrition, thus failing to capture the whole
    story told by the whole dataset together.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们没有明确的直觉关系，且没有能够立即显示这些关系的简单图表时，回归分析开始变得更有用。例如，我们可以使用三个预测变量来预测流失风险：上个月的活动、去年的活动以及客户的员工数量。如果我们想要同时绘制这三个变量与流失之间的关系，我们需要创建一个四维图表，这将很难读取和理解。如果我们不想创建四维图表，可以为每个单独的变量与流失之间的关系创建单独的图表。但每个图表只能显示一个变量与流失的关系，无法捕捉到整个数据集所传达的完整故事。
- en: 'Instead of trying to discover attrition risk through plotting and intuition,
    we can run a multivariate regression with the predictors we’re interested in:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过进行多变量回归分析，而不是通过绘制图表和直觉来发现流失风险，使用我们感兴趣的预测变量：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This is a multivariate linear regression, just like the multivariate linear
    regressions we introduced in Chapter 2. Since we’re running it to predict 0–1
    data, it’s a *multivariate linear probability model*. Just as we’ve done for previous
    regressions we’ve created, we can use this new multivariate regressor to predict
    probabilities for the `attrition_future` dataset:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When we run `print(attrition_future.nlargest(5,''predicted_multi''))`, we can
    see the five corporations with the highest predicted attrition risk, based on
    this new multivariate regressor. The output looks like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since we’re using three variables to predict attrition probability instead of
    one, it’s not as obvious which corporations will have the highest and lowest estimated
    attrition risk. The regression’s ability to predict for us will be helpful in
    this more complex scenario.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a list of all corporations, sorted by highest attrition risk
    to lowest risk based on this most recent regression:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You’ll see the following list of corporations:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: These are the same corporations we saw before, but they’re in a different order,
    since their attrition risk was predicted using `regressor_multi` instead of `regressor`.
    You can see that in some cases, the order is similar. For example, the `dmai`
    corporation was ranked sixth by `regressor` and ranked fourth by `regressor_multi`.
    In other cases, the order is quite different. For example, the `whsh` corporation
    was ranked first (tied with two other corporations) by `regressor`, but it’s seventeenth
    in the prediction by `regressor_multi`. The order changes because the distinct
    regressors take into account different information and find different patterns.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Creating New Metrics
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After running a regression that uses all the numeric predictors in the dataset,
    you may think that we’ve done all the regression that’s possible. But we can do
    more, because we’re not strictly limited to creating LPMs based on the columns
    of our attrition dataset in their raw form. We can also create a *derived feature*,
    or engineered feature—a feature or metric created by transforming and combining
    existing variables. The following is an example of a derived feature:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we create a new metric called `activity_per_employee`. This is simply
    the last month’s activity for the whole corporation divided by the number of employees
    at the corporation. This new derived metric could be a better predictor of attrition
    risk than the raw activity level or the raw number of employees alone.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, two companies may both have high activity levels at exactly 10,000
    each. However, if one of those companies has 10,000 employees, and the other has
    10 employees, we might have very different expectations about their attrition
    risk. The average employee at the smaller company is accessing our tool 1,000
    times per month, while the average employee at the larger company is accessing
    it only 1 time per month. Even though both companies have the same level of activity
    according to our raw measurement, the smaller company seems to have a lower likelihood
    of attrition because our tool appears to be much more important to the work of
    each of its employees, on average. We can use this new `activity_per_employee`
    metric in a regression that’s just like all the regressions we’ve done before:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This snippet contains a lot of code, but everything it does is something you’ve
    done before. First, we define the `activity_per_employee` metric, our new derived
    feature. Then, we define our `x` and `y` variables. The `x` variable will be our
    features: the four variables we’ll use to predict attrition. The `y` variable
    will be our target: the one variable we’re trying to predict. We create and fit
    a linear regression that uses `x` to predict `y`, and then we create `predicted3`,
    a new column that contains predictions of attrition risk made by this new regression.
    We create a `predicted3` column both for our past data and our present data.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did before, we can look at the predictions made by this model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Again, you’ll see that the order is different from the order given by the previous
    regressors we tried:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Just as we did before, we can check the confusion matrix for our latest model.
    First, we’ll put our predictions and actual values in the correct 0–1 format:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can calculate our latest confusion matrix:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This confusion matrix should immediately look better to you than our previous
    confusion matrix. If you need more evidence that our latest model is better, look
    at the precision and recall values for this model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You’ll see that our precision is about 0.69, and our recall is about 0.82—still
    not perfect, but big improvements on our previous, lower values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Considering the Weaknesses of LPMs
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LPMs have good points: it’s easy to interpret their values, it’s easy to estimate
    them with centuries-old methods and many useful Python modules, and they’re simple
    in a way only a straight line can be. However, LPMs also have weaknesses. One
    is that they don’t fit the points of a dataset well: they pass through the middle
    of the points and get close to only a few points.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest weakness of LPMs is apparent if you look at the right side of [Figure
    5-2](#figure5-2). There, you can see that the regression line dips below *y* =
    0\. If we try to interpret the value of the regression line at that part of the
    plot, we reach an absurd conclusion: we predict approximately a –20 percent probability
    of attrition for corporations with about 1,200 logins. There’s no reasonable way
    to interpret a negative probability; it’s just nonsense that our model has output.
    Unfortunately, this kind of nonsense is inevitable with every LPM that isn’t a
    horizontal line. Any non-horizontal regression line will make predictions that
    are below 0 percent or above 100 percent for certain values. The inevitability
    of these nonsensical predictions is the major weakness of LPMs and the reason
    you should learn alternative binary classification methods.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Binary Outcomes with Logistic Regression
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a method for binary classification that is not subject to the weaknesses
    of LPMs. If you think about [Figure 5-2](#figure5-2), you’ll realize that whatever
    method we use can’t rely on fitting straight lines to points, since any straight
    line besides a perfectly flat horizontal line will inevitably make predictions
    that are higher than 100 percent or lower than 0 percent. Any straight line will
    also be far from many of the points it’s trying to fit. If we’re going to fit
    a line to points to do binary classification, it will have to be a curve that
    doesn’t go below 0 or above 1, and that also gets close to many of the points
    (which are all at *y* = 0 or *y* = 1).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'One important curve that fits these criteria is called the *logistic curve*.
    Mathematically, the logistic curve can be described by the following function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c05/g05001.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: The logistic function is used to model populations, epidemics, chemical reactions,
    and linguistic shifts, among other things. If you look closely at the denominator
    of this function, you’ll see β[0] + β[1]· *x*. If that reminds you of the type
    of expression that we used when we were doing linear regression in Chapter 2,
    it should—it’s exactly the same expression as we find in a standard regression
    formula (one with an intercept, a slope, and an *x* variable).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Soon, we’ll go over a new type of regression using this logistic function. We’ll
    be working with many of the same elements that we’ve used before, so much of what
    we’ll do should feel familiar. We’ll use the logistic function to model attrition
    risk, and the way we’ll use it can be applied to any situation where you need
    a model of the probability of a yes/no or 0/1 answer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Drawing Logistic Curves
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can draw a simple logistic curve in Python as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can see the output of this code in [Figure 5-3](#figure5-3).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c05/f05003.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3: An example of a logistic curve'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The logistic curve has an S-like shape, so it stays close to *y* = 0 and *y*
    = 1 over most of its domain. Also, it never goes above 1 and never goes below
    0, so it resolves the weaknesses of LPMs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'If we change the coefficients in our logistic equation to be positive instead
    of negative, we reverse the direction of the logistic curve, so it’s a backward
    S instead of a standard S:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This code snippet is the same as the previous code snippet, except for the change
    of two numbers from negative to positive (shown in bold). We can see the final
    plot in [Figure 5-4](#figure5-4).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c05/f05004.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-4: Another example of a logistic curve, showing a backward S shape'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s use logistic curves with our data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Logistic Function to Our Data
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can fit a logistic curve to binary data in much the same way that we fit
    a straight line to binary data when we created our LPM. Fitting a logistic curve
    to binary data is also called performing *logistic regression*, and it’s a common,
    standard alternative to linear regression for binary classification. We can choose
    from several useful Python modules to perform logistic regression:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After we fit the model, we can access predicted probabilities for each element
    as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can then plot the results:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can see in the output plot in [Figure 5-5](#figure5-5) that we have exactly
    what we wanted: a regression that never predicts above 100 percent or below 0
    percent probability and gets very close to some of the points in our strange “cloud.”
    We’ve resolved the weaknesses of LPMs with this new method.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c05/f05005.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-5: A logistic regression predicting attrition risk'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: You may object that we introduced logistic regression as something that produces
    an S-shaped curve like the curves in Figures 5-3 and 5-4, and there’s no S-shaped
    curve in [Figure 5-5](#figure5-5). But [Figure 5-5](#figure5-5) shows only a portion
    of the full S; it’s like [Figure 5-5](#figure5-5) is zoomed in on the lower-right
    side of [Figure 5-4](#figure5-4), so we see only the right side of the backward
    S. If we zoomed out the plot and considered hypothetical activity levels that
    were negative, we would see a fuller backward S, including predicted attrition
    probabilities close to 1\. Since negative activity levels are impossible, we see
    only a portion of the full S that the logistic equation specifies.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we did with other regressions, we can look at the predictions our logistic
    regression makes. In particular, we can predict the probabilities of attrition
    for every company in our `attrition2` dataset and print them out in order from
    highest to lowest attrition risk:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can see that the output consists of every corporation in `attrition2`, sorted
    in order of highest to lowest predicted attrition probability based on the results
    of our logistic regression:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can look at these results and compare them to the predictions from our other
    regressions. Taking into account different information and using different functions
    to model the data can lead to different results each time we perform regression.
    In this case, since our logistic regression used the same predictor (last month’s
    activity) as our first LPM, it ranks corporations from highest to lowest risk
    in the same order.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Binary Classification
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regressions and LPMs are commonly used to predict binary outcomes.
    We can use them not only for attrition prediction but also for predicting whether
    a stock will go up, whether an applicant will be successful in a job, whether
    a project will be profitable, whether a team will win a game, or any other binary
    classification that can be expressed in a true/false, 0/1 framework.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The LPMs and logistic regressions you learned about in this chapter are statistical
    tools that can tell us the probability of attrition. But knowing the probability
    of attrition does not fully solve the business problem that attrition represents.
    A business leader needs to communicate these attrition predictions and make sure
    that client managers act on them effectively. A host of business considerations
    could alter the strategy a leader implements to manage an attrition problem. For
    example, attrition probability is not the only thing that could determine the
    priority assigned to a client. That priority will also depend on the relative
    importance of the client, probably including the revenue the company expects to
    gain from the client, the size of the client, and other strategic considerations.
    Data science is always part of a larger business process, every step of which
    is difficult and important.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'LPMs and logistic regressions have one important thing in common: they’re *monotonic*:
    they express a trend that moves in only one direction. In Figures 5-1, 5-2 and
    5-5, less activity is always associated with higher attrition risk, and vice versa.
    However, imagine a more complex situation, in which low activity is especially
    associated with high attrition risk, medium activity is associated with low attrition
    risk, and high activity again is associated with high attrition risk. A monotonic
    function like the ones examined in this chapter wouldn’t be able to capture this
    pattern, and we would have to turn to more complex models. The next chapter describes
    methods for machine learning—including methods to capture non-monotonic trends
    in complex, multivariate data—to make predictions and perform classifications
    even more accurately.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we discussed binary classification. We started with a simple
    business scenario and showed how linear regression can enable us to predict probabilities
    that help solve a business problem. We considered the weaknesses of those linear
    probability models and introduced logistic regression as a more complex model
    that overcomes those weaknesses. Binary classification may seem like an unimportant
    topic, but we can use it for analyzing risk, predicting the future, and making
    difficult yes/no decisions. In our discussion of machine learning in the next
    chapter, we’ll discuss prediction and classification methods that go beyond regressions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
