<html><head></head><body>
<h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_161"/><strong><span class="big">9</span><br/>WRITING HAIKU WITH MARKOV CHAIN ANALYSIS</strong></h2>
<div class="image1"><img src="../images/common01.jpg" alt="image"/></div>
<p class="noindent">Computers can write poetry by rearranging existing poems. This is basically what humans do. You and I didn’t invent the language we speak—we learned it. To talk or write, we just recombine existing words—and rarely in a truly original manner. As Sting once said about writing music, “I don’t think there’s such a thing as composition in pop music. I think what we do in pop music is collate . . . I’m a good collator.”</p>
<p class="indent">In this chapter, you’re going to write a program that puts the “best words in the best order” in the form of haiku. But to do this, Python needs good examples, so you’ll need to provide a training corpus of haiku by the Japanese masters.</p>
<p class="indent">To rearrange these words in a meaningful manner, you will use <em>Markov chains</em>, named after Russian mathematician Andrey Markov. <em>Markov chain analysis</em>, an important part of probability theory, is a process that attempts <span epub:type="pagebreak" id="page_162"/>to predict the subsequent state based on the properties of the current state. Modern-day applications include speech and handwriting recognition, computer performance evaluation, spam filtering, and Google’s PageRank algorithm for searching the web.</p>
<p class="indent">With Markov chain analysis, a training corpus, and the syllable-counting program from <a href="ch08.xhtml#ch08">Chapter 8</a>, you’ll be able to produce new haiku that follow the syllabic rules of the genre and stay “on subject” to a large degree. You’ll also learn how to use Python’s <code>logging</code> module to help monitor the behavior of your program with easy on-and-off feedback. And in “<a href="ch09.xhtml#lev215">Challenge Projects</a>” on <a href="ch09.xhtml#page_184">page 184</a>, you can enlist your friends on social media to see if they can distinguish your simulated haiku from the real thing.</p>
<h3 class="h3a" id="lev192"><strong>Project #16: Markov Chain Analysis</strong></h3>
<p class="noindent">Like the genetic algorithms in <a href="ch07.xhtml#ch07">Chapter 7</a>, Markov chain analysis sounds impressive but is easy to implement. You do it every day. If you hear some one say, “Elementary, my dear . . . ,” you automatically think, “Watson.” Every time your brain has heard this phrase, it has taken a sample. Based on the number of samples, it can predict the answer. On the other hand, if you heard someone say, “I want to go to . . . ,” you might think “the bathroom” or “the movies” but probably not “Houma, Louisiana.” There are many possible solutions, but some are more likely than others.</p>
<p class="indent">Back in the 1940s, Claude Shannon pioneered the use of Markov chains to statistically model the sequences of letters in a body of text. For example, for every occurrence of the digram <em>th</em> in an English-language book, the next most likely letter is <em>e</em>.</p>
<p class="indent">But you don’t just want to know what the most likely letter is; you want to know the actual probability of getting that letter, as well as the odds of getting every other letter, which is a problem tailor-made for a computer. To solve this problem, you need to map each two-letter digram in a piece of text to the letter that immediately follows it. This is a classic dictionary application, with the digrams as the keys and the letters as the values.</p>
<p class="indent">When applied to letters in words, a <em>Markov model</em> is a mathematical model that calculates a letter’s probability of occurrence based on the previous <em>k</em> consecutive letters, where <em>k</em> is an integer. A <em>model of order 2</em> means that the probability of a letter occurring depends on the two letters that precede it. A <em>model of order 0</em> means that each letter is independent. And this same logic applies to words. Consider these two haiku examples:</p>
<table class="topbot-d">
<tbody>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">A break in the clouds</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">Glorious the moon</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">The moon a bright mountaintop</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">Therefore our thanks dark clouds come</p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">Distant and aloof</p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba">To rest our tired necks</p>
</td>
</tr>
</tbody>
</table>
<p class="indent">A Python dictionary that maps each haiku word to each subsequent word looks like this:</p>
<pre>             'a': ['break', 'bright'],<br/>             'aloof': ['glorious'],<br/><span epub:type="pagebreak" id="page_163"/>             'and': ['aloof'],<br/>             'break': ['in'],<br/>             'bright': ['mountaintop'],<br/>             'clouds': ['the', 'come'],<br/>             'come': ['to'],<br/>             'dark': ['clouds'],<br/>             'distant': ['and'],<br/>             'glorious': ['the'],<br/>             'in': ['the'],<br/>             'moon': ['a', 'therefore'],<br/>             'mountaintop': ['distant'],<br/>             'our': ['thanks', 'tired'],<br/>             'rest': ['our'],<br/>             'thanks': ['dark'],<br/>             'the': ['clouds', 'moon', 'moon'],<br/>             'therefore': ['our'],<br/>             'tired': ['necks'],<br/>             'to': ['rest']</pre>
<p class="indent">Since there are only two haiku, most of the dictionary keys have only one value. But look at <em>the</em> near the bottom of the list: <em>moon</em> occurs twice. This is because the Markov model stores every occurrence of a word as a separate, duplicate value. So, for the key <em>the</em>, if you choose a value at random, the odds of selecting <em>moon</em> versus <em>clouds</em> are 2:1. Conversely, the model will automatically screen out extremely rare or impossible combinations. For example, many words can potentially follow <em>the</em>, but not another <em>the</em>!</p>
<p class="indent">The following dictionary maps every <em>pair</em> of words to the word immediately after; that means it’s a model of order 2.</p>
<pre>             'a break': ['in'],<br/>             'a bright': ['mountaintop'],<br/>             'aloof glorious': ['the'],<br/>             'and aloof': ['glorious'],<br/>             'break in': ['the'],<br/>             'bright mountaintop': ['distant'],<br/>             'clouds come': ['to'],<br/>             'clouds the': ['moon'],<br/>             'come to': ['rest'],<br/>             'dark clouds': ['come'],<br/>             'distant and': ['aloof'],<br/>             'glorious the': ['moon'],<br/>             'in the': ['clouds'],<br/>             'moon a': ['bright'],<br/>             'moon therefore': ['our'],<br/>             'mountaintop distant': ['and'],<br/>             'our thanks': ['dark'],<br/>             'our tired': ['necks'],<br/>             'rest our': ['tired'],<br/>             'thanks dark': ['clouds'],<br/>             'the clouds': ['the'],<br/>             'the moon': ['a', 'therefore'],<br/>             'therefore our': ['thanks'],<br/>             'to rest': ['our']</pre>
<p class="indent"><span epub:type="pagebreak" id="page_164"/>Note that the mapping continues from the first haiku to the second so the dictionary contains the items <code>'and aloof': ['glorious']</code> and <code>'aloof glorious': ['the']</code>. This behavior means your program can jump from one haiku to another and is not restricted to just the word pairs within a single haiku. It is free to form new word pairs that the masters may never have conceived.</p>
<p class="indent">Because of the very short training corpus, <em>the moon</em> is the only word pair with multiple values. For all the others, you are “locked in” to a single outcome. In this example, the size of the training corpus greatly determines the number of values per key, but with a larger corpus, the value of <em>k</em> in the Markov model will have a larger influence.</p>
<p class="indent">The size of <em>k</em> determines whether you produce poppycock, commit plagiarism, or produce a perspicuous piece of originality. If <em>k</em> equals 0, then you’ll be choosing words at random based on that word’s overall frequency in the corpus, and you’ll likely produce a lot of gibberish. If <em>k</em> is large, the results will be tightly constrained, and you’ll begin to reproduce the training text verbatim. So small values of <em>k</em> promote creativity, and large values promote duplication. The challenge is finding the proper balance between the two.</p>
<p class="indent">To illustrate, if you use a Markov model of order 3 on the previous haiku, all the resulting keys will have one value. The two values associated with the word pair <em>the moon</em> are lost because the former word pair becomes two keys, each with a unique value:</p>
<pre>             'the moon a': ['bright'],<br/>             'the moon therefore': ['our']</pre>
<p class="indent">Since haiku are short—only 17 syllables long—and available training corpora are relatively small, using a <em>k</em> of 2 should be sufficient to enforce <em>some</em> order while still allowing for creative word substitutions in your program.</p>
<div class="sidebar">
<p class="sidebart"><strong>THE OBJECTIVE</strong></p>
<p class="spara">Write a program that generates haiku using Markov chain analysis. Allow the user to modify the haiku by independently regenerating lines two and three.</p>
</div>
<h3 class="h3" id="lev193"><strong>The Strategy</strong></h3>
<p class="noindent">Your general strategy for simulating haiku will be to build Markov models of orders 1 and 2 with a training corpus of haiku written by humans. You’ll then use those models and the <em>count_syllables.py</em> program from <a href="ch08.xhtml#ch08">Chapter 8</a> to generate novel haiku that meet the required syllabic structure of 5-7-5 for the three lines of the haiku.</p>
<p class="indent"><span epub:type="pagebreak" id="page_165"/>The program should build the haiku one word at a time, initiating (or <em>seeding</em>) the haiku with a random word drawn from the corpus; selecting the haiku’s second word using a Markov model of order 1; and then selecting each subsequent word with the order 2 model.</p>
<p class="indent">Each word is derived from a <em>prefix</em>—a word or word pair that determines which word will be picked to go in the haiku; the key in the word-mapping dictionaries represents the prefix. As a consequence, the word that the prefix determines is the <em>suffix</em>.</p>
<h4 class="h4" id="lev194"><strong><em>Choosing and Discarding Words</em></strong></h4>
<p class="noindent">When the program selects a word, it first counts the word’s syllables, and if the word doesn’t fit, it chooses a new word. If there are no possible words based on the prefix in the poem, then the program resorts to what I call a <em>ghost prefix</em>, which is a prefix that doesn’t occur in the haiku. For example, if a word pair in a haiku is <em>temple gong</em>, and all the words that follow in the Markov model have too many syllables to complete the line, the program selects a new word pair at random and uses it to pick the next word in the haiku. The new word-pair prefix <em>should not be included in the line</em>—that is, <em>temple gong</em> will not be replaced. Although you could choose a suitable new word in a number of ways, I prefer this technique because it allows you to simplify by maintaining a consistent process throughout the program.</p>
<p class="indent">You can accomplish these steps with the functions in <a href="ch09.xhtml#ch09fig1">Figures 9-1</a> and <a href="ch09.xhtml#ch09fig2">9-2</a>. Assuming you’re working on a five-syllable line, <a href="ch09.xhtml#ch09fig1">Figure 9-1</a> is an example of what will happen, at a high level, if all the chosen words match the syllable target.</p>
<div class="image"><a id="ch09fig1"/><img src="../images/f0165-01.jpg" alt="image"/></div>
<p class="figcap"><em>Figure 9-1: High-level graphical pseudocode for a five-syllable haiku line</em></p>
<p class="indent"><span epub:type="pagebreak" id="page_166"/>The program randomly selects the seed word <em>the</em> from the corpus, and then counts its syllables. Next, it chooses <em>bright</em> from the model of order 1, based on the prefix <em>the</em>. Then it counts the number of syllables in <em>bright</em> and adds that number to the number of syllables in the line. Since the sum of syllables doesn’t exceed five, the program adds <em>bright</em> to the line, moves on to select <em>autumn</em> from the model of order 2 based on the prefix <em>The bright</em>, and then repeats the syllable-counting process. Finally, the program selects <em>moon</em> based on the prefix <em>bright autumn</em>, counts the syllables, and—since the line’s total number of syllables is equal to five—adds moon to the line, completing it.</p>
<p class="indent"><a href="ch09.xhtml#ch09fig2">Figure 9-2</a> demonstrates a case where the program needs to utilize a ghost prefix to successfully complete a five-syllable line.</p>
<div class="image"><a id="ch09fig2"/><img src="../images/f0166-01.jpg" alt="image"/></div>
<p class="figcap"><em>Figure 9-2: Choosing a new suffix with a randomly selected ghost prefix (</em>full white<em>)</em></p>
<p class="indent">Let’s assume that the only word that follows the prefix <em>temple gong</em> in the Markov model is <em>glorious</em>. This word has too many syllables for the line, so the program selects a ghost prefix, <em>full white</em>, at random. The word <em>moon</em> follows the ghost prefix and satisfies the remaining syllable count in the line, so the program adds it to the line. The program then discards the <em>full white</em> prefix, and the line is complete. With this ghost prefix technique, you can’t guarantee that the new suffix will make sense contextually, but at the same time, this is one way to incorporate creativity into the process.</p>
<h4 class="h4" id="lev195"><span epub:type="pagebreak" id="page_167"/><strong><em>Continuing from One Line to Another</em></strong></h4>
<p class="noindentb">The Markov model is the “special sauce” that allows you to imbue the haiku with context and meaning that continue from one line to another. The Japanese masters <em>generally</em> wrote haiku in which each line is a stand-alone phrase but the contextual thread continues across lines, as in this haiku from Bon Cho:</p>
<p class="poem">In silent midnight</p>
<p class="poem">Our old scarecrow topples down</p>
<p class="poem">Weird hollow echo</p>
<p class="poemr"><em>—Bon Cho</em></p>
<p class="indentb">Even though the masters preferred that each line of a haiku represent a complete thought, they didn’t strictly follow the rule. Here’s an example in Buson’s haiku:</p>
<p class="poem">My two plum trees are</p>
<p class="poem">So gracious see, they flower</p>
<p class="poem">One now, one later</p>
<p class="poemr"><em>—Buson</em></p>
<p class="indent">The first line of Buson’s haiku is not grammatical on its own, so the reader must continue to the next line without a break. When a phrase in poetry moves from one line to the next without a pause or syntactic break, it is said to be <em>enjambed</em>. According to Charles Hartman, author of <em>Virtual Muse</em>, enjambment is what gives metrical lines much of their supple liveliness. That’s a good thing, since it’s very hard to get an algorithm to write a coherent poem without some grammatical spillover from line to line. To get your program to continue a “thought” through multiple lines, you need to use the word pair from the end of the previous line as the starting prefix for the current line.</p>
<p class="indent">Finally, you should give the user the opportunity to not only build the poem but also to edit it interactively by regenerating lines two and three. Most of writing is rewriting, and it would be unconscionable to leave the user hanging with two perfect lines and no way to throw the dice again on an uncooperative line.</p>
<h3 class="h3" id="lev196"><strong>The Pseudocode</strong></h3>
<p class="noindent">If you follow the strategy I’ve just laid out, your high-level pseudocode should look like this:</p>
<pre>Import count_syllables module<br/>Load a training-corpus text file<br/>Process the training corpus for spaces, newline breaks, and so on<br/>Map each word in corpus to the word after (Markov model order 1)<br/>Map each word pair in corpus to the word after (Markov model order 2)<br/>Give user choice of generating full haiku, redoing lines 2 or 3, or exiting<br/><span epub:type="pagebreak" id="page_168"/>If first line:<br/>    Target syllables = 5<br/>    Get random word from corpus &lt;= 4 syllables (no 1-word lines)<br/>    Add word to line<br/>    Set random word = prefix variable<br/>    Get mapped words after prefix<br/>    If mapped words have too many syllables<br/>        Choose new prefix word at random &amp; repeat<br/>    Choose new word at random from mapped words<br/>    Add the new word to the line<br/>    Count syllables in word and calculate total in line<br/>    If syllables in line equal target syllables<br/>        Return line and last word pair in line<br/>Else if second or third line:<br/>    Target = 7 or 5<br/>    Line equals last word pair in previous line<br/>    While syllable target not reached:<br/>        Prefix = last word pair in line<br/>        Get mapped words after word-pair prefix<br/>        If mapped words have too many syllables<br/>            Choose new word-pair prefix at random and repeat<br/>        Choose new word at random from mapped words<br/>        Add the new word to the line<br/>        Count syllables in word and calculate total in line<br/>        If total is greater than target<br/>            Discard word, reset total, and repeat<br/>        Else if total is less than target<br/>            Add word to line, keep total, and repeat<br/>        Else if total is equal to target<br/>            Add word to line<br/>    Return line and last word pair in line<br/>Display results and choice menu</pre>
<h3 class="h3" id="lev197"><strong>The Training Corpus</strong></h3>
<p class="noindent">Markov models are built from a corpus, so they are unique to that corpus. A model built from the complete works of Edgar Rice Burroughs will be different and distinguishable from one built from the works of Anne Rice. We all have a signature style, or <em>voice</em>, and given a large enough sample, a Markov approach can produce a statistical model of your style. Like fingerprints, this model can link you to a document or manuscript.</p>
<p class="indent">To build the Markov models, the corpus you’ll use is a text file consisting of almost 300 ancient and modern haiku, more than 200 of which were written by the masters. Ideally, your training corpus would consist of thousands of haiku, all by the same author (for a consistent voice), but these are difficult to find, particularly since many of the old Japanese haiku don’t obey the syllabic rules, either intentionally or as a result of translation into English.</p>
<p class="indent"><span epub:type="pagebreak" id="page_169"/>To increase the number of values per key in the Markov model, the haiku in the initial corpus were duplicated 18 times and randomly distributed throughout the file. This has no impact on word associations <em>within</em> haiku but increases the interactions <em>between</em> haiku.</p>
<p class="indent">To illustrate, assume the word pair at the end of the following haiku is unique, mapping only to the starting word of the second haiku; this results in the fairly useless key/value pair of <code>'hollow frog': ['mirror-pond']</code>:</p>
<p class="poemt"><span class="gray">Open mouth reveals</span></p>
<p class="poem"><span class="gray">Your whole wet interior</span></p>
<p class="poem"><span class="gray">Silly</span> <strong>hollow frog</strong><span class="gray">!</span></p>
<p class="poemt"><strong>Mirror-pond</strong> <span class="gray">of stars</span></p>
<p class="poem"><span class="gray">Suddenly a summer shower</span></p>
<p class="poem"><span class="gray">Dimples the water</span></p>
<p class="indentt">If you duplicate and shuffle the haiku, you may introduce a preposition into the mix, greatly increasing the odds of linking the odd <em>hollow frog</em> to something sensible:</p>
<p class="poemt"><span class="gray">Open mouth reveals</span></p>
<p class="poem"><span class="gray">Your whole wet interior</span></p>
<p class="poem"><span class="gray">Silly</span> <strong>hollow frog</strong><span class="gray">!</span></p>
<p class="poemt"><strong>In</strong> <span class="gray">the city fields</span></p>
<p class="poem"><span class="gray">Contemplating cherry trees</span></p>
<p class="poem"><span class="gray">Strangers are like friends</span></p>
<p class="indentt">The Markov model now assigns two values to <code>'hollow frog'</code>: <code>'mirror-pond'</code> and <code>'in'</code>. And each time you duplicate the haiku, you’ll see an increase in the number of values per key for haiku-ending words or word pairs. But this is helpful only to a point; after a while, diminishing returns set in, and you start adding the same values over and over again, gaining nothing.</p>
<h3 class="h3" id="lev198"><strong>Debugging</strong></h3>
<p class="noindent">Debugging is the process of finding and fixing errors (bugs) in computer hardware and software. When you’re trying to code a solution to a complex problem, you need to keep a tight rein on your program in order to find the source of an issue when something unexpected arises. For example, if you end up with seven syllables rather than five in the first line of your haiku, you’ll want to know if the syllable-counting function failed, if there was a problem mapping words to words, or if the program thought it was on line two. To find out what went wrong, you need to monitor what your program is returning at each key step, and this calls for either <em>scaffolding</em> or <em>logging</em>. I’ll discuss each of these techniques in the following two sections.</p>
<h4 class="h4" id="lev199"><span epub:type="pagebreak" id="page_170"/><strong><em>Building the Scaffolding</em></strong></h4>
<p class="noindent"><em>Scaffolding</em>, as defined here, is temporary code you write to help develop your programs, then delete when you’re done. The name is an allusion to the scaffolding used in construction—necessary, but nobody wants it around forever.</p>
<p class="indent">One common piece of scaffolding is a <code>print()</code> statement that checks what a function or calculation returns. The user doesn’t need to see the output, so you delete it after you confirm the program is working properly.</p>
<p class="indent">Useful scaffolding output includes things like the type of a value or variable, the length of a dataset, and the results of incremental calculations. To quote Allen Downey in <em>Think Python</em>, “Time you spend building scaffolding can reduce the time you spend debugging.”</p>
<p class="indent">The downside to using <code>print()</code> statements for debugging is that you have to go back and delete (or comment out) all these statements later, and you risk accidentally removing a <code>print()</code> statement that’s useful to the end user. Fortunately, there’s an alternative to scaffolding that lets you avoid these problems. It’s called the <code>logging</code> module.</p>
<h4 class="h4" id="lev200"><strong><em>Using the logging Module</em></strong></h4>
<p class="noindent">The <code>logging</code> module is part of the Python Standard Library (<em><a href="https://docs.python.org/3/library/logging.html">https://docs.python.org/3/library/logging.html</a></em>). With <code>logging</code>, you can get a customized report on what your program is doing at any location you choose. You can even write the reports to a permanent logfile. The following interactive shell example uses <code>logging</code> to check that a vowel-counting program is working correctly:</p>
<pre><span class="ent">➊</span> &gt;&gt;&gt; <span class="codestrong1">import logging</span><br/><span class="ent">➋</span> &gt;&gt;&gt; <span class="codestrong1">logging.basicConfig(level=logging.DEBUG,</span><br/><span class="codestrong1">                           format='%(levelname)s - %(message)s')</span><br/><br/>   &gt;&gt;&gt; <span class="codestrong1">word = 'scarecrow'</span><br/>   &gt;&gt;&gt; <span class="codestrong1">VOWELS = 'aeiouy'</span><br/>   &gt;&gt;&gt; <span class="codestrong1">num_vowels = 0</span><br/>   &gt;&gt;&gt; <span class="codestrong1">for letter in word:</span><br/>           <span class="codestrong1">if letter in VOWELS:</span><br/>               <span class="codestrong1">num_vowels += 1</span><br/>        <span class="ent">➌</span> <span class="codestrong1">logging.debug('letter &amp; count = %s-%s', letter, num_vowels)</span><br/><br/>   DEBUG - letter &amp; count = s-0<br/>   DEBUG - letter &amp; count = c-0<br/>   DEBUG - letter &amp; count = a-1<br/>   DEBUG - letter &amp; count = r-1<br/>   DEBUG - letter &amp; count = e-2<br/>   DEBUG - letter &amp; count = c-2<br/>   DEBUG - letter &amp; count = r-2<br/>   DEBUG - letter &amp; count = o-3<br/>   DEBUG - letter &amp; count = w-3</pre>
<p class="indent">To use the <code>logging</code> module, first import it <span class="ent">➊</span>. Then set up what debugging information you want to see and in what format <span class="ent">➋</span>. The <code>DEBUG</code> level is the lowest level of information and is used for diagnosing the details. <span epub:type="pagebreak" id="page_171"/>Note that the output uses string formatting with <code>%s</code>. You can include more information—for example, the date and time are shown using <code>format='%(asctime)s'</code>—but for this snippet of code, all you really need to check is whether the program is counting vowels correctly.</p>
<p class="indent">For each letter evaluated, enter the custom text message to display along with the variable values. Note that you have to convert nonstring objects, such as integers and lists, to strings <span class="ent">➌</span>. The <code>logging</code> output follows. You can see the cumulative count, along with which letters actually change the count.</p>
<p class="indent">Like scaffolding, <code>logging</code> is for the developer, not the user. And like the <code>print()</code> function, <code>logging</code> can slow down your program. To disable the <code>logging</code> messages, simply insert the <code>logging.disable(logging.CRITICAL)</code> call after you import the module, as follows:</p>
<pre>&gt;&gt;&gt; <span class="codestrong1">import logging</span><br/>&gt;&gt;&gt; <span class="codestrong1">logging.disable(logging.CRITICAL)</span></pre>
<p class="indent">Placing the disable call near the top of the program will allow you to find it easily and toggle messages on and off. The <code>logging.disable()</code> function will suppress all messages at the designated level or lower. Since <code>CRITICAL</code> is the highest level, passing it to the <code>logging.disable()</code> function turns all messages off. This is a far better solution than manually finding and commenting out <code>print()</code> statements!</p>
<h3 class="h3" id="lev201"><strong>The Code</strong></h3>
<p class="noindent">The <em>markov_haiku.py</em> code in this section will take a training corpus called <em>train.txt</em>, prepare Markov models as dictionaries, and generate a haiku one word at a time. The <em>count_syllables.py</em> program and <em>missing_words.json</em> file from <a href="ch08.xhtml#ch08">Chapter 8</a> will ensure <em>markov_haiku.py</em> uses the correct number of syllables for each line. You can download all these files from <em><a href="https://www.nostarch.com/impracticalpython/">https://www.nostarch.com/impracticalpython/</a></em> (<a href="ch09.xhtml#ch09">Chapter 9</a> folder). Be sure to keep them together in the same directory.</p>
<h4 class="h4" id="lev202"><strong><em>Setting Up</em></strong></h4>
<p class="noindent"><a href="ch09.xhtml#ch09list1">Listing 9-1</a> imports the necessary modules, then loads and prepares external files.</p>
<p class="margin"><em>markov_haiku.py,</em> part 1</p>
<pre><span class="ent">➊</span> import sys<br/>   import logging<br/>   import random<br/>   from collections import defaultdict<br/>   from count_syllables import count_syllables<br/><br/><span class="ent">➋</span> logging.disable(logging.CRITICAL)  # comment out to enable debugging messages<br/>   logging.basicConfig(level=logging.DEBUG, format='%(message)s')<br/><br/><span class="ent">➌</span> def load_training_file(file):<br/>       """Return text file as a string."""<br/>       with open(file) as f:<br/><span epub:type="pagebreak" id="page_172"/>        <span class="ent">➍</span> raw_haiku = f.read()<br/>           return raw_haiku<br/><br/><span class="ent">➎</span> def prep_training(raw_haiku):<br/>       """Load string, remove newline, split words on spaces, and return list."""<br/>       corpus = raw_haiku.replace('\n', ' ').split()<br/>       return corpus</pre>
<p class="listing" id="ch09list1"><em>Listing 9-1: Imports, loads, and prepares the training corpus</em></p>
<p class="indent">Start with the imports listed on separate lines <span class="ent">➊</span>. You’ll need <code>logging</code> in order to receive debugging messages, and <code>defaultdict</code> will help you build a dictionary from a list by creating a new key automatically, rather than throwing an error. You also import the <code>count_syllables</code> function from the <em>count_syllables.py</em> program you wrote in <a href="ch08.xhtml#ch08">Chapter 8</a>. You should be familiar with the rest of these imports.</p>
<p class="indent">Put the statement to disable <code>logging</code> right after the imports so you can find it easily. To see logging messages, you need to comment out this statement <span class="ent">➋</span>. The following statement configures what you will see, as described in the previous section. I chose to omit the level designation from the display.</p>
<p class="indent">Next, define a function to load the training-corpus text file <span class="ent">➌</span>. Use the built-in <code>read()</code> function to read the data as a string that the program can prepare before converting it to a list <span class="ent">➍</span>. Return this string for use in the next function.</p>
<p class="indent">The <code>prep_training()</code> function <span class="ent">➎</span> takes the output from the <code>load_training_file()</code> function as an argument. It then replaces newline characters with spaces and splits the words into list items based on spaces. Finally, the function returns the corpus as a list.</p>
<h4 class="h4" id="lev203"><strong><em>Building Markov Models</em></strong></h4>
<p class="noindent">The Markov models are simply Python dictionaries that use a word or word pair as a key and the word that immediately follows them as a value. The statistical frequency of trailing words is captured by repetition of the trailing word in the list of values—similar to sets, dictionaries can’t have duplicate <em>keys</em>, but they can have duplicate <em>values</em>.</p>
<p class="indent"><a href="ch09.xhtml#ch09list2">Listing 9-2</a> defines two functions. Both functions take the corpus as an argument and return a Markov model.</p>
<p class="margin"><em>markov_haiku.py,</em> part 2</p>
<pre><span class="ent">➊</span> def map_word_to_word(corpus):<br/>       """Load list &amp; use dictionary to map word to word that follows."""<br/>    <span class="ent">➋</span> limit = len(corpus) - 1<br/>    <span class="ent">➌</span> dict1_to_1 = defaultdict(list)<br/>    <span class="ent">➍</span> for index, word in enumerate(corpus):<br/>           if index &lt; limit:<br/>            <span class="ent">➎</span> suffix = corpus[index + 1]<br/>               dict1_to_1[word].append(suffix)<br/>    <span class="ent">➏</span> logging.debug("map_word_to_word results for \"sake\" = %s\n",<br/>                    dict1_to_1['sake'])<br/>    <span class="ent">➐</span> return dict1_to_1<br/><span epub:type="pagebreak" id="page_173"/><span class="ent">➑</span> def map_2_words_to_word(corpus):<br/>       """Load list &amp; use dictionary to map word-pair to trailing word."""<br/>    <span class="ent">➒</span> limit = len(corpus) - 2<br/>       dict2_to_1 = defaultdict(list)<br/>       for index, word in enumerate(corpus):<br/>           if index &lt; limit:<br/>            <span class="ent">➓</span> key = word + ' ' + corpus[index + 1]<br/>               suffix = corpus[index + 2]<br/>               dict2_to_1[key].append(suffix)<br/>       logging.debug("map_2_words_to_word results for \"sake jug\" = %s\n",<br/>                     dict2_to_1['sake jug'])<br/>       return dict2_to_1</pre>
<p class="listing" id="ch09list2"><em>Listing 9-2: Defines functions that build Markov models of order 1 and 2</em></p>
<p class="indent">First, define a function to map every single word to its trailing word <span class="ent">➊</span>. The program will use this function only to select the haiku’s second word from the seed word. Its only parameter is the corpus list that the <code>prep_training()</code> function returns.</p>
<p class="indent">Set a limit so you can’t pick the last word in the corpus <span class="ent">➋</span>, because doing so would result in an index error. Now initialize a dictionary using <code>defaultdict</code> <span class="ent">➌</span>. You want the dictionary values to be lists that hold all the suffixes you find, so use <code>list</code> as the argument.</p>
<p class="indent">Start looping through every word in the corpus, using <code>enumerate</code> to turn each word’s index into an object <span class="ent">➍</span>. Use a conditional and the <code>limit</code> variable to prevent choosing the last word as a key. Assign a variable named <code>suffix</code> that will represent the trailing word <span class="ent">➎</span>. The value will be the index location of the current word plus 1—the next word in the list. Append this variable to the dictionary as a value of the current word.</p>
<p class="indent">To check that everything is working as planned, use <code>logging</code> to show the results <em>for a single key</em> <span class="ent">➏</span>. There are thousands of words in the corpus, so you’re not going to want to print them all. Choose a word that you know is in the corpus, like <em>sake</em>. Note that you are using the old string formatting with <code>%</code>, as it fits the current design of the logger. Finish by returning the dictionary <span class="ent">➐</span>.</p>
<p class="indent">The next function, <code>map_2_words_to_word()</code>, is basically the same function, except it uses two consecutive words as the key and maps to trailing single words <span class="ent">➑</span>. Important changes are to set the limit two words back from the end of the corpus <span class="ent">➒</span>, make the key consist of two words with a space between them <span class="ent">➓</span>, and add 2 to the index for the <code>suffix</code>.</p>
<h4 class="h4" id="lev204"><strong><em>Choosing a Random Word</em></strong></h4>
<p class="noindent">The program won’t be able to utilize a Markov model without a key, so either the user or the program must supply the first word in a simulated haiku. <a href="ch09.xhtml#ch09list3">Listing 9-3</a> defines a function that picks a first word at random, facilitating automated seeding.</p>
<p class="margin"><em>markov_haiku.py,</em> part 3</p>
<pre><span class="ent">➊</span> def random_word(corpus):<br/>       """Return random word and syllable count from training corpus."""<br/>    <span class="ent">➋</span> word = random.choice(corpus)<br/>    <span class="ent">➌</span> num_syls = count_syllables(word)<br/><span epub:type="pagebreak" id="page_174"/>    <span class="ent">➍</span> if num_syls &gt; 4:<br/>           random_word(corpus)<br/>       else:<br/>        <span class="ent">➎</span> logging.debug("random word &amp; syllables = %s %s\n", word, num_syls)<br/>           return (word, num_syls)</pre>
<p class="listing" id="ch09list3"><em>Listing 9-3: Randomly chooses a seed word to initiate the haiku</em></p>
<p class="indent">Define the function and pass it the <code>corpus</code> list <span class="ent">➊</span>. Then assign a <code>word</code> variable and use the <code>random</code> <code>choice()</code> method to pick a word from the corpus <span class="ent">➋</span>.</p>
<p class="indent">Use the <code>count_syllables()</code> function from the <code>count_syllables</code> module to count the syllables in the word; store the count in the <code>num_syls</code> variable <span class="ent">➌</span>. I’m not a fan of single-word lines in haiku, so don’t allow the function to choose a word with more than four syllables (recall that the shortest haiku lines have five syllables). If this occurs, call the <code>random_word()</code> function recursively until you get an acceptable word <span class="ent">➍</span>. Note that Python has a default maximum recursion depth of 1,000, but as long as you’re using a proper haiku-training corpus, there’s little chance you’ll exceed that prior to finding a suitable word. If that were not the case, you could address this condition later by calling the function using a <code>while</code> loop.</p>
<p class="indent">If the word has fewer than five syllables, use <code>logging</code> to display the word and its syllable count <span class="ent">➎</span>; then return the word and syllable count as a tuple.</p>
<h4 class="h4" id="lev205"><strong><em>Applying the Markov Models</em></strong></h4>
<p class="noindent">To choose the single word that follows the seed word, use the Markov model of order 1. After that, the program should select all subsequent words using the order 2 model, which uses word pairs as keys. <a href="ch09.xhtml#ch09list4">Listing 9-4</a> defines a separate function for each of these actions.</p>
<p class="margin"><em>markov_haiku.py,</em> part 4</p>
<pre><span class="ent">➊</span> def word_after_single(prefix, suffix_map_1, current_syls, target_syls):<br/>       """Return all acceptable words in a corpus that follow a single word."""<br/>    <span class="ent">➋</span> accepted_words = []<br/>    <span class="ent">➌</span> suffixes = suffix_map_1.get(prefix)<br/>    <span class="ent">➍</span> if suffixes != None:<br/>        <span class="ent">➎</span> for candidate in suffixes:<br/>               num_syls = count_syllables(candidate)<br/>               if current_syls + num_syls &lt;= target_syls:<br/>                <span class="ent">➏</span> accepted_words.append(candidate)<br/>    <span class="ent">➐</span> logging.debug("accepted words after \"%s\" = %s\n",<br/>                     prefix, set(accepted_words))<br/>       return accepted_words<br/><br/><span class="ent">➑</span> def word_after_double(prefix, suffix_map_2, current_syls, target_syls):<br/>       """Return all acceptable words in a corpus that follow a word pair."""<br/>       accepted_words = []<br/>    <span class="ent">➒</span> suffixes = suffix_map_2.get(prefix)<br/>       if suffixes != None:<br/>           for candidate in suffixes:<br/>               num_syls = count_syllables(candidate)<br/>               if current_syls + num_syls &lt;= target_syls:<br/>                   accepted_words.append(candidate)<br/><span epub:type="pagebreak" id="page_175"/>       logging.debug("accepted words after \"%s\" = %s\n",<br/>                     prefix, set(accepted_words))<br/>    <span class="ent">➓</span> return accepted_words</pre>
<p class="listing" id="ch09list4"><em>Listing 9-4: Two functions that select a word given a prefix, Markov model, and syllable count</em></p>
<p class="indent">Define a function named <code>word_after_single()</code> to select the next word in the haiku based on the preceding single seed word. For arguments, this function takes the preceding word, the Markov order 1 model, the current syllable count, and the target syllable count <span class="ent">➊</span>.</p>
<p class="indent">Start an empty list to hold the acceptable words, which are the words that both follow the prefix and whose syllable count doesn’t exceed the syllable target <span class="ent">➋</span>. Call these trailing words <code>suffixes</code> and use the dictionary <code>get()</code> method, which returns a dictionary value given a key, to assign them to the variable <span class="ent">➌</span>. Rather than raising a <code>KeyError</code>, the <code>get()</code> method will return <code>None</code> if you request a key that isn’t in the dictionary.</p>
<p class="indent">There is the extremely rare chance that the prefix will be the last word in a corpus and that it will be unique. In that case, there will be no suffixes. Use an <code>if</code> statement to anticipate this <span class="ent">➍</span>. If there are no suffixes, the function that calls <code>word_after_single()</code>, which you’ll define in the next section, chooses a new prefix.</p>
<p class="indent">Each of the suffixes represents a <em>candidate</em> word for the haiku, but the program hasn’t yet determined whether a candidate will “fit.” So use a <code>for</code> loop, the <code>count_syllables</code> module, and an <code>if</code> statement to determine whether adding the word to the line violates the target number of syllables per line <span class="ent">➎</span>. If the target isn’t exceeded, append the word to the list of accepted words <span class="ent">➏</span>. Display the acceptable words in a <code>logging</code> message and then return it <span class="ent">➐</span>.</p>
<p class="indent">The next function, <code>word_after_double()</code>, is like the previous function except that you pass it word pairs and the Markov order 2 model (<code>suffix_map_2</code>) <span class="ent">➑</span> and get the suffixes from this dictionary <span class="ent">➒</span>. But just like the <code>word_after_single()</code> function, <code>word_after_double()</code> returns a list of acceptable words <span class="ent">➓</span>.</p>
<h4 class="h4" id="lev206"><strong><em>Generating the Haiku Lines</em></strong></h4>
<p class="noindent">With all the helper functions ready, you can define the function that actually writes the lines of the haiku. The function can build either the whole haiku or just update lines two or three. There are two paths to take: one to use when the program has at most a one-word suffix to work with and another for every other situation.</p>
<h5 class="h5" id="lev207"><strong>Building the First Line</strong></h5>
<p class="noindent"><a href="ch09.xhtml#ch09list5">Listing 9-5</a> defines the function that writes haiku lines and initiates the haiku’s first line.</p>
<p class="margin"><em>markov_haiku.py,</em> part 5</p>
<pre><span class="ent">➊</span> def haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line, target_syls):<br/>       """Build a haiku line from a training corpus and return it."""<br/>    <span class="ent">➋</span> line = '2/3'<br/>       line_syls = 0<br/>       current_line = []<br/><span epub:type="pagebreak" id="page_176"/>    <span class="ent">➌</span> if len(end_prev_line) == 0:  # build first line<br/>        <span class="ent">➍</span> line = '1'<br/>        <span class="ent">➎</span> word, num_syls = random_word(corpus)<br/>           current_line.append(word)<br/>           line_syls += num_syls<br/>        <span class="ent">➏</span> word_choices = word_after_single(word, suffix_map_1,<br/>                                            line_syls, target_syls)<br/>        <span class="ent">➐</span> while len(word_choices) == 0:<br/>               prefix = random.choice(corpus)<br/>               logging.debug("new random prefix = %s", prefix)<br/>               word_choices = word_after_single(prefix, suffix_map_1,<br/>                                                line_syls, target_syls)<br/>        <span class="ent">➑</span> word = random.choice(word_choices)<br/>           num_syls = count_syllables(word)<br/>           logging.debug("word &amp; syllables = %s %s", word, num_syls)<br/>        <span class="ent">➒</span> line_syls += num_syls<br/>           current_line.append(word)<br/>        <span class="ent">➓</span> if line_syls == target_syls:<br/>               end_prev_line.extend(current_line[-2:])<br/>               return current_line, end_prev_line</pre>
<p class="listing" id="ch09list5"><em>Listing 9-5: Defines the function that writes haiku lines and initiates the first line</em></p>
<p class="indent">Define a function that takes as arguments both Markov models, the training corpus, the last word pair from the end of the preceding line, and the target number of syllables for the current line <span class="ent">➊</span>. Immediately use a variable to specify which haiku lines are being simulated <span class="ent">➋</span>. Most of the processing will be for lines two and three (and possibly the last part of line one), where you will be working with an existing word-pair prefix, so let these represent the base case. After this, start a counter for the running total of syllables in the line and start an empty list to hold the words in the current line.</p>
<p class="indent">Use an <code>if</code> statement that’s <code>True</code> under the condition that the <code>end_prev_line</code> parameter’s length—the number of syllables in the previous line’s last two words—is 0, meaning there was no preceding line and you are on line one <span class="ent">➌</span>. The first statement in that <code>if</code> block changes the <code>line</code> variable to 1 <span class="ent">➍</span>.</p>
<p class="indent">Choose the initial seed word and get its syllable count by calling the <code>random_word()</code> function <span class="ent">➎</span>. By assigning the <code>word</code> and <code>num_syls</code> variables together, you are “unpacking” the <code>(word, num_sylls)</code> tuple that the <code>random_word()</code> function returns. Functions end at <code>return</code> statements, so returning tuples is a great way to return multiple variables. In a more advanced version of this program, you could use generator functions with the <code>yield</code> keyword, as <code>yield</code> returns a value without relinquishing control of execution.</p>
<p class="indent">Next, append the <code>word</code> to <code>current_line</code> and add the <code>num_syls</code> to the running total. Now that you have a seed, collect all the seed’s possible suffixes with the <code>word_after_single()</code> function <span class="ent">➏</span>.</p>
<p class="indent">If there are no acceptable words, start a <code>while</code> loop to handle this situation. This loop will continue until a non-empty list of acceptable word choices has been returned <span class="ent">➐</span>. The program will choose a new prefix—a ghost prefix—using the <code>random</code> module’s <code>choice</code> method. (Remember that this prefix will not become part of the haiku but is used only to reaccess <span epub:type="pagebreak" id="page_177"/>the Markov model.) Inside the <code>while</code> loop, a <code>logging</code> message will let you know which ghost prefix has been chosen. Then the program will call <code>word_after_single()</code> function once more.</p>
<p class="indent">Once the list of acceptable words is built, use <code>choice</code> again to select a word from the <code>word_choices</code> list <span class="ent">➑</span>. Because the list may include duplicate words, this is where you see the statistical impact of the Markov model. Follow by counting the syllables in the word and <code>logging</code> the results.</p>
<p class="indent">Add the syllable count to the line’s running total and append the word to the <code>current_line</code> list <span class="ent">➒</span>.</p>
<p class="indent">If the number of syllables in the first two words equals 5 <span class="ent">➓</span>, name a variable <code>end_prev_line</code> and assign it the last two words of the previous line; this variable is the prefix for line two. Finally, return the whole line and the <code>end_prev_line</code> variable.</p>
<p class="indent">If the target number of syllables for the first line hasn’t been reached, the program will jump to the <code>while</code> loop in the next section to complete the line.</p>
<h5 class="h5" id="lev208"><strong>Building the Remaining Lines</strong></h5>
<p class="noindent">In <a href="ch09.xhtml#ch09list6">Listing 9-6</a>, the last part of the <code>haiku_line()</code> function addresses the case where the haiku already contains a word-pair prefix that the program can use in the Markov order 2 model. The program uses it to complete line one—assuming the first two words didn’t already total five syllables—and to build lines two and three. The user will also be able to regenerate lines two or three, after a complete haiku has been written.</p>
<p class="margin"><em>markov_haiku.py,</em> part 6</p>
<pre>    <span class="ent">➊</span> else: # build lines 2 and 3<br/>        <span class="ent">➋</span> current_line.extend(end_prev_line)<br/><br/>    <span class="ent">➌</span> while True:<br/>           logging.debug("line = %s\n", line)<br/>        <span class="ent">➍</span> prefix = current_line[-2] + ' ' + current_line[-1]<br/>        <span class="ent">➎</span> word_choices = word_after_double(prefix, suffix_map_2,<br/>                                            line_syls, target_syls)<br/>        <span class="ent">➏</span> while len(word_choices) == 0:<br/>               index = random.randint(0, len(corpus) - 2)<br/>               prefix = corpus[index] + ' ' + corpus[index + 1]<br/>               logging.debug("new random prefix = %s", prefix)<br/>               word_choices = word_after_double(prefix, suffix_map_2,<br/>                                                line_syls, target_syls)<br/>           word = random.choice(word_choices)<br/>           num_syls = count_syllables(word)<br/>           logging.debug("word &amp; syllables = %s %s", word, num_syls)<br/><br/>        <span class="ent">➐</span> if line_syls + num_syls &gt; target_syls:<br/>               continue<br/>           elif line_syls + num_syls &lt; target_syls:<br/>               current_line.append(word)<br/>               line_syls += num_syls<br/>           elif line_syls + num_syls == target_syls:<br/>               current_line.append(word)<br/>               break<br/><br/><span epub:type="pagebreak" id="page_178"/>    <span class="ent">➑</span> end_prev_line = []<br/>       end_prev_line.extend(current_line[-2:])<br/><br/>    <span class="ent">➒</span> if line == '1':<br/>           final_line = current_line[:]<br/>       else:<br/>           final_line = current_line[2:]<br/><br/>       return final_line, end_prev_line</pre>
<p class="listing" id="ch09list6"><em>Listing 9-6: Uses a Markov order 2 model to complete the function that writes haiku lines</em></p>
<p class="indent">Start with an <code>else</code> statement that’s executed if there is a suffix <span class="ent">➊</span>. Since the last part of the <code>haiku_line()</code> function has to handle line one as well as lines two and three, use a trick where you add the <code>end_prev_line</code> list (built outside the conditional at step <span class="ent">➑</span>) to the <code>current_line</code> list <span class="ent">➋</span>. Later, when you add the finalized line to the haiku, you’ll discard this leading word pair.</p>
<p class="indent">Start a <code>while</code> loop that continues until the target syllable count for the line has been reached <span class="ent">➌</span>. The start of each iteration begins with a debugging message that informs you of the path the loop is evaluating: <code>'1'</code> or <code>'2/3'</code>.</p>
<p class="indent">With the last two words of the previous line added to the start of the current line, the last two words of the current line will always be the prefix <span class="ent">➍</span>.</p>
<p class="indent">Using the Markov order 2 model, create a list of acceptable words <span class="ent">➎</span>. In the event the list is empty, the program uses the ghost prefix process <span class="ent">➏</span>.</p>
<p class="indent">Evaluate what to do next using the syllable count <span class="ent">➐</span>. If there are too many syllables, use a <code>continue</code> statement to restart the <code>while</code> loop. If there aren’t enough syllables, append the word and add its syllable count to the line’s syllable count. Otherwise, append the word and end the loop.</p>
<p class="indent">Assign the last two words in the line to the <code>end_prev_line</code> variable so the program can use it as a prefix for the next line <span class="ent">➑</span>. If the current path is line <code>'1'</code>, copy the current line to a variable called <code>final_line</code>; if the path is line <code>'2/3'</code>, use index slicing to exclude the first two words before assigning to <code>final_line</code> <span class="ent">➒</span>. This is how you remove the initial <code>end_prev_line</code> word pair from lines two or three.</p>
<h4 class="h4" id="lev209"><strong><em>Writing the User Interface</em></strong></h4>
<p class="noindent"><a href="ch09.xhtml#ch09list7">Listing 9-7</a> defines the <em>markov_haiku.py</em> program’s <code>main()</code> function, which runs the setup functions and the user interface. The interface presents the user with a menu of choices and displays the resulting haiku.</p>
<p class="margin"><em>markov_haiku.py,</em> part 7</p>
<pre>def main():<br/>    """Give user choice of building a haiku or modifying an existing haiku."""<br/>    intro = """\n<br/>    A thousand monkeys at a thousand typewriters...<br/>    or one computer...can sometimes produce a haiku.\n"""<br/>    print("{}".format(intro))<br/><br/> <span class="ent">➊</span> raw_haiku = load_training_file("train.txt")<br/>    corpus = prep_training(raw_haiku)<br/>    suffix_map_1 = map_word_to_word(corpus)<br/><span epub:type="pagebreak" id="page_179"/>    suffix_map_2 = map_2_words_to_word(corpus)<br/>    final = []<br/><br/>    choice = None<br/> <span class="ent">➋</span> while choice != "0":<br/><br/>     <span class="ent">➌</span> print(<br/>            """<br/>            Japanese Haiku Generator<br/><br/>            0 - Quit<br/>            1 - Generate a Haiku<br/>            2 - Regenerate Line 2<br/>            3 - Regenerate Line 3<br/>            """<br/>            )<br/><br/>     <span class="ent">➍</span> choice = input("Choice: ")<br/>        print()<br/><br/>        # exit<br/>     <span class="ent">➎</span> if choice == "0":<br/>            print("Sayonara.")<br/>            sys.exit()<br/><br/>        # generate a full haiku<br/>     <span class="ent">➏</span> elif choice == "1":<br/>            final = []<br/>            end_prev_line = []<br/>            first_line, end_prev_line1 = haiku_line(suffix_map_1, suffix_map_2,<br/>                                                    corpus, end_prev_line, 5)<br/>            final.append(first_line)<br/>            line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2,<br/>                                              corpus, end_prev_line1, 7)<br/>            final.append(line)<br/>            line, end_prev_line3 = haiku_line(suffix_map_1, suffix_map_2,<br/>                                              corpus, end_prev_line2, 5)<br/>            final.append(line)<br/><br/>        # regenerate line 2<br/>     <span class="ent">➐</span> elif choice == "2":<br/>            if not final:<br/>                print("Please generate a full haiku first (Option 1).")<br/>                continue<br/>            else:<br/>                line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2,<br/>                                                  corpus, end_prev_line1, 7)<br/>                final[1] = line<br/><br/>        # regenerate line 3<br/>     <span class="ent">➑</span> elif choice == "3":<br/>            if not final:<br/>                print("Please generate a full haiku first (Option 1).")<br/>                continue<br/><span epub:type="pagebreak" id="page_180"/>            else:<br/>                line, end_prev_line3 = haiku_line(suffix_map_1, suffix_map_2,<br/>                                                  corpus, end_prev_line2, 5)<br/>                final[2] = line<br/><br/>        # some unknown choice<br/>     <span class="ent">➒</span> else:<br/>            print("\nSorry, but that isn't a valid choice.", file=sys.stderr)<br/>            continue<br/><br/>     <span class="ent">➓</span> # display results<br/>        print()<br/>        print("First line = ", end="")<br/>        print(' '.join(final[0]), file=sys.stderr)<br/>        print("Second line = ", end="")<br/>        print(" ".join(final[1]), file=sys.stderr)<br/>        print("Third line = ", end="")<br/>        print(" ".join(final[2]), file=sys.stderr)<br/>        print()<br/><br/>    input("\n\nPress the Enter key to exit.")<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p class="listing" id="ch09list7"><em>Listing 9-7: Starts the program and presents a user interface</em></p>
<p class="indent">After the introduction message, load and prep the training corpus and build the two Markov models. Then create an empty list to hold the final haiku <span class="ent">➊</span>. Next, name a <code>choice</code> variable and set it to <code>None</code>. Start a <code>while</code> loop that continues until the user chooses choice 0 <span class="ent">➋</span>. By entering <code>0</code>, the user has decided to quit the program.</p>
<p class="indent">Use a <code>print()</code> statement with triple quotes to display a menu <span class="ent">➌</span>, and then get the user’s choice <span class="ent">➍</span>. If the user chooses 0, exit and say goodbye <span class="ent">➎</span>. If the user chooses 1, they want the program to generate a new haiku, so reinitialize the <code>final</code> list and the <code>end_prev_line</code> variable <span class="ent">➏</span>. Then call the <code>haiku_line()</code> function for all three lines and pass it the correct arguments—including the target syllable count for each line. Note that the <code>end_prev_line</code> variable name changes with each line; for example, <code>end_prev_line2</code> holds the final two words of line two. The last variable, <code>end_prev_line3</code>, is just a placeholder so you can reuse the function; in other words, it is never put to use. Each time the <code>haiku_line()</code> function is called, it returns a line that you need to append to the <code>final</code> list.</p>
<p class="indent">If the user chooses 2, the program regenerates the second line <span class="ent">➐</span>. There needs to be a full haiku before the program can rebuild a line, so use an <code>if</code> statement to handle the user jumping the gun. Then call the <code>haiku_line()</code> function, making sure to pass it the <code>end_prev_line1</code> variable to link it to the preceding line, and set the syllable target to seven syllables. Insert the rebuilt line in the <code>final</code> list at index 1.</p>
<p class="indent">Repeat the process if the user chooses 3, only make the syllable target <code>5</code> and pass <code>end_prev_line2</code> to the <code>haiku_line()</code> function <span class="ent">➑</span>. Insert the line at index 2 in <code>final</code>.</p>
<p class="indent"><span epub:type="pagebreak" id="page_181"/>If the user enters anything not on the menu, let them know and then continue the loop <span class="ent">➒</span>. Finish by displaying the haiku. Use the <code>join()</code> method and <code>file=sys.stderr</code> for an attractive printout in the shell <span class="ent">➓</span>.</p>
<p class="indent">End the program with the standard code for running the program as a module or in stand-alone mode.</p>
<h3 class="h3" id="lev210"><strong>The Results</strong></h3>
<p class="noindent">To evaluate a poetry-writing program, you need a way to measure something subjective—whether or not poems are “good”—using objective criteria. For the <em>markov_haiku.py</em> program, I propose the following categories based on two criteria, originality and humanness:</p>
<p class="hang"><strong>Duplicate</strong> Verbatim duplication of a haiku in the training corpus.</p>
<p class="hang"><strong>Good</strong> A haiku that is—at least to some people—indistinguishable from a haiku written by a human poet. It should represent the initial result or the result of regenerating line two or three a few times.</p>
<p class="hang"><strong>Seed</strong> A haiku that has merit but that many would suspect was written by a computer, or a haiku that you could transform into a good haiku by changing or repositioning no more than two words (described in more detail later). It may require multiple regenerations of lines two or three.</p>
<p class="hang"><strong>Rubbish</strong> A haiku that is clearly a random amalgam of words and has no merit as a poem.</p>
<p class="indentt">If you use the program to generate a large number of haiku and place the results in these categories, you’ll probably end up with the distribution in <a href="ch09.xhtml#ch09fig3">Figure 9-3</a>. About 5 percent of the time, you’ll duplicate an existing haiku in the training corpus; 10 percent of the time, you’ll produce a good haiku; around 25 percent will be passable or fixable haiku; and the rest will be rubbish.</p>
<div class="image"><a id="ch09fig3"/><img src="../images/f0181-01.jpg" alt="image"/></div>
<p class="figcap"><em>Figure 9-3: Subjective results of generating 500 haiku using</em> markov_haiku.py</p>
<p class="indent"><span epub:type="pagebreak" id="page_182"/>Considering how simple the Markov process is, the results in <a href="ch09.xhtml#ch09fig3">Figure 9-3</a> are impressive. To quote Charles Hartman once again, “Here is language creating itself out of nothing, out of mere statistical noise. . . . We can watch sense evolve and meaning stagger up onto its own miraculous feet.”</p>
<h4 class="h4" id="lev211"><strong><em>Good Haiku</em></strong></h4>
<p class="noindentb">The following are some examples of simulated haiku classified as “good.” In the first example, the program has subtly—you might say “skillfully,” if you didn’t know an algorithm was responsible—altered my haiku from <a href="ch08.xhtml#ch08">Chapter 8</a> to produce a new haiku with the same meaning.</p>
<p class="poem">Cloudbanks that I let</p>
<p class="poem">Myself pretend are distant</p>
<p class="poem">Mountains faraway</p>
<p class="indentt">In the next example, the program has managed to duplicate a common theme in traditional haiku: the juxtaposition of images or ideas.</p>
<p class="poemt">The mirror I stare</p>
<p class="poem">Into shows my father’s face</p>
<p class="poem">An old silent pond</p>
<p class="indentt">In this case, you find out that the mirror is really the surface of a still pond, though you may interpret the face itself as the pond.</p>
<p class="indentb">Running the program is a little like panning for gold: sometimes you find a nugget. The haiku on the left was written by Ringai over 300 years ago. In the haiku on the right, the program has made subtle changes so that the poem now evokes images of a late spring freeze—a setback for the progress of the season.</p>
<table class="topbot-d">
<tbody>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba">In these dark waters</p>
<p class="taba">Drawn up from my frozen well</p>
<p class="taba">Glittering of Spring</p>
<p class="taba">                    —<em>Ringai</em></p></td>
<td style="vertical-align: top;" class="table-b"><p class="tabaa">Waters drawn up from</p>
<p class="tabaa">My frozen well glittering</p>
<p class="tabaa">Of Spring standing still</p>
<p class="tabaa">                         —<em>Python</em></p></td>
</tr>
</tbody>
</table>
<p class="indent">The following are examples of a few more “good” haiku. The first one is remarkable since it was built from three separate haiku in the training corpus, yet maintains a clear contextual thread throughout.</p>
<p class="poemt">As I walk the path<br/>Eleven brave knights canter<br/>Through the stormy woods</p>
<p class="poemt">A line flip-flapping<br/>Across the dark crimson sky<br/>On this winter pond</p>
<p class="poemt"><span epub:type="pagebreak" id="page_183"/>Such a thing alive<br/>Rusted gate screeches open<br/>Even things feel pain</p>
<p class="poemt">The stone bridge! Sitting<br/>Quietly doing nothing<br/>Yet Spring comes grass grows</p>
<p class="poemt">Dark sky oh! Autumn<br/>Snowflakes! A rotting pumpkin<br/>Collapsed and covered</p>
<p class="poemt">Desolate moors fray<br/>Black cloudbank, broken, scatters<br/>In the pines, the graves</p>
<h4 class="h4" id="lev212"><strong><em>Seed Haiku</em></strong></h4>
<p class="noindent">The concept of computers helping humans write poetry has been around for some time. Poets often “prime the pump” by imitating earlier poems, and there’s no reason a computer can’t provide first drafts as part of a cyber-partnership. Even a fairly poor computer creation has the potential to “seed” the creative process and help a human overcome writer’s block.</p>
<p class="indent">The following are three examples of seed haiku from the <em>markov_haiku.py</em> program. On the left is the not-quite-right computer-generated haiku. On the right is the version that I adjusted. I changed only a single word, highlighted in bold, in each.</p>
<table class="topbot-d">
<tbody>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><span class="gray">My life must end like</span></p>
<p class="taba"><span class="gray">Another flower what a</span></p>
<p class="taba"><span class="gray">Hungry wind</span> <strong>it</strong> <span class="gray">is</span></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"><span class="gray">My life must end like</span></p>
<p class="taba"><span class="gray">Another flower what a</span></p>
<p class="taba"><span class="gray">Hungry wind is</span> <strong>death</strong></p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><span class="gray">The dock floating in</span></p>
<p class="taba"><span class="gray">The hot caressing night just</span></p>
<p class="taba"><span class="gray">Before the dawn</span> <strong>old</strong></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"><span class="gray">The dock floating in</span></p>
<p class="taba"><span class="gray">The hot caressing night just</span></p>
<p class="taba"><span class="gray">Before the dawn</span> <strong>rain</strong></p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"> </p></td>
</tr>
<tr>
<td style="vertical-align: top;" class="table-b"><p class="taba"><span class="gray">Moonrise on the grave</span></p>
<p class="taba"><span class="gray">And my old sadness a sharp</span></p>
<p class="taba"><span class="gray">Shovel thrust</span> <strong>the</strong> <span class="gray">stars</span></p></td>
<td style="vertical-align: top;" class="table-b"><p class="taba"><span class="gray">Moonrise on the grave</span></p>
<p class="taba"><span class="gray">And my old sadness a sharp</span></p>
<p class="taba"><span class="gray">Shovel thrust</span> <strong>of</strong> <span class="gray">stars</span></p></td>
</tr>
</tbody>
</table>
<p class="indent">The last has cryptic meaning but seems to work, as it is filled with natural associations (moon and stars, grave and shovel, grave and sadness). At any rate, you shouldn’t fret too much over meaning. To paraphrase T.S. Eliot: meaning is like the meat the burglar throws the dog, to keep the mind diverted while the poem does its work!</p>
<h3 class="h3" id="lev213"><span epub:type="pagebreak" id="page_184"/><strong>Summary</strong></h3>
<p class="noindent">It took two chapters, but you now have a program that can simulate Japanese haiku written by the masters—or at least provide a useful starting point for a human poet. In addition, you applied the <code>logging</code> module to monitor what the program was doing at key steps.</p>
<h3 class="h3" id="lev214"><strong>Further Reading</strong></h3>
<p class="noindent"><em>Virtual Muse: Experiments in Computer Poetry</em> (Wesleyan University Press, 1996) by Charles O. Hartman is an engaging look at the early collaboration between humans and computers to write poetry.</p>
<p class="indent">If you want to know more about Claude Shannon, check out <em>A Mind at Play: How Claude Shannon Invented the Information Age</em> (Simon &amp; Schuster, 2017) by Jimmy Soni and Rod Goodman.</p>
<p class="indent">You can find a digital version of <em>Japanese Haiku: Two Hundred Twenty Examples of Seventeen-Syllable Poems</em> (The Peter Pauper Press, 1955), translated by Peter Beilenson, online at Global Grey (<em><a href="https://www.globalgreyebooks.com/">https://www.globalgreyebooks.com/</a></em>).</p>
<p class="indent">In the paper “Gaiku: Generating Haiku with Word Association Norms” (Association for Computational Linguistics, 2009), Yael Netzer and coauthors explore the use of word association norms (WANs) to generate haiku. You can build WAN corpora by submitting trigger words to people and recording their immediate responses (for example, <em>house</em> to <em>fly</em>, <em>arrest</em>, <em>keeper</em>, and so on). This results in the kind of tightly linked, intuitive relationships characteristic of human-generated haiku. You can find the paper online at <em><a href="http://www.cs.brandeis.edu/~marc/misc/proceedings/naacl-hlt-2009/CALC-09/pdf/CALC-0905.pdf">http://www.cs.brandeis.edu/~marc/misc/proceedings/naacl-hlt-2009/CALC-09/pdf/CALC-0905.pdf</a></em>.</p>
<p class="indent"><em>Automate the Boring Stuff with Python</em> (No Starch Press, 2015) by Al Sweigart has a useful overview chapter on debugging techniques, including <code>logging</code>.</p>
<h3 class="h3" id="lev215"><strong>Challenge Projects</strong></h3>
<p class="noindent">I’ve described some suggestions for spin-off projects in this section. As with all challenge projects, you’re on your own—no solutions are provided.</p>
<h4 class="h4" id="lev216"><strong><em>New Word Generator</em></strong></h4>
<p class="noindent">In his award-winning 1961 sci-fi novel, <em>Stranger in a Strange Land</em>, author Robert A. Heinlein invented the word <em>grok</em> to represent deep, intuitive understanding. This word moved into the popular culture—especially the culture of computer programming—and is now in the <em>Oxford English Dictionary</em>.</p>
<p class="indent">Coming up with a new word that sounds legitimate is not easy, in part because humans are so anchored to the words we already know. But computers don’t suffer from this affliction. In <em>Virtual Muse</em>, Charles Hartman observed that his poetry-writing program would sometimes create intriguing letter combinations, such as <em>runkin</em> or <em>avatheformitor</em>, that could easily represent new words.</p>
<p class="indent"><span epub:type="pagebreak" id="page_185"/>Write a program that recombines letters using Markov order 2, 3, and 4 models and use the program to generate interesting new words. Give them a definition and start applying them. Who knows—you may coin the next <em>frickin</em>, <em>frabjous</em>, <em>chortle</em>, or <em>trill</em>!</p>
<h4 class="h4" id="lev217"><strong><em>Turing Test</em></strong></h4>
<p class="noindent">According to Alan Turing, “A computer would deserve to be called intelligent if it could deceive a human into believing that it was human.” Use your friends to test haiku generated by the <em>markov_haiku.py</em> program. Mix the computer haiku with a few haiku written by the masters or yourself. Since computer haiku are often enjambed, be careful to choose human haiku that are also enjambed in order to deny your cleverer friends a free ride. Using lowercase letters and minimal punctuation for all the haiku also helps. I’ve provided an example, using Facebook, in <a href="ch09.xhtml#ch09fig4">Figure 9-4</a>.</p>
<div class="image"><a id="ch09fig4"/><img src="../images/f0185-01.jpg" alt="image"/></div>
<p class="figcap"><em>Figure 9-4: Example Turing test experiment post on Facebook</em></p>
<h4 class="h4" id="lev218"><strong><em>Unbelievable! This Is Unbelievable! Unbelievable!</em></strong></h4>
<p class="noindent">President Trump is famous for speaking in short, simple sentences that use “the best words,” and short, simple sentences are great for haiku. In fact, the <em>Washington Post</em> published some inadvertent haiku found in some of his campaign speeches. Among these were:</p>
<p class="poemt">He’s a great, great guy.<br/>I saw him the other day.<br/>On television.</p>
<p class="poemt">They want to go out.<br/>They want to lead a good life.<br/>They want to work hard.</p>
<p class="poemt"><span epub:type="pagebreak" id="page_186"/>We have to do it.<br/>And we need the right people.<br/>So Ford will come back.</p>
<p class="indentt">Use online transcripts of Donald Trump’s speeches to build a new training corpus for the <em>markov_haiku.py</em> program. Remember that you’ll need to revisit <a href="ch08.xhtml#ch08">Chapter 8</a> and build a new “missing words” dictionary for any words not in the <em>Carnegie Mellon University Pronouncing Dictionary</em>. Then rerun the program and generate haiku that capture this moment in history. Save the best ones and revisit the Turing test challenge to see if your friends can separate your haiku from true Trump quotes.</p>
<h4 class="h4" id="lev219"><strong><em>To Haiku, or Not to Haiku</em></strong></h4>
<p class="noindent">William Shakespeare wrote many famous phrases that fit the syllabic structure of haiku, such as “all our yesterdays,” “dagger of the mind,” and “parting is such sweet sorrow.” Use one or more of the Bard’s plays as a training corpus for the <em>markov_haiku.py</em> program. The big challenge here will be counting the syllables for all that olde English.</p>
<h4 class="h4" id="lev220"><strong><em>Markov Music</em></strong></h4>
<p class="noindent">If you’re musically inclined, perform an online search for “composing music with Markov chains.” You should find a wealth of material on using Markov chain analysis to compose music, using the notes from existing songs as a training corpus. The resulting “Markov music” is used like our seed haiku—as inspiration for human songwriters.</p>
</body></html>