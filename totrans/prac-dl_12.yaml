- en: '**12'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: INTRODUCTION TO CONVOLUTIONAL NEURAL NETWORKS**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’ll introduce a new and potent approach to dealing with multidimensional
    information. In particular, we’ll work through the theory and high-level operation
    of *convolutional neural networks* (CNNs), a cornerstone of modern deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by presenting the motivations behind the development of CNNs. Convolutions
    are the heart of CNNs, so they’ll come next. We’ll discuss them in some detail,
    in particular how they’re used by the CNN. We’ll then introduce a basic CNN and
    work through its anatomy. We’ll use this basic CNN architecture for the remainder
    of the chapter. After we dissect a CNN, we’ll work through how convolutional layers
    work. Then come pooling layers. We’ll see what they do, what benefit they offer,
    and what price they exact in return. To round out our discussion of the fundamental
    components of a CNN, we’ll present the fully connected layers, which, in reality,
    are just the layers of a traditional, fully connected, feed-forward neural network
    like those of [Chapter 8](ch08.xhtml#ch08).
  prefs: []
  type: TYPE_NORMAL
- en: 'One topic will be conspicuously absent from this chapter: the mechanics of
    training a CNN. In part, we’ll gloss over training because it’s messy once convolutional
    layers are introduced, but primarily because we’ve already discussed backpropagation
    in [Chapter 9](ch09.xhtml#ch09), and we use the same algorithm to train a CNN.
    We calculate the weights and biases of all layers from the average loss over the
    training minibatch and use backprop to determine the derivatives we need to update
    the weights and biases for each stochastic gradient descent step.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Convolutional Neural Networks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs have several advantages over traditional neural networks. First, the convolutional
    layers of a CNN require vastly fewer parameters than fully connected neural networks,
    as we’ll see later in the chapter. CNNs require fewer parameters because the convolution
    operation applies parameters in each layer to small subsets of the input instead
    of the entire input at once, as is done with a traditional neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Second, CNNs introduce the idea of *spatial invariance*, the ability to detect
    a spatial relationship in the input regardless of where it appears. For example,
    if the input to a neural network is an image of a cat, a traditional neural network
    will take the image in as a single feature vector, meaning that if a cat appears
    in the upper-left corner of the image, the network will learn that cats can appear
    in the upper-left corners of the image but not that they can also appear in the
    lower-right corners (unless the training data contains examples with cats in the
    lower-right corners). For a CNN, however, the convolution operation can detect
    cats anywhere they appear.
  prefs: []
  type: TYPE_NORMAL
- en: While CNNs are usually used with two-dimensional inputs, they can also be used
    with one-dimensional inputs, like the feature vectors we have worked with up to
    now. However, the feature vectors we’ve worked with, like the iris measurements,
    don’t reflect any sort of spatial relationship as the parts of an image of a cat
    do. There’s nothing there for the convolution operation to take advantage of.
    This doesn’t mean that a CNN won’t work, but it does mean that it might not be
    the best sort of model to use. As always, we need to understand how various model
    types operate so we select the best model for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note** *Depending on who you ask, CNNs were either developed in 1980 by Fukushima
    to implement the Neocognitron model or in 1998 by LeCun et al. as presented in
    their famous paper “Gradient-Based Learning Applied to Document Recognition,”
    which, as of this writing, has been referenced over 21,000 times. My take is that
    both deserve credit, though LeCun used the phrase* convolutional neural network
    *or* convnet *as they are still sometimes called, and what is described in the
    paper is what we will work with in this book. The Neocognitron reflected some
    of the ideas in a CNN, but not CNNs themselves.*'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Convolution* involves sliding one thing over another. For us, this means sliding
    a *kernel*, a small 2D array, over the input, which might be the input image to
    the CNN or the output of a lower convolutional layer. There is a formal mathematical
    definition of convolution, but it really won’t help us right now. Luckily, all
    our inputs are discrete, which means we can get away with a bit of a hand-waving.
    For simplicity, we’ll focus on only the two-dimensional case.'
  prefs: []
  type: TYPE_NORMAL
- en: Scanning with the Kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *kernel* is the thing we are asking the convolutional layer to learn during
    training. It’s a collection of small 2D arrays that we move over the input. Ultimately,
    the kernels become the weights of a convolutional layer in a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: The essential operation of convolution is taking some small section of the input,
    the same size as the kernel, covering it with the kernel, performing some operation
    on the set of numbers to produce a single output number, and then repeating the
    process after moving the kernel to a new position in the input. Just how far the
    kernel is moved is known as the *stride*. Typically, the stride is 1, meaning
    the kernel slides over one element of the input.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-1](ch12.xhtml#ch12fig1) shows the effect of convolution on part
    of an MNIST digit image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-1: Convolving a kernel with an image*'
  prefs: []
  type: TYPE_NORMAL
- en: The image portion is on the left of [Figure 12-1](ch12.xhtml#ch12fig1), where
    you can see part of a handwritten 8\. The boxes correspond to pixel intensities,
    though for presentation purposes, we’ve expanded the original image so that many
    shades of gray are visible in each “pixel” box. The actual pixel values the convolution
    works with are given next, after the arrow.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the kernel is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/285equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the set of numbers we’ll slide over the input pixels. This is a 3 ×
    3 matrix, so we need to cover a 3 × 3 region of the input image. The first 3 ×
    3 region, the upper-left corner, is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/285equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We said that convolution performs an operation with the kernel and the covered
    region as the input. The operation is straightforward: multiply corresponding
    entries and sum them. Finding the first output value of the convolution begins
    with'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/286equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When the preceding elements are summed, this gives the output value as
  prefs: []
  type: TYPE_NORMAL
- en: 0 + (–248) + 0 + (–145) + 759 + (–54) + 0 + (–253) + 0 = 59
  prefs: []
  type: TYPE_NORMAL
- en: Okay, the output of the first convolution operation is 59\. What do we do with
    that number? The kernel is 3 × 3, an odd number along each side. This means that
    there is a middle element, the one with the 3 in it. The place in the output array
    where the middle number is gets replaced with the output value, the 59\. [Figure
    12-1](ch12.xhtml#ch12fig1) shows the full output of the convolution. Sure enough,
    the first element of the output is 59, located at the center of the kernel when
    the kernel is covering the upper-left corner.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining output values are calculated in precisely the same way but by
    moving the kernel over 1 pixel each time. When the end of a row is reached, the
    kernel moves back to the left side but down 1 pixel. In this way, it slides over
    the entire input image to produce the output shown in [Figure 12-1](ch12.xhtml#ch12fig1),
    just like the scan lines of an old analog television.
  prefs: []
  type: TYPE_NORMAL
- en: The next output value is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/286equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which sums to *–* 212, as we see on the right side of [Figure 12-1](ch12.xhtml#ch12fig1).
  prefs: []
  type: TYPE_NORMAL
- en: Repeating the convolution operation produces the output shown in [Figure 12-1](ch12.xhtml#ch12fig1).
    Notice the empty boxes around the output. These values are empty because the middle
    of our 3 × 3 kernel does not cover the edge of the input array. Therefore, the
    output matrix of numbers is two smaller in each dimension than the input. If the
    kernel were 5 × 5, there would be a border 2 pixels wide instead of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations of 2D convolution need to make a decision about these border
    pixels. There are options, and most toolkits support several of them. One is to
    simply ignore these pixels and make the output smaller than the input, as we’ve
    shown in [Figure 12-1](ch12.xhtml#ch12fig1). This approach is often known as *exact*
    or *valid* because we retain only values that are actually output by the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to imagine that a border of 0 values surrounds the input
    image. The border is as thick as is needed so that the kernel fits with its middle
    value matching the upper-left pixel of the input. For our example in [Figure 12-1](ch12.xhtml#ch12fig1),
    this means a border of 1 pixel because the kernel is 3 × 3 and there is one element
    on either side of the kernel’s center value. If the kernel were 5 × 5, the border
    would be 2 pixels since there are two values on either side of the kernel center.
    This is known as *zero-padding* and gives an output that is the same size as the
    input. Instead of convolving a 28×28 pixel MNIST digit image with a 3 × 3 kernel
    and getting a 26×26 pixel output as shown in [Figure 12-1](ch12.xhtml#ch12fig1),
    we get an output that is also 28×28 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: If we zero pad the example image in [Figure 12-1](ch12.xhtml#ch12fig1), we can
    fill in the first empty output square like so
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/287equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which sums to *–* 213\. This means that the upper-left corner of the output
    matrix in [Figure 12-1](ch12.xhtml#ch12fig1), which currently has an empty box,
    could be replaced by *–* 213\. Similarly, the rest of the empty boxes would have
    values, and the output of the convolution operation would be 28×28 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution for Image Processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolution, when used in a neural network, is sometimes viewed as magical,
    a special operation that lets convolutional neural networks do the wonderful things
    that they can do. This is more or less true, but the convolution operation is
    certainly not anything new. Even if we ignore mathematics entirely and think only
    of the discrete convolution of 2D images, we see that image scientists were using
    convolution for image processing decades before convolution was applied to machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation allows for all manner of image processing. For example,
    consider the images shown in [Figure 12-2](ch12.xhtml#ch12fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-2: 5 × 5 convolution kernels applied to an image*'
  prefs: []
  type: TYPE_NORMAL
- en: The original moon image is on the upper left. The other three images are the
    output from convolving the moon image with different 5 × 5 kernels. Moving clockwise
    from the upper right, the kernels either emphasize edges, diagonal structures
    (upper left to lower right), or blur the input image. All of this is accomplished
    by changing the values in the kernel, but the convolution operation remains the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: From a machine learning perspective, the power of a convolutional approach comes
    partially from the savings in terms of parameters. If a model can learn a set
    of kernels, that is a smaller set of numbers to learn than the weights for a fully
    connected model. This is a good thing on its own. The fact that a convolution
    can pull out other information about an image, such as its slowly changing components
    (the blur of [Figure 12-2](ch12.xhtml#ch12fig2)), its rapidly changing components
    (the edges of [Figure 12-2](ch12.xhtml#ch12fig2)), or even components along a
    specific direction (the diagonals of [Figure 12-2](ch12.xhtml#ch12fig2)), means
    that the model gains insight as to what is in the input. And, since we move the
    kernel over the image, we’re not dependent upon *where* in the image these structures
    occur.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of a Convolutional Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Medical students learn about anatomy by dissecting a cadaver to see the parts
    and how they relate to each other. In similar, though less challenging, fashion,
    we’ll start with the body of a CNN, an illustration of its basic architecture,
    and then pull it apart to learn what each component is and what it does.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-3](ch12.xhtml#ch12fig3) shows us our body. This is the default example
    CNN used by the Keras toolkit to train a model that classifies MNIST digits. We’ll
    use it as our standard for the remainder of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-3: The architecture of a basic convolutional neural network*'
  prefs: []
  type: TYPE_NORMAL
- en: How do we interpret this figure? Like a traditional neural network, a CNN has
    an input and an output. In this case, the input is the digit image on the upper
    left. The network then flows left to right, following the arrows. At the end of
    the top row, the network continues on the following row. Note, we’ve duplicated
    the layer at the end of the top row and placed it at the beginning of the next
    row for presentation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The flow continues along the bottom row, again left to right, until the output
    is reached. The output here is a softmax layer to give us the likelihoods of each
    of the possible digits, just as we saw for the traditional neural networks of
    [Chapter 10](ch10.xhtml#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Different Types of Layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Between each arrow is a layer of the network. The first thing we notice is
    that, unlike a traditional neural network, a CNN has many kinds of layers. Let’s
    list them here. We’ll discuss each in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional (*Conv*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling (*Pool*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flatten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should note that we’re using the Keras names for the layers. For instance,
    Keras uses *Dense* for what many other toolkits call *fully connected* or even
    *InnerProduct* layers.
  prefs: []
  type: TYPE_NORMAL
- en: Several of these layers should already be familiar. We know a ReLU layer implements
    a rectified linear unit that takes each of its inputs and asks if it is greater
    than or less than 0\. If the input is less than 0, the output is 0; otherwise,
    the output is the input. We can express this mathematically as
  prefs: []
  type: TYPE_NORMAL
- en: ReLU(*x*) = max(0, *x*)
  prefs: []
  type: TYPE_NORMAL
- en: where the *max* function returns the largest of its two arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, we mentioned dropout in [Chapter 9](ch09.xhtml#ch09). Dropout selects
    a percentage of its outputs at random during training and sets them to 0\. This
    provides a powerful form of regularization to help the network learn meaningful
    representations of the input data. There are two dropout layers in our basic CNN.
    The first uses a probability of 25 percent, meaning during any minibatch pass
    while training, some 25 percent of the outputs will be set to 0\. The second dropout
    layer uses a probability of 50 percent.
  prefs: []
  type: TYPE_NORMAL
- en: The *Flatten* and *Dense* layers are old friends, though we know them by another
    name and not as independent entities. Our traditional feedforward neural network
    uses fully connected layers to process a one-dimensional vector. Here, Flatten
    and Dense work together to implement a fully connected layer. The Flatten layer
    takes its input—usually a four-dimensional array (we’ll see why later)—and turns
    it into a vector. It does something similar to what we did to construct the vector
    form of the MNIST dataset, where we put the pixels of each row end-to-end to unravel
    the two-dimensional image. The Dense layer implements a traditional neural network
    layer, where each input value is mapped to each node of the Dense layer. Typically,
    the output of the Dense layer is passed to another Dense layer or a softmax layer
    to let the network make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, many layers of a CNN expect four-dimensional arrays as inputs and
    produce four-dimensional arrays as outputs. The first dimension is the number
    of inputs in the minibatch. So, if we have a minibatch of 24, the first dimension
    of the 4D array will be 24.
  prefs: []
  type: TYPE_NORMAL
- en: The second and third dimensions are called the *height* and *width*. If the
    input to a layer is the input to the model (say, an image), then these dimensions
    are truly the height and width dimensions of the image. If the input is really
    the output of some other layer, say a (yet to be described) convolutional layer,
    the *height* and *width* refer to the output from applying a convolutional kernel
    to some input. For example, the output in [Figure 12-1](ch12.xhtml#ch12fig1) has
    height and width of 26.
  prefs: []
  type: TYPE_NORMAL
- en: The last dimension is the number of channels, if an input image; or the number
    of *feature maps*, if the output of a convolutional or pooling layer. The number
    of channels in an image is simply the number of bands, where a grayscale image
    has a single band and a color image typically has three bands, one each for red,
    green, and blue. Some color images also have an alpha channel used to specify
    how transparent a pixel is, but these are typically dropped before passing the
    image through a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: The output in [Figure 12-1](ch12.xhtml#ch12fig1) is called a *feature map* because
    it is the response from convolving a kernel over an input. As we saw in [Figure
    12-2](ch12.xhtml#ch12fig2), convolving a kernel over an image can pull out features
    in the image, so the outputs of the kernels used by a convolutional layer are
    called *feature maps*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leaves two layers to investigate: *Convolutional* and *Pooling*. These
    layers are new.'
  prefs: []
  type: TYPE_NORMAL
- en: In our basic CNN, the convolutions operate on sets of two-dimensional inputs
    where by *set* I mean a stack of two-dimensional arrays, where the third dimension
    is the number of channels or feature maps. This means that unlike every other
    model we’ve looked at in this book, the input here really is the full image, not
    a vector created from the image. In terms of CNNs, however, the convolutions need
    not operate on only two-dimensional inputs. Three-dimensional convolutions exist,
    as do one-dimensional, though both are seldom used compared to two-dimensional
    convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: A pooling layer is used to reduce the spatial dimension of its input by combining
    input values according to some rule. The most common rule is *max*, where the
    largest value in the small block moved over the input is kept; the other values
    are discarded. Again, we’ll cover pooling layers at length in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Many other layer types can be used by modern networks, and many of these are
    directly supported in Keras already, though it’s possible to add your own layers.
    This flexibility is one reason Keras often quickly supports new deep learning
    developments. As with a traditional neural network, for a layer to have weights
    that can be learned, the layer needs to be differentiable in a mathematical sense
    so that the chain rule can continue, and the partial derivatives can be calculated
    to learn how to adjust the weights during gradient descent. If the previous sentence
    is not clear, it’s time to review the backprop section of [Chapter 9](ch09.xhtml#ch09).
  prefs: []
  type: TYPE_NORMAL
- en: Passing Data Through the CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s look again at [Figure 12-3](ch12.xhtml#ch12fig3). A lot is happening here
    beyond just the order and names of the layers. Many layers have numbers in italics
    running along the bottom. These numbers represent the dimensions of the output
    of the layer, the height, width, and number of feature maps. If the layer has
    only a single number, it outputs a vector with that many elements.
  prefs: []
  type: TYPE_NORMAL
- en: The input to the CNN is a 28 × 28 × 1 image. The output of a convolutional layer
    is a set of feature maps. Thus the output of the first convolutional layer is
    26 × 26 × 32, meaning there are 32 feature maps, each a 26 × 26 image calculated
    from the single 28 × 28 × 1 input image. Similarly, the output of the second convolutional
    layer is 24 × 24 × 64, a set of 64 feature maps derived from the 26 × 26 × 32
    input, which was itself the output of the first convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: We see that the pooling layer at the end of the first row takes its 24 × 24
    × 64 input and reduces it to 12 × 12 × 64\. The “max” label tells us what the
    pooling is doing; it takes a 2 × 2 region of the input and returns the largest
    value. Since the input is 2 × 2 and it returns only one value, this reduces each
    24 × 24 input to a 12 × 12 output. This process is applied to each feature map
    so that the output is 12 × 12 × 64.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the bottom row of [Figure 12-3](ch12.xhtml#ch12fig3) shows us that
    the Flatten layer takes the 12 × 12 × 64 output of the pooling layer and turns
    it into a vector of 9,216 elements. Why 9,216? Because 12 × 12 × 64 = 9,216\.
    Next, the Dense layer has 128 nodes, and, finally, our output softmax has 10 nodes
    because there are 10 classes, the digits 0 through 9.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 12-3](ch12.xhtml#ch12fig3), the ReLU and Dropout layers have no numbers
    below them. These layers do not alter the shape of their inputs. They simply perform
    some operation on each of the elements regardless of the shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolutional layers of our basic CNN have other numbers associated with
    them: “3 × 3” and “32” or “64”. The 3 × 3 tells us the size of the convolutional
    kernel, and the 32 or 64 tells us the number of feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: We already alluded to the 2 × 2 part of the pooling layer. This represents the
    size of the pooling kernel, which, much like a convolutional kernel, slides over
    the input, feature map by feature map (or channel by channel), to reduce the size
    of the input. Working with a 2 × 2 pooling kernel implies that, typically, the
    output will be one-half the size of the input in each of the row and column dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-3](ch12.xhtml#ch12fig3) has familiar parts, but the presentation
    is new to us, and we have these mysterious new layers to think about, like convolutional
    and pooling layers, so we are sure to be somewhat nebulous in our understanding
    right now. That is perfectly fine. We have new ideas and some visual indications
    of how they link together to make a CNN. For now, this is all we need. The remainder
    of this chapter will, I hope, be a series of “aha!” moments for you as you think
    back to this figure and its parts. When you understand what each is doing, you’ll
    start to see why they are where they are in the processing chain, leading from
    image input to output softmax predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If our discussion of convolution ended with the preceding sections, we’d understand
    the essential operation but still be in the dark about exactly *how* a convolutional
    layer in a CNN works. Bearing this in mind, let’s look at how the convolution
    idea generalizes across the inputs and outputs of a CNN’s convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: How a Convolution Layer Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input and output of a convolutional layer can both be thought of as stacks
    of 2D arrays (or matrices). The operation of the convolutional layer is best illustrated
    with a simple example showing how to map the input stack of arrays to the output
    stack.
  prefs: []
  type: TYPE_NORMAL
- en: Before we present our example, we need to introduce some terminology. We previously
    described the convolution operation in terms of applying a kernel to an input,
    both of which are 2D. We’ll continue to use the term *kernel* for this single,
    2D matrix. When implementing a convolutional layer, however, we’ll soon see that
    we need stacks of kernels, which are typically referred to in machine learning
    as *filters*. A filter is a stack of kernels. The filter, via its kernels, is
    applied over the input stack to produce the output stack. Since during training
    the model is learning kernels, it is fair to say that the model is also learning
    filters.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, the input is a stack of two 5 × 5 arrays, the kernel size is
    3 × 3, and we want an output stack that is three deep. Why three? Because, as
    the designer of the CNN architecture, we believe that learning three outputs will
    help the network learn the task at hand. The convolution operation determines
    the width and height of each output array; we select the depth. We’ll use valid
    convolution, losing a border of thickness one on the output, meaning our input
    will drop two in width and height. Therefore, a 5 × 5 input convolved with a 3
    × 3 kernel will create a 3 × 3 output.
  prefs: []
  type: TYPE_NORMAL
- en: That accounts for the change in dimension, but how do we go from a stack of
    two arrays to a stack of three? The key to mapping the 5 × 5 × 2 input to the
    desired 3 × 3 × 3 output is the set of kernels, the filter, learned during training.
    Let’s see how the filter gives us the mapping we want.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll assume we already know the filters at this point, each of which is a 3
    × 3 × 2 stack of kernels. In general, if there are *M* arrays in the input stack
    and we want *N* arrays in the output stack using a kernel that is *K* × *K*, then
    we need a set of *N* filters, each one of which is a stack of *K* × *K* kernels
    *M* deep. Let’s explore why.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we break up the stack so we can see each element clearly, our input stack
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/293equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have two 5 × 5 matrices labeled 0 and 1\. The values were selected at random.
  prefs: []
  type: TYPE_NORMAL
- en: To get an output stack of three, we need a set of three filters. The stack of
    kernels in each filter is two deep, to mirror the number of arrays in the input
    stack. The kernels themselves are 3 × 3, so we have three 3 × 3 × 2 filters, where
    we convolve each kernel in the filter with the corresponding input array. The
    three filters are
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/293equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we’ve added 0 and 1 labels to show which kernels are applied to which
    input stack arrays. We also have a bias vector, as we did for the traditional
    neural network layers. This is a vector, one value for each kernel stack, that
    we add in at the end to help align the output of the convolutional layer to the
    data, just as we did for the traditional neural network layers. The bias adds
    one more degree of freedom to the layer—one more thing that can be learned to
    help the layer learn the most it can from the data. For our example, the bias
    vector is *b* = *{*1,0,2*}*, selected at random.
  prefs: []
  type: TYPE_NORMAL
- en: To get the output stack, we convolve each kernel of each filter with the corresponding
    input array, sum the elements of the resulting output, and add the bias value.
    For filter *k*[0], we convolve the first input array with the first kernel to
    get
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/294equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note we’re using * to mean the full convolution operation, which is fairly
    standard. We repeat this operation for the second kernel in *k*[0], applying it
    to the second array of the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/294equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we sum the two convolution outputs and add in the bias value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/294equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This gives us the first output array, the application of filter *k*[0] to the
    input stack.
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process for filters *k*[1] and *k*[2] to get their outputs so
    that the final convolutional layer output for the given input is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/294equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we have written the stacked arrays side by side, a 3 × 3 × 3 output, as
    desired.
  prefs: []
  type: TYPE_NORMAL
- en: Our convolutional layer example mapped a 5 × 5 × 2 input to a 3 × 3 × 3 output.
    If we naïvely used a fully connected layer instead, we would need a weight matrix
    that has 50 × 27 = 1350 weights that need to be learned. In contrast, the convolutional
    layer used only 3 × 3 × 2 weights per filter and three filters for a total of
    54 weights, excluding bias values. This is a significant reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Convolutional Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The preceding example showed us how a convolutional layer works. Now let’s see
    the effect of one. Imagine that we’ve trained the network shown in [Figure 12-3](ch12.xhtml#ch12fig3),
    so we have the weights and biases we need to run unknown images through the network.
    (You’ll see how to train a CNN in Chapter Experiments with Keras and MNIST.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first layer of the network in [Figure 12-3](ch12.xhtml#ch12fig3) is a convolutional
    layer that maps a 28 × 28 × 1 input, the single-channel grayscale digit image,
    to a 26 × 26 × 32 output using a filter with 32 3 × 3 kernels. Therefore, we know
    that the weights between the input image and output fit in an array that is 3
    × 3 × 1 × 32: 3 × 3 for the kernel size, 1 for the number of input channels, and
    32 for the number of kernels in the filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, what do the 32 3 × 3 kernels of the filter actually look like?
    We can extract them from the trained model and print them as a set of 32 3 × 3
    matrices. Here are the first two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/295equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is nice, but not particularly helpful for building intuition about what
    the kernels do.
  prefs: []
  type: TYPE_NORMAL
- en: We can also visualize the kernels of a filter by converting the matrices to
    images. To get the kernels as images, we first note that all the kernel values
    happen to fit in the range [*–*0.5,+0.5], so if we add 0.5 to each kernel value,
    we’ve mapped the range to [0,1]. After this, multiplication by 255 converts the
    kernel values to byte values, the same values a grayscale image uses. Additionally,
    a value of 0 is now 127, which is a middle gray value.
  prefs: []
  type: TYPE_NORMAL
- en: After this conversion, the kernels can be shown as grayscale images, where negative
    kernel values are closer to black, and positive kernel values are closer to white.
    A final step is needed, however, because the mapped kernels are still only 3×3
    pixels. The last step is to upscale the 3 × 3 images to 64×64 pixels. We’ll upscale
    in two different ways. The first uses nearest-neighbor sampling to show the kernel
    in blocks. The second uses a Lanczos filter, which smooths the image, making it
    easier to see the orientation of the kernel. [Figure 12-4](ch12.xhtml#ch12fig4)
    shows the kernel images with the block versions on top and the smoothed versions
    on the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-4: The 32 learned kernels of the first convolutional layer (top).
    Smoothed versions to show the orientations more clearly (bottom).*'
  prefs: []
  type: TYPE_NORMAL
- en: These images represent the 32 kernels learned by the first convolutional layer
    of the model in [Figure 12-3](ch12.xhtml#ch12fig3). There is just enough detail
    in the images to hint that the kernels are selecting for structure in specific
    directions, just like the kernel that produced the image on the lower right of
    [Figure 12-2](ch12.xhtml#ch12fig2), which emphasized diagonal structures.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s turn our attention now to the effect of the kernels. What do the kernels
    do to an input MNIST image? We can run a sample MNIST image through the kernels
    by convolving each kernel with the sample, here a “3”, and following a process
    similar to the one that produced the preceding kernel images. The result is a
    set of 32 26 × 26 images, which we again upscale to 64 × 64 before displaying
    them. [Figure 12-5](ch12.xhtml#ch12fig5) shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-5: The 32 kernels applied to a sample MNIST input*'
  prefs: []
  type: TYPE_NORMAL
- en: The order of the kernels shown in [Figure 12-4](ch12.xhtml#ch12fig4) matches
    the images in [Figure 12-5](ch12.xhtml#ch12fig5). For example, the top-right image
    of [Figure 12-4](ch12.xhtml#ch12fig4) shows a kernel that is light on the upper
    left and dark on the lower right, meaning it will detect structures along the
    diagonal from lower left to upper right. The output from applying this kernel
    to the sample is the upper-right image of [Figure 12-5](ch12.xhtml#ch12fig5).
    We see that the kernel enhanced parts of the three that are primarily diagonal
    from the lower left to the upper right. Note, this example is easy to interpret
    because the input is a grayscale image with a single channel. This means that
    there is no summing of kernel outputs across channels as we previously saw for
    the more general operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the first convolutional layer of a CNN learns kernels that select
    for specific orientations, textures, or, if the input image is RGB, colors. For
    the grayscale MNIST images, orientation is most important. The kernels learned
    at higher convolutional layers in the CNN are also selecting for things, but the
    interpretation of *what* the kernel is selecting becomes more and more abstract
    and difficult to understand. It is worth noting that the kernels learned by a
    CNN’s first convolutional layer are very similar to the first layer of visual
    processing in the mammalian brain. This is the primary visual cortex or V1 layer
    that detects lines and edges. Additionally, always keep in mind that the set of
    convolutional and pooling layers are there to learn a new feature representation:
    a new representation of the input image. This new representation does a better
    job of separating classes so that the fully connected layers can more easily distinguish
    between them.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Convolutional Layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most CNNs have more than one convolutional layer. One reason for this is to
    build up features that are influenced by larger portions of the input as one goes
    deeper into the network. This introduces the ideas of *receptive field* and *effective
    receptive field*. The two concepts are similar and often confused. We can explain
    both by looking at [Figure 12-6](ch12.xhtml#ch12fig6).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-6: Receptive fields*'
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows the *output* of two convolutional layers and the input to the
    model. We’re showing only the relevant parts of the output, using a 3 × 3 kernel.
    We’re also ignoring the depth of the filters since the receptive fields (defined
    next) are the same across the depth of the convolutional layer outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-6](ch12.xhtml#ch12fig6) should be read right to left as the arrows
    indicate. This is the opposite direction to the flow of data through the network.
    Here, we are looking back to earlier layers to see what has influenced the output
    value at a higher layer. The squares are output values. The rightmost shaded square
    is one of the outputs of Conv[2]. This is our starting point for looking back
    to see what influences this value. The arrows point to the outputs of Conv[1]
    that influence the shaded value in Conv[2]. The value in Conv[2] then has a 3
    × 3 *receptive field* as it is directly influenced by the 3 × 3 shaded outputs
    of Conv[1]. This is how we’ll define *receptive field*: the set of outputs from
    the layer immediately before that directly influence the output of the current
    layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the set of input values that directly influence the 3 × 3 shaded
    region of Conv[1], we see a 5 × 5 region. This makes sense: each shaded output
    of Conv[1] has a receptive field that is a 3 × 3 region of the input. The receptive
    field is 3 × 3 because the kernels of Conv[1] are 3 × 3 kernels. They overlap
    so that the shaded 5 × 5 input region is what all the shaded Conv[1] outputs are
    influenced by.'
  prefs: []
  type: TYPE_NORMAL
- en: Look again at the rightmost shaded output value. If we trace back to the input
    all the values that can influence it, we see that the shaded 5 × 5 region of the
    input can affect its value. This region of the input is the *effective recep-
    tive field* for the rightmost shaded output of Conv[2]. This output value responds,
    ultimately, to what is happening in the input image in the leftmost shaded region.
    As the CNN gets deeper, with additional convolutional layers, we can see how the
    effective receptive field can change so that deeper convolutional layers are working
    with values ultimately derived from larger and larger portions of the input to
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a Convolutional Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [Chapter 9](ch09.xhtml#ch09), we saw that the performance of a traditional
    neural network was strongly influenced by the type of random initialization used
    for the learned weights and biases. The same is true for CNNs. Recall that the
    weights of a convolutional layer are the values of the kernels. They are learned
    during backprop, just like the weights of a traditional neural network. We need
    an intelligent way to initialize these values when we set up the network. Fortunately,
    the best initialization approaches for a traditional neural network apply directly
    to convolutional layers as well. For example, Keras defaults to Glorot initialization,
    which, as we saw in [Chapter 9](ch09.xhtml#ch09), is sometimes called Xavier initialization
    in other toolkits.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now from convolutional layers to pooling layers. These are simpler
    but perform an important, if somewhat controversial, function.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our favorite figure, [Figure 12-3](ch12.xhtml#ch12fig3), shows a pooling layer
    after the first two convolutional layers. This pooling layer takes an input stack
    of 24 × 24 × 64 and produces an output stack of 12 × 12 × 64\. The pooling part
    is marked as “2 × 2”. What’s going on here?
  prefs: []
  type: TYPE_NORMAL
- en: The key is the “2 × 2”. This means, for each of the 64 24 × 24 inputs, we move
    a 2 × 2 sliding window over the input and perform an operation similar to convolution.
    Not explicitly called out in [Figure 12-3](ch12.xhtml#ch12fig3) is that the stride
    is also 2 so that the sliding 2 × 2 window jumps by two to avoid overlapping itself.
    This is typically the case, but doesn’t need to be. Since the pooling operation
    is per input in the stack, the output leaves the stack size unchanged. This is
    contrary to what a convolutional layer often does.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the pooling operation applied to a single input in the stack,
    a 24 × 24 matrix. [Figure 12-7](ch12.xhtml#ch12fig7) shows us what’s going on.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-7: Applying 2 × 2 max pooling to an 8 × 8 input*'
  prefs: []
  type: TYPE_NORMAL
- en: The first 2 × 2 values are mapped to the first output value. Then we move over
    two and map the next 2 × 2 region to the output and so on until the entire input
    is mapped. The operation performed on each 2 × 2 region is up to the architect
    of the CNN. The most common operation is “select the largest value,” or *max pooling*,
    which is what we show in [Figure 12-7](ch12.xhtml#ch12fig7). This is also the
    operation the model in [Figure 12-3](ch12.xhtml#ch12fig3) is performing. Another
    fairly common pooling operation is to average the values.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from [Figure 12-7](ch12.xhtml#ch12fig7) that the 8 × 8 input matrix
    is mapped to a 4 × 4 output matrix. This explains why the output of the pooling
    layer in [Figure 12-3](ch12.xhtml#ch12fig3) is 12 × 12; each dimension is half
    the size of the input.
  prefs: []
  type: TYPE_NORMAL
- en: The pooling operation is straightforward but throws information away. So why
    do it at all? The primary motivation for pooling is to reduce the number of values
    in the network. Typically, as depth increases, the number of filters used by convolutional
    layers increases, by design. We see this for even the simple network of [Figure
    12-3](ch12.xhtml#ch12fig3), where the first convolutional layer has 32 filters,
    while the second has 64\. Therefore, the second convolutional layer outputs 24
    × 24 × 64 = 36,864 values, but after 2 × 2 pooling, there are only 12 × 12 × 64
    = 9,216 values to work with, a 75 percent reduction. It’s important to note that
    we’re talking about the number of values present as we move data through the network,
    not the number of learned parameters in the layers. The second convolutional layer
    in [Figure 12-3](ch12.xhtml#ch12fig3) has 3 × 3 × 32 × 64 = 18,432 learned parameters
    (ignoring bias values), while the pooling layer has no learned parameters.
  prefs: []
  type: TYPE_NORMAL
- en: This reduction in the number of values in the output, which is our representation
    of the input, speeds up computation and acts as a regularizer to guard against
    overfitting. The regularization techniques and rationales of [Chapter 9](ch09.xhtml#ch09)
    are equally valid for CNNs. However, since pooling throws information away and
    selects proxies to represent entire regions of the representation (the convolutional
    layer outputs), it alters the spatial relationship between parts of the input.
    This loss of spatial relationships might be critical for some applications and
    has motivated people like Geoffrey Hinton to eliminate pooling by introducing
    other types of networks (search for “capsule networks”).
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, Hinton said the following regarding pooling layers in response
    to a question on Reddit asking for his most controversial opinion on machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster. If the pools do not overlap,
    pooling loses valuable information about where things are. We need this information
    to detect precise relationships between the parts of an object.
  prefs: []
  type: TYPE_NORMAL
- en: He elaborates further in the answer, pointing out that allowing pooling operations
    to overlap does preserve some of the spatial relationships in a crude way. An
    overlapping pooling operation might be to use a 2 × 2 window as we used in [Figure
    12-7](ch12.xhtml#ch12fig7), but use a stride of 1 instead of 2.
  prefs: []
  type: TYPE_NORMAL
- en: Concerns aside, pooling layers are an essential part of CNNs as presently implemented,
    but be careful when adding them to a model. Let’s move on now to the top layers
    of a CNN, the fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second row of [Figure 12-3](ch12.xhtml#ch12fig3), all the layers starting
    with *Flatten* form the fully connected layer of the model. The figure uses Keras
    terminology; many people call the *Dense* layer the fully connected layer and
    assume there is a Flatten operation as part of it along with the activation (ReLU)
    and optional dropout before the softmax layer. Therefore, the model in [Figure
    12-3](ch12.xhtml#ch12fig3) has only one fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously stated that the net effect of the convolutional and pooling layers
    is to change the representation of the input feature (the image, say) into one
    that makes it easier for a model to reason about. During training, we are asking
    the network to learn a different, often more compact, representation of the input
    to help the model perform better on unseen inputs. For the model in [Figure 12-3](ch12.xhtml#ch12fig3),
    all the layers up to and including the pooling layer (and the dropout layer after
    it for training) are there to learn a new representation of the input image. In
    this case, the fully connected layer is the model: it will take that new representation
    and ultimately make a classification based on it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fully connected layers are just that, fully connected. The weights between
    the flattened final pooling layer of 9,216 elements for [Figure 12-3](ch12.xhtml#ch12fig3)
    (12 × 12 × 64 = 9,216) and the Dense layer of 128 elements are the same as if
    we were building a traditional neural network. This means that there are 9,216
    × 128 = 1,179,648 weights plus an additional 128 bias values that need to be learned
    during training. Therefore, of the 1,199,882 parameters (weights and biases) in
    the model of [Figure 12-3](ch12.xhtml#ch12fig3), 98.3 percent of them are in the
    transition between the final pooling layer and the fully connected layer. This
    illustrates an important point: fully connected layers are *expensive* in terms
    of parameters that need to be learned, just as they are for traditional neural
    networks. Ideally, if the feature learning layers, the convolutional and pooling
    layers, are doing their job well, we might expect to need only one or two fully
    connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fully connected layers have another disadvantage, besides memory use, that
    can impact their utility. To see what this disadvantage is, consider the following
    scenario: you want to be able to locate digits in grayscale images. Assume for
    simplicity that the background is black. If you use the model of [Figure 12-3](ch12.xhtml#ch12fig3)
    trained on MNIST digits, you will have a model that is very good at identifying
    digits centered in 28×28 pixel images, but what if the input images are large
    and you do not know where the digits are in the image, let alone how many digits
    there are? Then things get a little more interesting. The model of [Figure 12-3](ch12.xhtml#ch12fig3)
    expects input images that are 28×28 pixels in size and only that size. In [Chapter
    13](ch13.xhtml#ch13), we will work through this problem in detail as an experiment,
    but for now, let’s discuss fully convolutional layers, a possible solution to
    this disadvantage of using fully connected layers in CNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last section, I said that the model of [Figure 12-3](ch12.xhtml#ch12fig3)
    expects input images that are 28×28 pixels in size and only that size. Let’s see
    why.
  prefs: []
  type: TYPE_NORMAL
- en: There are many kinds of layers in this model. Some, like the ReLU and dropout
    layers, have no impact on the dimensionality of the data flowing through the network.
    The same cannot be said of the convolutional, pooling, and fully connected layers.
    Let’s look at these layers one by one to see how they are tied to the dimensionality
    of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layers implement convolutions. By definition, a convolution
    involves moving a fixed-size kernel over some input image (thinking purely 2D
    here). Nothing in that operation specifies the size of the input image. The output
    of the first convolutional layer in [Figure 12-3](ch12.xhtml#ch12fig3) is 26 ×
    26 × 32\. The 32 comes from the number of filters selected by the architecture.
    The 26 × 26 comes from using a 3 × 3 convolution kernel on a 28 × 28 input with
    no padding. If the input image were instead 64×64 pixels, the output of this layer
    would be 62 × 62 × 32, and we wouldn’t need to do anything to alter the architecture
    of the network. The convolutional layers of a CNN are agnostic to the spatial
    dimensions of their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pooling layer in [Figure 12-3](ch12.xhtml#ch12fig3) takes a 24 × 24 × 64
    input and produces a 12 × 12 × 64 output. As we previously saw, the pooling operation
    is much like the convolution operation: it slides a fixed size window over the
    input, spatially, and produces an output; in this case, the output is half the
    dimensionality of the input while leaving the depth the same. Again, nothing in
    this operation fixes the spatial dimensions of the input stack. If the input stack
    were 32 × 32 × 64, the output of this max pooling operation would be 16 × 16 ×
    64 without a change needed to the architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the fully connected layer that maps the 12 × 12 × 64 = 9,216
    pooling output to a 128 element fully connected (Dense) layer. As we saw in [Chapter
    8](ch08.xhtml#ch08), fully connected neural networks use matrices of weights between
    layers in their implementation. There are 9,216 elements in the output of the
    pooling layer and a fixed 128 in the dense layer, so we need a matrix that is
    9,216 × 128 elements. This size *is* fixed. If we use the network with a larger,
    say 32 × 32, input image, by the time we get through the pooling layer, the output
    size will be 14 × 14 × 64 = 12,544, which would require an existing 12,544 × 128
    weight matrix to map to the fully connected layer. Of course, this won’t work;
    we trained a network that uses a 9,216 × 128 matrix. The fully connected layers
    of a CNN fix the input size of the CNN. If we could get around this, we could
    apply inputs of any size to the CNN, assuming memory allows.
  prefs: []
  type: TYPE_NORMAL
- en: We could, naïvely, simply slide a 28 × 28 window over the larger input image,
    run each 28×28 pixel image through the model as we trained it, and output a larger
    map, where each pixel now has a probability of that digit being present. There
    are 10 digits, so we would have 10 output maps. This sliding window approach certainly
    works, but it’s very computationally expensive, as many simplistic implementations
    of algorithms often are.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, we can do better by converting the fully connected layer
    into an equivalent convolutional layer to make the model a *fully convolutional
    network*. In a fully convolutional network, there are no fully connected layers,
    and we’re not restricted to using a fixed input size. The relationship between
    input size and the output of the network when it is fully convolutional is something
    we will see in [Chapter 13](ch13.xhtml#ch13), but the essential operation is to
    look at the size of the last standard convolutional or pooling layer and replace
    the fully connected layer that follows with a convolutional layer using a kernel
    of the same size.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 12-3](ch12.xhtml#ch12fig3), the output of the pooling layer is 12
    × 12 × 64\. Therefore, instead of the 128-element fully connected layer that we
    saw fixes our input size, we can mathematically get the same calculation by changing
    the fully connected layer into a 12 × 12 × 128 convolutional layer. Convolving
    a 12 × 12 kernel over a 12 × 12 input produces a single number. Therefore, the
    output of the 12 × 12 × 128 convolutional layer will be a 1 × 1 × 128 array, which
    is functionally the same as the 128 outputs of the fully connected layer that
    we originally used. Additionally, the convolution operation between a 12 × 12
    kernel and a 12 × 12 input is to simply multiply the kernel values by the input
    values, element by element, and sum them. This is what a fully connected layer
    does for each of its nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We do not save anything in terms of the number of parameters when using a convolutional
    layer this way. We can see this from [Figure 12-3](ch12.xhtml#ch12fig3). The 9,216
    elements of the pooling layer output times the 128 nodes of the fully connected
    layer means we have 9,216 × 128 = 1,179,648 weights + 128 bias terms needed for
    both the fully connected and fully convolutional layers. When moving to the 12
    × 12 × 128 convolutional layer, we have 12 × 12 × 64 × 128 = 1,179,648 weights
    to learn, the same as before. However, now we also have the freedom to change
    the input size, as the 12 × 12 × 128 convolutional layer will automatically convolve
    over any larger input, giving us outputs that represent the application of the
    network to 28 × 28 regions of the input with a stride determined by the specific
    architecture of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fully convolutional networks stem from the 2014 paper by Long, Shelhamer, and
    Darrell, “Fully Convolutional Networks for Semantic Segmentation,” which has been
    referenced over 19,000 times as of this writing. The phrase *semantic segmentation*
    refers to assigning a class label to each pixel of the input image. Currently,
    the go-to architecture for semantic segmentation is the U-Net (see “U-Net: Convolutional
    Networks for Biomedical Image Segmentation” by Ronneberger, Fischer, and Brox,
    2015) which has seen widespread success, especially in medical domains.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed the primary CNN layers, those found in [Figure 12-3](ch12.xhtml#ch12fig3).
    There are many more that we could cover, but they are generally beyond what we
    want to present at this level, with one exception, batch normalization, which
    we’ll experiment with in [Chapter 15](ch15.xhtml#ch15). New layer types are being
    added all the time in response to active research projects. However, in the end,
    the core includes the layers we have discussed in this chapter. Let’s move on
    now and see how a trained CNN processes unknown inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Step by Step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we discussed the architecture and layers of our sample
    CNN, [Figure 12-3](ch12.xhtml#ch12fig3). In this section, we will illustrate the
    operation of the network to see how it responds to two new inputs, one a “4” and
    the other a “6”. We assume the network is fully trained; we’ll train for real
    it in [Chapter 13](ch13.xhtml#ch13).
  prefs: []
  type: TYPE_NORMAL
- en: The input image is passed through the model layer by layer
  prefs: []
  type: TYPE_NORMAL
- en: input → conv[0] → conv[1] → pool → dense → softmax
  prefs: []
  type: TYPE_NORMAL
- en: using the trained weights and biases to calculate outputs for each layer. We
    will refer to these as the *activations*. The output of the first convolutional
    layer is a stack of 32 26 × 26 images, the response of the input image to each
    of the 32 kernels. This stack then passes to the second convolutional layer to
    produce 64 24 × 24 outputs. Note, between the two convolutional layers is a ReLU
    operation that clips the output so that anything that would have been negative
    is now 0\. Doing this adds a nonlinearity to the data as it flows through the
    network. Without this nonlinearity, the net effect of the two convolutional layers
    is to act like a single convolutional layer. With the nonlinearity imposed by
    the ReLU, we enable the two convolutional layers to learn different things about
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: The second ReLU operation makes the stack of 64 24 × 24 outputs 0 or positive.
    Next, a 2 × 2 max pooling operation reduces the 64 outputs to 12 × 12 in size.
    After this, a standard fully connected layer produces 128 output values as a vector
    from the 9,216 values in the stack of 12 × 12 activations. From this, a set of
    10 outputs, one for each digit, is calculated via a softmax. These are the output
    values of the network representing the network’s confidence as to which class
    label should be assigned to the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can illustrate the activations by displaying the output images: either 26
    × 26 for the first convolutional layer, 24 × 24 for the second convolutional layer,
    or 12 × 12 for the pooling layer. To show the activations from the fully connected
    layer, we can make an image of 128 bars, where the intensity of each bar represents
    the vector value. [Figure 12-8](ch12.xhtml#ch12fig8) shows the activations for
    our two sample digits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-8: Model activations per layer. The output is inverted: darker implies
    stronger activation.*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the images are inverted so that darker corresponds to stronger activation
    values. We are not showing the softmax outputs. These values are
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.99 | 0.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: indicating that in both cases, the model is very confident of the class label
    that should be assigned and that it was, in fact, correct.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at [Figure 12-8](ch12.xhtml#ch12fig8), we see that the output of
    the first convolutional layer is simply the response of the single input image
    (grayscale) and the kernels of the layer. This hearkens back to [Figure 12-2](ch12.xhtml#ch12fig2),
    where we saw that convolution could be used to highlight aspects of the input
    image. After the ReLU operation, the responses of the 64 filters of the second
    convolutional layer, each a stack of 32 kernels, seems to be picking out different
    portions or strokes in the input images. These can be thought of as a set of smaller
    components from which the input is constructed. The second ReLU and pooling operation
    preserve much of the structure of the second convolutional layer outputs, but
    reduce the size to one quarter what it was previously. Finally, the output of
    the fully connected layer shows the pattern derived from the input image, the
    new representation that we expect to be easier to classify than the raw image
    input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dense layer outputs of [Figure 12-8](ch12.xhtml#ch12fig8) are different
    from each other. This begs the question: what do these outputs look like for several
    instances of four and six digits? Is there something in common that we can see,
    even in these values? We might expect that there is because we know this network
    has been trained and has achieved a very high accuracy of over 99 percent on the
    test set. Let’s take a look at running ten “4” and ten “6” images from the test
    set through the network and compare the dense layer activations. This gives us
    [Figure 12-9](ch12.xhtml#ch12fig9).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-9: Dense layer activations for ten instances of 4 and 6\. The output
    is inverted: darker implies stronger activation.*'
  prefs: []
  type: TYPE_NORMAL
- en: On the left, we see the actual input to the model. On the right is the representation
    of the 128 outputs in the fully connected layer, the one that feeds into the softmax.
    Each digit has a particular pattern that is common to each one of the digits.
    However, there are also variations. The middle “4” has a very short stem, and
    we see that its representation in the fully connected layer is also different
    from all the other examples. Still, this digit was successfully called a “4” by
    the model with a certainty of 0.999936.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-9](ch12.xhtml#ch12fig9) provides evidence that the model learned
    what we wanted it to learn in terms of representation of the input. The softmax
    layer maps the 128 elements of the dense layer to 10, the output nodes from which
    the softmax probabilities are calculated. This is, in effect, a simple traditional
    neural network with no hidden layers. This simpler model succeeds in correctly
    labeling the images because the new representation of the inputs does a much better
    job of separating the classes so that even a simple model can make solid predictions.
    It also succeeds because the training process jointly optimizes both the weights
    of this top layer model and the weights of the lower layers that generate the
    input to the model at the same time, so they reinforce each other. Sometimes you
    will see this referred to in the literature at *end-to-end* training.'
  prefs: []
  type: TYPE_NORMAL
- en: We can demonstrate the claim that the features are better separated by looking
    at a plot of the dense layer activations for the MNIST test data. Of course, we
    can’t look at the actual plot, as I have no idea how to visualize a plot in 128
    dimensions, but all is not lost. The machine learning community has created a
    powerful visualization tool called *t-SNE*, which, fortunately for us, is part
    of sklearn. This algorithm intelligently maps high-dimensional spaces to lower-dimensional
    spaces, including 2D. If we run a thousand randomly selected MNIST test images
    through the model and then run the resulting 128-dimension dense layer activations
    through t-SNE, we can produce a 2D plot where the separation between classes reflects
    the actual separation in the 128-dimensional space. [Figure 12-10](ch12.xhtml#ch12fig10)
    is the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/12fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12-10: How well the model separates test samples by class (t-SNE plot)*'
  prefs: []
  type: TYPE_NORMAL
- en: In this plot, each class uses a different plot symbol. If the model did not
    correctly classify the sample, it is shown as a larger star. In this case, only
    a handful of samples were misclassified. The separation by class type is very
    evident; the model has learned a representation that makes it straightforward
    to decide on the correct class label in most cases. We can readily count 10 different
    blobs in the t-SNE plot.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we introduced the major components of convolutional neural
    networks. These are workhorse networks for modern deep learning, especially for
    vision tasks because of their ability to learn from spatial relationships. We
    worked through a model to classify MNIST digits and detailed new processing layers,
    including convolutional layers and pooling layers. We then learned that the fully
    connected layers of a CNN are analogs of the traditional neural networks we learned
    about in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we saw how to modify the fully connected layers to enable operation on
    larger inputs. Finally, we looked at the activations generated by the network
    when a sample image was passed through and saw how the convolution and pooling
    layers worked together to produce a new representation of the input, one that
    helped to separate the classes in the feature space, thereby enabling high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll continue our look at CNNs, but instead of theory,
    we’ll work with actual examples to see how the various parameters of the network,
    and the hyperparameters used during training, affect model performance. This will
    help us build intuition about how to work with CNNs in the future.
  prefs: []
  type: TYPE_NORMAL
