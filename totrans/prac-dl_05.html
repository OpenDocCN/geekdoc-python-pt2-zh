<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_83"/><strong><span class="big">5</span><br/>BUILDING DATASETS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">The previous chapter had a lot of detailed advice. Now let’s put it all into practice to build the datasets we’ll use throughout the remainder of the book. Some of these datasets are well suited to traditional models, because they consist of feature vectors. Others are better suited to deep learning models that work with multidimensional inputs—in particular, images, or things that can be visualized as images.</p>&#13;
<p class="indent">We’ll work through acquiring the raw data and preprocessing the data to make it suitable for our tools. We won’t make actual training/validation/test splits until we use these datasets for specific models. It is worth noting here that preprocessing the data to make it suitable for a model is often one of the most labor-intensive of machine learning tasks. All the same, if it is not done, or not done well, your model may end up being far less useful than you want it to be.</p>&#13;
<h3 class="h3" id="lev1_32"><span epub:type="pagebreak" id="page_84"/>Irises</h3>&#13;
<p class="noindent">Perhaps the most classic of all machine learning datasets is the iris flower dataset, developed in 1936 by R. A. Fisher in his paper, “The Use of Multiple Measurements in Taxonomic Problems.” It’s a small dataset of three classes with 50 samples in each class. There are four features: sepal width, sepal length, petal width, and petal length, all in centimeters. The three classes are <em>I. setosa</em>, <em>I. versicolour</em>, and <em>I. virginica</em>. This dataset is built into sklearn, but we’ll instead download it from the University of California, Irvine, Machine Learning Repository to practice working with externally sourced data and introduce a rich collection of datasets suitable for many traditional machine learning models. The main repository is located at <em><a href="https://archive.ics.uci.edu/ml/index.php">https://archive.ics.uci.edu/ml/index.php</a></em>, but you can download the irises dataset directly from <em><a href="https://archive.ics.uci.edu/ml/datasets/iris/">https://archive.ics.uci.edu/ml/datasets/iris/</a></em>.</p>&#13;
<p class="indent">At the time of this writing, this dataset has been downloaded nearly 1.8 million times. You can download it by selecting the <strong>Data Folder</strong> link near the top of the page, then right-clicking and saving the <em>iris.data</em> file, ideally to a new directory called <em>iris</em>. Let’s take a look at the start of this file:</p>&#13;
<pre>5.1,3.5,1.4,0.2,Iris-setosa<br/>&#13;
4.9,3.0,1.4,0.2,Iris-setosa<br/>&#13;
4.7,3.2,1.3,0.2,Iris-setosa<br/>&#13;
4.6,3.1,1.5,0.2,Iris-setosa<br/>&#13;
5.0,3.6,1.4,0.2,Iris-setosa<br/>&#13;
5.4,3.9,1.7,0.4,Iris-setosa<br/>&#13;
4.6,3.4,1.4,0.3,Iris-setosa</pre>&#13;
<p class="indent">Because the class names at the end of each line are all the same, we should immediately suspect that the samples are sorted by class. Looking at the rest of the file confirms this. So, as emphasized in <a href="ch04.xhtml#ch04">Chapter 4</a>, we must be sure to randomize the data before training a model. Also, we need to replace the class names with integer labels. We can load the dataset into Python with the script in <a href="ch05.xhtml#ch5lis1">Listing 5-1</a>.</p>&#13;
<p class="programs" id="ch5lis1">  import numpy as np<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> with open("iris.data") as f:<br/>&#13;
       lines = [i[:-1] for i in f.readlines()]<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> n = ["Iris-setosa","Iris-versicolor","Iris-virginica"]<br/>&#13;
   x = [n.index(i.split(",")[-1]) for i in lines if i != ""]<br/>&#13;
   x = np.array(x, dtype="uint8")<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> y = [[float(j) for j in i.split(",")[:-1]] for i in lines if i != ""]<br/>&#13;
   y = np.array(y)<br/>&#13;
<br/>&#13;
<span class="ent">❹</span> i = np.argsort(np.random.random(x.shape[0]))<br/>&#13;
   x = x[i]<br/>&#13;
   y = y[i]<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_85"/><span class="ent">❺</span> np.save("iris_features.npy", y)<br/>&#13;
   np.save("iris_labels.npy", x)</p>&#13;
<p class="figcap"><em>Listing 5-1: Loading the raw iris dataset and mapping to our standard format</em></p>&#13;
<p class="indent">First, we load the text file containing the data. The list comprehension removes the extraneous newline character <span class="ent">❶</span>. Next, we create the vector of labels by converting the text label into an integer, 0–2. The last element in the list, created by splitting a line along commas, is the text label. We want NumPy arrays, so we turn the list into one. The <code>uint8</code> is unnecessary, but since the labels are never negative and they’re never larger than 2, we save a bit of space by making the data type an unsigned 8-bit integer <span class="ent">❷</span>.</p>&#13;
<p class="indent">Creating the feature vectors as a 150-row by 4-column matrix comes next via a double list comprehension. The outer comprehension (<code>i</code>) moves over lines from the file, and the inner one (<code>j</code>) takes the list of measurements for each sample and turns them into floating-point numbers. We then convert the list of lists into a 2D NumPy array <span class="ent">❸</span>. We finish by randomizing the dataset as we did previously <span class="ent">❹</span>, and, finally, we write the NumPy arrays to disk so we can use them later <span class="ent">❺</span>.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig1">Figure 5-1</a> shows a box plot of the features. This is a well-behaved dataset, but the second feature does have some possible outliers. Because the features all have similar scales, we’ll use the features as they are.</p>&#13;
<div class="image" id="ch5fig1"><img src="Images/05fig01.jpg" alt="image" width="679" height="539"/></div>&#13;
<p class="figcap"><em>Figure 5-1: Box plot of the four iris dataset features</em></p>&#13;
<h3 class="h3" id="lev1_33"><span epub:type="pagebreak" id="page_86"/>Breast Cancer</h3>&#13;
<p class="noindent">Our second dataset, the Wisconsin Diagnostic Breast Cancer dataset, is also in sklearn, and you can also download it from the UCI Machine Learning Repository. We’ll follow the preceding procedure and download the dataset to see how to process it. This seems unnecessary, true, but just as it’s crucial to build a good dataset to have any hope of training a good model, it’s equally important to learn how to work with data sources that are not in the format we want. Should you one day decide to make machine learning and data science a career, you’ll be faced with this issue on a near-daily basis.</p>&#13;
<p class="indent">Download the dataset by going to <em><a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)/</a></em>. Then, click the <strong>Data Folder</strong> link, and save the <em>wdbc.data</em> file.</p>&#13;
<p class="indent">This dataset contains cell measurements taken from slides of fine-needle biopsies of breast masses. There are 30 continuous features and two classes: malignant (cancer, 212 samples) and benign (no cancer, 357 samples). This is also a popular dataset, with over 670,000 downloads. The first line of the file is shown here:</p>&#13;
<pre>842302,M,17.99,10.38,122.8,1001,0.1184, ...</pre>&#13;
<p class="indent">The first element in that line is a patient ID number that we don’t need to worry about. The second element is the label—<em>M</em> for malignant, and <em>B</em> for benign. The rest of the numbers in the line are 30 measurements related to cell size. The features themselves are of different scales, so besides creating the raw dataset, we’ll also create a standardized version. As this is the entirety of the dataset and we’ll have to hold some of it back for testing, we don’t need to record the per feature means and standard deviations in this case. If we were able to acquire more data generated in the same way, perhaps from an old file that was forgotten about, we would need to keep these values so that we could standardize the new inputs. The script to build this dataset, and to generate a summary box plot, is in <a href="ch05.xhtml#ch5lis2">Listing 5-2</a>.</p>&#13;
<p class="programs" id="ch5lis2">  import numpy as np<br/>&#13;
  import matplotlib.pyplot as plt<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> with open("wdbc.data") as f:<br/>&#13;
       lines = [i[:-1] for i in f.readlines() if i != ""]<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> n = ["B","M"]<br/>&#13;
   x = np.array([n.index(i.split(",")[1]) for i in lines],dtype="uint8")<br/>&#13;
   y = np.array([[float(j) for j in i.split(",")[2:]] for i in lines])<br/>&#13;
   i = np.argsort(np.random.random(x.shape[0]))<br/>&#13;
   x = x[i]<br/>&#13;
   y = y[i]<br/>&#13;
   z = (y - y.mean(axis=0)) / y.std(axis=0)<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> np.save("bc_features.npy", y)<br/>&#13;
   np.save("bc_features_standard.npy", z)<br/>&#13;
<span epub:type="pagebreak" id="page_87"/>   np.save("bc_labels.npy", x)<br/>&#13;
   plt.boxplot(z)<br/>&#13;
   plt.show()</p>&#13;
<p class="figcap"><em>Listing 5-2: Loading the raw breast cancer dataset</em></p>&#13;
<p class="indent">The first thing we do is read in the raw text data <span class="ent">❶</span>. We then extract each label and map it to 0 for benign and 1 for malignant. Note here that we used 1 for the natural target case, so that a model outputting a probability value is indicating likelihood of finding cancer <span class="ent">❷</span>. We extract the 30 features per sample as floats using a nested list comprehension to first pull out the text of the features (<code>i</code>) and then map them to floats (<code>j</code>). This produces a nested list, which NumPy conveniently converts into a matrix of 569 rows and 30 columns.</p>&#13;
<p class="indent">Next, we randomize the dataset and calculate the standardized version by subtracting, per feature, the mean value of that feature and dividing by the standard deviation. We’ll work with this version and examine it in the box plot of <a href="ch05.xhtml#ch5fig2">Figure 5-2</a> <span class="ent">❸</span>, which shows all 30 features after standardization.</p>&#13;
<div class="image" id="ch5fig2"><img src="Images/05fig02.jpg" alt="image" width="678" height="532"/></div>&#13;
<p class="figcap"><em>Figure 5-2: Box plot of the 30 breast cancer dataset features</em></p>&#13;
<p class="indent">We don’t need to know in this case what the features represent. We’ll work with the dataset under the assumption that the selected features are sufficient to the task of determining malignancy. Our models will indicate to us whether or not this is the case. The features are now all of the same scale as we can see by the location of the boxes on the y-axis: they’re all covering basically the same range. One characteristic of the data is immediately <span epub:type="pagebreak" id="page_88"/>evident—namely, that there are many apparent outliers, as called out by the interquartile range (see <a href="ch04.xhtml#ch4fig6">Figure 4-6</a>). These aren’t necessarily bad values, but they are an indicator that the data isn’t normally distributed—it doesn’t, per feature, follow a bell-curve-type distribution.</p>&#13;
<h3 class="h3" id="lev1_34">MNIST Digits</h3>&#13;
<p class="noindent">Our next dataset isn’t typically composed of feature vectors, but is instead made up of thousands of small images of handwritten digits. This dataset is the workhorse of modern machine learning, and one of the first datasets deep learning researchers go to when looking to test new ideas. It’s overused, but that’s because it’s so well understood and simple to work with.</p>&#13;
<p class="indent">The dataset has a long history, but the version we’ll use, the most common version, is known simply as the <em>MNIST dataset</em>. The canonical source for the dataset, <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>, includes some background material. To save time, we’ll use Keras to download and format the dataset.</p>&#13;
<p class="indent">Keras will return the dataset as 3D NumPy arrays. The first dimension is the number of images—60,000 for training and 10,000 for test. The second and third dimensions are the pixels of the images. The images are 28×28 pixels in size. Each pixel is an unsigned 8-bit integer, [0,255].</p>&#13;
<p class="indent">Because we want to work with models that expect vectors as inputs, and because we want to use this dataset to illustrate certain properties of models later in the book, we’ll create additional datasets from this initial one. To do so, first we’ll unravel the images to form feature vectors so that we can use this dataset with traditional models that expect vector inputs. Second, we’ll use images, but we’ll permute the order of the images in the dataset. We’ll permute the order of the pixels of each image in the same way, so while the pixels will no longer be in the order that produces the digit image, the reordering will be deterministic, and applied consistently across all images. Third, we’ll create an unraveled feature vector version of these permuted images. We’ll use these additional datasets to explore differences between traditional neural networks and convolutional neural network models.</p>&#13;
<p class="indent">Use <a href="ch05.xhtml#ch5lis3">Listing 5-3</a> to build the dataset files.</p>&#13;
<p class="programs" id="ch5lis3">   import numpy as np<br/>&#13;
   import keras<br/>&#13;
   from keras.datasets import mnist<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> (xtrn, ytrn), (xtst, ytst) = mnist.load_data()<br/>&#13;
   idx = np.argsort(np.random.random(ytrn.shape[0]))<br/>&#13;
   xtrn = xtrn[idx]<br/>&#13;
   ytrn = ytrn[idx]<br/>&#13;
   idx = np.argsort(np.random.random(ytst.shape[0]))<br/>&#13;
   xtst = xtst[idx]<br/>&#13;
   ytst = ytst[idx]<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_89"/>   np.save("mnist_train_images.npy", xtrn)<br/>&#13;
   np.save("mnist_train_labels.npy", ytrn)<br/>&#13;
   np.save("mnist_test_images.npy", xtst)<br/>&#13;
   np.save("mnist_test_labels.npy", ytst)<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> xtrnv = xtrn.reshape((60000,28*28))<br/>&#13;
   xtstv = xtst.reshape((10000,28*28))<br/>&#13;
   np.save("mnist_train_vectors.npy", xtrnv)<br/>&#13;
   np.save("mnist_test_vectors.npy", xtstv)<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> idx = np.argsort(np.random.random(28*28))<br/>&#13;
   for i in range(60000):<br/>&#13;
       xtrnv[i,:] = xtrnv[i,idx]<br/>&#13;
   for i in range(10000):<br/>&#13;
       xtstv[i,:] = xtstv[i,idx]<br/>&#13;
   np.save("mnist_train_scrambled_vectors.npy", xtrnv)<br/>&#13;
   np.save("mnist_test_scrambled_vectors.npy", xtstv)<br/>&#13;
<br/>&#13;
<span class="ent">❹</span> t = np.zeros((60000,28,28))<br/>&#13;
   for i in range(60000):<br/>&#13;
       t[i,:,:] = xtrnv[i,:].reshape((28,28))<br/>&#13;
   np.save("mnist_train_scrambled_images.npy", t)<br/>&#13;
   t = np.zeros((10000,28,28))<br/>&#13;
   for i in range(10000):<br/>&#13;
       t[i,:,:] = xtstv[i,:].reshape((28,28))<br/>&#13;
   np.save("mnist_test_scrambled_images.npy", t)</p>&#13;
<p class="figcap"><em>Listing 5-3: Loading and building the various MNIST datasets</em></p>&#13;
<p class="indent">We start by telling Keras to load the MNIST dataset <span class="ent">❶</span>. When run for the first time, Keras will show a message about downloading the dataset. After that, it won’t show the message again.</p>&#13;
<p class="indent">The dataset itself is stored in four NumPy arrays. The first, <code>xtrn</code>, has a shape of (60000, 28, 28) for the 60,000 training images, each 28×28 pixels. The associated labels are in <code>ytrn</code> as integers, [0,9]. The 10,000 test images are in <code>xtst</code> with labels in <code>ytst</code>. We also randomize the order of the samples and write the arrays to disk for future use.</p>&#13;
<p class="indent">Next, we unravel the training and test images and turn them into vectors of 784 elements <span class="ent">❷</span>. Unraveling takes the first row of pixels followed by the second row and so on until all rows are laid end to end. We get 784 elements because 28 × 28 = 784.</p>&#13;
<p class="indent">Following this, we generate a permutation of the 784 elements in the unraveled vectors (<code>idx</code>) <span class="ent">❸</span>.</p>&#13;
<p class="indent">We use the permuted vectors to form new, scrambled, digit images and store them on disk <span class="ent">❹</span>. The scrambled images are made from the scrambled vectors by undoing the unravel operation. In NumPy, this is just a call to the <code>reshape</code> method of the vector arrays. Note that at no time do we alter the relative ordering of the images, so we need to store only one file each for the train and test labels.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig3">Figure 5-3</a> shows representative digits from the MNIST dataset.</p>&#13;
<div class="image" id="ch5fig3"><span epub:type="pagebreak" id="page_90"/><img src="Images/05fig03.jpg" alt="image" width="414" height="415"/></div>&#13;
<p class="figcap"><em>Figure 5-3: Representative MNIST digit images</em></p>&#13;
<p class="indent">We don’t need to standardize the images, as we know they’re all on the same scale already, since they’re pixels. We’ll sometimes scale them as we use them, but for now we can leave them on disk as byte grayscale images. The dataset is reasonably balanced; <a href="ch05.xhtml#ch5tab1">Table 5-1</a> shows the training distribution. Therefore, we don’t need to worry about imbalanced data.</p>&#13;
<p class="tabcap" id="ch5tab1"><strong>Table 5-1:</strong> Digit Frequencies for the MNIST Training Set</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Digit</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Count</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,923</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,742</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,958</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,131</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,842</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,421</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,918</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,265</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,851</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,949</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<h3 class="h3" id="lev1_35">CIFAR-10</h3>&#13;
<p class="noindent"><em>CIFAR-10</em> is another standard deep learning dataset that’s small enough for us to use without requiring a lot of training time or a GPU. As with MNIST, we can extract the dataset with Keras, which will download it the first time it’s requested. The source page for CIFAR-10 is at <em>https://www.cs.toronto.edu/\%7Ekriz/cifar.html</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_91"/>It’s worth perusing the page to learn more about where the dataset came from. It consists of 60,000 32×32 pixel RGB images from 10 classes, with 6,000 samples in each class. The training set contains 50,000 images, and the test set contains 10,000 images. The 10 classes are shown here in <a href="ch05.xhtml#ch5tab2">Table 5-2</a>.</p>&#13;
<p class="tabcap" id="ch5tab2"><strong>Table 5-2:</strong> CIFAR-10 Class Labels and Names</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Label</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Class</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">airplane</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">automobile</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">bird</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">cat</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">deer</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">dog</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">frog</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">horse</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">ship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">truck</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig4">Figure 5-4</a> shows, row by row, a collection of representative images from each class. Let’s extract the dataset, store it for future use, and create vector representations, much as we did for MNIST.</p>&#13;
<div class="image" id="ch5fig4"><img src="Images/05fig04.jpg" alt="image" width="532" height="533"/></div>&#13;
<p class="figcap"><em>Figure 5-4: Representative CIFAR-10 images</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_92"/>The script to do all of this is in <a href="ch05.xhtml#ch5lis4">Listing 5-4</a>.</p>&#13;
<p class="programs" id="ch5lis4">   import numpy as np<br/>&#13;
   import keras<br/>&#13;
   from keras.datasets import cifar10<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> (xtrn, ytrn), (xtst, ytst) = cifar10.load_data()<br/>&#13;
   idx = np.argsort(np.random.random(ytrn.shape[0]))<br/>&#13;
   xtrn = xtrn[idx]<br/>&#13;
   ytrn = ytrn[idx]<br/>&#13;
   idx = np.argsort(np.random.random(ytst.shape[0]))<br/>&#13;
   xtst = xtst[idx]<br/>&#13;
   ytst = ytst[idx]<br/>&#13;
<br/>&#13;
   np.save("cifar10_train_images.npy", xtrn)<br/>&#13;
   np.save("cifar10_train_labels.npy", ytrn)<br/>&#13;
   np.save("cifar10_test_images.npy", xtst)<br/>&#13;
   np.save("cifar10_test_labels.npy", ytst)<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> xtrnv = xtrn.reshape((50000,32*32*3))<br/>&#13;
   xtstv = xtst.reshape((10000,32*32*3))<br/>&#13;
   np.save("cifar10_train_vectors.npy", xtrnv)<br/>&#13;
   np.save("cifar10_test_vectors.npy", xtstv)</p>&#13;
<p class="figcap"><em>Listing 5-4: Loading and building the various CIFAR-10 datasets</em></p>&#13;
<p class="indent">We first load CIFAR-10 from Keras <span class="ent">❶</span>. As with MNIST, the dataset will download automatically the first time that the code is run. And, as with MNIST, we randomize the train and test splits. The training data is in <code>xtrn</code> as a (50,000; 32; 32; 3) array. The last dimension is for the three color components for each pixel: red, green, and blue. The test data is similar, and is in <code>xtst</code> as a (10,000; 32; 32; 3) array. Finally, we write the randomized train and test images to disk. Next, we unravel the images to produce 32 × 32 × 3 = 3072 element feature vectors representing the images <span class="ent">❷</span> and write them to disk.</p>&#13;
<h3 class="h3" id="lev1_36">Data Augmentation</h3>&#13;
<p class="noindent">As we saw in <a href="ch04.xhtml#ch04">Chapter 4</a>, the dataset is everything, so it needs to be as complete as possible. You’ll normally achieve this by carefully selecting samples that fit within the range of inputs the model will encounter when you use it. Thinking back to our earlier analogy, we need the model to <em>interpolate</em> and not <em>extrapolate</em>. But sometimes, even though we have a wide range of possible samples, we don’t have a lot of actual samples. This is where data augmentation can help.</p>&#13;
<p class="indent"><em>Data augmentation</em> uses the data in the existing dataset to generate new possible samples to add to the set. These samples are always based, in some way, on the existing data. Data augmentation is a powerful technique and is particularly helpful when our actual dataset is small. In a practical sense, data augmentation should probably be used whenever it’s feasible.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_93"/>Data augmentation takes the data we already have and modifies it to create new samples that could have plausibly come from the same parent distribution as our actual data. That means that if we were patient enough to keep collecting real data, we could measure those new samples. Sometimes data augmentation can go beyond what we would actually measure, yet still help the model learn to generalize to the actual data. For example, a model using images as input might benefit from unrealistic colors or backgrounds when the actual inputs to the model would never use those colors or backgrounds.</p>&#13;
<p class="indent">While data augmentation works in many situations and is a mainstay of deep learning, you won’t always be able to use it because not all data can be realistically enhanced.</p>&#13;
<p class="indent">In this section, we’ll take a look at why we’d want to consider using data augmentation and how we might go about doing it. We’ll then augment two of the datasets we developed previously, so when we build models, we can see how augmentation affects the models’ learning. As far as augmentation is concerned, here’s a rule of thumb: in general, you should perform data augmentation whenever possible, especially if the dataset is small.</p>&#13;
<h4 class="h4" id="lev2_42">Why Should You Augment Training Data?</h4>&#13;
<p class="noindent">In <a href="ch04.xhtml#ch04">Chapter 4</a>, we encountered the curse of dimensionality. We saw that the solution to it, for many models, is to fill in the space of possible inputs with more and more training data. Data augmentation is one way we can fill in this space. We’ll need to do this in the future; in <a href="ch06.xhtml#ch06">Chapter 6</a>, for example, we’ll meet the <em>k</em>-Nearest Neighbor classifier, perhaps the simplest of all classifiers.</p>&#13;
<p class="indent">This classifier depends, critically, on having enough training data to adequately fill in the input feature space. If there are three features, then the space is three-dimensional and the training data will fit into some cube in that space. The more training data we have, the more samples we’ll have in the cube, and the better the classifier will do. That’s because the classifier measures the distance between points in the training data and that of a new, unknown feature vector and votes on what label to assign. The denser the space is with training points, the more often the voting process will succeed. Loosely speaking, data augmentation fills in this space. For most datasets, acquiring more data, more samples of the parent distribution, will not fill in every part of the feature space but will create a more and more complete picture of what the parent distribution looks like in the feature space.</p>&#13;
<p class="indent">When we work with modern deep learning models (<a href="ch12.xhtml#ch12">Chapter 12</a>), we’ll see that data augmentation has additional benefits. During training, a neural network becomes conditioned to learn features of the training data. If the features the network learns to pay attention to are actually useful for distinguishing the classes, all is well. But, as we saw with the wolf and husky example of <a href="ch04.xhtml#ch04">Chapter 4</a>, sometimes the network learns the wrong thing, which can’t be used to generalize to new inputs—like the fact that the wolf class images had snow in the background and the husky images did not.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_94"/>Taking steps to avoid this tendency is known as <em>regularization</em>. Regularization helps the network learn important features of the training data, ones that generalize as we want them to. Data augmentation is—short of acquiring more actual data—perhaps the simplest way to regularize the network as it learns. It conditions the learning process to not pay attention to quirks of the particular samples selected for the training set but to instead focus on more general features of the data. At least, that is the hope.</p>&#13;
<p class="indent">An additional benefit of data augmentation is that it lessens the likelihood of overfitting when training. We’ll discuss overfitting more in <a href="ch09.xhtml#ch09">Chapter 9</a>, but succinctly, it’s what happens when the model learns the training data nearly perfectly without learning to generalize to new inputs. Using a small dataset can lead to overfitting if the model is able to basically memorize the training data. Data augmentation increases the dataset size, reducing the probability of overfitting and possibly allowing use of a model with a larger capacity. (Capacity is a nebulous concept. Think “bigger,” in that the model can learn more of what is important in the training data, while still generalizing to new data.)</p>&#13;
<p class="indent">One extremely important point needs to be made about data augmentation as it relates to the training/validation/test split of the dataset: you <em>must</em> be sure that every augmented sample belongs to the same set. For example, if we augment sample <em>X</em><sub>12345</sub>, and this sample has been assigned to the training set, then we must ensure that <em>all</em> of the augmented samples based on <em>X</em><sub>12345</sub> are also members of the training set. This is so important that it’s worth reiterating: <em>be sure to never mix an augmented sample based on an original sample between the training, validation, and test sets.</em></p>&#13;
<p class="indent">If we don’t follow this rule, our beliefs about the quality of the model will be unfounded, or at least partially unwarranted, because there will be samples in the validation and test sets that are, essentially, also in the training set, since they’re based on the training data. This warning may seem unnecessary, but it’s really easy to make this mistake, especially if working with others or with a database of some kind.</p>&#13;
<p class="indent">The correct way to augment data is <em>after</em> the training, validation, and test splits have been made. Then, augment at least the training data and label all the new samples as training data.</p>&#13;
<p class="indent">What about augmenting the validation and test splits? It isn’t wrong to do so, and might make sense if you don’t have a lot of either. I haven’t run across any studies that tried to be rigorous about the effects of augmenting the validation and test data, but, conceptually, it shouldn’t hurt, and might even help.</p>&#13;
<h4 class="h4" id="lev2_43">Ways to Augment Training Data</h4>&#13;
<p class="noindent">To augment a dataset, we need to generate new samples from it that are plausible, meaning they could really occur in the dataset. For images, this is straightforward; you can often rotate the image, or flip it horizontally or <span epub:type="pagebreak" id="page_95"/>vertically. Other times, you can manipulate the pixels themselves to change the contrast or alter the colors. Some have even gone so far as to simply swap entire color bands—swapping the red channel with the blue channel, for example.</p>&#13;
<p class="indent">Of course, the manipulations must make sense. A subtle rotation might mimic a change in the camera’s orientation, and a left-to-right flip might mimic the experience of looking in a mirror. But a top-to-bottom flip probably wouldn’t be as realistic. True, a monkey might hang upside-down in a picture, but flipping the picture would flip the tree and the ground, as well. On the other hand, you might be able to do a top-to-bottom flip in an aerial image, which shows objects in any orientation.</p>&#13;
<p class="indent">Okay, so images are generally straightforward to augment, and it’s easy to understand whether the augmentation makes sense. Augmentation of a feature vector is more subtle. It’s not always clear how to do it, or if it’s even possible. What can we do in that case?</p>&#13;
<p class="indent">Again, the guiding principle is that the augmentation makes sense. If we encoded color as a one-hot vector of, say, red, green, or blue, and an instance of a class can be red or green or blue, then one way to augment is to shift the color between red, green, and blue. If a sample can represent male or female, then we could also change those values to get a new sample of the same class but with a different gender.</p>&#13;
<p class="indent">These are unusual things to do, however. Typically, you try to augment continuous values, creating a new feature vector that still represents the original class. We’ll examine one way to do this next by augmenting the iris dataset. After that, we’ll augment the CIFAR-10 dataset to see how to work with images.</p>&#13;
<h4 class="h4" id="lev2_44">Augmenting the Iris Dataset</h4>&#13;
<p class="noindent">The iris dataset has 150 samples from three classes, each with four continuous features. We’ll augment it by using <em>principal component analysis (PCA)</em>. This is an old technique, in use for over a century. It was common in machine learning before the advent of deep learning to combat the curse of dimensionality, because it can reduce the number of features in a dataset. It also has a variety of uses outside of machine learning.</p>&#13;
<p class="indent">Imagine that we have a dataset with only two features—for example, the first two features of the iris dataset. A scatter plot of these features will show us where the samples fall in 2D space. <a href="ch05.xhtml#ch5fig5">Figure 5-5</a> shows a plot of the first two features of the iris dataset for classes 1 and 2. The plot has shifted the origin to (0,0) by subtracting the mean value of each feature. This does not change the variance or scatter of the data, only its origin.</p>&#13;
<p class="indent">The plot in <a href="ch05.xhtml#ch5fig5">Figure 5-5</a> also shows two arrows. These are the two principal components of the data. Since the data is 2D, we have two components. If we had 100 features, then we would have up to 100 principal components. This is what PCA does: it tells you the directions of the variance of the data. These directions are the <em>principal components</em>.</p>&#13;
<div class="image" id="ch5fig5"><span epub:type="pagebreak" id="page_96"/><img src="Images/05fig05.jpg" alt="image" width="670" height="504"/></div>&#13;
<p class="figcap"><em>Figure 5-5: The first two iris features for classes 1 and 2, with their principal components</em></p>&#13;
<p class="indent">The principal components also tell you how much of the variance of the data is explained by each of these directions. In the plot, the length of the arrow corresponds to the fraction of the total variance explained by each component. As you can see, the largest component is along the diagonal that matches the greatest scatter of the points. Traditional machine learning uses PCA to reduce the number of features while still, hopefully, representing the dataset well. This is how PCA can help fight the curse of dimensionality: find the principal components and then throw the less influential ones away. However, for data augmentation, we want to keep all the components.</p>&#13;
<p class="indent">The code that produced <a href="ch05.xhtml#ch5fig5">Figure 5-5</a> is in <a href="ch05.xhtml#ch5lis5">Listing 5-5</a>.</p>&#13;
<p class="programs" id="ch5lis5">   import numpy as np<br/>&#13;
   import matplotlib.pylab as plt<br/>&#13;
   from sklearn import decomposition<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> x = np.load("../data/iris/iris_features.npy")[:,:2]<br/>&#13;
   y = np.load("../data/iris/iris_labels.npy")<br/>&#13;
   idx = np.where(y != 0)<br/>&#13;
   x = x[idx]<br/>&#13;
   x[:,0] -= x[:,0].mean()<br/>&#13;
   x[:,1] -= x[:,1].mean()<br/>&#13;
<br/>&#13;
<span class="ent">❷</span> pca = decomposition.PCA(n_components=2)<br/>&#13;
   pca.fit(x)<br/>&#13;
   v = pca.explained_variance_ratio_<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_97"/><span class="ent">❸</span> plt.scatter(x[:,0],x[:,1],marker='o',color='b')<br/>&#13;
   ax = plt.axes()<br/>&#13;
   x0 = v[0]*pca.components_[0,0]<br/>&#13;
   y0 = v[0]*pca.components_[0,1]<br/>&#13;
   ax.arrow(0, 0, x0, y0, head_width=0.05, head_length=0.1, fc='r', ec='r')<br/>&#13;
   x1 = v[1]*pca.components_[1,0]<br/>&#13;
   y1 = v[1]*pca.components_[1,1]<br/>&#13;
   ax.arrow(0, 0, x1, y1, head_width=0.05, head_length=0.1, fc='r', ec='r')<br/>&#13;
   plt.xlabel("$x_0$", fontsize=16)<br/>&#13;
   plt.ylabel("$x_1$", fontsize=16)<br/>&#13;
   plt.show()</p>&#13;
<p class="figcap"><em>Listing 5-5: Iris PCA plot</em></p>&#13;
<p class="indent">Much of the preceding code is to make the plot <span class="ent">❸</span>. The imports are standard except for a new one from sklearn, the <code>decomposition</code> module. We load the iris dataset we previously saved, keeping only the first two features in <code>x</code> and the labels in <code>y</code>. We then keep only class 1 and class 2 features by excluding class 0. Next, we subtract the per feature means to center the data about the point (0,0) <span class="ent">❶</span>.</p>&#13;
<p class="indent">Then we create the PCA object and fit the iris data to it <span class="ent">❷</span>. There are two features, so the number of components in this case is also two. The PCA Python class mimics the standard approach of sklearn: it defines the model, then fits data to it. Once this is done, we have the principal components stored in <code>pca</code> and accessible via the <code>components_</code> member variable. We set <code>v</code> to a vector representing the fraction of the variance in the data explained by each of the principal component directions. Since there are two components, this vector also has two components.</p>&#13;
<p class="indent">The components are always listed in decreasing order, so that the first component is the direction describing the majority of the variance, the second component is the next most important, and so on. In this case, the first component describes some 84 percent of the variance and the second describes the remaining 16 percent. We’ll use this ordering when we generate new augmented samples. Here we use the fraction to scale the length of the arrows in the plot showing the principal component directions and relative importance.</p>&#13;
<p class="indent">How is <a href="ch05.xhtml#ch5fig5">Figure 5-5</a> useful for data augmentation? Once you know the principal components, you can use PCA to create derived variables, which means you rotate the data to align it with the principal components. The <code>transform</code> method of the PCA class does this by mapping an input—in our case, the original data—to a new representation where the variance is aligned with the principal components. This mapping is exact, and you can reverse it by using the <code>inverse_transform</code> method.</p>&#13;
<p class="indent">Doing this alone doesn’t generate new samples for us. If we take the original data, <code>x</code>, transform it to the new representation, and then inverse transform it, we’ll end up where we started, with <code>x</code>. But, if we transform <code>x</code> and then, before calling the inverse transform, <em>modify</em> some of the principal components, we’ll return a new set of samples that are not <code>x</code> but are based <span epub:type="pagebreak" id="page_98"/>on <code>x</code>. This is precisely what we want for data augmentation. Next, we’ll see which components to modify, and how.</p>&#13;
<p class="indent">The components are ordered in <code>pca</code> by their importance. We want to keep the most important components as they are, because we want the inverse transform to produce data that looks much like the original data. We don’t want to transform things too much, or the new samples won’t be plausible instances of the class we claim they represent. We’ll arbitrarily say that we want to keep the components that, cumulatively, represent some 90 percent to 95 percent of the variance in the data. These we won’t modify at all. The remaining components will be modified by adding normally distributed noise. Recall that <em>normally distributed</em> means it follows the bell curve so that most of the time the value will be near the middle, which we’ll set to 0, meaning no change to the component, and increasingly rarely to larger values. We’ll add the noise to the existing component and call the inverse transform to produce new samples that are very similar but not identical to the originals.</p>&#13;
<p class="indent">The previous paragraph is pretty dense. The code will make things easier to understand. Our approach to generating augmented data is shown in <a href="ch05.xhtml#ch5lis6">Listing 5-6</a>.</p>&#13;
<p class="programs" id="ch5lis6">   import numpy as np<br/>&#13;
   from sklearn import decomposition<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> def generateData(pca, x, start):<br/>&#13;
       original = pca.components_.copy()<br/>&#13;
       ncomp = pca.components_.shape[0]<br/>&#13;
       a = pca.transform(x)<br/>&#13;
       for i in range(start, ncomp):<br/>&#13;
           pca.components_[i,:] += np.random.normal(scale=0.1, size=ncomp)<br/>&#13;
       b = pca.inverse_transform(a)<br/>&#13;
       pca.components_ = original.copy()<br/>&#13;
       return b<br/>&#13;
<br/>&#13;
   def main():<br/>&#13;
     <span class="ent">❷</span> x = np.load("../../../data/iris/iris_features.npy")<br/>&#13;
       y = np.load("../../../data/iris/iris_labels.npy")<br/>&#13;
<br/>&#13;
       N = 120<br/>&#13;
       x_train = x[:N]<br/>&#13;
       y_train = y[:N]<br/>&#13;
       x_test = x[N:]<br/>&#13;
       y_test = y[N:]<br/>&#13;
<br/>&#13;
       pca = decomposition.PCA(n_components=4)<br/>&#13;
       pca.fit(x)<br/>&#13;
       print(pca.explained_variance_ratio_)<br/>&#13;
       start = 2<br/>&#13;
     <span class="ent">❸</span> nsets = 10<br/>&#13;
<span epub:type="pagebreak" id="page_99"/>       nsamp = x_train.shape[0]<br/>&#13;
       newx = np.zeros((nsets*nsamp, x_train.shape[1]))<br/>&#13;
       newy = np.zeros(nsets*nsamp, dtype="uint8")<br/>&#13;
<br/>&#13;
    <span class="ent">❹</span> for i in range(nsets):<br/>&#13;
           if (i == 0):<br/>&#13;
               newx[0:nsamp,:] = x_train<br/>&#13;
               newy[0:nsamp] = y_train<br/>&#13;
           else:<br/>&#13;
               newx[(i*nsamp):(i*nsamp+nsamp),:] =<br/>&#13;
                                  generateData(pca, x_train, start)<br/>&#13;
               newy[(i*nsamp):(i*nsamp+nsamp)] = y_train<br/>&#13;
<br/>&#13;
     <span class="ent">❺</span> idx = np.argsort(np.random.random(nsets*nsamp))<br/>&#13;
       newx = newx[idx]<br/>&#13;
       newy = newy[idx]<br/>&#13;
       np.save("iris_train_features_augmented.npy", newx)<br/>&#13;
       np.save("iris_train_labels_augmented.npy", newy)<br/>&#13;
       np.save("iris_test_features_augmented.npy", x_test)<br/>&#13;
       np.save("iris_test_labels_augmented.npy", y_test)<br/>&#13;
<br/>&#13;
   main()</p>&#13;
<p class="figcap"><em>Listing 5-6: Augmenting the iris data with PCA. See</em> iris_data_augmentation.py.</p>&#13;
<p class="indent">The <code>main</code> function <span class="ent">❷</span> loads the existing iris data, <code>x</code>, and the corresponding labels, <code>y</code>, and then calls PCA, this time using all four features of the dataset. This gives us the four principal components telling us how much of the variance is explained by each component:</p>&#13;
<pre>0.92461621 0.05301557 0.01718514 0.00518309</pre>&#13;
<p class="indent">The first two principal components describe over 97 percent of the variance. Therefore, we’ll leave the first two components alone, indices 0 and 1, and start with index 2 when we want to generate new samples.</p>&#13;
<p class="indent">We next declare the number of sets we’ll define <span class="ent">❸</span>. A <em>set</em> here means a new collection of samples. Since the samples are based on the original data, <code>x</code>, with 150 samples, each new set will contain 150 samples as well. In fact, they’ll be in the same order as the original samples, so that the class label that should go with each of these new samples is in the same order as the class labels in <code>y</code>. We don’t want to lose our original data, either, so <code>nsets=10</code> puts the original data and nine new sets of samples based on that original data—for a total of 1,500 samples—in the new dataset. We grab the number of samples in <code>x</code>, 150, and define the arrays to hold our new features (<code>newx</code>) and associated labels (<code>newy</code>).</p>&#13;
<p class="indent">Next, we loop to generate the new samples, one set of 150 at a time <span class="ent">❹</span>. The first pass simply copies the original data into the output arrays. The remaining passes are similar, updating the source and destination indices of the output arrays appropriately, but instead of assigning <code>x</code>, we assign the <span epub:type="pagebreak" id="page_100"/>output of <code>generateData</code>. When the loop is done, we scramble the order of the entire dataset and write it to disk <span class="ent">❺</span>.</p>&#13;
<p class="indent">All of the magic is in <code>generateData</code> <span class="ent">❶</span>. We pass in the PCA object (<code>pca</code>), the original data (<code>x</code>), and the starting principal component index (<code>start</code>). We set the last argument to 2 to leave the two most important components alone. We keep a copy of the actual components so we can reset the <code>pca</code> object before we return. Then we define <code>ncomp</code>, the number of principal components, for convenience and call the forward transformation mapping the original data along the principal components.</p>&#13;
<p class="indent">The loop updates the two least important components by adding a random value drawn from a normal curve with mean value 0 and a standard deviation of 0.1. Why 0.1? No special reason; if the standard deviation is small, then the new samples will be near the old samples, while if it’s larger, they’ll be farther away and possibly not representative of the class anymore. Next, we call the inverse transformation using the modified principal components and restore the actual components. Finally, we return the new set of samples.</p>&#13;
<p class="indent">Let’s look at the new dataset, shown in <a href="ch05.xhtml#ch5fig6">Figure 5-6</a>. The big gray dots are from our original dataset, and the smaller black dots are the augmented samples. As we can readily see, they all fall near an existing sample, which is what we would expect from modifying only the weakest of the principal components. Since we copied the original data into the augmented dataset, each big dot has a small dot at the center.</p>&#13;
<div class="image" id="ch5fig6"><img src="Images/05fig06.jpg" alt="image" width="668" height="504"/></div>&#13;
<p class="figcap"><em>Figure 5-6: The first two features of the original iris dataset (large dots) and the augmented features generated by <a href="ch05.xhtml#ch5lis6">Listing 5-6</a> (small points)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_101"/>This approach is appropriate for continuous features only, as was previously stated, and you should be careful to modify only the weakest of the principal components, and only by a small amount. Experimentation is important here. As an exercise, try applying the same technique to augment the breast cancer dataset, which also consists of continuous features.</p>&#13;
<h4 class="h4" id="lev2_45">Augmenting the CIFAR-10 Dataset</h4>&#13;
<p class="noindent">Augmenting the iris dataset involved a lot of discussion and some less than obvious math. Fortunately for us, augmenting images is generally a lot simpler, but still just as effective when training modern models. When we build convolutional neural network models (<a href="ch12.xhtml#ch12">Chapter 12</a>), we’ll see how to do augmentation on the fly when training, a particularly helpful approach, but for now we’ll do the augmentation first and build a new dataset with additional versions of the existing images.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5fig4">Figure 5-4</a> shows representative images from each class in the CIFAR-10 dataset. These are color images stored as RGB data for the red, green, and blue channels. They were taken from ground level, so top and bottom flips do not make sense here, while left and right flips do. Translations—shifting the image in the <em>x</em> or <em>y</em> direction, or both—are one common technique. Small rotations are another common technique.</p>&#13;
<p class="indent">However, each of these raises an issue: what to do with pixels that have no data after the shift or rotate? If I shift an image 3 pixels to the left, I need to fill in the three columns on the right with something. Or, if I rotate to the right, there will be pixels at the upper right and lower left that need to be filled in. There are several ways to handle this. One is to simply leave the pixels black, or all 0 values, and let the model learn that there is no helpful information there. Another is to replace the pixels with the mean value of the image, which also provides no information and will, we hope, be ignored by the model. However, the most popular solution is to crop the image.</p>&#13;
<p class="indent">The image is 32×32 pixels. Pulling a random patch from the image of, say, 28×28 pixels is the equivalent of shifting the image by a random <em>x</em> or <em>y</em> position of up to 4 pixels without needing to worry about filling in anything. If we rotate the image first, which will require interpolation of the pixels, and then crop to remove the edge regions, we’ll again have no empty pixels to worry about. Keras has tools for doing this via an image generator object used during training. When we’re using Keras to build models, we’ll make use of it, but for now, we’ll do all of the work ourselves in order to understand the process.</p>&#13;
<p class="indent">We need to mention one point here. So far, we’ve talked about building a dataset for training a model. What should we do when we want to use the model? Do we hand the model random croppings of the test inputs as well? No. Instead, we hand the model a cropping centered on the image. So, for CIFAR-10, we would take each 32 × 32 test input and crop it to 28 × 28 by dropping the outer 6 pixels, then present that to the model. We do this because the center crop still represents the actual test image and not some augmented version of it.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_102"/><a href="ch05.xhtml#ch5fig7">Figure 5-7</a> illustrates what we mean by rotations, flips, random croppings for training, and center cropping for testing. In (a) we rotate the image and take a center crop. The output image is in the white square. In (b) we flip left to right and crop randomly. In (c), we take two random crops without flipping, and in (d) we take a center crop for testing, without any rotation or flip. Some people augment test images, but we won’t do so here.</p>&#13;
<div class="image" id="ch5fig7"><img src="Images/05fig07.jpg" alt="image" width="678" height="510"/></div>&#13;
<p class="figcap"><em>Figure 5-7: Rotate, then center crop (a). Flip left to right, then crop randomly (b). Two random crops during training (c). Center crop for testing, with no rotation or flip (d).</em></p>&#13;
<p class="indent"><a href="ch05.xhtml#ch5lis7">Listing 5-7</a> shows how to augment the CIFAR-10 training set with random crops, rotations, and flips.</p>&#13;
<p class="programs" id="ch5lis7">   import numpy as np<br/>&#13;
   from PIL import Image<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> def augment(im, dim):<br/>&#13;
        img = Image.fromarray(im)<br/>&#13;
        if (np.random.random() &lt; 0.5):<br/>&#13;
            img = img.transpose(Image.FLIP_LEFT_RIGHT)<br/>&#13;
        if (np.random.random() &lt; 0.3333):<br/>&#13;
            z = (32-dim)/2<br/>&#13;
            r = 10*np.random.random()-5<br/>&#13;
            img = img.rotate(r, resample=Image.BILINEAR)<br/>&#13;
            img = img.crop((z,z,32-z,32-z))<br/>&#13;
        else:<br/>&#13;
            x = int((32-dim-1)*np.random.random())<br/>&#13;
            y = int((32-dim-1)*np.random.random())<br/>&#13;
<span epub:type="pagebreak" id="page_103"/>            img = img.crop((x,y,x+dim,y+dim))<br/>&#13;
        return np.array(img)<br/>&#13;
<br/>&#13;
   def main():<br/>&#13;
     <span class="ent">❷</span> x = np.load("../data/cifar10/cifar10_train_images.npy")<br/>&#13;
       y = np.load("../data/cifar10/cifar10_train_labels.npy")<br/>&#13;
       factor = 10<br/>&#13;
       dim = 28<br/>&#13;
       z = (32-dim)/2<br/>&#13;
       newx = np.zeros((x.shape[0]*factor, dim,dim,3), dtype="uint8")<br/>&#13;
       newy = np.zeros(y.shape[0]*factor, dtype="uint8")<br/>&#13;
       k=0<br/>&#13;
    <span class="ent">❸</span> for i in range(x.shape[0]):<br/>&#13;
            im = Image.fromarray(x[i,:])<br/>&#13;
            im = im.crop((z,z,32-z,32-z))<br/>&#13;
            newx[k,...] = np.array(im)<br/>&#13;
            newy[k] = y[i]<br/>&#13;
            k += 1<br/>&#13;
            for j in range(factor-1):<br/>&#13;
                newx[k,...] = augment(x[i,:], dim)<br/>&#13;
                newy[k] = y[i]<br/>&#13;
                k += 1<br/>&#13;
        idx = np.argsort(np.random.random(newx.shape[0]))<br/>&#13;
        newx = newx[idx]<br/>&#13;
        newy = newy[idx]<br/>&#13;
        np.save("../data/cifar10/cifar10_aug_train_images.npy", newx)<br/>&#13;
        np.save("../data/cifar10/cifar10_aug_train_labels.npy", newy)<br/>&#13;
<br/>&#13;
    <span class="ent">❹</span> x = np.load("../data/cifar10/cifar10_test_images.npy")<br/>&#13;
       newx = np.zeros((x.shape[0], dim,dim,3), dtype="uint8")<br/>&#13;
       for i in range(x.shape[0]):<br/>&#13;
           im = Image.fromarray(x[i,:])<br/>&#13;
           im = im.crop((z,z,32-z,32-z))<br/>&#13;
           newx[i,...] = np.array(im)<br/>&#13;
       np.save("../data/cifar10/cifar10_aug_test_images.npy", newx)</p>&#13;
<p class="figcap"><em>Listing 5-7: Augmenting the CIFAR-10 dataset. See</em> cifar10_augment.py.</p>&#13;
<p class="indent">The <code>main</code> function loads the existing dataset and defines our augmentation factor, crop size, and a constant for defining a center crop <span class="ent">❷</span>.</p>&#13;
<p class="indent">The new image will be put in <code>newx</code>, which has the following dimensions: (500,000;28;28;3); there are 50,000 training images, each with 32×32 pixels and three color bands. We set the augmentation factor to 10. Similarly, there will be 500,000 labels. The counter, <code>k</code>, will index into this new dataset. For every image in the old dataset, we’ll create nine completely new versions and center crop the original <span class="ent">❶ ❸</span>.</p>&#13;
<p class="indent">As the dataset consists of images, it’s easiest to work with the data in image form, so we make the current sample an actual <code>PIL</code> image in order to <span epub:type="pagebreak" id="page_104"/>easily crop it. This is the center crop of the original image. We store it in the new output array.</p>&#13;
<p class="indent">There are two Python idioms here that we’ll see more than once. The first is to turn a NumPy array representing an image into a <code>PIL</code> image:</p>&#13;
<pre>im = Image.fromarray(arr)</pre>&#13;
<p class="indent">The second is to go the other way and turn a <code>PIL</code> image into a NumPy array:</p>&#13;
<pre>arr = np.array(im)</pre>&#13;
<p class="indent">We must be sure that the NumPy array is a valid image data type like unsigned byte (<code>uint8</code>). Use the <code>astype</code> NumPy array method to cast between types, remembering that you bear all responsibility for understanding what that casting entails.</p>&#13;
<p class="indent">Referring back to <a href="ch05.xhtml#ch5lis7">Listing 5-7</a>, we are creating the nine versions of the current image. For each of these, we simply copy the label and assign the output array an augmented version. We’ll describe the <code>augment</code> function shortly. Once the new dataset has been constructed, we scramble the order and write the augmented training dataset to disk <span class="ent">❸</span>.</p>&#13;
<p class="indent">We’re not quite done, however. We created an augmented training set that cropped the original 32 × 32 images to 28 × 28. We must, therefore, at least crop the original test set <span class="ent">❹</span>. As we stated previously, we use a center crop and no augmentation of the test data. Therefore, we simply load the test dataset, define the new output test dataset, and run a loop that crops the 32 × 32 images to 28 × 28. When done, we write the cropped test data to disk. Note that we did not modify the <em>order</em> of the images in the test set; we simply cropped them, so we do not need to write a new file for the test labels.</p>&#13;
<p class="indent">The <code>augment</code> function <span class="ent">❶</span> is where all the action is. We immediately change the input NumPy array into an actual <code>PIL</code> image object. We next decide, with a 50-50 chance, whether or not we will flip the image left to right. Note that we do not crop the image just yet.</p>&#13;
<p class="indent">Next, we ask whether we should rotate the image or not. We select rotation with a probability of 33 percent (1 in 3 chance). Why 33 percent? No particular reason, but it seems that we might want to crop randomly more often than we rotate. We could even drop this probability down to 20 percent (1 in 5 chance). If we do rotate, we select the rotation angle, [<em>–</em>5,5] and then call the <code>rotate</code> method using bilinear interpolation to make the rotated image look a bit nicer than simply using the nearest neighbor, which is the <code>PIL</code> default. Next, we center crop the rotated image. This way, we will not get any black pixels on the edges where the rotation had no image information to work with.</p>&#13;
<p class="indent">If we do not rotate, we are free to select a random crop. We choose the upper-left corner of this random crop, ensuring that the cropped square will not exceed the dimensions of the original image. Finally, we convert the data back to a NumPy array and return.</p>&#13;
<h3 class="h3" id="lev1_37"><span epub:type="pagebreak" id="page_105"/>Summary</h3>&#13;
<p class="noindent">In this chapter, we built four datasets that we’ll use as examples throughout the rest of the book. The first two, irises and breast cancer histology, are based on feature vectors. The last two, MNIST and CIFAR-10, are represented as images. We then learned about two data augmentation methods: augmenting a feature vector of continuous values using PCA and, more critical for deep learning, augmenting images by basic transformations.</p>&#13;
<p class="indent">In the next chapter, we’ll transition to our discussion of classical machine learning models. In the chapter after that, we’ll use these datasets with those models.<span epub:type="pagebreak" id="page_106"/></p>&#13;
</div></body></html>