- en: '3'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SUMMARIZING SPEECHES WITH NATURAL LANGUAGE PROCESSING
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: “Water, water everywhere, but not a drop to drink.” This famous line, from *The
    Rime of the Ancient Mariner*, summarizes the present state of digital information.
    According to International Data Corporation, by 2025 we’ll be generating 175 trillion
    gigabytes of digital data *per year*. But most of this data—up to 95 percent—will
    be *unstructured*, which means it’s not organized into useful databases. Even
    now, the key to the cure for cancer may be right at our fingertips yet almost
    impossible to reach.
  prefs: []
  type: TYPE_NORMAL
- en: To make information easier to discover and consume, we need to reduce the volume
    of data by extracting and repackaging salient points into digestible summaries.
    Because of the sheer volume of data, there’s no way to do this manually. Luckily,
    natural language processing (NLP) helps computers understand both words and context.
    For example, NLP applications can summarize news feeds, analyze legal contracts,
    research patents, study financial markets, capture corporate knowledge, and produce
    study guides.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll use Python’s Natural Language Toolkit (NLTK) to generate
    a summary of one of the most famous speeches of all time, “I Have a Dream” by
    Martin Luther King Jr. With an understanding of the basics, you’ll then use a
    streamlined alternative, called gensim, to summarize the popular “Make Your Bed”
    speech by Admiral William H. McRaven. Finally, you’ll use a word cloud to produce
    a fun visual summary of the most frequently used words in Sir Arthur Conan Doyle’s
    novel *The Hound of the Baskervilles*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Project #3: I Have a Dream . . . to Summarize Speeches!**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In machine learning and data mining, there are two approaches to summarizing
    text: *extraction* and *abstraction*.'
  prefs: []
  type: TYPE_NORMAL
- en: Extraction-based summarization uses various weighting functions to rank sentences
    by perceived importance. Words used more often are considered more important.
    Consequently, sentences containing those words are considered more important.
    The overall behavior is like using a yellow highlighter to manually select keywords
    and sentences without altering the text. The results can be disjointed, but the
    technique is good at pulling out important words and phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Abstraction relies on deeper comprehension of the document to capture intent
    and produce more human-like paraphrasing. This includes creating completely new
    sentences. The results tend to be more cohesive and grammatically correct than
    those produced by extraction-based methods, but at a price. Abstraction algorithms
    require advanced and complicated deep learning methods and sophisticated language
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: For this project, you’ll use an extraction-based technique on the “I Have a
    Dream” speech, delivered by Martin Luther King Jr. at the Lincoln Memorial on
    August 28, 1963\. Like Lincoln’s “Gettysburg Address” a century before, it was
    the perfect speech at the perfect time. Dr. King’s masterful use of repetition
    also makes it tailor-made for extraction techniques, which correlate word frequency
    with importance.
  prefs: []
  type: TYPE_NORMAL
- en: THE OBJECTIVE
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that summarizes a speech using NLP text extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Strategy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Natural Language Toolkit includes the functions you’ll need to summarize
    Dr. King’s speech. If you skipped [Chapter 2](ch02.xhtml), see [page 29](ch02.xhtml#page_29)
    for installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize the speech, you’ll need a digital copy. In previous chapters, you
    manually downloaded files you needed from the internet. This time you’ll use a
    more efficient technique, called *web scraping*, which allows you to programmatically
    extract and save large amounts of data from websites.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve loaded the speech as a string, you can use NLTK to split out and
    count individual words. Then, you’ll “score” each sentence in the speech by summing
    the word counts within it. You can use those scores to print the top-ranked sentences,
    based on how many sentences you want in your summary.
  prefs: []
  type: TYPE_NORMAL
- en: '***Web Scraping***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scraping the web means using a program to download and process content. This
    is such a common task that prewritten scraping programs are freely available.
    You’ll use the requests library to download files and web pages, and you’ll use
    the Beautiful Soup (bs4) package to parse HTML. Short for *Hypertext Markup Language*,
    HTML is the standard format used to create web pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the two modules, use pip in a terminal window or Windows PowerShell
    (see [page 8](ch01.xhtml#page_8) in [Chapter 1](ch01.xhtml) for instructions on
    installing and using pip):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To check the installation, open the shell and import each module as shown next.
    If you don’t get an error, you’re good to go!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about requests, visit *[https://pypi.org/project/requests/](https://pypi.org/project/requests/)*.
    For Beautiful Soup, see *[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***The “I Have a Dream” Code***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The *dream_summary.py* program performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Opens a web page containing the “I Have a Dream” speech
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loads the text as a string
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizes the text into words and sentences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removes stop words with no contextual content
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Counts the remaining words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses the counts to rank the sentences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displays the highest-ranking sentences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you’ve already downloaded the book’s files, find the program in the *Chapter_3*
    folder. Otherwise, go to *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*
    and download it from the book’s GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing Modules and Defining the main() Function**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-1](ch03.xhtml#ch03list1) imports modules and defines the first part
    of the main() function, which scrapes the web and assigns the speech to a variable
    as a string.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-1: Importing modules and defining the main() function'
  prefs: []
  type: TYPE_NORMAL
- en: Start by importing Counter from the collections module to help you keep track
    of the sentence scoring. The collections module is part of the Python Standard
    Library and includes several container data types. A Counter is a dictionary subclass
    for counting hashable objects. Elements are stored as dictionary keys, and their
    counts are stored as dictionary values.
  prefs: []
  type: TYPE_NORMAL
- en: Next, to clean up the speech prior to summarizing its contents, import the re
    module. The *re* stands for *regular expressions*, also referred to as *regexes*,
    which are sequences of characters that define a search pattern. This module will
    help you clean up the speech by allowing you to selectively remove bits that you
    don’t want.
  prefs: []
  type: TYPE_NORMAL
- en: Finish the imports with the modules for scraping the web and doing natural language
    processing. The last module brings in the list of functional stop words (such
    as *if*, *and*, *but*, *for*) that contain no useful information. You’ll remove
    these from the speech prior to summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Next, define a main() function to run the program. To scrape the speech off
    the web, provide the url address as a string ➊. You can copy and paste this from
    the website from which you want to extract text.
  prefs: []
  type: TYPE_NORMAL
- en: The requests library abstracts the complexities of making HTTP requests in Python.
    HTTP, short for HyperText Transfer Protocol, is the foundation of data communication
    using hyperlinks on the World Wide Web. Use the requests.get() method to fetch
    the url and assign the output to the page variable, which references the Response
    object the web page returned for the request. This object’s text attribute holds
    the web page, including the speech, as a string.
  prefs: []
  type: TYPE_NORMAL
- en: To check that the download was successful, call the Response object’s raise_for_status()
    method. This does nothing if everything goes okay but otherwise will raise an
    exception and halt the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the data is in HTML, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, HTML has a lot of *tags*, such as <head> and <p>, that let your
    browser know how to format the web page. The text between starting and closing
    tags is called an *element*. For example, the text “Martin Luther King Jr.’s 1962
    Speech” is a title element sandwiched between the starting tag <title> and the
    closing tag </title>. Paragraphs are formatted using <p> and </p> tags.
  prefs: []
  type: TYPE_NORMAL
- en: Because these tags are not part of the original text, they should be removed
    prior to any natural language processing. To remove the tags, call the bs4.BeautifulSoup()
    method and pass it the string containing the HTML ➋. Note that I’ve explicitly
    specified html.parser. The program will run without this but complain bitterly
    with warnings in the shell.
  prefs: []
  type: TYPE_NORMAL
- en: The soup variable now references a BeautifulSoup object, which means you can
    use the object’s find_all() method to locate the speech buried in the HTML document.
    In this case, to find the text between paragraph tags (<p>), use list comprehension
    and find_all() to make a list of just the paragraph elements.
  prefs: []
  type: TYPE_NORMAL
- en: Finish by turning the speech into a continuous string. Use the join() method
    to turn the p_elems list into a string. Set the “joiner” character to a space,
    designated by ''.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that with Python, there is usually more than one way to accomplish a task.
    The last two lines of the listing can also be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The select() method is more limited overall than find_all(), but in this case
    it works the same and is more succinct. In the previous snippet, select() finds
    the <p> tags, and the results are converted to text when concatenated to the speech
    string.
  prefs: []
  type: TYPE_NORMAL
- en: '**Completing the main() Function**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, you’ll prep the speech to fix typos and remove punctuation, special characters,
    and spaces. Then you’ll call three functions to remove stop words, count word
    frequency, and score the sentences based on the word counts. Finally, you’ll rank
    the sentences and display those with the highest scores in the shell.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 3-2](ch03.xhtml#ch03list2) completes the definition of main() that
    performs these tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-2: Completing the main() function'
  prefs: []
  type: TYPE_NORMAL
- en: The original document contains a typo (*mowing* instead of *knowing*), so start
    by fixing this using the string.replace() method. Continue cleaning the speech
    using regex. Many casual programmers are turned off by this module’s arcane syntax,
    but it’s such a powerful and useful tool that everyone should be aware of the
    basic regex syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Remove extra spaces using the re.sub() function, which replaces substrings with
    new characters. Use the shorthand character class code \s+ to identify runs of
    whitespace and replace them with a single space, indicated by ' '. Finish by passing
    re.sub() the name of the string (speech).
  prefs: []
  type: TYPE_NORMAL
- en: Next, remove anything that’s *not* a letter by matching the [^a-zA-Z] pattern.
    The caret at the start instructs regex to “match any character that isn’t between
    the brackets.” So, numbers, punctuation marks, and so on, will be replaced by
    a space.
  prefs: []
  type: TYPE_NORMAL
- en: Removing characters like punctuation marks will leave an extra space. To get
    rid of these spaces, call the re.sub() method again.
  prefs: []
  type: TYPE_NORMAL
- en: Next, request that the user input the number of sentences to include in the
    summary and the maximum number of words per sentence. Use a while loop and Python’s
    built-in isdigit() function to ensure the user inputs an integer ➊.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*According to research by the American Press Institute, comprehension is best
    with sentences of fewer than 15 words. Similarly, the Oxford Guide to Plain English
    recommends using sentences that average 15 to 20 words over a full document.*'
  prefs: []
  type: TYPE_NORMAL
- en: Continue cleaning the text by calling the remove_stop_words() function. Then
    call functions get_word_freq() and score_sentences() to calculate the frequency
    of the remaining words and to score the sentences, respectively. You’ll define
    these functions after completing the main() function.
  prefs: []
  type: TYPE_NORMAL
- en: To rank the sentences, call the collection module’s Counter() method ➋. Pass
    it the sent_scores variable.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the summary, use the Counter object’s most_common() method. Pass
    it the num_sents variable input by the user. The resulting summary variable will
    hold a list of tuples. For each tuple, the sentence is at index [0], and its rank
    is at index [1].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For readability, print each sentence of the summary on a separate line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Removing Stop Words**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Remember from [Chapter 2](ch02.xhtml) that stop words are short, functional
    words like *if*, *but*, *for*, and *so*. Because they contain no important contextual
    information, you don’t want to use them to rank sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 3-3](ch03.xhtml#ch03list3) defines a function called remove_stop_words()
    to remove stop words from the speech.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-3: Defining a function to remove stop words from the speech'
  prefs: []
  type: TYPE_NORMAL
- en: Define the function to receive speech_edit, the edited speech string, as an
    argument. Then create a set of the English stop words in NLTK. Use a set, rather
    than a list, as searches are quicker in sets.
  prefs: []
  type: TYPE_NORMAL
- en: Assign an empty string to hold the edited speech sans stop words. The speech_edit
    variable is currently a string in which each element is a letter.
  prefs: []
  type: TYPE_NORMAL
- en: To work with words, call the NLTK word_tokenize() method. Note that you can
    do this while looping through words. Convert each word to lowercase and check
    its membership in the stop_words set. If it’s not a stop word, concatenate it
    to the new string, along with a space. Return this string to end the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'How you handle letter case in this program is important. You’ll want the summary
    to print with both uppercase and lowercase letters, but you must do the NLP work
    using all lowercase to avoid miscounting. To see why, look at the following code
    snippet, which counts words in a string (s) with mixed cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t convert the words to lowercase, *one* and *One* are considered
    distinct elements. For counting purposes, every instance of *one* regardless of
    its case should be treated as the same word. Otherwise, the contribution of *one*
    to the document will be diluted.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating the Frequency of Occurrence of Words**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To count the occurrence of each word in the speech, you’ll create the get_word_freq()
    function that returns a dictionary with the words as keys and the counts as values.
    [Listing 3-4](ch03.xhtml#ch03list4) defines this function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-4: Defining a function to calculate word frequency in the speech'
  prefs: []
  type: TYPE_NORMAL
- en: The get_word_freq() function takes the edited speech string with no stop words
    as an argument. NLTK’s FreqDist class acts like a dictionary with the words as
    keys and their counts as values. As part of the process, convert the input string
    to lowercase and tokenize it into words. End the function by returning the word_freq
    dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scoring Sentences**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-5](ch03.xhtml#ch03list5) defines a function that scores sentences
    based on the frequency distribution of the words they contain. It returns a dictionary
    with each sentence as the key and its score as the value.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-5: Defining a function to score sentences based on word frequency'
  prefs: []
  type: TYPE_NORMAL
- en: Define a function, called score_sentences(), with parameters for the original
    speech string, the word_freq object, and the max_words variable input by the user.
    You want the summary to contain stop words and capitalized words—hence the use
    of speech.
  prefs: []
  type: TYPE_NORMAL
- en: Start an empty dictionary, named sent_scores, to hold the scores for each sentence.
    Next, tokenize the speech string into sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Now, start looping through the sentences ➊. Start by updating the sent_scores
    dictionary, assigning the sentence as the key, and setting its initial value (count)
    to 0.
  prefs: []
  type: TYPE_NORMAL
- en: To count word frequency, you first need to tokenize the sentence into words.
    Be sure to use lowercase to be compatible with the word_freq dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to be careful when you sum up the word counts per sentence to create
    the scores so you don’t bias the results toward longer sentences. After all, longer
    sentences are more likely to have a greater number of important words. To avoid
    excluding short but important sentences, you need to *normalize* each count by
    dividing it by the sentence *length*. Store the length in a variable called sent_word_count.
  prefs: []
  type: TYPE_NORMAL
- en: Next, use a conditional that constrains sentences to the maximum length input
    by the user ➋. If the sentence passes the test, start looping through its words.
    If a word is in the word_freq dictionary, add it to the count stored in sent_scores.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of each loop through the sentences, divide the score for the current
    sentence by the number of words in the sentence ➌. This normalizes the score so
    long sentences don’t have an unfair advantage.
  prefs: []
  type: TYPE_NORMAL
- en: End the function by returning the sent_scores dictionary. Then, back in the
    global space, add the code for running the program as a module or in stand-alone
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Running the Program**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Run the *dream_summary.py* program with a maximum sentence length of 14 words.
    As mentioned previously, good, readable sentences tend to contain 14 words or
    fewer. Then truncate the summary at 15 sentences, about one-third of the speech.
    You should get the following results. Note that the sentences won’t necessarily
    appear in their original order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Not only does the summary capture the title of the speech, it captures the main
    points.
  prefs: []
  type: TYPE_NORMAL
- en: But if you run it again with 10 words per sentence, a lot of the sentences are
    clearly too long. Because there are only 7 sentences in the whole speech with
    10 or fewer words, the program can’t honor the input requirements. It defaults
    to printing the speech from the beginning until the sentence count is at least
    what was specified in the num_sents variable.
  prefs: []
  type: TYPE_NORMAL
- en: Now, rerun the program and try setting the word count limit to 1,000.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Although longer sentences don’t dominate the summary, a few slipped through,
    making this summary less poetic than the previous one. The lower word count limit
    forces the previous version to rely more on shorter phrases that act like a chorus.
  prefs: []
  type: TYPE_NORMAL
- en: '**Project #4: Summarizing Speeches with gensim**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In an Emmy award–winning episode of *The Simpsons*, Homer runs for sanitation
    commissioner using the campaign slogan, “Can’t someone else do it?” That’s certainly
    the case with many Python applications: often, when you need to write a script,
    you learn that someone else has already done it! One example is gensim, an open
    source library for natural language processing using statistical machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: The word *gensim* stands for “generate similar.” It uses a graph-based ranking
    algorithm called TextRank. This algorithm was inspired by PageRank, invented by
    Larry Page and used to rank web pages in Google searches. With PageRank, the importance
    of a website is determined by how many other pages link to it. To use this approach
    with text processing, algorithms measure how similar each sentence is to all the
    other sentences. The sentence that is the most like the others is considered the
    most important.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, you’ll use gensim to summarize Admiral William H. McRaven’s
    commencement address, “Make Your Bed,” given at the University of Texas at Austin
    in 2014\. This inspirational, 20-minute speech has been viewed more than 10 million
    times on YouTube and inspired a *New York Times* bestselling book in 2017.
  prefs: []
  type: TYPE_NORMAL
- en: THE OBJECTIVE
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that uses the gensim module to summarize a speech.
  prefs: []
  type: TYPE_NORMAL
- en: '***Installing gensim***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The gensim module runs on all the major operating systems but is dependent on
    NumPy and SciPy. If you don’t have them installed, go back to [Chapter 1](ch01.xhtml)
    and follow the instructions in “Installing the Python Libraries” on [page 6](ch01.xhtml#page_6).
  prefs: []
  type: TYPE_NORMAL
- en: To install gensim on Windows, use pip install -U gensim. To install it in a
    terminal, use pip install --upgrade gensim. For conda environments, use conda
    install -c conda-forge gensim. For more on gensim, go to *[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Make Your Bed Code***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the *dream_summary.py* program in Project 3, you learned the fundamentals
    of text extraction. Since you’ve seen some of the details, use gensim as a streamlined
    alternative to *dream_summary.py*. Name this new program *bed_summary.py* or download
    it from the book’s website.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing Modules, Scraping the Web, and Preparing the Speech String**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-6](ch03.xhtml#ch03list6) repeats the code used in *dream_summary.py*
    to prepare the speech as a string. To revisit the detailed code explanation, see
    [page 54](ch03.xhtml#page_54).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-6: Importing modules and loading the speech as a string'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll test gensim on the raw speech scraped from the web, so you won’t need
    modules for cleaning the text. The gensim module will also do any counting internally,
    so you don’t need Counter, but you will need gensim’s summarize() function to
    summarize the text ➊. The only other change is to the url address ➋.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarizing the Speech**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-7](ch03.xhtml#ch03list7) completes the program by summarizing the
    speech and printing the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-7: Running gensim, removing duplicate lines, and printing the summary'
  prefs: []
  type: TYPE_NORMAL
- en: Start by printing a header for your summary. Then, call the gensim summarize()
    function to summarize the speech in 225 words. This word count will produce about
    15 sentences, assuming the average sentence has 15 words. In addition to a word
    count, you can pass summarize() a ratio, such as ratio=0.01. This will produce
    a summary whose length is 1 percent of the full document.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you could summarize the speech and print the summary in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, gensim sometimes duplicates sentences in summaries, and that
    occurs here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To avoid duplicating text, you first need to break out the sentences in the
    summary variable using the NLTK sent_tokenize() function. Then make a set from
    these sentences, which will remove duplicates. Finish by printing the results.
  prefs: []
  type: TYPE_NORMAL
- en: Because sets are unordered, the arrangement of the sentences may change if you
    run the program multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If you take the time to read the full speech, you’ll probably conclude that
    gensim produced a fair summary. Although these two results are different, both
    extracted the key points of the speech, including the reference to making your
    bed. Given the size of the document, I find this impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we’ll look at a different way of summarizing text using keywords and
    word clouds.
  prefs: []
  type: TYPE_NORMAL
- en: '**Project #5: Summarizing Text with Word Clouds**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *word cloud* is a visual representation of text data used to display keyword
    metadata, called *tags* on websites. In a word cloud, font size or color shows
    the importance of each tag or word.
  prefs: []
  type: TYPE_NORMAL
- en: Word clouds are useful for highlighting keywords in a document. For example,
    generating word clouds for each US president’s State of the Union address can
    provide a quick overview of the issues facing the nation that year. In Bill Clinton’s
    first year, the emphasis was on peacetime concerns like healthcare, jobs, and
    taxes ([Figure 3-1](ch03.xhtml#ch03fig1)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1: Word cloud made from 1993 State of the Union address by Bill Clinton'
  prefs: []
  type: TYPE_NORMAL
- en: Less than 10 years later, George W. Bush’s word cloud reveals a focus on security
    ([Figure 3-2](ch03.xhtml#ch03fig2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2: Word cloud made from 2002 State of the Union address by George
    W. Bush'
  prefs: []
  type: TYPE_NORMAL
- en: Another use for word clouds is to extract keywords from customer feedback. If
    words like *poor*, *slow*, and *expensive* dominate, you’ve got a problem! Writers
    can also use the clouds to compare chapters in a book or scenes in a screenplay.
    If the author is using very similar language for action scenes and romantic interludes,
    some editing is needed. If you’re a copywriter, clouds can help you check your
    keyword density for search engine optimization (SEO).
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of ways to generate word clouds, including free websites like
    *[https://www.wordclouds.com/](https://www.wordclouds.com/)* and *[https://www.jasondavies.com/wordcloud/](https://www.jasondavies.com/wordcloud/)*.
    But if you want to fully customize your word cloud or embed the generator within
    another program, you need to do it yourself. In this project, you’ll use a word
    cloud to make a promotional flyer for a school play based on the Sherlock Holmes
    story *The Hound of the Baskervilles*.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using the basic rectangle shown in [Figures 3-1](ch03.xhtml#ch03fig1)
    and [3-2](ch03.xhtml#ch03fig2), you’ll fit the words into an outline of Holmes’s
    head ([Figure 3-3](ch03.xhtml#ch03fig3)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-3: Silhouette of Sherlock Holmes'
  prefs: []
  type: TYPE_NORMAL
- en: This will make for a more recognizable and eye-catching display.
  prefs: []
  type: TYPE_NORMAL
- en: THE OBJECTIVE
  prefs: []
  type: TYPE_NORMAL
- en: Use the wordcloud module to generate a shaped word cloud for a novel.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Word Cloud and PIL Modules***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You’ll use a module called wordcloud to generate the word cloud. You can install
    it using pip.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, if you’re using Anaconda, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find the web page for wordcloud here: *[http://amueller.github.io/word_cloud/](http://amueller.github.io/word_cloud/).*'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also need the Python Imaging Library (PIL) to work with images. Use pip
    again to install it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, for Anaconda, use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In case you’re wondering, pillow is the successor project of PIL, which was
    discontinued in 2011\. To learn more about it, visit *[https://pillow.readthedocs.io/en/stable/](https://pillow.readthedocs.io/en/stable/)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Word Cloud Code***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To make the shaped word cloud, you’ll need an image file and a text file. The
    image shown in [Figure 3-3](ch03.xhtml#ch03fig3) came from iStock by Getty Images
    (*[https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/](https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/)*).
    This represents the “small” resolution at around 500×600 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: A similar but copyright-free image (*holmes.png*) is provided with the book’s
    downloadable files. You can find the text file (*hound.txt*), image file (*holmes.png*),
    and code (*wc_hound.py*) in the *Chapter_3* folder.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing Modules, Text Files, Image Files, and Stop Words**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-8](ch03.xhtml#ch03list8) imports modules, loads the novel, loads
    the silhouette image of Holmes, and creates a set of stop words you’ll want to
    exclude from the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-8: Importing modules and loading text, image, and stop words'
  prefs: []
  type: TYPE_NORMAL
- en: Begin by importing NumPy and PIL. PIL will open the image, and NumPy will turn
    it into a mask. You started using NumPy in [Chapter 1](ch01.xhtml); in case you
    skipped it, see the “Installing the Python Libraries” section on [page 6](ch01.xhtml#page_6).
    Note that the pillow module continues to use the acronym PIL for backward compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need matplotlib, which you downloaded in the “Installing the Python Libraries”
    section of [Chapter 1](ch01.xhtml), to display the word cloud. The wordcloud module
    comes with its own list of stop words, so import STOPWORDS along with the cloud
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Next, load the novel’s text file and store it in a variable named text ➊. As
    described in the discussion of [Listing 2-2](ch02.xhtml#ch02list2) in [Chapter
    2](ch02.xhtml), you may encounter a UnicodeDecodeError when loading the text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In this case, try modifying the open() function by adding encoding and errors
    arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With the text loaded, use PIL’s Image.open() method to open the image of Holmes
    and use NumPy to turn it into an array. If you’re using the iStock image of Holmes,
    change the image’s filename as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Assign the STOPWORDS set imported from wordcloud to the stopwords variable.
    Then update the set with a list of additional words that you want to exclude ➋.
    These will be words like *said* and *now* that dominate the word cloud but add
    no useful content. Determining what they are is an iterative process. You generate
    the word cloud, remove words that you don’t think contribute, and repeat. You
    can comment out this line to see the benefit.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*To update a container like STOPWORDS, you need to know whether it’s a list,
    dictionary, set, and so on. Python’s built-in type() function returns the class
    type of any object passed as an argument. In this case, print(type(STOPWORDS))
    yields <class ''set''>.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating the Word Cloud**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-9](ch03.xhtml#ch03list9) generates the word cloud and uses the silhouette
    as a *mask*, or an image used to hide portions of another image. The process used
    by wordcloud is sophisticated enough to fit the words within the mask, rather
    than simply truncating them at the edges. In addition, numerous parameters are
    available for changing the appearance of the words within the mask.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-9: Generating the word cloud'
  prefs: []
  type: TYPE_NORMAL
- en: Name a variable wc and call WordCloud(). There are a lot of parameters, so I’ve
    placed each on its own line for clarity. For a list and description of all the
    parameters available, visit *[https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: Start by passing the maximum number of words you want to use. The number you
    set will display the *n* most common words in the text. The more words you choose
    to display, the easier it will be to define the edges of the mask and make it
    recognizable. Unfortunately, setting the maximum number too high will also result
    in a lot of tiny, illegible words. For this project, start with 500.
  prefs: []
  type: TYPE_NORMAL
- en: Next, to control the font size and relative importance of each word, set the
    relative_scaling parameter to 0.5. For example, a value of 0 gives preference
    to a word’s rank to determine the font size, while a value of 1 means that words
    that occur twice as often will appear twice as large. Values between 0 and 0.5
    tend to strike the best balance between rank and frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Reference the mask variable and set its background color to white. Assigning
    no color defaults to black. Then reference the stopwords set that you edited in
    the previous listing.
  prefs: []
  type: TYPE_NORMAL
- en: The margin parameter will control the spacing of the displayed words. Using
    0 will result in tightly packed words. Using 2 will allow for some whitespace
    padding.
  prefs: []
  type: TYPE_NORMAL
- en: To place the words around the word cloud, use a random number generator and
    set random_state to 7. There’s nothing special about this value; I just felt that
    it produced an attractive arrangement of words.
  prefs: []
  type: TYPE_NORMAL
- en: The random_state parameter fixes the seed number so that the results are repeatable,
    assuming no other parameters are changed. This means the words will always be
    arranged in the same way. Only integers are accepted.
  prefs: []
  type: TYPE_NORMAL
- en: Now, set contour_width to 2. Any value greater than zero creates an outline
    around a mask. In this case, the outline is squiggly due to the resolution of
    the image ([Figure 3-4](ch03.xhtml#ch03fig4)).
  prefs: []
  type: TYPE_NORMAL
- en: Set the color of the outline to brown using the contour_color parameter. Continue
    using a brownish palette by setting colormap to copper. In matplotlib, a *colormap*
    is a dictionary that maps numbers to colors. The copper colormap produces text
    ranging in color from pale flesh to black. You can see its spectrum, along with
    many other color options, at *[https://matplotlib.org/gallery/color/colormap_reference.html](https://matplotlib.org/gallery/color/colormap_reference.html)*.
    If you don’t specify a colormap, the program will use the default colors.
  prefs: []
  type: TYPE_NORMAL
- en: Use dot notation to call the generate() method to build the word cloud. Pass
    it the text string as an argument. End this listing by naming a colors variable
    and calling the to_array() method on the wc object. This method converts the word
    cloud image into a NumPy array for use with matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4: Example of masked word cloud with an outline (left) versus without
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Plotting the Word Cloud**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 3-10](ch03.xhtml#ch03list10) adds a title to the word cloud and uses
    matplotlib to display it. It also saves the word cloud image as a file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 3-10: Plotting and saving the word cloud'
  prefs: []
  type: TYPE_NORMAL
- en: Start by initializing a matplotlib figure. Then call the title() method and
    pass it the name of the school, along with a font size and color.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll want the name of the play to be bigger and bolder than the other titles.
    Since you can’t change the text style within a string with matplotlib, use the
    text() method to define a new title. Pass it (*x*, *y*) coordinates (based on
    the figure axes), a text string, and text style details. Use trial and error with
    the coordinates to optimize the placement of the text. If you’re using the iStock
    image of Holmes, you may need to change the *x* coordinate from -10 to something
    else to achieve the best balance with the asymmetrical silhouette.
  prefs: []
  type: TYPE_NORMAL
- en: Finish the titles by placing the play’s time and venue at the bottom of the
    figure. You could use the text() method again, but instead, let’s take a look
    at an alternative, pyplot’s suptitle() method. The name stands for “super titles.”
    Pass it the text, the (*x*, *y*) figure coordinates, and styling details.
  prefs: []
  type: TYPE_NORMAL
- en: To display the word cloud, call imshow()—for image show—and pass it the colors
    array you made previously. Specify bilinear for color interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: Turn off the figure axes and display the word cloud by calling show(). If you
    want to save the figure, uncomment the savefig() method. Note that matplotlib
    can read the extension in the filename and save the figure in the correct format.
    As written, the save command will not execute until you manually close the figure.
  prefs: []
  type: TYPE_NORMAL
- en: '***Fine-Tuning the Word Cloud***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 3-10](ch03.xhtml#ch03list10) will produce the word cloud in [Figure
    3-5](ch03.xhtml#ch03fig5). You may get a different arrangement of words as the
    algorithm is stochastic.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-5: The flyer generated by the wc_hound.py code'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change the size of the display by adding an argument when you initialize
    the figure. Here’s an example: plt.figure(figsize=(50, 60)).'
  prefs: []
  type: TYPE_NORMAL
- en: There are many other ways to change the results. For example, setting the margin
    parameter to 10 yields a sparser word cloud ([Figure 3-6](ch03.xhtml#ch03fig6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6: The word cloud generated with margin=10'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the random_state parameter will also rearrange the words within the
    mask ([Figure 3-7](ch03.xhtml#ch03fig7)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-7: The word cloud generated with margin=10 and random_state=6'
  prefs: []
  type: TYPE_NORMAL
- en: Tweaking the max_words and relative_scaling parameters will also change the
    appearance of the word cloud. Depending on how detail-oriented you are, all this
    can be a blessing or a curse!
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you used extraction-based summarization techniques to produce
    a synopsis of Martin Luther King Jr.’s “I Have a Dream” speech. You then used
    a free, off-the-shelf module called gensim to summarize Admiral McRaven’s “Make
    Your Bed” speech with even less code. Finally, you used the wordcloud module to
    create an interesting design with words.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Automate the Boring Stuff with Python: Practical Programming for Total Beginners*
    (No Starch Press, 2015), by Al Sweigart, covers regular expressions in [Chapter
    7](ch07.xhtml) and web scraping in [Chapter 11](ch11.xhtml), including use of
    the requests and Beautiful Soup modules.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Make Your Bed: Little Things That Can Change Your Life*…*And Maybe the World*,
    2nd ed. (Grand Central Publishing, 2017), by William H. McRaven, is a self-help
    book based on the admiral’s commencement address at the University of Texas. You
    can find the actual speech online on *[https://www.youtube.com/](https://www.youtube.com/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Project: Game Night**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use wordcloud to invent a new game for game night. Summarize Wikipedia or IMDb
    synopses of movies and see whether your friends can guess the movie title. [Figure
    3-8](ch03.xhtml#ch03fig8) shows some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-8: Word clouds for two movies released in 2010: How to Train Your
    Dragon and Prince of Persia'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re not into movies, pick something else. Alternatives include famous
    novels, *Star Trek* episodes, and song lyrics ([Figure 3-9](ch03.xhtml#ch03fig9)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-9: Word cloud made from song lyrics (Donald Fagen’s “I.G.Y.”)'
  prefs: []
  type: TYPE_NORMAL
- en: Board games have seen a resurgence in recent years, so you could follow this
    trend and print the word clouds on card stock. Alternatively, you could keep things
    digital and present the player with multiple-choice answers for each cloud. The
    game should keep track of the number of correct answers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Project: Summarizing Summaries**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Test your program from Project 3 on previously summarized text, such as Wikipedia
    pages. Only five sentences produced a good overview of gensim.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Next, try the gensim version from Project 4 on those boring services agreements
    no one ever reads. An example Microsoft agreement is available at *[https://www.microsoft.com/en-us/servicesagreement/default.aspx](https://www.microsoft.com/en-us/servicesagreement/default.aspx)*.
    Of course, to evaluate the results, you’ll have to read the full agreement, which
    almost no one ever does! Enjoy the catch-22!
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Project: Summarizing a Novel**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Write a program that summarizes *The Hound of the Baskervilles* by chapter.
    Keep the chapter summaries short, at around 75 words each.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a copy of the novel with chapter headings, scrape the text off the Project
    Gutenberg site using the following line of code: url = ''http://www.gutenberg.org/files/2852/2852-h/2852-h.htm''.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To break out chapter elements, rather than paragraph elements, use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You’ll also need to select paragraph elements (p_elems) from within each chapter,
    using the same methodology as in *dream_summary.py*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippets show some of the results from using a word count of
    75 per chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Challenge Project: It’s Not Just What You Say, It’s How You Say It!**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The text summarization programs you have written so far print sentences strictly
    by their *order of importance*. That means the last sentence in a speech (or any
    text) might become the first sentence in the summary. The goal of summarization
    is to find the important sentences, but there’s no reason you can’t alter the
    way that they’re displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Write a text summarization program that displays the most important sentences
    in their original *order of appearance*. Compare the results to those produced
    by the program in Project 3\. Does this make a noticeable improvement in the summaries?
  prefs: []
  type: TYPE_NORMAL
