- en: '3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3'
- en: SUMMARIZING SPEECHES WITH NATURAL LANGUAGE PROCESSING
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自然语言处理总结演讲
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: “Water, water everywhere, but not a drop to drink.” This famous line, from *The
    Rime of the Ancient Mariner*, summarizes the present state of digital information.
    According to International Data Corporation, by 2025 we’ll be generating 175 trillion
    gigabytes of digital data *per year*. But most of this data—up to 95 percent—will
    be *unstructured*, which means it’s not organized into useful databases. Even
    now, the key to the cure for cancer may be right at our fingertips yet almost
    impossible to reach.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: “四面水域皆是水，却无一滴可饮。”这句来自《古舟子之歌》的名句总结了数字信息的现状。根据国际数据公司（IDC）的预测，到2025年，我们将每年生成 175
    万亿千兆字节的数字数据。但大部分数据——多达 95%——将是*非结构化*的，这意味着它并未被组织成有用的数据库。即使现在，癌症的治愈方法可能就在我们手边，却几乎无法触及。
- en: To make information easier to discover and consume, we need to reduce the volume
    of data by extracting and repackaging salient points into digestible summaries.
    Because of the sheer volume of data, there’s no way to do this manually. Luckily,
    natural language processing (NLP) helps computers understand both words and context.
    For example, NLP applications can summarize news feeds, analyze legal contracts,
    research patents, study financial markets, capture corporate knowledge, and produce
    study guides.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让信息更容易被发现和消费，我们需要通过提取和重新包装关键点来减少数据量，形成易于理解的摘要。由于数据量庞大，人工处理几乎不可能。幸运的是，自然语言处理（NLP）可以帮助计算机理解单词和上下文。例如，NLP
    应用程序可以总结新闻源、分析法律合同、研究专利、研究金融市场、捕捉企业知识并生成学习指南。
- en: In this chapter, you’ll use Python’s Natural Language Toolkit (NLTK) to generate
    a summary of one of the most famous speeches of all time, “I Have a Dream” by
    Martin Luther King Jr. With an understanding of the basics, you’ll then use a
    streamlined alternative, called gensim, to summarize the popular “Make Your Bed”
    speech by Admiral William H. McRaven. Finally, you’ll use a word cloud to produce
    a fun visual summary of the most frequently used words in Sir Arthur Conan Doyle’s
    novel *The Hound of the Baskervilles*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用 Python 的自然语言工具包（NLTK）生成历史上最著名演讲之一——马丁·路德·金的《我有一个梦想》的摘要。在理解基本概念后，你将使用一种简化的替代方法，称为
    gensim，来总结威廉·H·麦克雷文海军上将的著名演讲《整理床铺》。最后，你将使用词云来制作一个有趣的可视化摘要，展示亚瑟·柯南·道尔的小说《巴斯克维尔的猎犬》中最常用的单词。
- en: '**Project #3: I Have a Dream . . . to Summarize Speeches!**'
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #3：我有一个梦想……来总结演讲！**'
- en: 'In machine learning and data mining, there are two approaches to summarizing
    text: *extraction* and *abstraction*.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和数据挖掘中，有两种总结文本的方法：*提取*和*抽象*。
- en: Extraction-based summarization uses various weighting functions to rank sentences
    by perceived importance. Words used more often are considered more important.
    Consequently, sentences containing those words are considered more important.
    The overall behavior is like using a yellow highlighter to manually select keywords
    and sentences without altering the text. The results can be disjointed, but the
    technique is good at pulling out important words and phrases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提取的总结方法使用各种加权函数根据感知的重要性对句子进行排序。使用频率更高的词被认为更重要。因此，包含这些词的句子被认为更重要。总体行为就像是用黄色荧光笔手动选择关键词和句子，而不改变文本。尽管结果可能会显得支离破碎，但这种技术擅长提取重要的单词和短语。
- en: Abstraction relies on deeper comprehension of the document to capture intent
    and produce more human-like paraphrasing. This includes creating completely new
    sentences. The results tend to be more cohesive and grammatically correct than
    those produced by extraction-based methods, but at a price. Abstraction algorithms
    require advanced and complicated deep learning methods and sophisticated language
    modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象依赖于对文档更深层次的理解，以捕捉意图并生成更具人类化的改写。这包括创造全新的句子。与基于提取的方法相比，抽象方法的结果往往更加连贯且语法正确，但也有其代价。抽象算法需要先进且复杂的深度学习方法和精密的语言建模。
- en: For this project, you’ll use an extraction-based technique on the “I Have a
    Dream” speech, delivered by Martin Luther King Jr. at the Lincoln Memorial on
    August 28, 1963\. Like Lincoln’s “Gettysburg Address” a century before, it was
    the perfect speech at the perfect time. Dr. King’s masterful use of repetition
    also makes it tailor-made for extraction techniques, which correlate word frequency
    with importance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将对马丁·路德·金（Dr. Martin Luther King Jr.）1963年8月28日在林肯纪念堂发表的“我有一个梦想”演讲使用提取式技术。与林肯100年前的“葛底斯堡演说”一样，这也是在完美的时刻发表的完美演讲。金博士巧妙地运用了重复手法，这使得这篇演讲特别适合使用提取技术，通过关联单词频率与重要性来提取关键信息。
- en: THE OBJECTIVE
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Write a Python program that summarizes a speech using NLP text extraction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个Python程序，使用自然语言处理（NLP）文本提取功能总结演讲内容。
- en: '***The Strategy***'
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***策略***'
- en: The Natural Language Toolkit includes the functions you’ll need to summarize
    Dr. King’s speech. If you skipped [Chapter 2](ch02.xhtml), see [page 29](ch02.xhtml#page_29)
    for installation instructions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言工具包（NLTK）包含了你需要用来总结马丁·路德·金（Dr. King）演讲的功能。如果你跳过了[第2章](ch02.xhtml)，请查看[第29页](ch02.xhtml#page_29)获取安装说明。
- en: To summarize the speech, you’ll need a digital copy. In previous chapters, you
    manually downloaded files you needed from the internet. This time you’ll use a
    more efficient technique, called *web scraping*, which allows you to programmatically
    extract and save large amounts of data from websites.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结演讲内容，你需要一个数字化副本。在之前的章节中，你手动从互联网上下载了需要的文件。这一次，你将使用一种更高效的技术——*网络爬取*，它可以让你编程方式从网站中提取并保存大量数据。
- en: Once you’ve loaded the speech as a string, you can use NLTK to split out and
    count individual words. Then, you’ll “score” each sentence in the speech by summing
    the word counts within it. You can use those scores to print the top-ranked sentences,
    based on how many sentences you want in your summary.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将演讲加载为字符串，你可以使用NLTK来分割并统计每个单词。接着，你将通过将每个句子中的单词计数求和，来“评分”每个句子。你可以根据句子的得分来打印排名最高的句子，基于你希望在总结中包含多少句子。
- en: '***Web Scraping***'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***网络爬取***'
- en: Scraping the web means using a program to download and process content. This
    is such a common task that prewritten scraping programs are freely available.
    You’ll use the requests library to download files and web pages, and you’ll use
    the Beautiful Soup (bs4) package to parse HTML. Short for *Hypertext Markup Language*,
    HTML is the standard format used to create web pages.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬取是指使用程序下载和处理内容。这是一个非常常见的任务，现成的爬虫程序可以自由获取。你将使用requests库来下载文件和网页，并使用Beautiful
    Soup（bs4）包来解析HTML。HTML是*超文本标记语言*（Hypertext Markup Language）的缩写，是用于创建网页的标准格式。
- en: 'To install the two modules, use pip in a terminal window or Windows PowerShell
    (see [page 8](ch01.xhtml#page_8) in [Chapter 1](ch01.xhtml) for instructions on
    installing and using pip):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装这两个模块，请在终端窗口或Windows PowerShell中使用pip（有关使用和安装pip的说明，请参见[第8页](ch01.xhtml#page_8)的[第1章](ch01.xhtml)）。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To check the installation, open the shell and import each module as shown next.
    If you don’t get an error, you’re good to go!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查安装情况，打开终端并按如下方式导入每个模块。如果没有报错，说明安装成功，可以开始使用了！
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To learn more about requests, visit *[https://pypi.org/project/requests/](https://pypi.org/project/requests/)*.
    For Beautiful Soup, see *[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于requests的信息，请访问*[https://pypi.org/project/requests/](https://pypi.org/project/requests/)*。有关Beautiful
    Soup的更多信息，请查看*[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)*。
- en: '***The “I Have a Dream” Code***'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***“我有一个梦想”代码***'
- en: 'The *dream_summary.py* program performs the following steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*dream_summary.py*程序执行以下步骤：'
- en: Opens a web page containing the “I Have a Dream” speech
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开包含“我有一个梦想”演讲的网页
- en: Loads the text as a string
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本加载为字符串
- en: Tokenizes the text into words and sentences
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本分解为单词和句子
- en: Removes stop words with no contextual content
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除没有上下文内容的停用词
- en: Counts the remaining words
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算剩余单词的数量
- en: Uses the counts to rank the sentences
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计数来对句子进行排名
- en: Displays the highest-ranking sentences
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示排名最高的句子
- en: If you’ve already downloaded the book’s files, find the program in the *Chapter_3*
    folder. Otherwise, go to *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*
    and download it from the book’s GitHub page.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经下载了书中的文件，请在*Chapter_3*文件夹中找到该程序。否则，访问*[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*并从书籍的GitHub页面下载。
- en: '**Importing Modules and Defining the main() Function**'
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入模块并定义main()函数**'
- en: '[Listing 3-1](ch03.xhtml#ch03list1) imports modules and defines the first part
    of the main() function, which scrapes the web and assigns the speech to a variable
    as a string.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 3-1](ch03.xhtml#ch03list1) 导入模块并定义了 main() 函数的第一部分，该部分用于抓取网页并将演讲内容作为字符串赋值给变量。'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 3-1: Importing modules and defining the main() function'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3-1：导入模块并定义 main() 函数
- en: Start by importing Counter from the collections module to help you keep track
    of the sentence scoring. The collections module is part of the Python Standard
    Library and includes several container data types. A Counter is a dictionary subclass
    for counting hashable objects. Elements are stored as dictionary keys, and their
    counts are stored as dictionary values.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从 collections 模块导入 Counter，用于帮助你跟踪句子的得分。collections 模块是 Python 标准库的一部分，包含了几种容器数据类型。Counter
    是字典的子类，用于计数可哈希对象。元素作为字典的键存储，而它们的计数作为字典的值存储。
- en: Next, to clean up the speech prior to summarizing its contents, import the re
    module. The *re* stands for *regular expressions*, also referred to as *regexes*,
    which are sequences of characters that define a search pattern. This module will
    help you clean up the speech by allowing you to selectively remove bits that you
    don’t want.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了在总结内容之前清理演讲内容，导入 re 模块。*re* 代表 *正则表达式*，也称为 *regex*，它们是定义搜索模式的字符序列。这个模块将帮助你通过允许你有选择地删除不需要的部分来清理演讲内容。
- en: Finish the imports with the modules for scraping the web and doing natural language
    processing. The last module brings in the list of functional stop words (such
    as *if*, *and*, *but*, *for*) that contain no useful information. You’ll remove
    these from the speech prior to summarization.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用抓取网页和进行自然语言处理的模块完成导入。最后一个模块引入了功能性停用词的列表（例如 *if*、*and*、*but*、*for*），这些词不包含有用信息。在总结之前，你将从演讲中移除这些词。
- en: Next, define a main() function to run the program. To scrape the speech off
    the web, provide the url address as a string ➊. You can copy and paste this from
    the website from which you want to extract text.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个 main() 函数来运行程序。为了从网上抓取演讲内容，提供 url 地址作为字符串 ➊。你可以从想要提取文本的网站复制并粘贴这个地址。
- en: The requests library abstracts the complexities of making HTTP requests in Python.
    HTTP, short for HyperText Transfer Protocol, is the foundation of data communication
    using hyperlinks on the World Wide Web. Use the requests.get() method to fetch
    the url and assign the output to the page variable, which references the Response
    object the web page returned for the request. This object’s text attribute holds
    the web page, including the speech, as a string.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: requests 库抽象了在 Python 中发起 HTTP 请求的复杂性。HTTP，即超文本传输协议，是通过超链接在万维网上进行数据通信的基础。使用
    requests.get() 方法来获取 url，并将输出赋值给 page 变量，该变量引用了网页为请求返回的 Response 对象。该对象的 text
    属性包含了网页内容，包括演讲内容，以字符串形式呈现。
- en: To check that the download was successful, call the Response object’s raise_for_status()
    method. This does nothing if everything goes okay but otherwise will raise an
    exception and halt the program.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查下载是否成功，调用 Response 对象的 raise_for_status() 方法。如果一切正常，这个方法什么也不做；否则，它会引发异常并终止程序。
- en: 'At this point, the data is in HTML, as shown here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，数据是 HTML 格式，如下所示：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, HTML has a lot of *tags*, such as <head> and <p>, that let your
    browser know how to format the web page. The text between starting and closing
    tags is called an *element*. For example, the text “Martin Luther King Jr.’s 1962
    Speech” is a title element sandwiched between the starting tag <title> and the
    closing tag </title>. Paragraphs are formatted using <p> and </p> tags.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，HTML 中有许多 *标签*，例如 <head> 和 <p>，它们告诉浏览器如何格式化网页。开始标签和结束标签之间的文本称为 *元素*。例如，“马丁·路德·金
    Jr. 的 1962 演讲”是一个标题元素，它被夹在开始标签 <title> 和结束标签 </title> 之间。段落使用 <p> 和 </p> 标签进行格式化。
- en: Because these tags are not part of the original text, they should be removed
    prior to any natural language processing. To remove the tags, call the bs4.BeautifulSoup()
    method and pass it the string containing the HTML ➋. Note that I’ve explicitly
    specified html.parser. The program will run without this but complain bitterly
    with warnings in the shell.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些标签不是原始文本的一部分，所以在进行任何自然语言处理之前应该删除它们。为了移除这些标签，调用 bs4.BeautifulSoup() 方法，并将包含
    HTML 的字符串传递给它 ➋。请注意，我已经明确指定了 html.parser。即使没有这个指定，程序仍然可以运行，但会在终端中发出警告。
- en: The soup variable now references a BeautifulSoup object, which means you can
    use the object’s find_all() method to locate the speech buried in the HTML document.
    In this case, to find the text between paragraph tags (<p>), use list comprehension
    and find_all() to make a list of just the paragraph elements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: soup变量现在引用一个BeautifulSoup对象，这意味着你可以使用该对象的find_all()方法来定位HTML文档中的演讲内容。在此案例中，通过列表推导和find_all()来获取仅包含段落元素的列表，以查找段落标签（<p>）之间的文本。
- en: Finish by turning the speech into a continuous string. Use the join() method
    to turn the p_elems list into a string. Set the “joiner” character to a space,
    designated by ''.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将演讲内容转化为连续的字符串。使用join()方法将p_elems列表转化为字符串。设置“连接符”字符为空格，用''表示。
- en: 'Note that with Python, there is usually more than one way to accomplish a task.
    The last two lines of the listing can also be written as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Python中，通常有不止一种方式可以完成任务。列表中的最后两行也可以按以下方式编写：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The select() method is more limited overall than find_all(), but in this case
    it works the same and is more succinct. In the previous snippet, select() finds
    the <p> tags, and the results are converted to text when concatenated to the speech
    string.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: select()方法总体上比find_all()方法功能更有限，但在这种情况下，它的作用与find_all()相同，而且更简洁。在前面的代码中，select()方法查找<p>标签，结果在与演讲内容字符串连接时转换为文本。
- en: '**Completing the main() Function**'
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**完成main()函数**'
- en: Next, you’ll prep the speech to fix typos and remove punctuation, special characters,
    and spaces. Then you’ll call three functions to remove stop words, count word
    frequency, and score the sentences based on the word counts. Finally, you’ll rank
    the sentences and display those with the highest scores in the shell.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将准备演讲内容，修正拼写错误并去除标点符号、特殊字符和空格。然后，你将调用三个函数，分别去除停用词、计算词频并根据词频对句子进行评分。最后，你将对句子进行排序，并在命令行显示得分最高的句子。
- en: '[Listing 3-2](ch03.xhtml#ch03list2) completes the definition of main() that
    performs these tasks.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3-2](ch03.xhtml#ch03list2)完成了执行这些任务的main()函数定义。'
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 3-2: Completing the main() function'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3-2：完成main()函数
- en: The original document contains a typo (*mowing* instead of *knowing*), so start
    by fixing this using the string.replace() method. Continue cleaning the speech
    using regex. Many casual programmers are turned off by this module’s arcane syntax,
    but it’s such a powerful and useful tool that everyone should be aware of the
    basic regex syntax.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文档包含一个拼写错误（*mowing* 应为 *knowing*），因此首先使用string.replace()方法修正这个错误。接着，继续使用正则表达式清理演讲内容。许多非专业程序员对这个模块的复杂语法感到厌烦，但它是一个强大且有用的工具，每个人都应该了解基本的正则表达式语法。
- en: Remove extra spaces using the re.sub() function, which replaces substrings with
    new characters. Use the shorthand character class code \s+ to identify runs of
    whitespace and replace them with a single space, indicated by ' '. Finish by passing
    re.sub() the name of the string (speech).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用re.sub()函数去除多余的空格，该函数将子字符串替换为新的字符。使用简写字符类代码\s+来识别连续的空白符并将其替换为一个空格，表示为' '。最后，将字符串（speech）传递给re.sub()方法。
- en: Next, remove anything that’s *not* a letter by matching the [^a-zA-Z] pattern.
    The caret at the start instructs regex to “match any character that isn’t between
    the brackets.” So, numbers, punctuation marks, and so on, will be replaced by
    a space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用[^a-zA-Z]模式删除任何*不是*字母的字符。开头的脱字符指示正则表达式“匹配任何不在括号中的字符”。因此，数字、标点符号等将被空格替换。
- en: Removing characters like punctuation marks will leave an extra space. To get
    rid of these spaces, call the re.sub() method again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 删除标点符号等字符会留下额外的空格。为了去除这些空格，再次调用re.sub()方法。
- en: Next, request that the user input the number of sentences to include in the
    summary and the maximum number of words per sentence. Use a while loop and Python’s
    built-in isdigit() function to ensure the user inputs an integer ➊.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要求用户输入要包括在摘要中的句子数量以及每个句子的最大字数。使用while循环和Python内置的isdigit()函数确保用户输入一个整数➊。
- en: '**NOTE**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*According to research by the American Press Institute, comprehension is best
    with sentences of fewer than 15 words. Similarly, the Oxford Guide to Plain English
    recommends using sentences that average 15 to 20 words over a full document.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*根据美国新闻研究所的研究，理解力最佳的句子长度为不超过15个单词。同样，《牛津简明英语指南》建议，整篇文档中的句子平均长度应为15到20个单词。*'
- en: Continue cleaning the text by calling the remove_stop_words() function. Then
    call functions get_word_freq() and score_sentences() to calculate the frequency
    of the remaining words and to score the sentences, respectively. You’ll define
    these functions after completing the main() function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 继续清理文本，调用remove_stop_words()函数。然后调用get_word_freq()和score_sentences()函数，分别计算剩余单词的频率并为句子打分。你将在完成main()函数后定义这些函数。
- en: To rank the sentences, call the collection module’s Counter() method ➋. Pass
    it the sent_scores variable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要对句子进行排名，调用集合模块的Counter()方法 ➋。将sent_scores变量传入该方法。
- en: To generate the summary, use the Counter object’s most_common() method. Pass
    it the num_sents variable input by the user. The resulting summary variable will
    hold a list of tuples. For each tuple, the sentence is at index [0], and its rank
    is at index [1].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成摘要，使用Counter对象的most_common()方法。将用户输入的num_sents变量传递给该方法。结果摘要变量将包含一个元组列表。每个元组中的第一个元素是句子，第二个元素是其排名。
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For readability, print each sentence of the summary on a separate line.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性，将摘要中的每个句子打印在单独的一行上。
- en: '**Removing Stop Words**'
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**去除停用词**'
- en: Remember from [Chapter 2](ch02.xhtml) that stop words are short, functional
    words like *if*, *but*, *for*, and *so*. Because they contain no important contextual
    information, you don’t want to use them to rank sentences.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 记住在[第2章](ch02.xhtml)中提到的，停用词是像*if*、*but*、*for*和*so*这样的短小功能词。由于它们不包含重要的上下文信息，因此不应使用它们来排名句子。
- en: '[Listing 3-3](ch03.xhtml#ch03list3) defines a function called remove_stop_words()
    to remove stop words from the speech.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-3](ch03.xhtml#ch03list3)定义了一个名为remove_stop_words()的函数，用来从语音中移除停用词。'
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Listing 3-3: Defining a function to remove stop words from the speech'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3-3：定义一个函数来移除语音中的停用词
- en: Define the function to receive speech_edit, the edited speech string, as an
    argument. Then create a set of the English stop words in NLTK. Use a set, rather
    than a list, as searches are quicker in sets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 定义函数时，接收speech_edit作为参数，该参数是编辑后的语音字符串。然后，创建一个包含NLTK中的英语停用词的集合。使用集合而不是列表，因为集合的查找速度更快。
- en: Assign an empty string to hold the edited speech sans stop words. The speech_edit
    variable is currently a string in which each element is a letter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保存去除停用词后的编辑语音，分配一个空字符串给该变量。speech_edit变量目前是一个字符串，其中每个元素都是一个字母。
- en: To work with words, call the NLTK word_tokenize() method. Note that you can
    do this while looping through words. Convert each word to lowercase and check
    its membership in the stop_words set. If it’s not a stop word, concatenate it
    to the new string, along with a space. Return this string to end the function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理单词，调用NLTK的word_tokenize()方法。注意，你可以在循环遍历单词时执行此操作。将每个单词转换为小写，并检查它是否在stop_words集合中。如果它不是停用词，就将其与一个空格一起拼接到新字符串中。函数返回该字符串并结束。
- en: 'How you handle letter case in this program is important. You’ll want the summary
    to print with both uppercase and lowercase letters, but you must do the NLP work
    using all lowercase to avoid miscounting. To see why, look at the following code
    snippet, which counts words in a string (s) with mixed cases:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序中，字母的大小写处理非常重要。你希望摘要能同时打印出大写和小写字母，但在进行自然语言处理时，必须将所有字母转换为小写，以避免错误计数。为了理解原因，请看下面这段代码示例，它统计了一个包含大小写混合的字符串（s）中的单词：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you don’t convert the words to lowercase, *one* and *One* are considered
    distinct elements. For counting purposes, every instance of *one* regardless of
    its case should be treated as the same word. Otherwise, the contribution of *one*
    to the document will be diluted.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不将单词转换为小写，*one* 和 *One* 会被认为是不同的元素。为了计数，每个*one*的实例，不论其大小写，应该被视为相同的单词。否则，*one*对文档的贡献将被稀释。
- en: '**Calculating the Frequency of Occurrence of Words**'
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**计算单词出现频率**'
- en: To count the occurrence of each word in the speech, you’ll create the get_word_freq()
    function that returns a dictionary with the words as keys and the counts as values.
    [Listing 3-4](ch03.xhtml#ch03list4) defines this function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了统计每个单词在语音中的出现次数，你需要创建一个名为get_word_freq()的函数，该函数返回一个字典，字典的键是单词，值是对应的出现次数。[示例
    3-4](ch03.xhtml#ch03list4)定义了这个函数。
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 3-4: Defining a function to calculate word frequency in the speech'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3-4：定义一个函数来计算语音中的单词频率
- en: The get_word_freq() function takes the edited speech string with no stop words
    as an argument. NLTK’s FreqDist class acts like a dictionary with the words as
    keys and their counts as values. As part of the process, convert the input string
    to lowercase and tokenize it into words. End the function by returning the word_freq
    dictionary.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: get_word_freq() 函数以编辑过的没有停用词的语音字符串作为参数。NLTK 的 FreqDist 类类似于一个字典，单词作为键，计数作为值。作为过程的一部分，将输入字符串转换为小写，并将其分割成单词。结束函数时，返回
    word_freq 字典。
- en: '**Scoring Sentences**'
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**评分句子**'
- en: '[Listing 3-5](ch03.xhtml#ch03list5) defines a function that scores sentences
    based on the frequency distribution of the words they contain. It returns a dictionary
    with each sentence as the key and its score as the value.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3-5](ch03.xhtml#ch03list5) 定义了一个函数，该函数根据句子中单词的频率分布对句子进行评分。它返回一个字典，每个句子作为键，其得分作为值。'
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Listing 3-5: Defining a function to score sentences based on word frequency'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3-5：定义一个根据单词频率为句子评分的函数
- en: Define a function, called score_sentences(), with parameters for the original
    speech string, the word_freq object, and the max_words variable input by the user.
    You want the summary to contain stop words and capitalized words—hence the use
    of speech.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为 score_sentences() 的函数，参数包括原始的语音字符串、word_freq 对象和用户输入的 max_words 变量。你希望摘要包含停用词和大写单词——因此使用语音字符串。
- en: Start an empty dictionary, named sent_scores, to hold the scores for each sentence.
    Next, tokenize the speech string into sentences.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空字典，命名为 sent_scores，用于保存每个句子的得分。接下来，将语音字符串分割成句子。
- en: Now, start looping through the sentences ➊. Start by updating the sent_scores
    dictionary, assigning the sentence as the key, and setting its initial value (count)
    to 0.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始遍历句子 ➊。首先更新 sent_scores 字典，将句子作为键，并将其初始值（计数）设为 0。
- en: To count word frequency, you first need to tokenize the sentence into words.
    Be sure to use lowercase to be compatible with the word_freq dictionary.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算单词频率，首先需要将句子分割成单词。确保使用小写字母，以便与 word_freq 字典兼容。
- en: You’ll need to be careful when you sum up the word counts per sentence to create
    the scores so you don’t bias the results toward longer sentences. After all, longer
    sentences are more likely to have a greater number of important words. To avoid
    excluding short but important sentences, you need to *normalize* each count by
    dividing it by the sentence *length*. Store the length in a variable called sent_word_count.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在对每个句子的单词数进行求和以创建得分时，你需要小心，以免将结果偏向较长的句子。毕竟，较长的句子更可能包含更多重要单词。为了避免排除短但重要的句子，你需要通过将其除以句子的
    *长度* 来对每个计数进行 *标准化*。将长度存储在一个名为 sent_word_count 的变量中。
- en: Next, use a conditional that constrains sentences to the maximum length input
    by the user ➋. If the sentence passes the test, start looping through its words.
    If a word is in the word_freq dictionary, add it to the count stored in sent_scores.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用一个条件语句，限制句子的长度为用户输入的最大值 ➋。如果句子通过测试，开始遍历其中的单词。如果一个单词出现在 word_freq 字典中，就将它添加到存储在
    sent_scores 中的计数值中。
- en: At the end of each loop through the sentences, divide the score for the current
    sentence by the number of words in the sentence ➌. This normalizes the score so
    long sentences don’t have an unfair advantage.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次循环结束时，将当前句子的得分除以句子中的单词数 ➌。这会将得分标准化，避免长句子占有不公平的优势。
- en: End the function by returning the sent_scores dictionary. Then, back in the
    global space, add the code for running the program as a module or in stand-alone
    mode.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过返回 sent_scores 字典来结束函数。然后，在全局空间中，添加运行程序的代码，无论是作为模块还是独立模式。
- en: '**Running the Program**'
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**运行程序**'
- en: Run the *dream_summary.py* program with a maximum sentence length of 14 words.
    As mentioned previously, good, readable sentences tend to contain 14 words or
    fewer. Then truncate the summary at 15 sentences, about one-third of the speech.
    You should get the following results. Note that the sentences won’t necessarily
    appear in their original order.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *dream_summary.py* 程序，设置最大句子长度为 14 个单词。如前所述，良好、易读的句子通常包含 14 个单词或更少。然后将摘要截断为
    15 个句子，约为演讲的三分之一。你应该得到以下结果。请注意，句子的顺序不一定与原文相同。
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Not only does the summary capture the title of the speech, it captures the main
    points.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要不仅捕捉了演讲的标题，还捕捉了主要内容。
- en: But if you run it again with 10 words per sentence, a lot of the sentences are
    clearly too long. Because there are only 7 sentences in the whole speech with
    10 or fewer words, the program can’t honor the input requirements. It defaults
    to printing the speech from the beginning until the sentence count is at least
    what was specified in the num_sents variable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你再次运行程序，将每个句子的字数限制为 10，很多句子明显太长。由于整个演讲中只有 7 个句子包含 10 个或更少的字，因此程序无法遵循输入要求。它会默认从演讲的开头开始打印，直到句子数至少达到
    num_sents 变量中指定的数目。
- en: Now, rerun the program and try setting the word count limit to 1,000.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重新运行程序并尝试将字数限制设置为 1,000。
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Although longer sentences don’t dominate the summary, a few slipped through,
    making this summary less poetic than the previous one. The lower word count limit
    forces the previous version to rely more on shorter phrases that act like a chorus.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管较长的句子并未主导摘要，但仍有一些被遗漏，使得这个摘要比之前的版本少了些诗意。较低的字数限制迫使之前的版本更多依赖于短语，这些短语起到了类似副歌的作用。
- en: '**Project #4: Summarizing Speeches with gensim**'
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #4：使用 gensim 摘要演讲**'
- en: 'In an Emmy award–winning episode of *The Simpsons*, Homer runs for sanitation
    commissioner using the campaign slogan, “Can’t someone else do it?” That’s certainly
    the case with many Python applications: often, when you need to write a script,
    you learn that someone else has already done it! One example is gensim, an open
    source library for natural language processing using statistical machine learning.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在一集获得艾美奖的 *辛普森一家*（The Simpsons）中，霍默竞选卫生委员会委员，使用的竞选口号是：“难道不能让别人做吗？”这也正是许多 Python
    应用程序的情况：经常在你需要编写脚本时，发现别人已经做过了！一个例子就是 gensim，这是一个用于自然语言处理的开源库，使用统计机器学习。
- en: The word *gensim* stands for “generate similar.” It uses a graph-based ranking
    algorithm called TextRank. This algorithm was inspired by PageRank, invented by
    Larry Page and used to rank web pages in Google searches. With PageRank, the importance
    of a website is determined by how many other pages link to it. To use this approach
    with text processing, algorithms measure how similar each sentence is to all the
    other sentences. The sentence that is the most like the others is considered the
    most important.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*gensim* 这个词代表“生成相似的（generate similar）”。它使用了一种基于图形的排名算法，称为 TextRank。这种算法受 PageRank
    的启发，PageRank 是由拉里·佩奇（Larry Page）发明的，用于在 Google 搜索中对网页进行排名。使用 PageRank 时，网站的重要性是由指向该网站的其他页面的数量来决定的。要将这种方法应用于文本处理，算法会衡量每个句子与其他句子的相似度。与其他句子最相似的句子被认为是最重要的。'
- en: In this project, you’ll use gensim to summarize Admiral William H. McRaven’s
    commencement address, “Make Your Bed,” given at the University of Texas at Austin
    in 2014\. This inspirational, 20-minute speech has been viewed more than 10 million
    times on YouTube and inspired a *New York Times* bestselling book in 2017.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用 gensim 来总结威廉·H·麦克雷文（William H. McRaven）海军上将于 2014 年在德克萨斯大学奥斯汀分校所做的毕业演讲《整理床铺》。这场鼓舞人心的
    20 分钟演讲在 YouTube 上已经观看超过 1,000 万次，并于 2017 年激发了一本 *纽约时报* 的畅销书。
- en: THE OBJECTIVE
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Write a Python program that uses the gensim module to summarize a speech.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个使用 gensim 模块来总结演讲的 Python 程序。
- en: '***Installing gensim***'
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***安装 gensim***'
- en: The gensim module runs on all the major operating systems but is dependent on
    NumPy and SciPy. If you don’t have them installed, go back to [Chapter 1](ch01.xhtml)
    and follow the instructions in “Installing the Python Libraries” on [page 6](ch01.xhtml#page_6).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: gensim 模块可以在所有主要操作系统上运行，但依赖于 NumPy 和 SciPy。如果你没有安装它们，请返回到 [第 1 章](ch01.xhtml)，并按照“安装
    Python 库”中的说明操作，[第 6 页](ch01.xhtml#page_6)。
- en: To install gensim on Windows, use pip install -U gensim. To install it in a
    terminal, use pip install --upgrade gensim. For conda environments, use conda
    install -c conda-forge gensim. For more on gensim, go to *[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上安装 gensim，使用 pip install -U gensim。在终端中安装，使用 pip install --upgrade
    gensim。对于 conda 环境，使用 conda install -c conda-forge gensim。有关 gensim 的更多信息，请访问
    *[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)*。
- en: '***The Make Your Bed Code***'
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***整理床铺代码***'
- en: With the *dream_summary.py* program in Project 3, you learned the fundamentals
    of text extraction. Since you’ve seen some of the details, use gensim as a streamlined
    alternative to *dream_summary.py*. Name this new program *bed_summary.py* or download
    it from the book’s website.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目 3 中，你通过 *dream_summary.py* 程序学习了文本提取的基本概念。既然你已经了解了一些细节，现在可以使用 gensim 作为
    *dream_summary.py* 的简化替代方案。命名这个新程序为 *bed_summary.py*，或者从本书网站下载它。
- en: '**Importing Modules, Scraping the Web, and Preparing the Speech String**'
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入模块、抓取网页并准备演讲字符串**'
- en: '[Listing 3-6](ch03.xhtml#ch03list6) repeats the code used in *dream_summary.py*
    to prepare the speech as a string. To revisit the detailed code explanation, see
    [page 54](ch03.xhtml#page_54).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 3-6](ch03.xhtml#ch03list6)重复了在*dream_summary.py*中用于准备演讲的代码。要重新查看详细的代码解释，请参阅[第
    54 页](ch03.xhtml#page_54)。'
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Listing 3-6: Importing modules and loading the speech as a string'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3-6：导入模块并将演讲加载为字符串
- en: You’ll test gensim on the raw speech scraped from the web, so you won’t need
    modules for cleaning the text. The gensim module will also do any counting internally,
    so you don’t need Counter, but you will need gensim’s summarize() function to
    summarize the text ➊. The only other change is to the url address ➋.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在从网页抓取的原始演讲文本上测试 gensim，因此不需要用于清理文本的模块。gensim 模块也会在内部进行任何计数，因此您不需要 Counter，但您需要
    gensim 的 summarize() 函数来总结文本 ➊。唯一的其他变化是 url 地址 ➋。
- en: '**Summarizing the Speech**'
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**总结演讲内容**'
- en: '[Listing 3-7](ch03.xhtml#ch03list7) completes the program by summarizing the
    speech and printing the results.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 3-7](ch03.xhtml#ch03list7)通过总结演讲并打印结果完成了程序。'
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Listing 3-7: Running gensim, removing duplicate lines, and printing the summary'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3-7：运行 gensim，删除重复行并打印摘要
- en: Start by printing a header for your summary. Then, call the gensim summarize()
    function to summarize the speech in 225 words. This word count will produce about
    15 sentences, assuming the average sentence has 15 words. In addition to a word
    count, you can pass summarize() a ratio, such as ratio=0.01. This will produce
    a summary whose length is 1 percent of the full document.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先打印摘要的标题。然后，调用 gensim 的 summarize() 函数来总结 225 个字的演讲。根据平均每句15个字的假设，这个字数将产生约15个句子的摘要。除了字数，您还可以传递一个比例给
    summarize()，例如 ratio=0.01。这将生成一个长度为完整文档1%的摘要。
- en: Ideally, you could summarize the speech and print the summary in one step.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，您可以一步完成演讲的总结和打印摘要。
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Unfortunately, gensim sometimes duplicates sentences in summaries, and that
    occurs here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，gensim 有时会在摘要中重复句子，这在这里发生了。
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To avoid duplicating text, you first need to break out the sentences in the
    summary variable using the NLTK sent_tokenize() function. Then make a set from
    these sentences, which will remove duplicates. Finish by printing the results.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重复文本，您首先需要使用 NLTK 的 sent_tokenize() 函数将摘要变量中的句子分解。然后从这些句子中创建一个集合，这将删除重复项。最后打印结果。
- en: Because sets are unordered, the arrangement of the sentences may change if you
    run the program multiple times.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因为集合是无序的，所以如果您多次运行程序，句子的排列可能会发生变化。
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If you take the time to read the full speech, you’ll probably conclude that
    gensim produced a fair summary. Although these two results are different, both
    extracted the key points of the speech, including the reference to making your
    bed. Given the size of the document, I find this impressive.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你花时间阅读全文，你可能会得出结论，gensim生成了一个公平的摘要。尽管这两个结果不同，但两者都提取了演讲的要点，包括提到如何整理床铺。考虑到文档的大小，我觉得这很令人印象深刻。
- en: Next up, we’ll look at a different way of summarizing text using keywords and
    word clouds.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看使用关键词和词云总结文本的另一种方法。
- en: '**Project #5: Summarizing Text with Word Clouds**'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #5：使用词云总结文本**'
- en: A *word cloud* is a visual representation of text data used to display keyword
    metadata, called *tags* on websites. In a word cloud, font size or color shows
    the importance of each tag or word.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*词云*是用于显示网站上关键词元数据的文本数据的视觉表示。在词云中，字体大小或颜色显示每个标签或单词的重要性。'
- en: Word clouds are useful for highlighting keywords in a document. For example,
    generating word clouds for each US president’s State of the Union address can
    provide a quick overview of the issues facing the nation that year. In Bill Clinton’s
    first year, the emphasis was on peacetime concerns like healthcare, jobs, and
    taxes ([Figure 3-1](ch03.xhtml#ch03fig1)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 词云对于突出文档中的关键词非常有用。例如，为每位美国总统的国情咨文生成词云可以快速概述该年国家面临的问题。在比尔·克林顿的首年，重点放在了像医疗保健、就业和税收等和平时期的关切事项上（[图
    3-1](ch03.xhtml#ch03fig1)）。
- en: '![Image](../images/fig03_01.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/fig03_01.jpg)'
- en: 'Figure 3-1: Word cloud made from 1993 State of the Union address by Bill Clinton'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-1：由比尔·克林顿1993年国情咨文制作的词云
- en: Less than 10 years later, George W. Bush’s word cloud reveals a focus on security
    ([Figure 3-2](ch03.xhtml#ch03fig2)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 不到10年后，乔治·W·布什的词云显示出对安全问题的关注（[图 3-2](ch03.xhtml#ch03fig2)）。
- en: '![Image](../images/fig03_02.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/fig03_02.jpg)'
- en: 'Figure 3-2: Word cloud made from 2002 State of the Union address by George
    W. Bush'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-2：由乔治·W·布什在 2002 年的国情咨文中演讲的词云。
- en: Another use for word clouds is to extract keywords from customer feedback. If
    words like *poor*, *slow*, and *expensive* dominate, you’ve got a problem! Writers
    can also use the clouds to compare chapters in a book or scenes in a screenplay.
    If the author is using very similar language for action scenes and romantic interludes,
    some editing is needed. If you’re a copywriter, clouds can help you check your
    keyword density for search engine optimization (SEO).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 词云的另一个用途是从客户反馈中提取关键词。如果像*差*、*慢*和*贵*这样的词占主导地位，那么你就有问题了！作家还可以使用词云来比较书中的章节或剧本中的场景。如果作者在动作场景和浪漫插曲中使用了非常相似的语言，就需要进行一些编辑。如果你是文案写手，词云可以帮助你检查关键词密度，以优化搜索引擎优化（SEO）。
- en: There are lots of ways to generate word clouds, including free websites like
    *[https://www.wordclouds.com/](https://www.wordclouds.com/)* and *[https://www.jasondavies.com/wordcloud/](https://www.jasondavies.com/wordcloud/)*.
    But if you want to fully customize your word cloud or embed the generator within
    another program, you need to do it yourself. In this project, you’ll use a word
    cloud to make a promotional flyer for a school play based on the Sherlock Holmes
    story *The Hound of the Baskervilles*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 生成词云的方法有很多，包括一些免费的在线网站，如 *[https://www.wordclouds.com/](https://www.wordclouds.com/)*
    和 *[https://www.jasondavies.com/wordcloud/](https://www.jasondavies.com/wordcloud/)*。但是，如果你想完全自定义你的词云，或者将生成器嵌入到另一个程序中，就需要自己动手。在本项目中，你将使用词云为学校戏剧制作一张宣传单，基于《福尔摩斯探案集》中的故事
    *《巴斯克维尔的猎犬》*。
- en: Instead of using the basic rectangle shown in [Figures 3-1](ch03.xhtml#ch03fig1)
    and [3-2](ch03.xhtml#ch03fig2), you’ll fit the words into an outline of Holmes’s
    head ([Figure 3-3](ch03.xhtml#ch03fig3)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你将不再使用[图 3-1](ch03.xhtml#ch03fig1)和[3-2](ch03.xhtml#ch03fig2)中显示的基本矩形，而是将单词嵌入到福尔摩斯头部的轮廓中（见[图
    3-3](ch03.xhtml#ch03fig3)）。
- en: '![Image](../images/fig03_03.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig03_03.jpg)'
- en: 'Figure 3-3: Silhouette of Sherlock Holmes'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-3：福尔摩斯的剪影
- en: This will make for a more recognizable and eye-catching display.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使得显示效果更加易于识别和引人注目。
- en: THE OBJECTIVE
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Use the wordcloud module to generate a shaped word cloud for a novel.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 wordcloud 模块为小说生成一个定形的词云。
- en: '***The Word Cloud and PIL Modules***'
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词云和 PIL 模块***'
- en: You’ll use a module called wordcloud to generate the word cloud. You can install
    it using pip.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用名为 wordcloud 的模块来生成词云。你可以通过 pip 安装它。
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Or, if you’re using Anaconda, use the following command:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你使用的是 Anaconda，可以使用以下命令：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can find the web page for wordcloud here: *[http://amueller.github.io/word_cloud/](http://amueller.github.io/word_cloud/).*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到 wordcloud 的网页：*[http://amueller.github.io/word_cloud/](http://amueller.github.io/word_cloud/)*。
- en: You’ll also need the Python Imaging Library (PIL) to work with images. Use pip
    again to install it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要使用 Python Imaging Library (PIL) 来处理图像。可以再次使用 pip 安装它。
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Or, for Anaconda, use this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，对于 Anaconda，请使用此命令：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In case you’re wondering, pillow is the successor project of PIL, which was
    discontinued in 2011\. To learn more about it, visit *[https://pillow.readthedocs.io/en/stable/](https://pillow.readthedocs.io/en/stable/)*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想，pillow 是 PIL 的继任项目，PIL 在 2011 年已停止更新。要了解更多信息，请访问 *[https://pillow.readthedocs.io/en/stable/](https://pillow.readthedocs.io/en/stable/)*。
- en: '***The Word Cloud Code***'
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***词云代码***'
- en: To make the shaped word cloud, you’ll need an image file and a text file. The
    image shown in [Figure 3-3](ch03.xhtml#ch03fig3) came from iStock by Getty Images
    (*[https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/](https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/)*).
    This represents the “small” resolution at around 500×600 pixels.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成定形的词云，你需要一个图像文件和一个文本文件。图 3-3 中显示的图像来自 Getty Images 的 iStock（*[https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/](https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/)*）。这是大约
    500×600 像素的“低”分辨率图像。
- en: A similar but copyright-free image (*holmes.png*) is provided with the book’s
    downloadable files. You can find the text file (*hound.txt*), image file (*holmes.png*),
    and code (*wc_hound.py*) in the *Chapter_3* folder.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的下载文件中提供了一个相似但不受版权保护的图像（*holmes.png*）。你可以在 *Chapter_3* 文件夹中找到文本文件（*hound.txt*）、图像文件（*holmes.png*）和代码（*wc_hound.py*）。
- en: '**Importing Modules, Text Files, Image Files, and Stop Words**'
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入模块、文本文件、图像文件和停用词**'
- en: '[Listing 3-8](ch03.xhtml#ch03list8) imports modules, loads the novel, loads
    the silhouette image of Holmes, and creates a set of stop words you’ll want to
    exclude from the cloud.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-8](ch03.xhtml#ch03list8) 导入模块，加载小说，加载福尔摩斯的轮廓图像，并创建一组你想从词云中排除的停用词。'
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Listing 3-8: Importing modules and loading text, image, and stop words'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3-8：导入模块并加载文本、图像和停用词
- en: Begin by importing NumPy and PIL. PIL will open the image, and NumPy will turn
    it into a mask. You started using NumPy in [Chapter 1](ch01.xhtml); in case you
    skipped it, see the “Installing the Python Libraries” section on [page 6](ch01.xhtml#page_6).
    Note that the pillow module continues to use the acronym PIL for backward compatibility.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入 NumPy 和 PIL。PIL 将打开图像，NumPy 将其转换为蒙版。你在[第 1 章](ch01.xhtml)中开始使用 NumPy；如果你跳过了该部分，请参阅[第
    1 章](ch01.xhtml#page_6)的“安装 Python 库”部分。请注意，Pillow 模块继续使用 PIL 的缩写，以保证向后兼容。
- en: You’ll need matplotlib, which you downloaded in the “Installing the Python Libraries”
    section of [Chapter 1](ch01.xhtml), to display the word cloud. The wordcloud module
    comes with its own list of stop words, so import STOPWORDS along with the cloud
    functionality.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要 matplotlib，它在[第 1 章](ch01.xhtml)的“安装 Python 库”部分中已经下载，用于显示词云。wordcloud
    模块自带一套停用词列表，因此需要同时导入 STOPWORDS 和词云功能。
- en: Next, load the novel’s text file and store it in a variable named text ➊. As
    described in the discussion of [Listing 2-2](ch02.xhtml#ch02list2) in [Chapter
    2](ch02.xhtml), you may encounter a UnicodeDecodeError when loading the text.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载小说的文本文件并将其存储在名为 text ➊ 的变量中。如[第 2 章](ch02.xhtml)中[示例 2-2](ch02.xhtml#ch02list2)的讨论所述，在加载文本时，你可能会遇到
    UnicodeDecodeError 错误。
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this case, try modifying the open() function by adding encoding and errors
    arguments.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，尝试通过添加编码和错误参数来修改 open() 函数。
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With the text loaded, use PIL’s Image.open() method to open the image of Holmes
    and use NumPy to turn it into an array. If you’re using the iStock image of Holmes,
    change the image’s filename as appropriate.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 加载文本后，使用 PIL 的 Image.open() 方法打开福尔摩斯的图像，并使用 NumPy 将其转换为数组。如果你使用的是 iStock 上的福尔摩斯图像，请适当更改图像文件名。
- en: Assign the STOPWORDS set imported from wordcloud to the stopwords variable.
    Then update the set with a list of additional words that you want to exclude ➋.
    These will be words like *said* and *now* that dominate the word cloud but add
    no useful content. Determining what they are is an iterative process. You generate
    the word cloud, remove words that you don’t think contribute, and repeat. You
    can comment out this line to see the benefit.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 将从 wordcloud 导入的 STOPWORDS 集合分配给 stopwords 变量。然后，使用你想排除的额外单词更新该集合 ➋。这些将是像 *said*
    和 *now* 这样的词，它们在词云中占主导地位，但并未增加有用内容。确定哪些是这样的词是一个迭代过程。你生成词云，移除你认为不贡献的词，并重复此过程。你可以注释掉这行代码以查看效果。
- en: '**NOTE**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*To update a container like STOPWORDS, you need to know whether it’s a list,
    dictionary, set, and so on. Python’s built-in type() function returns the class
    type of any object passed as an argument. In this case, print(type(STOPWORDS))
    yields <class ''set''>.*'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*要更新像 STOPWORDS 这样的容器，你需要知道它是列表、字典、集合等。Python 的内建函数 type() 会返回传入对象的类类型。在这种情况下，print(type(STOPWORDS))
    的输出为 <class ''set''>。*'
- en: '**Generating the Word Cloud**'
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**生成词云**'
- en: '[Listing 3-9](ch03.xhtml#ch03list9) generates the word cloud and uses the silhouette
    as a *mask*, or an image used to hide portions of another image. The process used
    by wordcloud is sophisticated enough to fit the words within the mask, rather
    than simply truncating them at the edges. In addition, numerous parameters are
    available for changing the appearance of the words within the mask.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-9](ch03.xhtml#ch03list9) 生成词云并使用轮廓图像作为 *蒙版*，即用来隐藏另一图像部分的图像。wordcloud
    使用的处理过程足够复杂，可以将词语适应于蒙版中，而不仅仅是将它们截断在边缘。此外，有多个参数可以更改蒙版中单词的外观。'
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Listing 3-9: Generating the word cloud'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3-9：生成词云
- en: Name a variable wc and call WordCloud(). There are a lot of parameters, so I’ve
    placed each on its own line for clarity. For a list and description of all the
    parameters available, visit *[https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 给变量命名为 wc 并调用 WordCloud()。有很多参数，因此我将每个参数单独放在一行以便清晰。有关所有可用参数的列表和描述，请访问 *[https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)*。
- en: Start by passing the maximum number of words you want to use. The number you
    set will display the *n* most common words in the text. The more words you choose
    to display, the easier it will be to define the edges of the mask and make it
    recognizable. Unfortunately, setting the maximum number too high will also result
    in a lot of tiny, illegible words. For this project, start with 500.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先传入你想要使用的最大单词数。你设置的数字将显示文本中最常见的 *n* 个单词。选择显示更多单词将更容易定义掩码的边缘并使其可识别。不幸的是，设置的最大数字过高也会导致许多微小的、难以辨认的单词。对于这个项目，从
    500 开始。
- en: Next, to control the font size and relative importance of each word, set the
    relative_scaling parameter to 0.5. For example, a value of 0 gives preference
    to a word’s rank to determine the font size, while a value of 1 means that words
    that occur twice as often will appear twice as large. Values between 0 and 0.5
    tend to strike the best balance between rank and frequency.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了控制字体大小和每个单词的相对重要性，将 `relative_scaling` 参数设置为 0.5。例如，值为 0 时，优先根据单词的排名来确定字体大小，而值为
    1 时，出现频率是两倍的单词将显示为两倍大。0 到 0.5 之间的值通常能在排名和频率之间取得最佳平衡。
- en: Reference the mask variable and set its background color to white. Assigning
    no color defaults to black. Then reference the stopwords set that you edited in
    the previous listing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 引用 mask 变量并将其背景颜色设置为白色。未指定颜色时，默认为黑色。然后引用你在前一个列表中编辑的 stopwords 集。
- en: The margin parameter will control the spacing of the displayed words. Using
    0 will result in tightly packed words. Using 2 will allow for some whitespace
    padding.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`margin` 参数将控制显示单词的间距。使用 0 会导致单词紧密排列。使用 2 会允许一些空白填充。'
- en: To place the words around the word cloud, use a random number generator and
    set random_state to 7. There’s nothing special about this value; I just felt that
    it produced an attractive arrangement of words.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要将单词放置在词云周围，使用随机数生成器并将 `random_state` 设置为 7。这个值并没有特别的含义；我只是觉得它产生了一个吸引人的单词排列。
- en: The random_state parameter fixes the seed number so that the results are repeatable,
    assuming no other parameters are changed. This means the words will always be
    arranged in the same way. Only integers are accepted.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_state` 参数固定种子值，以便结果可重复，前提是没有更改其他参数。这意味着单词的排列将始终相同。只接受整数。'
- en: Now, set contour_width to 2. Any value greater than zero creates an outline
    around a mask. In this case, the outline is squiggly due to the resolution of
    the image ([Figure 3-4](ch03.xhtml#ch03fig4)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将 contour_width 设置为 2。任何大于零的值都会在掩码周围创建一个轮廓。在这种情况下，由于图像的分辨率，轮廓是波浪形的（[图 3-4](ch03.xhtml#ch03fig4)）。
- en: Set the color of the outline to brown using the contour_color parameter. Continue
    using a brownish palette by setting colormap to copper. In matplotlib, a *colormap*
    is a dictionary that maps numbers to colors. The copper colormap produces text
    ranging in color from pale flesh to black. You can see its spectrum, along with
    many other color options, at *[https://matplotlib.org/gallery/color/colormap_reference.html](https://matplotlib.org/gallery/color/colormap_reference.html)*.
    If you don’t specify a colormap, the program will use the default colors.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 contour_color 参数将轮廓颜色设置为棕色。通过将 colormap 设置为 copper，继续使用棕色调的色盘。在 matplotlib
    中，*colormap* 是一个字典，它将数字映射到颜色。copper 色盘生成的文本颜色范围从浅肤色到黑色。你可以在 *[https://matplotlib.org/gallery/color/colormap_reference.html](https://matplotlib.org/gallery/color/colormap_reference.html)*
    查看它的色谱以及许多其他颜色选项。如果你没有指定 colormap，程序将使用默认颜色。
- en: Use dot notation to call the generate() method to build the word cloud. Pass
    it the text string as an argument. End this listing by naming a colors variable
    and calling the to_array() method on the wc object. This method converts the word
    cloud image into a NumPy array for use with matplotlib.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用点符号调用 `generate()` 方法来构建词云。将文本字符串作为参数传递给它。通过命名一个 colors 变量并在 wc 对象上调用 `to_array()`
    方法来结束这个列表。该方法将词云图像转换为 NumPy 数组，以便与 matplotlib 一起使用。
- en: '![Image](../images/fig03_04.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig03_04.jpg)'
- en: 'Figure 3-4: Example of masked word cloud with an outline (left) versus without
    (right)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-4：带有轮廓的掩码词云示例（左）与不带轮廓的示例（右）
- en: '**Plotting the Word Cloud**'
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**绘制词云**'
- en: '[Listing 3-10](ch03.xhtml#ch03list10) adds a title to the word cloud and uses
    matplotlib to display it. It also saves the word cloud image as a file.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3-10](ch03.xhtml#ch03list10) 为词云添加标题，并使用 matplotlib 显示它。它还将词云图像保存为文件。'
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Listing 3-10: Plotting and saving the word cloud'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3-10：绘制和保存词云
- en: Start by initializing a matplotlib figure. Then call the title() method and
    pass it the name of the school, along with a font size and color.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始化matplotlib图形开始。然后调用title()方法，传递学校的名称、字体大小和颜色。
- en: You’ll want the name of the play to be bigger and bolder than the other titles.
    Since you can’t change the text style within a string with matplotlib, use the
    text() method to define a new title. Pass it (*x*, *y*) coordinates (based on
    the figure axes), a text string, and text style details. Use trial and error with
    the coordinates to optimize the placement of the text. If you’re using the iStock
    image of Holmes, you may need to change the *x* coordinate from -10 to something
    else to achieve the best balance with the asymmetrical silhouette.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望剧本的名称比其他标题更大更粗。由于无法在matplotlib中改变字符串的文本样式，请使用text()方法定义一个新标题。传递给它(*x*, *y*)坐标（基于图形轴）、文本字符串和文本样式细节。通过反复试验坐标来优化文本位置。如果您使用的是Holmes的iStock图片，您可能需要将*x*坐标从-10更改为其他值，以达到最佳的平衡效果，尤其是在不对称的轮廓下。
- en: Finish the titles by placing the play’s time and venue at the bottom of the
    figure. You could use the text() method again, but instead, let’s take a look
    at an alternative, pyplot’s suptitle() method. The name stands for “super titles.”
    Pass it the text, the (*x*, *y*) figure coordinates, and styling details.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 完成标题时，将剧本的时间和地点放在图形的底部。您可以再次使用text()方法，但我们来看看另一种方法——pyplot的suptitle()方法。这个名字代表“超级标题”。传递给它文本、(*x*,
    *y*)图形坐标和样式细节。
- en: To display the word cloud, call imshow()—for image show—and pass it the colors
    array you made previously. Specify bilinear for color interpolation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要显示词云，请调用imshow()——用于图像显示——并传递之前创建的颜色数组。指定bilinear进行颜色插值。
- en: Turn off the figure axes and display the word cloud by calling show(). If you
    want to save the figure, uncomment the savefig() method. Note that matplotlib
    can read the extension in the filename and save the figure in the correct format.
    As written, the save command will not execute until you manually close the figure.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用show()关闭图形坐标轴并显示词云。如果您想保存图形，可以取消注释savefig()方法。请注意，matplotlib可以读取文件名中的扩展名，并以正确的格式保存图形。按原样，保存命令将在您手动关闭图形后才会执行。
- en: '***Fine-Tuning the Word Cloud***'
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***微调词云***'
- en: '[Listing 3-10](ch03.xhtml#ch03list10) will produce the word cloud in [Figure
    3-5](ch03.xhtml#ch03fig5). You may get a different arrangement of words as the
    algorithm is stochastic.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3-10](ch03.xhtml#ch03list10)将生成[图 3-5](ch03.xhtml#ch03fig5)中的词云。由于算法是随机的，您可能会得到不同的单词排列。'
- en: '![Image](../images/fig03_05.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig03_05.jpg)'
- en: 'Figure 3-5: The flyer generated by the wc_hound.py code'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-5：由wc_hound.py代码生成的传单
- en: 'You can change the size of the display by adding an argument when you initialize
    the figure. Here’s an example: plt.figure(figsize=(50, 60)).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在初始化图形时添加一个参数来更改显示的大小。示例如下：plt.figure(figsize=(50, 60))。
- en: There are many other ways to change the results. For example, setting the margin
    parameter to 10 yields a sparser word cloud ([Figure 3-6](ch03.xhtml#ch03fig6)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他方法可以更改结果。例如，将margin参数设置为10会生成一个更稀疏的词云（[图 3-6](ch03.xhtml#ch03fig6)）。
- en: '![Image](../images/fig03_06.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig03_06.jpg)'
- en: 'Figure 3-6: The word cloud generated with margin=10'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-6：使用margin=10生成的词云
- en: Changing the random_state parameter will also rearrange the words within the
    mask ([Figure 3-7](ch03.xhtml#ch03fig7)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 更改`random_state`参数也会重新排列遮罩中的单词（[图 3-7](ch03.xhtml#ch03fig7)）。
- en: '![Image](../images/fig03_07.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig03_07.jpg)'
- en: 'Figure 3-7: The word cloud generated with margin=10 and random_state=6'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-7：使用margin=10和random_state=6生成的词云
- en: Tweaking the max_words and relative_scaling parameters will also change the
    appearance of the word cloud. Depending on how detail-oriented you are, all this
    can be a blessing or a curse!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 调整max_words和relative_scaling参数也会改变词云的外观。根据您的细致程度，这一切可能是福是祸！
- en: '**Summary**'
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you used extraction-based summarization techniques to produce
    a synopsis of Martin Luther King Jr.’s “I Have a Dream” speech. You then used
    a free, off-the-shelf module called gensim to summarize Admiral McRaven’s “Make
    Your Bed” speech with even less code. Finally, you used the wordcloud module to
    create an interesting design with words.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您使用了基于提取的摘要技术，生成了马丁·路德·金“我有一个梦想”演讲的概要。然后，您使用了一个免费的现成模块gensim，以更少的代码总结了麦克雷文海军上将的“整理床铺”演讲。最后，您使用wordcloud模块创建了一个有趣的单词设计。
- en: '**Further Reading**'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: '*Automate the Boring Stuff with Python: Practical Programming for Total Beginners*
    (No Starch Press, 2015), by Al Sweigart, covers regular expressions in [Chapter
    7](ch07.xhtml) and web scraping in [Chapter 11](ch11.xhtml), including use of
    the requests and Beautiful Soup modules.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*用Python自动化枯燥的事情：完全初学者的实用编程*（No Starch Press，2015年），由阿尔·斯威加特编写，涵盖了第[7章](ch07.xhtml)中的正则表达式和第[11章](ch11.xhtml)中的网页抓取，包括使用requests和Beautiful
    Soup模块。'
- en: '*Make Your Bed: Little Things That Can Change Your Life*…*And Maybe the World*,
    2nd ed. (Grand Central Publishing, 2017), by William H. McRaven, is a self-help
    book based on the admiral’s commencement address at the University of Texas. You
    can find the actual speech online on *[https://www.youtube.com/](https://www.youtube.com/)*.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*整理床铺：那些能改变你生活的小事*……*也许还能改变世界*，第二版（大中央出版社，2017年），威廉·H·麦克雷文著，是一本基于这位海军上将于德克萨斯大学的毕业典礼演讲的自助书籍。你可以在
    *[https://www.youtube.com/](https://www.youtube.com/)* 上找到这篇演讲的实际视频。'
- en: '**Challenge Project: Game Night**'
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**挑战项目：游戏之夜**'
- en: Use wordcloud to invent a new game for game night. Summarize Wikipedia or IMDb
    synopses of movies and see whether your friends can guess the movie title. [Figure
    3-8](ch03.xhtml#ch03fig8) shows some examples.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用wordcloud为游戏之夜发明一个新游戏。总结维基百科或IMDb的电影简介，看看你的朋友们能否猜出电影名称。[图 3-8](ch03.xhtml#ch03fig8)展示了一些示例。
- en: '![Image](../images/fig03_08.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/fig03_08.jpg)'
- en: 'Figure 3-8: Word clouds for two movies released in 2010: How to Train Your
    Dragon and Prince of Persia'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-8：2010年发布的两部电影的词云：如何训练你的龙和波斯王子
- en: If you’re not into movies, pick something else. Alternatives include famous
    novels, *Star Trek* episodes, and song lyrics ([Figure 3-9](ch03.xhtml#ch03fig9)).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不喜欢电影，可以选择其他内容。替代品包括著名小说、*星际迷航*剧集以及歌词（见[图 3-9](ch03.xhtml#ch03fig9)）。
- en: '![Image](../images/fig03_09.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/fig03_09.jpg)'
- en: 'Figure 3-9: Word cloud made from song lyrics (Donald Fagen’s “I.G.Y.”)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-9：从歌曲歌词中制作的词云（唐纳德·费根的“I.G.Y.”）
- en: Board games have seen a resurgence in recent years, so you could follow this
    trend and print the word clouds on card stock. Alternatively, you could keep things
    digital and present the player with multiple-choice answers for each cloud. The
    game should keep track of the number of correct answers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 桌面游戏近年来有了复兴，所以你可以跟随这一趋势，将词云打印在卡纸上。或者，你可以保持数字化，给玩家提供每个词云的多项选择答案。游戏应跟踪正确答案的数量。
- en: '**Challenge Project: Summarizing Summaries**'
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**挑战项目：总结摘要**'
- en: Test your program from Project 3 on previously summarized text, such as Wikipedia
    pages. Only five sentences produced a good overview of gensim.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用项目3中的程序对之前总结过的文本进行测试，比如维基百科页面。仅五个句子就能提供一个关于gensim的良好概述。
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Next, try the gensim version from Project 4 on those boring services agreements
    no one ever reads. An example Microsoft agreement is available at *[https://www.microsoft.com/en-us/servicesagreement/default.aspx](https://www.microsoft.com/en-us/servicesagreement/default.aspx)*.
    Of course, to evaluate the results, you’ll have to read the full agreement, which
    almost no one ever does! Enjoy the catch-22!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，尝试在那些没人愿意阅读的无聊服务协议上使用项目4中的gensim版本。一个微软协议的示例可在 *[https://www.microsoft.com/en-us/servicesagreement/default.aspx](https://www.microsoft.com/en-us/servicesagreement/default.aspx)*
    获取。当然，要评估结果，你必须阅读完整的协议，而几乎没有人愿意这么做！享受这个进退两难的困境吧！
- en: '**Challenge Project: Summarizing a Novel**'
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**挑战项目：总结一部小说**'
- en: Write a program that summarizes *The Hound of the Baskervilles* by chapter.
    Keep the chapter summaries short, at around 75 words each.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个程序，通过章节总结《巴斯克维尔的猎犬》。每个章节的总结保持简短，大约75个单词。
- en: 'For a copy of the novel with chapter headings, scrape the text off the Project
    Gutenberg site using the following line of code: url = ''http://www.gutenberg.org/files/2852/2852-h/2852-h.htm''.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取带章节标题的小说副本，可以通过以下代码从古腾堡计划网站抓取文本：url = 'http://www.gutenberg.org/files/2852/2852-h/2852-h.htm'。
- en: 'To break out chapter elements, rather than paragraph elements, use this code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 若要分解章节元素，而非段落元素，使用此代码：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You’ll also need to select paragraph elements (p_elems) from within each chapter,
    using the same methodology as in *dream_summary.py*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要从每个章节中选择段落元素（p_elems），使用与 *dream_summary.py* 中相同的方法。
- en: 'The following snippets show some of the results from using a word count of
    75 per chapter:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了使用每章75个单词的结果：
- en: '[PRE29]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Challenge Project: It’s Not Just What You Say, It’s How You Say It!**'
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**挑战项目：不仅仅是你说了什么，更重要的是你怎么说！**'
- en: The text summarization programs you have written so far print sentences strictly
    by their *order of importance*. That means the last sentence in a speech (or any
    text) might become the first sentence in the summary. The goal of summarization
    is to find the important sentences, but there’s no reason you can’t alter the
    way that they’re displayed.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你迄今为止编写的文本摘要程序是严格按照句子的*重要性顺序*打印句子的。这意味着演讲（或任何文本）中的最后一句话可能会成为摘要中的第一句。摘要的目标是找到重要的句子，但你完全可以改变它们的展示方式。
- en: Write a text summarization program that displays the most important sentences
    in their original *order of appearance*. Compare the results to those produced
    by the program in Project 3\. Does this make a noticeable improvement in the summaries?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个文本摘要程序，显示最重要的句子，并保持它们原始的*出现顺序*。将结果与第3个项目中的程序生成的结果进行比较。这样做会对摘要产生明显的改进吗？
