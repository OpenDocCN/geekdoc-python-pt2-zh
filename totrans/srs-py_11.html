<html><head></head><body>
<h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_177"/><strong><span class="big">11</span></strong><br/><strong>SCALING AND ARCHITECTURE</strong></h2>&#13;
<div class="image1"><img alt="image" src="../images/common01.jpg"/></div>&#13;
<p class="noindent">Sooner or later, your development process will have to consider resiliency and scalability. An application’s scalability, concurrency, and parallelism depend largely on its initial architecture and design. As we’ll see in this chapter, there are some paradigms—such as multithreading—that don’t apply correctly to Python, whereas other techniques, such as service-oriented architecture, work better.</p>&#13;
<p class="indent">Covering scalability in its entirety would take an entire book, and has in fact been covered by many books. This chapter covers the essential scaling fundamentals, even if you’re not planning to build applications with millions of users.</p>&#13;
<h3 class="h3" id="lev1sec61"><span epub:type="pagebreak" id="page_178"/><strong>Multithreading in Python and Its Limitations</strong></h3>&#13;
<p class="noindent">By default, Python processes run on only one thread, called the <em>main thread</em>. This thread executes code on a single processor. <em>Multithreading</em> is a programming technique that allows code to run concurrently inside a single Python process by running several threads simultaneously. This is the primary mechanism through which we can introduce concurrency in Python. If the computer is equipped with multiple processors, you can even use <em>parallelism</em>, running threads in parallel over several processors, to make code execution faster.</p>&#13;
<p class="indent">Multithreading is most commonly used (though not always appropriately) when:</p>&#13;
<ul>&#13;
<li><p class="noindent">You need to run background or I/O-oriented tasks without stopping your main thread’s execution. For example, the main loop of a graphical user interface is busy waiting for an event (e.g., a user click or keyboard input), but the code needs to execute other tasks.</p></li>&#13;
<li><p class="noindent">You need to spread your workload across several CPUs.</p></li>&#13;
</ul>&#13;
<p class="indent">The first scenario is a good general case for multithreading. Though implementing multithreading in this circumstance would introduce extra complexity, controlling multithreading would be manageable, and performance likely wouldn’t suffer unless the CPU workload was intensive. The performance gain from using concurrency with workloads that are I/O intensive gets more interesting when the I/O has high latency: the more often you have to wait to read or write, the more beneficial it is to do something else in the meantime.</p>&#13;
<p class="indent">In the second scenario, you might want to start a new thread for each new request instead of handling them one at a time. This may seem like a good use for multithreading. However, if you spread your workload out like this, you will encounter the Python <em>global interpreter lock (GIL),</em> a lock that must be acquired each time CPython needs to execute bytecode. The lock means that only one thread can have control of the Python interpreter at any one time. This rule was introduced originally to prevent race conditions, but it unfortunately means that if you try to scale your application by making it run multiple threads, you’ll always be limited by this global lock.</p>&#13;
<p class="indent">So, while using threads seems like the ideal solution, most applications running requests in multiple threads struggle to attain 150 percent CPU usage, or usage of the equivalent of 1.5 cores. Most computers have 4 or 8 cores, and servers offer 24 or 48 cores, but the GIL prevents Python from using the full CPU. There are some initiatives underway to remove the GIL, but the effort is extremely complex because it requires performance and backward compatibility trade-offs.</p>&#13;
<p class="indent">Although CPython is the most commonly used implementation of the Python language, there are others that do not have a GIL. <em>Jython</em>, for example, can efficiently run multiple threads in parallel. Unfortunately, <span epub:type="pagebreak" id="page_179"/>projects such as Jython by their very nature lag behind CPython and so are not really useful targets; innovation happens in CPython, and the other implementations are just following in CPython’s footsteps.</p>&#13;
<p class="indent">So, let’s revisit our two use cases with what we now know and figure out a better solution:</p>&#13;
<ul>&#13;
<li><p class="noindent">When you need to run background tasks, you <em>can</em> use multithreading, but the easier solution is to build your application around an event loop. There are a lot of Python modules that provide for this, and the standard is now <span class="literal">asyncio</span>. There are also frameworks, such as Twisted, built around the same concept. The most advanced frameworks will give you access to events based on signals, timers, and file descriptor activity—we’ll talk about this later in the chapter in “<a href="ch11.xhtml#lev1sec63">Event-Driven Architecture</a>” on <a href="ch11.xhtml#page_181">page 181</a>.</p></li>&#13;
<li><p class="noindent">When you need to spread the workload, using multiple processes is the most efficient method. We’ll look at this technique in the next section.</p></li>&#13;
</ul>&#13;
<p class="indent">Developers should always think twice before using multithreading. As one example, I once used multithreading to dispatch jobs in rebuildd, a Debian-build daemon I wrote a few years ago. While it seemed handy to have a different thread to control each running build job, I very quickly fell into the threading-parallelism trap in Python. If I had the chance to begin again, I’d build something based on asynchronous event handling or multiprocessing and not have to worry about the GIL.</p>&#13;
<p class="indent">Multithreading is complex, and it’s hard to get multithreaded applications right. You need to handle thread synchronization and locking, which means there are a lot of opportunities to introduce bugs. Considering the small overall gain, it’s better to think twice before spending too much effort on it.</p>&#13;
<h3 class="h3" id="lev1sec62"><strong>Multiprocessing vs. Multithreading</strong></h3>&#13;
<p class="noindent">Since the GIL prevents multithreading from being a good scalability solution, look to the alternative solution offered by Python’s <em>multiprocessing</em> package. The package exposes the same kind of interface you’d achieve using the multithreading module, except that it starts new <em>processes</em> (via <span class="literal">os.fork()</span>) instead of new system threads.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11list1">Listing 11-1</a> shows a simple example in which one million random integers are summed eight times, with this activity spread across eight threads at the same time.</p>&#13;
<p class="programs">import random<br/>import threading<br/>results = []<br/>def compute():<br/>    results.append(sum(<br/>        [random.randint(1, 100) for i in range(1000000)]))<br/>workers = [threading.Thread(target=compute) for x in range(8)]<span epub:type="pagebreak" id="page_180"/><br/>for worker in workers:<br/>    worker.start()<br/>for worker in workers:<br/>    worker.join()<br/>print("Results: %s" % results)</p>&#13;
<p class="listing1"><a id="ch11list1"/><em>Listing 11-1: Using multithreading for concurrent activity</em></p>&#13;
<p class="indent">In <a href="ch11.xhtml#ch11list1">Listing 11-1</a>, we create eight threads using the <span class="literal">threading.Thread</span> class and store them in the <span class="literal">workers</span> array. Those threads will execute the <span class="literal">compute()</span> function. They then use the <span class="literal">start()</span> method to start. The <span class="literal">join()</span> method only returns once the thread has terminated its execution. At this stage, the result can be printed.</p>&#13;
<p class="indent">Running this program returns the following:</p>&#13;
<p class="programs">$ <span class="codestrong1">time python worker.py</span><br/>Results: [50517927, 50496846, 50494093, 50503078, 50512047, 50482863,<br/>50543387, 50511493]<br/>python worker.py  13.04s user 2.11s system 129% cpu 11.662 total</p>&#13;
<p class="indent">This has been run on an idle four-core CPU, which means that Python could potentially have used up to 400 percent of CPU. However, these results show that it was clearly unable to do that, even with eight threads running in parallel. Instead, its CPU usage maxed out at 129 percent, which is just 32 percent of the hardware’s capabilities (129/400).</p>&#13;
<p class="indent">Now, let’s rewrite this implementation using <em>multiprocessing</em>. For a simple case like this, switching to multiprocessing is pretty straightforward, as shown in <a href="ch11.xhtml#ch11list2">Listing 11-2</a>.</p>&#13;
<p class="programs">import multiprocessing<br/>import random<br/><br/>def compute(n):<br/>    return sum(<br/>        [random.randint(1, 100) for i in range(1000000)])<br/><br/># Start 8 workers<br/>pool = multiprocessing.Pool(processes=8)<br/>print("Results: %s" % pool.map(compute, range(8)))</p>&#13;
<p class="listing1"><a id="ch11list2"/><em>Listing 11-2: Using multiprocessing for concurrent activity</em></p>&#13;
<p class="indent">The <span class="literal">multiprocessing</span> module offers a <span class="literal">Pool</span> object that accepts as an argument the number of processes to start. Its <span class="literal">map()</span> method works in the same way as the native <span class="literal">map()</span> method, except that a different Python process will be responsible for the execution of the <span class="literal">compute()</span> function.</p>&#13;
<p class="indent">Running the program in <a href="ch11.xhtml#ch11list2">Listing 11-2</a> under the same conditions as <a href="ch11.xhtml#ch11list1">Listing 11-1</a> gives the following result:</p>&#13;
<p class="programs">$ <span class="codestrong1">time python workermp.py</span><br/>Results: [50495989, 50566997, 50474532, 50531418, 50522470, 50488087,<span epub:type="pagebreak" id="page_181"/><br/>0498016, 50537899]<br/>python workermp.py  16.53s user 0.12s system 363% cpu 4.581 total</p>&#13;
<p class="indent">Multiprocessing reduces the exectution time by 60 percent. Moreover, we’ve been able to consume up to 363 percent of CPU power, which is more than 90 percent (363/400) of the computer’s CPU capacity.</p>&#13;
<p class="indent">Each time you think that you can parallelize some work, it’s almost always better to rely on multiprocessing and to fork your jobs in order to spread the workload across several CPU cores. This wouldn’t be a good solution for very small execution times, as the cost of the <span class="literal">fork()</span> call would be too big, but for larger computing needs, it works well.</p>&#13;
<h3 class="h3" id="lev1sec63"><strong>Event-Driven Architecture</strong></h3>&#13;
<p class="noindent"><em>Event-driven programming</em> is characterized by the use of events, such as user input, to dictate how control flows through a program, and it is a good solution for organizing program flow. The event-driven program listens for various events happening on a queue and reacts based on those incoming events.</p>&#13;
<p class="indent">Let’s say you want to build an application that listens for a connection on a socket and then processes the connection it receives. There are basically three ways to approach the problem:</p>&#13;
<ul>&#13;
<li><p class="noindent">Fork a new process each time a new connection is established, relying on something like the <span class="literal">multiprocessing</span> module.</p></li>&#13;
<li><p class="noindent">Start a new thread each time a new connection is established, relying on something like the <span class="literal">threading</span> module.</p></li>&#13;
<li><p class="noindent">Add this new connection to your event loop and react to the event it will generate when it occurs.</p></li>&#13;
</ul>&#13;
<p class="indent">Determining how a modern computer should handle tens of thousands of connections simultaneously is known as the <em>C10K problem</em>. Among other things, the C10K resolution strategies explain how using an event loop to listen to hundreds of event sources is going to scale much better than, say, a one-thread-per-connection approach. This doesn’t mean that the two techniques are not compatible, but it does mean that you can usually replace the multiple-threads approach with an event-driven mechanism.</p>&#13;
<p class="indent">Event-driven architecture uses an event loop: the program calls a function that blocks execution until an event is received and ready to be processed. The idea is that your program can be kept busy doing other tasks while waiting for inputs and outputs to complete. The most basic events are “data ready to be read” and “data ready to be written.”</p>&#13;
<p class="indent">In Unix, the standard functions for building such an event loop are the system calls <span class="literal">select(2)</span> or <span class="literal">poll(2)</span>. These functions expect a list of file descriptors to listen for, and they will return as soon as at least one of the file descriptors is ready to be read from or written to.</p>&#13;
<p class="indent">In Python, we can access these system calls through the <span class="literal">select</span> module. It’s easy enough to build an event-driven system with these calls, though <span epub:type="pagebreak" id="page_182"/>doing so can be tedious. <a href="ch11.xhtml#ch11list3">Listing 11-3</a> shows an event-driven system that does our specified task: listening on a socket and processing any connections it receives.</p>&#13;
<p class="programs">import select<br/>import socket<br/><br/>server = socket.socket(socket.AF_INET,<br/>                       socket.SOCK_STREAM)<br/># Never block on read/write operations<br/>server.setblocking(0)<br/><br/># Bind the socket to the port<br/>server.bind(('localhost', 10000))<br/>server.listen(8)<br/><br/>while True:<br/>    # select() returns 3 arrays containing the object (sockets, files...)<br/><br/>    # that are ready to be read, written to or raised an error<br/>inputs,<br/>outputs, excepts = select.select([server], [], [server])<br/>    if server in inputs:<br/>        connection, client_address = server.accept()<br/>        connection.send("hello!\n")</p>&#13;
<p class="listing1"><a id="ch11list3"/><em>Listing 11-3: Event-driven program that listens for and processes connections</em></p>&#13;
<p class="indent">In <a href="ch11.xhtml#ch11list3">Listing 11-3</a>, a server socket is created and set to <em>non-blocking</em>, meaning that any read or write operation attempted on that socket won’t block the program. If the program tries to read from the socket when there is no data ready to be read, the socket <span class="literal">recv()</span> method will raise an OSError indicating that the socket is not ready. If we did not call <span class="literal">setblocking(0)</span>, the socket would stay in blocking mode rather than raise an error, which is not what we want here. The socket is then bound to a port and listens with a maximum backlog of eight connections.</p>&#13;
<p class="indent">The main loop is built using <span class="literal">select()</span>, which receives the list of file descriptors we want to read (the socket in this case), the list of file descriptors we want to write to (none in this case), and the list of file descriptors we want to get exceptions from (the socket in this case). The <span class="literal">select()</span> function returns as soon as one of the selected file descriptors is ready to read, is ready to write, or has raised an exception. The returned values are lists of file descriptors that match the requests. It’s then easy to check whether our socket is in the ready-to-be-read list and, if so, accept the connection and send a message.</p>&#13;
<h3 class="h3" id="lev1sec64"><strong>Other Options and asyncio</strong></h3>&#13;
<p class="noindent">Alternatively, there are many frameworks, such as Twisted or Tornado, that provide this kind of functionality in a more integrated manner; Twisted has been the de facto standard for years in this regard. C libraries that export <span epub:type="pagebreak" id="page_183"/>Python interfaces, such as <span class="literal">libevent</span>, <span class="literal">libev</span>, or <span class="literal">libuv</span>, also provide very efficient event loops.</p>&#13;
<p class="indent">These options all solve the same problem. The downside is that, while there are a wide variety of choices, most of them are not interoperable. Many are also <em>callback based</em>, meaning that the program flow is not very clear when reading the code; you have to jump to a lot of different places to read through the program.</p>&#13;
<p class="indent">Another option would be the <span class="literal">gevent</span> or <span class="literal">greenlet</span> libraries, which avoid callback use. However, the implementation details include CPython x86–specific code and dynamic modification of standard functions at runtime, meaning you wouldn’t want to use and maintain code using these libraries over the long term.</p>&#13;
<p class="indent">In 2012, Guido Van Rossum began work on a solution code-named <em>tulip</em>, documented under PEP 3156 (<em><a href="https://www.python.org/dev/peps/pep-3156">https://www.python.org/dev/peps/pep-3156</a></em>). The goal of this package was to provide a standard event loop interface that would be compatible with all frameworks and libraries and be interoperable.</p>&#13;
<p class="indent">The tulip code has since been renamed and merged into Python 3.4 as the <span class="literal">asyncio</span> module, and it is now the de facto standard. Not all libraries are compatible with <span class="literal">asyncio</span>, and most existing bindings need to be rewritten.</p>&#13;
<p class="indent">As of Python 3.6, <span class="literal">asyncio</span> has been so well integrated that it has its own <span class="literal">await</span> and <span class="literal">async</span> keywords, making it straightforward to use. <a href="ch11.xhtml#ch11list4">Listing 11-4</a> shows how the <span class="literal">aiohttp</span> library, which provides an asynchronous HTTP binding, can be used with <span class="literal">asyncio</span> to run several web page retrievals concurrently.</p>&#13;
<p class="programs">import aiohttp<br/>import asyncio<br/><br/><br/>async def get(url):<br/>    async with aiohttp.ClientSession() as session:<br/>        async with session.get(url) as response:<br/>            return response<br/><br/><br/>loop = asyncio.get_event_loop()<br/><br/>coroutines = [get("http://example.com") for _ in range(8)]<br/><br/>results = loop.run_until_complete(asyncio.gather(*coroutines))<br/><br/>print("Results: %s" % results)</p>&#13;
<p class="listing1"><a id="ch11list4"/><em>Listing 11-4: Retrieving web pages concurrently with <span class="codeitalic">aiohttp</span></em></p>&#13;
<p class="indent">We define the <span class="literal">get()</span> function as asynchronous, so it is technically a coroutine. The <span class="literal">get()</span> function’s two steps, the connection and the page retrieval, are defined as asynchronous operations that yield control to the caller until they are ready. That makes it possible for <span class="literal">asyncio</span> to schedule another coroutine at any point. The module resumes the execution of a <span epub:type="pagebreak" id="page_184"/>coroutine when the connection is established or the page is ready to be read. The eight coroutines are started and provided to the event loop at the same time, and it is <span class="literal">asyncio</span>’s job to schedule them efficiently.</p>&#13;
<p class="indent">The <span class="literal">asyncio</span> module is a great framework for writing asynchronous code and leveraging event loops. It supports files, sockets, and more, and a lot of third-party libraries are available to support various protocols. Don’t hesitate to use it!</p>&#13;
<h3 class="h3" id="lev1sec65"><strong>Service-Oriented Architecture</strong></h3>&#13;
<p class="noindent">Circumventing Python’s scaling shortcomings can seem tricky. However, Python <em>is</em> very good at implementing <em>service-oriented architecture (SOA),</em> a style of software design in which different components provide a set of services through a communication protocol. For example, OpenStack uses SOA architecture in all of its components. The components use HTTP REST to communicate with external clients (end users) and an abstracted remote procedure call (RPC) mechanism that is built on top of the Advanced Message Queuing Protocol (AMQP).</p>&#13;
<p class="indent">In your development situations, knowing which communication channels to use between those blocks is mainly a matter of knowing with whom you will be communicating.</p>&#13;
<p class="indent">When exposing a service to the outside world, the preferred channel is HTTP, especially for stateless designs such as REST-style (REpresentational State Transfer–style) architectures. These kinds of architectures make it easier to implement, scale, deploy, and comprehend services.</p>&#13;
<p class="indent">However, when exposing and using your API internally, HTTP may be not the best protocol. There are many other communication protocols and fully describing even one would likely fill an entire book.</p>&#13;
<p class="indent">In Python, there are plenty of libraries for building RPC systems. Kombu is interesting because it provides an RPC mechanism on top of a lot of backends, AMQ protocol being the main one. It also supports Redis, MongoDB, Beanstalk, Amazon SQS, CouchDB, or ZooKeeper.</p>&#13;
<p class="indent">In the end, you can indirectly gain a huge amount of performance from using such loosely coupled architecture. If we consider that each module provides and exposes an API, we can run multiple daemons that can also expose that API, allowing multiple processes—and therefore CPUs—to handle the workload. For example, <em>Apache httpd</em> would create a new <span class="literal">worker</span> using a new system process that handles new connections; we could then dispatch a connection to a different <span class="literal">worker</span> running on the same node. To do so, we just need a system for dispatching the work to our various <span class="literal">workers</span>, which this API provides. Each block will be a different Python process, and as we’ve seen previously, this approach is better than multithreading for spreading out your workload. You’ll be able to start multiple <span class="literal">workers</span> on each node. Even if stateless blocks are not strictly necessary, you should favor their use anytime you have the choice.</p>&#13;
<h3 class="h3" id="lev1sec66"><span epub:type="pagebreak" id="page_185"/><strong>Interprocess Communication with ZeroMQ</strong></h3>&#13;
<p class="noindent">As we’ve just discussed, a messaging bus is always needed when building distributed systems. Your processes need to communicate with each other in order to pass messages. <span class="literal">ZeroMQ</span> is a socket library that can act as a concurrency framework. <a href="ch11.xhtml#ch11list5">Listing 11-5</a> implements the same <span class="literal">worker</span> seen in <a href="ch11.xhtml#ch11list1">Listing 11-1</a> but uses <span class="literal">ZeroMQ</span> as a way to dispatch work and communicate between processes.</p>&#13;
<p class="programs">   import multiprocessing<br/>   import random<br/>   import zmq<br/><br/>   def compute():<br/>       return sum(<br/>           [random.randint(1, 100) for i in range(1000000)])<br/><br/>   def worker():<br/>       context = zmq.Context()<br/>       work_receiver = context.socket(zmq.PULL)<br/>       work_receiver.connect("tcp://0.0.0.0:5555")<br/>       result_sender = context.socket(zmq.PUSH)<br/>       result_sender.connect("tcp://0.0.0.0:5556")<br/>       poller = zmq.Poller()<br/>       poller.register(work_receiver, zmq.POLLIN)<br/><br/>       while True:<br/>           socks = dict(poller.poll())<br/>           if socks.get(work_receiver) == zmq.POLLIN:<br/>               obj = work_receiver.recv_pyobj()<br/>               result_sender.send_pyobj(obj())<br/><br/>   context = zmq.Context()<br/>   # Build a channel to send work to be done<br/><span class="ent">➊</span> work_sender = context.socket(zmq.PUSH)<br/>   work_sender.bind("tcp://0.0.0.0:5555")<br/>   # Build a channel to receive computed results<br/><span class="ent">➋</span> result_receiver = context.socket(zmq.PULL)<br/>   result_receiver.bind("tcp://0.0.0.0:5556")<br/>   # Start 8 workers<br/>   processes = []<br/>   for x in range(8):<br/><span class="ent">➌</span>     p = multiprocessing.Process(target=worker)<br/>       p.start()<br/>       processes.append(p)<br/>   # Send 8 jobs<br/>   for x in range(8):<br/>       work_sender.send_pyobj(compute)<br/>   # Read 8 results<br/><br/>   results = []<br/>   for x in range(8):<br/><span class="ent">➍</span>     results.append(result_receiver.recv_pyobj())<span epub:type="pagebreak" id="page_186"/><br/>   # Terminate all processes<br/>   for p in processes:<br/>       p.terminate()<br/>   print("Results: %s" % results)</p>&#13;
<p class="listing1"><a id="ch11list5"/><em>Listing 11-5: <span class="codeitalic">workers</span> using <span class="codeitalic">ZeroMQ</span></em></p>&#13;
<p class="indent">We create two sockets, one to send the function <span class="literal">(work_sender)</span> <span class="ent">➊</span> and one to receive the job <span class="literal">(result_receiver)</span> <span class="ent">➋</span>. Each <span class="literal">worker</span> started by <span class="literal">multiprocessing.Process</span> <span class="ent">➌</span> creates its own set of sockets and connects them to the master process. The <span class="literal">worker</span> then executes whatever function is sent to it and sends back the result. The master process just has to send eight jobs over its sender socket and wait for eight results to be sent back via the receiver socket <span class="ent">➍</span>.</p>&#13;
<p class="indent">As you can see, <span class="literal">ZeroMQ</span> provides an easy way to build communication channels. I’ve chosen to use the TCP transport layer here to illustrate the fact that we could run this over a network. It should be noted that <span class="literal">ZeroMQ</span> also provides an interprocess communication channel that works locally (without any network layer involved) by using Unix sockets. Obviously, the communication protocol built upon <span class="literal">ZeroMQ</span> in this example is very simple for the sake of being clear and concise, but it shouldn’t be hard to imagine building a more sophisticated communication layer on top of it. It’s also easy to imagine building an entirely distributed application communication with a network message bus such as ZeroMQ or AMQP.</p>&#13;
<p class="indent">Note that protocols such as HTTP, ZeroMQ, and AMQP are language agnostic: you can use different languages and platforms to implement each part of your system. While we all agree that Python is a good language, other teams might have other preferences, or another language might be a better solution for some part of a problem.</p>&#13;
<p class="indent">In the end, using a transport bus to decouple your application into several parts is a good option. This approach allows you to build both synchronous and asynchronous APIs that can be distributed from one computer to several thousand. It doesn’t tie you to a particular technology or language, so you can evolve everything in the right direction.</p>&#13;
<h3 class="h3" id="lev1sec67"><strong>Summary</strong></h3>&#13;
<p class="noindent">The rule of thumb in Python is to use threads only for I/O-intensive workloads and to switch to multiple processes as soon as a CPU-intensive workload is on the table. Distributing workloads on a wider scale—such as when building a distributed system over a network—requires external libraries and protocols. These are supported by Python, though provided externally.</p>&#13;
</body></html>