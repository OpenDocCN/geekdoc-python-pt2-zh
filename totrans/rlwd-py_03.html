<html><head></head><body>
<h2 class="h2" id="ch03"><span epub:type="pagebreak" id="page_51"/><span class="big">3</span><br/>SUMMARIZING SPEECHES WITH NATURAL LANGUAGE PROCESSING</h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>&#13;
<p class="noindent">“Water, water everywhere, but not a drop to drink.” This famous line, from <em>The Rime of the Ancient Mariner</em>, summarizes the present state of digital information. According to International Data Corporation, by 2025 we’ll be generating 175 trillion gigabytes of digital data <em>per year</em>. But most of this data—up to 95 percent—will be <em>unstructured</em>, which means it’s not organized into useful databases. Even now, the key to the cure for cancer may be right at our fingertips yet almost impossible to reach.</p>&#13;
<p class="indent">To make information easier to discover and consume, we need to reduce the volume of data by extracting and repackaging salient points into digestible summaries. Because of the sheer volume of data, there’s no way to do this manually. Luckily, natural language processing (NLP) helps computers understand both words and context. For example, NLP applications can summarize news feeds, analyze legal contracts, research patents, study financial markets, capture corporate knowledge, and produce study guides.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_52"/>In this chapter, you’ll use Python’s Natural Language Toolkit (NLTK) to generate a summary of one of the most famous speeches of all time, “I Have a Dream” by Martin Luther King Jr. With an understanding of the basics, you’ll then use a streamlined alternative, called <span class="literal">gensim</span>, to summarize the popular “Make Your Bed” speech by Admiral William H. McRaven. Finally, you’ll use a word cloud to produce a fun visual summary of the most frequently used words in Sir Arthur Conan Doyle’s novel <em>The Hound of the Baskervilles</em>.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec18"><strong>Project #3: I Have a Dream . . . to Summarize Speeches!</strong></h3>&#13;
<p class="noindent">In machine learning and data mining, there are two approaches to summarizing text: <em>extraction</em> and <em>abstraction</em>.</p>&#13;
<p class="indent">Extraction-based summarization uses various weighting functions to rank sentences by perceived importance. Words used more often are considered more important. Consequently, sentences containing those words are considered more important. The overall behavior is like using a yellow highlighter to manually select keywords and sentences without altering the text. The results can be disjointed, but the technique is good at pulling out important words and phrases.</p>&#13;
<p class="indent">Abstraction relies on deeper comprehension of the document to capture intent and produce more human-like paraphrasing. This includes creating completely new sentences. The results tend to be more cohesive and grammatically correct than those produced by extraction-based methods, but at a price. Abstraction algorithms require advanced and complicated deep learning methods and sophisticated language modeling.</p>&#13;
<p class="indent">For this project, you’ll use an extraction-based technique on the “I Have a Dream” speech, delivered by Martin Luther King Jr. at the Lincoln Memorial on August 28, 1963. Like Lincoln’s “Gettysburg Address” a century before, it was the perfect speech at the perfect time. Dr. King’s masterful use of repetition also makes it tailor-made for extraction techniques, which correlate word frequency with importance.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that summarizes a speech using NLP text extraction.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec13"><strong><em>The Strategy</em></strong></h4>&#13;
<p class="noindent">The Natural Language Toolkit includes the functions you’ll need to summarize Dr. King’s speech. If you skipped <a href="ch02.xhtml">Chapter 2</a>, see <a href="ch02.xhtml#page_29">page 29</a> for installation instructions.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_53"/>To summarize the speech, you’ll need a digital copy. In previous chapters, you manually downloaded files you needed from the internet. This time you’ll use a more efficient technique, called <em>web scraping</em>, which allows you to programmatically extract and save large amounts of data from websites.</p>&#13;
<p class="indent">Once you’ve loaded the speech as a string, you can use NLTK to split out and count individual words. Then, you’ll “score” each sentence in the speech by summing the word counts within it. You can use those scores to print the top-ranked sentences, based on how many sentences you want in your summary.</p>&#13;
<h4 class="h4" id="ch00lev2sec14"><strong><em>Web Scraping</em></strong></h4>&#13;
<p class="noindent">Scraping the web means using a program to download and process content. This is such a common task that prewritten scraping programs are freely available. You’ll use the <span class="literal">requests</span> library to download files and web pages, and you’ll use the Beautiful Soup (<span class="literal">bs4</span>) package to parse HTML. Short for <em>Hypertext Markup Language</em>, HTML is the standard format used to create web pages.</p>&#13;
<p class="indent">To install the two modules, use pip in a terminal window or Windows PowerShell (see <a href="ch01.xhtml#page_8">page 8</a> in <a href="ch01.xhtml">Chapter 1</a> for instructions on installing and using pip):</p>&#13;
<pre><span class="codestrong1">pip install requests</span> &#13;
<span class="codestrong1">pip install beautifulsoup4</span></pre>&#13;
<p class="indent">To check the installation, open the shell and import each module as shown next. If you don’t get an error, you’re good to go!</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import requests</span>&#13;
&gt;&gt;&gt; &#13;
&gt;&gt;&gt; <span class="codestrong1">import bs4</span>&#13;
&gt;&gt;&gt;</pre>&#13;
<p class="indent">To learn more about <span class="literal">requests</span>, visit <em><a href="https://pypi.org/project/requests/">https://pypi.org/project/requests/</a></em>. For Beautiful Soup, see <em><a href="https://www.crummy.com/software/BeautifulSoup/">https://www.crummy.com/software/BeautifulSoup/</a></em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec15"><strong><em>The “I Have a Dream” Code</em></strong></h4>&#13;
<p class="noindent">The <em>dream_summary.py</em> program performs the following steps:</p>&#13;
<ol>&#13;
<li class="noindent">Opens a web page containing the “I Have a Dream” speech</li>&#13;
<li class="noindent">Loads the text as a string</li>&#13;
<li class="noindent">Tokenizes the text into words and sentences</li>&#13;
<li class="noindent">Removes stop words with no contextual content</li>&#13;
<li class="noindent">Counts the remaining words</li>&#13;
<li class="noindent">Uses the counts to rank the sentences</li>&#13;
<li class="noindent">Displays the highest-ranking sentences</li>&#13;
</ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_54"/>If you’ve already downloaded the book’s files, find the program in the <em>Chapter_3</em> folder. Otherwise, go to <em><a href="https://nostarch.com/real-world-python/">https://nostarch.com/real-world-python/</a></em> and download it from the book’s GitHub page.</p>&#13;
<h5 class="h5"><strong>Importing Modules and Defining the main() Function</strong></h5>&#13;
<p class="noindent"><a href="ch03.xhtml#ch03list1">Listing 3-1</a> imports modules and defines the first part of the <span class="literal">main()</span> function, which scrapes the web and assigns the speech to a variable as a string.</p>&#13;
<pre><span class="codeitalic1">dream_summary.py,</span> <span class="normal">part 1</span>&#13;
from collections import Counter&#13;
import re&#13;
import requests&#13;
import bs4&#13;
import nltk&#13;
from nltk.corpus import stopwords&#13;
&#13;
def main():&#13;
 <span class="ent">➊</span> url = 'http://www.analytictech.com/mb021/mlk.htm'&#13;
    page = requests.get(url)&#13;
    page.raise_for_status()&#13;
 <span class="ent">➋</span> soup = bs4.BeautifulSoup(page.text, 'html.parser')&#13;
    p_elems = [element.text for element in soup.find_all('p')]&#13;
&#13;
    speech = ''.join(p_elems)</pre>&#13;
<p class="listing"><a id="ch03list1"/>Listing 3-1: Importing modules and defining the <span class="literal">main()</span> function</p>&#13;
<p class="indent">Start by importing <span class="literal">Counter</span> from the <span class="literal">collections</span> module to help you keep track of the sentence scoring. The <span class="literal">collections</span> module is part of the Python Standard Library and includes several container data types. A <span class="literal">Counter</span> is a dictionary subclass for counting hashable objects. Elements are stored as dictionary keys, and their counts are stored as dictionary values.</p>&#13;
<p class="indent">Next, to clean up the speech prior to summarizing its contents, import the <span class="literal">re</span> module. The <em>re</em> stands for <em>regular expressions</em>, also referred to as <em>regexes</em>, which are sequences of characters that define a search pattern. This module will help you clean up the speech by allowing you to selectively remove bits that you don’t want.</p>&#13;
<p class="indent">Finish the imports with the modules for scraping the web and doing natural language processing. The last module brings in the list of functional stop words (such as <em>if</em>, <em>and</em>, <em>but</em>, <em>for</em>) that contain no useful information. You’ll remove these from the speech prior to summarization.</p>&#13;
<p class="indent">Next, define a <span class="literal">main()</span> function to run the program. To scrape the speech off the web, provide the <span class="literal">url</span> address as a string <span class="ent">➊</span>. You can copy and paste this from the website from which you want to extract text.</p>&#13;
<p class="indent">The <span class="literal">requests</span> library abstracts the complexities of making HTTP requests in Python. HTTP, short for HyperText Transfer Protocol, is the foundation of data communication using hyperlinks on the World Wide Web. Use the <span class="literal">requests</span>.<span class="literal">get()</span> method to fetch the <span class="literal">url</span> and assign the output to the <span class="literal">page</span> variable, which references the <span class="literal">Response</span> object the web page returned for the request. This object’s text attribute holds the web page, including the speech, as a string.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_55"/>To check that the download was successful, call the <span class="literal">Response</span> object’s <span class="literal">raise_for_status()</span> method. This does nothing if everything goes okay but otherwise will raise an exception and halt the program.</p>&#13;
<p class="indent">At this point, the data is in HTML, as shown here:</p>&#13;
<pre>&lt;!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN"&gt;&#13;
&lt;html&gt;&#13;
&#13;
&lt;head&gt;&#13;
&lt;meta http-equiv="Content-Type"&#13;
content="text/html; charset=iso-8859-1"&gt;&#13;
&lt;meta name="GENERATOR" content="Microsoft FrontPage 4.0"&gt;&#13;
&lt;title&gt;Martin Luther King Jr.'s 1962 Speech&lt;/title&gt;&#13;
&lt;/head&gt;&#13;
<span class="codeitalic1">--snip--</span>&#13;
&lt;p&gt;I am happy to join with you today in what will go down in&#13;
history as the greatest demonstration for freedom in the history&#13;
of our nation. &lt;/p&gt;&#13;
<span class="codeitalic1">--snip--</span></pre>&#13;
<p class="indent">As you can see, HTML has a lot of <em>tags</em>, such as <span class="literal">&lt;head&gt;</span> and <span class="literal">&lt;p&gt;</span>, that let your browser know how to format the web page. The text between starting and closing tags is called an <em>element</em>. For example, the text “Martin Luther King Jr.’s 1962 Speech” is a title element sandwiched between the starting tag <span class="literal">&lt;title&gt;</span> and the closing tag <span class="literal">&lt;/title&gt;</span>. Paragraphs are formatted using <span class="literal">&lt;p&gt;</span> and <span class="literal">&lt;/p&gt;</span> tags.</p>&#13;
<p class="indent">Because these tags are not part of the original text, they should be removed prior to any natural language processing. To remove the tags, call the <span class="literal">bs4.BeautifulSoup()</span> method and pass it the string containing the HTML <span class="ent">➋</span>. Note that I’ve explicitly specified <span class="literal">html.parser</span>. The program will run without this but complain bitterly with warnings in the shell.</p>&#13;
<p class="indent">The <span class="literal">soup</span> variable now references a <span class="literal">BeautifulSoup</span> object, which means you can use the object’s <span class="literal">find_all()</span> method to locate the speech buried in the HTML document. In this case, to find the text between paragraph tags (<span class="literal">&lt;p&gt;</span>), use list comprehension and <span class="literal">find_all()</span> to make a list of just the paragraph elements.</p>&#13;
<p class="indent">Finish by turning the speech into a continuous string. Use the <span class="literal">join()</span> method to turn the <span class="literal">p_elems</span> list into a string. Set the “joiner” character to a space, designated by <span class="literal">''</span>.</p>&#13;
<p class="indent">Note that with Python, there is usually more than one way to accomplish a task. The last two lines of the listing can also be written as follows:</p>&#13;
<pre>    p_elems = soup.select('p')&#13;
    speech = ''.join(p_elems)</pre>&#13;
<p class="indent">The <span class="literal">select()</span> method is more limited overall than <span class="literal">find_all()</span>, but in this case it works the same and is more succinct. In the previous snippet, <span class="literal">select()</span> finds the <span class="literal">&lt;p&gt;</span> tags, and the results are converted to text when concatenated to the <span class="literal">speech</span> string.</p>&#13;
<h5 class="h5"><strong>Completing the main() Function</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_56"/>Next, you’ll prep the speech to fix typos and remove punctuation, special characters, and spaces. Then you’ll call three functions to remove stop words, count word frequency, and score the sentences based on the word counts. Finally, you’ll rank the sentences and display those with the highest scores in the shell.</p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03list2">Listing 3-2</a> completes the definition of <span class="literal">main()</span> that performs these tasks.</p>&#13;
<pre><span class="codeitalic1">dream_summary.py,</span> <span class="normal">part 2</span>&#13;
   speech = speech.replace(')mowing', 'knowing')&#13;
   speech = re.sub('\s+', ' ', speech) &#13;
   speech_edit = re.sub('[^a-zA-Z]', ' ', speech)&#13;
   speech_edit = re.sub('\s+', ' ', speech_edit)&#13;
&#13;
<span class="ent">➊</span> while True:&#13;
       max_words = input("Enter max words per sentence for summary: ")&#13;
       num_sents = input("Enter number of sentences for summary: ")&#13;
       if max_words.isdigit() and num_sents.isdigit():&#13;
           break&#13;
       else:&#13;
           print("\nInput must be in whole numbers.\n")&#13;
   &#13;
   speech_edit_no_stop = remove_stop_words(speech_edit)&#13;
   word_freq = get_word_freq(speech_edit_no_stop)&#13;
   sent_scores = score_sentences(speech, word_freq, max_words)&#13;
&#13;
<span class="ent">➋</span> counts = Counter(sent_scores)&#13;
   summary = counts.most_common(int(num_sents))&#13;
   print("\nSUMMARY:")&#13;
   for i in summary:&#13;
       print(i[0])</pre>&#13;
<p class="listing"><a id="ch03list2"/>Listing 3-2: Completing the <span class="literal">main()</span> function</p>&#13;
<p class="indent">The original document contains a typo (<em>mowing</em> instead of <em>knowing</em>), so start by fixing this using the <span class="literal">string.replace()</span> method. Continue cleaning the speech using <span class="literal">regex</span>. Many casual programmers are turned off by this module’s arcane syntax, but it’s such a powerful and useful tool that everyone should be aware of the basic <span class="literal">regex</span> syntax.</p>&#13;
<p class="indent">Remove extra spaces using the <span class="literal">re.sub()</span> function, which replaces substrings with new characters. Use the shorthand character class code <span class="literal">\s+</span> to identify runs of whitespace and replace them with a single space, indicated by <span class="literal">' '</span>. Finish by passing <span class="literal">re.sub()</span> the name of the string (<span class="literal">speech</span>).</p>&#13;
<p class="indent">Next, remove anything that’s <em>not</em> a letter by matching the <span class="literal">[^a-zA-Z]</span> pattern. The caret at the start instructs <span class="literal">regex</span> to “match any character that isn’t between the brackets.” So, numbers, punctuation marks, and so on, will be replaced by a space.</p>&#13;
<p class="indent">Removing characters like punctuation marks will leave an extra space. To get rid of these spaces, call the <span class="literal">re.sub()</span> method again.</p>&#13;
<p class="indent">Next, request that the user input the number of sentences to include in the summary and the maximum number of words per sentence. Use a <span class="literal">while</span> loop and Python’s built-in <span class="literal">isdigit()</span> function to ensure the user inputs an integer <span class="ent">➊</span>.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><span epub:type="pagebreak" id="page_57"/><em>According to research by the American Press Institute, comprehension is best with sentences of fewer than 15 words. Similarly, the <span class="normal">Oxford Guide to Plain English</span> recommends using sentences that average 15 to 20 words over a full document.</em></p>&#13;
</div>&#13;
<p class="indent">Continue cleaning the text by calling the <span class="literal">remove_stop_words()</span> function. Then call functions <span class="literal">get_word_freq()</span> and <span class="literal">score_sentences()</span> to calculate the frequency of the remaining words and to score the sentences, respectively. You’ll define these functions after completing the <span class="literal">main()</span> function.</p>&#13;
<p class="indent">To rank the sentences, call the <span class="literal">collection</span> module’s <span class="literal">Counter()</span> method <span class="ent">➋</span>. Pass it the <span class="literal">sent_scores</span> variable.</p>&#13;
<p class="indent">To generate the summary, use the <span class="literal">Counter</span> object’s <span class="literal">most_common()</span> method. Pass it the <span class="literal">num_sents</span> variable input by the user. The resulting <span class="literal">summary</span> variable will hold a list of tuples. For each tuple, the sentence is at index <span class="literal">[0]</span>, and its rank is at index <span class="literal">[1]</span>.</p>&#13;
<pre>[('From every mountainside, let freedom ring.', 4.625), <span class="codeitalic1">--snip--</span> ]</pre>&#13;
<p class="indent">For readability, print each sentence of the summary on a separate line.</p>&#13;
<h5 class="h5"><strong>Removing Stop Words</strong></h5>&#13;
<p class="noindent">Remember from <a href="ch02.xhtml">Chapter 2</a> that stop words are short, functional words like <em>if</em>, <em>but</em>, <em>for</em>, and <em>so</em>. Because they contain no important contextual information, you don’t want to use them to rank sentences.</p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03list3">Listing 3-3</a> defines a function called <span class="literal">remove_stop_words()</span> to remove stop words from the speech.</p>&#13;
<pre><span class="codeitalic1">dream_summary.py,</span> <span class="normal">part 3</span>&#13;
def remove_stop_words(speech_edit):&#13;
    """Remove stop words from string and return string."""&#13;
    stop_words = set(stopwords.words('english'))&#13;
    speech_edit_no_stop = ''&#13;
    for word in nltk.word_tokenize(speech_edit):&#13;
        if word.lower() not in stop_words:&#13;
            speech_edit_no_stop += word + ' '&#13;
    return speech_edit_no_stop</pre>&#13;
<p class="listing"><a id="ch03list3"/>Listing 3-3: Defining a function to remove stop words from the speech</p>&#13;
<p class="indent">Define the function to receive <span class="literal">speech_edit</span>, the edited speech string, as an argument. Then create a set of the English stop words in NLTK. Use a set, rather than a list, as searches are quicker in sets.</p>&#13;
<p class="indent">Assign an empty string to hold the edited speech sans stop words. The <span class="literal">speech_edit</span> variable is currently a string in which each element is a letter.</p>&#13;
<p class="indent">To work with words, call the NLTK <span class="literal">word_tokenize()</span> method. Note that you can do this while looping through words. Convert each word to lowercase and check its membership in the <span class="literal">stop_words</span> set. If it’s not a stop word, concatenate it to the new string, along with a space. Return this string to end the function.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_58"/>How you handle letter case in this program is important. You’ll want the summary to print with both uppercase and lowercase letters, but you must do the NLP work using all lowercase to avoid miscounting. To see why, look at the following code snippet, which counts words in a string (<span class="literal">s</span>) with mixed cases:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">s = 'one One one'</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">fd = nltk.FreqDist(nltk.word_tokenize(s))</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">fd</span>&#13;
FreqDist({'one': 2, 'One': 1})&#13;
&gt;&gt;&gt; <span class="codestrong1">fd_lower = nltk.FreqDist(nltk.word_tokenize(s.lower()))</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">fd_lower</span>&#13;
FreqDist({'one': 3})</pre>&#13;
<p class="indent">If you don’t convert the words to lowercase, <em>one</em> and <em>One</em> are considered distinct elements. For counting purposes, every instance of <em>one</em> regardless of its case should be treated as the same word. Otherwise, the contribution of <em>one</em> to the document will be diluted.</p>&#13;
<h5 class="h5"><strong>Calculating the Frequency of Occurrence of Words</strong></h5>&#13;
<p class="noindent">To count the occurrence of each word in the speech, you’ll create the <span class="literal">get_word_freq()</span> function that returns a dictionary with the words as keys and the counts as values. <a href="ch03.xhtml#ch03list4">Listing 3-4</a> defines this function.</p>&#13;
<pre><span class="codeitalic1">dream_summary.py,</span> <span class="normal">part 4</span> &#13;
def get_word_freq(speech_edit_no_stop):&#13;
    """Return a dictionary of word frequency in a string."""        &#13;
    word_freq = nltk.FreqDist(nltk.word_tokenize(speech_edit_no_stop.lower()))&#13;
    return word_freq</pre>&#13;
<p class="listing"><a id="ch03list4"/>Listing 3-4: Defining a function to calculate word frequency in the speech</p>&#13;
<p class="indent">The <span class="literal">get_word_freq()</span> function takes the edited speech string with no stop words as an argument. NLTK’s <span class="literal">FreqDist</span> class acts like a dictionary with the words as keys and their counts as values. As part of the process, convert the input string to lowercase and tokenize it into words. End the function by returning the <span class="literal">word_freq</span> dictionary.</p>&#13;
<h5 class="h5"><strong>Scoring Sentences</strong></h5>&#13;
<p class="noindent"><a href="ch03.xhtml#ch03list5">Listing 3-5</a> defines a function that scores sentences based on the frequency distribution of the words they contain. It returns a dictionary with each sentence as the key and its score as the value.</p>&#13;
<pre><span class="codeitalic1">dream_summary.py,</span> <span class="normal">part 5</span>&#13;
def score_sentences(speech, word_freq, max_words):&#13;
    """Return dictionary of sentence scores based on word frequency."""&#13;
    sent_scores = dict()&#13;
    sentences = nltk.sent_tokenize(speech)&#13;
 <span class="ent">➊</span> for sent in sentences:&#13;
        sent_scores[sent] = 0&#13;
        <span epub:type="pagebreak" id="page_59"/>words = nltk.word_tokenize(sent.lower())&#13;
        sent_word_count = len(words)&#13;
     <span class="ent">➋</span> if sent_word_count &lt;= int(max_words):&#13;
            for word in words:&#13;
                if word in word_freq.keys():&#13;
                    sent_scores[sent] += word_freq[word]&#13;
       <span class="ent">➌</span> sent_scores[sent] = sent_scores[sent] / sent_word_count&#13;
  return sent_scores&#13;
&#13;
if __name__ == '__main__':&#13;
    main()</pre>&#13;
<p class="listing"><a id="ch03list5"/>Listing 3-5: Defining a function to score sentences based on word frequency</p>&#13;
<p class="indent">Define a function, called <span class="literal">score_sentences()</span>, with parameters for the original <span class="literal">speech</span> string, the <span class="literal">word_freq</span> object, and the <span class="literal">max_words</span> variable input by the user. You want the summary to contain stop words and capitalized words—hence the use of <span class="literal">speech</span>.</p>&#13;
<p class="indent">Start an empty dictionary, named <span class="literal">sent_scores</span>, to hold the scores for each sentence. Next, tokenize the <span class="literal">speech</span> string into sentences.</p>&#13;
<p class="indent">Now, start looping through the sentences <span class="ent">➊</span>. Start by updating the <span class="literal">sent_scores</span> dictionary, assigning the sentence as the key, and setting its initial value (count) to 0.</p>&#13;
<p class="indent">To count word frequency, you first need to tokenize the sentence into words. Be sure to use lowercase to be compatible with the <span class="literal">word_freq</span> dictionary.</p>&#13;
<p class="indent">You’ll need to be careful when you sum up the word counts per sentence to create the scores so you don’t bias the results toward longer sentences. After all, longer sentences are more likely to have a greater number of important words. To avoid excluding short but important sentences, you need to <em>normalize</em> each count by dividing it by the sentence <em>length</em>. Store the length in a variable called <span class="literal">sent_word_count</span>.</p>&#13;
<p class="indent">Next, use a conditional that constrains sentences to the maximum length input by the user <span class="ent">➋</span>. If the sentence passes the test, start looping through its words. If a word is in the <span class="literal">word_freq</span> dictionary, add it to the count stored in <span class="literal">sent_scores</span>.</p>&#13;
<p class="indent">At the end of each loop through the sentences, divide the score for the current sentence by the number of words in the sentence <span class="ent">➌</span>. This normalizes the score so long sentences don’t have an unfair advantage.</p>&#13;
<p class="indent">End the function by returning the <span class="literal">sent_scores</span> dictionary. Then, back in the global space, add the code for running the program as a module or in stand-alone mode.</p>&#13;
<h5 class="h5"><strong>Running the Program</strong></h5>&#13;
<p class="noindent">Run the <em>dream_summary.py</em> program with a maximum sentence length of 14 words. As mentioned previously, good, readable sentences tend to contain 14 words or fewer. Then truncate the summary at 15 sentences, about one-third of the speech. You should get the following results. Note that the sentences won’t necessarily appear in their original order.</p>&#13;
<pre><span epub:type="pagebreak" id="page_60"/>Enter max words per sentence for summary: 14&#13;
Enter number of sentences for summary: 15&#13;
&#13;
SUMMARY:&#13;
From every mountainside, let freedom ring.&#13;
Let freedom ring from Lookout Mountain in Tennessee!&#13;
Let freedom ring from every hill and molehill in Mississippi.&#13;
Let freedom ring from the curvaceous slopes of California!&#13;
Let freedom ring from the snow capped Rockies of Colorado!&#13;
But one hundred years later the Negro is still not free.&#13;
From the mighty mountains of New York, let freedom ring.&#13;
From the prodigious hilltops of New Hampshire, let freedom ring.&#13;
And I say to you today my friends, let freedom ring.&#13;
I have a dream today.&#13;
It is a dream deeply rooted in the American dream.&#13;
Free at last!&#13;
Thank God almighty, we're free at last!"&#13;
We must not allow our creative protest to degenerate into physical violence.&#13;
This is the faith that I go back to the mount with.</pre>&#13;
<p class="indent">Not only does the summary capture the title of the speech, it captures the main points.</p>&#13;
<p class="indent">But if you run it again with 10 words per sentence, a lot of the sentences are clearly too long. Because there are only 7 sentences in the whole speech with 10 or fewer words, the program can’t honor the input requirements. It defaults to printing the speech from the beginning until the sentence count is at least what was specified in the <span class="literal">num_sents</span> variable.</p>&#13;
<p class="indent">Now, rerun the program and try setting the word count limit to 1,000.</p>&#13;
<pre>Enter max words per sentence for summary: 1000&#13;
Enter number of sentences for summary: 15&#13;
&#13;
SUMMARY:&#13;
From every mountainside, let freedom ring.&#13;
Let freedom ring from Lookout Mountain in Tennessee!&#13;
Let freedom ring from every hill and molehill in Mississippi.&#13;
Let freedom ring from the curvaceous slopes of California!&#13;
Let freedom ring from the snow capped Rockies of Colorado!&#13;
But one hundred years later the Negro is still not free.&#13;
From the mighty mountains of New York, let freedom ring.&#13;
From the prodigious hilltops of New Hampshire, let freedom ring.&#13;
And I say to you today my friends, let freedom ring.&#13;
I have a dream today.&#13;
But not only there; let freedom ring from the Stone Mountain of Georgia!&#13;
It is a dream deeply rooted in the American dream.&#13;
With this faith we will be able to work together, pray together; to struggle&#13;
together, to go to jail together, to stand up for freedom forever, knowing&#13;
that we will be free one day.&#13;
Free at last!&#13;
One hundred years later the life of the Negro is still sadly crippled by the&#13;
manacles of segregation and the chains of discrimination.</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_61"/>Although longer sentences don’t dominate the summary, a few slipped through, making this summary less poetic than the previous one. The lower word count limit forces the previous version to rely more on shorter phrases that act like a chorus.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec19"><strong>Project #4: Summarizing Speeches with gensim</strong></h3>&#13;
<p class="noindent">In an Emmy award–winning episode of <em>The Simpsons</em>, Homer runs for sanitation commissioner using the campaign slogan, “Can’t someone else do it?” That’s certainly the case with many Python applications: often, when you need to write a script, you learn that someone else has already done it! One example is <span class="literal">gensim</span>, an open source library for natural language processing using statistical machine learning.</p>&#13;
<p class="indent">The word <em>gensim</em> stands for “generate similar.” It uses a graph-based ranking algorithm called TextRank. This algorithm was inspired by PageRank, invented by Larry Page and used to rank web pages in Google searches. With PageRank, the importance of a website is determined by how many other pages link to it. To use this approach with text processing, algorithms measure how similar each sentence is to all the other sentences. The sentence that is the most like the others is considered the most important.</p>&#13;
<p class="indent">In this project, you’ll use <span class="literal">gensim</span> to summarize Admiral William H. McRaven’s commencement address, “Make Your Bed,” given at the University of Texas at Austin in 2014. This inspirational, 20-minute speech has been viewed more than 10 million times on YouTube and inspired a <em>New York Times</em> bestselling book in 2017.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that uses the <span class="literal">gensim</span> module to summarize a speech.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec16"><strong><em>Installing gensim</em></strong></h4>&#13;
<p class="noindent">The <span class="literal">gensim</span> module runs on all the major operating systems but is dependent on <span class="literal">NumPy</span> and <span class="literal">SciPy</span>. If you don’t have them installed, go back to <a href="ch01.xhtml">Chapter 1</a> and follow the instructions in “Installing the Python Libraries” on <a href="ch01.xhtml#page_6">page 6</a>.</p>&#13;
<p class="indent">To install <span class="literal">gensim</span> on Windows, use <span class="literal">pip install -U gensim</span>. To install it in a terminal, use <span class="literal">pip install --upgrade gensim</span>. For <span class="literal">conda</span> environments, use <span class="literal">conda install -c conda-forge gensim</span>. For more on <span class="literal">gensim</span>, go to <em><a href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a></em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec17"><strong><em>The Make Your Bed Code</em></strong></h4>&#13;
<p class="noindent">With the <em>dream_summary.py</em> program in Project 3, you learned the fundamentals of text extraction. Since you’ve seen some of the details, use <span class="literal">gensim</span> as a streamlined alternative to <em>dream_summary.py</em>. Name this new program <em>bed_summary.py</em> or download it from the book’s website.</p>&#13;
<h5 class="h5"><strong>Importing Modules, Scraping the Web, and Preparing the Speech String</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_62"/><a href="ch03.xhtml#ch03list6">Listing 3-6</a> repeats the code used in <em>dream_summary.py</em> to prepare the speech as a string. To revisit the detailed code explanation, see <a href="ch03.xhtml#page_54">page 54</a>.</p>&#13;
<pre><span class="codeitalic1">bed_summary.py,</span> <span class="normal">part 1</span>&#13;
   import requests&#13;
   import bs4&#13;
   from nltk.tokenize import sent_tokenize&#13;
<span class="ent">➊</span> from gensim.summarization import summarize&#13;
&#13;
<span class="ent">➋</span> url = 'https://jamesclear.com/great-speeches/make-your-bed-by-admiral&#13;
          -william-h-mcraven'&#13;
   page = requests.get(url)&#13;
   page.raise_for_status()&#13;
   soup = bs4.BeautifulSoup(page.text, 'html.parser')&#13;
   p_elems = [element.text for element in soup.find_all('p')]&#13;
&#13;
   speech = ''.join(p_elems)</pre>&#13;
<p class="listing"><a id="ch03list6"/>Listing 3-6: Importing modules and loading the speech as a string</p>&#13;
<p class="indent">You’ll test <span class="literal">gensim</span> on the raw speech scraped from the web, so you won’t need modules for cleaning the text. The <span class="literal">gensim</span> module will also do any counting internally, so you don’t need <span class="literal">Counter</span>, but you will need <span class="literal">gensim</span>’s <span class="literal">summarize()</span> function to summarize the text <span class="ent">➊</span>. The only other change is to the <span class="literal">url</span> address <span class="ent">➋</span>.</p>&#13;
<h5 class="h5"><strong>Summarizing the Speech</strong></h5>&#13;
<p class="noindent"><a href="ch03.xhtml#ch03list7">Listing 3-7</a> completes the program by summarizing the speech and printing the results.</p>&#13;
<pre><span class="codeitalic1">bed_summary.py,</span> <span class="normal">part 2</span>&#13;
print("\nSummary of Make Your Bed speech:")&#13;
summary = summarize(speech, word_count=225)&#13;
sentences = sent_tokenize(summary)&#13;
sents = set(sentences)&#13;
print(' '.join(sents))</pre>&#13;
<p class="listing"><a id="ch03list7"/>Listing 3-7: Running <span class="literal">gensim</span>, removing duplicate lines, and printing the summary</p>&#13;
<p class="indent">Start by printing a header for your summary. Then, call the <span class="literal">gensim</span> <span class="literal">summarize()</span> function to summarize the speech in 225 words. This word count will produce about 15 sentences, assuming the average sentence has 15 words. In addition to a word count, you can pass <span class="literal">summarize()</span> a ratio, such as <span class="literal">ratio=0.01</span>. This will produce a summary whose length is 1 percent of the full document.</p>&#13;
<p class="indent">Ideally, you could summarize the speech and print the summary in one step.</p>&#13;
<pre>print(summarize(speech, word_count=225))</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_63"/>Unfortunately, <span class="literal">gensim</span> sometimes duplicates sentences in summaries, and that occurs here:</p>&#13;
<pre>Summary of Make Your Bed speech:&#13;
Basic SEAL training is six months of long torturous runs in the soft sand,&#13;
midnight swims in the cold water off San Diego, obstacle courses, unending&#13;
calisthenics, days without sleep and always being cold, wet and miserable.&#13;
Basic SEAL training is six months of long torturous runs in the soft sand,&#13;
midnight swims in the cold water off San Diego, obstacle courses, unending&#13;
calisthenics, days without sleep and always being cold, wet and miserable.&#13;
<span class="codeitalic1">--snip--</span></pre>&#13;
<p class="indent">To avoid duplicating text, you first need to break out the sentences in the summary variable using the NLTK <span class="literal">sent_tokenize()</span> function. Then make a set from these sentences, which will remove duplicates. Finish by printing the results.</p>&#13;
<p class="indent">Because sets are unordered, the arrangement of the sentences may change if you run the program multiple times.</p>&#13;
<pre>Summary of Make Your Bed speech:&#13;
If you can't do the little things right, you will never do the big things&#13;
right.And, if by chance you have a miserable day, you will come home to a&#13;
bed that is made — that you made — and a made bed gives you encouragement&#13;
that tomorrow will be better.If you want to change the world, start off&#13;
by making your bed.During SEAL training the students are broken down into&#13;
boat crews. It's just the way life is sometimes.If you want to change the&#13;
world get over being a sugar cookie and keep moving forward.Every day during&#13;
training you were challenged with multiple physical events — long runs, long&#13;
swims, obstacle courses, hours of calisthenics — something designed to test&#13;
your mettle. Basic SEAL training is six months of long torturous runs in the&#13;
soft sand, midnight swims in the cold water off San Diego, obstacle courses,&#13;
unending calisthenics, days without sleep and always being cold, wet and&#13;
miserable.&#13;
&gt;&gt;&gt;&#13;
======= RESTART: C:\Python372\sequel\wordcloud\bed_summary.py =======&#13;
&#13;
Summary of Make Your Bed speech:&#13;
It's just the way life is sometimes.If you want to change the world get over&#13;
being a sugar cookie and keep moving forward.Every day during training you&#13;
were challenged with multiple physical events — long runs, long swims,&#13;
obstacle courses, hours of calisthenics — something designed to test your&#13;
mettle. If you can't do the little things right, you will never do the big&#13;
things right.And, if by chance you have a miserable day, you will come home to&#13;
a bed that is made — that you made — and a made bed gives you encouragement&#13;
that tomorrow will be better.If you want to change the world, start off by&#13;
making your bed.During SEAL training the students are broken down into boat&#13;
crews. Basic SEAL training is six months of long torturous runs in the soft&#13;
sand, midnight swims in the cold water off San Diego, obstacle courses,&#13;
unending calisthenics, days without sleep and always being cold, wet and&#13;
miserable.</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_64"/>If you take the time to read the full speech, you’ll probably conclude that <span class="literal">gensim</span> produced a fair summary. Although these two results are different, both extracted the key points of the speech, including the reference to making your bed. Given the size of the document, I find this impressive.</p>&#13;
<p class="indent">Next up, we’ll look at a different way of summarizing text using keywords and word clouds.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec20"><strong>Project #5: Summarizing Text with Word Clouds</strong></h3>&#13;
<p class="noindent">A <em>word cloud</em> is a visual representation of text data used to display keyword metadata, called <em>tags</em> on websites. In a word cloud, font size or color shows the importance of each tag or word.</p>&#13;
<p class="indent">Word clouds are useful for highlighting keywords in a document. For example, generating word clouds for each US president’s State of the Union address can provide a quick overview of the issues facing the nation that year. In Bill Clinton’s first year, the emphasis was on peacetime concerns like healthcare, jobs, and taxes (<a href="ch03.xhtml#ch03fig1">Figure 3-1</a>).</p>&#13;
<div class="image"><img src="../images/fig03_01.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig1"/>Figure 3-1: Word cloud made from 1993 State of the Union address by Bill Clinton</p>&#13;
<p class="indent">Less than 10 years later, George W. Bush’s word cloud reveals a focus on security (<a href="ch03.xhtml#ch03fig2">Figure 3-2</a>).</p>&#13;
<div class="image"><img src="../images/fig03_02.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig2"/>Figure 3-2: Word cloud made from 2002 State of the Union address by George W. Bush</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_65"/>Another use for word clouds is to extract keywords from customer feedback. If words like <em>poor</em>, <em>slow</em>, and <em>expensive</em> dominate, you’ve got a problem! Writers can also use the clouds to compare chapters in a book or scenes in a screenplay. If the author is using very similar language for action scenes and romantic interludes, some editing is needed. If you’re a copywriter, clouds can help you check your keyword density for search engine optimization (SEO).</p>&#13;
<p class="indent">There are lots of ways to generate word clouds, including free websites like <em><a href="https://www.wordclouds.com/">https://www.wordclouds.com/</a></em> and <em><a href="https://www.jasondavies.com/wordcloud/">https://www.jasondavies.com/wordcloud/</a></em>. But if you want to fully customize your word cloud or embed the generator within another program, you need to do it yourself. In this project, you’ll use a word cloud to make a promotional flyer for a school play based on the Sherlock Holmes story <em>The Hound of the Baskervilles</em>.</p>&#13;
<p class="indent">Instead of using the basic rectangle shown in <a href="ch03.xhtml#ch03fig1">Figures 3-1</a> and <a href="ch03.xhtml#ch03fig2">3-2</a>, you’ll fit the words into an outline of Holmes’s head (<a href="ch03.xhtml#ch03fig3">Figure 3-3</a>).</p>&#13;
<div class="image"><img src="../images/fig03_03.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig3"/>Figure 3-3: Silhouette of Sherlock Holmes</p>&#13;
<p class="indent">This will make for a more recognizable and eye-catching display.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Use the <span class="literal">wordcloud</span> module to generate a shaped word cloud for a novel.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec18"><strong><em>The Word Cloud and PIL Modules</em></strong></h4>&#13;
<p class="noindent">You’ll use a module called <span class="literal">wordcloud</span> to generate the word cloud. You can install it using pip.</p>&#13;
<pre>pip install wordcloud</pre>&#13;
<p class="indent">Or, if you’re using Anaconda, use the following command:</p>&#13;
<pre>conda install -c conda-forge wordcloud</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_66"/>You can find the web page for <span class="literal">wordcloud</span> here: <em><a href="http://amueller.github.io/word_cloud/">http://amueller.github.io/word_cloud/</a>.</em></p>&#13;
<p class="indent">You’ll also need the Python Imaging Library (PIL) to work with images. Use pip again to install it.</p>&#13;
<pre>pip install pillow</pre>&#13;
<p class="indent">Or, for Anaconda, use this:</p>&#13;
<pre>conda install -c anaconda pillow</pre>&#13;
<p class="indent">In case you’re wondering, <span class="literal">pillow</span> is the successor project of PIL, which was discontinued in 2011. To learn more about it, visit <em><a href="https://pillow.readthedocs.io/en/stable/">https://pillow.readthedocs.io/en/stable/</a></em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec19"><strong><em>The Word Cloud Code</em></strong></h4>&#13;
<p class="noindent">To make the shaped word cloud, you’ll need an image file and a text file. The image shown in <a href="ch03.xhtml#ch03fig3">Figure 3-3</a> came from iStock by Getty Images (<em><a href="https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/">https://www.istockphoto.com/vector/detective-hat-gm698950970-129478957/</a></em>). This represents the “small” resolution at around 500×600 pixels.</p>&#13;
<p class="indent">A similar but copyright-free image (<em>holmes.png</em>) is provided with the book’s downloadable files. You can find the text file (<em>hound.txt</em>), image file (<em>holmes.png</em>), and code (<em>wc_hound.py</em>) in the <em>Chapter_3</em>  folder.</p>&#13;
<h5 class="h5"><strong>Importing Modules, Text Files, Image Files, and Stop Words</strong></h5>&#13;
<p class="noindent"><a href="ch03.xhtml#ch03list8">Listing 3-8</a> imports modules, loads the novel, loads the silhouette image of Holmes, and creates a set of stop words you’ll want to exclude from the cloud.</p>&#13;
<pre><span class="codeitalic1">wc_hound.py</span><span class="normal">, part 1</span>&#13;
   import numpy as np&#13;
   from PIL import Image&#13;
   import matplotlib.pyplot as plt&#13;
   from wordcloud import WordCloud, STOPWORDS&#13;
  &#13;
   # Load a text file as a string.&#13;
<span class="ent">➊</span> with open('hound.txt') as infile:&#13;
      text = infile.read()&#13;
&#13;
   # Load an image as a NumPy array.&#13;
   mask = np.array(Image.open('holmes.png'))&#13;
&#13;
   # Get stop words as a set and add extra words.&#13;
   stopwords = STOPWORDS&#13;
<span class="ent">➋</span> stopwords.update(['us', 'one', 'will', 'said', 'now', 'well', 'man', 'may',&#13;
                     'little', 'say', 'must', 'way', 'long', 'yet', 'mean',&#13;
                     'put', 'seem', 'asked', 'made', 'half', 'much',&#13;
                     'certainly', 'might', 'came'])</pre>&#13;
<p class="listing"><a id="ch03list8"/>Listing 3-8: Importing modules and loading text, image, and stop words</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_67"/>Begin by importing <span class="literal">NumPy</span> and PIL. PIL will open the image, and <span class="literal">NumPy</span> will turn it into a mask. You started using <span class="literal">NumPy</span> in <a href="ch01.xhtml">Chapter 1</a>; in case you skipped it, see the “Installing the Python Libraries” section on <a href="ch01.xhtml#page_6">page 6</a>. Note that the <span class="literal">pillow</span> module continues to use the acronym PIL for backward compatibility.</p>&#13;
<p class="indent">You’ll need <span class="literal">matplotlib</span>, which you downloaded in the “Installing the Python Libraries” section of <a href="ch01.xhtml">Chapter 1</a>, to display the word cloud. The <span class="literal">wordcloud</span> module comes with its own list of stop words, so import <span class="literal">STOPWORDS</span> along with the cloud functionality.</p>&#13;
<p class="indent">Next, load the novel’s text file and store it in a variable named <span class="literal">text</span> <span class="ent">➊</span>. As described in the discussion of <a href="ch02.xhtml#ch02list2">Listing 2-2</a> in <a href="ch02.xhtml">Chapter 2</a>, you may encounter a <span class="literal">UnicodeDecodeError</span> when loading the text.</p>&#13;
<pre>UnicodeDecodeError: 'ascii' codec can't decode byte 0x93 in position 365:&#13;
ordinal not in range(128)</pre>&#13;
<p class="indent">In this case, try modifying the <span class="literal">open()</span> function by adding <span class="literal">encoding</span> and <span class="literal">errors</span> arguments.</p>&#13;
<pre>    with open('hound.txt', encoding='utf-8', errors='ignore') as infile:</pre>&#13;
<p class="indent">With the text loaded, use PIL’s <span class="literal">Image.open()</span> method to open the image of Holmes and use <span class="literal">NumPy</span> to turn it into an array. If you’re using the iStock image of Holmes, change the image’s filename as appropriate.</p>&#13;
<p class="indent">Assign the <span class="literal">STOPWORDS</span> set imported from <span class="literal">wordcloud</span> to the <span class="literal">stopwords</span> variable. Then update the set with a list of additional words that you want to exclude <span class="ent">➋</span>. These will be words like <em>said</em> and <em>now</em> that dominate the word cloud but add no useful content. Determining what they are is an iterative process. You generate the word cloud, remove words that you don’t think contribute, and repeat. You can comment out this line to see the benefit.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>To update a container like <span class="codeitalic">STOPWORDS</span>, you need to know whether it’s a list, dictionary, set, and so on. Python’s built-in <span class="codeitalic">type()</span> function returns the class type of any object passed as an argument. In this case, <span class="codeitalic">print(type(STOPWORDS))</span> yields <span class="codeitalic">&lt;class 'set'&gt;</span>.</em></p>&#13;
</div>&#13;
<h5 class="h5"><strong>Generating the Word Cloud</strong></h5>&#13;
<p class="noindent"><a href="ch03.xhtml#ch03list9">Listing 3-9</a> generates the word cloud and uses the silhouette as a <em>mask</em>, or an image used to hide portions of another image. The process used by <span class="literal">wordcloud</span> is sophisticated enough to fit the words within the mask, rather than simply truncating them at the edges. In addition, numerous parameters are available for changing the appearance of the words within the mask.</p>&#13;
<pre><span class="codeitalic1">wc_hound.py</span><span class="normal">, part 2</span>&#13;
wc = WordCloud(max_words=500,&#13;
               relative_scaling=0.5,&#13;
               mask=mask,&#13;
               background_color='white',&#13;
               stopwords=stopwords,&#13;
               margin=2,&#13;
               random_state=7,&#13;
               <span epub:type="pagebreak" id="page_68"/>contour_width=2,&#13;
               contour_color='brown',&#13;
               colormap='copper').generate(text)&#13;
&#13;
colors = wc.to_array()</pre>&#13;
<p class="listing"><a id="ch03list9"/>Listing 3-9: Generating the word cloud</p>&#13;
<p class="indent">Name a variable <span class="literal">wc</span> and call <span class="literal">WordCloud()</span>. There are a lot of parameters, so I’ve placed each on its own line for clarity. For a list and description of all the parameters available, visit <em><a href="https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html">https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html</a></em>.</p>&#13;
<p class="indent">Start by passing the maximum number of words you want to use. The number you set will display the <em>n</em> most common words in the text. The more words you choose to display, the easier it will be to define the edges of the mask and make it recognizable. Unfortunately, setting the maximum number too high will also result in a lot of tiny, illegible words. For this project, start with 500.</p>&#13;
<p class="indent">Next, to control the font size and relative importance of each word, set the <span class="literal">relative_scaling</span> parameter to <span class="literal">0.5</span>. For example, a value of <span class="literal">0</span> gives preference to a word’s rank to determine the font size, while a value of <span class="literal">1</span> means that words that occur twice as often will appear twice as large. Values between <span class="literal">0</span> and <span class="literal">0.5</span> tend to strike the best balance between rank and frequency.</p>&#13;
<p class="indent">Reference the <span class="literal">mask</span> variable and set its background color to <span class="literal">white</span>. Assigning no color defaults to black. Then reference the <span class="literal">stopwords</span> set that you edited in the previous listing.</p>&#13;
<p class="indent">The <span class="literal">margin</span> parameter will control the spacing of the displayed words. Using <span class="literal">0</span> will result in tightly packed words. Using <span class="literal">2</span> will allow for some whitespace padding.</p>&#13;
<p class="indent">To place the words around the word cloud, use a random number generator and set <span class="literal">random_state</span> to <span class="literal">7</span>. There’s nothing special about this value; I just felt that it produced an attractive arrangement of words.</p>&#13;
<p class="indent">The <span class="literal">random_state</span> parameter fixes the seed number so that the results are repeatable, assuming no other parameters are changed. This means the words will always be arranged in the same way. Only integers are accepted.</p>&#13;
<p class="indent">Now, set <span class="literal">contour_width</span> to <span class="literal">2</span>. Any value greater than zero creates an outline around a mask. In this case, the outline is squiggly due to the resolution of the image (<a href="ch03.xhtml#ch03fig4">Figure 3-4</a>).</p>&#13;
<p class="indent">Set the color of the outline to <span class="literal">brown</span> using the <span class="literal">contour_color</span> parameter. Continue using a brownish palette by setting <span class="literal">colormap</span> to <span class="literal">copper</span>. In <span class="literal">matplotlib</span>, a <em>colormap</em> is a dictionary that maps numbers to colors. The <span class="literal">copper</span> colormap produces text ranging in color from pale flesh to black. You can see its spectrum, along with many other color options, at <em><a href="https://matplotlib.org/gallery/color/colormap_reference.html">https://matplotlib.org/gallery/color/colormap_reference.html</a></em>. If you don’t specify a colormap, the program will use the default colors.</p>&#13;
<p class="indent">Use dot notation to call the <span class="literal">generate()</span> method to build the word cloud. Pass it the <span class="literal">text</span> string as an argument. End this listing by naming a <span class="literal">colors</span> variable and calling the <span class="literal">to_array()</span> method on the <span class="literal">wc</span> object. This method converts the word cloud image into a <span class="literal">NumPy</span> array for use with <span class="literal">matplotlib</span>.</p>&#13;
<div class="image"><img src="../images/fig03_04.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig4"/>Figure 3-4: Example of masked word cloud with an outline (left) versus without (right)</p>&#13;
<h5 class="h5"><strong>Plotting the Word Cloud</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_69"/><a href="ch03.xhtml#ch03list10">Listing 3-10</a> adds a title to the word cloud and uses <span class="literal">matplotlib</span> to display it. It also saves the word cloud image as a file.</p>&#13;
<pre><span class="codeitalic1">wc_hound.py</span><span class="normal">, part 3</span>&#13;
plt.figure()&#13;
plt.title("Chamberlain Hunt Academy Senior Class Presents:\n",&#13;
          fontsize=15, color='brown')&#13;
plt.text(-10, 0, "The Hound of the Baskervilles",&#13;
         fontsize=20, fontweight='bold', color='brown')&#13;
plt.suptitle("7:00 pm May 10-12 McComb Auditorium",&#13;
             x=0.52, y=0.095, fontsize=15, color='brown')&#13;
plt.imshow(colors, interpolation="bilinear")&#13;
plt.axis('off')&#13;
plt.show()&#13;
##plt.savefig('hound_wordcloud.png')</pre>&#13;
<p class="listing"><a id="ch03list10"/>Listing 3-10: Plotting and saving the word cloud</p>&#13;
<p class="indent">Start by initializing a <span class="literal">matplotlib</span> figure. Then call the <span class="literal">title()</span> method and pass it the name of the school, along with a font size and color.</p>&#13;
<p class="indent">You’ll want the name of the play to be bigger and bolder than the other titles. Since you can’t change the text style within a string with <span class="literal">matplotlib</span>, use the <span class="literal">text()</span> method to define a new title. Pass it (<em>x</em>, <em>y</em>) coordinates (based on the figure axes), a text string, and text style details. Use trial and error with the coordinates to optimize the placement of the text. If you’re using the iStock image of Holmes, you may need to change the <em>x</em> coordinate from <span class="literal">-10</span> to something else to achieve the best balance with the asymmetrical silhouette.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_70"/>Finish the titles by placing the play’s time and venue at the bottom of the figure. You could use the <span class="literal">text()</span> method again, but instead, let’s take a look at an alternative, <span class="literal">pyplot</span>’s <span class="literal">suptitle()</span> method. The name stands for “super titles.” Pass it the text, the (<em>x</em>, <em>y</em>) figure coordinates, and styling details.</p>&#13;
<p class="indent">To display the word cloud, call <span class="literal">imshow()</span>—for image show—and pass it the <span class="literal">colors</span> array you made previously. Specify <span class="literal">bilinear</span> for color interpolation.</p>&#13;
<p class="indent">Turn off the figure axes and display the word cloud by calling <span class="literal">show()</span>. If you want to save the figure, uncomment the <span class="literal">savefig()</span> method. Note that <span class="literal">matplotlib</span> can read the extension in the filename and save the figure in the correct format. As written, the save command will not execute until you manually close the figure.</p>&#13;
<h4 class="h4" id="ch00lev2sec20"><strong><em>Fine-Tuning the Word Cloud</em></strong></h4>&#13;
<p class="noindent"><a href="ch03.xhtml#ch03list10">Listing 3-10</a> will produce the word cloud in <a href="ch03.xhtml#ch03fig5">Figure 3-5</a>. You may get a different arrangement of words as the algorithm is stochastic.</p>&#13;
<div class="image"><img src="../images/fig03_05.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig5"/>Figure 3-5: The flyer generated by the <span class="normal">wc_hound.py</span> code</p>&#13;
<p class="indent">You can change the size of the display by adding an argument when you initialize the figure. Here’s an example: <span class="literal">plt.figure(figsize=(50, 60))</span>.</p>&#13;
<p class="indent">There are many other ways to change the results. For example, setting the <span class="literal">margin</span> parameter to <span class="literal">10</span> yields a sparser word cloud (<a href="ch03.xhtml#ch03fig6">Figure 3-6</a>).</p>&#13;
<div class="image"><img src="../images/fig03_06.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig6"/>Figure 3-6: The word cloud generated with <span class="literal">margin=10</span></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_71"/>Changing the <span class="literal">random_state</span> parameter will also rearrange the words within the mask (<a href="ch03.xhtml#ch03fig7">Figure 3-7</a>).</p>&#13;
<div class="image"><img src="../images/fig03_07.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig7"/>Figure 3-7: The word cloud generated with <span class="literal">margin=10</span> and <span class="literal">random_state=6</span></p>&#13;
<p class="indent">Tweaking the <span class="literal">max_words</span> and <span class="literal">relative_scaling</span> parameters will also change the appearance of the word cloud. Depending on how detail-oriented you are, all this can be a blessing or a curse!</p>&#13;
<h3 class="h3" id="ch00lev1sec21"><strong>Summary</strong></h3>&#13;
<p class="noindent">In this chapter, you used extraction-based summarization techniques to produce a synopsis of Martin Luther King Jr.’s “I Have a Dream” speech. You then used a free, off-the-shelf module called <span class="literal">gensim</span> to summarize Admiral McRaven’s “Make Your Bed” speech with even less code. Finally, you used the <span class="literal">wordcloud</span> module to create an interesting design with words.</p>&#13;
<h3 class="h3" id="ch00lev1sec22"><strong>Further Reading</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_72"/><em>Automate the Boring Stuff with Python: Practical Programming for Total Beginners</em> (No Starch Press, 2015), by Al Sweigart, covers regular expressions in <a href="ch07.xhtml">Chapter 7</a> and web scraping in <a href="ch11.xhtml">Chapter 11</a>, including use of the <span class="literal">requests</span> and Beautiful Soup modules.</p>&#13;
<p class="indent"><em>Make Your Bed: Little Things That Can Change Your Life</em>…<em>And Maybe the World</em>, 2nd ed. (Grand Central Publishing, 2017), by William H. McRaven, is a self-help book based on the admiral’s commencement address at the University of Texas. You can find the actual speech online on <em><a href="https://www.youtube.com/">https://www.youtube.com/</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev1sec23"><strong>Challenge Project: Game Night</strong></h3>&#13;
<p class="noindent">Use <span class="literal">wordcloud</span> to invent a new game for game night. Summarize Wikipedia or IMDb synopses of movies and see whether your friends can guess the movie title. <a href="ch03.xhtml#ch03fig8">Figure 3-8</a> shows some examples.</p>&#13;
<div class="image"><img src="../images/fig03_08.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig8"/>Figure 3-8: Word clouds for two movies released in 2010: <span class="normal">How to Train Your Dragon</span> and <span class="normal">Prince of Persia</span></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_73"/>If you’re not into movies, pick something else. Alternatives include famous novels, <em>Star Trek</em> episodes, and song lyrics (<a href="ch03.xhtml#ch03fig9">Figure 3-9</a>).</p>&#13;
<div class="image"><img src="../images/fig03_09.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch03fig9"/>Figure 3-9: Word cloud made from song lyrics (Donald Fagen’s “I.G.Y.”)</p>&#13;
<p class="indent">Board games have seen a resurgence in recent years, so you could follow this trend and print the word clouds on card stock. Alternatively, you could keep things digital and present the player with multiple-choice answers for each cloud. The game should keep track of the number of correct answers.</p>&#13;
<h3 class="h3" id="ch00lev1sec24"><strong>Challenge Project: Summarizing Summaries</strong></h3>&#13;
<p class="noindent">Test your program from Project 3 on previously summarized text, such as Wikipedia pages. Only five sentences produced a good overview of <span class="literal">gensim</span>.</p>&#13;
<pre>Enter max words per sentence for summary: 30&#13;
Enter number of sentences for summary: 5&#13;
&#13;
SUMMARY:&#13;
Gensim is implemented in Python and Cython.&#13;
Gensim is an open-source library for unsupervised topic modeling and natural&#13;
language processing, using modern statistical machine learning.&#13;
[12] Gensim is commercially supported by the company rare-technologies.com,&#13;
who also provide student mentorships and academic thesis projects for Gensim&#13;
via their Student Incubator programme.&#13;
The software has been covered in several new articles, podcasts and&#13;
interviews.&#13;
Gensim is designed to handle large text collections using data streaming and&#13;
incremental online algorithms, which differentiates it from most other machine&#13;
learning software packages that target only in-memory processing.</pre>&#13;
<p class="indent">Next, try the <span class="literal">gensim</span> version from Project 4 on those boring services agreements no one ever reads. An example Microsoft agreement is available at <em><a href="https://www.microsoft.com/en-us/servicesagreement/default.aspx">https://www.microsoft.com/en-us/servicesagreement/default.aspx</a></em>. Of course, to evaluate the results, you’ll have to read the full agreement, which almost no one ever does! Enjoy the catch-22!</p>&#13;
<h3 class="h3" id="ch00lev1sec25"><strong>Challenge Project: Summarizing a Novel</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_74"/>Write a program that summarizes <em>The Hound of the Baskervilles</em> by chapter. Keep the chapter summaries short, at around 75 words each.</p>&#13;
<p class="indent">For a copy of the novel with chapter headings, scrape the text off the Project Gutenberg site using the following line of code: <span class="literal">url = 'http://www.gutenberg.org/files/2852/2852-h/2852-h.htm'</span>.</p>&#13;
<p class="indent">To break out chapter elements, rather than paragraph elements, use this code:</p>&#13;
<pre>chapter_elems = soup.select('div[class="chapter"]')&#13;
chapters = chapter_elems[2:]</pre>&#13;
<p class="indent">You’ll also need to select paragraph elements (<span class="literal">p_elems</span>) from within each chapter, using the same methodology as in <em>dream_summary.py</em>.</p>&#13;
<p class="indent">The following snippets show some of the results from using a word count of 75 per chapter:</p>&#13;
<pre><span class="codeitalic1">--snip--</span>&#13;
&#13;
Chapter 3:&#13;
"Besides, besides—" "Why do you hesitate?” "There is a realm in which the most&#13;
acute and most experienced of detectives is helpless." "You mean that the&#13;
thing is supernatural?" "I did not positively say so." "No, but you evidently&#13;
think it." "Since the tragedy, Mr. Holmes, there have come to my ears several&#13;
incidents which are hard to reconcile with the settled order of Nature." "For&#13;
example?" "I find that before the terrible event occurred several people had&#13;
seen a creature upon the moor which corresponds with this Baskerville demon,&#13;
and which could not possibly be any animal known to science.&#13;
&#13;
<span class="codeitalic1">--snip--</span>&#13;
&#13;
Chapter 6:&#13;
"Bear in mind, Sir Henry, one of the phrases in that queer old legend which&#13;
Dr. Mortimer has read to us, and avoid the moor in those hours of darkness&#13;
when the powers of evil are exalted." I looked back at the platform when we&#13;
had left it far behind and saw the tall, austere figure of Holmes standing&#13;
motionless and gazing after us.&#13;
&#13;
Chapter 7:&#13;
I feared that some disaster might occur, for I was very fond of the old man,&#13;
and I knew that his heart was weak." "How did you know that?" "My friend&#13;
Mortimer told me." "You think, then, that some dog pursued Sir Charles, and&#13;
that he died of fright in consequence?" "Have you any better explanation?" "I&#13;
have not come to any conclusion." "Has Mr. Sherlock Holmes?" The words took&#13;
away my breath for an instant but a glance at the placid face and steadfast&#13;
eyes of my companion showed that no surprise was intended.&#13;
&#13;
<span class="codeitalic1">--snip--</span>&#13;
&#13;
<span epub:type="pagebreak" id="page_75"/>Chapter 14:&#13;
"What’s the game now?" "A waiting game." "My word, it does not seem a very&#13;
cheerful place," said the detective with a shiver, glancing round him at the&#13;
gloomy slopes of the hill and at the huge lake of fog which lay over the&#13;
Grimpen Mire.&#13;
&#13;
Far away on the path we saw Sir Henry looking back, his face white in the&#13;
moonlight, his hands raised in horror, glaring helplessly at the frightful&#13;
thing which was hunting him down.&#13;
&#13;
<span class="codeitalic1">--snip--</span></pre>&#13;
<h3 class="h3" id="ch00lev1sec26"><strong>Challenge Project: It’s Not Just What You Say, It’s How You Say It!</strong></h3>&#13;
<p class="noindent">The text summarization programs you have written so far print sentences strictly by their <em>order of importance</em>. That means the last sentence in a speech (or any text) might become the first sentence in the summary. The goal of summarization is to find the important sentences, but there’s no reason you can’t alter the way that they’re displayed.</p>&#13;
<p class="indent">Write a text summarization program that displays the most important sentences in their original <em>order of appearance</em>. Compare the results to those produced by the program in Project 3. Does this make a noticeable improvement in the summaries?</p>&#13;
</body></html>