<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch03"><span epub:type="pagebreak" id="page_41"/><strong><span class="big">3</span><br/>MORE PROBABILITY</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents"><a href="ch02.xhtml#ch02">Chapter 2</a> introduced us to basic concepts of probability. In this chapter, we’ll continue our exploration of probability by focusing on two essential topics often encountered in deep learning and machine learning: probability distributions and how to sample from them, and Bayes’ theorem. Bayes’ theorem is one of the most important concepts in probability theory, and it has produced a paradigm shift in the way many researchers think about probability and how to apply it.</p>&#13;
<h3 class="h3" id="ch03lev1_1">Probability Distributions</h3>&#13;
<p class="noindent">A probability distribution can be thought of as a function that generates values on demand. The values generated are random—we don’t know which one will appear—but the likelihood of any value appearing follows a general form. For example, if we roll a standard die many times and tally how many times each number comes up, we expect that in the long run, each number <span epub:type="pagebreak" id="page_42"/>is equally likely. Indeed, that’s the entire point of making the die in the first place. Therefore, the probability distribution of the die is known as a <em>uniform distribution</em>, since each number is equally likely to appear. We can imagine other distributions favoring one value or range of values over others, like a weighted die that might come up as six suspiciously often.</p>&#13;
<p class="indent">Deep learning’s primary reason for sampling from a probability distribution is to initialize the network before training. Modern networks select the initial weights and sometimes biases from different distributions, most notably uniform and normal. The uniform distribution is familiar to us, and I’ll discuss the normal distribution, a continuous distribution, later.</p>&#13;
<p class="indent">I’ll present several different kinds of probability distributions in this section. Our focus is to understand the shape of the distribution and to learn how to draw samples from it using NumPy. I’ll start with histograms to show you that we can often treat histograms as approximations of a probability distribution. Then I’ll discuss common <em>discrete probability distributions</em>. These are distributions returning integer values, like 3 or 7. Lastly, I’ll switch to continuous distributions yielding floating-point numbers, like 3.8 or 7.592.</p>&#13;
<h4 class="h4" id="ch03lev2_1">Histograms and Probabilities</h4>&#13;
<p class="noindent">Take a look at <a href="ch03.xhtml#ch03tab01">Table 3-1</a>, which we saw in <a href="ch02.xhtml#ch02">Chapter 2</a>.</p>&#13;
<p class="tabcap" id="ch03tab01"><strong>Table 3-1:</strong> The Number of Combinations of Two Dice Leading to Different Sums (Copied from <a href="ch02.xhtml#ch02tab01">Table 2-1</a>)</p>&#13;
<table class="borderta">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Sum</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Combinations</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Count</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Probability</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0278</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 + 2, 2 + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0556</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 + 3, 2 + 2, 3 + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0833</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 + 4, 2 + 3, 3 + 2, 4 + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1111</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 + 5, 2 + 4, 3 + 3, 4 + 2, 5 + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1389</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 + 6, 2 + 5, 3 + 4, 4 + 3, 5 + 2, 6 + 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1667</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2 + 6, 3 + 5, 4 + 4, 5 + 3, 6 + 2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1389</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3 + 6, 4 + 5, 5 + 4, 6 + 3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1111</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4 + 6, 5 + 5, 6 + 4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0833</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5 + 6, 6 + 5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0556</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td class="borderb" style="vertical-align: top"><p class="tab">12</p></td>&#13;
<td class="borderb" style="vertical-align: top"><p class="tab">6 + 6</p></td>&#13;
<td class="borderb" style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td class="borderb" style="vertical-align: top"><p class="tab">0.0278</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab"/></td>&#13;
<td style="vertical-align: top"><p class="tab">36</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0000</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">It shows how two dice add to different sums. Don’t look at the actual values; look at the shape the possible combinations make. If we chop off the last two columns, turn the table to the left, and replace each sum with an “X,” we should see something like the following.</p>&#13;
<span epub:type="pagebreak" id="page_43"/>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td>×</td>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td/>&#13;
<td/>&#13;
<td/>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td/>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td/>&#13;
<td/>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td/>&#13;
</tr>&#13;
<tr class="borderb">&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
<td>×</td>&#13;
</tr>&#13;
<tr>&#13;
<td>2</td>&#13;
<td>3</td>&#13;
<td>4</td>&#13;
<td>5</td>&#13;
<td>6</td>&#13;
<td>7</td>&#13;
<td>8</td>&#13;
<td>9</td>&#13;
<td>10</td>&#13;
<td>11</td>&#13;
<td>12</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">You can see that there’s a definite shape and symmetry to the number of ways to arrive at each sum. This kind of plot is called a <em>histogram</em>. A histogram is a plot tallying the number of things that fall into discrete bins. For <a href="ch03.xhtml#ch03tab01">Table 3-1</a>, the bins are the numbers 2 through 12. The tally is a possible way to get that sum. Histograms are often represented as bar graphs, usually vertical bars, though they need not be. <a href="ch03.xhtml#ch03tab01">Table 3-1</a> is basically a horizontal histogram. How many bins are used in the histogram is up to the maker. If you use too few, the histogram will be blocky and may not reveal necessary detail because interesting features have all been lumped into the same bin. Use too many bins, and the histogram will be sparse, with many bins having no tallies.</p>&#13;
<p class="indent">Let’s generate some histograms. First, we’ll randomly sample integers in [0,9] and count how many of each integer we get. The code for this is straightforward:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> import numpy as np</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> n = np.random.randint(0,10,10000)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> h = np.bincount(n)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> h</span><br/>&#13;
array([ 975, 987, 987, 1017, 981, 1043, 1031, 988, 1007, 984])</pre>&#13;
<p class="indent">We first set <code>n</code> to an array of 10,000 integers in [0, 9]. We then use <code>np .bincount</code> to count how many of each digit we have. We see that this run gave us 975 zeros and 984 nines. If the NumPy pseudorandom generator is doing its job, we expect, on average, to have 1,000 of each digit in a sample of 10,000 digits. We expect some variation, but most values are close enough to 1,000 to be convincing.</p>&#13;
<p class="indent">The counts above tell us how many times each digit appeared. If we divide each bin of a histogram by the total of all the bins, we change from simple counts to the probability of that bin appearing. For the random digits above, we get the probabilities with</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> h = h / h.sum()</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> h</span><br/>&#13;
array([0.0975, 0.0987, 0.0987, 0.1017, 0.0981, 0.1043, 0.1031, 0.0988,<br/>&#13;
       0.1007, 0.0984])</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_44"/>which tells us that each digit did appear with a probability of about 0.1, or 1 out of 10. This trick of dividing histogram values by the sum of the counts in the histogram allows us to estimate probability distributions from samples. It also tells us the likelihood of particular values appearing when sampling from whatever process generated the data used to make the histogram. You should note that I said we could <em>estimate</em> the probability distribution from a set of samples drawn from it. The larger the number of samples, the closer the estimated probability distribution will be to the actual population distribution generating the samples. We will never get to the actual population distribution, but given the limit of an infinite number of samples, we can get as close as we need to.</p>&#13;
<p class="indent">Histograms are frequently used to look at the distribution of pixel values in an image. Let’s make a plot of the histogram of the pixels in two images. You can find the code in the file <code>ricky.py</code>. (I won’t show it here, as it doesn’t add to the discussion.) The images used are two example grayscale images included with SciPy in <code>scipy.misc</code>. The first shows people walking up stairs (<code>ascent</code>), and the second is the face of a young raccoon (<code>face</code>), as shown in <a href="ch03.xhtml#ch03fig01">Figure 3-1</a>.</p>&#13;
<div class="image" id="ch03fig01"><img src="Images/03fig01.jpg" alt="image" width="669" height="322"/></div>&#13;
<p class="figcap"><em>Figure 3-1: People ascending (left) and “Ricky” raccoon (right)</em></p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03fig02">Figure 3-2</a> provides a plot of the histograms for each image, as probabilities. It shows two very different distributions of gray level values in the images. For the raccoon face, the distribution is more spread out and flatter, while the ascent image has a spike right around gray level 128 and a few bright pixels. The distributions tell us that if we pick a random pixel in the face image, we’re most likely to get one around gray level 100, but an arbitrary pixel in the ascent image will, with high relative likelihood, be closer to gray level 128.</p>&#13;
<span epub:type="pagebreak" id="page_45"/>&#13;
<div class="image" id="ch03fig02"><img src="Images/03fig02.jpg" alt="image" width="680" height="511"/></div>&#13;
<p class="figcap"><em>Figure 3-2: Histograms as probabilities for two 512×512-pixel grayscale sample images</em></p>&#13;
<p class="indent">Again, histograms tally the counts of how many items fall into the predefined bins. We saw for the images that the histogram as probability distribution tells us how likely we are to get a particular gray level value if we select a random pixel. Likewise, the probability distribution for the random digits in the example before that tells us the probability of getting each digit when we ask for a random integer in the range [0,9].</p>&#13;
<p class="indent">Histograms are discrete representations of a probability distribution. Let’s take a look at the more common discrete distributions now.</p>&#13;
<h4 class="h4" id="ch03lev2_2">Discrete Probability Distributions</h4>&#13;
<p class="noindent">We’ve already encountered the most common discrete distribution several times: it’s the uniform distribution. That’s the one we get naturally by rolling dice or flipping coins. In the uniform distribution, all possible outcomes are equally likely. A histogram of a simulation of a process drawing from a uniform distribution is flat; all outcomes show up with more or less the same frequency. We’ll see the uniform distribution again when we look at continuous distributions. For now, think dice.</p>&#13;
<p class="indent">Let’s look at a few other discrete distributions.</p>&#13;
<h5 class="h5" id="ch03lev3_1"><span epub:type="pagebreak" id="page_46"/>The Binomial Distribution</h5>&#13;
<p class="noindent">Perhaps the second most common discrete distribution is the <em>binomial distribution</em>. This distribution represents the expected number of events happening in a given number of trials if each event has a specified probability. Mathematically, the probability of <em>k</em> events happening in <em>n</em> trials if the probability of the event happening is <em>p</em> can be written as</p>&#13;
<div class="imagec"><img src="Images/046equ01.jpg" alt="image" width="248" height="38"/></div>&#13;
<p class="indent">For example, what’s the probability of getting three heads in a row when flipping a fair coin three times? From the product rule, we know the probability is</p>&#13;
<div class="imagec"><img src="Images/046equ02.jpg" alt="image" width="359" height="45"/></div>&#13;
<p class="noindent">Using the binomial formula, we get the same answer by calculating</p>&#13;
<div class="imagec"><img src="Images/046equ03.jpg" alt="image" width="371" height="43"/></div>&#13;
<p class="indent">So far, not particularly helpful. However, what if the probability of the event isn’t 0.5? What if we have an event, say the likelihood of a person winning <em>Let’s Make a Deal</em> by not changing doors, and we want to know the probability that 7 people out of 13 will win by not changing their guess? We know the probability of winning the game without changing doors is 1/3—that’s <em>p</em>. We then have 13 trials (<em>n</em>) and 7 winners (<em>k</em>). The binomial formula tells us the likelihood is</p>&#13;
<div class="imagec"><img src="Images/046equ04.jpg" alt="image" width="399" height="51"/></div>&#13;
<p class="noindent">and, if the players <em>do</em> switch doors,</p>&#13;
<div class="imagec"><img src="Images/046equ05.jpg" alt="image" width="400" height="51"/></div>&#13;
<p class="indent">The binomial formula gives us the probability of a given number of events in a given number of trials for a specified probability per event. If we fix <em>n</em> and <em>p</em> and vary <em>k</em>, 0 ≤ <em>k</em> ≤ <em>n</em>, we get the probability for each <em>k</em> value. <span epub:type="pagebreak" id="page_47"/>This gives us the distribution. For example, let <em>n</em> = 5 and <em>p</em> = 0.3, then 0 ≤ <em>k</em> ≤ 5 with the probability for each <em>k</em> value as</p>&#13;
<div class="imagec"><img src="Images/047equ01.jpg" alt="image" width="384" height="452"/></div>&#13;
<p class="noindent">Allowing for rounding, this sums to 1.0, as we know it must because the sum of probabilities over an entire sample space is always 1.0. Notice that we calculate all the possible values for the binomial distribution when <em>n</em> = 5. Collectively, this specifies the <em>probability mass function (pmf)</em>. The probability mass function tells us the probability associated with all possible outcomes.</p>&#13;
<p class="indent">The binomial distribution is parameterized by <em>n</em> and <em>p</em>. For <em>n</em> = 5 and = <em>p</em> = 0.3, we see from the results above that a random sample from such a binomial distribution will return 1 most often—some 36 percent of the time. How can we draw samples from a binomial distribution? In NumPy, we need only call the <code>binomial</code> function in the <code>random</code> module:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> t = np.random.binomial(5, 0.3, size=1000)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s = np.bincount(t)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s</span><br/>&#13;
array([159, 368, 299, 155,  17,   2])<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s / s.sum()</span><br/>&#13;
array([0.159, 0.368, 0.299, 0.155, 0.017, 0.002])</pre>&#13;
<p class="indent">We pass <code>binomial</code> the number of trials (<code>5</code>) and the probability of success for each trial (<code>0.3</code>). We then ask for 1,000 samples from a binomial distribution with these parameters. Using <code>np.bincount</code>, we see that the most commonly returned value was indeed 1, as we calculated above. By using our histogram summation trick, we get a probability of 0.368 for selecting a 1—close to the 0.3601 we calculated.</p>&#13;
<h5 class="h5" id="ch03lev3_2"><span epub:type="pagebreak" id="page_48"/>The Bernoulli Distribution</h5>&#13;
<p class="noindent">The <em>Bernoulli distribution</em> is a special case of the binomial distribution. In this case, we fix <em>n</em> = 1, meaning there’s only one trial. The only values we can sample are 0 or 1; either the event happens, or it doesn’t. For example, with <em>p</em> = 0.5, we get</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> t = np.random.binomial(1, 0.5, size=1000)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> np.bincount(t)</span><br/>&#13;
array([496, 504])</pre>&#13;
<p class="noindent">This is reasonable, since a probability of 0.5 means we’re flipping a fair coin, and we see that the proportion of heads or tails is roughly equal.</p>&#13;
<p class="indent">If we change to <em>p</em> = 0.3, we get</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> t = np.random.binomial(1, 0.3, size=1000)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> np.bincount(t)</span><br/>&#13;
array([665, 335])<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> 335/1000</span><br/>&#13;
0.335</pre>&#13;
<p class="noindent">Again, close to 0.3, as we expect to see.</p>&#13;
<p class="indent">Use samples from a binomial distribution when you want to simulate events with a known probability. With the Bernoulli form, we can sample binary outcomes, 0 or 1, where the likelihood of the event need not be that of the flip of a fair coin, 0.5.</p>&#13;
<h5 class="h5" id="ch03lev3_3">The Poisson Distribution</h5>&#13;
<p class="noindent">Sometimes, we don’t know the probability of an event happening for any particular trial. Instead, we might know the average number of events that happen over some interval, say of time. If the average number of events that happen over some time is λ (lambda), then the probability of <em>k</em> events happening in that interval is</p>&#13;
<div class="imagec"><img src="Images/048equ01.jpg" alt="image" width="119" height="47"/></div>&#13;
<p class="indent">This is the <em>Poisson distribution</em>, and it’s useful to model events like radioactive decay or the incidence of photons on an X-ray detector over some period of time. To sample events according to this distribution, we use <code>poisson</code> from the <code>random</code> module. For example, assume over some time interval there are five events on average (λ = 5). What sort of probability distribution do we get using the Poisson distribution? In code,</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> t = np.random.poisson(5, size=1000)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s = np.bincount(t)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s</span><br/>&#13;
array([  6,  36,  83, 135, 179, 173, 156, 107,  58,  40,  20,   4,   2,<br/>&#13;
         0,   0,   1])<br/>&#13;
<span epub:type="pagebreak" id="page_49"/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> t.max()</span><br/>&#13;
15<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s = s / s.sum()</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> s</span><br/>&#13;
array([0.006, 0.036, 0.083, 0.135, 0.179, 0.173, 0.156, 0.107, 0.058,<br/>&#13;
       0.04 , 0.02 , 0.004, 0.002, 0.   , 0.   , 0.001])</pre>&#13;
<p class="noindent">Here, we see that, unlike the binomial distribution, which could not select more than <em>n</em> events, the Poisson distribution can select numbers of events that exceed the value of λ. In this case, the largest number of events in the time interval was 15, which is three times the average. You’ll find that the most frequent number of events is right around the average of five, as you might expect, but significant deviations from the average are possible.</p>&#13;
<h5 class="h5" id="ch03lev3_4">The Fast Loaded Dice Roller</h5>&#13;
<p class="noindent">What if we need to draw samples according to an arbitrary discrete distribution? Earlier, we saw some histograms based on images. In that case, we could sample from the distribution represented by the histogram by picking pixels in the image at random. But what if we wanted to sample integers according to arbitrary weights? To do this, we can use the new Fast Loaded Dice Roller of Saad, et al.<a id="ch03fn01a" href="#ch03fn01"><sup>1</sup></a></p>&#13;
<p class="indent">The <em>Fast Loaded Dice Roller (FLDR)</em> lets us specify an arbitrary discrete distribution and then draw samples from it. The code is in Python and freely available. (See <em><a href="https://github.com/probcomp/fast-loaded-dice-roller/">https://github.com/probcomp/fast-loaded-dice-roller/</a></em>.) I’ll show how to use the code to sample according to a generic distribution. I recommend downloading just the <code>fldr.py</code> and <code>fldrf.py</code> files from the GitHub repository instead of running <code>setup.py</code>. Additionally, edit the <code>.fldr</code> import lines in <code>fldrf.py</code> to remove the “<code>.</code>” so they read</p>&#13;
<pre>&#13;
from fldr import fldr_preprocess_int<br/>&#13;
from fldr import fldr_s&#13;
</pre>&#13;
<p class="indent">Using FLDR requires two steps. The first is to tell it the particular distribution you want to sample from. You define the distribution as ratios. (For our purposes, we’ll use actual probabilities, meaning our distribution will always add up to 1.0.) This is the preprocessing step, which we only need to do once for each distribution. After that, we can draw samples. An example will clarify:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> from fldrf import fldr_preprocess_float_c</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> from fldr import fldr_sample</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> x = fldr_preprocess_float_c([0.6,0.2,0.1,0.1])</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> t = [fldr_sample(x) for i in range(1000)]</span><br/>&#13;
<span epub:type="pagebreak" id="page_50"/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> np.bincount(t)</span><br/>&#13;
array([598, 190, 108, 104])</pre>&#13;
<p class="indent">First, we import the two FLDR functions we need: <code>fldr_preprocess_float _c</code> and <code>fldr_sample</code>. Then we define the distribution using a list of four numbers. Four numbers imply samples will be integers in [0, 3]. However, unlike a uniform distribution, here we’re specifying we want zero 60 percent of the time, one 20 percent of the time, and two and three 10 percent of the time each. The information that FLDR needs to sample from the distribution is returned in <code>x</code>.</p>&#13;
<p class="indent">Calling <code>fldr_sample</code> returns a single sample from the distribution. Notice two things: first, we need to pass <code>x</code> in, and second, FLDR doesn’t use NumPy, so to draw 1,000 samples, we use a standard Python list comprehension. The 1,000 samples are in the list, <code>t</code>. Finally, we generate the histogram and see that nearly 60 percent of the samples are zero and slightly more than 10 percent are three, as we intended.</p>&#13;
<p class="indent">Let’s use the histogram of the raccoon face image we used earlier to see if FLDR will follow a more complex distribution. We’ll load the image, generate the histogram, convert it to a probability distribution, and use the probabilities to set up FLDR. After that, we’ll draw 25,000 samples from the distribution, compute the histogram of the samples, and plot that histogram along with the original histogram to see if FLDR follows the actual distribution we give it. The code we need is</p>&#13;
<pre>from scipy.misc import face<br/>&#13;
im = face(True)<br/>&#13;
b = np.bincount(im.ravel(), minlength=256)<br/>&#13;
b = b / b.sum()<br/>&#13;
x = fldr_preprocess_float_c(list(b))<br/>&#13;
t = [fldr_sample(x) for i in range(25000)]<br/>&#13;
q = np.bincount(t, minlength=256)<br/>&#13;
q = q / q.sum()</pre>&#13;
<p class="indent">Running this code leaves us with <code>b</code>, a probability distribution from the histogram of the face image, and <code>q</code>, the distribution created from 25,000 samples from the FLDR distribution. <a href="ch03.xhtml#ch03fig03">Figure 3-3</a> shows us a plot of the two distributions.</p>&#13;
<p class="indent">The solid line in <a href="ch03.xhtml#ch03fig03">Figure 3-3</a> is the probability distribution we supplied to <code>fldr_preprocess_float_c</code> representing the distribution of gray levels (intensities) in the raccoon image. The dashed line is the histogram of the 25,000 samples from this distribution. As we can see, they follow the requested distribution with the sort of variation we expect from such a small number of samples. As an exercise, change the number of samples from 25,000 to 500,000 and plot the two curves. You’ll see that they’re now virtually on top of each other.</p>&#13;
<span epub:type="pagebreak" id="page_51"/>&#13;
<div class="image" id="ch03fig03"><img src="Images/03fig03.jpg" alt="image" width="680" height="510"/></div>&#13;
<p class="figcap"><em>Figure 3-3: Comparing the Fast Loaded Dice Roller distribution (dashed) to the distribution generated from the SciPy face image (solid)</em></p>&#13;
<p class="indent">Discrete distributions generate integers with specific likelihoods. Let’s leave them now and consider continuous probability distributions, which return floating-point values instead.</p>&#13;
<h4 class="h4" id="ch03lev2_3">Continuous Probability Distributions</h4>&#13;
<p class="noindent">I haven’t discussed continuous probabilities yet in this chapter. In part, not doing so was to make the concepts behind probability easier to follow. A continuous probability distribution, like a discrete one, has a particular shape. However, instead of assigning a probability to a specific integer value, as we saw above, the probability of selecting a particular value from a continuous distribution is zero. The probability of a specific value, a real number, is zero because there are an infinite number of possible values from a continuous distribution; this means no particular value can be selected. Instead, what we talk about is the probability of selecting values in a specific range of values.</p>&#13;
<p class="indent">For example, the most common continuous distribution is the uniform distribution over [0, 1]. This distribution returns <em>any</em> real number in that range. Although the probability of returning a specific real number is zero, we can talk about the probability of returning a value in a range, such as [0, 0.25].</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_52"/>Consider again the uniform distribution over [0, 1]. We know that the sum of all the individual probabilities from zero to one is 1.0. So, what is the probability of sampling a value from this distribution and having that value be in the range [0, 0.25]? All values are equally likely, and all add to 1.0, so we must have a 25 percent chance of returning a value in [0, 0.25]. Similarly, we have a 25 percent chance of returning a value in [0.75, 1], as that also covers 1/4 of the possible range.</p>&#13;
<p class="indent">When we talk about summing infinitely small things over a range, we’re talking about integration, the part of calculus that we won’t cover in this book. Conceptually, however, we can understand what’s happening if we think about a discrete distribution in the limit where the number of values it can return goes to infinity, and we’re summing the probabilities over some range.</p>&#13;
<p class="indent">We also can think about this graphically. <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> shows the continuous probability distributions I’ll discuss.</p>&#13;
<div class="image" id="ch03fig04"><img src="Images/03fig04.jpg" alt="image" width="677" height="585"/></div>&#13;
<p class="figcap"><em>Figure 3-4: Some common continuous probability distributions</em></p>&#13;
<p class="indent">To get the probability of sampling a value in some range, we add up the area under the curve over that range. Indeed, this is precisely what integration does; the integration symbol (∫) is nothing more than a fancy “S” for <em>sum</em>. It’s the continuous version of ∑ for summing discrete values.</p>&#13;
<p class="indent">The distributions in <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> are the most common ones you’ll encounter, though there are many others useful enough to be given names. <span epub:type="pagebreak" id="page_53"/>All of these distributions have associated <em>probability density functions (pdfs)</em>, closed-form functions that generate the probabilities that sampling from the distribution will give. I generated the curves in <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> instead using the code in the file <code>continuous.py</code>. The curves are estimates of the probability density functions, and I created them from the histogram of a large number of samples. I did so intentionally to demonstrate that the NumPy random functions sampling from these distributions do what they claim.</p>&#13;
<p class="indent">Pay little attention to the x-axis in <a href="ch03.xhtml#ch03fig04">Figure 3-4</a>. The distributions have different ranges of output; they’re scaled here to fit all of them on the graph. The important thing to notice is their shapes. The uniform distribution is, well, uniform over the entire range. The normal curve, also frequently called a <em>Gaussian</em> or a <em>bell curve</em>, is the second most common distribution used in deep learning. For example, the He initialization strategy for neural networks samples initial weights from a normal distribution.</p>&#13;
<p class="indent">The code generating the data for <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> is worth considering, as it shows us how to use NumPy to get samples:</p>&#13;
<pre>N = 10000000<br/>&#13;
B = 100<br/>&#13;
t = np.random.random(N)<br/>&#13;
u = np.histogram(t, bins=B)[0]<br/>&#13;
u = u / u.sum()<br/>&#13;
t = np.random.normal(0, 1, size=N)<br/>&#13;
n = np.histogram(t, bins=B)[0]<br/>&#13;
n = n / n.sum()<br/>&#13;
t = np.random.gamma(5.0, size=N)<br/>&#13;
g = np.histogram(t, bins=B)[0]<br/>&#13;
g = g / g.sum()<br/>&#13;
t = np.random.beta(5,2, size=N)<br/>&#13;
b = np.histogram(t, bins=B)[0]<br/>&#13;
b = b / b.sum()</pre>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>We’re using the classic NumPy functions here, not the newer Generator-based functions. NumPy updated the pseudorandom number code in recent versions, but the overhead of using the new code will detract from what we want to see here. Unless you’re very serious about pseudorandom number generation, the older functions, and the Mersenne Twister pseudorandom number generator they’re based on, will be more than adequate.</em></p>&#13;
</div>&#13;
<p class="indent">To make the plots, we first use 10 million samples from each distribution (<code>N</code>). Then, we use 100 bins in the histogram (<code>B</code>). Again, the x-axis range when plotting isn’t of interest here, only the shapes of the curves.</p>&#13;
<p class="indent">The uniform samples use <code>random</code>, a function we’ve seen before. Passing the samples to <code>histogram</code> and applying the “divide by the sum” trick creates the probability curve data (<code>u</code>). We repeat this process for the Gaussian (<code>normal</code>), Gamma (<code>gamma</code>), and Beta (<code>beta</code>) distributions as well.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_54"/>You’ll notice that <code>normal</code>, <code>gamma</code>, and <code>beta</code> accept arguments. These distributions are parameterized; their shape is altered by changing these parameters. For the normal curve, the first parameter is the mean (μ), and the second is the standard deviation (σ). Some 68 percent of the normal curve lies within one standard deviation of the mean, [μ – σ, μ + σ]. The normal curve is ubiquitous in math and nature, and one could write an entire book on it alone. It’s always symmetric around its mean value. The standard deviation controls how wide or narrow the curve is.</p>&#13;
<p class="indent">The gamma distribution is also parameterized. It accepts two parameters: the shape (<em>k</em>) and the scale (θ). Here, <em>k</em> = 5, and the scale is left at its default value of θ = 1. As the shape increases, the gamma distribution becomes more and more like a Gaussian, with a bump that moves toward the center of the distribution. The scale parameter affects the horizontal size of the bump.</p>&#13;
<p class="indent">Likewise, the beta distribution uses two parameters, <em>a</em> and <em>b</em>. Here, <em>a</em> = 5 and <em>b</em> = 2. If <em>a</em> &gt; <em>b</em>, the hump of the distribution is on the right; if reversed, it is on the left. If <em>a</em> = <em>b</em>, the beta distribution becomes the uniform distribution. The flexibility of the beta distribution makes it quite handy for simulating different processes, as long as you can find <em>a</em> and <em>b</em> values approximating the probability distribution you want. However, depending on the precision you require, the new Fast Loaded Dice Roller we saw in the previous section might be a better option in practice if you have a sufficiently detailed discrete distribution approximation of the continuous distribution.</p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03tab02">Table 3-2</a> shows us the probability density functions for the normal, gamma, and beta distributions. An exercise for the reader is to use these functions to recreate <a href="ch03.xhtml#ch03fig04">Figure 3-4</a>. Your results will be smoother still than the curves in the figure. You can calculate the <em>B</em>(<em>a</em>, <em>b</em>) integral in <a href="ch03.xhtml#ch03tab02">Table 3-2</a> by using the function <code>scipy.special.beta</code>. For Γ(<em>k</em>), see <code>scipy.special.gamma</code>. Additionally, if the argument to the Γ function is an integer, Γ(<em>n</em> + 1) = <em>n</em>!, so Γ(5) = Γ(4 + 1) = 4! = 24.</p>&#13;
<p class="tabcap" id="ch03tab02"><strong>Table 3-2:</strong> The Probability Density Functions for the Normal, Gamma, and Beta Distributions</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:80%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td class="borderr" style="vertical-align: top"><p class="tab"><strong>normal</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><img src="Images/054equ01.jpg" alt="image" width="208" height="40"/></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td class="borderr" style="vertical-align: top"><p class="tab"><strong>gamma</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><img src="Images/054equ02.jpg" alt="image" width="334" height="59"/></p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td class="borderr" style="vertical-align: top"><p class="tab"><strong>beta</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><img src="Images/054equ03.jpg" alt="image" width="466" height="49"/></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">If you’re interested in ways to sample values from these distributions, my book <em>Random Numbers and Computers</em> (Springer, 2018) discusses these distributions and others in more depth than we can provide here, including implementations in C for generating samples from them. For now, let’s examine one of the most important theorems in probability theory.</p>&#13;
<h4 class="h4" id="ch03lev2_4"><span epub:type="pagebreak" id="page_55"/>Central Limit Theorem</h4>&#13;
<p class="noindent">Imagine we draw <em>N</em> samples from some distribution and calculate the mean value, <em>m</em>. If we repeat this exercise many times, we’ll get a set of mean values, <em>{m</em><sub>0</sub>, <em>m</em><sub>1</sub>, . . .}, each from a set of samples from the distribution. It doesn’t matter if <em>N</em> is the same each time, but <em>N</em> shouldn’t be too small. The rule of thumb is that <em>N</em> should be at least 30 samples.</p>&#13;
<p class="indent">The <em>central limit theorem</em> states that the histogram or probability distribution generated from this set of sample means, the <em>m</em>’s, will approach a Gaussian in shape regardless of the shape of the distribution the samples were drawn from in the first place.</p>&#13;
<p class="indent">For example, this code</p>&#13;
<pre>&#13;
M = 10000<br/>&#13;
m = np.zeros(M)<br/>&#13;
for i in range(M):<br/>&#13;
    t = np.random.beta(5,2,size=M)<br/>&#13;
    m[i] = t.mean()</pre>&#13;
<p class="noindent">creates 10,000 sets of samples from a beta distribution, Beta(5,2), each with 10,000 samples. The mean of each set of samples is stored in <code>m</code>. If we run this code and plot the histogram of <em>m</em>, we get <a href="ch03.xhtml#ch03fig05">Figure 3-5</a>.</p>&#13;
<div class="image" id="ch03fig05"><img src="Images/03fig05.jpg" alt="image" width="680" height="510"/></div>&#13;
<p class="figcap"><em>Figure 3-5: The distribution of mean values of 10,000 sets of samples of 10,000 from Beta(5,2)</em></p>&#13;
<p class="indent">The shape of <a href="ch03.xhtml#ch03fig05">Figure 3-5</a> is decidedly Gaussian. Again, the shape is a consequence of the central limit theorem and does not depend on the shape <span epub:type="pagebreak" id="page_56"/>of the underlying distribution. <a href="ch03.xhtml#ch03fig05">Figure 3-5</a> tells us that the sample means from many sets of samples from Beta(5,2) themselves have a mean of about 0.714. The mean of the sample means (<code>m.mean()</code>) is 0.7142929 for one run of the code above.</p>&#13;
<p class="indent">There’s a formula to calculate the mean value of a Beta distribution. The population mean value of a Beta(5,2) distribution is known to be <em>a</em>/<em>(a</em> + <em>b</em>) = 5/(5 + 2) = 5/7 = 0.714285. The mean of the plot in <a href="#ch03fig05">Figure 3-5</a> is a measurement of the true population mean, of which the many means from the Beta(5,2) samples are only estimates.</p>&#13;
<p class="indent">Let’s explain this again to really follow what’s going on. For any distribution, like the Beta(5,2) distribution, if we draw <em>N</em> samples, we can calculate the mean of those samples, a single number. If we repeat this process for many sets of <em>N</em> samples, each with its own mean, and we make a histogram of the distribution of the means we measured, we’ll get a plot like <a href="ch03.xhtml#ch03fig05">Figure 3-5</a>. That plot tells us that all of the many sample means are themselves clustered around a mean value. The mean value of the means is a measure of the population mean. It’s the mean we’d get if we could draw an infinite number of samples from the distribution. If we change the code above to use the uniform distribution, we’ll get a population mean of 0.5. Similarly, if we switch to a Gaussian distribution with a mean of 11, the resulting histogram will be centered at 11.</p>&#13;
<p class="indent">Let’s prove this claim again but this time with a discrete distribution. Let’s use the Fast Loaded Dice Roller to generate samples from a lopsided discrete distribution using this code:</p>&#13;
<pre>&#13;
from fldrf import fldr_preprocess_float_c<br/>&#13;
from fldr import fldr_sample<br/>&#13;
z = fldr_preprocess_float_c([0.1,0.6,0.1,0.1,0.1])<br/>&#13;
m = np.zeros(M)<br/>&#13;
for i in range(M):<br/>&#13;
    t = np.array([fldr_sample(z) for i in range(M)])<br/>&#13;
    m[i] = t.mean()</pre>&#13;
<p class="indent"><a href="ch03.xhtml#ch03fig06">Figure 3-6</a> shows the discrete distribution (top) and the corresponding distribution of the sample means (bottom).</p>&#13;
<p class="indent">From the probability mass function, we can see that the most frequent value we expect from the sample is 1, with a probability of 60 percent. However, the tail on the right means we’ll also get values 2 through 4 about 30 percent of the time. The weighted mean of these is 0.6(1) + 0.1(2) + 0.1(3) + 0.1(4) = 1.5, which is precisely the mean of the sample distribution on the bottom of <a href="ch03.xhtml#ch03fig06">Figure 3-6</a>. The central limit theorem works. We’ll revisit the central limit theorem in <a href="ch04.xhtml#ch04">Chapter 4</a> when we discuss hypothesis testing.</p>&#13;
<span epub:type="pagebreak" id="page_57"/>&#13;
<div class="image" id="ch03fig06"><img src="Images/03fig06.jpg" alt="image" width="650" height="1002"/></div>&#13;
<p class="figcap"><em>Figure 3-6: An arbitrary discrete distribution (top) and the distribution of sample means drawn from it (bottom)</em></p>&#13;
<h4 class="h4" id="ch03lev2_5"><span epub:type="pagebreak" id="page_58"/>The Law of Large Numbers</h4>&#13;
<p class="noindent">A concept related to the central limit theorem, and often confused with it, is the <em>law of large numbers</em>. The law of large numbers states that as the size of a sample from a distribution increases, the mean of the sample moves closer and closer to the mean of the population. In this case, we’re contemplating a single sample from the distribution and making a statement about how close we expect its mean to be to the true population mean. For the central limit theorem, we have many different sets of samples from the distribution and are making a statement about the distribution of the means of those sets of samples.</p>&#13;
<p class="indent">We can demonstrate the law of large numbers quite simply by selecting larger and larger size samples from a distribution and tracking the mean as a function of the sample size (the number of samples drawn). In code, then,</p>&#13;
<pre>&#13;
m = []<br/>&#13;
for n in np.linspace(1,8,30):<br/>&#13;
    t = np.random.normal(1,1,size=int(10**n))<br/>&#13;
    m.append(t.mean())</pre>&#13;
<p class="noindent">where we’re drawing ever-larger sample sizes from a normal distribution with a mean of 1. The first sample size is 10, and the last is 100 million. If we plot the mean of the samples as a function of sample size, we see the law of large numbers at work.</p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03fig07">Figure 3-7</a> shows the sample means as a function of the number of samples for the normal distribution with a mean of 1 (dashed line). As the number of samples from the distribution increases, the mean of the samples approaches the population mean, which illustrates the law of large numbers.</p>&#13;
<div class="image" id="ch03fig07"><img src="Images/03fig07.jpg" alt="image" width="638" height="475"/></div>&#13;
<p class="figcap"><em>Figure 3-7: The law of large numbers in action</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_59"/>Let’s change gears and move on to Bayes’ theorem, the last topic for this chapter.</p>&#13;
<h3 class="h3" id="ch03lev1_2">Bayes’ Theorem</h3>&#13;
<p class="noindent">In <a href="ch02.xhtml#ch02">Chapter 2</a>, we discussed an example where we determined whether a woman had cancer. There, I promised that Bayes’ theorem would tell us how to properly account for the probability of a randomly selected woman in her 40s having breast cancer. Let’s fulfill that promise in this section by learning what Bayes’ theorem is and how to use it.</p>&#13;
<p class="indent">Using the product rule, <a href="ch02.xhtml#ch02equ08">Equation 2.8</a>, we know the following two mathematical statements are true:</p>&#13;
<p class="center"><em>P</em>(<em>B</em>, <em>A</em>) = <em>P</em>(<em>B</em>|<em>A</em>)<em>P</em>(<em>A</em>)</p>&#13;
<p class="center"><em>P</em>(<em>A</em>, <em>B</em>) = <em>P</em>(<em>A</em>|<em>B</em>)<em>P</em>(<em>B</em>)</p>&#13;
<p class="noindent">Additionally, because the joint probability of both <em>A</em> and <em>B</em> doesn’t depend on which event we call <em>A</em> and which we call <em>B</em>,</p>&#13;
<p class="center"><em>P</em>(<em>A</em>, <em>B</em>) = <em>P</em>(<em>B</em>, <em>A</em>)</p>&#13;
<p class="noindent">Therefore,</p>&#13;
<p class="center"><em>P</em>(<em>B</em>|<em>A</em>)<em>P</em>(<em>A</em>) = <em>P</em>(<em>A</em>|<em>B</em>)<em>P</em>(<em>B</em>)</p>&#13;
<p class="noindent">Dividing by <em>P</em>(<em>A</em>), we get</p>&#13;
<div class="imagec" id="ch03equ01"><img src="Images/03equ01.jpg" alt="image" width="450" height="50"/></div>&#13;
<p class="noindent">This is <em>Bayes’ theorem</em>, the heart of the Bayesian approach to probability, and the proper way to compare two conditional probabilities: <em>P</em><em>(B</em>|<em>A</em>) and <em>P</em>(<em>A</em>|<em>B</em>). You’ll sometimes see <a href="ch03.xhtml#ch03equ01">Equation 3.1</a> referred to as <em>Bayes’ rule</em>. You’ll also often see no apostrophe after “Bayes,” which is a bit sloppy and ungrammatical, but common.</p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03equ01">Equation 3.1</a> has been enshrined in neon lights, tattoos, and even baby names: “Bayes.” The equation is named after Thomas Bayes (1701–1761), an English minister and statistician, and was published after his death. In words, <a href="ch03.xhtml#ch03equ01">Equation 3.1</a> says the following:</p>&#13;
<p class="block">The <em>posterior probability</em>, <em>P</em>(<em>B</em>|<em>A</em>), is the product of <em>P</em>(<em>A</em>|<em>B</em>), the <em>likelihood</em>, and <em>P</em>(<em>B</em>), the <em>prior</em>, normalized by <em>P</em>(<em>A</em>), the marginal probability or <em>evidence</em>.</p>&#13;
<p class="indent">Now that we know what Bayes’ theorem is, let’s see it in action so we can understand it.</p>&#13;
<h4 class="h4" id="ch03lev2_6"><span epub:type="pagebreak" id="page_60"/>Cancer or Not Redux</h4>&#13;
<p class="noindent">One way to think about the components of Bayes’ theorem is in the context of medical testing. At the beginning of <a href="ch02.xhtml#ch02">Chapter 2</a>, we calculated the probability of a woman having breast cancer given a positive mammogram and found that it was quite different from what we might naively have believed it to be. Let’s revisit that problem now using Bayes’ theorem. It might be helpful to reread the first section of <a href="ch02.xhtml#ch02">Chapter 2</a> before continuing.</p>&#13;
<p class="indent">We want to use Bayes’ theorem to find the posterior probability, the probability of breast cancer given a positive mammogram. We’ll write this as <em>P</em><em>(bc</em> + |+), meaning breast cancer (<em>bc</em>+) given a positive mammogram (+).</p>&#13;
<p class="indent">In the problem, we’re told that the mammogram returns a positive result, given the patient has breast cancer, 90 percent of the time. We write this as</p>&#13;
<p class="center"><em>P</em>(+|<em>bc</em>+) = 0.9</p>&#13;
<p class="noindent">This is the likelihood of a positive mammogram in terms of Bayes’ equation, <em>P</em>(<em>A</em>|<em>B</em>) = <em>P</em>(+|<em>bc</em>+).</p>&#13;
<p class="indent">Next, we’re told the probability of a random woman having breast cancer is 0.8 percent. Therefore, we know</p>&#13;
<p class="center"><em>P</em>(<em>bc</em>+) = 0.008</p>&#13;
<p class="noindent">This is the prior probability, <em>P</em>(<em>B</em>), in Bayes’ theorem.</p>&#13;
<p class="indent">We have all the components of <a href="ch03.xhtml#ch03equ01">Equation 3.1</a> except one: <em>P</em><em>(A</em>).  What is <em>P</em>(<em>A</em>) in this context? It’s <em>P</em>(+), the marginal probability of a positive mammogram regardless of any <em>B</em>, any breast cancer status. It’s also the evidence that we have, the thing we know: the mammogram was positive.</p>&#13;
<p class="indent">In the problem, we’re told there’s a 7 percent chance a woman without breast cancer has a positive mammogram. Is this <em>P</em>(+)? No, it is <em>P</em>(+|<em>bc</em>–), the probability of a positive mammogram <em>given</em> no breast cancer.</p>&#13;
<p class="indent">I’ve referred to <em>P</em><em>(A</em>) as the marginal probability twice now. We know what to do to get a marginal or total probability: we sum over all the other parts of a joint probability that don’t matter for what we want to know. Here, we have to sum over all the partitions of the sample space we don’t care about to get the marginal probability of a positive mammogram. What partitions are those? There are only two: either a woman has breast cancer or she doesn’t. Therefore, we need to find</p>&#13;
<p class="center"><em>P</em>(+) = <em>P</em>(+|<em>bc</em>+)<em>P</em>(<em>bc</em>+) + <em>P</em>(+|<em>bc</em>–)<em>P</em>(<em>bc</em>–)</p>&#13;
<p class="noindent">We know all of these quantities already, except <em>P</em><em>(bc</em>–). This is the prior probability that a randomly selected woman will <em>not</em> have breast cancer, <em>P</em><em>(bc</em>–) = 1 – <em>P</em>(<em>bc</em>+) = 0.992.</p>&#13;
<p class="indent">Sometimes, you’ll see the summation over other terms in the joint probability expressed in the denominator of Bayes’ theorem. Even if they’re not explicitly called out, they are there, implicit in what it takes to find <em>P</em><em>(A</em>).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_61"/>Finally, we have all the pieces and can calculate the probability using Bayes’ theorem:</p>&#13;
<div class="imagec"><img src="Images/061equ01.jpg" alt="image" width="453" height="201"/></div>&#13;
<p class="noindent">This is the result we found earlier. Recall, a large percentage of doctors in the study claimed the probability of cancer from a positive mammogram, <em>P</em><em>(A</em>|<em>B</em>), was 90 percent. Their mistake was incorrectly equating <em>P(A</em>|<em>B</em>) with <em>P(B</em>|<em>A</em>). Bayes’ theorem correctly relates the two by using the prior and the marginal probability.</p>&#13;
<h4 class="h4" id="ch03lev2_7">Updating the Prior</h4>&#13;
<p class="noindent">We don’t need to stop with this single calculation. Consider the following: what if, after a woman receives the news that her mammogram is positive, she decides to have a second mammogram at another facility with different radiologists reading the results, and that mammogram also comes back positive? Does she still believe that her probability of having breast cancer is 9 percent? Intuitively, we might think that she now has more reason to believe that she has cancer. Can this belief be quantified? It can, in the Bayesian view, by updating the prior, <em>P</em><em>(bc</em>+), with the posterior calculated from the first test, <em>P</em>(<em>bc</em> + |+). After all, she now has a stronger prior probability of cancer given the first positive mammogram.</p>&#13;
<p class="indent">Let’s calculate this new posterior based on the previous mammogram result:</p>&#13;
<div class="imagec"><img src="Images/061equ02.jpg" alt="image" width="453" height="201"/></div>&#13;
<p class="indent">As 57 percent is significantly higher than 9 percent, our hypothetical woman now has significantly more reason to believe she has breast cancer.</p>&#13;
<p class="indent">Notice what has changed in this new calculation, besides a dramatic increase in the posterior probability of breast cancer given the second mammogram’s positive result. First, the prior probability of breast cancer went <span epub:type="pagebreak" id="page_62"/> from 0.008 → 0.094, the posterior calculated based on the first test. Second, <em>P</em>(<em>bc</em>–) also changed from 0.992 → 0.906. Why? Because the prior changed and <em>P</em>(<em>bc</em>–) = 1 – <em>P</em>(<em>bc</em>+). The sum of <em>P</em>(<em>bc</em>+) and <em>P</em>(<em>bc</em>–) must still be 1.0—either she has breast cancer, or she doesn’t—that’s the entire sample space.</p>&#13;
<p class="indent">In the example above, we updated the prior based on the initial test result, and we had an initial prior given to us in the first example. What about the prior in general? In many cases, Bayesians select the prior, at least initially, based on an actual belief about the problem. Often the prior is a uniform distribution, known as the <em>uninformed prior</em> because there’s nothing to guide the selection of anything else. For the breast cancer example, the prior is something that can be estimated from an experiment using a random selection of women from the general population.</p>&#13;
<p class="indent">As mentioned earlier, don’t take the numbers here too seriously; they are for example use only. Also, while a woman certainly has the option to get a second opinion, the gold standard for a breast cancer diagnosis is biopsy, the likely next step after an initial positive mammogram. Finally, throughout this section, I’ve referred to women and breast cancer. Men also get breast cancer, though it is rare, with less than 1 percent of cases in men. However, it made the discussion simpler to refer only to women. I’ll note that breast cancer cases in men are more likely to be fatal, though the reasons why are not yet known.</p>&#13;
<h4 class="h4" id="ch03lev2_8">Bayes’ Theorem in Machine Learning</h4>&#13;
<p class="noindent">Bayes’ theorem is prevalent throughout machine learning and deep learning. One classic use of Bayes’ theorem, one that can work surprisingly well, is to use it as a classifier. This is known as the <em>Naive Bayes</em> classifier. Early email spam filters used this approach quite effectively.</p>&#13;
<p class="indent">Assume we have a dataset consisting of class labels, <em>y</em>, and feature vectors, <em><strong>x</strong></em>. The goal of a Naive Bayes classifier is to tell us, for each class, the probability that a given feature vector belongs to that class. With those probabilities, we can assign a class label by selecting the largest probability. That is, we want to find <em>P</em><em>(y</em>|<strong><em>x</em></strong>) for each class label, <em>y</em>. This is a conditional probability, so we can use Bayes’ theorem with it:</p>&#13;
<div class="imagec" id="ch03equ02"><img src="Images/03equ02.jpg" alt="image" width="438" height="50"/></div>&#13;
<p class="indent">The equation above is saying that the probability of feature vector <em>x</em> representing an instance of class label <em>y</em> is the probability of the class label <em>y</em> generating a feature vector <em>x</em> times the prior probability of class label <em>y</em> occurring, divided by the marginal probability of the feature vector over all class labels. Recall the implicit sum in calculating <em>P</em>(<em>x</em>).</p>&#13;
<p class="indent">How is this useful to us? Since we have a dataset, we can estimate <em>P</em><em>(y</em>) using it, assuming the dataset class distribution is a fair representation of what we’d encounter when using the model. And, since we have labels, we can partition the dataset into smaller, per class, collections. This might help <span epub:type="pagebreak" id="page_63"/>us do something useful to get the likelihoods per class, <em>P</em>(<em>x</em>|<em>y</em>). We’ll ignore the marginal <em>P</em>(<em>x</em>) completely. Let’s see why, in this case, we’re free to do so.</p>&#13;
<p class="indent"><a href="ch03.xhtml#ch03equ02">Equation 3.2</a> is for a particular class label, say <em>y</em> = 1. We’ll have other versions of it for all the class labels in the dataset. We said our classifier consists of calculating the posterior probabilities for each class label and selecting the largest one as the label assigned to an unknown feature vector. The denominator of <a href="ch03.xhtml#ch03equ02">Equation 3.2</a> is a scale factor, which makes the output a true probability. For our use case, however, we only care about the relative ordering of <em>P</em><em>(y</em>|<em><strong>x</strong></em>) over the different class labels. Since <em>P</em>(<em><strong>x</strong></em>) is the same for all <em>y</em>, it’s a common factor that will change the number associated with <em>P</em><em>(y</em>|<em><strong>x</strong></em>) but not the ordering over the different class labels. Therefore, we can ignore it and concentrate on finding the products of the likelihoods and the priors. Although the largest <em>P</em>(<em>y</em>|<em><strong>x</strong></em>) calculated this way is no longer a proper probability, it’s still the correct class label to assign.</p>&#13;
<p class="indent">Given that we can ignore <em>P</em><em><strong>(x</strong></em>) and the <em>P</em>(<em>y</em>) values are easily estimated from the dataset, we’re left with calculating <em>P</em><em><strong>(x</strong></em>|<em>y</em>), the likelihood that given the class label is <em>y</em>, we’d have a feature vector <em><strong>x</strong></em>. What can we do in this case?</p>&#13;
<p class="indent">First, we can think about what <em>P</em><em><strong>(x</strong></em>|<em>y</em>) is. It’s a conditional probability for feature vectors given the feature vectors are all representatives of class <em>y</em>. For the moment, let’s ignore the <em>y</em> part, since we know the feature vectors all come from class <em>y</em>.</p>&#13;
<p class="indent">This leaves only <em>P</em><em><strong>(x</strong></em>) because we fixed <em>y</em>. A feature vector is a collection of individual features, <em>x</em> = (<em><strong>x<sub>0</sub></strong></em>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . ., <em>x<sub>n</sub></em><sub>–1</sub>) for <em>n</em> features in the vector. Therefore, <em>P</em><em><strong>(x</strong></em>) is really a joint probability, the probability that all the individual features have their specific values <em>at the same time</em>. So, we can write</p>&#13;
<p class="center"><em>P</em>(<em>x</em>) = <em>P</em>(<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . ., <em>x</em><sub><em>n</em>–1</sub>)</p>&#13;
<p class="indent">How does this help? If we make one more assumption about our data, we’ll see that we can break up this joint probability in a convenient way. Let’s assume that all the features in our feature vector are independent. Recall that <em>independent</em> means the value of <em>x</em><sub>1</sub>, say, is in no way affected by the value of any other feature in the vector. This is typically not quite true, and for things like pixels in images <em>definitely</em> not true, but we’ll assume it’s true nonetheless. We’re naive to believe it’s true, hence the <em>Naive</em> in <em>Naive Bayes</em>.</p>&#13;
<p class="indent">If the features are independent, then the probability of a feature taking on any particular value is independent of all the others. In that case, the product rule tells us that we can break the joint probability up like so:</p>&#13;
<p class="center"><em>P</em>(<em>x</em>) = <em>P</em>(<em>x</em><sub>0</sub>)<em>P</em>(<em>x</em><sub>1</sub>)<em>P</em>(<em>x</em><sub>2</sub>) . . . <em>P</em>(<em>x</em><sub><em>n</em>–1</sub>)</p>&#13;
<p class="indent">This helps tremendously. We have a dataset, labeled by class, allowing us to estimate the probability of any feature for any specific class by counting how often each feature value happens for each class.</p>&#13;
<p class="indent">Let’s put it all together for a hypothetical dataset of three classes—0, 1, and 2—and four features. We first use the dataset, partitioned by class label, to estimate each feature value probability. This provides us the set of <em>P</em><em>(x</em><sub>0</sub>), <em>P</em>(<em>x</em><sub>1</sub>), and so on, for each feature for each class label. Combined <span epub:type="pagebreak" id="page_64"/>with the prior probability of the class label, estimated from the dataset as the number of each class divided by the total number of samples in the dataset, we calculate for a new unknown feature vector, <em><strong>x</strong></em>,</p>&#13;
<div class="imagec"><img src="Images/064equ01.jpg" alt="image" width="312" height="84"/></div>&#13;
<p class="noindent">Here, the <em>P</em>(<em>x</em><sub>0</sub>) feature probabilities are specific to class 0 only, and <em>P</em>(0) is the prior probability of class 0 in the dataset. <em>P</em>(0|<em><strong>x</strong></em>) is the unnormalized posterior probability that the unknown feature vector <em><strong>x</strong></em> belongs to class 0. We say <em>unnormalized</em> because we’re ignoring the denominator of Bayes’ theorem, knowing that including it would not change the ordering of the posterior probabilities, only their values.</p>&#13;
<p class="indent">We can repeat the calculation above to get <em>P</em>(1|<em><strong>x</strong></em>) and <em>P</em>(2|<em><strong>x</strong></em>), making sure to use the per feature probabilities calculated for those classes (the <em>P</em>(<em>x</em><sub>0</sub>)s). Finally, we give <em><strong>x</strong></em> the class label for the largest of the three posteriors calculated.</p>&#13;
<p class="indent">The description above assumes that the feature values are discrete. Usually they aren’t, but there are workarounds. One is to bin the feature values to make them discrete. For example, if the feature ranges over [0, 3], create a new feature that is 0, 1, or 2, and assign the continuous feature to one of those bins by truncating any fractional part.</p>&#13;
<p class="indent">Another workaround is to make one more assumption about the distribution the feature values come from and use that distribution to calculate the <em>P</em><em>(x</em><sub>0</sub>)s per class. Features are often based on measurements in the real world, and many things in the real world follow a normal distribution. Therefore, typically we’d assume that the individual features, while continuous, are normally distributed and we can find estimates of the mean (μ) and standard deviation (σ) from the dataset, per feature, and class label.</p>&#13;
<p class="indent">Bayes’ theorem is useful for calculating probabilities. It’s helpful in machine learning as well. The battle between Bayesians and frequentists appears to be waning, though philosophical differences remain. In practice, most researchers are learning that both approaches are valuable, and at times tools from both camps should be used. We’ll continue this trend in the next chapter, where we’ll examine statistics from a frequentist viewpoint. We defend this decision by pointing out that the vast majority of published scientific results in the last century used statistics this way, which includes the deep learning community, at least when it’s presenting the results of experiments.</p>&#13;
<h3 class="h3" id="ch03lev1_3"><span epub:type="pagebreak" id="page_65"/>Summary</h3>&#13;
<p class="noindent">This chapter taught us about probability distributions, what they are, and how to draw samples from them, both discrete and continuous. We’ll encounter different distributions during our exploration of deep learning. We also discovered Bayes’ theorem and saw how it lets us properly relate conditional probabilities. We saw how Bayes’ theorem allows us to evaluate the true likelihood of cancer given an imperfect medical test—a common situation. We also learned how to use Bayes’ theorem, along with some of the basic probability rules we learned in <a href="ch02.xhtml#ch02">Chapter 2</a>, to build a simple but often surprisingly effective classifier.</p>&#13;
<p class="indent">Let’s move now into the world of statistics.<span epub:type="pagebreak" id="page_66"/></p>&#13;
<p class="fnote"><a id="ch03fn01" href="#ch03fn01a">1</a>. Feras A. Saad, Cameron E. Freer, Martin C. Rinard, and Vikash K. Mansinghka, “The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for Discrete Probability Distributions,” in AISTATS 2020: Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, <em>Proceedings of Machine Learning Research</em> 108, Palermo, Sicily, Italy, 2020.</p>&#13;
</div></body></html>