- en: '**15'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: THE SCIENTIFIC LIBRARIES**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we’ll look at high-level summaries of the core Python libraries
    for mathematics, data analysis, machine learning, deep learning, computer vision,
    language processing, web scraping, and parallel processing ([Table 15-1](ch15.xhtml#ch015tab1)).
    We’ll also look at some guidelines for choosing among competing products. In subsequent
    chapters, we’ll dive deeper into the functionality of several of these libraries
    and then apply them in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 15-1](ch15.xhtml#ch015tab1) organizes these libraries into subcategories,
    lists their websites, and provides a brief description of each. As these are popular
    and, in many cases, mature libraries, you should have no problem finding additional
    guidance for each online and in bookstores.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-1:** Essential Python Scientific Libraries for Python'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Library** | **Description** | **Website** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Math and data analysis | NumPy | Numerical computing tools for arrays | *[https://numpy.org/](https://numpy.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| SciPy Library | Friendly and efficient numerical routines | *[https://www.scipy.org/](https://www.scipy.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| SymPy | Symbolic math/computer algebra tools | *[https://www.sympy.org/](https://www.sympy.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pandas | Data manipulation, analysis, and visualization tools | *[http://pandas.pydata.org/](http://pandas.pydata.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Machine and deep learning | Scikit-learn | General-purpose machine learning
    toolkit | *[https://scikit-learn.org/](https://scikit-learn.org/)* |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow | Symbolic math library for deep learning neural nets | *[https://www.tensorflow.org/](https://www.tensorflow.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Keras | Friendlier wrapper for TensorFlow | *[https://keras.io/](https://keras.io/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | Fast and efficient artificial neural networks | *[https://pytorch.org/](https://pytorch.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Image processing | OpenCV | Real-time computer vision library | *[https://opencv.org/](https://opencv.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scikit-image | Scientific image processing and analysis tools | *[https://scikit-image.org/](https://scikit-image.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pillow | Basic image processing tools | *[https://python-pillow.org/](https://python-pillow.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Language processing | NLTK | Symbolic and statistical language processing
    library | *[http://www.nltk.org/](http://www.nltk.org/)* |'
  prefs: []
  type: TYPE_TB
- en: '| spaCy | Fast production grade language processing library | *[https://spacy.io/](https://spacy.io/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Helper libraries | requests | Webscraper for HTTP requests | *[https://pypi.org/project/requests/](https://pypi.org/project/requests/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| BeautifulSoup | Tools to extract text from HTML and XML files | *[https://www.crummy.com/software/](https://www.crummy.com/software/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| re | Library for working with regular expressions | *[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dask | Library for parallel computing with Python | *[https://dask.org/](https://dask.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Spark | “Heavier” alternative to Dask for Big Data | *[https://spark.apache.org/](https://spark.apache.org/)*
    |'
  prefs: []
  type: TYPE_TB
- en: '**The SciPy Stack**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SciPy stack of open source libraries comes preinstalled on Anaconda and
    includes NumPy, the SciPy library, Matplotlib, IPython, SymPy, and pandas ([Figure
    15-1](ch15.xhtml#ch015fig1)). These have been called “the bedrock of number-crunching
    and visualization in Python” and are among the most used scientific libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-1: The core components of the SciPy ecosystem (courtesy of [https://SciPy.org](https://SciPy.org))*'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we take a high-level look at these libraries. Then,
    in later chapters, we take deeper dives into NumPy, Matplotlib, and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '***NumPy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Short for *Numerical Python*, NumPy is Python’s dedicated library for performing
    numerical calculations. It supports the creation of large, multidimensional arrays
    and matrices and provides a large collection of high-level mathematical functions
    to operate on these arrays. NumPy is considered a basic package for scientific
    computing with Python, but I would also call it *foundational* because many other
    important libraries such as pandas, Matplotlib, SymPy, and OpenCV are built on
    top of it.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy includes data structures, algorithms, and “glue” needed for most scientific
    applications involving numerical data. Operations in NumPy are faster and more
    efficient than competing functionality in the Standard Library that ships with
    Python. Having knowledge of NumPy is important to being able to use most, if not
    all, scientific Python packages, so we’ll take a closer look at it in [Chapter
    18](ch18.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '***SciPy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The scientific library *SciPy* is designed for mathematics, science, and engineering,
    and addresses many standard problem domains in scientific computing. It’s built
    on and supplements NumPy and provides many user-friendly and efficient numerical
    routines, such as routines for numerical integration, interpolation, optimization,
    linear algebra, statistics, fast Fourier transforms, signal and image processing,
    and the solving of differential equations. It extends the linear algebra routines
    and matrix decompositions provided in NumPy and provides access to many physical
    constants and conversion factors.
  prefs: []
  type: TYPE_NORMAL
- en: '***SymPy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*SymPy* is an open source library for symbolic mathematics. Its goal is to
    be a full-featured *computer algebra system (CAS)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Whereas most computer algebra systems invent their own language, SymPy is written
    and executed in Python. This makes it easier for those familiar with Python to
    use. It also allows you to use it as a library. So, in addition to using SymPy
    in an interactive environment, you can import it into your own Python application,
    where you can automate or extend it.
  prefs: []
  type: TYPE_NORMAL
- en: SymPy gives you the ability to do all sorts of computations symbolically. It
    can simplify expressions; compute derivatives, integrals, and limits; solve equations;
    work with matrices, and more. It includes packages for plotting, printing (including
    pretty printed output of math formulas or LaTeX), code generation, physics, statistics,
    combinatorics, number theory, geometry, logic, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple way to appreciate SymPy is to consider the irrational number √8, calculated
    with Python’s basic `math` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a truncated numeric answer, as √8 can’t be represented by a finite
    number. With SymPy, the square roots of numbers that are not perfect squares are
    left unevaluated by default; thus, the symbolic results are symbolically simplified
    by default (as 2 × √2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As stated previously, SymPy includes lots of useful methods, such as for solving
    equations. For example, to solve *x*2 – 2 = 0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'SymPy conveniently comes with its own plotting modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Image](../images/f0403-01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To see more of SymPy’s capability, go to *[https://docs.sympy.org/latest/tutorial/index.html](https://docs.sympy.org/latest/tutorial/index.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering, why use SymPy when there’s NumPy and the SciPy Library?
    The short answer is that SymPy is for working with algebra and doing theoretical
    math or physics; NumPy and SciPy are for performing analyses on actual data.
  prefs: []
  type: TYPE_NORMAL
- en: '***pandas***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Python Data Analysis* library is the most popular open source library for
    data science. Called *pandas* for short, it contains data structures and manipulation
    tools designed to facilitate data extraction, cleaning, analysis, and visualization.
    It adopts significant parts of NumPy and works well with other libraries like
    SciPy, statsmodels, scikit-learn, and Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, pandas is very useful for working with tabular data and common
    data sources, such as SQL relational databases and Excel spreadsheets. It’s especially
    well suited for handling time-indexed data and incorporates plotting functionality—based
    on Python’s core visualization library, Matplotlib—that makes it easy to visualize
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common data structure in pandas is the *DataFrame*, a tabular format
    similar to a spreadsheet, with columns, rows, and data. You can construct DataFrames
    from many types of input, in the case shown in the following example, from a list
    of lists using Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Element | Symbol | Atomic # |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Carbon | C | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Nitrogen | N | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Oxygen | O | 8 |'
  prefs: []
  type: TYPE_TB
- en: With DataFrames, you have the equivalent of an Excel spreadsheet or SQL table
    in Python. DataFrames, however, tend to be faster, easier to use, and more powerful
    because they’re an integral part of the Python and NumPy ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: The pandas library is one of the most important for scientists, who, as the
    old joke goes, spend 80 percent of their time finding and preparing data and the
    other 20 percent complaining about it! Mastering pandas is therefore essential,
    and you’ll get a good start in [Chapter 20](ch20.xhtml), which covers some of
    the basics.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Other libraries are beginning to challenge pandas by preserving its simplicity
    while addressing some of its efficiency issues such as the inability to scale
    projects through use of multicore processing, GPU processing, or cluster computing.
    Modin provides full drop-in replacement for pandas, letting you use pandas with
    more access to optimizations. Vaex ([https://vaex.io/](https://vaex.io/)) helps
    you to explore and visualize large datasets on normal hardware by using efficient
    lazy evaluation and clever memory mapping. Dask ([https://dask.org/](https://dask.org/))
    implements many of the same methods of pandas and offers more functionality than
    Modin or Vaex. Dask is more complex to use but helps you handle huge datasets
    and use computer clusters for improved processing speeds.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**A General Machine Learning Library: scikit-learn**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Part of data analysis is the construction and validation of *predictive models*
    that use known results to forecast future outcomes or explain past behaviors.
    This falls under the category of *machine learning*, itself a category of artificial
    intelligence ([Figure 15-2](ch15.xhtml#ch015fig2)). Machine learning deals with
    methods for pattern recognition in datasets, making it possible for machine learning
    algorithms to improve automatically through experience. These algorithms build
    *supervised* models based on training data, and *unsupervised* models, in which
    the model “discovers” patterns on its own. The algorithms can use these models
    to make decisions without being explicitly programmed to do so.
  prefs: []
  type: TYPE_NORMAL
- en: The open source scikit-learn library is built on NumPy, SciPy, and Matplotlib.
    Considered the premier general-purpose machine learning toolkit for Python programmers,
    scikit-learn has been critical for enabling Python to be a productive data science
    tool. Preinstalled on Anaconda, scikit-learn is also easy to use, making it a
    great entry point to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: Some of the branches of artificial intelligence*'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 15-3](ch15.xhtml#ch015fig3), the scikit-learn library includes
    packages for predictive data analysis, including classification (support for vector
    machines, random forests, nearest neighbors, and so on), regression, clustering
    (*k*-means, spectral, and so forth), dimensionality reduction (principal component
    analysis, matrix factorization, feature selection, and more), preprocessing (feature
    extraction and normalization), and model selection (metrics, cross-validation,
    and grid search). Both supervised and unsupervised methods are addressed. You
    can get a feel for how scikit-learn works in [Chapter 20](ch20.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: Examples of regression, classification, and clustering analysis
    using scikit-learn (courtesy of [https://scikit-learn.org/](https://scikit-learn.org/))*'
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Complementing scikit-learn is a library called statsmodels ([https://www.statsmodels.org/](https://www.statsmodels.org/)),
    which contains algorithms for classical statistics and economics. Whereas scikit-learn
    is more concerned with predictions, statsmodels is more about statistical inference,
    p-values, and uncertainty estimates.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Deep Learning Frameworks**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Deep learning* is a branch of machine learning that goes beyond the methods
    incorporated in scikit-learn. Rather than modify parameters of a fixed model through
    structured training sets *intended* for learning, deep learning networks are capable
    of learning *unsupervised* from data that is unstructured or unlabeled. Thus,
    deep learning systems imitate the nonlinear workings of the human brain in processing
    data and creating patterns for use in decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: The best-known of these systems are referred to as *artificial neural networks
    (ANNs)*. These networks generally require very complex mathematical operations
    with millions to billions of parameters and are only possible thanks to the speed
    and efficiency of graphics processing units (GPUs) developed for videogames. Example
    applications include self-driving cars and Google Translate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python libraries designed for deep learning are referred to as deep learning
    *frameworks*. These interfaces abstract away the details of the underlying algorithms,
    allowing you to define models quickly and easily using collections of prebuilt
    and optimized components. Of the many frameworks available, three dominate: TensorFlow,
    Keras, and PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Although these three systems are still evolving, they already have good documentation,
    training sets, tutorials, and support, and you can count on all three to provide
    robust deep learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '***TensorFlow***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The oldest, most popular deep learning framework for Python is an open source
    library called *TensorFlow*. Created by Google to support its large-scale applications,
    TensorFlow is an end-to-end platform for multiple machine learning tasks. Thanks
    to its large user base, good documentation, and ability to run on all major operating
    systems, TensorFlow is popular in industry and academia, across a variety of domains.
    You can find many articles on the web to help you implement solutions to complex
    problems. You can also complete a certification program online.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow is very powerful and can handle large datasets efficiently by distributing
    the computations across hundreds of mutli-GPU servers. It provides lots of functionality,
    including a tool called *TensorBoard* that helps you to create beautiful visualizations
    that are easy to understand and from which you can derive useful analytics.
  prefs: []
  type: TYPE_NORMAL
- en: '***Keras***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although TensorFlow is well documented and comes with walkthroughs to guide
    you, it’s still considered one of the most challenging deep learning frameworks,
    with an intricate interface and a steep learning curve. Fortunately, François
    Challet, a Google engineer, has written another library, *Keras*, that acts as
    an interface for TensorFlow. Although it’s now part of TensorFlow’s core application
    programming interface (API), you can also use Keras in a stand-alone manner.
  prefs: []
  type: TYPE_NORMAL
- en: Like TensorFlow, Keras is open source and works on all platforms. Unlike TensorFlow,
    Keras is written in Python, which makes it more user friendly. Designed for quick
    prototyping and fast experimentation on smaller datasets, its lightweight, simplistic
    interface takes a minimalist approach, making it easy to construct neural networks
    that can work with TensorFlow. And because Keras can act as a wrapper, you can
    always “drop down” into TensorFlow when you need to use a feature not included
    in Keras’s simpler interface.
  prefs: []
  type: TYPE_NORMAL
- en: Keras runs seamlessly on both CPUs and GPUs. It is primarily used for classification,
    speech recognition, and text generation, summarization, and tagging. By minimizing
    actions and making models easy to understand, Keras is a great deep learning tool
    for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: '***PyTorch***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*PyTorch*, developed by Facebook’s AI Research Lab, is a direct competitor
    to TensorFlow. PyTorch works on all platforms and has recently incorporated *Caffe*,
    a popular deep learning framework developed at Berkeley and geared toward the
    image processing field. PyTorch comes preinstalled on Anaconda.'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is becoming the preferred framework for academic research, though it’s
    still widely used among industry giants such as Facebook, Microsoft, and Wells
    Fargo. It excels at prototyping and is great for projects that have more of a
    non-production implementation. Among its strengths are flexibility, debugging
    capabilities, and short training durations.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike TensorFlow, PyTorch is described as feeling more “native” to Python,
    making it easy to develop and implement machine learning models. Its syntax and
    application are very Pythonic, and it integrates seamlessly with essential libraries
    like NumPy. Although Keras seems to hold the upper hand with respect to ease of
    use, especially for those new to deep learning, PyTorch is faster, more flexible,
    and has better memory optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Another strength of PyTorch is debugging. Keras hides a lot of the nitty-gritty
    details of building a neural network through encapsulation into various functions.
    This means that you can build an artificial neural network with only a few lines
    of code. With PyTorch, you need to specify a lot more details explicitly in your
    code; thus, finding an error becomes a much simpler task. It’s also simpler to
    change weights, biases, and network layers and rerun the model.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, PyTorch is considered easier to use than TensorFlow, but harder to
    use than Keras. TensorFlow’s visualization capabilities are also held in higher
    esteem.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHOOSING A DEEP LEARNING FRAMEWORK**'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Mark Twain, “All generalizations are wrong including this one.”
    Many personal and project-related issues can affect which deep learning framework
    you choose. Still, if you search the internet enough, you can find some general
    guidelines for the selection of a deep learning framework:'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re brand new to deep learning, consider Keras, followed by PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re new and part of a research community, consider PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re an experienced researcher, you’ll probably prefer PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers wanting a quick plug-and-play framework will prefer Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re experienced and want an industry job, consider TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re working with large datasets and need speed and performance, choose
    either PyTorch or TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If debugging is a concern, use PyTorch, as standard Python debuggers can be
    used (though with Keras, debugging is seldom needed due to the simple interface).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need multiple backend support, choose Keras or TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras and TensorFlow provide more deployment options and simplify model export
    to the web; with PyTorch you must use Flask or Django as the backend server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For fast prototyping, use Keras, followed by PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If visualization is a priority, choose Keras or TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re already working with Keras or TensorFlow, use Keras for deep neural
    networks and TensorFlow for machine learning applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Computer Vision Libraries**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Computer vision is a branch of artificial intelligence focused on training computers
    to see and process digital images and videos in much the same way as human vision.
    The goal is for computers to gain a high-level understanding of the state of the
    world from images and return appropriate outputs. For example, a self-driving
    car should detect when you’ve drifted out of your lane and either warn you or
    automatically steer the car back. This requires detecting, tracking, and classifying
    features in images. In addition to autonomous cars, common applications include
    face detection and recognition, skin cancer diagnoses, event detection, and camera
    autofocusing.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few Python libraries dedicated to computer vision and image
    manipulation, but three, OpenCV, scikit-image, and Pillow, should easily cover
    most of your needs. Let’s take a quick look at these in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '***OpenCV***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*OpenCV*, short for *Open-Source Computer Vision*, is the world’s most popular
    open source computer vision library. Its key focus is on real-time applications,
    like identifying faces in streaming video, but it can do everything from simple
    image editing to machine learning applications. OpenCV is written in C++ for speed
    but has a Python wrapper that works on Windows, Linux, Android, and macOS.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV has a modular structure that includes thousands of optimized algorithms,
    including ones for simple image processing, video analysis, 2D feature framework,
    object detection, object tracking, camera calibration, 3D reconstruction and more.
    OpenCV converts images into efficient NumPy arrays and, because it’s written in
    optimized C/C++, it can take advantage of fast multicore processing.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV has been around for more than 20 years and has a large and supportive
    user base. Many major companies such as Google, Yahoo, Microsoft, Intel, IBM,
    Sony, and Honda actively use OpenCV. Thanks to its maturity and popularity, you
    can find many books and online tutorials to help you use the library.
  prefs: []
  type: TYPE_NORMAL
- en: '***scikit-image***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The open-source *scikit-image* library is the image processing toolbox for SciPy.
    Its mission is to be *the* reference library for scientific image analysis in
    Python. It comes preinstalled with Anaconda.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-image library includes lots of algorithms and utilities for use in
    industry, research, and education. It’s written in Python and, like OpenCV, uses
    NumPy arrays as image objects by transforming the original pictures. Although
    it lacks some of the sophisticated OpenCV algorithms for working with images in
    real time, it still has a lot of algorithms useful for scientists, including feature
    and blob detection. It also contains a few algorithm implementations that OpenCV
    does not.
  prefs: []
  type: TYPE_NORMAL
- en: The library is fairly easy to use and well documented with lots of examples
    and use cases. All of the code is peer reviewed and of high quality. It provides
    a consistent interface to many machine-learning models, making it relatively easy
    to learn a new model. It also provides many options—with sensible defaults—for
    tuning the models for optimal performance. You can find a gallery of examples
    at *[https://scikit-image.org/docs/stable/auto_examples/](https://scikit-image.org/docs/stable/auto_examples/)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***PIL/Pillow***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Pillow* is the “friendly” fork of the *Python Image Library (PIL)*, one of
    the oldest core libraries for image manipulation in Python. Pillow runs on all
    major operating systems, comes preinstalled on Anaconda, and is primarily designed
    for basic image processing.'
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t need functionality from OpenCV or scikit-image, Pillow is widely
    used for image transformations in web projects given that it is more lightweight
    and usable. It supports a large selection of image file types and predefined image
    enhancement filters for sharpening, blurring, contouring, smoothing, finding edges,
    resizing, manipulating pixels, and more. It’s especially useful for automatically
    processing large numbers of images.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHOOSING AN IMAGE MANIPULATION LIBRARY**'
  prefs: []
  type: TYPE_NORMAL
- en: Here are some tips for choosing a library for manipulating images. Only open
    source libraries are considered.
  prefs: []
  type: TYPE_NORMAL
- en: If your job or research involves computer-vision applications in *real time*,
    you’ll want to learn OpenCV.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your datasets include a mixture of static images and streaming video, you
    should consider both OpenCV and scikit-image. Some of the latter’s methods and
    utilities can complement OpenCV. For a short example of how these two can work
    together, visit Adrian Rosebrock’s tutorial on detecting low-contrast images (*[https://www.pyimagesearch.com/2021/01/25/detecting-low-contrast-images-with-opencv-scikit-image-and-python/](https://www.pyimagesearch.com/2021/01/25/detecting-low-contrast-images-with-opencv-scikit-image-and-python/)*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you mainly work with static images, scikit-image or Pillow should suffice
    and save you all the “overhead” of OpenCV. Between the two, scikit-image will
    be more appropriate if you regularly work with images and perform fairly sophisticated
    analyses and manipulations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For basic image manipulation, such as loading images, cropping images, or simple
    filtering, Pillow should be sufficient. Likewise, you can realize a large number
    of simple operations directly within NumPy and SciPy’s `ndimage` module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Natural Language Processing Libraries**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Natural language processing (NLP)* is a branch of linguistics and artificial
    intelligence concerned with giving computers the ability to derive meaning from
    written and spoken words. Some familiar applications of NLP include speech recognition;
    text-to-speech conversion; machine translations; chatbots; spam detection; word
    segmentation (called tokenization); sentiment analysis, optical character recognition
    (OCR), in which an image of handwriting or printed text is converted into digital
    text; and, of course, Amazon’s Alexa.'
  prefs: []
  type: TYPE_NORMAL
- en: Among the more popular NLP libraries are NLTK, spaCy, Gensim, Pattern, and TextBlob.
    NLTK and spaCy are all-purpose NLP libraries and are discussed in more detail
    in the sections that follow. Others, like Gensim, are more specialized and focus
    on subdisciplines such as semantic analysis (detecting the meaning of words),
    topic modeling (determining a document’s meaning based on word statistics), and
    text mining.
  prefs: []
  type: TYPE_NORMAL
- en: '***NLTK***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Natural Language Tool Kit*, or *NLTK* for short, is one of the oldest,
    most powerful, and most popular NLP libraries for Python. NLTK is open source
    and works on Windows, macOS, and Linux. Created in 2001 as part of a computational
    linguistics course at the University of Pennsylvania, it has continued to develop
    and expand with the help of dozens of contributors. NLTK comes preinstalled on
    Anaconda.
  prefs: []
  type: TYPE_NORMAL
- en: Because it’s designed by and for an academic research audience, NLTK is versatile
    but can be somewhat slow for quick-paced production usage. It’s also considered
    a bit difficult to learn, though this is mitigated to a fair degree by the free
    and useful online textbook, *Natural Language Processing with Python* (*[http://www.nltk.org/book/](http://www.nltk.org/book/)*),
    written by its developers.
  prefs: []
  type: TYPE_NORMAL
- en: A strength of NLTK is that it comes packaged with lots of corpora (bodies of
    text) and pretrained models. As a result, it can be considered the de facto standard
    library for academic researchers in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: '***spaCy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *spaCy* library is younger than NLTK and designed to work well with machine
    learning frameworks like scikit-learn, TensorFlow, PyTorch, and other NLP libraries
    like Gensim. It’s advertised as being “industrial strength,” meaning that it’s
    scalable, optimized, and very fast for production applications. Like NLTK, it
    has great documentation and comes prepackaged with useful language models. Its
    support community is not as large as that for NLTK but it’s growing rapidly and
    may someday overtake NLTK in popularity.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHOOSING AN NLP LIBRARY**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are dozens of libraries in the NLP stack, you need to know only
    a few to be proficient in the field. Some guidelines for choosing an NLP library
    are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re in academia or otherwise doing research, you’ll probably want to take
    the time to learn NLTK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The spaCy library will be useful for mixing NLP with machine learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need highly optimized performance, consider spaCy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all you plan to do is scrape websites and analyze the results, consider *Pattern*
    (*[https://github.com/clips/pattern/](https://github.com/clips/pattern/)*), a
    specialized web miner with basic NLP capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re a beginner or plan to use NLP lightly in your work, consider *TextBlob*
    (*[https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/)*).
    TextBlob is a user-friendly frontend to the NLTK and Pattern libraries, wrapping
    both in high-level, easy-to-use interfaces. It’s good for learning and for quick
    prototyping, and as you become more confident, you can add functionality to refine
    your prototypes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re into topic modeling and statistical semantics (analyzing and scoring
    documents on their similarity), you may want to consider *Gensim* (*[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)*).
    Gensim can handle very large file sizes by streaming documents to its analysis
    engine and performing unsupervised learning on them incrementally. Its memory
    optimization and fast processing speed are achieved through the use of the NumPy
    library. Gensim is a specialized tool, and not for general-purpose NLP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to perform NLP on multiple languages at once, consider *Polyglot
    ([https://polyglot.readthedocs.io/en/latest/index.html](https://polyglot.readthedocs.io/en/latest/index.html)*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Helper Libraries**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Helper libraries* assist you in using the scientific libraries discussed in
    this chapter. The ones discussed here help you download data, prepare it for use,
    and analyze it as quickly as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Requests***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Data wrangling* (or *munging*) refers to the process of transforming data
    from its “raw” form into a more usable format for analysis. This involves processes
    such as checking, correcting, remapping, and so on. You can do a lot of this with
    the pandas library, discussed previously, but first you need to get your hands
    on the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the lion’s share of human knowledge is available online, you’re probably
    going to need a way to pull data off the World Wide Web. Note that I’m not talking
    about simply downloading an Excel spreadsheet from an online database, which is
    easy enough, or about manually copying and pasting text from a web page. I’m referring
    to automatically extracting and processing content, a process called *web scraping*.
    Let’s look at two open source libraries to help with this, requests and Beautiful
    Soup, and a third, re, that helps you clean and correct the data.
  prefs: []
  type: TYPE_NORMAL
- en: The popular and trusted requests library is designed to make *HyperText Transfer
    Protocol (HTTP)* requests simpler and more human friendly. HTTP is the foundation
    of data communication for the World Wide Web, where hypertext documents include
    hyperlinks to other resources that users can easily access with, for example,
    a mouse click or by tapping the screen in a web browser. The requests library
    comes preinstalled with Anaconda.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example where you scrape Dr. Martin Luthor King, Jr.’s “I
    Have a Dream” speech from a website (*[http://www.analytictech.com/mb021/mlk.htm](http://www.analytictech.com/mb021/mlk.htm)*)
    using Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After importing requests, you provide the `url` address as a string. You can
    copy and paste this from the website from which you want to extract text. The
    requests library abstracts the complexities of making HTTP requests in Python.
    The `get()` method retrieves the `url` and assigns the output to a `page` variable,
    which references the `Response` object the web page returned for the request.
    This object’s text attribute holds the web page, including the speech, as a readable
    text string.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the data is in *HyperText Markup Language (HTML)*, the standard
    format used to create web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, HTML has a lot of *tags* such as `<head>` and `<p>` that let
    your browser know how to format the web page. The text between starting and closing
    tags is called an *element*. For example, the text “Martin Luther King Jr.’s 1962
    Speech” is a title element sandwiched between the starting tag `<title>` and the
    closing tag `</title>`. Paragraphs are formatted using `<p>` and `</p>` tags.
  prefs: []
  type: TYPE_NORMAL
- en: Because these tags are not part of the original text, they should be removed
    prior to any further analysis, such as natural language processing. To remove
    the tags, you’ll need the Beautiful Soup library.
  prefs: []
  type: TYPE_NORMAL
- en: '***Beautiful Soup***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Beautiful Soup* is an open source Python library for extracting readable data
    from HTML and XML files. It comes preinstalled with Anaconda.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use Beautiful Soup (simplified as *bs4*) on the HTML file returned by
    requests in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing bs4, we call the `bs4.BeautifulSoup()` method and pass it the
    string containing the HTML. The `soup` variable now references a `BeautifulSoup`
    object, which means that you can use the `find_all()` method to locate the speech
    buried in the HTML document between paragraph tags (`<p>`). This makes a list,
    which you can turn into a continuous text string by joining the paragraph elements
    on a space (`'' ''`). The (truncated) printed results follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You now have text that you can easily read as well as analyze with Python’s
    many language processing tools.
  prefs: []
  type: TYPE_NORMAL
- en: '***Regex***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: No matter where you get your raw data, it will probably contain spelling errors,
    formatting issues, missing values, and other problems that will keep you from
    using it immediately. You’ll need to process it in some way, such as reformatting,
    replacing, or removing certain parts, and you’ll want to do it in *bulk*. Fortunately,
    *regular expressions* provide you with a wide variety of tools for parsing raw
    text and performing these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A regular (or *rational*) expression, usually shortened to *regex*, is a sequence
    of characters that specifies a *search pattern*. You’re probably familiar with
    these patterns if you’ve ever used “find” or “find and replace” operations in
    a text editor. Using pattern matching, regex helps you to isolate and extract
    text you want from text you don’t want.
  prefs: []
  type: TYPE_NORMAL
- en: Regex can do tedious but important things that you’d normally assign to an assistant
    or technician. For example, it can scan text for information related to your field
    of study. If you’re a seismologist interested in earthquakes, you can write programs
    that scan news feeds for reports on these events, grab the data, format it, and
    store it in a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python has a built-in module called re which you can use to work with regular
    expressions. Let’s look at an example in which you’re searching texts for 10-digit
    phone numbers. Within this database, people have entered phone numbers in multiple
    ways, such as with the area code in parentheses, using dashes, using spaces, and
    so on, but you want to use 10 consecutive numbers with no spaces. Here’s how re
    and Python can help you to extract and format the numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing re and entering the data, you assign a variable, named `nums`,
    and call the `re.findall()` method. This diabolical syntax looks like some kind
    of code, pun intended, and like any code you must know the key. Without going
    into the gory details, you’re basically telling the `findall()` method the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The matched text string might start with the `(`symbol or the number one to
    nine `[\(]?[1-9].`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be a number, space, period, dash, or parentheses in between `[0-9\
    \.\-\(\)].`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matched string must contain at least 10 characters `{10,}.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it must end with a number between zero and nine `[0-9].`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This first attempt finds all the input numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you need to remove the non-number characters using the `re.sub()` method,
    which substitutes a character you provide for the targeted characters. The `^`
    tells the method to find everything but digits between zero and nine and replace
    them with nothing, signified by `''''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a continuous string of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now use list comprehension to loop through this string and extract
    the 10-digit groupings you desire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields a list of numbers (as strings) in the format you desire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This simple example demonstrates both the power of regex and the daunting nature
    of its syntax. In fact, regex is probably the most unPythonic thing in Python.
    Fortunately, because just about everyone struggles with the syntax, there are
    a lot of tools, tutorials, books, and cheat sheets available to help you use it.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a nice “how to” tutorial at *[https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html)*
    and at *[https://realpython.com/regex-python/](https://realpython.com/regex-python/)*.
    [Chapter 7](ch07.xhtml) of Al Sweigart’s book, *Automate the Boring Stuff with
    Python*, 2nd edition (No Starch Press, 2019), provides a high-level overview of
    pattern matching with regular expressions, and Jeffrey Friedl’s *Mastering Regular
    Expressions* (O’Reilly, 2006) covers them in-depth. You can find cheat sheets
    with examples in many places online, including *[https://learnbyexample.github.io/python-regex-cheatsheet/](https://learnbyexample.github.io/python-regex-cheatsheet/)*.
    Other websites such as *[https://regexr.com/](https://regexr.com/)* and *[https://www.regexpal.com/](https://www.regexpal.com/)*
    let you play with regular expressions to learn how they work.
  prefs: []
  type: TYPE_NORMAL
- en: If you have to wrangle a lot of text, regular expressions will dramatically
    reduce the amount of code that you need to write, saving you both time and frustration.
    With a little effort, you’ll achieve complete mastery over your data, solve problems,
    and automate things you probably never realized could be automated.
  prefs: []
  type: TYPE_NORMAL
- en: '***Dask***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Dask* is an open source library for parallel computing written in Python.
    It was developed to scale Python ecosystem libraries such as pandas, NumPy, scikit-learn,
    Matplotlib, Jupyter Notebook, and so on, from a single computer to multicore machines
    and distributed clusters. Dask comes preinstalled on Anaconda.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand the benefits Dask provides, let’s talk terminology for a moment.
    A *thread* is the smallest sequence of programmed instructions that can be managed
    independently by a scheduler. *Parallel processing* refers to dividing different
    parts of a computing task—the threads—among two or more processors for the purpose
    of accelerating program execution.
  prefs: []
  type: TYPE_NORMAL
- en: In the old days, the central processing unit (CPU) of computers had a single
    microprocessor, or *core*, that executed code one step at a time, like an army
    marching in single file. Nowadays, computers come with at least a dual-core CPU
    consisting of a chip with two complete microprocessors that share a single path
    to memory and peripherals. High-end workstations can have eight or more cores.
    So, theoretically, your programs no longer need to walk single file; they can
    run abreast. That is, if there are non-dependent threads, they can run simultaneously,
    saving lots of time.
  prefs: []
  type: TYPE_NORMAL
- en: But Python has limitations when it comes to parallel computing. Even though
    computers now have more than one CPU, Python uses *Global Interpreter Lock (GIL)*
    to boost the performance of single threads by encouraging only a single thread
    to execute at a time. This hinders the use of multiple CPU cores for speedier
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: With Dask, you can use Python and perform parallel computations locally on a
    multicored machine or remotely across thousands of machines. And Dask does it
    efficiently, as well, by managing memory at the same time. To maintain a low-memory
    footprint, it stores large datasets on disk and copies off chunks of data for
    processing. It also discards intermediate values as soon as possible after they
    are generated. As a result, Dask permits manipulation of datasets of 100GB and
    larger on laptops, and larger than 1TB on workstations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dask is composed of two parts: distributed data structures, with APIs similar
    to pandas DataFrames and NumPy arrays, and a task grapher/scheduler ([Figure 15-4](ch15.xhtml#ch015fig4)).
    It implements many of the same methods as pandas, which means that it can fully
    replace it in many cases. Dask also offers NumPy and scikit-learn replacements
    and has the capability to scale *any* Python code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/15fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: Dask collections generate graphs that are executed by schedulers
    (courtesy of [https://dask.org/](https://dask.org/)).*'
  prefs: []
  type: TYPE_NORMAL
- en: Dask will add extra complexity to your projects, thus you should use it mainly
    when you have huge datasets and need to use cluster computing. Documentation for
    Dask is excellent, and there are many tutorials available online to help you use
    the library.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You might hear about Apache Spark, a more mature and “heavier” alternative
    to Dask that has become a dominant and well-trusted tool in the Big Data enterprise
    world. It’s an all-in-one project that has its own ecosystem and is written in
    Scala with some support for Python. You can find a comparison of the two libraries
    at [https://docs.dask.org/en/latest/spark.html](https://docs.dask.org/en/latest/spark.html).
    In general, if you’re already using Python and associated libraries like NumPy
    and pandas, you’ll probably prefer working with Dask.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python supports scientific work through its easy-to-use core language and the
    many libraries built upon it. Not only are these packages free, but they’re also
    robust, reliable, and well documented, thanks to the enormous and active user
    community.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about some of the most important and popular libraries for science,
    including NumPy and SciPy, for numerical and array calculations; pandas, for data
    analysis; scikit-learn, for machine learning; Tensorflow, Keras, and PyTorch,
    for neural networks; OpenCV, for computer vision; and NLTK, for language processing.
    In [Chapter 16](ch16.xhtml) and [Chapter 17](ch17.xhtml), we cover libraries for
    working with geographic data and creating visualizations. In [Chapters 18](ch18.xhtml),
    [19](ch19.xhtml), [20](ch20.xhtml), and [21](ch21.xhtml), we dive deeper into
    NumPy, Matplotlib, pandas, and scikit-learn.
  prefs: []
  type: TYPE_NORMAL
