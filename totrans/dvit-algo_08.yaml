- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Machine Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习
- en: '![](Images/circleart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/circleart.png)'
- en: 'Now that you understand the ideas behind many fundamental algorithms, we can
    turn to more advanced ideas. In this chapter, we explore machine learning. *Machine
    learning* refers to a broad range of methods, but they all share the same goal:
    finding patterns in data and using them to make predictions. We’ll discuss a method
    called *decision trees* and then build one that can predict a person’s level of
    happiness based on some of their personal characteristics.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了许多基础算法背后的思想，我们可以转向更高级的概念。本章我们将探索机器学习。*机器学习*指的是一系列广泛的方法，但它们都有一个共同的目标：从数据中发现模式并利用这些模式进行预测。我们将讨论一种名为*决策树*的方法，并构建一个能够根据个人的一些特征预测一个人幸福感水平的模型。
- en: Decision Trees
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are diagrams that have a branching structure resembling a tree.
    We can use decision trees in the same way we use flowcharts—by answering yes/no
    questions, we are guided along a path that leads to a final decision, prediction,
    or recommendation. The process of creating a decision tree that leads to optimal
    decisions is a paradigmatic example of a machine learning algorithm.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是具有分支结构、类似树形的图表。我们可以像使用流程图一样使用决策树——通过回答是/否问题，我们将沿着一条路径走向最终的决定、预测或建议。创建一个能够做出最佳决策的决策树的过程，是机器学习算法的典型例子。
- en: 'Let’s consider a real-world scenario in which we might use decision trees.
    In emergency rooms, an important decision-maker must perform triage for every
    newly admitted patient. *Triage* simply means assigning priority: someone who
    is minutes from death but can be saved by a timely operation will be admitted
    to treatment immediately, whereas someone who has a paper cut or a mild case of
    sniffles will be asked to wait until more urgent cases can be cleared up.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个现实场景，看看我们如何使用决策树。在急诊室，重要的决策者必须为每一位新入院的患者进行分诊。*分诊*意味着分配优先级：那些即将死亡但通过及时手术可以挽救的人将立即接受治疗，而那些只是纸割伤或轻微流感症状的人则会被要求等到更紧急的病例处理完再进行治疗。
- en: 'Triage is difficult because you have to make a reasonably accurate diagnosis
    with very little information or time. If a 50-year-old woman comes to the emergency
    room and complains of bad chest pain, the person in charge of triage has to decide
    whether her pain is more likely to be heartburn or a heart attack. The thought
    process of a person who makes triage decisions is necessarily complex. They’ll
    take into account a number of factors: the age and sex of the patient, whether
    they are obese or a smoker, the symptoms they report and the way they talk about
    them, the expression on their face, how busy the hospital is and what other patients
    are waiting for treatment, and factors that they may not even be consciously aware
    of. In order to become good at triage, a person has to learn many patterns.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分诊很困难，因为你必须在极短的时间或信息有限的情况下做出相对准确的诊断。如果一位50岁的女性来到急诊室，抱怨胸口剧烈疼痛，负责分诊的人必须决定她的疼痛是更可能是胃灼热还是心脏病发作。做出分诊决策的人的思维过程必然是复杂的。他们会考虑许多因素：患者的年龄和性别，是否肥胖或吸烟，报告的症状以及她们描述这些症状的方式，面部表情，医院的忙碌程度以及其他等待治疗的患者，甚至可能是她们没有意识到的因素。为了成为一名优秀的分诊员，必须学习许多模式。
- en: Understanding the way a triage professional makes a decision is not easy. [Figure
    9-1](#figure9-1) shows a hypothetical, totally made-up triage decision process
    (not meant as medical advice—don’t try this at home!).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理解一个分诊专业人员做决定的方式并不容易。[图 9-1](#figure9-1)展示了一个假设的、完全虚构的分诊决策过程（不作为医疗建议——请不要在家尝试！）。
- en: '![Figure_9-1](Images/figure_9-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![Figure_9-1](Images/figure_9-1.png)'
- en: '[Figure 9-1:](#figureanchor9-1) A simplified decision tree for heart attack
    triage'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1:](#figureanchor9-1) 简化版心脏病发作分诊决策树'
- en: You can read this diagram from top to bottom. At the top, we can see that the
    heart-attack diagnosis process begins with a patient reporting chest pain. After
    that, the process branches out depending on the sex of the patient. If the patient
    is a man, the diagnosis process continues in the left branch and we determine
    whether he is obese. If the patient is a woman, the process continues in the right
    branch instead, and we determine whether she is a smoker. At each point in the
    process, we follow the appropriate branch until we reach the bottom of the tree,
    where we find the tree’s classification of whether the patient is at high risk
    or low risk for a heart attack. This binary branching process resembles a tree
    whose trunk branches into smaller offshoots until reaching the ends of the farthest
    branches. Accordingly, the decision process illustrated in [Figure 9-1](#figure9-1)
    is called a decision tree*.*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从上到下阅读这个图表。在顶部，我们可以看到心脏病发作诊断过程从患者报告胸痛开始。之后，诊断过程根据患者的性别进行分支。如果患者是男性，诊断过程会沿左侧分支继续，我们会判断他是否肥胖。如果患者是女性，过程则沿右侧分支继续，我们会判断她是否吸烟。在每个步骤中，我们按照适当的分支进行，直到到达树的底部，在那里我们可以看到树的分类，判断患者是否处于高风险或低风险的心脏病发作状态。这个二叉分支过程类似于一棵树的结构，树干分支出越来越小的枝条，直到达到最远的分支。因此，图中展示的决策过程[图
    9-1](#figure9-1)被称为决策树*。
- en: Every place you see text in [Figure 9-1](#figure9-1) is a *node* of the decision
    tree. A node like “Not obese” is known as a *branching node* because there’s at
    least one more branch to follow before we’re able to make a prediction. The “Not
    diabetic = Low risk” node is a *terminal node* because if we’ve arrived there,
    we don’t need to branch anymore and we know the decision tree’s final classification
    (“Low risk”).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个你在[图 9-1](#figure9-1)中看到的文本都是决策树的*节点*。像“非肥胖”这样的节点被称为*分支节点*，因为在我们能够做出预测之前，至少还有一个分支需要跟随。“非糖尿病=低风险”节点是*终端节点*，因为一旦我们到达那里，就不再需要分支，我们已经知道决策树的最终分类（“低风险”）。
- en: 'If we could design a thorough, well-researched decision tree that always led
    to good triage decisions, it’s possible that someone without medical training
    could perform triage of heart attack patients, which would save every emergency
    room in the world plenty of money because they would no longer need to hire and
    train judicious, highly educated triage professionals. A sufficiently good decision
    tree could even make it possible to replace human triage professionals with robots,
    though whether that’s a good goal is debatable. A good decision tree may even
    lead to better decisions than the average human would make, since it could potentially
    eliminate the unconscious biases that we fallible humans possess. (And in fact,
    this has already happened: in 1996 and 2002, separate teams of researchers published
    papers about their success improving triage results for patients complaining of
    chest pain by using decision trees.)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够设计出一棵完善的、经过充分研究的决策树，始终能够做出正确的分诊决策，那么没有医学培训的人也有可能对心脏病发作的患者进行分诊，这将为全球所有的急诊室节省大量资金，因为他们将不再需要雇佣和培训那些谨慎且受过高等教育的分诊专家。一棵足够优秀的决策树甚至可能使机器人取代人工分诊专家，尽管这是否是一个好的目标仍有争议。一棵优秀的决策树甚至可能做出比普通人更好的决策，因为它有可能消除我们这些易犯错误的人的潜意识偏见。（事实上，这已经发生过：1996年和2002年，两个不同的研究团队分别发表了论文，讲述他们通过使用决策树改善胸痛患者分诊结果的成功经验。）
- en: 'The branching decision steps described in a decision tree constitute an algorithm.
    Executing such an algorithm is very simple: just decide which of the two branches
    you should be on at every node, and follow the branches to the end. But don’t
    obey the suggestions of every decision tree you encounter. Remember that anyone
    can make a decision tree that prescribes any conceivable decision process, even
    if it leads to wrong decisions. The hard part of decision trees is not executing
    the decision tree algorithm but designing the decision tree so that it leads to
    the best possible decisions. Creating an optimal decision tree is an application
    of machine learning, though merely following a decision tree is not. Let’s discuss
    the algorithm that creates an optimal decision tree—an algorithm to generate an
    algorithm—and proceed through the steps of the process to generate an accurate
    decision tree.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树中描述的分支决策步骤构成了一个算法。执行这样的算法非常简单：只需在每个节点上决定应该选择哪一条分支，然后沿着分支走到终点。但不要盲目遵循你遇到的每一棵决策树。记住，任何人都可以创建一个描述任何可想象的决策过程的决策树，即使它导致错误的决策。决策树的难点不在于执行决策树算法，而在于设计决策树，使其能够得出最佳决策。创建一个最优的决策树是机器学习的应用，虽然仅仅遵循一棵决策树并不是机器学习。让我们讨论创建最优决策树的算法——一种生成算法的算法——并通过步骤生成一个准确的决策树。
- en: Building a Decision Tree
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树
- en: Let’s build a decision tree that uses information about a person to predict
    how happy they are. Finding the secret of happiness has preoccupied millions of
    people for millennia, and social science researchers today spill plenty of ink
    (and burn through plenty of research grants) pursuing the answers. If we had a
    decision tree that could use a few pieces of information and reliably predict
    how happy a person is, it would give us important clues about what determines
    a person’s happiness, and maybe even some ideas about how to achieve it ourselves.
    By the end of this chapter, you’ll know how to build such a decision tree.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个决策树，利用关于一个人的信息来预测他们的幸福感。几千年来，找出幸福的秘密一直困扰着数百万的人，而今天的社会科学研究人员则倾尽笔墨（并消耗大量的研究经费）来追寻答案。如果我们有一个决策树，能够利用几条信息可靠地预测一个人的幸福感，这将为我们提供关于决定一个人幸福的关键线索，甚至可能给我们一些关于如何实现幸福的启示。在本章结束时，你将学会如何构建这样的决策树。
- en: Downloading Our Dataset
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载我们的数据集
- en: Machine learning algorithms find useful patterns in data, so they require a
    good dataset. We’ll use data from the European Social Survey (ESS) for our decision
    tree. You can download the files we’ll use from *http://bradfordtuckfield.com/ess.csv*
    and *http://bradfordtuckfield.com/variables.csv*. (We got our files originally
    from *https://www.kaggle.com/pascalbliem/european-social-survey-ess-8-ed21-201617*,
    where they’re publicly available for free). The ESS is a large-scale survey of
    adults across Europe that is conducted every two years. It asks a wide variety
    of personal questions, including religious affiliation, health status, social
    life, and level of happiness. The files we’ll look at are stored in *CSV* format.
    The file extension *.csv* is short for *comma-separated* values, and it’s a very
    common and simple way to store datasets so that they can be opened by Microsoft
    Excel, LibreOffice Calc, text editors, and some Python modules.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法从数据中发现有用的模式，因此它们需要一个好的数据集。我们将使用来自欧洲社会调查（ESS）的数据来构建我们的决策树。你可以从*http://bradfordtuckfield.com/ess.csv*
    和 *http://bradfordtuckfield.com/variables.csv* 下载我们将使用的文件。（我们最初是从*https://www.kaggle.com/pascalbliem/european-social-survey-ess-8-ed21-201617*获取的这些文件，那里它们是公开免费提供的）。ESS是一个大规模的欧洲成年人调查，每两年进行一次。它提出了各种各样的个人问题，包括宗教信仰、健康状况、社交生活和幸福感水平。我们将要查看的文件是以*CSV*格式存储的。文件扩展名*.csv*是*逗号分隔*值的缩写，它是一种非常常见且简单的存储数据集的方式，可以通过Microsoft
    Excel、LibreOffice Calc、文本编辑器和一些Python模块打开。
- en: The file *variables.csv* contains a detailed description of each question recorded
    in the survey. For example, in line 103 of *variables.csv*, we can see a description
    of a variable called `happy`. This variable records a survey-taker’s answer to
    the question “Taking all things together, how happy would you say you are?” The
    answers to this question range from 1 (not happy at all) to 10 (extremely happy).
    Look at the other variables in *variables.csv* to see the variety of information
    available to us. For example, the variable `sclmeet` records how often respondents
    meet socially with friends, relatives, or colleagues. The variable `health` records
    subjective general health. The variable `rlgdgr` records a subjective rating of
    how religious respondents are, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文件*variables.csv*包含了调查中每个问题的详细描述。例如，在*variables.csv*的第103行，我们可以看到一个名为`happy`的变量的描述。这个变量记录了受访者对问题“综合来看，你认为自己有多幸福？”的回答。这个问题的回答范围从1（一点也不幸福）到10（非常幸福）。查看*variables.csv*中的其他变量，了解可供我们使用的信息种类。例如，变量`sclmeet`记录了受访者与朋友、亲戚或同事社交的频率。变量`health`记录了受访者的主观健康状况。变量`rlgdgr`记录了受访者的宗教信仰程度，等等。
- en: After seeing our data, we can start to think of hypotheses related to happiness
    predictions. We might reasonably suppose that people who have active social lives
    and good health are happier than others. Other variables—like gender, household
    size, and age—may be less easy to hypothesize about.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看了数据后，我们可以开始考虑与幸福预测相关的假设。我们可能合理地假设，拥有积极社交生活和良好健康的人比其他人更幸福。其他变量——如性别、家庭规模和年龄——可能不太容易形成假设。
- en: Looking at the Data
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看数据
- en: 'Let’s start by reading in the data. Download the data from the link and save
    it locally as *ess.csv*. Then we can use the `pandas` module to work with it,
    storing it in our Python session in a variable called `ess`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从读取数据开始。下载数据并将其保存为本地文件*ess.csv*。然后，我们可以使用`pandas`模块来操作数据，并将其存储在一个名为`ess`的Python变量中：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Remember, in order to read the CSV file, you’ll have to be storing it in the
    same place as you’re running Python from, or you’ll have to change `''ess.csv''`
    in the previous snippet to reflect the exact filepath where you’re storing the
    CSV file. We can use the `shape` attribute of a `pandas` dataframe to see how
    many rows and columns are in our data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，为了读取CSV文件，你必须确保它和Python程序运行的路径在同一目录下，或者需要修改之前代码中的`'ess.csv'`，使其反映你存储CSV文件的准确路径。我们可以使用`pandas`数据框的`shape`属性来查看数据的行数和列数：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output should be `(44387, 534)`, indicating that our dataset has 44,387
    rows (one for each respondent) and 534 columns (one for each question in the survey).
    We can look more closely at some of the columns that interest us by using the
    `pandas` module’s slicing functions. For example, here’s how we look at the first
    five answers to the “happy” question:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是`(44387, 534)`，这表示我们的数据集有44,387行（每一行代表一个受访者）和534列（每一列代表调查中的一个问题）。我们可以通过使用`pandas`模块的切片功能，进一步查看我们感兴趣的某些列。例如，以下是如何查看“happy”问题的前五个回答：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our dataset, `ess`, has 534 columns, one for each question in the survey. For
    some purposes, we may want to work with all 534 columns at once. Here, we want
    to look only at the `happy` column, not the other 533\. That’s why we used the
    `loc()` function. Here, the `loc()` function has sliced the variable called `happy`
    from the `pandas` dataframe. In other words, it takes out only that column and
    ignores the other 533\. Then, the `head()` function shows us the first five rows
    of that column. You can see that the first five responses are `5`, `5`, `8`, `8`,
    and `5`. We can do the same with the `sclmeet` variable:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集`ess`有534列，每一列代表调查中的一个问题。出于某些目的，我们可能希望一次性处理所有534列。在这里，我们只想查看`happy`这一列，而不是其他533列。这就是我们使用`loc()`函数的原因。在这里，`loc()`函数从`pandas`数据框中提取了名为`happy`的变量。换句话说，它只取出了这一列，并忽略了其他533列。然后，`head()`函数展示了该列的前五行数据。你可以看到前五个回答分别是`5`、`5`、`8`、`8`和`5`。我们也可以对`sclmeet`变量做同样的操作：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The result should be `6`, `4`, `4`, `4`, and `6`. The `happy` responses and
    the `sclmeet` responses will line up in order. For example, the 134th element
    of `sclmeet` is a response given by the same person who gave the response in the
    134th element of `happy`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是`6`、`4`、`4`、`4`和`6`。`happy`的回答和`sclmeet`的回答会一一对应。例如，`sclmeet`的第134个元素是由与`happy`的第134个元素相同的受访者给出的回答。
- en: 'The ESS staff strives to get a complete set of responses from every survey
    participant. However, there are some cases where responses to some survey questions
    are missing, sometimes because a participant either refuses to answer or doesn’t
    know how to answer. Missing responses in the ESS dataset are assigned codes that
    are much higher than the possible range of real responses. For example, on a question
    that asks a respondent to choose a number on a scale from 1 to 10, the ESS records
    a 77 response if the respondent refuses to answer. For our analysis, we’ll consider
    only responses that are complete, with no missing values for variables that interest
    us. We can restrict the `ess` data so that it contains only full responses for
    the variables we care about as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ESS工作人员努力从每位调查参与者那里收集完整的回应。然而，也有一些情况下，调查问题的回答缺失，可能是因为参与者拒绝回答或者不知道如何回答。在ESS数据集中，缺失的回应会被赋予一个高于实际回答范围的代码。例如，如果问题要求受访者在1到10的范围内选择一个数字，而受访者拒绝回答，ESS将记录为77。在我们的分析中，我们只考虑完整的回应，即没有缺失的感兴趣变量的回答。我们可以限制`ess`数据，只保留包含我们关心的完整回应的记录，如下所示：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Splitting Our Data
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 划分我们的数据
- en: 'There are many ways we could use this data to explore the relationship between
    someone’s social life and their happiness. One of the simplest approaches is a
    `binary split`: we compare the happiness levels of people with highly active social
    lives to those of people with less active social lives ([Listing 9-1](#listing9-1)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式使用这些数据来探索个人社交生活与幸福感之间的关系。最简单的方法之一是进行`二元划分`：我们将社交生活非常活跃的人的幸福感与社交生活较少活跃的人的幸福感进行比较（参见[清单
    9-1](#listing9-1)）。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 9-1:](#listinganchor9-1) Calculating the mean happiness levels of
    people with inactive and active social lives'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9-1:](#listinganchor9-1) 计算社交生活不活跃和活跃人群的平均幸福感水平'
- en: 'In [Listing 9-1](#listing9-1), we imported the `numpy` module in order to calculate
    means. We defined two new variables, `social` and `happy`, by slicing them from
    the `ess` dataframe. Then, we used list comprehensions to find the happiness levels
    of all people with lower ratings of social activity (which we saved in the variable
    `low_social_happiness`) and the happiness levels of all people with higher ratings
    of social activity (which we saved in the variable `high_social_happiness`). Finally,
    we calculated the mean happiness rating of unsocial people (`meanlower`) and the
    mean happiness rating of highly social people (`meanhigher`). If you run `print(meanlower)`
    and `print(meanhigher)`, you should see that people who rated themselves as highly
    social also rated themselves as slightly happier than their less socially active
    peers: about `7.8` was the mean happiness level reported by the socially active,
    and about `7.2` was the mean happiness level for the socially inactive.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 9-1](#listing9-1)中，我们导入了`numpy`模块来计算平均值。我们通过从`ess`数据框中切片定义了两个新变量，`social`和`happy`。然后，我们使用列表推导式来找到所有社交活动评分较低的人的幸福感（我们将其保存在变量`low_social_happiness`中），以及所有社交活动评分较高的人的幸福感（我们将其保存在变量`high_social_happiness`中）。最后，我们计算了不活跃社交人群的平均幸福感（`meanlower`）和高度活跃社交人群的平均幸福感（`meanhigher`）。如果你运行`print(meanlower)`和`print(meanhigher)`，你应该会看到，认为自己社交活跃的人，幸福感评分也略高于那些社交不活跃的人：社交活跃者的平均幸福感为`7.8`，而社交不活跃者的平均幸福感为`7.2`。
- en: We can draw a simple diagram of what we just did, as in [Figure 9-2](#figure9-2).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以画出一个简单的图示，展示我们刚刚做的事情，如[图 9-2](#figure9-2)所示。
- en: '![Figure_9-2](Images/figure_9-2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图_9-2](Images/figure_9-2.png)'
- en: '[Figure 9-2:](#figureanchor9-2) A simple decision tree predicting happiness
    based on frequency of social outings'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2:](#figureanchor9-2) 一个简单的决策树，基于社交活动频率预测幸福感'
- en: 'This diagram of our simple binary split has already started to resemble a decision
    tree. This is not a coincidence: making a binary split in a dataset and comparing
    outcomes in each half is exactly the process at the heart of the decision tree
    generation algorithm. In fact, [Figure 9-2](#figure9-2) can rightfully be called
    a decision tree, albeit one that has only one branching node. We can use [Figure
    9-2](#figure9-2) as a very simple predictor of happiness: we find out how often
    someone goes out socially. If their `sclmeet` value is `5` or less, then we can
    predict that their happiness is `7.2`. If it is higher than `5`, then we can predict
    that their happiness is `7.8`. It will not be a perfect prediction, but it’s a
    start and it’s more accurate than random guessing.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单二元分割的图已经开始类似于决策树。这不是巧合：在数据集上做二元分割并比较每一半的结果，正是决策树生成算法的核心过程。事实上，[图9-2](#figure9-2)可以称之为一个决策树，尽管它只有一个分支节点。我们可以将[图9-2](#figure9-2)作为一个非常简单的幸福感预测器：我们了解某人有多频繁外出社交。如果他们的`sclmeet`值为`5`或更低，那么我们可以预测他们的幸福感为`7.2`。如果高于`5`，则可以预测他们的幸福感为`7.8`。这不是一个完美的预测，但它是一个起点，而且比随机猜测更准确。
- en: We can try to use our decision tree to draw conclusions about the impact of
    various characteristics and lifestyle choices. For example, we see that the difference
    between low social happiness and high social happiness is about 0.6, and we conclude
    that increasing one’s level of social activity from low to high could lead to
    a predicted increase in happiness of about 0.6 on a 10-point scale. Of course,
    trying to draw these sorts of conclusions is fraught with difficulties. It could
    be that social activity does not cause happiness, but rather that happiness causes
    social activity; maybe happy people are more often in the jovial mood that leads
    to ccalling their friends and arranging social meetings. Disentangling correlation
    from causation is beyond the scope of this chapter, but regardless of the direction
    of causation, our simple decision tree has at least given us the fact of the association,
    which we can investigate further if we care to. As cartoonist Randall Munroe put
    it, “Correlation doesn’t imply causation, but it does waggle its eyebrows suggestively
    and gesture furtively while mouthing ‘look over there.’”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用决策树来得出关于各种特征和生活方式选择对幸福感的影响。例如，我们看到低社交幸福感和高社交幸福感之间的差异大约是0.6，我们得出结论：将个人的社交活动水平从低提高到高，可能会使幸福感在10分制上提高大约0.6。当然，尝试得出这类结论充满了困难。也许社交活动并不直接导致幸福感，反而是幸福感促使社交活动；或许快乐的人更容易心情愉悦，从而打电话给朋友并安排社交活动。区分相关性与因果关系超出了本章的讨论范围，但无论因果关系的方向如何，我们简单的决策树至少告诉了我们这个关联的事实，如果我们愿意，还可以进一步研究。正如漫画家Randall
    Munroe所说：“相关性并不意味着因果关系，但它确实用挑眉示意并偷偷做手势，嘴里嘀咕着‘看那边’。”
- en: We know how to make a simple decision tree with two branches. Now we just need
    to perfect how we create branches and then make many of them for a better, more
    complete decision tree.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何创建一个简单的具有两个分支的决策树。现在，我们只需要完善如何创建分支，然后生成更多分支，来构建一个更好、更完整的决策树。
- en: Smarter Splitting
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更智能的分割
- en: When we compared the happiness levels of people with active versus inactive
    social lives, we used `5` as our *split point*, saying that those who were rated
    higher than `5` had an active social life and those who were rated at `5` or below
    had an inactive social life. We chose `5` because it is a natural middle point
    for ratings that go from 1 to 10\. However, remember that our goal is to build
    an accurate predictor of happiness. Rather than splitting based on intuitions
    about what a natural midpoint is or what seems like an active social life, it
    would be best to make our binary split in some place that leads to the best possible
    accuracy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们比较社交活跃与不活跃人群的幸福感时，我们将`5`作为*分割点*，认为那些评分高于`5`的人拥有活跃的社交生活，而评分为`5`或以下的人则拥有不活跃的社交生活。我们选择`5`是因为它是从1到10的评分中一个自然的中点。然而，请记住，我们的目标是构建一个准确的幸福感预测器。与其根据直觉判断什么是自然的中点或什么看起来像是活跃的社交生活，最好是将二元分割点放在能够带来最佳准确度的位置。
- en: 'In machine learning problems, there are a few different ways to measure accuracy.
    The most natural way is to find the sum of our errors. In our case, the error
    that interests us is the difference between our prediction of someone’s happiness
    rating and their actual happiness rating. If our decision tree predicts that your
    happiness is `6` but it’s actually `8`, then that tree’s error for your rating
    is `2`. If we add up the prediction errors for every respondent in some group,
    we can get an error sum that measures the decision tree’s accuracy for predicting
    the happiness of members of that group. The closer we can get our error sum to
    zero, the better our tree is (but please see "The Problem of Overfitting" on page
    179 for important caveats). This snippet shows a simple way to find the error
    sum:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习问题中，有几种不同的方式来衡量准确度。最自然的方式是求出我们所有错误的总和。在我们的例子中，我们关心的错误是我们对某人幸福感评分的预测与他们实际幸福感评分之间的差异。如果我们的决策树预测你的幸福感是`6`，但实际是`8`，那么该树在你的评分上的错误就是`2`。如果我们将某个群体中每个受访者的预测误差加起来，我们就可以得到一个误差总和，来衡量该决策树在预测该群体成员幸福感方面的准确度。我们越能将误差总和接近零，我们的树就越好（但请参见第179页的“过拟合问题”以了解重要的警告）。这个代码片段展示了一种简单的计算误差总和的方法：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code takes the sum of all prediction errors for all respondents. It defines
    `lowererrors`, a list containing the prediction error for each less social respondent,
    and `highererrors`, a list containing the prediction error for each more social
    respondent. Notice that we took the absolute value so that we’re adding only non-negative
    numbers to calculate the error sum. When we run this code, we find that our total
    error is about `60224`. This number is much higher than zero, but if you consider
    that this is a sum of errors for more than 40,000 respondents whose happiness
    we predicted using a tree with only two branches, suddenly it doesn’t seem so
    bad.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码计算了所有受访者的预测误差总和。它定义了`lowererrors`，一个包含每个低社交受访者预测误差的列表，以及`highererrors`，一个包含每个高社交受访者预测误差的列表。注意，我们使用了绝对值，这样我们只加上非负数来计算误差总和。当我们运行这段代码时，我们发现总误差大约是`60224`。这个数字远大于零，但如果考虑到这是一个超过40,000名受访者的错误总和，而且我们用的是一个仅有两个分支的决策树进行幸福感预测，突然间这就不显得那么糟糕了。
- en: We can try different split points to see if our error improves. For example,
    we can classify everyone with a social rating higher than `4` as high social and
    everyone with a social rating of `4` or lower as low social, and compare the resulting
    error rates. Or we could use `6` as our split point instead. In order to get the
    highest possible accuracy, we should check every possible split point in order,
    and choose the split point that leads to the lowest possible error. [Listing 9-2](#listing9-2)
    contains a function that accomplishes this.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试不同的切分点，看看是否能够改进误差。例如，我们可以将社交评分高于`4`的每个人归为高社交，将社交评分为`4`或以下的每个人归为低社交，然后比较得到的误差率。或者，我们也可以选择`6`作为切分点。为了获得最高的准确度，我们应该依次检查每一个可能的切分点，并选择导致最低误差的切分点。[列表9-2](#listing9-2)包含了一个实现此功能的函数。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Listing 9-2:](#listinganchor9-2) A function that finds the best point at which
    to split a variable for a branch point of a decision tree'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表9-2:](#listinganchor9-2) 一个找到决策树分支点处最优切分点的函数'
- en: In this function, we use a variable called `pctl` (short for *percentile*) to
    loop through every number from 0 to 100\. In the first line of the loop, we define
    a new `split_candidate` variable, which is the `pctl`-th percentile of the data.
    After that, we go through the same process we used in [Listing 9-2](#listing9-2).
    We create a list of the happiness levels of people whose `sclmeet` values are
    less than or equal to the split candidate, and the happiness levels of people
    whose `sclmeet` values are greater than the split candidate, and we check the
    errors that come from using that split candidate. If the error sum from using
    that split candidate is smaller than any of the error sums from using any previous
    split candidate, then we redefine the `best_split` variable to be equal to `split_candidate`.
    After the loop completes, the `best_split` variable is equal to the split point
    that led to the highest accuracy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们使用一个名为`pctl`的变量（代表*百分位数*）来循环遍历从0到100的每一个数字。在循环的第一行，我们定义了一个新的`split_candidate`变量，它是数据的`pctl`-百分位数。之后，我们遵循在[清单9-2](#listing9-2)中使用的相同过程。我们创建一个列表，其中包含`sclmeet`值小于或等于分割候选值的人的幸福感水平，以及`sclmeet`值大于分割候选值的人的幸福感水平，并检查使用该分割候选值时出现的误差。如果使用该分割候选值时的误差总和小于任何之前使用的分割候选值的误差总和，那么我们将`best_split`变量重新定义为等于`split_candidate`。当循环完成时，`best_split`变量就等于导致最高准确度的分割点。
- en: We can run this function for any variable, as in the following example where
    we run it for `hhmmb`, the variable recording the respondent’s number of household
    members.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对任何变量运行这个函数，例如以下示例中我们对`hhmmb`变量（记录受访者家庭成员数）运行该函数。
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output here shows us the correct split point as well as the predicted happiness
    level for the groups defined by that split point:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的输出显示了正确的分割点以及该分割点定义的各组的预测幸福感水平：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We interpret this output to mean that the best place to split the `hhmmb` variable
    is at `1.0`; we split the survey respondents into people who live alone (one household
    member) and those who live with others (more than one household member). We can
    also see the average happiness levels for those two groups: about `6.84` and about
    `7.62`, respectively.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释这个输出的意思是，最佳的分割点是`1.0`；我们将受访者分为独自居住（一个家庭成员）和与他人同住（超过一个家庭成员）两组。我们还可以看到这两组的平均幸福感水平：分别约为`6.84`和`7.62`。
- en: Choosing Splitting Variables
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择分割变量
- en: For any variable we choose in our data, we can find the optimal place to put
    our split point. However, remember that in a decision tree like the one in [Figure
    9-1](#figure9-1), we are not finding split points for only one variable. We split
    men from women, the obese from the non-obese, smokers from nonsmokers, and so
    on. A natural question is, how we should know which variable to split at each
    branching node? We could reorder the nodes in [Figure 9-1](#figure9-1) so that
    we split by weight first and sex second, or sex only on the left branch or not
    at all. Deciding which variable to split at each branch point is a crucial part
    of generating an optimal decision tree, so we should write code for that part
    of the process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在数据中选择的任何变量，我们都可以找到最佳的分割点。然而，请记住，在像[图9-1](#figure9-1)这样的决策树中，我们不仅仅是在为单个变量寻找分割点。我们会将男性和女性分开，将肥胖者和非肥胖者分开，将吸烟者和非吸烟者分开，依此类推。一个自然的问题是，我们如何知道在每个分支节点上应该选择哪个变量进行分割？我们可以重新排列[图9-1](#figure9-1)中的节点，使得首先按照体重分割，其次按性别分割，或者在左分支上仅按照性别分割，或者根本不分割性别。决定在每个分支点上选择哪个变量进行分割是生成最佳决策树的关键部分，因此我们应该为这个过程编写代码。
- en: 'We’ll use the same principle we used to get optimal split points to decide
    the best split variable: the best way to split is the one that leads to the smallest
    error. In order to determine that, we need to iterate over each available variable
    and check whether splitting on that variable leads to the smallest error. We then
    determine which variable leads to the split with the lowest error. We can accomplish
    this by using [Listing 9-3](#listing9-3).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与获取最佳分割点相同的原则来决定最佳分割变量：最好的分割方式是导致最小误差的方式。为了确定这一点，我们需要遍历每个可用的变量，并检查是否在该变量上进行分割会导致最小的误差。然后，我们确定哪个变量导致了误差最小的分割。我们可以通过使用[清单9-3](#listing9-3)来实现这一点。
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Listing 9-3:](#listinganchor9-3) A function that iterates over every variable
    and finds the best variable to split on'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单9-3:](#listinganchor9-3) 一个遍历每个变量并找到最佳分割变量的函数'
- en: 'In [Listing 9-3](#listing9-3), we’ve defined a function with a `for` loop that
    iterates over all the variables in a list of variables. For each of those variables,
    it finds the best split point by calling the `get_splitpoint()` function. Each
    variable, split at its best split point, will lead to a certain error sum for
    our predictions. If a particular variable has a lower error sum than any previous
    variable we considered, we’ll store that variable name as `best_var`. After looping
    through every variable name, it has found the variable with the lowest error sum,
    stored in `best_var`. We can run this code on a set of variables other than `sclmeet`
    as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 9-3](#listing9-3)中，我们定义了一个包含`for`循环的函数，该函数遍历一个变量列表中的所有变量。对于这些变量中的每一个，它都会通过调用`get_splitpoint()`函数来找到最佳切分点。每个变量在最佳切分点处的分割将导致我们预测的某个误差总和。如果某个变量的误差总和低于我们之前考虑的任何变量，我们将把该变量的名称存储为`best_var`。在遍历完每个变量名后，它找到了误差总和最低的变量，并将其存储在`best_var`中。我们可以在除了`sclmeet`之外的变量集上运行这段代码，方法如下：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this case, we see the following output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们会看到以下输出：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Our `getsplit()` function has output a very simple “tree” in the form of a
    nested list. This tree has only two branches. The first branch is represented
    by the first nested list, and the second branch is represented by the second nested
    list. Each element of both nested lists tells us something about their respective
    branches. The first list tells us that we’re looking at a branch based on a respondent’s
    value of `netusoft` (frequency of internet usage). Specifically, the first branch
    corresponds to people whose value of `netusoft` is between `-inf and 4.0`, where
    `inf` stands for infinity. In other words, people in this branch report their
    internet usage as 4 or less on a 5-point scale. The last element of each list
    shows an estimated happiness rating: about `7.0` for those who are not highly
    active internet users. We can draw a plot of this simple tree in [Figure 9-3](#figure9-3).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`getsplit()`函数输出了一个非常简单的“树形”结构，形式为一个嵌套列表。这个树形结构只有两条分支。第一条分支由第一个嵌套列表表示，第二条分支由第二个嵌套列表表示。两个嵌套列表的每个元素都告诉我们它们各自分支的某些信息。第一个列表告诉我们，我们正在查看一个基于受访者`netusoft`（互联网使用频率）值的分支。具体来说，第一个分支对应的是那些`netusoft`值在`-inf和4.0`之间的人，其中`inf`代表无限大。换句话说，位于这个分支的人在5分制中报告他们的互联网使用频率为4或更低。每个列表的最后一个元素显示了一个估计的幸福感评分：对于那些互联网使用频率不高的人，幸福感评分大约是`7.0`。我们可以在[图
    9-3](#figure9-3)中绘制这个简单的树形图。
- en: '![Figure_9-3](Images/figure_9-3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Figure_9-3](Images/figure_9-3.png)'
- en: '[Figure 9-3:](#figureanchor9-3) The tree generated by our first call to the
    `getsplit()` function'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3:](#figureanchor9-3) 我们第一次调用`getsplit()`函数生成的树形图'
- en: 'Our function so far is telling us that people with relatively low internet
    use report themselves as feeling less happy, with a mean happiness rating of about
    `7.0`, whereas people who report the highest level of internet use report happiness
    levels at about `7.7` on average. Again, we need to be careful about how we draw
    conclusions from this single fact: internet use may not be a true driver of happiness,
    but it may instead be correlated to happiness levels because of its strong correlations
    with age, wealth, health, education, and other characteristics. Machine learning
    alone doesn’t usually allow us to determine complex causal links with certainty,
    but, as it has with the simple tree in [Figure 9-3](#figure9-3), it enables us
    to make accurate predictions.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的函数告诉我们，互联网使用较少的人报告的幸福感较低，平均幸福感评分大约为`7.0`，而报告最高互联网使用频率的人则报告的幸福感水平平均为`7.7`。再次强调，我们需要小心从这个单一事实得出结论：互联网使用可能并不是幸福感的真正驱动因素，它可能只是与幸福感水平相关，因为它与年龄、财富、健康、教育等因素有着强相关性。仅凭机器学习通常无法让我们确定复杂的因果关系，但正如它在[图
    9-3](#figure9-3)中所示，它使我们能够做出准确的预测。
- en: Adding Depth
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加深度
- en: We’ve completed everything we need to make the best possible split at each branch
    point and generate a tree with two branches. Next, we need to grow the tree beyond
    just one branching node and two terminal nodes. Look at [Figure 9-1](#figure9-1)
    and notice that it has more than two branches. It has what we call a *depth* of
    three because there are up to three successive branches you have to follow in
    order to get the final diagnosis. The final step of our decision tree generation
    process is to specify a depth that we want to reach, and build new branches until
    we reach that depth. The way we accomplish this is by making the additions to
    our `getsplit()` function shown in [Listing 9-4](#listing9-4).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Listing 9-4:](#listinganchor9-4) A function that can generate a tree of a
    specified depth'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In this updated function, when we define the `generated_tree` variable, we now
    add empty lists to it, instead of means. We insert means only in terminal nodes,
    but if we want a tree that has greater depth, we need to insert other branches
    within each branch (that’s what the empty lists will contain). We also added an
    `if` statement with a long chunk of code at the end of the function. If the depth
    of the current branch is less than the maximum depth we want in a tree, this section
    will recursively call the `get_split()` function again to fill in another branch
    inside it. This process continues until the maximum depth is reached.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run this code to find the decision tree that leads to the lowest error
    in happiness predictions for our dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we do so, we should get the following output, which represents a tree
    with a depth of two:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 9-5: A representation of a decision tree using nested lists'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'What you see here is a collection of lists nested within each other. These
    nested lists represent our full decision tree, though it’s not as easy to read
    as [Figure 9-1](#figure9-1). In each level of nesting, we find a variable name
    and its range, just like we saw with the simple tree illustrated in [Figure 9-3](#figure9-3).
    The first level of nesting shows us the same branch we found in [Figure 9-3](#figure9-3):
    a branch that represents respondents whose value of `netusoft` was less than or
    equal to `4.0.` The next list, nested within the first, begins with `hhmmb, -inf,
    4.0`. This is another branch of our decision tree that branches from the branch
    we just examined, and consists of people whose self-reported household size is
    `4` or less. If we drew the portion of a decision tree that we’ve looked at in
    our nested list so far, it would look like [Figure 9-4](#figure9-4).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: We can continue to look at the nested lists to fill in more branches of our
    decision tree. Lists that are nested within other lists correspond to branches
    that are lower on the tree. A nested list branches from the list that contains
    it. The terminal nodes, instead of containing more nested lists, have an estimated
    happiness score.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure_9-4](Images/figure_9-4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: '[Figure 9-4:](#figureanchor9-4) A selection of branches from the decision tree'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We’ve successfully created a decision tree that enables us to predict happiness
    levels with relatively low error. You can examine the output to see the relative
    determinants of happiness, and the happiness levels associated with each branch.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'There is more exploring we can do with decision trees and our dataset. For
    example, we can try to run the same code but with a different or larger set of
    variables. We can also create a tree with a different maximum depth. Here is an
    example of running the code with a different variable list and depth:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When we run it with these parameters, we find a very different decision tree.
    You can see the output here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In particular, notice that the first branch is split on the variable `health`
    instead of the variable `netusoft`. Other branches at lower depths are split at
    different points and for different variables. The flexibility of the decision
    tree method means that starting with the same dataset and the same end goal, two
    researchers can potentially reach very different conclusions, depending on the
    parameters they use and decisions they make about how to work with the data. This
    is a common characteristic of machine learning methods, and part of what makes
    them so difficult to master.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Our Decision Tree
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to generate our decision tree, we compared error rates for each potential
    split point and each potential splitting variable, and we always chose the variable
    and split point that led to the lowest error rate for a particular branch. Now
    that we’ve successfully generated a decision tree, it makes sense to do similar
    error calculations, not just for a particular branch but for the whole tree. Evaluating
    the error rate for the whole tree can give us a sense of how well we’ve accomplished
    our prediction task, and how well we’re likely to perform on future tasks (for
    example, future hospital patients complaining of chest pain).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the decision tree output that we’ve generated so far, you’ll
    notice that it’s a little hard to read all the nested lists, and there’s no natural
    way to determine how happy we predict someone is without painstakingly reading
    through the nested branches and finding the right terminal node. It will be helpful
    for us to write code that can determine the predicted level of happiness for a
    person based on what we know about them from their ESS answers. The following
    function, `get_prediction()`, can accomplish this for us:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we can create a loop that goes through any portion of our dataset and
    gets any tree’s happiness prediction for that portion. In this case, let’s try
    a tree with a maximum depth of four:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This code just repeatedly calls the `get_prediction()` function and appends
    the result to our predictions list. In this case, we made predictions only for
    the first 30 observations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can check how these predictions compare to the actual happiness
    ratings, to see what our total error rate is. Here, we’ll make predictions for
    our entire dataset, and calculate the absolute differences between our predictions
    and the recorded happiness values:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When we run this, we find that the mean error made by predictions in our decision
    tree is `1.369`. This is higher than zero but lower than it might be if we used
    a worse prediction method. Our decision tree seems to make reasonably good predictions
    so far.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The Problem of Overfitting
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have noticed one very important way that our method for evaluating
    our decision tree doesn’t resemble how predictions work in real life. Remember
    what we did: we used the full set of survey respondents to generate our decision
    tree, and then we used that same set of respondents to judge the accuracy of our
    tree’s predictions. But it’s redundant to predict the happiness ratings of respondents
    who already took the survey—they took the survey, so we already know their happiness
    ratings and don’t need to predict them at all! This would be like getting a dataset
    of past heart attack patients, meticulously studying their pretreatment symptoms,
    and building a machine learning model that told us whether someone had a heart
    attack last week. By now, it’s already quite clear whether that person had a heart
    attack last week, and there are better ways to know than by looking at their initial
    triage diagnosis data. It’s easy to predict the past, but remember that true prediction
    is always about the future. As Wharton professor Joseph Simmons put it, “History
    is about what happened. Science is about what happens *next*.”'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: You may think that this isn’t a serious problem. After all, if we can make a
    decision tree that works well with last week’s heart attack patients, it’s reasonable
    to suppose that it will work well with next week’s heart attack patients. This
    is true to some extent. However, there is a danger that if we aren’t careful,
    we can encounter a common, dastardly peril called *overfitting*, the tendency
    of machine learning models to achieve very low error rates on the datasets used
    to create them (like data from the past) and then unexpectedly high error rates
    on other data (like the data that actually matters, from the future).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Consider the example of heart attack predictions. If we observe an emergency
    room for several days, maybe, by coincidence, every admitted patient who is wearing
    a blue shirt is suffering from a heart attack and every admitted patient who is
    wearing a green shirt is healthy. A decision tree model that included shirt color
    in its prediction variables would pick up this pattern and use it as a branching
    variable because it has such high diagnostic accuracy in our observations. However,
    if we then use that decision tree to predict heart attacks in another hospital,
    or for some future day, we’ll find that our predictions are often wrong, as many
    people in green shirts also suffer heart attacks and many people in blue shirts
    don’t. The observations we used to build our decision tree are called *in-sample
    observations*, and the observations that we then test our model on, which are
    not part of our decision tree generation process, are called *out-of-sample observations*.
    Overfitting means that by zealously seeking low error rates in predictions of
    our in-sample observations, we have caused our decision tree model to have inordinately
    high error rates when predicting our out-of-sample observations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a serious issue in all applications of machine learning, and
    it trips up even the best machine learning practitioners. To avoid it, we’ll take
    an important step that will make our decision tree creation process better resemble
    the real-life prediction scenario.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that real-life prediction is about the future, but when we build our
    decision tree we necessarily have data only from the past. We can’t possibly get
    data from the future, so we’ll split our dataset into two subsets: a *training
    set,* which we’ll use only to build our decision tree, and a *test set,* which
    we’ll use only to check the accuracy of our decision tree. Our test set is from
    the past, just like the rest of our data, but we treat it as if it’s the future;
    we don’t use it to create our decision tree (as if it hasn’t happened yet), but
    we do use it—only after completely building the decision tree—to test the accuracy
    of our decision tree (as if we got it later in the future).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: By doing this simple training/test split, we’ve made our decision tree generation
    process resemble the real-life problem of predicting the unknown future; the test
    set is like a simulated future. The error rate that we find on the test set gives
    us a reasonable expectation of the error rate we’ll get from the actual future.
    We’ll know that we’re guilty of overfitting if the error on our training set is
    very low and the error on our test set is very high.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define training and test sets as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this snippet, we used the `numpy` module to shuffle the data—in other words,
    keeping all the data but moving the rows randomly. We accomplished this with the
    `reindex()` method of the `pandas` module. The reindexing is done with a random
    shuffling of the row numbers, which we get by using the `numpy` module’s permutation
    capability. After shuffling the dataset, we select the first 37,000 shuffled rows
    as a training dataset, and the remainder of the rows as a test dataset. The command
    `np.random.seed(518)` is not necessary, but if you run it you’ll ensure that you’ll
    get the same pseudo-random results that we show here.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining our training and test data, we generate a decision tree using
    only the training data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we check the average error rate on the test data, which wasn’t used
    to train our decision tree:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We find that our mean error rate on the test data is `1.371`. This is just
    a hair higher than the `1.369` error rate we found when we used the whole dataset
    for both training and testing. This indicates that our model doesn’t suffer from
    overfitting: it’s good at predicting the past and almost exactly as good at predicting
    the future. Quite often, instead of getting this good news, we get bad news—that
    our model is worse than we thought it was—but it’s good to get this news because
    we can still make improvements before we start using our model in a real scenario.
    In such cases, before our model is ready to be deployed in real life, we’ll need
    to make improvements to it so that its error rate *on the test set* is minimized.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Improvements and Refinements
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may find that you’ve created a decision tree that has lower accuracy than
    you would like. For example, you might have worse accuracy than you should because
    you’re guilty of overfitting. Many of the strategies for dealing with overfitting
    issues boil down to some kind of simplification, since simple machine learning
    models are less likely to suffer from overfitting than are complex models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The first and easiest way to simplify our decision tree models is to limit their
    maximum depth; since depth is a variable that we can redefine in one short line,
    this is easy to do. To determine the right depth, we have to check the error rates
    on out-of-sample data for different depths. If the depth is too high, it’s likely
    to cause high error because of overfitting. If the depth is too low, it is likely
    to cause high error because of *underfitting*. You can think of underfitting as
    something like the mirror image of overfitting. Overfitting consists of attempting
    to learn from patterns that are arbitrary or irrelevant—in other words, learning
    “too much” from noise in our training data, like whether someone is wearing a
    green shirt. Underfitting consists of failing to learn enough—creating models
    that miss crucial patterns in the data, like whether someone is obese or uses
    tobacco.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting tends to result from models that have too many variables or too
    much depth, whereas underfitting tends to result from models that have too few
    variables or too little depth. Just as with many situations in algorithm design,
    the right place to be is a happy medium between too high and too low. Choosing
    the right parameters for a machine learning model, including the depth of a decision
    tree, is often referred to as *tuning*, because fixing the tightness of a string
    on a guitar or violin also relies on finding a happy medium between a pitch that’s
    too high and one that’s too low.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Another way to simplify our decision tree model is to do what’s called *pruning*.
    For this, we grow a decision tree to its full depth and then find branches that
    we can remove from the tree without increasing our error rate by much.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Another refinement worth mentioning is using different measures to choose the
    right split point and the right splitting variable. In this chapter, we introduced
    the idea of using the classification error sum to decide where to put the split
    point; the right split point is one that minimizes our error sum. But there are
    other ways to decide on the right split point for a decision tree, including Gini
    impurity, entropy, information gain, and variance reduction. In practice, these
    other measures, especially Gini impurity and information gain, are almost always
    used rather than classification error rate, because some mathematical properties
    make them better in many cases. Experiment with different ways to choose a split
    point and splitting variable to find one that seems to perform the best for your
    data and your decision problem.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Everything we do in machine learning is meant to enable us to make accurate
    predictions on new data. When you’re trying to improve a machine learning model,
    you can always judge whether an action is worthwhile by checking how much it improves
    your error rate on test data. And feel free to be creative to find improvements—anything
    that improves your error rate on test data is probably worth trying.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees are useful and valuable, but they are not regarded as the best
    machine learning method by professionals. This is in part because of their reputation
    for overfitting and relatively high error rates, and in part because of the invention
    of a method called *random forests*, which has become popular recently and provides
    an unequivocal performance improvement over decision trees.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'As its name suggests, a random forest model consists of a collection of decision
    tree models. Each decision tree in the random forest depends on some randomization.
    Using randomization, we get a diverse forest with many trees instead of a forest
    that is just one tree repeated over and over. The randomization occurs in two
    places. First, the training dataset is randomized: each tree is built considering
    only a subset of the training set, which is randomly selected and will be different
    for every tree. (The test set is randomly selected at the beginning of the process,
    but it’s not rerandomized or reselected for every tree.) Second, the variables
    used to build the tree are randomized: only a subset of the full set of variables
    is used for each tree, and the subset could be different every time as well.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: After building a collection of these different, randomized trees, we have a
    whole random forest. To make a prediction about a particular observation, we have
    to find what each of these different decision trees predicts, and then take the
    average of the prediction for every individual decision tree. Since the decision
    trees are randomized in both their data and their variables, taking an average
    of all of them helps avoid the problem of overfitting and often leads to more
    accurate predictions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code in this chapter creates decision trees “from scratch,” by directly
    manipulating datasets and lists and loops. When you work with decision trees and
    random forests in the future, you can rely on existing Python modules that do
    much of that heavy lifting for you. But don’t let these modules become a crutch:
    if you understand every step of these important algorithms well enough to code
    them from scratch yourself, you can be much more effective in your machine learning
    efforts.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter introduced machine learning and explored decision tree learning,
    a fundamental, simple, and useful machine learning method. Decision trees constitute
    a type of algorithm, and the generation of a decision tree is itself an algorithm,
    so this chapter contained an algorithm for generating an algorithm. By learning
    decision trees and the fundamental ideas of random forests, you have taken a big
    step toward becoming a machine learning expert. The knowledge you’ve gained in
    this chapter will be a solid foundation for other machine learning algorithms
    you may choose to learn, including advanced ones like neural networks. All machine
    learning methods attempt the type of task we tried here: prediction based on patterns
    in a dataset. In the next chapter, we explore artificial intelligence, one of
    the most advanced undertakings of our adventure.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
