["```py\n>>> [doc[i] for i in range(len(doc))]\n\n[A, severe, storm, hit, the, beach, .]\n```", "```py\n>>> from spacy.tokens.doc import Doc\n\n>>> from spacy.vocab import Vocab\n\n>>> doc = Doc(➊Vocab(), ➋words=[u'Hi', u'there'])\n\ndoc\n\nHi there\n```", "```py\nI want a green apple.\n```", "```py\n>>> doc = nlp(u'I want a green apple.')\n\n>>> [w for w in doc[4].lefts]\n\n[a, green]\n```", "```py\n>>> [w for w in doc[4].children]\n```", "```py\n   >>> doc = nlp(u'A severe storm hit the beach. It started to rain.')\n\n➊ >>> for sent in doc.sents:\n\n➋ ...   [sent[i] for i in range(len(sent))]\n\n   ...\n\n   [A, severe, storm, hit, the, beach, .]\n\n   [It, started, to, rain, .]\n\n   >>>\n```", "```py\n>>> [doc[i] for i in range(len(doc))]\n\n[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]\n```", "```py\n>>> for i,sent in enumerate(doc.sents):\n\n...   if i==1 and sent[0].pos_== 'PRON':\n\n...     print('The second sentence begins with a pronoun.')\n\nThe second sentence begins with a pronoun.\n```", "```py\n>>> counter = 0\n\n>>> for sent in doc.sents:\n\n...   if sent[len(sent)-2].pos_ == 'VERB':\n\n...     counter+=1\n\n>>> print(counter)\n\n1\n```", "```py\nA noun chunk\n\na phrase\n\na noun\n\nits head\n```", "```py\n>>> doc = nlp(u'A noun chunk is a phrase that has a noun as its head.')\n\n>>> for chunk in doc.noun_chunks:\n\n...   print(chunk)\n```", "```py\n for token in doc:\n\n➊ if token.pos_=='NOUN':\n\n     chunk = ''\n\n  ➋ for w in token.children:\n\n     ➌ if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n\n         chunk = chunk + w.text + ' '\n\n➍ chunk = chunk + token.text\n\n   print(chunk)\n```", "```py\n>>> doc=nlp('I want a green apple.')\n\n>>> doc[2:5]\n\na green apple\n```", "```py\n>>> doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')\n\n>>> [doc[i] for i in range(len(doc))]\n\n[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]\n```", "```py\n>>> span = doc[1:4]\n\n>>> lem_id = doc.vocab.strings[span.text]\n\n>>> span.merge(lemma = lem_id)\n\nGolden Gate Bridge\n```", "```py\n>>> for token in doc:\n\n      print(token.text, token.lemma_, token.pos_, token.dep_)\n```", "```py\n\nThe                the                DET   det\n\nGolden Gate Bridge Golden Gate Bridge PROPN nsubj\n\nis                 be                 VERB  ROOT\n\nan                 an                 DET   det\n\niconic             iconic             ADJ   amod\n\nlandmark           landmark           NOUN  attr\n\nin                 in                 ADP   prep\n\nSan                san                PROPN compound\n\nFrancisco          francisco          PROPN pobj\n\n.                  .                  PUNCT punct\n```", "```py\n>>> nlp.pipe_names\n\n['tagger', 'parser', 'ner']\n```", "```py\nnlp = spacy.load('en', disable=['parser'])\n```", "```py\n>>> doc = nlp(u'I want a green apple.')\n\n>>> for token in doc:\n\n...   print(➊token.text, ➋token.pos_, ➌token.dep_)\n\nI     PRON\n\nwant  VERB\n\na     DET\n\ngreen ADJ\n\napple NOUN\n\n.     PUNCT\n```", "```py\nnlp = spacy.load('en')\n```", "```py\n>>> print(nlp.meta['lang'] + '_' + nlp.meta['name'])\n\nen_core_web_sm\n```", "```py\n>>> from spacy import util\n\n>>> util.get_package_path('en_core_web_sm')\n\nPosixPath('/usr/local/lib/python3.5/site-packages/en_core_web_sm')\n```", "```py\n>>> print(nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp.\n\nmeta['version'])\n\nen_core_web_sm-2.0.0\n```", "```py\n>>> nlp.meta['pipeline']\n\n['tagger', 'parser', 'ner']\n```", "```py\n   >>> lang = 'en'\n\n   >>> pipeline = ['tagger', 'parser', 'ner']\n\n   >>> model_data_path = '/usr/local/lib/python3.5/site-packages/en_core_web_sm/\n\n   en_core_web_sm-2.0.0'\n\n➊ >>> lang_cls = spacy.util.get_lang_class(lang) \n\n   >>> nlp = lang_cls() \n\n➋ >>> for name in pipeline:\n\n➌ ...   component = nlp.create_pipe(name) \n\n➍ ...   nlp.add_pipe(component)\n\n➎ >>> nlp.from_disk(model_data_path)\n```", "```py\n>>> doc = nlp(u'I need a taxi to Festy.')\n\n>>> for ent in doc.ents:\n\n...  print(ent.text, ent.label_)\n\nFesty ORG\n```", "```py\nLABEL = 'DISTRICT'\n\nTRAIN_DATA = [\n\n➊ ('We need to deliver it to Festy.', {\n\n    ➋ 'entities': [(25, 30, 'DISTRICT')]\n\n  }),\n\n➌ ('I like red oranges', {\n\n'entities': []\n\n  })\n\n]\n```", "```py\nner = nlp.get_pipe('ner')\n```", "```py\nner.add_label(LABEL)\n```", "```py\nnlp.disable_pipes('tagger')\n\nnlp.disable_pipes('parser')\n```", "```py\noptimizer = nlp.entity.create_optimizer()\n\nimport random\n\nfor i in range(25):\n\n    random.shuffle(TRAIN_DATA)\n\n    for text, annotations in TRAIN_DATA:\n\n        nlp.update([text], [annotations], sgd=optimizer)\n```", "```py\n>>> doc = nlp(u'I need a taxi to Festy.')\n\n>>> for ent in doc.ents:\n\n... print(ent.text, ent.label_) \n\n...\n\nFesty DISTRICT\n```", "```py\n>>> ner.to_disk('/usr/to/ner')\n```", "```py\n   >>> import spacy\n\n   >>> from spacy.pipeline import EntityRecognizer\n\n➊ >>> nlp = spacy.load('en', disable=['ner'])\n\n➋ >>> ner = EntityRecognizer(nlp.vocab)\n\n➌ >>> ner.from_disk('/usr/to/ner')\n\n➍ >>> nlp.add_pipe(ner)\n```", "```py\n>>> doc = nlp(u'We need to deliver it to Festy.')\n\n>>> for ent in doc.ents:\n\n... print(ent.text, ent.label_)\n\nFesty DISTRICT\n```", "```py\npip install Cython\n```", "```py\n   from cymem.cymem cimport Pool\n\n   from spacy.tokens.doc cimport Doc\n\n   from spacy.structs cimport TokenC\n\n   from spacy.typedefs cimport hash_t\n\n➊ cdef struct DocStruct:\n\n       TokenC* c\n\n       int length\n\n➋ cdef int counter(DocStruct* doc, hash_t tag):\n\n       cdef int cnt = 0\n\n       for c in doc.c[:doc.length]:\n\n          if c.tag == tag:\n\n             cnt += 1\n\n       return cnt\n\n➌ cpdef main(Doc mydoc):\n\n       cdef int cnt\n\n       cdef Pool mem = Pool()\n\n       cdef DocStruct* doc_ptr = <DocStruct*>mem.alloc(1, sizeof(DocStruct))\n\n       doc_ptr.c = mydoc.c\n\n       doc_ptr.length = mydoc.length\n\n       tag = mydoc.vocab.strings.add('PRP')\n\n       cnt = counter(doc_ptr, tag)\n\n       print(doc_ptr.length)\n\n       print(cnt)\n```", "```py\n   from distutils.core import setup\n\n   from Cython.Build import cythonize\n\n➊ import numpy\n\n   setup(name='spacy text app',\n\n     ➋ ext_modules=cythonize(\"spacytext.pyx\", language=\"c++\"),\n\n      ➌ include_dirs=[numpy.get_include()]\n\n          )\n```", "```py\npython setup.py build_ext --inplace\n```", "```py\n#warning \"Using deprecated NumPy API ...\n```", "```py\n>>> from spacytext import main\n```", "```py\n   >>> import spacy\n\n   >>> nlp = spacy.load('en')\n\n➊ >>> f= open(\"test.txt\",\"rb\")\n\n   >>> contents =f.read()\n\n➋ >>> doc = nlp(contents[:100000].decode('utf8'))\n\n➌ >>> main(doc)\n\n   21498\n\n   216\n```"]