<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_117" title="117"/>6</span><br/>
<span class="ChapterTitle">Supervised Learning</span></h1>
</header>
<figure class="opener">
<img alt="" height="200" src="image_fi/book_art/chapterart.png" width="200"/>
</figure>
<p class="ChapterIntro">Computer scientists use the term <em>supervised learning</em> to refer to a broad range of quantitative methods that predict and classify. In fact, you’ve already done supervised learning: the linear regression you did in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span> and the LPMs and logistic regression from <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span> are all instances of supervised learning. By learning those methods, you’ve already become familiar with the basic ideas of supervised learning. This chapter introduces some advanced supervised learning methods and discusses the idea of supervised learning in general. We’re dwelling on this topic so much because it’s such a crucial component of data science.</p>
<p><span epub:type="pagebreak" id="Page_118" title="118"/>We’ll start by introducing yet another business challenge and describing how supervised learning can help us resolve it. We’ll talk about linear regression as an imperfect solution and discuss supervised learning in general. Then we’ll introduce k-NN, a simple but elegant supervised learning method. We’ll also briefly introduce decision trees, random forests, and neural networks, and discuss how to use them for prediction and classification. We’ll close with a discussion of how to measure accuracy and what unites each of these disparate methods.</p>
<h2 id="h1-502888c06-0001">Predicting Website Traffic</h2>
<p class="BodyFirst">Imagine that you’re running a website. Your website’s business model is simple: you post articles on interesting topics, and you earn money from the people who view your website’s articles. Whether your revenue comes from ad sales, subscriptions, or donations, you earn money in proportion to the number of people who visit your site: the more visitors, the higher your revenue.</p>
<p>Amateur writers submit articles to you with the hope that you’ll publish them on your site. You receive an enormous number of submissions and can’t possibly read, much less publish, everything you receive. So you have to do some curation. You may consider many factors as you’re deciding what to publish. Of course, you’ll try to consider the quality of submitted articles. You’ll also want to consider which articles fit with the “brand” of your site. But in the end, you’re trying to run a business, and maximizing your site’s revenue will be crucial to ensuring your business’s long-term survival. Since you earn revenue in proportion to the number of visitors to your site, maximizing revenue will depend on selecting articles to publish that are likely to get many visitors.</p>
<p>You could try to rely on intuition to decide which articles are likely to receive many visitors. This would require either you or your team to read every submission and make difficult judgments about which articles are likely to attract visitors. This would be extremely time-consuming, and even after spending all that time reading articles, it’s far from certain that your team would have the right judgment about which articles will attract the most visitors.</p>
<p>A faster and potentially more accurate approach to this problem is through supervised learning. Imagine that you could write code to read articles for you as soon as they arrived in your inbox and could then use information that the code gleans from each submitted article to accurately predict the number of visitors it will attract, before you publish it. If you had code like that, you could even fully automate your publishing process: a bot could read submissions from emails, predict the likely revenue expected from every submitted article, and publish every article that had an expected revenue above a particular threshold.</p>
<p>The hardest part of that process would be predicting an article’s expected revenue; that’s the part we need to rely on supervised learning to accomplish. In the rest of the chapter, we’ll go through the steps required for <span epub:type="pagebreak" id="Page_119" title="119"/>the supervised learning that would enable this kind of automated system to predict the number of visitors a given article will attract.</p>
<h2 id="h1-502888c06-0002">Reading and Plotting News Article Data</h2>
<p class="BodyFirst">Like most data science scenarios, supervised learning requires us to read in data. We’ll read in a dataset that’s available for free from the University of California, Irvine (UCI) Machine Learning Repository (<a class="LinkURL" href="https://archive-beta.ics.uci.edu/">https://archive-beta.ics.uci.edu/</a>). This repository contains hundreds of datasets that machine learning researchers and enthusiasts can use for research and fun.</p>
<p>The particular dataset we’ll use contains detailed information about news articles published on Mashable (<a class="LinkURL" href="https://mashable.com">https://mashable.com</a>) in 2013 and 2014. This Online News Popularity dataset has a web page at <a class="LinkURL" href="https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity">https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity</a> that presents more information about the data, including its source, the information it contains, and papers that have been published containing analyses of it.</p>
<p>You can obtain a ZIP file of the data from <a class="LinkURL" href="https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip">https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip</a>. After you download the ZIP archive, you must extract it on your computer. You’ll then see the <em>OnlineNewsPopularity.csv</em> file, which is the dataset itself. After extracting that <em>.csv</em>, you can read it into your Python session as follows:</p>
<pre><code>import pandas as pd
news=pd.read_csv('OnlineNewsPopularity.csv')</code></pre>
<p>We import our old friend the pandas package and read the news dataset into a variable called <code>news</code>. Each row of <code>news</code> contains detailed information about one particular article published on Mashable. The first column, <code>url</code>, contains the URL of the original article. If you visit the URL of a particular article, you can see the text and images associated with it.</p>
<p>In total, our <code>news</code> dataset has 61 columns. Each column after the first contains a numeric measurement of something about the article. For example, the third column is called <code>n_tokens_title</code>. This is a count of the <em>tokens</em> in the title, which in this case just means the number of words in the title. Many of the columns in the <code>news</code> dataset have names that refer to advanced methods in <em>natural language processing (NLP)</em>. NLP is a relatively new field concerned with using computer science and mathematical algorithms to analyze, generate, and translate natural human language in a way that’s quick and automatic and doesn’t require human effort.</p>
<p>Consider the 46th column, <code>global_sentiment_polarity</code>. This column contains a measure of each article’s overall <em>sentiment</em>, ranging from –1 (highly negative) to 0 (neutral) to 1 (highly positive). The ability to automatically measure the sentiment of text written in natural human language is one of the recent, exciting developments in the world of NLP. The most advanced sentiment analysis algorithms are able to closely match humans’ sentiment ratings, so an article about death, horror, and sadness will be ranked by both humans and NLP algorithms as having a highly negative sentiment (close to –1), while an article about joy, freedom, and data analysis <span epub:type="pagebreak" id="Page_120" title="120"/>will be universally agreed to have a highly positive sentiment (close to 1). The creators of our dataset have already run a sentiment analysis algorithm to measure the sentiment of each article in the dataset, and the result is stored in <code>global_sentiment_polarity</code>. Other columns have other measurements, including simple things like article length as well as other advanced NLP results.</p>
<p>The final column, <code>shares</code>, records the number of times each article was shared on social media platforms. Our true goal is to increase revenue by increasing the number of visitors. But our dataset doesn’t contain any direct measurement of either revenue or visitors! This is a common occurrence in the practice of data science: we want to analyze something, but our data contains only other things. In this case, it’s reasonable to suppose that the number of social media shares is correlated with the number of visitors to an article, both because highly visited articles will be shared often and because highly shared articles will be visited often. And, as we mentioned before, our revenue is directly related to the number of website visits. So, we can reasonably suppose that the number of social media shares of an article is closely related to the revenue obtained from the article. This means that we’ll use shares as a <em>proxy</em> for visits and revenue.</p>
<p>It will help our analysis if we can determine which features of an article are positively related to shares. For example, we might guess that articles with high sentiment scores will also get shared frequently, if we believe that people like to share happy things. If that’s true, knowing the sentiment of an article will help us predict the number of times an article will be shared. By learning how to predict shares, we suppose that we’ll be simultaneously learning how to predict both visitors and revenue as well. And, if we know the features of a highly shared article, we’ll know how to design future articles to maximize our revenue.</p>
<p>As we’ve done before (especially in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>), we can start with simple exploration. We’ll start by drawing a graph. Let’s consider a graph of the relationship between sentiment and shares:</p>
<pre><code>from matplotlib import pyplot as plt
plt.scatter(news[' global_sentiment_polarity'],news[' shares'])
plt.title('Popularity by Sentiment')
plt.xlabel('Sentiment Polarity')
plt.ylabel('Shares')
plt.show()</code></pre>
<p>You may notice that when we access our dataset’s columns in this Python snippet, we put a space at the beginning of every column name. For example, we write <code>news[' shares']</code> instead of <code>news['shares']</code> to refer to the column recording the number of shares. We do this because that’s the way the column names are recorded in the original data file. For whatever reason, that file contains a space before every column name instead of the column name alone, so we need to include that space when we tell Python to access each column by name. You’ll see these spaces throughout the chapter; every dataset has its own quirks, and part of being a successful data scientist is being able to understand and adapt to quirks like this one.</p>
<p><span epub:type="pagebreak" id="Page_121" title="121"/><a href="#figure6-1" id="figureanchor6-1">Figure 6-1</a> shows the relationship between sentiment polarity and shares.</p>
<figure>
<img alt="" class="" height="242" src="image_fi/502888c06/f06001.png" width="389"/>
<figcaption><p><a id="figure6-1">Figure 6-1</a>: The relationship between sentiment and shares for every article in our dataset</p></figcaption>
</figure>
<p>One thing we can notice about this plot is that, at least to the naked eye, no clear linear relationship exists between polarity and shares. High-sentiment articles don’t seem to be shared much more than low-sentiment articles, or vice versa. If anything, articles close to the middle of the polarity scale (articles that have close to neutral sentiment) seem to earn the most shares.</p>
<h2 id="h1-502888c06-0003">Using Linear Regression as a Prediction Method</h2>
<p class="BodyFirst">We can do a more rigorous test for this (lack of a) linear relationship by performing a linear regression, just as we did in <span class="xref" itemid="xref_target_Chapters 2">Chapters 2</span> and <span class="xref" itemid="xref_target_5">5</span>:</p>
<pre><code>from sklearn.linear_model import LinearRegression
x = news[' global_sentiment_polarity'].values.reshape(-1,1)
y = news[' shares'].values.reshape(-1,1)
regressor = LinearRegression()
regressor.fit(x, y)
print(regressor.coef_)
print(regressor.intercept_)</code></pre>
<p>This snippet performs a linear regression predicting shares using sentiment polarity. It does so in the same way we outlined in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. We start by importing from the module <code>sklearn.linear_model</code>, which contains the <code>LinearRegression()</code> function we want to use. Then, we reshape the data so that the module we’re importing can work with it. We create a variable called <code>regressor</code>, and we fit the regressor to our data. Finally, we print out the coefficient and intercept obtained by fitting the regression: 499.3 and 3,335.8.</p>
<p>You’ll remember from <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span> that we can interpret these numbers as the slope and intercept of the regression line, respectively. In other words, our linear regression estimates the relationship between sentiment and shares as follows:</p>
<p class="Equation"><em>shares</em> = 3335.8 + 499.3 · <em>sentiment</em></p>
<p><span epub:type="pagebreak" id="Page_122" title="122"/>We can plot this regression line together with our data as follows:</p>
<pre><code>regline=regressor.predict(x)
plt.scatter(news[' global_sentiment_polarity'],news[' shares'],color='blue')
plt.plot(sorted(news[' global_sentiment_polarity'].tolist()),regline,'r')
plt.title('Shares by Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Shares')
plt.show()</code></pre>
<p>The output should look like <a href="#figure6-2" id="figureanchor6-2">Figure 6-2</a>.</p>
<figure>
<img alt="" class="" height="256" src="image_fi/502888c06/f06002.png" width="389"/>
<figcaption><p><a id="figure6-2">Figure 6-2</a>: A regression line showing the estimated relationship between sentiment and shares</p></figcaption>
</figure>
<p>Our regression line, which should be red if you create the plot at home, appears quite flat, showing only a weak relationship between sentiment and shares. Using this regression line to predict shares probably wouldn’t help us much, since it predicts nearly identical numbers of shares for every sentiment value. We’ll want to explore other supervised learning methods that can lead to better, more accurate predictions. But first, let’s think about supervised learning in general, including what it is about linear regression that makes it a type of supervised learning, and what other types of supervised learning could also be applicable to our business scenario.</p>
<h2 id="h1-502888c06-0004">Understanding Supervised Learning</h2>
<p class="BodyFirst">The linear regression we just did is an example of supervised learning. We’ve mentioned supervised learning several times in this chapter, without precisely defining it. We can define it as the process of learning a function that maps feature variables to target variables. This may not sound immediately obvious or clear. To understand what we mean, consider <a href="#figure6-3" id="figureanchor6-3">Figure 6-3</a>.</p>
<figure>
<img alt="" class="" height="106" src="image_fi/502888c06/f06003.png" width="605"/>
<figcaption><p><a id="figure6-3">Figure 6-3</a>: The supervised learning process</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_123" title="123"/>Think about how this figure applies to the linear regression we completed earlier in the chapter. We used sentiment as our only feature (the oval on the left). Our target variable was shares (the oval on the right). The following equation shows our <em>learned function</em> (the arrow in the middle):</p>
<p class="Equation"><em>shares</em> = 3,335.8 + 499.3 · <em>sentiment</em></p>
<p>This function does what every learned function is supposed to do in supervised learning: it takes a feature (or multiple features) as its input, and it outputs a prediction of the value of a target variable. In our code, we imported capabilities from the sklearn module that determined the coefficients, or learned the function, for us. (For its part, sklearn learned the function by relying on linear algebra equations that are guaranteed to find the coefficients that minimize the mean squared error on our target variable, as we discussed in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>.)</p>
<p>The term <em>supervised learning</em> refers to the process of determining (learning) this function. The target variable is what supervises the process, because as we’re determining the learned function, we check whether it leads to accurate predictions of the target. Without a target variable, we would have no way to learn the function, because we’d have no way to determine which coefficients led to high accuracy and which led to low accuracy.</p>
<p>Every supervised learning method you’ll ever use can be described by <a href="#figure6-3">Figure 6-3</a>. In some cases, we can do<em> feature engineering</em>, in which we carefully select which variables in our dataset will lead to the most accurate possible predictions. In other cases, we’ll adjust our target variable—for example, by using a proxy or a transformation of the original variable. But the most important part of any supervised learning method is the learned function that maps the features to the target. Mastering new supervised learning methods consists of mastering new ways to determine these learned functions.</p>
<p>When we use linear regression as our chosen supervised learning method, the learned function we get is always in the form shown in <a href="#equation6-1" id="equationanchor6-1">Equation 6-1</a>:</p>
<p class="Equation"><em>target</em> = <em>intercept</em> + <em>coefficient</em><sub>1</sub> · <em>feature</em><sub>1</sub> + <em>coefficient</em><sub>2</sub> · <em>feature</em><sub>2</sub> + … + <em>coefficient</em><sub><em>n</em></sub> · <em>feature</em><sub><em>n</em></sub></p>
<p class="figcaption"><a id="equation6-1">Equation 6-1</a>: The general form of every linear regression’s learned function</p>
<p>For someone who has taken lots of algebra classes, this may seem like a natural form for a function to take. Coefficients are multiplied by features and added up. When we do this in two dimensions, we get a line, like the line in <a href="#figure6-2">Figure 6-2</a>.</p>
<p>However, this is not the only possible form for a learned function. If we think more deeply about this form, we can realize that the function for linear regression is implicitly expressing an assumed view, or <em>model</em>, of the world. In particular, linear regression is implicitly assuming that the world can be described by lines: that anytime we have two variables <em>x</em> and <em>y</em>, there’s a way to relate them accurately as the line <em>y </em>= <em>a </em>+ <em>bx</em>, for some <em>a</em> and <em>b</em>. Many things in the world can be described by lines, but not everything. The universe is a big place, and there are many models of the world, many <span epub:type="pagebreak" id="Page_124" title="124"/>learned functions, and many supervised learning methods that can give us more accurate predictions by abandoning this assumption of linearity.</p>
<p>If the world isn’t described by lines and linear relationships, what model of the world is the correct one, or the most accurate or useful one? Many answers are possible. For example, instead of a world made up of lines, we could think of the world as composed of unique little neighborhoods around points. Instead of using a line to make predictions, we could measure characteristics of neighborhoods around points, and use those neighborhoods to make predictions. (This approach will become clearer in the next section.)</p>
<p>If everything we observe in the world is related by lines and linear relationships, linear regression is the right model for studying it. If the world is instead made up of neighborhoods, another supervised learning model is more suitable: k-nearest neighbors. We’ll examine this method next.</p>
<h2 id="h1-502888c06-0005">k-Nearest Neighbors</h2>
<p class="BodyFirst">Suppose that you have an intern who has never studied statistics, linear regression, supervised learning, or data science at any level. You just received a new article from an author who wants to be published on your website. You give the intern the newly submitted article as well as the <code>news</code> dataset and some NLP software. You assign the intern to predict the number of times the new article will be shared. If your intern predicts a high number of shares, you’ll publish the article. Otherwise, you won’t.</p>
<p>Your intern uses NLP software to determine that this article has <code>global_sentiment_polarity</code> equal to 0.42. Your intern doesn’t know how to do the linear regression that we did at the beginning of the chapter. Instead, they have a simple idea of how they’ll predict shares. Their simple idea is to look through the <code>news</code> dataset until they find an article that closely resembles this new article. If an existing article in the dataset closely resembles the newly submitted article, it’s reasonable to suppose that the new article’s number of shares will resemble the existing article’s number of shares.</p>
<p>For example, suppose they find an existing article in the dataset that has <code>global_sentiment_polarity</code> equal to 0.4199. They’ll conclude, reasonably, that the existing article is similar to our new article, because their sentiment ratings are nearly identical. If the existing article achieved 1,200 shares, we can expect that our new article, with a nearly identical <code>global_sentiment_polarity</code>, should have a similar number of shares. “Similar articles get similar numbers of shares” is one way to sum up this simple thought process. In the context of supervised learning, we can rephrase this as “similar feature values lead to similar target values,” though of course your intern has never heard of supervised learning.</p>
<p>Since we’re working with numeric data, we don’t need to speak merely qualitatively about articles <em>resembling</em> each other. We can directly measure the <em>distance</em> between any two observations in our dataset. The existing article that resembles our new article has <code>global_sentiment_polarity</code> equal to 0.4199, which <span epub:type="pagebreak" id="Page_125" title="125"/>is 0.0001 different from our new article’s <code>global_sentiment_polarity</code> of 0.42. Since <code>global_sentiment_polarity</code> is the only variable we’ve considered so far, we can say that these two articles have a <em>distance</em> of 0.0001 between them.</p>
<p>You may think that distance is something that has one nonnegotiable definition. But in data science and machine learning, we often find ourselves measuring distances that don’t match what we mean by the term in everyday life. In this example, we’re using a difference between sentiment scores as our distance, even though it’s not a distance that can be walked or measured with a ruler. In other cases, we may find ourselves expressing a distance between true and false values, especially if we’re doing a classification as in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>. When we talk about distance, we’re often using the term as a loose analogy rather than a literal physical measurement.</p>
<p>Observations that have a small distance between them can be called <em>neighbors</em>, and in this case we’ve found two close neighbors. Another article with sentiment 0.41 would have distance 0.1 from our new article: still a neighbor, but a little further down the “street.” For any two articles, we can measure the distance between them on all variables that interest us and use this as a measurement of the extent to which any two articles are neighbors.</p>
<p>Instead of considering just one neighbor article, we can consider the entire neighborhood surrounding the new article we want to make predictions about. We might find the 15 nearest neighbors to our new article—the 15 points in our dataset with <code>global_sentiment_polarity</code> closest to 0.42. We can consider the number of shares associated with each of those 15 articles. The mean of the number of shares achieved by these 15 nearest neighbors is a reasonable prediction for the number of shares we can expect our new article to get.</p>
<p>Your intern didn’t think their prediction method was anything special. It just seemed like a natural, simple way to make a prediction without using any calculus or computer science. However, their simple process is actually a powerful supervised learning algorithm called <em>k-nearest neighbors (k-NN)</em>. We can describe the whole method in four simple steps; truly, it is simplicity itself:</p>
<ol class="decimal">
<li value="1">Choose a point <em>p</em> you want to make a prediction about for a target variable.</li>
<li value="2">Choose a natural number, <em>k</em>.</li>
<li value="3">Find the <em>k</em> nearest neighbors to point <em>p</em> in your dataset.</li>
<li value="4">The mean target value of the <em>k</em> nearest neighbors is the prediction for the target value of <em>p</em>.</li>
</ol>
<p>You may have noticed that the k-NN process doesn’t require any matrix multiplication or calculus or really any math at all. Though it’s usually taught in only postgraduate-level computer science classes, k-NN is nothing more than a simple idea that children and even interns already intuitively grasp: that if things resemble each other in some ways, they’re likely to resemble each other in other ways. If things live in the same neighborhood, they might be similar to each other.</p>
<h3 id="h2-502888c06-0001"><span epub:type="pagebreak" id="Page_126" title="126"/>Implementing k-NN</h3>
<p class="BodyFirst">Writing code for k-NN supervised learning is straightforward. We’ll start by defining <code>k</code>, the number of neighbors we’ll look at, and <code>newsentiment</code>, which will hold the <code>global_sentiment_polarity</code> of the hypothetical new article we want to make a prediction about. In this case, let’s suppose that we receive another new article, and this one has a sentiment score of 0.5:</p>
<pre><code>k=15
newsentiment=0.5</code></pre>
<p>So, we’ll be predicting the number of shares that will be achieved by a new article with a sentiment score of 0.5. We’ll look at the 15 nearest neighbors of our new article to make these predictions. It will be convenient to convert our polarity and shares data to lists, as follows:</p>
<pre><code>allsentiment=news[' global_sentiment_polarity'].tolist()
allshares=news[' shares'].tolist()</code></pre>
<p>Next, we can calculate the distance between every article in our dataset and the hypothetical new article:</p>
<pre><code>distances=[abs(x-newsentiment) for x in allsentiment]</code></pre>
<p>This snippet uses a list comprehension to calculate the absolute value of the difference between the sentiment of each existing article and the sentiment of our new article.</p>
<p>Now that we have all these distances, we need to find which are the smallest. Remember, the articles with the smallest distance to the new article are the nearest neighbors, and we’ll use them to make our final prediction. A useful function in Python’s NumPy package enables us to easily find the nearest neighbors:</p>
<pre><code>import numpy as np
idx = np.argsort(distances)</code></pre>
<p>In this snippet, we import NumPy and then define a variable called <code>idx</code>, which is short for <em>index</em>. If you run <code class="bold">print(idx[0:k])</code>, you can see what this variable consists of:</p>
<pre><code>[30230, 30670, 13035, 7284, 36029, 19361, 29598, 22546, 25556, 6744, 26473,\
7211, 9200, 15198, 31496]</code></pre>
<p>These 15 numbers are the index numbers of the nearest neighbors. The 30,230th article in our dataset has the <code>global_sentiment_polarity</code> that is closest to 0.5 out of all articles in the data. The 30,670th article has the <code>global_sentiment_polarity</code> that’s second closest, and so on. The <code>argsort()</code> method we use is a convenient method that sorts the distances list from smallest to largest, then provides the indices of the <code>k</code> smallest distances (the indices of the nearest neighbors) to us.</p>
<p><span epub:type="pagebreak" id="Page_127" title="127"/>After we know the indices of the nearest neighbors, we can create a list of the number of shares associated with each neighbor:</p>
<pre><code>nearbyshares=[allshares[i] for i in idx[0:k]]</code></pre>
<p>Our final prediction is just the mean of this list:</p>
<pre><code>print(np.mean(nearbyshares))</code></pre>
<p>You should get the output 7344.466666666666, indicating that past articles with sentiment equal to about 0.5 get about 7,344 social media shares, on average. If we trust the logic of k-NN, we should expect that any future article that has sentiment about equal to 0.5 will also get about 7,344 social media shares.</p>
<h3 id="h2-502888c06-0002">Performing k-NN with Python’s sklearn</h3>
<p class="BodyFirst">We don’t have to go through that whole process every time we want to use k-NN for prediction. Certain Python packages can perform k-NN for us, including the sklearn package, whose relevant module we can import into Python as follows:</p>
<pre><code>from sklearn.neighbors import KNeighborsRegressor</code></pre>
<p>You may be surprised that the module we import here is called <code>KNeighborsRegressor</code>. We just finished describing how k-NN is very different from linear regression, so why would a k-NN module be using the word <em>regressor</em> just like a linear regression module does?</p>
<p>The k-NN method is certainly not linear regression, and it doesn’t use any of the matrix algebra that linear regression relies on, and it doesn’t output regression lines like linear regression. However, since it’s a supervised learning method, it’s accomplishing the same goal as linear regression: determining a function that maps features to targets. Since regression was the dominant supervised learning method for well over a century, people began to think of <em>regression</em> as synonymous with <em>supervised learning</em>. So people started to call k-NN functions <em>k-NN regressors</em> because they accomplish the same goal as regression, though without doing any actual linear regression.</p>
<p>Today, the words <em>regression</em> and <em>regressors</em> are used for all supervised learning methods that make predictions about continuous, numeric target variables, regardless of whether they’re actually related to linear regression. Since supervised learning and data science are relatively new fields (compared to mathematics, which has been around for millennia), many instances of confusing or redundant terminology like these remain that haven’t been cleaned up; part of learning data science is getting used to these confusing names.</p>
<p>Just as we’ve done with linear regression, we need to reshape our sentiment list so that it’s in the format this package expects:</p>
<pre><code>x=np.array(allsentiment).reshape(-1,1)
y=np.array(allshares)</code></pre>
<p><span epub:type="pagebreak" id="Page_128" title="128"/>Now, instead of calculating distances and indices, we can simply create a “regressor” and fit it to our data:</p>
<pre><code>knnregressor = KNeighborsRegressor(n_neighbors=15)
knnregressor.fit(x,y)</code></pre>
<p>Now we can find the prediction our classifier makes for any sentiment, as long as it’s properly reshaped:</p>
<pre><code>print(knnregressor.predict(np.array([newsentiment]).reshape(1,-1)))</code></pre>
<p>This k-NN regressor has predicted that the new article will receive 7,344.46666667 shares. This exactly matches the number we got before, when doing the k-NN process manually. You should be pleased that the numbers match: it means that you know how to write code for k-NN at least as well as the authors of the respected and popular sklearn package.</p>
<p>Now that you’ve learned a new supervised learning method, think about how it’s similar to and different from linear regression. Both linear regression and k-NN rely on feature variables and a target variable, as shown in <a href="#figure6-3">Figure 6-3</a>. Both create a learned function that maps feature variables to the target variable. In the case of linear regression, the learned function is a linear sum of variables multiplied by coefficients, in the form shown in <a href="#equation6-1" id="equationanchor6-2">Equation 6-1</a>. In the case of k-NN, the learned function is a function that finds the mean target value for <em>k</em> nearest neighbors in the relevant dataset.</p>
<p>While linear regression implicitly expresses a model of the world in which all variables can be related to each other by lines, k-NN implicitly expresses a model of the world in which neighborhoods of points are all similar to each other. These models of the world, and the learned functions they imply, are quite different. Because the learned functions are different, they could make different predictions about numbers of article shares or anything else we want to predict. But the goal of accurately predicting a target variable is the same in both cases, and so both are commonly used supervised learning methods.</p>
<h2 id="h1-502888c06-0006">Using Other Supervised Learning Algorithms</h2>
<p class="BodyFirst">Linear regression and k-NN are only two of many supervised learning algorithms that can be used for our prediction scenario. The same sklearn package that allowed us to easily do k-NN regression can also enable us to use these other supervised learning algorithms. <a href="#listing6-1" id="listinganchor6-1">Listing 6-1</a> shows how to do supervised learning with five methods, each using the same features and target variables, but with different supervised learning algorithms (different learned functions):</p>
<pre><code>#linear regression
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(np.array(allsentiment).reshape(-1,1), np.array(allshares))
print(regressor.predict(np.array([newsentiment]).reshape(1,-1)))

<span epub:type="pagebreak" id="Page_129" title="129"/>#knn
from sklearn.neighbors import KNeighborsRegressor
knnregressor = KNeighborsRegressor(n_neighbors=15)
knnregressor.fit(np.array(allsentiment).reshape(-1,1), np.array(allshares))
print(knnregressor.predict(np.array([newsentiment]).reshape(1,-1)))

#decision tree
from sklearn.tree import DecisionTreeRegressor
dtregressor = DecisionTreeRegressor(max_depth=3)
dtregressor.fit(np.array(allsentiment).reshape(-1,1), np.array(allshares))
print(dtregressor.predict(np.array([newsentiment]).reshape(1,-1)))

#random forest
from sklearn.ensemble import RandomForestRegressor
rfregressor = RandomForestRegressor()
rfregressor.fit(np.array(allsentiment).reshape(-1,1), np.array(allshares))
print(rfregressor.predict(np.array([newsentiment]).reshape(1,-1)))

#neural network
from sklearn.neural_network import MLPRegressor
nnregressor = MLPRegressor()
nnregressor.fit(np.array(allsentiment).reshape(-1,1), np.array(allshares))
print(nnregressor.predict(np.array([newsentiment]).reshape(1,-1)))</code></pre>
<p class="CodeListingCaption"><a id="listing6-1">Listing 6-1</a>: A collection of five supervised learning methods</p>
<p>This snippet contains five sections of four code lines each. The first two sections are for linear regression and k-NN; they’re the same code we ran previously to use sklearn’s prebuilt packages to easily get linear regression and k-NN predictions. The other three sections have the exact same structure as the first two sections:</p>
<ol class="decimal">
<li value="1">Import the package.</li>
<li value="2">Define a “regressor.”</li>
<li value="3">Fit the regressor to our data.</li>
<li value="4">Use the fitted regressor to print a prediction.</li>
</ol>
<p>The difference is that each of the five sections uses a different kind of regressor. The third section uses a decision tree regressor, the fourth uses a random forest regressor, and the fifth uses a neural network regressor. You may not know what any of these types of regressors are, but you can think of that as a convenient thing: supervised learning is so easy that you can write code to build models and make predictions before you even know what the models are! (That’s not to say this is a good practice—it’s always better to have a solid theoretical understanding of every algorithm you use.)</p>
<p>Describing every detail of all these supervised learning algorithms goes beyond the scope of this book. But we can provide a sketch of the main ideas. Each approach accomplishes the same goal (prediction of a target variable), but does it using different learned functions. In turn, these learned functions implicitly express different assumptions and different math or, in other words, different models of the world.</p>
<h3 id="h2-502888c06-0003"><span epub:type="pagebreak" id="Page_130" title="130"/>Decision Trees</h3>
<p class="BodyFirst">Let’s begin by looking at decision trees, the first type of model in our code after our k-NN section. Instead of assuming that variables are related by lines (like linear regression) or by membership in neighborhoods (like k-NN), <em>decision trees</em> assume that the relationships among variables can be best expressed as a tree that consists of binary splits. Don’t worry if that description doesn’t sound immediately clear; we’ll use sklearn’s decision tree–plotting function to create a plot of the decision tree regressor called <code>dtregressor</code> that was created by the code in <a href="#listing6-1">Listing 6-1</a>:</p>
<pre><code>from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(16,5))
plot_tree(dtregressor, filled=True, fontsize=8)
plt.savefig('decisiontree.png')</code></pre>
<p>We can see the result in <a href="#figure6-4" id="figureanchor6-4">Figure 6-4</a>.</p>
<figure>
<img alt="" class="" height="306" src="image_fi/502888c06/f06004.png" width="694"/>
<figcaption><p><a id="figure6-4">Figure 6-4</a>: A decision tree for predicting article shares based on sentiment</p></figcaption>
</figure>
<p>We can follow this flowchart to make predictions about shares, given any <code>global_sentiment_polarity</code>. Since the flowchart has a branching structure that resembles a tree’s branches, and since it enables decision-making, we call it a <em>decision tree</em>.</p>
<p>We start at the box at the top of the tree. The first line of the box expresses a condition: <code>X[0] &lt;= 0.259</code>. Here, <code>X[0]</code> is referring to the <code>global_sentiment_polarity</code> variable, which is the only feature in our dataset. If that condition is true, we proceed along the leftward arrow to a box on the next lowest level. Otherwise, we proceed along the rightward arrow to the other side of the tree. We continue to check the conditions in each box until we arrive at a box that specifies no condition and has no arrows pointing to other, lower boxes. We then check the value specified there and use that as our prediction.</p>
<p>For the sentiment value we’ve been working with in our example (0.5), we go right from the first box because 0.5 &gt; 0.259, then we go right at the second box for the same reason, and then we go right yet again at our third <span epub:type="pagebreak" id="Page_131" title="131"/>box because 0.5 &gt; 0.263. Finally, we arrive at the fourth box, which doesn’t have any condition to check, and we get our prediction: about 3,979 shares for an article with sentiment polarity 0.5.</p>
<p>If you create this decision tree at home, you’ll see that some of the boxes are shaded or colored. This shading is done automatically, and the level of shading applied is proportional to the value predicted by the decision tree. For example, you can see that one box in <a href="#figure6-4">Figure 6-4</a> indicates a prediction of 57,100 shares, and it has the darkest shading. Boxes that predict lower numbers of shares will have lighter shading or no shading at all. This automatic shading is done to highlight especially high predicted values.</p>
<p>You can find the details of how sklearn creates the decision tree in <a href="#figure6-4">Figure 6-4</a> in advanced machine learning textbooks. For most standard business use cases, the details and math of optimizing decision trees is not as important as the much easier task of writing a few simple Python lines to create one and then read its plot.</p>
<p>The decision tree in <a href="#figure6-4">Figure 6-4</a> can be generated with only a few lines of code and can be interpreted without any special training. This means that decision trees are well suited to business applications. You can quickly generate a decision tree and show it to clients or company leaders, and explain it without needing to go into any math, computer science, or other difficult topics. Because of this, data scientists often say that decision trees are <em>interpretable models</em>, in contrast to other models like neural networks that are more opaque and difficult to quickly understand or explain. A decision tree can be a natural, quick addition to any presentation or report that can provide visual interest and can help others understand a dataset or prediction problem. These are important advantages of decision trees in business applications. On the other hand, decision trees tend to have lower accuracy than other, more complex methods like random forests (see the next section).</p>
<p>Just like linear regression and k-NN, a decision tree uses a feature of data (in this case, sentiment) to make a prediction about a target (in this case, shares). The difference is that decision trees don’t rely on an assumption that the variables are related by a line (the assumption of linear regression) or that the variables are related by small neighborhoods around points (the assumption of k-NN). Instead, decision trees are built with the assumption that the branching structure shown in <a href="#figure6-4">Figure 6-4</a> is the appropriate model of the world.</p>
<h3 id="h2-502888c06-0004">Random Forests</h3>
<p class="BodyFirst">The fourth section of <a href="#listing6-1">Listing 6-1</a> uses <em>random forests</em> for prediction. Random forests are a type of <em>ensemble method</em>. Ensemble methods got their name because they consist of a collection of many simpler methods. As you might surmise from the name, random forests consist of a collection of simpler decision trees. Every time you use a random forest regressor for prediction, the sklearn code creates many decision tree regressors. Each of the individual decision tree regressors is created with a different subset of the training data and a different subset of the training features. The final random forest prediction is the mean of the predictions made by each of the many individual decision trees.</p>
<p><span epub:type="pagebreak" id="Page_132" title="132"/>In the context of <a href="#figure6-3">Figure 6-3</a>, random forests learn a complicated function: one that consists of a mean of many learned functions from multiple randomly selected decision trees. Nevertheless, because random forests learn a function that maps features to a target variable, they are a standard supervised learning method, just like linear regression, k-NN, and all the rest.</p>
<p>Random forests have become popular because their code is relatively easy to write and they often have much better accuracy than decision trees or linear regressions. These are their main advantages. On the other hand, while we can draw an easily interpretable representation of a decision tree, like <a href="#figure6-4">Figure 6-4</a>, random forests often consist of hundreds of unique decision trees averaged together, and it’s not easy to draw a representation of a random forest in a way a human can understand. Choosing a random forest as your supervised learning method will probably increase your accuracy, but at the cost of interpretability and explainability. Every supervised learning method has advantages and disadvantages, and choosing the right trade-offs that are appropriate for your situation is important for any data scientist who wants to succeed at supervised learning.</p>
<h3 id="h2-502888c06-0005">Neural Networks</h3>
<p class="BodyFirst"><em>Neural networks</em> have become extremely popular in recent years as our computer hardware has matured to the point of being able to handle their computational complexity. The complexity of neural networks also makes them hard to describe succinctly, except to say that we can use them for supervised learning. We can start by showing a diagram of one particular neural network (<a href="#figure6-5" id="figureanchor6-5">Figure 6-5</a>).</p>
<figure>
<img alt="" class="" height="467" src="image_fi/502888c06/f06005.png" width="694"/>
<figcaption><p><a id="figure6-5">Figure 6-5</a>: A diagram of a neural network</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_133" title="133"/>This plot is a representation of a neural network’s learned function. In this plot, you can see a column of 13 circles, called <em>nodes</em>, on the left side. These 13 nodes are collectively called the <em>input layer</em> of the neural network. Each node of the input layer represents one feature of the training data. The single node on the far right represents the neural network’s final prediction of a target variable. All of the lines and nodes between the left and right represent a complex learned function that maps feature inputs to the final target prediction.</p>
<p>For example, you can see that the topmost node in the leftmost column (labeled <em>A</em>) has an arrow pointing to another node (labeled <em>B</em>), with the number 4.52768 written near it. This number is a <em>weight</em>, and we’re supposed to multiply this weight by the value of the feature corresponding to node A. We then add the result of that multiplication to a running total that corresponds to node B. You can see that node B has 13 arrows pointing to it, one for each node in the input layer. Each feature will be multiplied by a different weight, and the product of the feature value and the weight will be added to the running total for node B. Then, the number –0.14254 will be added to the result; this is the number drawn on an arrow between a blue node with a 1 inside it and node B. (This blue node is also called a <em>bias node</em>.)</p>
<p>After all this multiplication and addition, we’ll have a running total for node B, and we’ll apply a new function called an <em>activation function</em> to it. Many possible activation functions exist, one of which is the logistic function you met in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>. After we apply our activation function, we’ll have a final numeric value for node B. We’ve only barely begun the process of calculating the neural network’s learned function. You can see that node B has four arrows emanating from it, each pointing to other nodes further to the right. For each of those arrows, we’ll have to follow the same steps of multiplying weights by node values, adding to running totals for every node, and applying activation functions. After we do this for all the nodes and all the arrows in the diagram, we’ll have a final value for the rightmost node: this will be our prediction of the target value.</p>
<p>Neural networks are designed in such a way that this whole process, including repeated multiplication and addition and activation functions, should give us a highly accurate prediction as its final output. The complexity of neural networks can be a challenge, but it’s also what enables them to accurately model our complex nonlinear world.</p>
<p>These networks are called <em>neural</em> because the nodes and arrows in <a href="#figure6-5">Figure 6-5</a> resemble the neurons and synapses in a brain. This resemblance is mostly superficial. You could depict neural networks in a way that didn’t look like a brain, or you could write down other methods like linear regression in a way that did look like a brain.</p>
<p>To really master neural networks, you need to learn a lot more. Some of the interesting advances in neural networks have come from experimenting with different structures, or <em>architectures</em>, of the nodes. For example, <em>deep neural networks</em> have many layers between the leftmost input nodes and the rightmost output. <em>Convolutional neural networks</em> add an extra type of layer that performs a special operation called <em>convolution</em> to the network <span epub:type="pagebreak" id="Page_134" title="134"/>structure. <em>Recurrent neural networks</em> allow connections to flow in multiple directions, instead of just from left to right.</p>
<p>Researchers have found remarkable applications for neural networks in computer vision (like recognizing a dog or a cat or a car or a person), language processing (like machine translation and speech recognition), and much more. On the other hand, neural networks are hard to interpret, hard to understand, and hard to train properly, and they sometimes require specialized hardware. These downsides sometimes make neural networks an unattractive option in business applications despite their power.</p>
<h2 id="h1-502888c06-0007">Measuring Prediction Accuracy</h2>
<p class="BodyFirst">Whichever supervised learning model we choose, after we fit it, we’ll want to measure its prediction accuracy. Here is how we do it for our scenario of predicting shares of articles:</p>
<pre><code>allprediction=regressor.predict(np.array([allsentiment]).reshape(-1,1))
predictionerror=abs(allprediction-allsentiment)
print(np.mean(predictionerror))</code></pre>
<p>This simple snippet calculates the MAE, as we’ve done before. In the first line, we use our regressor’s <code>predict()</code> method to predict the number of shares for each article in our dataset. (Remember, this <code>regressor</code> is the linear regression model we created near the beginning of the chapter. If you’d like, you can replace <code>regressor</code> with <code>rfregressor</code> or <code>nnregressor</code> to measure the accuracy of our random forest or our neural network, respectively.) In the second line, we calculate the prediction error of these predictions: this is simply the absolute value of the difference between predicted and actual. The mean of our prediction error, calculated in the third line, is a measurement of how well our particular supervised learning method performed, where 0 is the best-possible value, and higher values are worse. We can use this process to calculate prediction accuracy for many supervised learning algorithms, and then choose the algorithm that leads to the highest accuracy (the lowest mean absolute error) as the best method for our scenario.</p>
<p>The only problem with this approach is that it doesn’t resemble a true prediction scenario. In real life, we’d have to make predictions for articles that were not in our training dataset—articles that our regressor had never seen during its training process. In contrast, we’ve taken a dataset of articles from 2013 and 2014, fit a regressor to that whole dataset, and then judged our accuracy based on the same 2013–14 dataset that was used to fit our regressor. Since we judged our accuracy based on the same data that was used to fit our regressor, what we’ve done isn’t truly prediction. It’s <em>postdiction—</em>saying what happened after it happened instead of before. When we do postdiction, we’re liable to be guilty of overfitting, the dastardly peril that we already encountered in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>.</p>
<p>To avoid the problems of postdiction and overfitting, we can take the same approach we took in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>: split our dataset into two mutually exclusive subsets, a training set and a test set. We use the training set to <span epub:type="pagebreak" id="Page_135" title="135"/>train the data or, in other words, to allow our supervised learning model to learn its learned function. After we train the data using only the training dataset, we test it using the test data. The test data, since it wasn’t used in the training process, is “as if” from the future, since our regressor hasn’t used it for learning, even if it’s actually from the past.</p>
<p>The sklearn package has a convenient function we can use to split our data into training and test sets:</p>
<pre><code>from sklearn.model_selection import train_test_split
x=np.array([allsentiment]).reshape(-1,1)
y=np.array(allshares)
trainingx,testx,trainingy,testy=train_test_split(x,y,random_state=1)</code></pre>
<p>The four outputs of this snippet are <code>trainingx</code> and <code>trainingy</code>—the <em>x</em> and <em>y</em> components of our training data—and <code>testx</code> and <code>testy</code>—the <em>x</em> and <em>y</em> components of our test data. Let’s check the length of each of these outputs:</p>
<pre><code>&gt;&gt;&gt; <b>print(len(trainingx))</b>
29733
&gt;&gt;&gt; <b>print(len(trainingy))</b>
29733
&gt;&gt;&gt; <b>print(len(testx))</b>
9911
&gt;&gt;&gt; <b>print(len(testy))</b>
9911</code></pre>
<p>You can see that our training data consists of <code>trainingx</code> (the sentiment scores of the training examples) and <code>trainingy</code> (the share statistics of the training examples). Both of these training datasets consist of 29,733 observations, or 75 percent of the data. The test datasets (<code>testx</code> and <code>testy</code>) consist of 9,911 observations, or the other 25 percent. This type of split follows the same approach that we took in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>: training our model with the majority of the data and testing our model with a smaller minority of the data.</p>
<p>One important difference between the training/test split we did here and the training/test split we did in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span> is that, in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>, we used earlier data (the first years of our dataset) as training data and later data (the last years of our dataset) as test data. Here, we don’t do a before/after split for our training and test data. Instead, the <code>train_test_split()</code> function we used performs a random split: randomly choosing training and test sets, instead of neatly selecting from earlier and later times. This is an important distinction to remember: for time-series data (data recorded at regular, ordered intervals), we choose training and test sets based on a split between earlier and later data, but for all other datasets, we select training and test sets randomly.</p>
<p>Next, we need to train our models by using these training sets, and we need to calculate prediction error by using these test sets:</p>
<pre><code>rfregressor = RandomForestRegressor(random_state=1)
rfregressor.fit(trainingx, trainingy)
predicted = rfregressor.predict(testx)
predictionerror = abs(predicted-testy)</code></pre>
<p><span epub:type="pagebreak" id="Page_136" title="136"/>You can see in this snippet that we fit the regressor by using only the training data. Then we calculate the prediction error by using only the test data. Even though all of our data comes from the past, by making predictions for data that weren’t included in our training, we’re making sure that our process resembles a true prediction process instead of postdiction.</p>
<p>We can see the error on the test set by running <code>print(np.mean(predictionerror))</code>. You’ll see that the mean prediction error on our test set is about 3,816 when using our random forest regressor.</p>
<p>We can also do the same with our other regressors. For example, this is how we can check the prediction error of our k-NN regressor:</p>
<pre><code>knnregressor = KNeighborsRegressor(n_neighbors=15)
knnregressor.fit(trainingx, trainingy)
predicted = knnregressor.predict(testx)
predictionerror = abs(predicted-testy)</code></pre>
<p>Again, we can use <code>print(np.mean(predictionerror))</code> to find out whether this method seems to perform better than our other supervised learning methods. When we do, we find that our k-NN regressor has a mean prediction error equal to about 3,292 on the test set. In this case, k-NN has better performance than random forests, as measured by prediction error on the test set. When we want to choose the best supervised learning method for a particular scenario, the simplest way to do it is to choose the one with the lowest prediction error <em>on a test set</em>.</p>
<h2 id="h1-502888c06-0008">Working with Multivariate Models</h2>
<p class="BodyFirst">So far in this chapter, we’ve worked with only univariate supervised learning, meaning that we’ve used only one feature (sentiment) to predict shares. Once you know how to do univariate supervised learning, jumping to <em>multivariate supervised learning</em>, where we use multiple features to predict a target, is completely straightforward. All we need to do is specify more features in our <em>x </em>variable, as follows:</p>
<pre><code>x=news[[' global_sentiment_polarity',' n_unique_tokens',' n_non_stop_words']]
y=np.array(allshares)
trainingx,testx,trainingy,testy=train_test_split(x,y,random_state=1)
from sklearn.ensemble import RandomForestRegressor
rfregressor = RandomForestRegressor(random_state=1)
rfregressor.fit(trainingx, trainingy)
predicted = rfregressor.predict(testx)
predictionerror = abs(predicted-testy)</code></pre>
<p>Here, we specify an <code>x</code> variable that contains not only the sentiment of an article but also two other features from other columns in our dataset. After that, the process is the same as we’ve followed before: splitting into a training and test set, creating and fitting a regressor using a training set, and calculating prediction error on a test set. When we run <code>print(np.mean(predictionerror))</code> now, we see that our multivariate model has a mean prediction error equal to <span epub:type="pagebreak" id="Page_137" title="137"/>about 3,474, indicating that our multivariate random forest model performs better than our univariate random forest model on our test set.</p>
<h2 id="h1-502888c06-0009">Using Classification Instead of Regression</h2>
<p class="BodyFirst">So far, this whole chapter has presented various ways to predict shares, given different features of an article. The <code>shares</code> variable can take any integer value from 0 to infinity. For data like that (continuous, numeric variables), it’s appropriate to use regression to predict the values it will take. We used linear regression, k-NN regression, decision tree regression, random forest regression, and neural network regression: five supervised learning methods, all of them used to predict targets that can take a wide range of values.</p>
<p>Instead of doing prediction and regression, we may want to do categorical classification, as we did in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>. In our business scenario, we might not be interested in predicting a precise number of shares. Instead, we may be interested only in whether an article will reach a number of shares that’s higher than the median number. Deciding whether something is above or below a median is a classification scenario, since it consists of deciding true/false to a question with only two possible answers.</p>
<p>We can create a variable that enables us to do classification as follows:</p>
<pre><code>themedian=np.median(news[' shares'])
news['abovemedianshares']=1*(news[' shares']&gt;themedian)</code></pre>
<p>Here, we create a <code>themedian</code> variable that represents the median value of shares in our dataset. Then we add a new column to the <code>news</code> dataset called <code>abovemedianshares</code>. This new column is 1 when an article’s share count is above the median, and it’s 0 otherwise. This new measurement is derived from a numeric measurement (number of shares), but we can think of it as a categorical measurement: one that expresses a true/false proposition of whether an article is in the high-share category. Since our business goal is to publish high-share articles and not publish low-share articles, being able to accurately classify new articles as likely high-share articles or likely low-share articles would be useful to us.</p>
<p>To perform classification instead of regression, we need to change our supervised learning code. But luckily, the changes we have to make are minor. In the following snippet, we use classifiers instead of regressors for our new categorical target variable:</p>
<pre><code>x=news[[' global_sentiment_polarity',' n_unique_tokens',' n_non_stop_words']]
y=np.array(news['abovemedianshares'])
from sklearn.neighbors import <b>KNeighborsClassifier</b>
<b>knnclassifier</b> = <b>KNeighborsClassifier</b>(n_neighbors=15)
trainingx,testx,trainingy,testy=train_test_split(x,y,random_state=1)
<b>knnclassifier</b>.fit(trainingx, trainingy)
predicted = <b>knnclassifier</b>.predict(testx)</code></pre>
<p><span epub:type="pagebreak" id="Page_138" title="138"/>You can see that the difference between the regression we were doing before and the classification we’re doing here is quite minor. The only changes are shown in bold. In particular, instead of importing the <code>KNeighborsRegressor</code> module, we import the <code>KNeighborsClassifier</code> module. Both modules use k-NN, but one is designed for regression and the other for classification. We name our variable <code>knnclassifier</code> instead of <code>knnregressor</code>, but beyond that, the supervised learning process is just the same: importing a supervised learning module, splitting data into training and test sets, fitting the model to a training dataset, and finally using the fit model for predictions on a test set.</p>
<p>You should remember from <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span> that we usually measure accuracy differently in classification scenarios than we do in regression scenarios. The following snippet creates a confusion matrix, just like the ones we made in <span class="xref" itemid="xref_target_Chapter 5">Chapter 5</span>:</p>
<pre><code>from sklearn.metrics import confusion_matrix
print(confusion_matrix(testy,predicted))</code></pre>
<p>Remember that the output of this code is a confusion matrix that shows the number of true positives, true negatives, false positives, and false negatives on our test set. The confusion matrix looks like this:</p>
<pre><code>[[2703 2280]
 [2370 2558]]</code></pre>
<p>Remember that every confusion matrix has the following structure:</p>
<pre><code>[[<var>true positives </var>      <var>false positives</var>]
[<var>false negatives</var>     <var>true negatives</var>]]</code></pre>
<p>So, when we look at our confusion matrix, we find that our model made 2,703 true-positive classifications: our model predicted above-median shares for 2,703 articles, and those articles did have above-median shares. We have 2,280 false positives: predictions of above-median shares for articles that instead had below-median shares. We have 2,370 false negatives: predictions for below-median shares for articles that instead had above-median shares. Finally, we have 2,558 true negatives: predictions of below-median shares for articles that did have below-median shares.</p>
<p>We can calculate our precision and recall as follows:</p>
<pre><code>from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

precision = precision_score(testy,predicted)
recall = recall_score(testy,predicted)</code></pre>
<p>You’ll see that our precision is equal to about 0.53, and our recall is equal to about 0.52. These are not extremely encouraging values; precision and recall are supposed to be as close to 1 as possible. One reason these values are so low is that we’re trying to make difficult predictions. It’s inherently <span epub:type="pagebreak" id="Page_139" title="139"/>hard to know the number of shares an article will get, no matter how good your algorithms are.</p>
<p>It’s important to remember that even though supervised learning is a sophisticated set of methods based on ingenious ideas and executed on powerful hardware, it’s not magic. Many things in the universe are inherently difficult to predict, even when using the best-possible methods. But just because perfect prediction may be impossible doesn’t mean we shouldn’t try to make predictions at all. In this case, a model that helps us even a little bit is better than nothing.</p>
<h2 id="h1-502888c06-0010">Summary</h2>
<p class="BodyFirst">In this chapter, we explored supervised learning. We started with a business scenario related to prediction. We reviewed linear regression, including its shortcomings. We then talked about supervised learning in general and introduced several other supervised learning methods. We went on to discuss some finer points of supervised learning, including multivariate supervised learning and classification.</p>
<p>In the next chapter, we’ll discuss supervised learning’s less popular younger sibling: unsupervised learning. Unsupervised learning gives us powerful ways to explore and understand hidden relationships in data, without even using a target variable for supervision. Supervised learning and unsupervised learning together make up the bulk of machine learning, one of the most essential data science skills.</p>
</section>
</div></body></html>