- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Forecasting
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: Let’s look at some data science tools that can help you predict the future.
    In this chapter, we’ll introduce a simple business scenario in which a company
    needs to forecast customer demand. We’ll then talk about how tools from data science
    can be applied to make an accurate forecast and how that forecast can lead to
    better business decision-making.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些可以帮助你预测未来的数据科学工具。在本章中，我们将介绍一个简单的商业场景，假设一家公司需要预测客户需求。然后，我们将讨论如何应用数据科学工具来做出准确的预测，以及如何通过这些预测做出更好的商业决策。
- en: We’ll use linear regression for our forecasting, and we’ll discuss both univariate
    and multivariate linear regression. Finally, we’ll look at extrapolation of regression
    lines and how to evaluate various regression models to choose the best one.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用线性回归进行预测，并讨论单变量和多变量线性回归。最后，我们将研究回归线的外推以及如何评估各种回归模型，以选择最佳模型。
- en: Predicting Customer Demand
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测客户需求
- en: 'Imagine that you’re running a car dealership in Quebec, Canada. You are using
    a standard business model for retail: you buy cars from a manufacturer at a low
    price and then sell those cars to individual customers at higher prices. Every
    month, you need to decide how many cars you’ll order from the manufacturer. If
    you order too many cars, you’ll be unable to sell them all quickly, resulting
    in high storage costs or cash flow problems. If you order too few cars, you won’t
    be able to meet your customers’ demands.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在加拿大魁北克经营一家汽车经销商。你使用的是标准的零售商业模式：你以低价从制造商那里购买汽车，然后以较高的价格卖给个人客户。每个月，你需要决定从制造商那里订购多少汽车。如果你订购了太多汽车，你将无法快速卖出，导致高额的存储成本或现金流问题。如果你订购的汽车太少，你将无法满足客户的需求。
- en: Ordering the right number of cars is important. But what is the right number?
    The answer depends on certain business considerations, such as the cash in your
    bank account and how much you want to grow—but in a typical month, the right number
    of cars to order is exactly the number of cars that customers will want to buy
    during the coming month. Since we can’t ◊see into the future, we need to forecast
    the demand and place an order based on our forecast.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 订购正确数量的汽车很重要。但什么是正确的数量呢？答案取决于一些商业考虑因素，比如你的银行账户中的现金和你希望增长的规模——但在一个典型的月份，正确的订购数量正好是客户在下个月愿意购买的汽车数量。由于我们无法预测未来，我们需要通过预测需求来下订单。
- en: We can choose from several proven quantitative methods to obtain a forecast
    of next month’s demand. One of the best methods is *linear regression*. In the
    remainder of this chapter, we’ll explain how to use linear regression for forecasting.
    We’ll use past data to predict future data, to learn the number of cars we need
    to order. We’ll start simply, just by reading in and looking at some data, and
    then proceed to the other steps of the forecasting process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择几种经过验证的定量方法来获得下个月需求的预测。最好的方法之一是*线性回归*。在本章的其余部分，我们将解释如何使用线性回归进行预测。我们将利用过去的数据来预测未来的数据，从而了解我们需要订购多少汽车。我们将从简单的步骤开始，先读取并查看一些数据，然后进入预测过程的其他步骤。
- en: Cleaning Erroneous Data
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理错误数据
- en: The data we’ll analyze to forecast the future is a record of the number of cars
    sold by dealerships in Quebec, Canada, for each of 108 consecutive months. This
    data was originally made available online by Rob Hyndman, a statistics professor
    and forecasting guru. You can download the data from [https://bradfordtuckfield.com/carsales.csv](https://bradfordtuckfield.com/carsales.csv).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析的数据是魁北克的汽车经销商在连续108个月中每月销售的汽车数量记录。这些数据最初由统计学教授和预测专家Rob Hyndman在线提供。你可以从[https://bradfordtuckfield.com/carsales.csv](https://bradfordtuckfield.com/carsales.csv)下载这些数据。
- en: This data is old; the most recent month recorded is December 1968\. Therefore,
    for this scenario, we’ll be imagining that we live in December 1968, and we’ll
    make forecasts for January 1969\. The forecasting principles we’ll discuss will
    be evergreen, so if you can use data from 1968 to forecast results in 1969, you’ll
    be able to use data from year *n* to forecast results from year *n* + 1, for *n*
    = 2,023 or 3,023 or any other number.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据较旧；记录的最近一个月是1968年12月。因此，在这个场景中，我们将假设我们生活在1968年12月，并为1969年1月做出预测。我们将讨论的预测原则是长久有效的，因此，如果你能用1968年的数据预测1969年的结果，你也能用年份*n*的数据预测年份*n*
    + 1的结果，*n* = 2,023 或 3,023 或任何其他年份。
- en: 'Save this file into the same directory where you’re running Python. Then we’ll
    read our data by using Python’s pandas package:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将此文件保存在你运行Python的相同目录下。然后我们将使用Python的pandas包读取数据：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we import pandas and give it the alias `pd`. We then use its `read_csv()`
    method to read our data into Python and store it in the variable `carsales`. The
    pandas package we import and use here is a powerful module that makes working
    with data in Python easier. The `carsales` object we create is a pandas dataframe,
    which is the standard pandas format for storing data in a Python session. Because
    the object is stored as a pandas dataframe, we’ll be able to use many helpful
    pandas methods to work with it, just as we did in Chapter 1. Let’s start by using
    the `head()` method that enables us to inspect pandas dataframes:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入pandas并给它取别名`pd`。然后我们使用它的`read_csv()`方法将数据读取到Python中，并存储在`carsales`变量中。我们在这里导入并使用的pandas包是一个强大的模块，它使得在Python中处理数据变得更加容易。我们创建的`carsales`对象是一个pandas数据框（dataframe），它是pandas在Python会话中存储数据的标准格式。由于这个对象被存储为pandas数据框，我们将能够像在第一章中一样，使用许多有用的pandas方法来处理它。让我们从使用`head()`方法开始，它让我们能够检查pandas数据框：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By looking at these rows, we can notice a few important points. First, we can
    see the column names. The column names in this dataset are `Month` and `Monthly
    car sales in Quebec 1960-1968`. The second column name will be easier to work
    with if we shorten it. We can do this easily in Python:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看这些行，我们可以注意到几个重要的点。首先，我们可以看到列名。这个数据集的列名是`Month`和`Monthly car sales in Quebec
    1960-1968`。第二个列名如果我们缩短它，将会更容易处理。在Python中我们可以很容易做到这一点：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this snippet, we access the columns of our dataframe and redefine them to
    have shorter names (`month` and `sales`, respectively).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们访问了数据框的列并重新定义它们，使用了更简短的名称（分别是`month`和`sales`）。
- en: 'Just as the `head()` method prints the top five rows of a dataset, the `tail()`
    method prints the bottom five rows. If you run `print(carsales.tail())`, you’ll
    see the following output:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`head()`方法打印数据集的前五行一样，`tail()`方法打印数据集的后五行。如果你运行`print(carsales.tail())`，你会看到以下输出：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can see that the column names are shorter now and easier to read. But we
    also see that the very last row doesn’t contain car sales data. Instead, its first
    entry is a *tag*, or label, that tells us about the whole dataset. Its second
    entry is `NaN`, which stands for *not a number*, meaning that the entry contains
    no data or undefined data. We don’t need the label entry or the empty (`NaN`)
    entry, so let’s remove the entire last row (row 108):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在列名更短且更易读。但我们也看到，最后一行没有包含汽车销售数据。相反，它的第一项是一个*标签*，或者说是描述整个数据集的标签。它的第二项是`NaN`，代表*不是数字*，意味着该项没有数据或数据未定义。我们不需要标签项和空的（`NaN`）项，所以让我们删除整行最后一行（第108行）：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, we use the pandas `loc()` method to specify a selection of rows that
    we want to keep: in this case, all the rows between row 0 and row 107, inclusive.
    We use the colon (`:`) after the comma to indicate that we want to keep both of
    the dataset’s columns. We store the result in our `carsales` variable, thereby
    removing the superfluous row 108\. If you run `print(carsales.tail())` again,
    you’ll see that that row has been removed.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用pandas的`loc()`方法来指定我们想要保留的行：在这种情况下，我们保留从第0行到第107行（包括第107行）的所有行。我们在逗号后面使用冒号（`:`）来表示我们想要保留数据集的两列。我们将结果存储在`carsales`变量中，从而删除了多余的第108行。如果你再次运行`print(carsales.tail())`，你会看到该行已经被删除。
- en: Another thing we can see by looking at the head and tail of our data is the
    format of the month data. The first entry is `1960-01` (January of 1960), the
    second entry is `1960-02` (February of 1960), and so on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看数据的头部和尾部，我们还可以看到月份数据的格式。第一项是`1960-01`（1960年1月），第二项是`1960-02`（1960年2月），依此类推。
- en: As data scientists, we’re interested in doing numeric analyses using math, statistics,
    and other quantitative methods. Dates can present several tedious challenges that
    make it hard to do math and statistics the way we want to. The first challenge
    is that dates are sometimes not stored in a numeric data type. Here, the dates
    are stored as *strings*, or collections of characters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们有兴趣使用数学、统计学和其他定量方法进行数值分析。日期有时会带来一些繁琐的挑战，使得我们很难按照想要的方式进行数学和统计分析。第一个挑战是日期有时不是以数字数据类型存储的。在这里，日期被存储为*字符串*，即字符的集合。
- en: To see why this is an issue, try `print(1960+1)` in the Python console; you’ll
    notice that the result is `1961`. Python has seen that we’re working with two
    numbers, and it’s added them in the way we expect. Then, try `print('1960'+'1')`
    in the Python console; now you get `19601` as the result. Instead of adding two
    numbers, Python has seen that we’ve input strings and assumes that the `+` sign
    means that we want to do *concatenation*, simply fusing the strings together in
    a way that doesn’t follow the rules of math.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge with dates is that even when they’re in numeric form, they
    follow logic that is different from the logic of natural numbers. For example,
    if we add 1 to month 11, we get month 12, which follows the arithmetic rule that
    11 + 1 = 12\. But if we add 1 to month 12, we get month 1 again (since every December
    is followed by January of the next year), which is not consistent with the simple
    arithmetic of 12 + 1 = 13.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the simplest way to address the issues with the data type of
    our date data is to define a new variable called `period`. We can define it as
    follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our new `period` variable is just a list of all the numbers from 0 to 107\.
    We’ll refer to January 1960 as period 0, February 1960 as period 1, and so on
    until December 1968, the last month in our data, which we’ll call period 107\.
    This new variable is numeric, so we can add to it, subtract from it, or do any
    other mathematical operation with it. Also, it will follow the rules of standard
    arithmetic, with period 13 coming after period 12, as we expect in numeric variables.
    This simple solution is possible because in this particular dataset, the rows
    are organized in chronological order, so we can be sure that each period number
    is being assigned to the correct month.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: These simple tasks, like adding a numeric column for months, removing an extra
    row, and changing column names, are part of *data cleaning*. This is not a glamorous
    or particularly exciting process, but doing it right is extremely important because
    it lays a foundation for the more thrilling steps of the data science process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Plotting Data to Find Trends
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After these basic data-cleaning tasks, we should definitely plot the data.
    Plotting should be done early and often in every data science project. Let’s use
    the Matplotlib module to create a simple plot of our data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this snippet, we import the Matplotlib `pyplot` module and give it the alias
    `plt`. Then, we use the `scatter()` method to create a scatterplot of all the
    sales numbers in our data, organized by period (month). We also use a few lines
    to add axis labels and a plot title and then show the plot. [Figure 2-1](#figure2-1)
    shows the result.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c02/f02001.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-1: Car sales by month over nine years'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: This simple plot shows our `period` variable on the x-axis and `sales` on the
    y-axis. Each point represents one row of data or, in other words, the number of
    car sales for one particular month.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'See what interesting information pops out at you in this plot. Probably the
    most obvious thing is the gradual upward trend from left to right: sales appear
    to be increasing gradually over time. Other than this trend, the data seems noisy
    and scattered, with huge variations between one month and another. The variations
    within a year or season look random, noisy, and unpredictable. The linear regression
    method that we’ll implement next will attempt to capture the order and patterns
    in our data, and help us be less distracted by the randomness and noise.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 看看在这个图表中有什么有趣的信息能引起你的注意。可能最明显的事情是从左到右的逐渐上升趋势：销售似乎随着时间的推移在逐步增加。除了这个趋势，数据似乎杂乱无章，散布着巨大的波动，且每个月之间的波动也很大。一年或季节内的波动看起来是随机的、嘈杂的，并且不可预测。我们接下来将实现的线性回归方法将尝试捕捉数据中的顺序和模式，帮助我们不那么被随机性和噪声所干扰。
- en: So far, all we’ve done is read in the data and draw a simple plot. But already
    we’re starting to see patterns that will be useful for making accurate forecasts.
    Let’s go on to some more serious forecasting steps.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的只是读取数据并绘制一个简单的图表。但我们已经开始看到一些模式，这些模式将有助于我们做出准确的预测。接下来，我们将进入一些更严谨的预测步骤。
- en: Performing Linear Regression
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行线性回归
- en: 'Now that we’ve cleaned the data, plotted it, and noticed some basic patterns,
    we’re ready to do forecasting in earnest. We’ll use linear regression for our
    forecasts. *Linear regression* is an essential part of every data scientist’s
    toolkit: it finds a line that captures a noisy relationship between variables,
    and we can use that line to make predictions about things we’ve never seen.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理了数据、绘制了图表，并注意到了一些基本的模式，我们已经准备好进行认真的预测了。我们将使用线性回归来进行预测。*线性回归*是每个数据科学家工具箱中的必备工具：它找到一条能够捕捉变量之间噪声关系的直线，我们可以利用这条直线来对我们从未见过的事物进行预测。
- en: Linear regression was invented more than a century before the term *machine
    learning* was coined, and it has historically been thought of as part of pure
    statistics. However, since it bears such a strong resemblance to many common machine
    learning methods, and since it shares some common theoretical foundations with
    machine learning, linear regression is sometimes considered part of the machine
    learning field. Like all the best scientific tools, it allows us to pull order
    from chaos.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是在“*机器学习*”一词被提出的一个多世纪之前就已经被发明的，历史上它一直被认为是纯统计学的一部分。然而，由于它与许多常见的机器学习方法有着非常相似的特点，并且与机器学习共享一些共同的理论基础，因此线性回归有时被视为机器学习领域的一部分。像所有最好的科学工具一样，它帮助我们从混乱中提取秩序。
- en: In this case, we have the chaos of car sales in Quebec, where seasonal variations,
    time trends, and plain randomness mingle together in a noisy dataset. When we
    apply simple linear regression to this data, our output will be a straight line
    that captures an underlying structure that will help us make sound forecasts for
    the future. [Figure 2-2](#figure2-2) shows an example of a typical output of linear
    regression.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有魁北克汽车销售的混乱数据，其中季节性变化、时间趋势和单纯的随机性交织在一起，形成一个嘈杂的数据集。当我们将简单的线性回归应用于这些数据时，输出将是一条直线，这条直线捕捉到了一种潜在的结构，帮助我们对未来做出准确的预测。[图
    2-2](#figure2-2)展示了线性回归的典型输出示例。
- en: '![](image_fi/502888c02/f02002.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c02/f02002.png)'
- en: 'Figure 2-2: A dashed line showing a typical output of linear regression'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-2：显示线性回归典型输出的虚线
- en: 'In this plot, you can see the points representing the data, just as in [Figure
    2-1](#figure2-1). Again, we see the chaos of our dataset: great variations occur
    between months across the whole dataset.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，你可以看到代表数据的点，就像在[图 2-1](#figure2-1)中一样。我们再次看到了数据集的混乱：整个数据集在每个月之间都有很大的波动。
- en: The dashed line that progresses slightly upward from left to right represents
    the output of a linear regression. It’s called a *regression line*, and we often
    say that this regression line *fits* the data. In other words, it goes through
    what looks like roughly the center of the cloud constituted by all the points
    together. It gets close to many of the points of our data, and no data point is
    particularly far away from it. It’s as if the line expresses or reveals the fundamental
    relationship between time and sales (a relationship of gradual growth). The notion
    of a line fitting a set of points is fundamental to linear regression. In fact,
    for reasons we’ll discuss later, a regression line is sometimes called the *line
    of best fit* to a dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右略微向上的虚线代表线性回归的输出。它被称为 *回归线*，我们常常说这条回归线 *拟合* 数据。换句话说，它通过看起来大致处于所有点构成的云的中心的地方。它接近我们数据中的许多点，并且没有数据点特别远离它。就像这条线表达或揭示了时间与销量之间的基本关系（一种逐渐增长的关系）。线拟合一组点的概念是线性回归的基础。事实上，出于我们稍后讨论的原因，回归线有时被称为数据集的
    *最佳拟合线*。
- en: Since our regression line is a straight line, it doesn’t have the random variation
    of the real data. The line proceeds in a predictable way. By removing that randomness,
    the regression line shows us a clear representation of the underlying pattern
    of the data. In this case, the regression line shows us that the data has a general
    trend upward over time, and if we measure the regression line carefully, we can
    find exactly the slope and height of that trend.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的回归线是一条直线，它没有真实数据中的随机波动。该线以可预测的方式前进。通过去除这些随机性，回归线向我们展示了数据潜在模式的清晰表示。在这种情况下，回归线显示数据随着时间的推移有一个整体向上的趋势，如果我们仔细测量回归线，我们可以精确找到该趋势的斜率和高度。
- en: We can interpret the value of the regression line for any particular month as
    the number of car sales expected in that month. Later we’ll extrapolate our simple
    line forward into the future (by continuing to draw it with the same slope until
    it extends past the right edge of the plot) to generate forecasts for sales in
    future months.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将回归线在任何特定月份的值解释为该月预期的汽车销量。稍后，我们将把这条简单的线向前外推到未来（通过继续用相同的斜率绘制它，直到它延伸到图表的右边缘之外），以生成未来几个月的销售预测。
- en: 'Let’s run the code that performs linear regression and outputs a regression
    line. We’ll use methods for linear regression that are very particular about the
    *shape* of the data we use, meaning whether sales numbers are stored as 108 rows
    × 1 column or 108 columns × 1 row. In this case, our linear regression code will
    run more smoothly if our data is stored as 108 rows of 1 list each, where each
    list contains one number. To get our data in this shape, we’ll use the pandas
    `reshape()` method as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行执行线性回归并输出回归线的代码。我们将使用对数据的 *形状* 非常敏感的线性回归方法，也就是说，销售数据是以 108 行 × 1 列的形式存储，还是以
    108 列 × 1 行的形式存储。在这种情况下，如果我们的数据存储为 108 行，每行包含一个数字的列表，我们的线性回归代码将运行得更顺畅。为了将数据转化为这种形状，我们将使用
    pandas 的 `reshape()` 方法，具体如下：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you run `print(x)` and `print(y)`, you can see the new shape of the data:
    108 rows of one-element lists. Actually performing the linear regression takes
    little code. We can do the whole thing, including importing the relevant module,
    with three lines:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 `print(x)` 和 `print(y)`，你可以看到数据的新形状：108 行的单元素列表。实际上，执行线性回归的代码非常简短。我们可以用三行代码完成整个过程，包括导入相关模块：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we import the linear regression capability from the scikit-learn package,
    which can be referred to by its standard abbreviation, sklearn. This package,
    which is extremely popular in the machine learning world, provides many useful
    machine learning capabilities, including linear regression. After importing sklearn,
    we define the variable `regressor`. A *regressor*, as its name tautologically
    suggests, is a Python object that we’ll use to perform regression. After creating
    the regressor, we tell it to `fit` our `x` and `y` variables. We are telling it
    to calculate the line shown in [Figure 2-2](#figure2-2) that fits the data by
    matching its location and general trend.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从 scikit-learn 包导入线性回归功能，该包可以通过其标准缩写 sklearn 来引用。这个在机器学习领域极为流行的包提供了许多有用的机器学习功能，包括线性回归。在导入
    sklearn 后，我们定义变量 `regressor`。如其名称所示，*回归器* 是一个 Python 对象，我们将使用它来执行回归操作。在创建回归器后，我们告诉它对
    `x` 和 `y` 变量进行 `fit` 操作。我们告诉它通过匹配数据的位置和总体趋势来计算如 [图 2-2](#figure2-2) 所示的拟合数据的直线。
- en: 'A more quantitative way to describe what *fitting our regression* means is
    that it’s determining precise, optimized values for two numbers: a coefficient
    and an intercept. After running the preceding snippet, we can look at both of
    these numbers as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 描述*拟合回归*的一个更定量的方式是：它是确定两个数字的精确、优化值：一个系数和一个截距。在运行了前面的代码片段后，我们可以如下查看这两个数字：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This code prints out two numbers that are output by the regressor’s `fit()`
    method: an intercept, which you should see is about 10,250.8; and a variable called
    `coef_`, which is short for *coefficients*, and should be equal to about 81.2\.
    Together, these two numbers specify the exact position and trend of the dashed
    regression line you see in [Figure 2-2](#figure2-2). You’ll see how they do this
    in the next section.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码打印出回归器`fit()`方法输出的两个数字：一个截距，你应该能看到大约是10,250.8；以及一个叫做`coef_`的变量，`coef_`是*系数*的缩写，它的值应该大约是81.2。结合这两个数字，可以指定[图2-2](#figure2-2)中虚线回归线的确切位置和趋势。在接下来的部分，你将看到它们是如何做到这一点的。
- en: Applying Algebra to the Regression Line
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将代数应用于回归线
- en: 'To see how these two numbers specify the regression line, think back to your
    high school math classes. You may remember learning that every straight line can
    be expressed in a form like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这两个数字如何指定回归线，回想一下你高中数学课学到的内容。你可能记得，每一条直线都可以用类似下面的形式表示：
- en: '*y* = *m* · *x* + *b*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *m* · *x* + *b*'
- en: 'Here, *m* is a slope, or coefficient, and *b* is the intercept (technically
    a *y-intercept*—the exact place where the line crosses the plot’s y-axis). In
    this case, the value of the `coef_` variable we found, about 81.2, is the value
    of *m*, and the value of the intercept variable we found, about 10,250.8, is the
    value of *b*. So, what we have learned from our regression process is that the
    relationship between time period and car sales can be expressed, at least approximately,
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m*是斜率或系数，*b*是截距（严格来说是*y-截距*——即直线穿过图表y轴的精确位置）。在这种情况下，我们找到的`coef_`变量值，大约是81.2，就是*m*的值，而我们找到的截距变量值，大约是10,250.8，就是*b*的值。所以，通过我们的回归过程，我们学到的是，时间段和汽车销量之间的关系可以至少大致表示为：
- en: '*car sales* = 81.2 · *period* + 10250.8'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*汽车销量* = 81.2 · *时间段* + 10250.8'
- en: The chaos of the car sales dataset’s apparently random variation (shown in [Figure
    2-1](#figure2-1)) is now reduced to the order of this simple equation. The line
    that this equation describes is the dashed line in [Figure 2-2](#figure2-2). We
    can think of every point on that line as a prediction of how many car sales are
    expected at each time period, ignoring the distracting randomness and noise.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车销售数据集看似随机变化的混乱（如[图2-1](#figure2-1)所示）现在已简化为这个简单方程的规律。这个方程描述的直线就是[图2-2](#figure2-2)中的虚线。我们可以将这条直线上的每一个点看作是对每个时间段内汽车销售预期的预测，忽略了干扰性随机性和噪音。
- en: 'The *m* and *b* values in our equation have useful interpretations. The interpretation
    of the line’s slope, 81.2, is the monthly growth trend of car sales. Based on
    the data we’ve observed in the past, we conclude that car sales in Quebec grow
    by about 81.2 cars per month. Randomness and other variation remain, but a growth
    of 81.2 is what we approximately expect. The interpretation of the intercept variable,
    10,250.8, is the *baseline* value of car sales: the expected car sales in month
    0 after “removing” or ignoring the chaos of seasonal variation, the passage of
    time, and other influences.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方程中的*m*和*b*值有着有用的解释。线性斜率81.2的解释是汽车销售的月度增长趋势。根据我们过去观察到的数据，我们得出结论，魁北克的汽车销量每月大约增长81.2辆。虽然仍然存在随机性和其他变动，但81.2的增长值是我们大致期望的。截距变量10,250.8的解释是汽车销售的*基准*值：即“移除”或忽略季节性变化、时间流逝和其他影响后，在第0个月预期的汽车销量。
- en: The equation that linear regression finds can also be called a *model*, a quantitative
    description of how two or more variables relate to each other. So when we perform
    the preceding steps, we can say that we *fit a regression*, or equivalently we
    can say that we *trained a model*. Our regression, or equivalently our model,
    tells us that we expect to sell about 10,250.8 cars at the beginning of the time
    frame in our data, and we expect to sell about 81.2 more cars every month than
    we sold in the previous month.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归找到的方程也可以叫做*模型*，即描述两个或多个变量之间关系的定量描述。因此，当我们执行前面的步骤时，我们可以说我们*拟合了回归*，或者我们可以等效地说我们*训练了一个模型*。我们的回归，或者等效地说我们的模型，告诉我们：在数据时间框架的开始，我们预期销售大约10,250.8辆汽车，并且每个月比前一个月多售出大约81.2辆汽车。
- en: 'It’s natural to wonder how our regressor determined that 81.2 and 10,250.8
    (the `coef_` and `intercept` outputs of our regressor) are the best values for
    *m* and *b* in our regression line. The line they specify looks good enough in
    [Figure 2-2](#figure2-2), but it’s not the only line we could draw through our
    cloud of points. A literally infinite number of conceivable lines also go through
    our cloud and could be said to fit our data. For example, we might hypothesize
    that the following line is a better approximation of the relationship between
    time period and sales:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自然会有人想知道，我们的回归器是如何确定 81.2 和 10,250.8（回归器的 `coef_` 和 `intercept` 输出值）是回归线中*m*和*b*的最佳值的。这条线在[图
    2-2](#figure2-2)中看起来足够好，但它并不是唯一一条可以穿过我们数据点云的线。实际上，还有无数条可以穿过数据点云的线，并且它们也可以被说成符合数据。例如，我们可能假设以下这条线更好地近似时间周期与销售之间的关系：
- en: '*car sales* = 125 · *period* + 8000'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*汽车销售* = 125 · *时间周期* + 8000'
- en: 'Let’s call this new line our *hypothesized line*. If we use it as a model of
    our data, we have a new *m* and *b*, and so we have a new interpretation. In particular,
    the slope of this line is 125, which we would interpret as an expectation that
    monthly car sales will increase by about 125 every month—significantly higher
    than 81.2, the estimate from our regression line. Let’s plot our regression line
    and this new hypothesized line together with the data as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这条新线称为我们的*假设线*。如果我们将其作为数据模型，我们就有了新的*m*和*b*，从而得到了新的解释。特别是，这条线的斜率是 125，我们将其解释为预期每个月的汽车销售将增加大约
    125 辆——这明显高于回归线估计的 81.2。接下来，我们将回归线和这条新的假设线与数据一起绘制如下：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see the output of this snippet in [Figure 2-3](#figure2-3), where we’ve
    drawn the data, our regression line (the shallow solid line), and our new hypothesized
    line (the steeper dashed line).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 2-3](#figure2-3)中看到这个代码片段的输出，我们绘制了数据、回归线（浅色实线）以及我们新的假设线（更陡的虚线）。
- en: '![](image_fi/502888c02/f02003.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c02/f02003.png)'
- en: 'Figure 2-3: A regression line and a steeper line that also fits the data'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-3：一条回归线和一条更陡的线，这两条线都符合数据
- en: Both lines go through our cloud of points. Both show an upward trend over time.
    Both are reasonable candidates to be approximations of the relationship between
    time and sales, and both could be said to fit the data. Why has our regressor
    output one line instead of the other? We said that the regression line output
    by the linear regression process is the *line of best fit*. What is it that enables
    us to say that it fits better than any other line?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 两条线都穿过我们的数据点云，且都表现出随时间的上升趋势。它们都是合理的候选线，可以近似表示时间与销售之间的关系，并且都可以说符合数据。为什么我们的回归器输出了一条线而不是另一条呢？我们之前说过，线性回归过程中输出的回归线是*最佳拟合线*。是什么让我们能说它比任何其他线拟合得更好呢？
- en: Calculating Error Measurements
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算误差测量值
- en: We can find the answer by looking at measurements related to *regression errors*.
    Remember that we interpret each point of a regression line as our prediction of
    what value we expect in the data. [Figure 2-4](#figure2-4) shows a regression
    line and the data used to create it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看与*回归误差*相关的测量值来找到答案。记住，我们将回归线上的每个点解释为我们对数据中预期值的预测。[图 2-4](#figure2-4)显示了一条回归线及其所用数据。
- en: '![](image_fi/502888c02/f02004.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c02/f02004.png)'
- en: 'Figure 2-4: Regression errors: vertical distances between points and a regression
    line'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-4：回归误差：点与回归线之间的垂直距离
- en: You can see that this regression line is a good fit to the data, meaning it
    gets close to most of the illustrated points. However, it’s not a perfect fit.
    For every data point, we can calculate the vertical distance between the data
    point and the regression line. The regression line predicts a certain value, and
    the point in the data has a particular distance from that prediction. This distance
    between a predicted and an actual value of a data point is called the regression’s
    *error* relative to that point. In [Figure 2-4](#figure2-4), the variable *e*[*i*]
    is an error measurement for one of the points in the data. You can see that *e*[*i*]
    is the vertical distance between a particular point and the regression line. We
    can calculate this distance for every point in our data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这条回归线很好地拟合了数据，意味着它接近大多数示例点。然而，它并不是完美拟合。对于每一个数据点，我们可以计算该数据点与回归线之间的垂直距离。回归线预测了一个特定值，而数据中的点与这个预测值之间有一个特定的距离。这个预测值与实际值之间的距离被称为回归的*误差*（error）相对于该点。在[图2-4](#figure2-4)中，变量*e*[*i*]是数据中某一点的误差测量。你可以看到，*e*[*i*]是该点与回归线之间的垂直距离。我们可以为数据中的每一个点计算这个距离。
- en: Calculating the error relative to each data point will give us a way to quantify
    how well any line fits our data. Lines with low errors fit the data well, and
    lines with high errors fit the data poorly. That’s why we say that measuring regression
    errors is one way to measure a regression line’s *goodness of fit*, the degree
    to which a line fits the data well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 计算相对于每个数据点的误差将为我们提供一个量化任何线如何拟合我们数据的方法。误差小的线能够很好地拟合数据，而误差大的线则拟合得不好。这就是为什么我们说，衡量回归误差是衡量回归线的*拟合优度*（goodness
    of fit）的一种方式，即衡量一条线与数据拟合的程度。
- en: 'Let’s calculate those error measurements for our car sales regression. We’ll
    calculate each point of the lines we’re interested in and compare those points
    to each point of our dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来计算这些关于汽车销售回归的误差测量。我们将计算我们感兴趣的线的每一个点，并将这些点与我们数据集中的每一个点进行比较：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this snippet, we create `saleslist`, a variable that includes the raw car
    sales numbers for every month. Then we create two variables, `regressionline`
    and `hypothesizedline`. These variables record every point on the regression and
    hypothesized lines, respectively. We want to measure how far each true sales number
    is from both of these lines, so we create two more variables: `error1` to record
    the distance between true sales numbers and the regression line, and `error2`
    to record the distance between true sales numbers and the hypothesized line.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们创建了`销售列表`（saleslist），一个包含每个月汽车销售数量的变量。接着我们创建了两个变量，`回归线`（regressionline）和`假设线`（hypothesizedline）。这些变量分别记录回归线和假设线上的每一个点。我们想要衡量每个实际销售数字与这两条线的距离，因此我们创建了两个额外的变量：`误差1`（error1），用于记录实际销售数字与回归线之间的距离，`误差2`（error2），用于记录实际销售数字与假设线之间的距离。
- en: 'We can print out these variables to look at what errors we find for both of
    our lines:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印出这些变量，以查看我们在两条线上的误差：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When you look at these lists of errors, you can see 108 separate measurements
    of how far these lines are from the raw data. These 108 measurements are an expression
    of how well these lines fit the raw data. However, looking at all 216 of these
    measurements at once is difficult. It would be easier if we could boil down all
    this information indicating how well a line fits to just one number. The following
    snippet shows one way to do this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看这些误差列表时，你可以看到108个分别测量这两条线与原始数据的距离的独立值。这108个测量值表达了这些线如何拟合原始数据。然而，一次性查看这216个测量值是很困难的。如果我们能够将所有这些表示线拟合情况的信息压缩为一个数字，那会更容易。以下代码片段展示了一种实现方法：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this snippet, we import Python’s NumPy package. NumPy is used often in data
    science, especially for calculations with arrays and matrices. Here, we import
    it because it gives us the ability to find the mean of a list. Then we define
    two new variables: `error1abs` and `error2abs`, each containing a list of the
    absolute values of our error measurements for our two respective lines. Finally,
    we take the means of these lists.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们导入了Python的NumPy包。NumPy在数据科学中非常常用，特别是在进行数组和矩阵计算时。这里，我们导入它是因为它能帮助我们计算一个列表的均值。然后，我们定义了两个新变量：`误差1绝对值`（error1abs）和`误差2绝对值`（error2abs），它们分别包含我们对两条线的误差测量的绝对值列表。最后，我们计算这些列表的均值。
- en: 'The means that we find are called the *mean absolute error (MAE)* measurements
    of each line. Hopefully, the MAE feels like an intuitive measurement of error
    to you: it’s just the average vertical distance between a line and the points
    in a dataset. A line that gets very close to the points in a dataset will have
    a low MAE, and a line that is very far from most points will have a higher MAE.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的均值被称为*均值绝对误差（MAE）*，即每条线的误差测量值。希望 MAE 对你来说是一个直观的误差衡量：它只是线与数据集中的点之间的平均垂直距离。与数据集中点非常接近的线会有较低的
    MAE，而与大多数点距离较远的线则会有较高的 MAE。
- en: The MAE is a reasonable way to express the degree of goodness of fit of a regression
    line or any other line. The lower the MAE, the better. In this case, we can see
    that the MAE for our regression line is 3,154.4, while the MAE for our hypothesized
    line is 3,239.8\. At least according to this measurement, the regression line
    fits the data better than our hypothesized line.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 是一种合理的方式来表示回归线或任何其他线的拟合优度。MAE 越小，拟合效果越好。在这种情况下，我们可以看到回归线的 MAE 为 3,154.4，而我们假设的线的
    MAE 为 3,239.8。至少根据这个测量，回归线比我们假设的线拟合得更好。
- en: 'The MAE has an easy interpretation: it’s the average error we expect to have
    if we use a particular line for prediction. When we say that the MAE for our regression
    line is 3,154.4, we mean that if we use this regression line to make predictions,
    we expect our predictions to be wrong by about 3,154.4 on average (either 3,154.4
    too low or too high).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 有一个简单的解释：它是我们使用特定回归线进行预测时，预计会有的平均误差。当我们说回归线的 MAE 为 3,154.4 时，我们的意思是，如果我们使用这条回归线进行预测，我们预计我们的预测结果平均会偏差大约
    3,154.4（可能偏低或偏高）。
- en: For example, suppose we predict that three months from now, we will sell exactly
    20,000 cars. We wait three months, count our monthly sales, and find that we actually
    sold 23,154 cars instead of 20,000\. Our prediction was wrong; we underestimated
    car sales by 3,154\. So, we’re not perfect at prediction, and the size of our
    prediction error tells us exactly how imperfect we are. Is the size of our error
    a surprise? The MAE we just measured (3,154.4) tells us that having an error this
    high isn’t surprising—in fact, underestimating by 3,154 is (after rounding) exactly
    the size of error we expect to encounter in any month when we’re using this regression.
    Sometimes we’ll overestimate instead of underestimating, and sometimes we’ll have
    lower or higher errors than 3,154\. Regardless, the MAE is telling us that having
    an error of about 3,154 is what we expect when using this regression for this
    prediction scenario.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们预测三个月后将销售 20,000 辆汽车。我们等待三个月，统计每月的销售量，发现实际销售量是 23,154 辆，而不是 20,000。我们的预测错了；我们低估了汽车销售量
    3,154 辆。所以，我们的预测并不完美，预测误差的大小正好告诉我们我们有多不完美。我们的误差大小是否令人惊讶？我们刚刚测得的 MAE（3,154.4）告诉我们，误差这么大并不奇怪——事实上，低估
    3,154 辆（四舍五入后）正是我们在使用这个回归模型进行任何月度预测时，预计会遇到的误差大小。有时我们会高估，而不是低估；有时我们的误差会低于或高于 3,154。但无论如何，MAE
    告诉我们，使用这个回归模型进行这种预测时，误差大约为 3,154 是我们所期望的。
- en: 'MAE is not the only measurement that indicates how well a line fits a dataset.
    Let’s look at another possible measurement:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 并不是唯一衡量线拟合数据集程度的指标。让我们看看另一个可能的指标：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we create lists of the squared values of each error. Then we take the
    square root of the sum of these errors. This measurement is called the *root mean
    squared error (RMSE).* Lower RMSE values indicate a line that is a better fit—one
    that’s expected to make better predictions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了每个误差的平方值列表。然后，我们取这些误差平方和的平方根。这个指标被称为*均方根误差（RMSE）*。较低的 RMSE 值表示拟合效果更好的回归线——它预计能做出更准确的预测。
- en: 'We can create simple Python functions that perform calculations of MAE and
    RMSE:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建简单的 Python 函数来计算 MAE 和 RMSE：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These functions just calculate MAE and RMSE, respectively, exactly as we did
    previously. If you run `print(get_rmse(regressionline,saleslist))`, you can see
    that the RMSE of our regression line is about 3,725, and if you run `print(get_rmse(hypothesizedline,saleslist))`,
    you can see that the RMSE of our hypothesized line is about 3,969.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数分别计算 MAE 和 RMSE，和我们之前做的一样。如果你运行 `print(get_rmse(regressionline,saleslist))`，你会看到回归线的
    RMSE 约为 3,725；如果你运行 `print(get_rmse(hypothesizedline,saleslist))`，你会看到我们假设的线的
    RMSE 约为 3,969。
- en: You’ll notice that the RMSE of our regression line is smaller than the RMSE
    of our hypothesized line. This enables us to say that the regression line is a
    better fit to the data than the hypothesized line, according to the RMSE metric.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们回归线的RMSE比假设线的RMSE小。这使我们可以根据RMSE指标说回归线比假设线更适合数据。
- en: It’s not a coincidence that our regression line has a lower RMSE than our hypothesized
    line. When we ran the command `regressor.fit(x,y)` in Python earlier, the `regressor.fit()`
    method performed linear algebraic calculations that were invented by the great
    mathematician Adrien-Marie Legendre and first published in 1805\. Legendre’s calculations
    take a collection of points as an input, and their output is the intercept and
    coefficients that minimize the value of the RMSE. In other words, the line whose
    coefficients are determined by Legendre’s method is mathematically guaranteed
    to have a lower RMSE than any of the other infinite possible lines that we could
    draw to try to fit our data. When we call the regression line the line of best
    fit, we mean that it is mathematically guaranteed to have the lowest possible
    RMSE of all possible lines that use the variables we specified. This guarantee
    is a reason for the enduring popularity of linear regression, and why it’s still
    a standard way to find a line that fits a dataset after all these years.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回归线的RMSE低于假设线的RMSE并非偶然。当我们之前在Python中运行命令`regressor.fit(x,y)`时，`regressor.fit()`方法执行了由伟大数学家阿德里安-玛丽·勒让德（Adrien-Marie
    Legendre）发明的线性代数计算，并首次在1805年发布。勒让德的计算方法接受一组点作为输入，输出的是最小化RMSE值的截距和系数。换句话说，勒让德方法确定的系数对应的直线，在数学上保证比任何其他我们尝试绘制的、用来拟合数据的无数条直线具有更低的RMSE。当我们称回归线为最优拟合线时，我们的意思是它在所有使用我们指定的变量的可能直线中，数学上保证具有最低的RMSE。这一保证是线性回归持续受欢迎的原因，也是为什么它多年来仍然是寻找适合数据集的直线的标准方法。
- en: The line that the regressor outputs is the best-fit line, not just in the loose
    sense that it looks like it fits the cloud very well, but in the strict quantitative
    sense that out of all the infinite lines that go through the cloud of points,
    it is guaranteed to have the lowest RMSE. You can feel free to try other straight
    lines and check their RMSE values—you won’t find one that performs better than
    our regression line.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 回归器输出的直线是最优拟合线，不仅在宽松的意义上它看起来很好地拟合了数据点云，而且在严格的定量意义上，所有通过数据点云的无数条直线中，它保证具有最低的RMSE。你可以随意尝试其他直线并检查它们的RMSE值——你不会找到比我们的回归线表现更好的直线。
- en: Using Regression to Forecast Future Trends
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用回归进行未来趋势预测
- en: 'So far, we’ve used linear regression to find the line that is the best fit
    of our historical data. But our historical data is all from the past, so we haven’t
    done any real forecasting yet. Going from a linear regression to a forecast is
    simple: we just need to extrapolate.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用线性回归找到了最适合历史数据的直线。但我们的历史数据都来自过去，因此我们还没有进行任何真正的预测。从线性回归到预测是简单的：我们只需要外推。
- en: 'The dashed regression line we drew in [Figure 2-2](#figure2-2) stops at the
    edges of our plot, at month 0 on the left and month 107 on the right, but there’s
    no reason it needs to stop there. If we continue to draw our regression line farther
    to the right, we can see the values we expect for any month, however far in the
    future. Of course, we’ll keep the same slope and intercept as we extend the line
    in this way. Let’s write code that will do this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 2-2](#figure2-2)中绘制的虚线回归线停止在图表的边缘，左边是第0个月，右边是第107个月，但没有理由它必须停在那里。如果我们继续将回归线向右延伸，我们可以看到任何未来月份的预期值，尽管时间跨度可能很远。当然，扩展回归线时，我们会保持相同的斜率和截距。让我们编写代码来实现这一点：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we create the variable `x_extended`. This variable is a combination of
    two sets of numbers. First, it includes the values of our dataset’s `period` column
    that records the periods from 0 to 107 in order. Second, it includes all the numbers
    108 through 115 in order—these are meant to represent future months after the
    end of our data (month 108, month 109, . . . all the way to month 115). We combine
    these two things by using the `np.append()` method, and the end result is an extended
    version of our original *x* variable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use our regressor’s `predict` method to calculate the values that
    will be on our regression line for each of the month numbers in `x_extended`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we have the forecast values stored in the variable `extended_prediction`.
    If you look at `extended_prediction`, you can see what these predictions are.
    These predictions follow a simple pattern: each is about 81.2 higher than the
    previous one. This is because 81.2 is the slope of our regression line. Remember,
    81.2 is not just the slope of the line but also the size of the increase we expect
    in car sales every month, ignoring randomness and seasonal variation.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction method we used here is helpful, but we don’t really need it.
    We can get any values we want on our regression line just by plugging in numbers
    to our regression equation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '*car sales* = 81.2 · *period* + 10250.8'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'However we get the next predicted values, we can plot them and see what they
    look like on a graph ([Figure 2-5](#figure2-5)):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](image_fi/502888c02/f02005.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-5: A regression line extrapolated several periods forward, for forecasting'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: This plot will probably not surprise you. It looks almost identical to [Figure
    2-2](#figure2-2), and it’s supposed to. The only difference is that we’ve extended
    our regression line out a few more periods to the right, to see what car sales
    we expect—that is, how many we forecast—in the near future. This extrapolation
    of a regression line is a simple but effective way to forecast.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: We’ve accomplished forecasting with linear regression, but there’s more we can
    do to improve our forecasts. In the next sections, we’ll talk about ways to evaluate
    and improve the performance of our forecasts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Trying More Regression Models
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The linear regression we did in the previous sections is a simple kind called
    *univariate linear regression*. This type of regression uses only one variable
    to predict one other variable. In our case, we used the period variable alone
    to predict sales. Using only one variable has a couple of advantages: first, it
    is easy; and second, it creates a simple, straight line that expresses some order
    in the data without also including its random noise. But we have other options.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Linear Regression to Predict Sales
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we use other variables to predict sales as well as just the period, we can
    perform a more complex kind of regression called *multivariate linear regression*.
    The details of multivariate linear regression are essentially the same as univariate
    linear regression; the only real difference is the number of variables we use
    for prediction. We can use any variables we like for multivariate regression:
    gross domestic product (GDP) growth rates, population estimates, car prices, inflation
    rates, or anything else we want.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: For now, we’re limited because our dataset doesn’t contain any of those variables.
    It contains only the period and the sales. However, we can still perform multivariate
    regression, by using variables that we derive from the period variable. For example,
    we could use *period*² as a new variable in a multivariate regression, or log(*period*),
    or any other mathematical transformation of the period variable.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that when we performed regression before, we found the *m* and the
    *b* (slope and intercept) variables in the following equation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *m* · *x* + *b*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use multiple variables to predict car sales, we’re also finding slope
    and intercept variables. The only difference is that we’re also finding more variables.
    If we’re using three variables to do prediction (which we can call *x*[1], *x*[2],
    and *x*[3]), then we’re finding the *m*[1], *m*[2], *m*[3], and *b* variables
    in the following equation:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *m*[1] · *x*[1] + *m*[2] · *x*[2] + *m*[3] · *x*[3] + *b*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is the same as in univariate regression, but we end up with more slopes
    for more predictor variables. If we want to use *period*, *period*², and *period*³
    to predict car sales in our regression, we’ll need to estimate the *m*[1], *m*[2],
    *m*[3], and *b* variables in [Equation 2-1](#equation2-1):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '*car sales* = *m*[1] · *period* + *m*[2] · *period*² + *m*[3] · *period*³ +
    *b*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 2-1: An equation for multivariate regression using our car sales data'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the code that will create these transformations of our period
    variable and do linear regression with three variables:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In this snippet, we define two new variables: `quadratic`, whose value is equal
    to *period*², and `cubic`, whose value is equal to *period*³. Then, we define
    a new `x3` dataframe that includes all three of these new variables, and we reshape
    it so that they will be the right shape for our regressor. The right shape for
    this three-variable multivariate regression is an array of 108 rows, in which
    each row is a list of the values of our three variables for a particular month.
    As long as the data is in the right shape, we can use the `fit()` method for any
    univariate or multivariate linear regression with any number of variables. After
    calling `fit()`, we calculate the values predicted by this regression for our
    data and plot them. This snippet creates the plot in [Figure 2-6](#figure2-6).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c02/f02006.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-6: A curve that also fits the data'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see two regression lines. One is the (solid) straight line that
    is the result of our previous (univariate) regression. The other, newer regression
    line is not a straight line, but rather a (dashed) curve—a *cubic curve*, to be
    precise. Linear regression was originally designed to work with straight lines
    (hence the name *linear*), but we can also use it to find best-fit curves and
    nonlinear functions like the cubic polynomial in [Figure 2-6](#figure2-6).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你可以看到两条回归线。一条是（实线）直线，是我们之前（单变量）回归的结果。另一条新的回归线不是直线，而是一条（虚线）曲线——更准确地说，是一条*三次曲线*。线性回归最初是为了拟合直线而设计的（因此得名*线性*），但我们也可以用它来找到最佳拟合曲线和非线性函数，如[图
    2-6](#figure2-6)中的三次多项式。
- en: 'Whether we find a best-fit straight line or a best-fit curve, the linear regression
    methods that we’re using are exactly the same. Similarly, using multiple variables
    for prediction is not really different from univariate regression with one variable:
    the output still fits our data, and in fact, our new curve goes very close to
    the straight line. Every time we select different variables for our regression,
    the output will look a little different: it may have a different shape or a different
    curve. But it will always fit the data. In this case, if you want to know the
    unknown variables in [Equation 2-1](#equation2-1), we can print them out as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是找到最佳拟合直线还是最佳拟合曲线，我们所使用的线性回归方法都是完全相同的。同样，使用多个变量进行预测与使用一个变量进行单变量回归其实并没有太大区别：输出依然能够拟合我们的数据，实际上，我们的新曲线与直线非常接近。每次选择不同的变量进行回归时，输出会看起来稍有不同：可能形状不同或曲线不同。但它总是能够拟合数据。在这种情况下，如果你想知道[方程
    2-1](#equation2-1)中的未知变量，我们可以像下面这样打印出来：
- en: '[PRE20]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The outputs from these `print()` statements are the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这些`print()`语句的输出结果如下：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'These outputs enable us to fill in all the variables in [Equation 2-1](#equation2-1)
    to get an equation for estimating car sales using a cubic polynomial of the period:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输出使我们能够填写[方程 2-1](#equation2-1)中的所有变量，从而得到一个使用三次多项式估算汽车销量的方程：
- en: '*car sales* = 81.34 · *period* + 0.79 · *period*² – 0.008 · *period*³ + 9746.41'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*汽车销量* = 81.34 · *周期* + 0.79 · *周期*² – 0.008 · *周期*³ + 9746.41'
- en: One important thing to notice about [Figure 2-6](#figure2-6) is the different
    behavior of our regression lines during the last few periods, on the right side
    of the plot. The straight line from our univariate regression increases by about
    81.2 every period, and when we extrapolate it farther to the right, it will continue
    to predict increases of about 81.2 every period. By contrast, the curved line
    from our multivariate regression begins to curve downward on the right side of
    the plot. If we extrapolated it farther to the right, it would predict a decrease
    in car sales every month forever.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[图 2-6](#figure2-6)，一个重要的注意点是我们的回归线在最后几个周期中的不同表现，即图表右侧的部分。我们单变量回归得到的直线每个周期增加约
    81.2，且当我们将其外推到更远的右侧时，它将继续预测每个周期约增加 81.2。相比之下，我们的多元回归得到的曲线在图表的右侧开始向下弯曲。如果我们将其外推到更远的右侧，它将预测每个月的汽车销量会永远下降。
- en: 'These two lines, though they behave similarly and are both the result of linear
    regression, make opposite predictions about the future: one predicts growth, and
    the other predicts contraction. Later in the chapter, we’ll talk more about how
    to choose which regression line to use for forecasting.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这两条线虽然行为相似，且都是线性回归的结果，但它们对未来的预测却正好相反：一条预测增长，另一条预测收缩。本章稍后我们将详细讨论如何选择哪条回归线用于预测。
- en: Trigonometry to Capture Variations
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用三角函数捕捉变化
- en: There’s no limit to the number of variables we can add to a multivariate regression.
    Each selection of variables will lead to a curve with a slightly different shape.
    One of the difficult choices we have to make in every regression problem is which
    variables to add to the regression.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在多元回归中添加任意数量的变量。每一次选择的变量都会导致一条形状略有不同的曲线。我们在每个回归问题中需要做出的一个困难选择是：选择哪些变量来添加到回归模型中。
- en: In this case, the univariate regression line (the straight line in [Figure 2-2](#figure2-2))
    and the cubic regression line (the curved line in [Figure 2-6](#figure2-6)) are
    both acceptable and can be used to forecast the future. However, though they both
    pass through what looks like the middle of our cloud of points, there is so much
    variation that they don’t capture—sales for many individual months are much higher
    or much lower than these lines. Ideally, we could find a collection of variables
    that, when fit using a linear regression, lead to a curve that better fits some
    of this variation. In this case, making one small change to the way we plot our
    data can make what we should do next clearer.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s change [Figure 2-1](#figure2-1) from a scatterplot to a line plot, by
    making just one small change in our code (shown in bold):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Figure 2-7](#figure2-7) shows the new plot.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c02/f02007.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-7: A line plot makes the patterns within years (high summers and low
    winters) more apparent.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: This new plot shows the same data, but plotted as a line rather than a collection
    of points. With a line plot, another pattern becomes much clearer. We can see
    that the noisy ups and downs of monthly sales within individual years are more
    ordered than they looked in the scatterplot.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, our data includes nine years of sales figures, and exactly nine
    major peaks are apparent in the contour of the line plot. What looked like totally
    random noise actually has some structure: a predictable peak in sales occurs every
    summer, with a corresponding trough every winter. If you think about it a little
    more, you might realize why variation could exist within a year: it’s because
    this data comes from Quebec, where very cold winters are associated with lower
    activity levels, and beautiful warm summers are associated with going outside
    and shopping and taking long road trips that require cars.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Now that you can see the way the number of car sales goes up and down during
    a year, maybe it reminds you of a mathematical function. In fact, the pattern
    of periodic increases and decreases looks like a trigonometric curve, like a sine
    or cosine curve. [Figure 2-8](#figure2-8) shows an example of sine and cosine
    curves.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c02/f02008.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-8: A plot of a sine curve and a cosine curve'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try a regression that uses the sine and cosine of the period in a multivariate
    regression:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this snippet, we define sine and cosine transformations of the `period` variable,
    and then we fit a regression that uses these new variables as predictors. Finally,
    we plot the result, which is shown in [Figure 2-9](#figure2-9).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c02/f02009.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-9: A trigonometric curve fit to our data'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 2-9](#figure2-9), you can see the raw sales data plotted as a solid
    line, and the trigonometric regression curve plotted as a dashed line. You can
    see that we’re really getting somewhere now. The regression that relies on trigonometric
    functions seems to fit the data especially well. In particular, it seems to go
    up during the yearly peaks and down during the yearly troughs, thereby getting
    much closer to the true sales numbers. We can verify that this trigonometric curve
    has a lower RMSE than the straight line as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The RMSE we get as output is the lowest one we’ve seen yet: about 2,681\. It
    is not entirely a coincidence that trigonometric functions enable us to fit the
    data well. In fact, the increases and decreases in temperature during seasons
    on our planet are due to a change in the angle of the Earth during its revolution
    around the sun. The change in the angle of the Earth with respect to the sun follows
    a curve that’s like a sine curve, and therefore temperature changes throughout
    each year also follow sine-like curves. If car sales are reacting to winter and
    summer weather changes due to temperature, it makes sense that they would also
    follow sine-like curves. Regardless of whether we found the trigonometric model
    by blind chance, by looking at our scatterplot in [Figure 2-1](#figure2-1), or
    because we know about the astronomy of the Earth’s rotation around the sun, it
    seems like we’ve found a good regression curve that fits the data well.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Best Regression to Use for Forecasting
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve observed that the regression line that includes terms for the sine and
    cosine of the period seem to fit the data well. When we say that this line fits
    the data well, we mean that, qualitatively, the dashed line in [Figure 2-9](#figure2-9)
    gets quite close to the solid line. More precisely, we mean that quantitatively,
    the RMSE for the trigonometric line is lower than the RMSE for the other lines
    we’ve looked at. Whenever we find a model with a lower RMSE, we are getting a
    model that fits our data better.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'The natural temptation is to keep looking for new regression specifications
    that have lower and lower RMSEs. For example, let’s try a new regression specification
    that includes seven prediction terms to forecast sales, and find the RMSE for
    that model:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this snippet, we repeated steps that we’ve done before: define some variables,
    use the variables in a linear regression, and check the RMSE of the regression.
    Notice the backslash (`\`) at the end of a few of the lines. These are *line continuation
    characters*: they’re telling Python that the line they’re on and the next line
    should be treated as one single line of code. We are using them here because the
    full line doesn’t fit on the book’s page. At home, you can use line continuation
    characters, or you can ignore them if you’re able to type the full lines without
    breaks.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the preceding snippet, we check the RMSE for this new regression
    model, and we find that it’s about 2,610: even lower than the RMSE for the trigonometric
    model shown in [Figure 2-9](#figure2-9). If RMSE is our metric for judging how
    well a model fits, and we have gotten the lowest RMSE yet, it probably seems natural
    to conclude that this is our best model yet, and we should use this model for
    forecasting.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'But be careful; this apparently reasonable conclusion is not correct. The approach
    we’ve been taking to model selection has a problem: it doesn’t fully resemble
    the reality of forecasting as we encounter it in real life. Think about what we’ve
    done. We’ve used past data to fit a regression line and then judged how good that
    regression line is, based on how close it gets to past data points (its RMSE).
    We’re using the *past* both for fitting our regression line and for judging its
    performance. In a real-world forecasting scenario, we’ll use the past to fit our
    regression line, but we should use the *future* for judging its performance. A
    forecasting method is worthwhile only if it can predict the unknown future.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: When we’re choosing the best regression line to use for forecasting, we want
    to find a way to evaluate various regression lines based on their performance
    on future data. This is not possible because the future hasn’t happened yet, so
    we can’t ever have future data. But we can make a small change in the way we perform
    and evaluate regressions so that our measurements of performance on past data
    give a good estimate of how they’ll perform when predicting the future.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need to do is split our full dataset into two separate, mutually exclusive
    subsets: a *training set*, consisting of the majority of our data, and a *test
    set*, consisting of the rest. We’ll use only the training set to fit our regressions,
    or in other words, to *train* them. After fitting/training our regressions, we’ll
    use the test set to evaluate how good the regressions are, using metrics like
    RMSE or MAE.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: This simple change makes an important difference. Instead of evaluating performance
    based on the same data used to fit our regressions, we are evaluating based on
    separate data that was not used during the fitting process. Our test set is from
    the past, but it’s *as if* it’s from the future, since it’s not used to determine
    the coefficients and intercept in our regression, and it’s used only to test how
    good the regression’s predictions are. Since the test set hasn’t been used to
    fit our regressions, we sometimes say that the regressions haven’t *learned from*
    the test data, or that it’s *as if* the test data is from the future. By having
    some data that’s as if it’s from the future, our regression evaluation more closely
    resembles a true forecasting process, where prediction of the future is the most
    important goal.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the code to accomplish this training/test split, and then we’ll
    see what makes it work so well:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, we split the data into two sets: a training set and a test set. We use
    the training set to train the data (to fit a regression line). We can then use
    the test set to test how well our regression performs. If you think about this
    approach, it resembles an actual forecasting situation: we train a model knowing
    only the past, but the model has to do well on data that wasn’t used to train
    it (the future, or data that’s as if it’s from the future). Creating a test set
    like this is essentially creating a simulated future.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding snippet, we use the first 81 time periods as our training
    data and the rest (27 time periods) as our test data. In percentage terms, we
    use 75 percent of our data for training and reserve about 25 percent for testing.
    Splitting training and test data in proportions close to this is common: 70 percent
    training data and 30 percent testing data is also common, as are 80/20 and 90/10
    splits. We usually keep the large majority of our data in the training set since
    finding the right regression line is crucial, and using more data for training
    can help us find the best regression line (the one with the most predictive accuracy).
    At the same time, we need more than a negligible amount of data in the test set,
    since we also need to get an accurate estimate of how our regression is expected
    to perform with new data.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'After we’ve created training and test sets, we can test our different regression
    models on the test set and check the RMSE or the MAE for each model. The model
    that has the lowest RMSE or MAE on the test set is a reasonable choice for the
    model we can use for forecasts of the actual future. Let’s check the RMSE for
    several of the regressions we’ve run so far:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After you run the preceding snippet, you can see that our univariate regression
    has an RMSE of about 4,116 on the test set. The trigonometric multivariate regression
    has an RMSE of about 3,461—much better than the univariate regression. By contrast,
    the complex regression model that includes nine prediction terms has an RMSE of
    about 6,006 on the test set—an awful performance. Though it had excellent performance
    on the training set, we find that it has awful performance on the test set.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: This complex model shows a particularly bad example of *overfitting*. In this
    common machine learning problem, a model is too complex and fits the data’s noise
    and coincidences instead of the data’s true patterns. Overfitting often happens
    when our attempts to get low errors on a training set lead to us getting much
    higher errors on a test set.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that by some coincidence, car sales in Quebec spiked every
    time the star Betelgeuse had a V-band magnitude greater than 0.6 between 1960
    and 1968\. If we included Betelguese’s V-band magnitude as a parameter in our
    regressions, we would find that our RMSE was quite low when predicting 1960 to
    1968 because of this coincidence. Finding a low RMSE might make us quite confident
    that we had a great model that would perform well. We might extrapolate this pattern
    into the future and forecast future sales spikes at future high points of Betelgeuse’s
    brightness cycle. However, since the past relationship between Betelgeuse and
    car sales was only coincidental, extrapolating this pattern into the future would
    give us huge errors; it would cause RMSE on future predictions to be quite high.
    The Betelgeuse/car sales relationship was only noise, and our regressions are
    supposed to capture only true signals, not noise. Including Betelguese’s brightness
    measurements in our regression would be an example of overfitting, since our zeal
    to decrease RMSE for the past would lead us to increase RMSE in the future.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: This example should make it clear that using error measurements on a training
    set to choose the best model could lead us to choose a model that has high error
    measurements on the test set. For that reason, error measurement on the test set
    is the right metric to use to compare models in all forecasting tasks. As a general
    rule, you can expect overfitting to happen when you’ve included too many irrelevant
    variables in your regression. So, you can avoid overfitting by removing the irrelevant
    variables (like the brightness of Betelgeuse) from your regressions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that we’re not always entirely sure which variables are irrelevant
    and which ones are actually useful. That’s why we have to try several models and
    check performance. Find the model that has the lowest RMSE on the test set, and
    that will be the one that has the right mix of variables and doesn’t lead you
    to get distracted by coincidences and overfit.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve compared models based on their RMSE on the test set, we can choose
    the trigonometric model as our best model so far. We can extrapolate one period
    forward in this model and determine a forecast for consumer demand next month,
    just as we extrapolated for our univariate model before. We can report this number
    back to the business as an estimate based on rigorous linear regression analysis.
    Not only that, we can explain why we made this prediction and why we used our
    model, including the idea of the best-fit line, the trigonometric modeling of
    the seasons, and the favorable (low) errors on the test set. If no objections
    or countervailing business considerations arise, we can order this number of cars
    next month, and we can expect that customers will want to purchase close to this
    number of cars.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Further
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression and forecasting are both topics that can fill many textbooks.
    If you continue in your data science education, you’ll have a chance to learn
    many of the subtleties and nuances related to these subjects.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: One thing you should consider studying if you want to get to an advanced level
    in data science is the linear algebra behind linear regression. You can think
    of each observation in your data as a row of a matrix, and then you can use matrix
    multiplication and matrix inversion to calculate the line of best fit, instead
    of relying on a Python library to do the calculations for you. If you deeply explore
    these linear algebra concepts, you’ll learn about the mathematical assumptions
    underlying linear regression. Understanding these mathematical assumptions will
    enable you to more accurately judge whether linear regression is the best method
    to use with your data, or whether you should use some of the methods described
    later in the book instead (especially the supervised learning topics discussed
    in Chapter 6).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Another issue you should become familiar with is the limitation of linear regression
    as a forecasting method. As the name indicates, linear regression is a linear
    method, and it’s meant to be used with variables that have linear relationships.
    For example, if customers order about 10 more units of your product each week
    than they did the previous week, a linear relationship exists between time and
    customer demand, and linear regression would be a perfect method to measure that
    increase and forecast future customer demand. On the other hand, if your sales
    double every week for a year and then suddenly crash and then slowly rise again
    for a while, the relationship between time and sales would be highly nonlinear,
    and linear regression may not yield accurate predictions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, remember that when we use linear regression for forecasting, we are
    extrapolating past growth to predict future growth. If certain circumstances aren’t
    present or accounted for in your historical data, your linear regression won’t
    be able to accurately predict their occurrence in the future. For example, if
    you use data from steady, prosperous years as your training data, you’ll probably
    predict steady, prosperous growth in the future. Instead, you may find that a
    global financial crisis or pandemic changes everything, and since the regression’s
    training data didn’t include a pandemic, no prediction of any pandemic will be
    given for the future. Regression works only when the future resembles the past.
    Some events like wars and pandemics are so inherently unpredictable that regression
    can never give us completely accurate predictions about them. In those cases,
    preparation is more important than forecasting; make sure your business is ready
    for hard times and surprises instead of assuming that linear regression will always
    give you completely correct answers. Though forecasting is important and linear
    regression is powerful, it’s important to remember that these limitations exist.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We began this chapter with a common business scenario: a company needs to decide
    how much new inventory it should order. We used linear regression as our main
    forecasting tool, and we looked a little at the programming side of it (how to
    write code for regression), the statistical side of it (which error metrics we
    can use to determine a model’s goodness of fit), and the math side of it (why
    our particular line is the best-fit line). After we went through all these aspects
    of the problem, we arrived at a model that we thought was best, which we could
    use to obtain a forecast of next month’s consumer demand.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: This scenario—considering a business problem and using programming, mathematical
    theory, and common sense to find a data-driven solution—is typical of data science.
    In the remaining chapters, we’ll examine other business scenarios and talk about
    how to use data science to find ideal solutions to them. In the next chapter,
    we’ll go over data distributions and show how to test two groups to see whether
    they’re significantly different from each other.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
