<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch09"><span epub:type="pagebreak" id="page_221"/><strong><span class="big">9</span><br/>DATA FLOW IN NEURAL NETWORKS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In this chapter, I’ll present how data flows through a trained neural network. In other words, we’ll look at how to go from an input vector or tensor to the output, and the form the data takes along the way. If you’re already familiar with how neural networks function, great, but if not, walking through how data flows from layer to layer will help you build an understanding of the processes involved.</p>&#13;
<p class="indent">First, we’ll look at how we represent data in two different kinds of networks. Then, we’ll work through a traditional feedforward network to give ourselves a solid foundation. We’ll see just how compact inference with a neural network can be in terms of code. Finally, we’ll follow data through a convolutional neural network by introducing convolutional and pooling layers. The goal of this chapter isn’t to present how popular toolkits pass data around. The toolkits are highly optimized pieces of software, and such low-level knowledge isn’t helpful to us at this stage. Instead, the goal is to help you see how the data flows from input to output.</p>&#13;
<h3 class="h3" id="ch09lev1_1"><span epub:type="pagebreak" id="page_222"/>Representing Data</h3>&#13;
<p class="noindent">In the end, everything in deep learning is about data. We have data that we’re using to create a model, which we test with more data, ultimately letting us make predictions about even more data. We’ll start by looking at how we represent data in two types of neural networks: traditional neural networks and deep convolutional networks.</p>&#13;
<h4 class="h4" id="ch09lev2_1">Traditional Neural Networks</h4>&#13;
<p class="noindent">For a <em>traditional neural network</em> or other classical machine learning models, the input is a vector of numbers, the feature vector. The training data is a collection of these vectors, each with an associated label. (We’ll restrict ourselves to basic supervised learning in this chapter.) A collection of feature vectors is conveniently implemented as a matrix, where each row is a feature vector and the number of rows matches the number of samples in the dataset. As we now know, a computer conveniently represents a matrix using a 2D array. Therefore, when working with traditional neural networks or other classical models (support vector machines, random forests, and so on), we’ll represent datasets as 2D arrays.</p>&#13;
<p class="indent">For example, the iris dataset, which we first encountered in <a href="ch06.xhtml#ch06">Chapter 6</a>, has four features in each feature vector. We represented it as a matrix:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import numpy as np</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">from sklearn import datasets</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">iris = datasets.load_iris()</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">X = iris.data[:5]</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">X</span><br/>&#13;
array([[5.1, 3.5, 1.4, 0.2],<br/>&#13;
       [4.9, 3. , 1.4, 0.2],<br/>&#13;
       [4.7, 3.2, 1.3, 0.2],<br/>&#13;
       [4.6, 3.1, 1.5, 0.2],<br/>&#13;
       [5. , 3.6, 1.4, 0.2]])<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Y = iris.target[:5]</span></pre>&#13;
<p class="noindent">Here, we’ve shown the first five samples as we did in <a href="ch06.xhtml#ch06">Chapter 6</a>. The samples above are all for class 0 (<em>I. setosa</em>). To pass this knowledge to the model, we need a matching vector of class labels; <code>X[i]</code> returns the feature vector for sample <code>i</code>, and <code>Y[i]</code> returns the class label. The class label is usually an integer and counts up from zero for each class in the dataset. Some toolkits prefer one-hot-encoded class labels, but we can easily create them from the more standard integer labels.</p>&#13;
<p class="indent">Therefore, a traditional dataset uses matrices between layers to hold weights, with the input and output of each layer a vector. This is straightforward enough. What about a more modern, deep network?</p>&#13;
<h4 class="h4" id="ch09lev2_2"><span epub:type="pagebreak" id="page_223"/>Deep Convolutional Networks</h4>&#13;
<p class="noindent">Deep networks might use feature vectors, especially if the model implements 1D convolutions, but more often than not, the entire point of using a deep network is to allow <em>convolutional layers</em> to take advantage of spatial relationships in the data. Usually, this means the inputs are images, which we represent using 2D arrays. But the input doesn’t always need to be an image. The model is blissfully unaware of <em>what</em> the input represents; only the model designer knows, and they decide the architecture based on that knowledge. For simplicity, we’ll assume the inputs are images, since we’re already aware of how computers work with images, at least at a high level.</p>&#13;
<p class="indent">A black-and-white image, or one with shades of gray, known as a grayscale image, uses a single number to represent each pixel’s intensity. Therefore, a grayscale image consists of a single matrix represented in the computer as a 2D array. However, most of the images we see on our computers are color images, not grayscale. Most software represents a pixel’s color by three numbers: the amount of red, the amount of green, and the amount of blue. This is the origin of the <em>RGB</em> label given to color images on a computer. There are many other ways of representing colors, but RGB is by far the most common. The blending of these primary colors allows computers to display millions of colors. If each pixel needs three numbers, then a color image isn’t a single 2D array, but three 2D arrays, one for each color.</p>&#13;
<p class="indent">For example, in <a href="ch04.xhtml#ch04">Chapter 4</a>, we loaded a color image from <code>sklearn</code>. Let’s look at it again to see how it’s arranged in memory:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from sklearn.datasets import load_sample_image</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">china = load_sample_image('china.jpg')</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">china.shape</span><br/>&#13;
(427, 640, 3)</pre>&#13;
<p class="noindent">The image is returned as a NumPy array. Asking for the shape of the array returns a tuple: (427, 640, 3). The array has three dimensions. The first is the height of the image, 427 pixels. The second is the width of the image, 640 pixels. The third is the number of <em>bands</em> or <em>channels</em>, here three because it’s an RGB image. The first channel is the red component of the color of each pixel, the second the green, and the last the blue. We can look at each channel as a grayscale image if we want:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from PIL import Image</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Image.fromarray(china).show()</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Image.fromarray(china[:,:,0]).show()</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Image.fromarray(china[:,:,1]).show()</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Image.fromarray(china[:,:,2]).show()</span></pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_224"/>PIL refers to Pillow, Python’s library for working with images. If you don’t already have it installed, this will install it for you:</p>&#13;
<pre>pip3 install pillow</pre>&#13;
<p class="indent">Each image looks similar, but if you place them side by side, you’ll notice differences. See <a href="ch09.xhtml#ch09fig01">Figure 9-1</a>. The net effect of each per-channel image creates the actual color displayed. Replace <code>china[:,:,0]</code> with just <code>china</code> to see the full color image.</p>&#13;
<div class="image" id="ch09fig01"><img src="Images/09fig01.jpg" alt="image" width="631" height="138"/></div>&#13;
<p class="figcap"><em>Figure 9-1: The red (left), green (middle), and blue (right)</em> <code><em>china</em></code> <em>image channels</em></p>&#13;
<p class="indent">Inputs to deep networks are often multidimensional. If the input’s a color image, we need to use a 3D tensor to contain the image. We’re not quite done, however. Each input sample to the model is a 3D tensor, but we seldom work with a single sample at a time. When training a deep network, we use <em>minibatches</em>, sets of samples processed together to get an average loss. This implies yet another dimension to the input tensor, one that lets us specify <em>which</em> member of the minibatch we want. Therefore, the input is a 4D tensor: <em>N</em> × <em>H</em> × <em>W</em> × <em>C</em>, with <em>N</em> being the number of samples in the minibatch, <em>H</em> the height of each image in the minibatch, <em>W</em> the width of each image, and <em>C</em> the number of channels. We’ll sometimes write this in tuple form as (<em>N</em>, <em>H</em>, <em>W</em>, <em>C</em>).</p>&#13;
<p class="indent">Let’s take a look at some actual data meant for a deep network. The data is the CIFAR-10 dataset. It’s a widely used benchmark dataset and is available here: <em><a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></em>. You don’t need to download the raw dataset, however. We’ve included NumPy versions with the code for this book. As mentioned above, we need two arrays: one for the images and the other for the associated labels. You’ll find them in the <em>cifar10_test_images.npy</em> and <em>cifar10_test_labels.npy</em> files, respectively. Let’s take a look:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">images = np.load("cifar10_test_images.npy")</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">labels = np.load("cifar10_test_labels.npy")</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">images.shape</span><br/>&#13;
(10000, 32, 32, 3)<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">labels.shape</span><br/>&#13;
(10000,)</pre>&#13;
<p class="indent">Notice that the <code>images</code> array has four dimensions. The first is the number of images in the array (<em>N</em> = 10,000). The second and third tell us that the images are 32×32 pixels. The last tells us that there are three channels, implying the dataset consists of color images. Note that, in general, the number of <span epub:type="pagebreak" id="page_225"/>channels might refer to any collection of data grouped that way—it need not be an actual image. The <code>labels</code> vector has 10,000 elements as well. These are the class labels, of which there are 10 classes, a mix of animals and vehicles. For example,</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">labels[123]</span><br/>&#13;
2<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">Image.fromarray(images[123]).show()</span></pre>&#13;
<p class="noindent">This indicates that image 123 is of class 2 (bird) and that the label is correct; the image displayed should be that of a bird. Recall that, in NumPy, asking for a single index returns the entire subarray, so <code>images[123]</code> is equivalent to <code>images[123,:,:,:]</code>. The <code>fromarray</code> method of the <code>Image</code> class converts a NumPy array to an image so <code>show</code> can display it.</p>&#13;
<p class="indent">Working with minibatches means we pass a subset of the entire dataset through the model. If our model uses minibatches of 24, then the input to the deep network, if using CIFAR-10, is a (24, 32, 32, 3) array: 24 images, each of which has 32 rows, 32 columns, and 3 channels. We’ll see below that the idea of channels is not restricted to the input to a deep network; it also applies to the shape of the data passed between layers.</p>&#13;
<p class="indent">We’ll return to data for deep networks shortly. But for now, let’s switch gears to the more straightforward topic of dataflow in a traditional, feedforward neural network.</p>&#13;
<h3 class="h3" id="ch09lev1_2">Data Flow in Traditional Neural Networks</h3>&#13;
<p class="noindent">As we indicated above, in a traditional neural network, the weights between layers are stored as matrices. If layer <em>i</em> has <em>n</em> nodes and layer <em>i</em> − 1 has <em>m</em> outputs, then the weight matrix between the two layers, <em><strong>W<sub>i</sub></strong></em>, is an <em>n</em> × <em>m</em> matrix. When this matrix is multiplied on the right by the <em>m</em> × 1 column vector of outputs from layer <em>i</em> − 1, the result is an <em>n</em> × 1 output representing the input to the <em>n</em> nodes for layer <em>i</em>. Specifically, we calculate</p>&#13;
<p class="center"><em><strong>a<sub>i</sub></strong></em> = σ(<em><strong>W<sub>i</sub>a<sub>i−1</sub></strong></em> + <em><strong>b<sub>i</sub></strong></em>)</p>&#13;
<p class="noindent">where <em><strong>a<sub>i</sub></strong></em><sub>−1</sub>, the <em>m</em> × 1 vector of outputs from layer <em>i</em> − 1, multiplies <em><strong>W<sub>i</sub></strong></em> to produce an <em>n</em> × 1 column vector. We add <em><strong>b<sub>i</sub></strong></em>, the bias values for layer <em>i</em>, to this vector and apply the activation function, σ, to every element of the resulting vector, <strong><em>W<sub>i</sub>a<sub>i</sub></em></strong><sub>−1</sub> + <em><strong>b<sub>i</sub></strong></em>, to produce <em><strong>a<sub>i</sub></strong></em>, the activations for layer <em>i</em>. We feed the activations to layer <em>i</em> + 1 as the output of layer <em>i</em>. By using matrices and vectors, the rules of matrix multiplication automatically calculate all the necessary products without explicit loops in the code.</p>&#13;
<p class="indent">Let’s see an example with a simple neural network. We’ll generate a random dataset with two features and then split this dataset into train and test groups. We’ll use <code>sklearn</code> to train a simple feedforward neural network on the training set. The network has a single hidden layer with five nodes and uses a rectified linear activation function (ReLU). We’ll then test the trained <span epub:type="pagebreak" id="page_226"/>network to see how well it learned and, most importantly, look at the actual weight matrices and bias vectors.</p>&#13;
<p class="indent">To build the dataset, we’ll select a set of points in 2D space that are clustered but slightly overlapping. We want the network to have to learn something that isn’t completely trivial. Here is the code:</p>&#13;
<pre>   from sklearn.neural_network import MLPClassifier<br/>&#13;
<br/>&#13;
   np.random.seed(8675309)<br/>&#13;
<span class="ent">❶</span> x0 = np.random.random(50)-0.3<br/>&#13;
   y0 = np.random.random(50)+0.3<br/>&#13;
   x1 = np.random.random(50)+0.3<br/>&#13;
   y1 = np.random.random(50)-0.3<br/>&#13;
   x = np.zeros((100,2))<br/>&#13;
   x[:50,0] = x0; x[:50,1] = y0<br/>&#13;
   x[50:,0] = x1; x[50:,1] = y1<br/>&#13;
<span class="ent">❷</span> y = np.array([0]*50+[1]*50)<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> idx = np.argsort(np.random.random(100))<br/>&#13;
   x = x[idx]; y = y[idx]<br/>&#13;
   x_train = x[:75]; x_test = x[75:]<br/>&#13;
   y_train = y[:75]; y_test = y[75:]</pre>&#13;
<p class="indent">We need the <code>MLPClassifier</code> class from <code>sklearn</code>, so we load it first. We then define a 2D dataset, <code>x</code>, consisting of two clouds of 50 points each. The points are randomly distributed (<code>x0</code>, <code>y0</code> and <code>x1</code>, <code>y1</code>) but centered at (0.2, 0.8) and (0.8, 0.2), respectively <span class="ent">❶</span>. Note, we set the NumPy random number seed to a fixed value, so each run produces the same set of numbers we’ll see below. Feel free to remove this line and experiment with how well the network trains for various generations of the dataset.</p>&#13;
<p class="indent">We know the first 50 points in <code>x</code> are from what we’ll call class 0, and the next 50 points are class 1, so we define a label vector, <code>y</code> <span class="ent">❷</span>. Finally, we randomize the order of the points in <code>x</code> <span class="ent">❸</span>, being careful to alter the labels in the same way, and we split them into a training set (<code>x_train</code>) and labels (<code>y_train</code>) and a test set (<code>x_test</code>) and labels (<code>y_test</code>). We keep 75 percent of the data for training and leave the remaining 25 percent for testing.</p>&#13;
<p class="indent"><a href="ch09.xhtml#ch09fig02">Figure 9-2</a> shows a plot of the full dataset, with each feature on one of the axes. The circles correspond to class 0 instances and the squares to class 1 instances. There is clear overlap between the two classes.</p>&#13;
<span epub:type="pagebreak" id="page_227"/>&#13;
<div class="image" id="ch09fig02"><img src="Images/09fig02.jpg" alt="image" width="680" height="511"/></div>&#13;
<p class="figcap"><em>Figure 9-2: The dataset used to train the neural network, with the class 0 instances shown as circles and the class 1 instances as squares</em></p>&#13;
<p class="indent">We’re now ready to train the model. The <code>sklearn</code> toolkit makes it easy for us, if we use the defaults:</p>&#13;
<pre><span class="ent">❶</span> clf = MLPClassifier(hidden_layer_sizes=(5,))<br/>&#13;
   clf.fit(x_train, y_train)<br/><br/>&#13;
<span class="ent">❷</span> score = clf.score(x_test, y_test)<br/>&#13;
   print("Model accuracy on test set: %0.4f" % score)<br/>&#13;
<br/>&#13;
<span class="ent">❸</span> W0 = clf.coefs_[0].T<br/>&#13;
   b0 = clf.intercepts_[0].reshape((5,1))<br/>&#13;
   W1 = clf.coefs_[1].T<br/>&#13;
   b1 = clf.intercepts_[1]</pre>&#13;
<p class="indent">Training involves creating an instance of the model class <span class="ent">❶</span>. Notice that by using the defaults, which include using a ReLU activation function, we only need to specify the number of nodes in the hidden layers. We want one hidden layer with five nodes, so we pass in the tuple <code>(5,)</code>. Training is a single call to the <code>fit</code> function passing in the training data, <code>x_train</code>, and the associated labels, <code>y_train</code>. When complete, we test the model by computing the accuracy (<code>score</code>) on the test set (<code>x_test</code>, <code>y_test</code>) and display the result.</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_228"/>Neural networks are initialized randomly, but because we fixed the NumPy random number seed when we generated the dataset, and because <code>sklearn</code> uses the NumPy random number generator as well, the outcome of training the network should be the same for each run of the code. The model has an accuracy of 92 percent on the test data <span class="ent">❷</span>. This is convenient for us but concerning as well—so many toolkits use NumPy under the hood that interactions due to fixing the random number seed are probable, usually undesirable, and perhaps challenging to detect.</p>&#13;
<p class="indent">We’re now finally ready to get the weight matrices and bias vectors from the trained network <span class="ent">❸</span>. Because <code>sklearn</code> uses <code>np.dot</code> for matrix multiplication, we take the transpose of the weight matrices, <code>W0</code> and <code>W1</code>, to get them in a form that’s easier to follow mathematically. We’ll see precisely why this is necessary below. Likewise, <code>b0</code>, the bias vector for the hidden layer, is a 1D NumPy array, so we change it to a column vector. The output layer bias, <code>b1</code>, is a scalar, as there is only one output for this network, the value we pass to the sigmoid function to get the probability of class 1 membership.</p>&#13;
<p class="indent">Let’s walk through the network for the first test sample. To save space, we’ll only show the first three digits of the numeric values, but our calculations will use full precision. The input to the network is</p>&#13;
<div class="imagec"><img src="Images/228equ01.jpg" alt="Image" width="103" height="50"/></div>&#13;
<p class="noindent">We want the network to give us an output leading to the likelihood of this input belonging to class 1.</p>&#13;
<p class="indent">To get the output of the hidden layer, we multiply <em><strong>x</strong></em> by the weight matrix, <em><strong>W</strong></em><sub>0</sub>, add the bias vector, <em><strong>b</strong></em><sub>0</sub>, and pass that result through the ReLU:</p>&#13;
<div class="imagec"><img src="Images/228equ02.jpg" alt="Image" width="501" height="417"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_229"/>The hidden layer to output transition uses the same form, with <em><strong>a</strong></em><sub>0</sub> in place of <em><strong>x</strong></em>, but here, there is no ReLU applied:</p>&#13;
<div class="imagec"><img src="Images/229equ01.jpg" alt="Image" width="580" height="181"/></div>&#13;
<p class="indent">To get the final output probability, we use <em><strong>a</strong></em><strong><sub>1</sub></strong>, a scalar value, as the argument to the <em>sigmoid function</em>, also called the <em>logistic function</em>:</p>&#13;
<div class="imagec"><img src="Images/229equ02.jpg" alt="Image" width="483" height="45"/></div>&#13;
<p class="noindent">This means the network has assigned a 35.5 percent likelihood of the input value being a member of class 1. The usual threshold for class assignment for a binary model is 50 percent, so the network would assign <em><strong>x</strong></em> to class 0. A peek at <code>y_test[0]</code> tells us the network is correct in this case: <em><strong>x</strong></em> is from class 0.</p>&#13;
<h3 class="h3" id="ch09lev1_3">Data Flow in Convolutional Neural Networks</h3>&#13;
<p class="noindent">We saw above how data flow through a traditional neural network was straightforward matrix-vector math. To track data flow through a <em>convolutional neural network (CNN)</em>, we need to learn first what the convolution operation is and how it works. Specifically, we’ll learn how to pass data through convolutional and pooling layers to a fully connected layer at the top of the model. This sequence accounts for many CNN architectures, at least at a conceptual level.</p>&#13;
<h4 class="h4" id="ch09lev2_3">Convolution</h4>&#13;
<p class="noindent">Convolution involves two functions and the <em>sliding</em> of one over the other. If the functions are <em>f</em>(<em>x</em>) and <em>g</em>(<em>x</em>), convolution is defined as</p>&#13;
<div class="imagec" id="ch09equ01"><img src="Images/09equ01.jpg" alt="Image" width="477" height="53"/></div>&#13;
<p class="indent">Fortunately for us, we’re working in a discrete domain and more often than not with 2D inputs, so the integral is not actually used, though * is still a useful notation for the operation.</p>&#13;
<p class="indent">The net effect of <a href="ch09.xhtml#ch09equ01">Equation 9.1</a> is to slide <em>g</em>(<em>x</em>) over <em>f</em>(<em>x</em>) for different shifts. Let’s clarify using a 1D, discrete example.</p>&#13;
<h5 class="h5" id="ch09lev3_1"><span epub:type="pagebreak" id="page_230"/>Convolution in One Dimension</h5>&#13;
<p class="noindent"><a href="ch09.xhtml#ch09fig03">Figure 9-3</a> shows a plot on the bottom and two sets of numbers labeled <em>f</em> and <em>g</em> on the top.</p>&#13;
<div class="image" id="ch09fig03"><img src="Images/09fig03.jpg" alt="image" width="680" height="811"/></div>&#13;
<p class="figcap"><em>Figure 9-3: A 1D, discrete convolution</em></p>&#13;
<p class="indent">Let’s start with the numbers shown at the top of <a href="ch09.xhtml#ch09fig03">Figure 9-3</a>. The first row lists the discrete values of <em>f</em>. Below that is <em>g</em>, a three-element linear ramp. Convolution aligns <em>g</em> with the left edge of <em>f</em> as shown. We multiply corresponding elements between the two arrays,</p>&#13;
<p class="center">[2, 6, 15] × [−1, 0, 1] = [−2, 0, 15]</p>&#13;
<p class="noindent">and then sum the resulting values,</p>&#13;
<p class="center"><span epub:type="pagebreak" id="page_231"/>−2 + 0 + 15 = 13</p>&#13;
<p class="noindent">to produce the value that goes in the indicated element of the output, <em>f</em> * <em>g</em>. To complete the convolution, <em>g</em> slides one element to the right, and the process repeats. Note that in <a href="ch09.xhtml#ch09fig03">Figure 9-3</a>, we’re showing every other alignment of <em>f</em> and <em>g</em> for clarity, so it’ll appear as though <em>g</em> is sliding two elements to the right. In general, we refer to <em>g</em> as a <em>kernel</em>, the set of values that slide over the input, <em>f</em>.</p>&#13;
<p class="indent">The plot on the bottom of <a href="ch09.xhtml#ch09fig03">Figure 9-3</a> is <em>f</em>(<em>x</em>) = ⌊255 exp(−0.5<em>x</em><sup>2</sup>)⌋ for <em>x</em> in [−3, 3] at the points marked with circles. The floor operation makes the output an integer to simplify the discussion below.</p>&#13;
<p class="indent">The square points in <a href="ch09.xhtml#ch09fig03">Figure 9-3</a> are the output of the convolution of <em>f</em>(<em>x</em>) with <em>g</em>(<em>x</em>) = [−1, 0, 1].</p>&#13;
<p class="indent">The <em>f</em> and <em>f</em> * <em>g</em> points in <a href="ch09.xhtml#ch09fig03">Figure 9-3</a> are generated via</p>&#13;
<pre>x = np.linspace(-3,3,20)<br/>&#13;
f = (255*np.exp(-0.5*x**2)).astype("int32")<br/>&#13;
g = np.array([-1,0,1])<br/>&#13;
fp= np.convolve(f,g[::-1], mode='same')</pre>&#13;
<p class="noindent">This code requires some explanation.</p>&#13;
<p class="indent">First, we have <code>x</code>, a vector spanning [−3, 3] in 20 steps; this vector generates <code>f</code> (<em>f</em>(<em>x</em>) above). We want <code>f</code> to be of integer type, which is what <code>astype</code> does for us. Next, we define <code>g</code>, the small linear ramp. As we’ll see, the convolution operation slides <code>g</code> over the elements of <code>f</code> to produce the output.</p>&#13;
<p class="indent">The convolution operation comes next. As convolution is commonly used, NumPy supplies a 1D convolution function, <code>np.convolve</code>. The first argument is <em>f</em>, and the second is <em>g</em>. I’ll explain shortly why we added <code>[::-1]</code> to <code>g</code> to reverse it. I’ll also explain the meaning of <code>mode='same'</code>. The output of the convolution is stored in <code>fp</code>.</p>&#13;
<p class="indent">The first position shown in the top part of <a href="ch09.xhtml#ch09fig03">Figure 9-3</a> fills in the 13 in the output. Where does the 6 to the left of the 13 come from? Convolution has issues at the edges of <em>f</em>, where the kernel does not entirely cover the input. For a three-element kernel, there will be one edge element on each end of <em>f</em>. Kernels typically have an odd number of values, so there is a clear middle element. If <em>g</em> had five elements, there would be two elements on each end of <em>f</em> that <em>g</em> wouldn’t cover.</p>&#13;
<p class="indent">Convolution functions need to make a choice about these edge cases. One option would be to return only the valid portion of the convolution, to ignore the edge cases. If we had used this approach, called <em>valid convolution</em>, the output, <code>yp</code>, would start with element 13 and be two less in length than the input, <code>y</code>.</p>&#13;
<p class="indent">Another approach is to fill in missing values in <em>f</em> with zero. This is known as <em>zero padding</em>, and we typically use it to make the output of a convolution operation the same size as the input.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_232"/>Using <code>mode='same'</code> with <code>np.convolve</code> selects zero padding. This explains the 6 to the left of the 13. It’s what we get when adding a zero before the 2 in <em>f</em> and applying the kernel:</p>&#13;
<p class="center">[0, 2, 6] × [−1, 0, 1] = [0, 0, 6], 0 + 0 + 6 = 6</p>&#13;
<p class="noindent">If we wanted only the valid output values, we would have used <code>mode='valid'</code> instead.</p>&#13;
<p class="indent">The call to <code>np.convolve</code> above didn’t use <code>g</code>. We passed <code>g[::-1]</code> instead, the reverse of <code>g</code>. We did this to make <code>np.convolve</code> act like the convolutions used in deep neural networks. From a mathematical and signal processing perspective, convolution uses the reverse of the kernel. The <code>np.convolve</code> function, therefore, reverses the kernel, meaning we need to reverse it beforehand to get the effect we want. To be technical, if we perform the operation we’ve called <em>convolution</em> without flipping the kernel, we’re actually performing <em>cross-correlation</em>. This issue seldom comes up in deep learning because we <em>learn</em> the kernel elements during training—we don’t assign them ahead of time. With that in mind, any flipping of the kernel by the toolkit process implementing the convolution operation won’t affect the outcome, because the learned kernel values were learned with that flip in place. We’ll assume going forward that there is no flip and, when necessary, we’ll flip the kernels we give to NumPy and SciPy functions. Additionally, we’ll continue to use the term <em>convolution</em> in this no-flip-of-the-kernel deep learning sense.</p>&#13;
<p class="indent">In general, convolution with discrete inputs involves placing the kernel over the input starting on the left, multiplying matching elements, summing, and putting the result in the output at the point where the center of the kernel matches. The kernel then slides one element to the right, and the process repeats. We can extend the discrete convolution operation to two dimensions. Most modern deep CNNs use 2D kernels, though it’s possible to use 1D and 3D kernels as well.</p>&#13;
<h5 class="h5" id="ch09lev3_2">Convolution in Two Dimensions</h5>&#13;
<p class="noindent">Convolution with a 2D kernel requires a 2D array. Images are 2D arrays of values, and convolution is a common image processing operation. For example, let’s load an image, the face of the raccoon we saw in <a href="ch03.xhtml#ch03">Chapter 3</a>, and alter it with a 2D convolution. Consider the following:</p>&#13;
<pre>from scipy.signal import convolve2d<br/>&#13;
from scipy.misc import face<br/>&#13;
<br/>&#13;
img = face(True) <br/>&#13;
img = img[:512,(img.shape[1]-612):(img.shape[1]-100)]<br/>&#13;
<br/>&#13;
k = np.array([[1,0,0],[0,-8,0],[0,0,3]])<br/>&#13;
c = convolve2d(img, k, mode='same')</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_233"/>Here, we’re using the SciPy <code>convolve2d</code> function from the <code>signal</code> module. First, we load the raccoon image and subset it to a 512×512-pixel image of the raccoon’s face (<code>img</code>). Next, we define a 3 × 3 kernel, <code>k</code>. Lastly, we convolve the kernel, as it is, with the face image, storing the result in <code>c</code>. The <code>mode='same'</code> keyword zero pads the image to handle the edge cases.</p>&#13;
<p class="indent">The code above leads to</p>&#13;
<pre>img[:8,:8]:<br/>&#13;
    [[ 88 97 112 127 116  97  84  84]<br/>&#13;
     [ 62 70 100 131 126  88  52  51]<br/>&#13;
     [ 41 46  87 127 146 116  78  56]<br/>&#13;
     [ 42 45  76 107 145 137 112  76]<br/>&#13;
     [ 58 59  69  79 111 106  90  68]<br/>&#13;
     [ 74 73  68  60  72  74  72  67]<br/>&#13;
     [ 92 87  75  63  57  74  91  93]<br/>&#13;
     [105 97  85  74  60  79 102 110]]<br/>&#13;
<br/>&#13;
k:<br/>&#13;
    [[ 1  0 0]<br/>&#13;
     [ 0 -8 0]<br/>&#13;
     [ 0  0 3]]<br/>&#13;
<br/>&#13;
c[1:8,1:8]:<br/>&#13;
    [[-209 -382 -566 -511 -278  -69 -101]<br/>&#13;
     [-106 -379 -571 -638 -438 -284 -241]<br/>&#13;
     [-168 -391 -484 -673 -568 -480 -318]<br/>&#13;
     [-278 -357 -332 -493 -341 -242 -143]<br/>&#13;
     [-335 -304 -216 -265 -168 -165 -184]<br/>&#13;
     [-389 -307 -240 -197 -274 -396 -427]<br/>&#13;
     [-404 -331 -289 -215 -368 -476 -488]]</pre>&#13;
<p class="noindent">Here, we’re showing the upper 8 × 8 corner of the image and the valid portion of the convolution. Recall, the valid portion is the part where the kernel completely covers the input array.</p>&#13;
<p class="indent">For the kernel and the image, the first valid convolution output is −209. Mathematically, the first step is element-wise multiplication with the kernel,</p>&#13;
<div class="imagec"><img src="Images/233equ01.jpg" alt="Image" width="458" height="69"/></div>&#13;
<p class="noindent">followed by a summation,</p>&#13;
<p class="center">264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209</p>&#13;
<p class="noindent">Notice how the kernel used wasn’t <code>k</code> as we defined it. Instead, <code>convolve2d</code> flipped the kernel top to bottom and then left to right before it was applied. The remainder of <code>c</code> flows from moving the kernel one position to the right <span epub:type="pagebreak" id="page_234"/>and repeating the multiplication and addition. At the end of a row, the kernel moves down one position and back to the left, until the entire image has been processed. Deep learning toolkits refer to this motion as the <em>stride</em>, and it need not be one position or equal in the horizontal and vertical directions.</p>&#13;
<p class="indent"><a href="ch09.xhtml#ch09fig04">Figure 9-4</a> shows the effect of the convolution.</p>&#13;
<div class="image" id="ch09fig04"><img src="Images/09fig04.jpg" alt="image" width="643" height="319"/></div>&#13;
<p class="figcap"><em>Figure 9-4: The original raccoon face image (left) and the convolution result (right)</em></p>&#13;
<p class="indent">To make the image, <code>c</code> was shifted up, so the minimum value was zero, and then divided by the maximum to map to [0, 1]. Finally, the output was multiplied by 255 and displayed as a grayscale image. The original face image is on the left. The convolved image is on the right. Convolution of the image with the kernel has altered the image, emphasizing some features while suppressing others.</p>&#13;
<p class="indent">Convolving kernels with images isn’t merely an exercise to help us understand the convolution operation. It’s of profound importance in the training of CNNs. Conceptually, a CNN consists of two main parts: a set of convolution and other layers taught to learn a new representation of the input, and a top-level classifier taught to use the new representation to classify the inputs. It’s the joint learning of the new representation and the classifier that makes CNNs so powerful. The key to learning a new representation of the input is the set of learned convolution kernels. How the kernels alter the input as data flows through the CNN creates the new representation. Training with gradient descent and backpropagation teaches the network which kernels to create.</p>&#13;
<p class="indent">We’re now in a position to follow data through a CNN’s convolutional layers. Let’s take a look.</p>&#13;
<h4 class="h4" id="ch09lev2_4">Convolutional Layers</h4>&#13;
<p class="noindent">Above, we discussed how deep networks pass tensors from layer to layer and how the tensor usually has four dimensions, <em>N</em> × <em>H</em> × <em>W</em> × <em>C</em>. To follow data through a convolutional layer, we’ll ignore <em>N</em>, knowing that what we discuss is applied to each sample in the tensor. This leaves us with inputs to the convolutional layer that are <em>H</em> × <em>W</em> × <em>C</em>, a 3D tensor.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_235"/>The output of a convolutional layer is another 3D tensor. The height and width of the output depend on the convolution kernels’ particulars and how we decide to handle the edges. We’ll use valid convolution for the examples here, meaning we’ll discard parts of the input that the kernel doesn’t wholly cover. If the kernel is 3 × 3, the output will be two less in height and width, one less for each edge. A 5 × 5 kernel loses four in height and width, two less for each edge.</p>&#13;
<p class="indent">The convolutional layer uses sets of <em>filters</em> to accomplish its goal. A filter is a stack of kernels. We need one filter for each of the desired output channels. The number of kernels in the stack of each filter matches the number of channels in the input. So, if the input has <em>M</em> channels, and we want <em>N</em> output channels using <em>K</em> × <em>K</em> kernels, we need <em>N</em> filters, each of which is a stack of <em>M K</em> × <em>K</em> kernels.</p>&#13;
<p class="indent">Additionally, we have a bias value for each of the <em>N</em> filters. We’ll see below how the bias is used, but we now know how many parameters we need to learn to implement a convolutional layer with <em>M</em> input channels, <em>K</em> × <em>K</em> kernels, and <em>N</em> outputs. It’s <em>K</em> × <em>K</em> × <em>M</em> × <em>N</em> for <em>N</em> filters with <em>K</em> × <em>K</em> × <em>M</em> parameters each, plus <em>N</em> bias terms—one per filter.</p>&#13;
<p class="indent">Let’s make all of this concrete. We have a convolutional layer. The input to the layer is an (<em>H</em>,<em>W</em>,<em>C</em>) = (5,5,2) tensor, meaning a height and width of five and two channels. We’ll use a 3 × 3 kernel with valid convolution, so the output in height and width is 3 × 3 from the 5 × 5 input. We get to select the number of output channels. Let’s use three. Therefore, we need to use convolution and kernels to map a (5,5,2) input to a (3,3,3) output. From what we discussed above, we know we need three filters, and each filter has 3 × 3 × 2 parameters, plus a bias term.</p>&#13;
<p class="indent">Our input stack is</p>&#13;
<div class="imagec"><img src="Images/235equ01.jpg" alt="Image" width="241" height="222"/></div>&#13;
<p class="noindent">We’ve split the third dimension to show the two input channels, each 5 × 5.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_236"/>The three filters are</p>&#13;
<div class="imagec"><img src="Images/236equ01.jpg" alt="Image" width="469" height="161"/></div>&#13;
<p class="noindent">Again, we’ve separated the third dimension. Notice how each filter has two 3 × 3 kernels, one for each channel of the 5 × 5 × 2 input.</p>&#13;
<p class="indent">Let’s work through applying the first filter, <em>f</em><sub>0</sub>. We need to convolve the first channel of the input with the first kernel of <em>f</em><sub>0</sub>:</p>&#13;
<div class="imagec"><img src="Images/236equ02.jpg" alt="Image" width="522" height="116"/></div>&#13;
<p class="noindent">Then, we need to convolve the second input channel with the second kernel of <em>f</em><sub>0</sub>:</p>&#13;
<div class="imagec"><img src="Images/236equ03.jpg" alt="Image" width="566" height="117"/></div>&#13;
<p class="noindent">Finally, we add the two convolution outputs along with the single bias scalar:</p>&#13;
<div class="imagec"><img src="Images/236equ04.jpg" alt="Image" width="692" height="69"/></div>&#13;
<p class="noindent">We now have the first 3 × 3 output.</p>&#13;
<p class="indent">Repeating the process above for <em>f</em><sub>1</sub> and <em>f</em><sub>2</sub> gives</p>&#13;
<div class="imagec"><img src="Images/236equ05.jpg" alt="Image" width="409" height="69"/></div>&#13;
<p class="noindent">We’ve completed the convolutional layer and generated the 3 × 3 × 3 output.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_237"/>Many toolkits make it easy to add operations in the call that sets up the convolutional layer, but, conceptually, these are layers of their own that accept the 3 × 3 × 3 output as an input. For example, if requested, Keras will apply a ReLU to the output. Applying a ReLU, a nonlinearity, to the output of the convolution would give us</p>&#13;
<div class="imagec"><img src="Images/237equ01.jpg" alt="Image" width="510" height="68"/></div>&#13;
<p class="noindent">Note that all elements less than zero are now zero. We use a nonlinearity between convolutional layers for the same reason we use a nonlinear activation function in a traditional neural network: to keep the convolutional layers from collapsing into a single layer. Notice how the operation to generate the filter outputs is purely linear; each output element is a linear combination of input values. Adding the ReLU breaks this linearity.</p>&#13;
<p class="indent">One reason for the creation of convolutional layers was to reduce the number of learned parameters. For the example above, the input was 5 × 5 × 2 = 50 elements. The desired output was 3 × 3 × 3 = 27 elements. A fully connected layer between these would need to learn 50 × 27 = 1,350 weights, plus another 27 bias values. However, the convolutional layer learned three filters, each with 3 × 3 × 2 weights, as well as three bias values, for a total of 3(3 × 3 × 2) + 3 = 57 parameters. Adding the convolutional layer saved learning some 1,300 additional weights.</p>&#13;
<p class="indent">The output of a convolutional layer is often the input to a pooling layer. Let’s consider that type of layer next.</p>&#13;
<h4 class="h4" id="ch09lev2_5">Pooling Layers</h4>&#13;
<p class="noindent">Convolutional networks often use <em>pooling layers</em> after convolutional layers. Their use is a bit controversial, as they discard information, and the loss of information might make it harder for the network to learn spatial relationships. Pooling is generally performed in the spatial domain along the input tensor’s height and width while preserving the number of channels.</p>&#13;
<p class="indent">The pooling operation is straightforward: you move a window over the image, usually 2 × 2 with a stride of two, to group values. The specific pooling operation performed on each group is either max or average. The max-pooling operation preserves the maximum value in the window and discards the rest. Average pooling takes the mean of the values in the window.</p>&#13;
<p class="indent">A 2 × 2 window with a stride of two results in a reduction of a factor of two in each spatial direction. Therefore, a (24,24,32) input tensor leads to a (12,12,32) output tensor. <a href="ch09.xhtml#ch09fig05">Figure 9-5</a> illustrates the process for maximum pooling.</p>&#13;
<span epub:type="pagebreak" id="page_238"/>&#13;
<div class="image" id="ch09fig05"><img src="Images/09fig05.jpg" alt="image" width="467" height="298"/></div>&#13;
<p class="figcap"><em>Figure 9-5: Max pooling with a 2</em> × <em>2 window and a stride of two</em></p>&#13;
<p class="indent">One channel of the input, with a height and width of eight, is on the left. The 2 × 2 window slides over the input, jumping by two, so there is no overlap of windows. The output for each 2 × 2 region of the input is the maximum value. Average pooling would instead output the mean of the four numbers. As with normal convolution, at the end of the row, the window slides down two positions, and the process repeats to change the 8 × 8 input channel to a 4 × 4 output channel.</p>&#13;
<p class="indent">As mentioned above, pooling without overlap in the windows loses spatial information. This has caused some in the deep learning community, most notably Geoffrey Hinton, to lament its use, as dropping spatial information distorts the relationship between objects or parts of objects in the input. For example, applying a 2 × 2 max pooling window with a stride of one instead of two to the input matrix of <a href="ch09.xhtml#ch09fig05">Figure 9-5</a> produces</p>&#13;
<div class="imagec"><img src="Images/238equ01.jpg" alt="Image" width="214" height="166"/></div>&#13;
<p class="noindent">This is a 7 × 7 output, which only loses one row and column of the original 8 × 8 input. In this case, the input matrix was randomly generated, so we should expect a max-pooling operation biased toward eights and nines—there is no structure to capture. This is not usually the case in an actual CNN, of course, as it’s the spatial structure inherent in the inputs we wish to utilize.</p>&#13;
<p class="indent">Pooling is commonly used in deep learning, especially for CNNs, so it’s essential to understand what a pooling operation is doing and be aware of its potential pitfalls. Let’s move on now to the output end of a CNN, typically the fully connected layers.</p>&#13;
<h4 class="h4" id="ch09lev2_6"><span epub:type="pagebreak" id="page_239"/>Fully Connected Layers</h4>&#13;
<p class="noindent">A fully connected layer in a deep network is, in terms of weights and data, identical to a regular layer in a traditional neural network. Many deep networks concerned with classification pass the output of a set of convolution and pooling layers to the first fully connected layer via a layer that flattens the tensor, essentially unraveling it into a vector. Once the output is a vector, the fully connected layer uses a weight matrix in the same way a traditional neural network does to map a vector input to a vector output.</p>&#13;
<h4 class="h4" id="ch09lev2_7">Data Flow Through a Convolutional Neural Network</h4>&#13;
<p class="noindent">Let’s put all the pieces together to see how data flows through a CNN from input to output. We’ll use a simple CNN trained on the MNIST dataset, a collection of 28×28-pixel grayscale images of handwritten digits. The architecture is shown next.</p>&#13;
<p class="program1">Input → Conv(32) → Conv(64) → Pool → Flatten → Dense(128) → Dense(10)</p>&#13;
<p class="noindent">The input is a 28×28-pixel grayscale image (one channel). The convolutional layers (conv) use 3 × 3 kernels and valid convolution, so their output’s height and width are two less than their input. The first convolutional layer learns 32 filters while the second learns 64. We’re ignoring layers that do not affect the amount of data in the network, like the ReLU layers after the convolutional layers. The max-pooling layer is assumed to use a 2 × 2 window with a stride of two. The first fully connected layer (dense) has 128 nodes, followed by an output layer of 10 nodes, one for each digit, 0 to 9.</p>&#13;
<p class="indent">The tensors passed through this network for a single input sample are</p>&#13;
<p class="program1">(28,28,1) →(26,26,32) →(24,24,64) →(12,12,64) → 9216 → 128 → 10<br/>&#13;
  Input        Conv        Conv         Pool      Flatten  Dense   Dense<br/></p>&#13;
<p class="noindent">The flatten layer unravels the (12,12,64) tensor to form a vector of 9,216 elements (12 × 12 × 64 = 9,216). We pass the 9,216 elements that the flatten layer outputs through the first dense layer to generate 128 output values, and the last step takes the 128-element vector and maps it to 10 output values.</p>&#13;
<p class="indent">Note, the values above refer to the <em>data</em> passed through the network for each input sample, one of the <em>N</em> samples in the minibatch. This is not the same as the number parameters (weights and biases) the network needed to learn during training.</p>&#13;
<p class="indent">The network shown above was trained on the MNIST digits using Keras. <a href="ch09.xhtml#ch09fig06">Figure 9-6</a> illustrates the action of the network for two inputs by showing, visually, the output of each layer. Specifically, it shows each layer’s output for two input images, depicting a 4 and a 6.</p>&#13;
<span epub:type="pagebreak" id="page_240"/>&#13;
<div class="image" id="ch09fig06"><img src="Images/09fig06.jpg" alt="image" width="647" height="554"/></div>&#13;
<p class="figcap"><em>Figure 9-6: A visual representation of the output of a CNN for two sample inputs</em></p>&#13;
<p class="indent">Starting at the top, we see the two inputs. For the figure, intensities have been reversed, so darker represents higher numeric values. The input is a (28,28,1) tensor, the 1 indicating a single-channel grayscale image. Valid convolution with a 3 × 3 kernel returns a 26 × 26 output. The first convolutional layer learned 32 filters, so the output is a (26,26,32) tensor. In the figure, we show the output of each filter as an image. Zero is scaled to midlevel gray (intensity 128), more positive values are darker, and more negative values are lighter. We see differences in how the inputs have been affected by the learned filters. The single input channel means each filter in this layer is a single 3 × 3 kernel. Transitions between light and dark indicate edges in particular orientations.</p>&#13;
<p class="indent">We pass the (26,26,32) tensor through a ReLU (not shown here) and then through the second convolutional layer. The output of this layer is a (24,24,64) tensor shown as an 8 × 8 grid of images in the figure. We can see many parts of the input digits highlighted.</p>&#13;
<p class="indent">The pooling layer preserves the number of channels but reduces the spatial dimension by two. In image form, the 8 × 8 grid of 24×24-pixel images is now an 8 × 8 grid of 12×12-pixel images. The flatten operation maps the (12,12,64) tensor to a 9,216-element vector.</p>&#13;
<p class="indent">The output of the first dense layer is a vector of 128 numbers. For <a href="ch09.xhtml#ch09fig06">Figure 9-6</a>, we show this as a 128-element bar code. The values run from left to right. The height of each bar is unimportant and was selected only to make the bar code easy to see. The bar code generated from the input <span epub:type="pagebreak" id="page_241"/>image is the final representation that the top layer of 10 nodes uses to create the output passed through the softmax function. The highest softmax output is used to select the class label, “4” or “6.”</p>&#13;
<p class="indent">Therefore, we can think of all the CNN layers through the first dense layer as mapping inputs to a new representation, one that makes it easy for a simple classifier to handle. Indeed, if we pass 10 examples of “4” and “6” digits through this network and display the resulting 128-node feature vectors, we get <a href="ch09.xhtml#ch09fig07">Figure 9-7</a>, where we can easily see the difference between the digit patterns.</p>&#13;
<div class="image" id="ch09fig07"><img src="Images/09fig07.jpg" alt="image" width="650" height="535"/></div>&#13;
<p class="figcap"><em>Figure 9-7: The first fully connected layer outputs for multiple “4” and “6” inputs</em></p>&#13;
<p class="indent">Of course, the entire point of writing digits as we do is to make it easy for humans to see the differences between them. While we could teach ourselves to differentiate digits using the 128-element vector images, we naturally prefer to use the written digits because of habitual use and the fact we already employ highly sophisticated hierarchical feature detectors via our brain’s visual system.</p>&#13;
<p class="indent">The example of a CNN learning a new input representation that’s more conducive to interpretation by a machine is worth bearing in mind, since what a human might use in an image as a clue to its classification is not necessarily what a network learns to use. This might explain, in part, why certain preprocessing steps, like the changes made to training samples during data augmentation, are so effective in helping the network learn to generalize, when many of those alterations seem strange to us.</p>&#13;
<h3 class="h3" id="ch09lev1_4"><span epub:type="pagebreak" id="page_242"/>Summary</h3>&#13;
<p class="noindent">The goal of this chapter was to demonstrate how neural networks manipulate data from input to output. Naturally, we couldn’t cover all network types, but, in general, the principles are the same: for traditional neural networks, data is passed from layer to layer as a vector, and for deep networks, it’s passed as a tensor, typically of four dimensions.</p>&#13;
<p class="indent">We learned how to present data to a network, either as a feature vector or a multidimensional input. We followed this by looking at how to pass data through a traditional neural network. We saw how the vectors used as input to, and output from, a layer made the implementation of a traditional neural network a straightforward exercise in matrix-vector multiplication and addition.</p>&#13;
<p class="indent">Next, we saw how a deep convolutional network passes data from layer to layer. We learned first about the convolution operation and then about the specifics of how convolutional and pooling layers manipulate data as tensors—a 3D tensor for each sample in the input minibatch. At the top of a CNN meant for classification are fully connected layers, which we saw act precisely as they do in a traditional neural network.</p>&#13;
<p class="indent">We ended the chapter by showing, visually, how input images moved through a CNN to produce an output representation, allowing the network to label the inputs correctly. We briefly discussed what this process might mean in terms of what a network picks up on during training and how that might differ from what a human naturally sees in an image.</p>&#13;
<p class="indent">We are now in a position to discuss backpropagation, the first of the two critical algorithms that, together with gradient descent, make training deep neural networks possible.</p>&#13;
</div></body></html>