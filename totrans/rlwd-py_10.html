<html><head></head><body>
<h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_224"/><span epub:type="pagebreak" id="page_225"/><span class="big">10</span><br/>RESTRICTING ACCESS WITH FACE RECOGNITION</h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>&#13;
<p class="noindent">In the previous chapter, you were a technician in the Coalition Marines, a branch of the Space Force. In this chapter, you’re that same technician, only your job just got harder. Your role is now to <em>recognize</em> faces, rather than just detect them. Your commander, Captain Demming, has discovered the lab containing the mutant-producing interdimensional portal, and he wants access to it restricted to just himself.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_226"/>As in the previous chapter, you’ll need to act quickly, so you’ll rely on Python and OpenCV for speed and efficiency. Specifically, you’ll use OpenCV’s local binary pattern histogram (LBPH) algorithm, one of the oldest and easiest to use face recognition algorithms, to help lock down the lab. If you haven’t installed and used OpenCV before, check out “Installing the Python Libraries” on <a href="ch01.xhtml#page_6">page 6</a>.</p>&#13;
<h3 class="h3" id="ch00lev1sec81"><strong>Recognizing Faces with Local Binary Pattern Histograms</strong></h3>&#13;
<p class="noindent">The LBPH algorithm relies on feature vectors to recognize faces. Remember from <a href="ch05.xhtml">Chapter 5</a> that a feature vector is basically a list of numbers in a specific order. In the case of LBPH, the numbers represent some qualities of a face. For instance, suppose you could discriminate between faces with just a few measurements, such as the separation of the eyes, the width of the mouth, the length of the nose, and the width of the face. These four measurements, in the order listed and expressed in centimeters, could compose the following feature vector: (5.1, 7.4, 5.3, 11.8). Reducing faces in a database to these vectors enables rapid searches, and it allows us to express the difference between them as the numerical difference, or <em>distance</em>, between two vectors.</p>&#13;
<p class="indent">Recognizing faces computationally requires more than four features, of course, and the many available algorithms work on different features. Among these algorithms are Eigenfaces, LBPH, Fisherfaces, scale-invariant feature transform (SIFT), speeded-up robust features (SURF), and various neural network approaches. When the face images are acquired under controlled conditions, these algorithms can have a high accuracy rate, about as high as that of humans.</p>&#13;
<p class="indent">Controlled conditions for images of faces might involve a frontal view of each face with a normal, relaxed expression and, to be usable by all algorithms, consistent lighting conditions and resolutions. The face should be unobscured by facial hair and glasses, assuming the algorithm was taught to recognize the face under those conditions.</p>&#13;
<h4 class="h4" id="ch00lev2sec48"><strong><em>The Face Recognition Flowchart</em></strong></h4>&#13;
<p class="noindent">Before getting into the details of the LBPH algorithm, let’s look at how face recognition works in general. The process consists of three main steps: capturing, training, and predicting.</p>&#13;
<p class="indent">In the capture phase, you gather the images that you’ll use to train the face recognizer (<a href="ch10.xhtml#ch010fig1">Figure 10-1</a>). For each face you want to recognize, you should take a dozen or more images with multiple expressions.</p>&#13;
<div class="image"><img src="../images/fig10_01.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig1"/>Figure 10-1: Capturing facial images to train the face recognizer</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_227"/>The next step in the capture process is to detect the face in the image, draw a rectangle around it, crop the image to the rectangle, resize the cropped images to the same dimensions (depending on the algorithm), and convert them to grayscale. The algorithms typically keep track of faces using integers, so each subject will need a unique ID number. Once processed, the faces are stored in a single folder, which we’ll call the database.</p>&#13;
<p class="indent">The next step is to train the face recognizer (<a href="ch10.xhtml#ch010fig2">Figure 10-2</a>). The algorithm —in our case, LBPH—analyzes each of the training images and then writes the results to a YAML (<em>.yml</em>) file, a human-readable data-serialization language used for data storage. YAML originally meant “Yet Another Markup Language” but now stands for “YAML Ain’t Markup Language” to stress that it’s more than just a document markup tool.</p>&#13;
<div class="image"><img src="../images/fig10_02.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig2"/>Figure 10-2: Training the face recognizer and writing the results to a file</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_228"/>With the face recognizer trained, the final step is to load a new, untrained face and predict its identity (<a href="ch10.xhtml#ch010fig3">Figure 10-3</a>). These unknown faces are prepped in the same manner as the training images—that is, cropped, resized, and converted to grayscale. The recognizer then analyzes them, compares the results to the faces in the YAML file, and predicts which face matches best.</p>&#13;
<div class="image"><img src="../images/fig10_03.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig3"/>Figure 10-3: Predicting unknown faces using the trained recognizer</p>&#13;
<p class="indent">Note that the recognizer will make a prediction about the identity of every face. If there’s only one trained face in the YAML file, the recognizer will assign every face the trained face’s ID number. It will also output a <em>confidence</em> factor, which is really a measurement of the distance between the new face and the trained face. The larger the number, the worse the match. We’ll talk about this more in a moment, but for now, know that you’ll use a threshold value to decide whether the predicted face is correct. If the confidence exceeds the accepted threshold value, the program will discard the match and classify the face as “unknown” (see <a href="ch10.xhtml#ch010fig3">Figure 10-3</a>).</p>&#13;
<h4 class="h4" id="ch00lev2sec49"><strong><em>Extracting Local Binary Pattern Histograms</em></strong></h4>&#13;
<p class="noindent">The OpenCV face recognizer you’ll use is based on local binary patterns. These texture descriptors were first used around 1994 to describe and classify surface textures, differentiating concrete from carpeting, for example. Faces are also composed of textures, so the technique works for face recognition.</p>&#13;
<p class="indent">Before you can extract histograms, you first need to generate the binary patterns. An LBP algorithm computes a local representation of texture by comparing each pixel with its surrounding neighbors. The first computational step is to slide a small window across the face image and capture the pixel information. <a href="ch10.xhtml#ch010fig4">Figure 10-4</a> shows an example window.</p>&#13;
<div class="image"><img src="../images/fig10_04.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig4"/>Figure 10-4: Example 3<span class="normal">×</span>3 pixel sliding window used to capture local binary patterns</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_229"/>The next step is to convert the pixels into a binary number, using the central value (in this case 90) as a threshold. You do this by comparing the eight neighboring values to the threshold. If a neighboring value is equal to or higher than the threshold, assign it 1; if it’s lower than the threshold, assign it 0. Next, ignoring the central value, concatenate the binary values line by line (some methods use a clockwise rotation) to form a new binary value (11010111). Finish by converting this binary number into a decimal number (215) and storing it at the central pixel location.</p>&#13;
<p class="indent">Continue sliding the window until all the pixels have been converted to LBP values. In addition to using a square window to capture neighboring pixels, the algorithm can use a radius, a process called <em>circular LBP</em>.</p>&#13;
<p class="indent">Now it’s time to extract histograms from the LBP image produced in the previous step. To do this, you use a grid to divide the LBP image into rectangular regions (<a href="ch10.xhtml#ch010fig5">Figure 10-5</a>). Within each region, you construct a histogram of the LBP values (labeled “Local Region Histogram” in <a href="ch10.xhtml#ch010fig5">Figure 10-5</a>).</p>&#13;
<div class="image"><img src="../images/fig10_05.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig5"/>Figure 10-5: Extracting the LBP histograms</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_230"/>After constructing the local region histograms, you follow a predetermined order to normalize and concatenate them into one long histogram (shown truncated in <a href="ch10.xhtml#ch010fig5">Figure 10-5</a>). Because you’re using a grayscale image with intensity values between 0 and 255, there are 256 positions in each histogram. If you’re using a 10×10 grid, as in <a href="ch10.xhtml#ch010fig5">Figure 10-5</a>, then there are 10×10×256 = 25,600 positions in the final histogram. The assumption is that this composite histogram includes diagnostic features needed to recognize a face. They are thus <em>representations</em> of a face image, and face recognition consists of comparing these representations, rather than the images themselves.</p>&#13;
<p class="indent">To predict the identity of a new, unknown face, you extract its concatenated histogram and compare it to the existing histograms in the trained database. The comparison is a measure of the distance between histograms. This calculation may use various methods, including Euclidian distance, absolute distance, chi-square, and so on. The algorithm returns the ID number of the trained image with the closest histogram match, along with the confidence measurement. You can then apply a threshold to the confidence value, as in <a href="ch10.xhtml#ch010fig3">Figure 10-3</a>. If the confidence for the new image is below the threshold value, assume you have a positive match.</p>&#13;
<p class="indent">Because OpenCV encapsulates all these steps, the LBPH algorithm is easy to implement. It also produces great results in a controlled environment and is unaffected by changes in lighting conditions (<a href="ch10.xhtml#ch010fig6">Figure 10-6</a>).</p>&#13;
<div class="image"><img src="../images/fig10_06.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig6"/>Figure 10-6: LBPs are robust against changes in illumination</p>&#13;
<p class="indent">The LBPH algorithm handles changes to lighting conditions well because it relies on comparisons among pixel intensities. Even if illumination is much brighter in one image than another, the relative reflectivity of the face remains the same, and LBPH can capture it.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec82"><strong>Project #14: Restricting Access to the Alien Artifact</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_231"/>Your squad has fought its way to the lab containing the portal-producing alien artifact. Captain Demming orders it locked down immediately, with access restricted to just him. Another technician will override the current system with a military laptop. Captain Demming will gain access through this laptop using two levels of security: a typed password and face verification. Aware of your skills with OpenCV, he’s ordered <em>you</em> to handle the facial verification part.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that recognizes Captain Demming’s face.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec50"><strong><em>The Strategy</em></strong></h4>&#13;
<p class="noindent">You’re pressed for time and working under adverse conditions, so you want to use a fast and easy tool with a good performance record, like OpenCV’s LBPH face recognizer. You’re aware that LBPH works best under controlled conditions, so you’ll use the same laptop webcam to capture both the training images and the face of anyone trying to access the lab.</p>&#13;
<p class="indent">In addition to pictures of Demming’s face, you’ll want to capture some faces that don’t belong to Captain Demming. You’ll use these faces to ensure that all the positive matches really belong to the captain. Don’t worry about setting up the password, isolating the program from the user, or hacking into the current system; the other technician will handle these tasks while you go out and blast some mutants.</p>&#13;
<h4 class="h4" id="ch00lev2sec51"><strong><em>Supporting Modules and Files</em></strong></h4>&#13;
<p class="noindent">You’ll use both OpenCV and <span class="literal">NumPy</span> to do most of the work in this project. If you don’t already have them installed, see “Installing the Python Libraries” on <a href="ch01.xhtml#page_6">page 6</a>. You’ll also need <span class="literal">playsound</span>, for playing sounds, and <span class="literal">pyttsx3</span>, for text-to-speech functionality. You can find out more about these modules, including installation instructions, in “The Code” on <a href="ch09.xhtml#page_207">page 207</a>.</p>&#13;
<p class="indent">The code and supporting files are in the <em>Chapter_10</em>  folder from the book’s website, <em><a href="https://nostarch.com/real-world-python/">https://nostarch.com/real-world-python/</a></em>. Keep the folder structure and filenames the same after downloading them (<a href="ch10.xhtml#ch010fig7">Figure 10-7</a>). Note that the <em>tester</em> and <em>trainer</em> folders are created later and will not be included in the download.</p>&#13;
<div class="image"><img src="../images/fig10_07.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig7"/>Figure 10-7: File structure for Project 14</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_232"/>The <em>demming_trainer</em> and <em>demming_tester</em> folders contain images of Captain Demming and others that you can use for this project. The code currently references these folders.</p>&#13;
<p class="indent">If you want to supply your own images—for example, to use your own face to represent Captain Demming’s—then you’ll use the folders named <em>trainer</em> and <em>tester</em>. The code that follows will create the <em>trainer</em> folder for you. You’ll need to manually create the <em>tester</em> folder and add some images of yourself, as described later. Of course, you’ll need to edit the code so that it points to these new folders.</p>&#13;
<h4 class="h4" id="ch00lev2sec52"><strong><em>The Video Capture Code</em></strong></h4>&#13;
<p class="noindent">The first step (performed by the <em>1_capture.py</em> code) is to capture the facial images that you’ll need for training the recognizer. You can skip this step if you plan to use the images provided in the <em>demming_trainer</em> folder.</p>&#13;
<p class="indent">To use your own face for Captain Demming, use your computer’s camera to record about a dozen face shots with various expressions and no glasses. If you don’t have a webcam, you can skip this step, take selfies with your phone, and save them to a folder named <em>trainer</em>, as shown in <a href="ch10.xhtml#ch010fig7">Figure 10-7</a>.</p>&#13;
<h5 class="h5"><strong>Importing Modules, and Setting Up Audio, a Webcam, Instructions, and File Paths</strong></h5>&#13;
<p class="noindent"><a href="ch10.xhtml#ch010list1">Listing 10-1</a> imports modules, initializes and sets up the audio engine and the Haar cascade classifier, initializes the camera, and provides user <span epub:type="pagebreak" id="page_233"/>instructions. You need the Haar cascades because you must detect a face before you can recognize it. For a refresher on Haar cascades and face detection, see “Detecting Faces in Photographs” on <a href="ch09.xhtml#page_204">page 204</a>.</p>&#13;
<pre><span class="codeitalic1">1_capture.py</span>, part 1&#13;
   import os&#13;
   import pyttsx3&#13;
   import cv2 as cv&#13;
   from playsound import playsound&#13;
   &#13;
   engine = pyttsx3.init()&#13;
<span class="ent">➊</span> engine.setProperty('rate', 145)&#13;
   engine.setProperty('volume', 1.0)&#13;
   &#13;
   root_dir = os.path.abspath('.')&#13;
   tone_path = os.path.join(root_dir, 'tone.wav')&#13;
   &#13;
<span class="ent">➋</span> path = "C:/Python372/Lib/site-packages/cv2/data/"&#13;
   face_detector = cv.CascadeClassifier(path + &#13;
                                        'haarcascade_frontalface_default.xml')&#13;
   cap = cv.VideoCapture(0)&#13;
   if not cap.isOpened(): &#13;
       print("Could not open video device.")&#13;
<span class="ent">➌</span> cap.set(3, 640)  # Frame width.&#13;
   cap.set(4, 480)  # Frame height.&#13;
   &#13;
   engine.say("Enter your information when prompted on screen. \&#13;
              Then remove glasses and look directly at webcam. \&#13;
              Make multiple faces including normal, happy, sad, sleepy. \&#13;
              Continue until you hear the tone.")&#13;
   engine.runAndWait()&#13;
   &#13;
<span class="ent">➍</span> name = input("\nEnter last name: ")&#13;
   user_id = input("Enter assigned ID Number: ")&#13;
   print("\nCapturing face. Look at the camera now!")</pre>&#13;
<p class="listing"><a id="ch010list1"/>Listing 10-1: Importing modules and setting up audio and detector files, a webcam, and instructions</p>&#13;
<p class="indent">The imports are the same as those used to detect faces in the previous chapter. You’ll use the operating system (via the <span class="literal">os</span> module) to manipulate file paths, <span class="literal">pyttsx3</span> to play text-to-speech audio instructions, <span class="literal">cv</span> to work with images and run the face detector and recognizer, and <span class="literal">playsound</span> to play a tone that lets users know when the program has finished capturing their image.</p>&#13;
<p class="indent">Next, set up the text-to-speech engine. You’ll use this to tell the user how to run the program. The default voice is dependent on your particular operating system. The engine’s <span class="literal">rate</span> parameter is currently optimized for the American “David” voice on Windows <span class="ent">➊</span>. You may want to edit the argument if you find the speech to be too fast or too slow. If you want to change the voice, see the instructions accompanying <a href="ch09.xhtml#ch09list1">Listing 9-1</a> on <a href="ch09.xhtml#page_209">page 209</a>.</p>&#13;
<p class="indent">You’ll use a tone to alert the user that the video capture process has ended. Set up the path to the <em>tone.wav</em> audio file as you did in <a href="ch09.xhtml">Chapter 9</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_234"/>Now, provide the path to the Haar cascade file <span class="ent">➋</span> and assign the classifier to a variable named <span class="literal">face_detector</span>. The path shown here is for my Windows machine; your path may be different. On macOS, for example, you can find the files under <em>opencv/data/haarcascades</em>. You can also find them online at <em><a href="https://github.com/opencv/opencv/tree/master/data/haarcascades/">https://github.com/opencv/opencv/tree/master/data/haarcascades/</a></em>.</p>&#13;
<p class="indent">In <a href="ch09.xhtml">Chapter 9</a>, you learned how to capture your face using your computer’s webcam. You’ll use similar code in this program, starting with a call to <span class="literal">cv.VideoCapture(0)</span>. The <span class="literal">0</span> argument refers to the active camera. If you have multiple cameras, you may need to use another number, such as <span class="literal">1</span>, which you can determine through trial and error. Use a conditional to check that the camera opened, and if it did, set the frame width and height, respectively <span class="ent">➌</span>. The first argument in both methods refers to the position of the width or height parameter in the list of arguments.</p>&#13;
<p class="indent">For security reasons, you’ll be present to supervise the video capture phase of the process. Nevertheless, use the <span class="literal">pyttsx3</span> engine to explain the procedure to the user (this way you don’t have to remember it). To control the acquisition conditions to ensure accurate recognition later, the user will need to remove any glasses or face coverings and adopt multiple expressions. Among these should be the expression they plan to use each time they access the lab.</p>&#13;
<p class="indent">Finally, they’ll need to follow some printed instructions on the screen. First, they’ll enter their last name <span class="ent">➍</span>. You don’t need to worry about duplicates right now, as Captain Demming will be the only user. Plus, you’ll assign the user a unique ID number. OpenCV will use this variable, <span class="literal">user_id</span>, to keep track of all the faces during training and prediction. Later, you’ll create a dictionary so you can keep track of which user ID belongs to which person, assuming more people are granted access in the future.</p>&#13;
<p class="indent">As soon as the user enters their ID number and presses <small>ENTER</small>, the camera will turn on and begin capturing images, so let them know this with another call to <span class="literal">print()</span>. Remember from the previous chapter that the Haar cascade face detector is sensitive to head orientation. For it to function properly, the user must look right at the webcam and keep their head as straight as possible.</p>&#13;
<h5 class="h5"><strong>Capturing the Training Images</strong></h5>&#13;
<p class="noindent"><a href="ch10.xhtml#ch010list2">Listing 10-2</a> uses the webcam and a <span class="literal">while</span> loop to capture a specified number of face images. The code saves the images to a folder and sounds a tone when the operation is complete.</p>&#13;
<pre><span class="codeitalic1">1_capture.py</span>, part 2&#13;
if not os.path.isdir('trainer'):&#13;
    os.mkdir('trainer')&#13;
os.chdir('trainer')&#13;
&#13;
frame_count = 0&#13;
 &#13;
while True:&#13;
    # Capture frame-by-frame for total of 30 frames.&#13;
    _, frame = cap.read()&#13;
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)&#13;
 <span class="ent">➊</span> face_rects = face_detector.detectMultiScale(gray, scaleFactor=1.2,&#13;
                                                minNeighbors=5)     &#13;
    for (x, y, w, h) in face_rects:&#13;
        frame_count += 1&#13;
        cv.imwrite(str(name) + '.' + str(user_id) + '.'&#13;
                   + str(frame_count) + '.jpg', gray[y:y+h, x:x+w])&#13;
        cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)&#13;
        cv.imshow('image', frame)&#13;
        cv.waitKey(400)&#13;
 <span class="ent">➋</span> if frame_count &gt;= 30:&#13;
        break&#13;
     &#13;
print("\nImage collection complete. Exiting...")&#13;
playsound(tone_path, block=False)&#13;
cap.release()&#13;
cv.destroyAllWindows()</pre>&#13;
<p class="listing"><a id="ch010list2"/>Listing 10-2: Capturing video images using a loop</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_235"/>Start by checking for a directory named <em>trainer</em>. If it doesn’t exist, use the operating system module’s <span class="literal">mkdir()</span> method to make the directory. Then change the current working directory to this <em>trainer</em> folder.</p>&#13;
<p class="indent">Now, initialize a <span class="literal">frame_count</span> variable to 0. The code will capture and save a video frame only if it detects a face. To know when to end the program, you’ll need to keep count of the number of captured frames.</p>&#13;
<p class="indent">Next, start a <span class="literal">while</span> loop set to <span class="literal">True</span>. Then call the <span class="literal">cap</span> object’s <span class="literal">read()</span> method. As noted in the previous chapter, this method returns a tuple consisting of a Boolean return code and a <span class="literal">numpy</span> <span class="literal">ndarray</span> object representing the current frame. The return code is typically used to check whether you’ve run out of frames when reading from a file. Since we’re not reading from a file here, assign it to an underscore to indicate an unused variable.</p>&#13;
<p class="indent">Both face detection and face recognition work on grayscale images, so convert the frame to grayscale and name the resulting array <span class="literal">gray</span>. Then, call the <span class="literal">detectMultiscale()</span> method to detect faces in the image <span class="ent">➊</span>. You can find details of how this method works in the discussion of <a href="ch09.xhtml#ch09list2">Listing 9-2</a> on <a href="ch09.xhtml#page_212">page 212</a>. Because you’re controlling conditions by having the user look into a laptop’s webcam, you can rest assured that the algorithm will work well, though you should certainly check the results.</p>&#13;
<p class="indent">The previous method should output the coordinates for a rectangle around the face. Start a <span class="literal">for</span> loop through each set of coordinates and immediately advance the <span class="literal">frame_count</span> variable by 1.</p>&#13;
<p class="indent">Use OpenCV’s <span class="literal">imwrite()</span> method to save the image to the <em>trainer</em> folder. The folders use the following naming logic: <em>name.user_id.frame_count.jpg</em> (such as <em>demming.1.9.jpg</em>). Save only the portion of the image within the face rectangle. This will help ensure you aren’t training the algorithm to recognize background features.</p>&#13;
<p class="indent">The next two lines draw a face rectangle on the original frame and show it. This is so the user—Captain Demming—can check that his head is upright and his expressions are suitable. The <span class="literal">waitKey()</span> method delays the capture process enough for the user to cycle through multiple expressions.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_236"/>Even if Captain Demming will always adopt a relaxed, neutral expression when having his identity verified, training the software on a range of expressions will lead to more robust results. Along these lines, it’s also helpful if the user tilts their head <em>slightly</em> from side to side during the capture phase.</p>&#13;
<p class="indent">Next, check whether the target frame count has been reached, and if it has, break out of the loop <span class="ent">➋</span>. Note that, if no one is looking at the camera, the loop will run forever. It counts frames only if the cascade classifier detects a face and returns a face rectangle.</p>&#13;
<p class="indent">Let the user know that the camera has turned off by printing a message and sounding the tone. Then end the program by releasing the camera and destroying all the image windows.</p>&#13;
<p class="indent">At this point, the <em>trainer</em> folder should contain 30 images of the user’s closely cropped face. In the next section, you’ll use these images—or the set provided in the <em>demming_trainer</em> folder—to train OpenCV’s face recognizer.</p>&#13;
<h4 class="h4" id="ch00lev2sec53"><strong><em>The Face Trainer Code</em></strong></h4>&#13;
<p class="noindent">The next step is to use OpenCV to create an LBPH-based face recognizer, train it with the training images, and save the results as a reusable file. If you’re using your own face to represent Captain Demming’s, you’ll point the program to the <em>trainer</em> folder. Otherwise, you’ll need to use the <em>demming _trainer</em> folder, which, along with the <em>2_train.py</em> file containing the code, is in the downloadable <em>Chapter_10</em> folder.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch010list3">Listing 10-3</a> sets up paths to the Haar cascades used for face detection and the training images captured by the previous program. OpenCV keeps track of faces using label integers, rather than name strings, and the listing also initializes lists to hold the labels and their related images. It then loops through the training images, loads them, extracts a user ID number from the filename, and detects the faces. Finally, it trains the recognizer and saves the results to a file.</p>&#13;
<pre><span class="codeitalic1">2_train.py</span>&#13;
   import os&#13;
   import numpy as np&#13;
   import cv2 as cv&#13;
   &#13;
   cascade_path = "C:/Python372/Lib/site-packages/cv2/data/"&#13;
   face_detector = cv.CascadeClassifier(cascade_path +&#13;
                                       'haarcascade_frontalface_default.xml')&#13;
   &#13;
<span class="ent">➊</span> train_path = './demming_trainer'  # Use for provided Demming face.   &#13;
   #train_path = './trainer'  # Uncomment to use your face.&#13;
   image_paths = [os.path.join(train_path, f) for f in os.listdir(train_path)]&#13;
   images, labels = [], []&#13;
   &#13;
   for image in image_paths:&#13;
       train_image = cv.imread(image, cv.IMREAD_GRAYSCALE)&#13;
    <span class="ent">➋</span> label = int(os.path.split(image)[-1].split('.')[1])&#13;
       name = os.path.split(image)[-1].split('.')[0]&#13;
       frame_num = os.path.split(image)[-1].split('.')[2]&#13;
    <span class="ent">➌</span> faces = face_detector.detectMultiScale(train_image)&#13;
       for (x, y, w, h) in faces:&#13;
           images.append(train_image[y:y + h, x:x + w])&#13;
           labels.append(label)&#13;
           print(f"Preparing training images for {name}.{label}.{frame_num}")&#13;
           cv.imshow("Training Image", train_image[y:y + h, x:x + w])&#13;
           cv.waitKey(50) &#13;
   &#13;
   cv.destroyAllWindows()&#13;
   &#13;
<span class="ent">➍</span> recognizer = cv.face.LBPHFaceRecognizer_create()&#13;
   recognizer.train(images, np.array(labels))&#13;
   recognizer.write('lbph_trainer.yml')&#13;
   print("Training complete. Exiting...")</pre>&#13;
<p class="listing"><a id="ch010list3"/>Listing 10-3: Training and saving the LBPH face recognizer</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_237"/>You’ve seen the imports and the face detector code before. Although you’ve already cropped the training images to face rectangles in <em>1_capture.py</em>, it doesn’t hurt to repeat this procedure. Since <em>2_train.py</em> is a stand-alone program, it’s best not to take anything for granted.</p>&#13;
<p class="indent">Next, you must choose which set of training images to use: the ones you captured yourself in the <em>trainer</em> folder or the set provided in the <em>demming_trainer</em> folder <span class="ent">➊</span>. Comment out or delete the line for the one you don’t use. Remember, because you’re not providing a full path to the folder, you’ll need to launch your program from the folder containing it, which should be one level above the <em>trainer</em> and <em>demming_trainer</em> folders.</p>&#13;
<p class="indent">Create a list named <span class="literal">image_paths</span> using list comprehension. This will hold the directory path and filename for each image in the training folder. Then create empty lists for the images and their labels.</p>&#13;
<p class="indent">Start a <span class="literal">for</span> loop through the image paths. Read the image in grayscale; then extract its numeric label from the filename and convert it to an integer <span class="ent">➋</span>. Remember that the label corresponds to the user ID input through <em>1_capture.py</em> right before it captured the video frames.</p>&#13;
<p class="indent">Let’s take a moment to unpack what’s happening in this extraction and conversion process. The <span class="literal">os.path.split()</span> method takes a directory path and returns a tuple of the directory path and the filename, as shown in the following snippet:</p>&#13;
<pre>&gt;&gt;&gt; import os&#13;
&gt;&gt;&gt; path = 'C:\demming_trainer\demming.1.5.jpg'&#13;
&gt;&gt;&gt; os.path.split(path)&#13;
('C:\\demming_trainer', 'demming.1.5.jpg')</pre>&#13;
<p class="indent">You can then select the last item in the tuple, using an index of <span class="literal">-1</span>, and split it on the dot. This yields a list with four items (the user’s name, user ID, frame number, and file extension).</p>&#13;
<pre>&gt;&gt;&gt; os.path.split(path)[-1].split('.')&#13;
['demming', '1', '5', 'jpg']</pre>&#13;
<p class="indent">To extract the label value, you choose the second item in the list using index <span class="literal">1</span>.</p>&#13;
<pre>&gt;&gt;&gt; os.path.split(path)[-1].split('.')[1]&#13;
'1'</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_238"/>Repeat this process to extract the <span class="literal">name</span> and <span class="literal">frame_num</span> for each image. These are all strings at this point, which is why you need to turn the user ID into an integer for use as a label.</p>&#13;
<p class="indent">Now, call the face detector on each training image <span class="ent">➌</span>. This will return a <span class="literal">numpy.ndarray</span>, which you’ll call <span class="literal">faces</span>. Start looping through the array, which contains the coordinates of the detected face rectangles. Append the part of the image in the rectangle to the <span class="literal">images</span> list you made earlier. Also append the image’s user ID to the <span class="literal">labels</span> list.</p>&#13;
<p class="indent">Let the user know what’s going on by printing a message in the shell. Then, as a check, show each training image for 50 milliseconds. If you’ve ever seen Peter Gabriel’s popular 1986 music video for “Sledgehammer,” you’ll appreciate this display.</p>&#13;
<p class="indent">It’s time to train the face recognizer. Just as you do when using OpenCV’s face detector, you start by instantiating a recognizer object <span class="ent">➍</span>. Next, you call the <span class="literal">train()</span> method and pass it the <span class="literal">images</span> list and the <span class="literal">labels</span> list, which you turn into a <span class="literal">NumPy</span> array on the fly.</p>&#13;
<p class="indent">You don’t want to train the recognizer every time someone verifies their face, so write the results of the training process to a file called <em>lbph_trainer.yml</em>. Then let the user know the program has ended.</p>&#13;
<h4 class="h4" id="ch00lev2sec54"><strong><em>The Face Predictor Code</em></strong></h4>&#13;
<p class="noindent">It’s time to start recognizing faces, a process we’ll call <em>predicting</em>, because it all comes down to probability. The program in <em>3_predict.py</em> will first calculate the concatenated LBP histogram for each face. It will then find the distance between this histogram and all the histograms in the training set. Next, it will assign the new face the label and name of the trained face that’s closest to it, but only if the distance falls below a threshold value that you specify.</p>&#13;
<h5 class="h5"><strong>Importing Modules and Preparing the Face Recognizer</strong></h5>&#13;
<p class="noindent"><a href="ch10.xhtml#ch010list4">Listing 10-4</a> imports modules, prepares a dictionary to hold user ID numbers and names, sets up the face detector and recognizer, and establishes the path to the test data. The test data includes images of Captain Demming, along with several other faces. An image of Captain Demming from the training folder is included to test the results. If everything is working as it should, the algorithm should positively identify this image with a low distance measurement.</p>&#13;
<pre><span class="codeitalic1">3_predict.py</span>, part 1&#13;
   import os&#13;
   from datetime import datetime&#13;
   import cv2 as cv&#13;
   &#13;
   names = {1: "Demming"}&#13;
   cascade_path = "C:/Python372/Lib/site-packages/cv2/data/"&#13;
   face_detector = cv.CascadeClassifier(cascade_path +&#13;
                                        'haarcascade_frontalface_default.xml')&#13;
   &#13;
<span class="ent">➊</span> recognizer = cv.face.LBPHFaceRecognizer_create()&#13;
   recognizer.read('lbph_trainer.yml')&#13;
&#13;
   #test_path = './tester'&#13;
<span class="ent">➋</span> test_path = './demming_tester'&#13;
   image_paths = [os.path.join(test_path, f) for f in os.listdir(test_path)]</pre>&#13;
<p class="listing"><a id="ch010list4"/>Listing 10-4: Importing modules and preparing for face detection and recognition</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_239"/>After some familiar imports, create a dictionary to link user ID numbers to usernames. Although there’s only one entry currently, this <span class="literal">name</span> dictionary makes it easy to add more entries in the future. If you’re using your own face, feel free to change the last name, but leave the ID number set to <span class="literal">1</span>.</p>&#13;
<p class="indent">Next, repeat the code that sets up the <span class="literal">face_detector</span> object. You’ll need to input your own <span class="literal">cascade_path</span> (see <a href="ch10.xhtml#ch010list1">Listing 10-1</a> on <a href="ch10.xhtml#page_233">page 233</a>).</p>&#13;
<p class="indent">Create a recognizer object as you did in the <em>2_train.py</em> code <span class="ent">➊</span>. Then use the <span class="literal">read()</span> method to load the <em>.yml</em> file that contains the training information.</p>&#13;
<p class="indent">You’ll want to test the recognizer using face images in a folder. If you’re using the Demming images provided, set up a path to the <em>demming_tester</em> folder <span class="ent">➋</span>. Otherwise, use the <em>tester</em> folder you created earlier. You can add your own images to this blank folder. If you’re using your face to represent Captain Demming’s, you shouldn’t reuse the training images here, although you might consider using one as a control. Instead, use the <em>1_capture.py</em> program to produce some new images. If you wear glasses, include some images of you with and without them. You’ll want to include some strangers from the <em>demming_tester</em> folder, as well.</p>&#13;
<h5 class="h5"><strong>Recognizing Faces and Updating an Access Log</strong></h5>&#13;
<p class="noindent"><a href="ch10.xhtml#ch010list5">Listing 10-5</a> loops through the images in the test folder, detects any faces present, compares the face histogram to those in the training file, names the face, assigns a confidence value, and then logs the name and access time in a persistent text file. As part of this process, the program would theoretically unlock the lab if the ID is positive, but since we don’t have a lab, we’ll skip that part.</p>&#13;
<pre><span class="codeitalic1">3_predict.py</span>, part 2&#13;
for image in image_paths:&#13;
    predict_image = cv.imread(image, cv.IMREAD_GRAYSCALE)&#13;
    faces = face_detector.detectMultiScale(predict_image,&#13;
                                          scaleFactor=1.05,&#13;
                                          minNeighbors=5)&#13;
    for (x, y, w, h) in faces:&#13;
        print(f"\nAccess requested at {datetime.now()}.")&#13;
 <span class="ent">➊</span> face = cv.resize(predict_image[y:y + h, x:x + w], (100, 100))   &#13;
    predicted_id, dist = recognizer.predict(face)&#13;
 <span class="ent">➋</span> if predicted_id == 1 and dist &lt;= 95:&#13;
        name = names[predicted_id]&#13;
        print("{} identified as {} with distance={}"&#13;
              .format(image, name, round(dist, 1)))&#13;
     <span class="ent">➌</span> print(f"Access granted to {name} at {datetime.now()}.",&#13;
              file=open('lab_access_log.txt', 'a'))&#13;
    else:&#13;
        name = 'unknown'&#13;
        print(f"{image} is {name}.")&#13;
&#13;
    cv.rectangle(predict_image, (x, y), (x + w, y + h), 255, 2)&#13;
    cv.putText(predict_image, name, (x + 1, y + h - 5),&#13;
               cv.FONT_HERSHEY_SIMPLEX, 0.5, 255, 1)        &#13;
    cv.imshow('ID', predict_image)        &#13;
    cv.waitKey(2000)&#13;
    cv.destroyAllWindows()</pre>&#13;
<p class="listing"><a id="ch010list5"/>Listing 10-5: Running face recognition and updating the access log file</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_240"/>Start by looping through the images in the test folder. This will be either the <em>demming_tester</em> folder or the <em>tester</em> folder. Read each image in as grayscale and assign the resulting array to a variable named <span class="literal">predict_image</span>. Then run the face detector on it.</p>&#13;
<p class="indent">Now loop through the face rectangles, as you’ve done before. Print a message about access being requested; then use OpenCV to resize the <span class="literal">face</span> subarray to 100×100 pixels <span class="ent">➊</span>. This is close to the dimensions of the training images in the <em>demming_trainer</em> folder. Synchronizing the size of the images isn’t strictly necessary but helps to improve results in my experience. If you’re using your own images to represent Captain Demming, you should check that the training image and test image dimensions are similar.</p>&#13;
<p class="indent">Now it’s time to predict the identity of the face. Doing so takes only one line. Just call the <span class="literal">predict()</span> method on the <span class="literal">recognizer</span> object and pass it the <span class="literal">face</span> subarray. This method will return an ID number and a distance value.</p>&#13;
<p class="indent">The lower the distance value, the more likely the predicted face has been correctly identified. You can use the distance value as a threshold: all images that are predicted to be Captain Demming and score <em>at or below</em> the threshold will be positively identified as Captain Demming. All the others will be assigned to <span class="literal">'unknown'</span>.</p>&#13;
<p class="indent">To apply the threshold, use an <span class="literal">if</span> statement <span class="ent">➋</span>. If you’re using your own training and test images, set the distance value to 1,000 the first time you run the program. Review the distance values for all the images in the test folder, both known and unknown. Find a threshold value below which all the faces are correctly identified as Captain Demming. This will be your discriminator going forward. For the images in the <em>demming_trainer</em> and <em>demming_tester</em> folders, the threshold distance should be 95.</p>&#13;
<p class="indent">Next, get the name for the image by using the <span class="literal">predicted_id</span> value as a key in the <span class="literal">names</span> dictionary. Print a message in the shell stating that the image has been identified and include the image filename, the name from the dictionary, and the distance value.</p>&#13;
<p class="indent">For the log, print a message indicating that <span class="literal">name</span> (in this case, Captain Demming) has been granted access to the lab and include the time using the <span class="literal">datetime</span> module <span class="ent">➌</span>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_241"/>You’ll want to keep a persistent file of people’s comings and goings. Here’s a neat trick for doing so: just write to a file using the <span class="literal">print()</span> function. Open the <em>lab_access_log.txt</em> file and include the <span class="literal">a</span> parameter for “append.” This way, instead of overwriting the file for each new image, you’ll add a new line at the bottom. Here’s an example of the file contents:</p>&#13;
<pre>Access granted to Demming at 2020-01-20 09:31:17.415802.&#13;
Access granted to Demming at 2020-01-20 09:31:19.556307.&#13;
Access granted to Demming at 2020-01-20 09:31:21.644038.&#13;
Access granted to Demming at 2020-01-20 09:31:23.691760.&#13;
<span class="codeitalic1">--snip--</span></pre>&#13;
<p class="indent">If the conditional is not met, set <span class="literal">name</span> to <span class="literal">'unknown'</span> and print a message to that effect. Then draw a rectangle around the face and post the user’s name using OpenCV’s <span class="literal">putText()</span> method. Show the image for two seconds before destroying it.</p>&#13;
<h4 class="h4" id="ch00lev2sec55"><strong><em>Results</em></strong></h4>&#13;
<p class="noindent">You can see some example results, from the 20 images in the <em>demming_tester</em> folder, in <a href="ch10.xhtml#ch010fig8">Figure 10-8</a>. The predictor code correctly identified the eight images of Captain Demming with no false positives.</p>&#13;
<div class="image"><img src="../images/fig10_08.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch010fig8"/>Figure 10-8: Demmings and non-Demmings</p>&#13;
<p class="indent">For the LBPH algorithm to be highly accurate, you need to use it under controlled conditions. Remember that by forcing the user to gain access through the laptop, you controlled their pose, the size of their face, the image resolution, and the lighting.</p>&#13;
<h3 class="h3" id="ch00lev1sec83"><strong>Summary</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_242"/>In this chapter, you got to work with OpenCV’s local binary pattern histogram algorithm for recognizing human faces. With only a few lines of code, you produced a robust face recognizer that can easily handle variable lighting conditions. You also used the Standard Library’s <span class="literal">os.path.split()</span> method to break apart directory paths and filenames to produce customized variable names.</p>&#13;
<h3 class="h3" id="ch00lev1sec84"><strong>Further Reading</strong></h3>&#13;
<p class="noindent">“Local Binary Patterns Applied to Face Detection and Recognition” (Polytechnic University of Catalonia, 2010), by Laura María Sánchez López, is a thorough review of the LBPH approach. The PDF can be found online at sites such as <em><a href="https://www.semanticscholar.org/">https://www.semanticscholar.org/</a></em>.</p>&#13;
<p class="indent">“Look at the LBP Histogram,” on the AURlabCVsimulator site (<em><a href="https://aurlabcvsimulator.readthedocs.io/en/latest/">https://aurlabcvsimulator.readthedocs.io/en/latest/</a></em>), includes Python code that lets you visualize an LBPH image.</p>&#13;
<p class="indent">If you’re a macOS or Linux user, be sure to check out Adam Geitgey’s <span class="literal">face_recognition</span> library, a simple-to-use and highly accurate face recognition system that utilizes deep learning. You can find installation instructions and an overview at the Python Software Foundation site: <em><a href="https://pypi.org/project/face_recognition/">https://pypi.org/project/face_recognition/</a></em>.</p>&#13;
<p class="indent">“Machine Learning Is Fun! Part 4: Modern Face Recognition with Deep Learning” (Medium, 2016), by Adam Geitgey, is a short and enjoyable overview of modern face recognition using Python, OpenFace, and <span class="literal">dlib</span>.</p>&#13;
<p class="indent">“Liveness Detection with OpenCV” (PyImageSearch, 2019), by Adrian Rosebrock, is an online tutorial that teaches you how to protect your face recognition system against spoofing by fake faces, such as a photograph of Captain Demming held up to the webcam.</p>&#13;
<p class="indent">Cities and colleges around the world have begun banning facial recognition systems. Inventors have also gotten into the act, designing clothing that can confound the systems and protect your identity. “These Clothes Use Outlandish Designs to Trick Facial Recognition Software into Thinking You’re Not Human” (Business Insider, 2020), by Aaron Holmes, and “How to Hack Your Face to Dodge the Rise of Facial Recognition Tech” (Wired, 2019), by Elise Thomas, review some recent practical—and impractical— solutions to the problem.</p>&#13;
<p class="indent">“OpenCV Age Detection with Deep Learning” (PyImageSearch, 2020) by Adrian Rosebrock, is an online tutorial for using OpenCV to predict a person’s age from their photograph.</p>&#13;
<h3 class="h3" id="ch00lev1sec85"><strong>Challenge Project: Adding a Password and Video Capture</strong></h3>&#13;
<p class="noindent">The <em>3_predict.py</em> program you wrote in Project 14 loops through a folder of photographs to perform face recognition. Rewrite the program so that it <span epub:type="pagebreak" id="page_243"/>dynamically recognizes faces in the webcam’s video stream. The face rectangle and name should appear in the video frame as they do on the folder images.</p>&#13;
<p class="indent">To start the program, have the user enter a password that you verify. If it’s correct, add audio instructions telling the user to look at the camera. If the program positively identifies Captain Demming, use audio to announce that access is granted. Otherwise, play an audio message stating that access is denied.</p>&#13;
<p class="indent">If you need help with identifying the face from the video stream, see the <em>challenge_video_recognize.py</em> program in the appendix. Note that you may need to use a higher confidence value for the video frame than the value you used for the still photographs.</p>&#13;
<p class="indent">So that you can keep track of who has tried to enter the lab, save a single frame to the same folder as the <em>lab_access_log.txt</em> file. Use the logged results from <span class="literal">datetime.now()</span> as the filename so you can match the face to the access attempt. Note that you’ll need to reformat the string returned from <span class="literal">datetime.now()</span> so that it only contains characters acceptable for filenames, as defined by your operating system.</p>&#13;
<h3 class="h3" id="ch00lev1sec86"><strong>Challenge Project: Look-Alikes and Twins</strong></h3>&#13;
<p class="noindent">Use the code from Project 14 to compare celebrity look-alikes and twins. Train it with images from the internet and see whether you can fool the LBPH algorithm. Some pairings to consider are Scarlett Johansson and Amber Heard, Emma Watson and Kiernan Shipka, Liam Hemsworth and Karen Khachanov, Rob Lowe and Ian Somerhalder, Hilary Duff and Victoria Pedretti, Bryce Dallas Howard and Jessica Chastain, and Will Ferrell and Chad Smith.</p>&#13;
<p class="indent">For famous twins, look at astronaut twins Mark and Scott Kelly and celebrity twins Mary-Kate and Ashley Olsen.</p>&#13;
<h3 class="h3" id="ch00lev1sec87"><strong>Challenge Project: Time Machine</strong></h3>&#13;
<p class="noindent">If you ever watch reruns of old shows, you’ll encounter famous actors in their younger—sometimes <em>much</em> younger—days. Even though humans excel at face recognition, we may still struggle to identify a young Ian McKellen or Patrick Stewart. That’s why sometimes it takes a certain inflection of voice or curious mannerism to send us scurrying to Google to check the cast members.</p>&#13;
<p class="indent">Face recognition algorithms are also prone to fail when identifying faces across time. To see how the LBPH algorithm performs under these conditions, use the code from Project 14 and train it on faces of yourself (or your relatives) at a certain age. Then test it with images over a range of ages.</p>&#13;
</body></html>