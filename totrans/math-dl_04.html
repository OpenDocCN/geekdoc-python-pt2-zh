<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch04"><span epub:type="pagebreak" id="page_67"/><strong><span class="big">4</span><br/>STATISTICS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">Bad datasets lead to bad models. We’d like to understand our data before we build a model, and then use that understanding to create a useful dataset, one that leads to models that do what we expect them to do. Knowing basic statistics will enable us to do just that.</p>&#13;
<p class="indent">A <em>statistic</em> is any number that’s calculated from a sample and used to characterize it in some way. In deep learning, when we talk about samples, we’re usually talking about datasets. Maybe the most basic statistic is the arithmetic mean, commonly known as the average. The mean of a dataset is a single-number summary of the dataset.</p>&#13;
<p class="indent">We’ll see many different statistics in this chapter. We’ll begin by learning about the types of data and characterizing a dataset with summary statistics. Next, we’ll learn about quantiles and plotting data to understand what it contains. After that comes a discussion of outliers and missing data. Datasets are seldom perfect, so we need to have some way of detecting bad data and dealing with missing data. We’ll follow our discussion of imperfect datasets with a discussion of the correlation between variables. Then we’ll close the chapter out by discussing hypothesis testing, where we attempt to answer questions like “How likely is it that the same parent process generated two datasets?” Hypothesis testing is widely used in science, including deep learning.</p>&#13;
<h3 class="h3" id="ch04lev1_1"><span epub:type="pagebreak" id="page_68"/>Types of Data</h3>&#13;
<p class="noindent">The four types of data are nominal, ordinal, interval, and ratio. Let’s look at each in turn.</p>&#13;
<h4 class="h4" id="ch04lev2_1">Nominal Data</h4>&#13;
<p class="noindent"><em>Nominal data</em>, sometimes called <em>categorical data</em>, is data that has no ordering between the different values. An example of this is eye color; there is no relationship between brown, blue, and green.</p>&#13;
<h4 class="h4" id="ch04lev2_2">Ordinal Data</h4>&#13;
<p class="noindent">For <em>ordinal data</em>, the data has a ranking or order, though differences aren’t meaningful in a mathematical sense. For example, if a questionnaire asks you to select from “strongly disagree,” “disagree,” “neutral,” “agree,” and “strongly agree,” it’s pretty clear that there is an order. Still, it’s also clear that “agree” isn’t three more than “strongly disagree.” All we can say is that “strongly disagree” is to the left of “agree” (and “neutral” and “disagree”).</p>&#13;
<p class="indent">Another example of ordinal data is education level. If one person has a fourth-grade education and another has an eighth-grade education, we can say that the latter person is more educated than the former, but we can’t say that the latter person is twice as educated, because “twice as educated” has no fixed meaning.</p>&#13;
<h4 class="h4" id="ch04lev2_3">Interval Data</h4>&#13;
<p class="noindent"><em>Interval data</em> has meaningful differences. For example, if one cup of water is at 40 degrees Fahrenheit and another is at 80 degrees Fahrenheit, we can say that there is a 40-degree difference between the two cups of water. We can’t, however, say that there is twice as much heat in the second cup, because the zero for the Fahrenheit scale is arbitrary. Colloquially, we do say it’s twice as hot, but in reality, it isn’t. To see this, think about what happens if we change the temperature scale to another scale with an arbitrary, though more sensible, zero: the Celsius scale. We see that the first cup is at about 4.4 degrees Celsius, and the second is at 26.7 degrees Celsius. Clearly, the second cup doesn’t suddenly now have six times the heat of the first.</p>&#13;
<h4 class="h4" id="ch04lev2_4">Ratio Data</h4>&#13;
<p class="noindent">Finally, <em>ratio data</em> is data where differences are meaningful, and there is a true zero point. Height is a ratio value because a height of zero is just that—no height at all. Similarly, age is also a ratio value because an age of zero means no age at all. If we were to adopt a new age scale and call a person zero when they reach, say, voting age, we’d then have an interval scale, not a ratio scale.</p>&#13;
<p class="indent">Let’s look at temperature again. We said above that temperature is an interval quantity. This isn’t always the case. If we measure temperature in <span epub:type="pagebreak" id="page_69"/>Fahrenheit or Celsius, then, yes, it is an interval quantity. However, if we measure temperature in Kelvin, the absolute temperature scale, then it becomes a ratio value. Why? Because a temperature of 0 Kelvin (or <em>K</em>) is just that, no temperature at all. If our first cup is at 40°F, 277.59 K, and the second is at 80°F, 299.82 K, then we can truthfully say that the second cup is 1.08 times hotter than the first, since (277.59)(1.08) ≈ 299.8.</p>&#13;
<p class="indent"><a href="ch04.xhtml#ch04fig01">Figure 4-1</a> names the scales and shows their relationships to each other.</p>&#13;
<div class="image" id="ch04fig01"><img src="Images/04fig01.jpg" alt="image" width="678" height="214"/></div>&#13;
<p class="figcap"><em>Figure 4-1: The four types of data</em></p>&#13;
<p class="indent">Each step in <a href="ch04.xhtml#ch04fig01">Figure 4-1</a> from left to right adds something to the data that the type of data on the left is lacking. For nominal to ordinal, we add ordering. For ordinal to interval, we add meaningful differences. Lastly, moving from interval to ratio adds a true zero point.</p>&#13;
<p class="indent">In practical use, as far as statistics are concerned, we should be aware of the types of data so we don’t do something meaningless. If we have a questionnaire, and the mean value of question A on a 1-to-5 rating scale is 2, while for question B it’s 4, we can’t say that B is rated twice as high as A, only that B was rated higher than A. What “twice” means in this context is unclear and quite probably meaningless.</p>&#13;
<p class="indent">Interval and ratio data may be continuous (floating-points) or discrete (integers). From a deep learning perspective, models typically treat continuous and discrete data the same way, and we don’t need to do anything special for discrete data.</p>&#13;
<h4 class="h4" id="ch04lev2_5">Using Nominal Data in Deep Learning</h4>&#13;
<p class="noindent">If we have a nominal value, say a set of colors, such as red, green, and blue, and we want to pass that value into a deep network, we need to change the data before we can use it. As we just saw, nominal data has no order, so while it’s tempting to assign a value of 1 to red, 2 to green, and 3 to blue, it would be wrong to do so, since the network will interpret those numbers as interval data. In that case, to the network, blue = 3(red), which is of course nonsense. If we want to use nominal data with a deep network, we need to alter it so that the interval is meaningful. We do this with <em>one-hot encoding</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_70"/>In one-hot encoding, we turn the single nominal variable into a vector, where each element of the vector corresponds to one of the nominal values. For the color example, the one nominal variable becomes a three-element vector with one element representing red, another green, and the last blue. Then, we set the value corresponding to the color to one and all the others to zero, like so:</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Value</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"/></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Vector</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">red</p></td>&#13;
<td style="vertical-align: top"><p class="tab">→</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1 0 0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">green</p></td>&#13;
<td style="vertical-align: top"><p class="tab">→</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0 1 0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">blue</p></td>&#13;
<td style="vertical-align: top"><p class="tab">→</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0 0 1</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">Now the vector values are meaningful because either it’s red (1) or it’s not (0), green (1) or it’s not (0), or blue (1) or it’s not (0). The interval between zero and one has mathematical meaning because the presence of the value, say red, is genuinely greater than its absence, and that works the same way for each color. The values are now interval, so the network can use them. In some toolkits, like Keras, class labels are one-hot encoded before passing them to the model. This is done so vector output operates nicely with the one-hot encoded class label when computing the loss function.</p>&#13;
<h3 class="h3" id="ch04lev1_2">Summary Statistics</h3>&#13;
<p class="noindent">We’re given a dataset. How do we make sense of it? How should we characterize it to understand it better before we use it to build a model?</p>&#13;
<p class="indent">To answer these questions, we need to learn about <em>summary statistics</em>. Calculating summary statistics should be the first thing you do when handed a new dataset. Not looking at your dataset before building a model is like buying a used car without checking the tires, taking it for a test drive, and looking under the hood.</p>&#13;
<p class="indent">People have different notions of what makes a good set of summary statistics. We’ll focus on the following: means; the median; and measures of variation, including variance, standard deviation, and standard error. The range and mode are also often mentioned. The <em>range</em> is the difference between the maximum and minimum of the dataset. The <em>mode</em> is the most frequent value in the dataset. We generally get a sense of the mode visually from the histogram, as the histogram shows us the shape of the distribution of the data.</p>&#13;
<h4 class="h4" id="ch04lev2_6">Means and Median</h4>&#13;
<p class="noindent">Most of us learned how to calculate the average of a set of numbers in elementary school: add the numbers and divide by how many there are. This is the <em>arithmetic mean</em>, or, more specifically, the <em>unweighted</em> arithmetic mean. If the dataset consists of a set of values, <em>{x<sub>0</sub></em>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x<sub>n–1</sub></em>}, then the arithmetic mean is the sum of the data divided by the number of elements in the dataset (<em>n</em>). Notationally, we write this as the following.</p>&#13;
<span epub:type="pagebreak" id="page_71"/>&#13;
<div class="imagec" id="ch04equ01"><img src="Images/04equ01.jpg" alt="image" width="399" height="57"/></div>&#13;
<p class="indent">The <span class="middle"><img src="Images/xbar.jpg" alt="image" width="10" height="15"/></span> is the typical way to denote the mean of a sample.</p>&#13;
<p class="indent"><a href="ch04.xhtml#ch04equ01">Equation 4.1</a> calculates the unweighted mean. Each value is given a weight of 1/<em>n</em>, where the sum of all the weights is 1.0. Sometimes, we might want to weight elements of the dataset differently; in other words, not all of them should count equally. In that case, we calculate a weighted mean,</p>&#13;
<div class="imagec"><img src="Images/071equ01.jpg" alt="image" width="103" height="57"/></div>&#13;
<p class="indent">where <em>w<sub>i</sub></em> is the weight given to <em>x<sub>i</sub></em> and Σ<em><sub>i</sub>w<sub>i</sub></em> = 1. The weights are not part of the dataset; they need to come from somewhere else. The grade point average (GPA) used by many universities is an example of a weighted mean. The grade for each course is multiplied by the number of course credits, and the sum is divided by the total number of credits. Algebraically, this is equivalent to multiplying each grade by a weight, <em>w<sub>i</sub></em> = <em>c<sub>i</sub></em>/Σ<em><sub>i</sub></em><em>c<sub>i</sub></em>, with <em>c<sub>i</sub></em> the number of credits for course <em>i</em> and Σ<em><sub>i</sub></em><em>c<sub>i</sub></em> the total number of credits for the semester.</p>&#13;
<h5 class="h5">Geometric Mean</h5>&#13;
<p class="noindent">The arithmetic mean is by far the most commonly used mean. However, there are others. The <em>geometric mean</em> of two positive numbers, <em>a</em> and <em>b</em>, is the square root of their product:</p>&#13;
<div class="imagec"><img src="Images/071equ02.jpg" alt="image" width="82" height="29"/></div>&#13;
<p class="noindent">In general, the geometric mean of <em>n</em> positive numbers is the <em>n</em>th root of their product:</p>&#13;
<div class="imagec"><img src="Images/071equ03.jpg" alt="image" width="178" height="24"/></div>&#13;
<p class="indent">The geometric mean is used in finance to calculate average growth rates. In image processing, the geometric mean can be used as a filter to help reduce image noise. In deep learning, the geometric mean appears in the <em>Matthews correlation coefficient (MCC)</em>, one of the metrics we use to evaluate deep learning models. The MCC is the geometric mean of two other metrics, the informedness and the markedness.</p>&#13;
<h5 class="h5">Harmonic Mean</h5>&#13;
<p class="noindent">The <em>harmonic mean</em> of two numbers, <em>a</em> and <em>b</em>, is the reciprocal of the arithmetic mean of their reciprocals:</p>&#13;
<div class="imagec"><img src="Images/071equ04.jpg" alt="image" width="195" height="51"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_72"/>In general,</p>&#13;
<div class="imagec"><img src="Images/072equ01.jpg" alt="image" width="371" height="70"/></div>&#13;
<p class="indent">The harmonic mean shows up in deep learning as the F1 score. This is a frequently used metric for evaluating classifiers. The F1 score is the harmonic mean of the recall (sensitivity) and the precision:</p>&#13;
<div class="imagec"><img src="Images/072equ02.jpg" alt="image" width="317" height="144"/></div>&#13;
<p class="indent">Despite its frequent use, it’s not a good idea to use the F1 score to evaluate a deep learning model. To see this, consider the definitions of recall and precision:</p>&#13;
<div class="imagec"><img src="Images/072equ03.jpg" alt="image" width="194" height="128"/></div>&#13;
<p class="noindent">Here, TP is the number of true positives, FN is the number of false negatives, and FP is the number of false positives. These values come from the test set used to evaluate the model. A fourth number that’s important for classifiers, TN, is the number of correctly classified true negatives (assuming a binary classifier). The F1 score ignores TN, but to understand how well the model performs, we need to consider both positive and negative classifications. Therefore, the F1 score is misleading and often too optimistic. Better metrics are the MCC mentioned above or Cohen’s κ (kappa), which is similar to MCC and usually tracks it closely.</p>&#13;
<h5 class="h5">Median</h5>&#13;
<p class="noindent">Before moving on to measures of variation, there’s one more commonly used summary statistic we’ll mention here. It’ll show up again a little later in the chapter too. The <em>median</em> of a dataset is the middle value. It’s the value where, when the dataset is sorted numerically, half the values are below it and half are above it. Let’s use this dataset:</p>&#13;
<p class="center"><em>X</em> = {55,63,65,37,74,71,73,87,69,44}</p>&#13;
<p class="noindent">If we sort <em>X</em>, we get</p>&#13;
<p class="center">{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_73"/>We immediately see a potential problem. I said we need the middle value when the data is sorted. With 10 things in <em>X</em>, there is no middle value. The middle lies between 65 and 69. When the number of elements in the data-set is even, the median is the arithmetic mean of the two middle numbers. Therefore, the median in this case is</p>&#13;
<div class="imagec"><img src="Images/073equ01.jpg" alt="image" width="207" height="43"/></div>&#13;
<p class="indent">The arithmetic mean of the data is 63.8. What’s the difference between the mean and the median?</p>&#13;
<p class="indent">By design, the median tells us the value that splits the dataset, so the number of samples above equals the number below. It’s the number of samples that matters. For the mean, it’s a sum over the actual data values. Therefore, the mean is sensitive to the values themselves, while the median is sensitive to the ordering of the values.</p>&#13;
<p class="indent">If we look at <em>X</em>, we see that most values are in the 60s and 70s, with one low value of 37. It’s the low value of 37 that drags the mean down relative to the median. An excellent example of this effect is income. The current median annual family income in the United States is about $62,000. A recent measure of the mean family income in the United States is closer to $72,000. The difference is because of the small portion of the population who make significantly more money than everyone else. They pull the overall mean up. For income, then, the most meaningful statistic is the median.</p>&#13;
<p class="indent">Consider <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>.</p>&#13;
<div class="image" id="ch04fig02"><img src="Images/04fig02.jpg" alt="image" width="680" height="507"/></div>&#13;
<p class="figcap"><em>Figure 4-2: The mean (solid) and median (dashed) plotted over the histogram of a sample dataset</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_74"/><a href="ch04.xhtml#ch04fig02">Figure 4-2</a> shows the histogram generated from 1,000 samples of a simulated dataset. Also plotted are the mean (solid line) and median (dashed line). The two do not match; the long tail in the histogram drags the mean up. If we were to count, 500 samples would fall in the bins below the dashed line and 500 in the bins above.</p>&#13;
<p class="indent">Are there times when the mean and median are the same? Yes. If the data distribution is completely symmetric, then the mean and median will be the same. The classic example of this situation is the normal distribution. <a href="ch03.xhtml#ch03fig04">Figure 3-4</a> showed a normal distribution where the left-right symmetry was clear. The normal distribution is special. We’ll see it again throughout the chapter. For now, remember that the closer the distribution of the dataset is to a normal distribution, the closer the mean and median will be.</p>&#13;
<p class="indent">The opposite is also worth remembering: if the dataset’s distribution is far from normal, like in <a href="ch04.xhtml#ch04fig02">Figure 4-2</a>, then the median is likely the better statistic to consider when summarizing the data.</p>&#13;
<h4 class="h4" id="ch04lev2_7">Measures of Variation</h4>&#13;
<p class="noindent">A beginning archer shoots 10 arrows at a target. Eight of the beginner’s arrows hit the target, two miss completely, and the eight that do hit the target are spread uniformly across it. An expert archer shoots 10 arrows at a target. All of the expert’s arrows hit within a few centimeters of the center. Think about the mean position of the arrows. For the expert, all of the arrows are near the center of the target, so we can see that the mean position of the arrows will be near the center. For the beginner, none of the arrows are near the center of the target, but they are scattered more or less equally to the left and right or above and below the center. Because of this, the average position will balance out and be near the center of the target as well.</p>&#13;
<p class="indent">However, the first archer’s arrows are scattered; their location varies greatly. The second archer’s arrows, on the other hand, are tightly clustered, and there is little variation in their position. One meaningful way to summarize and understand a dataset is to quantify its variation. Let’s see how we might do this.</p>&#13;
<h5 class="h5">Deviation vs. Variance</h5>&#13;
<p class="noindent">One way we might measure the variation of a dataset is to find the <em>range</em>, the difference between the largest and smallest values. However, the range is a crude measurement, as it pays no attention to most of the values in the dataset, only the extremes. We can do better by calculating the mean of the difference between the data values and the mean of the data. The formula is</p>&#13;
<div class="imagec" id="ch04equ02"><img src="Images/04equ02.jpg" alt="image" width="439" height="58"/></div>&#13;
<p class="indent"><a href="ch04.xhtml#ch04equ02">Equation 4.2</a> is the <em>mean deviation</em>. It’s a natural measure and gives just what we want: an idea of how far, on average, each sample is from the mean. While there’s nothing wrong with calculating the mean deviation, you’ll find <span epub:type="pagebreak" id="page_75"/>that it’s rarely used in practice. One reason has to do with algebra and calculus. The absolute value is annoying to deal with mathematically.</p>&#13;
<p class="indent">Instead of the natural measure of variation, let’s calculate this one using squared differences:</p>&#13;
<div class="imagec" id="ch04equ03"><img src="Images/04equ03.jpg" alt="image" width="431" height="58"/></div>&#13;
<p class="indent"><a href="ch04.xhtml#ch04equ03">Equation 4.3</a> is known as the <em>biased sample variance</em>. It’s the mean of the squared difference between each value in the dataset and the mean. It’s an alternate way of characterizing the scatter in the dataset. Why it’s biased, we’ll discuss in a second. We’ll get into why it’s <span class="middle"><img src="Images/ssub-bar.jpg" alt="image" width="16" height="24"/></span> and not <em>s<sub>n</sub></em> shortly after that.</p>&#13;
<p class="indent">Before we do, it’s worth noting that you’ll often see a slightly different equation:</p>&#13;
<div class="imagec" id="ch04equ04"><img src="Images/04equ04.jpg" alt="image" width="449" height="57"/></div>&#13;
<p class="noindent">This equation is the <em>unbiased sample variance</em>. Using <em>n</em> – 1 in place of <em>n</em> is known as Bessel’s correction. It’s related to the number of degrees of freedom in the residuals, where the residuals are what’s left when the mean is subtracted from each of the values in the dataset. The sum of the residuals is zero, so if there are <em>n</em> values in the dataset, knowing <em>n</em> – 1 of the residuals allows the last residual to be calculated. This gives us the degrees of freedom for the residuals. We are “free” to calculate <em>n</em> – 1 of them knowing that we’ll get the last one from the fact that the residuals sum to zero. Dividing by <em>n</em>–1 gives a less biased estimate of the variance, assuming <span class="middle"><img src="Images/ssub-bar.jpg" alt="image" width="16" height="24"/></span> is biased in some way to begin with.</p>&#13;
<p class="indent">Why are we talking about biased variance and unbiased variance? Biased how? We should always remember that a dataset is a sample from some parent data-generating process, the population. The true population variance (σ<sup>2</sup>) is the scatter of the population around the true population mean (μ). However, we don’t know μ or σ<sup>2</sup>, so instead, we estimate them from the dataset we do have. The mean of the sample is <span class="middle"><img src="Images/xbar.jpg" alt="image" width="10" height="15"/></span>. That’s our estimate for μ. It’s then natural to calculate the mean of the squared deviations around <span class="middle"><img src="Images/xbar.jpg" alt="image" width="10" height="15"/></span> and call that our estimate for σ<sup>2</sup>. That’s <span class="middle"><img src="Images/ssub-bar.jpg" alt="image" width="16" height="24"/></span> (<a href="ch04.xhtml#ch04equ03">Equation 4.3</a>). The claim, which is true but beyond our scope to demonstrate, is that <span class="middle"><img src="Images/ssub-bar.jpg" alt="image" width="16" height="24"/></span> is biased and not the best estimate of σ<sup>2</sup>, but if Bessel’s correction is applied, we’ll have a better estimate of the population variance. So we should use <em>s</em><sup>2</sup> (<a href="ch04.xhtml#ch04equ04">Equation 4.4</a>) to characterize the variance of the dataset around the mean.</p>&#13;
<p class="indent">In summary, we should use <span class="middle"><img src="Images/xbar.jpg" alt="image" width="10" height="15"/></span> and <em>s</em><sup>2</sup> to quantify the variance of the data-set. Now, why is it <em>s</em><sup>2</sup>? The square root of the variance is the <em>standard deviation</em> denoted as σ for the population and <em>s</em> for the estimate of σ calculated from the dataset. Most often, we want to work with the standard deviation. Writing square roots becomes tiresome, so convention has adopted the σ or <em>s</em> notation for the standard deviation and uses the squared form when discussing the variance.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_76"/>And, because life isn’t already ambiguous enough, you’ll often see σ used for <em>s</em>, and <a href="ch04.xhtml#ch04equ03">Equation 4.3</a> used when it really should be <a href="ch04.xhtml#ch04equ04">Equation 4.4</a>. Some toolkits, including our beloved NumPy, make it easy to use the wrong formula.</p>&#13;
<p class="indent">However, as the number of samples in our dataset increases, the difference between the biased and unbiased variance decreases because dividing by <em>n</em> or <em>n</em> – 1 matters less and less. A few lines of code illustrate this:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> import numpy as np</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> n = 10</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> a = np.random.random(n)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> (1/n)*((a-a.mean())**2).sum()</span><br/>&#13;
0.08081748204006689<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> (1/(n-1))*((a-a.mean())**2).sum()</span><br/>&#13;
0.08979720226674098</pre>&#13;
<p class="noindent">Here, a sample with only 10 values (<code>a</code>) shows a difference in the biased and unbiased variance in the third decimal. If we increase our dataset size from 10 to 10,000, we get</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> n = 10000</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> a = np.random.random(n)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> (1/n)*((a-a.mean())**2).sum()</span><br/>&#13;
0.08304350577482553<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> (1/(n-1))*((a-a.mean())**2).sum()</span><br/>&#13;
0.08305181095592111</pre>&#13;
<p class="noindent">The difference between the biased and unbiased estimate of the variance is now in the fifth decimal. Therefore, for the large datasets we typically work with in deep learning, it matters little in practice whether we use <em>s<sub>n</sub></em> or <em>s</em> for the standard deviation.</p>&#13;
<div class="sidebar">&#13;
<p class="sb-title"><strong>MEDIAN ABSOLUTE DEVIATION</strong></p>&#13;
<p class="sb-noindent">The standard deviation is based on the mean. The mean, as we saw above, is sensitive to extreme values, and the standard deviation is doubly so because we square the deviation from the mean for each sample. A measure of variability that is insensitive to extreme values in the dataset is the <em>median absolute deviation (MAD)</em>. The MAD is defined as the median of the absolute values of the difference between the data and the median:</p>&#13;
<p class="center">MAD = median(|<em>X<sub>i</sub></em> – median(<em>X</em>)|)</p>&#13;
<p class="sb-noindent"><span epub:type="pagebreak" id="page_77"/>Procedurally, first calculate the median of the data, then subtract it from each data value, making the result positive, and report the median of that set. The implementation is straightforward:</p>&#13;
<pre>&#13;
def MAD(x):<br/>&#13;
    return np.median(np.abs(x-np.median(x)))</pre>&#13;
<p class="sb-noindent">The MAD is not often used, but its insensitivity to extreme values in the dataset argues toward more frequent use, especially for outlier detection.</p>&#13;
</div>&#13;
<h5 class="h5">Standard Error vs. Standard Deviation</h5>&#13;
<p class="noindent">We have one more measure of variance to discuss: the <em>standard error of the mean (SEM)</em>. The SEM is often simply called the <em>standard error (SE)</em>. We need to go back to the population to understand what the SE is and when to use it. If we select a sample from the population, a dataset, we can calculate the mean of the sample, <span class="middle"><img src="Images/xbar.jpg" alt="image" width="10" height="15"/></span>. If we choose repeated samples and calculate those sample means, we’ll generate a dataset of means of the samples from the population. This might sound familiar; it’s the process we used to illustrate the central limit theorem in <a href="ch03.xhtml#ch03">Chapter 3</a>. The standard deviation of the set of means is the standard error.</p>&#13;
<p class="indent">The formula for the standard error from the standard deviation is straightforward,</p>&#13;
<div class="imagec"><img src="Images/077equ01.jpg" alt="image" width="83" height="43"/></div>&#13;
<p class="noindent">and is nothing more than a scaling of the sample standard deviation by the square root of the number of samples.</p>&#13;
<p class="indent">When should we use the standard deviation, and when should we use the standard error? Use the standard deviation to learn about the distribution of the samples around the mean. Use the standard error to say something about how good an estimate of the population mean a sample mean is. In a sense, the standard error is related to both the central limit theorem, as that affects the standard deviation of the means of multiple samples from the parent population, and the law of large numbers, since a larger dataset is more likely to give a better estimate of the population mean.</p>&#13;
<p class="indent">From a deep learning point of view, we might use the standard deviation to describe the dataset used to train a model. If we train and test several models, remembering the stochastic nature of deep network initialization, we can calculate a mean over the models for some metric, say the accuracy. In that case, we might want to report the mean accuracy plus or minus the <span epub:type="pagebreak" id="page_78"/>standard error. As we train more models and gain confidence that the mean accuracy represents the sort of accuracy the model architecture can provide, we should expect that the error in the mean accuracy over the models will decrease.</p>&#13;
<p class="indent">To recap, in this section, we discussed different summary statistics, values we can use to start to understand a dataset. These include the various means (arithmetic, geometric, and harmonic), the median, the standard deviation, and, when appropriate, the standard error. For now, let’s see how we can use plots to help understand a dataset.</p>&#13;
<h3 class="h3" id="ch04lev1_3">Quantiles and Box Plots</h3>&#13;
<p class="noindent">To calculate the median, we need to find the middle value, the number splitting the dataset into two halves. Mathematically, we say that the median divides the dataset into two quantiles.</p>&#13;
<p class="indent">A <em>quantile</em> splits the dataset into fixed-sized groups where the fixed size is the number of data values in the quantile. Since the median splits the dataset into two equally sized groups, it’s a <em>2-quantile</em>. Sometimes you’ll see the median referred to as the <em>50th percentile</em>, meaning 50 percent of the data values are less than this value. By similar reasoning, then, the 95th percentile is the value that 95 percent of the dataset is less than. Researchers often calculate 4-quantiles and refer to them as <em>quartiles</em>, since they split the dataset into four groups such that 25 percent of the data values are in the first quartile, 50 percent are in the first and second, and 75 percent are in the first, second, and third, with the final 25 percent in the fourth quartile.</p>&#13;
<p class="indent">Let’s work through an example to understand what we mean by quantiles. The example uses a synthetic exam dataset representing 1,000 test scores. See the file <em>exams.npy</em>. We’ll use NumPy to calculate the quartile values for us and then plot a histogram of the dataset with the quartile values marked. First, let’s calculate the quartile positions:</p>&#13;
<pre>&#13;
d = np.load("exams.npy")<br/>&#13;
p = d[:,0].astype("uint32")<br/>&#13;
q = np.quantile(p, [0.0, 0.25, 0.5, 0.75, 1.0])<br/>&#13;
<br/>&#13;
print("Quartiles: ", q)<br/>&#13;
print("Counts by quartile:")<br/>&#13;
print("    %d" % ((q[0] &lt;= p) &amp; (p &lt; q[1])).sum())<br/>&#13;
print("    %d" % ((q[1] &lt;= p) &amp; (p &lt; q[2])).sum())<br/>&#13;
print("    %d" % ((q[2] &lt;= p) &amp; (p &lt; q[3])).sum())<br/>&#13;
print("    %d" % ((q[3] &lt;= p) &amp; (p &lt; q[4])).sum())</pre>&#13;
<p class="noindent">This code, along with code to generate the plot, is in the file <em>quantiles.py</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_79"/>First we load the synthetic exam data and keep the first exam scores (<code>p</code>). Note, we make <code>p</code> an integer array so we can use <code>np.bincount</code> later to make the histogram. (That code is not shown above.) We then use NumPy’s <code>np.quantile</code> function to calculate the quartile values. This function takes the source array and an array of quantile values in the range [0, 1]. The values are fractions of the distance from the minimum value of the array to its maximum. So, asking for the 0.5 quantile is asking for the value that is half the distance between the minimum of <code>p</code> and its maximum such that the number of values in each set is equal.</p>&#13;
<p class="indent">To get quartiles, we ask for the 0.25, 0.5, and 0.75 quantiles to get the values such that 25 percent, 50 percent, and 75 percent of the elements of <code>p</code> are less than the values. We also ask for the 0.0 and 1.0 quantiles, the minimum and maximum of <code>p</code>. We do this for convenience when we count the number of elements in each range. Note, we could have instead used the <code>np.percentile</code> function. It returns the same values as <code>np.quantile</code> but uses percentage values instead of fractions. In that case, the second argument would have been <code>[0,25,50,75,100]</code>.</p>&#13;
<p class="indent">The returned quartile values are in <code>q</code>. We print them to get</p>&#13;
<pre>&#13;
18.0, 56.75, 68.0, 78.0, 100.0</pre>&#13;
<p class="noindent">Here, 18 is the minimum, 100 is the maximum, and the three cutoff values for the quartiles are 56.75, 68, and 78. Note that the cutoff for the second quartile is the median, 68.</p>&#13;
<p class="indent">The remaining code counts the number of values in <code>p</code> in each range. With 1,000 values, we’d expect to have 250 in each range, but because the math doesn’t always fall along existing data values, we get instead</p>&#13;
<pre>&#13;
250, 237, 253, 248</pre>&#13;
<p class="noindent">meaning 250 elements of <code>p</code> are less than 56.75, 237 are in [56.75, 68], and so forth.</p>&#13;
<p class="indent">The code above uses a clever counting trick worth explaining. We want to count the number of values in <code>p</code> in some range. We can’t use NumPy’s <code>np.where</code> function, as it doesn’t like the compound conditional statement. However, if we use an expression like <code>10 &lt;= p</code>, we’ll be given an array the same size as <code>p</code> where each element is either <code>True</code> if the condition is true for that element or <code>False</code> if it is not. Therefore, asking for <code>10 &lt;= p</code> and <code>p &lt; 90</code> will return two Boolean arrays. To get the elements where both conditions are true, we need to logically AND them together (<code>&amp;</code>). This gives us a final array the same size and shape as <code>p</code>, where all <code>True</code> elements represent values in <code>p</code> in [10, 90). To get the count, we apply the <code>sum</code> method that for a Boolean array treats <code>True</code> as one and <code>False</code> as zero.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_80"/><a href="ch04.xhtml#ch04fig03">Figure 4-3</a> shows the histogram of the exam data with the quartiles marked.</p>&#13;
<div class="image" id="ch04fig03"><img src="Images/04fig03.jpg" alt="image" width="677" height="573"/></div>&#13;
<p class="figcap"><em>Figure 4-3: A histogram of 1,000 exam scores with the quartiles marked</em></p>&#13;
<p class="indent">The example above shows yet again how useful a histogram is for visualizing and understanding data. We should use histograms whenever possible to help understand what’s going on with a dataset. <a href="ch04.xhtml#ch04fig03">Figure 4-3</a> superimposes the quartile values on the histogram. This helps us understand what the quartiles are and their relationship to the data values, but this is not a typical presentation style. More typical, and useful because it can show multiple features of a dataset, is the <em>box plot</em>. Let’s use it now for the exam scores above, but this time we’ll also include the two other sets of exam scores we ignored previously.</p>&#13;
<p class="indent">We’ll show a box plot first, and then explain it. To see the box plot for the three exams in the <em>exams.npy</em> file, use</p>&#13;
<pre>&#13;
d = np.load("exams.npy")<br/>&#13;
plt.boxplot(d)<br/>&#13;
plt.xlabel("Test")<br/>&#13;
plt.ylabel("Scores")<br/>&#13;
plt.show()</pre>&#13;
<p class="noindent">where we’re loading the full set of exam scores and then using the Matplotlib <code>boxplot</code> function.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_81"/>Take a look at the output, shown in <a href="ch04.xhtml#ch04fig04">Figure 4-4</a>.</p>&#13;
<div class="image" id="ch04fig04"><img src="Images/04fig04.jpg" alt="image" width="680" height="1046"/></div>&#13;
<p class="figcap"><em>Figure 4-4: Box plots for the three exams (top), and the box plot for the first exam with the components marked (bottom)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_82"/>The top chart in <a href="ch04.xhtml#ch04fig04">Figure 4-4</a> shows the box plot for the three sets of exam scores in <em>|</em>exams.npy|. The first of these is plotted again on the bottom of <a href="ch04.xhtml#ch04fig04">Figure 4-4</a>, along with labels describing the parts of the plot.</p>&#13;
<p class="indent">A box plot shows us a visual summary of the data. The box in the bottom chart in <a href="ch04.xhtml#ch04fig04">Figure 4-4</a> illustrates the range between the cutoffs for the first quartile (Q1) and the third quartile (Q3). The numerical difference between Q3 and Q1 is known as the <em>interquartile range (IQR)</em>. The larger the IQR, the more spread out the data is around the median. Notice that the score is on the y-axis this time. We could have easily made the plot horizontal, but vertical is the default. The median (Q2) is marked near the middle of the box. The mean is not shown in a box plot.</p>&#13;
<p class="indent">The box plot includes two additional lines, the <em>whiskers</em>, though Matplotlib calls them <em>fliers</em>. As indicated, they are 1.5 times the IQR above Q3 or below Q1. Finally, there are some circles labeled “possible outliers.” By convention, values outside of the whiskers are considered <em>possible outliers</em>, meaning they might represent erroneous data, either entered incorrectly by hand or, more likely these days, received from faulty sensors. For example, bright or dead pixels on a CCD camera might be considered outliers. When evaluating a potential dataset, we should be sensitive to outliers and use our best judgment about what to do with them. Usually, there are only a few, and we can drop the samples from the dataset without harm. However, it’s also possible that the outliers are actually real and are highly indicative of a particular class. If that’s the case, we want to keep them in the dataset in the hopes that the model will use them effectively. Experience, intuition, and common sense must guide us here.</p>&#13;
<p class="indent">Let’s interpret the top chart in <a href="ch04.xhtml#ch04fig04">Figure 4-4</a> showing the three sets of exam scores. The top of the whiskers is at 100 each time, which makes sense: a 100 is a perfect score, and there were 100s in the dataset. Notice that the box portion of the plot is not centered vertically in the whiskers. Recalling that 50 percent of the data values are between Q1 and Q3, with 25 percent above and below Q2 in the box, we see that the data is not rigorously normal; its distribution deviates from a normal curve. A glance back to the histogram in <a href="ch04.xhtml#ch04fig03">Figure 4-3</a> confirms this for the first exam. Similarly, we see the second and third exams deviate from normality as well. So, a box plot can tell us how similar the distribution of the dataset is to a normal distribution. When we discuss hypothesis testing below, we’ll want to know if the data is normally distributed or not.</p>&#13;
<p class="indent">What about possible outliers, the values below Q1 – 1.5 × IQR? We know the dataset represents test scores, so common sense tells us that these are not outliers but valid scores by particularly confused (or lazy) students. If the dataset contained values above 100 or below zero, those would be fair game to label outliers.</p>&#13;
<p class="indent">Sometimes dropping samples with outliers is the right thing to do. However, if the outlier is caused by missing data, cutting the sample might not be an option. Let’s take a look at what we might do with missing data, and why we should generally avoid it like the plague.</p>&#13;
<h3 class="h3" id="ch04lev1_4"><span epub:type="pagebreak" id="page_83"/>Missing Data</h3>&#13;
<p class="noindent">Missing data is just that, data we don’t have. If the dataset consists of samples representing feature vectors, missing data shows up as one or more features in a sample that were not measured for some reason. Often, missing data is encoded in some way. If the value is only positive, a missing feature might be marked with a –1 or, historically, –999. If the feature is given to us as a string, the string might be empty. For floating-point values, a not a number (NaN) might be used. NumPy makes it easy for us to check for NaNs in an array by using <code>np.isnan</code>:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> a = np.arange(10, dtype="float64")</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> a[3] = np.nan</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> np.isnan(a[3])</span><br/>&#13;
True<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> a[3] == np.nan</span><br/>&#13;
False<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> a[3] is np.nan</span><br/>&#13;
False</pre>&#13;
<p class="noindent">Notice that direct comparison to <code>np.nan</code> with either <code>==</code> or <code>is</code> doesn’t work; only testing with <code>np.isnan</code> works.</p>&#13;
<p class="indent">Detecting missing data is dataset-specific. Assuming we’ve convinced ourselves there is missing data, how do we handle it?</p>&#13;
<p class="indent">Let’s generate a small dataset with missing values and use our existing statistics knowledge to see how to handle them. The code for the following is in <code>missing.py</code>. First, we generate a dataset of 1,000 samples, each with four features:</p>&#13;
<pre>&#13;
N = 1000<br/>&#13;
np.random.seed(73939133)<br/>&#13;
x = np.zeros((N,4))<br/>&#13;
x[:,0] = 5*np.random.random(N)<br/>&#13;
x[:,1] = np.random.normal(10,1,size=N)<br/>&#13;
x[:,2] = 3*np.random.beta(5,2,N)<br/>&#13;
x[:,3] = 0.3*np.random.lognormal(size=N)</pre>&#13;
<p class="noindent">The dataset is in <code>x</code>. We fix the random number seed to get a reproducible result. The first feature is uniformly distributed. The second is normally distributed, while the third follows a beta distribution and the fourth a lognormal distribution.</p>&#13;
<p class="indent">At the moment, <code>x</code> has no missing values. Let’s add some by making random elements NaNs:</p>&#13;
<pre>&#13;
i = np.random.randint(0,N, size=int(0.05*N))<br/>&#13;
x[i,0] = np.nan<br/>&#13;
i = np.random.randint(0,N, size=int(0.05*N))<br/>&#13;
x[i,1] = np.nan<br/>&#13;
i = np.random.randint(0,N, size=int(0.05*N))<br/>&#13;
<span epub:type="pagebreak" id="page_84"/>x[i,2] = np.nan<br/>&#13;
i = np.random.randint(0,N, size=int(0.05*N))<br/>&#13;
x[i,3] = np.nan</pre>&#13;
<p class="noindent">The dataset now has NaNs across 5 percent of its values.</p>&#13;
<p class="indent">If a few samples in a large dataset have missing data, we can remove them from the dataset with little worry. However, if 5 percent of the samples have missing data, we probably don’t want to lose that much data. More worrisome still, what if there’s a correlation between the missing data and a particular class? Throwing the samples away might bias the dataset in some way that’ll make the model less useful.</p>&#13;
<p class="indent">So, what can we do? We just spent many pages learning how to summarize a dataset with basic descriptive statistics. Can we use those? Of course. We can look at the distributions of the features, ignoring the missing values, and use those distributions to decide how we might want to replace the missing data. Naively, we’d use the mean of the data we do have, but looking at the distribution may or may not push us toward the median instead, depending on how far the distribution is from normal. This sounds like a job for a box plot. Fortunately for us, Matplotlib’s <code>boxplot</code> function is smart; it ignores the NaNs. Therefore, making the box plot is a straightforward call to <code>boxplot(x)</code>.</p>&#13;
<p class="indent"><a href="ch04.xhtml#ch04fig05">Figure 4-5</a> shows us the dataset with the NaNs ignored.</p>&#13;
<div class="image" id="ch04fig05"><img src="Images/04fig05.jpg" alt="image" width="650" height="486"/></div>&#13;
<p class="figcap"><em>Figure 4-5: Box plot of the dataset ignoring missing values</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_85"/>The boxes in <a href="ch04.xhtml#ch04fig05">Figure 4-5</a> make sense for the distributions of the features. Feature 1 is uniformly distributed, so we expect a symmetric box around the mean/median. (These are the same for the uniform distribution.) Feature 2 is normally distributed, so we get a similar box structure as Feature 1, but, with only 1,000 samples, some asymmetry is evident. The beta distribution of Feature 3 is skewed toward the top of its range, which we see in the box plot. Finally, the lognormal distribution of Feature 4 should be skewed toward lower values, with a long tail visible as the many “outliers” above the whiskers, an object lesson against mindlessly calling such values outliers.</p>&#13;
<p class="indent">Because we have features that are highly not normally distributed, we’ll update missing values with the median instead of the mean. The code is straightforward:</p>&#13;
<pre>&#13;
good_idx = np.where(np.isnan(x[:,0]) == False)<br/>&#13;
m = np.median(x[good_idx,0])<br/>&#13;
bad_idx = np.where(np.isnan(x[:,0]) == True)<br/>&#13;
x[bad_idx,0] = m</pre>&#13;
<p class="noindent">Here, <code>i</code> first holds the indices of Feature 1 that are not NaNs. We use these to calculate the median (<code>m</code>). Next, we set <code>i</code> to the indices that are NaNs and replace them with the median. We can do the same for the other features, updating the entire dataset so we no longer have missing values.</p>&#13;
<p class="indent">Did we cause much of a change from the earlier distributions? No, because we only updated 5 percent of the values. For example, for Feature 3, based on the beta distribution, the mean and standard deviations change like so:</p>&#13;
<pre>&#13;
non-NaN mean, std = 2.169986, 0.474514<br/>&#13;
updated mean, std = 2.173269, 0.462957</pre>&#13;
<p class="indent">The moral of the story is that if there’s enough missing data that the dataset might become biased by dropping it, the safest thing to do is replace the missing data with the mean or median. To decide whether to use the mean or median, consult descriptive statistics, a box plot, or a histogram.</p>&#13;
<p class="indent">Additionally, if the dataset is labeled, as a deep learning dataset would be, the process described above needs to be completed with the mean or median of samples grouped by each class. Otherwise, the calculated value might be inappropriate for the class.</p>&#13;
<p class="indent">With missing data eliminated, deep learning models can be trained on the dataset.</p>&#13;
<h3 class="h3" id="ch04lev1_5">Correlation</h3>&#13;
<p class="noindent">At times, there is an association between the features in a dataset. If one goes up, the other might go up as well, though not necessarily in a simple linear way. Or, the other might go down—a negative association. The proper <span epub:type="pagebreak" id="page_86"/>word for this type of association is <em>correlation</em>. A statistic that measures correlation is a handy way to understand how the features in a dataset are related.</p>&#13;
<p class="indent">For example, it isn’t hard to see that the pixels of most images are highly correlated. This means if we select a pixel at random and then an adjacent pixel, there’s a good chance the second pixel will be similar to the first pixel. Images where this is not true look to us like random noise.</p>&#13;
<p class="indent">In traditional machine learning, highly correlated features were undesirable, as they didn’t add any new information and only served to confuse the models. The entire art of feature selection was developed, in part, to remove this effect. For modern deep learning, where the network itself learns a new representation of the input data, it’s less critical to have uncorrelated inputs. This is, in part, why images work as inputs to deep networks when they usually fail to work at all with older machine learning models.</p>&#13;
<p class="indent">Whether the learning is traditional or modern, as part of summarizing and exploring a dataset, correlations among the features are worth examining and understanding. In this section, we’ll discuss two types of correlations. Each type returns a single number that measures the strength of the correlation between two features in the dataset.</p>&#13;
<h4 class="h4" id="ch04lev2_8">Pearson Correlation</h4>&#13;
<p class="noindent">The <em>Pearson correlation coefficient</em> returns a number, <em>r</em> ϵ [–1, +1], that indicates the strength of the <em>linear</em> correlation between two features. By <em>linear</em> we mean how strongly we can describe the correlation between the features by a line. If the correlation is such that one feature goes up exactly as the other feature goes up, the correlation coefficient is +1. Conversely, if the second feature goes down exactly as the other goes up, the correlation is –1. A correlation of zero means there is no association between the two features; they are (possibly) independent.</p>&#13;
<p class="indent">I slipped the word <em>possibly</em> in the sentence above because there are situations where a nonlinear dependence between two features might lead to a zero Pearson correlation coefficient. These situations are not common, however, and for our purposes, we can claim a correlation coefficient near zero indicates the two features are independent. The closer the correlation coefficient is to zero, either positive or negative, the weaker the correlation between the features.</p>&#13;
<p class="indent">The Pearson correlation is defined using the means of the two features or the means of products of the two features. The inputs are two features, two columns of the dataset. We’ll call these inputs <em>X</em> and <em>Y</em>, where the capital letter refers to a vector of data values. Note, since these are two features from the dataset, <em>X<sub>i</sub></em> is paired with <em>Y<sub>i</sub></em>, meaning they both come from the same feature vector.</p>&#13;
<p class="indent">The formula for the Pearson correlation coefficient is</p>&#13;
<div class="imagec" id="ch04equ05"><img src="Images/04equ05.jpg" alt="image" width="554" height="53"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_87"/>We’ve introduced a new, but commonly used, notation. The mean of <em>X</em> is the <em>expectation</em> of <em>X</em>, denoted as E(<em>X</em>). Therefore, in <a href="ch04.xhtml#ch04equ05">Equation 4.5</a>, we see the mean of <em>X</em>, E(<em>X</em>), and the mean of <em>Y</em>, E(<em>Y</em>). As we might suspect, E(<em>XY</em>) is the mean of the product of <em>X</em> and <em>Y</em>, element by element. Similarly, E(<em>X</em><sup>2</sup>) is the mean of the product of <em>X</em> with itself, and E(<em>X</em>)<sup>2</sup> is the square of the mean of <em>X</em>. With this notation in hand, we can easily write our own function to calculate the Pearson correlation of two vectors of features:</p>&#13;
<pre>&#13;
import numpy as np<br/>&#13;
def pearson(x,y):<br/>&#13;
    exy = (x*y).mean()<br/>&#13;
    ex = x.mean()<br/>&#13;
    ey = y.mean()<br/>&#13;
    exx = (x*x).mean()<br/>&#13;
    ex2 = x.mean()**2<br/>&#13;
    eyy = (y*y).mean()<br/>&#13;
    ey2 = y.mean()**2<br/>&#13;
    return (exy - ex*ey)/(np.sqrt(exx-ex2)*np.sqrt(eyy-ey2))</pre>&#13;
<p class="noindent">The <code>pearson</code> function directly implements <a href="ch04.xhtml#ch04equ05">Equation 4.5</a>.</p>&#13;
<p class="indent">Let’s set up a scenario where we can use <code>pearson</code> and compare it to what NumPy and SciPy provide. The code that follows, including the definition of <code>pearson</code> above, is in the file <em>correlation.py</em>.</p>&#13;
<p class="indent">First, we’ll create three correlated vectors, <code>x</code>, <code>y</code>, and <code>z</code>. We imagine that these are features from a dataset so that <code>x[0]</code> is paired with <code>y[0]</code> and <code>z[0]</code>. The code we need is</p>&#13;
<pre>&#13;
np.random.seed(8675309)<br/>&#13;
N = 100<br/>&#13;
x = np.linspace(0,1,N) + (np.random.random(N)-0.5)<br/>&#13;
y = np.random.random(N)*x<br/>&#13;
z = -0.1*np.random.random(N)*x</pre>&#13;
<p class="noindent">Notice that we’re again fixing the NumPy pseudorandom seed to make the output reproducible. The first feature, <code>x</code>, is a noisy line from zero to one. The second, <code>y</code>, tracks <code>x</code> but is also noisy because of the multiplication by a random value in [0, 1). Finally, <code>z</code> is negatively correlated to <code>x</code> because of the –0.1 coefficient.</p>&#13;
<p class="indent">The top chart in <a href="ch04.xhtml#ch04fig06">Figure 4-6</a> plots the three feature values sequentially to see how they track each other. The bottom chart shows the three as paired points, with one value on the x-axis and the other on the y-axis.</p>&#13;
<span epub:type="pagebreak" id="page_88"/>&#13;
<div class="image" id="ch04fig06"><img src="Images/04fig06.jpg" alt="image" width="680" height="1047"/></div>&#13;
<p class="figcap"><em>Figure 4-6: Three features in sequence to show how they track (top), and a scatter plot of the features as pairs (bottom)</em></p>&#13;
<p class="indent">The NumPy function to calculate the Pearson correlation is <code>np.corrcoef</code>. Unlike our version, this function returns a matrix showing the correlations <span epub:type="pagebreak" id="page_89"/>between all pairs of variables passed to it. For example, using our <code>pearson</code> function, we get the following as the correlation coefficients between <code>x</code>, <code>y</code>, and <code>z</code>:</p>&#13;
<pre>&#13;
pearson(x,y):  0.682852<br/>&#13;
pearson(x,z): -0.850475<br/>&#13;
pearson(y,z): -0.565361</pre>&#13;
<p class="noindent">NumPy returns the following, with <code>x</code>, <code>y</code>, and <code>z</code> stacked as a single 3 × 100 array:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> d = np.vstack((x,y,z))</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> print(np.corrcoef(d))</span><br/>&#13;
[[ 1.          0.68285166 -0.85047468]<br/>&#13;
 [ 0.68285166  1.         -0.56536104]<br/>&#13;
 [-0.85047468 -0.56536104  1.        ]]</pre>&#13;
<p class="noindent">The diagonal corresponds to the correlation with each feature and itself, which is naturally perfect and therefore 1.0. The correlation between <code>x</code> and <code>y</code> is in element 0,1 and matches our <code>pearson</code> function value. Similarly, the correlation between <code>x</code> and <code>z</code> is in element 0,2, and the correlation between <code>y</code> and <code>z</code> is in element 1,2. Notice also that the matrix is symmetric, which we expect because corr(<em>X, Y</em>) = corr(<em>Y, X</em>).</p>&#13;
<p class="indent">SciPy’s correlation function is <code>stats.pearsonr</code>, which acts like ours but returns a <em>p</em>-value along with the <em>r</em> value. We’ll discuss <em>p</em>-values more later in the chapter. We use the returned <em>p</em>-value as the probability of an uncorrelated system producing the calculated correlation value. For our example features, the <em>p</em>-value is virtually identical to zero, implying there’s no reasonable likelihood that an uncorrelated system produced the features.</p>&#13;
<p class="indent">We stated earlier that for images, nearby pixels are usually highly correlated. Let’s see if this is actually true for a sample image. We’ll use the China image included with <code>sklearn</code> and treat specific rows of the green band as the paired vectors. We’ll calculate the correlation coefficient for two adjacent rows, a row further away, and a random vector:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> from sklearn.datasets import load_sample_image</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> china = load_sample_image('china.jpg')</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> a = china[230,:,1].astype("float64")</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> b = china[231,:,1].astype("float64")</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> c = china[400,:,1].astype("float64")</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> d = np.random.random(640)</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> pearson(a,b)</span><br/>&#13;
0.8979360<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> pearson(a,c)</span><br/>&#13;
-0.276082<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> pearson(a,d)</span><br/>&#13;
-0.038199</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_90"/>Comparing row 230 and row 231 shows that they are highly positively correlated. Comparing rows 230 and 400 shows a weaker and, in this case, negative correlation. Finally, as we might expect, correlation with a random vector gives a value approaching zero.</p>&#13;
<p class="indent">The Pearson correlation coefficient is so widely used that you’ll often see it referred to as merely <em>the correlation coefficient</em>. Let’s now take a look at a second correlation function and see how it differs from the Pearson coefficient.</p>&#13;
<h4 class="h4" id="ch04lev2_9">Spearman Correlation</h4>&#13;
<p class="noindent">The second correlation measure we’ll explore is the <em>Spearman correlation coefficient</em>, ρ ϵ [–1, +1]. It’s a measure based on the ranks of the feature values instead of the values themselves.</p>&#13;
<p class="indent">To rank <em>X</em>, we replace each value in <em>X</em> with the index to that value in the sorted version of <em>X</em>. If <em>X</em> is</p>&#13;
<pre>&#13;
[86, 62, 28, 43, 3, 92, 38, 87, 74, 11]</pre>&#13;
<p class="noindent">then the ranks are</p>&#13;
<pre>&#13;
[7, 5, 2, 4, 0, 9, 3, 8, 6, 1]</pre>&#13;
<p class="noindent">because when <em>X</em> is sorted, 86 goes in the eighth place (counting from zero), and 3 goes first.</p>&#13;
<p class="indent">The Pearson correlation looks for a linear relationship, whereas the Spearman looks for any monotonic association between the inputs.</p>&#13;
<p class="indent">If we have the ranks for the feature values, then the Spearman coefficient is</p>&#13;
<div class="imagec" id="ch04equ06"><img src="Images/04equ06.jpg" alt="image" width="471" height="58"/></div>&#13;
<p class="noindent">where <em>n</em> is the number of samples and <em>d</em> = rank(<em>X</em>) – rank(<em>Y</em>) is the difference of the rank of the paired <em>X</em> and <em>Y</em> values. Note how <a href="ch04.xhtml#ch04equ06">Equation 4.6</a> is only valid if the rankings are unique (that is, there are no repeated values in <em>X</em> or <em>Y</em>).</p>&#13;
<p class="indent">To calculate <em>d</em> in <a href="ch04.xhtml#ch04equ06">Equation 4.6</a>, we need to rank <em>X</em> and <em>Y</em> and use the difference of the ranks. The Spearman correlation is the Pearson correlation of the ranks.</p>&#13;
<p class="indent">The example above points the way to an implementation of the Spearman correlation:</p>&#13;
<pre>&#13;
import numpy as np<br/>&#13;
def spearman(x,y):<br/>&#13;
    n = len(x)<br/>&#13;
    t = x[np.argsort(x)]<br/>&#13;
    rx = []<br/>&#13;
    for i in range(n):<br/>&#13;
<span epub:type="pagebreak" id="page_91"/>    rx.append(np.where(x[i] == t)[0][0])<br/>&#13;
rx = np.array(rx, dtype="float64")<br/>&#13;
t = y[np.argsort(y)]<br/>&#13;
ry = []<br/>&#13;
for i in range(n):<br/>&#13;
    ry.append(np.where(y[i] == t)[0][0])<br/>&#13;
ry = np.array(ry, dtype="float64")<br/>&#13;
d = rx - ry<br/>&#13;
return 1.0 - (6.0/(n*(n*n-1)))*(d**2).sum()</pre>&#13;
<p class="noindent">To get the ranks, we need to first sort <em>X</em> (<code>t</code>). Then, for each value in <em>X</em> (<code>x</code>), we find where it occurs in <code>t</code> via <code>np.where</code> and take the first element, the first match. After building the <code>rx</code> list, we make it a floating-point NumPy array. We do the same for <em>Y</em> to get <code>ry</code>. With the ranks, <code>d</code> is set to their difference, and <a href="ch04.xhtml#ch04equ06">Equation 4.6</a> is used to return the Spearman ρ value.</p>&#13;
<p class="indent">Please note that this version of the Spearman correlation is limited by <a href="ch04.xhtml#ch04equ06">Equation 4.6</a> and should be used when there are no duplicate values in <em>X</em> or <em>Y</em>. Our example in this section uses random floating-point values, so the probability of an exact duplicate is quite low.</p>&#13;
<p class="indent">We’ll compare our <code>spearman</code> implementation to the SciPy version, <code>stats .spearmanr</code>. Like the SciPy version of the Pearson correlation, <code>stats.spearmanr</code> returns a <em>p</em>-value. We’ll ignore it. Let’s see how our function compares:</p>&#13;
<pre>&gt;&gt;&gt;<span class="codestrong1"> from scipy.stats import spearmanr</span><br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> print(spearman(x,y), spearmanr(x,y)[0])</span><br/>&#13;
0.694017401740174 0.6940174017401739<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> print(spearman(x,z), spearmanr(x,z)[0])</span><br/>&#13;
-0.8950855085508551 -0.895085508550855<br/>&#13;
&gt;&gt;&gt;<span class="codestrong1"> print(spearman(y,z), spearmanr(y,z)[0])</span><br/>&#13;
-0.6414041404140414 -0.6414041404140414</pre>&#13;
<p class="noindent">We have complete agreement with the SciPy function out to the last bit or so of the floating-point value.</p>&#13;
<p class="indent">It’s important to remember the fundamental difference between the Pearson and Spearman correlations. For example, consider the correlation between a linear ramp and the sigmoid function:</p>&#13;
<pre>&#13;
ramp = np.linspace(-20,20,1000)<br/>&#13;
sig = 1.0 / (1.0 + np.exp(-ramp))<br/>&#13;
print(pearson(ramp,sig))<br/>&#13;
print(spearman(ramp,sig))</pre>&#13;
<p class="noindent">Here, <code>ramp</code> increases linearly from –20 to 20 and <code>sig</code> follows a sigmoid shape (“S” curve). The Pearson correlation will be on the high side, since both are increasing as <em>x</em> becomes more positive, but the association is not purely linear. Running the example gives</p>&#13;
<pre>&#13;
0.905328<br/>&#13;
1.0</pre>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_92"/>indicating a Pearson correlation of 0.9 but a perfect Spearman correlation of 1.0, since for every increase in <code>ramp</code> there is an increase in <code>sig</code> and <em>only</em> an increase. The Spearman correlation has captured the nonlinear relationship between the arguments, while the Pearson correlation has only hinted at it. If we’re analyzing a dataset intended for a classical machine learning algorithm, the Spearman correlation might help us decide which features to keep and which to discard.</p>&#13;
<p class="indent">This concludes our examination of statistics for describing and understanding data. Let’s now learn how to use hypothesis testing to interpret experimental results and answer questions like “Are these two sets of data samples from the same parent distribution?”</p>&#13;
<h3 class="h3" id="ch04lev1_6">Hypothesis Testing</h3>&#13;
<p class="noindent">We have two independent sets of 50 students studying cell biology. We have no reason to believe the groups differ in any significant way, as students from the larger population were assigned randomly. Group 1 attended the lectures and, in addition, worked through a structured set of computer exercises. Group 2 only attended the lectures. Both groups took the same final examination, leading to the test scores given in <a href="ch04.xhtml#ch04tab01">Table 4-1</a>. We want to know if asking the students to work through the computer exercises made a difference in their final test scores.</p>&#13;
<p class="tabcap" id="ch04tab01"><strong>Table 4-1:</strong> Group 1 and Group 2 Test Scores</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:90%"/>&#13;
</colgroup>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Group 1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88 89 92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85 85 78 94 100</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>Group 2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86 77 94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88 91 83 85 73</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><a href="ch04.xhtml#ch04fig07">Figure 4-7</a> shows a box plot of <a href="ch04.xhtml#ch04tab01">Table 4-1</a>.</p>&#13;
<p class="indent">To understand if there is a significant change in final test scores between the two groups, we need to test some hypotheses. The method we’ll use to test the hypotheses is known as <em>hypothesis testing</em>, and it’s a critical piece of modern science.</p>&#13;
<span epub:type="pagebreak" id="page_93"/>&#13;
<div class="image" id="ch04fig07"><img src="Images/04fig07.jpg" alt="image" width="694" height="520"/></div>&#13;
<p class="figcap"><em>Figure 4-7: Box plot for the data in <a href="ch04.xhtml#ch04tab01">Table 4-1</a></em></p>&#13;
<p class="indent">Hypothesis testing is a broad topic, too extensive for us to provide more than a minimal introduction here. As this is a book on deep learning, we’ll focus on the scenario a deep learning researcher is likely to encounter. We’ll consider only two hypothesis tests: the t-test for unpaired samples of differing variance (a parametric test) and the Mann-Whitney U (a nonparametric test). As we progress, we’ll understand what these tests are and why we’re restricting ourselves to them, as well as the meaning of <em>parametric</em> and <em>nonparametric</em>.</p>&#13;
<p class="indent">To be successful with hypothesis testing, we need to know what we mean by <em>hypothesis</em>, so we’ll address that first, along with our rationale for limiting the types of hypothesis testing we’ll consider. With the hypothesis concept in hand, we’ll discuss the t-test and the Mann-Whitney U test in turn, using the data in <a href="ch04.xhtml#ch04tab01">Table 4-1</a> as our example. Let’s get started.</p>&#13;
<h4 class="h4" id="ch04lev2_10">Hypotheses</h4>&#13;
<p class="noindent">To understand if two sets of data are from the same parent distribution or not, we might look at summary statistics. <a href="ch04.xhtml#ch04fig07">Figure 4-7</a> shows us the box plot for Group 1 and Group 2. It appears that the two groups have different means and standard deviations. How do we know? The box plot shows us <span epub:type="pagebreak" id="page_94"/>the location of the medians, and the whiskers tell us something about the variance. Both of these together hint that the means will be different because the medians are different, and both sets of data are reasonably symmetric around the median. The space between the whiskers hints at the standard deviation. So, let’s make hypotheses using the means of the datasets.</p>&#13;
<p class="indent">In hypothesis testing, we have two hypotheses. The first, known as the <em>null hypothesis</em> (<em>H</em><sub>0</sub>), is that the two sets of data <em>are</em> from the same parent distribution, that there is nothing special to differentiate them. The second hypothesis, the <em>alternative hypothesis</em> (<em>H<sub>a</sub></em>), is that the two groups are not from the same distribution. Since we’ll be using the means, <em>H</em><sub>0</sub> is saying that the means, really the means of the parent population that generated the data, are the same. Similarly, if we reject <em>H</em><sub>0</sub>, we are implicitly accepting <em>H<sub>a</sub></em> and claiming we have evidence that the means are different. We don’t have the true population means, so we’ll use the sample means and standard deviations instead.</p>&#13;
<p class="indent">Hypothesis testing doesn’t tell us definitively whether <em>H</em><sub>0</sub> is true. Instead, it gives us evidence in favor of rejecting or accepting the null hypothesis. It’s critical to remember this.</p>&#13;
<p class="indent">We’re testing two independent samples to see if we should think of them as coming from the same parent distribution. There are other ways to use hypothesis testing, but we rarely encounter them in deep learning. For the task at hand, we need the sample means and the sample standard deviations. Our tests will ask the question, “Is there a meaningful difference in the means of these two sets?”</p>&#13;
<p class="indent">We’re only interested in detecting whether the two groups of data are from the same parent distribution, so another simplification we’ll make is that all of our tests will be <em>two-sided</em>, or <em>two-tailed</em>. When we use a test, like the t-test we’ll describe next, we’re comparing our calculated test statistic (the t-value) to the distribution of the test statistic and asking questions about how likely our calculated t-value is. If we want to know about the test statistic being above or below some fraction of that distribution, we’re making a two-sided test. If instead we want to know about the likelihood of the test statistic being above a particular value without caring about it being below, or vice versa, then we’re making a one-sided test.</p>&#13;
<p class="indent">Let’s lay out our assumptions and approach:</p>&#13;
<ol>&#13;
<li class="noindent">We have two independent sets of data we wish to compare.</li>&#13;
<li class="noindent">We’re making no assumption as to whether the standard deviations of the data are the same.</li>&#13;
<li class="noindent">Our null hypothesis is that the means of the parent distributions of the datasets are the same, <em>H</em><sub>0</sub> : μ<sub>1</sub> = μ<sub>2</sub>. We’ll use the sample means <span class="middle"><img src="Images/094equ01a.jpg" alt="image" width="59" height="21"/></span> and sample standard deviations (<em>s</em><sub>1</sub>, <em>s</em><sub>2</sub>) to help us decide to accept or reject <em>H</em><sub>0</sub>.</li>&#13;
<li class="noindent">Hypothesis tests assume that the data is <em>independent and identically distributed (i.i.d.)</em>. We interpret this as a statement that the data is a fair random sample.</li>&#13;
</ol>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_95"/>With these assumptions understood, let’s start with the t-test, the most widely used hypothesis test.</p>&#13;
<h4 class="h4" id="ch04lev2_11">The t-test</h4>&#13;
<p class="noindent">The <em>t-test</em> depends on <em>t</em>, the test statistic. This statistic is compared to the t-distribution and used to generate a <em>p</em>-value, a probability we’ll use to reach a conclusion about <em>H</em><sub>0</sub>. There’s a rich history behind the t-test and the related z-test that we’ll ignore here. I encourage you to dive more deeply into hypothesis testing when you have the chance or, at a minimum, review thoughtful articles about the proper way to do a hypothesis test and interpret its results.</p>&#13;
<p class="indent">The t-test is a <em>parametric</em> test. This means there are assumptions about the data and the distribution of the data. Specifically, the t-test assumes, beyond the data being i.i.d., that the distribution (histogram) of the data is normal. We’ve stated before that many physical processes do seem to follow a normal distribution, so there’s reason to think that data from actual measurements might do so.</p>&#13;
<p class="indent">There are many ways to test if a dataset is normally distributed, but we’ll ignore them, as there’s some debate about the utility of such tests. Instead, I’ll (somewhat recklessly) suggest you use the t-test and the Mann-Whitney U test together to help make your decision about accepting or rejecting <em>H</em><sub>0</sub>. Using both tests might lead to a situation where they disagree, where one test says there’s evidence against the null hypothesis and the other says there isn’t. In general, if the nonparametric test is claiming evidence against <em>H</em><sub>0</sub>, then one should probably accept that evidence regardless of the t-test result. If the t-test result is against <em>H</em><sub>0</sub>, but the Mann-Whitney U test isn’t, and you think the data is normal, then you might also accept the t-test result.</p>&#13;
<p class="indent">The t-test has different versions. We explicitly stated above that we’ll use a version designed for datasets of differing size and variance. The specific version of the t-test we’ll use is <em>Welch’s t-test</em>, which doesn’t assume the variance of the two datasets is the same.</p>&#13;
<p class="indent">The t-score for Welch’s t-test is</p>&#13;
<div class="imagec"><img src="Images/095equ01.jpg" alt="image" width="125" height="63"/></div>&#13;
<p class="noindent">where <em>n</em><sub>1</sub> and <em>n</em><sub>2</sub> are the size of the two groups.</p>&#13;
<p class="indent">The t-score, and an associated value known as the <em>degrees of freedom</em>, which is similar to but also different from the degrees of freedom mentioned above, generates the appropriate t-distribution curve. To get a <em>p</em>-value, we calculate the area under the curve, both above and below (positive and negative t-score), and return it. Since the integral of a probability distribution is 1, the total area under the tails from the positive and negative t-score value to positive and negative infinity will be the <em>p</em>-value. We’ll use the degrees of freedom below to help us calculate confidence intervals.</p>&#13;
<p class="indent">What does the <em>p</em>-value tell us? It tells us the probability of seeing the difference between the two means we see, or larger, <em>if</em> the null hypothesis <span epub:type="pagebreak" id="page_96"/>is true. Typically, if this probability is below some threshold we’ve chosen, we reject the null hypothesis and say we have evidence that the two groups have different means—that they come from different parent distributions. When we reject <em>H</em><sub>0</sub>, we say that the difference is <em>statistically significant</em>. The threshold for accepting/rejecting <em>H</em><sub>0</sub> is called α, usually with α = 0.05 as a typical, if problematic, value. We’ll discuss why 0.05 is problematic below.</p>&#13;
<p class="indent">The point to remember is that the <em>p</em>-value assumes the null hypothesis is true. It tells us the likelihood of a true <em>H</em><sub>0</sub> giving us at least the difference we see, or greater, between the groups. If the <em>p</em>-value is small, that has two possible meanings: (1) the null hypothesis is false, or (2) a random sampling error has given us samples that fall outside what we might expect. Since the <em>p</em>-value assumes <em>H</em><sub>0</sub> is true, a small <em>p</em>-value helps us believe less and less in (2) and boosts our confidence that (1) might be correct. However, the <em>p</em>-value alone cannot confirm (1); other knowledge needs to come into play.</p>&#13;
<p class="indent">I mentioned that using α = 0.05 is problematic. The main reason it’s problematic is that it’s too generous; it leads to too many rejections of a true null hypothesis. According to James Berger and Thomas Sellke in their article “Testing a Point Null Hypothesis: The Irreconcilability of <em>P</em> Values and Evidence” (<em>Journal of the American Statistical Association</em>, 1987), when α = 0.05, about 30 percent of true null hypotheses will be rejected. When we use something like α ≤ 0.001, the chance of falsely rejecting a true null hypothesis goes down to less than 3 percent. The moral of the story is that <em>p</em> &lt; 0.05 is not magic and, frankly, is unconvincing for a single study. Look for highly significant <em>p</em>-values of at least 0.001 or, preferably, much smaller. At <em>p</em>  = 0.05, all you have is a suggestion, and you should repeat the experiment. If repeated experiments all have a <em>p</em>-value around 0.05, then rejecting the null hypothesis begins to make sense.</p>&#13;
<h5 class="h5">Confidence Intervals</h5>&#13;
<p class="noindent">Along with a <em>p</em>-value, you’ll often see <em>confidence intervals (CIs)</em>. The confidence interval gives bounds within which we believe the true population difference in the means will lie, with a given confidence for repeated samples of the two datasets we’re comparing. Typically, we report 95 percent confidence intervals. Our hypothesis tests check for equality of means by asking if the difference of the sample means is zero or not. Therefore, any CI that includes zero signals to us that we cannot reject the null hypothesis.</p>&#13;
<p class="indent">For Welch’s t-test, the degrees of freedom is</p>&#13;
<div class="imagec" id="ch04equ07"><img src="Images/04equ07.jpg" alt="image" width="445" height="82"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_97"/>which we can use to calculate confidence intervals,</p>&#13;
<div class="imagec" id="ch04equ08"><img src="Images/04equ08.jpg" alt="image" width="511" height="63"/></div>&#13;
<p class="noindent">where <em>t</em><sub>1–α/2,<em>df</em></sub> is the critical value, and the t-value for the given confidence level (α) and the degrees of freedom, <em>df</em>, come from <a href="ch04.xhtml#ch04equ07">Equation 4.7</a>.</p>&#13;
<p class="indent">How should we interpret the 95 percent confidence interval? There is a population value: the true difference between the group means. The 95 percent confidence interval is such that if we could draw repeated samples from the distribution that produced the two datasets, 95 percent of the calculated confidence intervals would contain the true difference between the means. It is <em>not</em> the range that includes the true difference in the means at 95 percent certainty.</p>&#13;
<p class="indent">Beyond checking if zero is in the CI, the CI is useful because its width tells us something about the magnitude of the effect. Here, the effect is related to the difference between the means. We may have a statistically significant difference based on the <em>p</em>-value, but the effect might be practically meaningless. The CI will be narrow when the effect is large because small CIs imply a narrow range encompassing the true effect. We’ll see shortly how, when possible, to calculate another useful measure of effect.</p>&#13;
<p class="indent">Finally, a <em>p</em>-value less than α also will have a <em>CI</em><sub>α</sub> that does not include <em>H</em><sub>0</sub>. In other words, what the <em>p</em>-value tells us and what the confidence interval tells us track–they will not contradict each other.</p>&#13;
<h5 class="h5">Effect Size</h5>&#13;
<p class="noindent">It’s one thing to have a statistically significant <em>p</em>-value. It’s another for the difference represented by that <em>p</em>-value to be meaningful in the real world. A popular measure of the size of an effect, the <em>effect size</em>, is <em>Cohen’s d</em>. For us, since we’re using Welch’s t-test, Cohen’s <em>d</em> is found by calculating</p>&#13;
<div class="imagec" id="ch04equ09"><img src="Images/04equ09.jpg" alt="image" width="421" height="64"/></div>&#13;
<p class="indent">Cohen’s <em>d</em> is usually interpreted subjectively, though we should report the numeric value as well. Subjectively, the size of the effect could be</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong><em>d</em></strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Effect</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Small</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Medium</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Large</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_98"/>Cohen’s <em>d</em> makes sense. The difference between the means is a natural way to think about the effect. Scaling it by the mean variance puts it in a consistent range. From <a href="ch04.xhtml#ch04equ09">Equation 4.9</a>, we see that a <em>p</em>-value corresponding to a statistically significant result might lead to a small effect that isn’t of any true practical importance.</p>&#13;
<h5 class="h5">Evaluating the Test Scores</h5>&#13;
<p class="noindent">Let’s put all of the above together to apply the t-test to our test data from <a href="ch04.xhtml#ch04tab01">Table 4-1</a>. You’ll find the code in the file <em>hypothesis.py</em>. We generate the data-sets first:</p>&#13;
<pre>&#13;
np.random.seed(65535)<br/>&#13;
a = np.random.normal(85,6,50).astype("int32")<br/>&#13;
a[np.where(a &gt; 100)] = 100<br/>&#13;
b = np.random.normal(82,7,50).astype("int32")<br/>&#13;
b[np.where(b &gt; 100)] = 100</pre>&#13;
<p class="noindent">Once again, we’re using a fixed NumPy pseudorandom number seed for repeatability. We make <code>a</code> a sample from a normal distribution with a mean of 85 and a standard deviation of 6.0. We select <code>b</code> from a normal distribution with a mean of 82 and a standard deviation of 7.0. For both, we cap any values over 100 to 100. These are test scores, after all, without extra credit.</p>&#13;
<p class="indent">We apply the t-test next:</p>&#13;
<pre>&#13;
from scipy.stats import ttest_ind<br/>&#13;
t,p = ttest_ind(a,b, equal_var=False)<br/>&#13;
print("(t=%0.5f, p=%0.5f)" % (t,p))</pre>&#13;
<p class="noindent">We get <em>(t</em> = 2.40234, <em>p</em> = 0.01852). The <em>t</em> is the statistic, and <em>p</em> is the computed <em>p</em>-value. It’s 0.019, which is less than 0.05 but only by a factor of two. We have a weak result telling us we might want to reject the null hypothesis and believe that the two groups, <code>a</code> and <code>b</code>, come from different distributions. Of course, we know they do because we generated them, but it’s nice to see the test pointing in the right direction.</p>&#13;
<p class="indent">Notice that the function we import from SciPy is <code>ttest_ind</code>. This is the function to use for independent samples, which are not paired. Also, notice that we added <code>equal_var=False</code> to the call. This is how to use Welch’s t-test, which doesn’t assume that the variance between the two datasets is equal. We know they’re not equal, since <code>a</code> uses a standard deviation of 6.0 while <code>b</code> uses 7.0.</p>&#13;
<p class="indent">To get the confidence intervals, we’ll write a CI function, since NumPy and SciPy don’t include one. The function directly implements <a href="ch04.xhtml#ch04equ07">Equations 4.7</a> and <a href="ch04.xhtml#ch04equ08">4.8</a>:</p>&#13;
<pre>&#13;
from scipy import stats<br/>&#13;
def CI(a, b, alpha=0.05):<br/>&#13;
    n1, n2 = len(a), len(b)<br/>&#13;
<span epub:type="pagebreak" id="page_99"/>    s1, s2 = np.std(a, ddof=1)**2, np.std(b, ddof=1)**2<br/>&#13;
    df = (s1/n1 + s2/n2)**2 / ((s1/n1)**2/(n1-1) + (s2/n2)**2/(n2-1))<br/>&#13;
    tc = stats.t.ppf(1 - alpha/2, df)<br/>&#13;
    lo = (a.mean()-b.mean()) - tc*np.sqrt(s1/n1 + s2/n2)<br/>&#13;
    hi = (a.mean()-b.mean()) + tc*np.sqrt(s1/n1 + s2/n2)<br/>&#13;
    return lo, hi</pre>&#13;
<p class="noindent">The critical <em>t</em> value is given by calling <code>stats.t.ppf</code>, passing in the α/2 value and the proper degrees of freedom, <em>df</em>. The critical <em>t</em> value is the 97.5 percent percentile value, for α = 0.05, which is what the <em>percent point function (ppf)</em> returns. We divide by two to cover the tails of the t-distribution.</p>&#13;
<p class="indent">For our test example, the confidence interval is [0.56105, 5.95895]. Notice how this does not include zero, so the CI also indicates a statistically significant result. However, the range is rather large, so this is not a particularly robust result. The CI range can be difficult to interpret on its own, so, finally, let’s calculate Cohen’s <em>d</em> to see if it makes sense given the width of the confidence interval. In code, we implement <a href="ch04.xhtml#ch04equ09">Equation 4.9</a>:</p>&#13;
<pre>&#13;
def Cohen_d(a,b):<br/>&#13;
    s1 = np.std(a, ddof=1)**2<br/>&#13;
    s2 = np.std(b, ddof=1)**2<br/>&#13;
    return (a.mean() - b.mean()) / np.sqrt(0.5*(s1+s2))</pre>&#13;
<p class="noindent">We get <em>d</em> = 0.48047, corresponding to a medium effect size.</p>&#13;
<h4 class="h4" id="ch04lev2_12">The Mann-Whitney U Test</h4>&#13;
<p class="noindent">The t-test assumes the distribution of the source data is normal. If the data is not normally distributed, we should instead use a <em>nonparametric test</em>. Nonparametric tests make no assumptions about the underlying distribution of the data. The <em>Mann-Whitney U test</em>, sometimes called the <em>Wilcoxon rank-sum test</em>, is a nonparametric test to help decide if two different sets of data come from the same parent distribution. The Mann-Whitney U test does not rely directly on the values of the data, but instead uses the data’s ranking.</p>&#13;
<p class="indent">The null hypothesis for this test is the following: the probability that a randomly selected value from Group 1 is larger than a randomly selected value from Group 2 is 0.5. Let’s think a bit about that. If the data is from the same parent distribution, then we should expect any randomly selected pair of values from the two groups to show no preference as to which is larger than the other.</p>&#13;
<p class="indent">The alternative hypothesis is that the probability of a randomly selected value from Group 1 being larger than a randomly selected value from Group 2 is not 0.5. Notice, there is no statement as to the probability being greater or less than 0.5, only that it isn’t 0.5; thus, the Mann-Whitney U test, as we’ll use it, is two-sided.</p>&#13;
<p class="indent">The null hypothesis for the Mann-Whitney U test is not the same as the null hypothesis for the t-test. For the t-test, we’re asking whether the means <span epub:type="pagebreak" id="page_100"/>between the two groups are the same. (Really, we’re asking if the difference in the means is zero.) However, if two sets of data <em>are</em> from different parent distributions, both null hypotheses are false, so we can use the Mann-Whitney U test in place of the t-test, especially when the underlying data is not normally distributed.</p>&#13;
<p class="indent">To generate <em>U</em>, the Mann-Whitney statistic, we first pool both sets of data and rank them. Ties are replaced with the mean between the tie value rank and the next rank value. We also keep track of the source group so we can separate the list of ranks again. The ranks, by group, are summed to give <em>R</em><sub>1</sub> and <em>R</em><sub>2</sub> (using the ranks from the pooled data). We calculate two values,</p>&#13;
<div class="imagec"><img src="Images/100equ01.jpg" alt="image" width="264" height="128"/></div>&#13;
<p class="noindent">with the smaller called <em>U</em>, the test statistic. It’s possible to generate a <em>p</em>-value from <em>U</em>, keeping in mind all the discussion above about the meaning and use of <em>p</em>-values. As before, <em>n</em><sub>1</sub> and <em>n</em><sub>2</sub> are the number of samples in the two groups. The Mann-Whitney U test requires the smaller of these two numbers to be at least 21 samples. If you don’t have that many, the results may not be reliable when using the SciPy <code>mannwhitneyu</code> function.</p>&#13;
<p class="indent">We can run the Mann-Whitney U test on our test data from <a href="ch04.xhtml#ch04tab01">Table 4-1</a>,</p>&#13;
<pre>&#13;
from scipy.stats import mannwhitneyu<br/>&#13;
u,p = mannwhitneyu(a,b)<br/>&#13;
print("(U=%0.5f, p=%0.5f)" % (u,p))</pre>&#13;
<p class="noindent">with <code>a</code> and <code>b</code> as we used above for the t-test. This gives us <em>(U</em> = 997.00000, <em>p</em> = 0.04058). The <em>p</em>-value is barely below the minimum threshold of 0.05.</p>&#13;
<p class="indent">The means of <code>a</code> and <code>b</code> are 85 and 82, respectively. What happens to the <em>p</em>-values if we make the mean value of <code>b</code> 83 or 81? Changing the mean of <code>b</code> means changing the first argument to <code>np.random.normal</code>. Doing this gives us <a href="ch04.xhtml#ch04tab02">Table 4-2</a>, where I’ve included all results for completeness.</p>&#13;
<p class="tabcap" id="ch04tab02"><strong>Table 4-2:</strong> Mann-Whitney U Test and t-test Results for the Simulated Test Scores with Different Means (<em>n</em><sub>1</sub>=<em>n</em><sub>2</sub>=50)</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Means</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Mann-Whitney U</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>t-test</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">85 vs. 83</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(<em>U</em>=1104.50000, <em>p</em>=0.15839)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(<em>t</em>=1.66543, <em>p</em>=0.09959)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">85 vs. 82</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(<em>U</em>=997.00000, <em>p</em>=0.04058)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(<em>t</em>=2.40234, <em>p</em>=0.01852)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">85 vs. 81</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(<em>U</em>=883.50000, <em>p</em>=0.00575)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">(<em>t</em>=3.13925, <em>p</em>=0.00234)</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><a href="ch04.xhtml#ch04tab02">Table 4-2</a> should make sense to us. When the means are close, it’s harder to tell them apart, so we expect larger <em>p</em>-values. Recall how we have <span epub:type="pagebreak" id="page_101"/>only 50 samples in each group. As the difference between the means increases, the <em>p</em>-values go down. A difference of three in the means leads to barely significant <em>p</em>-values. When the difference is larger still, the <em>p</em>-values become truly significant—again, as we expect.</p>&#13;
<p class="indent">The analysis above begs the question: for a small difference in the means between the two groups, how do the <em>p</em>-values change as a function of the sample size?</p>&#13;
<p class="indent"><a href="ch04.xhtml#ch04fig08">Figure 4-8</a> shows the <em>p</em>-value (mean ± standard error) over 25 runs for both the Mann-Whitney U test and the t-test as a function of sample size for the case where the means are 85 and 84.</p>&#13;
<div class="image" id="ch04fig08"><img src="Images/04fig08.jpg" alt="image" width="650" height="490"/></div>&#13;
<p class="figcap"><em>Figure 4-8: Mean p-value as a function of sample size for a difference in the sample means of one, <img src="Images/101equ01.jpg" alt="image" width="134" height="22"/></em></p>&#13;
<p class="indent">Small datasets make it difficult to differentiate between cases when the difference in the means is small. We also see that larger sample sizes reveal the difference, regardless of the test. It is interesting that in <a href="ch04.xhtml#ch04fig08">Figure 4-8</a>, the Mann-Whitney U <em>p</em>-value is less than that of the t-test even though the underlying data is normally distributed. Conventional wisdom states that it’s usually the other way around.</p>&#13;
<p class="indent"><a href="ch04.xhtml#ch04fig08">Figure 4-8</a> is an object lesson in the power of large-sample tests to detect real differences. When the sample size is large enough, a weak difference becomes significant. However, we need to balance this with the effect size. When we have 1,000 samples in each group, we have a statistically significant <em>p</em>-value, but we also have a Cohen’s <em>d</em> of about 0.13, signaling a weak effect. A large sample study might find a significant effect that is so weak as to be practically meaningless.</p>&#13;
<h3 class="h3" id="ch04lev1_7"><span epub:type="pagebreak" id="page_102"/>Summary</h3>&#13;
<p class="noindent">This chapter touched on the key aspects of statistics you’ll encounter during your sojourn through the world of deep learning. Specifically, we learned about different types of data and how to ensure the data is useful for building models. We then learned about summary statistics and saw examples that used them to help us understand a dataset. Understanding our data is key to successful deep learning. We investigated the different types of means, learned about measures of variation, and saw the utility of visualizing the data via box plots.</p>&#13;
<p class="indent">Missing data is a bane of deep learning. In this chapter, we investigated how to compensate for missing data. Next, we discussed correlation, how to detect and measure the relationships between elements of a dataset. Finally, we introduced hypothesis testing. Restricting ourselves to the most likely scenario we’ll encounter in deep learning, we learned how to apply both the t-test and the Mann-Whitney U test. Hypothesis testing introduced us to the <em>p</em>-value. We saw examples of it and discussed how to interpret it correctly.</p>&#13;
<p class="indent">In the next chapter we’ll leave statistics behind and dive headfirst into the world of linear algebra. Linear algebra is how we implement neural networks.</p>&#13;
</div></body></html>