- en: '**16'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GOING FURTHER**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You now have what I feel is a good introduction to modern machine learning.
    We have covered building datasets, classical models, model evaluation, and introductory
    deep learning, from traditional neural networks to convolutional neural networks.
    This short chapter is intended to help you go further.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at both short-term “what’s next” sorts of things as well as longer-term
    forks in the road you may wish to explore. We’ll also include online resources
    where you will find the latest and greatest (always cognizant that anything online
    is ephemeral). After that comes a necessarily subjective list of conferences you
    may wish to attend. We’ll close the chapter and book with a thank you and a goodbye.
  prefs: []
  type: TYPE_NORMAL
- en: Going Further with CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even after four chapters’ worth of material, we’ve barely scratched the surface
    of what convolutional neural networks can do. In part, we limited ourselves so
    you could grasp the fundamentals. And, in part, we were limited because we made
    a conscious decision not to require a GPU. Training complex models with a GPU
    is, in general, 20 to 25 times faster than using a CPU. With a GPU in your system,
    preferably designed for deep learning applications, the possibilities increase
    dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: The models we developed were small, reminiscent of the original LeNet models
    LeCun developed in the 1990s. They get the point across, but they will not go
    too far in many cases. Modern CNNs come in a variety of flavors and now “standard”
    architectures. With a GPU, you can explore these larger architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'These architectures should be on your list of what to look at next:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: U-Net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlexNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunately, the Keras toolkit we introduced (but also barely explored) supports
    all of these architectures. The two that seem especially useful to me are ResNet
    and U-Net. The latter is for semantic segmentation of inputs and has been widely
    successful, especially in medical imaging. To successfully train any of these
    architectures before your computer’s power supply or hard drive has failed, to
    say nothing of your heart, you do need a GPU. Medium to higher-end gaming GPUs
    (from NVIDIA, for example) will support new enough versions of CUDA that you can
    get going with a card for under 500 USD. The real trick is ensuring that your
    computer will support the card. The power requirements are high, typically requiring
    a power supply of 600W or more, and a slot that supports a double-wide PCIe card.
    Go for RAM over performance; the more RAM the GPU has, the larger a model it will
    support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if you don’t upgrade your system with a GPU, it’s worth your time to study
    the aforementioned architectures to see what makes them special and to understand
    how additional layers work. Check out the Keras documentation for more details:
    [keras.io](http://keras.io).'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning and Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This book has dealt exclusively with supervised learning. Of the three main
    branches of machine learning, supervised learning is probably the most widely
    used. Recalling the Marx brothers, supervised learning is like Groucho, the one
    everyone remembers. That isn’t an insult to the memory of Harpo and Chico, nor
    is it an insult to the other two branches of machine learning: reinforcement learning
    and unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reinforcement learning* is goal-oriented; it encourages models to learn how
    to behave and act to maximize a reward. Instead of learning how to take an input
    and map it to a specific output class, as in supervised learning, reinforcement
    learning learns how to act in the current situation to maximize an overall goal,
    like winning a game. Many of the impressive news stories related to machine learning
    have involved reinforcement learning. These include the first Atari 2600 game-playing
    systems capable of beating the best humans, as well as the fall of the world Go
    champion to AlphaGo, and the even more impressive achievement of AlphaGo Zero,
    which mastered Go from scratch without learning from millions of games played
    by humans. Any self-driving car system is likely extremely complex, but it’s a
    sure bet that reinforcement learning is a key part of that system.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unsupervised learning* refers to systems that learn on their own from unlabeled
    input data. Historically, this meant clustering, algorithms like *k*-means that
    take unlabeled feature vectors and attempt to group them by some similarity metric.
    Currently, one might argue that unsupervised learning is viewed as somewhat unimportant,
    given the insane amount of work being done with supervised learning and reinforcement
    learning. This is only half true; a lot of supervised learning is attempting to
    use unlabeled data (search for *domain adaptation*). How much of our own learning
    is unsupervised? An autonomous system set loose on an alien world will likely
    be more successful if it can learn things its creators didn’t know it would need
    to know. This suggests the importance of unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Generative adversarial networks (GANs)* burst on the scene in 2014, the brainchild
    of deep learning researcher Ian Goodfellow. GANs were quickly heralded as the
    most significant advance in machine learning in 20 years (Yann LeCun, spoken at
    NIPS 2016, Barcelona).'
  prefs: []
  type: TYPE_NORMAL
- en: Recent news about models that can generate an infinite number of photo-quality
    human faces use GANs. So do models that create simulated scenes and convert images
    of one style (say, a painting) to another (like a photograph). GANs wed a network
    that generates outputs, often based on some random setting of its input, to a
    discriminative network that tries to learn how to tell the difference between
    real inputs and inputs that came from the generative part. The two networks are
    trained together so that the generative network gets better and better at fooling
    the discriminative network. In contrast, the discriminative network gets better
    and better at learning how to tell the difference. The result is a generative
    network that is pretty good at outputting what you want it to output.
  prefs: []
  type: TYPE_NORMAL
- en: A proper study of GANs would require a book, but they are well worth a look
    and some of your time, at least to develop an intuitive sense of what is going
    on. A good place to start is with the particularly popular GAN architecture, CycleGAN,
    which has, in turn, spawned a small army of similar models.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A major topic entirely ignored by this book is *recurrent neural networks (RNNs)*.
    These are networks with feedback loops, and they work well for processing sequences
    like a time series of measurements—think sound samples or video frames. The most
    common form is the LSTM, the long short-term memory network. Recurrent networks
    are widely used in neural translation models like Google Translate that have made
    it possible to do real-time translation between dozens of languages.
  prefs: []
  type: TYPE_NORMAL
- en: Online Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The online resources for machine learning are legion and growing daily. Here
    are a few places that I find helpful and that are likely to stand the test of
    time. In no particular order:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reddit Machine Learning (*[www.reddit.com/r/MachineLearning/](http://www.reddit.com/r/MachineLearning/)*)**
    Look here for up-to-the-minute news and discussions of the latest papers and research.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Arxiv (*[https://arxiv.org/](https://arxiv.org/)*)** Machine learning progresses
    too quickly for most papers to go through the lengthy peer-review process print
    journals require. Instead, almost without exception, researchers and many conferences
    place all their papers on this preprint server, providing free access to the very
    latest in machine learning research. It can be daunting to sift through. Personally,
    I use the Arxiv app for my phone and several times a week peruse the following
    categories: Computer Vision and Pattern Recognition, Artificial Intelligence,
    Neural and Evolutionary Computing, and Machine Learning. The number of papers
    appearing in just these categories per week is impressive and a good indication
    of how active this field really is. To address the insane quantity of papers,
    deep learning researcher Andrej Karpathy created the useful Arxiv Sanity site
    at *[http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GitHub (*[https://github.com/](https://github.com/)*)** This is a place where
    people can host software projects. Go to the site directly and search for machine
    learning projects or use a standard search engine and add the keyword *github*
    to the search. With the explosion of machine learning projects, a beautiful thing
    has happened. The vast majority of the projects are freely available, even for
    commercial use. This typically includes full source code and datasets. If you
    read about something in a paper on Arxiv, you’ll likely find an implementation
    of it on Github.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coursera (*[https://www.coursera.org/](https://www.coursera.org/)*)** Coursera
    is a premier site for online courses, the vast majority of which can be audited
    for free. There are other sites, but Coursera was co-founded by Andrew Ng, and
    his machine learning course is very popular.'
  prefs: []
  type: TYPE_NORMAL
- en: '**YouTube (*[https://www.youtube.com/](https://www.youtube.com/)*)** YouTube
    is a force of nature at this point, but it is chock-full of machine learning videos.
    Let the viewer beware, but with some digging and judicious selection, you’ll find
    a lot here, including demonstrations of the latest and greatest. Search for “Neural
    Networks for Machine Learning” taught by Geoffrey Hinton.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kaggle (*[https://www.kaggle.com/](https://www.kaggle.com/)*)** Kaggle hosts
    machine learning competitions and is a good resource for datasets. Winners detail
    their models and training processes, providing ample opportunity to learn the
    art.'
  prefs: []
  type: TYPE_NORMAL
- en: Conferences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the best ways to learn a new language is to immerse yourself in a culture
    that speaks the language. The same is true for machine learning. The way to immerse
    yourself in the culture of machine learning is to attend conferences. This can
    be expensive, but many schools and companies view it as important, so you might
    be able to get support for attending.
  prefs: []
  type: TYPE_NORMAL
- en: 'The massive explosion of interest in machine learning has caused a new phenomenon,
    one that I haven’t seen happen in other academic disciplines: conferences selling
    out. This is true of the biggest conferences, but it might be happening to other
    conferences as well. If you want to attend, be aware that timing matters. Again,
    in no particular order, and missing many good but smaller conferences, consider
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NeurIPS (formerly NIPS)** Short for *Neural Information Processing Systems*,
    this is likely the biggest machine learning conference. At this academic conference,
    you can expect to see the latest research presented. NeurIPS has sold out quickly
    in recent years, in under 12 minutes in 2018 (!), and has now switched to a lottery
    system, so unless you are a presenter of some kind, getting the golden ticket
    email allowing you to register is not assured. It’s usually held in Canada.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ICML** Short for *International Conference on Machine Learning*, this is
    perhaps the second largest annual conference. This academic conference has several
    tracks and workshops and is typically held in Europe or North America.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ICLR** The International Conference on Learning Representations is a deep
    learning–focused academic conference. If you want in-the-weeds technical presentations
    on deep learning, this is the place to be.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CVPR** Computer Vision and Pattern Recognition is another large conference
    that’s perhaps slightly less academic than ICLR. CVPR is popular and not exclusively
    machine learning–oriented.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GTC** The GPU Technology Conference, sponsored by NVIDIA, is a technical
    conference as opposed to an academic conference. The annual presentation of new
    NVIDIA hardware happens here, along with a large expo, in San Jose, California.'
  prefs: []
  type: TYPE_NORMAL
- en: The Book
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Saying there are a few machine learning books out there is like saying there
    are a few fish in the sea. However, as far as deep learning is concerned, one
    stands head-and-shoulders above the rest: *Deep Learning* by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press, 2016). See *[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep Learning* is the book you should go to if you want to get serious about
    being a machine learning researcher. Even if you don’t, it covers the key topics
    in depth and with mathematical rigor. The book is not for those looking to get
    better at using one toolkit or another, but for those who want to see the theory
    behind machine learning and the math that goes with it. In essence, it’s an advanced
    undergraduate—if not graduate-level text, but that shouldn’t put you off. At some
    point, you will want to take a look at this book, so keep it in the back of your
    mind—or on your bookshelf.'
  prefs: []
  type: TYPE_NORMAL
- en: So Long and Thanks for All the Fish
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve reached the end of the book. There’s no monster here, only ourselves,
    and the knowledge and intuition we’ve gained by working through the preceding
    chapters. Thank you for persevering. It’s been fun for me to write; I genuinely
    hope it’s been fun for you to read and contemplate. Don’t stop now—take what we’ve
    developed and run with it. If you’re like me, you’ll see applications for machine
    learning everywhere. Go forth and classify!
  prefs: []
  type: TYPE_NORMAL
