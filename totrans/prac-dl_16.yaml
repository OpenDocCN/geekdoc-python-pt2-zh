- en: '**16'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GOING FURTHER**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: You now have what I feel is a good introduction to modern machine learning.
    We have covered building datasets, classical models, model evaluation, and introductory
    deep learning, from traditional neural networks to convolutional neural networks.
    This short chapter is intended to help you go further.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at both short-term “what’s next” sorts of things as well as longer-term
    forks in the road you may wish to explore. We’ll also include online resources
    where you will find the latest and greatest (always cognizant that anything online
    is ephemeral). After that comes a necessarily subjective list of conferences you
    may wish to attend. We’ll close the chapter and book with a thank you and a goodbye.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Going Further with CNNs
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even after four chapters’ worth of material, we’ve barely scratched the surface
    of what convolutional neural networks can do. In part, we limited ourselves so
    you could grasp the fundamentals. And, in part, we were limited because we made
    a conscious decision not to require a GPU. Training complex models with a GPU
    is, in general, 20 to 25 times faster than using a CPU. With a GPU in your system,
    preferably designed for deep learning applications, the possibilities increase
    dramatically.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The models we developed were small, reminiscent of the original LeNet models
    LeCun developed in the 1990s. They get the point across, but they will not go
    too far in many cases. Modern CNNs come in a variety of flavors and now “standard”
    architectures. With a GPU, you can explore these larger architectures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'These architectures should be on your list of what to look at next:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: U-Net
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlexNet
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunately, the Keras toolkit we introduced (but also barely explored) supports
    all of these architectures. The two that seem especially useful to me are ResNet
    and U-Net. The latter is for semantic segmentation of inputs and has been widely
    successful, especially in medical imaging. To successfully train any of these
    architectures before your computer’s power supply or hard drive has failed, to
    say nothing of your heart, you do need a GPU. Medium to higher-end gaming GPUs
    (from NVIDIA, for example) will support new enough versions of CUDA that you can
    get going with a card for under 500 USD. The real trick is ensuring that your
    computer will support the card. The power requirements are high, typically requiring
    a power supply of 600W or more, and a slot that supports a double-wide PCIe card.
    Go for RAM over performance; the more RAM the GPU has, the larger a model it will
    support.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if you don’t upgrade your system with a GPU, it’s worth your time to study
    the aforementioned architectures to see what makes them special and to understand
    how additional layers work. Check out the Keras documentation for more details:
    [keras.io](http://keras.io).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning and Unsupervised Learning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This book has dealt exclusively with supervised learning. Of the three main
    branches of machine learning, supervised learning is probably the most widely
    used. Recalling the Marx brothers, supervised learning is like Groucho, the one
    everyone remembers. That isn’t an insult to the memory of Harpo and Chico, nor
    is it an insult to the other two branches of machine learning: reinforcement learning
    and unsupervised learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '*Reinforcement learning* is goal-oriented; it encourages models to learn how
    to behave and act to maximize a reward. Instead of learning how to take an input
    and map it to a specific output class, as in supervised learning, reinforcement
    learning learns how to act in the current situation to maximize an overall goal,
    like winning a game. Many of the impressive news stories related to machine learning
    have involved reinforcement learning. These include the first Atari 2600 game-playing
    systems capable of beating the best humans, as well as the fall of the world Go
    champion to AlphaGo, and the even more impressive achievement of AlphaGo Zero,
    which mastered Go from scratch without learning from millions of games played
    by humans. Any self-driving car system is likely extremely complex, but it’s a
    sure bet that reinforcement learning is a key part of that system.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '*Unsupervised learning* refers to systems that learn on their own from unlabeled
    input data. Historically, this meant clustering, algorithms like *k*-means that
    take unlabeled feature vectors and attempt to group them by some similarity metric.
    Currently, one might argue that unsupervised learning is viewed as somewhat unimportant,
    given the insane amount of work being done with supervised learning and reinforcement
    learning. This is only half true; a lot of supervised learning is attempting to
    use unlabeled data (search for *domain adaptation*). How much of our own learning
    is unsupervised? An autonomous system set loose on an alien world will likely
    be more successful if it can learn things its creators didn’t know it would need
    to know. This suggests the importance of unsupervised learning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Generative adversarial networks (GANs)* burst on the scene in 2014, the brainchild
    of deep learning researcher Ian Goodfellow. GANs were quickly heralded as the
    most significant advance in machine learning in 20 years (Yann LeCun, spoken at
    NIPS 2016, Barcelona).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Recent news about models that can generate an infinite number of photo-quality
    human faces use GANs. So do models that create simulated scenes and convert images
    of one style (say, a painting) to another (like a photograph). GANs wed a network
    that generates outputs, often based on some random setting of its input, to a
    discriminative network that tries to learn how to tell the difference between
    real inputs and inputs that came from the generative part. The two networks are
    trained together so that the generative network gets better and better at fooling
    the discriminative network. In contrast, the discriminative network gets better
    and better at learning how to tell the difference. The result is a generative
    network that is pretty good at outputting what you want it to output.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于能够生成无限数量高质量人类面孔的模型使用了 GANs（生成对抗网络）。同样，创建模拟场景和将一种风格（比如画作）转化为另一种风格（如照片）的模型也使用了
    GANs。GANs 将生成网络（通常基于输入的随机设置生成输出）与判别网络（尝试学会区分真实输入与来自生成部分的输入）结合在一起。这两个网络共同训练，以便生成网络越来越擅长欺骗判别网络，而判别网络则越来越擅长学习如何辨别真伪。最终的结果是，生成网络能相当不错地输出你希望的内容。
- en: A proper study of GANs would require a book, but they are well worth a look
    and some of your time, at least to develop an intuitive sense of what is going
    on. A good place to start is with the particularly popular GAN architecture, CycleGAN,
    which has, in turn, spawned a small army of similar models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对 GANs 的深入研究需要一本书，但它们绝对值得一看，至少可以帮助你形成对这一技术的直观理解。一个很好的入门点是特别流行的 GAN 架构——CycleGAN，它又衍生出了大量类似的模型。
- en: Recurrent Neural Networks
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: A major topic entirely ignored by this book is *recurrent neural networks (RNNs)*.
    These are networks with feedback loops, and they work well for processing sequences
    like a time series of measurements—think sound samples or video frames. The most
    common form is the LSTM, the long short-term memory network. Recurrent networks
    are widely used in neural translation models like Google Translate that have made
    it possible to do real-time translation between dozens of languages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书完全忽略的一个重要主题是 *递归神经网络 (RNNs)*。这些网络有反馈回路，非常适合处理像时间序列测量数据这样的序列——比如声音样本或视频帧。最常见的形式是
    LSTM（长短期记忆网络）。递归网络被广泛应用于神经翻译模型中，比如 Google Translate，使得实时翻译数十种语言成为可能。
- en: Online Resources
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线资源
- en: 'The online resources for machine learning are legion and growing daily. Here
    are a few places that I find helpful and that are likely to stand the test of
    time. In no particular order:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的在线资源数不胜数，而且每天都在增加。以下是我认为有帮助且可能经得起时间考验的几个地方，顺序不分先后：
- en: '**Reddit Machine Learning (*[www.reddit.com/r/MachineLearning/](http://www.reddit.com/r/MachineLearning/)*)**
    Look here for up-to-the-minute news and discussions of the latest papers and research.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Reddit 机器学习 (*[www.reddit.com/r/MachineLearning/](http://www.reddit.com/r/MachineLearning/)*)**
    在这里查看最新的新闻和关于最新论文及研究的讨论。'
- en: '**Arxiv (*[https://arxiv.org/](https://arxiv.org/)*)** Machine learning progresses
    too quickly for most papers to go through the lengthy peer-review process print
    journals require. Instead, almost without exception, researchers and many conferences
    place all their papers on this preprint server, providing free access to the very
    latest in machine learning research. It can be daunting to sift through. Personally,
    I use the Arxiv app for my phone and several times a week peruse the following
    categories: Computer Vision and Pattern Recognition, Artificial Intelligence,
    Neural and Evolutionary Computing, and Machine Learning. The number of papers
    appearing in just these categories per week is impressive and a good indication
    of how active this field really is. To address the insane quantity of papers,
    deep learning researcher Andrej Karpathy created the useful Arxiv Sanity site
    at *[http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Arxiv (*[https://arxiv.org/](https://arxiv.org/)*)** 机器学习发展迅速，以至于大多数论文无法通过传统的同行评审过程发表在印刷期刊上。相反，几乎没有例外，研究人员和许多会议将所有论文上传到这个预印本服务器，提供免费访问最新的机器学习研究。浏览这些论文可能会让人感到不知所措。就个人而言，我使用
    Arxiv 的手机应用，每周会浏览以下几个类别：计算机视觉与模式识别、人工智能、神经与进化计算和机器学习。仅这些类别每周发布的论文数量就令人印象深刻，也能很好地反映出这个领域的活跃程度。为了应对海量的论文，深度学习研究者
    Andrej Karpathy 创建了实用的 Arxiv Sanity 网站，地址为 *[http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)*。'
- en: '**GitHub (*[https://github.com/](https://github.com/)*)** This is a place where
    people can host software projects. Go to the site directly and search for machine
    learning projects or use a standard search engine and add the keyword *github*
    to the search. With the explosion of machine learning projects, a beautiful thing
    has happened. The vast majority of the projects are freely available, even for
    commercial use. This typically includes full source code and datasets. If you
    read about something in a paper on Arxiv, you’ll likely find an implementation
    of it on Github.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**GitHub (*[https://github.com/](https://github.com/)*)** 这是一个人们可以托管软件项目的地方。你可以直接访问该网站，搜索机器学习项目，或者使用标准搜索引擎并在搜索时加入*github*关键词。随着机器学习项目的爆炸式增长，发生了一件美好的事情。绝大多数项目都是免费的，甚至可以商业使用。这通常包括完整的源代码和数据集。如果你在Arxiv上看到了一篇文章，通常可以在GitHub上找到其实现版本。'
- en: '**Coursera (*[https://www.coursera.org/](https://www.coursera.org/)*)** Coursera
    is a premier site for online courses, the vast majority of which can be audited
    for free. There are other sites, but Coursera was co-founded by Andrew Ng, and
    his machine learning course is very popular.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Coursera (*[https://www.coursera.org/](https://www.coursera.org/)*)** Coursera是一个在线课程的首选网站，其中绝大多数课程可以免费旁听。虽然也有其他网站，但Coursera由Andrew
    Ng共同创办，他的机器学习课程非常受欢迎。'
- en: '**YouTube (*[https://www.youtube.com/](https://www.youtube.com/)*)** YouTube
    is a force of nature at this point, but it is chock-full of machine learning videos.
    Let the viewer beware, but with some digging and judicious selection, you’ll find
    a lot here, including demonstrations of the latest and greatest. Search for “Neural
    Networks for Machine Learning” taught by Geoffrey Hinton.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**YouTube (*[https://www.youtube.com/](https://www.youtube.com/)*)** YouTube目前已经是一个巨大的平台，里面充满了机器学习视频。观众需要谨慎选择，但通过一些挖掘和明智的筛选，你会在这里找到很多内容，包括最新最伟大的演示。搜索“Neural
    Networks for Machine Learning”，这是Geoffrey Hinton讲授的课程。'
- en: '**Kaggle (*[https://www.kaggle.com/](https://www.kaggle.com/)*)** Kaggle hosts
    machine learning competitions and is a good resource for datasets. Winners detail
    their models and training processes, providing ample opportunity to learn the
    art.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kaggle (*[https://www.kaggle.com/](https://www.kaggle.com/)*)** Kaggle举办机器学习竞赛，并且是一个很好的数据集资源库。获胜者会详细介绍他们的模型和训练过程，提供了大量学习技术的机会。'
- en: Conferences
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 会议
- en: One of the best ways to learn a new language is to immerse yourself in a culture
    that speaks the language. The same is true for machine learning. The way to immerse
    yourself in the culture of machine learning is to attend conferences. This can
    be expensive, but many schools and companies view it as important, so you might
    be able to get support for attending.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 学习新语言的最佳方法之一是将自己沉浸在讲这种语言的文化中。机器学习也是如此。将自己沉浸在机器学习文化中的方法是参加会议。这可能会很昂贵，但许多学校和公司都认为这很重要，所以你可能能够获得参加的支持。
- en: 'The massive explosion of interest in machine learning has caused a new phenomenon,
    one that I haven’t seen happen in other academic disciplines: conferences selling
    out. This is true of the biggest conferences, but it might be happening to other
    conferences as well. If you want to attend, be aware that timing matters. Again,
    in no particular order, and missing many good but smaller conferences, consider
    the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习兴趣的爆炸性增长带来了一个新现象，这是我在其他学术领域未曾见过的：会议售罄。这在最大规模的会议上尤其如此，但其他会议也可能出现这种情况。如果你想参加，请注意时机非常重要。以下是一些会议，顺序无关紧要，且未提及许多优秀但规模较小的会议，请参考：
- en: '**NeurIPS (formerly NIPS)** Short for *Neural Information Processing Systems*,
    this is likely the biggest machine learning conference. At this academic conference,
    you can expect to see the latest research presented. NeurIPS has sold out quickly
    in recent years, in under 12 minutes in 2018 (!), and has now switched to a lottery
    system, so unless you are a presenter of some kind, getting the golden ticket
    email allowing you to register is not assured. It’s usually held in Canada.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**NeurIPS（前身为NIPS）** *Neural Information Processing Systems*（神经信息处理系统）的缩写，可能是最大规模的机器学习会议。在这个学术会议上，你可以期待看到最新的研究成果。近年来，NeurIPS的门票很快售罄，2018年甚至在12分钟内就售罄(!)，现在已转为抽签系统，所以除非你是某种形式的报告人，否则无法保证收到允许注册的黄金票邮件。通常在加拿大举办。'
- en: '**ICML** Short for *International Conference on Machine Learning*, this is
    perhaps the second largest annual conference. This academic conference has several
    tracks and workshops and is typically held in Europe or North America.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**ICML** *International Conference on Machine Learning*（国际机器学习会议）的缩写，可能是第二大年度会议。这个学术会议有多个分会场和工作坊，通常在欧洲或北美举办。'
- en: '**ICLR** The International Conference on Learning Representations is a deep
    learning–focused academic conference. If you want in-the-weeds technical presentations
    on deep learning, this is the place to be.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**ICLR** 国际学习表示会议是一个聚焦深度学习的学术会议。如果你想要深度技术讲解，关于深度学习的专业展示，这就是最好的地方。'
- en: '**CVPR** Computer Vision and Pattern Recognition is another large conference
    that’s perhaps slightly less academic than ICLR. CVPR is popular and not exclusively
    machine learning–oriented.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**CVPR** 计算机视觉与模式识别大会是另一个大型会议，或许比ICLR稍微少些学术性。CVPR非常受欢迎，并且不仅仅是以机器学习为导向。'
- en: '**GTC** The GPU Technology Conference, sponsored by NVIDIA, is a technical
    conference as opposed to an academic conference. The annual presentation of new
    NVIDIA hardware happens here, along with a large expo, in San Jose, California.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**GTC** GPU技术大会，由NVIDIA赞助，是一个技术大会，而非学术会议。每年NVIDIA的新硬件都会在这里展示，并且还会有一个大型展览，地点在加利福尼亚州圣荷西。'
- en: The Book
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这本书
- en: 'Saying there are a few machine learning books out there is like saying there
    are a few fish in the sea. However, as far as deep learning is concerned, one
    stands head-and-shoulders above the rest: *Deep Learning* by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press, 2016). See *[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 说市面上有一些机器学习书籍，就像说海里有一些鱼一样。然而，就深度学习而言，有一本书遥遥领先于其他：由Ian Goodfellow、Yoshua Bengio和Aaron
    Courville合著的*《深度学习》*（MIT出版社，2016年）。请见 *[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)*
- en: '*Deep Learning* is the book you should go to if you want to get serious about
    being a machine learning researcher. Even if you don’t, it covers the key topics
    in depth and with mathematical rigor. The book is not for those looking to get
    better at using one toolkit or another, but for those who want to see the theory
    behind machine learning and the math that goes with it. In essence, it’s an advanced
    undergraduate—if not graduate-level text, but that shouldn’t put you off. At some
    point, you will want to take a look at this book, so keep it in the back of your
    mind—or on your bookshelf.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*是你想要认真成为机器学习研究员时应该参考的书籍。即使你不是，它也深入讲解了关键主题，并且具备数学严谨性。这本书并不是针对那些想提高使用某个工具包技能的人，而是为那些想了解机器学习背后的理论和相关数学的人而写的。实质上，这本书是一本高级本科教材——如果不是研究生级别的话，但这并不应该让你却步。总有一天，你会想看看这本书，所以把它放在脑海里，或者放在你的书架上。'
- en: So Long and Thanks for All the Fish
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 《再见，谢谢所有的鱼》
- en: We’ve reached the end of the book. There’s no monster here, only ourselves,
    and the knowledge and intuition we’ve gained by working through the preceding
    chapters. Thank you for persevering. It’s been fun for me to write; I genuinely
    hope it’s been fun for you to read and contemplate. Don’t stop now—take what we’ve
    developed and run with it. If you’re like me, you’ll see applications for machine
    learning everywhere. Go forth and classify!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走到了这本书的尽头。这里没有怪物，只有我们自己，还有通过前面章节的学习所获得的知识和直觉。感谢你们的坚持。对我来说，写这本书很有趣；我真心希望你阅读和思考这本书时也同样愉快。别停下来——把我们所学到的带走并付诸实践。如果你像我一样，你会在任何地方看到机器学习的应用。走出去，开始分类吧！
