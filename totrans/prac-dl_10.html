<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch10"><span epub:type="pagebreak" id="page_221"/><strong><span class="big">10</span><br/>EXPERIMENTS WITH NEURAL NETWORKS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In <a href="ch09.xhtml#ch09">Chapter 9</a>, we discussed the theory behind neural networks. In this chapter, we’ll trade equations for code and run a number of experiments designed to increase our intuition regarding the essential parameters of neural networks: architecture and activation functions, batch size, base learning rate, training set size, L2 regularization, momentum, weight initialization, feature ordering, and the precision of the weights and biases.</p>&#13;
<p class="indent">To save space and eliminate tedious repetition, we won’t show the specific code for each experiment. In most cases, the code is only trivially different from the previous example; we’re usually changing only the particular argument to the <span class="literal">MLPClassifier</span> constructor we’re interested in. The code for each experiment is included in the set of files associated with this book, and we’ll list the network parameters and the name of the file. When necessary, we’ll provide code to clarify a particular approach. We’ll show the code for the first experiment in its entirety.</p>&#13;
<h3 class="h3" id="lev1_61"><span epub:type="pagebreak" id="page_222"/>Our Dataset</h3>&#13;
<p class="noindent">We’ll be working with the MNIST dataset’s vector form, which we assembled in <a href="ch05.xhtml#ch05">Chapter 5</a>. Recall that this dataset consists of 28×28 pixel 8-bit grayscale images of handwritten digits, [0,9]. In vector form, each 28 × 28 image is unraveled into a vector of 28 × 28 = 784 elements, all bytes ([0,255]). The unraveling lays each row end to end. Therefore, each sample has 784 elements and an associated label. The training set has 60,000 samples, while the test set has 10,000. For our experiments, we won’t use all of the data in the training set. This is to help illustrate the effect of network parameters and to keep our training times reasonable. Refer back to <a href="ch05.xhtml#ch5fig3">Figure 5-3</a> for representative MNIST digits.</p>&#13;
<h3 class="h3" id="lev1_62">The MLPClassifier Class</h3>&#13;
<p class="noindent">The <span class="literal">MLPClassifier</span> class follows the same format as the other sklearn classifiers. There is a constructor and the expected methods: <span class="literal">fit</span> for training, <span class="literal">score</span> for applying the classifier to test data, and <span class="literal">predict</span> to make a prediction on unknown inputs. We’ll also use <span class="literal">predict_proba</span> to return the actual predicted per class probabilities. The constructor has many options:</p>&#13;
<p class="programs">MLPClassifier(hidden_layer_sizes=(100, ), activation='relu',<br/>&#13;
  solver='adam', alpha=0.0001, batch_size='auto',<br/>&#13;
  learning_rate='constant', learning_rate_init=0.001,<br/>&#13;
  power_t=0.5, max_iter=200, shuffle=True,<br/>&#13;
  random_state=None, tol=0.0001, verbose=False,<br/>&#13;
  warm_start=False, momentum=0.9, nesterovs_momentum=True,<br/>&#13;
  early_stopping=False, validation_fraction=0.1, beta_1=0.9,<br/>&#13;
  beta_2=0.999, epsilon=1e-08)</p>&#13;
<p class="indent">Here we’ve provided the default values for each parameter. See the sklearn documentation page at <a href="http://scikit-learn.org/">http://scikit-learn.org/</a> for a complete description of each parameter. We’ll set some of these to specific values, and others will be changed for the experiments while still others are relevant in only specific situations. The key parameters we’ll work with are in <a href="ch10.xhtml#ch10tab1">Table 10-1</a>.</p>&#13;
<p class="indent">The following set of experiments explores the effect of various <span class="literal">MLPClassifier</span> parameters. As mentioned, we’ll show all the code used for the first experiment, understanding that only small changes are needed to perform the other experiments. At times, we’ll show little code snippets to make the change concrete.</p>&#13;
<p class="tabcap" id="ch10tab1"><span epub:type="pagebreak" id="page_223"/><strong>Table 10-1:</strong> Important <span class="literal">MLPClassifier</span> Constructor Keywords and Our Default Values for Them</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Keyword</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Description</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">hidden_layer_sizes</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Tuple giving the hidden layer sizes</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">activation</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Activation function type; for example, ReLU</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">alpha</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">L2 parameter—we called it <em>λ</em> (lambda)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">batch_size</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Minibatch size</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">learning_rate_init</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">The learning rate, <em>η</em> (eta)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">max_iter</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Number of training epochs</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">warm_start</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Continue training or start again</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">momentum</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Momentum</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">solver</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Solver algorithm ("sgd")</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">nesterovs_momentum</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Use Nesterov momentum (False)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">early_stopping</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Use early stopping (False)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">learning_rate</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Learning rate schedule ("constant")</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">tol</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Stop early if loss change &lt; tol (1e-8)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">verbose</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Output to console while training (False)</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<h3 class="h3" id="lev1_63">Architecture and Activation Functions</h3>&#13;
<p class="noindent">When designing a neural network, we immediately face two fundamental questions: what architecture and what activation function? These are arguably the most important deciding factors for a model’s success. Let’s explore what happens when we train a model using different architectures and activation functions while holding the training dataset fixed.</p>&#13;
<h4 class="h4" id="lev2_96">The Code</h4>&#13;
<p class="noindent">As promised, for this first experiment we’ll show the code in its entirety, starting with the helper functions in <a href="ch10.xhtml#ch10lis1">Listing 10-1</a>.</p>&#13;
<p class="programs" id="ch10lis1">import numpy as np<br/>&#13;
import time<br/>&#13;
from sklearn.neural_network import MLPClassifier<br/>&#13;
<br/>&#13;
def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    s = time.time()<br/>&#13;
    (*\pagebreak*)<br/>&#13;
<span epub:type="pagebreak" id="page_224"/>    clf.fit(x_train, y_train)<br/>&#13;
    e = time.time()-s<br/>&#13;
    loss = clf.loss_<br/>&#13;
    weights = clf.coefs_<br/>&#13;
    biases = clf.intercepts_<br/>&#13;
    params = 0<br/>&#13;
    for w in weights:<br/>&#13;
        params += w.shape[0]*w.shape[1]<br/>&#13;
    for b in biases:<br/>&#13;
        params += b.shape[0]<br/>&#13;
    return [clf.score(x_test, y_test), loss, params, e]<br/>&#13;
<br/>&#13;
def nn(layers, act):<br/>&#13;
    return MLPClassifier(solver="sgd", verbose=False, tol=1e-8,<br/>&#13;
            nesterovs_momentum=False, early_stopping=False,<br/>&#13;
            learning_rate_init=0.001, momentum=0.9, max_iter=200,<br/>&#13;
            hidden_layer_sizes=layers, activation=act)</p>&#13;
<p class="figcap"><em>Listing 10-1: Helper functions for experimenting with the architecture and activation function. See</em> mnist_nn_experiments.py.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10lis1">Listing 10-1</a> imports the usual modules and then defines two helper functions, <span class="literal">run</span> and <span class="literal">nn</span>. Starting with <span class="literal">nn</span>, we see that all it does is return an instance of <span class="literal">MLPClassifier</span> using the hidden layer sizes and the given activation function type.</p>&#13;
<p class="indent">The hidden layer sizes are given as a tuple, where each element is the number of nodes in the corresponding layer. Recall that sklearn works with only fully connected layers, so a single number is all we need to specify the size. The input samples given for training determine the size of the input layer. Here the input samples are vectors representing the digit images, so there are 28 × 28 = 784 nodes in the input layer.</p>&#13;
<p class="indent">What about the output layer? It’s not specified explicitly because it depends on the number of classes in the training labels. The MNIST dataset has 10 classes, so there will be 10 nodes in the output layer. When the <span class="literal">predict_proba</span> method is called to get an output probability, sklearn applies a softmax over the 10 outputs. If the model is binary, meaning the only class labels are 0 and 1, then there is only one output node, a logistic (sigmoid), representing the probability of belonging to class 1.</p>&#13;
<p class="indent">Now let’s look at the parameters we passed in to <span class="literal">MLPClassifier</span>. First, we explicitly state that we want to use the SGD solver. The solver is the approach used to modify the weights and biases during training. All the solvers use backprop to calculate the gradients; how we use those gradients varies. Plain vanilla SGD is good enough for us right now.</p>&#13;
<p class="indent">Next, we set a low tolerance so that we’ll train the requested number of epochs (<span class="literal">max_iter</span>). We also turn off Nesterov momentum (a variant of standard momentum) and early stopping (generally useful but not desired here).</p>&#13;
<p class="indent">The initial learning rate is set to the default value of 0.001, as is the value of standard momentum, 0.9. The number of epochs is arbitrarily set to 200 (the default), but we’ll explore this more in the experiments that follow. <span epub:type="pagebreak" id="page_225"/>Please indulge your curiosity at all times and see what changing these values does to things. For consistency’s sake, we’ll use these values as defaults throughout unless they are the parameters we want to experiment with.</p>&#13;
<p class="indent">The other helper function in <a href="ch10.xhtml#ch10lis1">Listing 10-1</a> is <span class="literal">run</span>. This function will train and test the classifier object it’s passed using the standard sklearn <span class="literal">fit</span> and <span class="literal">score</span> methods. It also does some other things that we have not seen before.</p>&#13;
<p class="indent">In particular, after timing how long training takes, we extract the final training loss value, the network weights, and the network biases from the <span class="literal">MLPClassifier</span> object so that we can return them. The <span class="literal">MLPClassifier</span> class minimizes the log-loss, which we described in <a href="ch09.xhtml#ch09">Chapter 9</a>. We store the log-loss in the <span class="literal">loss_</span> member variable. The size of this value, and how it changes during training, gives us a clue as to how well the network is learning. In general, the smaller the log-loss, the better the network is doing. As you explore neural networks more and more, you’ll begin to develop intuition for what a good loss value is and whether the training process is learning quickly or not by how rapidly the loss changes.</p>&#13;
<p class="indent">The weights and biases are stored in the <span class="literal">coefs_</span> and <span class="literal">intercepts_</span> member variables. These are lists of NumPy matrices (weights) and vectors (biases), respectively. Here we use them to calculate the number of parameters in the network by summing the number of elements in each matrix and vector. This is what the two small loops in the <span class="literal">run</span> function do. Finally, we return all this information, including the score against the test set, to the <span class="literal">main</span> function. The <span class="literal">main</span> function is shown in <a href="ch10.xhtml#ch10lis2">Listing 10-2</a>.</p>&#13;
<p class="programs" id="ch10lis2">def main():<br/>&#13;
    x_train = np.load("mnist_train_vectors.npy").astype("float64")/256.0<br/>&#13;
    y_train = np.load("mnist_train_labels.npy")<br/>&#13;
    x_test = np.load("mnist_test_vectors.npy").astype("float64")/256.0<br/>&#13;
    y_test = np.load("mnist_test_labels.npy")<br/>&#13;
<br/>&#13;
    N = 1000<br/>&#13;
    x_train = x_train[:N]<br/>&#13;
    y_train = y_train[:N]<br/>&#13;
    x_test  = x_test[:N]<br/>&#13;
    y_test  = y_test[:N]<br/>&#13;
<br/>&#13;
    layers = [<br/>&#13;
        (1,), (500,), (800,), (1000,), (2000,), (3000,),<br/>&#13;
        (1000,500), (3000,1500),<br/>&#13;
        (2,2,2), (1000,500,250), (2000,1000,500),<br/>&#13;
    ]<br/>&#13;
<br/>&#13;
    for act in ["relu", "logistic", "tanh"]:<br/>&#13;
        print("%s:" % act)<br/>&#13;
        for layer in layers:<br/>&#13;
            scores = []<br/>&#13;
            loss = []<br/>&#13;
            tm = []<br/>&#13;
<span epub:type="pagebreak" id="page_226"/>            for i in range(10):<br/>&#13;
                s,l,params,e = run(x_train, y_train, x_test, y_test,<br/>&#13;
                                   nn(layer,act))<br/>&#13;
                scores.append(s)<br/>&#13;
                loss.append(l)<br/>&#13;
                tm.append(e)<br/>&#13;
            s = np.array(scores)<br/>&#13;
            l = np.array(loss)<br/>&#13;
            t = np.array(tm)<br/>&#13;
            n = np.sqrt(s.shape[0])<br/>&#13;
            print("    layers: %14s, score= %0.4f +/- %0.4f,<br/>&#13;
                  loss = %0.4f +/- %0.4f (params = %6d, time = %0.2f s)" % \<br/>&#13;
                  (str(layer), s.mean(), s.std()/n, l.mean(),<br/>&#13;
                   l.std()/n, params, t.mean()))</p>&#13;
<p class="figcap"><em>Listing 10-2: The <span class="literal">main</span> function for experimenting with the architecture and activation function. See</em> mnist_nn_experiments.py.</p>&#13;
<p class="indent">We first load the MNIST train and test data stored in <span class="literal">x_train</span> (samples) and <span class="literal">y_train</span> (labels), and <span class="literal">x_test</span> and <span class="literal">y_test</span>. Notice that we divide the samples by 256.0 to make them floats in the range [0,1). This normalization is the only preprocessing we’ll do in this chapter.</p>&#13;
<p class="indent">As the full training set has 60,000 samples and we want to run many training sessions, we’ll use only the first 1,000 samples for training. We’ll likewise keep the first 1,000 test samples. Our goal in this chapter is to see relative differences as we change parameters, not to build the best model possible, so we’ll sacrifice the quality of the model to get results in a reasonable timeframe. With 1,000 training samples, we’ll have only 100 instances of each digit type, on average. We’ll vary the number of training samples for specific experiments.</p>&#13;
<p class="indent">The <span class="literal">layers</span> list holds the different architectures we’ll explore. Ultimately, we’ll pass these values to the <span class="literal">hidden_layer_sizes</span> argument of the <span class="literal">MLPClassifier</span> constructor. Notice that we’ll examine architectures ranging from a single hidden layer with a single node to three hidden layers with up to 2,000 nodes per layer.</p>&#13;
<p class="indent">The <span class="literal">main</span> loop runs over three activation function types: rectified linear unit, logistic (sigmoid) unit, and the hyperbolic tangent. We’ll train a model for each combination of activation function type and architecture (<span class="literal">layers</span>). Moreover, since we know neural network training is stochastic, we’ll train 10 models for each combination and report the mean and standard error of the mean, so we’re not thrown off by a particularly bad model that isn’t representative.</p>&#13;
<p class="note"><span epub:type="pagebreak" id="page_227"/><strong><span class="black">Note</span></strong> <em>When you run the code in the experiments that follow, you’ll likely generate warning messages from sklearn like this one:</em></p>&#13;
<p class="programs">ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached<br/>&#13;
and the optimization hasn't converged yet.</p>&#13;
<p class="noindent"><em>The messages are sklearn’s way of telling you that the number of training iterations completed before sklearn felt that the network had converged to a good set of weights.The warnings are safe to ignore and can be disabled completely by adding <span class="literal">-W ignore</span> to the command line when you run the code; for example:</em></p>&#13;
<p class="programs">$ python3 -W ignore mnist_nn_experiments.py</p>&#13;
<h4 class="h4" id="lev2_97">The Results</h4>&#13;
<p class="noindent">Running this code takes several hours to complete, and produces output with lines that look something like this:</p>&#13;
<p class="programs">layers:(3000,1500), score=0.8822+/-0.0007, loss=0.2107+/-0.0006<br/>&#13;
(params=6871510, time=253.42s)</p>&#13;
<p class="indent">This tells us that using a ReLU activation function, and an architecture with two hidden layers of 3,000 and 1,500 nodes each, the models had an average score of 88.2 percent and an average final training loss of 0.21 (remember that lower is better). It also tells us that the neural network had a total of nearly 6.9 million parameters and took, on average, a little more than four minutes to train.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10tab2">Table 10-2</a> summarizes the scores for the various network architectures and activation function types.</p>&#13;
<p class="tabcap" id="ch10tab2"><strong>Table 10-2:</strong> Mean Score (mean ± SE) on the MNIST Test Set as a Function of the Architecture and Activation Function Type</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Architecture</p></th>&#13;
<th style="vertical-align: top"><p class="tab">ReLU</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Tanh</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Logistic (sigmoid)</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2066 ± 0.0046</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2192 ± 0.0047</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1718 ± 0.0118</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">500</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8616 ± 0.0014</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8576 ± 0.0011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6645 ± 0.0029</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">800</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8669 ± 0.0014</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8612 ± 0.0011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6841 ± 0.0030</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8670 ± 0.001</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8592 ± 0.0014</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6874 ± 0.0028</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8682 ± 0.0008</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8630 ± 0.0012</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7092 ± 0.0029</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8691 ± 0.0005</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8652 ± 0.0011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7088 ± 0.0024</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1000; 500</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8779 ± 0.0011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8720 ± 0.0011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1184 ± 0.0033</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3000; 1500</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8822 ± 0.0007</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8758 ± 0.0009</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1221 ± 0.0001</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1000; 500; 250</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8829 ± 0.0011</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8746 ± 0.0012</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1220 ± 0.0000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2000; 1000; 500</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8850 ± 0.0007</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8771 ± 0.0010</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1220 ± 0.0000</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_228"/>In each case, we show the mean score on the reduced test set averaged over the 10 models trained (plus or minus the standard error of the mean). There is quite a bit of information in this table, so let’s look at it carefully.</p>&#13;
<p class="indent">If we look at the activation type, we immediately see something is off. The results for the logistic activation function show improved scores as the single hidden layer gets larger, something we might expect to see, but when we move to more than one hidden layer, the network fails to train. We know that it was unable to train because the scores on the test set are abysmal. If you check the output, you’ll see that the loss values do not go down. If the loss value does not decrease while training proceeds, something is wrong.</p>&#13;
<p class="indent">It’s not immediately evident why training failed for the logistic activation function case. One possibility is a bug in sklearn, but this is rather unlikely given how widely used the toolkit is. The most likely culprit has to do with network initialization. The sklearn toolkit uses the standard, commonly used initialization schemes we discussed in <a href="ch08.xhtml#ch08">Chapter 8</a>. But these are tailored for ReLU and tanh activation functions and may not be performing well for the logistic case.</p>&#13;
<p class="indent">For our purposes, we can view this failure as a glaring sign that the logistic activation function is not a good one to use for the hidden layers. Sadly, this is precisely the activation function that was widely used throughout much of the early history of neural networks, so we were shooting ourselves in the foot from the beginning. No wonder it took so long for neural networks to finally find their proper place! From here on out, we’ll ignore the logistic activation function results.</p>&#13;
<p class="indent">Consider again the scores for the single hidden layer networks (see <a href="ch10.xhtml#ch10tab2">Table 10-2</a>, rows 1–6). For the ReLU and tanh activation functions, we see a steady improvement in the performance of the networks. Also, note that in each case, the ReLU activation function slightly outperforms tanh for the same number of nodes in the hidden layer, though these differences are likely not statistically significant with only 10 models per architecture. Still, it follows a general observation prevalent in the community: ReLU is preferred to tanh.</p>&#13;
<p class="indent">If we look at the remaining rows of <a href="ch10.xhtml#ch10tab2">Table 10-2</a>, we see that adding a second and even third hidden layer continues to improve the test scores but with diminishing returns. This is also a widely experienced phenomenon that we should look at a little more closely. In particular, we should consider the number of parameters in the models of <a href="ch10.xhtml#ch10tab2">Table 10-2</a>. This makes the comparison a bit unfair. If, instead, we train models that have closely matched <span epub:type="pagebreak" id="page_229"/>numbers of parameters, then we can more fairly compare the performance of the models. Any differences in performance we see can be plausibly attributed to the number of layers used since the overall number of parameters will be virtually the same.</p>&#13;
<p class="indent">By modifying the layers array in <a href="ch10.xhtml#ch10lis2">Listing 10-2</a>, we can train multiple versions of the architectures shown in <a href="ch10.xhtml#ch10tab3">Table 10-3</a>. The number of nodes per layer was selected to parallel the overall number of parameters in the models.</p>&#13;
<p class="tabcap" id="ch10tab3"><strong>Table 10-3:</strong> Model Architectures Tested to Produce <a href="ch10.xhtml#ch10fig1">Figure 10-1</a></p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Architecture</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Number of parameters</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">795,010</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1,590,010</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">4000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3,180,010</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">8000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,360,010</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">700; 350</p></td>&#13;
<td style="vertical-align: top"><p class="tab">798,360</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1150; 575</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1,570,335</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1850; 925</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3,173,685</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2850; 1425</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,314,185</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">660; 330; 165</p></td>&#13;
<td style="vertical-align: top"><p class="tab">792,505</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1080; 540; 270</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1,580,320</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1714; 857; 429</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3,187,627</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">2620; 1310; 655</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,355,475</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Where did the magic numbers in <a href="ch10.xhtml#ch10tab3">Table 10-3</a> come from? We first picked the single-layer sizes we wanted to test. We then determined the number of parameters in models with those architectures. Next, we crafted two-layer architectures using the rules of thumb from <a href="ch08.xhtml#ch08">Chapter 8</a> so that the number of parameters in those models will be close to the corresponding number of parameters in the single-layer models. Finally, we repeated the process for three-layer models. Doing things this way lets us compare the performance of the models for very similar numbers of parameters. In essence, we’re fixing the number of parameters in the model and altering only the way they interact with each other.</p>&#13;
<p class="indent">Training models as we did in <a href="ch10.xhtml#ch10lis2">Listing 10-2</a>, but this time averaging 25 models instead of just 10, gives us <a href="ch10.xhtml#ch10fig1">Figure 10-1</a>.</p>&#13;
<div class="image" id="ch10fig1"><span epub:type="pagebreak" id="page_230"/><img src="Images/10fig01.jpg" alt="image" width="656" height="480"/></div>&#13;
<p class="figcap"><em>Figure 10-1: Scores (mean ± E) on the MNIST test set for the architectures of <a href="ch10.xhtml#ch10tab3">Table 10-3</a> as a function of the number of parameters in the network</em></p>&#13;
<p class="indent">Let’s parse <a href="ch10.xhtml#ch10fig1">Figure 10-1</a>. First, note that the x-axis, the number of parameters in the model, is given in millions. Second, we can compare the three lines going vertically as those models all have similar numbers of parameters. The legend tells us which plot represents models with one, two, or three hidden layers.</p>&#13;
<p class="indent">Looking at the leftmost points, representing the smallest models in each case, we see that changing from a single layer to two layers gives us a jump in model performance. Also, moving from two layers to three results in another, smaller rise. This repeats for all the layer sizes moving left to right. We’ll address the dip in performance between the two largest models for single- and double-layer architectures in a bit. Fixing the number of parameters but increasing the depth of the network (number of layers) results in better performance. We might be tempted here to say, “Go deep, not wide,” but there will be cases where this doesn’t work. Still, it’s worth remembering: more layers can help, not just a wider layer with more nodes.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_231"/>What about the dip for the largest models in the one- and two-layer cases? These are the rightmost points of <a href="ch10.xhtml#ch10fig1">Figure 10-1</a>. Recall, the models used to make the plot were trained with only 1,000 samples each. For the largest models, there likely wasn’t enough data to adequately train such a wide model. If we were to increase the number of training samples, which we can do because we have 60,000 to choose from for MNIST, we might see the dip go away. I’ll leave this as an exercise for the reader.</p>&#13;
<h3 class="h3" id="lev1_64">Batch Size</h3>&#13;
<p class="noindent">Let’s now turn our attention to how batch size affects training. Recall that here <em>batch size</em> means minibatch size, a subset of the full training set used in the forward pass to calculate the average loss over the minibatch. From this loss, we use backprop to update the weights and biases. Processing a single minibatch, then, results in a single gradient-descent step—a single update to the parameters of the network.</p>&#13;
<p class="indent">We’ll train a fixed-size subset of MNIST for a set number of epochs with different minibatch sizes to see how that affects the final test scores. Before we do that, however, we need to understand, for epochs and minibatches, the process sklearn uses to train a neural network.</p>&#13;
<p class="indent">Let’s look briefly at the actual sklearn source code for the <span class="literal">MLPClassifier</span> class, in the <span class="literal">_fit_stochastic</span> method, found at <a href="https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py">https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py</a>. Understanding that this method is an internal one and might change from version to version, we see code that looks like this:</p>&#13;
<p class="programs">for it in range(self.max_iter):<br/>&#13;
  X, y = shuffle(X, y, random_state=self._random_state)<br/>&#13;
  accumulated_loss = 0.0<br/>&#13;
  for batch_slice in gen_batches(n_samples, batch_size):<br/>&#13;
    activations[0] = X[batch_slice]<br/>&#13;
    batch_loss, coef_grads, intercept_grads = self._backprop(<br/>&#13;
      X[batch_slice], y[batch_slice], activations, deltas,<br/>&#13;
      coef_grads, intercept_grads)<br/>&#13;
    accumulated_loss += batch_loss * (batch_slice.stop -<br/>&#13;
                                      batch_slice.start)<br/>&#13;
    grads = coef_grads + intercept_grads<br/>&#13;
    self._optimizer.update_params(grads)<br/>&#13;
  self.n_iter_ += 1</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_232"/>There are two <span class="literal">for</span> loops, the first over the number of epochs (<span class="literal">max_iter</span>), and the second over the number of minibatches present in the training data. The <span class="literal">gen_batches</span> function returns minibatches from the training set. In reality, it returns slice indices with <span class="literal">X[batch_slice]</span> returning the actual training samples, but the effect is the same. The calls to <span class="literal">_backprop</span> and <span class="literal">update_params</span> complete the gradient descent step for the current minibatch.</p>&#13;
<p class="indent">An <em>epoch</em> is a full pass through the minibatches present in the training set. The minibatches themselves are groupings of the training data so that looping over the minibatches uses all the samples in the training set once. If the number of training samples is not an integer multiple of the minibatch size, the final minibatch will be smaller than expected, but that will not affect training in the long run.</p>&#13;
<p class="indent">We can view this graphically as in <a href="ch10.xhtml#ch10fig2">Figure 10-2</a>, where we see how an epoch is built from the minibatches in the training set. In <a href="ch10.xhtml#ch10fig2">Figure 10-2</a>, the entire training set is represented as the epoch with <em>n</em> samples. A minibatch has <em>m</em> samples, as indicated. The last minibatch is smaller than the rest to indicate that the <em>n</em>/<em>m</em> might not be an integer.</p>&#13;
<div class="image" id="ch10fig2"><img src="Images/10fig02.jpg" alt="image" width="680" height="217"/></div>&#13;
<p class="figcap"><em>Figure 10-2: The relationship between epochs (</em>n<em>), minibatches (</em>m<em>), and samples</em> {x<sub><em>0</em></sub>,x<sub><em>1</em></sub>, …,x<sub><em>n-1</em></sub>}</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig2">Figure 10-2</a> also implies that the order of the samples in the training set is essential, which is why we shuffled the datasets when we made them. The sklearn toolkit will also rearrange the samples after every epoch during training if desired. As long as a minibatch is, statistically, a random sample from the training set as a whole, things should be okay. If the minibatch is not, then it might give a biased view of the gradient direction during backprop.</p>&#13;
<p class="indent">Our minibatch experiment will fix the number of MNIST training samples at 16,384 while we vary the minibatch size. We’ll also fix the number of epochs at 100. The scores we report are the mean and standard error for <span epub:type="pagebreak" id="page_233"/>five different runs of the same model, each with a different random initialization. The <span class="literal">MLPClassifier</span> object is therefore instantiated via</p>&#13;
<p class="programs">MLPClassifier(solver="sgd", verbose=False, tol=1e-8,<br/>&#13;
     nesterovs_momentum=False, early_stopping=False,<br/>&#13;
     learning_rate_init=0.001, momentum=0.9, max_iter=100,<br/>&#13;
     hidden_layer_sizes=(1000,500), activation="relu",<br/>&#13;
     batch_size=bz)</p>&#13;
<p class="indent">This code indicates that all of the models have two hidden layers of 1,000 and 500 nodes, respectively, making the architecture of the entire network 784-1000-500-10 when adding in the nodes of the input and output layers. The only parameter that varies when defining a network is the <span class="literal">batch_size</span>. We’ll use the batch sizes in <a href="ch10.xhtml#ch10tab4">Table 10-4</a> along with the number of gradient descent steps taken for each epoch (see <a href="ch10.xhtml#ch10fig2">Figure 10-2</a>).</p>&#13;
<p class="tabcap" id="ch10tab4"><strong>Table 10-4:</strong> Minibatch Sizes and the Corresponding Number of Gradient Descent Steps per Epoch</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Minibatch size</p></th>&#13;
<th style="vertical-align: top"><p class="tab">SGD steps per epoch</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8,192</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4,096</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2,048</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">16</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1,024</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">32</p></td>&#13;
<td style="vertical-align: top"><p class="tab">512</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">64</p></td>&#13;
<td style="vertical-align: top"><p class="tab">256</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">128</p></td>&#13;
<td style="vertical-align: top"><p class="tab">128</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">256</p></td>&#13;
<td style="vertical-align: top"><p class="tab">64</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">512</p></td>&#13;
<td style="vertical-align: top"><p class="tab">32</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1,024</p></td>&#13;
<td style="vertical-align: top"><p class="tab">16</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2,048</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">4,096</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">8,192</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">16,384</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">When the minibatch size is 2, over 8,000 gradient descent steps will be taken per epoch, but when the minibatch size is 8,192, only 2 gradient descent steps are taken. Fixing the number of epochs should favor a smaller <span epub:type="pagebreak" id="page_234"/>minibatch size since there will be correspondingly more gradient descent steps, implying more opportunity to move toward the optimal set of network parameters.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig3">Figure 10-3</a> plots the mean score as a function of the minibatch size. The code that generated the data for the plot is in the <em>mnist_nn_experiments _batch_size.py</em> file. The plotting code itself is in <em>mnist_nn_experiments_batch _size_plot.py</em>. The curve that concerns us for the moment is the one using circles. We’ll explain the square symbol curve shortly.</p>&#13;
<div class="image" id="ch10fig3"><img src="Images/10fig03.jpg" alt="image" width="656" height="482"/></div>&#13;
<p class="figcap"><em>Figure 10-3: Average score on the MNIST test set as a function of minibatch size (mean</em> ± <em>SE) for a fixed number of epochs (100) regardless of the minibatch size (circles) or a fixed number of minibatches (squares)</em></p>&#13;
<p class="indent">Here we’ve fixed the number of epochs at 100, so by varying the minibatch size, we vary the number of gradient steps: the larger the minibatch, the <em>fewer</em> gradient steps we take. Because the minibatch is larger, the steps themselves are based on a more faithful representation of the actual gradient direction; however, the number of steps is reduced because there are fewer minibatches per epoch, leading to poorer convergence: we are not reaching a good minimum of the loss function.</p>&#13;
<p class="indent">A more “fair” test might be to see what happens when we adjust the number of epochs so that the number of <em>minibatches</em> examined is constant regardless of the minibatch size. One way to do that is to note that the number of minibatches per epoch is <em>n</em>/<em>m</em>, where <em>n</em> is the number of training <span epub:type="pagebreak" id="page_235"/>samples, and <em>m</em> is the number of minibatches. If we call the overall number of minibatches we want to run <em>M</em>, then, to hold it fixed, we need to set the number of <em>epochs</em> to</p>&#13;
<div class="imagec"><img src="Images/235equ01.jpg" alt="image" width="73" height="43"/></div>&#13;
<p class="noindent">so that regardless of <em>m</em>, we perform a total of <em>M</em> gradient descent steps during training.</p>&#13;
<p class="indent">Let’s keep the same set of minibatches but alter the number of epochs according to the preceding equation. We need to select <em>M</em>, the overall number of minibatches (gradient descent steps). Let’s set it to <em>M</em> = 8,192 so that the number of epochs is an integer in each case. When the minibatch size is 2, we use one epoch to get 8,192 minibatches. And when the minibatch size is 16,384 (<em>n</em> is still also 16,384 samples), we get 8,192 epochs. If we do this, we get a completely different set of results, the square symbol curve in <a href="ch10.xhtml#ch10fig3">Figure 10-3</a>, where we see that the mean score is pretty much a constant representing the constant number of gradient descent updates performed during training. When the minibatch size is small, corresponding to points near 0 in <a href="ch10.xhtml#ch10fig3">Figure 10-3</a>, we do see a degradation in performance, but after a certain minibatch size, the performance levels off, reflecting the constant number of gradient descent updates combined with a reasonable estimate of the true gradient from using a large enough minibatch.</p>&#13;
<p class="indent">For the set of base neural network parameters, specifically for a fixed learning rate, fixing the number of epochs results in reduced performance because of the design of sklearn. Fixing the number of minibatches examined results in mainly constant performance.</p>&#13;
<h3 class="h3" id="lev1_65">Base Learning Rate</h3>&#13;
<p class="noindent">In <a href="ch09.xhtml#ch09">Chapter 9</a>, we introduced the basic equation for updating the weights of a neural network during training:</p>&#13;
<p class="center"><em>w</em> ← <em>w</em> – <em>η</em>Δ<em>w</em></p>&#13;
<p class="noindent">Here <em>η</em> (eta) is the learning rate, the parameter that controls the step size based on the gradient value, <em>Δw</em>. In sklearn, <em>η</em> is specified via the <span class="literal">learning</span> <span class="literal">_rate_init</span> parameter. During training, the learning rate is often reduced, so that the step sizes get smaller the closer we get to the training minimum (hopefully!). For our experiments here, however, we’re using a constant learning rate, so whatever value we set <span class="literal">learning_rate_init</span> to persists throughout the entire training session. Let’s see how this value affects learning.</p>&#13;
<p class="indent">For this experiment, we fix the minibatch size at 64 samples and the architecture to (1000,500), meaning two hidden layers with 1,000 and 500 nodes, respectively. We then look at two main effects. The first is what we <span epub:type="pagebreak" id="page_236"/>get when we fix the number of epochs regardless of the base learning rate. In this case, we’ll always take a set number of gradient descent steps during training. The second case fixes the <em>product</em> of the base learning rate and the number of epochs. This case is interesting because it looks at the effect on the test score of fewer large steps versus many small steps. The code for these experiments is in <em>mnist_experiments_base_lr.py</em>. The training set is the first 20,000 MNIST samples.</p>&#13;
<p class="indent">The first experiment fixes the epochs at 50 and loops over different base learning rates:</p>&#13;
<p class="programs">[0.2, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]</p>&#13;
<p class="indent">The second uses the same base learning rates but varies the number of epochs so that in each case the product of the base learning rate and epochs is 1.5. This leads to the following number of epochs matched to the preceding base learning rates:</p>&#13;
<p class="programs">[8, 15, 30, 150, 300, 1500, 3000, 15000]</p>&#13;
<p class="indent">Running the two experiments takes some time. When they’re complete, we can plot the test score as a function of the base learning rate size. Doing this gives us <a href="ch10.xhtml#ch10fig4">Figure 10-4</a>.</p>&#13;
<div class="image" id="ch10fig4"><img src="Images/10fig04.jpg" alt="image" width="649" height="475"/></div>&#13;
<p class="figcap"><em>Figure 10-4: MNIST test scores as a function of the base learning rate. The circles represent the fixed epochs case. The squares are the fixed product of the base learning rate and the epochs case.</em></p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig4">Figure 10-4</a> shows two plots. In the first plot, using circles, the number of epochs was fixed at 50. Fixing the number of epochs fixes the number of <span epub:type="pagebreak" id="page_237"/>gradient descent steps taken during training. We then vary the learning rate. The larger the learning rate, the bigger the steps we’ll take.</p>&#13;
<p class="indent">Imagine walking over a football field, attempting to get to the very center from one of the corners in a limited number of steps. If we take large steps, we might move over a lot of ground quickly, but we won’t be able to zero in on the center because we’ll keep stepping past it. If we take tiny steps, we’ll cover only a short distance from the corner toward the center. We might be on track, but since we’re allowed only a certain number steps, we can’t reach the center. Intuitively, we can perhaps convince ourselves that there is a sweet spot where the step size and the number of steps we get to take combine to get us to the center.</p>&#13;
<p class="indent">We see this effect in the circle plot of <a href="ch10.xhtml#ch10fig4">Figure 10-4</a>. The leftmost point represents the case of tiny steps. We do relatively poorly because we haven’t traversed enough of the error space to find the minimum. Similarly, the rightmost point represents taking very large steps. We do poorly because we keep stepping past the minimum. The best score happens when the number of steps we get to make and the size of those steps work together to move us to the minimum. In the figure, this happens when the base learning rate is 0.1.</p>&#13;
<p class="indent">Now let’s look at the square symbol plot in <a href="ch10.xhtml#ch10fig4">Figure 10-4</a>. This plot comes from the scores found when the product of the base learning rate and the number of epochs is constant, meaning small learning rates will run for a large number of epochs. For the most part, the test scores are the same for all base learning rates except the very largest. In our walking over the football field thought experiment, the square symbol plot corresponds to taking a few large steps or very many small steps. We can imagine both approaches getting us close to the center of the field, at least until our step size is too large to let us land at the center.</p>&#13;
<p class="indent">Some readers might be objecting at this point. If we compare the first three points of both the circle and square plots in <a href="ch10.xhtml#ch10fig4">Figure 10-4</a>, we see a large gap. For the circles, the performance improves as the base learning rate increases. For the squares, however, the performance remains high and constant regardless of the base learning rate. For the circles, we trained for 50 epochs, always. This is a more significant number of epochs than were used for the squares plot for the corresponding base learning rates. This means that in the circles’ case, we stomped around quite a bit after we got near the center of the field. For the case of the squares, however, we limited the number of epochs, so we stopped walking when we were near the center of the field, hence the improved performance. This implies that we need to adjust the number of epochs (gradient descent steps taken) to match the learning rate so that we get near to the minimum of the loss function quickly, without a lot of stomping around, but not so quickly that we are taking large steps that won’t let us converge on the minimum.</p>&#13;
<p class="indent">Thus far we’ve been holding the learning rate constant throughout training. Because of space considerations, we can’t fully explore the effect of changing the learning rate during training. Still, we can at least use our football field thought experiment to help us visualize why changing the <span epub:type="pagebreak" id="page_238"/>learning rate during training makes sense. Recall, the network is initialized intelligently but randomly. This means we start somewhere on the field at random. The odds are low that this arbitrary position is near the center, the minimum of the error surface, so we do need to apply gradient descent to move us closer to the center. At first, we might as well take significant steps to move quickly through the field. Since we are following the gradient, this moves us toward the center. If we keep taking large steps, however, we might overshoot the center. After taking a few large steps, we might think it wise to start taking smaller steps, believing that we are now closer to our goal of reaching the center. The more we walk, the smaller our steps so we can get as close to the center as possible. This is why the learning rate is typically reduced during training.</p>&#13;
<h3 class="h3" id="lev1_66">Training Set Size</h3>&#13;
<p class="noindent">We’ve mentioned that the number of samples in the training set affects performance significantly. Let’s use the MNIST data to quantify this assertion. For this experiment, we’ll vary the number of training set samples while adjusting the number of epochs so that in each case, we take (approximately) 1,000 gradient descent steps during training. The code for this experiment is in <em>mnist_nn_experiments_samples.py</em>. In all cases, the minibatch size is 100, and the architecture of the network has two hidden layers of 1,000 and 500 nodes, respectively. <a href="ch10.xhtml#ch10fig5">Figure 10-5</a> shows the results of this experiment.</p>&#13;
<div class="image" id="ch10fig5"><img src="Images/10fig05.jpg" alt="image" width="660" height="485"/></div>&#13;
<p class="figcap"><em>Figure 10-5: MNIST test scores as a function of the number of training samples</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_239"/><a href="ch10.xhtml#ch10fig5">Figure 10-5</a> is particularly satisfying because it shows exactly what we’d expect to see. If we have too little training data, we cannot learn to generalize well because we’re training the model with a very sparse sample from the parent distribution. As we add more and more training data, we’d expect a potentially rapid rise in the performance of the network since the training set is a better and better sample of the parent distribution we’re asking the model to learn.</p>&#13;
<p class="indent"><a href="ch10.xhtml#ch10fig5">Figure 10-5</a> shows that increasing the training set size results in diminishing returns. Moving from 1,000 to 5,000 training set samples results in a substantial improvement in performance, but moving from 5,000 to even 10,000 samples gives us only a small performance boost, and further increases in the training set size level off at some ceiling performance. We can think of this level region as having reached some capacity—that the model has pretty much learned all it will learn from the dataset. At this point, we might think of enlarging the network architecture to see if we get a jump in test set scores provided we have enough training samples available.</p>&#13;
<h3 class="h3" id="lev1_67">L2 Regularization</h3>&#13;
<p class="noindent">In <a href="ch09.xhtml#ch09">Chapter 9</a>, we discussed regularization techniques that improve network generalization, including L2 regularization. We saw that L2 regularization, which adds a new term to the loss function during training, is functionally equivalent to weight decay and penalizes the network during training if the weights get large.</p>&#13;
<p class="indent">In sklearn, the parameter controlling the strength of L2 regularization is <span class="literal">alpha</span>. If this parameter is 0, there is no L2 regularization, while the regularization increases in intensity as <span class="literal">alpha</span> increases. Let’s explore the effect of L2 regularization on our MNIST networks.</p>&#13;
<p class="indent">For this experiment, we’ll fix the minibatch size at 64. We’ll also set the momentum to 0 so that the effect we see is due to L2 regularization alone. Finally, we’ll use a smaller network with two hidden layers of 100 and 50 nodes each and a small training set of the first 3,000 MNIST samples. The code is in <em>mnist_nn_experiments_L2.py</em>.</p>&#13;
<p class="indent">Unlike the previous experiments, in this case, we’d like to evaluate the test data after each training epoch so that we can watch the network learn over the training process. If it is learning, the error on the test set will go down as the number of training epochs increases. We know that sklearn will loop over all the minibatches in the dataset for one epoch, so we can set the number of training epochs to 1. However, if we set <span class="literal">max_iter</span> to 1 and then call the <span class="literal">fit</span> method, the next time we call <span class="literal">fit</span>, we’ll start over with a newly initialized network. This won’t help us at all; we need to preserve the weights and biases between calls to <span class="literal">fit</span>.</p>&#13;
<p class="indent">Fortunately for us, the creators of sklearn thought ahead and added the <span class="literal">warm_start</span> parameter. If this parameter is set to <span class="literal">True</span>, a call to <span class="literal">fit</span> will <em>not</em> re-initialize the network but will use the existing weights and biases. If we set <span epub:type="pagebreak" id="page_240"/><span class="literal">max_iter</span> to 1 and <span class="literal">warm_start</span> to <span class="literal">True</span>, we’ll be able to watch the network learn by calling <span class="literal">score</span> after each epoch of training. Calling <span class="literal">score</span> gives us the accuracy on the test data. If we want the error, the value we need to track is 1 – <span class="literal">score</span>. This is the value we plot as a function of epoch. The <span class="literal">alpha</span> values we’ll plot are</p>&#13;
<p class="programs">[0.0, 0.1, 0.2, 0.3, 0.4]</p>&#13;
<p class="indent">We’ve made these rather large compared to the default so we can see the effect.</p>&#13;
<p class="indent">Focusing on the test error only, the code for evaluating a single epoch is:</p>&#13;
<p class="programs">def epoch(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    clf.fit(x_train, y_train)<br/>&#13;
    val_err = 1.0 - clf.score(x_test, y_test)<br/>&#13;
    clf.warm_start = True<br/>&#13;
    return val_err</p>&#13;
<p class="noindent">Here, <span class="literal">fit</span> is called to perform one epoch of training. Then we calculate the error on the test set and store it in <span class="literal">val_err</span>. Setting <span class="literal">warm_start</span> to <span class="literal">True</span> after calling <span class="literal">fit</span> ensures that the first call to <span class="literal">epoch</span> will properly initialize the network, but subsequent calls will keep the weights and biases from the previous call.</p>&#13;
<p class="indent">Training then happens in a simple loop:</p>&#13;
<p class="programs">def run(x_train, y_train, x_test, y_test, clf, epochs):<br/>&#13;
    val_err = []<br/>&#13;
    clf.max_iter = 1<br/>&#13;
    for i in range(epochs):<br/>&#13;
        verr = epoch(x_train, y_train, x_test, y_test, clf)<br/>&#13;
        val_err.append(verr)<br/>&#13;
    return val_err</p>&#13;
<p class="indent">This loop collects the per epoch results and returns them to the <span class="literal">main</span> function, which itself loops over the <em>α</em> values we’re interested in.</p>&#13;
<p class="indent">Let’s run this code and plot <span class="literal">val_err</span>, the test error, as a function of the number of epochs for each <span class="literal">alpha</span>. <a href="ch10.xhtml#ch10fig6">Figure 10-6</a> is the result.</p>&#13;
<div class="image" id="ch10fig6"><img src="Images/10fig06.jpg" alt="image" width="653" height="491"/></div>&#13;
<p class="figcap"><em>Figure 10-6: MNIST test error as a function of training epoch for different values of α</em></p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_241"/>The first thing we notice in <a href="ch10.xhtml#ch10fig6">Figure 10-6</a> is that any nonzero value for <em>α</em> produces a lower test error compared to not using L2 regularization at all. We can conclude that L2 regularization is helpful. The different <em>α</em> values all result in approximately the same test error, but larger values are slightly more effective and reach a lower test error sooner. Compare <em>α</em> = 0.1 to <em>α</em> = 0.4, for example.</p>&#13;
<p class="indent">Notice that larger <em>α</em> values seem noisier: the plot is thicker as the error jumps around more, relative to the smaller <em>α</em> values. To understand this, think about the total loss minimized during training. When <em>α</em> is large, we’re placing more importance on the L2 term relative to the network’s error over the minibatch. This means that when we ask the network to adjust the weights and biases during backprop, it’ll be more strongly affected by the magnitude of the parameters of the network than the training data itself. Because the network is focusing less on reducing the loss due to the training data, we might expect the per epoch test error to vary more.</p>&#13;
<h3 class="h3" id="lev1_68"><span epub:type="pagebreak" id="page_242"/>Momentum</h3>&#13;
<p class="noindent">Momentum alters the weight update during training by adding in a fraction of the gradient value used to update the weight in the previous minibatch. The fraction is specified as a multiplier on the previous gradient value, [0,1]. We covered momentum in <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<p class="indent">Let’s see how changing this parameter affects training. In this case, the setup for the experiment is simple. It’s identical to the approach used previously for L2 regularization, but instead of fixing the momentum parameter (<em>μ</em>) and varying the L2 weight (<em>α</em>), we’ll fix <em>α</em> = 0.0001 and vary <em>μ</em>. All the other parts remain the same: training by single epochs, the network configuration, and so forth. See the file <em>mnist_nn_experiments_momentum.py</em>.</p>&#13;
<p class="indent">We’ll explore these momentum values:</p>&#13;
<p class="programs">[0.0, 0.3, 0.5, 0.7, 0.9, 0.99]</p>&#13;
<p class="indent">They range from no momentum term (<em>μ</em> = 0) to a large momentum term (<em>μ</em> = 0.99). Running the experiment produces <a href="ch10.xhtml#ch10fig7">Figure 10-7</a>.</p>&#13;
<p class="indent">In <a href="ch10.xhtml#ch10fig7">Figure 10-7</a>, we see three distinct regions. The first, represented by no momentum or relatively small momentum values (<em>μ</em> = 0.3, <em>μ</em> = 0.5), shows the highest test set error. The second shows improvement with moderate momentum values (<em>μ</em> = 0.7, <em>μ</em> = 0.9), including the “standard” (sklearn default) value of 0.9. In this case, however, a large momentum of 0.99 lowers the test set error from about 7.5 percent to about 6 percent. Momentum helps and should be used, especially with values near the standard of 0.9. In practice, people seldom seem to alter the momentum much, but as this example shows, sometimes it makes a big difference to the results.</p>&#13;
<div class="image" id="ch10fig7"><img src="Images/10fig07.jpg" alt="image" width="653" height="484"/></div>&#13;
<p class="figcap"><em>Figure 10-7: MNIST test error as a function of training epoch for different values of μ</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_243"/>Note that we severely limited the training set to a mere 3,000 samples, about 300 per digit, which likely made momentum matter more because the training set was a small and less complete of a sample of the parent distribution we want the model to learn. Increasing the training set size to 30,000 results in a different, and more typical, ordering of the plot, where a momentum of 0.9 is the best option.</p>&#13;
<h3 class="h3" id="lev1_69">Weight Initialization</h3>&#13;
<p class="noindent">Once treated rather cavalierly, the initial set of values used for the weights and biases of a network is now known to be extremely important. The simple experiment of this section shows this plainly.</p>&#13;
<p class="indent">The sklearn toolkit initializes the weights and biases of a neural network by calling the <span class="literal">_init_coef</span> method of the <span class="literal">MLPClassifier</span> class. This method selects weights and biases randomly according to the Glorot algorithm we discussed in <a href="ch09.xhtml#ch09">Chapter 9</a>. This algorithm sets the weights and biases to values sampled uniformly from the range</p>&#13;
<div class="imagec"><img src="Images/243equ01.jpg" alt="image" width="237" height="64"/></div>&#13;
<p class="noindent">where <em>f</em><sub><em>in</em></sub> is the number of inputs and <em>f</em><sub><em>out</em></sub> is the number of outputs for the current layer being initialized. If the activation function is a sigmoid, <em>A</em> = 2; otherwise, <em>A</em> = 6.</p>&#13;
<p class="indent">If we play a little trick, we can change the way that sklearn initializes the network and thereby experiment with alternative initialization schemes. The trick uses Python’s object-oriented programming abilities. If we make a subclass of <span class="literal">MLPClassifier</span>, let’s call it simply <span class="literal">Classifier</span>, we can override the <span class="literal">_init_coef</span> method with our own. Python also allows us to add new member variables to a class instance arbitrarily, which gives us all we need.</p>&#13;
<p class="indent">The remainder of the experiment follows the format of the previous sections. We’ll ultimately plot the test error by epoch of the MNIST digits trained on a subset of the full data for different initialization approaches. The model itself will use the first 6,000 training samples, a minibatch size of 64, a constant learning rate of 0.01, a momentum of 0.9, an L2 regularization parameter of 0.2, and an architecture with two hidden layers of 100 and 50 nodes each. See <em>mnist_nn_experiments_init.py</em> for this experiment’s code.</p>&#13;
<p class="indent">We’ll test four new weight initialization schemes along with the standard Glorot approach of sklearn. The schemes are shown in <a href="ch10.xhtml#ch10tab5">Table 10-5</a>.</p>&#13;
<p class="tabcap" id="ch10tab5"><span epub:type="pagebreak" id="page_244"/><strong>Table 10-5:</strong> Weight Initialization Schemes</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:40%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Name</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Equation</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Description</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Glorot</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="middle"><img src="Images/244equ01.jpg" alt="Image" width="250" height="52"/></span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">sklearn default</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">He</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="middle"><img src="Images/244equ02.jpg" alt="Image" width="93" height="38"/></span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">He initialization for ReLU</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Xavier</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><span class="middle"><img src="Images/244equ03.jpg" alt="Image" width="125" height="51"/></span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">Alternate Xavier</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Uniform</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.01(<em>U</em>(0,1)-0.5)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Classic small uniform</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Gaussian</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.005<em>N</em>(0,1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Classic small Gaussian</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">Recall that <em>N</em>(0,1) refers to a sample from a bell curve with a mean of 0 and a standard deviation of 1 while <em>U</em>(0,1) refers to a sample drawn uniformly from [0,1), meaning all values in that range are equally likely except 1.0. Each of the new initialization methods sets the bias values to 0, always. However, sklearn’s Glorot implementation sets the bias values in the same way it sets the weights.</p>&#13;
<p class="note"><strong><span class="black">Note</span></strong> <em>As mentioned in <a href="ch09.xhtml#ch09">Chapter 9</a> , both</em> Xavier <em>and</em> Glorot <em>refer to the same person, Xavier Glorot. We’re differentiating here because the form we’re calling</em> Xavier <em>is referred to as such in other machine learning toolkits like Caffe, and the equation used is different from the equation used in the original paper.</em></p>&#13;
<p class="indent">This all sounds nice and neat, but how to implement it in code? First, we define a new Python class, <span class="literal">Classifier</span>, which is a subclass of <span class="literal">MLPClassifier</span>. As a subclass, the new class immediately inherits all the functionality of the superclass (<span class="literal">MLPClassifier</span>) while allowing us the freedom to override any of the superclass methods with our own implementation. We simply need to define our own version of <span class="literal">_init_coef</span> with the same arguments and return values. In code, it looks like this:</p>&#13;
<p class="programs">class Classifier(MLPClassifier):<br/>&#13;
    def _init_coef(self, fan_in, fan_out):<br/>&#13;
        if (self.init_scheme == 0):<br/>&#13;
            return super(Classifier, self)._init_coef(fan_in, fan_out)<br/>&#13;
        elif (self.init_scheme == 1):<br/>&#13;
            weights = 0.01*(np.random.random((fan_in, fan_out))-0.5)<br/>&#13;
            biases = np.zeros(fan_out)<br/>&#13;
        elif (self.init_scheme == 2):<br/>&#13;
            weights = 0.005*(np.random.normal(size=(fan_in, fan_out)))<br/>&#13;
            biases = np.zeros(fan_out)<br/>&#13;
        elif (self.init_scheme == 3):<br/>&#13;
            weights = np.random.normal(size=(fan_in, fan_out))*  \<br/>&#13;
                        np.sqrt(2.0/fan_in)<br/>&#13;
            biases = np.zeros(fan_out)<br/>&#13;
        elif (self.init_scheme == 4):<br/>&#13;
            weights = np.random.normal(size=(fan_in, fan_out))*  \<br/>&#13;
<span epub:type="pagebreak" id="page_245"/>                        np.sqrt(1.0/fan_in)<br/>&#13;
            biases = np.zeros(fan_out)</p>&#13;
<p class="indent">The initialization we perform depends on the value of <span class="literal">init_scheme</span>. This is a new member variable that we use to select the initialization method (see <a href="ch10.xhtml#ch10tab6">Table 10-6</a>).</p>&#13;
<p class="tabcap" id="ch10tab6"><strong>Table 10-6:</strong> Initialization Scheme and <span class="literal">init_scheme</span> Value</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Value</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Initialization method</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">sklearn default</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Classic small uniform</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Classic small Gaussian</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">He initialization</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Alternate Xavier</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">We set the variable immediately after creating the <span class="literal">Classifier</span> object.</p>&#13;
<p class="indent">We know that training a network more than once results in slightly different performance because of the way the network is initialized. Therefore, training a single network for each initialization type will likely lead to a wrong view of how well the initialization performs because we might hit a bad set of initial weights and biases. To mitigate this, we need to train multiple versions of the network and report the average performance. Since we want to plot the test error as a function of the training epoch, we need to track the test error at each epoch for each training of each initialization scheme. This suggests a three-dimensional array:</p>&#13;
<p class="programs">test_err = np.zeros((trainings, init_types, epochs))</p>&#13;
<p class="indent">We have <span class="literal">trainings</span> trainings of each initialization type (<span class="literal">init_types</span>) for a maximum of <span class="literal">epochs</span> epochs.</p>&#13;
<p class="indent">With all of this in place, the generation and storage of the actual experiment output is straightforward, if rather slow, taking the better part of a day to run:</p>&#13;
<p class="programs">for i in range(trainings):<br/>&#13;
    for k in range(init_types):<br/>&#13;
        nn = Classifier(solver="sgd", verbose=False, tol=0,<br/>&#13;
               nesterovs_momentum=False, early_stopping=False,<br/>&#13;
               learning_rate_init=0.01, momentum=0.9,<br/>&#13;
               hidden_layer_sizes=(100,50), activation="relu", alpha=0.2,<br/>&#13;
               learning_rate="constant", batch_size=64, max_iter=1)<br/>&#13;
        nn.init_scheme = k<br/>&#13;
        test_err[i,k,:] = run(x_train, y_train, x_test, y_test, nn, epochs)<br/>&#13;
np.save("mnist_nn_experiments_init_results.npy", test_err)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_246"/>Here <span class="literal">nn</span> is the classifier instance to train, <span class="literal">init_scheme</span> sets the initialization scheme to use, and <span class="literal">run</span> is the function we defined earlier to train and test the network incrementally.</p>&#13;
<p class="indent">If we set the number of training sessions to 10, the number of epochs to 4,000, and plot the mean test error per epoch, we get <a href="ch10.xhtml#ch10fig8">Figure 10-8</a>.</p>&#13;
<div class="image" id="ch10fig8"><img src="Images/10fig08.jpg" alt="image" width="649" height="483"/></div>&#13;
<p class="figcap"><em>Figure 10-8: MNIST test error as a function of training epoch for different weight initialization methods (mean over 10 training runs)</em></p>&#13;
<p class="indent">Let’s understand what the figure is showing us. The five initialization approaches are marked, each pointing to one of the five curves in the figure. The curves themselves are familiar to us by now; they show the test set error as a function of the training epoch. In this case, the value plotted for each curve is the average over 10 training runs of the same network architecture initialized with the same approach but different random values.</p>&#13;
<p class="indent">We immediately see two distinct groups of results. On the top, we have the test error for the classic initialization approaches using small uniform or normally distributed values (Gaussian). On the bottom, we have the results for the more principled initialization in current use. Even this basic experiment shows the effectiveness of modern initialization approaches quite clearly. Recall, the classic approaches were part of the reason neural networks had a bad name a few decades ago. Networks were finicky and difficult to train in large part because of improper initialization.</p>&#13;
<p class="indent">Looking at the bottom set of results, we see that for this experiment, there is little difference between the sklearn default initialization, which we are calling <em>Glorot</em>, and the initialization approach of He. The two plots are <span epub:type="pagebreak" id="page_247"/>virtually identical. The plot labeled <em>Xavier</em> is slightly worse at first , but toward the end of our training runs matches the other two. Sklearn is using a good initialization strategy.</p>&#13;
<p class="indent">The plot also shows us something else. For the classic initialization approaches, we see the test set error level off and remain more or less constant. For the modern initialization approaches, we observe the test error increase slightly with the training epoch. This is particularly true for the Glorot and He methods. This increase is a telltale sign of overfitting: as we keep training, the model stops learning general features of the parent distribution and starts to focus on specific features of the training set. We didn’t plot the training set error, but it would be going down even as the test set error starts to rise. The lowest test set error is at about 1,200 epochs. Ideally, this would be where we stop training because we have the most reliable evidence that the model is in a good place to correctly predict new, unseen inputs. Further training tends to degrade model generalization.</p>&#13;
<p class="indent">Why did the increase in the test error happen? The likely cause of this effect is too small of a training set, only 6,000 samples. Also, the model architecture is not very large, with only 100 and 50 nodes in the hidden layers.</p>&#13;
<p class="indent">This section dramatically demonstrates the benefit of using current, state-of-the-art network initialization. When we explore convolutional neural networks in <a href="ch12.xhtml#ch12">Chapter 12</a>, we’ll use these approaches exclusively.</p>&#13;
<h3 class="h3" id="lev1_70">Feature Ordering</h3>&#13;
<p class="noindent">We’ll end our MNIST experiments with a bit of fun that we’ll return to again when we’re exploring convolutional neural networks. All of the experiments so far use the MNIST digits as a vector made by laying the rows of the digit images end to end. When we do this, we know that the elements of the vector are related to each other in a way that will reconstruct the digit should we take the vector and reshape it into a 28 × 28 element array. This means, except for the end of one row and the beginning of the next, that the pixels in the row are still part of the digit—the spatial relationship of the components of the image is preserved.</p>&#13;
<p class="indent">However, if we scramble the pixels of the image, but always scramble the pixels in the same way, we’ll destroy the local spatial relationship between the pixels. This local relationship is what we use when we look at the image to decide what digit it represents. We look for the top part of a 5 to be a straight line segment and the bottom portion to curve on the right side, and so forth.</p>&#13;
<p class="indent">Look at <a href="ch07.xhtml#ch7fig3">Figure 7-3</a>. The figure shows MNIST digit images on the top row and what the same digit images look like after scrambling (bottom). In <a href="ch07.xhtml#ch07">Chapter 7</a>, we showed that this scrambling does not affect the accuracy of classic machine learning models; the models consider the inputs holistically, not by local spatial relationships as we do. Is this true for neural networks as well? Also, if true, will the network learn as quickly with the scrambled inputs as it does with the original images? Let’s find out.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_248"/>The code for this experiment is found in <em>mnist_nn_experiments_scrambled .py</em>, where we simply define our now expected neural network model</p>&#13;
<p class="programs">MLPClassifier(solver="sgd", verbose=False, tol=0,<br/>&#13;
  nesterovs_momentum=False, early_stopping=False,<br/>&#13;
  learning_rate_init=0.01, momentum=0.9,<br/>&#13;
  hidden_layer_sizes=(100,50), activation="relu",<br/>&#13;
  alpha=0.2, learning_rate="constant", batch_size=64, max_iter=1)</p>&#13;
<p class="noindent">and train it on the first 6,000 MNIST digit samples—first as usual, and then using the scrambled versions. We compute the test set error as a function of the epoch and average the results over 10 runs before plotting. The result is <a href="ch10.xhtml#ch10fig9">Figure 10-9</a>.</p>&#13;
<div class="image" id="ch10fig9"><img src="Images/10fig09.jpg" alt="image" width="649" height="482"/></div>&#13;
<p class="figcap"><em>Figure 10-9: MNIST test error as a function of training epoch for scrambled and unscrambled digits</em></p>&#13;
<p class="indent">In the figure, we see the answer to our earlier questions. First, yes, traditional neural networks do interpret their input vectors holistically, like the classic models. Second, yes, the network learns just as rapidly with the scrambled digits as it does with the unscrambled ones. The difference between the scrambled and unscrambled curves in <a href="ch10.xhtml#ch10fig9">Figure 10-9</a> is not statistically significant.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_249"/>These results indicate that (traditional) neural networks “understand” their inputs in their entirety and do not look for local spatial relationships. We’ll see a different outcome to this experiment when we work with convolutional neural networks (<a href="ch12.xhtml#ch12">Chapter 12</a>). It’s precisely this lack of spatial awareness (assuming images as inputs) that limited neural networks for so long and led to the development of convolutional neural networks, which are spatially aware.</p>&#13;
<h3 class="h3" id="lev1_71">Summary</h3>&#13;
<p class="noindent">In this chapter, we explored the concepts developed in <a href="ch08.xhtml#ch08">Chapters 8</a> and <a href="ch09.xhtml#ch09">9</a> via experiments with the MNIST dataset. By varying key parameters associated with the network architecture and gradient descent learning process, we increased our intuition as to how the parameters influence the overall performance of the network. Space considerations prevented us from thoroughly exploring all the <span class="literal">MLPClassifier</span> options, so I encourage you to experiment more on your own. In particular, experiment with using the different solvers, Nesterov momentum, early stopping, and, particularly crucial for training convolutional neural networks, nonconstant learning rates.</p>&#13;
<p class="indent">The next chapter explores techniques and metrics for evaluating the performance of machine learning models. This interlude before we jump to convolutional neural networks will supply us with tools we can use to help understand the performance of more advanced model types.<span epub:type="pagebreak" id="page_250"/></p>&#13;
</div></body></html>