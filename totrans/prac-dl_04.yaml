- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: WORKING WITH DATA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Developing a proper dataset is the single most important part of building a
    successful machine learning model. Machine learning models live and die by the
    phrase “garbage in, garbage out.” As you saw in [Chapter 1](ch01.xhtml#ch01),
    the model uses the training data to configure itself to the problem. If the training
    data is not a good representation of the data the model will receive when it is
    used, we can’t expect our model to perform well. In this chapter, we’ll learn
    how to create a good dataset that represents the data the model will encounter
    in the wild.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一个合适的数据集是构建成功机器学习模型最重要的部分。机器学习模型的成败往往取决于“垃圾进，垃圾出”这一原则。正如你在[第1章](ch01.xhtml#ch01)中看到的，模型通过使用训练数据来配置自己以应对问题。如果训练数据不能很好地代表模型在实际使用时将会接收到的数据，我们就不能指望模型表现良好。在本章中，我们将学习如何创建一个良好的数据集，代表模型在实际环境中会遇到的数据。
- en: Classes and Labels
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别与标签
- en: 'In this book, we’re exploring *classification*: we’re building models that
    put things into discrete categories, or *classes*, like dog breed, flower type,
    digit, and so on. To represent classes, we give each input in our training set
    an identifier called a *label*. A label could be the string “Border Collie" or,
    better still, a number like 0 or 1.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们正在探索*分类*：我们正在构建将事物分到不同类别或*类*中的模型，例如狗的品种、花的种类、数字等。为了表示这些类别，我们为训练集中的每个输入赋予一个叫做*标签*的标识符。标签可以是“边境牧羊犬”这样的字符串，或者更好的是像0或1这样的数字。
- en: Models don’t know what their inputs represent. They don’t care whether the input
    is a picture of a border collie or the value of Google stock. To the model, it’s
    all numbers. The same is true of labels. Because the label for the input has no
    intrinsic meaning to the model, we can represent classes however we choose. In
    practice, class labels are usually integers starting with 0\. So, if there are
    10 classes, the class labels are 0, 1, 2, …, 9\. In [Chapter 5](ch05.xhtml#ch05),
    we’ll work with a dataset that has 10 classes representing images of different
    real-world things. We’ll simply map them to the integers as in [Table 4-1](ch04.xhtml#ch4tab1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并不知道它们的输入代表什么。它们并不关心输入是边境牧羊犬的图片，还是谷歌股票的值。对模型而言，这些全都是数字。标签也是如此。因为输入的标签对模型来说没有内在意义，我们可以按照任何方式来表示类别。实际上，类标签通常是从0开始的整数。因此，如果有10个类别，类标签是0、1、2、……、9。在[第5章](ch05.xhtml#ch05)中，我们将处理一个包含10个类别的
    数据集，这些类别代表不同的真实世界物品的图像。我们将简单地将它们映射到整数，如[表4-1](ch04.xhtml#ch4tab1)所示。
- en: '**Table 4-1:** Label Classes with Integers: 0, 1, 2, . . .'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-1：** 使用整数标记类别：0、1、2、……'
- en: '| **Label** | **Actual class** |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| **标签** | **实际类别** |'
- en: '| --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | airplanes |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 飞机 |'
- en: '| 1 | cars |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 汽车 |'
- en: '| 2 | birds |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 鸟 |'
- en: '| 3 | cats |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 猫 |'
- en: '| 4 | deer |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 鹿 |'
- en: '| 5 | dogs |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 狗 |'
- en: '| 6 | frogs |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 蛙 |'
- en: '| 7 | horses |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 马 |'
- en: '| 8 | ships |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 船 |'
- en: '| 9 | trucks |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 卡车 |'
- en: With that labeling, every training input that is a dog is labeled 5, while every
    input that is a truck is labeled 9\. But what exactly is it that we’re labeling?
    In the next section, we’ll cover features and feature vectors, the very lifeblood
    of machine learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样的标签，每个训练输入如果是狗，则标签为5，而如果是卡车，则标签为9。那么我们究竟在标注什么呢？在下一部分，我们将讨论特征与特征向量，机器学习的命脉。
- en: Features and Feature Vectors
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征与特征向量
- en: Machine learning models take *features* as inputs and deliver, in the case of
    a classifier, a label as output. So what are these features and where do they
    come from?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型以*特征*作为输入，并输出，若为分类器，则输出一个标签。那么这些特征是什么，它们从何而来呢？
- en: For most models, features are numbers. What the numbers represent depends upon
    the task at hand. If we’re interested in identifying flowers based on measurements
    of their physical properties, our features are those measurements. If we’re interested
    in using the dimensions of cells in a medical sample to predict whether a tumor
    is breast cancer or not, the features are those dimensions. With modern techniques,
    the features might be the pixels of an image (numbers), or a sound’s frequency
    (numbers) or even how many foxes were counted by a camera trap over a two-week
    period (numbers).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数模型，特征是数字。数字代表什么取决于当前的任务。如果我们对根据花卉的物理属性来识别它们感兴趣，那么我们的特征就是这些测量值。如果我们对使用医学样本中细胞的尺寸来预测肿瘤是否为乳腺癌感兴趣，那么特征就是这些尺寸。通过现代技术，特征可能是图像的像素（数字），或者声音的频率（数字），甚至是相机陷阱在两周内拍到的狐狸数量（数字）。
- en: Features, then, are whatever numbers we want to use as inputs. The goal of training
    the model is to get it to learn a relationship between the input features and
    the output label. We assume that a relationship exists between the input features
    and output label before training the model. If the model fails to train, it might
    be that there is no relationship to learn.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 特征是我们想用作输入的任何数字。训练模型的目标是让它学习输入特征与输出标签之间的关系。我们假设在训练模型之前，输入特征和输出标签之间存在某种关系。如果模型训练失败，可能是因为没有可以学习的关系。
- en: After training, feature vectors with unknown class labels are given to the model,
    and the model’s output predicts the class label based on the relationships it
    discovered during training. If the model is repeatedly making poor predictions,
    one possibility is that the selected features are not sufficiently capturing that
    relationship. Before we go into what makes a good feature, let’s take a closer
    look at the features themselves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，带有未知类别标签的特征向量被提供给模型，模型根据在训练过程中发现的关系预测类别标签。如果模型反复做出错误预测，一种可能性是所选择的特征不足以捕捉到这些关系。在我们深入探讨什么是好的特征之前，先仔细看看特征本身。
- en: Types of Features
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征的类型
- en: To recap, features are numbers representing something that is measured or known,
    and *feature vectors* are sets of these numbers used as inputs to the model. There
    are different kinds of numbers you could use as features, and as you’ll see, they’re
    not all created equal. Sometimes you’ll have to manipulate them before you can
    input them into your model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，特征是代表某些已测量或已知事物的数字，*特征向量*是这些数字的集合，用作模型的输入。你可以使用不同种类的数字作为特征，正如你将看到的，并非所有特征都一样。有时候，你需要在将它们输入模型之前对其进行处理。
- en: Floating-Point Numbers
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 浮点数
- en: In [Chapter 5](ch05.xhtml#ch05), we’ll be building a historic flower dataset.
    The features of that dataset are actual measurements of things like a flower’s
    sepal width and height (in centimeters). A typical measurement might be 2.33 cm.
    This is a *floating-point* number—a number with a decimal point, or, if you remember
    your high school math courses, a *real* number. Most models want to work with
    floating-point numbers, so you can just use the measurements as they are. Floating-point
    numbers are *continuous*, meaning there are an infinite number of values between
    one integer and the next, so we have a smooth transition between them. As we’ll
    see later on, some models expect continuous values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.xhtml#ch05)中，我们将构建一个历史花卉数据集。该数据集的特征是实际的测量值，比如花朵的萼片宽度和高度（以厘米为单位）。一个典型的测量值可能是2.33厘米。这是一个*浮点数*——一个有小数点的数字，或者，如果你记得高中数学课程的话，这是一个*实数*。大多数模型希望使用浮点数，因此你可以直接使用这些测量值。浮点数是*连续的*，意味着在两个整数之间有无限多个值，所以它们之间有平滑的过渡。正如我们稍后会看到的，一些模型需要连续值。
- en: Interval Values
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 间隔值
- en: 'Floating-point numbers don’t work for everything, however. Clearly, flowers
    cannot have 10.14 petals, though they might have 9, 10, or 11\. These numbers
    are *integers*: whole numbers without a fractional part or a decimal point. Unlike
    floating-point numbers, they are *discrete*, which means they pick out only certain
    values, leaving gaps in between. Fortunately for us, integers are just special
    real numbers, so models can use them as they are.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，浮点数并不适用于所有情况。显然，花朵不可能有10.14片花瓣，尽管它们可能有9、10或11片。这些数字是*整数*：没有小数部分或小数点的整数。与浮点数不同，整数是*离散的*，意味着它们只能取特定的值，中间留有间隔。幸运的是，整数只是特殊的实数，因此模型可以直接使用它们。
- en: 'In our petal example, the difference between 9, 10, and 11 is meaningful in
    that 11 is bigger than 10, and 10 is bigger than 9\. Not only that, but 11 is
    bigger than 10 in exactly the same way that 10 is bigger than 9\. The difference,
    or interval, between the values is the same: 1\. This value is called an *interval*
    value.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的花瓣示例中，9、10和11之间的差异是有意义的，因为11大于10，10大于9\. 不仅如此，11大于10的方式与10大于9的方式完全相同。它们之间的差异或间隔是相同的：1\.
    这个值被称为*间隔*值。
- en: The pixels in an image are interval values, because they represent the (assumed
    linear) response of some measurement device, like a camera or an MRI machine,
    to some physical process like intensity and color of visible light or the number
    of hydrogen protons in free water in tissue. The key point is that if value *x*
    is the next number in the sequence after value *y*, and value *z* is the number
    before value *y*, then the difference between *x* and *y* is the same difference
    as between *y* and *z*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的像素是区间值，因为它们代表了某些测量设备（如相机或MRI机）对一些物理过程（如可见光的强度和颜色，或组织中自由水中的氢质子数量）的（假定的线性）响应。关键点是，如果值
    *x* 是值 *y* 之后的下一个数字，且值 *z* 是值 *y* 之前的数字，那么 *x* 和 *y* 之间的差异与 *y* 和 *z* 之间的差异是相同的。
- en: Ordinal Values
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 顺序值
- en: Sometimes the interval between the values is not the same. For example, some
    models include someone’s educational level to predict whether or not they will
    default on a loan. If we encode someone’s educational level by counting their
    years of schooling, we could use that safely since the difference between 10 years
    of schooling and 8 is the same as the difference between 8 years of schooling
    and 6\. However, if we simply assign 1 for “completed high school,” 2 for “has
    an undergraduate degree,” and 3 for “has a doctorate or other professional degree,”
    we’d probably be in trouble; while 3 > 2 > 1 is true, whether or not meaningful
    for our model, the difference between the values represented by 3 and 2 and 2
    and 1 is not the same. Features like these are called *ordinal* because they express
    an ordering, but the differences between the values are not necessarily always
    the same.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，值之间的间隔是不相等的。例如，一些模型包括某人的教育水平来预测他们是否会违约。如果我们通过计算某人受教育的年数来编码他们的教育水平，我们可以安全地使用这种方式，因为10年学识与8年学识之间的差异，与8年学识与6年学识之间的差异是相同的。然而，如果我们简单地将“完成高中”标记为1，“拥有本科学位”标记为2，“拥有博士或其他专业学位”标记为3，我们可能会遇到问题；虽然3
    > 2 > 1是成立的，但对于我们的模型而言，这些值之间的差异并不总是相同。像这样的特征被称为*顺序*特征，因为它们表达了排序关系，但不同值之间的差异不一定总是相同的。
- en: Categorical Values
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类别值
- en: Sometimes we use numbers as codes. We might encode sex as 0 for male and 1 for
    female, for example. In this case, 1 is not understood to be greater than 0 or
    less than 0, so these are not interval or ordinal values. Instead, these are *categorical*
    values. They express a category but say nothing about any relationship between
    the categories.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们使用数字作为代码。例如，我们可能将性别编码为男性为0，女性为1。在这种情况下，1并不被理解为大于0或小于0，因此这些并不是区间值或顺序值。相反，这些是*类别*值。它们表示一个类别，但没有表达类别之间的任何关系。
- en: Another common example, perhaps relevant to classifying flowers, is color. We
    might use 0 for red, 1 for green, and 2 for blue. Again, no relationship exists
    between 0, 1, or 2 in this case. This doesn’t mean we can’t use categorical features
    with our models, but it does mean that we usually can’t use them as they are since
    most types of machine learning models expect at least ordinal, if not interval
    numbers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的例子，可能与分类花卉相关，是颜色。我们可能用0表示红色，1表示绿色，2表示蓝色。同样，0、1和2之间不存在任何关系。这并不意味着我们不能将类别特征用于我们的模型，但它确实意味着我们通常不能直接使用它们，因为大多数机器学习模型期望至少是顺序的，甚至是区间数值。
- en: We can make categorical values at least ordinal by using the following trick.
    If we wanted to use a person’s sex as an input, instead of saying 0 for male and
    1 for female, we would create a two-element vector, one element for each possibility.
    The first digit in the vector will indicate whether the input is male by signaling
    either 0 (meaning they’re not male) or 1 (meaning they are). The second digit
    will indicate whether or not they are female. We map the categorical values to
    a binary vector, as shown in [Table 4-2](ch04.xhtml#ch4tab2).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方法将类别值至少转化为顺序值。如果我们想使用一个人的性别作为输入，而不是将男性标记为0，女性标记为1，我们可以创建一个二元素向量，每个元素表示一种可能性。向量中的第一个数字将通过信号0（表示不是男性）或1（表示是男性）来指示输入是否为男性。第二个数字将指示其是否为女性。我们将类别值映射到二进制向量，如[表
    4-2](ch04.xhtml#ch4tab2)所示。
- en: '**Table 4-2:** Representing Categories as Vectors'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-2：** 将类别表示为向量'
- en: '| **Categorical value** |  | **Vector representation** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **类别值** |  | **向量表示** |'
- en: '| --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | → | 1 0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 0 | → | 1 0 |'
- en: '| 1 | → | 0 1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 1 | → | 0 1 |'
- en: Here a 0 in the “is male” feature is meaningfully less than a 1 in that feature,
    which fits the definition of an ordinal value. The price we pay is to expand the
    number of features in our feature vector, as we need one feature for each of the
    possible categorical values. With five colors, for example, we would need a five-element
    vector; with five thousand, a five-thousand-element vector.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“是否为男性”特征中的0在意义上小于1，这符合有序值的定义。我们为此付出的代价是扩展特征向量的维度，因为我们需要为每个可能的类别值提供一个特征。例如，如果有五种颜色，我们就需要一个五维向量；如果有五千种颜色，就需要一个五千维的向量。
- en: To use this scheme, the categories must be mutually exclusive, meaning there
    will be only one 1 in each row. Because there’s always only one nonzero value
    per row, this approach is sometimes called a *one-hot encoding*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方案时，类别必须是互斥的，这意味着每行中只能有一个1。由于每行中总是只有一个非零值，这种方法有时被称为*独热编码*。
- en: Feature Selection and the Curse of Dimensionality
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征选择与维度灾难
- en: 'This section is about *feature selection*, the process of selecting which features
    to use in your feature vectors, and why you shouldn’t include features you don’t
    need. Here’s a good rule of thumb: the feature vector should contain only features
    that capture aspects of the data that allow the model to generalize to new data.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的是*特征选择*，即选择哪些特征用于特征向量的过程，以及为什么不应该包括不需要的特征。这里有一个好的经验法则：特征向量应仅包含那些捕捉数据中能够让模型推广到新数据的方面的特征。
- en: In other words, features should capture aspects of the data that help the model
    separate the classes. It’s impossible to be more explicit, since the set of best
    features are always dataset specific, unknowable in advance. But that doesn’t
    mean we can’t say things that might be helpful in guiding us toward a useful set
    of features for whatever dataset we’re working with.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，特征应该捕捉数据中有助于模型区分类别的方面。无法更明确地说明这一点，因为最佳特征的集合始终是特定于数据集的，无法事先知道。但这并不意味着我们不能说出一些可能有助于指导我们选择适合当前数据集的特征。
- en: Like many things in machine learning, selecting features comes with trade-offs.
    We need enough features to capture all the relevant parts of the data so that
    the model has something to learn from, but if we have too many features, we fall
    victim to the *curse of dimensionality*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 像机器学习中的许多事情一样，选择特征也存在权衡。我们需要足够的特征来捕捉数据中的所有相关部分，以便模型能够从中学习，但如果特征过多，我们就会陷入*维度灾难*。
- en: To explain what this means, let’s look at an example. Suppose our features are
    all restricted to the range [0,1). That’s not a typo; we’re using interval notation,
    where a square bracket means the bound is included in the range, and a parenthesis
    means the bound is excluded. So here 0 is allowed but 1 isn’t. We’ll also assume
    our feature vectors are either two-dimensional or three-dimensional. That way,
    we can plot each feature vector as a point in a 2D or 3D space. Finally, we’ll
    simulate datasets by selecting feature vectors, 2D or 3D, uniformly at random
    so that each element of the vector is in [0,1).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这意味着什么，假设我们的特征都限制在区间[0,1)内。这不是打字错误；我们使用的是区间表示法，其中方括号表示包含边界，圆括号表示不包含边界。所以这里允许0，但不允许1。我们还假设我们的特征向量是二维或三维的。这样，我们就可以将每个特征向量绘制为二维或三维空间中的一个点。最后，我们将通过从区间[0,1)内均匀随机选择特征向量来模拟数据集，确保向量的每个元素都在[0,1)区间内。
- en: Let’s fix the number of samples at 100\. If we have two features, or a 2D space,
    we can represent 100 randomly selected 2D vectors as in the top of [Figure 4-1](ch04.xhtml#ch4fig1).
    Now, if we have three features, or a 3D space, those same 100 features look like
    the bottom of [Figure 4-1](ch04.xhtml#ch4fig1).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将样本数量固定为100。如果我们有两个特征，或者是二维空间，我们可以像[图4-1](ch04.xhtml#ch4fig1)的顶部那样表示100个随机选择的二维向量。现在，如果我们有三个特征，或者是三维空间，那这100个特征就像[图4-1](ch04.xhtml#ch4fig1)的底部那样。
- en: '![image](Images/04fig01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig01.jpg)'
- en: '*Figure 4-1: One hundred random samples in 2D space (top) and in 3D space (bottom)*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-1：100个随机样本在二维空间（顶部）和三维空间（底部）中的分布*'
- en: Since we’re assuming our feature vectors can come from anywhere in the 2D or
    3D space, we want our dataset to sample as much of that space as possible so that
    it represents the space well. We can get a measure of how well the 100 points
    are filling the space by splitting each axis into 10 equal sections. Let’s call
    these sections *bins*. We’ll end up with 100 bins in the 2D space, because it
    has two axes (10 × 10), and 1,000 in the 3D space, because it has three axes (10
    × 10 × 10). Now, if we count the number of bins occupied by at least one point
    and divide that number by the total number of bins, we’ll get the fraction of
    bins that are occupied.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设我们的特征向量可以来自2D或3D空间中的任何地方，我们希望我们的数据集能够尽可能地采样这些空间，以便能够很好地代表这些空间。我们可以通过将每个轴分成10个相等的部分来衡量100个点填充空间的效果。我们称这些部分为*箱子*。在2D空间中，我们会有100个箱子，因为它有两个轴（10
    × 10），而在3D空间中，我们会有1,000个箱子，因为它有三个轴（10 × 10 × 10）。现在，如果我们计算至少有一个点占据的箱子数量，并将其除以箱子的总数，我们就能得到被占据的箱子的比例。
- en: 'Doing this gives us 0.410 (out of a maximum of 1.0) for the 2D space and 0.048
    for the 3D space. This means that 100 samples were able to sample about half of
    the 2D feature space. Not bad! But 100 samples in the 3D feature space sampled
    only about 5 percent of the space. To fill the 3D space to the same fraction as
    the 2D space, we’d need about 1,000—or 10 times as many as we have. This general
    rule applies as the dimensionality increases: a 4D feature space would need about
    10,000 samples, while a 10D feature space would need about 10,000,000,000! As
    the number of features increases, the amount of training data we need to get a
    representative sampling of the possible feature space increases dramatically,
    approximately as 10^(*d*), where *d* is the number of dimensions. This is the
    *curse of dimensionality*, and it was the bane of machine learning for decades.
    Fortunately for us, modern deep learning has overcome this curse, but it’s still
    relevant when working with traditional models like the ones we will explore in
    [Chapter 6](ch06.xhtml#ch06).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做后，我们得到了2D空间的0.410（最大为1.0）和3D空间的0.048。这意味着100个样本能够覆盖约一半的2D特征空间。还不错！但100个样本在3D特征空间中只覆盖了大约5%的空间。为了使3D空间的采样与2D空间相同的比例，我们需要大约1,000个样本——也就是我们当前样本数的10倍。随着维度的增加，这一普遍规则适用：一个4D特征空间需要大约10,000个样本，而一个10D特征空间则需要约10,000,000,000个样本！随着特征数量的增加，我们需要的训练数据量以10^(*d*)的指数增长，其中*d*是维度的数量。这就是*维度诅咒*，多年来一直是机器学习的噩梦。幸运的是，现代深度学习已经克服了这个诅咒，但在使用传统模型时，像我们在[第6章](ch06.xhtml#ch06)中探讨的那样，它仍然是一个重要的问题。
- en: For example, a typical color image on your computer might have 1,024 pixels
    on a side where each pixel requires 3 bytes to specify the color as a mix of red,
    green, and blue. If we wanted to use this image as input to a model, we’d need
    a feature vector with *d* = 1024 × 1024 × 3 = 3,145,728 elements. This means we’d
    need some 10^(3,145,728) samples to populate our feature space. Clearly, this
    is not possible. We’ll see in [Chapter 12](ch12.xhtml#ch12) how to overcome this
    curse by using a convolutional neural network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你电脑上的典型彩色图像可能每边有1,024个像素，每个像素需要3个字节来指定颜色，表示红、绿、蓝三种颜色的混合。如果我们想把这张图片作为模型的输入，我们需要一个特征向量，*d*
    = 1024 × 1024 × 3 = 3,145,728个元素。这意味着我们需要大约10^(3,145,728)个样本来填充我们的特征空间。显然，这是不可能的。我们将在[第12章](ch12.xhtml#ch12)中看到，如何通过使用卷积神经网络来克服这个诅咒。
- en: Now that we know about classes, features, and feature vectors, let’s describe
    what it means to have a good dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了类别、特征和特征向量，让我们来描述一个好的数据集意味着什么。
- en: Features of a Good Dataset
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个好的数据集的特征
- en: 'The dataset is everything. This is no exaggeration, since we build the model
    from the dataset. The model has parameters—be they the weights and biases of a
    neural network, the probabilities of each feature occurring in a Naïve Bayes model,
    or the training data itself in the case of Nearest Neighbors. The parameters are
    what we use the training data to find out: they encode the knowledge of the model
    and are learned by the training algorithm.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集就是一切。这并不是夸张，因为我们是从数据集中构建模型的。模型有参数——无论是神经网络中的权重和偏差，还是朴素贝叶斯模型中每个特征发生的概率，或是在最近邻方法中训练数据本身。参数是我们用训练数据来找出的内容：它们编码了模型的知识，并通过训练算法进行学习。
- en: Let’s back up a little bit and define the term *dataset* as we we’ll use it
    in this book. Intuitively, we understand what a dataset is, but let’s be more
    scientific and define it as a collection of pairs of values, {*X,Y*}, where *X*
    is an *input* to the model and *Y* is a label. Here *X* is some set of values
    that we’ve measured and grouped together, like length and width of flower parts,
    and *Y* is the thing we want to teach the model to tell us, such as which flower
    or which animal the data best represents.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微回顾一下，并定义一下本书中使用的*数据集*这个术语。直观上，我们理解什么是数据集，但让我们更加科学地定义它：数据集是值对的集合，{*X,Y*}，其中*X*是模型的*输入*，*Y*是标签。这里的*X*是一组我们测量并聚集在一起的值，比如花卉部位的长度和宽度，*Y*则是我们希望模型告诉我们的东西，比如数据最好代表哪种花或哪种动物。
- en: For *supervised* learning, we act as the teacher, and the model acts as the
    student. We are teaching the student by presenting example after example, saying
    things like “this is a cat” and “this is a dog,” much as we would teach a small
    child with a picture book. In this case, the *dataset* is a collection of examples,
    and *training* consists of showing the examples to the model repeatedly, until
    the model “gets it”—that is, until the parameters of the model are conditioned
    and adjusted to minimize the error made by the model for this particular dataset.
    This is the learning part of machine learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*监督学习*，我们充当老师，而模型充当学生。我们通过不断提供示例来教学生，比如“这是一只猫”和“这是一只狗”，就像我们用图画书教小孩子一样。在这种情况下，*数据集*是一个示例集合，*训练*则是不断地向模型展示这些示例，直到模型“明白”为止——也就是说，直到模型的参数被调整以最小化该数据集中的误差。这就是机器学习中的学习部分。
- en: Interpolation and Extrapolation
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 插值与外推
- en: '*Interpolation* is the process of estimating within a certain known range.
    And *extrapolation* occurs when we use the data we have to estimate outside the
    known range. Generally speaking, our models are more accurate when they in some
    sense interpolate, which means we need a dataset that is a comprehensive representation
    of the range of values that could be used as inputs to the model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*插值*是指在某个已知范围内进行估算的过程。而*外推*则是在已知数据的基础上估算超出已知范围的值。一般来说，当我们的模型进行插值时，它的准确度较高，这意味着我们需要一个数据集，这个数据集能够全面代表可能作为模型输入值的范围。'
- en: As an example, let’s look at world population, in billions, from 1910 to 1960
    ([Table 4-3](ch04.xhtml#ch4tab3)). We have data for every 10 years in our known
    range, 1910 to 1960.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，我们来看一下从1910年到1960年（见[表4-3](ch04.xhtml#ch4tab3)）的世界人口数据，以十亿为单位。我们拥有1910年到1960年每10年的数据。
- en: '**Table 4-3:** The World Population by Decade'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-3：** 按十年划分的世界人口'
- en: '| **Year** | **Population (billions)** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **年份** | **人口（十亿）** |'
- en: '| --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1910 | 1.750 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1910 | 1.750 |'
- en: '| 1920 | 1.860 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 1920 | 1.860 |'
- en: '| 1930 | 2.070 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 1930 | 2.070 |'
- en: '| 1940 | 2.300 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1940 | 2.300 |'
- en: '| 1950 | 2.557 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1950 | 2.557 |'
- en: '| 1960 | 3.042 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1960 | 3.042 |'
- en: 'If we find the “best fitting” line to plot through this data, we can use it
    as a model to predict values. This is called *linear regression*, and it allows
    us to estimate the population for any year we choose. We’ll skip the actual fitting
    process, which you can do simply with online tools, and jump to the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到一条“最佳拟合”直线来绘制这些数据，我们可以将其作为模型来预测值。这就是所谓的*线性回归*，它允许我们估算任何我们选择年份的人口。我们将跳过实际的拟合过程，您可以通过在线工具轻松完成此过程，直接进入模型：
- en: '*p* = 0.02509*y* – 46.28'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 0.02509*y* – 46.28'
- en: 'For any year, *y*, we can get an estimate of the population, *p*. What was
    the world population in 1952? We don’t have actual data for 1952 in our table,
    but using the model, we can estimate it like so:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何年份*y*，我们都可以估算出人口*p*。1952年世界人口是多少？我们在表格中没有1952年的实际数据，但通过使用模型，我们可以这样估算：
- en: '*p* = 0.02509(1952) – 46.28 = 2.696 billion'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 0.02509(1952) – 46.28 = 2.696十亿'
- en: By checking the actual world population data for 1952, we know that it was 2.637
    billion, so our estimate of 2.696 billion was only some 60 million off. The model
    seems to be pretty good!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看1952年实际的世界人口数据，我们知道它是26.37亿，因此我们的估算值2.696亿仅差了大约6000万。模型看起来还挺不错的！
- en: In using the model to estimate the world population in 1952, we performed interpolation.
    We made an estimate for a value that was between data points we had, and the model
    gave us a good result. Extrapolation, on the other hand, is measuring beyond what
    is known, outside the range of our data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用该模型估算1952年世界人口时，我们进行了插值。我们为介于已知数据点之间的值进行了估算，模型给出了一个不错的结果。另一方面，外推是指测量超出已知范围的值，超出了我们数据的范围。
- en: 'Let’s use our model to estimate world population in 2000, 40 years after the
    data we used to build our model ends:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用我们的模型来估计2000年世界人口，即在我们用来构建模型的数据结束后的40年：
- en: '*p* = 0.02509(2000) – 46.28 = 3.900 billion'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 0.02509(2000) – 46.28 = 39.00亿'
- en: According to the model, it should be close to 3.9 billion, but we know from
    actual data that the world population in 2000 was 6.089 billion. Our model is
    off by over 2 billion people. What happened here is that we applied the model
    to input it wasn’t suited for. If we remain in the range of inputs that the model
    is “trained” to know about, namely, dates from 1910 through 1960, then the model
    performs well enough. Once we went beyond the model’s training, however, it fell
    apart because it assumed knowledge we didn’t possess.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据该模型，结果应该接近39亿，但我们从实际数据中知道，2000年世界人口为60.89亿。我们的模型偏差超过了20亿人。发生的情况是我们将模型应用于了不适合的输入。如果我们保持在模型“训练”过的输入范围内，即1910年到1960年之间的日期，模型表现得还不错。然而，一旦超出了模型的训练范围，它就会崩溃，因为它假设了我们没有的知识。
- en: When we interpolate, the model will see examples that are similar to the set
    of examples it saw during training. Perhaps unsurprisingly, it will do better
    on these examples than when we extrapolate and ask the model to go beyond its
    training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行插值时，模型将看到与训练时所见示例相似的例子。也许不足为奇的是，在这些例子上它表现得比在外推时更好，外推要求模型超出它的训练范围。
- en: 'When it comes to classification, it’s essential we have comprehensive training
    data. Let’s assume we’re training a model to identify dog breeds. In our dataset,
    we have hundreds of images of classic black-and-white border collies like the
    one on the left in [Figure 4-2](ch04.xhtml#ch4fig2). If we then give the model
    a new image of a classic border collie, we will, hopefully, get back a correct
    label: “Border Collie.” This is akin to asking the model to interpolate: it’s
    working with something is has already seen before because the “Border Collie”
    label in the training data included many examples of classic border collies.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，确保有全面的训练数据至关重要。假设我们正在训练一个模型来识别狗的品种。在我们的数据集中，我们有成百上千张经典的黑白色边境牧羊犬的图片，就像[图
    4-2](ch04.xhtml#ch4fig2)中左边的那只。如果我们接着给模型提供一张经典边境牧羊犬的新图片，我们希望能够得到一个正确的标签：“边境牧羊犬”。这类似于要求模型进行插值：它正在处理它已经见过的东西，因为训练数据中的“边境牧羊犬”标签包括了很多经典边境牧羊犬的例子。
- en: '![image](Images/04fig02.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig02.jpg)'
- en: '*Figure 4-2: A border collie with classic markings (left), a border collie
    with liver-colored markings (middle), an Australian shepherd (right)*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-2：一只拥有经典标记的边境牧羊犬（左），一只拥有肝色标记的边境牧羊犬（中），一只澳大利亚牧羊犬（右）*'
- en: However, not every border collie has the classic border collie markings. Some
    are marked like the collie in the middle of [Figure 4-2](ch04.xhtml#ch4fig2).
    Since we didn’t include images like this in the training set, the model must now
    try to go beyond what it was trained to do and give a correct output label for
    an instance of a class it was trained on but of a type it was not trained with.
    It will likely fail, giving a false output like “Australian Shepherd,” a breed
    similar to a border collie, as seen on the right of [Figure 4-2](ch04.xhtml#ch4fig2).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不是每只边境牧羊犬都有经典的边境牧羊犬标记。一些边境牧羊犬的标记像[图 4-2](ch04.xhtml#ch4fig2)中间的那只。由于我们没有在训练集中包含这种类型的图片，模型现在必须尝试超出它训练的范围，为它已训练过的类别实例提供正确的输出标签，但这种实例的类型并没有出现在训练中。它很可能会失败，给出一个错误的输出，如“澳大利亚牧羊犬”，这是一种与边境牧羊犬相似的品种，正如[图
    4-2](ch04.xhtml#ch4fig2)右侧所示。
- en: The key concept to remember, however, is that the dataset must cover the full
    range of variation *within* the classes the model will see when the model is predicting
    labels for unknown inputs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关键概念是，数据集必须覆盖模型在预测未知输入标签时将会看到的类别中的所有变异*范围*。
- en: The Parent Distribution
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 母体分布
- en: The dataset must be representative of the classes it’s modeling. Buried in this
    idea is the assumption that our data has a *parent distribution*, an unknown data
    generator that created the particular dataset we’re using.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集必须能代表它所建模的类别。这一思想中隐含的假设是，我们的数据有一个*母体分布*，即一个未知的数据生成器，它生成了我们正在使用的特定数据集。
- en: Consider this parallel from philosophy. The ancient Greek philosopher Plato
    uses the concept of ideals. In his view, there was an ideal chair somewhere “out
    there,” and all existing chairs were more or less perfect copies of that ideal
    chair. This is what we mean by the relationship between the dataset we are using,
    the copy, and the parent distribution, the ideal generator. We want the dataset
    to be a representation of the ideal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑一下哲学中的平行类比。古希腊哲学家柏拉图使用理想概念。在他看来，某个地方“存在”一个理想的椅子，所有现存的椅子都是这个理想椅子的或多或少完美的复制品。这就是我们所说的使用的数据集、复制品和父分布——理想生成器之间的关系。我们希望数据集能够代表这一理想。
- en: We can think of a dataset as a sample from some unknown process that produces
    data according to the parent distribution. The type of data it produces—the values
    and ranges of the features—will follow some unknown, statistical rule. For example,
    when you roll a die, each of the six values is equally likely in the long run.
    We call this a *uniform parent distribution*. If you make a bar graph of the number
    of times each value appears as you roll the die many times, then you will get
    a (more or less) horizontal line since each value is equally likely to happen.
    We see a different distribution when we measure the height of adults. The distribution
    of heights will have a form with two humps, one around mean male height and the
    other around mean female height.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将数据集看作是从某个未知过程中的样本，该过程根据父分布生成数据。它生成的数据类型——特征的值和范围——将遵循某种未知的统计规则。例如，当你掷骰子时，每个六个值在长时间内出现的概率是相等的。我们称这种分布为*均匀父分布*。如果你制作一个柱状图，统计每个值出现的次数，当你掷骰子多次时，你会看到一条（或多或少）水平的线，因为每个值出现的概率是相等的。我们在测量成人身高时看到的则是不同的分布。身高分布呈现出双峰的形状，一峰围绕男性的平均身高，另一峰围绕女性的平均身高。
- en: The parent distribution is what generates this overall shape. The training data,
    the test data, and the data you give the model to make decisions must all come
    from the same parent distribution. This is a fundamental assumption models make,
    and one that shouldn’t seem too surprising to us. Still, sometimes it’s easy to
    mix things up and train with data from one parent distribution while testing or
    using the model with data from a different parent distribution. (How to train
    with one parent distribution and use that model with data from a different distribution
    is a very active research area at the moment. Search for “domain adaptation.”)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 父分布就是生成这种整体形状的原因。训练数据、测试数据以及你用于让模型做决策的数据必须来自同一个父分布。这是模型做出的基本假设，也是我们不应该感到意外的假设。然而，有时很容易混淆，在使用来自不同父分布的数据进行测试或使用模型时，训练却使用了来自一个父分布的数据。（如何用一个父分布训练模型，并用来自不同分布的数据测试该模型，是当前一个非常活跃的研究领域。可以搜索“领域适应”）
- en: Prior Class Probabilities
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 先验类概率
- en: The *prior class probability* is the probability with which each class in the
    dataset appears in the wild.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*先验类概率*是数据集中每个类在实际中出现的概率。'
- en: In general, we want our dataset to match the prior probabilities of the classes.
    If class A appears 85 percent of the time and class B only 15 percent of the time,
    then we want class A to appear 85 percent of the time and class B to appear 15
    percent of the time in our training set.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望我们的数据集能够匹配各类的先验概率。如果类A出现的概率为85%，而类B仅为15%，那么我们希望类A在训练集中出现85%的时间，类B则出现15%的时间。
- en: There are exceptions, however. Say one of the classes we want the model to learn
    is rare, showing up only once for every 10,000 inputs. If we make the dataset
    strictly follow the actual prior probabilities, the model might not see enough
    examples of the rare class to learn anything helpful about it. And, worse yet,
    what if the rare class is the class we are most interested in?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有例外。假设我们希望模型学习的某一类是稀有的，它每出现一次，输入数据中就有10,000个样本。如果我们让数据集严格遵循实际的先验概率，模型可能无法看到足够的稀有类样本，从而无法学到任何有用的信息。而更糟糕的是，假设这个稀有类恰恰是我们最感兴趣的类呢？
- en: For example, let’s pretend we’re building a robot that locates four-leaf clovers.
    We’ll assume that we already know that the input to the model is a clover; we
    just want to know whether it has three or four leaves. We know that an estimated
    1 in every 5,000 clovers is a four-leaf clover. Building a dataset with 5,000
    three-leaf clovers for every instance of a four-leaf clover seems reasonable until
    we realize that a model that simply says every input is a three-leaf clover will
    be right, on average, 4,999 times out of 5,000! It will be an extremely accurate
    but completely useless model because it never finds the class we’re interested
    in.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在构建一个能定位四叶草的机器人。我们假设我们已经知道模型的输入是三叶草或四叶草；我们只想知道它是三叶草还是四叶草。我们知道，估计每5000株三叶草中有1株是四叶草。为每个四叶草实例构建一个包含5000株三叶草的数据集似乎是合理的，直到我们意识到，一个简单地将所有输入都认为是三叶草的模型，在5000次中平均正确4999次！它将是一个非常准确，但完全无用的模型，因为它永远无法找到我们感兴趣的类别。
- en: Instead, we might use a 10:1 ratio of three-leaf to four-leaf clovers. Or, when
    training the model, we might start with an even number of three- and four-leaf
    clovers, and then, after training for a time, change to a mix that is increasingly
    closer to the actual prior probability. This trick doesn’t work for all model
    types, but it does work for neural networks. Why this trick works is poorly understood
    but, intuitively, we can imagine the network learning first about the visual difference
    between a three-leaf and four-leaf clover and then learning something about the
    actual likelihood of encountering a four-leaf clover as the mix changes to be
    closer to the actual prior probabilities.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可能使用三叶草和四叶草的比例为10:1。或者，在训练模型时，我们可能从数量相等的三叶草和四叶草开始，然后，在训练一段时间后，改变为越来越接近实际先验概率的混合比例。这个技巧并不是对所有模型类型都有效，但对于神经网络有效。为什么这个技巧有效尚不完全清楚，但直观上，我们可以想象，网络首先学习三叶草和四叶草之间的视觉差异，然后随着混合比例逐渐接近实际先验概率，学习关于四叶草遇到概率的知识。
- en: In reality, the trick is used because it often results in better-performing
    models. For much of machine learning, especially deep learning, empirical tricks
    and techniques are well in advance of any theory to back them up. “It just works
    better; that’s why” is still a valid, though ultimately unsatisfying, answer to
    many questions about why a particular approach works well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个技巧之所以被使用，是因为它往往能带来更好的模型性能。对于机器学习，尤其是深度学习来说，经验性的技巧和技术远远领先于任何理论的支持。“它就是更有效；这就是原因”仍然是许多关于某种方法为何有效的问题的一个有效答案，尽管这最终是令人不满意的。
- en: How to work with imbalanced data is something the research community is still
    actively investigating. Some choose to start with a more balanced ratio of classes;
    others use data augmentation (see [Chapter 5](ch05.xhtml#ch05)) to boost the number
    of samples from the underrepresented class.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理不平衡数据是研究界仍在积极探讨的问题。有些人选择从更平衡的类别比例开始；还有一些人使用数据增强（参见[第5章](ch05.xhtml#ch05)）来增加来自代表性不足类别的样本数量。
- en: Confusers
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混淆器
- en: We said that we need to include examples in our dataset that reflect all the
    natural variation in the classes we want to learn. This is definitely true, but
    at times it is particularly important to include training samples that are similar
    to one or more of our classes but really are not examples of that class.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说过，我们需要在数据集中包含反映我们想要学习的类别所有自然变异的示例。这一点是确实的，但有时特别重要的是包含与我们某个或多个类别相似，但实际上并非该类别的训练样本。
- en: Consider two models. The first learns the difference between images of dogs
    and images of cats. The second learns the difference between images of dogs and
    images that are not dogs. The first model has it easy. The input is either a dog
    or a cat, and the model is trained using images of dogs and images of cats. The
    second model, however, has it rougher. It’s obvious that we need images of dogs
    for training. But, what should the “not dog” images be? Given the preceding discussion,
    we should be starting to intuit that we’ll need images that cover the space of
    images the model will see in the wild.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两种模型。第一个模型学习区分狗的图像和猫的图像。第二个模型学习区分狗的图像和不是狗的图像。第一个模型相对容易。输入要么是狗，要么是猫，模型通过狗和猫的图像进行训练。然而，第二个模型的情况就复杂得多。显然，我们需要狗的图像来进行训练。但“不是狗”的图像应该是什么呢？根据前面的讨论，我们应该开始直觉地意识到，我们需要的图像应该涵盖模型在现实中可能遇到的图像空间。
- en: We can take this one step further. If we want to tell the difference between
    dogs and not dogs, we should be sure to include wolves in the “not a dog” class
    when training. If we don’t, the model might not learn enough to tell the difference
    when it encounters a wolf and will return a “dog” classification. If we build
    the dataset by using hundreds of “not dog” images that are all pictures of penguins
    and parrots, should we be surprised if the model decides to call a wolf a dog?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更进一步。如果我们想区分狗和非狗，我们应该确保在训练时将狼包含在“非狗”类别中。如果不这样做，模型可能无法学会在遇到狼时做出区分，从而错误地将狼分类为“狗”。如果我们通过使用数百张“非狗”的图像（全是企鹅和鹦鹉的图片）来构建数据集，那么如果模型决定把狼叫做狗，难道我们应该感到惊讶吗？
- en: In general, we need to make sure the dataset includes *confusers*, or *hard
    negatives*—examples that are similar enough to other classes to be mistaken for
    them, but don’t belong in the class. Confusers give the model a chance to learn
    the more precise features of a class. Hard negatives are particularly useful when
    distinguishing between something and everything else, as in “dog” versus “not
    dog.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们需要确保数据集包含*干扰项*，或者*困难负样本*——这些样本与其他类别足够相似，容易被误分类，但实际上不属于该类别。干扰项使模型有机会学习类别的更精确特征。困难负样本在区分某物与其他所有物品时特别有用，例如区分“狗”和“非狗”。
- en: Dataset Size
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集大小
- en: So far we’ve talked about what kind of data to include in a dataset, but how
    much of it do we need? “All of it" is a temptingly cheeky answer. For our model
    to be as precise as possible, we should use as many examples as possible. But
    it’s rarely possible to get all of the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了应该在数据集中包含什么样的数据，但我们需要多少数据呢？“全部”是一个引人发笑的回答。为了让我们的模型尽可能精确，我们应该使用尽可能多的样本。但通常情况下，完全获得所有数据是不可能的。
- en: Choosing the size of your dataset means considering a trade-off between accuracy
    and the time and energy it takes to acquire the data. Acquiring data can be expensive
    or slow, or, as we saw with our clover example, sometimes the key class of the
    dataset is rare and seldom encountered. Because labeled data is generally expensive
    and slow to acquire, we should have some idea of how much we need before we get
    started.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 选择数据集的大小意味着在准确性和获取数据所需的时间和精力之间进行权衡。获取数据可能很昂贵或者很慢，或者正如我们在三叶草的例子中看到的那样，有时候数据集中的关键类别很稀有，鲜少遇到。由于标注数据通常很昂贵且获取较慢，因此在开始之前我们应该对自己需要多少数据有所了解。
- en: Unfortunately, the truth is that there is no formula that answers the question
    of how much data is enough data. After a certain point, there are diminishing
    returns on the benefit of additional data. Moving from 100 examples to 1,000 examples
    might boost the accuracy of the model dramatically, but moving from 1,000 to 10,000
    examples might offer only a small increase in accuracy. The increased accuracy
    needs to be balanced against the effort and expense of acquiring an additional
    9,000 training examples.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，事实是没有一个公式可以回答多少数据才足够的问题。在某个临界点之后，增加更多数据的效益会递减。从100个样本增加到1,000个样本可能会大幅提升模型的准确性，但从1,000个增加到10,000个样本可能只会带来很小的准确度提升。提高的准确度需要与获取额外9,000个训练样本的努力和成本进行平衡。
- en: Another factor to consider is the model itself. Models have a *capacity*, which
    determines the complexity they can support relative to the amount of training
    data available. The capacity of a model is directly related to its number of parameters.
    A larger model with more parameters will require a lot of training data to be
    able to find the proper parameter settings. And though it’s often a good idea
    to have more training examples than model parameters, deep learning can work well
    when there is less training data than parameters. For example, if the classes
    are very different from each other—think buildings versus oranges—and it’s easy
    for us to tell the difference, the model likely will also learn the difference
    quickly, so we can get away with fewer training examples. On the other hand, if
    we’re trying to separate wolves from huskies, we might need a lot more data. We
    will discuss what to do when you don’t have a lot of training data in [Chapter
    5](ch05.xhtml#ch05), but none of those tricks are a good substitute for simply
    getting more data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是模型本身。模型有一个*容量*，决定了它能支持的复杂度与可用训练数据量之间的关系。模型的容量与其参数数量直接相关。一个具有更多参数的较大模型需要大量的训练数据才能找到合适的参数设置。尽管通常建议训练示例的数量多于模型的参数数量，但当训练数据少于参数时，深度学习也能很好地工作。例如，如果类别之间的差异非常大——比如建筑物与橙子——且我们很容易区分它们，那么模型可能也会迅速学习到这个差异，从而能够用较少的训练示例进行训练。另一方面，如果我们试图区分狼和哈士奇犬，可能需要更多的数据。在[第5章](ch05.xhtml#ch05)中，我们会讨论当数据量不足时该怎么办，但这些技巧并不能替代获取更多数据的做法。
- en: 'The only correct answer to the question of how much data is needed is “all
    of it.” Get as much as is *practical*, given the constraints of the problem: expense,
    time, rarity, and so forth.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“需要多少数据”的问题，唯一正确的答案是“所有的数据”。在考虑问题的限制条件（如费用、时间、稀缺性等）下，尽可能多地获取数据是*可行*的。
- en: Data Preparation
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Before we move on to building actual datasets, we’re going to cover two situations
    you’ll likely encounter before you can feed your dataset to a model: how to scale
    features, and what to do if a feature value is missing.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建实际的数据集之前，首先需要讨论两个你在将数据集输入模型之前可能会遇到的情况：如何对特征进行缩放，以及当特征值缺失时该怎么办。
- en: Scaling Features
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征缩放
- en: A feature vector built from a set of different features might have a variety
    of ranges. One feature might take on a wide range of values, say, –1000 to 1000,
    while another might be restricted to a range of 0 to 1\. Some models will not
    work well when this happens, as one feature dominates the others because of its
    range. Also, some model types are happiest when features have a mean value that
    is close to 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从一组不同特征构建的特征向量可能具有不同的取值范围。一个特征的取值范围可能很广，例如从–1000到1000，而另一个特征的取值范围可能被限制在0到1之间。当发生这种情况时，一些模型可能无法正常工作，因为某个特征由于其取值范围的原因主导了其他特征。此外，一些模型类型在特征的均值接近0时表现最好。
- en: The solution to these issues is scaling. We’ll assume for the time being that
    every feature in the feature vector is continuous. We’ll work with a fake dataset
    consisting of five features and 15 samples. This means that our dataset has 15
    samples—feature vectors and their labels—and each of the feature vectors has five
    elements. We’ll assume there are three classes. The dataset looks like [Table
    4-4](ch04.xhtml#ch4tab4).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的方法是进行缩放。暂时假设特征向量中的每个特征都是连续的。我们将使用一个包含五个特征和15个样本的虚拟数据集。这意味着我们的数据集有15个样本——特征向量及其标签——每个特征向量包含五个元素。我们假设有三类。该数据集如[表
    4-4](ch04.xhtml#ch4tab4)所示。
- en: '**Table 4-4:** A Hypothetical Dataset'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-4：** 一个假设的数据集'
- en: '| **Sample** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]**
    | **Label** |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **示例** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |
    **标签** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
- en: '| *1* | 6580 | 0.4908 | 3.0150 | 0.00004484 | 38462706 | 1 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| *1* | 6580 | 0.4908 | 3.0150 | 0.00004484 | 38462706 | 1 |'
- en: '| *2* | 7563 | 0.9349 | 4.3465 | 0.00001003 | 6700340 | 2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| *2* | 7563 | 0.9349 | 4.3465 | 0.00001003 | 6700340 | 2 |'
- en: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
- en: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
- en: '| *5* | 9498 | 0.0244 | 2.7887 | 0.00008880 | 78543394 | 2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| *5* | 9498 | 0.0244 | 2.7887 | 0.00008880 | 78543394 | 2 |'
- en: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 | 19101443 | 2 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 | 19101443 | 2 |'
- en: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
- en: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
- en: '| *9* | 843 | 0.7892 | 0.9804 | 0.00005798 | 10179751 | 1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| *9* | 843 | 0.7892 | 0.9804 | 0.00005798 | 10179751 | 1 |'
- en: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
- en: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
- en: '| *12* | 923 | 0.4159 | 1.7821 | 0.00002467 | 52404615 | 1 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| *12* | 923 | 0.4159 | 1.7821 | 0.00002467 | 52404615 | 1 |'
- en: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
- en: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
- en: As this is the first dataset covered in the book, let’s go over it thoroughly
    to introduce some notation and see what is what. The first column in [Table 4-4](ch04.xhtml#ch4tab4)
    is the sample number. The sample is an input, in this case a collection of five
    features representing a feature vector. Notice that the numbering starts at 0\.
    As we’ll be using Python arrays (NumPy arrays) for data, we’ll start counting
    at 0 in all cases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是本书中介绍的第一个数据集，我们将详细介绍它，介绍一些符号并看看每项内容。 [表4-4](ch04.xhtml#ch4tab4)中的第一列是样本编号。样本是输入，在本例中是表示特征向量的五个特征的集合。注意，编号从0开始。由于我们将使用Python数组（NumPy数组）来处理数据，因此我们在所有情况下都会从0开始计数。
- en: The next five columns are the features in each sample, labeled *x*[0] to *x*[4],
    again starting indices at 0\. The final column is the class label. Since there
    are three classes, the labels run from 0 through 2\. There are five samples from
    class 0, five from class 1, and five from class 2\. Therefore, this is a small
    but balanced dataset; the prior probability of each class is 33 percent, which
    should, ideally, be close to the actual prior probability of the classes appearing
    in the wild.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的五列是每个样本的特征，标记为 *x*[0] 到 *x*[4]，索引从 0 开始。最后一列是类别标签。由于有三类，标签从 0 到 2。类 0、类
    1 和类 2 分别有五个样本。因此，这是一个小但平衡的数据集；每个类的先验概率是 33%，这应该接近实际情况下各类出现的先验概率。
- en: If we had a model, then each row would be its own input. Writing {*x*[0], *x*[1],
    *x*[2], *x*[3], *x*[4]} to refer to these is tedious, so instead, when we are
    referring to a full feature vector, we’ll use an uppercase letter. For example,
    we’d refer to Sample 2 as *X*[2] for dataset *X*. We’ll also sometimes use matrices—2D
    arrays of numbers—that are also labeled with uppercase letters, for clarity. When
    we want to refer to a single feature, we’ll use a lowercase letter with subscript,
    for example, *x*[3].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个模型，那么每一行将是其自身的输入。写 {*x*[0], *x*[1], *x*[2], *x*[3], *x*[4]} 来表示这些特征向量是繁琐的，因此，当我们提到一个完整的特征向量时，我们将使用大写字母。例如，我们会将样本
    2 称为 *X*[2]，表示数据集 *X*。我们还将有时使用矩阵——二维数组——它们也用大写字母标记，以便清晰。当我们想引用单个特征时，我们将使用带下标的小写字母，例如
    *x*[3]。
- en: Let’s look at the ranges of the features. The minimum, maximum, and range (the
    difference between the maximum and minimum) of each feature are shown in [Table
    4-5](ch04.xhtml#ch4tab5).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下特征的范围。每个特征的最小值、最大值和范围（最大值与最小值的差）在[表4-5](ch04.xhtml#ch4tab5)中展示。
- en: '**Table 4-5:** The Minimum, Maximum, and Range of the Features in [Table 4-4](ch04.xhtml#ch4tab4)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-5:** [表4-4](ch04.xhtml#ch4tab4)中各特征的最小值、最大值和范围'
- en: '| **Feature** | **Minimum** | **Maximum** | **Range** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **最小值** | **最大值** | **范围** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *x*[0] | 843.0 | 9498.0 | 8655.0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| *x*[0] | 843.0 | 9498.0 | 8655.0 |'
- en: '| *x*[1] | 0.0002 | 0.9564 | 0.9562 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | 0.0002 | 0.9564 | 0.9562 |'
- en: '| *x*[2] | 0.0705 | 4.8231 | 4.7526 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| *x*[2] | 0.0705 | 4.8231 | 4.7526 |'
- en: '| *x*[3] | 4.03e-06 | 9.564e-05 | 9.161e-05 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| *x*[3] | 4.03e-06 | 9.564e-05 | 9.161e-05 |'
- en: '| *x*[4] | 6700340.0 | 96703562.0 | 90003222.0 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| *x*[4] | 6700340.0 | 96703562.0 | 90003222.0 |'
- en: 'Note the use of computer notation like 9.161e-05\. This how computers represent
    scientific notation: 9.161 × 10^(*–*5) = 0.00009161\. Notice, also, that each
    feature covers a very different range. Because of this, we’ll want to scale the
    features so their ranges are more similar. Scaling is a valid thing to do prior
    to training a model as long as you scale all new inputs the same way.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意使用计算机表示法，如9.161e-05。这是计算机表示科学计数法的方式：9.161 × 10^(*–*5) = 0.00009161。还要注意，每个特征的范围差异很大。因为这个原因，我们需要对特征进行缩放，使它们的范围更加相似。缩放是在训练模型之前进行的有效操作，只要你以相同的方式缩放所有新的输入。
- en: Mean Centering
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 均值中心化
- en: 'The simplest form of scaling is *mean centering*. This is easy to do: from
    each feature, simply subtract the mean (average) value of the feature over the
    entire dataset. The mean over a set of values, *x*[*i*] *i* = 0, 1, 2, … is simply
    the sum of each value divided by the number of values:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的缩放形式是 *均值中心化*。这很容易做到：对于每个特征，简单地从每个特征中减去整个数据集的均值。一个值集合 *x*[*i*]，其中 *i* =
    0, 1, 2, … 的均值就是每个值的总和除以值的数量：
- en: '![image](Images/064equ01.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/064equ01.jpg)'
- en: 'The mean value for feature *x*[0] is 5022, so to center *x*[0], we replace
    each value like so:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 *x*[0] 的均值是 5022，因此为了对 *x*[0] 进行中心化，我们将每个值替换为：
- en: '*x[i]* ← *x[i]* – 5022, *i* = 0, 1, 2, . . .'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*x[i]* ← *x[i]* – 5022, *i* = 0, 1, 2, . . .'
- en: where in this case the *i* index is across the samples, not the other elements
    of the feature vector.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*i* 索引是针对样本的，而不是特征向量的其他元素。
- en: Repeating the preceding steps for the mean value of all the other features will
    center the entire dataset. The result is that the mean value of each feature,
    over the dataset, is now 0, meaning the feature values themselves are all above
    and below 0\. For deep learning, mean centering is often done by subtracting a
    mean image from each input image.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有其他特征的均值重复前述步骤，将会对整个数据集进行均值中心化。结果是，每个特征的均值在数据集上现在为 0，意味着特征值本身都位于 0 的上下。对于深度学习，均值中心化通常通过从每个输入图像中减去均值图像来完成。
- en: Changing the Standard Deviation to 1
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将标准差改为 1
- en: 'Mean centering helps, but the distribution of values around 0 remains the same
    as before the mean was subtracted. All we did was shift the data down toward 0\.
    The spread of values around the mean has a formal name: it’s called the *standard
    deviation*, and it’s computed as the average difference of the data values and
    the mean:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化有助于改善数据，但围绕 0 的值的分布仍然与减去均值之前相同。我们所做的只是将数据向 0 移动。围绕均值的值的分布有一个正式的名称：它被称为
    *标准差*，其计算方式是数据值与均值之间的平均差：
- en: '![image](Images/065equ01.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/065equ01.jpg)'
- en: The letter *σ* (sigma) is the usual name for the standard deviation in mathematics.
    You don’t need to memorize this formula. It’s there to show us how to calculate
    a measure of the spread, or range, of the data relative to the mean value of the
    data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 字母 *σ*（sigma）是数学中标准差的常用名称。你不需要记住这个公式。它在这里是为了向我们展示如何计算数据相对于均值的分布或范围的度量。
- en: Mean centering changes ![Image](Images/xbar.jpg) to 0, but it does not change
    *σ*. Sometimes we want to go further and, along with mean centering, change the
    spread of the data so that the ranges are the same, meaning the standard deviation
    for each feature is 1\. Fortunately, doing this is straightforward. We replace
    each feature value, *x*, with
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化将 ![Image](Images/xbar.jpg) 变为 0，但不会改变 *σ*。有时我们希望进一步处理，除了进行均值中心化，还要改变数据的分布，使得数据的范围相同，即每个特征的标准差为
    1。幸运的是，做到这一点是很直接的。我们将每个特征值 *x* 替换为
- en: '![image](Images/065equ02.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/065equ02.jpg)'
- en: where ![Image](Images/xbar.jpg) and *σ* are the mean and standard deviation
    of each feature across the dataset. For example, the preceding toy dataset can
    be stored as a 2D NumPy array
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/xbar.jpg) 和 *σ* 是数据集中每个特征的均值和标准差。例如，前面的示例数据集可以存储为一个 2D NumPy
    数组
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'so that the entire dataset can be processed in one line of code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这样整个数据集就可以通过一行代码进行处理：
- en: '[PRE1]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This approach is called *standardization* or *normalizing*, and you should do
    it to most datasets, especially when using one of the traditional models discussed
    in [Chapter 6](ch06.xhtml#ch06). Whenever possible, standardize your dataset so
    that the features have 0 mean and a standard deviation of 1.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法叫做 *标准化* 或 *归一化*，你应该对大多数数据集执行此操作，特别是在使用[第六章](ch06.xhtml#ch06)中讨论的传统模型时。只要可能，应该标准化你的数据集，使得特征具有
    0 的均值和 1 的标准差。
- en: If we standardize the preceding dataset, what will it look like? Subtracting,
    per feature, the mean value of that feature and dividing by the standard deviation
    gives us a new dataset ([Table 4-6](ch04.xhtml#ch4tab6)). Here, we’ve shortened
    the numbers to four decimal digits for display and have dropped the label.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们标准化前述数据集，它会是什么样子？每个特征减去该特征的均值并除以标准差，将给我们一个新的数据集（[表 4-6](ch04.xhtml#ch4tab6)）。在这里，我们将数字显示精度缩短为四位小数并省略了标签。
- en: '**Table 4-6:** The Data in [Table 4-4](ch04.xhtml#ch4tab4) Standardized'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-6：** [表 4-4](ch04.xhtml#ch4tab4) 中的数据已标准化'
- en: '| **Sample** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]**
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **样本** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| *0* | 0.6930 | –1.1259 | –1.5318 | 0.9525 | 1.1824 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| *1* | 0.5464 | –0.0120 | 0.5051 | –0.0192 | –0.1141 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| *2* | 0.8912 | 1.3826 | 1.5193 | –1.1996 | –1.1403 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| *3* | 1.1690 | 0.4970 | –0.1712 | –0.5340 | 0.3047 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| *4* | –0.9221 | –0.1071 | 0.3079 | –0.3885 | –0.4753 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| *5* | 1.5699 | –1.4767 | 0.3327 | 1.4714 | 1.1807 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| *6* | –0.3479 | 0.4775 | 1.8823 | –1.4031 | –0.7396 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| *7* | 0.0887 | –0.4353 | –1.7377 | –1.2349 | 1.7456 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| *8* | 1.0775 | 0.9524 | 1.2475 | 0.7291 | –1.1207 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| *9* | –1.4657 | 0.9250 | –1.0446 | 0.4262 | –1.0279 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| *10* | –1.3332 | 1.4501 | 0.0323 | 1.1102 | –0.8966 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| *11* | 0.3005 | –1.4500 | –0.2615 | 1.7033 | –0.2505 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| *12* | –1.4377 | –0.2472 | –0.4340 | –0.7032 | 0.3362 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| *13* | 0.3016 | –1.5527 | –0.6213 | 0.1780 | –0.7517 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| *14* | –1.1315 | 0.7225 | –0.0250 | –1.0881 | 1.7674 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: If you compare the two tables, you’ll see that after our manipulations, the
    features are more similar than they were in the original set. If we look at *x*[3],
    we’ll see that the mean of the values is *–* 1.33*e –* 16 = *–*1.33 × 10^(*–*16)
    = *–*0.000000000000000133, which is virtually 0\. Good! This is what we want.
    If you do the calculations, you’d see that the means of the other features are
    similarly close to 0\. What about the standard deviation? For *x*[3] it’s 0.99999999,
    which is virtually 1—again, this is what we’d like. We’ll use this new, transformed,
    dataset to train the model.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we must apply the per feature means and standard deviations, as
    measured on the training set, to any new inputs we’re giving to the model:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/067equ01.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Here, *x*[new] is the new feature vector we want to apply to the model, and
    ![Image](Images/067equ02.jpg) and *σ*[train] are the mean and standard deviation,
    per feature, from the training set.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Missing Features
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes we don’t have all the features we need for a sample. We might have
    forgotten to make a measurement, for example. These are *missing features*, and
    we need to find a way to correct them, since most models don’t have the ability
    to accept missing data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to fill in the missing values with values that are outside of
    the feature’s range, in the hopes that the model will learn to ignore those values
    or make more use of other features. Indeed, some more advanced deep learning models
    intentionally zero some of the input as a form of regularization (we’ll see what
    that means in later chapters).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we’ll learn the second most obvious solution: replacing missing features
    with the mean value of features over the dataset. Let’s look again at our practice
    dataset from earlier. This time, we’ll have some missing data to deal with ([Table
    4-7](ch04.xhtml#ch4tab7)).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-7:** Our Sample Dataset ([Table 4-4](ch04.xhtml#ch4tab4)) with Some
    Holes'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]**
    | **Label** |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| *1* |  | 0.4908 |  | 0.00004484 | 38462706 | 1 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| *1* |  | 0.4908 |  | 0.00004484 | 38462706 | 1 |'
- en: '| *2* | 7563 | 0.9349 | 4.3465 |  | 6700340 | 2 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| *2* | 7563 | 0.9349 | 4.3465 |  | 6700340 | 2 |'
- en: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
- en: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
- en: '| *5* | 9498 |  | 2.7887 | 0.00008880 | 78543394 | 2 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| *5* | 9498 |  | 2.7887 | 0.00008880 | 78543394 | 2 |'
- en: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 |  | 2 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 |  | 2 |'
- en: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
- en: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
- en: '| *9* |  |  | 0.9804 |  | 10179751 | 1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| *9* |  |  | 0.9804 |  | 10179751 | 1 |'
- en: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
- en: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
- en: '| *12* | 923 |  |  | 0.00002467 |  | 1 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| *12* | 923 |  |  | 0.00002467 |  | 1 |'
- en: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
- en: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
- en: The blank spaces indicate missing values. The means of each feature, ignoring
    missing values, are shown in [Table 4-8](ch04.xhtml#ch4tab8).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 空白区域表示缺失值。每个特征的均值（忽略缺失值）见[表 4-8](ch04.xhtml#ch4tab8)。
- en: '**Table 4-8:** The Means for Features in [Table 4-7](ch04.xhtml#ch4tab7)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-8：** [表 4-7](ch04.xhtml#ch4tab7)中各特征的均值'
- en: '| ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 5223.6 | 0.5158 | 2.345 | 4.71e-05 | 42957735.0 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 5223.6 | 0.5158 | 2.345 | 4.71e-05 | 42957735.0 |'
- en: If we replace each missing value with the mean, we’ll get a dataset we can standardize
    and use to train a model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用均值替换每个缺失值，我们就能得到一个可以标准化并用于训练模型的数据集。
- en: Of course, real data is better, but the mean is the simplest substitute we can
    reasonably use. If the dataset is large enough, we might instead generate a histogram
    of the values of each feature and select the mode—the most common value—but using
    the mean should work out just fine, especially if your dataset has a lot of samples
    and the number of missing features is fairly small.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，真实数据更好，但均值是我们可以合理使用的最简单替代方法。如果数据集足够大，我们可以生成每个特征值的直方图，并选择众数——最常见的值——但使用均值应该没问题，特别是当数据集样本量大且缺失特征的数量较少时。
- en: Training, Validation, and Test Data
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练、验证和测试数据
- en: Now that we have a dataset—a collection of feature vectors—we’re ready to start
    training a model, right? Well, actually, no. That’s because we don’t want to use
    the entire dataset for training. We’ll need to use some of the data for other
    purposes, and so we need to split it into at least two subsets, although ideally
    we’d have three. We call these subsets the training data, validation data, and
    test data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个数据集——一个特征向量的集合——我们准备开始训练模型，对吧？其实，答案是否定的。因为我们并不想用整个数据集来训练模型。我们需要将数据集分割成至少两个子集，理想情况下应该有三个。我们将这三个子集称为训练数据、验证数据和测试数据。
- en: The Three Subsets
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 三个子集
- en: The *training data* is the subset we use to train the model. The important thing
    here is selecting feature vectors that well represent the parent distribution
    of the data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练数据*是我们用来训练模型的子集。这里重要的是选择能够很好地代表数据母体分布的特征向量。'
- en: The *test data* is the subset used to evaluate how well the trained model is
    doing. We *never* use the test data when training the model; that would be cheating,
    because we’d be testing the model on data it has seen before. Put the test dataset
    aside, resist the temptation to touch it until the model is complete, and then
    use it to evaluate the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*测试数据*是用来评估训练后的模型表现的子集。在训练模型时，我们*绝不会*使用测试数据；那样做是作弊，因为我们会在模型已经见过的数据上进行测试。将测试数据集放在一边，直到模型完成后再使用它来评估模型。'
- en: The third dataset is the *validation data*. Not every model needs a validation
    dataset, but for deep learning models, having one is helpful. We use the validation
    dataset during training as though it’s test data to get an idea of how well the
    training is working. It can help us decide things like when to stop training and
    whether we’re using the proper model.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个数据集是*验证数据*。并非每个模型都需要验证数据集，但对于深度学习模型，拥有一个验证集是很有帮助的。在训练过程中，我们将验证数据集当作测试数据使用，目的是了解训练效果如何。它有助于我们决定什么时候停止训练，以及是否使用了合适的模型。
- en: For example, a neural network has some number of layers, each with some number
    of nodes. We call this the *architecture* of the model. During training, we can
    test the performance of the neural network with the validation data to figure
    out whether we should continue training or stop and try a different architecture.
    We don’t train the model with the validation set, and we don’t use the validation
    set to modify model parameters. We also can’t use validation data when reporting
    actual model performance, since we used results based on the validation data to
    select the model in the first place. Again, this would make it seem like the model
    is doing better than it is.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个神经网络有若干层，每层包含一定数量的节点。我们称之为模型的*架构*。在训练过程中，我们可以使用验证数据来测试神经网络的性能，以判断是否应该继续训练，或者停止并尝试不同的架构。我们不会使用验证集来训练模型，也不会用验证集来修改模型参数。我们在报告实际模型性能时也不能使用验证数据，因为我们最初选择模型时是基于验证数据的结果。再说一遍，这样会让模型看起来比实际情况更好。
- en: '[Figure 4-3](ch04.xhtml#ch4fig3) illustrates the three subsets and their relationships
    to one another. On the left is the whole dataset. This is the entire collection
    of feature vectors and associated labels. On the right are the three subsets.
    The training data and the validation data work together to train and develop the
    model, while the test data is held back until the model is ready for it. The size
    of the cylinders reflects the relative amount of data that should fall into each
    subset, though in practice the validation and test subsets might be even smaller.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](ch04.xhtml#ch4fig3)展示了三个子集及其相互关系。左边是整个数据集，包含所有特征向量及相关标签。右边是三个子集。训练数据和验证数据共同用于训练和开发模型，而测试数据则在模型准备好之前被保留。圆柱体的大小反映了应该分配给每个子集的相对数据量，尽管在实际应用中，验证集和测试集可能会更小。'
- en: '![image](Images/04fig03.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig03.jpg)'
- en: '*Figure 4-3: Relationships among training, validation, and test subsets*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-3：训练集、验证集和测试集之间的关系*'
- en: 'To recap: use the training and validation sets to build the model and the test
    set to evaluate it.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：使用训练集和验证集来构建模型，使用测试集来评估模型。
- en: Partitioning the Dataset
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集划分
- en: How much data should go into each dataset?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集应该包含多少数据？
- en: A typical split is 90 percent for training, 5 percent for validation, and 5
    percent for testing. For deep learning models, this is fairly standard. If you’re
    working with a very large dataset, you could go as low as 1 percent each for validation
    and testing. For classic models, which might not learn as well, we might want
    to make the test dataset larger to ensure we are able to generalize to a wide
    variety of possible inputs. In those cases, you might try something like 80 percent
    for training and 10 percent each for validation and test. If you’re not using
    validation data, the full 20 percent might go to testing. These larger test sets
    might be appropriate for multiclass models that have classes with low prior probabilities.
    Or, since the test set is not used to define the model, you might increase the
    number of rare classes in the test set. This might be of particular value should
    missing the rare class be a costly event (think missing a tumor in a medical image).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的划分是，90%用于训练，5%用于验证，5%用于测试。对于深度学习模型，这个比例是相当标准的。如果你处理的是一个非常大的数据集，验证集和测试集的比例可以低至各自1%。对于经典模型，由于可能不太擅长学习，我们可能希望将测试数据集做得更大，以确保能够对各种输入进行广泛的泛化。在这种情况下，你可能会尝试80%用于训练，10%用于验证和测试。如果你不使用验证数据，全部20%可能用于测试。这些较大的测试集可能适用于多类别模型，特别是当类别的先验概率较低时。或者，因为测试集不用于定义模型，你可以增加测试集中稀有类别的数量。如果漏掉稀有类别会带来高成本（例如在医学影像中漏掉肿瘤），这种做法可能特别有价值。
- en: 'Now that we’ve determined how much data to put into each set, let’s use `sklearn`
    to generate a dummy dataset that we can partition:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了每个数据集应该包含多少数据，让我们使用`sklearn`生成一个虚拟数据集，进行划分：
- en: '[PRE2]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we’ve used two classes and 20 features to generate 10,000 samples. The
    dataset is imbalanced, with 90 percent of the samples in class 0 and 10 percent
    in class 1\. The output is a 2D array of samples (`x`) and associated 0 or 1 labels
    (`y`). The dataset is generated from multidimensional Gaussians that are the analogs
    of the normal bell curve in more than one dimension, but that doesn’t matter to
    us right now. The useful part for us is that we have a collection of feature vectors
    and labels, so that we can look at ways in which the dataset might be split into
    subsets.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用了两个类别和 20 个特征生成了 10,000 个样本。数据集是不平衡的，类 0 占 90%，类 1 占 10%。输出是一个二维数组的样本（`x`）和相应的
    0 或 1 标签（`y`）。数据集是由多维高斯分布生成的，类似于正态分布的钟形曲线，但这对我们现在来说并不重要。对我们有用的部分是我们有一组特征向量和标签，因此我们可以查看如何将数据集划分成子集。
- en: The key to the preceding code is the call to `make_classification`, which accepts
    the number of samples requested and the fraction for each class. The `np.where`
    calls simply find all the class 0 and class 1 instances so that `len` can count
    them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的关键是调用了 `make_classification`，该函数接受请求的样本数以及每个类别的比例。`np.where` 调用只是用来找到所有类
    0 和类 1 的实例，以便 `len` 可以计算它们的数量。
- en: 'Earlier, we talked about the importance of preserving—or at least approaching—the
    actual prior probabilities of the different classes in our dataset. If one class
    makes up 10 percent of real world cases, it would ideally make up 10 percent of
    our dataset. Now we need to find a way to preserve this prior class probability
    in the subsets we make for training, validation, and test. There are two main
    ways to do this: partitioning by class and random sampling.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们讨论了保持—或至少接近—数据集中不同类别的真实先验概率的重要性。如果某个类别在现实世界中占比为 10%，那么它在数据集中也应该理想地占据 10%。现在我们需要找到一种方法，在我们为训练、验证和测试所划分的子集中保持这一先验类别概率。主要有两种方法可以做到这一点：按类划分和随机采样。
- en: Partitioning by Class
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 按类划分
- en: The exact approach, which is suitable when the dataset is small or perhaps when
    one class is rare, is to determine the number of samples representing each class,
    and then set aside selected percentages of each, by class, before merging them
    together. So, if there are 9,000 samples from class 0, and 1,000 samples from
    class 1, and we want to put 90 percent of the data into training and 5 percent
    each into validation and test, we would select 8,100 samples, *at random*, from
    the class 0 collection and 900 samples, *at random*, from the class 1 collection
    to make up the training set. Similarly, we would randomly select 450 of the remaining
    900 unused class 0 samples for the validation set along with 50 of the remaining
    unused class 1 data. The remaining class 0 and class 1 samples become the test
    set.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法适用于数据集较小或某一类较为稀有的情况，其具体做法是先确定每个类别的样本数量，然后按类别将一定比例的样本分配到不同的子集，最后再将它们合并。如果类
    0 有 9,000 个样本，类 1 有 1,000 个样本，并且我们希望将 90% 的数据用于训练，5% 的数据用于验证和测试，我们会随机从类 0 中选择
    8,100 个样本，*随机* 从类 1 中选择 900 个样本来组成训练集。类似地，我们会从剩余的 900 个未使用的类 0 样本中随机选择 450 个用于验证集，同时从剩余的未使用的类
    1 数据中选择 50 个。剩下的类 0 和类 1 样本将构成测试集。
- en: '[Listing 4-1](ch04.xhtml#ch4lis1) shows the code to construct the subsets using
    a 90/5/5 split of the original data.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4-1](ch04.xhtml#ch4lis1) 显示了使用 90/5/5 划分原始数据来构造子集的代码。'
- en: import numpy as np
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 numpy 作为 np
- en: from sklearn.datasets import make_classification
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.datasets 导入 make_classification
- en: ❶ a,b = make_classification(n_samples=10000, weights=(0.9,0.1))
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ a,b = make_classification(n_samples=10000, weights=(0.9,0.1))
- en: idx = np.where(b == 0)[0]
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.where(b == 0)[0]
- en: x0 = a[idx,:]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: x0 = a[idx,:]
- en: y0 = b[idx]
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = b[idx]
- en: idx = np.where(b == 1)[0]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.where(b == 1)[0]
- en: x1 = a[idx,:]
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = a[idx,:]
- en: y1 = b[idx]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = b[idx]
- en: ❷ idx = np.argsort(np.random.random(y0.shape))
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ idx = np.argsort(np.random.random(y0.shape))
- en: y0 = y0[idx]
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = y0[idx]
- en: x0 = x0[idx]
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: x0 = x0[idx]
- en: idx = np.argsort(np.random.random(y1.shape))
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y1.shape))
- en: y1 = y1[idx]
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = y1[idx]
- en: x1 = x1[idx]
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = x1[idx]
- en: ❸ ntrn0 = int(0.9*x0.shape[0])
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ ntrn0 = int(0.9*x0.shape[0])
- en: ntrn1 = int(0.9*x1.shape[0])
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ntrn1 = int(0.9*x1.shape[0])
- en: xtrn = np.zeros((int(ntrn0+ntrn1),20))
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn = np.zeros((int(ntrn0+ntrn1),20))
- en: ytrn = np.zeros(int(ntrn0+ntrn1))
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = np.zeros(int(ntrn0+ntrn1))
- en: xtrn[:ntrn0] = x0[:ntrn0]
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn[:ntrn0] = x0[:ntrn0]
- en: xtrn[ntrn0:] = x1[:ntrn1]
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn[ntrn0:] = x1[:ntrn1]
- en: ytrn[:ntrn0] = y0[:ntrn0]
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn[:ntrn0] = y0[:ntrn0]
- en: ytrn[ntrn0:] = y1[:ntrn1]
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn[ntrn0:] = y1[:ntrn1]
- en: ❹ n0 = int(x0.shape[0]-ntrn0)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ n0 = int(x0.shape[0]-ntrn0)
- en: n1 = int(x1.shape[0]-ntrn1)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: n1 = int(x1.shape[0]-ntrn1)
- en: xval = np.zeros((int(n0/2+n1/2),20))
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: xval = np.zeros((int(n0/2+n1/2),20))
- en: yval = np.zeros(int(n0/2+n1/2))
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: yval = np.zeros(int(n0/2+n1/2))
- en: xval[:(n0//2)] = x0[ntrn0:(ntrn0+n0//2)]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: xval[:(n0//2)] = x0[ntrn0:(ntrn0+n0//2)]
- en: xval[(n0//2):] = x1[ntrn1:(ntrn1+n1//2)]
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: xval[(n0//2):] = x1[ntrn1:(ntrn1+n1//2)]
- en: yval[:(n0//2)] = y0[ntrn0:(ntrn0+n0//2)]
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: yval[:(n0//2)] = y0[ntrn0:(ntrn0+n0//2)]
- en: yval[(n0//2):] = y1[ntrn1:(ntrn1+n1//2)]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: yval[(n0//2):] = y1[ntrn1:(ntrn1+n1//2)]
- en: ❺ xtst = np.concatenate((x0[(ntrn0+n0//2):],x1[(ntrn1+n1//2):]))
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ xtst = np.concatenate((x0[(ntrn0+n0//2):],x1[(ntrn1+n1//2):]))
- en: ytst = np.concatenate((y0[(ntrn0+n0//2):],y1[(ntrn1+n1//2):]))
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = np.concatenate((y0[(ntrn0+n0//2):],y1[(ntrn1+n1//2):]))
- en: '*Listing 4-1: Exact construction of training, validation, and test datasets*'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-1：精确构建训练集、验证集和测试集*'
- en: There’s a lot of bookkeeping in this code. First, we create the dummy dataset
    ❶ and split it into class 0 and class 1 collections, stored in `x0,y0` and `x1,y1`,
    respectively. We then randomize the ordering ❷. This will let us pull off the
    first *n* samples for the subsets without worrying that we might be introducing
    a bias because of ordering in the data. Because of how `sklearn` generates the
    dummy dataset, this step isn’t required, but it’s always a good idea to ensure
    randomness in the ordering of samples.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中有很多记录。首先，我们创建了虚拟数据集 ❶ 并将其拆分为类别0和类别1集合，分别存储在`x0,y0`和`x1,y1`中。然后我们随机化数据的顺序
    ❷。这样，我们可以从子集中提取前 *n* 个样本，而不必担心因数据的顺序引入偏差。由于`sklearn`生成虚拟数据集的方式，这一步并非必须，但确保样本顺序的随机性始终是一个好主意。
- en: We use a trick that’s helpful when reordering samples. Because we store the
    feature vectors in one array and the labels in another, the NumPy shuffle methods
    will not work. Instead, we generate a random vector of the same length as our
    number of samples and then use `argsort` to return the indices of the vector that
    would put it in sorted order. Since the values in the vector are random, the ordering
    of the indices used to sort it will also be random. These indices then reorder
    the samples and labels so that the each label is still associated with the correct
    feature vector.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个在重新排序样本时非常有用的小技巧。因为我们将特征向量存储在一个数组中，标签存储在另一个数组中，所以NumPy的shuffle方法无法直接使用。相反，我们生成一个与样本数相同长度的随机向量，然后使用`argsort`返回一个将该向量按排序顺序排列的索引。由于向量中的值是随机的，用来排序它的索引顺序也会是随机的。这些索引随后会重新排序样本和标签，确保每个标签仍然与正确的特征向量相关联。
- en: Next, we extract the first 90 percent of samples for the two classes and build
    the training subset with samples in `xtrn` and labels in `ytrn` ❸. We do the same
    for the 5 percent validation set ❹ and the remaining 5 percent for the test set
    ❺.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取两类的前90%的样本，并使用`xtrn`中的样本和`ytrn`中的标签构建训练子集 ❸。我们对5%的验证集 ❹ 和剩下的5%的测试集 ❺
    进行相同的操作。
- en: Partitioning by class is tedious, to say the least. We do know, however, that
    the class 0 to class 1 ratio in each of the subsets is exactly the same.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别划分是相当繁琐的，但至少我们知道，每个子集中的类别0与类别1的比例完全相同。
- en: Random Sampling
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机抽样
- en: Must we be so precise? In general, no. The second common method for partitioning
    the full dataset is via random sampling. If we have enough data—and 10,000 samples
    is enough data—we can build our subsets by randomizing the full dataset and then
    extracting the first 90 percent of samples as the training set, the next 5 percent
    as the validation set, and the last 5 percent as the test set. This is what we
    show in [Listing 4-2](ch04.xhtml#ch4lis2).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须如此精确吗？一般来说，不必如此。第二种常见的划分完整数据集的方法是通过随机抽样。如果我们有足够的数据——10,000个样本已经足够——我们可以通过随机化完整数据集来构建我们的子集，然后提取前90%的样本作为训练集，接下来的5%作为验证集，最后5%作为测试集。这就是我们在[清单
    4-2](ch04.xhtml#ch4lis2)中展示的内容。
- en: ❶ x,y = make_classification(n_samples=10000, weights=(0.9,0.1))
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x,y = make_classification(n_samples=10000, weights=(0.9,0.1))
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y.shape[0]))
- en: x = x[idx]
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: y = y[idx]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[idx]
- en: ❷ ntrn = int(0.9*y.shape[0])
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ntrn = int(0.9*y.shape[0])
- en: nval = int(0.05*y.shape[0])
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: nval = int(0.05*y.shape[0])
- en: ❸ xtrn = x[:ntrn]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xtrn = x[:ntrn]
- en: ytrn = y[:ntrn]
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = y[:ntrn]
- en: xval = x[ntrn:(ntrn+nval)]
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: xval = x[ntrn:(ntrn+nval)]
- en: yval = y[ntrn:(ntrn+nval)]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: yval = y[ntrn:(ntrn+nval)]
- en: xtst = x[(ntrn+nval):]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: xtst = x[(ntrn+nval):]
- en: ytst = y[(ntrn+nval):]
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = y[(ntrn+nval):]
- en: '*Listing 4-2: Random construction of training, validation, and test datasets*'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-2：随机构建训练集、验证集和测试集*'
- en: We randomize the dummy dataset stored in `x` and `y` ❶. We need to know how
    many samples to include in each of the subsets. First, the number of samples for
    the training set is 90 percent of the total in the dataset ❷, while the number
    in the validation set is 5 percent of the total. The remainder, also 5 percent,
    is the test set ❸.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机化存储在`x`和`y`中的虚拟数据集❶。我们需要知道每个子集中应该包含多少样本。首先，训练集中的样本数量是数据集中总样本的90％❷，而验证集中的样本数量是总样本的5％。剩下的5％就是测试集❸。
- en: This method is so much simpler than the one shown in [Listing 4-1](ch04.xhtml#ch4lis1).
    What’s the downside of using it? The possible downside is that the mix of classes
    in each of these subsets might not quite be the fractions we want. For example,
    imagine we want a training set of 9,000 samples, or 90 percent of the original
    10,000 samples, with 8,100 of them from class 0, and 900 of them from class 1\.
    Running the [Listing 4-2](ch04.xhtml#ch4lis2) code 10 times gives the splits between
    class 0 and class 1 in the training set that are shown in [Table 4-9](ch04.xhtml#ch4tab9).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法比在[清单 4-1](ch04.xhtml#ch4lis1)中展示的方法要简单得多。使用它有什么缺点？可能的缺点是每个子集中的类的混合可能不完全是我们想要的比例。例如，假设我们想要一个包含9,000个样本的训练集，或者说占原始10,000个样本的90％，其中8,100个样本来自类0，900个样本来自类1。运行[清单
    4-2](ch04.xhtml#ch4lis2)中的代码10次，得到的训练集中的类0和类1的分布如[表 4-9](ch04.xhtml#ch4tab9)所示。
- en: '**Table 4-9:** Ten Training Splits Generated by Random Sampling'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-9：** 通过随机抽样生成的十个训练集划分'
- en: '| **Run** | **Class 0** | **Class 1** |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **运行** | **类0** | **类1** |'
- en: '| --- | --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 8058 (89.5) | 942 (10.5) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8058 (89.5) | 942 (10.5) |'
- en: '| 2 | 8093 (89.9) | 907 (10.1) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8093 (89.9) | 907 (10.1) |'
- en: '| 3 | 8065 (89.6) | 935 (10.4) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 8065 (89.6) | 935 (10.4) |'
- en: '| 4 | 8081 (89.8) | 919 (10.2) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 8081 (89.8) | 919 (10.2) |'
- en: '| 5 | 8045 (89.4) | 955 (10.6) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 8045 (89.4) | 955 (10.6) |'
- en: '| 6 | 8045 (89.4) | 955 (10.6) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 8045 (89.4) | 955 (10.6) |'
- en: '| 7 | 8066 (89.6) | 934 (10.4) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 8066 (89.6) | 934 (10.4) |'
- en: '| 8 | 8064 (89.6) | 936 (10.4) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8064 (89.6) | 936 (10.4) |'
- en: '| 9 | 8071 (89.7) | 929 (10.3) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 8071 (89.7) | 929 (10.3) |'
- en: '| 10 | 8063 (89.6) | 937 (10.4) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 8063 (89.6) | 937 (10.4) |'
- en: The number of samples in class 1 ranges from as few as 907 samples to as many
    as 955 samples. As the number of samples of a particular class in the full dataset
    decreases, the number in the subsets will start to vary more. This is especially
    true of smaller subsets, like the validation and test sets. Let’s do a separate
    run, this time looking at the number of samples from each class in the *test*
    set ([Table 4-10](ch04.xhtml#ch4tab10)).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 类1的样本数量从最少907个样本到最多955个样本不等。随着完整数据集中某个特定类的样本数量减少，子集中的样本数量将开始变化更多。这在较小的子集上尤为明显，比如验证集和测试集。让我们进行一次单独的运行，这次查看*测试*集中的每个类的样本数量（见[表
    4-10](ch04.xhtml#ch4tab10)）。
- en: '**Table 4-10:** Ten Test Splits Generated by Random Sampling'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-10：** 通过随机抽样生成的十个测试集划分'
- en: '| **Run** | **Class 0** | **Class 1** |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| **运行** | **类0** | **类1** |'
- en: '| --- | --- | --- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 446 (89.2) | 54 (10.8) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 446 (89.2) | 54 (10.8) |'
- en: '| 2 | 450 (90.0) | 50 (10.0) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 450 (90.0) | 50 (10.0) |'
- en: '| 3 | 444 (88.8) | 56 (11.2) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 444 (88.8) | 56 (11.2) |'
- en: '| 4 | 450 (90.0) | 50 (10.0) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 450 (90.0) | 50 (10.0) |'
- en: '| 5 | 451 (90.2) | 49 (9.8) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 451 (90.2) | 49 (9.8) |'
- en: '| 6 | 462 (92.4) | 38 (7.6) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 462 (92.4) | 38 (7.6) |'
- en: '| 7 | 441 (88.2) | 59 (11.8) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 441 (88.2) | 59 (11.8) |'
- en: '| 8 | 449 (89.8) | 51 (10.2) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 449 (89.8) | 51 (10.2) |'
- en: '| 9 | 449 (89.8) | 51 (10.2) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 449 (89.8) | 51 (10.2) |'
- en: '| 10 | 438 (87.6) | 62 (12.4) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 438 (87.6) | 62 (12.4) |'
- en: In the test set, the number of samples from class 1 ranges from 38 to 62.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集中，类1的样本数量从38到62不等。
- en: Will these differences influence how the model learns? Probably not, but they
    might make the test results look better than they are, as most models struggle
    to identify the classes that are least common in the training set. The possibility
    exists of a pathological split that results in having no examples from a particular
    class, but in practice, it’s not really that likely unless your pseudorandom number
    generator is particularly poor. Still, it’s worth keeping the possibility in mind.
    If concerned, use the exact split approach in [Listing 4-1](ch04.xhtml#ch4lis1).
    In truth, the better solution is, as always, to get more data.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异会影响模型的学习吗？可能不会，但它们可能使测试结果看起来比实际情况要好，因为大多数模型在识别训练集中最少见的类别时都会遇到困难。确实存在某种病态划分的可能性，这种划分会导致某个类别没有任何示例，但在实际应用中，除非你的伪随机数生成器特别差，否则这种情况并不常见。不过，还是值得注意这个可能性。如果担心，可以使用[清单
    4-1](ch04.xhtml#ch4lis1)中的精确划分方法。事实上，最好的解决方法一如既往，是获取更多的数据。
- en: 'Algorithmically, the steps to produce the training, validation, and test splits
    are as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 从算法的角度来看，生成训练、验证和测试划分的步骤如下：
- en: Randomize the order of the full dataset so that classes are evenly mixed.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化完整数据集的顺序，以确保各类别均匀混合。
- en: Calculate the number of samples in the training (`ntrn`) and validation (`nval`)
    sets by multiplying the number of samples in the full dataset by the desired fraction.
    The remaining samples will fall into the test set.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将完整数据集中样本的数量乘以所需的比例，计算训练集（`ntrn`）和验证集（`nval`）中的样本数量。剩余的样本将进入测试集。
- en: Assign the first `ntrn` samples to the training set.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前`ntrn`个样本分配到训练集。
- en: Assign the next `nval` samples to the validation set.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将接下来的`nval`个样本分配到验证集。
- en: Finally, assign the remaining samples to the test set.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将剩余的样本分配到测试集。
- en: At all times, ensure that the order of the samples is truly random, and that
    when reordering the feature vectors, you’re sure to reorder the labels in the
    exact same sequence. If this is done, this simple splitting process will give
    a good split unless the dataset is very small or some classes are very rare.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 始终确保样本的顺序是真正随机的，并且在重新排序特征向量时，确保标签按完全相同的顺序进行重新排序。如果这样做，除非数据集非常小或某些类别非常稀有，否则这个简单的划分过程会给出一个良好的划分。
- en: We neglected to discuss one consequence of this approach. If the full dataset
    is small to begin with, partitioning it will make the training set even smaller.
    In [Chapter 7](ch07.xhtml#ch07), we’ll see a powerful approach to dealing with
    a small dataset, one that’s used heavily in deep learning. But first, let’s look
    at a principled way to work with a small dataset to get an idea of how well it
    will perform on new data.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们忽略了讨论这种方法的一个后果。如果原始数据集本身很小，那么将其划分后，训练集将会变得更小。在[第7章](ch07.xhtml#ch07)中，我们将看到一种强大的处理小数据集的方法，这种方法在深度学习中被广泛使用。但首先，让我们看一种有原则的处理小数据集的方式，以便了解它在新数据上的表现如何。
- en: k-Fold Cross Validation
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*k*-折交叉验证'
- en: Modern deep learning models typically need very large datasets, and therefore,
    you’re able to use a single training/validation/test split as described previously.
    More traditional machine learning models, like those in [Chapter 6](ch06.xhtml#ch06),
    however, often work with datasets that are too small (in general) for deep learning
    models. If we use a single training/validation/test split on those datasets, we
    might be holding too much data back for testing, or else have too few samples
    in the test set to get a meaningful measurement of how well the model is working.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习模型通常需要非常大的数据集，因此，你可以像之前描述的那样使用单一的训练/验证/测试划分。然而，像[第6章](ch06.xhtml#ch06)中的那些传统机器学习模型，通常使用的数据集（一般来说）对深度学习模型来说过小。如果我们对这些数据集使用单一的训练/验证/测试划分，我们可能会将太多数据保留用于测试，或者测试集中的样本太少，无法对模型的表现进行有意义的测量。
- en: One way to address this issue is to use *k-fold cross validation*, a technique
    that ensures each sample in the dataset is used at some point for training and
    testing. Use this technique for small datasets intended for traditional machine
    learning models. It can also be helpful as a way to decide between different models.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用*k*折交叉验证，这是一种确保数据集中每个样本都至少在某个时刻用于训练和测试的技术。对传统机器学习模型的小数据集使用此技术也非常有帮助。它还可以作为不同模型之间选择的辅助工具。
- en: To do *k*-fold cross validation, first partition the full, randomized dataset
    into *k* non-overlapping groups, *x*[0],*x*[1],*x*[2],…,*x*[*k–*1]. Your *k* value
    is arbitrary, though it typically ranges from 5 to 10\. [Figure 4-4](ch04.xhtml#ch4fig4)a
    shows this split, imagining the entire dataset laid out horizontally.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行*k*折交叉验证，首先将完整的随机化数据集划分为*k*个不重叠的组，*x*[0]，*x*[1]，*x*[2]，…，*x*[*k–1]。你的*k*值是任意的，通常在5到10之间。[图4-4](ch04.xhtml#ch4fig4)a展示了这种划分，假设整个数据集水平地展开。
- en: We can train a model by holding *x*[0] back as test data and using the other
    groups, *x*[1],*x*[2],…,*x*[*k–*1] as training data. We’ll ignore validation data
    for the time being; after building the current training data, we can always hold
    some of it back as validation data if we want. Call this trained model *m*[0].
    You can then start over from scratch, this time holding back *x*[1] as test data
    and training with all the other groups, including *x*[0]. We’ll get a new trained
    model. Call it *m*[1]. By design, *m*[0] and *m*[1] are the same *type* of model.
    What we are interested in here is multiple instances of the same type of model
    trained with different subsets of the full dataset.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将* x*[0] 作为测试数据，使用其他组* x*[1]、*x*[2]、…、*x*[*k–*1] 作为训练数据来训练模型。我们暂时忽略验证数据；在构建当前训练数据后，如果需要，我们可以随时将部分数据保留作为验证数据。将这个训练好的模型称为*m*[0]。然后你可以从头开始，重新进行训练，这次将*
    x*[1] 作为测试数据，并使用其他所有组进行训练，包括* x*[0]。我们将得到一个新的训练模型，称其为*m*[1]。根据设计，*m*[0]和*m*[1]是相同*类型*的模型。我们在这里关心的是，使用不同数据子集训练的同一类型模型的多个实例。
- en: Repeat this process for each of the groups, as in [Figure 4-4](ch04.xhtml#ch4fig4)b,
    and we’ll have *k* models trained with (*k –* 1)/*k* of the data each, holding
    1/*k* of the data back for testing. What *k* should be depends upon how much data
    is in the full dataset. Larger *k* means more training data but less test data.
    If the per model training time is low, tend toward a larger *k* as this increases
    the per model training set size.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个组重复这个过程，如[图 4-4](ch04.xhtml#ch4fig4)b所示，我们将拥有* k *个模型，每个模型训练时使用(*k–*1)/*k*的数据，并将1/*k*的数据保留用于测试。*k*的选择应取决于完整数据集的大小。较大的*k*意味着更多的训练数据，但测试数据较少。如果每个模型的训练时间较短，可以倾向选择较大的*k*，因为这会增加每个模型的训练集大小。
- en: '![image](Images/04fig04.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig04.jpg)'
- en: '*Figure 4-4:* k-*fold cross validation. Partitioning the dataset into non-overlapping
    regions*, k=*7(a). The first three train/test splits using first* x[0] *for test,
    then* x[1] *for test, and so on (b)*.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-4：* k-*折交叉验证。将数据集划分为不重叠的区域*，k=*7(a)。前三次的训练/测试划分使用* x[0] *作为测试集，然后是* x[1]
    *作为测试集，以此类推 (b)*。'
- en: Once the *k* models are trained, you can evaluate them individually and average
    their metrics to get an idea of how a model trained on the full dataset would
    behave. See [Chapter 11](ch11.xhtml#ch11) to learn about ways to evaluate a model.
    If using *k*-fold cross validation to select among two or more models (say, between
    using *k*-NN or a Support Vector Machine^([1](ch04.xhtml#ch04fn1))), repeat the
    full training and evaluation process for each type of model and compare their
    results.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦* k *个模型训练完成，你可以单独评估它们并平均它们的评估指标，从而大致了解在整个数据集上训练的模型会表现如何。请参见[第11章](ch11.xhtml#ch11)，了解如何评估模型。如果使用*
    k *折交叉验证来选择两种或更多模型（比如，在使用* k *-NN或支持向量机^([1](ch04.xhtml#ch04fn1)))之间进行选择，请对每种模型类型重复整个训练和评估过程，并比较它们的结果。
- en: 'Once we have an idea of how well the model is performing on the averaged evaluation
    metrics, we can start over again and train the selected model type using *all*
    of the dataset for training. This is the advantage of *k*-fold cross validation:
    it lets you have your cake and eat it, too.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们了解了模型在平均评估指标上的表现，我们就可以重新开始，使用*所有*数据集来训练选择的模型类型。这就是* k *折交叉验证的优势：它让你既能“吃蛋糕”，又能“享受蛋糕”。
- en: Look at Your Data
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看你的数据
- en: 'It’s quite easy to assemble features and feature vectors, and then go ahead
    and put the training, validation, and test sets together without pausing to *look*
    at the data to see if it makes sense. This is especially true with deep learning
    models using huge collections of images or other multidimensional data. Here are
    a few problems you’ll want to look out for:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 组装特征和特征向量非常容易，然后继续将训练集、验证集和测试集组合在一起，而不暂停去*查看*数据以确认其是否合理。对于使用大量图像或其他多维数据的深度学习模型，这种情况尤为常见。以下是你需要注意的一些问题：
- en: '**Mislabeled data** Assume we’re building a large dataset—one with hundreds
    of thousands of labeled samples. Further, assume that we’re going to use the dataset
    to build a model that will be able to tell the difference between dogs and cats.
    Naturally, we need to feed the model many, many dog images and many, many cat
    images. No problem, you say; we’ll just collect a lot of images using something
    like Google Images. Okay, that’ll work. But if you simply set up a script to download
    image search results matching “dog” and “cat,” you’ll also get a lot of other
    images that are not of dogs or cats, or images that contain dogs and cats along
    with other things. The labels won’t be perfect. While it is true that deep learning
    models can be resistant to such label noise, you want to avoid it whenever possible.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误标记的数据** 假设我们正在构建一个大型数据集——一个包含数十万标注样本的数据集。进一步假设我们将使用该数据集构建一个模型，该模型能够区分狗和猫。自然，我们需要给模型提供大量的狗狗图片和大量的猫猫图片。你可能会说，这没问题，我们只需要用类似
    Google 图片这样的工具收集很多图片。好吧，这样可以。但如果你只是设置一个脚本，下载匹配“狗”和“猫”的图片搜索结果，你也会得到很多其他的图片，这些图片不是狗或猫的，或者是包含狗和猫以及其他东西的图片。标签就不太完美了。虽然深度学习模型对于这种标签噪声有一定的抵抗力，但尽可能避免它总是更好的。'
- en: '**Missing or outlier data** Imagine you have a collection of feature vectors,
    and you have no idea how common it is that features are missing. If a large percentage
    of a particular feature is missing, that feature will become a hindrance to the
    model and you should eliminate it. Or, if there are extreme outliers in the data,
    you might want to remove those samples, especially if you’re going to standardize,
    since outliers will strongly affect the mean subtracted from the feature values.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺失或异常数据** 想象一下，你有一组特征向量，但你不知道哪些特征丢失的情况有多常见。如果某个特征的大部分数据缺失，那么这个特征将成为模型的障碍，你应该将其删除。或者，如果数据中有极端的异常值，你可能会希望删除这些样本，特别是如果你打算进行标准化，因为异常值会强烈影响从特征值中减去的均值。'
- en: Searching for Problems in the Data
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 搜索数据中的问题
- en: How can we look for these problems in the data? Well, for feature vectors, we
    can often load the dataset into a spreadsheet, if it isn’t too large. Or we could
    write a Python script to summarize the data, feature by feature, or bring the
    data into a statistics program and examine it that way.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在数据中寻找这些问题呢？嗯，对于特征向量，如果数据集不太大，我们通常可以将其加载到电子表格中。或者，我们可以写一个 Python 脚本逐一总结每个特征的数据，或者将数据导入统计程序，通过这种方式进行检查。
- en: Typically, when summarizing values statistically, we look at the mean and standard
    deviation, both defined previously, as well as the largest value and the smallest
    value. We could also look at the median, which is the value we get when we sort
    the values from smallest to largest and pick the one in the middle. (If the number
    of values is even, we’d average the two middle values.) Let’s look at one of the
    features from our earlier example. After sorting the values from smallest to largest,
    we can summarize the data in the following way.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在对值进行统计汇总时，我们会查看均值和标准差，这两者在之前已定义，以及最大值和最小值。我们也可以查看中位数，即我们从最小到大排序值后，选取中间的那个值。（如果值的个数是偶数，我们会对两个中间值求平均。）让我们来看一下之前例子中的一个特征。排序后的数据可以这样汇总：
- en: '| ***x*[2]** |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| ***x*[2]** |'
- en: '| --- |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| 0.0705 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 0.0705 |'
- en: '| 0.3408 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 0.3408 |'
- en: '| 0.9804 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 0.9804 |'
- en: '| 1.5362 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 1.5362 |'
- en: '| 1.7821 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 1.7821 |'
- en: '| 2.0085 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 2.0085 |'
- en: '| 2.1271 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 2.1271 |'
- en: '| **2.3190** |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| **2.3190** |'
- en: '| 2.3944 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 2.3944 |'
- en: '| 2.7561 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 2.7561 |'
- en: '| 2.7887 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 2.7887 |'
- en: '| 3.0150 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 3.0150 |'
- en: '| 3.9897 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 3.9897 |'
- en: '| 4.3465 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 4.3465 |'
- en: '| 4.8231 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 4.8231 |'
- en: '| Mean (![Image](Images/xbar.jpg)) | = | 2.3519 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 均值 (![Image](Images/xbar.jpg)) | = | 2.3519 |'
- en: '| Standard deviation (*σ*) | = | 1.3128 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 (*σ*) | = | 1.3128 |'
- en: '| Standard error (SE) | = | 0.3390 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 标准误差（SE） | = | 0.3390 |'
- en: '| Median | = | 2.3190 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | = | 2.3190 |'
- en: '| Minimum | = | 0.0705 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 最小值 | = | 0.0705 |'
- en: '| Maximum | = | 4.8231 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | = | 4.8231 |'
- en: We’ve already explored the concepts of mean, minimum, maximum, and standard
    deviation. The median is there, as well; I’ve highlighted it in the list of features
    on the left. Notice that after sorting, the median appears in the exact middle
    of the list. It’s often known as the *50th percentile*, because the same amount
    of data is above it as below.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了均值、最小值、最大值和标准差的概念。中位数也在其中；我在左边的特征列表中做了突出显示。注意，排序后，中位数正好位于列表的正中间。它通常被称为
    *50th percentile*，因为它上方和下方的数据量相等。
- en: 'There is also a new value listed, the *standard error*, also called the *standard
    error of the mean*. This is the standard deviation divided by the square root
    of the number of values in the dataset:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还列出了一个新值，*标准误差*，也叫做*均值的标准误差*。这是标准差除以数据集中值的平方根：
- en: '![image](Images/077equ01.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/077equ01.jpg)'
- en: 'The standard error is a measure of the difference between our mean value, ![Image](Images/xbar.jpg),
    and the mean value of the parent distribution. The basic idea is this: if we have
    more measurements, we’ll have a better idea of the parent distribution that is
    generating the data, and so the mean value of the measurements will be closer
    to the mean value of the parent distribution.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 标准误差是衡量我们均值与母体分布均值之间差异的一个指标。基本思想是这样的：如果我们有更多的测量值，我们就能更好地了解生成数据的母体分布，因此测量值的均值会更接近母体分布的均值。
- en: Notice also that the mean and the median are relatively close to each other.
    The phrase *relatively close* has no rigorous mathematical meaning, of course,
    but we can use it as an ad hoc indicator that the data might be normally distributed,
    meaning we could reasonably replace the missing values by the mean (or median),
    as we saw previously.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 另外请注意，均值和中位数彼此相对接近。术语*相对接近*当然没有严格的数学意义，但我们可以将其作为一个临时指示器，表明数据可能是正态分布的，这意味着我们可以合理地通过均值（或中位数）来替换缺失值，正如我们之前所看到的那样。
- en: The preceding values were computed easily using NumPy, as seen in [Listing 4-3](ch04.xhtml#ch4lis3).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的值通过NumPy轻松计算，如[Listing 4-3](ch04.xhtml#ch4lis3)所示。
- en: import numpy as np
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: ❶ f = [0.3408,3.0150,4.3465,2.1271,2.7561,
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f = [0.3408,3.0150,4.3465,2.1271,2.7561,
- en: 2.7887,4.8231,0.0705,3.9897,0.9804,
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 2.7887,4.8231,0.0705,3.9897,0.9804,
- en: 2.3944,2.0085,1.7821,1.5362,2.3190]
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3944,2.0085,1.7821,1.5362,2.3190]
- en: f = np.array(f)
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: f = np.array(f)
- en: print
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: print
- en: print("mean  = %0.4f" % f.mean())
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: print("mean  = %0.4f" % f.mean())
- en: print("std   = %0.4f" % f.std())
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: print("std   = %0.4f" % f.std())
- en: ❷ print("SE    = %0.4f" % (f.std()/np.sqrt(f.shape[0])))
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ print("SE    = %0.4f" % (f.std()/np.sqrt(f.shape[0])))
- en: print("median= %0.4f" % np.median(f))
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: print("median= %0.4f" % np.median(f))
- en: print("min   = %0.4f" % f.min())
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: print("min   = %0.4f" % f.min())
- en: print("max   = %0.4f" % f.max())
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: print("max   = %0.4f" % f.max())
- en: '*Listing 4-3: Calculating basic statistics. See* feature_stats.py.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 4-3: 计算基本统计信息。请参见* feature_stats.py。'
- en: After loading NumPy, we manually define the *x*[2] features (`f`) and turn them
    into a NumPy array ❶. Once the data is a NumPy array, calculating the desired
    values is straightforward, as all of them, except the standard error, are simple
    method or function calls. The standard error is calculated via the preceding formula
    ❷ where the first element of the tuple NumPy returns for the shape is the number
    of elements in a vector.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 加载NumPy后，我们手动定义了*x*[2]特征（`f`），并将其转换为NumPy数组❶。一旦数据是NumPy数组，计算所需的值就变得简单，因为除标准误差外，所有值都是简单的方法或函数调用。标准误差通过前面的公式❷计算，其中NumPy返回的元组的第一个元素是向量中元素的数量。
- en: Numbers are nice, but pictures are often better. You can visualize the data
    with a *box plot* in Python. Let’s generate one to view the standardized values
    of our dataset. Then we’ll discuss what the plot is showing us. The code to create
    the plot is in [Listing 4-4](ch04.xhtml#ch4lis4).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 数字很好，但图片通常更好。你可以用Python中的*箱形图*来可视化数据。让我们生成一个来查看我们数据集的标准化值。然后我们将讨论图表展示了什么。创建图表的代码见[Listing
    4-4](ch04.xhtml#ch4lis4)。
- en: import numpy as np
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pyplot as plt
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: ❶ d = [[ 0.6930, -1.1259, -1.5318,  0.9525,  1.1824],
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ d = [[ 0.6930, -1.1259, -1.5318,  0.9525,  1.1824],
- en: '[ 0.5464, -0.0120,  0.5051, -0.0192, -0.1141],'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.5464, -0.0120,  0.5051, -0.0192, -0.1141],'
- en: '[ 0.8912,  1.3826,  1.5193, -1.1996, -1.1403],'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.8912,  1.3826,  1.5193, -1.1996, -1.1403],'
- en: '[ 1.1690,  0.4970, -0.1712, -0.5340,  0.3047],'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.1690,  0.4970, -0.1712, -0.5340,  0.3047],'
- en: '[-0.9221, -0.1071,  0.3079, -0.3885, -0.4753],'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[-0.9221, -0.1071,  0.3079, -0.3885, -0.4753],'
- en: '[ 1.5699, -1.4767,  0.3327,  1.4714,  1.1807],'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.5699, -1.4767,  0.3327,  1.4714,  1.1807],'
- en: '[-0.3479,  0.4775,  1.8823, -1.4031, -0.7396],'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[-0.3479,  0.4775,  1.8823, -1.4031, -0.7396],'
- en: '[ 0.0887, -0.4353, -1.7377, -1.2349,  1.7456],'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.0887, -0.4353, -1.7377, -1.2349,  1.7456],'
- en: '[ 1.0775,  0.9524,  1.2475,  0.7291, -1.1207],'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.0775,  0.9524,  1.2475,  0.7291, -1.1207],'
- en: '[-1.4657,  0.9250, -1.0446,  0.4262, -1.0279],'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.4657,  0.9250, -1.0446,  0.4262, -1.0279],'
- en: '[-1.3332,  1.4501,  0.0323,  1.1102, -0.8966],'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.3332,  1.4501,  0.0323,  1.1102, -0.8966],'
- en: '[ 0.3005, -1.4500, -0.2615,  1.7033, -0.2505],'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.3005, -1.4500, -0.2615,  1.7033, -0.2505],'
- en: '[-1.4377, -0.2472, -0.4340, -0.7032,  0.3362],'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.4377, -0.2472, -0.4340, -0.7032,  0.3362],'
- en: '[ 0.3016, -1.5527, -0.6213,  0.1780, -0.7517],'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.3016, -1.5527, -0.6213,  0.1780, -0.7517],'
- en: '[-1.1315,  0.7225, -0.0250, -1.0881,  1.7674]]'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.1315,  0.7225, -0.0250, -1.0881,  1.7674]]'
- en: ❷ d = np.array(d)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: plt.boxplot(d)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 4-4: A box plot of the standardized toy dataset*. See box_plot.py.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: The values themselves are in [Table 4-6](ch04.xhtml#ch4tab6). We can store the
    data as a 2D array and make the box plot using [Listing 4-4](ch04.xhtml#ch4lis4).
    We manually define the array ❶ and then plot it ❷. The plot is interactive, so
    experiment with the environment provided until you feel comfortable with it. The
    old-school floppy disk icon will store the plot to your disk.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: The box plot generated by the program is shown in [Figure 4-5](ch04.xhtml#ch4fig5).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig05.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-5: The box plot produced by [Listing 4-4](ch04.xhtml#ch4lis4)*'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: How do we interpret the box plot? I’ll show you by examining the box representing
    the standardized feature *x*[2], shown in [Figure 4-6](ch04.xhtml#ch4fig6).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: The lower box line, Q1, marks the end of the first quartile. This means that
    25 percent of the data values for a feature are less than this value. The median,
    Q2, is the 50 percent mark, and therefore is the end of the second quartile. Half
    the data values are less than this value. The upper box line, Q3, is the 75 percent
    mark. The remaining 25 percent of the data values are above Q3.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig06.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-6: The standardized feature x2 from our dataset*'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Two lines above and below the box are also shown. These are the *whiskers*.
    (Matplotlib calls them *fliers*, but this is an unconventional term.) The whiskers
    are the values at Q1 *–* 1.5 × IQR and Q3 + 1.5 × IQR. By convention, values outside
    this range are considered *outliers*.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Looking at outliers can be helpful, because you might realize they’re mistakes
    in data entry and drop them from the dataset. Whatever you do with the outliers,
    however, be prepared to justify it should you ever plan on publishing or otherwise
    presenting results based on the dataset. Similarly, you might be able to drop
    samples with missing values, but make sure there’s no systematic error causing
    the missing data, and check that you’re not introducing bias into the data by
    dropping those samples. In the end, common sense should override slavish adherence
    to convention.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Cautionary Tales
  id: totrans-426
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: So, at the risk of being repetitive, *look at your data*. The more you work
    with it, the more you will understand it, and the more effectively you will be
    able to make reasonable decisions about what goes in and what comes out, and *why*.
    Recall that the goal of the dataset is to faithfully and completely capture the
    parent distribution, or what the data will look like in the wild when the model
    is used.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Two quick anecdotes come to mind. They both illustrate ways models may well
    learn things we did not intend or even consider.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: The first was told to me as an undergraduate student in the 1980s. In this story,
    an early form of neural network was tasked with detecting tank and non-tank images.
    The neural network seemed to work well in testing, but when used in the field,
    the detection rate dropped rapidly. The researchers realized that the tank images
    were taken on a cloudy day, and the non-tank were taken on a sunny day. The recognition
    system had not learned the difference between tanks and non-tanks at all; instead,
    it had learned the difference between cloudy and sunny days. The moral of this
    story is that the training set needs to include *all* of the conditions the model
    will see in the wild.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: The second anecdote is more recent. I heard it in a talk at the Neural Information
    Processing Systems (NIPS) 2016 conference in Barcelona, Spain, and later found
    it repeated in the researchers’ paper.^([2](ch04.xhtml#ch04fn2)) In this case,
    the authors, who were demonstrating their technique for getting a model to explain
    its decisions, trained a model that claimed to tell the difference between images
    of huskies and images of wolves. The model appeared to work rather well, and during
    the talk, the authors polled the audience composed of machine learning researchers
    about how believable the model was. Most thought it was a good model. Then, using
    their technique, the speaker revealed that the network had not learned much, if
    anything, about the difference between huskies and wolves. Instead, it had learned
    that the wolf pictures had snow in the background and the husky pictures did not.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Think about your data and be on the lookout for unintended consequences. Models
    are not human. We bring a lot of preconceived notions and unintended biases to
    the dataset.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we described the components of a dataset (classes, labels,
    features, feature vectors) and then characterized a good dataset, emphasizing
    the importance of ensuring that the dataset well represents the parent distribution.
    We then described basic data preparation techniques including how to scale data
    and one approach for dealing with missing features. After that, we learned how
    to separate the full dataset into training, validation, and test subsets and how
    to apply *k*-fold cross validation, which is especially useful with small datasets.
    We ended the chapter with tips on how to simply examine the data to make sure
    it makes sense.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll take what we have learned in this chapter and apply
    it directly to construct the datasets we will use throughout the remainder of
    this book.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[1.](ch04.xhtml#Rch04fn1) These are examples of classical machine learning
    models. We’ll learn more about them later in the book.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[2.](ch04.xhtml#Rch04fn2) Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin.
    “Why Should I Trust You?: Explaining the Predictions of Any Classifier.” *In Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, pp. 1135–1144\. ACM, 2016.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.](ch04.xhtml#Rch04fn2) Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin.
    “为什么我应该信任你？：解释任何分类器的预测。” *在第22届ACM SIGKDD国际数据挖掘和知识发现会议论文集*，pp. 1135–1144\. ACM,
    2016.'
