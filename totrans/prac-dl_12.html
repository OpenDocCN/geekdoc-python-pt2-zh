<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch12"><span epub:type="pagebreak" id="page_283"/><strong><span class="big">12</span><br/>INTRODUCTION TO CONVOLUTIONAL NEURAL NETWORKS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In this chapter, we’ll introduce a new and potent approach to dealing with multidimensional information. In particular, we’ll work through the theory and high-level operation of <em>convolutional neural networks</em> (CNNs), a cornerstone of modern deep learning.</p>&#13;
<p class="indent">We’ll begin by presenting the motivations behind the development of CNNs. Convolutions are the heart of CNNs, so they’ll come next. We’ll discuss them in some detail, in particular how they’re used by the CNN. We’ll then introduce a basic CNN and work through its anatomy. We’ll use this basic CNN architecture for the remainder of the chapter. After we dissect a CNN, we’ll work through how convolutional layers work. Then come pooling layers. We’ll see what they do, what benefit they offer, and what price they exact in return. To round out our discussion of the fundamental components of a CNN, we’ll present the fully connected layers, which, in reality, are just the layers of a traditional, fully connected, feed-forward neural network like those of <a href="ch08.xhtml#ch08">Chapter 8</a>.</p>&#13;
<p class="indent">One topic will be conspicuously absent from this chapter: the mechanics of training a CNN. In part, we’ll gloss over training because it’s messy once <span epub:type="pagebreak" id="page_284"/>convolutional layers are introduced, but primarily because we’ve already discussed backpropagation in <a href="ch09.xhtml#ch09">Chapter 9</a>, and we use the same algorithm to train a CNN. We calculate the weights and biases of all layers from the average loss over the training minibatch and use backprop to determine the derivatives we need to update the weights and biases for each stochastic gradient descent step.</p>&#13;
<h3 class="h3" id="lev1_80">Why Convolutional Neural Networks?</h3>&#13;
<p class="noindent">CNNs have several advantages over traditional neural networks. First, the convolutional layers of a CNN require vastly fewer parameters than fully connected neural networks, as we’ll see later in the chapter. CNNs require fewer parameters because the convolution operation applies parameters in each layer to small subsets of the input instead of the entire input at once, as is done with a traditional neural network.</p>&#13;
<p class="indent">Second, CNNs introduce the idea of <em>spatial invariance</em>, the ability to detect a spatial relationship in the input regardless of where it appears. For example, if the input to a neural network is an image of a cat, a traditional neural network will take the image in as a single feature vector, meaning that if a cat appears in the upper-left corner of the image, the network will learn that cats can appear in the upper-left corners of the image but not that they can also appear in the lower-right corners (unless the training data contains examples with cats in the lower-right corners). For a CNN, however, the convolution operation can detect cats anywhere they appear.</p>&#13;
<p class="indent">While CNNs are usually used with two-dimensional inputs, they can also be used with one-dimensional inputs, like the feature vectors we have worked with up to now. However, the feature vectors we’ve worked with, like the iris measurements, don’t reflect any sort of spatial relationship as the parts of an image of a cat do. There’s nothing there for the convolution operation to take advantage of. This doesn’t mean that a CNN won’t work, but it does mean that it might not be the best sort of model to use. As always, we need to understand how various model types operate so we select the best model for the task at hand.</p>&#13;
<p class="note"><strong><span class="black">Note</span></strong> <em>Depending on who you ask, CNNs were either developed in 1980 by Fukushima to implement the Neocognitron model or in 1998 by LeCun et al. as presented in their famous paper “Gradient-Based Learning Applied to Document Recognition,” which, as of this writing, has been referenced over 21,000 times. My take is that both deserve credit, though LeCun used the phrase</em> convolutional neural network <em>or</em> convnet <em>as they are still sometimes called, and what is described in the paper is what we will work with in this book. The Neocognitron reflected some of the ideas in a CNN, but not CNNs themselves.</em></p>&#13;
<h3 class="h3" id="lev1_81">Convolution</h3>&#13;
<p class="noindent"><em>Convolution</em> involves sliding one thing over another. For us, this means sliding a <em>kernel</em>, a small 2D array, over the input, which might be the input image to the CNN or the output of a lower convolutional layer. There is <span epub:type="pagebreak" id="page_285"/>a formal mathematical definition of convolution, but it really won’t help us right now. Luckily, all our inputs are discrete, which means we can get away with a bit of a hand-waving. For simplicity, we’ll focus on only the two-dimensional case.</p>&#13;
<h4 class="h4" id="lev2_114">Scanning with the Kernel</h4>&#13;
<p class="noindent">The <em>kernel</em> is the thing we are asking the convolutional layer to learn during training. It’s a collection of small 2D arrays that we move over the input. Ultimately, the kernels become the weights of a convolutional layer in a CNN.</p>&#13;
<p class="indent">The essential operation of convolution is taking some small section of the input, the same size as the kernel, covering it with the kernel, performing some operation on the set of numbers to produce a single output number, and then repeating the process after moving the kernel to a new position in the input. Just how far the kernel is moved is known as the <em>stride</em>. Typically, the stride is 1, meaning the kernel slides over one element of the input.</p>&#13;
<p class="indent"><a href="ch12.xhtml#ch12fig1">Figure 12-1</a> shows the effect of convolution on part of an MNIST digit image.</p>&#13;
<div class="image" id="ch12fig1"><img src="Images/12fig01.jpg" alt="image" width="810" height="207"/></div>&#13;
<p class="figcap"><em>Figure 12-1: Convolving a kernel with an image</em></p>&#13;
<p class="indent">The image portion is on the left of <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, where you can see part of a handwritten 8. The boxes correspond to pixel intensities, though for presentation purposes, we’ve expanded the original image so that many shades of gray are visible in each “pixel” box. The actual pixel values the convolution works with are given next, after the arrow.</p>&#13;
<p class="indent">Here, the kernel is</p>&#13;
<div class="imagec"><img src="Images/285equ01.jpg" alt="image" width="132" height="63"/></div>&#13;
<p class="noindent">This is the set of numbers we’ll slide over the input pixels. This is a 3 × 3 matrix, so we need to cover a 3 × 3 region of the input image. The first 3 × 3 region, the upper-left corner, is</p>&#13;
<div class="imagec"><img src="Images/285equ02.jpg" alt="image" width="175" height="63"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_286"/>We said that convolution performs an operation with the kernel and the covered region as the input. The operation is straightforward: multiply corresponding entries and sum them. Finding the first output value of the convolution begins with</p>&#13;
<div class="imagec"><img src="Images/286equ01.jpg" alt="image" width="514" height="75"/></div>&#13;
<p class="noindent">When the preceding elements are summed, this gives the output value as</p>&#13;
<p class="center">0 + (–248) + 0 + (–145) + 759 + (–54) + 0 + (–253) + 0 = 59</p>&#13;
<p class="indent">Okay, the output of the first convolution operation is 59. What do we do with that number? The kernel is 3 × 3, an odd number along each side. This means that there is a middle element, the one with the 3 in it. The place in the output array where the middle number is gets replaced with the output value, the 59. <a href="ch12.xhtml#ch12fig1">Figure 12-1</a> shows the full output of the convolution. Sure enough, the first element of the output is 59, located at the center of the kernel when the kernel is covering the upper-left corner.</p>&#13;
<p class="indent">The remaining output values are calculated in precisely the same way but by moving the kernel over 1 pixel each time. When the end of a row is reached, the kernel moves back to the left side but down 1 pixel. In this way, it slides over the entire input image to produce the output shown in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, just like the scan lines of an old analog television.</p>&#13;
<p class="indent">The next output value is</p>&#13;
<div class="imagec"><img src="Images/286equ02.jpg" alt="image" width="472" height="75"/></div>&#13;
<p class="noindent">which sums to <em>–</em> 212, as we see on the right side of <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>.</p>&#13;
<p class="indent">Repeating the convolution operation produces the output shown in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>. Notice the empty boxes around the output. These values are empty because the middle of our 3 × 3 kernel does not cover the edge of the input array. Therefore, the output matrix of numbers is two smaller in each dimension than the input. If the kernel were 5 × 5, there would be a border 2 pixels wide instead of 1.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_287"/>Implementations of 2D convolution need to make a decision about these border pixels. There are options, and most toolkits support several of them. One is to simply ignore these pixels and make the output smaller than the input, as we’ve shown in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>. This approach is often known as <em>exact</em> or <em>valid</em> because we retain only values that are actually output by the operation.</p>&#13;
<p class="indent">Another approach is to imagine that a border of 0 values surrounds the input image. The border is as thick as is needed so that the kernel fits with its middle value matching the upper-left pixel of the input. For our example in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, this means a border of 1 pixel because the kernel is 3 × 3 and there is one element on either side of the kernel’s center value. If the kernel were 5 × 5, the border would be 2 pixels since there are two values on either side of the kernel center. This is known as <em>zero-padding</em> and gives an output that is the same size as the input. Instead of convolving a 28×28 pixel MNIST digit image with a 3 × 3 kernel and getting a 26×26 pixel output as shown in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, we get an output that is also 28×28 pixels.</p>&#13;
<p class="indent">If we zero pad the example image in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, we can fill in the first empty output square like so</p>&#13;
<div class="imagec"><img src="Images/287equ01.jpg" alt="image" width="482" height="75"/></div>&#13;
<p class="noindent">which sums to <em>–</em> 213. This means that the upper-left corner of the output matrix in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a>, which currently has an empty box, could be replaced by <em>–</em> 213. Similarly, the rest of the empty boxes would have values, and the output of the convolution operation would be 28×28 pixels.</p>&#13;
<h4 class="h4" id="lev2_115">Convolution for Image Processing</h4>&#13;
<p class="noindent">Convolution, when used in a neural network, is sometimes viewed as magical, a special operation that lets convolutional neural networks do the wonderful things that they can do. This is more or less true, but the convolution operation is certainly not anything new. Even if we ignore mathematics entirely and think only of the discrete convolution of 2D images, we see that image scientists were using convolution for image processing decades before convolution was applied to machine learning.</p>&#13;
<p class="indent">The convolution operation allows for all manner of image processing. For example, consider the images shown in <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>.</p>&#13;
<div class="image" id="ch12fig2"><span epub:type="pagebreak" id="page_288"/><img src="Images/12fig02.jpg" alt="image" width="557" height="539"/></div>&#13;
<p class="figcap"><em>Figure 12-2: 5 × 5 convolution kernels applied to an image</em></p>&#13;
<p class="indent">The original moon image is on the upper left. The other three images are the output from convolving the moon image with different 5 × 5 kernels. Moving clockwise from the upper right, the kernels either emphasize edges, diagonal structures (upper left to lower right), or blur the input image. All of this is accomplished by changing the values in the kernel, but the convolution operation remains the same.</p>&#13;
<p class="indent">From a machine learning perspective, the power of a convolutional approach comes partially from the savings in terms of parameters. If a model can learn a set of kernels, that is a smaller set of numbers to learn than the weights for a fully connected model. This is a good thing on its own. The fact that a convolution can pull out other information about an image, such as its slowly changing components (the blur of <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>), its rapidly changing components (the edges of <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>), or even components along a specific direction (the diagonals of <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>), means that the model gains insight as to what is in the input. And, since we move the kernel over the image, we’re not dependent upon <em>where</em> in the image these structures occur.</p>&#13;
<h3 class="h3" id="lev1_82">Anatomy of a Convolutional Neural Network</h3>&#13;
<p class="noindent">Medical students learn about anatomy by dissecting a cadaver to see the parts and how they relate to each other. In similar, though less challenging, <span epub:type="pagebreak" id="page_289"/>fashion, we’ll start with the body of a CNN, an illustration of its basic architecture, and then pull it apart to learn what each component is and what it does.</p>&#13;
<p class="indent"><a href="ch12.xhtml#ch12fig3">Figure 12-3</a> shows us our body. This is the default example CNN used by the Keras toolkit to train a model that classifies MNIST digits. We’ll use it as our standard for the remainder of this chapter.</p>&#13;
<div class="image" id="ch12fig3"><img src="Images/12fig03.jpg" alt="image" width="639" height="362"/></div>&#13;
<p class="figcap"><em>Figure 12-3: The architecture of a basic convolutional neural network</em></p>&#13;
<p class="indent">How do we interpret this figure? Like a traditional neural network, a CNN has an input and an output. In this case, the input is the digit image on the upper left. The network then flows left to right, following the arrows. At the end of the top row, the network continues on the following row. Note, we’ve duplicated the layer at the end of the top row and placed it at the beginning of the next row for presentation purposes.</p>&#13;
<p class="indent">The flow continues along the bottom row, again left to right, until the output is reached. The output here is a softmax layer to give us the likelihoods of each of the possible digits, just as we saw for the traditional neural networks of <a href="ch10.xhtml#ch10">Chapter 10</a>.</p>&#13;
<h4 class="h4" id="lev2_116">Different Types of Layers</h4>&#13;
<p class="noindent">Between each arrow is a layer of the network. The first thing we notice is that, unlike a traditional neural network, a CNN has many kinds of layers. Let’s list them here. We’ll discuss each in turn:</p>&#13;
<ul>&#13;
<li class="noindent">Convolutional (<em>Conv</em>)</li>&#13;
<li class="noindent">ReLU</li>&#13;
<li class="noindent">Pooling (<em>Pool</em>)</li>&#13;
<li class="noindent">Dropout</li>&#13;
<li class="noindent">Flatten</li>&#13;
<li class="noindent">Dense</li>&#13;
</ul>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_290"/>We should note that we’re using the Keras names for the layers. For instance, Keras uses <em>Dense</em> for what many other toolkits call <em>fully connected</em> or even <em>InnerProduct</em> layers.</p>&#13;
<p class="indent">Several of these layers should already be familiar. We know a ReLU layer implements a rectified linear unit that takes each of its inputs and asks if it is greater than or less than 0. If the input is less than 0, the output is 0; otherwise, the output is the input. We can express this mathematically as</p>&#13;
<p class="center">ReLU(<em>x</em>) = max(0, <em>x</em>)</p>&#13;
<p class="noindent">where the <em>max</em> function returns the largest of its two arguments.</p>&#13;
<p class="indent">Likewise, we mentioned dropout in <a href="ch09.xhtml#ch09">Chapter 9</a>. Dropout selects a percentage of its outputs at random during training and sets them to 0. This provides a powerful form of regularization to help the network learn meaningful representations of the input data. There are two dropout layers in our basic CNN. The first uses a probability of 25 percent, meaning during any minibatch pass while training, some 25 percent of the outputs will be set to 0. The second dropout layer uses a probability of 50 percent.</p>&#13;
<p class="indent">The <em>Flatten</em> and <em>Dense</em> layers are old friends, though we know them by another name and not as independent entities. Our traditional feedforward neural network uses fully connected layers to process a one-dimensional vector. Here, Flatten and Dense work together to implement a fully connected layer. The Flatten layer takes its input—usually a four-dimensional array (we’ll see why later)—and turns it into a vector. It does something similar to what we did to construct the vector form of the MNIST dataset, where we put the pixels of each row end-to-end to unravel the two-dimensional image. The Dense layer implements a traditional neural network layer, where each input value is mapped to each node of the Dense layer. Typically, the output of the Dense layer is passed to another Dense layer or a softmax layer to let the network make predictions.</p>&#13;
<p class="indent">Internally, many layers of a CNN expect four-dimensional arrays as inputs and produce four-dimensional arrays as outputs. The first dimension is the number of inputs in the minibatch. So, if we have a minibatch of 24, the first dimension of the 4D array will be 24.</p>&#13;
<p class="indent">The second and third dimensions are called the <em>height</em> and <em>width</em>. If the input to a layer is the input to the model (say, an image), then these dimensions are truly the height and width dimensions of the image. If the input is really the output of some other layer, say a (yet to be described) convolutional layer, the <em>height</em> and <em>width</em> refer to the output from applying a convolutional kernel to some input. For example, the output in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a> has height and width of 26.</p>&#13;
<p class="indent">The last dimension is the number of channels, if an input image; or the number of <em>feature maps</em>, if the output of a convolutional or pooling layer. The number of channels in an image is simply the number of bands, where a grayscale image has a single band and a color image typically has three bands, one each for red, green, and blue. Some color images also have an alpha channel used to specify how transparent a pixel is, but these are typically dropped before passing the image through a CNN.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_291"/>The output in <a href="ch12.xhtml#ch12fig1">Figure 12-1</a> is called a <em>feature map</em> because it is the response from convolving a kernel over an input. As we saw in <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>, convolving a kernel over an image can pull out features in the image, so the outputs of the kernels used by a convolutional layer are called <em>feature maps</em>.</p>&#13;
<p class="indent">This leaves two layers to investigate: <em>Convolutional</em> and <em>Pooling</em>. These layers are new.</p>&#13;
<p class="indent">In our basic CNN, the convolutions operate on sets of two-dimensional inputs where by <em>set</em> I mean a stack of two-dimensional arrays, where the third dimension is the number of channels or feature maps. This means that unlike every other model we’ve looked at in this book, the input here really is the full image, not a vector created from the image. In terms of CNNs, however, the convolutions need not operate on only two-dimensional inputs. Three-dimensional convolutions exist, as do one-dimensional, though both are seldom used compared to two-dimensional convolutions.</p>&#13;
<p class="indent">A pooling layer is used to reduce the spatial dimension of its input by combining input values according to some rule. The most common rule is <em>max</em>, where the largest value in the small block moved over the input is kept; the other values are discarded. Again, we’ll cover pooling layers at length in this chapter.</p>&#13;
<p class="indent">Many other layer types can be used by modern networks, and many of these are directly supported in Keras already, though it’s possible to add your own layers. This flexibility is one reason Keras often quickly supports new deep learning developments. As with a traditional neural network, for a layer to have weights that can be learned, the layer needs to be differentiable in a mathematical sense so that the chain rule can continue, and the partial derivatives can be calculated to learn how to adjust the weights during gradient descent. If the previous sentence is not clear, it’s time to review the backprop section of <a href="ch09.xhtml#ch09">Chapter 9</a>.</p>&#13;
<h4 class="h4" id="lev2_117">Passing Data Through the CNN</h4>&#13;
<p class="noindent">Let’s look again at <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>. A lot is happening here beyond just the order and names of the layers. Many layers have numbers in italics running along the bottom. These numbers represent the dimensions of the output of the layer, the height, width, and number of feature maps. If the layer has only a single number, it outputs a vector with that many elements.</p>&#13;
<p class="indent">The input to the CNN is a 28 × 28 × 1 image. The output of a convolutional layer is a set of feature maps. Thus the output of the first convolutional layer is 26 × 26 × 32, meaning there are 32 feature maps, each a 26 × 26 image calculated from the single 28 × 28 × 1 input image. Similarly, the output of the second convolutional layer is 24 × 24 × 64, a set of 64 feature maps derived from the 26 × 26 × 32 input, which was itself the output of the first convolutional layer.</p>&#13;
<p class="indent">We see that the pooling layer at the end of the first row takes its 24 × 24 × 64 input and reduces it to 12 × 12 × 64. The “max” label tells us what the pooling is doing; it takes a 2 × 2 region of the input and returns the largest value. Since the input is 2 × 2 and it returns only one value, this <span epub:type="pagebreak" id="page_292"/>reduces each 24 × 24 input to a 12 × 12 output. This process is applied to each feature map so that the output is 12 × 12 × 64.</p>&#13;
<p class="indent">Looking at the bottom row of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> shows us that the Flatten layer takes the 12 × 12 × 64 output of the pooling layer and turns it into a vector of 9,216 elements. Why 9,216? Because 12 × 12 × 64 = 9,216. Next, the Dense layer has 128 nodes, and, finally, our output softmax has 10 nodes because there are 10 classes, the digits 0 through 9.</p>&#13;
<p class="indent">In <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, the ReLU and Dropout layers have no numbers below them. These layers do not alter the shape of their inputs. They simply perform some operation on each of the elements regardless of the shape.</p>&#13;
<p class="indent">The convolutional layers of our basic CNN have other numbers associated with them: “3 × 3” and “32” or “64”. The 3 × 3 tells us the size of the convolutional kernel, and the 32 or 64 tells us the number of feature maps.</p>&#13;
<p class="indent">We already alluded to the 2 × 2 part of the pooling layer. This represents the size of the pooling kernel, which, much like a convolutional kernel, slides over the input, feature map by feature map (or channel by channel), to reduce the size of the input. Working with a 2 × 2 pooling kernel implies that, typically, the output will be one-half the size of the input in each of the row and column dimensions.</p>&#13;
<p class="indent"><a href="ch12.xhtml#ch12fig3">Figure 12-3</a> has familiar parts, but the presentation is new to us, and we have these mysterious new layers to think about, like convolutional and pooling layers, so we are sure to be somewhat nebulous in our understanding right now. That is perfectly fine. We have new ideas and some visual indications of how they link together to make a CNN. For now, this is all we need. The remainder of this chapter will, I hope, be a series of “aha!” moments for you as you think back to this figure and its parts. When you understand what each is doing, you’ll start to see why they are where they are in the processing chain, leading from image input to output softmax predictions.</p>&#13;
<h3 class="h3" id="lev1_83">Convolutional Layers</h3>&#13;
<p class="noindent">If our discussion of convolution ended with the preceding sections, we’d understand the essential operation but still be in the dark about exactly <em>how</em> a convolutional layer in a CNN works. Bearing this in mind, let’s look at how the convolution idea generalizes across the inputs and outputs of a CNN’s convolutional layer.</p>&#13;
<h4 class="h4" id="lev2_118">How a Convolution Layer Works</h4>&#13;
<p class="noindent">The input and output of a convolutional layer can both be thought of as stacks of 2D arrays (or matrices). The operation of the convolutional layer is best illustrated with a simple example showing how to map the input stack of arrays to the output stack.</p>&#13;
<p class="indent">Before we present our example, we need to introduce some terminology. We previously described the convolution operation in terms of applying a kernel to an input, both of which are 2D. We’ll continue to use the term <em>kernel</em> for this single, 2D matrix. When implementing a convolutional <span epub:type="pagebreak" id="page_293"/>layer, however, we’ll soon see that we need stacks of kernels, which are typically referred to in machine learning as <em>filters</em>. A filter is a stack of kernels. The filter, via its kernels, is applied over the input stack to produce the output stack. Since during training the model is learning kernels, it is fair to say that the model is also learning filters.</p>&#13;
<p class="indent">For our example, the input is a stack of two 5 × 5 arrays, the kernel size is 3 × 3, and we want an output stack that is three deep. Why three? Because, as the designer of the CNN architecture, we believe that learning three outputs will help the network learn the task at hand. The convolution operation determines the width and height of each output array; we select the depth. We’ll use valid convolution, losing a border of thickness one on the output, meaning our input will drop two in width and height. Therefore, a 5 × 5 input convolved with a 3 × 3 kernel will create a 3 × 3 output.</p>&#13;
<p class="indent">That accounts for the change in dimension, but how do we go from a stack of two arrays to a stack of three? The key to mapping the 5 × 5 × 2 input to the desired 3 × 3 × 3 output is the set of kernels, the filter, learned during training. Let’s see how the filter gives us the mapping we want.</p>&#13;
<p class="indent">We’ll assume we already know the filters at this point, each of which is a 3 × 3 × 2 stack of kernels. In general, if there are <em>M</em> arrays in the input stack and we want <em>N</em> arrays in the output stack using a kernel that is <em>K</em> × <em>K</em>, then we need a set of <em>N</em> filters, each one of which is a stack of <em>K</em> × <em>K</em> kernels <em>M</em> deep. Let’s explore why.</p>&#13;
<p class="indent">If we break up the stack so we can see each element clearly, our input stack looks like this:</p>&#13;
<div class="imagec"><img src="Images/293equ01.jpg" alt="image" width="236" height="225"/></div>&#13;
<p class="noindent">We have two 5 × 5 matrices labeled 0 and 1. The values were selected at random.</p>&#13;
<p class="indent">To get an output stack of three, we need a set of three filters. The stack of kernels in each filter is two deep, to mirror the number of arrays in the input stack. The kernels themselves are 3 × 3, so we have three 3 × 3 × 2 filters, where we convolve each kernel in the filter with the corresponding input array. The three filters are</p>&#13;
<div class="imagec"><img src="Images/293equ02.jpg" alt="image" width="429" height="165"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_294"/>where we’ve added 0 and 1 labels to show which kernels are applied to which input stack arrays. We also have a bias vector, as we did for the traditional neural network layers. This is a vector, one value for each kernel stack, that we add in at the end to help align the output of the convolutional layer to the data, just as we did for the traditional neural network layers. The bias adds one more degree of freedom to the layer—one more thing that can be learned to help the layer learn the most it can from the data. For our example, the bias vector is <em>b</em> = <em>{</em>1,0,2<em>}</em>, selected at random.</p>&#13;
<p class="indent">To get the output stack, we convolve each kernel of each filter with the corresponding input array, sum the elements of the resulting output, and add the bias value. For filter <em>k</em><sub>0</sub>, we convolve the first input array with the first kernel to get</p>&#13;
<div class="imagec"><img src="Images/294equ01.jpg" alt="image" width="493" height="126"/></div>&#13;
<p class="noindent">Note we’re using * to mean the full convolution operation, which is fairly standard. We repeat this operation for the second kernel in <em>k</em><sub>0</sub>, applying it to the second array of the input:</p>&#13;
<div class="imagec"><img src="Images/294equ02.jpg" alt="image" width="503" height="125"/></div>&#13;
<p class="noindent">Finally, we sum the two convolution outputs and add in the bias value:</p>&#13;
<div class="imagec"><img src="Images/294equ03.jpg" alt="image" width="616" height="75"/></div>&#13;
<p class="noindent">This gives us the first output array, the application of filter <em>k</em><sub>0</sub> to the input stack.</p>&#13;
<p class="indent">We repeat this process for filters <em>k</em><sub>1</sub> and <em>k</em><sub>2</sub> to get their outputs so that the final convolutional layer output for the given input is</p>&#13;
<div class="imagec"><img src="Images/294equ04.jpg" alt="image" width="394" height="75"/></div>&#13;
<p class="noindent">where we have written the stacked arrays side by side, a 3 × 3 × 3 output, as desired.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_295"/>Our convolutional layer example mapped a 5 × 5 × 2 input to a 3 × 3 × 3 output. If we naïvely used a fully connected layer instead, we would need a weight matrix that has 50 × 27 = 1350 weights that need to be learned. In contrast, the convolutional layer used only 3 × 3 × 2 weights per filter and three filters for a total of 54 weights, excluding bias values. This is a significant reduction.</p>&#13;
<h4 class="h4" id="lev2_119">Using a Convolutional Layer</h4>&#13;
<p class="noindent">The preceding example showed us how a convolutional layer works. Now let’s see the effect of one. Imagine that we’ve trained the network shown in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, so we have the weights and biases we need to run unknown images through the network. (You’ll see how to train a CNN in Chapter Experiments with Keras and MNIST.)</p>&#13;
<p class="indent">The first layer of the network in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> is a convolutional layer that maps a 28 × 28 × 1 input, the single-channel grayscale digit image, to a 26 × 26 × 32 output using a filter with 32 3 × 3 kernels. Therefore, we know that the weights between the input image and output fit in an array that is 3 × 3 × 1 × 32: 3 × 3 for the kernel size, 1 for the number of input channels, and 32 for the number of kernels in the filter.</p>&#13;
<p class="indent">After training, what do the 32 3 × 3 kernels of the filter actually look like? We can extract them from the trained model and print them as a set of 32 3 × 3 matrices. Here are the first two:</p>&#13;
<div class="imagec"><img src="Images/295equ01.jpg" alt="image" width="500" height="76"/></div>&#13;
<p class="noindent">This is nice, but not particularly helpful for building intuition about what the kernels do.</p>&#13;
<p class="indent">We can also visualize the kernels of a filter by converting the matrices to images. To get the kernels as images, we first note that all the kernel values happen to fit in the range [<em>–</em>0.5,+0.5], so if we add 0.5 to each kernel value, we’ve mapped the range to [0,1]. After this, multiplication by 255 converts the kernel values to byte values, the same values a grayscale image uses. Additionally, a value of 0 is now 127, which is a middle gray value.</p>&#13;
<p class="indent">After this conversion, the kernels can be shown as grayscale images, where negative kernel values are closer to black, and positive kernel values are closer to white. A final step is needed, however, because the mapped kernels are still only 3×3 pixels. The last step is to upscale the 3 × 3 images to 64×64 pixels. We’ll upscale in two different ways. The first uses nearest-neighbor sampling to show the kernel in blocks. The second uses a Lanczos filter, which smooths the image, making it easier to see the orientation of the kernel. <a href="ch12.xhtml#ch12fig4">Figure 12-4</a> shows the kernel images with the block versions on top and the smoothed versions on the bottom.</p>&#13;
<div class="image" id="ch12fig4"><span epub:type="pagebreak" id="page_296"/><img src="Images/12fig04.jpg" alt="image" width="591" height="598"/></div>&#13;
<p class="figcap"><em>Figure 12-4: The 32 learned kernels of the first convolutional layer (top). Smoothed versions to show the orientations more clearly (bottom).</em></p>&#13;
<p class="indent">These images represent the 32 kernels learned by the first convolutional layer of the model in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>. There is just enough detail in the images to hint that the kernels are selecting for structure in specific directions, just like the kernel that produced the image on the lower right of <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>, which emphasized diagonal structures.</p>&#13;
<p class="indent">Let’s turn our attention now to the effect of the kernels. What do the kernels do to an input MNIST image? We can run a sample MNIST image through the kernels by convolving each kernel with the sample, here a “3”, and following a process similar to the one that produced the preceding kernel images. The result is a set of 32 26 × 26 images, which we again upscale to 64 × 64 before displaying them. <a href="ch12.xhtml#ch12fig5">Figure 12-5</a> shows the result.</p>&#13;
<div class="image" id="ch12fig5"><span epub:type="pagebreak" id="page_297"/><img src="Images/12fig05.jpg" alt="image" width="675" height="339"/></div>&#13;
<p class="figcap"><em>Figure 12-5: The 32 kernels applied to a sample MNIST input</em></p>&#13;
<p class="indent">The order of the kernels shown in <a href="ch12.xhtml#ch12fig4">Figure 12-4</a> matches the images in <a href="ch12.xhtml#ch12fig5">Figure 12-5</a>. For example, the top-right image of <a href="ch12.xhtml#ch12fig4">Figure 12-4</a> shows a kernel that is light on the upper left and dark on the lower right, meaning it will detect structures along the diagonal from lower left to upper right. The output from applying this kernel to the sample is the upper-right image of <a href="ch12.xhtml#ch12fig5">Figure 12-5</a>. We see that the kernel enhanced parts of the three that are primarily diagonal from the lower left to the upper right. Note, this example is easy to interpret because the input is a grayscale image with a single channel. This means that there is no summing of kernel outputs across channels as we previously saw for the more general operation.</p>&#13;
<p class="indent">Typically, the first convolutional layer of a CNN learns kernels that select for specific orientations, textures, or, if the input image is RGB, colors. For the grayscale MNIST images, orientation is most important. The kernels learned at higher convolutional layers in the CNN are also selecting for things, but the interpretation of <em>what</em> the kernel is selecting becomes more and more abstract and difficult to understand. It is worth noting that the kernels learned by a CNN’s first convolutional layer are very similar to the first layer of visual processing in the mammalian brain. This is the primary visual cortex or V1 layer that detects lines and edges. Additionally, always keep in mind that the set of convolutional and pooling layers are there to learn a new feature representation: a new representation of the input image. This new representation does a better job of separating classes so that the fully connected layers can more easily distinguish between them.</p>&#13;
<h4 class="h4" id="lev2_120"><span epub:type="pagebreak" id="page_298"/>Multiple Convolutional Layers</h4>&#13;
<p class="noindent">Most CNNs have more than one convolutional layer. One reason for this is to build up features that are influenced by larger portions of the input as one goes deeper into the network. This introduces the ideas of <em>receptive field</em> and <em>effective receptive field</em>. The two concepts are similar and often confused. We can explain both by looking at <a href="ch12.xhtml#ch12fig6">Figure 12-6</a>.</p>&#13;
<div class="image" id="ch12fig6"><img src="Images/12fig06.jpg" alt="image" width="666" height="302"/></div>&#13;
<p class="figcap"><em>Figure 12-6: Receptive fields</em></p>&#13;
<p class="indent">The figure shows the <em>output</em> of two convolutional layers and the input to the model. We’re showing only the relevant parts of the output, using a 3 × 3 kernel. We’re also ignoring the depth of the filters since the receptive fields (defined next) are the same across the depth of the convolutional layer outputs.</p>&#13;
<p class="indent"><a href="ch12.xhtml#ch12fig6">Figure 12-6</a> should be read right to left as the arrows indicate. This is the opposite direction to the flow of data through the network. Here, we are looking back to earlier layers to see what has influenced the output value at a higher layer. The squares are output values. The rightmost shaded square is one of the outputs of Conv<sub>2</sub>. This is our starting point for looking back to see what influences this value. The arrows point to the outputs of Conv<sub>1</sub> that influence the shaded value in Conv<sub>2</sub>. The value in Conv<sub>2</sub> then has a 3 × 3 <em>receptive field</em> as it is directly influenced by the 3 × 3 shaded outputs of Conv<sub>1</sub>. This is how we’ll define <em>receptive field</em>: the set of outputs from the layer immediately before that directly influence the output of the current layer.</p>&#13;
<p class="indent">If we look at the set of input values that directly influence the 3 × 3 shaded region of Conv<sub>1</sub>, we see a 5 × 5 region. This makes sense: each shaded output of Conv<sub>1</sub> has a receptive field that is a 3 × 3 region of the input. The receptive field is 3 × 3 because the kernels of Conv<sub>1</sub> are 3 × 3 kernels. They overlap so that the shaded 5 × 5 input region is what all the shaded Conv<sub>1</sub> outputs are influenced by.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_299"/>Look again at the rightmost shaded output value. If we trace back to the input all the values that can influence it, we see that the shaded 5 × 5 region of the input can affect its value. This region of the input is the <em>effective recep- tive field</em> for the rightmost shaded output of Conv<sub>2</sub>. This output value responds, ultimately, to what is happening in the input image in the leftmost shaded region. As the CNN gets deeper, with additional convolutional layers, we can see how the effective receptive field can change so that deeper convolutional layers are working with values ultimately derived from larger and larger portions of the input to the model.</p>&#13;
<h4 class="h4" id="lev2_121">Initializing a Convolutional Layer</h4>&#13;
<p class="noindent">In <a href="ch09.xhtml#ch09">Chapter 9</a>, we saw that the performance of a traditional neural network was strongly influenced by the type of random initialization used for the learned weights and biases. The same is true for CNNs. Recall that the weights of a convolutional layer are the values of the kernels. They are learned during backprop, just like the weights of a traditional neural network. We need an intelligent way to initialize these values when we set up the network. Fortunately, the best initialization approaches for a traditional neural network apply directly to convolutional layers as well. For example, Keras defaults to Glorot initialization, which, as we saw in <a href="ch09.xhtml#ch09">Chapter 9</a>, is sometimes called Xavier initialization in other toolkits.</p>&#13;
<p class="indent">Let’s move on now from convolutional layers to pooling layers. These are simpler but perform an important, if somewhat controversial, function.</p>&#13;
<h3 class="h3" id="lev1_84">Pooling Layers</h3>&#13;
<p class="noindent">Our favorite figure, <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, shows a pooling layer after the first two convolutional layers. This pooling layer takes an input stack of 24 × 24 × 64 and produces an output stack of 12 × 12 × 64. The pooling part is marked as “2 × 2”. What’s going on here?</p>&#13;
<p class="indent">The key is the “2 × 2”. This means, for each of the 64 24 × 24 inputs, we move a 2 × 2 sliding window over the input and perform an operation similar to convolution. Not explicitly called out in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> is that the stride is also 2 so that the sliding 2 × 2 window jumps by two to avoid overlapping itself. This is typically the case, but doesn’t need to be. Since the pooling operation is per input in the stack, the output leaves the stack size unchanged. This is contrary to what a convolutional layer often does.</p>&#13;
<p class="indent">Let’s look at the pooling operation applied to a single input in the stack, a 24 × 24 matrix. <a href="ch12.xhtml#ch12fig7">Figure 12-7</a> shows us what’s going on.</p>&#13;
<div class="image" id="ch12fig7"><span epub:type="pagebreak" id="page_300"/><img src="Images/12fig07.jpg" alt="image" width="646" height="419"/></div>&#13;
<p class="figcap"><em>Figure 12-7: Applying 2 × 2 max pooling to an 8 × 8 input</em></p>&#13;
<p class="indent">The first 2 × 2 values are mapped to the first output value. Then we move over two and map the next 2 × 2 region to the output and so on until the entire input is mapped. The operation performed on each 2 × 2 region is up to the architect of the CNN. The most common operation is “select the largest value,” or <em>max pooling</em>, which is what we show in <a href="ch12.xhtml#ch12fig7">Figure 12-7</a>. This is also the operation the model in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> is performing. Another fairly common pooling operation is to average the values.</p>&#13;
<p class="indent">We can see from <a href="ch12.xhtml#ch12fig7">Figure 12-7</a> that the 8 × 8 input matrix is mapped to a 4 × 4 output matrix. This explains why the output of the pooling layer in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> is 12 × 12; each dimension is half the size of the input.</p>&#13;
<p class="indent">The pooling operation is straightforward but throws information away. So why do it at all? The primary motivation for pooling is to reduce the number of values in the network. Typically, as depth increases, the number of filters used by convolutional layers increases, by design. We see this for even the simple network of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, where the first convolutional layer has 32 filters, while the second has 64. Therefore, the second convolutional layer outputs 24 × 24 × 64 = 36,864 values, but after 2 × 2 pooling, there are only 12 × 12 × 64 = 9,216 values to work with, a 75 percent reduction. It’s important to note that we’re talking about the number of values present as we move data through the network, not the number of learned parameters in the layers. The second convolutional layer in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> has 3 × 3 × 32 × 64 = 18,432 learned parameters (ignoring bias values), while the pooling layer has no learned parameters.</p>&#13;
<p class="indent">This reduction in the number of values in the output, which is our representation of the input, speeds up computation and acts as a regularizer to guard against overfitting. The regularization techniques and rationales of <a href="ch09.xhtml#ch09">Chapter 9</a> are equally valid for CNNs. However, since pooling throws information away and selects proxies to represent entire regions of the representation (the convolutional layer outputs), it alters the spatial relationship <span epub:type="pagebreak" id="page_301"/>between parts of the input. This loss of spatial relationships might be critical for some applications and has motivated people like Geoffrey Hinton to eliminate pooling by introducing other types of networks (search for “capsule networks”).</p>&#13;
<p class="indentb">Specifically, Hinton said the following regarding pooling layers in response to a question on Reddit asking for his most controversial opinion on machine learning:</p>&#13;
<p class="block2">The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster. If the pools do not overlap, pooling loses valuable information about where things are. We need this information to detect precise relationships between the parts of an object.</p>&#13;
<p class="indenta">He elaborates further in the answer, pointing out that allowing pooling operations to overlap does preserve some of the spatial relationships in a crude way. An overlapping pooling operation might be to use a 2 × 2 window as we used in <a href="ch12.xhtml#ch12fig7">Figure 12-7</a>, but use a stride of 1 instead of 2.</p>&#13;
<p class="indent">Concerns aside, pooling layers are an essential part of CNNs as presently implemented, but be careful when adding them to a model. Let’s move on now to the top layers of a CNN, the fully connected layers.</p>&#13;
<h3 class="h3" id="lev1_85">Fully Connected Layers</h3>&#13;
<p class="noindent">In the second row of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, all the layers starting with <em>Flatten</em> form the fully connected layer of the model. The figure uses Keras terminology; many people call the <em>Dense</em> layer the fully connected layer and assume there is a Flatten operation as part of it along with the activation (ReLU) and optional dropout before the softmax layer. Therefore, the model in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> has only one fully connected layer.</p>&#13;
<p class="indent">We previously stated that the net effect of the convolutional and pooling layers is to change the representation of the input feature (the image, say) into one that makes it easier for a model to reason about. During training, we are asking the network to learn a different, often more compact, representation of the input to help the model perform better on unseen inputs. For the model in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, all the layers up to and including the pooling layer (and the dropout layer after it for training) are there to learn a new representation of the input image. In this case, the fully connected layer is the model: it will take that new representation and ultimately make a classification based on it.</p>&#13;
<p class="indent">Fully connected layers are just that, fully connected. The weights between the flattened final pooling layer of 9,216 elements for <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> (12 × 12 × 64 = 9,216) and the Dense layer of 128 elements are the same as if we were building a traditional neural network. This means that there are 9,216 × 128 = 1,179,648 weights plus an additional 128 bias values that need to be learned during training. Therefore, of the 1,199,882 parameters (weights and biases) in the model of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, 98.3 percent of them are in the transition between the final pooling layer and the fully connected layer. This illustrates an important point: fully connected layers are <em>expensive</em> in <span epub:type="pagebreak" id="page_302"/>terms of parameters that need to be learned, just as they are for traditional neural networks. Ideally, if the feature learning layers, the convolutional and pooling layers, are doing their job well, we might expect to need only one or two fully connected layers.</p>&#13;
<p class="indent">Fully connected layers have another disadvantage, besides memory use, that can impact their utility. To see what this disadvantage is, consider the following scenario: you want to be able to locate digits in grayscale images. Assume for simplicity that the background is black. If you use the model of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> trained on MNIST digits, you will have a model that is very good at identifying digits centered in 28×28 pixel images, but what if the input images are large and you do not know where the digits are in the image, let alone how many digits there are? Then things get a little more interesting. The model of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> expects input images that are 28×28 pixels in size and only that size. In <a href="ch13.xhtml#ch13">Chapter 13</a>, we will work through this problem in detail as an experiment, but for now, let’s discuss fully convolutional layers, a possible solution to this disadvantage of using fully connected layers in CNNs.</p>&#13;
<h3 class="h3" id="lev1_86">Fully Convolutional Layers</h3>&#13;
<p class="noindent">In the last section, I said that the model of <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> expects input images that are 28×28 pixels in size and only that size. Let’s see why.</p>&#13;
<p class="indent">There are many kinds of layers in this model. Some, like the ReLU and dropout layers, have no impact on the dimensionality of the data flowing through the network. The same cannot be said of the convolutional, pooling, and fully connected layers. Let’s look at these layers one by one to see how they are tied to the dimensionality of the input image.</p>&#13;
<p class="indent">The convolutional layers implement convolutions. By definition, a convolution involves moving a fixed-size kernel over some input image (thinking purely 2D here). Nothing in that operation specifies the size of the input image. The output of the first convolutional layer in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> is 26 × 26 × 32. The 32 comes from the number of filters selected by the architecture. The 26 × 26 comes from using a 3 × 3 convolution kernel on a 28 × 28 input with no padding. If the input image were instead 64×64 pixels, the output of this layer would be 62 × 62 × 32, and we wouldn’t need to do anything to alter the architecture of the network. The convolutional layers of a CNN are agnostic to the spatial dimensions of their inputs.</p>&#13;
<p class="indent">The pooling layer in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a> takes a 24 × 24 × 64 input and produces a 12 × 12 × 64 output. As we previously saw, the pooling operation is much like the convolution operation: it slides a fixed size window over the input, spatially, and produces an output; in this case, the output is half the dimensionality of the input while leaving the depth the same. Again, nothing in this operation fixes the spatial dimensions of the input stack. If the input stack were 32 × 32 × 64, the output of this max pooling operation would be 16 × 16 × 64 without a change needed to the architecture.</p>&#13;
<p class="indent">Finally, we have the fully connected layer that maps the 12 × 12 × 64 = 9,216 pooling output to a 128 element fully connected (Dense) layer. As we <span epub:type="pagebreak" id="page_303"/>saw in <a href="ch08.xhtml#ch08">Chapter 8</a>, fully connected neural networks use matrices of weights between layers in their implementation. There are 9,216 elements in the output of the pooling layer and a fixed 128 in the dense layer, so we need a matrix that is 9,216 × 128 elements. This size <em>is</em> fixed. If we use the network with a larger, say 32 × 32, input image, by the time we get through the pooling layer, the output size will be 14 × 14 × 64 = 12,544, which would require an existing 12,544 × 128 weight matrix to map to the fully connected layer. Of course, this won’t work; we trained a network that uses a 9,216 × 128 matrix. The fully connected layers of a CNN fix the input size of the CNN. If we could get around this, we could apply inputs of any size to the CNN, assuming memory allows.</p>&#13;
<p class="indent">We could, naïvely, simply slide a 28 × 28 window over the larger input image, run each 28×28 pixel image through the model as we trained it, and output a larger map, where each pixel now has a probability of that digit being present. There are 10 digits, so we would have 10 output maps. This sliding window approach certainly works, but it’s very computationally expensive, as many simplistic implementations of algorithms often are.</p>&#13;
<p class="indent">Fortunately for us, we can do better by converting the fully connected layer into an equivalent convolutional layer to make the model a <em>fully convolutional network</em>. In a fully convolutional network, there are no fully connected layers, and we’re not restricted to using a fixed input size. The relationship between input size and the output of the network when it is fully convolutional is something we will see in <a href="ch13.xhtml#ch13">Chapter 13</a>, but the essential operation is to look at the size of the last standard convolutional or pooling layer and replace the fully connected layer that follows with a convolutional layer using a kernel of the same size.</p>&#13;
<p class="indent">In <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>, the output of the pooling layer is 12 × 12 × 64. Therefore, instead of the 128-element fully connected layer that we saw fixes our input size, we can mathematically get the same calculation by changing the fully connected layer into a 12 × 12 × 128 convolutional layer. Convolving a 12 × 12 kernel over a 12 × 12 input produces a single number. Therefore, the output of the 12 × 12 × 128 convolutional layer will be a 1 × 1 × 128 array, which is functionally the same as the 128 outputs of the fully connected layer that we originally used. Additionally, the convolution operation between a 12 × 12 kernel and a 12 × 12 input is to simply multiply the kernel values by the input values, element by element, and sum them. This is what a fully connected layer does for each of its nodes.</p>&#13;
<p class="indent">We do not save anything in terms of the number of parameters when using a convolutional layer this way. We can see this from <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>. The 9,216 elements of the pooling layer output times the 128 nodes of the fully connected layer means we have 9,216 × 128 = 1,179,648 weights + 128 bias terms needed for both the fully connected and fully convolutional layers. When moving to the 12 × 12 × 128 convolutional layer, we have 12 × 12 × 64 × 128 = 1,179,648 weights to learn, the same as before. However, now we also have the freedom to change the input size, as the 12 × 12 × 128 convolutional layer will automatically convolve over any larger input, giving us outputs that represent the application of the network to 28 × 28 <span epub:type="pagebreak" id="page_304"/>regions of the input with a stride determined by the specific architecture of the network.</p>&#13;
<p class="indent">Fully convolutional networks stem from the 2014 paper by Long, Shelhamer, and Darrell, “Fully Convolutional Networks for Semantic Segmentation,” which has been referenced over 19,000 times as of this writing. The phrase <em>semantic segmentation</em> refers to assigning a class label to each pixel of the input image. Currently, the go-to architecture for semantic segmentation is the U-Net (see “U-Net: Convolutional Networks for Biomedical Image Segmentation” by Ronneberger, Fischer, and Brox, 2015) which has seen widespread success, especially in medical domains.</p>&#13;
<p class="indent">We’ve discussed the primary CNN layers, those found in <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>. There are many more that we could cover, but they are generally beyond what we want to present at this level, with one exception, batch normalization, which we’ll experiment with in <a href="ch15.xhtml#ch15">Chapter 15</a>. New layer types are being added all the time in response to active research projects. However, in the end, the core includes the layers we have discussed in this chapter. Let’s move on now and see how a trained CNN processes unknown inputs.</p>&#13;
<h3 class="h3" id="lev1_87">Step by Step</h3>&#13;
<p class="noindent">In the previous sections, we discussed the architecture and layers of our sample CNN, <a href="ch12.xhtml#ch12fig3">Figure 12-3</a>. In this section, we will illustrate the operation of the network to see how it responds to two new inputs, one a “4” and the other a “6”. We assume the network is fully trained; we’ll train for real it in <a href="ch13.xhtml#ch13">Chapter 13</a>.</p>&#13;
<p class="indent">The input image is passed through the model layer by layer</p>&#13;
<p class="center">input → conv<sub>0</sub> → conv<sub>1</sub> → pool → dense → softmax</p>&#13;
<p class="noindent">using the trained weights and biases to calculate outputs for each layer. We will refer to these as the <em>activations</em>. The output of the first convolutional layer is a stack of 32 26 × 26 images, the response of the input image to each of the 32 kernels. This stack then passes to the second convolutional layer to produce 64 24 × 24 outputs. Note, between the two convolutional layers is a ReLU operation that clips the output so that anything that would have been negative is now 0. Doing this adds a nonlinearity to the data as it flows through the network. Without this nonlinearity, the net effect of the two convolutional layers is to act like a single convolutional layer. With the nonlinearity imposed by the ReLU, we enable the two convolutional layers to learn different things about the data.</p>&#13;
<p class="indent">The second ReLU operation makes the stack of 64 24 × 24 outputs 0 or positive. Next, a 2 × 2 max pooling operation reduces the 64 outputs to 12 × 12 in size. After this, a standard fully connected layer produces 128 output values as a vector from the 9,216 values in the stack of 12 × 12 activations. From this, a set of 10 outputs, one for each digit, is calculated via a softmax. These are the output values of the network representing the network’s confidence as to which class label should be assigned to the input image.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_305"/>We can illustrate the activations by displaying the output images: either 26 × 26 for the first convolutional layer, 24 × 24 for the second convolutional layer, or 12 × 12 for the pooling layer. To show the activations from the fully connected layer, we can make an image of 128 bars, where the intensity of each bar represents the vector value. <a href="ch12.xhtml#ch12fig8">Figure 12-8</a> shows the activations for our two sample digits.</p>&#13;
<div class="image" id="ch12fig8"><img src="Images/12fig08.jpg" alt="image" width="649" height="549"/></div>&#13;
<p class="figcap"><em>Figure 12-8: Model activations per layer. The output is inverted: darker implies stronger activation.</em></p>&#13;
<p class="indent"> Note that the images are inverted so that darker corresponds to stronger activation values. We are not showing the softmax outputs. These values are</p>&#13;
<table>&#13;
<colgroup>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top" class="borderr"/>&#13;
<th style="vertical-align: top"><p class="tab-c">0</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">1</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">2</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">3</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">4</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">5</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">6</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">7</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">8</p></th>&#13;
<th style="vertical-align: top"><p class="tab-c">9</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab-c">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.99</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab-c">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.99</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
<td style="vertical-align: top"><p class="tab-c">0.00</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">indicating that in both cases, the model is very confident of the class label that should be assigned and that it was, in fact, correct.</p>&#13;
<p class="indent">Looking back at <a href="ch12.xhtml#ch12fig8">Figure 12-8</a>, we see that the output of the first convolutional layer is simply the response of the single input image (grayscale) and the kernels of the layer. This hearkens back to <a href="ch12.xhtml#ch12fig2">Figure 12-2</a>, where we saw that convolution could be used to highlight aspects of the input image. After the ReLU operation, the responses of the 64 filters of the second convolutional layer, each a stack of 32 kernels, seems to be picking out different portions or strokes in the input images. These can be thought of as a set of smaller components from which the input is constructed. The second <span epub:type="pagebreak" id="page_306"/>ReLU and pooling operation preserve much of the structure of the second convolutional layer outputs, but reduce the size to one quarter what it was previously. Finally, the output of the fully connected layer shows the pattern derived from the input image, the new representation that we expect to be easier to classify than the raw image input.</p>&#13;
<p class="indent">The dense layer outputs of <a href="ch12.xhtml#ch12fig8">Figure 12-8</a> are different from each other. This begs the question: what do these outputs look like for several instances of four and six digits? Is there something in common that we can see, even in these values? We might expect that there is because we know this network has been trained and has achieved a very high accuracy of over 99 percent on the test set. Let’s take a look at running ten “4” and ten “6” images from the test set through the network and compare the dense layer activations. This gives us <a href="ch12.xhtml#ch12fig9">Figure 12-9</a>.</p>&#13;
<div class="image" id="ch12fig9"><img src="Images/12fig09.jpg" alt="image" width="657" height="540"/></div>&#13;
<p class="figcap"><em>Figure 12-9: Dense layer activations for ten instances of 4 and 6. The output is inverted: darker implies stronger activation.</em></p>&#13;
<p class="indent">On the left, we see the actual input to the model. On the right is the representation of the 128 outputs in the fully connected layer, the one that feeds into the softmax. Each digit has a particular pattern that is common to each one of the digits. However, there are also variations. The middle “4” has a very short stem, and we see that its representation in the fully connected layer is also different from all the other examples. Still, this digit was successfully called a “4” by the model with a certainty of 0.999936.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_307"/><a href="ch12.xhtml#ch12fig9">Figure 12-9</a> provides evidence that the model learned what we wanted it to learn in terms of representation of the input. The softmax layer maps the 128 elements of the dense layer to 10, the output nodes from which the softmax probabilities are calculated. This is, in effect, a simple traditional neural network with no hidden layers. This simpler model succeeds in correctly labeling the images because the new representation of the inputs does a much better job of separating the classes so that even a simple model can make solid predictions. It also succeeds because the training process jointly optimizes both the weights of this top layer model and the weights of the lower layers that generate the input to the model at the same time, so they reinforce each other. Sometimes you will see this referred to in the literature at <em>end-to-end</em> training.</p>&#13;
<p class="indent">We can demonstrate the claim that the features are better separated by looking at a plot of the dense layer activations for the MNIST test data. Of course, we can’t look at the actual plot, as I have no idea how to visualize a plot in 128 dimensions, but all is not lost. The machine learning community has created a powerful visualization tool called <em>t-SNE</em>, which, fortunately for us, is part of sklearn. This algorithm intelligently maps high-dimensional spaces to lower-dimensional spaces, including 2D. If we run a thousand randomly selected MNIST test images through the model and then run the resulting 128-dimension dense layer activations through t-SNE, we can produce a 2D plot where the separation between classes reflects the actual separation in the 128-dimensional space. <a href="ch12.xhtml#ch12fig10">Figure 12-10</a> is the result.</p>&#13;
<div class="image" id="ch12fig10"><img src="Images/12fig10.jpg" alt="image" width="675" height="506"/></div>&#13;
<p class="figcap"><em>Figure 12-10: How well the model separates test samples by class (t-SNE plot)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_308"/>In this plot, each class uses a different plot symbol. If the model did not correctly classify the sample, it is shown as a larger star. In this case, only a handful of samples were misclassified. The separation by class type is very evident; the model has learned a representation that makes it straightforward to decide on the correct class label in most cases. We can readily count 10 different blobs in the t-SNE plot.</p>&#13;
<h3 class="h3" id="lev1_88">Summary</h3>&#13;
<p class="noindent">In this chapter, we introduced the major components of convolutional neural networks. These are workhorse networks for modern deep learning, especially for vision tasks because of their ability to learn from spatial relationships. We worked through a model to classify MNIST digits and detailed new processing layers, including convolutional layers and pooling layers. We then learned that the fully connected layers of a CNN are analogs of the traditional neural networks we learned about in earlier chapters.</p>&#13;
<p class="indent">Next, we saw how to modify the fully connected layers to enable operation on larger inputs. Finally, we looked at the activations generated by the network when a sample image was passed through and saw how the convolution and pooling layers worked together to produce a new representation of the input, one that helped to separate the classes in the feature space, thereby enabling high accuracy.</p>&#13;
<p class="indent">In the next chapter, we’ll continue our look at CNNs, but instead of theory, we’ll work with actual examples to see how the various parameters of the network, and the hyperparameters used during training, affect model performance. This will help us build intuition about how to work with CNNs in the future.</p>&#13;
</div></body></html>