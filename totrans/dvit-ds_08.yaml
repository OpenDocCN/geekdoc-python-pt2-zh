- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Web Scraping
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: You need data to do data science, and when you don’t have a dataset on hand,
    you can try *web scraping*, a set of techniques for reading information directly
    from public websites and converting it to usable datasets. In this chapter, we’ll
    cover some common web-scraping techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要数据来进行数据科学，当你手头没有数据集时，你可以尝试*网页抓取*，这是一种从公共网站直接读取信息并将其转换为可用数据集的技术。在本章中，我们将介绍一些常见的网页抓取技术。
- en: 'We’ll start with the simplest possible kind of scraping: downloading a web
    page’s code and looking for relevant text. We’ll then discuss regular expressions,
    a set of methods for searching logically through text, and Beautiful Soup, a free
    Python library that can help you parse websites more easily by directly accessing
    HyperText Markup Language (HTML) elements and attributes. We’ll explore tables
    and conclude by going over some advanced topics related to scraping. Let’s start
    by looking at how websites work.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最简单的抓取开始：下载网页代码并查找相关文本。然后，我们将讨论正则表达式，这是一组通过文本进行逻辑搜索的方法，以及 Beautiful Soup，这是一个免费的
    Python 库，可以帮助你通过直接访问超文本标记语言（HTML）元素和属性来更容易地解析网站。我们将探讨表格，最后讨论一些与抓取相关的高级话题。让我们首先看看网站是如何工作的。
- en: Understanding How Websites Work
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解网站是如何工作的
- en: Suppose you want to see the website of No Starch Press, the publisher of this
    book. You open a browser like Mozilla Firefox, Google Chrome, or Apple Safari.
    You enter the URL of No Starch’s home page, [https://nostarch.com](https://nostarch.com).
    Then your browser shows you the page, which, at the time of writing, looks like
    [Figure 8-1](#figure8-1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想查看 No Starch Press 网站，这是本书的出版商。你打开一个浏览器，如 Mozilla Firefox、Google Chrome
    或 Apple Safari。你输入 No Starch 首页的网址，[https://nostarch.com](https://nostarch.com)。然后，浏览器会展示给你页面，当前页面在写作时看起来像[图
    8-1](#figure8-1)。
- en: '![](image_fi/502888c08/f08001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c08/f08001.png)'
- en: 'Figure 8-1: The home page of the publisher of this book, accessible at [https://nostarch.com](https://nostarch.com)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-1：本书出版商的主页，可以通过[https://nostarch.com](https://nostarch.com)访问
- en: You can see a lot on this page, including text, images, and links, all arranged
    and formatted carefully so that the page is easy for humans to read and understand.
    This careful formatting doesn’t happen by accident. Every web page has source
    code that specifies the page’s text and images, as well as its formatting and
    arrangement. When you visit a website, you see a browser’s *interpretation* of
    this code.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个页面上看到很多内容，包括文本、图片和链接，所有这些都经过精心排列和格式化，使得页面对人类来说易于阅读和理解。这种精心的格式化不是偶然发生的。每个网页都有源代码，指定了页面的文本和图像，以及它的格式和排列。当你访问一个网站时，你看到的是浏览器对这些代码的*解释*。
- en: If you’re interested in seeing the actual code of a website, rather than the
    browser’s visual interpretation of it, you can use special commands. In Chrome
    and Firefox, you can see the source code for [https://nostarch.com](https://nostarch.com)
    by opening the page, right-clicking (in Windows, or CTRL+clicking in macOS) on
    blank space on the page, and then clicking View Page Source. When you do that,
    you’ll see a tab that looks like [Figure 8-2](#figure8-2).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣查看网站的实际代码，而不是浏览器的视觉呈现，你可以使用特殊命令。在 Chrome 和 Firefox 中，你可以通过打开页面，右键点击（在
    Windows 上，或在 macOS 上按住 CTRL 并点击）页面上的空白处，然后点击查看页面源代码，来查看[https://nostarch.com](https://nostarch.com)的源代码。当你这样做时，你会看到一个类似于[图
    8-2](#figure8-2)的标签。
- en: '![](image_fi/502888c08/f08002.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c08/f08002.png)'
- en: 'Figure 8-2: The HTML source code of the No Starch Press home page'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-2：No Starch Press 首页的 HTML 源代码
- en: This tab contains the code that specifies all the content on the No Starch Press
    home page. It’s in the form of raw text, without the visual interpretation that
    browsers usually provide. Code for web pages is usually written in the HTML and
    JavaScript languages.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标签包含了指定 No Starch Press 首页所有内容的代码。它是以原始文本的形式呈现的，没有浏览器通常提供的视觉解释。网页的代码通常是用 HTML
    和 JavaScript 语言编写的。
- en: In this chapter, we’re interested in this raw data. We’re going to write Python
    scripts that automatically scan through HTML code, like the code shown in [Figure
    8-2](#figure8-2), to find useful information that can be used for data science
    projects.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对这些原始数据感兴趣。我们将编写 Python 脚本，自动扫描 HTML 代码，像[图 8-2](#figure8-2)中显示的代码一样，找到可以用于数据科学项目的有用信息。
- en: Creating Your First Web Scraper
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建你的第一个网页抓取工具
- en: 'Let’s start with the simplest possible scraper. This scraper will take a URL,
    get the source code of the page associated with that URL, and print out the first
    part of the source code it got:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的抓取器开始。这个抓取器将接受一个 URL，获取与该 URL 关联的网页的源代码，并打印出它获得的源代码的第一部分：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This snippet starts by importing the `requests` package, which we used in Chapter
    7; here we’ll use it to get a page’s source code. Next, we specify the `urltoget`
    variable, which will be the URL of the web page whose code we want to request.
    In this case, we’re requesting an archived page from my personal website. Finally,
    we use the `requests.get()` method to get the code of our web page. We store this
    code in the `pagecode` variable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段首先导入了我们在第7章中使用过的 `requests` 包；这里我们将用它来获取网页的源代码。接下来，我们指定 `urltoget` 变量，它将是我们想要请求的网页的
    URL。在这个例子中，我们请求的是我个人网站上的一个归档页面。最后，我们使用 `requests.get()` 方法来获取网页的代码，并将这些代码存储在 `pagecode`
    变量中。
- en: The `pagecode` variable has a `text` attribute that contains all of the web
    page’s code. If you run `print(pagecode.text)`, you should be able to see all
    the HTML code of the page, stored as one long text string. Some pages have a huge
    amount of code, so printing out all the code at once may be unwieldy. If so, you
    can specify that you want to print only part of the code. That’s why we specify
    that we want only the first 600 characters of the page’s code by running `print(pagecode.text[0:600])`
    in the preceding snippet.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`pagecode` 变量有一个 `text` 属性，其中包含了网页的所有代码。如果你运行 `print(pagecode.text)`，你应该能够看到网页的所有
    HTML 代码，它会作为一个长文本字符串存储。一些页面的代码量非常大，所以一次性打印所有的代码可能会不方便。如果是这样，你可以指定只打印部分代码。因此，在前面的代码片段中，我们通过运行
    `print(pagecode.text[0:600])` 来指定只打印页面代码的前600个字符。'
- en: 'The output looks like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来是这样的：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This output is HTML, which consists largely of *elements* that are marked with
    angle brackets (`<` and `>`). Each element gives a browser like Firefox or Chrome
    information about how to display the website to visitors. For example, you can
    see a `<title>` tag in the output; also called a *start tag*, this marks the beginning
    of the title element. At the end of the seventh line, `</title>` is another tag,
    this time called an *end tag*, which marks the end of the title element. The actual
    title of the site is the text that appears between the beginning and ending tags;
    in this case, it’s `Bradford Tuckfield`. When a browser visits the site, it will
    interpret the meaning of the start and end tags of the title element and then
    display the title text `Bradford Tuckfield` at the top of the browser tab. This
    isn’t an HTML book, so we’re not going to go over every detail of the code we
    see here. We can be successful at scraping even without deep HTML expertise.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出是 HTML，它主要由用尖括号（`<` 和 `>`）标记的*元素*组成。每个元素向浏览器（如 Firefox 或 Chrome）提供有关如何向访客展示网站的信息。例如，你可以看到输出中有一个
    `<title>` 标签；它也叫做*开始标签*，标记了标题元素的开始。在第七行的末尾，`</title>` 是另一个标签，这次叫做*结束标签*，标记了标题元素的结束。站点的实际标题是出现在开始标签和结束标签之间的文本；在这个例子中，它是
    `Bradford Tuckfield`。当浏览器访问该站点时，它会解析标题元素的开始和结束标签的含义，然后在浏览器标签顶部显示标题文本 `Bradford
    Tuckfield`。这不是一本 HTML 书，所以我们不会详细讲解我们看到的每一个代码细节。即使没有深入的 HTML 专业知识，我们也可以成功地进行抓取。
- en: Now that we’ve scraped a web page, you may feel like you have all the scraping
    skills you need. However, you have much more to learn. Most web pages have a great
    deal of HTML code and content, but a data scientist rarely needs a web page’s
    entire source code. In business scenarios, you’ll more likely need only one specific
    piece of information or data on a web page. To find the specific information you
    need, it will be useful to be able to quickly and automatically search through
    long strings of HTML code. In other words, you will need to *parse* the HTML code.
    Let’s look at how to do this.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经抓取了一个网页，你可能觉得你已经掌握了所有的抓取技能。然而，你还有很多东西需要学习。大多数网页都有大量的 HTML 代码和内容，但数据科学家很少需要网页的全部源代码。在商业场景中，你更可能只需要网页上某个特定的信息或数据。为了找到你需要的特定信息，能够快速自动地在长字符串的
    HTML 代码中进行搜索会非常有用。换句话说，你将需要*解析*HTML代码。让我们来看一下如何做到这一点。
- en: Parsing HTML Code
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析 HTML 代码
- en: In the previous section, we went over how to download any public web page’s
    code to a Python session. Now let’s talk about how to parse the downloaded code
    to get the exact data you need.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何将任何公共网页的代码下载到 Python 会话中。现在让我们来谈谈如何解析下载的代码，以获取所需的准确数据。
- en: Scraping an Email Address
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抓取电子邮件地址
- en: Suppose you’re interested in automatically harvesting email addresses to create
    a marketing list. You might use the scraper we introduced previously to download
    the source code for many web pages. But you won’t need all the information in
    the long strings that represent the full code for each page. Instead, you will
    want only the small substrings that represent the email addresses that appear
    on the pages you scraped. So you will want to search through each page you scrape
    to find these smaller substrings.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有兴趣自动收集电子邮件地址以创建营销名单。你可能会使用我们之前介绍的抓取工具，下载多个网页的源代码。但你不需要每个页面代码中那些长字符串中的所有信息。相反，你只需要代表页面中电子邮件地址的小子字符串。所以，你需要在每个抓取的页面中搜索这些较小的子字符串。
- en: Suppose that one of the pages whose code you’ve downloaded is [https://bradfordtuckfield.com/contactscrape.xhtml](https://bradfordtuckfield.com/contactscrape.xhtml).
    If you visit this web page, you’ll see that a browser displays its content, as
    in [Figure 8-3](#figure8-3).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你下载的其中一个页面的代码是 [https://bradfordtuckfield.com/contactscrape.xhtml](https://bradfordtuckfield.com/contactscrape.xhtml)。如果你访问这个网页，你会看到浏览器显示的内容，如
    [图 8-3](#figure8-3) 所示。
- en: '![](image_fi/502888c08/f08003.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c08/f08003.png)'
- en: 'Figure 8-3: The content of a demo page that can be scraped easily'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-3：可以轻松抓取的示例页面内容
- en: 'This page displays only one email address, which is not hard to find after
    glancing at the page content for a moment. If we want to write a script that finds
    the email address on pages that are formatted like this one, we could search for
    the text `Email:` and look at the characters immediately following that text.
    Let’s do this, with a simple text search through the page’s code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个页面只显示一个电子邮件地址，在浏览页面内容一眼看去就不难找到。如果我们想写一个脚本，找到格式像这样页面上的电子邮件地址，我们可以搜索 `Email:`
    文本，并查看紧随其后的字符。让我们通过简单的文本搜索来实现这一点：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first two lines of this snippet follow the same scraping process used in
    the previous section: we specify a URL, download the page code for that URL, and
    store the code in the `pagecode` variable. After that, we use `find()` to search
    for the email text. This method takes a string of text as its input and returns
    the location of that text as its output. In this case, we use the `Email:` string
    as the input to the `find()` method, and we store the location of this text in
    the `mail_beginning` variable. The final output is `511`, indicating that the
    text `Email:` begins at the 511th character in the page’s code.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的前两行遵循与前一节相同的抓取流程：我们指定一个 URL，下载该 URL 的页面代码，并将代码存储在 `pagecode` 变量中。之后，我们使用
    `find()` 方法搜索邮件文本。该方法以文本字符串作为输入，并返回该文本的位置。在这种情况下，我们将 `Email:` 字符串作为 `find()` 方法的输入，并将该文本的位置存储在
    `mail_beginning` 变量中。最终输出为 `511`，表示 `Email:` 文本在页面代码中的位置是第 511 个字符。
- en: 'After we know the location of the `Email:` text, we can try to get the actual
    email address by looking at characters just after that text:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们知道了 `Email:` 文本的位置后，我们可以尝试通过查看该文本后面的字符来获取实际的电子邮件地址：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we print out the 80 characters that immediately follow the beginning
    of the `Email:` text (which starts at the 511th character). The output looks like
    this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打印出紧跟 `Email:` 文本开头（即第 511 个字符）的 80 个字符。输出如下所示：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can see that the code contains more than just the text visible in [Figure
    8-3](#figure8-3). In particular, an HTML element called `label` appears between
    the `Email:` text and the actual email address. If you want the email address
    alone, you have to skip the characters associated with the `<label>` tag, and
    you also have to remove the characters that appear after the email address:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，代码中包含的内容不仅仅是 [图 8-3](#figure8-3) 中可见的文本。特别是，一个名为 `label` 的 HTML 元素出现在
    `Email:` 文本和实际电子邮件地址之间。如果你只想要电子邮件地址，你必须跳过与 `<label>` 标签相关的字符，同时还需要删除电子邮件地址后的字符：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This snippet will print out `demo@bradfordtuckfield.com`, exactly the text we
    wanted to find on the page, since it skips the 38 characters of the `Email:` text
    and the `<label>` tag, and it trims off the final characters after the email address,
    which ends at 64 characters after the `Email:` text.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输出 `demo@bradfordtuckfield.com`，正是我们想要在页面上找到的文本，因为它跳过了 `Email:` 文本的 38
    个字符和 `<label>` 标签，且去除了电子邮件地址之后的字符，电子邮件地址在 `Email:` 文本后第 64 个字符结束。
- en: Searching for Addresses Directly
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接搜索地址
- en: We were able to find the email address in the page’s HTML code by looking for
    the 38th through 64th characters after the `Email:` text. The problem with this
    approach is that it’s not likely to work automatically when we try it on a different
    web page. If other pages don’t have the same `<label>` tag we found, looking at
    the 38th character after `Email:` won’t work. Or if the email address has a different
    length, stopping our search at the 64th character after `Email:` won’t work. Since
    scraping is usually supposed to be performed on many websites in rapid, automatic
    succession, it probably won’t be feasible to manually check for which characters
    we should look at instead of the 38th and 64th characters. So this technique probably
    won’t work for a scraper in an actual business scenario.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过查找 `Email:` 文本后第 38 到第 64 个字符，成功找到了页面 HTML 代码中的电子邮件地址。这个方法的问题在于，当我们尝试在其他网页上使用时，它不太可能自动生效。如果其他页面没有我们找到的相同
    `<label>` 标签，那么查找 `Email:` 后第 38 个字符就不适用了。或者，如果电子邮件地址的长度不同，那么在 `Email:` 后第 64
    个字符停止搜索也无法奏效。由于抓取通常是快速、自动地在多个网站上进行的，因此手动检查应该查找哪个字符而不是第 38 和第 64 个字符，显然是不可行的。所以这种技术在实际业务场景中的抓取器中很可能不可行。
- en: 'Instead of searching for the text `Email:` and looking at the following characters,
    we could try searching for the at sign (`@`) itself. Every email address should
    contain an `@`, so if we find this, we’re likely to have found an email address.
    There won’t be any HTML tags in the middle of an email address, so we won’t have
    to worry about skipping HTML tags to find the address. We can search for the `@`
    in the same way we searched for the `Email:` text:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试搜索 `Email:` 文本，而不是查看后续的字符，尝试搜索 `@` 符号本身。每个电子邮件地址应该包含一个 `@`，所以如果找到它，我们很可能找到了一个电子邮件地址。电子邮件地址中不会有
    HTML 标签，所以我们不需要担心跳过 HTML 标签来找到地址。我们可以像搜索 `Email:` 文本那样搜索 `@`：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is the same scraping code we used before. The only difference is that
    we are searching for `@` instead of `Email:`. The final output shows that `@`
    appears as the 553rd character in the code. We can print out the characters immediately
    before and after the `@` to get the email address itself:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们之前使用的相同抓取代码。唯一的区别是我们搜索的是 `@`，而不是 `Email:`。最终的输出显示，`@` 出现在代码的第 553 个字符处。我们可以打印出
    `@` 前后立即的字符，从而得到电子邮件地址本身：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There were no HTML tags to skip over. But we still have a problem: to get the
    email address without other extra characters, we have to know the number of characters
    before and after the `@` (4 and 22, respectively). Again, this wouldn’t work if
    we tried to repeat it to automatically scrape multiple email addresses from many
    websites.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有 HTML 标签需要跳过。但我们仍然面临一个问题：为了获得没有其他额外字符的电子邮件地址，我们必须知道 `@` 前后各有多少个字符（分别是 4
    和 22）。如果我们尝试重复这一方法来自动抓取多个网站上的电子邮件地址，它就无法奏效。
- en: 'Our searches would be more successful and easier to automate if we had a way
    to do intelligent searches. For example, imagine that we could search for text
    that matches the following pattern:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一种方法可以进行智能搜索，我们的搜索将更成功，并且更容易自动化。例如，假设我们可以搜索符合以下模式的文本：
- en: '`<characters matching the beginning of an email address>`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`<匹配电子邮件地址开头的字符>`'
- en: '`@`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`@`'
- en: '`<characters matching the end of an email address>`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`<匹配电子邮件地址结尾的字符>`'
- en: In fact, there is a way to perform automated searches through text in a way
    that can recognize patterns like the one described here. We’ll introduce this
    approach now.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有一种方法可以通过文本执行自动化搜索，从而识别像这里描述的模式。我们现在就来介绍这种方法。
- en: Performing Searches with Regular Expressions
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式进行搜索
- en: '*Regular expressions* are special strings that enable advanced, flexible, custom
    searches of patterns in text. In Python, we can do regular expression searches
    by using the `re` module, which is part of the Python standard library that comes
    preinstalled with Python. The following is an example of a regular expression
    search that uses the `re` module:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则表达式*是特殊的字符串，用于在文本中进行高级、灵活、定制化的模式搜索。在 Python 中，我们可以使用 `re` 模块进行正则表达式搜索，`re`
    是 Python 标准库的一部分，Python 安装时会预先安装。以下是使用 `re` 模块进行正则表达式搜索的示例：'
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this snippet, we import the `re` module. As its abbreviation indicates,
    this module is used for regular expressions. This module provides a `search()`
    method that can be used to search for text in any string. In this case, we specify
    two arguments: the string `recommend` and a string of text that contains the word
    *recommend*. We’re asking the method to search for the substring `recommend` within
    the larger string that also has some irrelevant text. Note that we add a single
    `r` character before the `recommend` string. This `r` tells Python to treat the
    `recommend` string as a *raw* string, meaning that Python won’t process or adjust
    it before using it in a search. The `span()` method will give us the beginning
    and end locations of this substring.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们导入了 `re` 模块。正如其缩写所示，该模块用于正则表达式。该模块提供了一个 `search()` 方法，可以用于在任何字符串中搜索文本。在这种情况下，我们指定了两个参数：字符串
    `recommend` 和一个包含 *recommend* 单词的文本字符串。我们要求该方法在包含一些无关文本的大字符串中搜索子字符串 `recommend`。请注意，我们在
    `recommend` 字符串前添加了一个单独的 `r` 字符。这个 `r` 告诉 Python 将 `recommend` 字符串视为*原始*字符串，这意味着
    Python 在搜索前不会处理或调整该字符串。`span()` 方法将为我们提供这个子字符串的起始和结束位置。
- en: The output, `(18,27)`, indicates that `recommend` exists in the second string,
    starting at index 18 in the string and ending at index 27\. This `search()` method
    is similar to the `find()` method that we used in the previous section; both are
    finding the locations of substrings within longer strings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 `(18,27)` 表明 `recommend` 出现在第二个字符串中，从字符串的第 18 个索引开始，到第 27 个索引结束。这个 `search()`
    方法类似于我们在前一部分使用的 `find()` 方法；两者都是用来查找子字符串在长字符串中的位置。
- en: 'But suppose you are searching a web page written by someone who has a tendency
    to misspell words. By default, the `re.search()` method looks for exact matches,
    so if you’re searching a web page that contains `recommend` spelled incorrectly,
    you won’t find any matches. In this case, we may want to ask Python to look for
    `recommend`, but to look for different spellings of it. The following is one way
    to accomplish this with regular expressions:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是假设你正在搜索一个由某个容易拼错单词的人编写的网页。默认情况下，`re.search()` 方法查找的是精确匹配，因此如果你正在搜索一个拼错了 `recommend`
    的网页，你将无法找到任何匹配项。在这种情况下，我们可能希望让 Python 查找 `recommend`，但查找它的不同拼写。以下是使用正则表达式实现这一目标的一种方法：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we change the argument of our code: instead of searching for `recommend`
    spelled correctly, we search for `rec+om+end`. This works because the `re` module
    interprets the plus sign (`+`) as a *metacharacter*. When this special type of
    character is used in a search, it has a special logical interpretation that can
    help you do flexible searches instead of requiring exact matches. The `+` metacharacter
    indicates repetition: it specifies that Python should search for one or more repetitions
    of the preceding character. So when we write `c+`, Python knows that it should
    search for one or more repetitions of the letter `c`, and when we write `m+`,
    Python knows that it should search for one or more repetitions of the letter `m`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们改变了代码的参数：我们不再搜索正确拼写的 `recommend`，而是搜索 `rec+om+end`。之所以有效，是因为 `re` 模块将加号（`+`）解释为*元字符*。当这种特殊类型的字符用于搜索时，它有一个特殊的逻辑解释，可以帮助你进行灵活的搜索，而不需要精确匹配。`+`
    元字符表示重复：它指定 Python 应该搜索前一个字符的一次或多次重复。所以当我们写 `c+` 时，Python 知道应该搜索字母 `c` 的一次或多次重复，而当我们写
    `m+` 时，Python 知道应该搜索字母 `m` 的一次或多次重复。
- en: A string that uses a metacharacter like `+` with a special, logical meaning
    is called a *regular expression*. Regular expressions are used in every major
    programming language and are extremely important in all code applications that
    deal with text.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 `+` 这样的元字符并具有特殊逻辑意义的字符串称为*正则表达式*。正则表达式在每种主要编程语言中都有使用，并且在所有涉及文本的代码应用中非常重要。
- en: 'You should try to experiment with the `+` metacharacter to get more comfortable
    with the way it works. For example, you could try to search for various misspellings
    of `recommend` as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该尝试使用 `+` 元字符进行实验，以便更好地理解它的工作方式。例如，你可以尝试搜索 `recommend` 的各种拼写错误，如下所示：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This snippet contains four regular expression searches. The output of the first
    search is `(18,26)`, indicating that the misspelled word `recomend` matches the
    regular expression `rec+om+end` that we searched for. Remember that the `+` metacharacter
    searches for one or more repetitions of the preceding character, so it will match
    the single `c` and single `m` in the misspelled `recomend`. The output of the
    second search is `(18,28)`, indicating that the misspelling `reccommend` also
    matches the regular expression `rec+om+end`, again because the `+` metacharacter
    specifies one or more repetitions of a character, and `c` and `m` are both repeated
    twice here. In this case, our regular expression using `+` has provided flexibility
    to our search so it can match multiple alternative spellings of a word.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码包含了四个正则表达式搜索。第一个搜索的输出是 `(18,26)`，表示拼写错误的单词`recomend`匹配了我们搜索的正则表达式`rec+om+end`。记住，`+`元字符用于搜索前一个字符的一个或多个重复，所以它将匹配拼写错误的`recomend`中的单个`c`和单个`m`。第二个搜索的输出是
    `(18,28)`，表示拼写错误的`reccommend`也匹配了正则表达式`rec+om+end`，因为`+`元字符指定了一个或多个字符重复，在这里`c`和`m`都重复了两次。在这种情况下，我们使用`+`的正则表达式为搜索提供了灵活性，使其能够匹配多个拼写变体。
- en: But the flexibility of regular expressions is not absolute. Our third and fourth
    searches return errors when you run them in Python, because the regular expression
    `rec+om+end` doesn’t match any part of the specified strings (`reommend` and `recomment`).
    The third search doesn’t return any matches because `c+` specifies one or more
    repetitions of `c`, and there are zero repetitions of `c` in `reommend`. The fourth
    search doesn’t return any matches because, even though the number of `c` and `m`
    characters is correct, searching for `rec+om+end` requires a `d` character at
    the end, and `recomment` doesn’t have a match for the `d`. When you use regular
    expressions, you need to be careful to make sure that they’re expressing precisely
    what you want, with the exact amount of flexibility you want.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但是正则表达式的灵活性并非绝对。当你在 Python 中运行第三个和第四个搜索时，它们会返回错误，因为正则表达式`rec+om+end`没有匹配指定字符串（`reommend`和`recomment`）中的任何部分。第三个搜索没有返回匹配项，因为`c+`指定了一个或多个`c`的重复，而`reommend`中没有任何`c`的重复。第四个搜索没有返回匹配项，因为虽然`c`和`m`字符的数量是正确的，但搜索`rec+om+end`要求在末尾有一个`d`字符，而`recomment`没有匹配的`d`。使用正则表达式时，你需要小心确保它们准确表达了你想要的内容，并具备你所需的灵活性。
- en: Using Metacharacters for Flexible Searches
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用元字符进行灵活的搜索
- en: 'In addition to `+`, several other important metacharacters can be used in Python
    regular expressions. Several metacharacters, like `+`, are used to specify repetitions.
    For example, the asterisk (`*`) specifies that the preceding character is repeated
    *zero* or more times. Notice that this is different from `+`, which represents
    a character repeated *one* or more times. We can use `*` in a regular expression
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`+`，Python 正则表达式中还可以使用其他几个重要的元字符。像`+`这样的元字符用于指定重复次数。例如，星号（`*`）表示前一个字符重复*零次*或更多次。请注意，这与`+`不同，`+`表示字符至少重复*一次*或更多次。我们可以按如下方式在正则表达式中使用`*`：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This regular expression would find the location of a bank balance in a string
    that specifies 1, 10, 100, 1,000, or indeed any number of 0s (even zero 0s). Here
    are examples of using `*` as a metacharacter:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则表达式将找到字符串中表示银行余额的位置，可以是1、10、100、1,000，甚至是任何数量的0（甚至是零个0）。以下是使用`*`作为元字符的示例：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this snippet, we again perform searches for the regular expression `10*`
    in four strings. We find matches for the first, second, and fourth strings, because,
    though all specify different amounts of money, each contains the character `1`
    followed by zero or more repetitions of the character `0`. The third string also
    contains repetitions of the `0` character, but no match occurs because the string
    doesn’t contain a `1` character adjacent to the 0s.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们再次对四个字符串进行`10*`正则表达式搜索。我们发现第一个、第二个和第四个字符串匹配，因为尽管它们表示不同的金额，每个字符串都包含字符`1`，后面跟着零次或多次重复的字符`0`。第三个字符串虽然也包含重复的`0`字符，但没有匹配，因为该字符串没有与0相邻的`1`字符。
- en: 'In practice, having characters in plaintext that repeat more than twice is
    uncommon, so the `*` may not always be useful to you. If you don’t want to allow
    more than one repetition of a character, the question mark (`?`) is useful as
    a metacharacter. The `?`, when used as a metacharacter, specifies that the preceding
    character appears either zero or one times:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，文本中出现重复超过两次的字符是不常见的，因此`*`对你来说可能并不总是有用。如果你不想允许字符重复超过一次，问号（`?`）作为元字符非常有用。当`?`作为元字符使用时，它指定前面的字符出现零次或一次：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, we use the `?` because we want to search for either Clark or Clarke,
    but not for Clarkee or Clarkeee or Clark with more *e*’s.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用`?`，因为我们想搜索Clark或Clarke，而不是Clarkee、Clarkeee或含有更多*e*的Clark。
- en: Fine-Tuning Searches with Escape Sequences
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用转义序列微调搜索
- en: 'Metacharacters enable you to perform useful, flexible text searches, allowing
    for many spellings and formats. However, they can also lead to confusion. For
    example, suppose that you want to search some text for a particular math equation,
    like 99 + 12 = 111\. You could try to search for it as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 元字符使你能够执行有用且灵活的文本搜索，允许多种拼写和格式。然而，它们也可能导致混淆。例如，假设你想在文本中搜索一个特定的数学方程式，比如99 + 12
    = 111\。你可以尝试如下搜索：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When you run this code, you’ll get an error, because Python doesn’t find any
    matches for the search string. This may surprise you, since it’s easy to see an
    exact match for the equation we specified in the string we searched. This search
    returns no results because the default interpretation of `+` is as a metacharacter,
    not a literal addition sign. Remember that `+` specifies that the preceding character
    is repeated one or more times. We would find a match if we did a search like this
    one:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这段代码时，你会遇到错误，因为Python没有找到与搜索字符串匹配的项。这可能会让你感到惊讶，因为你很容易看到我们在搜索字符串中指定的方程式的精确匹配。这个搜索没有返回结果，因为`+`的默认解释是作为元字符，而不是字面加号。记住，`+`表示前面的字符重复一次或多次。如果我们做如下搜索，就能找到匹配项：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In this case, Python finds an exact match by interpreting the `+` sign as a
    metacharacter, since `9` is repeated in the string on the right. If you want to
    search for an actual addition sign rather than using `+` as a metacharacter, you
    need to use yet another metacharacter to specify this preference. You can do it
    as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Python通过将`+`号解释为元字符找到精确匹配，因为`9`在右侧的字符串中重复。如果你想搜索实际的加号，而不是将`+`作为元字符，你需要使用另一个元字符来指定这个偏好。你可以这样做：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we use the backslash (`\`) as a special metacharacter. The `\` is called
    an *escape character*. It allows the `+` addition sign to “escape” from its metacharacter
    status and be interpreted literally instead. We call the `\+` string an *escape
    sequence*. In the preceding snippet, we find a match for our math equation because
    we escape the `+` addition sign, so Python looks for a literal addition sign instead
    of interpreting `+` as a metacharacter and looking for repetitions of the `9`
    character.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用反斜杠（`\`）作为特殊的元字符。`\`称为*转义字符*。它允许`+`加号“逃脱”其元字符的身份，转而被字面解释。我们将`\+`字符串称为*转义序列*。在前面的代码片段中，我们找到了匹配项，因为我们对`+`加号进行了转义，因此Python查找字面上的加号，而不是将`+`解释为元字符并查找重复的`9`字符。
- en: 'You can do a literal search for any metacharacter by using an escape sequence.
    For example, imagine that you want to look for a question mark, instead of doing
    a search with a question mark as a metacharacter. You could do the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用转义序列对任何元字符进行字面搜索。例如，假设你想查找一个问号，而不是将问号作为元字符进行搜索。你可以这样做：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This finds a match for `Clarke?`, but it won’t find a match for `Clark?` Because
    we escape the question mark, Python searches for a literal question mark instead
    of interpreting it as a metacharacter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以找到`Clarke?`的匹配项，但找不到`Clark?`的匹配项。因为我们对问号进行了转义，Python会查找字面上的问号，而不是将其解释为元字符。
- en: 'If you ever need to search for a backslash, you’ll need two backslashes—one
    to escape from metacharacter interpretation and another to tell Python which literal
    character to search for:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要搜索反斜杠，你需要使用两个反斜杠——一个用来逃避元字符解释，另一个用来告诉Python搜索哪个字面字符：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this snippet, we use the `r` character again to specify that we want to
    interpret the strings as raw text and to make sure Python doesn’t do any adjustment
    or processing before our search. Escape sequences are common and useful in regular
    expressions. Some escape sequences give special meaning to standard characters
    (not metacharacters). For example, `\d` will search for any digit (numbers 0 to
    9) in a string, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们再次使用 `r` 字符来指定我们希望将字符串解释为原始文本，并确保 Python 在我们的搜索之前不做任何调整或处理。转义序列在正则表达式中很常见且有用。一些转义序列赋予标准字符（而非元字符）特殊意义。例如，`\d`
    将搜索字符串中的任何数字（0 到 9），如下所示：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This snippet finds the location of the character `1` because the `\d` escape
    sequence refers to any digit. The following are other useful escape sequences
    using non-metacharacters:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码找到字符 `1` 的位置，因为 `\d` 转义序列表示任何数字。以下是其他有用的转义序列，它们使用非元字符：
- en: '`\D` Searches for anything that’s not a digit'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`\D` 用于搜索任何非数字字符'
- en: '`\s` Searches for whitespace (spaces, tabs, and newlines)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`\s` 用于搜索空白字符（空格、制表符和换行符）'
- en: '`\w` Searches for any alphabetic characters (letters, numbers, or underscores)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`\w` 用于搜索任何字母字符（字母、数字或下划线）'
- en: 'Other important metacharacters are the square brackets `[` and `]`. These can
    be used as a pair in regular expressions to represent types of characters. For
    example, we can look for any lowercase alphabetic character as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其他重要的元字符是方括号 `[` 和 `]`。它们可以成对出现在正则表达式中，用来表示字符类型。例如，我们可以如下搜索任何小写字母：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This snippet is specifying that we want to find any characters that are in
    the “class” of characters between `a` and `z`. This returns the output `(1,2)`,
    which consists of only the character `y`, since this is the first lowercase character
    in the string. We could search for any uppercase characters similarly:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码指定我们要查找位于 `a` 到 `z` 之间的任何字符。它返回了输出 `(1,2)`，其中只有字符 `y`，因为这是字符串中的第一个小写字母。我们也可以类似地搜索任何大写字母：
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This search outputs `(0,1)`, because the first uppercase character it finds
    is the `M` at the beginning of the string.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个搜索返回 `(0,1)`，因为它找到的第一个大写字母是字符串开头的 `M`。
- en: 'Another important metacharacter is the pipe (`|`), which can be used as an
    *or* logical expression. This can be especially useful if you’re not sure which
    of two ways is the correct way to spell something. For example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的元字符是管道符号（`|`），它可以用作 *或* 逻辑表达式。如果你不确定两种拼写方式中哪一种正确，这尤其有用。例如：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we specify that we want the string `Manchac` with either an `a` or a `k`
    at the end. It would also return a match if we searched `Lets drive on Manchack.`
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定要搜索以 `Manchac` 结尾，结尾可以是 `a` 或 `k`。如果我们搜索 `Lets drive on Manchack.`，它也会返回匹配项。
- en: Combining Metacharacters for Advanced Searches
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合元字符进行高级搜索
- en: 'The following are other metacharacters you should know:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你应该知道的其他元字符：
- en: '`$` For the end of a line or string'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`$` 表示行或字符串的结尾'
- en: '`^` For the beginning of a line or string'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`^` 表示行或字符串的开头'
- en: '`.` For a wildcard, meaning any character except the end of a line (`\n`)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`.` 用作通配符，表示除了行末（`\n`）之外的任何字符'
- en: 'You can combine text and metacharacters for advanced searches. For example,
    suppose you have a list of all the files on your computer. You want to search
    through all the filenames to find a certain *.pdf* file. Maybe you remember that
    the name of your *.pdf* has something to do with *school*, but you can’t remember
    anything else about the name. You could use this flexible search to find the file:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将文本和元字符结合起来进行高级搜索。例如，假设你有一份计算机上所有文件的列表，你想在所有文件名中搜索某个 *.pdf* 文件。也许你记得 *.pdf*
    文件名和 *school* 有关系，但你不记得其他信息。你可以使用这个灵活的搜索来找到文件：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s look at the regular expression in this snippet. It starts with `school`
    since you remember that the filename contains that word. Then, it has two metacharacters
    together: `.*`. The `.` is a wildcard metacharacter, and the `*` refers to any
    amount of repetition. So, `.*` specifies any number of any other characters coming
    after `school`. Next, we have an escaped period (full stop): `\.`, which refers
    to an actual period sign rather than a wildcard. Next, we search for the string
    `pdf`, but only if it appears at the end of the filename (specified by `$`). In
    summation, this regular expression specifies a filename that starts with `school`,
    ends with `.pdf`, and may have any other characters in between.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个片段中的正则表达式。它以`school`开始，因为你记得文件名中包含这个词。然后，它有两个元字符在一起：`.*`。`.`是一个通配符元字符，`*`表示任何数量的重复。因此，`.*`表示在`school`之后的任何字符的任意数量。接下来，我们有一个转义的句点（圆点）：`\.`，它指的是一个实际的句点符号，而不是通配符。接着，我们搜索字符串`pdf`，但只有在文件名末尾出现时才匹配（由`$`指定）。总结来说，这个正则表达式指定了一个文件名，它以`school`开头，以`.pdf`结尾，并且中间可能有任何其他字符。
- en: 'Let’s search different strings for this regular expression to make sure you’re
    comfortable with the patterns it’s searching for:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在不同的字符串中搜索这个正则表达式，确保你熟悉它正在搜索的模式：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Some of these searches will find matches, and some will throw errors because
    they don’t find matches. Look closely at the searches that throw errors to make
    sure you understand why they’re not finding matches. As you get more comfortable
    with regular expressions and the metacharacters they use, you’ll be able to quickly
    grasp the logic of any regular expression you see instead of seeing it as a meaningless
    jumble of punctuation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些搜索中有一些会找到匹配项，而有一些会抛出错误，因为没有找到匹配项。仔细观察抛出错误的搜索，确保你理解为什么它们没有找到匹配项。当你对正则表达式和它们使用的元字符越来越熟悉时，你将能够迅速掌握任何你看到的正则表达式的逻辑，而不是把它看作一堆毫无意义的标点符号。
- en: You can use regular expressions for many kinds of searches. For example, you
    can specify a regular expression that searches for street addresses, URLs, particular
    types of filenames, or email addresses. As long as a logical pattern occurs in
    the text you’re searching for, you can specify that pattern in a regular expression.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用正则表达式进行多种搜索。例如，你可以指定一个正则表达式来搜索街道地址、URL、特定类型的文件名或电子邮件地址。只要你要搜索的文本中存在一个逻辑模式，你就可以在正则表达式中指定该模式。
- en: To learn more about regular expressions, you can check out the official Python
    documentation at [https://docs.python.org/3/howto/regex.xhtml](https://docs.python.org/3/howto/regex.xhtml).
    But really, the best way to get comfortable with regular expressions is to simply
    practice them on your own.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于正则表达式的内容，你可以查看官方的Python文档：[https://docs.python.org/3/howto/regex.xhtml](https://docs.python.org/3/howto/regex.xhtml)。但实际上，掌握正则表达式的最佳方式是自己动手实践。
- en: Using Regular Expressions to Search for Email Addresses
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式搜索电子邮件地址
- en: 'Regular expressions enable you to search flexibly and intelligently for many
    types of patterns. Let’s return to our initial example of searching for email
    addresses and see how we can use regular expressions there. Remember that we want
    to search for text that matches the following pattern:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式使你能够灵活且智能地搜索多种类型的模式。让我们回到最初的例子，搜索电子邮件地址，看看我们如何在这里使用正则表达式。记住，我们想要搜索与以下模式匹配的文本：
- en: '`<some text>``@``<some more text>`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`<some text>``@``<some more text>`'
- en: 'Here’s a regular expression that will accomplish this search:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可以完成此搜索的正则表达式：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s look closely at the elements of this snippet:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这个片段的元素：
- en: It starts with `[a-zA-Z]`. This includes the square bracket metacharacters,
    which specify a class of characters. In this case, it will look for the characters
    represented by `a-zA-Z`, which refers to any lowercase or uppercase alphabetic
    character.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它以`[a-zA-Z]`开始。这包括方括号元字符，指定了一个字符类。在这种情况下，它会查找由`a-zA-Z`表示的字符，即任何小写或大写的字母字符。
- en: The `[a-zA-Z]` is followed by `+`, specifying one or more instances of any alphabetic
    character.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`[a-zA-Z]`后面跟着`+`，指定一个或多个字母字符。'
- en: The `@` is next. This is not a metacharacter but rather searches for the literal
    at sign (`@`).
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是`@`。它不是元字符，而是搜索字面上的“at”符号（`@`）。
- en: Next, we have `[a-zA-Z]+` again, specifying that after the `@`, any number of
    alphabetic characters should appear. This should be the first part of an email
    domain, like the *protonmail* in *protonmail.com*.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们再次遇到`[a-zA-Z]+`，它指定在`@`后，任何数量的字母字符应该出现。这应该是电子邮件域名的第一部分，比如*protonmail*在*protonmail.com*中。
- en: The `\.` specifies a period or full-stop character, to search for this character
    in *.com* or *.org* or any other top-level domain.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`\.`指定了一个句点或圆点字符，用来在*.com*、*.org*或任何其他顶级域名中查找这个字符。'
- en: Finally, we have `[a-zA-Z]+` again, specifying that some alphabetic characters
    should come after the full stop. This is the *com* in *.com* or the *org* in *.org*
    addresses.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们再次遇到`[a-zA-Z]+`，它指定在句号后应该跟着一些字母字符。这就是*.com*中的*com*或*.org*中的*org*部分。
- en: Together, these six elements specify the general pattern of an email address.
    If you weren’t familiar with regular expressions, it would be strange to think
    that `[a-zA-Z]+@[a-zA-Z]+\.[a-zA-Z]+` is specifying an email address. But because
    of Python’s ability to interpret metacharacters in regular expressions, Python
    has been able to interpret this search and return email addresses. Just as important,
    you have learned regular expressions and understand what this regular expression
    means too.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这六个元素一起指定了电子邮件地址的一般模式。如果你不熟悉正则表达式，可能会觉得`[a-zA-Z]+@[a-zA-Z]+\.[a-zA-Z]+`是在指定一个电子邮件地址。但由于Python能够解析正则表达式中的元字符，Python能够理解这个搜索并返回电子邮件地址。同样重要的是，你也已经学会了正则表达式，并且理解了这个正则表达式的含义。
- en: One important thing to remember is that there are many email addresses in the
    world. The regular expression in the preceding snippet will identify many email
    addresses, but not every possible one. For example, some domain names use characters
    that aren’t part of the standard Roman alphabet used for the English language.
    The preceding regular expression wouldn’t capture those email addresses. Also,
    email addresses can include numerals, and our regular expression wouldn’t match
    those either. A regular expression that could reliably capture every possible
    combination of characters in every possible email address would be extremely complex,
    and going to that level of complexity is beyond the scope of this book. If you’re
    interested in advanced regular expressions, you can look at a regular expression
    written by a professional that is meant to find email addresses at [https://web.archive.org/web/20220721014244/https://emailregex.com/](https://web.archive.org/web/20220721014244/https://emailregex.com/).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 记住一个重要的事情：世界上有很多电子邮件地址。前面的正则表达式会识别许多电子邮件地址，但不是每一个可能的电子邮件地址。例如，有些域名使用的字符并不是英语中标准罗马字母的一部分。前面的正则表达式无法捕捉这些电子邮件地址。另外，电子邮件地址可能包含数字，而我们的正则表达式也无法匹配这些地址。能够可靠捕捉每一个可能的电子邮件地址中所有字符组合的正则表达式将会非常复杂，而达到这种复杂度超出了本书的范围。如果你对高级正则表达式感兴趣，可以查看由专业人士编写的正则表达式，专门用于查找电子邮件地址，网址是[https://web.archive.org/web/20220721014244/https://emailregex.com/](https://web.archive.org/web/20220721014244/https://emailregex.com/)。
- en: Converting Results to Usable Data
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将结果转换为可用数据
- en: Remember that we’re data scientists, not only web scrapers. After scraping web
    pages, we’ll want to convert the results of our scraping to usable data. We can
    do this by importing everything we scrape into a pandas dataframe.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们是数据科学家，不仅仅是网页抓取者。在抓取网页之后，我们需要将抓取的结果转化为可用的数据。我们可以通过将所有抓取到的内容导入pandas数据框来实现。
- en: 'Let’s scrape all of the (fake) email addresses listed in a paragraph at the
    following URL: [https://bradfordtuckfield.com/contactscrape2.xhtml](https://bradfordtuckfield.com/contactscrape2.xhtml).
    We can start by reading all of the text from the site, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们抓取以下网址中段落里列出的所有（虚假的）电子邮件地址：[https://bradfordtuckfield.com/contactscrape2.xhtml](https://bradfordtuckfield.com/contactscrape2.xhtml)。我们可以从读取网站的所有文本开始，具体如下：
- en: '[PRE26]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is the same code we used before: we simply download the HTML code and
    store it in our `pagecode` variable. If you’d like, you can look at all the code
    for this page by running `print(pagecode.text)`.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码和我们之前使用的是一样的：我们只是下载HTML代码并将其存储在`pagecode`变量中。如果你愿意，可以通过运行`print(pagecode.text)`来查看此页面的所有代码。
- en: 'Next, we can specify our regular expression to look for all email addresses
    in the paragraph:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以指定正则表达式来查找段落中的所有电子邮件地址：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, we use the same characters for our regular expression. But we’re using
    a new method: `re.finditer()` instead of `re.search()`. We do this because `re.finditer()`
    is able to obtain multiple matches, and we need to do this to get all of the email
    addresses. (By default, `re.search()` finds only the first match of any string
    or regular expression.)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用相同的字符作为我们的正则表达式。但我们使用了一种新的方法：`re.finditer()`，而不是 `re.search()`。我们这样做是因为
    `re.finditer()` 能够获取多个匹配项，而我们需要这样做来获取所有的电子邮件地址。（默认情况下，`re.search()` 只会找到字符串或正则表达式的第一个匹配项。）
- en: 'Next, we need to compile these email addresses together:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将这些电子邮件地址编译在一起：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We start with an empty list called `alladdresses`. Then we append each element
    of our `allmatches` object to the list. Finally, we print out the list.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个空列表 `alladdresses` 开始。然后我们将每个 `allmatches` 对象中的元素追加到该列表中。最后，我们打印出这个列表。
- en: 'We can also convert our list to a pandas dataframe:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将我们的列表转换为 pandas 数据框：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now that our addresses are in a pandas dataframe, we can use the huge number
    of methods provided by the pandas library to do anything that we may have done
    with any other pandas dataframe. For example, we can put it in reverse alphabetical
    order if that’s useful to us, and then export it to a *.csv* file:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将地址放入一个 pandas 数据框中，可以使用 pandas 库提供的大量方法来执行任何我们可能用其他 pandas 数据框完成的操作。例如，如果对我们有用，我们可以将其按字母顺序反转，然后将其导出为一个
    *.csv* 文件：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s think about what we did so far. Starting with only a URL, we downloaded
    the full HTML code of the web page specified by the URL. We used a regular expression
    to find all emails listed on the page. We compiled the emails into a pandas dataframe,
    which can then be exported to a *.csv* or Excel file or otherwise transformed
    as we see fit.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想一下到目前为止我们做了什么。从一个 URL 开始，我们下载了该 URL 指定的网页的完整 HTML 代码。我们使用正则表达式查找页面上列出的所有电子邮件。我们将这些电子邮件编译成
    pandas 数据框，然后可以将其导出为 *.csv* 或 Excel 文件，或者根据需要进行其他转换。
- en: Downloading HTML code and specifying regular expressions to search for certain
    information, as we have done, is a reasonable way to accomplish any scraping task.
    However, in some cases, it may be difficult or inconvenient to write a complex
    regular expression for a difficult-to-match pattern. In these cases, you can use
    other libraries that include advanced HTML parsing and scraping capabilities without
    requiring you to write any regular expressions. One such library is called Beautiful
    Soup.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 HTML 代码并指定正则表达式以查找特定信息，正如我们所做的那样，是完成任何抓取任务的合理方式。然而，在某些情况下，编写复杂的正则表达式来匹配难以匹配的模式可能会变得困难或不方便。在这些情况下，您可以使用其他库，这些库包含先进的
    HTML 解析和抓取功能，而无需您编写任何正则表达式。一个这样的库叫做 Beautiful Soup。
- en: Using Beautiful Soup
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Beautiful Soup
- en: 'The *Beautiful Soup library* allows us to search for the contents of particular
    HTML elements without writing any regular expressions. For example, imagine that
    you want to collect all the hyperlinks in a page. HTML code uses an *anchor* element
    to specify hyperlinks. This special element is specified with a simple `<a>` start
    tag. The following is an example of what an anchor element might look like in
    the HTML code for a web page:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*Beautiful Soup 库* 允许我们在不编写任何正则表达式的情况下，查找特定 HTML 元素的内容。例如，假设你想收集页面中的所有超链接。HTML
    代码使用 *anchor* 元素来指定超链接。这个特殊元素通过一个简单的 `<a>` 起始标签来指定。以下是一个锚点元素在网页 HTML 代码中的示例：'
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This snippet specifies the text `Click here`. When users click this text on
    an HTML web page, their browsers will navigate to [https://bradfordtuckfield.com](https://bradfordtuckfield.com).
    The HTML element starts with an `<a>`, which indicates that it’s an anchor, or
    hyperlink, to a web page or file. Then it has an attribute called `href`. In HTML
    code, an *attribute* is a variable that provides more information about elements.
    In this case, the `href` attribute contains the URL that a hyperlink should “point”
    to: when someone clicks the `Click here` text, their browser navigates to the
    URL contained in the `href` attribute. After the `href` attribute, there’s an
    angle bracket, then the text that appears on the page. A final `</a>` indicates
    the end of the hyperlink element.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码指定了文本`Click here`。当用户点击 HTML 网页上的这个文本时，浏览器将跳转到[https://bradfordtuckfield.com](https://bradfordtuckfield.com)。HTML
    元素以`<a>`开头，表示这是一个锚点或超链接，指向一个网页或文件。接下来，它有一个名为`href`的属性。在 HTML 代码中，*属性*是一个变量，提供有关元素的更多信息。在这个例子中，`href`属性包含了超链接应该“指向”的
    URL：当某人点击`Click here`文本时，浏览器将跳转到`href`属性中包含的 URL。`href`属性后面是一个尖括号，然后是页面上显示的文本。最后，`</a>`表示超链接元素的结束。
- en: 'We could find all the anchor elements in a web page’s code by doing a regular
    expression search for the `<a>` pattern or by specifying a regular expression
    to find URLs themselves. However, the Beautiful Soup module enables us to find
    the anchor elements more easily without worrying about regular expressions. We
    can find all the URLs that are linked from a website as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在网页代码中进行正则表达式搜索`<a>`模式，或者指定一个正则表达式来查找网址，从而找到网页中的所有锚点元素。然而，Beautiful Soup
    模块使得我们可以更加轻松地找到这些锚点元素，而无需担心正则表达式。我们可以通过以下方式找到网站中所有链接的 URL：
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we import the `requests` and `BeautifulSoup` modules. Just like every
    other third-party Python package, you will need to install `BeautifulSoup` before
    using it in a script. The `BeautifulSoup` module is part of a package called bs4\.
    The bs4 package has what are called *dependencies*: other packages that need to
    be installed for bs4 to work correctly. One of its dependencies is a package called
    lxml. You will need to install lxml before you can use bs4 and `BeautifulSoup`.
    After importing the modules we need, we use the `requests.get()` method to download
    a web page’s code, just as we’ve done previously in the chapter. But then we use
    the `BeautifulSoup()` method to parse the code and store the result in a variable
    called `soup`.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入了`requests`和`BeautifulSoup`模块。就像其他所有的第三方 Python 包一样，你需要在脚本中使用`BeautifulSoup`之前先安装它。`BeautifulSoup`模块是一个名为
    bs4 的包的一部分。bs4 包有一些所谓的*依赖项*：需要安装的其他包，以确保 bs4 正常工作。它的依赖项之一是名为 lxml 的包。在使用 bs4 和`BeautifulSoup`之前，你需要安装
    lxml。导入所需的模块后，我们使用`requests.get()`方法下载网页代码，正如我们在本章之前做过的那样。然后，我们使用`BeautifulSoup()`方法解析代码，并将结果存储在一个名为`soup`的变量中。
- en: Having the `soup` variable enables us to use particular methods from Beautiful
    Soup. In particular, we can use the `find_all()` method to look for particular
    types of elements in the web page code. In this case, we search for all anchor
    elements, which are identified by the character `a`. After getting all the anchor
    elements, we print out the value of their `href` attributes—the URLs of the pages
    or files they’re linking to. You can see that with Beautiful Soup, we can do useful
    parsing with only a few lines of code, all without using complicated regular expressions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有`soup`变量后，我们就可以使用 Beautiful Soup 的特定方法。特别是，我们可以使用`find_all()`方法来查找网页代码中的特定类型的元素。在这个例子中，我们搜索所有的锚点元素，它们由字符`a`表示。获取所有锚点元素后，我们打印出它们的`href`属性值——即它们链接到的页面或文件的
    URL。你可以看到，使用 Beautiful Soup，我们可以用几行代码完成有用的解析，而无需使用复杂的正则表达式。
- en: Parsing HTML Label Elements
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析 HTML 标签元素
- en: The anchor element is not the only type of element in HTML code. We saw the
    `<title>` element earlier in the chapter. Sometimes web pages also use the `<label>`
    element to put labels on text or content on their page. For example, imagine that
    you want to scrape contact information from the [http://bradfordtuckfield.com/contactscrape.xhtml](http://bradfordtuckfield.com/contactscrape.xhtml)
    web page that we saw earlier. We’ve reproduced [Figure 8-3](#figure8-3) as [Figure
    8-4](#figure8-4) here.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点元素并不是HTML代码中唯一的一种元素。我们在本章前面看到过`<title>`元素。有时网页也会使用`<label>`元素来为页面上的文本或内容添加标签。例如，假设你想从我们之前看到的[http://bradfordtuckfield.com/contactscrape.xhtml](http://bradfordtuckfield.com/contactscrape.xhtml)网页中抓取联系信息。我们已经将[图
    8-3](#figure8-3)复现为[图 8-4](#figure8-4)在这里。
- en: '![](image_fi/502888c08/f08004.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c08/f08004.png)'
- en: 'Figure 8-4: The content of a demo page that can be scraped easily'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-4：一个可以轻松抓取的示例页面内容
- en: 'You may be doing a project to search web pages for email addresses, phone numbers,
    or websites. Again, you could try to use regular expressions to search for these
    items. But the phone numbers and email addresses on this page are labeled with
    an HTML `<label>` element, so Beautiful Soup makes it easier to get the information
    we need. First, let’s look at how this `<label>` element is used in the HTML code
    for this web page. Here’s a small sample of this page’s code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能正在做一个项目，目的是搜索网页中的电子邮件地址、电话号码或网站。再一次，你可以尝试使用正则表达式来搜索这些内容。但是该网页上的电话号码和电子邮件地址是用HTML的`<label>`元素标注的，因此Beautiful
    Soup使得获取这些信息更加容易。首先，让我们看看该网页HTML代码中是如何使用`<label>`元素的。以下是该页面代码的一个小样本：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you saw earlier in the chapter, the `<label>` tag is used to indicate that
    a part of the HTML code is of a particular type. In this case, the `class` attribute
    identifies that this is a label for an email address. If the web page you’re scraping
    has these `<label>` elements, you can search for email addresses, phone numbers,
    and websites as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本章前面看到的，`<label>`标签用于表示HTML代码中的某个部分属于某种特定类型。在这种情况下，`class`属性标识这是一个电子邮件地址的标签。如果你抓取的网页中有这些`<label>`元素，你可以按如下方式搜索电子邮件地址、电话号码和网站：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here, we use the `soup.find()` method again. But instead of finding only elements
    labeled with `a`, as we did when we searched for hyperlinks, this time we also
    search for elements with the `<label>` tag. Each `<label>` tag in the code specifies
    a different `class`. We find the text with each kind of label (for email and mobile)
    and print out the text. For the website link, we search for an anchor tag with
    the `website` class. The final result is that we’ve been able to find every type
    of data we wanted: an email address, a cell phone number, and a website.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们再次使用`soup.find()`方法。但这次不仅仅是查找标签为`a`的元素，正如我们在查找超链接时所做的那样，而是还要查找带有`<label>`标签的元素。代码中的每个`<label>`标签指定了不同的`class`。我们找到每种标签（如电子邮件和手机）的文本并打印出来。对于网站链接，我们查找带有`website`类的锚点标签。最终的结果是我们能够找到每一种我们需要的数据：电子邮件地址、手机号码和网站。
- en: Scraping and Parsing HTML Tables
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抓取和解析HTML表格
- en: Tables are common on websites, so it’s worth knowing a little about how to scrape
    data from website tables. You can see a simple example of an HTML table if you
    visit [https://bradfordtuckfield.com/user_detailsscrape.xhtml](https://bradfordtuckfield.com/user_detailsscrape.xhtml).
    This web page contains a table with information about several fictional people,
    shown in [Figure 8-5](#figure8-5).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表格在网站上很常见，因此了解如何从网站表格中抓取数据是很有必要的。如果访问[https://bradfordtuckfield.com/user_detailsscrape.xhtml](https://bradfordtuckfield.com/user_detailsscrape.xhtml)，你可以看到一个简单的HTML表格示例。该网页包含了一个关于几个虚构人物的表格，展示在[图
    8-5](#figure8-5)中。
- en: '![](image_fi/502888c08/f08005.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c08/f08005.png)'
- en: 'Figure 8-5: A table that can be scraped using Beautiful Soup'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-5：可以使用Beautiful Soup抓取的表格
- en: 'Say we want to scrape information about these people from this table. Let’s
    look at the HTML code that specifies this table:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想从这个表格中抓取这些人物的信息。让我们来看看指定这个表格的HTML代码：
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The `<table>` tag specifies the beginning of the table, and `</table>` specifies
    the end of it. Between the beginning and the end are some `<tr>` and `</tr>` tags.
    Each `<tr>` tag specifies the beginning of a table row (`tr` is an abbreviation
    for *table row*). Within each table row, the `<td>` tags specify the content of
    particular table cells (`td` is short for *table data*). You can see that the
    first row is the header of the table, and it contains the names of every column.
    After the first row, each subsequent row specifies information about one person:
    their first name first, their surname second, and their age third, in three different
    `<td>` elements.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`<table>`标签指定表格的开始，`</table>`指定表格的结束。在开始和结束之间是一些`<tr>`和`</tr>`标签。每个`<tr>`标签指定一个表格行的开始（`tr`是*table
    row*的缩写）。在每个表格行内，`<td>`标签指定特定单元格的内容（`td`是*table data*的缩写）。你可以看到第一行是表格的头部，包含了每一列的名称。第一行之后的每一行都指定了一个人的信息：首先是名字，其次是姓氏，再次是年龄，分别放在三个不同的`<td>`元素中。'
- en: 'We can parse the table as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照如下方式解析表格：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we use Beautiful Soup again. We create a `soup` variable that contains
    the parsed version of the website. Then we use the `find_all()` method to find
    every `tr` element (table row) on the page. For every table row, we use `find_all()`
    again to look for every `td` element (table data) in the row. After finding the
    contents of each row, we print them out, with formatting to label first names,
    last names, and ages. In addition to printing these elements, you could also consider
    adding them to a pandas dataframe to more easily export them, sort them, or do
    any other analysis you prefer.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次使用Beautiful Soup。我们创建一个`soup`变量，包含网页的解析版本。然后，我们使用`find_all()`方法查找页面上的每一个`tr`元素（表格行）。对于每一行，我们再次使用`find_all()`来查找该行中的每一个`td`元素（表格数据）。找到每行的内容后，我们将它们打印出来，并格式化以标记名字、姓氏和年龄。除了打印这些元素外，你还可以考虑将它们添加到pandas数据框中，这样可以更轻松地导出、排序或进行其他分析。
- en: Advanced Scraping
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级爬虫
- en: Scraping is a deep topic, and there is more to learn beyond the material covered
    in this chapter. You could start with a few areas outlined in this section.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 爬虫是一个深奥的话题，本章所覆盖的内容只是冰山一角，仍有很多需要学习的地方。你可以从本节所列出的一些领域开始。
- en: First, consider that some web pages are dynamic; they change depending on interaction
    from the user, such as clicking elements or scrolling. Often the dynamic parts
    of web pages are rendered using JavaScript, a language with syntax that’s very
    different from the HTML we’ve focused on scraping in this chapter. The `requests`
    package that we used to download HTML code, and the Beautiful Soup module that
    we used to parse the code, are meant to be used with static web pages. With dynamic
    web pages, you may want to use another tool such as the Selenium library, which
    is designed for scraping dynamic web pages. With Selenium, your script can do
    things like enter information into website forms and click CAPTCHA-type challenges
    without requiring direct human input.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑到某些网页是动态的；它们会根据用户的交互发生变化，例如点击元素或滚动页面。网页的动态部分通常是通过JavaScript渲染的，而JavaScript的语法与我们在本章专注于抓取的HTML有很大不同。我们用来下载HTML代码的`requests`包，以及用来解析代码的Beautiful
    Soup模块，都是用于静态网页的。对于动态网页，你可能需要使用其他工具，如Selenium库，它专门用于抓取动态网页。使用Selenium时，你的脚本可以执行诸如填写网站表单、点击验证码挑战等操作，而无需直接的人工输入。
- en: You should also consider strategies to deal with being blocked. Many websites
    are hostile to all attempts to scrape their data. They have strategies to block
    scrapers, and if they detect that you’re trying to scrape and harvest their information,
    they may try to block you. One response to being blocked is to give up; this will
    avoid any legal problems or ethical issues that may come with scraping hostile
    websites.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该考虑应对被封锁的策略。许多网站对所有抓取其数据的尝试都抱有敌意。它们有策略来阻止爬虫，如果它们检测到你正在尝试抓取并获取它们的信息，它们可能会试图封锁你。应对被封锁的一种方式是放弃，这样可以避免因抓取敌对网站而产生的法律问题或道德问题。
- en: If you decide to scrape sites that are trying to block you anyway, you can take
    some actions to avoid being blocked. One is to set up one or more *proxy servers*.
    A website might block your IP address from accessing its data, so you can set
    up a different server with a different IP address that the website hasn’t blocked.
    If the website continues to try to block the IP address of your proxy server as
    well, you can set up *rotating proxies* so that you continuously get new IP addresses
    that are not blocked, and scrape only with those fresh, unblocked IP addresses.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定爬取那些试图阻止你的站点，你可以采取一些措施来避免被封锁。一个方法是设置一个或多个*代理服务器*。网站可能会阻止你的IP地址访问其数据，所以你可以设置一个不同的服务器，使用一个网站没有封锁的IP地址。如果网站继续试图封锁你的代理服务器的IP地址，你可以设置*轮换代理*，这样你可以不断获取新的、没有被封锁的IP地址，并仅使用这些新的、未封锁的IP地址进行爬取。
- en: 'When you take this kind of approach, you should consider its ethical implications:
    Do you feel comfortable using strategies like these to access a site that doesn’t
    want you to access it? Remember that in rare cases, unauthorized scraping can
    lead to lawsuits or even criminal prosecution. You should always be cautious and
    ensure that you’ve thought through the practical and ethical implications of everything
    you do.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当你采取这种方法时，你应该考虑其伦理影响：你是否愿意使用这些策略来访问一个不希望你访问的网站？记住，在极少数情况下，未经授权的爬取可能会导致诉讼甚至刑事起诉。你应该始终保持谨慎，确保你已经仔细思考了你所做一切的实际和伦理影响。
- en: Not all websites are averse to letting people access and scrape their data.
    Some websites allow scraping, and some even set up an *application programming
    interface (API)* to facilitate data access. An API allows you to query a website’s
    data automatically and receive data that’s in a user-friendly format. If you ever
    need to scrape a website, check whether it has an API that you can access. If
    a website has an API, the API documentation should indicate the data that the
    API provides and how you can access it. Many of the tools and ideas we’ve discussed
    in this chapter also apply to API usage. For example, the `requests` package can
    be used to interact with APIs, and after getting API data, the data can be used
    to populate a pandas dataframe.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有网站都反感让人访问和爬取它们的数据。有些网站允许爬取，甚至有些网站还设置了*应用程序接口（API）*来方便数据访问。API允许你自动查询网站的数据，并以用户友好的格式接收数据。如果你需要爬取某个网站，检查它是否有你可以访问的API。如果网站有API，API文档应指明API提供的数据以及如何访问它。我们在本章讨论的许多工具和思路也适用于API使用。例如，`requests`包可以用来与API进行交互，获取API数据后，这些数据可以用于填充pandas数据框。
- en: Finally, timing is an important issue to consider when you set up scraping scripts.
    Sometimes a scraping script makes many requests to a website in quick succession,
    trying to download as much data as possible, as quickly as possible. This could
    cause a website to be overwhelmed and crash, or it may block the scraper to avoid
    getting overwhelmed. To prevent the target site from crashing or blocking you,
    you can adjust your scraper so that it works more slowly. One way to slow down
    your script is to deliberately add pauses. For example, after downloading one
    row from a table, the script can pause and do nothing (the script can *sleep*)
    for 1 second or 2 seconds or 10 seconds, and then download the next row from the
    table. Going slowly on purpose can be frustrating for those of us who like to
    get things done quickly, but it can often make scraping success more likely over
    the long term.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，定时是设置爬虫脚本时需要考虑的一个重要问题。有时候，爬虫脚本会快速连续地向一个网站发起多个请求，尽可能快速地下载尽可能多的数据。这可能会导致网站被压垮而崩溃，或者网站可能会阻止爬虫以防止过载。为了避免目标网站崩溃或阻止你，你可以调整爬虫，让它的运行速度变得更慢。一种减慢脚本速度的方法是故意添加暂停。例如，在从一个表格下载一行数据后，脚本可以暂停并什么也不做（脚本可以*休眠*）1秒、2秒或10秒，然后再下载表格的下一行。故意慢下来对那些喜欢快速完成任务的人来说可能会让人沮丧，但从长远来看，它往往能让爬取成功的概率更高。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered web scraping. We outlined the concept of scraping,
    including a brief introduction to how HTML code works. We went on to build a simple
    scraper, one that merely downloads and prints out the code for a web page. We
    also searched through and parsed a website’s code, including using regular expressions
    for advanced searches. We showed how to convert the data we scrape from websites
    to usable datasets. We also used Python’s Beautiful Soup to easily find hyperlinks
    and tagged information on web pages. Finally, we briefly discussed some advanced
    applications of scraping skills, including API integrations and scraping dynamic
    websites. In the next chapter, we’ll be going over recommendation systems. Let’s
    continue!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了网页抓取技术。我们概述了抓取的概念，并简要介绍了HTML代码的工作原理。接着，我们构建了一个简单的抓取工具，它仅仅是下载并打印出网页的代码。我们还通过解析网站代码进行了搜索，包括使用正则表达式进行高级搜索。我们展示了如何将抓取自网站的数据转换为可用的数据集。我们还使用了Python的Beautiful
    Soup库，轻松地在网页上找到超链接和标签信息。最后，我们简要讨论了一些抓取技能的高级应用，包括API集成和抓取动态网站。在下一章中，我们将讨论推荐系统。让我们继续！
