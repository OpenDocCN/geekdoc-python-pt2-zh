<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch14"><span epub:type="pagebreak" id="page_343"/><strong><span class="big">14</span><br/>EXPERIMENTS WITH CIFAR-10</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">In this chapter, we’ll perform a series of experiments with the CIFAR-10 dataset we built in <a href="ch05.xhtml#ch05">Chapter 5</a>. First, we’ll see how two models, one shallow, the other deeper, perform on the full dataset. After that, we’ll work with grouped subsets of the entire dataset to see if we can tell the difference between animals and vehicles. Next, we’ll answer the question of what’s better for the CIFAR-10 dataset, a single multiclass model or a set of binary models, one per class.</p>&#13;
<p class="indent">We’ll close the chapter by introducing transfer learning and fine-tuning. These are important concepts, often confounded, that are widely used in the machine learning community, so we should develop an intuitive feel for how they work.</p>&#13;
<h3 class="h3" id="lev1_94">A CIFAR-10 Refresher</h3>&#13;
<p class="noindent">Before we dive into the experiments, let’s refamiliarize ourselves with the dataset we’re working with. CIFAR-10 is a 10-class dataset from the Canadian Institute for Advanced Research (CIFAR). We built this dataset in <span epub:type="pagebreak" id="page_344"/><a href="ch05.xhtml#ch05">Chapter 5</a> but deferred its use until now. CIFAR-10 consists of 32×32-pixel RGB images of animals (six classes) and vehicles (four classes). Take a look at <a href="ch05.xhtml#ch5fig4">Figure 5-4</a> for some sample images. The training set has 50,000 images, 5,000 from each class, so it is a balanced dataset. The test set consists of 10,000 images, 1,000 from each class. CIFAR-10 is probably the second most widely used standard dataset in machine learning after MNIST. There is also a 100-class version, CIFAR-100, that we’ll not work with in this book, but you’ll see it pop up often in the literature.</p>&#13;
<p class="indent">As of this writing, the best performing model on unaugmented CIFAR-10 has achieved a 1 percent error on the test set (<a href="http://benchmarks.ai">benchmarks.ai</a>). The model that did this has 557 million parameters. Our models will be significantly smaller and have a much larger test error. However, this is a true image dataset, unlike MNIST, which is very clean and has a uniform black background for every digit. Because of the variation in natural images, especially in their backgrounds, we might expect models to have a harder time learning the CIFAR-10 classes compared to MNIST.</p>&#13;
<p class="indent">For reference throughout the chapter, here are CIFAR-10 classes:</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Label</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Class</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Label</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Class</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">airplane</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">dog</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">automobile</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">frog</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">bird</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">horse</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">cat</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">ship</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">deer</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">truck</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<h3 class="h3" id="lev1_95">Working with the Full CIFAR-10 Dataset</h3>&#13;
<p class="noindent">Let’s train two different models on the entire CIFAR-10 dataset. The first model is the same one we used in <a href="ch13.xhtml#ch13">Chapter 13</a> for the MNIST dataset. We’ll refer to this model as our <em>shallow model</em> because it has only two convolutional layers. We’ll need to adapt it a touch for the 32 × 32 RGB inputs, but that’s straightforward enough to do. The second model, which we’ll call our <em>deep model</em>, uses multiple convolutional layers before the pooling and fully connected layers.</p>&#13;
<p class="indent">Additionally, we’ll experiment with both stochastic gradient descent and Adadelta as our optimization algorithms. We’ll fix the minibatch size at 64 and train for 60 epochs for a total of 46,875 gradient descent steps. For SGD, we’ll use a learning rate of 0.01 and a momentum of 0.9. Recall, Adadelta is adaptive and alters the learning rate on the fly. We can decrease the learning rate for SGD as training progresses, but 0.01 is relatively small, and we have a large number of gradient descent steps, so we’ll just leave it a constant.</p>&#13;
<p class="indent">The shallow model has 1,626,442 parameters, while the deep model has only 1,139,338. The deep model is deep because it has more layers, but <span epub:type="pagebreak" id="page_345"/>because each convolutional layer is using exact convolution, the output decreases by two each time (for a 3 × 3 kernel). Therefore, the flatten layer after the pooling layer has only 7,744 values compared to 12,544 for the shallow model. The weight matrix between the flatten layer and the dense layer of 128 nodes contains the vast majority of the parameters, 7,744 × 128 = 991,232 compared to 12,544 × 128 = 1,605,632. Thus, going deeper has actually reduced the number of parameters to learn. This slightly counterintuitive result reminds us of the large expense incurred by fully connected layers and some of the initial motivation for the creation of CNNs.</p>&#13;
<h4 class="h4" id="lev2_132">Building the Models</h4>&#13;
<p class="noindent">You’ll find the code for the shallow model in <em>cifar10_cnn.py</em> (Adadelta) and <em>cifar10_cnn_SGD.py</em> (SGD). We’ll work through the code in pieces. The shallow model starts in much the same way as for the MNIST dataset, as shown in <a href="ch14.xhtml#ch14lis1">Listing 14-1</a>.</p>&#13;
<p class="programs" id="ch14lis1">import keras<br/>&#13;
from keras.models import Sequential<br/>&#13;
from keras.layers import Dense, Dropout, Flatten<br/>&#13;
from keras.layers import Conv2D, MaxPooling2D<br/>&#13;
from keras import backend as K<br/>&#13;
import numpy as np<br/>&#13;
<br/>&#13;
batch_size = 64<br/>&#13;
num_classes = 10<br/>&#13;
epochs = 60<br/>&#13;
img_rows, img_cols = 32, 32<br/>&#13;
<br/>&#13;
x_train = np.load("cifar10_train_images.npy")<br/>&#13;
y_train = np.load("cifar10_train_labels.npy")<br/>&#13;
x_test = np.load("cifar10_test_images.npy")<br/>&#13;
y_test = np.load("cifar10_test_labels.npy")<br/>&#13;
<br/>&#13;
if K.image_data_format() == 'channels_first':<br/>&#13;
    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)<br/>&#13;
    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)<br/>&#13;
    input_shape = (3, img_rows, img_cols)<br/>&#13;
else:<br/>&#13;
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)<br/>&#13;
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)<br/>&#13;
    input_shape = (img_rows, img_cols, 3)<br/>&#13;
<br/>&#13;
x_train = x_train.astype('float32')<br/>&#13;
x_test = x_test.astype('float32')<br/>&#13;
(*\newpage*)<br/>&#13;
<span epub:type="pagebreak" id="page_346"/>x_train /= 255<br/>&#13;
x_test /= 255<br/>&#13;
<br/>&#13;
y_train = keras.utils.to_categorical(y_train, num_classes)<br/>&#13;
y_test = keras.utils.to_categorical(y_test, num_classes)</p>&#13;
<p class="figcap"><em>Listing 14-1: Preparing the CIFAR-10 dataset</em></p>&#13;
<p class="indent">We import the necessary modules and load the CIFAR-10 dataset from the NumPy files we created in <a href="ch05.xhtml#ch05">Chapter 5</a>. Notice that the image dimensions are now 32 × 32, not 28 × 28, and that the number of channels is 3 (RGB) instead of 1 (grayscale). As before, we scale the inputs by 255 to map the images to [0,1] and convert the label numbers to one-hot vectors using <code>to_categorical</code>.</p>&#13;
<p class="indent">Next, we define the model architecture (<a href="ch14.xhtml#ch14lis2">Listing 14-2</a>).</p>&#13;
<p class="programs" id="ch14lis2">model = Sequential()<br/>&#13;
model.add(Conv2D(32, kernel_size=(3, 3),<br/>&#13;
                 activation='relu',<br/>&#13;
                 input_shape=input_shape))<br/>&#13;
model.add(Conv2D(64, (3, 3), activation='relu'))<br/>&#13;
model.add(MaxPooling2D(pool_size=(2, 2)))<br/>&#13;
model.add(Dropout(0.25))<br/>&#13;
model.add(Flatten())<br/>&#13;
model.add(Dense(128, activation='relu'))<br/>&#13;
model.add(Dropout(0.5))<br/>&#13;
model.add(Dense(num_classes, activation='softmax'))<br/>&#13;
<br/>&#13;
model.compile(loss=keras.losses.categorical_crossentropy,<br/>&#13;
              optimizer=keras.optimizers.Adadelta(),<br/>&#13;
              metrics=['accuracy'])</p>&#13;
<p class="figcap"><em>Listing 14-2: Building the shallow CIFAR-10 model</em></p>&#13;
<p class="indent">This step is identical to the MNIST version for the shallow model (see <a href="ch13.xhtml#ch13lis1">Listing 13-1</a>). For the deep model, we add more convolutional layers, as shown in <a href="ch14.xhtml#ch14lis3">Listing 14-3</a>.</p>&#13;
<p class="programs" id="ch14lis3">model = Sequential()<br/>&#13;
model.add(Conv2D(32, kernel_size=(3, 3),<br/>&#13;
                 activation='relu',<br/>&#13;
                 input_shape=input_shape))<br/>&#13;
<br/>&#13;
model.add(Conv2D(64, (3,3), activation='relu'))<br/>&#13;
model.add(Conv2D(64, (3,3), activation='relu'))<br/>&#13;
model.add(Conv2D(64, (3,3), activation='relu'))<br/>&#13;
model.add(Conv2D(64, (3,3), activation='relu'))<br/>&#13;
<br/>&#13;
model.add(MaxPooling2D(pool_size=(2,2)))<br/>&#13;
model.add(Dropout(0.25))<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_347"/>model.add(Flatten())<br/>&#13;
model.add(Dense(128, activation='relu'))<br/>&#13;
model.add(Dropout(0.5))<br/>&#13;
model.add(Dense(128, activation='relu'))<br/>&#13;
model.add(Dropout(0.5))<br/>&#13;
model.add(Dense(num_classes, activation='softmax'))<br/>&#13;
<br/>&#13;
model.compile(loss=keras.losses.categorical_crossentropy,<br/>&#13;
              optimizer=keras.optimizers.Adadelta(),<br/>&#13;
              metrics=['accuracy'])</p>&#13;
<p class="figcap"><em>Listing 14-3: Building the deep CIFAR-10 model</em></p>&#13;
<p class="indent">The extra convolutional layers give the model the opportunity to learn a better representation of the input data, which for CIFAR-10 is more complex than the simple MNIST images. The representation might be better because a deeper network can learn more abstract representations that encompass larger structures in the inputs.</p>&#13;
<p class="indent">The code snippets in <a href="ch14.xhtml#ch14lis2">Listings 14-2</a> and <a href="ch14.xhtml#ch14lis3">14-3</a> compile the model using Adadelta as the optimization algorithm. We also want a version of each that uses SGD. If we replace the reference to <code>Adadelta()</code> in the <code>compile</code> method with the following</p>&#13;
<pre>optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9)</pre>&#13;
<p class="noindent">we’ll use SGD with the learning rate and momentum values we indicated earlier. For completeness, the rest of the code for both shallow and deep models is shown in <a href="ch14.xhtml#ch14lis4">Listing 14-4</a>.</p>&#13;
<p class="programs" id="ch14lis4">print("Model parameters = %d" % model.count_params())<br/>&#13;
print(model.summary())<br/>&#13;
<br/>&#13;
history = model.fit(x_train, y_train,<br/>&#13;
          batch_size=batch_size,<br/>&#13;
          epochs=epochs,<br/>&#13;
          verbose=1,<br/>&#13;
          validation_data=(x_test[:1000], y_test[:1000]))<br/>&#13;
<br/>&#13;
score = model.evaluate(x_test[1000:], y_test[1000:], verbose=0)<br/>&#13;
print('Test loss:', score[0])<br/>&#13;
print('Test accuracy:', score[1])<br/>&#13;
<br/>&#13;
model.save("cifar10_cnn_model.h5")</p>&#13;
<p class="figcap"><em>Listing 14-4: Training and testing the CIFAR-10 models</em></p>&#13;
<p class="indent">This code summarizes the model architecture and number of parameters, trains by calling the <code>fit</code> method using the first 1,000 test samples for validation, and then evaluates the trained model on the remaining 9,000 test <span epub:type="pagebreak" id="page_348"/>samples by calling the <code>evaluate</code> method. We report the test loss and accuracy. Then we write the model to disk (<code>save</code>), and store the history showing the per epoch loss and accuracy during training. We’ll use the history files to generate plots showing the loss and error (1 – accuracy) as a function of the training epoch.</p>&#13;
<p class="indent"><a href="ch14.xhtml#ch14lis4">Listing 14-4</a> gives us four files: shallow model + Adadelta, shallow model + SGD, deep model + Adadelta, and deep model + SGD. Let’s run each of these to see our final test accuracy and then look at plots of the training process to see what we can learn.</p>&#13;
<p class="indent">Running the code trains and evaluates the models. This takes some time on our CPU-only system, about eight hours total. The random initialization Keras uses means that when you run the code yourself, you should see slightly different answers. When I ran the code, I got <a href="ch14.xhtml#ch14tab1">Table 14-1</a>.</p>&#13;
<p class="tabcap" id="ch14tab1"><strong>Table 14-1:</strong> Test Set Accuracies by Model Size and Optimizer</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:40%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Shallow</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Deep</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Adadelta</p></td>&#13;
<td style="vertical-align: top"><p class="tab">71.9%</p></td>&#13;
<td style="vertical-align: top"><p class="tab">74.8%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">SGD</p></td>&#13;
<td style="vertical-align: top"><p class="tab">70.0%</p></td>&#13;
<td style="vertical-align: top"><p class="tab">72.8%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The table tells us that using Adadelta gave us a more accurate model for both shallow and deep models compared to SGD. We also see that the deep model outperforms the shallow model regardless of optimizer. Adaptive optimizers like Adadelta and Adam (also in Keras) are generally preferred to plain old SGD for this reason. However, I have seen claims that SGD is ultimately just as good or better once the learning rate is set up right and decreased as training proceeds. Of course, nothing prevents us from starting with an adaptive optimizer and then switching to SGD after some number of epochs. The idea here is that the adaptive optimizer “gets close” to a minimum of the loss function while SGD fine-tunes the process at that point.</p>&#13;
<h4 class="h4" id="lev2_133">Analyzing the Models</h4>&#13;
<p class="noindent">Let’s look at how the loss changes during training. <a href="ch14.xhtml#ch14fig1">Figure 14-1</a> shows the loss per epoch for the shallow and deep models using Adadelta (top) and SGD (bottom).</p>&#13;
<div class="image" id="ch14fig1"><span epub:type="pagebreak" id="page_349"/><img src="Images/14fig01.jpg" alt="image" width="668" height="1009"/></div>&#13;
<p class="figcap"><em>Figure 14-1: Training loss for shallow and deep models using Adadelta (top) and SGD (bottom)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_350"/>Starting with the Adadelta loss plot, we see that compared to SGD, the loss is not as low. We also see that for the shallow model, the loss is increasing slightly per epoch. This is a counterintuitive result and seems to contradict conventional wisdom that the training loss should only decrease. There are reports of this happening with Adam, another adaptive optimizer, so it is likely an artifact of the adaptation algorithm. Regardless, as we saw in <a href="ch14.xhtml#ch14tab1">Table 14-1</a>, Adadelta leads to higher accuracies for both shallow and deep models.</p>&#13;
<p class="indent">On the bottom of <a href="ch14.xhtml#ch14fig1">Figure 14-1</a>, we see that SGD leads to a smaller loss for the shallow model compared to the deep model. This is typically interpreted as a hint for potential overfitting. The model is learning the details of the training set as the loss tends to 0. The shallow model using SGD was the least performant model according to <a href="ch14.xhtml#ch14tab1">Table 14-1</a>. The deep model using SGD did not have such a small loss, at least up to 60 training epochs.</p>&#13;
<p class="indent">What about the validation set accuracy during training? <a href="ch14.xhtml#ch14fig2">Figure 14-2</a> plots the <em>error</em> by epoch. The error is easier to understand visually; it should tend toward 0 as accuracy increases. Again, the Adadelta models are on the top, and the SGD models are on the bottom.</p>&#13;
<p class="indent">As expected, regardless of optimizer, the deeper model performed better and had a lower validation set error during training. Note, the validation set error is not the final, held-out test set error, but instead the portion of the test set used during training, the first 1,000 samples in this case.</p>&#13;
<p class="indent">The SGD curves on the bottom of <a href="ch14.xhtml#ch14fig2">Figure 14-2</a> follow what our intuition should tell us: as the model trains, it gets better, leading to a smaller error. The deep model quickly overtakes the shallow model—again, an intuitive result. Also, the curves are relatively smooth as the model gets better and better.</p>&#13;
<p class="indent">The Adadelta error plots on the top of <a href="ch14.xhtml#ch14fig2">Figure 14-2</a> are a different story. There is an obvious decrease in the error after the first few epochs. However, after that, the validation set error jumps around somewhat chaotically though still following our intuition that the deep model should have a smaller error than the shallow model. This chaotic result is due to the adaptive nature of the Adadelta algorithm, which is adjusting the learning rate on the fly to search for a better minimum. From the results of <a href="ch14.xhtml#ch14tab1">Table 14-1</a>, it’s clear that Adadelta is finding better-performing models.</p>&#13;
<div class="image" id="ch14fig2"><span epub:type="pagebreak" id="page_351"/><img src="Images/14fig02.jpg" alt="image" width="670" height="1012"/></div>&#13;
<p class="figcap"><em>Figure 14-2: Validation set error for shallow and deep models using Adadelta (top) and SGD (bottom)</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_352"/>These experiments tell us that adaptive optimization algorithms and deeper networks (to a point) tend toward better-performing models. While recognizing the danger inherent in attempting to offer advice in this field, it seems safe to say that one should start with adaptive optimization and use a large enough model. To find out just what <em>large enough</em> means, I suggest starting with a modest model and, after training, making it deeper and seeing if that improves things. Eventually, the model will be too large for the training set, so there will be a cutoff point where increasing the size of the model no longer helps. In that case, get more training data, if possible.</p>&#13;
<p class="indent">Let’s now shift our attention to working with subsets of CIFAR-10.</p>&#13;
<h3 class="h3" id="lev1_96">Animal or Vehicle?</h3>&#13;
<p class="noindent">Four of the ten classes in CIFAR-10 are vehicles; the remaining six are animals. Let’s build a model to separate the two and see what we can learn from it. We already have the images; all we need do is recode the labels so that all the vehicles are marked as class 0 and all the animals as class 1. Doing this is straightforward, as shown in <a href="ch14.xhtml#ch14lis5">Listing 14-5</a>.</p>&#13;
<p class="programs" id="ch14lis5">import numpy as np<br/>&#13;
y_train = np.load("cifar10_train_labels.npy")<br/>&#13;
y_test  = np.load("cifar10_test_labels.npy")<br/>&#13;
for i in range(len(y_train)):<br/>&#13;
    if (y_train[i] in [0,1,8,9]):<br/>&#13;
        y_train[i] = 0<br/>&#13;
    else:<br/>&#13;
        y_train[i] = 1<br/>&#13;
for i in range(len(y_test)):<br/>&#13;
    if (y_test[i] in [0,1,8,9]):<br/>&#13;
        y_test[i] = 0<br/>&#13;
    else:<br/>&#13;
        y_test[i] = 1<br/>&#13;
np.save("cifar10_train_animal_vehicle_labels.npy", y_train)<br/>&#13;
np.save("cifar10_test_animal_vehicle_labels.npy", y_test)</p>&#13;
<p class="figcap"><em>Listing 14-5: Adjusting the labels of CIFAR-10 into vehicles (class 0) and animals (class 1)</em></p>&#13;
<p class="indent">We load the existing train and test label files, already matched in order with the train and test image files, and build new label vectors mapping the vehicle classes—classes 0, 1, 8, and 9—to 0 and all the others to 1.</p>&#13;
<p class="indent">The code in the previous section for building and training the model remains the same except for the definition of the model architecture and the particular file we load for the train and test labels. The number of classes (<code>num_classes</code>) is set to 2, the minibatch size is 128, and we’ll train for 12 epochs. The training set isn’t completely balanced—there are 20,000 vehicles and 30,000 animals—but the imbalance isn’t severe, so we should be <span epub:type="pagebreak" id="page_353"/>in good shape. Remember that when one class is scarce, it becomes difficult for the model to learn it well. We’ll stick with Adadelta as the optimizer and use the first 1,000 test samples for validation and the remaining 9,000 for final test. We’ll use the same shallow architecture used in the previous section.</p>&#13;
<p class="indent">Training this model on the CIFAR-10 images with the recoded labels gives us a final test accuracy of 93.6 percent. Let’s be a little pedantic and calculate all the performance metrics from <a href="ch11.xhtml#ch11">Chapter 11</a>. To do this, we update the <code>tally_predictions</code> function defined in that chapter (<a href="ch11.xhtml#ch11lis1">Listing 11-1</a>) to work with a Keras model. We’ll also use <code>basic_metrics</code> (<a href="ch11.xhtml#ch11lis2">Listing 11-2</a>) and <code>advanced_metrics</code> (<a href="ch11.xhtml#ch11lis3">Listing 11-3</a>) from <a href="ch11.xhtml#ch11">Chapter 11</a>. The updated code for <code>tally_predictions</code> is shown in <a href="ch14.xhtml#ch14lis6">Listing 14-6</a>.</p>&#13;
<p class="programs" id="ch14lis6">def tally_predictions(model, x, y):<br/>&#13;
    pp = model.predict(x)<br/>&#13;
    p = np.zeros(pp.shape[0], dtype="uint8")<br/>&#13;
 <span class="ent">❶</span> for i in range(pp.shape[0]):<br/>&#13;
        p[i] = 0 if (pp[i,0] &gt; pp[i,1]) else 1<br/>&#13;
    tp = tn = fp = fn = 0<br/>&#13;
    for i in range(len(y)):<br/>&#13;
        if (p[i] == 0) and (y[i] == 0):<br/>&#13;
            tn += 1<br/>&#13;
        elif (p[i] == 0) and (y[i] == 1):<br/>&#13;
            fn += 1<br/>&#13;
        elif (p[i] == 1) and (y[i] == 0):<br/>&#13;
            fp += 1<br/>&#13;
        else:<br/>&#13;
            tp += 1<br/>&#13;
    score = float(tp+tn) / float(tp+tn+fp+fn)<br/>&#13;
    return [tp, tn, fp, fn, score]</p>&#13;
<p class="figcap"><em>Listing 14-6: Calculating basic metrics for Keras models</em></p>&#13;
<p class="indent">We pass in the model, test samples (<code>x</code>), and test labels (<code>y</code>). Unlike the sklearn version of <code>tally_predictions</code>, here we first use the model to predict per class probabilities (<code>pp</code>). This returns a 2D array, one row for each sample in <code>x</code>, where the columns are the probabilities assigned per class. Here there are two columns because there are only two classes: vehicle or animal.</p>&#13;
<p class="indent">Before we can tally the true positives, true negatives, false positives (vehicle classified as animal), and false negatives (animal classified as vehicle), we need to assign a class label to each test sample. We do this by looping over the predictions, row by row, and asking whether the probability for class 0 is greater than class 1 or not <span class="ent">❶</span>. Once we have assigned a predicted class label (<code>p</code>), we can calculate the tallies and return them along with the overall score (accuracy). We pass the list returned by <code>tally_predictions</code> to <code>basic_metrics</code> and then pass the output of both of these functions to <code>advanced_metrics</code>, as in <a href="ch11.xhtml#ch11">Chapter 11</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_354"/>The full set of binary classifier metrics gives us the following:</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Metric</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Result</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">TP</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5,841</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">FP</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4,80</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">TN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3,520</p></td>&#13;
</tr>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab">FN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">159</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">TPR (sensitivity, recall)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9735</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">TNR (specificity)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8800</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">PPV (precision)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9241</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">NPV</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9568</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">FPR</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1200</p></td>&#13;
</tr>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab">FNR</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0265</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">F1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9481</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">MCC</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8671</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>κ</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8651</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Informedness</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8535</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Markedness</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8808</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Accuracy</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9361</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">We see that this is a well-performing model although a specificity of 88 percent is a little on the low side. As argued in <a href="ch11.xhtml#ch11">Chapter 11</a>, the Matthews correlation coefficient (MCC) is possibly the best single number for characterizing a binary classifier. Here we have an MCC of 0.8671 out of 1.0, indicative of a good model.</p>&#13;
<p class="indent">Recall that the <em>sensitivity</em> is the probability that an animal is called an “animal” by this model, and the <em>specificity</em> is the probability that a vehicle is called a “vehicle.” The <em>precision</em> is the probability that when the model assigns a label of “animal,” it is correct, and the <em>NPV (negative predictive value)</em> is the probability of the model being correct when it assigns a label of “vehicle.” Note also that the false positive rate (FPR) is 1 – specificity, and the false negative rate (FNR) is 1 – sensitivity.</p>&#13;
<p class="indent">A little more code will calculate the ROC curve and its area:</p>&#13;
<pre>from sklearn.metrics import roc_auc_score, roc_curve<br/>&#13;
def roc_curve_area(model, x, y):<br/>&#13;
    pp = model.predict(x)<br/>&#13;
    p = np.zeros(pp.shape[0], dtype="uint8")<br/>&#13;
    for i in range(pp.shape[0]):<br/>&#13;
        p[i] = 0 if (pp[i,0] &gt; pp[i,1]) else 1<br/>&#13;
    auc = roc_auc_score(y,p)<br/>&#13;
    roc = roc_curve(y,pp[:,1])<br/>&#13;
    return [auc, roc]</pre>&#13;
<p class="indent">Again, we pass in the trained model, the test samples (<code>x</code>), and the animal or vehicle labels (<code>y</code>). We also convert the output probabilities to class predictions, as we did in <a href="ch14.xhtml#ch14lis6">Listing 14-6</a>. The AUC is 0.9267, and <a href="ch14.xhtml#ch14fig3">Figure 14-3</a> shows the ROC curve (note the zoomed axes). This curve is steep and close to the upper-left corner of the plot—all good signs of a well-performing model.</p>&#13;
<div class="image" id="ch14fig3"><span epub:type="pagebreak" id="page_355"/><img src="Images/14fig03.jpg" alt="image" width="670" height="500"/></div>&#13;
<p class="figcap"><em>Figure 14-3: ROC curve for the animal or vehicle model</em></p>&#13;
<p class="indent">We grouped animals and vehicles and asked a single model to learn something about the difference between them. Clearly, some characteristics differentiate the two classes, and the model has learned to use them successfully. However, unlike most binary classifiers, we know finer label assignments for the test data. For example, we know which of the animals are birds or deer or frogs. Likewise, we know which samples are airplanes, ships, or trucks.</p>&#13;
<p class="indent">When the model makes a mistake, the mistake is either a false positive (calling a vehicle an animal) or a false negative (calling an animal a vehicle). We chose animals to be class 1, so false positives are cases where a vehicle was called an animal. The converse is true for false negatives. We can use the full class labels to tell us how many of the false positives are represented by which vehicle classes, and we can do the same for the false negatives to tell us which animal classes were assigned to the vehicle class. A few lines of code in <a href="ch14.xhtml#ch14lis7">Listing 14-7</a> give us what we are after.</p>&#13;
<p class="programs" id="ch14lis7">   import numpy as np<br/>&#13;
   from keras.models import load_model<br/>&#13;
   x_test = np.load("cifar10_test_images.npy")/255.0<br/>&#13;
   y_label= np.load("cifar10_test_labels.npy")<br/>&#13;
   y_test = np.load("cifar10_test_animal_vehicle_labels.npy")<br/>&#13;
   model = load_model("cifar10_cnn_animal_vehicle_model.h5")<br/>&#13;
   pp = model.predict(x_test)<br/>&#13;
   p = np.zeros(pp.shape[0], dtype="uint8")<br/>&#13;
   for i in range(pp.shape[0]):<br/>&#13;
       p[i] = 0 if (pp[i,0] &gt; pp[i,1]) else 1<br/>&#13;
<span epub:type="pagebreak" id="page_356"/>   hp = []; hn = []<br/>&#13;
<span class="ent">❶</span> for i in range(len(y_test)):<br/>&#13;
       if (p[i] == 0) and (y_test[i] == 1):<br/>&#13;
           hn.append(y_label[i])<br/>&#13;
       elif (p[i] == 1) and (y_test[i] == 0):<br/>&#13;
           hp.append(y_label[i])<br/>&#13;
   hp = np.array(hp)<br/>&#13;
   hn = np.array(hn)<br/>&#13;
   a = np.histogram(hp, bins=10, range=[0,9])[0]<br/>&#13;
   b = np.histogram(hn, bins=10, range=[0,9])[0]<br/>&#13;
   print("vehicles as animals: %s" % np.array2string(a))<br/>&#13;
   print("animals as vehicles: %s" % np.array2string(b))</p>&#13;
<p class="figcap"><em>Listing 14-7: Using the fine class labels to determine which classes account for false positives and false negatives</em></p>&#13;
<p class="indent">First, we load the test set images, actual labels (<code>y_label</code>), and animal or vehicle labels (<code>y_test</code>). Then, as before, we load the model and get the model predictions (<code>p</code>). We want to keep track of the actual class label for each false positive and false negative, the mistakes the classifier has made. We do this by looping over the predictions and comparing them to the animal or vehicle labels <span class="ent">❶</span>. When there is an error, we keep the actual label of the sample, be it an FN (<code>hn</code>) or FP (<code>hp</code>). Note that this works because when we defined the animal or vehicle labels, we were careful to keep the order the same as the original label set.</p>&#13;
<p class="indent">Once we have the actual labels for all FP and FN cases, we use <code>histogram</code> to do the tallying for us. There are 10 actual class labels, so we tell <code>histogram</code> that we want to use 10 bins. We also need to specify the range for the bins (<code>range=[0,9]</code>). We want only the counts themselves, so we need to keep only the first array returned by <code>histogram</code>, hence the <code>[0]</code> at the end of the call. Finally, we print the arrays to get</p>&#13;
<pre>vehicles as animals: [189  69   0   0   0   0   0   0 105 117]<br/>&#13;
animals as vehicles: [ 0  0 64 34 23 11 12 15  0  0]</pre>&#13;
<p class="indent">This means that of the vehicles the model called “animal,” 189 of them were of class 0, airplane. The vehicle class least likely to be identified as an animal is class 1, automobile. Ships and trucks were similarly likely to be mistaken for an animal. Going the other way, we see that class 2, birds, were most likely to be mistaken for vehicles and class 5, dogs, were least likely to be misclassified, though frogs were a close second.</p>&#13;
<p class="indent">What to make of this? The most commonly misclassified vehicle is an airplane, while the most commonly misclassified animal is a bird. This makes sense: a picture of an airplane and a picture of a bird flying do look similar. I’ll leave it to you to make connections among the other categories.</p>&#13;
<h3 class="h3" id="lev1_97"><span epub:type="pagebreak" id="page_357"/>Binary or Multiclass?</h3>&#13;
<p class="noindent">Conventional wisdom in machine learning is that a multiclass model will generally outperform multiple binary models. While this is almost certainly true for large datasets, large models, and situations with many classes, like the ImageNet dataset of 1,000 classes, how does it pan out for small models like the ones we’re working with in this chapter? Let’s find out.</p>&#13;
<p class="indent">There are 5,000 instances of each class in the CIFAR-10 dataset and 10 classes. This means we can train 10 binary models where the target class (class 1) is one of the 10 classes, and the other class is everything else. This is known as a <em>one-vs-rest</em> approach. To classify an unknown sample, we run it through each of the 10 classifiers and assign the label of the model returning the most confident answer. The datasets are all imbalanced, 5,000 class 1 instances to 45,000 class 0, but, as we’ll see, there is still enough data to learn the difference between classes.</p>&#13;
<p class="indent">We need some code to train 10 one-vs-rest models. We’ll use the shallow architecture we’ve used before, with a minibatch size of 128, and we’ll train for 12 epochs. Before we can train, however, we need to reassign the class labels for the train and test sets so that all instances of the target class are a 1 and everything else is a 0. To build the per class labels, we’ll use <a href="ch14.xhtml#ch14lis8">Listing 14-8</a>.</p>&#13;
<p class="programs" id="ch14lis8">   import sys<br/>&#13;
   import numpy as np<br/>&#13;
<span class="ent">❶</span> class1 = eval("["+sys.argv[1]+"]")<br/>&#13;
   y_train = np.load("cifar10_train_labels.npy")<br/>&#13;
   y_test  = np.load("cifar10_test_labels.npy")<br/>&#13;
   for i in range(len(y_train)):<br/>&#13;
       if (y_train[i] in class1):<br/>&#13;
           y_train[i] = 1<br/>&#13;
       else:<br/>&#13;
           y_train[i] = 0<br/>&#13;
   for i in range(len(y_test)):<br/>&#13;
       if (y_test[i] in class1):<br/>&#13;
           y_test[i] = 1<br/>&#13;
       else:<br/>&#13;
           y_test[i] = 0<br/>&#13;
   np.save(sys.argv[2], y_train)<br/>&#13;
   np.save(sys.argv[3], y_test)</p>&#13;
<p class="figcap"><em>Listing 14-8: Building the per class labels</em></p>&#13;
<p class="indent">This code makes use of the command line. To call it, use something like</p>&#13;
<pre>$ <span class="codestrong1">python3 make_label_files.py 1 train_1.npy test_1.npy</span></pre>&#13;
<p class="indent">The first argument is the desired target class label, here 1 for automobiles, and the next two arguments are the names in which to store the new <span epub:type="pagebreak" id="page_358"/>label assignments for the train and test images. The code itself loops over the actual train and test labels, and if the label is the target class, the corresponding output label is 1; otherwise, it is 0.</p>&#13;
<p class="indent">This code is more flexible than mapping a single class. By using <code>eval</code> <span class="ent">❶</span>, we can pass in a comma-separated string of all the CIFAR-10 labels we want to treat as the target class. For example, to use this code to make labels for the animal versus vehicle example of the previous section, we’d make the first argument <code>2,3,4,5,6,7</code>.</p>&#13;
<p class="indent">Once we have new labels for each of the 10 classes, we can use them to train 10 models. All we need do is change <code>num_classes</code> to 2 and load each of the respective reassigned label files for <code>y_train</code> and <code>y_test</code>. At the bottom of the file, we need to change the call to <code>model.save</code> to store the per class models as well. We’ll assume the models are in files named <em>cifar10_cnn_&lt;X&gt;_model.h5</em> where <em>&lt;X&gt;</em> is a digit, 0–9, representing a CIFAR-10 class label. Our multiclass model is the shallow architecture trained on the full CIFAR-10 dataset for 12 epochs (<em>cifar10_cnn_model.h5</em>). To train the binary models, use the <code>train_single_models</code> script. This script calls <em>cifar10_cnn_arbitrary.py</em> to train a model using a specified binary dataset.</p>&#13;
<p class="indent">To test the models, we need to first load them all from disk along with the test set data. Then we need to run all the data through the multiclass model and each of the individual class models keeping the predictions. From the predictions, we can assign class labels and build confusion matrices to see how well each approach does. First, let’s load the test set and the models:</p>&#13;
<pre>x_test = np.load("cifar10_test_images.npy")/255.0<br/>&#13;
y_test = np.load("cifar10_test_labels.npy")<br/>&#13;
mm = load_model("cifar10_cnn_model.h5")<br/>&#13;
m = []<br/>&#13;
for i in range(10):<br/>&#13;
    m.append(load_model("cifar10_cnn_%d_model.h5" % i))</pre>&#13;
<p class="indent">Notice that we are scaling the test set by 255, as we did with the training data. We’ll keep the multiclass model in <code>mm</code> and load the 10 single class models into the list, <code>m</code>.</p>&#13;
<p class="indent">Next, we apply the models to each test set sample:</p>&#13;
<pre>mp = np.argmax(mm.predict(x_test), axis=1)<br/>&#13;
p = np.zeros((10,10000), dtype="float32")<br/>&#13;
for i in range(10):<br/>&#13;
    p[i,:] = m[i].predict(x_test)[:,1]<br/>&#13;
bp = np.argmax(p, axis=0)</pre>&#13;
<p class="indent">Calling <code>predict</code> with the 10,000 test samples returns a 10,000 × 10 matrix for the multiclass model or 10,000 × 2 for the individual models. Each row corresponds to a test sample, and each column is the model’s output for each class. For the multiclass case, we set <code>mp</code> to the maximum value across the columns (<code>axis=1</code>) to get a vector of 10,000 values, each of which is the predicted class label.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_359"/>We loop over the individual models and call <code>predict</code>, keeping only the class 1 probabilities. These are placed into <code>p</code>, where the rows are the individual model outputs for that class label, and the columns are the specific class 1 prediction probabilities for each of the 10,000 test samples. If we return the maximum value across the rows by using <code>argmax</code> and <code>axis=0</code>, we’ll get the class label of the model that had the highest predicted probability for each test sample. This is what is in <code>bp</code>.</p>&#13;
<p class="indent">With our predictions in hand, we can generate the confusion matrices:</p>&#13;
<pre>cm = np.zeros((10,10), dtype="uint16")<br/>&#13;
cb = np.zeros((10,10), dtype="uint16")<br/>&#13;
<br/>&#13;
for i in range(10000):<br/>&#13;
    cm[y_test[i],mp[i]] += 1<br/>&#13;
    cb[y_test[i],bp[i]] += 1<br/>&#13;
<br/>&#13;
np.save("cifar10_multiclass_conf_mat.npy", cm)<br/>&#13;
np.save("cifar10_binary_conf_mat.npy", cb)</pre>&#13;
<p class="indent">Here rows represent the true class label, and columns represent the model’s predicted label. We also store the confusion matrices for future use.</p>&#13;
<p class="indent">We can display the confusion matrices with the code in <a href="ch14.xhtml#ch14lis9">Listing 14-9</a>:</p>&#13;
<p class="programs" id="ch14lis9">print("One-vs-rest confusion matrix (rows true, cols predicted):")<br/>&#13;
print("%s" % np.array2string(100*(cb/1000.0), precision=1))<br/>&#13;
print()<br/>&#13;
print("Multiclass confusion matrix:")<br/>&#13;
print("%s"  % np.array2string(100*(cm/1000.0), precision=1))</p>&#13;
<p class="figcap"><em>Listing 14-9: Displaying the confusion matrices. See</em> cifar10_one_vs_many.py</p>&#13;
<p class="indent">We divide the counts in <code>cb</code> and <code>cm</code> by 1,000 because each class is represented by that many samples in the test set. This converts the confusion matrix entries to a fraction and then a percent when multiplied by 100.</p>&#13;
<p class="indent">So, how did we do? The multiple one-vs-rest classifiers produced</p>&#13;
<table class="borderb">&#13;
<colgroup>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>Class</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>75.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>84.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">11.2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>54.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>52.1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">12.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>67.6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">16.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>61.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>86.4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>71.5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.1</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>79.1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>91.2</strong></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_360"/>And the multiclass classifier came up with</p>&#13;
<table class="borderb">&#13;
<colgroup>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>Class</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>70.2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>79.4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10.8</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>56.2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">13.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>57.7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">10.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">11.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>77.4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">20.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>56.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>82.4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>71.7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>82.6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>80.3</strong></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table><p class="indent">The diagonals are the correct class assignments. Ideally, the matrix would be only diagonal elements. All other elements are mistakes, cases where the model or models chose the wrong label. Since each class is equally represented in the test set, we can calculate an overall accuracy for both models by using the unweighted average of the diagonals. If we do this, we get the following:</p>&#13;
<p class="center">one-vs-rest: 72.3%<br/>multiclass: 71.5%</p>&#13;
<p class="noindent">The one-vs-rest classifiers have the slight edge in this case, though the difference is less than 1 percent. Of course, we needed to do 10 times the work to get the one-vs-rest confusion matrix—ten classifiers were used instead of just one. The multiclass model was about 10 percent better on class 4 (deer) than the one-vs-rest models, but it was approximately 11 percent worse on class 9 (trucks). These are the two most substantial per class differences in accuracy. The multiclass model is confusing trucks with class 8, ships (3.2 percent), and class 1, cars (6.1 percent), more often than the one-vs-rest models. We can see how this might happen. Trucks and cars have wheels, and trucks and ships are (especially at the low resolution of CIFAR-10) both box-like.</p>&#13;
<p class="indent">Did we arrive at a definitive answer regarding one-vs-rest or multiclass models? No, nor could we in general. However, we did, objectively, get slightly better performance by using the multiple models.</p>&#13;
<p class="indent">One argument given against using multiple models, besides the extra computation necessary, is that using a single model for multiple classes provides the model with the opportunity to see examples that are similar to a particular class but are not instances of that class. These hard negatives serve to regularize the model by forcing it to (indirectly) pay attention to features that are dissimilar between classes instead of features that might be strongly associated with a class but are also present in other classes. We first encountered hard negatives in <a href="ch04.xhtml#ch04">Chapter 4</a>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_361"/>However, in this case, it’s difficult to say that the argument holds. For the multiclass model, class 9 (trucks) was more likely to be confused with class 1 (car, 6.1 percent) than one-vs-rest models (4.0 percent). One possible explanation might be that the multiclass model was forced, with limited training data, to try to learn the difference between trucks, cars, and other vehicles, while the one-vs-rest models were, individually, trying to learn only the difference between a truck and any other vehicle.</p>&#13;
<h3 class="h3" id="lev1_98">Transfer Learning</h3>&#13;
<p class="noindent">We’ll use the term <em>transfer learning</em> to refer to taking a pretrained deep network and using it to produce new features for another machine learning model. Our transfer learning example will be a toy model meant to show the process, but many models have been built using features generated by large pretrained networks that used huge datasets. In particular, many models have been built using features generated by AlexNet and the various ResNet architectures, which were pretrained on the ImageNet dataset.</p>&#13;
<p class="indent">We’ll use the pretrained model to turn input images into output feature vectors, which we’ll then use to train classical machine learning models. When a model is used to turn an input into another feature representation, typically a new feature vector, the output is often called an <em>embedding</em>: we are using the pretrained network to embed the inputs we want to classify into another space—one that we hope will let us build a useful model. We can use transfer learning when the model we want to develop has too few training examples to make a good model on its own.</p>&#13;
<p class="indent">When using transfer learning, it is helpful to know or believe that both models were trained using similar data. If you read the literature, you’ll find that this is true for many of the typical transfer learning examples. The inputs are natural images of some class, and the embedding models were trained on natural images. By natural image, I mean a photograph of something in the world as opposed to an x-ray or other medical image. Clearly, the CIFAR-10 images and the MNIST images are quite different from each other, so we shouldn’t hope for too much success with transfer learning. We’re using what we have on hand to demonstrate the technique.</p>&#13;
<p class="indent">We’ll use a shallow CIFAR-10 model like the ones we just saw to generate the embedding vectors. This model was trained on the full CIFAR-10 dataset for 12 epochs. We’ll embed the MNIST dataset by passing the MNIST digit images through the pretrained model, keeping the output of the Dense layer, the 128-node vectors used to generate the 10-class softmax predictions.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_362"/>We need to consider a few things before making the embedding. First, the CIFAR-10 model was trained on 32 × 32 RGB images. Therefore, we need to make the MNIST digit images fit this input expectation. Second, even though there are 10 classes for both CIFAR-10 and MNIST, this is only a coincidence; in practice, the number of classes between the two datasets do not need to match.</p>&#13;
<p class="indent">How should we get the 28 × 28 MNIST images into a model that’s expecting 32 × 32 RGB images? Typically, when working with image data and transfer learning, we’ll resize the images to make them fit. Here, since the MNIST digits are smaller than the CIFAR-10 images, we can center the 28 × 28 digit image in the middle of a 32 × 32 input. Moreover, we can turn a grayscale image into an RGB image by setting each channel (red, green, and blue) to the single grayscale input.</p>&#13;
<p class="indent">The code for all of the following is in <em>transfer_learning.py</em>. Setting up the embedding process looks like this:</p>&#13;
<pre>import numpy as np<br/>&#13;
from keras.models import load_model<br/>&#13;
from keras import backend as K<br/>&#13;
from keras.datasets import mnist<br/>&#13;
<br/>&#13;
(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>&#13;
x_train = x_train/255.0<br/>&#13;
x_test = x_test/255.0<br/>&#13;
model = load_model("cifar10_cnn_model.h5")</pre>&#13;
<p class="indent">We first load the modules we need from Keras. Then we load the Keras model file, <em>cifar10_cnn_model.h5</em>, which contains a shallow model from the first section of this chapter trained for 12 epochs on the full CIFAR-10 dataset.</p>&#13;
<p class="indent">Once we have the data loaded and scaled, we can pass each MNIST train and test image through the Keras model and extract the 128-node vector from the Dense layer. This turns each MNIST image into a 128-element vector; see <a href="ch14.xhtml#ch14lis10">Listing 14-10</a>.</p>&#13;
<p class="programs" id="ch14lis10">train = np.zeros((60000,128))<br/>&#13;
k = 0<br/>&#13;
for i in range(600):<br/>&#13;
    t = np.zeros((100,32,32,3))<br/>&#13;
 <span class="ent">❶</span> t[:,2:30,2:30,0] = x_train[k:(k+100)]<br/>&#13;
    t[:,2:30,2:30,1] = x_train[k:(k+100)]<br/>&#13;
    t[:,2:30,2:30,2] = x_train[k:(k+100)]<br/>&#13;
    _ = model.predict(t)<br/>&#13;
 <span class="ent">❷</span> out = [model.layers[5].output]<br/>&#13;
    func = K.function([model.input, K.learning_phase()], out)<br/>&#13;
    (*\newpage*)<br/>&#13;
<span epub:type="pagebreak" id="page_363"/>    train[k:(k+100),:] = func([t, 1.])[0]<br/>&#13;
    k += 100<br/>&#13;
np.save("mnist_train_embedded.npy", train)<br/>&#13;
<br/>&#13;
test = np.zeros((10000,128))<br/>&#13;
k = 0<br/>&#13;
for i in range(100):<br/>&#13;
    t = np.zeros((100,32,32,3))<br/>&#13;
    t[:,2:30,2:30,0] = x_test[k:(k+100)]<br/>&#13;
    t[:,2:30,2:30,1] = x_test[k:(k+100)]<br/>&#13;
    t[:,2:30,2:30,2] = x_test[k:(k+100)]<br/>&#13;
    _ = model.predict(t)<br/>&#13;
    out = [model.layers[5].output]<br/>&#13;
    func = K.function([model.input, K.learning_phase()], out)<br/>&#13;
    test[k:(k+100),:] = func([t, 1.])[0]<br/>&#13;
    k += 100<br/>&#13;
np.save("mnist_test_embedded.npy", test)</p>&#13;
<p class="figcap"><em>Listing 14-10: Running the MNIST images through the pretrained CIFAR-10 model</em></p>&#13;
<p class="indent">There are 60,000 MNIST training images. We pass each through the Keras model in blocks of 100 to be more efficient than processing each image individually; this means we need to process 600 sets of 100. We do the same for the test images, of which there are 10,000, so we process 100 sets of 100. We’ll store the output vectors in <code>train</code> and <code>test</code>.</p>&#13;
<p class="indent">The processing loop for both the train and test images first creates a temporary array, <code>t</code>, to hold the current set of 100 images. To use the Keras model <code>predict</code> method, we need a four-dimensional input: the number of images, height, width, and number of channels. We load <code>t</code> by copying the current set of 100 train or test images, indexed by <code>k</code>, to <code>t</code>; we do that three times, once for each channel <span class="ent">❶</span>. With <code>t</code> loaded, we call the <code>predict</code> method of the model. We throw the output away, since we’re after the values output by the Dense layer of the Keras model. This is layer 5 for the shallow architecture <span class="ent">❷</span>. The output of <code>func</code> is the 100 output vectors of the Dense layer we get after passing the inputs through the network. We assign these to the current block of 100 in <code>train</code> and move to the next set of 100. When we’ve processed the entire MNIST dataset, we keep the embedded vectors in a NumPy file. Then we repeat every step we used to process the training set for the test set.</p>&#13;
<p class="indent">At this point, we have our embedded vectors, so it’s natural to ask whether or not the embedding is helping separate the classes. We can see if this is true by using a t-SNE plot of the vectors by class label (<a href="ch14.xhtml#ch14fig4">Figure 14-4</a>).</p>&#13;
<div class="image" id="ch14fig4"><span epub:type="pagebreak" id="page_364"/><img src="Images/14fig04.jpg" alt="image" width="675" height="505"/></div>&#13;
<p class="figcap"><em>Figure 14-4: t-SNE plot showing the separation by class for the embedded MNIST digit vectors</em></p>&#13;
<p class="indent">Compare this figure with <a href="ch12.xhtml#ch12fig10">Figure 12-10</a>, which shows the separation for a model trained explicitly on MNIST digits. That model shows a clear, unambiguous separation of the classes, but <a href="ch14.xhtml#ch14fig4">Figure 14-4</a> is far less clear. However, even though there is overlap, there are concentrations of the classes in different parts of the plot, so we have some reason to hope that a model might be able to learn how to classify digits using these vectors.</p>&#13;
<p class="indent">Let’s train some models using the embedded vectors. For this, we’ll head back into the world of classical machine learning. We’ll train some of the models we trained in <a href="ch07.xhtml#ch07">Chapter 7</a> by using the vector form of the MNIST digits images.</p>&#13;
<p class="indent">The code to train and test the models is straightforward. We’ll train a Nearest Centroid, 3-Nearest Neighbor, Random Forest with 50 trees, and a linear SVM with <em>C</em> = 0.1, as shown in <a href="ch14.xhtml#ch14lis11">Listing 14-11</a>.</p>&#13;
<p class="programs" id="ch14lis11">from sklearn.neighbors import KNeighborsClassifier<br/>&#13;
from sklearn.ensemble import RandomForestClassifier<br/>&#13;
from sklearn.neighbors import NearestCentroid<br/>&#13;
from sklearn.svm import LinearSVC<br/>&#13;
<br/>&#13;
clf0 = NearestCentroid()<br/>&#13;
clf0.fit(train, y_train)<br/>&#13;
nscore = clf0.score(test, y_test)<br/>&#13;
<br/>&#13;
clf1 = KNeighborsClassifier(n_neighbors=3)<br/>&#13;
clf1.fit(train, y_train)<br/>&#13;
<span epub:type="pagebreak" id="page_365"/>kscore = clf1.score(test, y_test)<br/>&#13;
<br/>&#13;
clf2 = RandomForestClassifier(n_estimators=50)<br/>&#13;
clf2.fit(train, y_train)<br/>&#13;
rscore = clf2.score(test, y_test)<br/>&#13;
<br/>&#13;
clf3 = LinearSVC(C=0.1)<br/>&#13;
clf3.fit(train, y_train)<br/>&#13;
sscore = clf3.score(test, y_test)<br/>&#13;
<br/>&#13;
print("Nearest Centroid    : %0.2f" % nscore)<br/>&#13;
print("3-NN                : %0.2f" % kscore)<br/>&#13;
print("Random Forest       : %0.2f" % rscore)<br/>&#13;
print("SVM                 : %0.2f" % sscore)</p>&#13;
<p class="figcap"><em>Listing 14-11: Training classical models using the MNIST embedded vectors</em></p>&#13;
<p class="indent">We load the relevant sklearn modules, create the specific model instances, and call <code>fit</code>, passing in the 128-element training vectors and the associated class labels. The <code>score</code> method returns the overall accuracy of the now trained model on the test set.</p>&#13;
<p class="indent">Running this code gives us scores of</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Score</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6799</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-Nearest Neighbors</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9010</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8837</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">SVM (C=0.1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8983</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">which we can compare to the scaled scores for same models in <a href="ch07.xhtml#ch7tab10">Table 7-10</a>:</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Score</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Nearest Centroid</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8203</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3-Nearest Neighbors</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9705</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Random Forest (50)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9661</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">SVM (C =0.1)</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9181</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Clearly, in this case, our embedding is not giving us a head start over the raw data. We should not be surprised by this: we knew our two datasets were fairly distinct, and the t-SNE plot showed that the pretrained CIFAR-10 model was not ideally suited to separating the MNIST images in the embedding space. The poor separation of the classes in <a href="ch14.xhtml#ch14fig4">Figure 14-4</a> explains the poor performance of the Nearest Centroid model: 68 percent accuracy versus 82 percent when trained on the digit images themselves. Moreover, by their very nature, digit images are already distinct from each other, especially on a uniform background, since the digits were intended by humans to be easily distinguished by sight.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_366"/>A bit of code gives us the confusion matrix for any of these models:</p>&#13;
<pre>def conf_mat(clf,x,y):<br/>&#13;
    p = clf.predict(x)<br/>&#13;
    c = np.zeros((10,10))<br/>&#13;
    for i in range(p.shape[0]):<br/>&#13;
        c[y[i],p[i]] += 1<br/>&#13;
    return c<br/>&#13;
cs = conf_mat(clf, test, y_test)<br/>&#13;
cs = 100.0*cs / cs.sum(axis=1)<br/>&#13;
np.set_printoptions(suppress=True)<br/>&#13;
print(np.array2string(cs, precision=1, floatmode="fixed"))</pre>&#13;
<p class="indent">Here <code>clf</code> is any of the models, <code>test</code> is the embedded test set, and <code>y_test</code> is the labels. We return the confusion matrix with counts in each element, so we divide by the sum of the rows, since the row represents the true label, and multiply by 100 to get percents. Then we print the array using NumPy commands to get a single digit of accuracy and no scientific notation.</p>&#13;
<p class="indent">We know already why the Nearest Centroid result is so poor. What about the Random Forest and SVM? The confusion matrix for the Random Forest model is shown here:</p>&#13;
<table class="borderb">&#13;
<colgroup>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>Class</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">96.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">98.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">87.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>80.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>6.7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>6.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">88.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>78.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">93.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">87.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.4</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">84.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top" class="borderr"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">86.2</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">We’ve highlighted the two lowest-performing classes, 3 and 5, along with the two digits they are most often confused with. We see that the model is confusing 3’s with 5’s and 8’s. The SVM confusion matrix shows the same effect. If we take <a href="ch14.xhtml#ch14fig4">Figure 14-4</a> and show only classes 3, 5, and 8, then we get <a href="ch14.xhtml#ch14fig5">Figure 14-5</a>. Considerable mixing between the classes is plain to see.</p>&#13;
<div class="image" id="ch14fig5"><span epub:type="pagebreak" id="page_367"/><img src="Images/14fig05.jpg" alt="image" width="675" height="505"/></div>&#13;
<p class="figcap"><em>Figure 14-5: t-SNE plot showing class 3 (plus), class 5 (cross), and class 8 (triangle right)</em></p>&#13;
<p class="indent">The purpose of this section was to introduce the idea of transfer learning through an example that used the datasets we had on hand. As you can see, this experiment was not a success. The datasets we used were very different from each other, so we might have expected this to be the case, but it was useful to verify for ourselves. In the next section, we’ll see how we can go one step beyond transfer learning.</p>&#13;
<h3 class="h3" id="lev1_99">Fine-Tuning a Model</h3>&#13;
<p class="noindent">In the previous section, we defined transfer learning as using weights from a model trained on one dataset with data from a (hopefully very similar) dataset. We used the weights to map the inputs to a new space and trained models on the mapped data. In this section, we’ll do something similar, but instead of leaving the weights as they are, we’ll let the weights vary while we continue training the model with a new, smaller dataset. We are calling this <em>fine-tuning</em>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_368"/>In fine-tuning, we are training a neural network, but instead of initializing the weights to random values, selected according to an intelligent initialization scheme, we start with the weights from a model trained on a similar but different dataset. We might use fine-tuning when we do not have a lot of training data, but we believe our data comes from a distribution that’s very similar to one for which we have either a lot of data or a trained model. For example, we might have access to the weights of a large model trained with a large dataset, like the ImageNet dataset we’ve mentioned previously. It is quite simple to download such a pretrained model. Additionally, we might have a small dataset of images for classes that are not in ImageNet; say, photographs of guppies, angelfish, and tetras. These are popular freshwater aquarium fish not in ImageNet. We can start with a larger model pretrained on ImageNet and fine-tune using the smaller fish dataset. That way, we can take advantage of the fact that the model is already well adapted to inputs of this kind and, hopefully, get a good model with a small dataset.</p>&#13;
<p class="indent">Our experiment will use CIFAR-10. Our goal is to train a model to differentiate between images of dogs and cats using the deep architecture from the first section of this chapter. However, our dataset is small; we have approximately 500 images of each class to work with. We also have a larger dataset, all the vehicles from CIFAR-10.</p>&#13;
<p class="indent">Therefore, we’ll train the following models with this data:</p>&#13;
<ol>&#13;
<li class="noindent">The shallow architecture using the small dog and cat dataset.</li>&#13;
<li class="noindent">The deep architecture using the small dog and cat dataset.</li>&#13;
<li class="noindent">The deep architecture pretrained on the vehicle data and fine-tuned on the small dog and cat dataset.</li>&#13;
</ol>&#13;
<p class="indent">For the last case, we’ll train several variations using different combinations of frozen weights.</p>&#13;
<h4 class="h4" id="lev2_134">Building Our Datasets</h4>&#13;
<p class="noindent">Before we get to fine-tuning, we need to build our datasets. We’ll use unaugmented CIFAR-10 to construct the small dog and cat dataset. We’ll use <em>augmented</em> CIFAR-10 to construct the vehicle dataset. We augmented CIFAR-10 in <a href="ch05.xhtml#ch05">Chapter 5</a>.</p>&#13;
<p class="indent">Building the small dog and cat dataset is straightforward, as shown in <a href="ch14.xhtml#ch14lis12">Listing 14-12</a>.</p>&#13;
<p class="programs" id="ch14lis12">x_train = np.load("cifar10_train_images.npy")[:,2:30,2:30,:]<br/>&#13;
y_train = np.load("cifar10_train_labels.npy")<br/>&#13;
x_test = np.load("cifar10_test_images.npy")[:,2:30,2:30,:]<br/>&#13;
y_test = np.load("cifar10_test_labels.npy")<br/>&#13;
xtrn = []; ytrn = []<br/>&#13;
xtst = []; ytst = []<br/>&#13;
<br/>&#13;
for i in range(y_train.shape[0]):<br/>&#13;
    if (y_train[i]==3):<br/>&#13;
<span epub:type="pagebreak" id="page_369"/>        xtrn.append(x_train[i])<br/>&#13;
        ytrn.append(0)<br/>&#13;
    if (y_train[i]==5):<br/>&#13;
        xtrn.append(x_train[i])<br/>&#13;
        ytrn.append(1)<br/>&#13;
for i in range(y_test.shape[0]):<br/>&#13;
    if (y_test[i]==3):<br/>&#13;
        xtst.append(x_test[i])<br/>&#13;
        ytst.append(0)<br/>&#13;
    if (y_test[i]==5):<br/>&#13;
        xtst.append(x_test[i])<br/>&#13;
        ytst.append(1)<br/>&#13;
<br/>&#13;
np.save("cifar10_train_cat_dog_small_images.npy", np.array(xtrn)[:1000])<br/>&#13;
np.save("cifar10_train_cat_dog_small_labels.npy", np.array(ytrn)[:1000])<br/>&#13;
np.save("cifar10_test_cat_dog_small_images.npy", np.array(xtst)[:1000])<br/>&#13;
np.save("cifar10_test_cat_dog_small_labels.npy", np.array(ytst)[:1000])</p>&#13;
<p class="figcap"><em>Listing 14-12: Building the small dog and cat dataset</em></p>&#13;
<p class="indent">We load the full CIFAR-10 data, train and test, and then loop over each sample. If the class is 3, cat, or 5, dog, we add the image and label to our lists, making sure to recode the class label so that 0 is cat and 1 is dog. When all the samples have been added, we keep the first 1,000 and write them to disk to be our small dog and cat training and test sets. Keeping the first 1,000 samples gives us a dataset that is close to split 50/50 between classes.</p>&#13;
<p class="indent">Notice that immediately after loading the CIFAR-10 images, we subscript them with <code>[:,2:30,2:30,:]</code>. Recall, the augmented version of the dataset includes small shifts of the image, so when we built it in <a href="ch05.xhtml#ch05">Chapter 5</a>, we reduced the size from 32 × 32 to 28 × 8. Therefore, when we build our vehicle dataset, we’ll be working with images that are 28×28 pixels. The subscript extracts the center 28 × 28 region of each image. The first dimension is the number of images in the train or test set. The last dimension is the number of channels—three since these are RGB images.</p>&#13;
<p class="indent">Building the vehicle dataset is equally straightforward (<a href="ch14.xhtml#ch14lis13">Listing 14-13</a>).</p>&#13;
<p class="programs" id="ch14lis13">x_train = np.load("cifar10_aug_train_images.npy")<br/>&#13;
y_train = np.load("cifar10_aug_train_labels.npy")<br/>&#13;
x_test = np.load("cifar10_aug_test_images.npy")<br/>&#13;
y_test = np.load("cifar10_test_labels.npy")<br/>&#13;
<br/>&#13;
vehicles= [0,1,8,9]<br/>&#13;
xv_train = []; xv_test = []<br/>&#13;
yv_train = []; yv_test = []<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_370"/>for i in range(y_train.shape[0]):<br/>&#13;
    if (y_train[i] in vehicles):<br/>&#13;
        xv_train.append(x_train[i])<br/>&#13;
        yv_train.append(vehicles.index(y_train[i]))<br/>&#13;
for i in range(y_test.shape[0]):<br/>&#13;
    if (y_test[i] in vehicles):<br/>&#13;
        xv_test.append(x_test[i])<br/>&#13;
        yv_test.append(vehicles.index(y_test[i]))<br/>&#13;
<br/>&#13;
np.save("cifar10_train_vehicles_images.npy", np.array(xv_train))<br/>&#13;
np.save("cifar10_train_vehicles_labels.npy", np.array(yv_train))<br/>&#13;
np.save("cifar10_test_vehicles_images.npy", np.array(xv_test))<br/>&#13;
np.save("cifar10_test_vehicles_labels.npy", np.array(yv_test))</p>&#13;
<p class="figcap"><em>Listing 14-13: Building the vehicle dataset</em></p>&#13;
<p class="indent">Here we work with the augmented versions. The augmented test set is 28×28 pixels per image using the central region of the original test set. Also, as we loop through the train and test sets looking for samples that are in one of the vehicle classes, we can do our recoding of the class label by asking for the index into the vehicles list of the element matching the current sample’s class label, hence using <code>index</code> on <code>vehicles</code>. The vehicle dataset has 200,000 samples in the training set, 50,000 from each of the four classes.</p>&#13;
<p class="indent">To proceed then, we need to (1) train the deep model on the vehicle dataset; (2) adapt the model to the dog and cat dataset; and (3) train the deep model initialized with the weights from the vehicle model. We’ll also train the shallow and deep models from scratch, using the dog and cat dataset for comparison purposes.</p>&#13;
<p class="indent">We gave the code for the deep model in the first section of this chapter so we won’t reproduce it here. In particular, see <a href="ch14.xhtml#ch14lis3">Listing 14-3</a>. The code itself is in the file <em>cifar10_cnn_vehicles.py</em>. The relevant changes for the vehicle model are shown here:</p>&#13;
<pre>batch_size = 64<br/>&#13;
num_classes = 4<br/>&#13;
epochs = 12<br/>&#13;
img_rows, img_cols = 28,28<br/>&#13;
<br/>&#13;
x_train = np.load("cifar10_train_vehicles_images.npy")<br/>&#13;
y_train = np.load("cifar10_train_vehicles_labels.npy")<br/>&#13;
x_test = np.load("cifar10_test_vehicles_images.npy")<br/>&#13;
y_test = np.load("cifar10_test_vehicles_labels.npy")</pre>&#13;
<p class="indent">We use a minibatch size of 64. There are four classes (airplane, automobile, ship, truck), and we’ll train for 12 epochs. When we’re done training, we’ll store the model in <em>cifar10_cnn_vehicles_model.h5</em> so we can use its weights and biases for fine-tuning the dog and cat model. Training this model takes several hours on our CPU system. The final test accuracy is 88.2 percent, so it is performing well enough for our purposes.</p>&#13;
<h4 class="h4" id="lev2_135"><span epub:type="pagebreak" id="page_371"/>Adapting Our Model for Fine-Tuning</h4>&#13;
<p class="noindent">Now we need to adapt the vehicle model for the dog and cat dataset and fine-tuning. Specifically, we need to replace the top softmax layer that expects four classes with one that expects two. We also need to decide which layer’s weights we’ll freeze and which we’ll update during training. This step is essential, and we’ll see how our choices affect the fine-tuning results.</p>&#13;
<p class="indent">When fine-tuning, it’s standard practice to freeze lower-level weights; they are not updated at all when training. The idea here is that if our new data is similar to the data used for the pretraining step, the lower levels of the model are already adapted, and we should not change them. We allow only the higher-level layers to change as these are the ones that need to learn about the representation of the new data. Which layers we freeze and which we allow to train depends on the size of the model and the data itself. Experimentation is required. Note that the transfer learning of the previous section can be considered fine-tuning with all the weights frozen.</p>&#13;
<p class="indent">Note that if we’re using SGD with fine-tuning, we typically reduce the learning rate by a factor of, say, 10. The rationale is the same as for freezing the lower-level weights: the model is already “close” to a desired minimum of the error function, so we don’t need big steps to find it. Our experiment will use Adadelta, which will adjust the learning rate step size for us.</p>&#13;
<p class="indent">The deep model has multiple convolutional layers. We’ll experiment with freezing the first two; these are the lowest and are most likely already tuned to the low-level features of the CIFAR-10 dataset, at least the vehicles. Of course, since our dog and cat images come from CIFAR-10 as well, we know that they are from the same parent distribution or domain as the vehicle images. We’ll also experiment with a model that freezes all the convolutional layers and allows only the dense layers to be adapted during training. Doing this is reminiscent of transfer learning, though we’ll allow the dense layers to update their weights.</p>&#13;
<p class="indent">Let’s create the code for fine-tuning by using the vehicle model that we trained earlier (<a href="ch14.xhtml#ch14lis14">Listing 14-14</a>).</p>&#13;
<p class="programs" id="ch14lis14">import keras<br/>&#13;
from keras.models import load_model<br/>&#13;
from keras.layers import Dense<br/>&#13;
from keras import backend as K<br/>&#13;
import numpy as np<br/>&#13;
<br/>&#13;
batch_size = 64<br/>&#13;
num_classes = 2<br/>&#13;
epochs = 36<br/>&#13;
img_rows, img_cols = 28,28<br/>&#13;
<br/>&#13;
x_train = np.load("cifar10_train_cat_dog_small_images.npy")<br/>&#13;
<span epub:type="pagebreak" id="page_372"/>y_train = np.load("cifar10_train_cat_dog_small_labels.npy")<br/>&#13;
x_test = np.load("cifar10_test_cat_dog_small_images.npy")<br/>&#13;
y_test = np.load("cifar10_test_cat_dog_small_labels.npy")<br/>&#13;
<br/>&#13;
if K.image_data_format() == 'channels_first':<br/>&#13;
    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)<br/>&#13;
    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)<br/>&#13;
    input_shape = (3, img_rows, img_cols)<br/>&#13;
else:<br/>&#13;
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)<br/>&#13;
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)<br/>&#13;
    input_shape = (img_rows, img_cols, 3)<br/>&#13;
<br/>&#13;
x_train = x_train.astype('float32')<br/>&#13;
x_test = x_test.astype('float32')<br/>&#13;
x_train /= 255<br/>&#13;
x_test /= 255<br/>&#13;
<br/>&#13;
y_train = keras.utils.to_categorical(y_train, num_classes)<br/>&#13;
y_test = keras.utils.to_categorical(y_test, num_classes)</p>&#13;
<p class="figcap"><em>Listing 14-14: Fine-tuning the vehicle model. See</em> cifar10_cnn_cat_dog_fine_tune_3.py.</p>&#13;
<p class="indent">These lines should be familiar by now. First we load and preprocess the small dog and cat dataset. Note that we are using a minibatch size of 64, two classes (0 = cat, 1 = dog), and 36 epochs.</p>&#13;
<p class="indent">Next, we need to load the vehicle model, strip off its top layer, and replace it with a two-class softmax (<a href="ch14.xhtml#ch14lis15">Listing 14-15</a>). This is also where we will freeze some combination of the first two convolutional layers.</p>&#13;
<p class="programs" id="ch14lis15">   model = load_model("cifar10_cnn_vehicles_model.h5")<br/>&#13;
<br/>&#13;
<span class="ent">❶</span> model.layers.pop()<br/>&#13;
<span class="ent">❷</span> model.outputs = [model.layers[-1].output]<br/>&#13;
   model.layers[-1].outbound_nodes = []<br/>&#13;
<span class="ent">❸</span> model.add(Dense(num_classes, name="softmax", activation='softmax'))<br/>&#13;
<br/>&#13;
<span class="ent">❹</span> model.layers[0].trainable = False<br/>&#13;
   model.layers[1].trainable = False<br/>&#13;
<br/>&#13;
   model.compile(loss=keras.losses.categorical_crossentropy,<br/>&#13;
                 optimizer=keras.optimizers.Adadelta(),<br/>&#13;
                 metrics=['accuracy'])</p>&#13;
<p class="figcap"><em>Listing 14-15: Adjusting the vehicle model for dogs and cats</em></p>&#13;
<p class="indent">After we load the model, we use Keras to remove the top layer <span class="ent">❶</span>. We need to patch the model to make the next-to-top layer look like the top layer; this allows the <code>add</code> method to work correctly <span class="ent">❷</span>. Then, we add a new softmax layer for two classes <span class="ent">❸</span>. This example is set to freeze the weights of the first <span epub:type="pagebreak" id="page_373"/>two convolutional layers <span class="ent">❹</span>. We’ll test each possible combination involving the first two convolutional layers. Finally, we compile the updated model and specify the Adadelta optimizer.</p>&#13;
<p class="indent">We train the model by calling the <code>fit</code> method, as before as shown in <a href="ch14.xhtml#ch14lis16">Listing 14-16</a>.</p>&#13;
<p class="programs" id="ch14lis16">score = model.evaluate(x_test[100:], y_test[100:], verbose=0)<br/>&#13;
print('Initial test loss:', score[0])<br/>&#13;
print('Initial test accuracy:', score[1])<br/>&#13;
<br/>&#13;
history = model.fit(x_train, y_train,<br/>&#13;
          batch_size=batch_size,<br/>&#13;
          epochs=epochs,<br/>&#13;
          verbose=0,<br/>&#13;
          validation_data=(x_test[:100], y_test[:100]))<br/>&#13;
<br/>&#13;
score = model.evaluate(x_test[100:], y_test[100:], verbose=0)<br/>&#13;
print('Test loss:', score[0])<br/>&#13;
print('Test accuracy:', score[1])<br/>&#13;
<br/>&#13;
model.save("cifar10_cnn_cat_dog_fine_tune_3_model.h5")</p>&#13;
<p class="figcap"><em>Listing 14-16: Training and testing the dog and cat model</em></p>&#13;
<p class="indent">We are calling <code>evaluate</code> using the last 90 percent of the test data, <em>before</em> calling <code>fit</code>. This will give us an indication of how well the dog and cat model does when using the vehicle weights as they are. Then we call <code>fit</code> and <code>evaluate</code> a second time. Finally, we save the model and the training history. This model froze both of the first two convolutional layers. Other models will freeze or unfreeze these layers for the remaining three possibilities.</p>&#13;
<p class="indent">We mentioned earlier that we’d also train a model by freezing all of the convolutional layers. In essence, this is saying that we want to preserve whatever new representation the vehicle model learned and apply it directly to the dog and cat model , allowing only the top fully connected layers to adjust themselves. This is almost the same as the transfer learning approach of the previous section. To freeze all the convolutional layers, we replace the direct assignments to specific layers’ <code>trainable</code> property with a loop over all the layers:</p>&#13;
<pre>for i in range(5):<br/>&#13;
    model.layers[i].trainable = False</pre>&#13;
<h4 class="h4" id="lev2_136">Testing Our Model</h4>&#13;
<p class="noindent">Let’s run the fine-tuning tests. We’ll train each possible combination six times so we can get statistics on the mean accuracies. This accounts for the stochastic nature of the initialization process. Although we initialized the model with pretrained weights, we added a new top softmax layer with two <span epub:type="pagebreak" id="page_374"/>outputs. The output of the dense layer below it has 128 nodes, so each model needs to randomly initialize 128 × 2 + 2 = 258 weights and biases for the new layer. This is the source of the difference.</p>&#13;
<p class="indent">Without training, the initial model accuracy hovers around 50 to 51 percent, with each model slightly different because of the initialization we just mentioned. This is a two-class model, so this means that without any training, it is randomly guessing between dog and cat.</p>&#13;
<p class="indent">After we have trained all of the models and tallied all of the per model accuracies, we get <a href="ch14.xhtml#ch14tab2">Table 14-2</a>, where we present accuracy as mean ± standard error.</p>&#13;
<p class="tabcap" id="ch14tab2"><strong>Table 14-2:</strong> Dog and Cat Test Set Accuracies for the Shallow, Deep, and Fine-Tuned Deep Models</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
<col style="width:25%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Freeze Conv0</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Freeze Conv1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Accuracy (%)</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Shallow</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–</p></td>&#13;
<td style="vertical-align: top"><p class="tab">64.375 ± 0.388</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Deep</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–</p></td>&#13;
<td style="vertical-align: top"><p class="tab">61.142 ± 0.509</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Fine-tune 0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">False</p></td>&#13;
<td style="vertical-align: top"><p class="tab">False</p></td>&#13;
<td style="vertical-align: top"><p class="tab">62.683 ± 3.689</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Fine-tune 1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">True</p></td>&#13;
<td style="vertical-align: top"><p class="tab">False</p></td>&#13;
<td style="vertical-align: top"><p class="tab">69.142 ± 0.934</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Fine-tune 2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">False</p></td>&#13;
<td style="vertical-align: top"><p class="tab">True</p></td>&#13;
<td style="vertical-align: top"><p class="tab">68.842 ± 0.715</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">Fine-tune 3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">True</p></td>&#13;
<td style="vertical-align: top"><p class="tab">True</p></td>&#13;
<td style="vertical-align: top"><p class="tab">70.050 ± 0.297</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">Freeze all</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–</p></td>&#13;
<td style="vertical-align: top"><p class="tab">–</p></td>&#13;
<td style="vertical-align: top"><p class="tab">57.042 ± 0.518</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">What to make of these results? First, we see that training the deep architecture from scratch with the small dog and cat dataset is not particularly effective: only about 61 percent accurate. Training the shallow architecture from scratch does better, with an accuracy of around 64 percent. These are our baselines. Will fine-tuning a model trained on different data help? From looking at the fine-tune results, the answer is “yes,” but clearly, not all the fine-tuning options are equally effective: two are even worse than the best from-scratch result (“Fine-tune 0” and “Freeze all”). So, we do not want to freeze all the convolutional layers, nor do we want to be free to update all of them.</p>&#13;
<p class="indent">This leaves fine-tune models 1, 2, and 3 to consider. The “Fine-tune 3” model performed best, though the differences between these models are not statistically significant. Let’s go with freezing the first two convolutional layers, then. What might be happening to make this approach better than the other models? By freezing these lowest layers, we are fixing them and preventing them from being changed by training. These layers were trained on a much larger vehicle dataset that included standard augmentations like shifts and rotates. And, as we already saw in <a href="ch12.xhtml#ch12fig4">Figure 12-4</a>, the kernels learned by these lower layers are edge and texture detectors. They have been conditioned to learn about the sorts of structures present in CIFAR-10 images, and, since our dog and cat dataset is also from CIFAR-10, it is reasonable to believe that the same kernels will be useful with those images as well.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_375"/>However, when we froze all the convolutional layers of the deep architecture, we saw a significant decrease in performance. This implies that higher-level convolutional layers are not well-adapted to the dog and cat structures, which, again, makes perfect sense. Because of their effective receptive fields, higher layers are learning about larger structures in the input images; these are also the larger structures that distinguish dogs from cats. If we cannot modify these layers, there is no opportunity for them to be conditioned on the very things that we need them to learn.</p>&#13;
<p class="indent">This fine-tuning example shows the power of the technique when it is applicable. However, like most things in machine learning, there is only intuition as to why and when it’s successful. Recent work has shown that sometimes, fine-tuning a large model trained on a dataset that is not very close to the intended dataset can lead to performance that is no better than training a shallower model, provided enough data is present. For example, see “Transfusion: Understanding Transfer Learning for Medical Imaging” by Maithra Raghu et al. This paper uses transfer learning/fine-tuning between pretrained ImageNet models and medical images and shows that shallow models trained from scratch are often just as good.</p>&#13;
<h3 class="h3" id="lev1_100">Summary</h3>&#13;
<p class="noindent">This chapter explored convolutional neural networks applied to the CIFAR-10 dataset. We started by training two architectures, one shallow, the other deep, on the full dataset. We then asked whether or not we can train a model to distinguish between animals and vehicles. Next, we answered the question of whether or not a single multiclass model or multiple binary models performed better for CIFAR-10. After this, we introduced two fundamental techniques, transfer learning and fine-tuning, and showed how to implement them in Keras. These techniques should be understood and in your deep learning bag of tricks going forward.</p>&#13;
<p class="indent">In the next chapter, we’ll present a case study with a dataset we have not yet worked with. We’ll assume the role of data scientists tasked with making a model for this dataset and work our way through, from initial data processing to model exploration and final model construction.<span epub:type="pagebreak" id="page_376"/></p>&#13;
</div></body></html>