- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A/B Testing
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding chapter, we discussed the scientific practice of observing
    two groups and making quantitative judgments about how they relate to each other.
    But scientists (including data scientists) do more than just observe preexisting
    differences. A huge part of science consists of creating differences experimentally
    and then drawing conclusions. In this chapter, we’ll discuss how to conduct those
    kinds of experiments in business.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by discussing the need for experimentation and our motivations for
    testing. We’ll cover how to properly set up experiments, including the need for
    randomization. Next, we’ll detail the steps of A/B testing and the champion/challenger
    framework. We’ll conclude by describing nuances like the exploration/exploitation
    trade-off, as well as ethical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: The Need for Experimentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s return to the scenario we outlined in the second half of Chapter 3. Imagine
    that you’re running a computer company and maintain email marketing lists that
    your customers can choose to subscribe to. One email list is designed for customers
    who are interested in your desktop computers, and the other email list is for
    customers interested in your laptops. You can download two fabricated datasets
    for this scenario from [https://bradfordtuckfield.com/desktop.csv](https://bradfordtuckfield.com/desktop.csv)
    and [https://bradfordtuckfield.com/laptop.csv](https://bradfordtuckfield.com/laptop.csv).
    If you save them to the same directory you’re running Python from, you can read
    these hypothetical lists into Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can run `print(desktop.head())` and `print(laptop.head())` to see the first
    five rows of each dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Chapter 3, you learned how to use simple t-tests to detect differences between
    our datasets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we import the SciPy package’s `stats` module so we can use it for t-tests.
    Then we print the results of three separate t-tests: one comparing the spending
    of desktop and laptop subscribers, one comparing the ages of desktop and laptop
    subscribers, and one comparing the number of recorded website visits of desktop
    and laptop subscribers. We can see that the first *p*-value is less than 0.05,
    indicating that these groups are significantly different in their spending levels
    (at the 5 percent significance level), just as we concluded in Chapter 3.'
  prefs: []
  type: TYPE_NORMAL
- en: After determining that desktop subscribers are different from laptop subscribers,
    we can conclude that we should send them different marketing emails. However,
    this fact alone is not enough to completely guide our marketing strategy. Just
    knowing that our desktop subscriber group spends a little less than the laptop
    subscriber group doesn’t tell us whether crafting long messages or short ones
    would lead to better sales, or whether using red text or blue text would get us
    more clicks, or whether informal or formal language would improve customer loyalty
    most. In some cases, past research published in academic marketing journals can
    give us hints about what will work best. But even when relevant research exists,
    every company has its own unique set of customers that may not respond to marketing
    in exactly the same way that past research indicates.
  prefs: []
  type: TYPE_NORMAL
- en: We need a way to generate new data that’s never been collected or published
    before, so we can use that data to answer new questions about the new situations
    that we regularly face. Only if we can generate this kind of new data can we reliably
    learn about what will work best in our efforts to grow our business with our particular
    set of unique customers. We’ll spend the rest of this chapter discussing an approach
    that will accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: '*A/B testing*, the focus of this chapter, uses experiments to help businesses
    determine which practices will give them the greatest chances of success. It consists
    of a few steps: experimental design, random assignment into treatment and control
    groups, careful measurement of outcomes, and finally, statistical comparison of
    outcomes between groups.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The way we’ll do statistical comparisons will be familiar: we’ll use the t-tests
    introduced in the previous chapter. While t-tests are a part of the A/B testing
    process, they are not the only part. A/B testing is a process for collecting new
    data, which can then be analyzed using tests like the t-test. Since we’ve already
    introduced t-tests, we won’t focus on them in this chapter. Instead, we’ll focus
    on all the other steps of A/B testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Running Experiments to Test New Hypotheses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider just one hypothesis about our customers that might interest
    us. Suppose we’re interested in studying whether changing the color of text in
    our marketing emails from black to blue will increase the revenue we earn as a
    result of the emails. Let’s express two hypotheses related to this:'
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis 0 Changing the color of text in our emails from black to blue will
    have no effect on revenues.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis 1 Changing the color of text in our emails from black to blue will
    lead to a change in revenues (either an increase or a decrease).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the testing hypothesis framework covered in Chapter 3 to test our
    null hypothesis (Hypothesis 0) and decide whether we want to reject it in favor
    of its alternative hypothesis (Hypothesis 1). The only difference is that in Chapter
    3, we tested hypotheses related to data we had already collected. Here, our datasets
    do not include information about blue-text and black-text emails. So, extra steps
    are required before we perform hypothesis testing: designing an experiment, running
    an experiment, and collecting data related to the experiment’s results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running experiments may not sound so difficult, but some tricky parts are important
    to get exactly right. To do the hypothesis test we just outlined, we’ll need data
    from two groups: a group that has received a blue-text email and a group that
    has received a black-text email. We’ll need to know how much revenue we received
    from each member of the group that received the blue-text email and how much revenue
    we received from each member of the group that received the black-text email.'
  prefs: []
  type: TYPE_NORMAL
- en: After we have that, we can do a simple t-test to determine whether the revenue
    collected from the blue-text group differed significantly from the revenue collected
    from the black-text group. In this chapter, we’ll use a 5 percent significance
    level for all of our tests—that is, we’ll reject the null hypothesis and accept
    our alternative hypothesis if our *p*-value is less than 0.05\. When we do our
    t-test, if the revenues are significantly different, we can reject our null hypothesis
    (Hypothesis 0). Otherwise, we won’t reject our null hypothesis, and until something
    convinces us otherwise, we’ll accept its assertion that blue and black text lead
    to equal revenues.
  prefs: []
  type: TYPE_NORMAL
- en: We need to split our population of interest into two subgroups and send a blue-text
    email to one subgroup and a black-text email to our other subgroup so we can compare
    revenues from each group. For now, let’s focus on desktop subscribers only and
    split our desktop dataframe into two subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split a group into two subgroups in many ways. One possible choice is
    to split our dataset into a group of younger people and a group of older people.
    We might split our data this way because we believe that younger people and older
    people might be interested in different products, or we might do it this way just
    because age is one of the few variables that appears in our data. Later, we’ll
    see that this way of splitting our group into subgroups will lead to problems
    in our analysis, and we’ll discuss better ways to create subgroups. But since
    this method of splitting into subgroups is simple and easy, let’s start by trying
    it to see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we import the NumPy package, giving it the alias `np`, so we can use its
    `median()` method. Then we simply take the median age of our group of desktop
    subscribers and create `groupa`, a subset of our desktop subscribers whose age
    is below or equal to the median age, and `groupb`, a subset of our desktop subscribers
    whose age is above the median age.
  prefs: []
  type: TYPE_NORMAL
- en: After creating `groupa` and `groupb`, you can send these two dataframes to your
    marketing team members and instruct them to send different emails to each group.
    Suppose they send the black-text email to `groupa` and the blue-text email to
    `groupb`. In every email, they include links to new products they want to sell,
    and by tracking who clicks which links and their purchases, the team members can
    measure the total revenue earned from each individual email recipient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s read in some fabricated data that shows hypothetical outcomes for members
    of our two groups. This data can be downloaded from [https://bradfordtuckfield.com/emailresults1.csv](https://bradfordtuckfield.com/emailresults1.csv);
    store it in the same directory where you run Python. Then you can read it into
    your Python session as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run `print(emailresults1.head())` in Python, you can see the first rows
    of this new data. It’s a simple dataset: each row corresponds to one individual
    desktop email subscriber, whose ID is identified in the `userid` column. The `revenue`
    column records the revenue your company earned from each user as a result of this
    email campaign.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will be useful to have this new revenue information in the same dataframe
    as our other information about each user. Let’s join the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we use the pandas `merge()` method to combine our dataframes.
    We specify `on='userid'`, meaning that we take the row of `emailresults1` that
    corresponds to a particular `userid` and merge it with the row of `groupa` that
    corresponds to that same `userid`. The end result of using `merge()` is a dataframe
    in which every row corresponds to a particular user identified by their unique
    `userid`. The columns tell us not only about their characteristics like age but
    also about the revenue we earned from them as a result of our recent email campaign.
  prefs: []
  type: TYPE_NORMAL
- en: 'After preparing our data, it’s simple to perform a t-test to check whether
    our groups are different. We can do it in one line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you’ll get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The important part of this output is the `pvalue` variable, which tells us
    the *p*-value of our test. We can see that the result says that *p* = 0.037, approximately.
    Since *p* < 0.05, we can conclude that this is a statistically significant difference.
    We can check the size of the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output is 125.0\. The average `groupb` customer has outspent the average
    `groupa` customer by $125\. This difference is statistically significant, so we
    reject Hypothesis 0 in favor of Hypothesis 1, concluding (for now, at least) that
    the blue text in marketing emails leads to about $125 more in revenue per user
    than black text.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have just done was an *experiment*. We split a population into two
    groups, performed different actions on each group, and compared the results. In
    the context of business, such an experiment is often called an *A/B test*. The
    *A/B* part of the name refers to the two groups, Group A and Group B, whose different
    responses to emails we compared. Every A/B test follows the same pattern we went
    through here: a split into two groups, application of a different treatment (for
    example, sending different emails) to each group, and statistical analysis to
    compare the groups’ outcomes and draw conclusions about which treatment is better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve successfully conducted an A/B test, we may want to conclude
    that the effect of blue text is to increase spending by $125\. However, something
    is wrong with the A/B test we ran: it’s *confounded*. To see what we mean, consider
    [Table 4-1](#table4-1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4-1: Differences Between Groups'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Group A** | **Group B** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Personal characteristics** | Younger (Same as B in other ways) | Older
    (Same as A in other ways) |'
  prefs: []
  type: TYPE_TB
- en: '| **Email text color** | Black | Blue |'
  prefs: []
  type: TYPE_TB
- en: '| **Average revenue per user** | $104 | $229 |'
  prefs: []
  type: TYPE_TB
- en: 'We can see the important features of Group A and Group B. Our t-test comparing
    spending found that their spending levels were significantly different. We want
    an explanation for why they’re different, and any explanation of different outcomes
    will have to rely on the differences listed in [Table 4-1](#table4-1). We want
    to be able to conclude that the difference in spending can be explained by the
    difference in the text color. However, that difference coexists with another difference:
    age.'
  prefs: []
  type: TYPE_NORMAL
- en: We can’t be certain that the difference in spending levels is due to text color
    rather than age. For example, perhaps no one even noticed the text difference,
    but older people tend to be wealthier and more eager to buy your products than
    young people. If so, our A/B test didn’t test for the effect of blue text, but
    rather for the effect of age or wealth. We intended to study only the effect of
    text color in this A/B test, and now we don’t know whether we truly studied that
    or whether we studied age, wealth, or something else. It would be better if our
    A/B test had a simpler, non-confounded design like the one illustrated in [Table
    4-2](#table4-2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4-2: A Non-confounded A/B Test Design'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Group C** | **Group D** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Personal characteristics** | (Same as D in every way) | (Same as C in every
    way) |'
  prefs: []
  type: TYPE_TB
- en: '| **Email text color** | Black | Blue |'
  prefs: []
  type: TYPE_TB
- en: '| **Average revenue per user** | $104 | $229 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 4-2](#table4-2) imagines that we had split the users into hypothetical
    groups called C and D, which are identical in all personal characteristics, but
    differ only in the text of the emails they received. In this hypothetical scenario,
    the spending difference can be explained only by the different text colors sent
    to each group because that’s the only difference between them. We should have
    split our groups in a way that ensured that the only differences between groups
    were in our experimental treatment, not in the group members’ preexisting characteristics.
    If we had done so, we would have avoided having a confounded experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Math of A/B Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also express these notions mathematically. We can use the common statistical
    notation *E*() to refer to the expected value. So *E*(*A’s revenue with blk text*)
    will mean *the expected value of revenue we would earn by sending a black-text
    email to Group A*. We can write two simple equations that describe the relationship
    between the revenue we expect to earn from black text, the effect of our experiment,
    and the revenue we expect to earn from blue text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*A’s revenue with blk text*) + *E*(*effect of changing blk → blue on A*)
    = *E*(*A’s revenue with blue text*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*B’s revenue with blk text*) + *E*(*effect of changing blk → blue on B*)
    = *E*(*B’s revenue with blue text*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To decide whether to reject Hypothesis 0, we need to solve for the effect sizes:
    *E*(*effect of changing blk → blue on A*) and *E*(*effect of changing blk → blue
    on B*). If either of these effect sizes is different from 0, we should reject
    Hypothesis 0\. By performing our experiment, we found *E*(*A’s revenue with blk
    text*) = 104 and *E*(*B’s revenue with blue text*) = 229\. After knowing these
    values, we have the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 104 + *E*(*effect of changing blk → blue on A*) = *E*(*A’s revenue with blue
    text*)
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*B’s revenue with blk text*) + *E*(*effect of changing blk → blue on B*)
    = 229'
  prefs: []
  type: TYPE_NORMAL
- en: 'But this still leaves many variables we don’t know, and we’re not yet able
    to solve for *E*(*effect of changing blk → blue on A*) and *E*(*effect of changing
    blk → blue on B*). The only way we’ll be able to solve for our effect sizes will
    be if we can simplify these two equations. For example, if we knew that *E*(*A’s
    revenue with blk text*) = *E*(*B’s revenue with blk text*), and *E*(*effect of
    changing blk → blue on A*) = *E*(*effect of changing blk → blue on B*), and *E*(*A’s
    revenue with blue Text*) = *E*(*B’s revenue with blue text*), then we could reduce
    these two equations to just one simple equation. If we knew that our groups were
    identical before our experiment, we would know that all of these expected values
    were equal, and we could simplify our two equations to the following easily solvable
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: 104 + *E*(*effect of changing blk → blue on everyone*) = 229
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can be sure that the effect of blue text is a $125 revenue increase.
    This is why we consider it so important to design non-confounded experiments in
    which the groups have equal expected values for personal characteristics. By doing
    so, we’re able to solve the preceding equations and be confident that our measured
    effect size is actually the effect of what we’re studying and not the result of
    different underlying characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Translating the Math into Practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We know what to do mathematically, but we need to translate that into practical
    action. How should we ensure that *E*(*A’s revenue with blk text*) = *E*(*B’s
    revenue with blk text*), and how should we ensure that the other expected values
    are all the same? In other words, how can we ensure that our study design looks
    like [Table 4-2](#table4-2) instead of [Table 4-1](#table4-1)? We need to find
    a way to select subgroups of our desktop subscriber list that are expected to
    be identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to select subgroups that are expected to be identical is to
    select them randomly. We mentioned this briefly in Chapter 3: every random sample
    from a population has an expected value equal to the population mean. So, we expect
    that two random samples from the same population won’t differ from each other
    significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s perform an A/B test on our laptop subscriber list, but this time we’ll
    use randomization to select our groups to avoid having a confounded experimental
    design. Suppose that in this new A/B test, we want to test whether adding a picture
    to a marketing email will improve revenue. We can proceed just as we did before:
    we split the laptop subscriber list into two subgroups, and we send different
    emails to each subgroup. The difference is that this time, instead of splitting
    based on age, we perform a random split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, we use the NumPy `random.random()` method to generate a column
    that consists of randomly generated 0s and 1s. We can interpret a 0 to mean that
    a user belongs to Group C, and a 1 to mean that a user belongs to group D. When
    we generate 0s and 1s randomly like this, the groups could end up with different
    sizes. However, here we use a *random seed* (in the first line, `np.random.seed(18811015)`).
    Every time anyone uses this random seed, their “randomly” generated column of
    0s and 1s will be identical. That means that if you use this random seed, your
    results at home should be the same as the results here in the book. Using a random
    seed is not necessary, but if you use the same random seed we used here, you should
    find that both Group C and Group D have 15 members.
  prefs: []
  type: TYPE_NORMAL
- en: After generating this random column of 0s and 1s that indicates the group assignment
    of each customer, we create two smaller dataframes, `groupc` and `groupd`, that
    contain user IDs and information about the users in each subgroup.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can send the group membership information to your marketing team members
    and ask them to send the right emails to the right groups. One group, either C
    or D, should receive an email without a picture, and the other group, either D
    or C, should receive an email with a picture. Then, suppose that the marketing
    team sends you a file containing the results of this latest A/B test. You can
    download a fabricated dataset containing hypothetical results from [https://bradfordtuckfield.com/emailresults2.csv](https://bradfordtuckfield.com/emailresults2.csv).
    After you store it in the same place where you’re running Python, let’s read the
    results of this email campaign into Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, let’s join our email results to our group dataframes, just as we did
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And again, we can use a t-test to check whether the revenue resulting from
    Group C is different from the revenue we get from Group D:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We find that the *p*-value is less than 0.05, indicating that the difference
    between the groups is statistically significant. This time, our experiment isn’t
    confounded, because we used random assignment to ensure that the differences between
    groups are the result of our different emails, not the result of different characteristics
    of each group. Since our experiment isn’t confounded, and since we find a significant
    difference between the revenues earned from Group C and Group D, we conclude that
    including the picture in the email has a nonzero effect. If the marketing team
    tells us that it sent the picture only to Group D, we can find the estimated size
    of the effect easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the estimated effect here with subtraction: the mean revenue obtained
    from subjects in Group D minus the mean revenue obtained from subjects in Group
    C. The difference between mean revenue from Group C and mean revenue from Group
    D, about $260, is the size of the effect of our experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: The process we follow for A/B testing is really quite simple, but it’s also
    powerful. We can use it for a wide variety of questions that we might want to
    answer. Anytime you’re unsure about an approach to take in business, especially
    in user interactions and product design, considering an A/B test as an approach
    to learn the answer is worthwhile. Now that you know the process, let’s move on
    and understand its nuances.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing with the Champion/Challenger Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we’ve crafted a great email, we might call it our *champion* email design:
    the one that, according to what we know so far, we think will perform the best.
    After we have a champion email design, we may wish to stop doing A/B testing and
    simply rest on our laurels, collecting money indefinitely from our “perfect” email
    campaigns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But this isn’t a good idea, for a few reasons. The first is that times change.
    Fads in design and marketing change quickly, and a marketing effort that seems
    exciting and effective today may soon seem dated and outmoded. Like all champions,
    your champion email design will become weaker and less effective as it ages. Even
    if design and marketing fads *don’t* change, your champion will eventually seem
    boring as the novelty wears off: new stimuli are more likely to get people’s attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Another reason that you shouldn’t stop A/B testing is that your customer base
    will change. You’ll lose some old customers and gain new ones. You’ll release
    new products and enter new markets. As your customer mix changes, the types of
    emails that they tend to respond to will change as well, and constant A/B testing
    will enable you to keep up with their changing characteristics and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: A final reason to continue A/B testing is that although your champion likely
    is good, you might not have optimized it in every possible way. A dimension you
    haven’t tested yet could enable you to have an even better champion that gets
    even better performance. If we can successfully run one A/B test and learn one
    thing, we’ll naturally want to continue to use our A/B testing skills to learn
    more and more and to increase profits higher and higher.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a champion email and want to continue A/B testing to try to
    improve it. You do another random split of your users, into a new Group A and
    a new Group B. You send the champion email to Group A. You send another email
    to Group B that differs from the champion email in one way that you want to learn
    about; for example, maybe it uses formal rather than informal language. When we
    compare the revenues from Group A and Group B after the email campaign, we’ll
    be able to see whether this new email performs better than the champion email.
  prefs: []
  type: TYPE_NORMAL
- en: Since the new email is in direct competition with the champion email, we call
    it the *challenger*. If the champion performs better than the challenger, the
    champion retains its champion status. If the challenger performs better than the
    champion, that challenger becomes the new champion.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process can continue indefinitely: we have a champion that represents
    the state of the art of whatever we’re doing (marketing emails, in this case).
    We constantly test the champion by putting it in direct competition with a succession
    of challengers in A/B tests. Each challenger that leads to significantly better
    outcomes than the champion becomes the new champion and is, in turn, put into
    competition against new challengers later.'
  prefs: []
  type: TYPE_NORMAL
- en: This endless process is called the *champion/challenger framework* for A/B tests.
    It’s meant to lead to continuous improvement, continuous refinement, and asymptotic
    optimization to get to the best-possible performance in all aspects of business.
    The biggest tech companies in the world run literally hundreds of A/B tests per
    day, with hundreds of challengers taking on hundreds of champions, sometimes defeating
    them and sometimes being defeated. The champion/challenger framework is a common
    approach for setting up and running A/B tests for the most important and most
    challenging parts of your business.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing Mistakes with Twyman’s Law and A/A Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A/B testing is a relatively simple process from beginning to end. Nevertheless,
    we are all human and make mistakes. In any data science effort, not just A/B testing,
    it’s important to proceed carefully and constantly check whether we’ve done something
    wrong. One piece of evidence that often indicates that we’ve done something wrong
    is that things are going too well.
  prefs: []
  type: TYPE_NORMAL
- en: 'How could it be bad for things to go too well? Consider a simple example. You
    perform an A/B test: Group A gets one email, and Group B gets a different one.
    You measure revenue from each group afterward and find that the average revenue
    earned from members of Group A is about $25, while the average revenue earned
    from members of Group B is $99,999\. You feel thrilled about the enormous revenue
    you earned from Group B. You call all your colleagues to an emergency meeting
    and tell them to stop everything they’re doing and immediately work on implementing
    the email that Group B got and pivot the whole company strategy around this miracle
    email.'
  prefs: []
  type: TYPE_NORMAL
- en: As your colleagues are working around the clock on sending the new email to
    everyone they know, you start to feel a nagging sense of doubt. You think about
    how unlikely it is that a single email campaign could plausibly earn almost $100,000
    in revenue per recipient, especially when your other campaigns are earning only
    about $25 per user. You think about how $99,999, the amount of revenue you supposedly
    earned per user, is five identical digits repeated. Maybe you remember a conversation
    you had with a database administrator who told you that your company database
    automatically inserts 99999 every time a database error occurs or data is missing.
    Suddenly, you realize that your email campaign didn’t really earn $99,999 per
    user, but rather a database error for Group B caused the appearance of the apparently
    miraculous result.
  prefs: []
  type: TYPE_NORMAL
- en: 'A/B testing is a simple process from a data science point of view, but it can
    be quite complex from a practical and social point of view. For example, in any
    company larger than a tiny startup, the creative people designing marketing emails
    will be different from the technology people who maintain the databases that record
    revenues per user. Other groups may be involved in little parts of A/B testing:
    maybe a group that maintains the software used to schedule and send out emails,
    maybe a group that creates art that the email marketing team asks for, and maybe
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: With all these groups and steps involved, many possible chances exist for miscommunication
    and small errors. Maybe two different emails are designed, but the person who’s
    in charge of sending them out doesn’t understand A/B testing and copies and pastes
    the same email to both groups. Maybe they accidentally paste in something that’s
    not even supposed to be in the A/B test at all. In our example, maybe the database
    that records revenues encounters an error and puts 99999 in the results as an
    error code, which others mistakenly interpret as a high revenue. No matter how
    careful we try to be, mistakes and miscommunications will always find a way to
    happen.
  prefs: []
  type: TYPE_NORMAL
- en: The inevitability of mistakes should lead us to be naturally suspicious of anything
    that seems too good, bad, interesting, or strange to be true. This natural suspicion
    is advocated by *Twyman’s law*, which states that “any figure that looks interesting
    or different is usually wrong.” This law has been restated in several ways, including
    “any statistic that appears interesting is almost certainly a mistake” and “the
    more unusual or interesting the data, the more likely it is to have been the result
    of an error.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides extreme carefulness and natural suspicion of good news, we have another
    good way to prevent the kinds of interpretive mistakes that Twyman’s law warns
    against: *A/A testing*. This type of testing is just what it sounds like; we go
    through the steps of randomization, treatment, and comparison of two groups just
    as in A/B testing, but instead of sending two different emails to our two randomized
    groups, we send the identical email to each group. In this case, we expect the
    null hypothesis to be true, and we won’t be gullibly convinced by a group that
    appears to get $100,000 more revenue than the other group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consistently find that A/A tests lead to statistically significant differences
    between groups, we can conclude that our process has a problem: a database gone
    haywire, a t-test being run incorrectly, an email being pasted wrong, randomization
    performed incorrectly, or something else. An A/A test would also help us realize
    that the first test described in this chapter (where Group A consists of younger
    people and Group B consists of older people) was confounded, since we would know
    that differences between the results of an A/A test must be due to the differences
    in age rather than differences between emails. A/A testing can be a useful sanity
    check that can prevent us from getting carried away by the kind of unusual, interesting,
    too-good-to-be-true results that Twyman’s law warns us about.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Effect Sizes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first A/B test we ran, we observed a difference of $125 between the Group
    A users who received a black-text email and the Group B users who received a blue-text
    email. This $125 difference between groups is also called the A/B test’s *effect
    size*. It’s natural to try to form a judgment about whether we should consider
    this $125 effect size a small effect, a medium effect, or a large effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'To judge whether an effect is small or large, we have to compare it to something
    else. Consider the following list of nominal GDP figures (in US dollars, as of
    2019) for Malaysia, Myanmar, and the Marshall Islands, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When we look at these numbers, $125 starts to seem pretty small. For example,
    consider the standard deviation of our `gdps` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result is 158884197328.32672, or about $158,884,197,328 (almost $159 billion).
    The standard deviation is a common way to measure how dispersed a dataset is.
    If we observe a difference between two countries’ GDPs that’s about $80 billion,
    we don’t think of that as outrageously big or outrageously small, because it means
    those countries are about half of a standard deviation apart, a common magnitude
    of difference. Instead of expressing the difference as an $80 billion difference,
    you might say that the two countries’ GDPs differed by about half of a standard
    deviation, and expect to be understood by anyone with some statistical training.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, if someone tells you that two countries have GDPs that differ by
    112 trillion kyat (the currency of Myanmar), you might be unsure whether that
    difference is large or small if you’ve never learned the value of 1 kyat (112
    trillion kyat is equal to about $80 billion at the time of writing). Many currencies
    exist in the world, and their relative and absolute values change all the time.
    A standard deviation, on the other hand, isn’t specific to any particular country
    and is not affected by inflation, making it a useful unit of measurement.
  prefs: []
  type: TYPE_NORMAL
- en: We can use standard deviations as measurements in other domains as well. Someone
    from Europe may be used to using meters to express heights. When you tell your
    European data scientist friend about a man who’s 75 inches tall, they may feel
    puzzled about whether that’s tall or short or average if they’re not used to conversion
    from inches. But, if you tell them that he’s about two standard deviations taller
    than the mean, they should immediately be able to understand that he’s pretty
    tall but not a record-breaking height. Observing someone who’s more than three
    standard deviations above the mean height will be much rarer, and we can know
    that whether we’re measuring in meters or inches or any other units.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we talk about the $125 effect size of our A/B test, let’s try to think
    of it in terms of standard deviations as well. Compared to the standard deviation
    of the GDP measurements we’ve seen, $125 is small potatoes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The output is about 7.9 · 10^(–10), which shows us that the $125 effect size
    is a little more than 1 one-billionth of the standard deviation of our GDP figures.
    Compared to the world of GDP measurements, a $125 difference in GDP is like being
    a micrometer taller than your friend—not even enough to notice without extremely
    precise measurement technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, suppose we conduct a survey of the prices of burgers at local
    restaurants. Maybe we find the following prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check this standard deviation as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The standard deviation of our burger price data is about 1.65\. So, two countries’
    GDPs differing by about $80 billion is roughly comparable to two burger prices
    differing by about 80 cents: both represent about half of a standard deviation
    in their respective domains. When we compare a $125 effect size to this, we see
    that it’s huge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We see that $125 is about 75.9 burger price standard deviations. Seeing a $125
    difference in burger prices in your town is therefore something like seeing a
    man who is over 20 feet tall—unheard of.
  prefs: []
  type: TYPE_NORMAL
- en: 'By measuring our effect size in terms of the standard deviation of different
    datasets, we can easily make comparisons, not just between different domains with
    the same units (GDP in dollars versus burger prices in dollars) but also between
    different domains that use totally different units (burger prices in dollars versus
    height in inches). The metric we’ve calculated several times here—an effect size
    divided by a relevant standard deviation—is called *Cohen’s d*, a common metric
    for measuring effect sizes. Cohen’s *d* is just the number of standard deviations
    that two populations’ means are apart from each other. We can calculate Cohen’s
    *d* for our first A/B test as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We see that the result is about 0.76\. A common convention when we’re working
    with Cohen’s *d* is to say that if Cohen’s *d* is about 0.2 or lower, we have
    a small effect; if Cohen’s *d* is about 0.5, we have a medium effect; and if Cohen’s
    *d* is around 0.8 or even higher, we have a large effect. Since our result is
    about 0.76—quite close to 0.8—we can say that we’re working with a large effect
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the Significance of Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We typically use statistical significance as the key piece of evidence that
    convinces us that an effect that we study in an A/B test is real. Mathematically,
    statistical significance depends on three things:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the effect being studied (like the increase in revenue that results
    from changing an email’s text color). Bigger effects make statistical significance
    more likely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the sample being studied (the number of people on a subscriber list
    who are receiving our marketing emails). Bigger samples make statistical significance
    more likely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The significance threshold we’re using (typically 0.05). A higher threshold
    makes statistical significance more likely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we have a big sample size, and we’re studying a big effect, our t-tests
    will likely reach statistical significance. On the other hand, if we study an
    effect that’s very small, with a sample that’s very small, we may have predestined
    our own failure: the probability that we detect a statistically significant result
    is essentially 0—even if the email truly does have an effect. Since running an
    A/B test costs time and money, we’d rather not waste resources running tests like
    this that are predestined to fail to reach statistical significance.'
  prefs: []
  type: TYPE_NORMAL
- en: The probability that a correctly run A/B test will reject a false null hypothesis
    is called the A/B test’s *statistical power*. If changing the color of text leads
    to a $125 increase in revenue per user, we can say that $125 is the effect size,
    and since the effect size is nonzero, we know the null hypothesis (that changing
    the text color has no effect on revenue) is false. But if we study this true effect
    by using a sample of only three or four email subscribers, it’s very possible
    that, by chance, none of these subscribers purchase anything, so we fail to detect
    the true $125 effect. By contrast, if we study the effect of changing the text
    color by using an email list of a million subscribers, we’re much more likely
    to detect the $125 effect and measure it as statistically significant. With the
    million-subscriber list, we have greater statistical power.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can import a module into Python that makes calculating statistical power
    easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate power with this module, we’ll need to define parameters for the
    three things that determine statistical significance (see the preceding bulleted
    list). We’ll define `alpha`, which is our chosen statistical significance threshold,
    as discussed in Chapter 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We choose the standard 0.05 threshold for `alpha`, as is standard in much empirical
    research. We also need to define our sample size. Suppose we’re running an A/B
    test on a group of email subscribers that consists of 90 people total. That means
    we’ll have 45 people in Group A and 45 people in Group B, so we define the number
    of observations in each of our groups as 45\. We’ll store this number in a variable
    called `nobs`, short for *number of observations*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have to define an estimated effect size. In our previous A/B test,
    we observed an effect size of $125\. However, for the statistical power calculations
    this module performs, we can’t express the effect size in dollar units or units
    of any other currency. We’ll use Cohen’s *d* instead, and we’ll specify a medium
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use a function that will take the three parameters we’ve defined
    and calculate the statistical power we should expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you run `print(power)`, you can see that the estimated statistical power
    for our hypothetical A/B test is about 0.65\. This means that we expect about
    a 65 percent chance of detecting an effect from our A/B test and about a 35 percent
    chance that even though a true effect exists, our A/B test doesn’t find it. These
    odds might seem unfavorable if a given A/B test is expected to be expensive; you’ll
    have to make your own decisions about the minimum level of power that is acceptable
    to you. Power calculations can help at the planning stage to understand what to
    expect and be prepared. One common convention is to authorize only A/B tests that
    are expected to have at least 80 percent power.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the same `solve_power()` method we used in the previous snippet
    to “reverse” the power calculation: you’d start by assuming a certain power level
    and then calculate the parameters required to achieve that level of statistical
    power. For example, in the following snippet, we define `power`, `alpha`, and
    our effect size, and run the `solve_power()` command not to calculate the power
    but to calculate `observations`, the number of observations we’ll need in each
    group to achieve the power level we specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If you run `print(observations)`, you’ll see that the result is about 63.8\.
    This means that if we want to have 80 percent statistical power for our planned
    A/B test, we’ll need to recruit at least 64 participants for both groups. Being
    able to perform these kinds of calculations can be helpful in the planning stages
    of A/B tests.
  prefs: []
  type: TYPE_NORMAL
- en: Applications and Advanced Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve considered only A/B tests related to marketing emails. But A/B
    tests are applicable to a wide variety of business challenges beyond optimal email
    design. One of the most common applications of A/B testing is user interface/experience
    design. A website might randomly assign visitors to two groups (called Group A
    and Group B, as usual) and show different versions of the site to each group.
    The site can then measure which version leads to more user satisfaction, higher
    revenue, more link clicks, more time spent on the site, or whatever else interests
    the company. The whole process can be completely automated, which is what enables
    the high-speed, high-volume A/B testing that today’s top tech companies are doing.
  prefs: []
  type: TYPE_NORMAL
- en: E-commerce companies run tests, including A/B tests, on product pricing. By
    running an A/B test on pricing, you can measure what economists call the *price
    elasticity of demand*, meaning how much demand changes in response to price changes.
    If your A/B test finds only a very small change in demand when you increase the
    price, you should increase the price for everyone and take advantage of their
    greater willingness to pay. If your A/B test finds that demand drops off significantly
    when you increase the price slightly, you can conclude that customers are sensitive
    to price, and their purchase decisions depend heavily on price considerations.
    If customers are sensitive to price and constantly thinking about it, they’ll
    likely respond positively to a price decrease. If so, you should decrease the
    price for everyone instead, and expect a large increase in demand. Some businesses
    have to set prices based on intuition or other painstaking calculations, but A/B
    testing makes determining the right price relatively simple.
  prefs: []
  type: TYPE_NORMAL
- en: Email design, user-interface design, and product pricing are all common concerns
    for *business-to-consumer (B2C)* business models, in which businesses sell directly
    to consumers. B2C scenarios are a natural fit for A/B testing because the number
    of customers, products, and transactions tends to be higher for B2C businesses
    than for other businesses, so we can get large sample sizes and higher statistical
    power.
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t mean that business-to-business (B2B) companies can’t do A/B testing.
    Indeed, A/B testing has been practiced for centuries around the world and in many
    domains, though it used to be called just *science*. For example, medical researchers
    do *randomized controlled trials* for new drugs, with an approach that’s often
    essentially the same as A/B testing in a champion/challenger framework. Businesses
    of all kinds have always needed to learn about the market and their customers,
    and A/B testing is a natural, rigorous way to learn nearly anything.
  prefs: []
  type: TYPE_NORMAL
- en: As you apply A/B testing in your business, you should try to learn as much as
    you can about it, beyond the content in the limited space of this chapter. One
    huge field you might want to wade into is Bayesian statistics. Some data scientists
    prefer to use Bayesian methods instead of significance tests and *p*-values to
    test for the success of A/B tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting, useful topic to learn more about is the *exploration/exploitation
    trade-off* in A/B tests. In this trade-off, two goals are in constant tension:
    to explore (for example, to run A/B tests with possibly bad email designs to learn
    which is best) and to exploit (for example, to send out only the champion email
    because it seems to perform the best). Exploration can lead to missed opportunities
    if one of your challengers performs much worse than the champion; you would have
    been better off just sending out the champion to everyone. Exploitation can lead
    to missed opportunities if your champion is not as good as another challenger
    that you haven’t tested yet because you’re too busy exploiting your champion to
    do the requisite exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: In operations research, you’ll find a huge body of research on the *multi-armed
    bandit problem*, which is a mathematical formalization of the exploration/exploitation
    dilemma. If you’re really interested in doing A/B testing optimally, you can peer
    into some of the strategies that researchers have come up with to solve the multi-armed
    bandit problem and run A/B tests as efficiently as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The Ethics of A/B Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A/B testing is fraught with difficult ethical issues. This may seem surprising,
    but remember, A/B testing is an experimental method in which we intentionally
    alter human subjects’ experiences in order to study the results for our own gain.
    This means that A/B testing is human experimentation. Think about other examples
    of human experimentation to see why people have ethical concerns about it:'
  prefs: []
  type: TYPE_NORMAL
- en: Jonas Salk developed an untested, unprecedented polio vaccine, tried it on himself
    and his family, and then tried it on millions of American children to ensure that
    it worked. (It worked and helped eliminate a terrible disease from much of the
    world.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: My grandmother made a pie for her grandchildren, observed how we reacted to
    it, and then the next day made a different pie and checked whether we reacted
    more or less positively. (Both were delicious.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A professor posed as a student and emailed 6,300 professors to ask them to schedule
    time to talk to her, lying about herself and her intentions in an attempt to determine
    whether her false persona would be a target of discrimination so she could publish
    a paper about the replies. She didn’t compensate any of the unwitting study participants
    for the deception or the schedule disruption, nor did she receive their consent
    beforehand to be an experimental subject. (Every detail of this study was approved
    by a university ethics board.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A corporation intentionally manipulated the emotions of its users to better
    understand and sell products to them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Josef Mengele performed painful and deadly sadistic experiments on unwilling
    human subjects in the Auschwitz concentration camp.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You perform an A/B test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first five entries on this list of human experiments actually happened,
    and all but the second inspired public discussions about ethics among social scientists.
    You’ll have to decide whether the sixth will happen and what position you will
    take concerning the ethical issues involved. Because of the broad range of activities
    that could be called human experimentation, making a single ethical judgment about
    all of its forms isn’t possible. We have to consider several important ethical
    concepts when we’re deciding whether our A/B tests make us a hero like my grandmother
    or Salk, a villain like Mengele, or something in between.
  prefs: []
  type: TYPE_NORMAL
- en: The first concept we should consider is consent. Salk tested his vaccine on
    himself before large-scale tests of others. Mengele, by contrast, performed experiments
    on unwilling subjects confined in concentration camps. Informed consent always
    makes human experimentation more ethical. In some cases, obtaining informed consent
    is not feasible. For example, if we perform experiments about which outdoor billboard
    designs are most effective, we can’t obtain informed consent from every possible
    human research subject, since any person in the world could conceivably see a
    public billboard, and we don’t have a way to contact every living human.
  prefs: []
  type: TYPE_NORMAL
- en: Other cases form a large gray area. For example, a website that performs A/B
    tests may have a Terms and Conditions section, with small print and legalese that
    claims that every website visitor provides consent to be experimented on (via
    A/B tests of user-interface features) whenever they navigate to the site. This
    may technically meet the definition of informed consent, but only a tiny percentage
    of any website’s visitors likely visit and understand these conditions. In gray-area
    cases, it helps to consider other ethical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important ethical consideration related to A/B testing is risk. Risk
    itself involves two considerations: potential downsides to participation as a
    human subject and the probability of experiencing those downsides. Salk’s vaccine
    had a large potential downside—contracting polio—but because of Salk’s preparation
    and knowledge, the probability of subjects experiencing it was remarkably low.
    A/B testing for marketing campaigns usually has potential downsides that are minuscule
    or smaller, as it’s hard to even imagine any downside that could occur because
    (for example) someone was exposed to blue rather than black text in one marketing
    email. Experiments with low risks to subjects are more ethical than risky experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: We should also consider the potential benefits that could result from our experimentation.
    Salk’s vaccine experiments had the potential (later realized) of eradicating polio
    from most of the Earth. A/B tests are designed to improve profits, not cure diseases,
    so your judgment of their benefits will have to depend on your opinion of the
    moral status of corporate profits. The only other benefit likely to come from
    a corporation’s marketing experiment would be an advance in understanding of human
    psychology. Indeed, corporate marketing practitioners occasionally publish the
    results of marketing experiments in psychology journals, so this isn’t unheard
    of.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical and philosophical questions can never reach a definitive, final conclusion
    that everyone agrees on. You can make up your own mind about whether you feel
    that A/B testing is fundamentally good, like Salk’s vaccine experiments, or fundamentally
    abhorrent, like Mengele’s horrors. Most people agree that the extremely low risks
    of most online A/B testing, and the fact that people rarely refuse consent to
    benign A/B tests, mean that A/B testing is an ethically justifiable activity when
    performed properly. Regardless, you should think carefully through your own situation
    and come to your own conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we discussed A/B testing. We started with a simple t-test,
    and then looked at the need for random, non-confounded data collection as part
    of the A/B testing process. We covered some nuances of A/B testing, including
    the champion/challenger framework and Twyman’s law, as well as ethical concerns.
    In the next chapter, we’ll discuss binary classification, an essential skill for
    any data scientist.
  prefs: []
  type: TYPE_NORMAL
