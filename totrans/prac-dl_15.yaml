- en: '**15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**15'
- en: 'A CASE STUDY: CLASSIFYING AUDIO SAMPLES**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个案例研究：分类音频样本**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: 'Let’s bring together everything that we’ve learned throughout the book. We’ll
    be looking at a single case study. The scenario is this: we are data scientists,
    and our boss has tasked us with building a classifier for audio samples stored
    as *.wav* files. We’ll begin with the data itself. We first want to build some
    basic intuition for how it’s structured. From there, we’ll build augmented datasets
    we can use for training models. The first dataset uses the sound samples themselves,
    a one-dimensional dataset. We’ll see that this approach isn’t as successful as
    we would like it to be.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将整本书中学到的内容汇总在一起。我们将通过一个单一的案例研究来进行。场景是这样的：我们是数据科学家，老板要求我们为存储为*.wav*文件的音频样本构建一个分类器。我们将从数据本身开始。首先，我们希望构建一些基本的直觉，了解数据是如何结构化的。从这里开始，我们将构建可以用于训练模型的增强数据集。第一个数据集使用的是音频样本本身，这是一个一维的数据集。我们会发现，这种方法并不像我们希望的那样成功。
- en: We’ll then turn the audio data into images to allow us to explore two-dimensional
    CNNs. This change of representation will lead to a big improvement in model performance.
    Finally, we’ll combine multiple models in ensembles to see how to leverage the
    relative strengths and weaknesses of the individual models to boost overall performance
    still more.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把音频数据转化为图像，以便我们可以探索二维卷积神经网络（CNN）。这种表示方式的变化将大大改善模型的表现。最后，我们将通过集成多个模型，看看如何利用单个模型的相对优势和劣势来进一步提升整体表现。
- en: Building the Dataset
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建数据集
- en: There are 10 classes in our dataset, which consists of 400 samples total, 40
    samples per class, each 5 seconds long. We’ll assume we cannot get any more data
    because it’s time-consuming and expensive to record the samples and label them.
    We must work with the data we are given and no more.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有10个类别，总共400个样本，每个类别40个样本，每个样本时长为5秒。我们假设我们不能再获得更多的数据，因为录制样本并进行标注是费时且昂贵的。我们必须使用我们已经得到的数据，不能再增加其他数据。
- en: Throughout this book, we have consistently preached about the necessity of having
    a good dataset. We’ll assume that the dataset we have been handed is complete
    in the sense that our system will encounter only types of sound samples in the
    dataset; there will be no unknown class or classes. Additionally, we’ll also assume
    that the balanced nature of the dataset is real, and all classes are indeed equally
    likely.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们始终强调拥有良好数据集的重要性。我们假设我们得到的数据集是完整的，也就是说，我们的系统将只会遇到数据集中已存在的音频样本类型，不会有未知的类别。此外，我们还假设数据集的平衡性是真实的，所有类别的样本都是等可能的。
- en: 'The audio dataset we’ll use is called ESC-10\. For a complete description,
    see “ESC: Dataset for Environmental Sound Classification” by Karol J. Piczal (2015).
    The dataset is available at [https://github.com/karoldvl/ESC-50/](https://github.com/karoldvl/ESC-50/).
    But it needs to be extracted from the larger ESC-50 dataset, which doesn’t have
    a license we can use. The ESC-10 subset does.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用的音频数据集叫做ESC-10。有关完整的描述，请参阅Karol J. Piczal（2015年）的《ESC: Dataset for Environmental
    Sound Classification》。数据集可以在[https://github.com/karoldvl/ESC-50/](https://github.com/karoldvl/ESC-50/)找到。但它需要从更大的ESC-50数据集中提取，而该数据集没有我们可以使用的许可证。ESC-10子集是有许可证的。'
- en: Let’s do some preprocessing to extract the ESC-10 *.wav* files from the larger
    ESC-50 dataset. Download the single ZIP-file version of the dataset from the preceding
    URL and expand it. This will create a directory called *ESC-50-master*. Then,
    use the code in [Listing 15-1](ch15.xhtml#ch15lis1) to build the ESC-10 dataset
    from it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些预处理，从更大的ESC-50数据集中提取ESC-10 *.wav*文件。从上面的URL下载数据集的单个ZIP文件版本并解压。这将创建一个名为*ESC-50-master*的目录。然后，使用[Listing
    15-1](ch15.xhtml#ch15lis1)中的代码从中构建ESC-10数据集。
- en: import sys
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: import os
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import shutil
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: import shutil
- en: classes = {
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: classes = {
- en: '"rain":0,'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"rain":0,'
- en: '"rooster":1,'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '"rooster":1,'
- en: '"crying_baby":2,'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '"crying_baby":2,'
- en: '"sea_waves":3,'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '"sea_waves":3,'
- en: '"clock_tick":4,'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '"clock_tick":4,'
- en: '"sneezing":5,'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '"sneezing":5,'
- en: '"dog":6,'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '"dog":6,'
- en: '"crackling_fire":7,'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '"crackling_fire":7,'
- en: '"helicopter":8,'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '"helicopter":8,'
- en: '"chainsaw":9,'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '"chainsaw":9,'
- en: '}'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'with open("ESC-50-master/meta/esc50.csv") as f:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open("ESC-50-master/meta/esc50.csv") as f:'
- en: lines = [i[:-1] for i in f.readlines()]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: lines = [i[:-1] for i in f.readlines()]
- en: lines = lines[1:]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: lines = lines[1:]
- en: os.system("rm -rf ESC-10")
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf ESC-10")
- en: os.system("mkdir ESC-10")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("mkdir ESC-10")
- en: os.system("mkdir ESC-10/audio")
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("mkdir ESC-10/audio")
- en: meta = []
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: meta = []
- en: 'for line in lines:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'for line in lines:'
- en: t = line.split(",")
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: t = line.split(",")
- en: 'if (t[-3] == ''True''):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (t[-3] == ''True''):'
- en: meta.append("ESC-10/audio/%s %d" % (t[0],classes[t[3]]))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: meta.append("ESC-10/audio/%s %d" % (t[0], classes[t[3]]))
- en: src = "ESC-50-master/audio/"+t[0]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: dst = "ESC-10/audio/"+t[0]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: shutil.copy(src,dst)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'with open("ESC-10/filelist.txt","w") as f:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'for m in meta:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: f.write(m+"\n")
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-1: Building the ESC-10 dataset*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The code uses the ESC-50 metadata to identify the sound samples that belong
    to the 10 classes of the ESC-10 dataset and then copies them to the *ESC-10/audio*
    directory. It also writes a list of the audio files to *filelist.txt*. After running
    this code, we’ll use only the ESC-10 files.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'If all is well, we should now have 400 five-second *.wav* files, 40 from each
    of the 10 classes: rain, rooster, crying baby, sea waves, clock tick, sneezing,
    dog, crackling fire, helicopter, and chainsaw. We’ll politely refrain from asking
    our boss exactly why she wants to discriminate between these particular classes
    of sound.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting the Dataset
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our first instinct should be that our dataset is too small. After all, we have
    only 40 examples of each sound, and we know that some of those will need to be
    held back for testing, leaving even fewer per class for training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: We could resort to *k*-fold validation, but in this case, we’ll instead opt
    for data augmentation. So, how do we augment audio data?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Recall, the goal of data augmentation is to create new data samples that could
    plausibly come from the classes in the dataset. With images, we can make obvious
    changes like shifting, flipping left and right, and so on. With continuous vectors,
    we’ve seen how to use PCA to augment the data (see [Chapter 5](ch05.xhtml#ch05)).
    To augment the audio files, we need to think of things we can do that will produce
    new files that still sound like the original class. Four thoughts come to mind.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: First, we can shift the sample in time, much as we can shift an image to the
    left or right a few pixels. Second, we can simulate a noisy environment by adding
    a small amount of random noise to the sound itself. Third, we can shift the pitch
    of the sound, and make it higher or lower by some small amount. Not surprisingly,
    this is known as *pitch shifting*. Finally, we can lengthen or compress the sound
    in time. This is known as *time shifting*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Doing all of this sounds complicated, especially if we haven’t worked with audio
    data before. I should point out that in practice, being presented with unfamiliar
    data is a very real possibility; we don’t all get to choose what we need to work
    with.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, we’re working in Python, and the Python community is vast
    and talented. It turns out that adding one library to our system will allow us
    to easily do time stretching and pitch shifting. Let’s install the `librosa` library.
    This should do the trick for us:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the necessary library installed, we can augment the ESC-10 dataset with
    the code in [Listing 15-2](ch15.xhtml#ch15lis2).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: import os
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: import random
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: from scipy.io.wavfile import read, write
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: import librosa as rosa
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: N = 8
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf augmented; mkdir augmented")
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: os.system("mkdir augmented/train augmented/test")
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: ❶ src_list = [i[:-1] for i in open("ESC-10/filelist.txt")]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ src_list = [i[:-1] for i in open("ESC-10/filelist.txt")]
- en: z = [[] for i in range(10)]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: z = [[] for i in range(10)]
- en: 'for s in src_list:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'for s in src_list:'
- en: _,c = s.split()
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: _,c = s.split()
- en: z[int(c)].append(s)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: z[int(c)].append(s)
- en: ❷ train = []
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ train = []
- en: test = []
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: test = []
- en: 'for i in range(10):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: p = z[i]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: p = z[i]
- en: random.shuffle(p)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(p)
- en: test += p[:8]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: test += p[:8]
- en: train += p[8:]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: train += p[8:]
- en: random.shuffle(train)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(train)
- en: random.shuffle(test)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(test)
- en: augment_audio(train, "train")
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: augment_audio(train, "train")
- en: augment_audio(test, "test")
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: augment_audio(test, "test")
- en: '*Listing 15-2: Augmenting the ESC-10 dataset, part 1*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-2：增强ESC-10数据集，第一部分*'
- en: This code loads the necessary modules, including the `librosa` module, which
    we’ll just call `rosa`, and two functions from the SciPy `wavfile` module that
    let us read and write NumPy arrays as *.wav* files.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码加载了必要的模块，包括`librosa`模块（我们将其简称为`rosa`），以及来自SciPy `wavfile`模块的两个函数，允许我们将NumPy数组读写为*.wav*文件。
- en: We set the number of samples per class that we’ll hold back for testing (`N=8`)
    and create the output directory where the augmented sound files will reside (`augmented`).
    Then we read the file list we created with [Listing 15-1](ch15.xhtml#ch15lis1)
    ❶. Next, we create a nested list (`z`) to hold the names of the audio files associated
    with each of the 10 classes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置每个类别的测试样本数（`N=8`），并创建输出目录（即增强后的声音文件所在的目录，命名为`augmented`）。然后，我们读取通过[列表 15-1](ch15.xhtml#ch15lis1)创建的文件列表❶。接下来，我们创建一个嵌套列表（`z`），用于存储与10个类别相关联的音频文件名。
- en: Using the list of files per class, we pull it apart and create `train` and `test`
    file lists ❷. Notice that we randomly shuffle the list of files per class and
    the final `train` and `test` lists. This code follows the convention we discussed
    in [Chapter 4](ch04.xhtml#ch04) of separating train and test first, then augmenting.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每个类别的文件列表，我们将其拆分并创建`train`和`test`文件列表❷。注意，我们对每个类别的文件列表和最终的`train`与`test`列表进行了随机打乱。该代码遵循了我们在[第4章](ch04.xhtml#ch04)中讨论的先分离训练集和测试集，再进行增强的惯例。
- en: We can augment the train and test files by calling `augment_audio`. This function
    is in [Listing 15-3](ch15.xhtml#ch15lis3).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用`augment_audio`来增强训练和测试文件。这个函数在[列表 15-3](ch15.xhtml#ch15lis3)中。
- en: 'def augment_audio(src_list, typ):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'def augment_audio(src_list, typ):'
- en: flist = []
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: flist = []
- en: 'for i,s in enumerate(src_list):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,s in enumerate(src_list):'
- en: f,c = s.split()
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: f,c = s.split()
- en: '❶ wav = read(f) # (sample rate, data)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ wav = read(f) # (采样率, 数据)'
- en: base = os.path.abspath("augmented/%s/%s" %
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: base = os.path.abspath("augmented/%s/%s" %
- en: (typ, os.path.basename(f)[:-4]))
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (typ, os.path.basename(f)[:-4]))
- en: fname = base+".wav"
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: fname = base+".wav"
- en: ❷ write(fname, wav[0], wav[1])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ write(fname, wav[0], wav[1])
- en: flist.append("%s %s" % (fname,c))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: flist.append("%s %s" % (fname,c))
- en: 'for j in range(19):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'for j in range(19):'
- en: d = augment(wav)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: d = augment(wav)
- en: fname = base+("_%04d.wav" % j)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: fname = base+("_%04d.wav" % j)
- en: ❸ write(fname, wav[0], d.astype(wav[1].dtype))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ write(fname, wav[0], d.astype(wav[1].dtype))
- en: flist.append("%s %s" % (fname,c))
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: flist.append("%s %s" % (fname,c))
- en: random.shuffle(flist)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(flist)
- en: 'with open("augmented_%s_filelist.txt" % typ,"w") as f:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open("augmented_%s_filelist.txt" % typ,"w") as f:'
- en: 'for z in flist:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'for z in flist:'
- en: f.write("%s\n" % z)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: f.write("%s\n" % z)
- en: '*Listing 15-3: Augmenting the ESC-10 dataset, part 2*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-3：增强ESC-10数据集，第二部分*'
- en: The function loops over all the filenames in the given list (`src_list`), which
    will be either train or test. The filename is separated from the class label,
    and then the file is read from disk ❶. As indicated in the comment, `wav` is a
    list of two elements. The first is the sampling rate in Hz (cycles per second).
    This is how often the analog waveform was digitized to produce the *.wav* file.
    For ESC-10, the sampling rate is always 44,100 Hz, which is the standard rate
    for a compact disc. The second element is a NumPy array containing the actual
    digitized sound samples. These are the values we’ll augment to produce new data
    files.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数遍历给定列表中的所有文件名（`src_list`），该列表可以是训练集或测试集。文件名与类别标签分开，然后从磁盘读取文件❶。如注释所示，`wav`是一个包含两个元素的列表。第一个是采样率（单位：Hz，每秒周期数），即模拟波形被数字化生成*.wav*文件的频率。对于ESC-10数据集，采样率始终为44,100
    Hz，这是CD的标准采样率。第二个元素是一个NumPy数组，包含实际的数字化声音样本。这些值将被增强以生成新的数据文件。
- en: After setting up some output pathnames, we write the original sound sample to
    the augmented directory ❷. Then, we start a loop to generate 19 more augmented
    versions of the current sound sample. The augmented dataset, as a whole, will
    be 20 times larger, for a total of 8,000 sound files, 6,400 for training and 1,600
    for testing. Note, the sound samples for an augmented source file are assigned
    to `d`. The new sound file is written to disk using the sample rate of 44,100
    Hz and the augmented data matching the datatype of the source ❸.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置一些输出路径后，我们将原始声音样本写入增强目录❷。然后，我们开始一个循环，生成当前声音样本的另外19个增强版本。增强后的数据集整体上会变大20倍，总共有8,000个声音文件，其中6,400个用于训练，1,600个用于测试。请注意，增强源文件的声音样本分配给`d`。新声音文件以44,100
    Hz的采样率写入磁盘，并且增强数据与源数据的类型相匹配❸。
- en: As we create the augmented sound files, we also keep track of the filename and
    class and write them to a new file list. Here `typ` is a string indicating train
    or test.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建增强声音文件时，我们还会跟踪文件名和类别，并将其写入一个新的文件列表。这里`typ`是一个字符串，表示训练（train）或测试（test）。
- en: 'This function calls yet another function, `augment`. This is the function that
    generates an augmented version of a single sound file by randomly applying some
    subset of the four augmentation strategies mentioned previously: shifting, noise,
    pitch shifting, or time-shifting. Some or all of these might be used for any call
    to `augment`. The `augment` function itself is shown in [Listing 15-4](ch15.xhtml#ch15lis4).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数调用了另一个函数`augment`。这个函数通过随机应用先前提到的四种增强策略中的某些子集（包括移位、噪声、音高移位或时间移位），生成单个声音文件的增强版本。任何对`augment`的调用可能会使用这些策略中的某些或全部。`augment`函数本身见于[清单
    15-4](ch15.xhtml#ch15lis4)。
- en: 'def augment(wav):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'def augment(wav):'
- en: sr = wav[0]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: sr = wav[0]
- en: d = wav[1].astype("float32")
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: d = wav[1].astype("float32")
- en: '❶ if (random.random() < 0.5):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ 如果 (random.random() < 0.5):'
- en: s = int(sr/4.0*(np.random.random()-0.5))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: s = int(sr/4.0*(np.random.random()-0.5))
- en: d = np.roll(d,s)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: d = np.roll(d,s)
- en: 'if (s < 0):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (s < 0):'
- en: d[s:] = 0
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: d[s:] = 0
- en: 'else:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: d[:s] = 0
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: d[:s] = 0
- en: '❷ if (random.random() < 0.5):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ 如果 (random.random() < 0.5):'
- en: d += 0.1*(d.max()-d.min())*np.random.random(d.shape[0])
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: d += 0.1*(d.max()-d.min())*np.random.random(d.shape[0])
- en: '❸ if (random.random() < 0.5):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ 如果 (random.random() < 0.5):'
- en: pf = 20.0*(np.random.random()-0.5)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: pf = 20.0*(np.random.random()-0.5)
- en: d = rosa.effects.pitch_shift(d, sr, pf)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: d = rosa.effects.pitch_shift(d, sr, pf)
- en: '❹ if (random.random() < 0.5):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ 如果 (random.random() < 0.5):'
- en: rate = 1.0 + (np.random.random()-0.5)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: rate = 1.0 + (np.random.random()-0.5)
- en: d = rosa.effects.time_stretch(d,rate)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: d = rosa.effects.time_stretch(d,rate)
- en: 'if (d.shape[0] > wav[1].shape[0]):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (d.shape[0] > wav[1].shape[0]):'
- en: d = d[:wav[1].shape[0]]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: d = d[:wav[1].shape[0]]
- en: 'else:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: w = np.zeros(wav[1].shape[0], dtype="float32")
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: w = np.zeros(wav[1].shape[0], dtype="float32")
- en: w[:d.shape[0]] = d
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: w[:d.shape[0]] = d
- en: d = w.copy()
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: d = w.copy()
- en: return d
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: return d
- en: '*Listing 15-4: Augmenting the ESC-10 dataset, part 3*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-4: 增强ESC-10数据集，第三部分*'
- en: This function separates the samples (`d`) from the sample rate (`sr`) and makes
    sure the samples are floating-point numbers. For ESC-10, the source samples are
    all of type `int16` (signed 16-bit integers). Next come four `if` statements.
    Each one asks for a single random float, and if that float is less than 0.5, we
    execute the body of the `if`. This means that we apply each possible augmentation
    with a probability of 50 percent.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将样本（`d`）与采样率（`sr`）分开，并确保样本是浮动点数。对于ESC-10，源样本都是`int16`类型（有符号16位整数）。接下来是四个`if`语句。每个语句会生成一个随机浮动数，如果该浮动数小于0.5，我们就会执行`if`的主体。这意味着我们以50%的概率应用每种可能的增强。
- en: The first `if` shifts the sound samples in time ❶ by rolling the NumPy array,
    a vector, by some number of samples, `s`. This value amounts to at most an eighth
    of a second, `sr/4.0`. Note that the shift can be positive or negative. The quantity
    `sr/4.0` is the number of samples in a quarter of a second. However, the random
    float is in the range [*–*0.5,+0.5], so the ultimate shift is at most an eighth
    of a second. If the shift is negative, we need to zero samples at the end of the
    data; otherwise, we zero samples at the start.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`if`通过滚动NumPy数组（向量）`d`，按一定数量的样本`s`来移位声音样本❶。这个值最多为八分之一秒，即`sr/4.0`。注意，移位可以是正向或负向。`sr/4.0`表示四分之一秒的样本数。然而，随机浮动数的范围是[*–*0.5,
    +0.5]，因此最终的移位最大为八分之一秒。如果移位为负，我们需要将数据末尾的样本置零；否则，我们将数据开始的样本置零。
- en: Random noise is added by literally adding a random value of up to one-tenth
    of the range of the audio signal back in ❷. When played, this adds hiss, as you
    might hear on an old cassette tape.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 随机噪声是通过将一个随机值（最大为音频信号范围的十分之一）加回去来添加的❷。播放时，这会产生嘶嘶声，类似于老式卡带录音带上的噪声。
- en: Next comes shifting the pitch of the sample by using `librosa`. The pitch shift
    is expressed in musical steps, or fractions thereof. We randomly pick a float
    in the range [*–*10,+10] (`pf`) and pass it along with the data (`d`) and sampling
    rate (`sr`) to the `librosa` `pitch_shift` effect function ❸.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来使用`librosa`进行音高移位。音高移位以音符步进或其分数表示。我们在范围[*–*10,+10]内随机选取一个浮动值（`pf`），并将其与数据（`d`）和采样率（`sr`）一起传递给`librosa`的`pitch_shift`效果函数
    ❸。
- en: The last augmentation uses the `librosa` function to stretch or compress time
    (`time_stretch`) ❹. We adjust using an amount of time (`rate`) that is in the
    range [*–*0.5,+0.5]. If time was stretched, we need to chop off the extra samples
    to ensure that the sample length remains constant. If time was compressed, we
    need to add zero samples at the end.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的增强操作使用`librosa`函数进行时间拉伸或压缩（`time_stretch`）❹。我们使用一个在[*–*0.5,+0.5]范围内的时间量（`rate`）来进行调整。如果时间被拉伸，我们需要截断多余的样本，以确保样本长度保持不变。如果时间被压缩，我们需要在末尾添加零样本。
- en: Lastly, we return the new, augmented samples.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回新的、增强后的样本。
- en: Running the code in [Listing 15-2](ch15.xhtml#ch15lis2) creates a new *augmented*
    data directory with subdirectories *train* and *test*. These are the raw sound
    files that we’ll work with going forward. I encourage you to listen to some of
    them to understand what the augmentations have done. The filenames should help
    you quickly tell the originals from the augmentations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 运行[Listing 15-2](ch15.xhtml#ch15lis2)中的代码会创建一个新的*增强*数据目录，包含子目录*train*和*test*。这些是我们接下来要使用的原始音频文件。我鼓励你听听其中的一些文件，以了解增强操作所做的修改。文件名应该能帮助你快速区分原始文件和增强文件。
- en: Preprocessing Our Data
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Are we ready to start building models? Not yet. Our experience told us that
    the dataset was too small, and we augmented accordingly. However, we haven’t yet
    turned the raw data into something we can pass to a model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好开始构建模型了吗？还没有。我们的经验告诉我们，数据集太小了，因此我们进行了增强。但我们还没有将原始数据转化成可以传递给模型的格式。
- en: A first thought is to use the raw sound samples. These are already vectors representing
    the audio signal, with the time between the samples set by the sampling rate of
    44,100 Hz. But we don’t want to use them as they are. The samples are all exactly
    five seconds long. At 44,100 samples per second, that means each sample is a vector
    of 44,100 × 5 = 220,500 samples. That’s too long for us to work with effectively.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个初步的想法是使用原始音频样本。这些样本已经是代表音频信号的向量，样本之间的时间间隔由采样率44,100 Hz确定。但我们不想直接使用这些样本。每个样本都是精确的五秒钟长。以每秒44,100个样本计算，这意味着每个样本是一个包含44,100
    × 5 = 220,500个样本的向量。这个长度对于我们有效处理来说太长了。
- en: With a bit more thought, we might be able to convince ourselves that distinguishing
    between a crying baby and a barking dog might not need such a high sampling rate.
    What if instead of keeping all the samples, we kept only every 100th sample? Moreover,
    do we really need five seconds’ worth of data to identify the sounds? What if
    we kept only the first two seconds worth?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 经过进一步思考，我们或许能够说服自己，区分哭泣的婴儿和叫喊的狗可能不需要如此高的采样率。如果我们不保留所有样本，而是仅保留每第100个样本呢？此外，我们真的需要五秒钟的音频数据来识别声音吗？如果我们只保留前两秒钟的数据呢？
- en: Let’s keep only the first two seconds of each sound file; that’s 88,200 samples.
    And let’s keep only every 100th sample, so each sound file now becomes a vector
    of 882 elements. That’s hardly more than an unraveled MNIST digit image, and we
    know we can work with those.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只保留每个音频文件的前两秒钟，即88,200个样本。而且我们只保留每第100个样本，这样每个音频文件现在就变成了一个包含882个元素的向量。这几乎与解开的MNIST数字图像一样，我们知道我们可以有效地处理这些数据。
- en: '[Listing 15-5](ch15.xhtml#ch15lis5) has the code to build the actual initial
    version of the dataset we’ll use to build the models.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 15-5](ch15.xhtml#ch15lis5)包含了构建我们将用来建立模型的实际数据集初始版本的代码。'
- en: import os
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import random
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: import numpy as np
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from scipy.io.wavfile import read
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.io.wavfile import read
- en: 'sr = 44100 # Hz'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'sr = 44100 # Hz'
- en: N = 2*sr   # number of samples to keep
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: N = 2*sr    # 保留的样本数
- en: w = 100    # every 100
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: w = 100    # 每隔100
- en: afiles = [i[:-1] for i in open("augmented_train_filelist.txt")]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: afiles = [i[:-1] for i in open("augmented_train_filelist.txt")]
- en: trn = np.zeros((len(afiles),N//w,1), dtype="int16")
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: trn = np.zeros((len(afiles),N//w,1), dtype="int16")
- en: lbl = np.zeros(len(afiles), dtype="uint8")
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: lbl = np.zeros(len(afiles), dtype="uint8")
- en: 'for i,t in enumerate(afiles):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,t in enumerate(afiles):'
- en: ❶ f,c = t.split()
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f,c = t.split()
- en: trn[i,:,0] = read(f)[1][:N:w]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: trn[i,:,0] = read(f)[1][:N:w]
- en: lbl[i] = int(c)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: lbl[i] = int(c)
- en: np.save("esc10_raw_train_audio.npy", trn)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_train_audio.npy", trn)
- en: np.save("esc10_raw_train_labels.npy", lbl)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_train_labels.npy", lbl)
- en: afiles = [i[:-1] for i in open("augmented_test_filelist.txt")]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: afiles = [i[:-1] for i in open("augmented_test_filelist.txt")]
- en: tst = np.zeros((len(afiles),N//w,1), dtype="int16")
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: tst = np.zeros((len(afiles),N//w,1), dtype="int16")
- en: lbl = np.zeros(len(afiles), dtype="uint8")
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: lbl = np.zeros(len(afiles), dtype="uint8")
- en: 'for i,t in enumerate(afiles):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,t in enumerate(afiles):'
- en: f,c = t.split()
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: f,c = t.split()
- en: tst[i,:,0] = read(f)[1][:N:w]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: tst[i,:,0] = read(f)[1][:N:w]
- en: lbl[i] = int(c)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: lbl[i] = int(c)
- en: np.save("esc10_raw_test_audio.npy", tst)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_test_audio.npy", tst)
- en: np.save("esc10_raw_test_labels.npy", lbl)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_test_labels.npy", lbl)
- en: '*Listing 15-5: Building reduced samples dataset*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-5：构建减少样本数据集*'
- en: This code builds train and test NumPy files containing the raw data. The data
    is from the augmented sound files we built in [Listing 15-2](ch15.xhtml#ch15lis2).
    The file list contains the file location and class label ❶. We load each file
    in the list and put it into an array, either the train or test array.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码构建了包含原始数据的训练和测试 NumPy 文件。这些数据来自我们在[示例 15-2](ch15.xhtml#ch15lis2)中构建的增强音频文件。文件列表包含文件位置和类别标签❶。我们加载列表中的每个文件，并将其放入数组中，分别是训练数组或测试数组。
- en: 'We have a one-dimensional feature vector and a number of train or test files,
    so we might expect we need a two-dimensional array to store our data, either 6400
    × 882 for the training set or 1600 × 882 for the test set. However, we know we’ll
    ultimately be working with Keras, and we know that Keras wants a dimension for
    the number of channels, so we define the arrays to be 6400 × 882 × 1 and 1600
    × 882 × 1 instead. The most substantial line in this code is the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个一维特征向量和一些训练或测试文件，因此我们可能需要一个二维数组来存储数据，训练集为 6400 × 882 或测试集为 1600 × 882。然而，我们知道最终会使用
    Keras，而且 Keras 需要一个表示通道数的维度，因此我们将数组定义为 6400 × 882 × 1 和 1600 × 882 × 1。此代码中最重要的一行是：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It reads the current sound file, keeps only the sound samples (`[1]`), and from
    the sound samples keeps only the first two seconds, worth at every 100th sample,
    `[:N:w]`. Spend a little time with this code. If you’re confused, I’d suggest
    experimenting with NumPy at the interactive Python prompt to understand what it’s
    doing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 它读取当前的音频文件，仅保留音频样本（`[1]`），并从音频样本中仅保留前两秒，每隔第100个样本提取一次，`[:N:w]`。花些时间理解这段代码。如果你感到困惑，我建议在交互式
    Python 环境中尝试使用 NumPy 来理解它的作用。
- en: In the end, we have train and test files for the 882 element vectors and associated
    labels. We’ll build our first models with these. [Figure 15-1](ch15.xhtml#ch15fig1)
    shows the resulting vector for a crying baby.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们得到了训练和测试文件，这些文件包含 882 元素的向量和相关标签。我们将使用这些数据构建我们的第一个模型。[图 15-1](ch15.xhtml#ch15fig1)显示了哭泣婴儿的特征向量。
- en: '![image](Images/15fig01.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/15fig01.jpg)'
- en: '*Figure 15-1: Feature vector for a crying baby*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-1：哭泣婴儿的特征向量*'
- en: The x-axis is sample number (think “time”), and the y-axis is the sample value.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: x 轴是样本编号（可以理解为“时间”），y 轴是样本值。
- en: Classifying the Audio Features
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类音频特征
- en: We have our training and test sets. Let’s build some models and see how they
    do. Since we have feature vectors, we can start quickly with classical models.
    After those, we can build some one-dimensional convolutional networks and see
    if they perform any better.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了训练集和测试集。让我们构建一些模型，看看它们的表现如何。由于我们有特征向量，可以很快地开始使用经典模型。之后，我们可以构建一些一维卷积神经网络，看看它们是否表现得更好。
- en: Using Classical Models
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用经典模型
- en: We can test the same suite of classical models we used in [Chapter 7](ch07.xhtml#ch07)
    with the breast cancer dataset. [Listing 15-6](ch15.xhtml#ch15lis6) has the setup
    code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在[第 7 章](ch07.xhtml#ch07)中使用的同一组经典模型来测试乳腺癌数据集。[示例 15-6](ch15.xhtml#ch15lis6)包含了设置代码。
- en: import numpy as np
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import LinearSVC
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import LinearSVC
- en: x_train = np.load("esc10_raw_train_audio.npy")[:,:,0]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_raw_train_audio.npy")[:,:,0]
- en: y_train = np.load("esc10_raw_train_labels.npy")
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_raw_train_labels.npy")
- en: (*\pagebreak*)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: x_test  = np.load("esc10_raw_test_audio.npy")[:,:,0]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("esc10_raw_test_audio.npy")[:,:,0]
- en: y_test  = np.load("esc10_raw_test_labels.npy")
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_raw_test_labels.npy")
- en: ❶ x_train = (x_train.astype('float32') + 32768) / 65536
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x_train = (x_train.astype('float32') + 32768) / 65536
- en: x_test = (x_test.astype('float32') + 32768) / 65536
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = (x_test.astype('float32') + 32768) / 65536
- en: train(x_train, y_train, x_test, y_test)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: train(x_train, y_train, x_test, y_test)
- en: '*Listing 15-6: Classifying the audio features with classical models, part 1*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-6：使用经典模型对音频特征进行分类，第1部分*'
- en: Here we import the necessary model types, load the dataset, scale it, and then
    call a `train` function that we’ll introduce shortly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入必要的模型类型，加载数据集，进行缩放，然后调用一个`train`函数，我们稍后会介绍。
- en: Scaling is crucial here. Consider the y-axis range for [Figure 15-1](ch15.xhtml#ch15fig1).
    It goes from about –4000 to 4000\. We need to scale the data so that the range
    is smaller and the values are closer to being centered around 0\. Recall, for
    the MNIST and CIFAR-10 datasets, we divided by the maximum value to scale to [0,1].
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放在这里至关重要。考虑[图15-1](ch15.xhtml#ch15fig1)的y轴范围。它从大约–4000到4000。我们需要对数据进行缩放，以便范围更小，且数值更接近于围绕0中心。回想一下，对于MNIST和CIFAR-10数据集，我们通过最大值进行除法，将数据缩放到[0,1]。
- en: The sound samples are 16-bit signed integers. This means the full range of values
    they can take on covers [*–*32,768,+32,767]. If we make the samples floats, add
    32,768, and then divide by 65,536 (twice the lower value) ❶, we’ll get samples
    in the range [0,1), which is what we want.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 声音样本是16位带符号整数。这意味着它们可以取的值的完整范围是[*–*32,768,+32,767]。如果我们将样本转换为浮点数，添加32,768，然后除以65,536（即两倍的最小值）❶，我们将得到范围在[0,1)之间的样本，这正是我们所需要的。
- en: Training and evaluating the classical models is straightforward, as shown in
    [Listing 15-7](ch15.xhtml#ch15lis7).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估经典模型是直观的，如[示例 15-7](ch15.xhtml#ch15lis7)所示。
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: score = 100.0*clf.score(x_test, y_test)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: score = 100.0*clf.score(x_test, y_test)
- en: print("score = %0.2f%%" % score)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: print("score = %0.2f%%" % score)
- en: 'def train(x_train, y_train, x_test, y_test):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 'def train(x_train, y_train, x_test, y_test):'
- en: 'print("Nearest Centroid          : ", end='''')'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("最近质心          : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: 'print("k-NN classifier (k=3)     : ", end='''')'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("k-NN分类器（k=3）     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))
- en: 'print("k-NN classifier (k=7)     : ", end='''')'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("k-NN分类器（k=7）     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))
- en: 'print("Naive Bayes (Gaussian)    : ", end='''')'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("朴素贝叶斯（高斯）    : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, GaussianNB())
- en: 'print("Random Forest (trees=  5) : ", end='''')'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木= 5) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: 'print("Random Forest (trees= 50) : ", end='''')'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木= 50) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: 'print("Random Forest (trees=500) : ", end='''')'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木=500) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=500))
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=500))
- en: 'print("Random Forest (trees=1000): ", end='''')'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木=1000): ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=1000))
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=1000))
- en: 'print("LinearSVM (C=0.01)        : ", end='''')'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=0.01)        : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
- en: 'print("LinearSVM (C=0.1)         : ", end='''')'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=0.1)         : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
- en: 'print("LinearSVM (C=1.0)         : ", end='''')'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=1.0)         : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
- en: 'print("LinearSVM (C=10.0)        : ", end='''')'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=10.0)        : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
- en: '*Listing 15-7: Classifying the audio features with classical models, part 2*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-7：使用经典模型对音频特征进行分类，第2部分*'
- en: 'The `train` function creates the particular model instances and then calls
    `run`. We saw this same code structure in [Chapter 7](ch07.xhtml#ch07). The `run`
    function uses `fit` to train the model and `score` to score the model on the test
    set. For the time being, we’ll evaluate the models based solely on their overall
    accuracy (the score). Running this code produces output like this:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`函数创建特定的模型实例，然后调用`run`。我们在[第7章](ch07.xhtml#ch07)中看到过这种相同的代码结构。`run`函数使用`fit`来训练模型，并使用`score`来在测试集上评估模型。暂时，我们将仅根据模型的总体准确度（得分）来评估这些模型。运行这段代码会产生类似如下的输出：'
- en: '[PRE2]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see very quickly that the classical models have performed terribly. Many
    of them are essentially guessing the class label. There are 10 classes, so random
    chance guessing should have an accuracy around 10 percent. The best-performing
    classical model is a Random Forest with 1,000 trees, but even that is performing
    at only 34.44 percent—far too low an overall accuracy to make the model one we’d
    care to use in most cases. The dataset is not a simple one, at least not for old-school
    approaches. Somewhat surprisingly, the Gaussian Naïve Bayes model is right 28
    percent of the time. Recall that the Gaussian Naïve Bayes expects the samples
    to be independent from one another. Here the independence assumption between the
    sound samples for a particular test input is not valid. The feature vector, in
    this case, represents a signal evolving in time, not a collection of features
    that are independent of each other.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就能看到经典模型表现得非常糟糕。其中许多模型基本上是在猜测类别标签。总共有10个类别，所以随机猜测的准确率应该大约为10%。表现最好的经典模型是一个拥有1,000棵树的随机森林，但即使如此，它的表现也仅为34.44%——总体准确率远远不足以使该模型在大多数情况下可用。数据集并不简单，至少对于老式方法来说并非如此。有些令人惊讶的是，高斯朴素贝叶斯模型的准确率达到了28%。请回忆，高斯朴素贝叶斯模型期望样本彼此独立。在这里，特定测试输入的声音样本之间的独立性假设是不成立的。在这种情况下，特征向量表示的是随时间演变的信号，而不是彼此独立的特征集合。
- en: 'The models that failed the most are Nearest Centroid, *k*-NN, and the linear
    SVMs. We have a reasonably high-dimensional input, 882 elements, but only 6,400
    of them in the training set. That is likely too few samples for the nearest neighbor
    classifiers to make use of—the feature space is too sparsely populated. Once again,
    the curse of dimensionality is rearing its ugly head. The linear SVM fails because
    the features seem not to be linearly separable. We did not try an RBF (Gaussian
    kernel) SVM, but we’ll leave that as an exercise for the reader. If you do try
    it, remember that there are now two hyperparameters to tune: *C* and *γ*.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 失败最多的模型是最近质心、*k*-NN和线性SVM。我们有一个相当高维的输入，882个元素，但训练集中只有6,400个元素。这对于最近邻分类器来说可能是太少的样本，因为特征空间的稀疏性太高。再次，维度灾难的问题又出现了。线性SVM失败是因为特征似乎不是线性可分的。我们没有尝试RBF（高斯核）SVM，但我们将其留给读者作为一个练习。如果你尝试了它，请记住现在有两个超参数要调整：*C*和*γ*。
- en: Using a Traditional Neural Network
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用传统神经网络
- en: We haven’t yet tried a traditional neural network. We could use the sklearn
    `MLPClassifier` class as we did before, but this is a good time to show how to
    implement a traditional network in Keras. [Listing 15-8](ch15.xhtml#ch15lis8)
    has the code.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有尝试传统的神经网络。我们可以像之前那样使用sklearn的`MLPClassifier`类，但现在是展示如何在Keras中实现传统网络的好时机。[列表15-8](ch15.xhtml#ch15lis8)包含了代码。
- en: import keras
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.models import Sequential
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras import backend as K
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: from keras import backend as K
- en: import numpy as np
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: batch_size = 32
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 32
- en: num_classes = 10
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: num_classes = 10
- en: epochs = 16
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 16
- en: nsamp = (882,1)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: nsamp = (882,1)
- en: x_train = np.load("esc10_raw_train_audio.npy")
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_raw_train_audio.npy")
- en: y_train = np.load("esc10_raw_train_labels.npy")
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_raw_train_labels.npy")
- en: x_test  = np.load("esc10_raw_test_audio.npy")
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: x_test  = np.load("esc10_raw_test_audio.npy")
- en: y_test  = np.load("esc10_raw_test_labels.npy")
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: y_test  = np.load("esc10_raw_test_labels.npy")
- en: x_train = (x_train.astype('float32') + 32768) / 65536
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = (x_train.astype('float32') + 32768) / 65536
- en: x_test = (x_test.astype('float32') + 32768) / 65536
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = (x_test.astype('float32') + 32768) / 65536
- en: y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = keras.utils.to_categorical(y_train, num_classes)
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = keras.utils.to_categorical(y_test, num_classes)
- en: model = Sequential()
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Dense(1024, activation='relu', input_shape=nsamp))
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(1024, activation='relu', input_shape=nsamp))
- en: model.add(Dropout(0.5))
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(512, activation='relu'))
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(512, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Flatten())
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(loss=keras.losses.categorical_crossentropy,
- en: optimizer=keras.optimizers.Adam(),
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=keras.optimizers.Adam(),
- en: metrics=['accuracy'])
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'])
- en: model.fit(x_train, y_train,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(x_train, y_train,
- en: batch_size=batch_size,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size,
- en: epochs=epochs,
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=epochs,
- en: verbose=0,
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=0,
- en: validation_data=(x_test, y_test))
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=(x_test, y_test))
- en: (*\pagebreak*)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: score = model.evaluate(x_test, y_test, verbose=0)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test, y_test, verbose=0)
- en: print('Test accuracy:', score[1])
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: print('测试准确率:', score[1])
- en: '*Listing 15-8: A traditional neural network in Keras*'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-8：Keras 中的传统神经网络*'
- en: 'After loading the necessary modules, we load the data itself and scale it as
    we did for the classical models. Next, we build the model architecture. We need
    only `Dense` layers and `Dropout` layers. We do put in a `Flatten` layer to eliminate
    the extra dimension (note the shape of `nsamp`) before the final softmax output.
    Unfortunately, this model does not improve things for us: we achieve an accuracy
    of only 27.6 percent.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载必要的模块后，我们加载数据并像处理经典模型时一样进行缩放。接下来，我们构建模型架构。我们只需要`Dense`层和`Dropout`层。为了消除额外的维度（注意`nsamp`的形状），我们在最终的
    softmax 输出之前加了一个`Flatten`层。不幸的是，这个模型对我们没有改善效果：我们只得到了27.6%的准确率。
- en: Using a Convolutional Neural Network
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用卷积神经网络
- en: Classical models and the traditional neural network don’t cut it. We should
    not be too surprised, but it was easy to give them a try. Let’s move on and apply
    a one-dimensional convolutional neural network to this dataset to see if it performs
    any better.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型和传统神经网络无法胜任。我们不应感到太惊讶，但尝试它们也很简单。接下来，我们将应用一维卷积神经网络（CNN）来看看它是否表现得更好。
- en: We haven’t worked with one-dimensional CNNs yet. Besides the structure of the
    input data, the only difference is that we replace calls to `Conv2D` and `MaxPooling2D`
    with calls to `Conv1D` and `MaxPooling1D`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有使用过一维的 CNN。除了输入数据的结构外，唯一的区别是我们将`Conv2D`和`MaxPooling2D`的调用替换为`Conv1D`和`MaxPooling1D`的调用。
- en: The code for the first model we’ll try is shown in [Listing 15-9](ch15.xhtml#ch15lis9).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试的第一个模型的代码显示在[清单 15-9](ch15.xhtml#ch15lis9)中。
- en: import keras
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.models import Sequential
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras.layers import Conv1D, MaxPooling1D
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Conv1D, MaxPooling1D
- en: import numpy as np
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: batch_size = 32
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 32
- en: num_classes = 10
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: num_classes = 10
- en: epochs = 16
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 16
- en: nsamp = (882,1)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: nsamp = (882,1)
- en: x_train = np.load("esc10_raw_train_audio.npy")
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_raw_train_audio.npy")
- en: y_train = np.load("esc10_raw_train_labels.npy")
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_raw_train_labels.npy")
- en: x_test  = np.load("esc10_raw_test_audio.npy")
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("esc10_raw_test_audio.npy")
- en: y_test  = np.load("esc10_raw_test_labels.npy")
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_raw_test_labels.npy")
- en: x_train = (x_train.astype('float32') + 32768) / 65536
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = (x_train.astype('float32') + 32768) / 65536
- en: x_test = (x_test.astype('float32') + 32768) / 65536
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = (x_test.astype('float32') + 32768) / 65536
- en: y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = keras.utils.to_categorical(y_train, num_classes)
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = keras.utils.to_categorical(y_test, num_classes)
- en: model = Sequential()
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(32, kernel_size=3, activation='relu',
- en: input_shape=nsamp))
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape=nsamp))
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling1D(pool_size=3))
- en: model.add(Dropout(0.25))
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Flatten())
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(512, activation='relu'))
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(512, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(loss=keras.losses.categorical_crossentropy,
- en: optimizer=keras.optimizers.Adam(),
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=keras.optimizers.Adam(),
- en: metrics=['accuracy'])
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'])
- en: history = model.fit(x_train, y_train,
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: history = model.fit(x_train, y_train,
- en: batch_size=batch_size,
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size,
- en: epochs=epochs,
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=epochs,
- en: verbose=1,
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=1,
- en: validation_data=(x_test[:160], y_test[:160]))
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=(x_test[:160], y_test[:160]))
- en: score = model.evaluate(x_test[160:], y_test[160:], verbose=0)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test[160:], y_test[160:], verbose=0)
- en: print('Test accuracy:', score[1])
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: print('测试准确率:', score[1])
- en: '*Listing 15-9: A 1D CNN in Keras*'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-9：Keras 中的 1D CNN*'
- en: This model loads and preprocesses the dataset as before. This architecture,
    which we’ll call the *shallow* architecture, has a single convolutional layer
    of 32 filters with a kernel size of 3\. We’ll vary this kernel size in the same
    way we tried different 2D kernel sizes for the MNIST models. Following the `Conv1D`
    layer is a single max-pooling layer with a pool kernel size of 3\. `Dropout` and
    `Flatten` layers come next before a single `Dense` layer of 512 nodes with dropout.
    A softmax layer completes the architecture.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型和之前一样加载并预处理数据集。这个架构，我们称之为*浅层*架构，只有一个卷积层，包含32个大小为3的滤波器。我们会像尝试不同的2D卷积核大小一样，变化这个卷积核大小。`Conv1D`层之后是一个大小为3的最大池化层。接下来是`Dropout`层和`Flatten`层，最后是一个包含512个节点的`Dense`层，并添加了
    dropout。最后是一个 softmax 层来完成架构。
- en: We’ll train for 16 epochs using a batch size of 32\. We’ll keep the training
    history so we can examine the losses and validation performance as a function
    of epoch. There are 1,600 test samples. We’ll use 10 percent for the training
    validation and the remaining 90 percent for the overall accuracy. Finally, we’ll
    vary the `Conv1D` kernel size from 3 to 33 in an attempt to find one that works
    well with the training data.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Let’s define four other architectures. We’ll refer to them as *medium*, *deep0*,
    *deep1*, and *deep2*. With no prior experience working with this data, it makes
    sense to try multiple architectures. At present, there’s no way to know ahead
    of time what the best architecture is for a new dataset. All we have is our previous
    experience.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 15-10](ch15.xhtml#ch15lis10) lists the specific architectures, separated
    by comments.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: medium
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=nsamp))
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(512, activation='relu'))
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: deep0
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=nsamp))
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(512, activation='relu'))
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: deep1
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=nsamp))
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(512, activation='relu'))
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: deep2
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=nsamp))
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(512, activation='relu'))
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-10: Different 1D CNN architect*'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: If we train multiple models, varying the first `Conv1D` kernel size each time,
    we get the results in [Table 15-1](ch15.xhtml#ch15tab1). We’ve highlighted the
    best-performing model for each architecture.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-1:** Test Set Accuracies by Convolutional Kernel Size and Model
    Architecture'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '| **Kernel size** | **Shallow** | **Medium** | **Deep0** | **Deep1** | **Deep2**
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| 3 | **44.51** | 41.39 | **48.75** | **54.03** | 9.93 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| 5 | 43.47 | 41.74 | 44.72 | 53.96 | 48.47 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| 7 | 38.47 | 40.97 | 46.18 | 52.64 | 49.31 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| 9 | 41.46 | **43.06** | 46.88 | 48.96 | 9.72 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| 11 | 39.65 | 40.21 | 45.21 | 52.99 | 10.07 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| 13 | 42.71 | 41.67 | 46.53 | 50.56 | **52.57** |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| 15 | 40.00 | 42.78 | 46.53 | 50.14 | 47.08 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| 33 | 27.57 | 42.22 | 41.39 | 48.75 | 9.86 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: Looking at [Table 15-1](ch15.xhtml#ch15tab1), we see a general trend of accuracy
    improving as the model depth increases. However, at the deep2 model, things start
    to fall apart. Some of the models fail to converge, showing an accuracy equivalent
    to random guessing. The deep1 model is the best performing for all kernel sizes.
    When looking across by kernel size, the kernel with width 3 is the best performing
    for three of the five architectures. All of this implies that the best combination
    for the 1D CNNs is to use an initial kernel of width 3 and the deep1 architecture.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: We trained this architecture for only 16 epochs. Will things improve if we train
    for more? Let’s train the deep1 model for 60 epochs and plot the training and
    validation loss and error to see how they converge (or don’t). Doing this produces
    [Figure 15-2](ch15.xhtml#ch15fig2), where we see the training and validation loss
    (top) and error (bottom) as a function of epoch.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/15fig02.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-2: Training and validation loss (top) and error (bottom) for the
    deep1 architecture*'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Immediately, we should pick up on the explosion of the loss for the validation
    set. The training loss is continually decreasing until after about epoch 18 or
    so; then the validation loss goes up and becomes oscillatory. This is a clear
    example of overfitting. The likely source of this overfitting is our limited training
    set size, only 6,400 samples, even after data augmentation. The validation error
    remains more or less constant after initially decreasing. The conclusion is that
    we cannot expect to do much better than an overall accuracy of about 54 percent
    for this dataset using one-dimensional vectors.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: If we want to improve, we need to be more expressive with our dataset. Fortunately
    for us, we have another preprocessing trick up our sleeves.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Spectrograms
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s return to our augmented set of audio files. To build the dataset, we took
    the sound samples, keeping only two seconds’ worth and only every 100th sample.
    The best we could do is an accuracy of a little more than 50 percent.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: However, if we work with a small set of sound samples from an input audio file,
    say 200 milliseconds worth, we can use the vector of samples to calculate the
    *Fourier transform*. The Fourier transform of a signal measured at regular intervals
    tells us the frequencies that went into building the signal. Any signal can be
    thought of as the sum of many different sine and cosine waves. If the signal is
    composed of only a few waves, like the sound you might get from an instrument
    like the ocarina, then the Fourier transform will have essentially a few peaks
    at those frequencies. If the signal is complex, like speech or music, then the
    Fourier transform will have many different frequencies, leading to many different
    peaks.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fourier transform itself is complex-valued: each element has both a real
    and an imaginary component. You can write it as *a* + *bi*, where *a* and *b*
    are real numbers and ![Image](Images/394equ01.jpg). If we use the absolute value
    of these quantities, we’ll get a real number representing the energy of a particular
    frequency. This is called the *power spectrum* of the signal. A simple tone might
    have energy in only a few frequencies, while something like a cymbal crash or
    white noise will have energy more or less evenly distributed among all frequencies.
    [Figure 15-3](ch15.xhtml#ch15fig3) shows two power spectra.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/15fig03.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-3: Power spectrum of an ocarina (top) and cymbal (bottom)*'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: On the top is the spectrum of an ocarina, and on the bottom is a cymbal crash.
    As expected, the ocarina has energy in only a few frequencies, while the cymbal
    uses all the frequencies. The important point for us is that *visually* the spectra
    are quite different from each other. (The spectra were made with Audacity, an
    excellent open source audio processing tool.)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: We could use these power spectra as feature vectors, but they represent only
    the spectra of tiny slices of time. The sound samples are five seconds long. Instead
    of using a spectrum, we will use a *spectrogram*. The spectrogram is an image
    made up of columns that represent individual spectra. This means that the x-axis
    represents time and the y-axis represents frequency. The color of a pixel is proportional
    to the energy in that frequency at that time.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: In other words, a spectrogram is what we get if we orient the power spectra
    vertically and use color to represent intensity at a given frequency. With this
    approach, we can turn an entire sound sample into an image. For example, [Figure
    15-4](ch15.xhtml#ch15fig4) shows the spectrogram of a crying baby. Compare this
    to the feature vector of [Figure 15-1](ch15.xhtml#ch15fig1).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/15fig04.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-4: Spectrogram of a crying baby*'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'To create spectrograms of the augmented audio files, we need a new tool and
    a bit of code. The tool we need is called `sox`. It’s not a Python library, but
    a command line tool. Odds are that it is already installed if you are using our
    canonical Ubuntu Linux distribution. If not, you can install it:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ll use `sox` from inside a Python script to produce the spectrogram images
    we want. Each sound file becomes a new spectrogram image.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: The source code to process the training images is in [Listing 15-11](ch15.xhtml#ch15lis11).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: import os
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: from PIL import Image
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: rows = 100
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: cols = 160
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: ❶ flist = [i[:-1] for i in open("augmented_train_filelist.txt")]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: N = len(flist)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: img = np.zeros((N,rows,cols,3), dtype="uint8")
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: lbl = np.zeros(N, dtype="uint8")
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: p = []
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'for i,f in enumerate(flist):'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: src, c = f.split()
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: ❷ os.system("sox %s -n spectrogram" % src)
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: im = np.array(Image.open("spectrogram.png").convert("RGB"))
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: ❸ im = im[42:542,58:858,:]
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: im = Image.fromarray(im).resize((cols,rows))
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: img[i,:,:,:] = np.array(im)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: lbl[i] = int(c)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: p.append(os.path.abspath(src))
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: os.system("rm -rf spectrogram.png")
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: p = np.array(p)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: ❹ idx = np.argsort(np.random.random(N))
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: img = img[idx]
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: lbl = lbl[idx]
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: p = p[idx]
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: np.save("esc10_spect_train_images.npy", img)
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: np.save("esc10_spect_train_labels.npy", lbl)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: np.save("esc10_spect_train_paths.npy", p)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-11: Building the spectrograms*'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: We start by defining the size of the spectrogram. This is the input to our model,
    and we don’t want it to be too big because we’re limited in the size of the inputs
    we can process. We’ll settle for 100×160 pixels. We then load the training file
    list ❶ and create NumPy arrays to hold the spectrogram images and associated labels.
    The list `p` will hold the pathname of the source for each spectrogram in case
    we want to get back to the original sound file at some point. In general, it’s
    a good idea to preserve information to get back to the source of derived datasets.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Then we loop over the file list. We get the filename and class label and then
    call `sox`, passing in the source sound filename ❷. The `sox` application is sophisticated.
    The syntax here turns the given sound file into a spectrogram image with the name
    *spectrogram.png*. We immediately load the output spectrogram into `im`, making
    sure it’s an RGB file with no transparency layer (hence the call to `convert("RGB")`).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: The spectrogram created by `sox` has a border with frequency and time information.
    We want only the spectrogram image portion, so we subset the image ❸. We determined
    the indices we’re using empirically. It’s possible, but somewhat unlikely, that
    a newer version of `sox` will require tweaking these to avoid including any border
    pixels.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Next, we resize the spectrogram so that it fits in our 100×160 pixel array.
    This is downsampling, true, but hopefully enough characteristic information is
    still present to allow a model to learn the difference between classes. We keep
    the downsampled spectrogram and the associated class label and sound file path.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: When we’ve generated all the spectrograms, the loop ends, and we remove the
    final extraneous spectrogram PNG file. We convert the list of sound file paths
    to a NumPy array so we can store it in the same manner as the images and labels.
    Finally, we randomize the order of the images as a precaution against any implicit
    sorting that might group classes ❹. This is so that minibatches extracted sequentially
    are representative of the mix of classes as a whole. To conclude, we write the
    images, labels, and pathnames to disk. We repeat this entire process for the test
    set.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Are we able to visually tell the difference between the spectrograms of different
    classes? If we can do that easily, then we have a good shot of getting a model
    to tell the difference, too. [Figure 15-5](ch15.xhtml#ch15fig5) shows 10 spectrograms
    of the same class in each row.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/15fig05.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-5: Sample spectrograms for each class in ESC-10\. Each row shows
    10 examples from the same class.*'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Visually, we can usually tell the spectra apart, which is encouraging. With
    our spectrograms in hand, we are ready to try some 2D CNNs to see if they do better
    than the 1D CNNs.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Classifying Spectrograms
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To work with the spectrogram dataset, we need 2D CNNs. A possible starting point
    is to convert the shallow 1D CNN architecture to 2D by changing `Conv1D` to `Conv2D`,
    and `MaxPooling1D` to `MaxPooling2D`. However, if we do this, the resulting model
    has 30.7 million parameters, which is many more than we want to work with. Instead,
    let’s opt for a deeper architecture that has fewer parameters and then explore
    the effect of different first convolutional layer kernel sizes. The code is in
    [Listing 15-12](ch15.xhtml#ch15lis12).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: import keras
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import Sequential
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: batch_size = 16
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: num_classes = 10
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: epochs = 16
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: img_rows, img_cols = 100, 160
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: input_shape = (img_rows, img_cols, 3)
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: x_train = np.load("esc10_spect_train_images.npy")
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: y_train = np.load("esc10_spect_train_labels.npy")
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("esc10_spect_test_images.npy")
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("esc10_spect_test_labels.npy")
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: x_train = x_train.astype('float32') / 255
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: x_test = x_test.astype('float32') / 255
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(3,3), activation='relu',
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: input_shape=input_shape))
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.25))
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dropout(0.5))
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: optimizer=keras.optimizers.Adam(),
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: metrics=['accuracy'])
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: history = model.fit(x_train, y_train,
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: batch_size=batch_size, epochs=epochs,
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: verbose=0, validation_data=(x_test, y_test))
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: score = model.evaluate(x_test, y_test, verbose=0)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: print('Test accuracy:', score[1])
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: model.save("esc10_cnn_deep_3x3_model.h5")
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-12: Classifying spectrograms*'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: Here we are using a minibatch size of 16 for 16 epochs along with the Adam optimizer.
    The model architecture has two convolutional layers, a max-pooling layer with
    dropout, another convolutional layer, and a second max-pooling layer with dropout.
    There is a single dense layer of 128 nodes before the softmax output.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll test two kernel sizes for the first convolutional layer: 3 × 3 and 7
    × 7\. The 3 × 3 configuration is shown in [Listing 15-12](ch15.xhtml#ch15lis12).
    Replace `(3,3)` with `(7,7)` to alter the size. All the initial 1D convolutional
    runs used a single training of the model for evaluation. We know that because
    of random initialization, we’ll get slightly different results from training to
    training, even if nothing else changes. For the 2D CNNs, let’s train each model
    six times and present the overall accuracy as a mean ± standard error of the mean.
    Doing just this gives us the following overall accuracies:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '| **Kernel size** | **Score** |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| 3 × 3 | 78.78 ± 0.60% |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| 7 × 7 | 78.44 ± 0.72% |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: This indicates that there is no meaningful difference between using a 3 × 3
    initial convolutional layer kernel size or a 7 × 7\. Therefore, we’ll stick with
    3 × 3 going forward.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) shows the training and validation loss (top)
    and error (bottom) for one run of the 2D CNN trained on the spectrograms. As we
    saw in the 1D CNN case, after only a few epochs, the validation error starts to
    increase.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 'The 2D CNN performs significantly better than the 1D CNN did: 79 percent accuracy
    versus only 54 percent. This level of accuracy is still not particularly useful
    for many applications, but for others, it might be completely acceptable. Nevertheless,
    we’d like to do better if we can. It’s worth noting that we have a few limitations
    in our data and, for that matter, our hardware, since we are restricting ourselves
    to a CPU-only approach, which limits the amount of time we are willing to wait
    for models to train. Here is where the some 25-fold increase in performance possible
    with GPUs would be helpful, assuming our use case allows for using GPUs. If we’re
    planning to run the model on an embedded system, for example, we might not have
    a GPU available, so we’d want to stick with a smaller model anyway.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/15fig06.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15-6: Training and validation loss (top) and error (bottom) for the
    2D CNN architecture*'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Initialization, Regularization, and Batch Normalization
  id: totrans-523
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The literature tells us that there are other things we can try. We already augmented
    the dataset, a powerful technique, and we are using dropout, another powerful
    technique. We can try using a new initialization strategy, He initialization,
    which has been shown to often work better than Glorot initialization, the Keras
    default. We can also try applying L2 regularization, which Keras implements as
    weight decay per layer. See [Chapter 10](ch10.xhtml#ch10) for a refresher on these
    techniques.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: 'To set the layer initialization algorithm, we need to add the following keyword
    to the `Conv2D` and first `Dense` layer:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To add L2 regularization, we add the following keyword to the `Conv2D` and
    first `Dense` layer:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here *λ* = 0.001\. Recall, *λ* is the L2 regularization scale factor.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 'We could test these together, but instead we’ve tested them individually to
    see what effect, if any, they have for this dataset. Training six models as before
    gives the following overall accuracies:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '| **Regularizer** | **Score** |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: '| He initialization | 78.5 ± 0.5% |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
- en: '| L2 regularization | 78.3 ± 0.4% |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
- en: This is no different, statistically, from the previous results. In this case,
    these approaches are neither beneficial nor detrimental.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization is another well-tested, go-to technique widely used by
    the machine learning community. We mentioned batch normalization briefly in [Chapter
    12](ch12.xhtml#ch12). Batch normalization does just what its name suggests: it
    normalizes the inputs to a layer of the network, subtracting per feature means
    and dividing by the per feature standard deviations. The output of the layer multiplies
    the normalized input by a constant and adds an offset. The net effect is the input
    values are mapped to new output values by a two-step process: normalize the input
    and then apply a linear transform to get the output. The parameters of the linear
    transform are learned during backprop. At inference time, means and standard deviations
    learned from the dataset are applied to unknown inputs.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization has shown itself time and again to be effective, especially
    in speeding up training. Machine learning researchers are still debating the exact
    reasons *why* it works as it does. To use it in Keras, you simply insert batch
    normalization after the convolutional and dense layers of the network (and after
    any activation function like ReLU used by those layers). Batch normalization is
    known to not work well with dropout, so we’ll also remove the Dropout layers.
    The relevant architecture portion of the model code is shown in [Listing 15-13](ch15.xhtml#ch15lis13).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: from keras.layers import BatchNormalization
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: model = Sequential()
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: activation='relu', input_shape=input_shape))
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: model.add(BatchNormalization())
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: model.add(BatchNormalization())
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: model.add(BatchNormalization())
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Flatten())
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(128, activation='relu'))
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: model.add(BatchNormalization())
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-13: Adding in batch normalization*'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: If we repeat our training process, six models with mean and standard error reporting
    of the overall accuracy, we get
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization     75.56 ± 0.59%
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: which is significantly less than the mean accuracy found without batch normalization
    but including dropout.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: Examining the Confusion Matrix
  id: totrans-557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve seen in this section that our dataset is a tough one. Augmentation and
    dropout have been effective, but other things like ReLU-specific initialization,
    L2 regularization (weight decay), and even batch normalization have not improved
    things for us. That doesn’t mean these techniques are ineffective, just that they
    are not effective for this particular small dataset.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a quick look at the confusion matrix generated by one of the models
    using our chosen architecture. We’ve seen previously how to calculate the matrix;
    we’ll show it here for discussion and for comparison with the confusion matrices
    we’ll make in the next section. [Table 15-2](ch15.xhtml#ch15tab2) shows the matrix;
    as always, rows are the true class label, and columns are the model-assigned label.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 15-2:** Confusion Matrix for the Spectrogram Model'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| **0** | **85.6** | 0.0 | 0.0 | 5.6 | 0.0 | 0.0 | 0.0 | 5.0 | 0.6 | 3.1 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.0 | **97.5** | 1.2 | 0.0 | 0.6 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.0 | 13.8 | **72.5** | 0.6 | 0.6 | 3.8 | 6.2 | 0.0 | 0.6 | 1.9 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| **3** | 25.0 | 0.0 | 0.0 | **68.1** | 0.0 | 2.5 | 0.6 | 0.0 | 2.5 | 1.2 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **84.4** | 6.2 | 5.0 | 3.8 | 0.0 | 0.0 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | **94.4** | 4.4 | 0.6 | 0.0 | 0.0 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '| **6** | 0.0 | 0.0 | 1.2 | 0.0 | 0.0 | 10.6 | **88.1** | 0.0 | 0.0 | 0.0 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| **7** | 9.4 | 0.0 | 0.6 | 0.0 | 15.6 | 1.9 | 0.0 | **63.8** | 7.5 | 1.2 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| **8** | 18.1 | 1.9 | 0.0 | 5.6 | 0.0 | 1.2 | 2.5 | 6.9 | **55.6** | 8.1 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| **9** | 7.5 | 0.0 | 8.1 | 0.6 | 0.0 | 0.6 | 0.0 | 1.9 | 10.0 | **71.2** |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: The three worst-performing classes are helicopter (8), fire (7), and waves (3).
    Both waves and helicopter are most often confused with rain (0), while fire is
    most often confused with clock (4) and rain. The best performing classes are rooster
    (1) and sneezing (5). These results make sense. A rooster’s crow and a person
    sneezing are distinct sounds; nothing really sounds like them. However, it is
    easy to see how waves and a helicopter could be confused with rain, or the crackle
    of a fire with the tick of a clock.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: Does this mean we’re stuck at 78.8 percent accuracy? No, we have one more trick
    to try. We’ve been training and evaluating the performance of single models. Nothing
    is stopping us from training multiple models and combining their results. This
    is *ensembling*. We presented ensembles briefly in [Chapter 6](ch06.xhtml#ch06)
    and again in [Chapter 9](ch09.xhtml#ch09) when discussing dropout. Now, let’s
    use the idea directly to see if we can improve our sound sample classifier.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles
  id: totrans-575
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The core idea of an ensemble is to take the output of multiple models trained
    on the same, or extremely similar, dataset(s) and combine them. It embodies the
    “wisdom of the crowds” concept: one model might be better at certain classes or
    types of inputs for a particular class than another, so it makes sense that if
    they work together, they might arrive at a final result better than either one
    could do on its own.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ll use the same machine learning architecture we used in the previous
    section. Our different models will be separate trainings of this architecture
    using the spectrograms as input. This is a weaker form of ensembling. Typically,
    the models in the ensemble are quite different from each other, either different
    architectures of neural networks, or completely different types of models like
    Random Forests and *k*-Nearest Neighbors. The variation between models here is
    due to the random initialization of the networks and the different parts of the
    loss landscape the network finds itself in when training stops.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach works like this:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: Train multiple models (*n* = 6) using the spectrogram dataset.
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the softmax output of these models on the test set in some manner.
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the resulting output from the combination to predict the assigned class
    label.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We hope that the set of class labels assigned after combining the individual
    model outputs is superior to the set assigned by the model architecture used alone.
    Intuitively, we feel that this approach should buy us something. It makes sense.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a question immediately arises: how do we best combine the outputs
    of the individual networks? We have total freedom in the answer to that question.
    What we are looking for is an *f* () such that'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[predict] = *f*(*y*[0], *y*[1], *y*[2], … , *y[n]*)'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: where *y*[*i*], *i* = 0, 1, …, *n* are the outputs of the *n* models in the
    ensemble and *f* () is some function, operation, or algorithm that best combines
    them into a single new prediction, *y*[predict].
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: 'Some combination approaches come readily to mind: we could average the outputs
    and select the largest, keep maximum per class output across the ensemble and
    then choose the largest of those, or use voting to decide which class label should
    be assigned. We’ll try all three of these.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first three approaches. We already have the six ensemble
    models: they’re the models we trained in the previous section to give us the mean
    accuracy on the test set. This model architecture uses dropout, but no alternate
    initialization, L2 regularization, or batch normalization.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s straightforward enough to run the test set through each of the models
    trained in the previous section ([Listing 15-14](ch15.xhtml#ch15lis14)):'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: import sys
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: from keras.models import load_model
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: model = load_model(sys.argv[1])
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: x_test = np.load("esc10_spect_test_images.npy")/255.0
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("esc10_spect_test_labels.npy")
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: ❶ prob = model.predict(x_test)
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: ❷ p = np.argmax(prob, axis=1)
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: cc = np.zeros((10,10))
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(y_test)):'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: cc[y_test[i],p[i]] += 1
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: ❸ print(np.array2string(cc.astype("uint32")))
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: cp = 100.0 * cc / cc.sum(axis=1)
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: ❹ print(np.array2string(cp, precision=1))
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: print("Overall accuracy = %0.2f%%" % (100.0*np.diag(cc).sum()/cc.sum(),))
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: np.save(sys.argv[2], prob)
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-14: Applying multiple models to the test set*'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: This code expects the name of the trained model file as the first argument and
    the name of an output file to store the model predictions as the second argument.
    Then, it loads the model and spectrogram test data, applies the model to the test
    data ❶, and predicts class labels by selecting the highest output value ❷.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: The code also calculates the confusion matrix and displays it twice, first as
    actual counts ❸ and again as a percentage of the actual class ❹. Finally, it displays
    the overall accuracy and writes the probabilities to the disk. With this code,
    we can store the predictions of each of the six models.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the predictions, let’s combine them in the first of the three
    ways mentioned previously. To calculate the average of the model predictions,
    we first load each model’s predictions, and then average and select the maximum
    per sample as shown in [Listing 15-15](ch15.xhtml#ch15lis15).
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: p0 = np.load("prob_run0.npy")
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: p1 = np.load("prob_run1.npy")
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: p2 = np.load("prob_run2.npy")
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: p3 = np.load("prob_run3.npy")
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: p4 = np.load("prob_run4.npy")
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: p5 = np.load("prob_run5.npy")
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: y_test = np.load("esc10_spect_test_labels.npy")
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: prob = (p0+p1+p2+p3+p4+p5)/6.0
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: p = np.argmax(prob, axis=1)
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-15: Averaging the test set results*'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: The resulting percentage confusion matrix is
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: '| **0** | **83.8** | 0.0 | 0.0 | 7.5 | 0.0 | 0.0 | 0.0 | 4.4 | 0.0 | 4.4 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.0 | **97.5** | 1.9 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.0 | 10.0 | **78.1** | 0.0 | 0.0 | 3.1 | 6.2 | 0.0 | 0.0 | 2.5 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| **3** | 9.4 | 0.0 | 0.0 | **86.2** | 0.0 | 3.1 | 0.6 | 0.0 | 0.0 | 0.6 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **83.1** | 5.6 | 5.0 | 5.6 | 0.0 | 0.0 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **93.8** | 5.6 | 0.0 | 0.0 | 0.0 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: '| **6** | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 8.8 | **90.6** | 0.0 | 0.0 | 0.0 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| **7** | 8.1 | 0.0 | 0.0 | 0.0 | 17.5 | 1.9 | 0.0 | **64.4** | 7.5 | 0.6 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| **8** | 6.2 | 0.0 | 0.0 | 7.5 | 0.0 | 1.9 | 4.4 | 8.8 | **66.2** | 5.0 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| **9** | 5.0 | 0.0 | 5.0 | 1.2 | 0.0 | 0.6 | 0.0 | 1.9 | 10.6 | **75.6** |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: with an overall accuracy of 82.0 percent.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is helpful: we went from 79 percent to 82 percent in overall
    accuracy. The most significant improvements were in class 3 (waves) and class
    8 (helicopter).'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: Our next approach, shown in [Listing 15-16](ch15.xhtml#ch15lis16), keeps the
    maximum probability across the six models for each class and then selects the
    largest to assign the class label.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: p = np.zeros(len(y_test), dtype="uint8")
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(y_test)):'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: t = np.array([p0[i],p1[i],p2[i],p3[i],p4[i],p5[i]])
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: p[i] = np.argmax(t.reshape(60)) % 10
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-16: Keeping the test set maximum*'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: 'This code defines a vector, `p`, of the same length as the vector of actual
    labels, `y_test`. Then, for each test sample, we form `t`, a concatenation of
    all six models’ predictions for each class. We reshape `t` so that it is a one-dimensional
    vector of 60 elements. Why 60? We have 10 class predictions times 6 models. The
    maximum of this vector is the largest value, the index of which is returned by
    `argmax`. We really don’t want this index; instead, we want the class label this
    index maps to. Therefore, if we take this index modulo 10, we will get the proper
    class label, which we assign to `p`. With `p` and `y_test`, we can calculate the
    confusion matrix:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
- en: '| **0** | **82.5** | 0.0 | 0.0 | 9.4 | 0.0 | 0.0 | 0.0 | 4.4 | 0.6 | 3.1 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.0 | **95.0** | 4.4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.0 | 10.0 | **78.8** | 0.0 | 0.0 | 3.1 | 5.6 | 0.0 | 0.0 | 2.5 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
- en: '| **3** | 5.0 | 0.0 | 0.0 | **90.6** | 0.0 | 2.5 | 0.6 | 0.0 | 0.6 | 0.6 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1.2 | 0.0 | 0.0 | 0.0 | **81.2** | 6.2 | 5.0 | 6.2 | 0.0 | 0.0 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **93.8** | 5.6 | 0.0 | 0.0 | 0.0 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
- en: '| **6** | 0.0 | 0.0 | 0.6 | 0.0 | 0.6 | 8.8 | **90.0** | 0.0 | 0.0 | 0.0 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| **7** | 8.8 | 0.0 | 0.0 | 0.0 | 16.2 | 2.5 | 0.0 | **65.0** | 6.9 | 0.6 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| **8** | 8.1 | 0.0 | 0.0 | 6.2 | 0.0 | 1.9 | 4.4 | 9.4 | **63.1** | 6.9 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| **9** | 3.8 | 0.0 | 4.4 | 3.1 | 0.0 | 0.0 | 0.0 | 1.9 | 10.6 | **76.2** |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: This gives us an overall accuracy of 81.6 percent.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: Voting is the typical approach used to combine outputs from several models.
    To implement voting in this case, we’ll use [Listing 15-17](ch15.xhtml#ch15lis17).
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: t = np.zeros((6,len(y_test)), dtype="uint32")
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: ❶ t[0,:] = np.argmax(p0, axis=1)
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: t[1,:] = np.argmax(p1, axis=1)
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: t[2,:] = np.argmax(p2, axis=1)
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: t[3,:] = np.argmax(p3, axis=1)
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: t[4,:] = np.argmax(p4, axis=1)
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: t[5,:] = np.argmax(p5, axis=1)
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: p = np.zeros(len(y_test), dtype="uint8")
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(len(y_test)):'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: q = np.bincount(t[:,i])
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: p[i] = np.argmax(q)
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15-17: Voting to select the best class label*'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: We first apply `argmax` across the six model predictions to get the associated
    labels ❶, storing them in a combined matrix, `t`. We then define `p` as before
    to hold the final assigned class label. We loop over each of the test samples,
    where we use a new NumPy function, `bincount`, to give us the number of times
    each class label occurs for the current test sample. The largest such count is
    the most often selected label, so we use `argmax` again to assign the proper output
    label to `p`. Note, this code works because our class labels are integers running
    consecutively from 0 through 9\. This alone is a good enough reason to use such
    simple and ordered class labels.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the confusion matrix produced by this voting procedure:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| **0** | **86.2** | 0.0 | 0.0 | 8.8 | 0.0 | 0.0 | 0.0 | 3.8 | 0.0 | 1.2 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.0 | **98.1** | 1.2 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.0 | 10.6 | **78.1** | 0.0 | 0.0 | 3.1 | 5.6 | 0.0 | 0.0 | 2.5 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| **3** | 14.4 | 0.0 | 0.0 | **81.2** | 0.0 | 3.1 | 0.6 | 0.0 | 0.0 | 0.6 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **83.8** | 5.6 | 5.0 | 5.0 | 0.0 | 0.0 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **94.4** | 5.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| **6** | 0.0 | 0.0 | 1.2 | 0.0 | 0.6 | 9.4 | **88.8** | 0.0 | 0.0 | 0.0 |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| **7** | 8.8 | 0.0 | 0.0 | 0.0 | 18.1 | 1.9 | 0.0 | **65.6** | 5.0 | 0.6 |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| **8** | 7.5 | 0.0 | 0.0 | 6.9 | 0.0 | 3.1 | 3.8 | 8.8 | **67.5** | 2.5 |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| **9** | 5.6 | 0.0 | 6.2 | 1.2 | 0.0 | 0.6 | 0.0 | 1.9 | 11.2 | **73.1** |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: This gives us an overall accuracy of 81.7 percent.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: Each of these three ensemble approaches improved our results, almost identically.
    A simple combination of the model outputs gave us, essentially, an accuracy boost
    of 3 percent over the base model alone, thereby demonstrating the utility of ensemble
    techniques.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-683
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter presented a case study, a new dataset, and the steps we need to
    take to work through building a useful model. We started by working with the dataset
    as given to us, as raw sound samples, which we were able to augment successfully.
    We noticed that we had a feature vector and attempted to use classical models.
    From there, we moved on to 1D convolutional neural networks. Neither of these
    approaches was particularly successful.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, our dataset allowed for a new representation, one that illustrated
    more effectively what composed the data and, especially important for us, introduced
    spatial elements so that we could work with 2D convolutional networks. With these
    networks, we improved quite a bit on the best 1D results, but we were still not
    at a level that was likely to be useful.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: After exhausting our bag of CNN training tricks, we moved to ensembles of classifiers.
    With these, we discovered a modest improvement by using simple approaches to combining
    the base model outputs (for example, averaging).
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: 'We can show the progression of models and their overall accuracies to see how
    our case study evolved:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Data source** | **Accuracy** |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Gaussian Naïve Bayes | 1D sound sample | 28.1% |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| Random Forest (1,000 trees) | 1D sound sample | 34.4% |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| 1D CNN | 1D sound sample | 54.0% |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| 2D CNN | Spectrogram | 78.8% |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| Ensemble (average) | Spectrogram | 82.0% |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: This table shows the power of modern deep learning and the utility of combining
    it with well-proven classical approaches like ensembles.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes our exploration of machine learning. We started at the
    beginning, with data and datasets. We moved on to the classical machine learning
    models, and then dove into traditional neural networks so that we would have a
    solid foundation from which to understand modern convolutional neural networks.
    We explored CNNs in detail and concluded with a case study as an illustration
    of how you might approach a new dataset to build a successful model. Along the
    way, we learned about how to evaluate models. We became familiar with the metrics
    used by the community so that we can understand what people are talking about
    and presenting in their papers.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this entire book has been an introduction, and we have barely scratched
    the surface of the ever-expanding world that is machine learning. Our final chapter
    will serve as a jumping-off point—a guide to where you may want to wander next
    to expand your machine learning knowledge beyond the tight bounds we’ve been required
    to set for ourselves here.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
