- en: '**15'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**15'
- en: 'A CASE STUDY: CLASSIFYING AUDIO SAMPLES**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个案例研究：分类音频样本**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: 'Let’s bring together everything that we’ve learned throughout the book. We’ll
    be looking at a single case study. The scenario is this: we are data scientists,
    and our boss has tasked us with building a classifier for audio samples stored
    as *.wav* files. We’ll begin with the data itself. We first want to build some
    basic intuition for how it’s structured. From there, we’ll build augmented datasets
    we can use for training models. The first dataset uses the sound samples themselves,
    a one-dimensional dataset. We’ll see that this approach isn’t as successful as
    we would like it to be.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将整本书中学到的内容汇总在一起。我们将通过一个单一的案例研究来进行。场景是这样的：我们是数据科学家，老板要求我们为存储为*.wav*文件的音频样本构建一个分类器。我们将从数据本身开始。首先，我们希望构建一些基本的直觉，了解数据是如何结构化的。从这里开始，我们将构建可以用于训练模型的增强数据集。第一个数据集使用的是音频样本本身，这是一个一维的数据集。我们会发现，这种方法并不像我们希望的那样成功。
- en: We’ll then turn the audio data into images to allow us to explore two-dimensional
    CNNs. This change of representation will lead to a big improvement in model performance.
    Finally, we’ll combine multiple models in ensembles to see how to leverage the
    relative strengths and weaknesses of the individual models to boost overall performance
    still more.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把音频数据转化为图像，以便我们可以探索二维卷积神经网络（CNN）。这种表示方式的变化将大大改善模型的表现。最后，我们将通过集成多个模型，看看如何利用单个模型的相对优势和劣势来进一步提升整体表现。
- en: Building the Dataset
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建数据集
- en: There are 10 classes in our dataset, which consists of 400 samples total, 40
    samples per class, each 5 seconds long. We’ll assume we cannot get any more data
    because it’s time-consuming and expensive to record the samples and label them.
    We must work with the data we are given and no more.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有10个类别，总共400个样本，每个类别40个样本，每个样本时长为5秒。我们假设我们不能再获得更多的数据，因为录制样本并进行标注是费时且昂贵的。我们必须使用我们已经得到的数据，不能再增加其他数据。
- en: Throughout this book, we have consistently preached about the necessity of having
    a good dataset. We’ll assume that the dataset we have been handed is complete
    in the sense that our system will encounter only types of sound samples in the
    dataset; there will be no unknown class or classes. Additionally, we’ll also assume
    that the balanced nature of the dataset is real, and all classes are indeed equally
    likely.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们始终强调拥有良好数据集的重要性。我们假设我们得到的数据集是完整的，也就是说，我们的系统将只会遇到数据集中已存在的音频样本类型，不会有未知的类别。此外，我们还假设数据集的平衡性是真实的，所有类别的样本都是等可能的。
- en: 'The audio dataset we’ll use is called ESC-10\. For a complete description,
    see “ESC: Dataset for Environmental Sound Classification” by Karol J. Piczal (2015).
    The dataset is available at [https://github.com/karoldvl/ESC-50/](https://github.com/karoldvl/ESC-50/).
    But it needs to be extracted from the larger ESC-50 dataset, which doesn’t have
    a license we can use. The ESC-10 subset does.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用的音频数据集叫做ESC-10。有关完整的描述，请参阅Karol J. Piczal（2015年）的《ESC: Dataset for Environmental
    Sound Classification》。数据集可以在[https://github.com/karoldvl/ESC-50/](https://github.com/karoldvl/ESC-50/)找到。但它需要从更大的ESC-50数据集中提取，而该数据集没有我们可以使用的许可证。ESC-10子集是有许可证的。'
- en: Let’s do some preprocessing to extract the ESC-10 *.wav* files from the larger
    ESC-50 dataset. Download the single ZIP-file version of the dataset from the preceding
    URL and expand it. This will create a directory called *ESC-50-master*. Then,
    use the code in [Listing 15-1](ch15.xhtml#ch15lis1) to build the ESC-10 dataset
    from it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一些预处理，从更大的ESC-50数据集中提取ESC-10 *.wav*文件。从上面的URL下载数据集的单个ZIP文件版本并解压。这将创建一个名为*ESC-50-master*的目录。然后，使用[Listing
    15-1](ch15.xhtml#ch15lis1)中的代码从中构建ESC-10数据集。
- en: import sys
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: import os
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import shutil
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: import shutil
- en: classes = {
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: classes = {
- en: '"rain":0,'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '"rain":0,'
- en: '"rooster":1,'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '"rooster":1,'
- en: '"crying_baby":2,'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '"crying_baby":2,'
- en: '"sea_waves":3,'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '"sea_waves":3,'
- en: '"clock_tick":4,'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '"clock_tick":4,'
- en: '"sneezing":5,'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '"sneezing":5,'
- en: '"dog":6,'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '"dog":6,'
- en: '"crackling_fire":7,'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '"crackling_fire":7,'
- en: '"helicopter":8,'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '"helicopter":8,'
- en: '"chainsaw":9,'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '"chainsaw":9,'
- en: '}'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'with open("ESC-50-master/meta/esc50.csv") as f:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open("ESC-50-master/meta/esc50.csv") as f:'
- en: lines = [i[:-1] for i in f.readlines()]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: lines = [i[:-1] for i in f.readlines()]
- en: lines = lines[1:]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: lines = lines[1:]
- en: os.system("rm -rf ESC-10")
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf ESC-10")
- en: os.system("mkdir ESC-10")
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("mkdir ESC-10")
- en: os.system("mkdir ESC-10/audio")
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("mkdir ESC-10/audio")
- en: meta = []
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: meta = []
- en: 'for line in lines:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'for line in lines:'
- en: t = line.split(",")
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: t = line.split(",")
- en: 'if (t[-3] == ''True''):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (t[-3] == ''True''):'
- en: meta.append("ESC-10/audio/%s %d" % (t[0],classes[t[3]]))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: meta.append("ESC-10/audio/%s %d" % (t[0], classes[t[3]]))
- en: src = "ESC-50-master/audio/"+t[0]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: src = "ESC-50-master/audio/"+t[0]
- en: dst = "ESC-10/audio/"+t[0]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: dst = "ESC-10/audio/"+t[0]
- en: shutil.copy(src,dst)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: shutil.copy(src,dst)
- en: 'with open("ESC-10/filelist.txt","w") as f:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open("ESC-10/filelist.txt","w") as f:'
- en: 'for m in meta:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'for m in meta:'
- en: f.write(m+"\n")
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(m+"\n")
- en: '*Listing 15-1: Building the ESC-10 dataset*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-1: 构建 ESC-10 数据集*'
- en: The code uses the ESC-50 metadata to identify the sound samples that belong
    to the 10 classes of the ESC-10 dataset and then copies them to the *ESC-10/audio*
    directory. It also writes a list of the audio files to *filelist.txt*. After running
    this code, we’ll use only the ESC-10 files.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用 ESC-50 的元数据来识别属于 ESC-10 数据集 10 个类别的声音样本，然后将它们复制到 *ESC-10/audio* 目录。它还将音频文件的列表写入
    *filelist.txt*。运行这段代码后，我们将只使用 ESC-10 文件。
- en: 'If all is well, we should now have 400 five-second *.wav* files, 40 from each
    of the 10 classes: rain, rooster, crying baby, sea waves, clock tick, sneezing,
    dog, crackling fire, helicopter, and chainsaw. We’ll politely refrain from asking
    our boss exactly why she wants to discriminate between these particular classes
    of sound.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们现在应该拥有 400 个 5 秒钟的 *.wav* 文件，每个类别 40 个：雨声、雄鸡叫、婴儿哭声、海浪声、时钟滴答声、打喷嚏声、狗叫、火焰噼啪声、直升机声和链锯声。我们会有礼貌地避免问老板她为什么想区分这些特定类别的声音。
- en: Augmenting the Dataset
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集增强
- en: Our first instinct should be that our dataset is too small. After all, we have
    only 40 examples of each sound, and we know that some of those will need to be
    held back for testing, leaving even fewer per class for training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一直觉应该是数据集太小了。毕竟，我们每种声音只有 40 个样本，而且我们知道其中一些将需要保留用于测试，这样每个类别用于训练的数据就更少了。
- en: We could resort to *k*-fold validation, but in this case, we’ll instead opt
    for data augmentation. So, how do we augment audio data?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用 *k* 折交叉验证，但在这种情况下，我们将选择数据增强。那么，如何增强音频数据呢？
- en: Recall, the goal of data augmentation is to create new data samples that could
    plausibly come from the classes in the dataset. With images, we can make obvious
    changes like shifting, flipping left and right, and so on. With continuous vectors,
    we’ve seen how to use PCA to augment the data (see [Chapter 5](ch05.xhtml#ch05)).
    To augment the audio files, we need to think of things we can do that will produce
    new files that still sound like the original class. Four thoughts come to mind.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，数据增强的目标是创建新的数据样本，这些样本看起来应该来自数据集中的某个类别。对于图像，我们可以进行明显的变化，比如平移、左右翻转等。对于连续向量，我们已经看到如何使用
    PCA 来增强数据（参见 [第 5 章](ch05.xhtml#ch05)）。为了增强音频文件，我们需要考虑一些方法来生成新文件，同时保持它们听起来像原始类别。这里有四个思路。
- en: First, we can shift the sample in time, much as we can shift an image to the
    left or right a few pixels. Second, we can simulate a noisy environment by adding
    a small amount of random noise to the sound itself. Third, we can shift the pitch
    of the sound, and make it higher or lower by some small amount. Not surprisingly,
    this is known as *pitch shifting*. Finally, we can lengthen or compress the sound
    in time. This is known as *time shifting*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以在时间上平移样本，就像我们可以将图像平移几像素一样。第二，我们可以通过在声音中添加少量随机噪声来模拟嘈杂的环境。第三，我们可以改变声音的音高，使其升高或降低一些。毫不奇怪，这就是
    *音高变化*。最后，我们可以在时间上延长或压缩声音，这被称为 *时间变化*。
- en: Doing all of this sounds complicated, especially if we haven’t worked with audio
    data before. I should point out that in practice, being presented with unfamiliar
    data is a very real possibility; we don’t all get to choose what we need to work
    with.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 做这一切听起来很复杂，尤其是如果我们之前没有处理过音频数据。我应该指出，实际上，面对陌生的数据是一个非常真实的可能性；我们不总是能选择需要处理的数据。
- en: 'Fortunately for us, we’re working in Python, and the Python community is vast
    and talented. It turns out that adding one library to our system will allow us
    to easily do time stretching and pitch shifting. Let’s install the `librosa` library.
    This should do the trick for us:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们正在使用 Python，而且 Python 社区非常庞大且充满才华。事实证明，向我们的系统中添加一个库就可以轻松实现时间拉伸和音高变化。让我们安装
    `librosa` 库。这应该能解决我们的需求：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the necessary library installed, we can augment the ESC-10 dataset with
    the code in [Listing 15-2](ch15.xhtml#ch15lis2).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了必要的库后，我们可以使用 [Listing 15-2](ch15.xhtml#ch15lis2) 中的代码来增强 ESC-10 数据集。
- en: import os
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import random
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: import numpy as np
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from scipy.io.wavfile import read, write
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.io.wavfile import read, write
- en: import librosa as rosa
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: import librosa as rosa
- en: N = 8
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: N = 8
- en: os.system("rm -rf augmented; mkdir augmented")
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf augmented; mkdir augmented")
- en: os.system("mkdir augmented/train augmented/test")
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("mkdir augmented/train augmented/test")
- en: ❶ src_list = [i[:-1] for i in open("ESC-10/filelist.txt")]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ src_list = [i[:-1] for i in open("ESC-10/filelist.txt")]
- en: z = [[] for i in range(10)]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: z = [[] for i in range(10)]
- en: 'for s in src_list:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'for s in src_list:'
- en: _,c = s.split()
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: _,c = s.split()
- en: z[int(c)].append(s)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: z[int(c)].append(s)
- en: ❷ train = []
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ train = []
- en: test = []
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: test = []
- en: 'for i in range(10):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: p = z[i]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: p = z[i]
- en: random.shuffle(p)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(p)
- en: test += p[:8]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: test += p[:8]
- en: train += p[8:]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: train += p[8:]
- en: random.shuffle(train)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(train)
- en: random.shuffle(test)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(test)
- en: augment_audio(train, "train")
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: augment_audio(train, "train")
- en: augment_audio(test, "test")
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: augment_audio(test, "test")
- en: '*Listing 15-2: Augmenting the ESC-10 dataset, part 1*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-2：增强ESC-10数据集，第一部分*'
- en: This code loads the necessary modules, including the `librosa` module, which
    we’ll just call `rosa`, and two functions from the SciPy `wavfile` module that
    let us read and write NumPy arrays as *.wav* files.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码加载了必要的模块，包括`librosa`模块（我们将其简称为`rosa`），以及来自SciPy `wavfile`模块的两个函数，允许我们将NumPy数组读写为*.wav*文件。
- en: We set the number of samples per class that we’ll hold back for testing (`N=8`)
    and create the output directory where the augmented sound files will reside (`augmented`).
    Then we read the file list we created with [Listing 15-1](ch15.xhtml#ch15lis1)
    ❶. Next, we create a nested list (`z`) to hold the names of the audio files associated
    with each of the 10 classes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置每个类别的测试样本数（`N=8`），并创建输出目录（即增强后的声音文件所在的目录，命名为`augmented`）。然后，我们读取通过[列表 15-1](ch15.xhtml#ch15lis1)创建的文件列表❶。接下来，我们创建一个嵌套列表（`z`），用于存储与10个类别相关联的音频文件名。
- en: Using the list of files per class, we pull it apart and create `train` and `test`
    file lists ❷. Notice that we randomly shuffle the list of files per class and
    the final `train` and `test` lists. This code follows the convention we discussed
    in [Chapter 4](ch04.xhtml#ch04) of separating train and test first, then augmenting.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每个类别的文件列表，我们将其拆分并创建`train`和`test`文件列表❷。注意，我们对每个类别的文件列表和最终的`train`与`test`列表进行了随机打乱。该代码遵循了我们在[第4章](ch04.xhtml#ch04)中讨论的先分离训练集和测试集，再进行增强的惯例。
- en: We can augment the train and test files by calling `augment_audio`. This function
    is in [Listing 15-3](ch15.xhtml#ch15lis3).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用`augment_audio`来增强训练和测试文件。这个函数在[列表 15-3](ch15.xhtml#ch15lis3)中。
- en: 'def augment_audio(src_list, typ):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'def augment_audio(src_list, typ):'
- en: flist = []
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: flist = []
- en: 'for i,s in enumerate(src_list):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,s in enumerate(src_list):'
- en: f,c = s.split()
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: f,c = s.split()
- en: '❶ wav = read(f) # (sample rate, data)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ wav = read(f) # (采样率, 数据)'
- en: base = os.path.abspath("augmented/%s/%s" %
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: base = os.path.abspath("augmented/%s/%s" %
- en: (typ, os.path.basename(f)[:-4]))
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (typ, os.path.basename(f)[:-4]))
- en: fname = base+".wav"
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: fname = base+".wav"
- en: ❷ write(fname, wav[0], wav[1])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ write(fname, wav[0], wav[1])
- en: flist.append("%s %s" % (fname,c))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: flist.append("%s %s" % (fname,c))
- en: 'for j in range(19):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'for j in range(19):'
- en: d = augment(wav)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: d = augment(wav)
- en: fname = base+("_%04d.wav" % j)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: fname = base+("_%04d.wav" % j)
- en: ❸ write(fname, wav[0], d.astype(wav[1].dtype))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ write(fname, wav[0], d.astype(wav[1].dtype))
- en: flist.append("%s %s" % (fname,c))
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: flist.append("%s %s" % (fname,c))
- en: random.shuffle(flist)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(flist)
- en: 'with open("augmented_%s_filelist.txt" % typ,"w") as f:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open("augmented_%s_filelist.txt" % typ,"w") as f:'
- en: 'for z in flist:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'for z in flist:'
- en: f.write("%s\n" % z)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: f.write("%s\n" % z)
- en: '*Listing 15-3: Augmenting the ESC-10 dataset, part 2*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-3：增强ESC-10数据集，第二部分*'
- en: The function loops over all the filenames in the given list (`src_list`), which
    will be either train or test. The filename is separated from the class label,
    and then the file is read from disk ❶. As indicated in the comment, `wav` is a
    list of two elements. The first is the sampling rate in Hz (cycles per second).
    This is how often the analog waveform was digitized to produce the *.wav* file.
    For ESC-10, the sampling rate is always 44,100 Hz, which is the standard rate
    for a compact disc. The second element is a NumPy array containing the actual
    digitized sound samples. These are the values we’ll augment to produce new data
    files.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数遍历给定列表中的所有文件名（`src_list`），该列表可以是训练集或测试集。文件名与类别标签分开，然后从磁盘读取文件❶。如注释所示，`wav`是一个包含两个元素的列表。第一个是采样率（单位：Hz，每秒周期数），即模拟波形被数字化生成*.wav*文件的频率。对于ESC-10数据集，采样率始终为44,100
    Hz，这是CD的标准采样率。第二个元素是一个NumPy数组，包含实际的数字化声音样本。这些值将被增强以生成新的数据文件。
- en: After setting up some output pathnames, we write the original sound sample to
    the augmented directory ❷. Then, we start a loop to generate 19 more augmented
    versions of the current sound sample. The augmented dataset, as a whole, will
    be 20 times larger, for a total of 8,000 sound files, 6,400 for training and 1,600
    for testing. Note, the sound samples for an augmented source file are assigned
    to `d`. The new sound file is written to disk using the sample rate of 44,100
    Hz and the augmented data matching the datatype of the source ❸.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置一些输出路径后，我们将原始声音样本写入增强目录❷。然后，我们开始一个循环，生成当前声音样本的另外19个增强版本。增强后的数据集整体上会变大20倍，总共有8,000个声音文件，其中6,400个用于训练，1,600个用于测试。请注意，增强源文件的声音样本分配给`d`。新声音文件以44,100
    Hz的采样率写入磁盘，并且增强数据与源数据的类型相匹配❸。
- en: As we create the augmented sound files, we also keep track of the filename and
    class and write them to a new file list. Here `typ` is a string indicating train
    or test.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建增强声音文件时，我们还会跟踪文件名和类别，并将其写入一个新的文件列表。这里`typ`是一个字符串，表示训练（train）或测试（test）。
- en: 'This function calls yet another function, `augment`. This is the function that
    generates an augmented version of a single sound file by randomly applying some
    subset of the four augmentation strategies mentioned previously: shifting, noise,
    pitch shifting, or time-shifting. Some or all of these might be used for any call
    to `augment`. The `augment` function itself is shown in [Listing 15-4](ch15.xhtml#ch15lis4).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数调用了另一个函数`augment`。这个函数通过随机应用先前提到的四种增强策略中的某些子集（包括移位、噪声、音高移位或时间移位），生成单个声音文件的增强版本。任何对`augment`的调用可能会使用这些策略中的某些或全部。`augment`函数本身见于[清单
    15-4](ch15.xhtml#ch15lis4)。
- en: 'def augment(wav):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'def augment(wav):'
- en: sr = wav[0]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: sr = wav[0]
- en: d = wav[1].astype("float32")
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: d = wav[1].astype("float32")
- en: '❶ if (random.random() < 0.5):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ 如果 (random.random() < 0.5):'
- en: s = int(sr/4.0*(np.random.random()-0.5))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: s = int(sr/4.0*(np.random.random()-0.5))
- en: d = np.roll(d,s)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: d = np.roll(d,s)
- en: 'if (s < 0):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (s < 0):'
- en: d[s:] = 0
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: d[s:] = 0
- en: 'else:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: d[:s] = 0
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: d[:s] = 0
- en: '❷ if (random.random() < 0.5):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ 如果 (random.random() < 0.5):'
- en: d += 0.1*(d.max()-d.min())*np.random.random(d.shape[0])
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: d += 0.1*(d.max()-d.min())*np.random.random(d.shape[0])
- en: '❸ if (random.random() < 0.5):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ 如果 (random.random() < 0.5):'
- en: pf = 20.0*(np.random.random()-0.5)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: pf = 20.0*(np.random.random()-0.5)
- en: d = rosa.effects.pitch_shift(d, sr, pf)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: d = rosa.effects.pitch_shift(d, sr, pf)
- en: '❹ if (random.random() < 0.5):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ 如果 (random.random() < 0.5):'
- en: rate = 1.0 + (np.random.random()-0.5)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: rate = 1.0 + (np.random.random()-0.5)
- en: d = rosa.effects.time_stretch(d,rate)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: d = rosa.effects.time_stretch(d,rate)
- en: 'if (d.shape[0] > wav[1].shape[0]):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (d.shape[0] > wav[1].shape[0]):'
- en: d = d[:wav[1].shape[0]]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: d = d[:wav[1].shape[0]]
- en: 'else:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: w = np.zeros(wav[1].shape[0], dtype="float32")
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: w = np.zeros(wav[1].shape[0], dtype="float32")
- en: w[:d.shape[0]] = d
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: w[:d.shape[0]] = d
- en: d = w.copy()
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: d = w.copy()
- en: return d
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: return d
- en: '*Listing 15-4: Augmenting the ESC-10 dataset, part 3*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-4: 增强ESC-10数据集，第三部分*'
- en: This function separates the samples (`d`) from the sample rate (`sr`) and makes
    sure the samples are floating-point numbers. For ESC-10, the source samples are
    all of type `int16` (signed 16-bit integers). Next come four `if` statements.
    Each one asks for a single random float, and if that float is less than 0.5, we
    execute the body of the `if`. This means that we apply each possible augmentation
    with a probability of 50 percent.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将样本（`d`）与采样率（`sr`）分开，并确保样本是浮动点数。对于ESC-10，源样本都是`int16`类型（有符号16位整数）。接下来是四个`if`语句。每个语句会生成一个随机浮动数，如果该浮动数小于0.5，我们就会执行`if`的主体。这意味着我们以50%的概率应用每种可能的增强。
- en: The first `if` shifts the sound samples in time ❶ by rolling the NumPy array,
    a vector, by some number of samples, `s`. This value amounts to at most an eighth
    of a second, `sr/4.0`. Note that the shift can be positive or negative. The quantity
    `sr/4.0` is the number of samples in a quarter of a second. However, the random
    float is in the range [*–*0.5,+0.5], so the ultimate shift is at most an eighth
    of a second. If the shift is negative, we need to zero samples at the end of the
    data; otherwise, we zero samples at the start.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`if`通过滚动NumPy数组（向量）`d`，按一定数量的样本`s`来移位声音样本❶。这个值最多为八分之一秒，即`sr/4.0`。注意，移位可以是正向或负向。`sr/4.0`表示四分之一秒的样本数。然而，随机浮动数的范围是[*–*0.5,
    +0.5]，因此最终的移位最大为八分之一秒。如果移位为负，我们需要将数据末尾的样本置零；否则，我们将数据开始的样本置零。
- en: Random noise is added by literally adding a random value of up to one-tenth
    of the range of the audio signal back in ❷. When played, this adds hiss, as you
    might hear on an old cassette tape.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 随机噪声是通过将一个随机值（最大为音频信号范围的十分之一）加回去来添加的❷。播放时，这会产生嘶嘶声，类似于老式卡带录音带上的噪声。
- en: Next comes shifting the pitch of the sample by using `librosa`. The pitch shift
    is expressed in musical steps, or fractions thereof. We randomly pick a float
    in the range [*–*10,+10] (`pf`) and pass it along with the data (`d`) and sampling
    rate (`sr`) to the `librosa` `pitch_shift` effect function ❸.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来使用`librosa`进行音高移位。音高移位以音符步进或其分数表示。我们在范围[*–*10,+10]内随机选取一个浮动值（`pf`），并将其与数据（`d`）和采样率（`sr`）一起传递给`librosa`的`pitch_shift`效果函数
    ❸。
- en: The last augmentation uses the `librosa` function to stretch or compress time
    (`time_stretch`) ❹. We adjust using an amount of time (`rate`) that is in the
    range [*–*0.5,+0.5]. If time was stretched, we need to chop off the extra samples
    to ensure that the sample length remains constant. If time was compressed, we
    need to add zero samples at the end.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的增强操作使用`librosa`函数进行时间拉伸或压缩（`time_stretch`）❹。我们使用一个在[*–*0.5,+0.5]范围内的时间量（`rate`）来进行调整。如果时间被拉伸，我们需要截断多余的样本，以确保样本长度保持不变。如果时间被压缩，我们需要在末尾添加零样本。
- en: Lastly, we return the new, augmented samples.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回新的、增强后的样本。
- en: Running the code in [Listing 15-2](ch15.xhtml#ch15lis2) creates a new *augmented*
    data directory with subdirectories *train* and *test*. These are the raw sound
    files that we’ll work with going forward. I encourage you to listen to some of
    them to understand what the augmentations have done. The filenames should help
    you quickly tell the originals from the augmentations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 运行[Listing 15-2](ch15.xhtml#ch15lis2)中的代码会创建一个新的*增强*数据目录，包含子目录*train*和*test*。这些是我们接下来要使用的原始音频文件。我鼓励你听听其中的一些文件，以了解增强操作所做的修改。文件名应该能帮助你快速区分原始文件和增强文件。
- en: Preprocessing Our Data
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Are we ready to start building models? Not yet. Our experience told us that
    the dataset was too small, and we augmented accordingly. However, we haven’t yet
    turned the raw data into something we can pass to a model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好开始构建模型了吗？还没有。我们的经验告诉我们，数据集太小了，因此我们进行了增强。但我们还没有将原始数据转化成可以传递给模型的格式。
- en: A first thought is to use the raw sound samples. These are already vectors representing
    the audio signal, with the time between the samples set by the sampling rate of
    44,100 Hz. But we don’t want to use them as they are. The samples are all exactly
    five seconds long. At 44,100 samples per second, that means each sample is a vector
    of 44,100 × 5 = 220,500 samples. That’s too long for us to work with effectively.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个初步的想法是使用原始音频样本。这些样本已经是代表音频信号的向量，样本之间的时间间隔由采样率44,100 Hz确定。但我们不想直接使用这些样本。每个样本都是精确的五秒钟长。以每秒44,100个样本计算，这意味着每个样本是一个包含44,100
    × 5 = 220,500个样本的向量。这个长度对于我们有效处理来说太长了。
- en: With a bit more thought, we might be able to convince ourselves that distinguishing
    between a crying baby and a barking dog might not need such a high sampling rate.
    What if instead of keeping all the samples, we kept only every 100th sample? Moreover,
    do we really need five seconds’ worth of data to identify the sounds? What if
    we kept only the first two seconds worth?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 经过进一步思考，我们或许能够说服自己，区分哭泣的婴儿和叫喊的狗可能不需要如此高的采样率。如果我们不保留所有样本，而是仅保留每第100个样本呢？此外，我们真的需要五秒钟的音频数据来识别声音吗？如果我们只保留前两秒钟的数据呢？
- en: Let’s keep only the first two seconds of each sound file; that’s 88,200 samples.
    And let’s keep only every 100th sample, so each sound file now becomes a vector
    of 882 elements. That’s hardly more than an unraveled MNIST digit image, and we
    know we can work with those.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只保留每个音频文件的前两秒钟，即88,200个样本。而且我们只保留每第100个样本，这样每个音频文件现在就变成了一个包含882个元素的向量。这几乎与解开的MNIST数字图像一样，我们知道我们可以有效地处理这些数据。
- en: '[Listing 15-5](ch15.xhtml#ch15lis5) has the code to build the actual initial
    version of the dataset we’ll use to build the models.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 15-5](ch15.xhtml#ch15lis5)包含了构建我们将用来建立模型的实际数据集初始版本的代码。'
- en: import os
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import random
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: import numpy as np
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from scipy.io.wavfile import read
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.io.wavfile import read
- en: 'sr = 44100 # Hz'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'sr = 44100 # Hz'
- en: N = 2*sr   # number of samples to keep
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: N = 2*sr    # 保留的样本数
- en: w = 100    # every 100
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: w = 100    # 每隔100
- en: afiles = [i[:-1] for i in open("augmented_train_filelist.txt")]
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: afiles = [i[:-1] for i in open("augmented_train_filelist.txt")]
- en: trn = np.zeros((len(afiles),N//w,1), dtype="int16")
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: trn = np.zeros((len(afiles),N//w,1), dtype="int16")
- en: lbl = np.zeros(len(afiles), dtype="uint8")
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: lbl = np.zeros(len(afiles), dtype="uint8")
- en: 'for i,t in enumerate(afiles):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,t in enumerate(afiles):'
- en: ❶ f,c = t.split()
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f,c = t.split()
- en: trn[i,:,0] = read(f)[1][:N:w]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: trn[i,:,0] = read(f)[1][:N:w]
- en: lbl[i] = int(c)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: lbl[i] = int(c)
- en: np.save("esc10_raw_train_audio.npy", trn)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_train_audio.npy", trn)
- en: np.save("esc10_raw_train_labels.npy", lbl)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_train_labels.npy", lbl)
- en: afiles = [i[:-1] for i in open("augmented_test_filelist.txt")]
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: afiles = [i[:-1] for i in open("augmented_test_filelist.txt")]
- en: tst = np.zeros((len(afiles),N//w,1), dtype="int16")
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: tst = np.zeros((len(afiles),N//w,1), dtype="int16")
- en: lbl = np.zeros(len(afiles), dtype="uint8")
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: lbl = np.zeros(len(afiles), dtype="uint8")
- en: 'for i,t in enumerate(afiles):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,t in enumerate(afiles):'
- en: f,c = t.split()
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: f,c = t.split()
- en: tst[i,:,0] = read(f)[1][:N:w]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: tst[i,:,0] = read(f)[1][:N:w]
- en: lbl[i] = int(c)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: lbl[i] = int(c)
- en: np.save("esc10_raw_test_audio.npy", tst)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_test_audio.npy", tst)
- en: np.save("esc10_raw_test_labels.npy", lbl)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_raw_test_labels.npy", lbl)
- en: '*Listing 15-5: Building reduced samples dataset*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-5：构建减少样本数据集*'
- en: This code builds train and test NumPy files containing the raw data. The data
    is from the augmented sound files we built in [Listing 15-2](ch15.xhtml#ch15lis2).
    The file list contains the file location and class label ❶. We load each file
    in the list and put it into an array, either the train or test array.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码构建了包含原始数据的训练和测试 NumPy 文件。这些数据来自我们在[示例 15-2](ch15.xhtml#ch15lis2)中构建的增强音频文件。文件列表包含文件位置和类别标签❶。我们加载列表中的每个文件，并将其放入数组中，分别是训练数组或测试数组。
- en: 'We have a one-dimensional feature vector and a number of train or test files,
    so we might expect we need a two-dimensional array to store our data, either 6400
    × 882 for the training set or 1600 × 882 for the test set. However, we know we’ll
    ultimately be working with Keras, and we know that Keras wants a dimension for
    the number of channels, so we define the arrays to be 6400 × 882 × 1 and 1600
    × 882 × 1 instead. The most substantial line in this code is the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个一维特征向量和一些训练或测试文件，因此我们可能需要一个二维数组来存储数据，训练集为 6400 × 882 或测试集为 1600 × 882。然而，我们知道最终会使用
    Keras，而且 Keras 需要一个表示通道数的维度，因此我们将数组定义为 6400 × 882 × 1 和 1600 × 882 × 1。此代码中最重要的一行是：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It reads the current sound file, keeps only the sound samples (`[1]`), and from
    the sound samples keeps only the first two seconds, worth at every 100th sample,
    `[:N:w]`. Spend a little time with this code. If you’re confused, I’d suggest
    experimenting with NumPy at the interactive Python prompt to understand what it’s
    doing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 它读取当前的音频文件，仅保留音频样本（`[1]`），并从音频样本中仅保留前两秒，每隔第100个样本提取一次，`[:N:w]`。花些时间理解这段代码。如果你感到困惑，我建议在交互式
    Python 环境中尝试使用 NumPy 来理解它的作用。
- en: In the end, we have train and test files for the 882 element vectors and associated
    labels. We’ll build our first models with these. [Figure 15-1](ch15.xhtml#ch15fig1)
    shows the resulting vector for a crying baby.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们得到了训练和测试文件，这些文件包含 882 元素的向量和相关标签。我们将使用这些数据构建我们的第一个模型。[图 15-1](ch15.xhtml#ch15fig1)显示了哭泣婴儿的特征向量。
- en: '![image](Images/15fig01.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/15fig01.jpg)'
- en: '*Figure 15-1: Feature vector for a crying baby*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-1：哭泣婴儿的特征向量*'
- en: The x-axis is sample number (think “time”), and the y-axis is the sample value.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: x 轴是样本编号（可以理解为“时间”），y 轴是样本值。
- en: Classifying the Audio Features
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类音频特征
- en: We have our training and test sets. Let’s build some models and see how they
    do. Since we have feature vectors, we can start quickly with classical models.
    After those, we can build some one-dimensional convolutional networks and see
    if they perform any better.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了训练集和测试集。让我们构建一些模型，看看它们的表现如何。由于我们有特征向量，可以很快地开始使用经典模型。之后，我们可以构建一些一维卷积神经网络，看看它们是否表现得更好。
- en: Using Classical Models
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用经典模型
- en: We can test the same suite of classical models we used in [Chapter 7](ch07.xhtml#ch07)
    with the breast cancer dataset. [Listing 15-6](ch15.xhtml#ch15lis6) has the setup
    code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在[第 7 章](ch07.xhtml#ch07)中使用的同一组经典模型来测试乳腺癌数据集。[示例 15-6](ch15.xhtml#ch15lis6)包含了设置代码。
- en: import numpy as np
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.neighbors import NearestCentroid
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import NearestCentroid
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier
- en: from sklearn.naive_bayes import GaussianNB
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.naive_bayes import GaussianNB
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import RandomForestClassifier
- en: from sklearn.svm import LinearSVC
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.svm import LinearSVC
- en: x_train = np.load("esc10_raw_train_audio.npy")[:,:,0]
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_raw_train_audio.npy")[:,:,0]
- en: y_train = np.load("esc10_raw_train_labels.npy")
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_raw_train_labels.npy")
- en: (*\pagebreak*)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: x_test  = np.load("esc10_raw_test_audio.npy")[:,:,0]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("esc10_raw_test_audio.npy")[:,:,0]
- en: y_test  = np.load("esc10_raw_test_labels.npy")
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_raw_test_labels.npy")
- en: ❶ x_train = (x_train.astype('float32') + 32768) / 65536
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x_train = (x_train.astype('float32') + 32768) / 65536
- en: x_test = (x_test.astype('float32') + 32768) / 65536
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = (x_test.astype('float32') + 32768) / 65536
- en: train(x_train, y_train, x_test, y_test)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: train(x_train, y_train, x_test, y_test)
- en: '*Listing 15-6: Classifying the audio features with classical models, part 1*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-6：使用经典模型对音频特征进行分类，第1部分*'
- en: Here we import the necessary model types, load the dataset, scale it, and then
    call a `train` function that we’ll introduce shortly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入必要的模型类型，加载数据集，进行缩放，然后调用一个`train`函数，我们稍后会介绍。
- en: Scaling is crucial here. Consider the y-axis range for [Figure 15-1](ch15.xhtml#ch15fig1).
    It goes from about –4000 to 4000\. We need to scale the data so that the range
    is smaller and the values are closer to being centered around 0\. Recall, for
    the MNIST and CIFAR-10 datasets, we divided by the maximum value to scale to [0,1].
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放在这里至关重要。考虑[图15-1](ch15.xhtml#ch15fig1)的y轴范围。它从大约–4000到4000。我们需要对数据进行缩放，以便范围更小，且数值更接近于围绕0中心。回想一下，对于MNIST和CIFAR-10数据集，我们通过最大值进行除法，将数据缩放到[0,1]。
- en: The sound samples are 16-bit signed integers. This means the full range of values
    they can take on covers [*–*32,768,+32,767]. If we make the samples floats, add
    32,768, and then divide by 65,536 (twice the lower value) ❶, we’ll get samples
    in the range [0,1), which is what we want.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 声音样本是16位带符号整数。这意味着它们可以取的值的完整范围是[*–*32,768,+32,767]。如果我们将样本转换为浮点数，添加32,768，然后除以65,536（即两倍的最小值）❶，我们将得到范围在[0,1)之间的样本，这正是我们所需要的。
- en: Training and evaluating the classical models is straightforward, as shown in
    [Listing 15-7](ch15.xhtml#ch15lis7).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估经典模型是直观的，如[示例 15-7](ch15.xhtml#ch15lis7)所示。
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: clf.fit(x_train, y_train)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: score = 100.0*clf.score(x_test, y_test)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: score = 100.0*clf.score(x_test, y_test)
- en: print("score = %0.2f%%" % score)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: print("score = %0.2f%%" % score)
- en: 'def train(x_train, y_train, x_test, y_test):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 'def train(x_train, y_train, x_test, y_test):'
- en: 'print("Nearest Centroid          : ", end='''')'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("最近质心          : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, NearestCentroid())
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, NearestCentroid())
- en: 'print("k-NN classifier (k=3)     : ", end='''')'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("k-NN分类器（k=3）     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=3))
- en: 'print("k-NN classifier (k=7)     : ", end='''')'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("k-NN分类器（k=7）     : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, KNeighborsClassifier(n_neighbors=7))
- en: 'print("Naive Bayes (Gaussian)    : ", end='''')'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("朴素贝叶斯（高斯）    : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, GaussianNB())
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, GaussianNB())
- en: 'print("Random Forest (trees=  5) : ", end='''')'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木= 5) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=5))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=5))
- en: 'print("Random Forest (trees= 50) : ", end='''')'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木= 50) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=50))
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=50))
- en: 'print("Random Forest (trees=500) : ", end='''')'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木=500) : ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=500))
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=500))
- en: 'print("Random Forest (trees=1000): ", end='''')'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("随机森林 (树木=1000): ", end='''')'
- en: run(x_train, y_train, x_test, y_test,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test,
- en: RandomForestClassifier(n_estimators=1000))
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier(n_estimators=1000))
- en: 'print("LinearSVM (C=0.01)        : ", end='''')'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=0.01)        : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.01))
- en: 'print("LinearSVM (C=0.1)         : ", end='''')'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=0.1)         : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=0.1))
- en: 'print("LinearSVM (C=1.0)         : ", end='''')'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=1.0)         : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=1.0))
- en: 'print("LinearSVM (C=10.0)        : ", end='''')'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("LinearSVM (C=10.0)        : ", end='''')'
- en: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: run(x_train, y_train, x_test, y_test, LinearSVC(C=10.0))
- en: '*Listing 15-7: Classifying the audio features with classical models, part 2*'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 15-7：使用经典模型对音频特征进行分类，第2部分*'
- en: 'The `train` function creates the particular model instances and then calls
    `run`. We saw this same code structure in [Chapter 7](ch07.xhtml#ch07). The `run`
    function uses `fit` to train the model and `score` to score the model on the test
    set. For the time being, we’ll evaluate the models based solely on their overall
    accuracy (the score). Running this code produces output like this:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`函数创建特定的模型实例，然后调用`run`。我们在[第7章](ch07.xhtml#ch07)中看到过这种相同的代码结构。`run`函数使用`fit`来训练模型，并使用`score`来在测试集上评估模型。暂时，我们将仅根据模型的总体准确度（得分）来评估这些模型。运行这段代码会产生类似如下的输出：'
- en: '[PRE2]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see very quickly that the classical models have performed terribly. Many
    of them are essentially guessing the class label. There are 10 classes, so random
    chance guessing should have an accuracy around 10 percent. The best-performing
    classical model is a Random Forest with 1,000 trees, but even that is performing
    at only 34.44 percent—far too low an overall accuracy to make the model one we’d
    care to use in most cases. The dataset is not a simple one, at least not for old-school
    approaches. Somewhat surprisingly, the Gaussian Naïve Bayes model is right 28
    percent of the time. Recall that the Gaussian Naïve Bayes expects the samples
    to be independent from one another. Here the independence assumption between the
    sound samples for a particular test input is not valid. The feature vector, in
    this case, represents a signal evolving in time, not a collection of features
    that are independent of each other.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就能看到经典模型表现得非常糟糕。其中许多模型基本上是在猜测类别标签。总共有10个类别，所以随机猜测的准确率应该大约为10%。表现最好的经典模型是一个拥有1,000棵树的随机森林，但即使如此，它的表现也仅为34.44%——总体准确率远远不足以使该模型在大多数情况下可用。数据集并不简单，至少对于老式方法来说并非如此。有些令人惊讶的是，高斯朴素贝叶斯模型的准确率达到了28%。请回忆，高斯朴素贝叶斯模型期望样本彼此独立。在这里，特定测试输入的声音样本之间的独立性假设是不成立的。在这种情况下，特征向量表示的是随时间演变的信号，而不是彼此独立的特征集合。
- en: 'The models that failed the most are Nearest Centroid, *k*-NN, and the linear
    SVMs. We have a reasonably high-dimensional input, 882 elements, but only 6,400
    of them in the training set. That is likely too few samples for the nearest neighbor
    classifiers to make use of—the feature space is too sparsely populated. Once again,
    the curse of dimensionality is rearing its ugly head. The linear SVM fails because
    the features seem not to be linearly separable. We did not try an RBF (Gaussian
    kernel) SVM, but we’ll leave that as an exercise for the reader. If you do try
    it, remember that there are now two hyperparameters to tune: *C* and *γ*.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 失败最多的模型是最近质心、*k*-NN和线性SVM。我们有一个相当高维的输入，882个元素，但训练集中只有6,400个元素。这对于最近邻分类器来说可能是太少的样本，因为特征空间的稀疏性太高。再次，维度灾难的问题又出现了。线性SVM失败是因为特征似乎不是线性可分的。我们没有尝试RBF（高斯核）SVM，但我们将其留给读者作为一个练习。如果你尝试了它，请记住现在有两个超参数要调整：*C*和*γ*。
- en: Using a Traditional Neural Network
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用传统神经网络
- en: We haven’t yet tried a traditional neural network. We could use the sklearn
    `MLPClassifier` class as we did before, but this is a good time to show how to
    implement a traditional network in Keras. [Listing 15-8](ch15.xhtml#ch15lis8)
    has the code.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有尝试传统的神经网络。我们可以像之前那样使用sklearn的`MLPClassifier`类，但现在是展示如何在Keras中实现传统网络的好时机。[列表15-8](ch15.xhtml#ch15lis8)包含了代码。
- en: import keras
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.models import Sequential
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras import backend as K
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: from keras import backend as K
- en: import numpy as np
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: batch_size = 32
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 32
- en: num_classes = 10
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: num_classes = 10
- en: epochs = 16
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 16
- en: nsamp = (882,1)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: nsamp = (882,1)
- en: x_train = np.load("esc10_raw_train_audio.npy")
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_raw_train_audio.npy")
- en: y_train = np.load("esc10_raw_train_labels.npy")
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_raw_train_labels.npy")
- en: x_test  = np.load("esc10_raw_test_audio.npy")
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: x_test  = np.load("esc10_raw_test_audio.npy")
- en: y_test  = np.load("esc10_raw_test_labels.npy")
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: y_test  = np.load("esc10_raw_test_labels.npy")
- en: x_train = (x_train.astype('float32') + 32768) / 65536
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = (x_train.astype('float32') + 32768) / 65536
- en: x_test = (x_test.astype('float32') + 32768) / 65536
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = (x_test.astype('float32') + 32768) / 65536
- en: y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = keras.utils.to_categorical(y_train, num_classes)
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = keras.utils.to_categorical(y_test, num_classes)
- en: model = Sequential()
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Dense(1024, activation='relu', input_shape=nsamp))
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(1024, activation='relu', input_shape=nsamp))
- en: model.add(Dropout(0.5))
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(512, activation='relu'))
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(512, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Flatten())
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(loss=keras.losses.categorical_crossentropy,
- en: optimizer=keras.optimizers.Adam(),
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=keras.optimizers.Adam(),
- en: metrics=['accuracy'])
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'])
- en: model.fit(x_train, y_train,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(x_train, y_train,
- en: batch_size=batch_size,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size,
- en: epochs=epochs,
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=epochs,
- en: verbose=0,
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=0,
- en: validation_data=(x_test, y_test))
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=(x_test, y_test))
- en: (*\pagebreak*)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: score = model.evaluate(x_test, y_test, verbose=0)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test, y_test, verbose=0)
- en: print('Test accuracy:', score[1])
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: print('测试准确率:', score[1])
- en: '*Listing 15-8: A traditional neural network in Keras*'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-8：Keras 中的传统神经网络*'
- en: 'After loading the necessary modules, we load the data itself and scale it as
    we did for the classical models. Next, we build the model architecture. We need
    only `Dense` layers and `Dropout` layers. We do put in a `Flatten` layer to eliminate
    the extra dimension (note the shape of `nsamp`) before the final softmax output.
    Unfortunately, this model does not improve things for us: we achieve an accuracy
    of only 27.6 percent.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载必要的模块后，我们加载数据并像处理经典模型时一样进行缩放。接下来，我们构建模型架构。我们只需要`Dense`层和`Dropout`层。为了消除额外的维度（注意`nsamp`的形状），我们在最终的
    softmax 输出之前加了一个`Flatten`层。不幸的是，这个模型对我们没有改善效果：我们只得到了27.6%的准确率。
- en: Using a Convolutional Neural Network
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用卷积神经网络
- en: Classical models and the traditional neural network don’t cut it. We should
    not be too surprised, but it was easy to give them a try. Let’s move on and apply
    a one-dimensional convolutional neural network to this dataset to see if it performs
    any better.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 经典模型和传统神经网络无法胜任。我们不应感到太惊讶，但尝试它们也很简单。接下来，我们将应用一维卷积神经网络（CNN）来看看它是否表现得更好。
- en: We haven’t worked with one-dimensional CNNs yet. Besides the structure of the
    input data, the only difference is that we replace calls to `Conv2D` and `MaxPooling2D`
    with calls to `Conv1D` and `MaxPooling1D`.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有使用过一维的 CNN。除了输入数据的结构外，唯一的区别是我们将`Conv2D`和`MaxPooling2D`的调用替换为`Conv1D`和`MaxPooling1D`的调用。
- en: The code for the first model we’ll try is shown in [Listing 15-9](ch15.xhtml#ch15lis9).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试的第一个模型的代码显示在[清单 15-9](ch15.xhtml#ch15lis9)中。
- en: import keras
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.models import Sequential
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras.layers import Conv1D, MaxPooling1D
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Conv1D, MaxPooling1D
- en: import numpy as np
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: batch_size = 32
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 32
- en: num_classes = 10
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: num_classes = 10
- en: epochs = 16
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 16
- en: nsamp = (882,1)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: nsamp = (882,1)
- en: x_train = np.load("esc10_raw_train_audio.npy")
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_raw_train_audio.npy")
- en: y_train = np.load("esc10_raw_train_labels.npy")
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_raw_train_labels.npy")
- en: x_test  = np.load("esc10_raw_test_audio.npy")
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("esc10_raw_test_audio.npy")
- en: y_test  = np.load("esc10_raw_test_labels.npy")
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_raw_test_labels.npy")
- en: x_train = (x_train.astype('float32') + 32768) / 65536
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = (x_train.astype('float32') + 32768) / 65536
- en: x_test = (x_test.astype('float32') + 32768) / 65536
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = (x_test.astype('float32') + 32768) / 65536
- en: y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = keras.utils.to_categorical(y_train, num_classes)
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = keras.utils.to_categorical(y_test, num_classes)
- en: model = Sequential()
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(32, kernel_size=3, activation='relu',
- en: input_shape=nsamp))
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape=nsamp))
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling1D(pool_size=3))
- en: model.add(Dropout(0.25))
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Flatten())
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(512, activation='relu'))
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(512, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(loss=keras.losses.categorical_crossentropy,
- en: optimizer=keras.optimizers.Adam(),
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=keras.optimizers.Adam(),
- en: metrics=['accuracy'])
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'])
- en: history = model.fit(x_train, y_train,
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: history = model.fit(x_train, y_train,
- en: batch_size=batch_size,
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size,
- en: epochs=epochs,
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=epochs,
- en: verbose=1,
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=1,
- en: validation_data=(x_test[:160], y_test[:160]))
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=(x_test[:160], y_test[:160]))
- en: score = model.evaluate(x_test[160:], y_test[160:], verbose=0)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test[160:], y_test[160:], verbose=0)
- en: print('Test accuracy:', score[1])
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: print('测试准确率:', score[1])
- en: '*Listing 15-9: A 1D CNN in Keras*'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-9：Keras 中的 1D CNN*'
- en: This model loads and preprocesses the dataset as before. This architecture,
    which we’ll call the *shallow* architecture, has a single convolutional layer
    of 32 filters with a kernel size of 3\. We’ll vary this kernel size in the same
    way we tried different 2D kernel sizes for the MNIST models. Following the `Conv1D`
    layer is a single max-pooling layer with a pool kernel size of 3\. `Dropout` and
    `Flatten` layers come next before a single `Dense` layer of 512 nodes with dropout.
    A softmax layer completes the architecture.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型和之前一样加载并预处理数据集。这个架构，我们称之为*浅层*架构，只有一个卷积层，包含32个大小为3的滤波器。我们会像尝试不同的2D卷积核大小一样，变化这个卷积核大小。`Conv1D`层之后是一个大小为3的最大池化层。接下来是`Dropout`层和`Flatten`层，最后是一个包含512个节点的`Dense`层，并添加了
    dropout。最后是一个 softmax 层来完成架构。
- en: We’ll train for 16 epochs using a batch size of 32\. We’ll keep the training
    history so we can examine the losses and validation performance as a function
    of epoch. There are 1,600 test samples. We’ll use 10 percent for the training
    validation and the remaining 90 percent for the overall accuracy. Finally, we’ll
    vary the `Conv1D` kernel size from 3 to 33 in an attempt to find one that works
    well with the training data.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将训练16个周期，使用批量大小32。我们会保留训练历史，以便能够查看每个周期的损失和验证性能。共有1,600个测试样本。我们将使用10%的数据作为训练验证，剩余的90%用于整体准确度。最后，我们将尝试将`Conv1D`的内核大小从3到33进行变化，以寻找一个与训练数据配合良好的大小。  '
- en: Let’s define four other architectures. We’ll refer to them as *medium*, *deep0*,
    *deep1*, and *deep2*. With no prior experience working with this data, it makes
    sense to try multiple architectures. At present, there’s no way to know ahead
    of time what the best architecture is for a new dataset. All we have is our previous
    experience.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们定义另外四种架构。我们将它们分别称为*medium*、*deep0*、*deep1*和*deep2*。由于没有处理过这些数据的经验，尝试多种架构是有意义的。目前，我们无法提前知道对于新数据集来说，最好的架构是什么。我们唯一能依靠的就是之前的经验。  '
- en: '[Listing 15-10](ch15.xhtml#ch15lis10) lists the specific architectures, separated
    by comments.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 15-10](ch15.xhtml#ch15lis10)列出了具体的架构，并用注释分隔开。  '
- en: medium
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'medium  '
- en: model = Sequential()
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 'model = Sequential()  '
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(32, kernel_size=3, activation=''relu'',  '
- en: input_shape=nsamp))
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 'input_shape=nsamp))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(64, kernel_size=3, activation='relu'))
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Flatten())
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Flatten())  '
- en: model.add(Dense(512, activation='relu'))
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dense(512, activation=''relu''))  '
- en: model.add(Dropout(0.5))
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.5))  '
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dense(num_classes, activation=''softmax''))  '
- en: deep0
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'deep0  '
- en: model = Sequential()
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 'model = Sequential()  '
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(32, kernel_size=3, activation=''relu'',  '
- en: input_shape=nsamp))
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 'input_shape=nsamp))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Flatten())
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Flatten())  '
- en: model.add(Dense(512, activation='relu'))
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dense(512, activation=''relu''))  '
- en: model.add(Dropout(0.5))
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.5))  '
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dense(num_classes, activation=''softmax''))  '
- en: deep1
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'deep1  '
- en: model = Sequential()
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 'model = Sequential()  '
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(32, kernel_size=3, activation=''relu'',  '
- en: input_shape=nsamp))
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 'input_shape=nsamp))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Flatten())
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Flatten())  '
- en: model.add(Dense(512, activation='relu'))
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dense(512, activation=''relu''))  '
- en: model.add(Dropout(0.5))
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.5))  '
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dense(num_classes, activation=''softmax''))  '
- en: deep2
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'deep2  '
- en: model = Sequential()
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 'model = Sequential()  '
- en: model.add(Conv1D(32, kernel_size=3, activation='relu',
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(32, kernel_size=3, activation=''relu'',  '
- en: input_shape=nsamp))
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 'input_shape=nsamp))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(MaxPooling1D(pool_size=3))  '
- en: model.add(Dropout(0.25))
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Dropout(0.25))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 'model.add(Conv1D(64, kernel_size=3, activation=''relu''))  '
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(64, kernel_size=3, activation='relu'))
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling1D(pool_size=3))
- en: model.add(Dropout(0.25))
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(64, kernel_size=3, activation='relu'))
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(64, kernel_size=3, activation='relu'))
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling1D(pool_size=3))
- en: model.add(Dropout(0.25))
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(64, kernel_size=3, activation='relu'))
- en: model.add(Conv1D(64, kernel_size=3, activation='relu'))
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv1D(64, kernel_size=3, activation='relu'))
- en: model.add(MaxPooling1D(pool_size=3))
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling1D(pool_size=3))
- en: model.add(Dropout(0.25))
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Flatten())
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(512, activation='relu'))
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(512, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: '*Listing 15-10: Different 1D CNN architect*'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-10：不同的 1D CNN 架构*'
- en: If we train multiple models, varying the first `Conv1D` kernel size each time,
    we get the results in [Table 15-1](ch15.xhtml#ch15tab1). We’ve highlighted the
    best-performing model for each architecture.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练多个模型，每次改变第一个`Conv1D`卷积核的大小，就能得到[表 15-1](ch15.xhtml#ch15tab1)中的结果。我们已经标出每种架构下表现最好的模型。
- en: '**Table 15-1:** Test Set Accuracies by Convolutional Kernel Size and Model
    Architecture'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 15-1：** 通过卷积核大小和模型架构测试集准确率'
- en: '| **Kernel size** | **Shallow** | **Medium** | **Deep0** | **Deep1** | **Deep2**
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| **核大小** | **浅层** | **中层** | **深层0** | **深层1** | **深层2** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 3 | **44.51** | 41.39 | **48.75** | **54.03** | 9.93 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 3 | **44.51** | 41.39 | **48.75** | **54.03** | 9.93 |'
- en: '| 5 | 43.47 | 41.74 | 44.72 | 53.96 | 48.47 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 43.47 | 41.74 | 44.72 | 53.96 | 48.47 |'
- en: '| 7 | 38.47 | 40.97 | 46.18 | 52.64 | 49.31 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 38.47 | 40.97 | 46.18 | 52.64 | 49.31 |'
- en: '| 9 | 41.46 | **43.06** | 46.88 | 48.96 | 9.72 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 41.46 | **43.06** | 46.88 | 48.96 | 9.72 |'
- en: '| 11 | 39.65 | 40.21 | 45.21 | 52.99 | 10.07 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 39.65 | 40.21 | 45.21 | 52.99 | 10.07 |'
- en: '| 13 | 42.71 | 41.67 | 46.53 | 50.56 | **52.57** |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 42.71 | 41.67 | 46.53 | 50.56 | **52.57** |'
- en: '| 15 | 40.00 | 42.78 | 46.53 | 50.14 | 47.08 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 40.00 | 42.78 | 46.53 | 50.14 | 47.08 |'
- en: '| 33 | 27.57 | 42.22 | 41.39 | 48.75 | 9.86 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 33 | 27.57 | 42.22 | 41.39 | 48.75 | 9.86 |'
- en: Looking at [Table 15-1](ch15.xhtml#ch15tab1), we see a general trend of accuracy
    improving as the model depth increases. However, at the deep2 model, things start
    to fall apart. Some of the models fail to converge, showing an accuracy equivalent
    to random guessing. The deep1 model is the best performing for all kernel sizes.
    When looking across by kernel size, the kernel with width 3 is the best performing
    for three of the five architectures. All of this implies that the best combination
    for the 1D CNNs is to use an initial kernel of width 3 and the deep1 architecture.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 看[表 15-1](ch15.xhtml#ch15tab1)，我们可以看到一个普遍的趋势，即随着模型深度的增加，准确率逐渐提高。然而，在深层2模型下，情况开始变差。部分模型未能收敛，显示出与随机猜测相当的准确率。深层1模型在所有卷积核大小下表现最佳。在不同的卷积核大小下，宽度为3的卷积核在五种架构中有三种表现最好。所有这些都表明，对于1D卷积神经网络，最佳的组合是使用宽度为3的初始卷积核和深层1架构。
- en: We trained this architecture for only 16 epochs. Will things improve if we train
    for more? Let’s train the deep1 model for 60 epochs and plot the training and
    validation loss and error to see how they converge (or don’t). Doing this produces
    [Figure 15-2](ch15.xhtml#ch15fig2), where we see the training and validation loss
    (top) and error (bottom) as a function of epoch.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只训练了这个架构16个周期。如果我们训练更多周期，结果会有所改善吗？让我们训练深层1模型60个周期，并绘制训练和验证的损失及错误，看看它们是否收敛（或者没有）。这样做会产生[图
    15-2](ch15.xhtml#ch15fig2)，我们可以看到训练和验证损失（顶部）以及错误（底部）随着周期数变化的情况。
- en: '![image](Images/15fig02.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/15fig02.jpg)'
- en: '*Figure 15-2: Training and validation loss (top) and error (bottom) for the
    deep1 architecture*'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-2：深层1架构的训练和验证损失（顶部）以及错误（底部）*'
- en: Immediately, we should pick up on the explosion of the loss for the validation
    set. The training loss is continually decreasing until after about epoch 18 or
    so; then the validation loss goes up and becomes oscillatory. This is a clear
    example of overfitting. The likely source of this overfitting is our limited training
    set size, only 6,400 samples, even after data augmentation. The validation error
    remains more or less constant after initially decreasing. The conclusion is that
    we cannot expect to do much better than an overall accuracy of about 54 percent
    for this dataset using one-dimensional vectors.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 立即，我们应该注意到验证集的损失突然爆炸。训练损失持续减少，直到大约第18个周期左右，然后验证损失开始上升并变得波动。这是过拟合的明显例子。导致这种过拟合的可能原因是我们的训练集规模有限，只有6400个样本，即使经过了数据增强。验证误差在最初下降后保持大致恒定。结论是，我们不能指望在使用一维向量的情况下，对这个数据集的总体准确率做得更好，最多大约是54%。
- en: If we want to improve, we need to be more expressive with our dataset. Fortunately
    for us, we have another preprocessing trick up our sleeves.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要改进，我们需要对我们的数据集进行更具表现力的处理。幸运的是，我们还有另一个预处理技巧可以使用。
- en: Spectrograms
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 声谱图
- en: Let’s return to our augmented set of audio files. To build the dataset, we took
    the sound samples, keeping only two seconds’ worth and only every 100th sample.
    The best we could do is an accuracy of a little more than 50 percent.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到扩充后的音频文件集。为了构建数据集，我们提取了声音样本，只保留了2秒钟的样本，并且只选取每100个样本中的一个。我们能做到的最好结果是准确率略高于50%。
- en: However, if we work with a small set of sound samples from an input audio file,
    say 200 milliseconds worth, we can use the vector of samples to calculate the
    *Fourier transform*. The Fourier transform of a signal measured at regular intervals
    tells us the frequencies that went into building the signal. Any signal can be
    thought of as the sum of many different sine and cosine waves. If the signal is
    composed of only a few waves, like the sound you might get from an instrument
    like the ocarina, then the Fourier transform will have essentially a few peaks
    at those frequencies. If the signal is complex, like speech or music, then the
    Fourier transform will have many different frequencies, leading to many different
    peaks.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们仅使用输入音频文件中的一小段声音样本，例如200毫秒的样本，我们可以使用这些样本的向量来计算*傅里叶变换*。在规则时间间隔内测量的信号的傅里叶变换告诉我们构建该信号所使用的频率。任何信号都可以看作是许多不同的正弦波和余弦波的和。如果信号只由少数几个波形组成，比如你可能从短笛等乐器中听到的声音，那么傅里叶变换将会在这些频率上有几个峰值。如果信号很复杂，比如语音或音乐，那么傅里叶变换将包含许多不同的频率，导致多个不同的峰值。
- en: 'The Fourier transform itself is complex-valued: each element has both a real
    and an imaginary component. You can write it as *a* + *bi*, where *a* and *b*
    are real numbers and ![Image](Images/394equ01.jpg). If we use the absolute value
    of these quantities, we’ll get a real number representing the energy of a particular
    frequency. This is called the *power spectrum* of the signal. A simple tone might
    have energy in only a few frequencies, while something like a cymbal crash or
    white noise will have energy more or less evenly distributed among all frequencies.
    [Figure 15-3](ch15.xhtml#ch15fig3) shows two power spectra.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 傅里叶变换本身是复值的：每个元素都有实部和虚部。你可以将其写成 *a* + *bi*，其中 *a* 和 *b* 是实数，且![Image](Images/394equ01.jpg)。如果我们使用这些量的绝对值，我们将得到一个实数，表示某一特定频率的能量。这被称为信号的*功率谱*。一个简单的音调可能只有几个频率上的能量，而像钹撞击声或白噪声之类的声音将在所有频率之间更均匀地分布能量。[图
    15-3](ch15.xhtml#ch15fig3)展示了两个功率谱。
- en: '![image](Images/15fig03.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/15fig03.jpg)'
- en: '*Figure 15-3: Power spectrum of an ocarina (top) and cymbal (bottom)*'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-3：短笛（上）和钹（下）的功率谱*'
- en: On the top is the spectrum of an ocarina, and on the bottom is a cymbal crash.
    As expected, the ocarina has energy in only a few frequencies, while the cymbal
    uses all the frequencies. The important point for us is that *visually* the spectra
    are quite different from each other. (The spectra were made with Audacity, an
    excellent open source audio processing tool.)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是短笛的频谱，下面是钹的撞击声。正如预期，短笛仅在少数几个频率上有能量，而钹则使用了所有频率。对我们来说，重要的一点是，*从视觉上看*，这两个频谱差异非常大。（这些频谱是用Audacity制作的，Audacity是一个出色的开源音频处理工具。）
- en: We could use these power spectra as feature vectors, but they represent only
    the spectra of tiny slices of time. The sound samples are five seconds long. Instead
    of using a spectrum, we will use a *spectrogram*. The spectrogram is an image
    made up of columns that represent individual spectra. This means that the x-axis
    represents time and the y-axis represents frequency. The color of a pixel is proportional
    to the energy in that frequency at that time.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些功率谱作为特征向量，但它们仅代表极短时间片段的谱。声音样本的时长为五秒钟。我们将使用*谱图*而不是使用频谱。谱图是由表示单个频谱的列组成的图像。这意味着x轴代表时间，y轴代表频率。像素的颜色与该时间点该频率的能量成正比。
- en: In other words, a spectrogram is what we get if we orient the power spectra
    vertically and use color to represent intensity at a given frequency. With this
    approach, we can turn an entire sound sample into an image. For example, [Figure
    15-4](ch15.xhtml#ch15fig4) shows the spectrogram of a crying baby. Compare this
    to the feature vector of [Figure 15-1](ch15.xhtml#ch15fig1).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，**谱图**是我们将功率谱垂直排列并使用颜色来表示给定频率下的强度时得到的结果。通过这种方法，我们可以将整个声音样本转化为一幅图像。例如，[图
    15-4](ch15.xhtml#ch15fig4)展示了一个哭泣婴儿的谱图。可以将其与[图 15-1](ch15.xhtml#ch15fig1)中的特征向量进行对比。
- en: '![image](Images/15fig04.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/15fig04.jpg)'
- en: '*Figure 15-4: Spectrogram of a crying baby*'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-4：哭泣婴儿的谱图*'
- en: 'To create spectrograms of the augmented audio files, we need a new tool and
    a bit of code. The tool we need is called `sox`. It’s not a Python library, but
    a command line tool. Odds are that it is already installed if you are using our
    canonical Ubuntu Linux distribution. If not, you can install it:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建增强音频文件的谱图，我们需要一个新的工具和一些代码。我们需要的工具叫做`sox`。它不是Python库，而是一个命令行工具。如果你正在使用我们的标准Ubuntu
    Linux发行版，它很可能已经安装。如果没有，你可以安装它：
- en: '[PRE3]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ll use `sox` from inside a Python script to produce the spectrogram images
    we want. Each sound file becomes a new spectrogram image.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Python脚本中使用`sox`来生成我们想要的谱图图像。每个声音文件都会变成一张新的谱图图像。
- en: The source code to process the training images is in [Listing 15-11](ch15.xhtml#ch15lis11).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 处理训练图像的源代码在[清单 15-11](ch15.xhtml#ch15lis11)中。
- en: import os
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import numpy as np
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from PIL import Image
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: from PIL import Image
- en: rows = 100
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: rows = 100
- en: cols = 160
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: cols = 160
- en: ❶ flist = [i[:-1] for i in open("augmented_train_filelist.txt")]
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ flist = [i[:-1] for i in open("augmented_train_filelist.txt")]
- en: N = len(flist)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: N = len(flist)
- en: img = np.zeros((N,rows,cols,3), dtype="uint8")
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: img = np.zeros((N,rows,cols,3), dtype="uint8")
- en: lbl = np.zeros(N, dtype="uint8")
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: lbl = np.zeros(N, dtype="uint8")
- en: p = []
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: p = []
- en: 'for i,f in enumerate(flist):'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,f in enumerate(flist):'
- en: src, c = f.split()
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: src, c = f.split()
- en: ❷ os.system("sox %s -n spectrogram" % src)
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ os.system("sox %s -n spectrogram" % src)
- en: im = np.array(Image.open("spectrogram.png").convert("RGB"))
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: im = np.array(Image.open("spectrogram.png").convert("RGB"))
- en: ❸ im = im[42:542,58:858,:]
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ im = im[42:542,58:858,:]
- en: im = Image.fromarray(im).resize((cols,rows))
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: im = Image.fromarray(im).resize((cols,rows))
- en: img[i,:,:,:] = np.array(im)
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: img[i,:,:,:] = np.array(im)
- en: lbl[i] = int(c)
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: lbl[i] = int(c)
- en: p.append(os.path.abspath(src))
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: p.append(os.path.abspath(src))
- en: os.system("rm -rf spectrogram.png")
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: os.system("rm -rf spectrogram.png")
- en: p = np.array(p)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: p = np.array(p)
- en: ❹ idx = np.argsort(np.random.random(N))
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ idx = np.argsort(np.random.random(N))
- en: img = img[idx]
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: img = img[idx]
- en: lbl = lbl[idx]
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: lbl = lbl[idx]
- en: p = p[idx]
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: p = p[idx]
- en: np.save("esc10_spect_train_images.npy", img)
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_spect_train_images.npy", img)
- en: np.save("esc10_spect_train_labels.npy", lbl)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_spect_train_labels.npy", lbl)
- en: np.save("esc10_spect_train_paths.npy", p)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("esc10_spect_train_paths.npy", p)
- en: '*Listing 15-11: Building the spectrograms*'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15-11：构建谱图*'
- en: We start by defining the size of the spectrogram. This is the input to our model,
    and we don’t want it to be too big because we’re limited in the size of the inputs
    we can process. We’ll settle for 100×160 pixels. We then load the training file
    list ❶ and create NumPy arrays to hold the spectrogram images and associated labels.
    The list `p` will hold the pathname of the source for each spectrogram in case
    we want to get back to the original sound file at some point. In general, it’s
    a good idea to preserve information to get back to the source of derived datasets.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义谱图的大小。这是模型的输入，我们不希望它太大，因为我们处理输入的大小是有限制的。我们决定将其设为100×160像素。然后我们加载训练文件列表❶，并创建NumPy数组来存储谱图图像和相关标签。列表`p`将保存每个谱图源的路径，以便我们在某个时候需要回到原始声音文件时使用。一般来说，保存返回源数据集的信息是个好主意。
- en: Then we loop over the file list. We get the filename and class label and then
    call `sox`, passing in the source sound filename ❷. The `sox` application is sophisticated.
    The syntax here turns the given sound file into a spectrogram image with the name
    *spectrogram.png*. We immediately load the output spectrogram into `im`, making
    sure it’s an RGB file with no transparency layer (hence the call to `convert("RGB")`).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历文件列表。获取文件名和类别标签，然后调用 `sox`，传入源声音文件名❷。`sox` 应用程序非常复杂。此处的语法将给定的声音文件转换为名为
    *spectrogram.png* 的频谱图图像。我们立即将输出的频谱图加载到 `im` 中，并确保它是一个没有透明层的 RGB 文件（因此调用 `convert("RGB")`）。
- en: The spectrogram created by `sox` has a border with frequency and time information.
    We want only the spectrogram image portion, so we subset the image ❸. We determined
    the indices we’re using empirically. It’s possible, but somewhat unlikely, that
    a newer version of `sox` will require tweaking these to avoid including any border
    pixels.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '`sox` 创建的频谱图有一个包含频率和时间信息的边框。我们只需要频谱图的图像部分，因此我们对图像进行子集化❸。我们使用的索引是通过经验确定的。虽然不太可能，但新版的
    `sox` 可能需要调整这些索引，以避免包含任何边框像素。'
- en: Next, we resize the spectrogram so that it fits in our 100×160 pixel array.
    This is downsampling, true, but hopefully enough characteristic information is
    still present to allow a model to learn the difference between classes. We keep
    the downsampled spectrogram and the associated class label and sound file path.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将频谱图调整大小，使其适应我们的 100×160 像素数组。这是下采样，没错，但希望足够的特征信息仍然保留，可以让模型学习类别之间的差异。我们保留下采样后的频谱图以及相关的类别标签和声音文件路径。
- en: When we’ve generated all the spectrograms, the loop ends, and we remove the
    final extraneous spectrogram PNG file. We convert the list of sound file paths
    to a NumPy array so we can store it in the same manner as the images and labels.
    Finally, we randomize the order of the images as a precaution against any implicit
    sorting that might group classes ❹. This is so that minibatches extracted sequentially
    are representative of the mix of classes as a whole. To conclude, we write the
    images, labels, and pathnames to disk. We repeat this entire process for the test
    set.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们生成了所有的频谱图后，循环结束，我们删除最后一个多余的频谱图 PNG 文件。我们将声音文件路径的列表转换为 NumPy 数组，以便可以像存储图像和标签一样存储它。最后，为了防止任何可能按顺序排序而将同类归为一组的隐式排序，我们随机打乱图像的顺序❹。这样，顺序提取的小批量数据就能代表所有类别的混合。最后，我们将图像、标签和路径名写入磁盘。我们为测试集重复整个过程。
- en: Are we able to visually tell the difference between the spectrograms of different
    classes? If we can do that easily, then we have a good shot of getting a model
    to tell the difference, too. [Figure 15-5](ch15.xhtml#ch15fig5) shows 10 spectrograms
    of the same class in each row.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否能够从视觉上区分不同类别的频谱图？如果我们能轻松做到这一点，那么我们就有很好的机会让模型也能区分它们。[图 15-5](ch15.xhtml#ch15fig5)
    显示了每一行来自同一类别的 10 个频谱图。
- en: '![image](Images/15fig05.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/15fig05.jpg)'
- en: '*Figure 15-5: Sample spectrograms for each class in ESC-10\. Each row shows
    10 examples from the same class.*'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15-5：ESC-10 中每个类别的样本频谱图。每一行展示了来自同一类别的 10 个示例。*'
- en: Visually, we can usually tell the spectra apart, which is encouraging. With
    our spectrograms in hand, we are ready to try some 2D CNNs to see if they do better
    than the 1D CNNs.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上来看，我们通常能够区分不同的频谱图，这令人鼓舞。有了这些频谱图，我们可以尝试一些 2D CNN，看看它们是否比 1D CNN 更有效。
- en: Classifying Spectrograms
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 频谱图分类
- en: To work with the spectrogram dataset, we need 2D CNNs. A possible starting point
    is to convert the shallow 1D CNN architecture to 2D by changing `Conv1D` to `Conv2D`,
    and `MaxPooling1D` to `MaxPooling2D`. However, if we do this, the resulting model
    has 30.7 million parameters, which is many more than we want to work with. Instead,
    let’s opt for a deeper architecture that has fewer parameters and then explore
    the effect of different first convolutional layer kernel sizes. The code is in
    [Listing 15-12](ch15.xhtml#ch15lis12).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理频谱图数据集，我们需要 2D CNN。一个可能的起点是将浅层的 1D CNN 架构转化为 2D，通过将 `Conv1D` 改为 `Conv2D`，将
    `MaxPooling1D` 改为 `MaxPooling2D`。然而，如果我们这么做，得到的模型将有 3070 万个参数，这比我们想要处理的要多得多。因此，我们选择一种参数较少的更深层架构，并探索不同的第一个卷积层内核大小的效果。代码见[清单
    15-12](ch15.xhtml#ch15lis12)。
- en: import keras
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: import keras
- en: from keras.models import Sequential
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import Sequential
- en: from keras.layers import Dense, Dropout, Flatten
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Dense, Dropout, Flatten
- en: from keras.layers import Conv2D, MaxPooling2D
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import Conv2D, MaxPooling2D
- en: import numpy as np
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: batch_size = 16
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 16
- en: num_classes = 10
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: num_classes = 10
- en: epochs = 16
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 16
- en: img_rows, img_cols = 100, 160
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: img_rows, img_cols = 100, 160
- en: input_shape = (img_rows, img_cols, 3)
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape = (img_rows, img_cols, 3)
- en: x_train = np.load("esc10_spect_train_images.npy")
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("esc10_spect_train_images.npy")
- en: y_train = np.load("esc10_spect_train_labels.npy")
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("esc10_spect_train_labels.npy")
- en: x_test = np.load("esc10_spect_test_images.npy")
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("esc10_spect_test_images.npy")
- en: y_test = np.load("esc10_spect_test_labels.npy")
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_spect_test_labels.npy")
- en: x_train = x_train.astype('float32') / 255
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x_train.astype('float32') / 255
- en: x_test = x_test.astype('float32') / 255
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x_test.astype('float32') / 255
- en: y_train = keras.utils.to_categorical(y_train, num_classes)
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = keras.utils.to_categorical(y_train, num_classes)
- en: y_test = keras.utils.to_categorical(y_test, num_classes)
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = keras.utils.to_categorical(y_test, num_classes)
- en: model = Sequential()
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv2D(32, kernel_size=(3,3), activation='relu',
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(32, kernel_size=(3,3), activation='relu',
- en: input_shape=input_shape))
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: input_shape=input_shape))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Dropout(0.25))
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Dropout(0.25))
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.25))
- en: model.add(Flatten())
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(128, activation='relu'))
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(128, activation='relu'))
- en: model.add(Dropout(0.5))
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dropout(0.5))
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: model.compile(loss=keras.losses.categorical_crossentropy,
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(loss=keras.losses.categorical_crossentropy,
- en: optimizer=keras.optimizers.Adam(),
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=keras.optimizers.Adam(),
- en: metrics=['accuracy'])
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'])
- en: history = model.fit(x_train, y_train,
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: history = model.fit(x_train, y_train,
- en: batch_size=batch_size, epochs=epochs,
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size=batch_size, epochs=epochs,
- en: verbose=0, validation_data=(x_test, y_test))
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=0, validation_data=(x_test, y_test))
- en: score = model.evaluate(x_test, y_test, verbose=0)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: score = model.evaluate(x_test, y_test, verbose=0)
- en: print('Test accuracy:', score[1])
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: print('测试准确率:', score[1])
- en: model.save("esc10_cnn_deep_3x3_model.h5")
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: model.save("esc10_cnn_deep_3x3_model.h5")
- en: '*Listing 15-12: Classifying spectrograms*'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-12: 分类声谱图*'
- en: Here we are using a minibatch size of 16 for 16 epochs along with the Adam optimizer.
    The model architecture has two convolutional layers, a max-pooling layer with
    dropout, another convolutional layer, and a second max-pooling layer with dropout.
    There is a single dense layer of 128 nodes before the softmax output.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了大小为16的迷你批次，训练16个周期，并使用Adam优化器。模型架构包含两个卷积层，一个带有丢弃的最大池化层，再一个卷积层，和第二个带丢弃的最大池化层。在softmax输出层之前，有一个128节点的全连接层。
- en: 'We’ll test two kernel sizes for the first convolutional layer: 3 × 3 and 7
    × 7\. The 3 × 3 configuration is shown in [Listing 15-12](ch15.xhtml#ch15lis12).
    Replace `(3,3)` with `(7,7)` to alter the size. All the initial 1D convolutional
    runs used a single training of the model for evaluation. We know that because
    of random initialization, we’ll get slightly different results from training to
    training, even if nothing else changes. For the 2D CNNs, let’s train each model
    six times and present the overall accuracy as a mean ± standard error of the mean.
    Doing just this gives us the following overall accuracies:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试第一卷积层的两种卷积核大小：3 × 3 和 7 × 7。3 × 3 配置示于[Listing 15-12](ch15.xhtml#ch15lis12)。要更改大小，只需将
    `(3,3)` 替换为 `(7,7)`。所有最初的1D卷积运行都使用了模型的单次训练进行评估。我们知道，由于随机初始化，即使其他条件不变，训练结果也会略有不同。对于2D卷积神经网络，我们将每个模型训练六次，并呈现整体准确率的均值
    ± 标准误差。仅此操作，我们得到以下的整体准确率：
- en: '| **Kernel size** | **Score** |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| **卷积核大小** | **准确率** |'
- en: '| --- | --- |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 3 × 3 | 78.78 ± 0.60% |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 3 × 3 | 78.78 ± 0.60% |'
- en: '| 7 × 7 | 78.44 ± 0.72% |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 7 × 7 | 78.44 ± 0.72% |'
- en: This indicates that there is no meaningful difference between using a 3 × 3
    initial convolutional layer kernel size or a 7 × 7\. Therefore, we’ll stick with
    3 × 3 going forward.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明使用3 × 3的初始卷积核大小与7 × 7之间没有显著差异。因此，我们将在接下来的训练中继续使用3 × 3。
- en: '[Figure 15-6](ch15.xhtml#ch15fig6) shows the training and validation loss (top)
    and error (bottom) for one run of the 2D CNN trained on the spectrograms. As we
    saw in the 1D CNN case, after only a few epochs, the validation error starts to
    increase.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 15-6](ch15.xhtml#ch15fig6) 显示了一个2D CNN在声谱图上训练和验证的损失（顶部）和错误（底部）。正如我们在1D
    CNN的情况下所看到的，经过几个周期后，验证错误开始增加。'
- en: 'The 2D CNN performs significantly better than the 1D CNN did: 79 percent accuracy
    versus only 54 percent. This level of accuracy is still not particularly useful
    for many applications, but for others, it might be completely acceptable. Nevertheless,
    we’d like to do better if we can. It’s worth noting that we have a few limitations
    in our data and, for that matter, our hardware, since we are restricting ourselves
    to a CPU-only approach, which limits the amount of time we are willing to wait
    for models to train. Here is where the some 25-fold increase in performance possible
    with GPUs would be helpful, assuming our use case allows for using GPUs. If we’re
    planning to run the model on an embedded system, for example, we might not have
    a GPU available, so we’d want to stick with a smaller model anyway.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 2D CNN的表现明显优于1D CNN：79% 的准确率，而1D CNN仅为54%。对于许多应用来说，这个准确率水平仍然不是特别有用，但对于其他应用可能完全可以接受。尽管如此，我们仍希望能够做得更好。值得注意的是，我们的数据和硬件都有一些限制，因为我们限制在仅使用CPU的方法上，这限制了我们愿意等待模型训练的时间。这正是GPU性能可以有25倍增长的情况有所帮助的地方，假设我们的使用情况允许使用GPU。例如，如果我们计划在嵌入式系统上运行模型，可能没有GPU可用，因此无论如何我们都希望坚持使用较小的模型。
- en: '![image](Images/15fig06.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![图15-6：2D CNN架构的训练和验证损失（顶部）和错误（底部）](Images/15fig06.jpg)'
- en: '*Figure 15-6: Training and validation loss (top) and error (bottom) for the
    2D CNN architecture*'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15-6：2D CNN架构的训练和验证损失（顶部）和错误（底部）*'
- en: Initialization, Regularization, and Batch Normalization
  id: totrans-523
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始化、正则化和批量归一化
- en: The literature tells us that there are other things we can try. We already augmented
    the dataset, a powerful technique, and we are using dropout, another powerful
    technique. We can try using a new initialization strategy, He initialization,
    which has been shown to often work better than Glorot initialization, the Keras
    default. We can also try applying L2 regularization, which Keras implements as
    weight decay per layer. See [Chapter 10](ch10.xhtml#ch10) for a refresher on these
    techniques.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 文献告诉我们还有其他一些可以尝试的方法。我们已经增加了数据集，这是一种强大的技术，我们正在使用的是dropout，另一种强大的技术。我们可以尝试使用新的初始化策略，He初始化，据称通常比Glorot初始化效果更好，后者是Keras的默认设置。我们还可以尝试应用L2正则化，Keras将其实现为每层的权重衰减。有关这些技术的刷新，请参阅[第10章](ch10.xhtml#ch10)。
- en: 'To set the layer initialization algorithm, we need to add the following keyword
    to the `Conv2D` and first `Dense` layer:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置层初始化算法，我们需要向`Conv2D`和第一个`Dense`层添加以下关键字：
- en: '[PRE4]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To add L2 regularization, we add the following keyword to the `Conv2D` and
    first `Dense` layer:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加L2正则化，我们需要向`Conv2D`和第一个`Dense`层添加以下关键字：
- en: '[PRE5]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here *λ* = 0.001\. Recall, *λ* is the L2 regularization scale factor.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 *λ* = 0.001\. 请记住，*λ* 是L2正则化的比例因子。
- en: 'We could test these together, but instead we’ve tested them individually to
    see what effect, if any, they have for this dataset. Training six models as before
    gives the following overall accuracies:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以测试这些方法的联合效果，但我们选择单独测试它们，以查看它们是否对该数据集有任何影响。与以前相同，训练六个模型得到以下总体准确率：
- en: '| **Regularizer** | **Score** |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| **正则化器** | **得分** |'
- en: '| --- | --- |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| He initialization | 78.5 ± 0.5% |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| He初始化 | 78.5 ± 0.5% |'
- en: '| L2 regularization | 78.3 ± 0.4% |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| L2正则化 | 78.3 ± 0.4% |'
- en: This is no different, statistically, from the previous results. In this case,
    these approaches are neither beneficial nor detrimental.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计上讲，这与先前的结果没有什么不同。在这种情况下，这些方法既不有利也不有害。
- en: 'Batch normalization is another well-tested, go-to technique widely used by
    the machine learning community. We mentioned batch normalization briefly in [Chapter
    12](ch12.xhtml#ch12). Batch normalization does just what its name suggests: it
    normalizes the inputs to a layer of the network, subtracting per feature means
    and dividing by the per feature standard deviations. The output of the layer multiplies
    the normalized input by a constant and adds an offset. The net effect is the input
    values are mapped to new output values by a two-step process: normalize the input
    and then apply a linear transform to get the output. The parameters of the linear
    transform are learned during backprop. At inference time, means and standard deviations
    learned from the dataset are applied to unknown inputs.'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是另一种经过充分验证的、广泛应用于机器学习社区的技术。我们在[第12章](ch12.xhtml#ch12)中简要提到了批量归一化。批量归一化正如其名称所示：它对网络层的输入进行归一化，减去每个特征的均值，并除以每个特征的标准差。该层的输出将归一化后的输入乘以一个常数并加上一个偏移量。最终效果是：通过两步过程将输入值映射到新的输出值：首先归一化输入，然后应用线性变换得到输出。线性变换的参数在反向传播过程中学习得到。在推理时，来自数据集学习到的均值和标准差会应用到未知的输入上。
- en: Batch normalization has shown itself time and again to be effective, especially
    in speeding up training. Machine learning researchers are still debating the exact
    reasons *why* it works as it does. To use it in Keras, you simply insert batch
    normalization after the convolutional and dense layers of the network (and after
    any activation function like ReLU used by those layers). Batch normalization is
    known to not work well with dropout, so we’ll also remove the Dropout layers.
    The relevant architecture portion of the model code is shown in [Listing 15-13](ch15.xhtml#ch15lis13).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化一次又一次地证明了其有效性，尤其是在加速训练方面。机器学习研究人员仍在讨论它为什么会如此有效的具体原因。要在 Keras 中使用批量归一化，您只需在网络的卷积层和全连接层之后（以及这些层使用的任何激活函数，如
    ReLU 之后）插入批量归一化。已知批量归一化与丢弃法配合使用效果不佳，因此我们也将移除丢弃层。模型代码的相关架构部分显示在[列表 15-13](ch15.xhtml#ch15lis13)中。
- en: from keras.layers import BatchNormalization
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.layers import BatchNormalization
- en: model = Sequential()
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: model = Sequential()
- en: model.add(Conv2D(32, kernel_size=(3, 3),
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(32, kernel_size=(3, 3),
- en: activation='relu', input_shape=input_shape))
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: activation='relu', input_shape=input_shape))
- en: model.add(BatchNormalization())
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(BatchNormalization())
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(BatchNormalization())
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(BatchNormalization())
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Conv2D(64, (3, 3), activation='relu'))
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Conv2D(64, (3, 3), activation='relu'))
- en: model.add(BatchNormalization())
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(BatchNormalization())
- en: model.add(MaxPooling2D(pool_size=(2, 2)))
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(MaxPooling2D(pool_size=(2, 2)))
- en: model.add(Flatten())
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Flatten())
- en: model.add(Dense(128, activation='relu'))
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(128, activation='relu'))
- en: model.add(BatchNormalization())
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(BatchNormalization())
- en: model.add(Dense(num_classes, activation='softmax'))
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: model.add(Dense(num_classes, activation='softmax'))
- en: '*Listing 15-13: Adding in batch normalization*'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-13：添加批量归一化*'
- en: If we repeat our training process, six models with mean and standard error reporting
    of the overall accuracy, we get
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重复我们的训练过程，六个模型的整体准确率的均值和标准误差报告如下：
- en: Batch normalization     75.56 ± 0.59%
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化     75.56 ± 0.59%
- en: which is significantly less than the mean accuracy found without batch normalization
    but including dropout.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 这明显低于没有批量归一化，但包括丢弃法时的平均准确率。
- en: Examining the Confusion Matrix
  id: totrans-557
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检查混淆矩阵
- en: We’ve seen in this section that our dataset is a tough one. Augmentation and
    dropout have been effective, but other things like ReLU-specific initialization,
    L2 regularization (weight decay), and even batch normalization have not improved
    things for us. That doesn’t mean these techniques are ineffective, just that they
    are not effective for this particular small dataset.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一节中看到，我们的数据集是一个比较棘手的。数据增强和丢弃法已经有效，但像 ReLU 特定初始化、L2 正则化（权重衰减）甚至批量归一化等其他技术，并没有改善我们的结果。这并不意味着这些技术无效，只是它们对于这个特定的小数据集没有效果。
- en: Let’s take a quick look at the confusion matrix generated by one of the models
    using our chosen architecture. We’ve seen previously how to calculate the matrix;
    we’ll show it here for discussion and for comparison with the confusion matrices
    we’ll make in the next section. [Table 15-2](ch15.xhtml#ch15tab2) shows the matrix;
    as always, rows are the true class label, and columns are the model-assigned label.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看使用我们选择的架构生成的某个模型的混淆矩阵。我们之前已经看到如何计算混淆矩阵；我们将在这里展示它，以便进行讨论，并与接下来我们将制作的混淆矩阵进行对比。[表
    15-2](ch15.xhtml#ch15tab2)显示了该矩阵；一如既往，行是实际类别标签，列是模型分配的标签。
- en: '**Table 15-2:** Confusion Matrix for the Spectrogram Model'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 15-2：** 声谱图模型的混淆矩阵'
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** |
    **8** | **9** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | **85.6** | 0.0 | 0.0 | 5.6 | 0.0 | 0.0 | 0.0 | 5.0 | 0.6 | 3.1 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| **0** | **85.6** | 0.0 | 0.0 | 5.6 | 0.0 | 0.0 | 0.0 | 5.0 | 0.6 | 3.1 |'
- en: '| **1** | 0.0 | **97.5** | 1.2 | 0.0 | 0.6 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.0 | **97.5** | 1.2 | 0.0 | 0.6 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| **2** | 0.0 | 13.8 | **72.5** | 0.6 | 0.6 | 3.8 | 6.2 | 0.0 | 0.6 | 1.9 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.0 | 13.8 | **72.5** | 0.6 | 0.6 | 3.8 | 6.2 | 0.0 | 0.6 | 1.9 |'
- en: '| **3** | 25.0 | 0.0 | 0.0 | **68.1** | 0.0 | 2.5 | 0.6 | 0.0 | 2.5 | 1.2 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 25.0 | 0.0 | 0.0 | **68.1** | 0.0 | 2.5 | 0.6 | 0.0 | 2.5 | 1.2 |'
- en: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **84.4** | 6.2 | 5.0 | 3.8 | 0.0 | 0.0 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **84.4** | 6.2 | 5.0 | 3.8 | 0.0 | 0.0 |'
- en: '| **5** | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | **94.4** | 4.4 | 0.6 | 0.0 | 0.0 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | **94.4** | 4.4 | 0.6 | 0.0 | 0.0 |'
- en: '| **6** | 0.0 | 0.0 | 1.2 | 0.0 | 0.0 | 10.6 | **88.1** | 0.0 | 0.0 | 0.0 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 0.0 | 0.0 | 1.2 | 0.0 | 0.0 | 10.6 | **88.1** | 0.0 | 0.0 | 0.0 |'
- en: '| **7** | 9.4 | 0.0 | 0.6 | 0.0 | 15.6 | 1.9 | 0.0 | **63.8** | 7.5 | 1.2 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 9.4 | 0.0 | 0.6 | 0.0 | 15.6 | 1.9 | 0.0 | **63.8** | 7.5 | 1.2 |'
- en: '| **8** | 18.1 | 1.9 | 0.0 | 5.6 | 0.0 | 1.2 | 2.5 | 6.9 | **55.6** | 8.1 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 18.1 | 1.9 | 0.0 | 5.6 | 0.0 | 1.2 | 2.5 | 6.9 | **55.6** | 8.1 |'
- en: '| **9** | 7.5 | 0.0 | 8.1 | 0.6 | 0.0 | 0.6 | 0.0 | 1.9 | 10.0 | **71.2** |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 7.5 | 0.0 | 8.1 | 0.6 | 0.0 | 0.6 | 0.0 | 1.9 | 10.0 | **71.2** |'
- en: The three worst-performing classes are helicopter (8), fire (7), and waves (3).
    Both waves and helicopter are most often confused with rain (0), while fire is
    most often confused with clock (4) and rain. The best performing classes are rooster
    (1) and sneezing (5). These results make sense. A rooster’s crow and a person
    sneezing are distinct sounds; nothing really sounds like them. However, it is
    easy to see how waves and a helicopter could be confused with rain, or the crackle
    of a fire with the tick of a clock.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最差的三个类别是直升机（8）、火灾（7）和波浪（3）。波浪和直升机最常被误识别为雨（0），而火灾则最常被误识别为时钟（4）和雨。表现最好的类别是公鸡（1）和打喷嚏（5）。这些结果是有道理的。公鸡的打鸣和人类打喷嚏是截然不同的声音，没有什么声音能像它们一样。但很容易理解，波浪和直升机可能会与雨声混淆，或者火灾的噼啪声可能会与时钟的滴答声混淆。
- en: Does this mean we’re stuck at 78.8 percent accuracy? No, we have one more trick
    to try. We’ve been training and evaluating the performance of single models. Nothing
    is stopping us from training multiple models and combining their results. This
    is *ensembling*. We presented ensembles briefly in [Chapter 6](ch06.xhtml#ch06)
    and again in [Chapter 9](ch09.xhtml#ch09) when discussing dropout. Now, let’s
    use the idea directly to see if we can improve our sound sample classifier.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着我们只能停留在 78.8% 的准确率上？不，我们还有一个方法可以尝试。我们一直在训练和评估单个模型的性能。没有什么阻止我们训练多个模型并将它们的结果结合起来。这就是
    *集成学习*。我们在[第6章](ch06.xhtml#ch06)和[第9章](ch09.xhtml#ch09)讨论丢弃法时简要介绍了集成学习。现在，让我们直接使用这个思想，看看能否提高我们的声音样本分类器的性能。
- en: Ensembles
  id: totrans-575
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习
- en: 'The core idea of an ensemble is to take the output of multiple models trained
    on the same, or extremely similar, dataset(s) and combine them. It embodies the
    “wisdom of the crowds” concept: one model might be better at certain classes or
    types of inputs for a particular class than another, so it makes sense that if
    they work together, they might arrive at a final result better than either one
    could do on its own.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的核心思想是利用多个在相同或极其相似的数据集上训练的模型的输出并将它们结合起来。它体现了“群体智慧”这一概念：一个模型可能在某些类别或某类输入上比另一个模型表现更好，因此如果它们合作，可能得到比单独模型更好的最终结果。
- en: Here, we’ll use the same machine learning architecture we used in the previous
    section. Our different models will be separate trainings of this architecture
    using the spectrograms as input. This is a weaker form of ensembling. Typically,
    the models in the ensemble are quite different from each other, either different
    architectures of neural networks, or completely different types of models like
    Random Forests and *k*-Nearest Neighbors. The variation between models here is
    due to the random initialization of the networks and the different parts of the
    loss landscape the network finds itself in when training stops.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用上一节中使用的相同机器学习架构。我们的不同模型将是使用 spectrogram 作为输入的这个架构的不同训练。这是集成方法的一种较弱形式。通常，集成中的模型彼此差异较大，要么是神经网络的不同架构，要么是完全不同类型的模型，如随机森林和
    *k* 最近邻。在这里，模型之间的变化来自于网络的随机初始化以及训练停止时网络所处的不同损失景观区域。
- en: 'Our approach works like this:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法是这样的：
- en: Train multiple models (*n* = 6) using the spectrogram dataset.
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 spectrogram 数据集训练多个模型 (*n* = 6)。
- en: Combine the softmax output of these models on the test set in some manner.
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以某种方式将这些模型在测试集上的 softmax 输出结合起来。
- en: Use the resulting output from the combination to predict the assigned class
    label.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用组合结果输出进行预测，分配类别标签。
- en: We hope that the set of class labels assigned after combining the individual
    model outputs is superior to the set assigned by the model architecture used alone.
    Intuitively, we feel that this approach should buy us something. It makes sense.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望，在结合单个模型输出后分配的类别标签集，比仅使用单一模型架构分配的标签集要优越。直观上，我们认为这种方法应该带来一些好处。这是有道理的。
- en: 'However, a question immediately arises: how do we best combine the outputs
    of the individual networks? We have total freedom in the answer to that question.
    What we are looking for is an *f* () such that'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个问题立即出现了：我们如何最好地结合单个网络的输出？对于这个问题，我们有完全的自由度。我们寻找的是一个 *f* ()，使得
- en: '*y*[predict] = *f*(*y*[0], *y*[1], *y*[2], … , *y[n]*)'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*[predict] = *f*(*y*[0], *y*[1], *y*[2], … , *y[n]*)'
- en: where *y*[*i*], *i* = 0, 1, …, *n* are the outputs of the *n* models in the
    ensemble and *f* () is some function, operation, or algorithm that best combines
    them into a single new prediction, *y*[predict].
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y*[*i*], *i* = 0, 1, …, *n* 是集成中 *n* 个模型的输出，*f* () 是某个函数、操作或算法，用于最好地将它们结合成一个新的预测结果
    *y*[predict]。
- en: 'Some combination approaches come readily to mind: we could average the outputs
    and select the largest, keep maximum per class output across the ensemble and
    then choose the largest of those, or use voting to decide which class label should
    be assigned. We’ll try all three of these.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 一些组合方法显而易见：我们可以对输出进行平均并选择最大的，保持每个类别的最大输出，然后从中选择最大的，或者使用投票来决定应该分配哪个类别标签。我们将尝试这三种方法。
- en: 'Let’s start with the first three approaches. We already have the six ensemble
    models: they’re the models we trained in the previous section to give us the mean
    accuracy on the test set. This model architecture uses dropout, but no alternate
    initialization, L2 regularization, or batch normalization.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从前三种方法开始。我们已经有了六个集成模型：它们是我们在上一节中训练的模型，用于提供测试集上的平均准确率。这个模型架构使用了 dropout，但没有使用替代初始化、L2
    正则化或批归一化。
- en: 'It’s straightforward enough to run the test set through each of the models
    trained in the previous section ([Listing 15-14](ch15.xhtml#ch15lis14)):'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 运行测试集通过每个在上一节中训练的模型是相当直接的（[列表 15-14](ch15.xhtml#ch15lis14)）：
- en: import sys
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: import numpy as np
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from keras.models import load_model
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: from keras.models import load_model
- en: model = load_model(sys.argv[1])
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: model = load_model(sys.argv[1])
- en: x_test = np.load("esc10_spect_test_images.npy")/255.0
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("esc10_spect_test_images.npy")/255.0
- en: y_test = np.load("esc10_spect_test_labels.npy")
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_spect_test_labels.npy")
- en: ❶ prob = model.predict(x_test)
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ prob = model.predict(x_test)
- en: ❷ p = np.argmax(prob, axis=1)
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ p = np.argmax(prob, axis=1)
- en: cc = np.zeros((10,10))
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: cc = np.zeros((10,10))
- en: 'for i in range(len(y_test)):'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(y_test)):'
- en: cc[y_test[i],p[i]] += 1
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: cc[y_test[i],p[i]] += 1
- en: ❸ print(np.array2string(cc.astype("uint32")))
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ print(np.array2string(cc.astype("uint32")))
- en: cp = 100.0 * cc / cc.sum(axis=1)
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: cp = 100.0 * cc / cc.sum(axis=1)
- en: ❹ print(np.array2string(cp, precision=1))
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ print(np.array2string(cp, precision=1))
- en: print("Overall accuracy = %0.2f%%" % (100.0*np.diag(cc).sum()/cc.sum(),))
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: print("总体准确率 = %0.2f%%" % (100.0*np.diag(cc).sum()/cc.sum(),))
- en: np.save(sys.argv[2], prob)
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: np.save(sys.argv[2], prob)
- en: '*Listing 15-14: Applying multiple models to the test set*'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-14：将多个模型应用于测试集*'
- en: This code expects the name of the trained model file as the first argument and
    the name of an output file to store the model predictions as the second argument.
    Then, it loads the model and spectrogram test data, applies the model to the test
    data ❶, and predicts class labels by selecting the highest output value ❷.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要将训练好的模型文件名作为第一个参数，将用于存储模型预测的输出文件名作为第二个参数。然后，它加载模型和谱图测试数据，将模型应用于测试数据❶，并通过选择最大输出值来预测类别标签❷。
- en: The code also calculates the confusion matrix and displays it twice, first as
    actual counts ❸ and again as a percentage of the actual class ❹. Finally, it displays
    the overall accuracy and writes the probabilities to the disk. With this code,
    we can store the predictions of each of the six models.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码还计算了混淆矩阵并将其显示两次，第一次显示为实际计数❸，第二次则显示为实际类别的百分比❹。最后，它会显示整体准确率并将概率写入磁盘。通过这段代码，我们可以存储每个模型的预测结果。
- en: Now that we have the predictions, let’s combine them in the first of the three
    ways mentioned previously. To calculate the average of the model predictions,
    we first load each model’s predictions, and then average and select the maximum
    per sample as shown in [Listing 15-15](ch15.xhtml#ch15lis15).
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了预测结果，让我们按照之前提到的三种方式中的第一种将它们组合起来。为了计算模型预测的平均值，我们首先加载每个模型的预测结果，然后对每个样本进行平均并选择最大值，如[列表
    15-15](ch15.xhtml#ch15lis15)所示。
- en: p0 = np.load("prob_run0.npy")
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: p0 = np.load("prob_run0.npy")
- en: p1 = np.load("prob_run1.npy")
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: p1 = np.load("prob_run1.npy")
- en: p2 = np.load("prob_run2.npy")
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: p2 = np.load("prob_run2.npy")
- en: p3 = np.load("prob_run3.npy")
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: p3 = np.load("prob_run3.npy")
- en: p4 = np.load("prob_run4.npy")
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: p4 = np.load("prob_run4.npy")
- en: p5 = np.load("prob_run5.npy")
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: p5 = np.load("prob_run5.npy")
- en: y_test = np.load("esc10_spect_test_labels.npy")
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("esc10_spect_test_labels.npy")
- en: prob = (p0+p1+p2+p3+p4+p5)/6.0
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: prob = (p0+p1+p2+p3+p4+p5)/6.0
- en: p = np.argmax(prob, axis=1)
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: p = np.argmax(prob, axis=1)
- en: '*Listing 15-15: Averaging the test set results*'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-15：对测试集结果取平均*'
- en: The resulting percentage confusion matrix is
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 结果百分比混淆矩阵是：
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** |
    **8** | **9** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | **83.8** | 0.0 | 0.0 | 7.5 | 0.0 | 0.0 | 0.0 | 4.4 | 0.0 | 4.4 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| **0** | **83.8** | 0.0 | 0.0 | 7.5 | 0.0 | 0.0 | 0.0 | 4.4 | 0.0 | 4.4 |'
- en: '| **1** | 0.0 | **97.5** | 1.9 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.0 | **97.5** | 1.9 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| **2** | 0.0 | 10.0 | **78.1** | 0.0 | 0.0 | 3.1 | 6.2 | 0.0 | 0.0 | 2.5 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.0 | 10.0 | **78.1** | 0.0 | 0.0 | 3.1 | 6.2 | 0.0 | 0.0 | 2.5 |'
- en: '| **3** | 9.4 | 0.0 | 0.0 | **86.2** | 0.0 | 3.1 | 0.6 | 0.0 | 0.0 | 0.6 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 9.4 | 0.0 | 0.0 | **86.2** | 0.0 | 3.1 | 0.6 | 0.0 | 0.0 | 0.6 |'
- en: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **83.1** | 5.6 | 5.0 | 5.6 | 0.0 | 0.0 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **83.1** | 5.6 | 5.0 | 5.6 | 0.0 | 0.0 |'
- en: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **93.8** | 5.6 | 0.0 | 0.0 | 0.0 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **93.8** | 5.6 | 0.0 | 0.0 | 0.0 |'
- en: '| **6** | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 8.8 | **90.6** | 0.0 | 0.0 | 0.0 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 8.8 | **90.6** | 0.0 | 0.0 | 0.0 |'
- en: '| **7** | 8.1 | 0.0 | 0.0 | 0.0 | 17.5 | 1.9 | 0.0 | **64.4** | 7.5 | 0.6 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 8.1 | 0.0 | 0.0 | 0.0 | 17.5 | 1.9 | 0.0 | **64.4** | 7.5 | 0.6 |'
- en: '| **8** | 6.2 | 0.0 | 0.0 | 7.5 | 0.0 | 1.9 | 4.4 | 8.8 | **66.2** | 5.0 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 6.2 | 0.0 | 0.0 | 7.5 | 0.0 | 1.9 | 4.4 | 8.8 | **66.2** | 5.0 |'
- en: '| **9** | 5.0 | 0.0 | 5.0 | 1.2 | 0.0 | 0.6 | 0.0 | 1.9 | 10.6 | **75.6** |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 5.0 | 0.0 | 5.0 | 1.2 | 0.0 | 0.6 | 0.0 | 1.9 | 10.6 | **75.6** |'
- en: with an overall accuracy of 82.0 percent.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 总体准确率为82.0%。
- en: 'This approach is helpful: we went from 79 percent to 82 percent in overall
    accuracy. The most significant improvements were in class 3 (waves) and class
    8 (helicopter).'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常有效：我们从79%的准确率提升到了82%。最显著的提升发生在类别 3（波形）和类别 8（直升机）上。
- en: Our next approach, shown in [Listing 15-16](ch15.xhtml#ch15lis16), keeps the
    maximum probability across the six models for each class and then selects the
    largest to assign the class label.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个方法，如[列表 15-16](ch15.xhtml#ch15lis16)所示，保持每个类别在六个模型中的最大概率，然后选择最大的概率来分配类别标签。
- en: p = np.zeros(len(y_test), dtype="uint8")
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: p = np.zeros(len(y_test), dtype="uint8")
- en: 'for i in range(len(y_test)):'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(y_test)):'
- en: t = np.array([p0[i],p1[i],p2[i],p3[i],p4[i],p5[i]])
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.array([p0[i],p1[i],p2[i],p3[i],p4[i],p5[i]])
- en: p[i] = np.argmax(t.reshape(60)) % 10
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: p[i] = np.argmax(t.reshape(60)) % 10
- en: '*Listing 15-16: Keeping the test set maximum*'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15-16：保持测试集最大值*'
- en: 'This code defines a vector, `p`, of the same length as the vector of actual
    labels, `y_test`. Then, for each test sample, we form `t`, a concatenation of
    all six models’ predictions for each class. We reshape `t` so that it is a one-dimensional
    vector of 60 elements. Why 60? We have 10 class predictions times 6 models. The
    maximum of this vector is the largest value, the index of which is returned by
    `argmax`. We really don’t want this index; instead, we want the class label this
    index maps to. Therefore, if we take this index modulo 10, we will get the proper
    class label, which we assign to `p`. With `p` and `y_test`, we can calculate the
    confusion matrix:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码定义了一个与实际标签向量`y_test`长度相同的向量`p`。然后，对于每个测试样本，我们形成`t`，它是所有六个模型对每个类别的预测结果的拼接。我们将`t`重塑为一个包含60个元素的一维向量。为什么是60？因为我们有10个类别的预测结果，乘以6个模型。这个向量的最大值是最大值，其索引由`argmax`返回。我们实际上并不需要这个索引；相反，我们需要的是这个索引映射到的类别标签。因此，如果我们对该索引取模10，就会得到正确的类别标签，我们将其分配给`p`。通过`p`和`y_test`，我们可以计算混淆矩阵：
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | **82.5** | 0.0 | 0.0 | 9.4 | 0.0 | 0.0 | 0.0 | 4.4 | 0.6 | 3.1 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| **0** | **82.5** | 0.0 | 0.0 | 9.4 | 0.0 | 0.0 | 0.0 | 4.4 | 0.6 | 3.1 |'
- en: '| **1** | 0.0 | **95.0** | 4.4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.0 | **95.0** | 4.4 | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 |'
- en: '| **2** | 0.0 | 10.0 | **78.8** | 0.0 | 0.0 | 3.1 | 5.6 | 0.0 | 0.0 | 2.5 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.0 | 10.0 | **78.8** | 0.0 | 0.0 | 3.1 | 5.6 | 0.0 | 0.0 | 2.5 |'
- en: '| **3** | 5.0 | 0.0 | 0.0 | **90.6** | 0.0 | 2.5 | 0.6 | 0.0 | 0.6 | 0.6 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 5.0 | 0.0 | 0.0 | **90.6** | 0.0 | 2.5 | 0.6 | 0.0 | 0.6 | 0.6 |'
- en: '| **4** | 1.2 | 0.0 | 0.0 | 0.0 | **81.2** | 6.2 | 5.0 | 6.2 | 0.0 | 0.0 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 1.2 | 0.0 | 0.0 | 0.0 | **81.2** | 6.2 | 5.0 | 6.2 | 0.0 | 0.0 |'
- en: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **93.8** | 5.6 | 0.0 | 0.0 | 0.0 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **93.8** | 5.6 | 0.0 | 0.0 | 0.0 |'
- en: '| **6** | 0.0 | 0.0 | 0.6 | 0.0 | 0.6 | 8.8 | **90.0** | 0.0 | 0.0 | 0.0 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 0.0 | 0.0 | 0.6 | 0.0 | 0.6 | 8.8 | **90.0** | 0.0 | 0.0 | 0.0 |'
- en: '| **7** | 8.8 | 0.0 | 0.0 | 0.0 | 16.2 | 2.5 | 0.0 | **65.0** | 6.9 | 0.6 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 8.8 | 0.0 | 0.0 | 0.0 | 16.2 | 2.5 | 0.0 | **65.0** | 6.9 | 0.6 |'
- en: '| **8** | 8.1 | 0.0 | 0.0 | 6.2 | 0.0 | 1.9 | 4.4 | 9.4 | **63.1** | 6.9 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 8.1 | 0.0 | 0.0 | 6.2 | 0.0 | 1.9 | 4.4 | 9.4 | **63.1** | 6.9 |'
- en: '| **9** | 3.8 | 0.0 | 4.4 | 3.1 | 0.0 | 0.0 | 0.0 | 1.9 | 10.6 | **76.2** |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 3.8 | 0.0 | 4.4 | 3.1 | 0.0 | 0.0 | 0.0 | 1.9 | 10.6 | **76.2** |'
- en: This gives us an overall accuracy of 81.6 percent.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了整体的准确率为81.6%。
- en: Voting is the typical approach used to combine outputs from several models.
    To implement voting in this case, we’ll use [Listing 15-17](ch15.xhtml#ch15lis17).
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 投票是结合多个模型输出的典型方法。为了在本例中实现投票，我们将使用[Listing 15-17](ch15.xhtml#ch15lis17)。
- en: t = np.zeros((6,len(y_test)), dtype="uint32")
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.zeros((6,len(y_test)), dtype="uint32")
- en: ❶ t[0,:] = np.argmax(p0, axis=1)
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ t[0,:] = np.argmax(p0, axis=1)
- en: t[1,:] = np.argmax(p1, axis=1)
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: t[1,:] = np.argmax(p1, axis=1)
- en: t[2,:] = np.argmax(p2, axis=1)
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: t[2,:] = np.argmax(p2, axis=1)
- en: t[3,:] = np.argmax(p3, axis=1)
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: t[3,:] = np.argmax(p3, axis=1)
- en: t[4,:] = np.argmax(p4, axis=1)
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: t[4,:] = np.argmax(p4, axis=1)
- en: t[5,:] = np.argmax(p5, axis=1)
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: t[5,:] = np.argmax(p5, axis=1)
- en: p = np.zeros(len(y_test), dtype="uint8")
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: p = np.zeros(len(y_test), dtype="uint8")
- en: 'for i in range(len(y_test)):'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(y_test)):'
- en: q = np.bincount(t[:,i])
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: q = np.bincount(t[:,i])
- en: p[i] = np.argmax(q)
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: p[i] = np.argmax(q)
- en: '*Listing 15-17: Voting to select the best class label*'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15-17: 投票选择最佳类别标签*'
- en: We first apply `argmax` across the six model predictions to get the associated
    labels ❶, storing them in a combined matrix, `t`. We then define `p` as before
    to hold the final assigned class label. We loop over each of the test samples,
    where we use a new NumPy function, `bincount`, to give us the number of times
    each class label occurs for the current test sample. The largest such count is
    the most often selected label, so we use `argmax` again to assign the proper output
    label to `p`. Note, this code works because our class labels are integers running
    consecutively from 0 through 9\. This alone is a good enough reason to use such
    simple and ordered class labels.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先对六个模型的预测结果应用`argmax`，得到关联的标签❶，并将其存储在一个组合矩阵`t`中。然后，我们像之前一样定义`p`来保存最终分配的类别标签。我们对每个测试样本进行循环，在循环中，我们使用一个新的NumPy函数`bincount`，它会给出当前测试样本中每个类别标签出现的次数。最大的计数值是最常被选择的标签，因此我们再次使用`argmax`为`p`分配正确的输出标签。请注意，这段代码之所以有效，是因为我们的类别标签是从0到9连续递增的整数。仅此一点，就足以说明使用如此简单且有序的类别标签是合理的。
- en: 'Here is the confusion matrix produced by this voting procedure:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过此投票程序产生的混淆矩阵：
- en: '| **Class** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7**
    | **8** | **9** |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** |
    **8** | **9** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | **86.2** | 0.0 | 0.0 | 8.8 | 0.0 | 0.0 | 0.0 | 3.8 | 0.0 | 1.2 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| **0** | **86.2** | 0.0 | 0.0 | 8.8 | 0.0 | 0.0 | 0.0 | 3.8 | 0.0 | 1.2 |'
- en: '| **1** | 0.0 | **98.1** | 1.2 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0.0 | **98.1** | 1.2 | 0.0 | 0.0 | 0.6 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| **2** | 0.0 | 10.6 | **78.1** | 0.0 | 0.0 | 3.1 | 5.6 | 0.0 | 0.0 | 2.5 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0.0 | 10.6 | **78.1** | 0.0 | 0.0 | 3.1 | 5.6 | 0.0 | 0.0 | 2.5 |'
- en: '| **3** | 14.4 | 0.0 | 0.0 | **81.2** | 0.0 | 3.1 | 0.6 | 0.0 | 0.0 | 0.6 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 14.4 | 0.0 | 0.0 | **81.2** | 0.0 | 3.1 | 0.6 | 0.0 | 0.0 | 0.6 |'
- en: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **83.8** | 5.6 | 5.0 | 5.0 | 0.0 | 0.0 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0.6 | 0.0 | 0.0 | 0.0 | **83.8** | 5.6 | 5.0 | 5.0 | 0.0 | 0.0 |'
- en: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **94.4** | 5.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0.0 | 0.0 | 0.0 | 0.0 | 0.6 | **94.4** | 5.0 | 0.0 | 0.0 | 0.0 |'
- en: '| **6** | 0.0 | 0.0 | 1.2 | 0.0 | 0.6 | 9.4 | **88.8** | 0.0 | 0.0 | 0.0 |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 0.0 | 0.0 | 1.2 | 0.0 | 0.6 | 9.4 | **88.8** | 0.0 | 0.0 | 0.0 |'
- en: '| **7** | 8.8 | 0.0 | 0.0 | 0.0 | 18.1 | 1.9 | 0.0 | **65.6** | 5.0 | 0.6 |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 8.8 | 0.0 | 0.0 | 0.0 | 18.1 | 1.9 | 0.0 | **65.6** | 5.0 | 0.6 |'
- en: '| **8** | 7.5 | 0.0 | 0.0 | 6.9 | 0.0 | 3.1 | 3.8 | 8.8 | **67.5** | 2.5 |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 7.5 | 0.0 | 0.0 | 6.9 | 0.0 | 3.1 | 3.8 | 8.8 | **67.5** | 2.5 |'
- en: '| **9** | 5.6 | 0.0 | 6.2 | 1.2 | 0.0 | 0.6 | 0.0 | 1.9 | 11.2 | **73.1** |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 5.6 | 0.0 | 6.2 | 1.2 | 0.0 | 0.6 | 0.0 | 1.9 | 11.2 | **73.1** |'
- en: This gives us an overall accuracy of 81.7 percent.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个总体准确率为 81.7% 的结果。
- en: Each of these three ensemble approaches improved our results, almost identically.
    A simple combination of the model outputs gave us, essentially, an accuracy boost
    of 3 percent over the base model alone, thereby demonstrating the utility of ensemble
    techniques.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种集成方法都提高了我们的结果，几乎是相同的。通过简单地组合模型输出，我们基本上比单一模型提高了 3% 的准确率，从而证明了集成技术的有效性。
- en: Summary
  id: totrans-683
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter presented a case study, a new dataset, and the steps we need to
    take to work through building a useful model. We started by working with the dataset
    as given to us, as raw sound samples, which we were able to augment successfully.
    We noticed that we had a feature vector and attempted to use classical models.
    From there, we moved on to 1D convolutional neural networks. Neither of these
    approaches was particularly successful.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一个案例研究，一个新的数据集，以及我们为构建一个有用模型所需要采取的步骤。我们从处理原始音频样本数据集开始，并成功地进行了数据增强。我们注意到我们有一个特征向量，并尝试使用经典模型。接着，我们转向了
    1D 卷积神经网络，但这两种方法都没有特别成功。
- en: Fortunately for us, our dataset allowed for a new representation, one that illustrated
    more effectively what composed the data and, especially important for us, introduced
    spatial elements so that we could work with 2D convolutional networks. With these
    networks, we improved quite a bit on the best 1D results, but we were still not
    at a level that was likely to be useful.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们的数据集允许一种新的表示方式，这种方式能更有效地展示数据的组成，特别是对我们来说，它引入了空间元素，使我们能够使用 2D 卷积网络。通过这些网络，我们在最好的
    1D 结果上有了相当大的提升，但我们仍然没有达到一个可能有用的水平。
- en: After exhausting our bag of CNN training tricks, we moved to ensembles of classifiers.
    With these, we discovered a modest improvement by using simple approaches to combining
    the base model outputs (for example, averaging).
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 在耗尽了我们所有的 CNN 训练技巧后，我们转向了分类器的集成方法。通过这些方法，我们发现通过简单的模型输出组合方式（例如，取平均值）可以取得一定的改进。
- en: 'We can show the progression of models and their overall accuracies to see how
    our case study evolved:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示模型的进展及其总体准确率，以查看我们的案例研究是如何发展的：
- en: '| **Model** | **Data source** | **Accuracy** |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **数据源** | **准确率** |'
- en: '| --- | --- | --- |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Gaussian Naïve Bayes | 1D sound sample | 28.1% |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| 高斯朴素贝叶斯 | 1D 音频样本 | 28.1% |'
- en: '| Random Forest (1,000 trees) | 1D sound sample | 34.4% |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林（1,000 棵树） | 1D 音频样本 | 34.4% |'
- en: '| 1D CNN | 1D sound sample | 54.0% |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| 1D CNN | 1D 音频样本 | 54.0% |'
- en: '| 2D CNN | Spectrogram | 78.8% |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| 2D CNN | 声谱图 | 78.8% |'
- en: '| Ensemble (average) | Spectrogram | 82.0% |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| 集成（平均值） | 声谱图 | 82.0% |'
- en: This table shows the power of modern deep learning and the utility of combining
    it with well-proven classical approaches like ensembles.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 这张表格展示了现代深度学习的强大功能，以及将其与经典的集成方法结合使用的实用性。
- en: This chapter concludes our exploration of machine learning. We started at the
    beginning, with data and datasets. We moved on to the classical machine learning
    models, and then dove into traditional neural networks so that we would have a
    solid foundation from which to understand modern convolutional neural networks.
    We explored CNNs in detail and concluded with a case study as an illustration
    of how you might approach a new dataset to build a successful model. Along the
    way, we learned about how to evaluate models. We became familiar with the metrics
    used by the community so that we can understand what people are talking about
    and presenting in their papers.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了我们对机器学习的探索。我们从最基础的部分开始，讲解了数据和数据集。接着，我们介绍了经典的机器学习模型，随后深入探讨了传统神经网络，以便为理解现代卷积神经网络打下坚实的基础。我们详细研究了卷积神经网络（CNN），并通过一个案例研究来说明如何处理一个新数据集并构建一个成功的模型。在这个过程中，我们学习了如何评估模型，并熟悉了社区中使用的各种评估指标，以便能够理解人们在论文中讨论和呈现的内容。
- en: Of course, this entire book has been an introduction, and we have barely scratched
    the surface of the ever-expanding world that is machine learning. Our final chapter
    will serve as a jumping-off point—a guide to where you may want to wander next
    to expand your machine learning knowledge beyond the tight bounds we’ve been required
    to set for ourselves here.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，本书的内容一直以来都是一个引导，我们仅仅触及了机器学习这一日益扩展的领域的表面。我们的最后一章将作为一个起点——一份指南，指引你接下来可能想要探索的方向，帮助你在我们在本书中所设限的框架之外，拓展自己的机器学习知识。
