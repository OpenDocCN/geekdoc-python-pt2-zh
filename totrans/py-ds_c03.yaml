- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python Data Science Libraries
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Python provides access to a robust ecosystem of third-party libraries that
    you’ll find useful for data analysis and manipulation. This chapter introduces
    you to three of the more popular data science libraries: NumPy, pandas, and scikit-learn.
    As you’ll see, many data analysis applications use these libraries extensively,
    either explicitly or implicitly.'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NumPy, or the Numeric Python library, is useful for working with *arrays*, which
    are data structures that store values of the same data type. Many other Python
    libraries that perform numerical computations rely on NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: The NumPy array, a grid of elements of the same type, is the key component of
    the NumPy library. The elements in a NumPy array are indexed by a tuple of nonnegative
    integers. NumPy arrays are similar to Python lists except that they require less
    memory and are usually faster because they use optimized, precompiled C code.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy arrays support *element-wise operations*, which allow you to perform basic
    arithmetic on entire arrays using compact and readable code. An element-wise operation
    is an operation on two arrays of the same dimensions that produces another array
    of the same dimensions, where each element *i*, *j* is the result of a calculation
    performed on elements *i*, *j* of the original two arrays. [Figure 3-1](#figure3-1)
    illustrates an element-wise operation performed against two NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '![A two-by-two array with numbers 0, 1, 2, and 3 is added to a two-by-two array
    with numbers 3, 2, 1, and 0\. The result is a two-by-two array with numbers 3,
    3, 3, and 3.](image_fi/502208c03/f03001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1: Adding two NumPy arrays'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the resulting array has the same dimensions as the original
    two arrays, and each new element is the sum of the corresponding elements in the
    original arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Installing NumPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NumPy is a third-party library, meaning it’s not part of Python’s standard
    library. The simplest way to install it is with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Python considers NumPy a module, so you’ll need to import it into your script
    before you can use it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a NumPy Array
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can create a NumPy array from the data in one or more Python lists. Suppose
    you have a list for each employee at a company containing that employee’s base
    salary payments over the past three months. You can use code like the following
    to get all the salary information into one data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You start by importing the NumPy library ❶. Then you define a set of lists,
    where each list contains the base salary data of an employee over the past three
    months ❷. Finally, you combine these lists into a NumPy array ❸. The array looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is a 2D array. It has two axes, which are indexed by integers, starting
    with 0\. Axis 0 runs vertically downward across the array’s rows, while axis 1
    runs horizontally across the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow the same process to create an array containing the employees’
    monthly bonuses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Performing Element-Wise Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s easy to perform element-wise operations on multiple NumPy arrays of the
    same dimensions. For example, you can add the `base_salary` and `bonus` arrays
    together to determine the total amount paid each month to each employee:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the addition operation is a one-liner ❶. The resulting dataset
    is a NumPy array too, in which each element is the sum of the corresponding elements
    in the `base_salary` and `bonus` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Using NumPy Statistical Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NumPy’s statistical functions allow you to analyze the contents of an array.
    For example, you can find the maximum value of an entire array or the maximum
    value of an array along a given axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you want to find the maximum value in the `salary_bonus` array you
    created in the previous section. You can do this with the NumPy array’s `max()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The function returns the maximum amount paid in the past three months to any
    employee in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy can also find the maximum value of an array along a given axis. If you
    want to determine the maximum amount paid to each employee in the past three months,
    you can use NumPy’s `amax()` function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying `axis = 1`, you instruct `amax()` to search horizontally across
    the columns for a maximum in the `salary_bonus` array, thus applying the function
    across each row. This calculates the maximum monthly amount paid to each employee
    in the past three months:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can calculate the maximum amount paid each month to any employee
    by changing the `axis` parameter to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pandas library is the de facto standard for data-oriented Python applications.
    (In case you’re wondering, the name is somehow derived from Python Data Analysis
    Library.) The library includes two data structures: the *Series*, which is 1D,
    and the *DataFrame*, which is 2D. While the DataFrame is the primary pandas data
    structure, a DataFrame is actually a collection of Series objects. Therefore,
    it’s important to understand Series as well as DataFrames.'
  prefs: []
  type: TYPE_NORMAL
- en: pandas Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The standard Python distribution does not ship with the pandas module. You
    can install pandas with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `pip` command also resolves the library’s dependencies, installing the NumPy,
    pytz, and python-dateutil packages implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Just like with NumPy, you’ll need to import the pandas module into your script
    before you can use it.
  prefs: []
  type: TYPE_NORMAL
- en: pandas Series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A pandas Series is a 1D labeled array. By default, elements in a Series are
    labeled with integers according to their position, like in a Python list. However,
    you can specify custom labels instead. These labels need not be unique, but they
    must be of a hashable type, such as integers, floats, strings, or tuples.
  prefs: []
  type: TYPE_NORMAL
- en: The elements of a Series can be of any type (integers, strings, floats, Python
    objects, and so on), but a Series works best if all its elements are of the same
    type. Ultimately, a Series may become one column in a larger DataFrame, and it’s
    unlikely you’ll want to store different kinds of data in the same column.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Series
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are several ways to create a Series. In most cases, you feed it some
    kind of 1D dataset. Here’s how you create a Series from a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You start by importing the pandas library and aliasing it as `pd` ❶. Then you
    create a list of items to be used as the data for the Series ❷. Finally, you create
    the Series, passing the list in to the `Series` constructor method ❸.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives you a single list with numeric indices set by default, starting
    from 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `dtype` attribute indicates the type of the underlying data for the given
    Series. By default, pandas uses the data type `object` to store strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a Series with user-defined indices as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This time the data in the `emps_names` Series object appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Accessing Data in a Series
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To access an element in a Series, specify the Series name followed by the element’s
    index within square brackets, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the element corresponding to index 9001:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the `loc` property of the Series object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Although you’re using custom indices in this Series object, you can still access
    its elements by position (that is, use integer location–based indexing) via the
    `iloc` property. Here, for example, you print the first element in the Series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can access multiple elements by their indices with a slice operation, as
    discussed in Chapter 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Notice that slicing with `loc` includes the right endpoint (in this case, index
    9002), whereas usually Python slice syntax does not.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use slicing to define the range of elements by position rather
    than by index. For instance, the preceding results could instead be generated
    by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'or simply as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, unlike slicing with `loc`, slicing with `[]` or `iloc` works
    the same as usual Python slicing: the start position is included but the stop
    is not. Thus, `[0:2]` leaves out the element in position 2 and returns only the
    first two elements.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining Series into a DataFrame
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multiple Series can be combined to form a DataFrame. Let’s try this by creating
    another Series and combining it with the `emps_names` Series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the new Series, you call the `Series()` constructor ❶, passing the
    following arguments: the list to be converted to a Series, the indices of the
    Series, and the name of the Series.'
  prefs: []
  type: TYPE_NORMAL
- en: You need to name Series before concatenating them into a DataFrame, because
    their names will become the names of the corresponding DataFrame columns. Since
    you didn’t name the `emps_names` Series when you created it earlier, you name
    it here by setting its `name` property to `'names'` ❷. After that, you can concatenate
    it with the `emps_emails` Series ❸. You specify `axis=1` in order to concatenate
    along the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting DataFrame looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A pandas DataFrame is a 2D labeled data structure with columns that can be of
    different types. A DataFrame can be thought of as a dictionary-like container
    for Series objects, where each key in the dictionary is a column label and each
    value is a Series.
  prefs: []
  type: TYPE_NORMAL
- en: If you are familiar with relational databases, you’ll notice that a pandas DataFrame
    is similar to a regular SQL table. [Figure 3-2](#figure3-2) illustrates an example
    of a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '![A DataFrame with rows of stock market data. The leftmost column, with row
    numbers from 0 to 4, is highlighted as the index.](image_fi/502208c03/f03002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2: An example of a pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the DataFrame includes an index column. Like with Series, pandas
    uses zero-based numeric indexing for DataFrames by default. However, you can replace
    the default index with one or more existing columns. [Figure 3-3](#figure3-3)
    shows the same DataFrame but with the Date column set as the index.
  prefs: []
  type: TYPE_NORMAL
- en: '![The same DataFrame as in the previous figure, but instead of row numbers,
    the Date column is highlighted as the index.](image_fi/502208c03/f03003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-3: A pandas DataFrame that uses a column as the index'
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, the index is a column of type `date`. In fact, pandas
    allows you to have DataFrame indexes of any type. The most commonly used index
    types are integers and strings. However, you are not limited to using only simple
    types. You might define an index of a sequence type, such as List or Tuple, or
    even use an object type that is not built into Python; this could be a third-party
    type or even your own object type.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pandas DataFrame
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You’ve seen how you can create a pandas DataFrame by combining multiple Series
    objects. You can also create a DataFrame by loading data from a database, a CSV
    file, an API request, or another external source using one of the pandas library’s
    *reader* methods. Readers allow you to read different types of data, like JSON
    and Excel, into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the DataFrame shown in [Figure 3-2](#figure3-2). It might have been
    created as the result of a request to the Yahoo Finance API via the yfinance library.
    To create the DataFrame for yourself, first install yfinance with `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then request the stock data, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this script, you send a request to the API for stock price data for a given
    ticker ❶ and use yfinance’s `history()` method to specify that you want data for
    a five-day period ❷. The resulting data, stored in the variable `hist`, is already
    in the form of a pandas DataFrame. You don’t need to create the DataFrame explicitly;
    yfinance does this for you behind the scenes. After obtaining the DataFrame, you
    remove some of its columns ❸ and switch to numeric indexing ❹, yielding the structure
    that was shown in [Figure 3-2](#figure3-2).
  prefs: []
  type: TYPE_NORMAL
- en: 'To set the index to the Date column, as was shown in [Figure 3-3](#figure3-3),
    you’ll need to execute the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now try converting a JSON document to a pandas object. The sample dataset
    used here contains the monthly salary data of three employees identified by their
    IDs in the `Empno` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: You use the pandas `read_json()` reader method to pass a JSON string into a
    DataFrame ❷. For simplicity, this example uses a JSON string converted from a
    list by `json.dumps()` ❶. Alternatively, you could pass a `path` object into the
    reader that points to a JSON file of interest or a URL to an HTTP API that publishes
    data in JSON format. Finally, you set the `Empno` column as the DataFrame index
    ❸, thus replacing the default numeric index.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting DataFrame will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Another common practice is to create pandas DataFrames from the standard Python
    data structures introduced in the previous chapter. For example, here’s how to
    create a DataFrame from a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you initialize a list of lists with the data to be sent to the DataFrame
    ❶. Each nested list will become a row in the DataFrame. Then you explicitly create
    the DataFrame, defining the columns to be used ❷. Next, you use a dictionary,
    `column_types`, to change the data types set for the columns by default ❸. This
    step is optional, but it can be crucial if you’re planning to join the DataFrame
    to another one. This is because you can join two DataFrames only on columns of
    the same data type. Finally, you set the `Empno` column as the DataFrame index
    ❹. The resulting DataFrame will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `emps` and `salary` DataFrames both use `Empno` as the index
    column to uniquely identify each row. In both cases, you set this column as the
    DataFrame index to simplify the process of merging the two DataFrames into a single
    one, which we’ll discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Combining DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'pandas allows you to merge (or join) DataFrames together, in the same way that
    you can join different tables in a relational database. This lets you gather together
    data for analysis. DataFrames support database-style join operations through two
    methods: `merge()` and `join()`. Although these methods come with some different
    parameters, you can more or less use them interchangeably.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, let’s join the `emps` and `salary` DataFrames defined in the previous
    section. This is an example of a *one-to-one join* since one row in one DataFrame
    is associated with a single row in the other DataFrame. [Figure 3-4](#figure3-4)
    illustrates how this works.
  prefs: []
  type: TYPE_NORMAL
- en: '![A row  from the emps  DataFrame  combines with a row from the salary  DataFrame  to
    form a row in the emps_salary  DataFrame . A key icon s hows  the  column shared
    between emps and salary.](image_fi/502208c03/f03004-r.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4: Joining two DataFrames connected with a one-to-one relationship'
  prefs: []
  type: TYPE_NORMAL
- en: Here we see an entry from the `emps` DataFrame and one from the `salary` DataFrame.
    The entries share the same index value, `9001`, so they can be joined into a single
    entry in the new `emps_salary` DataFrame. In relational database terminology,
    the columns through which tables are related are known as *key columns*. Although
    pandas uses the term *index* for such columns, [Figure 3-4](#figure3-4) uses the
    key icon for visual association.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of the `join()` method, the implementation is quite straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `join()` method is designed to easily join DataFrames based on their indexes.
    In this particular example, you didn’t even need to provide any additional parameters
    to join these two DataFrames; joining them index-on-index is the default behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, you may need to join two DataFrames even if one of them has rows
    with no matches in the other DataFrame. Suppose you have one more row in the `emps`
    DataFrame and that there is no corresponding row in the `salary` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here you create a pandas Series object and then add it to the `emps` DataFrame
    using the `append()` method. This is a common way to add new rows to a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated `emps` DataFrame will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now apply the join operation again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'the result is the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that the row added to the `emps` DataFrame appears in the resulting dataset
    even though it doesn’t have a related row in the `salary` DataFrame. The `NaN`
    entry in the Salary field of the last row denotes that the salary value is missing.
    In some cases you may want to allow incomplete rows like this, but in other cases
    you may want to exclude the rows that don’t have a related row in the other DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the `join()` method uses the index of the calling DataFrame in
    the resulting joined DataFrame, thus performing a *left join*. In this example,
    the calling DataFrame is `emps`. It’s considered to be the left DataFrame in the
    join operation, and therefore all the rows from it are included in the resulting
    dataset. You can change this default behavior by passing the `how` parameter in
    to the `join()` method. This parameter takes the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`left` Uses the index of the calling DataFrame (or another column if the `on`
    parameter is specified), returning all rows from the calling (left) DataFrame
    and only the matching rows from the other (right) DataFrame'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`right` Uses the other (right) DataFrame’s index, returning all rows from that
    DataFrame and only the matching rows from the calling (left) DataFrame'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`outer` Forms the combination of the calling DataFrame’s index (or another
    column if the `on` parameter is specified) and the other DataFrame’s index, returning
    all rows from both DataFrames'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`inner` Forms the intersection of the calling DataFrame’s index (or another
    column if the `on` parameter is specified) with the other DataFrame’s index, returning
    only those rows whose indexes appear in both DataFrames'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 3-5](#figure3-5) illustrates each type of join.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram shows how left, right, inner, and outer joins handle rows that
    don’t have a match in the other DataFrame.](image_fi/502208c03/f03005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-5: The results of the different types of joins'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want the resulting DataFrame to include only those rows from `emps`
    that have a related row in the `salary` DataFrame, set the `how` parameter of
    `join()` to `inner`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Another way to get this result is to pass `right` in as the `how` parameter.
    In this case, `join()` returns all the rows from the `salary` DataFrame, attaching
    to them the fields of the matched rows from the `emps` DataFrame. It’s important
    to realize, however, that a right join will not be equal to an inner join in many
    other cases. For example, if you add a row to the `salary` DataFrame with no match
    in the `emps` DataFrame, the right join will include this row along with the rows
    that have matches in `emps`.
  prefs: []
  type: TYPE_NORMAL
- en: One-to-Many Joins
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a *one-to-many join*, a row from one DataFrame can match multiple rows from
    the other DataFrame. Consider the situation where each salesperson in the `emps`
    DataFrame has processed a number of orders. This might be reflected in an `orders`
    DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s how the `orders` DataFrame will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you have a DataFrame of orders, you can combine it with the DataFrame
    of employees defined previously. This is a one-to-many join since one employee
    from the `emps` DataFrame can be associated with multiple rows in the `orders`
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In this code, you use the `merge()` method to define a one-to-many join, combining
    the data from the `emps` and `orders` DataFrames. The `merge()` method allows
    you to specify the columns to join on in both DataFrames, using `left_on` to specify
    the column in the calling DataFrame and `right_on` for the column in the other
    DataFrame. With `join()`, you can only specify the column to join on for the calling
    DataFrame. For the other DataFrame, `join()` uses the index column.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, you use the inner type of join to include only related rows
    from both DataFrames. The resulting dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-6](#figure3-6) illustrates how this one-to-many join works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A row  in  the emps  DataFrame  combines with three rows  in  the o rders  DataFrame  to
    form three rows in the emps_orders  DataFrame . A key icon shows the column shared
    between emps and orders.](image_fi/502208c03/f03006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6: Joining two DataFrames connected with a one-to-many relationship'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the one-to-many join in the figure includes one row for each
    row in the dataset on the *many* side of the join. Since you’re using the inner
    type of join, no other rows will be included. In the case of a left or outer join,
    however, the join would also include those rows from the dataset on the *one*
    side of the join that have no matches on the *many* side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from one-to-many and one-to-one joins, there are also *many-to-many joins*.
    As an example of this relationship, consider two datasets: one that lists books
    and the other that lists authors. Every record in the authors dataset can be linked
    to one or more records in the books dataset, and every record in the books dataset
    can be linked to one or more author records. We’ll discuss this type of relationship
    in Chapter 7, which covers concatenating, merging, and joining datasets in greater
    detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Data with groupby()
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pandas `groupby()` function lets you aggregate data over multiple rows of
    a DataFrame. For example, it can find the sum of a column or get the mean of a
    subset of the values in a column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that you need to calculate an average total for the orders processed
    by each employee in the `orders` DataFrame you created earlier. You can use the
    `groupby()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`groupby()` returns a GroupBy object that supports several aggregate functions.
    In this particular example, you use `mean()` to calculate an average total for
    the group of orders associated with an employee. To achieve this, you group the
    rows in the `orders` DataFrame by the `Empno` column and then apply the `mean()`
    operation to the `Total` column. The generated dataset is a Series object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now suppose you want to sum up the totals of the orders within the groups.
    This is where the `sum()` function of the GroupBy object comes in handy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The data in the generated Series object will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn is a Python package designed for machine learning applications.
    Together with NumPy and pandas, it’s another core component in the Python data
    science ecosystem. scikit-learn provides efficient, easy-to-use tools that can
    be applied to common machine learning problems, including exploratory and predictive
    data analysis. We’ll take a deeper dive into machine learning at the end of the
    book. For now, this section offers a taste of how Python can be applied in the
    field of machine learning, particularly for predictive data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictive data analysis is an area of machine learning that relies on classification
    and regression algorithms. Both classification and regression use past data to
    make predictions about new data, but classification sorts data into discrete categories,
    while regression can output a continuous range of numerical values. In this section,
    we’ll look at an example of classification implemented with scikit-learn. We’ll
    build a predictive model that can analyze customer product reviews and sort them
    into two classes: positive reviews and negative reviews. The model will learn
    from already classified samples how to predict the class of other samples. Once
    we have trained the model, we’ll show it some new reviews for classification as
    either positive or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like NumPy and pandas, scikit-learn is a third-party Python library. You can
    install it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The scikit-learn package has many submodules, each with its own specific functionality.
    It’s therefore common to only import the submodule needed for a particular task
    (say, `sklearn.model_selection`) rather than the entire module.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining a Sample Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to be accurate, a predictive model should be trained on a large number
    of labeled samples, so your first step toward building a model that can classify
    product reviews is to obtain a set of reviews that are already labeled either
    positive or negative. This saves you from having to gather reviews yourself and
    manually label them.
  prefs: []
  type: TYPE_NORMAL
- en: There are several online sources for labeled datasets. One of the best is UC
    Irvine’s Machine Learning Repository, at [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php).
    Search the repository using the term “customer product reviews” and you’ll find
    a link to the Sentiment Labelled Sentences Data Set (or alternatively, just point
    your browser to [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)).
    Download and unpack the *sentiment labelled sentences.zip* file from the dataset’s
    web page.
  prefs: []
  type: TYPE_NORMAL
- en: The *.zip* file contains reviews from IMDb, Amazon, and Yelp, in three different
    *.txt* files. The reviews are labeled with positive or negative sentiment (`1`
    or `0`, respectively); there are 500 positive and 500 negative reviews from each
    source, for a total of 3,000 labeled reviews in the entire dataset. For simplicity,
    we’ll use the instances from Amazon only. They can be found in the *amazon_cells_labelled.txt*
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Sample Dataset into a pandas DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To simplify further computations, you need to load the reviews from the text
    file into a more manageable data structure. You can read the data from *amazon_cells_labelled.txt*
    into a pandas DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you use the pandas `read_csv()` reader method to load the data into a
    DataFrame. You specify two columns: the first one to hold reviews ❶ and the second
    for the corresponding sentiment scores ❷. Since tabs separate the reviews from
    their corresponding sentiment scores in the original file, you specify `\t` as
    the separator ❸.'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the Sample Dataset into a Training Set and a Test Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you’ve imported your dataset, the next step is to split it into two
    parts: one to train the predictive model and one to test its accuracy. scikit-learn
    lets you do this with just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: You split your dataset using the `train_test_split()` function from the `sklearn.model_selection`
    module. The reviews and their corresponding sentiments (the sentiment scores)
    are passed in to the function as NumPy arrays obtained via the `values` property
    of the corresponding Series objects extracted from the DataFrame. You pass the
    `test_size` parameter ❶ to control how the dataset is split. The value `0.2` means
    20 percent of the reviews will be randomly assigned to the test set. Thus, you
    follow the 80/20 pattern; the remaining 80 percent of reviews will make up the
    training set. The `random_state` parameter ❷ initializes the internal random number
    generator needed to randomly split up the data.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming Text into Numerical Feature Vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train and test your model, you need a way to represent the text data numerically.
    This is where the *bag of words (BoW)* model comes in. This model represents a
    text as the set (bag) of its words in order to generate numerical data about the
    text. The most typical numerical feature generated from the BoW model is word
    frequency, the number of times each word occurs in the text. This simple example
    shows how the BoW model transforms a text into a numerical feature vector based
    on word frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: You can use scikit-learn’s `CountVectorizer()` function to create a BoW matrix
    for text data. `CountVectorizer()` converts text data into numerical feature vectors
    (*n*-dimensional vectors of numerical features representing some object) and performs
    tokenization (separating a text into individual words and punctuation symbols)
    using either the default tokenizer or a custom one. A custom tokenizer can be
    implemented with a natural language processing tool like spaCy, introduced in
    the previous chapter. In this example, we’ll use the default option to keep things
    simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to convert the reviews into feature vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: First, you create a `vectorizer` object. Then, you apply the vectorizer’s `fit()`
    method to build the vocabulary of tokens found in the `reviews` dataset, which
    contains all reviews from both the training and test sets. After that, you use
    the `transform()` method of the `vectorizer` object to transform the text data
    in the training and test sets into numerical feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Evaluating the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have the training and test sets in the form of numerical vectors,
    you’re ready to train and test the model. You’ll first train scikit-learn’s `LogisticRegression()`
    classifier to predict the sentiment of a review. *Logistic regression* is a basic
    yet popular algorithm for solving classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you create a `LogisticRegression()` classifier, then use its `fit()`
    method to train the model according to the given training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you must evaluate how accurately the model can make predictions on new
    data. You’ll need a set of labeled data for this, which is why it’s common to
    split a labeled dataset into a training set and a test set, as you did earlier.
    Here, you evaluate the model using the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy rating typically appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This means that the model is 81 percent accurate. If you experiment with the
    `random_state` parameter in the `train_test_split()` function, you may get a slightly
    different value because the instances for the training and test sets are chosen
    randomly from the original set.
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions on New Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you’ve trained and tested your model, it’s ready to analyze new, unlabeled
    data. This will give you a more complete picture of how well the model is working.
    Try it out by feeding your model some new sample reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'You start by creating a list of new reviews, then you transform that new text
    into numerical feature vectors. Finally, you predict class sentiments for the
    new samples. The sentiments are returned as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Remember, `0` indicates a negative review and `1` indicates a positive review.
    As you can see, the model has worked for these sample reviews, showing that the
    first one is negative and the other two are positive.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter introduced you to some of the most popular third-party Python libraries
    for data science applications. First we explored the NumPy library and its multidimensional
    array objects, then we looked at the pandas library and its Series and DataFrame
    data structures. You learned to create NumPy arrays, pandas Series, and pandas
    DataFrame objects from built-in Python structures such as lists and from data
    sources stored in standard formats such as JSON. You also saw how to access and
    manipulate data in those objects. Finally, you used scikit-learn, a popular machine
    learning Python library, to build a predictive model for classification.
  prefs: []
  type: TYPE_NORMAL
