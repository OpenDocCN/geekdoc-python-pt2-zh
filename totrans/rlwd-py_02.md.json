["```py\npython -m pip install nltk\n```", "```py\npy -3.7 -m pip install nltk\n```", "```py\n>>> import nltk\n>>>\n```", "```py\n>>> import nltk\n>>> nltk.download()\n```", "```py\n>>> import nltk\n>>> nltk.download('punkt')\n```", "```py\n>>> import nltk\n>>> nltk.download('stopwords')\n```", "```py\n>>> import nltk\n>>> nltk.download('averaged_perceptron_tagger')\n```", "```py\n>>> from nltk import punkt\n```", "```py\n>>> from nltk.corpus import stopwords\n```", "```py\nstylometry.py, part 1\nimport nltk\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n\nLINES = ['-', ':', '--']  # Line style for plots.\n\ndef main():\n  ➊ strings_by_author = dict()\n    strings_by_author['doyle'] = text_to_string('hound.txt')\n    strings_by_author['wells'] = text_to_string('war.txt')\n    strings_by_author['unknown'] = text_to_string('lost.txt')\n\n    print(strings_by_author['doyle'][:300])\n\n ➋ words_by_author = make_word_dict(strings_by_author)\n    len_shortest_corpus = find_shortest_corpus(words_by_author)\n ➌ word_length_test(words_by_author, len_shortest_corpus)\n    stopwords_test(words_by_author, len_shortest_corpus)    \n    parts_of_speech_test(words_by_author, len_shortest_corpus)\n    vocab_test(words_by_author)\n    jaccard_test(words_by_author, len_shortest_corpus)\n```", "```py\n{'Doyle': 'Mr. Sherlock Holmes, who was usually very late in the mornings --snip--'}\n```", "```py\nMr. Sherlock Holmes, who was usually very late in the mornings, save\nupon those not infrequent occasions when he was up all night, was seated\nat the breakfast table. I stood upon the hearth-rug and picked up the\nstick which our visitor had left behind him the night before. It was a\nfine, thick piec\n```", "```py\nstylometry.py, part 2\n  def text_to_string(filename):\n      \"\"\"Read a text file and return a string.\"\"\"\n      with open(filename) as infile:\n          return infile.read()\n\n➊ def make_word_dict(strings_by_author):\n      \"\"\"Return dictionary of tokenized words by corpus by author.\"\"\"\n      words_by_author = dict()\n      for author in strings_by_author:\n          tokens = nltk.word_tokenize(strings_by_author[author])\n       ➋ words_by_author[author] = ([token.lower() for token in tokens\n                                      if token.isalpha()])\n      return words_by_author\n```", "```py\nUnicodeDecodeError: 'ascii' codec can't decode byte 0x93 in position 365:\nordinal not in range(128)\n```", "```py\n>>> import locale\n>>> locale.getpreferredencoding()\n'cp1252'\n```", "```py\n    with open(filename, encoding='utf-8') as infile:\n```", "```py\n    with open(filename, encoding='utf-8', errors='ignore') as infile:\n```", "```py\n>>> import nltk\t\t  \n>>> str1 = 'The rain in Spain falls mainly on the plain.'\t\t  \n>>> tokens = nltk.word_tokenize(str1)\t\t  \n>>> print(type(tokens))\t\t  \n<class 'list'>\n>>> tokens\t\t  \n['The', 'rain', 'in', 'Spain', 'falls', 'mainly', 'on', 'the', 'plain', '.']\n```", "```py\n>>> my_tokens = str1.split()\t\t  \n>>> my_tokens\t\t  \n['The', 'rain', 'in', 'Spain', 'falls', 'mainly', 'on', 'the', 'plain.']\n```", "```py\nstylometry.py, part 3\ndef find_shortest_corpus(words_by_author):\n    \"\"\"Return length of shortest corpus.\"\"\"\n    word_count = []\n    for author in words_by_author:\n        word_count.append(len(words_by_author[author]))\n        print('\\nNumber of words for {} = {}\\n'.\n              format(author, len(words_by_author[author])))\n    len_shortest_corpus = min(word_count)\n    print('length shortest corpus = {}\\n'.format(len_shortest_corpus))        \n    return len_shortest_corpus\n```", "```py\nstylometry.py, part 4\ndef word_length_test(words_by_author, len_shortest_corpus):\n    \"\"\"Plot word length freq by author, truncated to shortest corpus length.\"\"\"\n    by_author_length_freq_dist = dict()\n    plt.figure(1)    \n    plt.ion()\n\n ➊ for i, author in enumerate(words_by_author):\n        word_lengths = [len(word) for word in words_by_author[author]\n                        [:len_shortest_corpus]]\n        by_author_length_freq_dist[author] = nltk.FreqDist(word_lengths)\n     ➋ by_author_length_freq_dist[author].plot(15, \n                                                linestyle=LINES[i],                                                  \n                                                label=author,   \n                                                title='Word Length')\n    plt.legend()\n    #plt.show()  # Uncomment to see plot while coding.\n```", "```py\nstylometry.py, part 5\ndef stopwords_test(words_by_author, len_shortest_corpus):\n    \"\"\"Plot stopwords freq by author, truncated to shortest corpus length.\"\"\"\n    stopwords_by_author_freq_dist = dict()\n    plt.figure(2)\n    stop_words = set(stopwords.words('english'))  # Use set for speed.\n    #print('Number of stopwords = {}\\n'.format(len(stop_words)))\n    #print('Stopwords = {}\\n'.format(stop_words))\n\n    for i, author in enumerate(words_by_author):        \n        stopwords_by_author = [word for word in words_by_author[author]\n                               [:len_shortest_corpus] if word in stop_words]    \n        stopwords_by_author_freq_dist[author] = nltk.FreqDist(stopwords_by_\n        author)    \n        stopwords_by_author_freq_dist[author].plot(50, \n                                                   label=author,\n                                                   linestyle=LINES[i],\n                                                   title=\n                                                   '50 Most Common Stopwords')\n    plt.legend()\n##    plt.show()  # Uncomment to see plot while coding function.\n```", "```py\nstylometry.py, part 6\ndef parts_of_speech_test(words_by_author, len_shortest_corpus):\n    \"\"\"Plot author use of parts-of-speech such as nouns, verbs, adverbs.\"\"\"\n    by_author_pos_freq_dist = dict()\n    plt.figure(3)\n    for i, author in enumerate(words_by_author):\n        pos_by_author = [pos[1] for pos in nltk.pos_tag(words_by_author[author]\n                            [:len_shortest_corpus])] \n        by_author_pos_freq_dist[author] = nltk.FreqDist(pos_by_author)\n        by_author_pos_freq_dist[author].plot(35, \n                                             label=author,\n                                             linestyle=LINES[i],\n                                             title='Part of Speech')\n    plt.legend()\n    plt.show()\n```", "```py\n['NN', 'NNS', 'WP', 'VBD', 'RB', 'RB', 'RB', 'IN', 'DT', 'NNS', --snip--]\n```", "```py\nstylometry.py, part 7\ndef vocab_test(words_by_author):\n    \"\"\"Compare author vocabularies using the chi-squared statistical test.\"\"\"\n    chisquared_by_author = dict()\n    for author in words_by_author:\n     ➊ if author != 'unknown': \n           combined_corpus = (words_by_author[author] +\n                              words_by_author['unknown'])\n           author_proportion = (len(words_by_author[author])/\n                                len(combined_corpus))\n           combined_freq_dist = nltk.FreqDist(combined_corpus)\t\t\n           most_common_words = list(combined_freq_dist.most_common(1000))\n           chisquared = 0\n        ➋ for word, combined_count in most_common_words:\n              observed_count_author = words_by_author[author].count(word)\n              expected_count_author = combined_count * author_proportion\n              chisquared += ((observed_count_author -\n                              expected_count_author)**2 /\n                             expected_count_author)\n           ➌ chisquared_by_author[author] = chisquared    \n         print('Chi-squared for {} = {:.1f}'.format(author, chisquared))\n most_likely_author = min(chisquared_by_author, key=chisquared_by_author.get)\n print('Most-likely author by vocabulary is {}\\n'.format(most_likely_author))\n```", "```py\n[('the', 7778), ('of', 4112), ('and', 3713), ('i', 3203), ('a', 3195), --snip--]\n```", "```py\n>>> print(mydict)\n{'doyle': 100, 'wells': 5}\n>>> minimum = min(mydict)\n>>> print(minimum)\n'doyle'\n>>> minimum = min(mydict, key=mydict.get)\n>>> print(minimum)\n'wells'\n```", "```py\nChi-squared for doyle = 4744.4\nChi-squared for wells = 6856.3\nMost-likely author by vocabulary is doyle\n```", "```py\nstylometry.py, part 8\ndef jaccard_test(words_by_author, len_shortest_corpus):\n    \"\"\"Calculate Jaccard similarity of each known corpus to unknown corpus.\"\"\"\n    jaccard_by_author = dict()\n    unique_words_unknown = set(words_by_author['unknown']\n                               [:len_shortest_corpus])\n ➊ authors = (author for author in words_by_author if author != 'unknown')\n    for author in authors:\n        unique_words_author = set(words_by_author[author][:len_shortest_corpus]) \n        shared_words = unique_words_author.intersection(unique_words_unknown)\n     ➋ jaccard_sim = (float(len(shared_words))/ (len(unique_words_author) +\n                                                  len(unique_words_unknown) -\n                                                  len(shared_words)))\n        jaccard_by_author[author] = jaccard_sim\n        print('Jaccard Similarity for {} = {}'.format(author, jaccard_sim))\n ➌ most_likely_author = max(jaccard_by_author, key=jaccard_by_author.get)\n    print('Most-likely author by similarity is {}'.format(most_likely_author))\n\nif __name__ == '__main__':\n    main()\n```", "```py\n>>> mylist = [i for i in range(4)]\n>>> mylist\n[0, 1, 2, 3]\n>>> mygen = (i for i in range(4))\n>>> mygen\n<generator object <genexpr> at 0x000002717F547390>\n```", "```py\ndef generator(my_range):\n    for i in range(my_range):\n        yield i\n```", "```py\nJaccard Similarity for doyle = 0.34847801578354004\nJaccard Similarity for wells = 0.30786921307869214\nMost-likely author by similarity is doyle\n```"]