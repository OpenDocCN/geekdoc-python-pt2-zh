<html><head></head><body>
<h2 class="h2" id="ch02"><span epub:type="pagebreak" id="page_27"/><span class="big">2</span><br/>ATTRIBUTING AUTHORSHIP WITH STYLOMETRY</h2>&#13;
<div class="image1"><img src="../images/common.jpg" alt="Image"/></div>&#13;
<p class="noindent"><em>Stylometry</em> is the quantitative study of literary style through computational text analysis. It’s based on the idea that we all have a unique, consistent, and recognizable style to our writing. This includes our vocabulary, our use of punctuation, the average length of our sentences and words, and so on.</p>&#13;
<p class="indent">A common application of stylometry is authorship attribution. Do you ever wonder if Shakespeare really wrote all his plays? Or if John Lennon or Paul McCartney wrote the song “In My Life”? Could Robert Galbraith, author of <em>A Cuckoo’s Calling</em>, really be J. K. Rowling in disguise? Stylometry can find the answer!</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_28"/>Stylometry has been used to overturn murder convictions and even helped identify and convict the Unabomber in 1996. Other uses include detecting plagiarism and determining the emotional tone behind words, such as in social media posts. Stylometry can even be used to detect signs of mental depression and suicidal tendencies.</p>&#13;
<p class="indent">In this chapter, you’ll use multiple stylometric techniques to determine whether Sir Arthur Conan Doyle or H. G. Wells wrote the novel <em>The Lost World</em>.</p>&#13;
<h3 class="h3ab" id="ch00lev1sec12"><strong>Project #2: The Hound, The War, and The Lost World</strong></h3>&#13;
<p class="noindent">Sir Arthur Conan Doyle (1859–1930) is best known for the Sherlock Holmes stories, considered milestones in the field of crime fiction. H. G. Wells (1866–1946) is famous for several groundbreaking science fiction novels including <em>The War of The Worlds</em>, <em>The Time Machine</em>, <em>The Invisible Man</em>, and <em>The Island of Dr. Moreau</em>.</p>&#13;
<p class="indent">In 1912, the <em>Strand Magazine</em> published <em>The Lost World</em>, a serialized version of a science fiction novel. It told the story of an Amazon basin expedition, led by zoology professor George Edward Challenger, that encountered living dinosaurs and a vicious tribe of ape-like creatures.</p>&#13;
<p class="indent">Although the author of the novel is known, for this project, let’s pretend it’s in dispute and it’s your job to solve the mystery. Experts have narrowed the field down to two authors, Doyle and Wells. Wells is slightly favored because <em>The Lost World</em> is a work of science fiction, which is his purview. It also includes brutish troglodytes redolent of the morlocks in his 1895 work <em>The Time Machine</em>. Doyle, on the other hand, is known for detective stories and historical fiction.</p>&#13;
<div class="sidebar96">&#13;
<p class="Problem-Head">THE OBJECTIVE</p>&#13;
<p class="Body-Problem">Write a Python program that uses stylometry to determine whether Sir Arthur Conan Doyle or H. G. Wells wrote the novel <em>The Lost World</em>.</p>&#13;
</div>&#13;
<h4 class="h4" id="ch00lev2sec9"><strong><em>The Strategy</em></strong></h4>&#13;
<p class="noindent">The science of <em>natural language processing (NLP)</em> deals with the interactions between the precise and structured language of computers and the nuanced, frequently ambiguous “natural” language used by humans. Example uses for NLP include machine translations, spam detection, comprehension of search engine questions, and predictive text recognition for cell phone users.</p>&#13;
<p class="indent">The most common NLP tests for authorship analyze the following features of a text:</p>&#13;
<div class="bq_1">&#13;
<p class="noindent"><strong>Word length</strong> A frequency distribution plot of the length of words in a document</p>&#13;
<p class="noindent"><strong>Stop words</strong> A frequency distribution plot of stop words (short, noncontextual function words like <em>the</em>, <em>but</em>, and <em>if</em>)</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_29"/><strong>Parts of speech</strong> A frequency distribution plot of words based on their syntactic functions (such as nouns, pronouns, verbs, adverbs, adjectives, and so on)</p>&#13;
<p class="noindent"><strong>Most common words</strong> A comparison of the most commonly used words in a text</p>&#13;
<p class="noindent"><strong>Jaccard similarity</strong> A statistic used for gauging the similarity and diversity of a sample set</p>&#13;
</div>&#13;
<p class="indent">If Doyle and Wells have distinctive writing styles, these five tests should be enough to distinguish between them. We’ll talk about each test in more detail in the coding section.</p>&#13;
<p class="indent">To capture and analyze each author’s style, you’ll need a representative <em>corpus</em>, or a body of text. For Doyle, use the famous Sherlock Holmes novel <em>The Hound of the Baskervilles</em>, published in 1902. For Wells, use <em>The War of the Worlds</em>, published in 1898. Both these novels contain more than 50,000 words, more than enough for a sound statistical sampling. You’ll then compare each author’s sample to <em>The Lost World</em> to determine how closely the writing styles match.</p>&#13;
<p class="indent">To perform stylometry, you’ll use the <em>Natural Language Toolkit (NLTK)</em>, a popular suite of programs and libraries for working with human language data in Python. It’s free and works on Windows, macOS, and Linux. Created in 2001 as part of a computational linguistics course at the University of Pennsylvania, NLTK has continued to develop and expand with the help of dozens of contributors. To learn more, check out the official NLTK website at <em><a href="http://www.nltk.org/">http://www.nltk.org/</a></em>.</p>&#13;
<h4 class="h4" id="ch00lev2sec10"><strong><em>Installing NLTK</em></strong></h4>&#13;
<p class="noindent">You can find installation instructions for NLTK at <em><a href="http://www.nltk.org/install.html">http://www.nltk.org/install.html</a></em>. To install NLTK on Windows, open PowerShell and install it with Preferred Installer Program (pip).</p>&#13;
<pre><span class="codestrong1">python -m pip install nltk</span></pre>&#13;
<p class="indent">If you have multiple versions of Python installed, you’ll need to specify the version. Here’s the command for Python 3.7:</p>&#13;
<pre><span class="codestrong1">py -3.7 -m pip install nltk</span></pre>&#13;
<p class="indent">To check that the installation was successful, open the Python interactive shell and enter the following:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>&#13;
&gt;&gt;&gt;</pre>&#13;
<p class="indent">If you don’t get an error, you’re good to go. Otherwise, follow the installation instructions at <em><a href="http://www.nltk.org/install.html">http://www.nltk.org/install.html</a></em>.</p>&#13;
<h5 class="h5"><strong>Downloading the Tokenizer</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_30"/>To run the stylometric tests, you’ll need to break the multiple texts—or <em>corpora</em>—into individual words, referred to as <em>tokens</em>. At the time of this writing, the <span class="literal">word_tokenize()</span> method in NLTK implicitly calls <span class="literal">sent_tokenize()</span>, used to break a corpus into individual sentences. For handling <span class="literal">sent_tokenize()</span>, you’ll need the <em>Punkt Tokenizer Models</em>. Although this is part of NLTK, you’ll have to download it separately with the handy NLTK Downloader. To launch it, enter the following into the Python shell:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">nltk.download()</span></pre>&#13;
<p class="indent">The NLTK Downloader window should now be open (<a href="ch02.xhtml#ch02fig1">Figure 2-1</a>). Click either the <strong>Models</strong> or <strong>All Packages</strong> tab near the top; then click <strong>punkt</strong> in the Identifier column. Scroll to the bottom of the window and set the Download Directory for your platform (see <em><a href="https://www.nltk.org/data.html">https://www.nltk.org/data.html</a></em>). Finally, click the <strong>Download</strong> button to download the Punkt Tokenizer Models.</p>&#13;
<div class="image"><img src="../images/fig02_01.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig1"/>Figure 2-1: Downloading the Punkt Tokenizer Models</p>&#13;
<p class="indent">Note that you can also download NLTK packages directly in the shell. Here’s an example:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">nltk.download('punkt')</span></pre>&#13;
<p class="indent">You’ll also need access to the Stopwords Corpus, which can be downloaded in a similar manner.</p>&#13;
<h5 class="h5"><strong>Downloading the Stopwords Corpus</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_31"/>Click the <strong>Corpora</strong> tab in the NLTK Downloader window and download the Stopwords Corpus, as shown in <a href="ch02.xhtml#ch02fig2">Figure 2-2</a>.</p>&#13;
<div class="image"><img src="../images/fig02_02.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig2"/>Figure 2-2: Downloading the Stopwords Corpus</p>&#13;
<p class="indent">Alternatively, you can use the shell.</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">nltk.download('stopwords')</span></pre>&#13;
<p class="indent">Let’s download one more package to help you analyze parts of speech, like nouns and verbs. Click the <strong>All Packages</strong> tab in the NLTK Downloader window and download the Averaged Perceptron Tagger.</p>&#13;
<p class="indent">To use the shell, enter the following:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">nltk.download('averaged_perceptron_tagger')</span></pre>&#13;
<p class="indent">When NLTK has finished downloading, exit the NLTK Downloader window and enter the following into the Python interactive shell:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from nltk import punkt</span></pre>&#13;
<p class="indent">Then enter the following:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">from nltk.corpus import stopwords</span></pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_32"/>If you don’t encounter an error, the models and corpus successfully downloaded.</p>&#13;
<p class="indent">Finally, you’ll need <span class="literal">matplotlib</span> to make plots. If you haven’t installed it already, see the instructions for installing scientific packages on <a href="ch01.xhtml#page_6">page 6</a>.</p>&#13;
<h4 class="h4" id="ch00lev2sec11"><strong><em>The Corpora</em></strong></h4>&#13;
<p class="noindent">You can download the text files for <em>The Hound of the Baskervilles</em> (<em>hound.txt</em>), <em>The War of the Worlds</em> (<em>war.txt</em>), and <em>The Lost World</em> (<em>lost.txt</em>), along with the book’s code, from <em><a href="https://nostarch.com/real-world-python/">https://nostarch.com/real-world-python/</a>.</em></p>&#13;
<p class="indent">These came from Project Gutenberg (<em><a href="http://www.gutenberg.org/">http://www.gutenberg.org/</a></em>), a great source for public domain literature. So that you can use these texts right away, I’ve stripped them of extraneous material such as table of contents, chapter titles, copyright information, and so on.</p>&#13;
<h4 class="h4" id="ch00lev2sec12"><strong><em>The Stylometry Code</em></strong></h4>&#13;
<p class="noindent">The <em>stylometry.py</em> program you’ll write next loads the text files as strings, tokenizes them into words, and then runs the five stylometric analyses listed on <a href="ch02.xhtml#page_28">pages 28</a>–<a href="ch02.xhtml#page_29">29</a>. The program will output a combination of plots and shell messages that will help you determine who wrote <em>The Lost World</em>.</p>&#13;
<p class="indent">Keep the program in the same folder as the three text files. If you don’t want to enter the code yourself, just follow along with the downloadable code available at <em><a href="https://nostarch.com/real-world-python/">https://nostarch.com/real-world-python/</a></em>.</p>&#13;
<h5 class="h5"><strong>Importing Modules and Defining the main() Function</strong></h5>&#13;
<p class="noindent"><a href="ch02.xhtml#ch02list1">Listing 2-1</a> imports NLTK and <span class="literal">matplotlib</span>, assigns a constant, and defines the <span class="literal">main()</span> function to run the program. The functions used in <span class="literal">main()</span> will be described in detail later in the chapter.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 1</span>&#13;
import nltk&#13;
from nltk.corpus import stopwords&#13;
import matplotlib.pyplot as plt&#13;
&#13;
LINES = ['-', ':', '--']  # Line style for plots.&#13;
&#13;
def main():&#13;
  <span class="ent">➊</span> strings_by_author = dict()&#13;
    strings_by_author['doyle'] = text_to_string('hound.txt')&#13;
    strings_by_author['wells'] = text_to_string('war.txt')&#13;
    strings_by_author['unknown'] = text_to_string('lost.txt')&#13;
&#13;
    print(strings_by_author['doyle'][:300])&#13;
&#13;
 <span class="ent">➋</span> words_by_author = make_word_dict(strings_by_author)&#13;
    len_shortest_corpus = find_shortest_corpus(words_by_author)&#13;
 <span class="ent">➌</span> word_length_test(words_by_author, len_shortest_corpus)&#13;
    stopwords_test(words_by_author, len_shortest_corpus)    &#13;
    parts_of_speech_test(words_by_author, len_shortest_corpus)&#13;
    <span epub:type="pagebreak" id="page_33"/>vocab_test(words_by_author)&#13;
    jaccard_test(words_by_author, len_shortest_corpus)</pre>&#13;
<p class="listing"><a id="ch02list1"/>Listing 2-1: Importing modules and defining the <span class="literal">main()</span> function</p>&#13;
<p class="indent">Start by importing NLTK and the Stopwords Corpus. Then import <span class="literal">matplotlib</span>.</p>&#13;
<p class="indent">Create a variable called <span class="literal">LINES</span> and use the all-caps convention to indicate it should be treated as a constant. By default, <span class="literal">matplotlib</span> plots in color, but you’ll still want to designate a list of symbols for color-blind people and this black-and-white book!</p>&#13;
<p class="indent">Define <span class="literal">main()</span> at the start of the program. The steps in this function are almost as readable as pseudocode and provide a good overview of what the program will do. The first step will be to initialize a dictionary to hold the text for each author <span class="ent">➊</span>. The <span class="literal">text_to_string()</span> function will load each corpus into this dictionary as a string. The name of each author will be the dictionary key (using <span class="literal">unknown</span> for <em>The Lost World</em>), and the string of text from their novel will be the value. For example, here’s the key, <span class="literal">Doyle</span>, with the value text string greatly truncated:</p>&#13;
<pre>{'Doyle': 'Mr. Sherlock Holmes, who was usually very late in the mornings <span class="codeitalic1">--snip--</span>'}</pre>&#13;
<p class="indent">Immediately after populating the dictionary, print the first 300 items for the <span class="literal">doyle</span> key to ensure things went as planned. This should produce the following printout:</p>&#13;
<pre>Mr. Sherlock Holmes, who was usually very late in the mornings, save&#13;
upon those not infrequent occasions when he was up all night, was seated&#13;
at the breakfast table. I stood upon the hearth-rug and picked up the&#13;
stick which our visitor had left behind him the night before. It was a&#13;
fine, thick piec</pre>&#13;
<p class="indent">With the corpora loaded correctly, the next step is to tokenize the strings into words. Currently, Python doesn’t recognize words but instead works on <em>characters</em>, such as letters, numbers, and punctuation marks. To remedy this, you’ll use the <span class="literal">make_word_dict()</span> function to take the <span class="literal">strings_by_author</span> dictionary as an argument, split out the words in the strings, and return a dictionary called <span class="literal">words_by_author</span> with the authors as keys and a list of words as values <span class="ent">➋</span>.</p>&#13;
<p class="indent">Stylometry relies on word counts, so it works best when each corpus is the same length. There are multiple ways to ensure apples-to-apples comparisons. With <em>chunking</em>, you divide the text into blocks of, say, 5,000 words, and compare the blocks. You can also normalize by using relative frequencies, rather than direct counts, or by truncating to the shortest corpus.</p>&#13;
<p class="indent">Let’s explore the truncation option. Pass the words dictionary to another function, <span class="literal">find_shortest_corpus()</span>, which calculates the number of words in each author’s list and returns the length of the shortest corpus. <a href="ch02.xhtml#ch02table1">Table 2-1</a> shows the length of each corpus.<span epub:type="pagebreak" id="page_34"/></p>&#13;
<p class="tabcap"><a id="ch02table1"/><strong>Table 2-1:</strong> Length (Word Count) of Each Corpus</p>&#13;
<table class="topbot-d">&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Corpus</p></th>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Length</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="taba">Hound (Doyle)</p></td>&#13;
<td style="vertical-align: top;"><p class="taba">58,387</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;"><p class="taba">War (Wells)</p></td>&#13;
<td style="vertical-align: top;"><p class="taba">59,469</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-h1aa"><p class="taba">World (Unknown)</p></td>&#13;
<td style="vertical-align: top;" class="table-h1aa"><p class="taba">74,961</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Since the shortest corpus here represents a robust dataset of almost 60,000 words, you’ll use the <span class="literal">len_shortest_corpus</span> variable to truncate the other two corpora to this length, prior to doing any analysis. The assumption, of course, is that the backend content of the truncated texts is not significantly different from that in the front.</p>&#13;
<p class="indent">The next five lines call functions that perform the stylometric analysis, as listed in “The Strategy” on <a href="ch02.xhtml#page_28">page 28</a> <span class="ent">➌</span>. All the functions take the <span class="literal">words_by_author</span> dictionary as an argument, and most take <span class="literal">len_shortest_corpus</span>, as well. We’ll look at these functions as soon as we finish preparing the texts for analysis.</p>&#13;
<h5 class="h5"><strong>Loading Text and Building a Word Dictionary</strong></h5>&#13;
<p class="noindent"><a href="ch02.xhtml#ch02list2">Listing 2-2</a> defines two functions. The first reads in a text file as a string. The second builds a dictionary with each author’s name as the key and his novel, now tokenized into individual words rather than a continuous string, as the value.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 2</span>&#13;
  def text_to_string(filename):&#13;
      """Read a text file and return a string."""&#13;
      with open(filename) as infile:&#13;
          return infile.read()&#13;
&#13;
<span class="ent">➊</span> def make_word_dict(strings_by_author):&#13;
      """Return dictionary of tokenized words by corpus by author."""&#13;
      words_by_author = dict()&#13;
      for author in strings_by_author:&#13;
          tokens = nltk.word_tokenize(strings_by_author[author])&#13;
       <span class="ent">➋</span> words_by_author[author] = ([token.lower() for token in tokens&#13;
                                      if token.isalpha()])&#13;
      return words_by_author</pre>&#13;
<p class="listing"><a id="ch02list2"/>Listing 2-2: Defining the <span class="literal">text_to_string()</span> and <span class="literal">make_word_dict()</span> functions</p>&#13;
<p class="indent">First, define the <span class="literal">text_to_string()</span> function to load a text file. The built-in <span class="literal">read()</span> function reads the whole file as an individual string, allowing relatively easy file-wide manipulations. Use <span class="literal">with</span> to open the file so that it will be closed automatically regardless of how the block terminates. Just like putting away your toys, closing files is good practice. It prevents bad things from happening, like running out of file descriptors, locking files from further access, corrupting files, or losing data if writing to files.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_35"/>Some users may encounter a <span class="literal">UnicodeDecodeError</span> like the following one when loading the text:</p>&#13;
<pre>UnicodeDecodeError: 'ascii' codec can't decode byte 0x93 in position 365:&#13;
ordinal not in range(128)</pre>&#13;
<p class="indent"><em>Encoding</em> and <em>decoding</em> refer to the process of converting from characters stored as bytes to human-readable strings. The problem is that the default encoding for the built-in function <span class="literal">open()</span> is platform dependent and depends on the value of <span class="literal">locale.getpreferredencoding()</span>. For example, you’ll get the following encoding if you run this on Windows 10:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import locale</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">locale.getpreferredencoding()</span>&#13;
'cp1252'</pre>&#13;
<p class="indent">CP-1252 is a legacy Windows character encoding. If you run the same code on a Mac, it may return something different, like <span class="literal">'US-ASCII'</span> or <span class="literal">'UTF-8'</span>.</p>&#13;
<p class="indent">UTF stands for <em>Unicode Transformational Format</em>, which is a text character format designed for backward compatibility with ASCII. Although UTF-8 can handle all character sets—and is the dominant form of encoding used on the World Wide Web—it’s not the default option for many text editors.</p>&#13;
<p class="indent">Additionally, Python 2 assumed all text files were encoded with <span class="literal">latin-1</span>, used for the Latin alphabet. Python 3 is more sophisticated and tries to detect encoding problems as early as possible. It may throw an error, however, if the encoding isn’t specified.</p>&#13;
<p class="indent">So, the first troubleshooting step should be to pass <span class="literal">open()</span> the <span class="literal">encoding</span> argument and specify UTF-8.</p>&#13;
<pre>    with open(filename, encoding='utf-8') as infile:</pre>&#13;
<p class="indent">If you still have problems loading the corpora files, try adding an <span class="literal">errors</span> argument as follows:</p>&#13;
<pre>    with open(filename, encoding='utf-8', errors='ignore') as infile:</pre>&#13;
<p class="indent">You can ignore errors because these text files were downloaded as UTF-8 and have already been tested using this approach. For more on UTF-8, see <em><a href="https://docs.python.org/3/howto/unicode.html">https://docs.python.org/3/howto/unicode.html</a></em>.</p>&#13;
<p class="indent">Next, define the <span class="literal">make_word_dict()</span> function that will take the dictionary of strings by author and return a dictionary of words by author <span class="ent">➊</span>. First, initialize an empty dictionary named <span class="literal">words_by_author</span>. Then, loop through the keys in the <span class="literal">strings_by_author</span> dictionary. Use NLTK’s <span class="literal">word_tokenize()</span> method and pass it the string dictionary’s key. The result will be a list of tokens that will serve as the dictionary value for each author. Tokens are just chopped up pieces of a corpus, typically sentences or words.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_36"/>The following snippet demonstrates how the process turns a continuous string into a list of tokens (words and punctuation):</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">import nltk</span>		  &#13;
&gt;&gt;&gt; <span class="codestrong1">str1 = 'The rain in Spain falls mainly on the plain.'</span>		  &#13;
&gt;&gt;&gt; <span class="codestrong1">tokens = nltk.word_tokenize(str1)</span>		  &#13;
&gt;&gt;&gt; <span class="codestrong1">print(type(tokens))</span>		  &#13;
&lt;class 'list'&gt;&#13;
&gt;&gt;&gt; <span class="codestrong1">tokens</span>		  &#13;
['The', 'rain', 'in', 'Spain', 'falls', 'mainly', 'on', 'the', 'plain', '.']</pre>&#13;
<p class="indent">This is similar to using Python’s built-in <span class="literal">split()</span> function, but <span class="literal">split()</span> doesn’t achieve tokens from a linguistic standpoint (note that the period is not tokenized).</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">my_tokens = str1.split()</span>		  &#13;
&gt;&gt;&gt; <span class="codestrong1">my_tokens</span>		  &#13;
['The', 'rain', 'in', 'Spain', 'falls', 'mainly', 'on', 'the', 'plain.']</pre>&#13;
<p class="indent">Once you have the tokens, populate the <span class="literal">words_by_author</span> dictionary using list comprehension <span class="ent">➋</span>. <em>List comprehension</em> is a shorthand way to execute loops in Python. You need to surround the code with square brackets to indicate a list. Convert the tokens to lowercase and use the built-in <span class="literal">isalpha()</span> method, which returns <span class="literal">True</span> if all the characters in a token are part of the alphabet and <span class="literal">False</span> otherwise. This will filter out numbers and punctuation. It will also filter out hyphenated words or names. Finish by returning the <span class="literal">words_by_author</span> dictionary.</p>&#13;
<h5 class="h5"><strong>Finding the Shortest Corpus</strong></h5>&#13;
<p class="noindent">In computational linguistics, <em>frequency</em> refers to the number of occurrences in a corpus. Thus, frequency means the <em>count</em>, and methods you’ll use later return a dictionary of words and their counts. To compare counts in a meaningful way, the corpora should all have the same number of words.</p>&#13;
<p class="indent">Because the three corpora used here are large (see <a href="ch02.xhtml#ch02table1">Table 2-1</a>), you can safely normalize the corpora by truncating them all to the length of the shortest. <a href="ch02.xhtml#ch02list3">Listing 2-3</a> defines a function that finds the shortest corpus in the <span class="literal">words_by_author</span> dictionary and returns its length.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 3</span>&#13;
def find_shortest_corpus(words_by_author):&#13;
    """Return length of shortest corpus."""&#13;
    word_count = []&#13;
    for author in words_by_author:&#13;
        word_count.append(len(words_by_author[author]))&#13;
        print('\nNumber of words for {} = {}\n'.&#13;
              format(author, len(words_by_author[author])))&#13;
    len_shortest_corpus = min(word_count)&#13;
    print('length shortest corpus = {}\n'.format(len_shortest_corpus))        &#13;
    return len_shortest_corpus</pre>&#13;
<p class="listing"><a id="ch02list3"/>Listing 2-3: Defining the <span class="literal">find_shortest_corpus()</span> function</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_37"/>Define the function that takes the <span class="literal">words_by_author</span> dictionary as an argument. Immediately start an empty list to hold a word count.</p>&#13;
<p class="indent">Loop through the authors (keys) in the dictionary. Get the length of the value for each key, which is a list object, and append the length to the <span class="literal">word_count</span> list. The length here represents the number of words in the corpus. For each pass through the loop, print the author’s name and the length of his tokenized corpus.</p>&#13;
<p class="indent">When the loop ends, use the built-in <span class="literal">min()</span> function to get the lowest count and assign it to the <span class="literal">len_shortest_corpus</span> variable. Print the answer and then return the variable.</p>&#13;
<h5 class="h5"><strong>Comparing Word Lengths</strong></h5>&#13;
<p class="noindent">Part of an author’s distinctive style is the words they use. Faulkner observed that Hemingway never sent a reader running to the dictionary; Hemingway accused Faulkner of using “10-dollar words.” Authorial style is expressed in the length of words and in vocabulary, which we’ll look at later in the chapter.</p>&#13;
<p class="indent"><a href="ch02.xhtml#ch02list4">Listing 2-4</a> defines a function to compare the length of words per corpus and plot the results as a frequency distribution. In a frequency distribution, the lengths of words are plotted against the number of counts for each length. For words that are six letters long, for example, one author may have a count of 4,000, and another may have a count of 5,500. A frequency distribution allows comparison across a range of word lengths, rather than just at the average word length.</p>&#13;
<p class="indent">The function in <a href="ch02.xhtml#ch02list4">Listing 2-4</a> uses list slicing to truncate the word lists to the length of the shortest corpus so the results aren’t skewed by the size of the novel.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 4</span>&#13;
def word_length_test(words_by_author, len_shortest_corpus):&#13;
    """Plot word length freq by author, truncated to shortest corpus length."""&#13;
    by_author_length_freq_dist = dict()&#13;
    plt.figure(1)    &#13;
    plt.ion()&#13;
&#13;
 <span class="ent">➊</span> for i, author in enumerate(words_by_author):&#13;
        word_lengths = [len(word) for word in words_by_author[author]&#13;
                        [:len_shortest_corpus]]&#13;
        by_author_length_freq_dist[author] = nltk.FreqDist(word_lengths)&#13;
     <span class="ent">➋</span> by_author_length_freq_dist[author].plot(15, &#13;
                                                linestyle=LINES[i],                                                  &#13;
                                                label=author,   &#13;
                                                title='Word Length')&#13;
    plt.legend()&#13;
    #plt.show()  # Uncomment to see plot while coding.</pre>&#13;
<p class="listing"><a id="ch02list4"/>Listing 2-4: Defining the <span class="literal">word_length_test()</span> function</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_38"/>All the stylometric functions will use the dictionary of tokens; almost all will use the length of the shortest corpus parameter to ensure consistent sample sizes. Use these variable names as the function parameters.</p>&#13;
<p class="indent">Start an empty dictionary to hold the frequency distribution of word lengths by author and then start making plots. Since you are going to make multiple plots, start by instantiating a figure object named <span class="literal">1</span>. So that all the plots stay up after creation, turn on the interactive plot mode with <span class="literal">plt.ion()</span>.<span class="literal"/></p>&#13;
<p class="indent">Next, start looping through the authors in the tokenized dictionary <span class="ent">➊</span>. Use the <span class="literal">enumerate()</span> function to generate an index for each author that you’ll use to choose a line style for the plot. For each author, use list comprehension to get the length of each word in the value list, with the range truncated to the length of the shortest corpus. The result will be a list where each word has been replaced by an integer representing its length.</p>&#13;
<p class="indent">Now, start populating your new by-author dictionary to hold frequency distributions. Use <span class="literal">nltk.FreqDist()</span>, which takes the list of word lengths and creates a data object of word frequency information that can be plotted.</p>&#13;
<p class="indent">You can plot the dictionary directly using the class method <span class="literal">plot()</span>, without the need to reference <span class="literal">pyplot</span> through <span class="literal">plt</span> <span class="ent">➋</span>. This will plot the most frequently occurring sample first, followed by the number of samples you specify, in this case, <span class="literal">15</span>. This means you will see the frequency distribution of words from 1 to 15 letters long. Use <span class="literal">i</span> to select from the <span class="literal">LINES</span> list and finish by providing a label and a title. The label will be used in the legend, called using <span class="literal">plt.legend()</span>.</p>&#13;
<p class="indent">Note that you can change how the frequency data plots using the <span class="literal">cumulative</span> parameter. If you specify <span class="literal">cumulative=True</span>, you will see a cumulative distribution (<a href="ch02.xhtml#ch02fig3">Figure 2-3</a>, left). Otherwise, <span class="literal">plot()</span> will default to <span class="literal">cumulative=False</span>, and you will see the actual counts, arranged from highest to lowest (<a href="ch02.xhtml#ch02fig3">Figure 2-3</a>, right). Continue to use the default option for this project.</p>&#13;
<div class="image"><img src="../images/fig02_03.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig3"/>Figure 2-3: The NLTK cumulative plot (left) versus the default frequency plot (right)</p>&#13;
<p class="indent">Finish by calling the <span class="literal">plt.show()</span> method to display the plot, but leave it commented out. If you want to see the plot immediately after coding this function, you can uncomment it. Also note that if you launch this program via <span epub:type="pagebreak" id="page_39"/>Windows PowerShell, the plots may close immediately unless you use the <span class="literal">block</span> flag: <span class="literal">plt.show(block=True)</span>. This will keep the plot up but halt execution of the program until the plot is closed.</p>&#13;
<p class="indent">Based solely on the word length frequency plot in <a href="ch02.xhtml#ch02fig3">Figure 2-3</a>, Doyle’s style matches the unknown author’s more closely, though there are segments where Wells compares the same or better. Now let’s run some other tests to see whether we can confirm that finding.</p>&#13;
<h5 class="h5"><strong>Comparing Stop Words</strong></h5>&#13;
<p class="noindent">A <em>stop word</em> is a small word used often, like <em>the</em>, <em>by</em>, and <em>but</em>. These words are filtered out for tasks like online searches, because they provide no contextual information, and they were once thought to be of little value in identifying authorship.</p>&#13;
<p class="indent">But stop words, used frequently and without much thought, are perhaps the best signature for an author’s style. And since the texts you’re comparing are usually about different subjects, these stop words become important, as they are agnostic to content and common across all texts.</p>&#13;
<p class="indent"><a href="ch02.xhtml#ch02list5">Listing 2-5</a> defines a function to compare the use of stop words in the three corpora.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 5</span>&#13;
def stopwords_test(words_by_author, len_shortest_corpus):&#13;
    """Plot stopwords freq by author, truncated to shortest corpus length."""&#13;
    stopwords_by_author_freq_dist = dict()&#13;
    plt.figure(2)&#13;
    stop_words = set(stopwords.words('english'))  # Use set for speed.&#13;
    #print('Number of stopwords = {}\n'.format(len(stop_words)))&#13;
    #print('Stopwords = {}\n'.format(stop_words))&#13;
&#13;
    for i, author in enumerate(words_by_author):        &#13;
        stopwords_by_author = [word for word in words_by_author[author]&#13;
                               [:len_shortest_corpus] if word in stop_words]    &#13;
        stopwords_by_author_freq_dist[author] = nltk.FreqDist(stopwords_by_&#13;
        author)    &#13;
        stopwords_by_author_freq_dist[author].plot(50, &#13;
                                                   label=author,&#13;
                                                   linestyle=LINES[i],&#13;
                                                   title=&#13;
                                                   '50 Most Common Stopwords')&#13;
    plt.legend()&#13;
##    plt.show()  # Uncomment to see plot while coding function.</pre>&#13;
<p class="listing"><a id="ch02list5"/>Listing 2-5: Defining the <span class="literal">stopwords_test()</span> function</p>&#13;
<p class="indent">Define a function that takes the words dictionary and the length of the shortest corpus variables as arguments. Then initialize a dictionary to hold the frequency distribution of stop words for each author. You don’t want to cram all the plots in the same figure, so start a new figure named <span class="literal">2</span>.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_40"/>Assign a local variable, <span class="literal">stop_words</span>, to the NLTK stop words corpus for English. Sets are quicker to search than lists, so make the corpus a set for faster lookups later. The next two lines, currently commented out, print the number of stop words (179) and the stop words themselves.</p>&#13;
<p class="indent">Now, start looping through the authors in the <span class="literal">words_by_author</span> dictionary. Use list comprehension to pull out all the stop words in each author’s corpus and use these as the value in a new dictionary named <span class="literal">stopwords_by_author</span>. In the next line, you’ll pass this dictionary to NLTK’s <span class="literal">FreqDist()</span> method and use the output to populate the <span class="literal">stopwords_by_author_freq_dist</span> dictionary. This dictionary will contain the data needed to make the frequency distribution plots for each author.</p>&#13;
<p class="indent">Repeat the code you used to plot the word lengths in <a href="ch02.xhtml#ch02list4">Listing 2-4</a>, but set the number of samples to <span class="literal">50</span> and give it a different title. This will plot the top 50 stop words in use (<a href="ch02.xhtml#ch02fig4">Figure 2-4</a>).</p>&#13;
<div class="image"><img src="../images/fig02_04.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig4"/>Figure 2-4: Frequency plot of top 50 stop words by author</p>&#13;
<p class="indent">Both Doyle and the unknown author use stop words in a similar manner. At this point, two analyses have favored Doyle as the most likely author of the unknown text, but there’s still more to do.</p>&#13;
<h5 class="h5"><strong>Comparing Parts of Speech</strong></h5>&#13;
<p class="noindent">Now let’s compare the parts of speech used in the three corpora. NLTK uses a part-of-speech (POS) tagger, called <span class="literal">PerceptronTagger</span>, to identify parts of speech. POS taggers process a sequence of tokenized words and attach a POS tag to each word (see <a href="ch02.xhtml#ch02table2">Table 2-2</a>).<span epub:type="pagebreak" id="page_41"/></p>&#13;
<p class="tabcap"><a id="ch02table2"/><strong>Table 2-2:</strong> Parts of Speech with Tag Values</p>&#13;
<table class="topbot-d">&#13;
<thead>&#13;
<tr>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Part of Speech</p></th>&#13;
<th style="vertical-align: top;" class="table-h_ch2"><p class="taba">Tag</p></th>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Part of Speech</p></th>&#13;
<th style="vertical-align: top;" class="table-h"><p class="taba">Tag</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Coordinating conjunction</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">CC</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Possessive pronoun</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">PRP$</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Cardinal number</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">CD</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Adverb</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">RB</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Determiner</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">DT</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Adverb, comparative</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">RBR</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Existential there</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">EX</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Adverb, superlative</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">RBS</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Foreign word</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">FW</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Particle</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">RP</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Preposition or subordinating conjunction</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">IN</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Symbol</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">SYM</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Adjective</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">JJ</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">To</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">TO</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Adjective, comparative</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">JJR</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Interjection</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">UH</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Adjective, superlative</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">JJS</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Verb, base form</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">VB</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">List item marker</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">LS</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Verb, past tense</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">VBD</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Modal</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">MD</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Verb, gerund or present participle</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">VBG</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Noun, singular or mass</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">NN</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Verb, past participle</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">VBN</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Noun, plural</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">NNS</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Verb, non-third-person singular present</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">VBP</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Noun, proper noun, singular</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">NNP</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Verb, third-person singular present</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">VBZ</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Noun, proper noun, plural</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">NNPS</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Wh-determiner, which</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">WDT</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Predeterminer</p></td>&#13;
<td style="vertical-align: top;" class="table-b1_ch2"><p class="taba">PDT</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">Wh-pronoun, who, what</p></td>&#13;
<td style="vertical-align: top;" class="table-b1"><p class="taba">WP</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Possessive ending</p></td>&#13;
<td style="vertical-align: top;" class="table-a_ch2"><p class="taba">POS</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">Possessive wh-pronoun, whose</p></td>&#13;
<td style="vertical-align: top;" class="table-a"><p class="taba">WP$</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top;" class="table-h1"><p class="taba">Personal pronoun</p></td>&#13;
<td style="vertical-align: top;" class="table-h1_ch2"><p class="taba">PRP</p></td>&#13;
<td style="vertical-align: top;" class="table-h1"><p class="taba">Wh-adverb, where, when</p></td>&#13;
<td style="vertical-align: top;" class="table-h1"><p class="taba">WRB</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The taggers are typically trained on large datasets like the <em>Penn Treebank</em> or <em>Brown Corpus</em>, making them highly accurate though not perfect. You can also find training data and taggers for languages other than English. You don’t need to worry about all these various terms and their abbreviations. As with the previous tests, you’ll just need to compare lines in a chart.</p>&#13;
<p class="indent"><a href="ch02.xhtml#ch02list6">Listing 2-6</a> defines a function to plot the frequency distribution of POS in the three corpora.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 6</span>&#13;
def parts_of_speech_test(words_by_author, len_shortest_corpus):&#13;
    """Plot author use of parts-of-speech such as nouns, verbs, adverbs."""&#13;
    by_author_pos_freq_dist = dict()&#13;
    plt.figure(3)&#13;
    for i, author in enumerate(words_by_author):&#13;
        pos_by_author = [pos[1] for pos in nltk.pos_tag(words_by_author[author]&#13;
                            [:len_shortest_corpus])] &#13;
        by_author_pos_freq_dist[author] = nltk.FreqDist(pos_by_author)&#13;
        by_author_pos_freq_dist[author].plot(35, &#13;
                                             label=author,&#13;
                                             linestyle=LINES[i],&#13;
                                             title='Part of Speech')&#13;
    plt.legend()&#13;
    plt.show()</pre>&#13;
<p class="listing"><a id="ch02list6"/>Listing 2-6: Defining the <span class="literal">parts_of_speech_test()</span> function</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_42"/>Define a function that takes as arguments—you guessed it—the words dictionary and the length of the shortest corpus. Then initialize a dictionary to hold the frequency distribution for the POS for each author, followed by a function call for a third figure.</p>&#13;
<p class="indent">Start looping through the authors in the <span class="literal">words_by_author</span> dictionary and use list comprehension and the NLTK <span class="literal">pos_tag()</span> method to build a list called <span class="literal">pos_by_author</span>. For each author, this creates a list with each word in the author’s corpus replaced by its corresponding POS tag, as shown here:</p>&#13;
<pre>['NN', 'NNS', 'WP', 'VBD', 'RB', 'RB', 'RB', 'IN', 'DT', 'NNS', <span class="codeitalic1">--snip--</span>]</pre>&#13;
<p class="indent">Next, make a frequency distribution of the POS list and with each loop plot the curve, using the top 35 samples. Note that there are only 36 POS tags and several, such as <em>list item markers</em>, rarely appear in novels.</p>&#13;
<p class="indent">This is the final plot you’ll make, so call <span class="literal">plt.show()</span> to draw all the plots to the screen. As pointed out in the discussion of <a href="ch02.xhtml#ch02list4">Listing 2-4</a>, if you’re using Windows PowerShell to launch the program, you may need to use <span class="literal">plt.show(block=True)</span> to keep the plots from closing automatically.</p>&#13;
<p class="indent">The previous plots, along with the current one (<a href="ch02.xhtml#ch02fig5">Figure 2-5</a>), should appear after about 10 seconds.</p>&#13;
<div class="image"><img src="../images/fig02_05.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig5"/>Figure 2-5: Frequency plot of top 35 parts of speech by author</p>&#13;
<p class="indent">Once again, the match between the Doyle and unknown curves is clearly better than the match of unknown to Wells. This suggests that Doyle is the author of the unknown corpus.</p>&#13;
<h5 class="h5"><strong>Comparing Author Vocabularies</strong></h5>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_43"/>To compare the vocabularies among the three corpora, you’ll use the <em>chi-squared random variable</em> (X<sup>2</sup>), also known as the <em>test statistic</em>, to measure the “distance” between the vocabularies employed in the unknown corpus and each of the known corpora. The closest vocabularies will be the most similar. The formula is</p>&#13;
<div class="image1"><img src="../images/equ_page_43_01.jpg" alt="Image"/></div>&#13;
<p class="noindent">where <em>O</em> is the observed word count and <em>E</em> is the expected word count assuming the corpora being compared are both by the same author.</p>&#13;
<p class="indent">If Doyle wrote both novels, they should both have the same—or a similar—proportion of the most common words. The test statistic lets you quantify how similar they are by measuring how much the counts for each word differ. The lower the chi-squared test statistic, the greater the similarity between two distributions.</p>&#13;
<p class="indent"><a href="ch02.xhtml#ch02list7">Listing 2-7</a> defines a function to compare vocabularies among the three corpora.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 7</span>&#13;
def vocab_test(words_by_author):&#13;
    """Compare author vocabularies using the chi-squared statistical test."""&#13;
    chisquared_by_author = dict()&#13;
    for author in words_by_author:&#13;
     <span class="ent">➊</span> if author != 'unknown': &#13;
           combined_corpus = (words_by_author[author] +&#13;
                              words_by_author['unknown'])&#13;
           author_proportion = (len(words_by_author[author])/&#13;
                                len(combined_corpus))&#13;
           combined_freq_dist = nltk.FreqDist(combined_corpus)		&#13;
           most_common_words = list(combined_freq_dist.most_common(1000))&#13;
           chisquared = 0&#13;
        <span class="ent">➋</span> for word, combined_count in most_common_words:&#13;
              observed_count_author = words_by_author[author].count(word)&#13;
              expected_count_author = combined_count * author_proportion&#13;
              chisquared += ((observed_count_author -&#13;
                              expected_count_author)**2 /&#13;
                             expected_count_author)&#13;
           <span class="ent">➌</span> chisquared_by_author[author] = chisquared    &#13;
         print('Chi-squared for {} = {:.1f}'.format(author, chisquared))&#13;
 most_likely_author = min(chisquared_by_author, key=chisquared_by_author.get)&#13;
 print('Most-likely author by vocabulary is {}\n'.format(most_likely_author))</pre>&#13;
<p class="listing"><a id="ch02list7"/>Listing 2-7: Defining the <span class="literal">vocab_test()</span> function</p>&#13;
<p class="indent">The <span class="literal">vocab_test()</span> function needs the word dictionary but not the length of the shortest corpus. Like the previous functions, however, it starts by creating a new dictionary to hold the chi-squared value per author and then loops through the word dictionary.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_44"/>To calculate chi-squared, you’ll need to join each author’s corpus with the unknown corpus. You don’t want to combine <span class="literal">unknown</span> with itself, so use a conditional to avoid this <span class="ent">➊</span>. For the current loop, combine the author’s corpus with the unknown one and then get the current author’s proportion by dividing the length of his corpus by the length of the combined corpus. Then get the frequency distribution of the combined corpus by calling <span class="literal">nltk.FreqDist()</span>.</p>&#13;
<p class="indent">Now, make a list of the 1,000 most common words in the combined text by using the <span class="literal">most_common()</span> method and passing it <span class="literal">1000</span>. There is no hard-and-fast rule for how many words you should consider in a stylometric analysis. Suggestions in the literature call for the most common 100 to 1,000 words. Since you are working with large texts, err on the side of the larger value.</p>&#13;
<p class="indent">Initialize the <span class="literal">chisquared</span> variable with <span class="literal">0</span>; then start a nested <span class="literal">for</span> loop that works through the <span class="literal">most_common_words</span> list <span class="ent">➋</span>. The <span class="literal">most_common()</span> method returns a list of tuples, with each tuple containing the word and its count.</p>&#13;
<pre>[('the', 7778), ('of', 4112), ('and', 3713), ('i', 3203), ('a', 3195), <span class="codeitalic1">--snip--</span>]</pre>&#13;
<p class="indent">Next, you get the observed count per author from the word dictionary. For Doyle, this would be the count of the most common words in the corpus of <em>The Hound of the Baskervilles</em>. Then, you get the expected count, which for Doyle would be the count you would expect if he wrote both <em>The Hound of the Baskervilles</em> and the unknown corpus. To do this, multiply the number of counts in the combined corpus by the previously calculated author’s proportion. Then apply the formula for chi-squared and add the result to the dictionary that tracks each author’s chi-squared score <span class="ent">➌</span>. Display the result for each author.</p>&#13;
<p class="indent">To find the author with the lowest chi-squared score, call the built-in <span class="literal">min()</span> function and pass it the dictionary and dictionary key, which you obtain with the <span class="literal">get()</span> method. This will yield the <em>key</em> corresponding to the minimum <em>value</em>. This is important. If you omit this last argument, <span class="literal">min()</span> will return the minimum <em>key</em> based on the alphabetical order of the names, <em>not</em> their chi-squared score! You can see this mistake in the following snippet:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">print(mydict)</span>&#13;
{'doyle': 100, 'wells': 5}&#13;
&gt;&gt;&gt; <span class="codestrong1">minimum = min(mydict)</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(minimum)</span>&#13;
'doyle'&#13;
&gt;&gt;&gt; <span class="codestrong1">minimum = min(mydict, key=mydict.get)</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(minimum)</span>&#13;
'wells'</pre>&#13;
<p class="indent">It’s easy to assume that the <span class="literal">min()</span> function returns the minimum numerical <em>value</em>, but as you saw, it looks at dictionary <em>keys</em> by default.</p>&#13;
<p class="indent">Complete the function by printing the most likely author based on the chi-squared score.</p>&#13;
<pre><span epub:type="pagebreak" id="page_45"/>Chi-squared for doyle = 4744.4&#13;
Chi-squared for wells = 6856.3&#13;
Most-likely author by vocabulary is doyle</pre>&#13;
<p class="indent">Yet another test suggests that Doyle is the author!</p>&#13;
<h5 class="h5"><strong>Calculating Jaccard Similarity</strong></h5>&#13;
<p class="noindent">To determine the degree of similarity among sets created from the corpora, you’ll use the <em>Jaccard similarity coefficient</em>. Also called the <em>intersection over union</em>, this is simply the area of overlap between two sets divided by the area of union of the two sets (<a href="ch02.xhtml#ch02fig6">Figure 2-6</a>).</p>&#13;
<div class="image"><img src="../images/fig02_06.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig6"/>Figure 2-6: Intersection-over-union for a set is the area of overlap divided by the area of union.</p>&#13;
<p class="indent">The more overlap there is between sets created from two texts, the more likely they were written by the same author. <a href="ch02.xhtml#ch02list8">Listing 2-8</a> defines a function for gauging the similarity of sample sets.</p>&#13;
<pre><span class="codeitalic1">stylometry.py,</span> <span class="normal">part 8</span>&#13;
def jaccard_test(words_by_author, len_shortest_corpus):&#13;
    """Calculate Jaccard similarity of each known corpus to unknown corpus."""&#13;
    jaccard_by_author = dict()&#13;
    unique_words_unknown = set(words_by_author['unknown']&#13;
                               [:len_shortest_corpus])&#13;
 <span epub:type="pagebreak" id="page_46"/><span class="ent">➊</span> authors = (author for author in words_by_author if author != 'unknown')&#13;
    for author in authors:&#13;
        unique_words_author = set(words_by_author[author][:len_shortest_corpus]) &#13;
        shared_words = unique_words_author.intersection(unique_words_unknown)&#13;
     <span class="ent">➋</span> jaccard_sim = (float(len(shared_words))/ (len(unique_words_author) +&#13;
                                                  len(unique_words_unknown) -&#13;
                                                  len(shared_words)))&#13;
        jaccard_by_author[author] = jaccard_sim&#13;
        print('Jaccard Similarity for {} = {}'.format(author, jaccard_sim))&#13;
 <span class="ent">➌</span> most_likely_author = max(jaccard_by_author, key=jaccard_by_author.get)&#13;
    print('Most-likely author by similarity is {}'.format(most_likely_author))&#13;
&#13;
if __name__ == '__main__':&#13;
    main()</pre>&#13;
<p class="listing"><a id="ch02list8"/>Listing 2-8:  Defining the <span class="literal">jaccard_test()</span> function</p>&#13;
<p class="indent">Like most of the previous tests, the <span class="literal">jaccard_test()</span> function takes the word dictionary and length of the shortest corpus as arguments. You’ll also need a dictionary to hold the Jaccard coefficient for each author.</p>&#13;
<p class="indent">Jaccard similarity works with unique words, so you’ll need to turn the corpora into sets to remove duplicates. First, you’ll build a set from the <span class="literal">unknown</span> corpus. Then you’ll loop through the known corpora, turning them into sets and comparing them to the unknown set. Be sure to truncate all the corpora to the length of the shortest corpus when making the sets.</p>&#13;
<p class="indent">Prior to running the loop, use a generator expression to get the names of the authors, other than <span class="literal">unknown</span>, from the <span class="literal">words_by_author</span> dictionary <span class="ent">➊</span>. A <em>generator expression</em> is a function that returns an object that you can iterate over one value at a time. It looks a lot like list comprehension, but instead of square brackets, it’s surrounded by parentheses. And instead of constructing a potentially memory-intensive list of items, the generator yields them in real time. Generators are useful when you have a large set of values that you need to use only once. I use one here as an opportunity to demonstrate the process.</p>&#13;
<p class="indent">When you assign a generator expression to a variable, all you get is a type of iterator called a <em>generator object</em>. Compare this to making a list, as shown here:</p>&#13;
<pre>&gt;&gt;&gt; <span class="codestrong1">mylist = [i for i in range(4)]</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">mylist</span>&#13;
[0, 1, 2, 3]&#13;
&gt;&gt;&gt; <span class="codestrong1">mygen = (i for i in range(4))</span>&#13;
&gt;&gt;&gt; <span class="codestrong1">mygen</span>&#13;
&lt;generator object &lt;genexpr&gt; at 0x000002717F547390&gt;</pre>&#13;
<p class="indent">The generator expression in the previous snippet is the same as this generator function:</p>&#13;
<pre>def generator(my_range):&#13;
    for i in range(my_range):&#13;
        yield i</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_47"/>Whereas the <span class="literal">return</span> statement ends a function, the <span class="literal">yield</span> statement <em>suspends</em> the function’s execution and sends a value back to the caller. Later, the function can resume where it left off. When a generator reaches its end, it’s “empty” and can’t be called again.</p>&#13;
<p class="indent">Back to the code, start a <span class="literal">for</span> loop using the <span class="literal">authors</span> generator. Find the unique words for each known author, just as you did for <span class="literal">unknown</span>. Then use the built-in <span class="literal">intersection()</span> function to find all the words shared between the current author’s set of words and the set for <span class="literal">unknown</span>. The <em>intersection</em> of two given sets is the largest set that contains all the elements that are common to both. With this information, you can calculate the Jaccard similarity coefficient <span class="ent">➋</span>.</p>&#13;
<p class="indent">Update the <span class="literal">jaccard_by_author</span> dictionary and print each outcome in the interpreter window. Then find the author with the maximum Jaccard value <span class="ent">➌</span> and print the results.</p>&#13;
<pre>Jaccard Similarity for doyle = 0.34847801578354004&#13;
Jaccard Similarity for wells = 0.30786921307869214&#13;
Most-likely author by similarity is doyle</pre>&#13;
<p class="indent">The outcome should favor Doyle.</p>&#13;
<p class="indent">Finish <em>stylometry.py</em> with the code to run the program as an imported module or in stand-alone mode.</p>&#13;
<h3 class="h3" id="ch00lev1sec13"><strong>Summary</strong></h3>&#13;
<p class="noindent">The true author of <em>The Lost World</em> is Doyle, so we’ll stop here and declare victory. If you want to explore further, a next step might be to add more known texts to <span class="literal">doyle</span> and <span class="literal">wells</span> so that their combined length is closer to that for <em>The Lost World</em> and you don’t have to truncate it. You could also test for sentence length and punctuation style or employ more sophisticated techniques like neural nets and genetic algorithms.</p>&#13;
<p class="indent">You can also refine existing functions, like <span class="literal">vocab_test()</span> and <span class="literal">jaccard_test()</span>, with <em>stemming</em> and <em>lemmatization</em> techniques that reduce words to their root forms for better comparisons. As the program is currently written, <em>talk</em>, <em>talking</em>, and <em>talked</em> are all considered completely different words even though they share the same root.</p>&#13;
<p class="indent">At the end of the day, stylometry can’t prove with absolute certainty that Sir Arthur Conan Doyle wrote <em>The Lost World</em>. It can only suggest, through weight of evidence, that he is the more likely author than Wells. Framing the question very specifically is important, since you can’t evaluate all possible authors. For this reason, successful authorship attribution begins with good old-fashioned detective work that trims the list of candidates to a manageable length.</p>&#13;
<h3 class="h3" id="ch00lev1sec14"><strong>Further Reading</strong></h3>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_48"/><em>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</em> (O’Reilly, 2009), by Steven Bird, Ewan Klein, and Edward Loper, is an accessible introduction to NLP using Python, with lots of exercises and useful integration with the NLTK website. A new version of the book, updated for Python 3 and NLTK 3, is available online at <em><a href="http://www.nltk.org/book/">http://www.nltk.org/book/</a></em>.</p>&#13;
<p class="indent">In 1995, novelist Kurt Vonnegut proposed the idea that “stories have shapes that can be drawn on graph paper” and suggested “feeding them into computers.” In 2018, researchers followed up on this idea using more than 1,700 English novels. They applied an NLP technique called <em>sentiment analysis</em> that finds the emotional tone behind words. An interesting summary of their results, “Every Story in the World Has One of These Six Basic Plots,” can be found on the <a href="http://BBC.com">BBC.com</a> website: <em><a href="http://www.bbc.com/culture/story/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots/">http://www.bbc.com/culture/story/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots/</a></em>.</p>&#13;
<h3 class="h3" id="ch00lev1sec15"><strong>Practice Project: Hunting the Hound with Dispersion</strong></h3>&#13;
<p class="noindent">NLTK comes with a fun little feature, called a <em>dispersion plot</em>, that lets you post the location of a word in a text. More specifically, it plots the occurrences of a word versus how many words from the beginning of the corpus that it appears.</p>&#13;
<p class="indent"><a href="ch02.xhtml#ch02fig7">Figure 2-7</a> is a dispersion plot for major characters in <em>The Hound of the Baskervilles</em>.</p>&#13;
<div class="image"><img src="../images/fig02_07.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig7"/>Figure 2-7: Dispersion plot for major characters in <span class="normal">The Hound of the Baskervilles</span></p>&#13;
<p class="indent">If you’re familiar with the story—and I won’t spoil it if you’re not—then you’ll appreciate the sparse occurrence of Holmes in the middle, the almost</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_49"/>bimodal distribution of Mortimer, and the late story overlap of Barrymore, Selden, and the hound.</p>&#13;
<p class="indent">Dispersion plots can have more practical applications. For example, as the author of technical books, I need to define a new term when it first appears. This sounds easy, but sometimes the editing process can shuffle whole chapters, and issues like this can fall through the cracks. A dispersion plot, built with a long list of technical terms, can make finding these first occurrences a lot easier.</p>&#13;
<p class="indent">For another use case, imagine you’re a data scientist working with paralegals on a criminal case involving insider trading. To find out whether the accused talked to a certain board member just prior to making the illegal trades, you can load the subpoenaed emails of the accused as a continuous string and generate a dispersion plot. If the board member’s name appears as expected, case closed!</p>&#13;
<p class="indent">For this practice project, write a Python program that reproduces the dispersion plot shown in <a href="ch02.xhtml#ch02fig7">Figure 2-7</a>. If you have problems loading the <em>hound.txt</em> corpus, revisit the discussion of Unicode on <a href="ch02.xhtml#page_35">page 35</a>. You can find a solution, <em>practice_hound_dispersion.py</em>, in the appendix and online.</p>&#13;
<h3 class="h3" id="ch00lev1sec16"><strong>Practice Project: Punctuation Heatmap</strong></h3>&#13;
<p class="noindent">A <em>heatmap</em> is a diagram that uses colors to represent data values. Heatmaps have been used to visualize the punctuation habits of famous authors (<em><a href="https://www.fastcompany.com/3057101/the-surprising-punctuation-habits-of-famous-authors-visualized/">https://www.fastcompany.com/3057101/the-surprising-punctuation-habits-of-famous-authors-visualized/</a></em>) and may prove helpful in attributing authorship for <em>The Lost World</em>.</p>&#13;
<p class="indent">Write a Python program that tokenizes the three novels used in this chapter based solely on punctuation. Then focus on the use of semicolons. For each author, plot a heatmap that displays semicolons as blue and all other marks as yellow or red. <a href="ch02.xhtml#ch02fig8">Figure 2-8</a> shows example heatmaps for Wells’ <em>The War of the Worlds</em> and Doyle’s <em>The Hound of the Baskervilles</em>.</p>&#13;
<div class="image"><img src="../images/fig02_08.jpg" alt="Image"/></div>&#13;
<p class="figcap"><a id="ch02fig8"/>Figure 2-8: Heatmap of semicolon use (dark squares) for Wells (left) and Doyle (right)</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_50"/>Compare the three heatmaps. Do the results favor Doyle or Wells as the author for <em>The Lost World</em>?</p>&#13;
<p class="indent">You can find a solution, <em>practice_heatmap_semicolon.py</em>, in the appendix and online.</p>&#13;
<h3 class="h3" id="ch00lev1sec17"><strong>Challenge Project: Fixing Frequency</strong></h3>&#13;
<p class="noindent">As noted previously, frequency in NLP refers to counts, but it can also be expressed as the number of occurrences per unit time. Alternatively, it can be expressed as a ratio or percent.</p>&#13;
<p class="indent">Define a new version of the <span class="literal">nltk.FreqDist()</span> method that uses percentages, rather than counts, and use it to make the charts in the <em>stylometry.py</em> program. For help, see the Clearly Erroneous blog (<em><a href="https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/">https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/</a></em>).</p>&#13;
</body></html>