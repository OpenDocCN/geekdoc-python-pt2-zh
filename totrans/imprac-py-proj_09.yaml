- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9**'
- en: WRITING HAIKU WITH MARKOV CHAIN ANALYSIS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用马尔可夫链分析写俳句**
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Computers can write poetry by rearranging existing poems. This is basically
    what humans do. You and I didn’t invent the language we speak—we learned it. To
    talk or write, we just recombine existing words—and rarely in a truly original
    manner. As Sting once said about writing music, “I don’t think there’s such a
    thing as composition in pop music. I think what we do in pop music is collate
    . . . I’m a good collator.”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机可以通过重新排列现有的诗歌来写诗。这基本上是人类所做的事。你我并没有发明我们所说的语言——我们是通过学习它来掌握的。要说话或写作，我们只是重新组合现有的词汇——而且很少以真正原创的方式进行。正如Sting曾经说过的关于创作音乐的话，“我不认为流行音乐中有创作这种东西。我认为我们在流行音乐中做的是整理……我擅长整理。”
- en: In this chapter, you’re going to write a program that puts the “best words in
    the best order” in the form of haiku. But to do this, Python needs good examples,
    so you’ll need to provide a training corpus of haiku by the Japanese masters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将编写一个程序，按照俳句的形式将“最好的词放在最好的位置”。但是为了做到这一点，Python 需要优秀的示例，因此你需要提供由日本大师创作的俳句训练语料库。
- en: To rearrange these words in a meaningful manner, you will use *Markov chains*,
    named after Russian mathematician Andrey Markov. *Markov chain analysis*, an important
    part of probability theory, is a process that attempts to predict the subsequent
    state based on the properties of the current state. Modern-day applications include
    speech and handwriting recognition, computer performance evaluation, spam filtering,
    and Google’s PageRank algorithm for searching the web.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以有意义的方式重新排列这些词语，你将使用*马尔可夫链*，它以俄罗斯数学家Andrey Markov的名字命名。*马尔可夫链分析*是概率论中的一个重要部分，它是一个尝试根据当前状态的属性预测后续状态的过程。现代应用包括语音和手写识别、计算机性能评估、垃圾邮件过滤，以及Google的PageRank算法来搜索网络。
- en: With Markov chain analysis, a training corpus, and the syllable-counting program
    from [Chapter 8](ch08.xhtml#ch08), you’ll be able to produce new haiku that follow
    the syllabic rules of the genre and stay “on subject” to a large degree. You’ll
    also learn how to use Python’s logging module to help monitor the behavior of
    your program with easy on-and-off feedback. And in “[Challenge Projects](ch09.xhtml#lev215)”
    on [page 184](ch09.xhtml#page_184), you can enlist your friends on social media
    to see if they can distinguish your simulated haiku from the real thing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过马尔可夫链分析、训练语料库和来自[第8章](ch08.xhtml#ch08)的音节计数程序，你将能够生成符合音节规则并且在很大程度上“紧扣主题”的新俳句。你还将学习如何使用Python的日志模块，通过简单的开关反馈来帮助监控程序的行为。在[第184页](ch09.xhtml#page_184)的“[挑战项目](ch09.xhtml#lev215)”中，你可以邀请社交媒体上的朋友，看看他们能否区分你模拟的俳句和真实的俳句。
- en: '**Project #16: Markov Chain Analysis**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #16: 马尔可夫链分析**'
- en: Like the genetic algorithms in [Chapter 7](ch07.xhtml#ch07), Markov chain analysis
    sounds impressive but is easy to implement. You do it every day. If you hear some
    one say, “Elementary, my dear . . . ,” you automatically think, “Watson.” Every
    time your brain has heard this phrase, it has taken a sample. Based on the number
    of samples, it can predict the answer. On the other hand, if you heard someone
    say, “I want to go to . . . ,” you might think “the bathroom” or “the movies”
    but probably not “Houma, Louisiana.” There are many possible solutions, but some
    are more likely than others.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 像第[7章](ch07.xhtml#ch07)中的遗传算法一样，马尔可夫链分析听起来很有影响力，但其实很容易实现。你每天都在做这件事。如果你听到有人说：“Elementary,
    my dear . . .”，你会自动想：“Watson。”每次你的大脑听到这个短语，它都会进行采样。基于采样的数量，它可以预测出答案。另一方面，如果你听到有人说：“I
    want to go to . . .”，你可能会想“厕所”或“电影院”，但很可能不会想到“Louisiana的Houma”。虽然有很多可能的答案，但有些答案比其他的更可能。
- en: Back in the 1940s, Claude Shannon pioneered the use of Markov chains to statistically
    model the sequences of letters in a body of text. For example, for every occurrence
    of the digram *th* in an English-language book, the next most likely letter is
    *e*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪40年代，Claude Shannon开创了使用马尔可夫链来统计建模文本中的字母序列。例如，在一本英文书籍中，每次出现二元组*th*时，接下来的字母最可能是*e*。
- en: But you don’t just want to know what the most likely letter is; you want to
    know the actual probability of getting that letter, as well as the odds of getting
    every other letter, which is a problem tailor-made for a computer. To solve this
    problem, you need to map each two-letter digram in a piece of text to the letter
    that immediately follows it. This is a classic dictionary application, with the
    digrams as the keys and the letters as the values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但你不仅想知道最可能的字母是什么；你还想知道得到该字母的实际概率，以及得到每个其他字母的概率，这是一个非常适合计算机解决的问题。为了解决这个问题，你需要将文本中的每个二字母对映射到紧随其后的字母。这是一个经典的字典应用，二字母对是键，字母是值。
- en: 'When applied to letters in words, a *Markov model* is a mathematical model
    that calculates a letter’s probability of occurrence based on the previous *k*
    consecutive letters, where *k* is an integer. A *model of order 2* means that
    the probability of a letter occurring depends on the two letters that precede
    it. A *model of order 0* means that each letter is independent. And this same
    logic applies to words. Consider these two haiku examples:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用到单词中的字母时，*马尔科夫模型*是一个数学模型，通过计算一个字母出现的概率，基于前一个*k*连续的字母，其中*k*是一个整数。*二阶模型*意味着字母出现的概率取决于前面两个字母。*零阶模型*意味着每个字母是独立的。这同样的逻辑适用于单词。考虑以下这两个俳句的例子：
- en: '| A break in the clouds | Glorious the moon |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 在云中休息 | 光辉的月亮 |'
- en: '| The moon a bright mountaintop | Therefore our thanks dark clouds come |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 明亮的月亮山顶 | 因此我们的感谢黑暗的云朵来 |'
- en: '| Distant and aloof | To rest our tired necks |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 遥远而孤高 | 休息我们疲惫的脖子 |'
- en: 'A Python dictionary that maps each haiku word to each subsequent word looks
    like this:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将每个俳句单词映射到其后续单词的Python字典如下所示：
- en: '''a'': [''break'', ''bright''],'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '''a'': [''休息'', ''明亮''],'
- en: '''aloof'': [''glorious''],'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '''aloof'': [''光辉的''],'
- en: '''and'': [''aloof''],'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '''and'': [''孤高的''],'
- en: '''break'': [''in''],'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '''break'': [''在''],'
- en: '''bright'': [''mountaintop''],'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '''bright'': [''山顶''],'
- en: '''clouds'': [''the'', ''come''],'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '''clouds'': [''那个'', ''来''],'
- en: '''come'': [''to''],'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '''come'': [''到''],'
- en: '''dark'': [''clouds''],'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '''dark'': [''云''],'
- en: '''distant'': [''and''],'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '''distant'': [''和''],'
- en: '''glorious'': [''the''],'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '''glorious'': [''那个''],'
- en: '''in'': [''the''],'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '''in'': [''那个''],'
- en: '''moon'': [''a'', ''therefore''],'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '''moon'': [''一个'', ''因此''],'
- en: '''mountaintop'': [''distant''],'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '''mountaintop'': [''遥远的''],'
- en: '''our'': [''thanks'', ''tired''],'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '''our'': [''感谢'', ''疲惫''],'
- en: '''rest'': [''our''],'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '''rest'': [''我们的''],'
- en: '''thanks'': [''dark''],'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '''thanks'': [''黑暗''],'
- en: '''the'': [''clouds'', ''moon'', ''moon''],'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '''the'': [''云'', ''月亮'', ''月亮''],'
- en: '''therefore'': [''our''],'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '''therefore'': [''我们的''],'
- en: '''tired'': [''necks''],'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '''tired'': [''脖子''],'
- en: '''to'': [''rest'']'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '''to'': [''休息'']'
- en: 'Since there are only two haiku, most of the dictionary keys have only one value.
    But look at *the* near the bottom of the list: *moon* occurs twice. This is because
    the Markov model stores every occurrence of a word as a separate, duplicate value.
    So, for the key *the*, if you choose a value at random, the odds of selecting
    *moon* versus *clouds* are 2:1\. Conversely, the model will automatically screen
    out extremely rare or impossible combinations. For example, many words can potentially
    follow *the*, but not another *the*!'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只有两个俳句，大多数词典的键只有一个值。但请看列表底部的*the*，*moon*出现了两次。这是因为马尔科夫模型将每个单词的出现视为单独的重复值。因此，对于键*the*，如果随机选择一个值，选择*moon*与*clouds*的概率是2:1。相反，模型会自动筛选出极为罕见或不可能的组合。例如，许多单词可以跟在*the*后面，但不可能再跟另一个*the*！
- en: The following dictionary maps every *pair* of words to the word immediately
    after; that means it’s a model of order 2.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下字典将每个*词对*映射到紧随其后的单词；这意味着它是一个二阶模型。
- en: '''a break'': [''in''],'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '''a break'': [''在''],'
- en: '''a bright'': [''mountaintop''],'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '''a bright'': [''山顶''],'
- en: '''aloof glorious'': [''the''],'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '''aloof glorious'': [''那个''],'
- en: '''and aloof'': [''glorious''],'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '''and aloof'': [''光辉的''],'
- en: '''break in'': [''the''],'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '''break in'': [''那个''],'
- en: '''bright mountaintop'': [''distant''],'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '''bright mountaintop'': [''遥远的''],'
- en: '''clouds come'': [''to''],'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '''clouds come'': [''到''],'
- en: '''clouds the'': [''moon''],'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '''clouds the'': [''月亮''],'
- en: '''come to'': [''rest''],'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '''come to'': [''休息''],'
- en: '''dark clouds'': [''come''],'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '''dark clouds'': [''来''],'
- en: '''distant and'': [''aloof''],'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '''distant and'': [''孤高的''],'
- en: '''glorious the'': [''moon''],'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '''glorious the'': [''月亮''],'
- en: '''in the'': [''clouds''],'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '''in the'': [''云''],'
- en: '''moon a'': [''bright''],'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '''moon a'': [''明亮的''],'
- en: '''moon therefore'': [''our''],'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '''moon therefore'': [''我们的''],'
- en: '''mountaintop distant'': [''and''],'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '''mountaintop distant'': [''和''],'
- en: '''our thanks'': [''dark''],'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '''our thanks'': [''黑暗''],'
- en: '''our tired'': [''necks''],'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '''our tired'': [''脖子''],'
- en: '''rest our'': [''tired''],'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '''rest our'': [''疲惫''],'
- en: '''thanks dark'': [''clouds''],'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '''thanks dark'': [''云''],'
- en: '''the clouds'': [''the''],'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '''the clouds'': [''那个''],'
- en: '''the moon'': [''a'', ''therefore''],'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '''the moon'': [''一个'', ''因此''],'
- en: '''therefore our'': [''thanks''],'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '''therefore our'': [''感谢''],'
- en: '''to rest'': [''our'']'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '''to rest'': [''我们的'']'
- en: 'Note that the mapping continues from the first haiku to the second so the dictionary
    contains the items ''and aloof'': [''glorious''] and ''aloof glorious'': [''the''].
    This behavior means your program can jump from one haiku to another and is not
    restricted to just the word pairs within a single haiku. It is free to form new
    word pairs that the masters may never have conceived.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，映射从第一个俳句延续到第二个，因此字典包含项目 ''and aloof'': [''glorious''] 和 ''aloof glorious'':
    [''the'']。这种行为意味着你的程序可以从一个俳句跳到另一个俳句，而不仅仅局限于单个俳句中的词对。它可以自由地形成新的词对，这些词对可能是大师们从未想到过的。'
- en: Because of the very short training corpus, *the moon* is the only word pair
    with multiple values. For all the others, you are “locked in” to a single outcome.
    In this example, the size of the training corpus greatly determines the number
    of values per key, but with a larger corpus, the value of *k* in the Markov model
    will have a larger influence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练语料非常短，*the moon* 是唯一一个有多个值的词对。对于其他所有词对，你都“被锁定”在一个唯一的结果上。在这个例子中，训练语料的大小极大地决定了每个键的值的数量，但如果语料库更大，马尔可夫模型中
    *k* 的值将会有更大的影响。
- en: The size of *k* determines whether you produce poppycock, commit plagiarism,
    or produce a perspicuous piece of originality. If *k* equals 0, then you’ll be
    choosing words at random based on that word’s overall frequency in the corpus,
    and you’ll likely produce a lot of gibberish. If *k* is large, the results will
    be tightly constrained, and you’ll begin to reproduce the training text verbatim.
    So small values of *k* promote creativity, and large values promote duplication.
    The challenge is finding the proper balance between the two.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 的大小决定了你生成的内容是胡说八道、剽窃，还是一篇清晰的原创作品。如果 *k* 等于 0，那么你将根据该词在语料库中的整体频率随机选择词语，结果很可能是大量的无意义话语。如果
    *k* 很大，结果会受到严格的限制，你将开始逐字复制训练文本。因此，小值的 *k* 促进创造力，而大值的 *k* 则促进重复。挑战在于找到两者之间的适当平衡。'
- en: 'To illustrate, if you use a Markov model of order 3 on the previous haiku,
    all the resulting keys will have one value. The two values associated with the
    word pair *the moon* are lost because the former word pair becomes two keys, each
    with a unique value:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，如果你使用一个三阶的马尔可夫模型来生成前面的俳句，所有结果键都会只有一个值。与词对 *the moon* 相关的两个值将丢失，因为原先的词对变成了两个键，每个键有一个唯一的值：
- en: '''the moon a'': [''bright''],'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '''the moon a'': [''bright''],'
- en: '''the moon therefore'': [''our'']'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '''the moon therefore'': [''our'']'
- en: Since haiku are short—only 17 syllables long—and available training corpora
    are relatively small, using a *k* of 2 should be sufficient to enforce *some*
    order while still allowing for creative word substitutions in your program.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于俳句很短——只有 17 个音节——且现有的训练语料库相对较小，使用 *k* 为 2 应该足以在保证*某种*顺序的同时，仍然允许程序中的创造性词汇替换。
- en: '**THE OBJECTIVE**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**'
- en: Write a program that generates haiku using Markov chain analysis. Allow the
    user to modify the haiku by independently regenerating lines two and three.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个程序，通过马尔可夫链分析生成俳句。允许用户通过独立重新生成第二行和第三行来修改俳句。
- en: '**The Strategy**'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**策略**'
- en: Your general strategy for simulating haiku will be to build Markov models of
    orders 1 and 2 with a training corpus of haiku written by humans. You’ll then
    use those models and the *count_syllables.py* program from [Chapter 8](ch08.xhtml#ch08)
    to generate novel haiku that meet the required syllabic structure of 5-7-5 for
    the three lines of the haiku.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你模拟俳句的总体策略是，基于人类写作的俳句训练语料库构建一阶和二阶的马尔可夫模型。然后，使用这些模型和 [第8章](ch08.xhtml#ch08)中的
    *count_syllables.py* 程序生成符合要求音节结构（5-7-5）的新俳句。
- en: The program should build the haiku one word at a time, initiating (or *seeding*)
    the haiku with a random word drawn from the corpus; selecting the haiku’s second
    word using a Markov model of order 1; and then selecting each subsequent word
    with the order 2 model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 程序应该一次生成一个词来构建俳句，从语料库中随机抽取一个词来启动（或*播种*）俳句；使用一个一阶的马尔可夫模型选择俳句的第二个词；然后使用二阶模型选择每个后续的词。
- en: Each word is derived from a *prefix*—a word or word pair that determines which
    word will be picked to go in the haiku; the key in the word-mapping dictionaries
    represents the prefix. As a consequence, the word that the prefix determines is
    the *suffix*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词都来自一个 *前缀*——一个决定将选择哪个词加入俳句的词或词对；在词映射字典中，键代表前缀。因此，前缀决定的词就是 *后缀*。
- en: '***Choosing and Discarding Words***'
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***选择和丢弃词汇***'
- en: When the program selects a word, it first counts the word’s syllables, and if
    the word doesn’t fit, it chooses a new word. If there are no possible words based
    on the prefix in the poem, then the program resorts to what I call a *ghost prefix*,
    which is a prefix that doesn’t occur in the haiku. For example, if a word pair
    in a haiku is *temple gong*, and all the words that follow in the Markov model
    have too many syllables to complete the line, the program selects a new word pair
    at random and uses it to pick the next word in the haiku. The new word-pair prefix
    *should not be included in the line*—that is, *temple gong* will not be replaced.
    Although you could choose a suitable new word in a number of ways, I prefer this
    technique because it allows you to simplify by maintaining a consistent process
    throughout the program.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序选择一个单词时，它首先计算该单词的音节数，如果该单词不合适，它会选择一个新单词。如果根据诗句中的前缀没有可能的单词，程序会采用我称之为*幽灵前缀*的策略，这是一个在俳句中并不存在的前缀。例如，如果俳句中的一个词对是*temple
    gong*，而在马尔可夫模型中所有后续单词的音节数都太多，无法完成这一行，程序会随机选择一个新词对，并利用它来选择下一个俳句中的单词。新的词对前缀*不应该包含在这一行中*——也就是说，*temple
    gong*不会被替换。虽然你可以通过多种方式选择一个合适的新单词，但我更喜欢这种技术，因为它允许你通过保持整个程序的一致过程来简化操作。
- en: You can accomplish these steps with the functions in [Figures 9-1](ch09.xhtml#ch09fig1)
    and [9-2](ch09.xhtml#ch09fig2). Assuming you’re working on a five-syllable line,
    [Figure 9-1](ch09.xhtml#ch09fig1) is an example of what will happen, at a high
    level, if all the chosen words match the syllable target.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过[图 9-1](ch09.xhtml#ch09fig1)和[9-2](ch09.xhtml#ch09fig2)中的函数来完成这些步骤。假设你正在处理一个五音节的诗句，[图
    9-1](ch09.xhtml#ch09fig1)展示了如果所有选择的单词都符合音节目标，程序在高级别上会发生的情况。
- en: '![image](../images/f0165-01.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0165-01.jpg)'
- en: '*Figure 9-1: High-level graphical pseudocode for a five-syllable haiku line*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-1：五音节俳句行的高级图形伪代码*'
- en: The program randomly selects the seed word *the* from the corpus, and then counts
    its syllables. Next, it chooses *bright* from the model of order 1, based on the
    prefix *the*. Then it counts the number of syllables in *bright* and adds that
    number to the number of syllables in the line. Since the sum of syllables doesn’t
    exceed five, the program adds *bright* to the line, moves on to select *autumn*
    from the model of order 2 based on the prefix *The bright*, and then repeats the
    syllable-counting process. Finally, the program selects *moon* based on the prefix
    *bright autumn*, counts the syllables, and—since the line’s total number of syllables
    is equal to five—adds moon to the line, completing it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 程序从语料库中随机选择种子词*the*，然后计算它的音节数。接下来，根据前缀*the*，它从一级模型中选择*bright*。然后它计算*bright*的音节数，并将该数字加到行中的音节总数中。由于音节总数不超过五，程序将*bright*加入到行中，继续根据前缀*The
    bright*从二级模型中选择*autumn*，然后重复音节计数过程。最后，程序根据前缀*bright autumn*选择*moon*，计算音节数，并且——由于该行的音节总数恰好为五——将*moon*加入行中，完成该行。
- en: '[Figure 9-2](ch09.xhtml#ch09fig2) demonstrates a case where the program needs
    to utilize a ghost prefix to successfully complete a five-syllable line.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2](ch09.xhtml#ch09fig2)展示了程序需要利用幽灵前缀来成功完成一个五音节诗句的情况。'
- en: '![image](../images/f0166-01.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0166-01.jpg)'
- en: '*Figure 9-2: Choosing a new suffix with a randomly selected ghost prefix (*full
    white*)*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-2：使用随机选择的幽灵前缀（*full white*）选择新后缀*'
- en: Let’s assume that the only word that follows the prefix *temple gong* in the
    Markov model is *glorious*. This word has too many syllables for the line, so
    the program selects a ghost prefix, *full white*, at random. The word *moon* follows
    the ghost prefix and satisfies the remaining syllable count in the line, so the
    program adds it to the line. The program then discards the *full white* prefix,
    and the line is complete. With this ghost prefix technique, you can’t guarantee
    that the new suffix will make sense contextually, but at the same time, this is
    one way to incorporate creativity into the process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在马尔可夫模型中，紧跟前缀*temple gong*的唯一单词是*glorious*。这个词的音节数太多，无法满足该行的要求，于是程序随机选择了一个幽灵前缀*full
    white*。单词*moon*跟随幽灵前缀，并满足了这一行的剩余音节数，因此程序将其添加到这一行中。程序随后丢弃了*full white*前缀，这一行完成了。使用这种幽灵前缀技术，你不能保证新的后缀在语境上完全合适，但同时，这也是将创造性融入过程的一种方式。
- en: '***Continuing from One Line to Another***'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***从一行继续到另一行***'
- en: 'The Markov model is the “special sauce” that allows you to imbue the haiku
    with context and meaning that continue from one line to another. The Japanese
    masters *generally* wrote haiku in which each line is a stand-alone phrase but
    the contextual thread continues across lines, as in this haiku from Bon Cho:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科夫模型是“特别酱料”，它使你能够赋予俳句从一行到另一行的上下文和意义。日本的大师们*通常*会写俳句，每一行都是独立的短语，但上下文的脉络会跨越行延续，就像这首来自
    Bon Cho 的俳句：
- en: In silent midnight
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在寂静的午夜
- en: Our old scarecrow topples down
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的老稻草人倒下了
- en: Weird hollow echo
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的空洞回声
- en: '*—Bon Cho*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*—Bon Cho*'
- en: 'Even though the masters preferred that each line of a haiku represent a complete
    thought, they didn’t strictly follow the rule. Here’s an example in Buson’s haiku:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大师们更喜欢每行俳句代表一个完整的思想，但他们并不严格遵循这一规则。以下是 Buson 俳句中的一个例子：
- en: My two plum trees are
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我的两棵梅树是
- en: So gracious see, they flower
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如此优雅，看看，它们开花了
- en: One now, one later
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现在，一个稍后
- en: '*—Buson*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*—Buson*'
- en: The first line of Buson’s haiku is not grammatical on its own, so the reader
    must continue to the next line without a break. When a phrase in poetry moves
    from one line to the next without a pause or syntactic break, it is said to be
    *enjambed*. According to Charles Hartman, author of *Virtual Muse*, enjambment
    is what gives metrical lines much of their supple liveliness. That’s a good thing,
    since it’s very hard to get an algorithm to write a coherent poem without some
    grammatical spillover from line to line. To get your program to continue a “thought”
    through multiple lines, you need to use the word pair from the end of the previous
    line as the starting prefix for the current line.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Buson 俳句的第一行本身并不符合语法，因此读者必须继续到下一行，而没有停顿。当诗句中的一句话从一行移到下一行时没有停顿或语法中断时，这种现象称为*
    enjambment*（延续式）。根据《虚拟缪斯》一书的作者 Charles Hartman 的说法，enjambment 是赋予韵律线条柔韧生动的原因之一。这是件好事，因为很难让算法写出一个连贯的诗句，而不借用上一行的语法内容。为了让你的程序通过多行继续一个“思想”，你需要使用上一行结束时的单词对作为当前行的起始前缀。
- en: Finally, you should give the user the opportunity to not only build the poem
    but also to edit it interactively by regenerating lines two and three. Most of
    writing is rewriting, and it would be unconscionable to leave the user hanging
    with two perfect lines and no way to throw the dice again on an uncooperative
    line.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该给用户一个机会，不仅可以构建诗句，还能通过重新生成第二行和第三行来交互式编辑它。写作的许多部分都是重写，如果用户有两行完美的诗句却没有办法对不合作的行重新投掷骰子，那将是不可理喻的。
- en: '**The Pseudocode**'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**伪代码**'
- en: 'If you follow the strategy I’ve just laid out, your high-level pseudocode should
    look like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照我刚才概述的策略进行操作，你的高级伪代码应该如下所示：
- en: Import count_syllables module
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 count_syllables 模块
- en: Load a training-corpus text file
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个训练语料库文本文件
- en: Process the training corpus for spaces, newline breaks, and so on
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 处理训练语料库中的空格、换行符等
- en: Map each word in corpus to the word after (Markov model order 1)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将语料库中的每个单词映射到后面的单词（马尔科夫模型顺序1）
- en: Map each word pair in corpus to the word after (Markov model order 2)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将语料库中的每一对单词映射到后面的单词（马尔科夫模型顺序2）
- en: Give user choice of generating full haiku, redoing lines 2 or 3, or exiting
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给用户选择生成完整俳句、重新做第2行或第3行，或退出的选项
- en: 'If first line:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是第一行：
- en: Target syllables = 5
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 目标音节数 = 5
- en: Get random word from corpus <= 4 syllables (no 1-word lines)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从语料库中获取一个随机单词，音节数 ≤ 4（没有单词行）
- en: Add word to line
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词添加到行中
- en: Set random word = prefix variable
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设置随机单词 = 前缀变量
- en: Get mapped words after prefix
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 获取前缀后的映射单词
- en: If mapped words have too many syllables
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果映射的单词音节数过多
- en: Choose new prefix word at random & repeat
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择新的前缀词并重复
- en: Choose new word at random from mapped words
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从映射单词中随机选择新单词
- en: Add the new word to the line
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将新单词添加到行中
- en: Count syllables in word and calculate total in line
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单词中的音节并计算行中的总音节数
- en: If syllables in line equal target syllables
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果行中的音节数等于目标音节数
- en: Return line and last word pair in line
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 返回行和行中的最后一对单词
- en: 'Else if second or third line:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 否则如果是第二行或第三行：
- en: Target = 7 or 5
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目标 = 7 或 5
- en: Line equals last word pair in previous line
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 行等于上一行中的最后一对单词
- en: 'While syllable target not reached:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在音节目标未达到时：
- en: Prefix = last word pair in line
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀 = 行中的最后一对单词
- en: Get mapped words after word-pair prefix
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 获取单词对前缀后的映射单词
- en: If mapped words have too many syllables
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果映射的单词音节数过多
- en: Choose new word-pair prefix at random and repeat
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择新的单词对前缀并重复
- en: Choose new word at random from mapped words
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从映射单词中随机选择新单词
- en: Add the new word to the line
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将新单词添加到行中
- en: Count syllables in word and calculate total in line
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单词中的音节并计算行中的总音节数
- en: If total is greater than target
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果总和大于目标
- en: Discard word, reset total, and repeat
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃词语，重置总和，重复
- en: Else if total is less than target
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果总和小于目标
- en: Add word to line, keep total, and repeat
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将词语添加到行中，保持总和，并重复
- en: Else if total is equal to target
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果总和等于目标
- en: Add word to line
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将词语添加到行中
- en: Return line and last word pair in line
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 返回行和行中的最后一个词对
- en: Display results and choice menu
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 显示结果和选择菜单
- en: '**The Training Corpus**'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**训练语料库**'
- en: Markov models are built from a corpus, so they are unique to that corpus. A
    model built from the complete works of Edgar Rice Burroughs will be different
    and distinguishable from one built from the works of Anne Rice. We all have a
    signature style, or *voice*, and given a large enough sample, a Markov approach
    can produce a statistical model of your style. Like fingerprints, this model can
    link you to a document or manuscript.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫模型是从语料库构建的，因此它们是该语料库特有的。从埃德加·赖斯·巴勒斯的完整作品构建的模型将与从安妮·赖斯的作品构建的模型不同。我们每个人都有独特的风格或*声音*，只要样本足够大，马尔可夫方法就能生成你风格的统计模型。就像指纹一样，这个模型可以将你与某个文档或手稿联系起来。
- en: To build the Markov models, the corpus you’ll use is a text file consisting
    of almost 300 ancient and modern haiku, more than 200 of which were written by
    the masters. Ideally, your training corpus would consist of thousands of haiku,
    all by the same author (for a consistent voice), but these are difficult to find,
    particularly since many of the old Japanese haiku don’t obey the syllabic rules,
    either intentionally or as a result of translation into English.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建马尔可夫模型，你将使用的语料库是一个包含近300首古今俳句的文本文件，其中超过200首是由大师们写的。理想情况下，你的训练语料库应该包含成千上万的俳句，且全部出自同一作者（以保持一致的声音），但这些很难找到，尤其是许多古老的日本俳句并不遵循音节规则，可能是故意为之，或者是翻译成英文时出现了偏差。
- en: To increase the number of values per key in the Markov model, the haiku in the
    initial corpus were duplicated 18 times and randomly distributed throughout the
    file. This has no impact on word associations *within* haiku but increases the
    interactions *between* haiku.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加马尔可夫模型中每个关键字的值的数量，初始语料库中的俳句被复制了18次，并随机分布在文件中。这不会影响俳句内部的词汇关联，但会增加俳句之间的互动。
- en: 'To illustrate, assume the word pair at the end of the following haiku is unique,
    mapping only to the starting word of the second haiku; this results in the fairly
    useless key/value pair of ''hollow frog'': [''mirror-pond'']:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，假设以下俳句结尾的词对是唯一的，只映射到第二个俳句的起始词；这将导致一个几乎无用的键值对“空心青蛙”：['镜面池塘']：
- en: Open mouth reveals
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 张开嘴巴，露出
- en: Your whole wet interior
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你整个湿漉漉的内部
- en: Silly **hollow frog**!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 傻乎乎的**空心青蛙**！
- en: '**Mirror-pond** of stars'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**镜面池塘**的星星'
- en: Suddenly a summer shower
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 突然的一场夏雨
- en: Dimples the water
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 水面上的涟漪
- en: 'If you duplicate and shuffle the haiku, you may introduce a preposition into
    the mix, greatly increasing the odds of linking the odd *hollow frog* to something
    sensible:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你复制并打乱俳句，可能会引入一个介词，从而大大增加将“空心青蛙”与某些合理内容关联的概率：
- en: Open mouth reveals
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 张开嘴巴，露出
- en: Your whole wet interior
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你整个湿漉漉的内部
- en: Silly **hollow frog**!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 傻乎乎的**空心青蛙**！
- en: '**In** the city fields'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**在**城市的田野里'
- en: Contemplating cherry trees
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 沉思樱花树
- en: Strangers are like friends
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 陌生人像朋友
- en: 'The Markov model now assigns two values to ''hollow frog'': ''mirror-pond''
    and ''in''. And each time you duplicate the haiku, you’ll see an increase in the
    number of values per key for haiku-ending words or word pairs. But this is helpful
    only to a point; after a while, diminishing returns set in, and you start adding
    the same values over and over again, gaining nothing.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫模型现在将两个值分配给“空心青蛙”：“镜面池塘”和“在”。每次你复制俳句时，俳句结尾的词或词对的每个关键字的值数量都会增加。但这只在某个点上有帮助；一段时间后，收益递减，你开始反复添加相同的值，毫无意义。
- en: '**Debugging**'
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**调试**'
- en: Debugging is the process of finding and fixing errors (bugs) in computer hardware
    and software. When you’re trying to code a solution to a complex problem, you
    need to keep a tight rein on your program in order to find the source of an issue
    when something unexpected arises. For example, if you end up with seven syllables
    rather than five in the first line of your haiku, you’ll want to know if the syllable-counting
    function failed, if there was a problem mapping words to words, or if the program
    thought it was on line two. To find out what went wrong, you need to monitor what
    your program is returning at each key step, and this calls for either *scaffolding*
    or *logging*. I’ll discuss each of these techniques in the following two sections.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 调试是寻找和修复计算机硬件和软件中的错误（bug）的过程。当你试图为一个复杂的问题编写解决方案时，你需要紧密控制程序，以便在出现意外情况时找到问题的根源。例如，如果你在俳句的第一行中得到了七个音节，而不是五个，你会想知道是音节计数功能失败了，还是在将单词映射到单词时出现了问题，或者程序认为它在第二行。为了找出哪里出错了，你需要监控程序在每个关键步骤中返回的内容，这就需要使用*脚手架*或*日志记录*。我将在接下来的两个部分中讨论这两种技术。
- en: '***Building the Scaffolding***'
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***构建脚手架***'
- en: '*Scaffolding*, as defined here, is temporary code you write to help develop
    your programs, then delete when you’re done. The name is an allusion to the scaffolding
    used in construction—necessary, but nobody wants it around forever.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*脚手架*，在这里定义，是你编写的临时代码，用于帮助开发程序，然后在完成后删除。这个名称暗指建筑中使用的脚手架——必要的，但没有人希望它永远存在。'
- en: One common piece of scaffolding is a print() statement that checks what a function
    or calculation returns. The user doesn’t need to see the output, so you delete
    it after you confirm the program is working properly.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的脚手架是一个 print() 语句，用来检查一个函数或计算返回了什么。用户不需要看到输出，因此在确认程序正常工作后，你可以删除它。
- en: Useful scaffolding output includes things like the type of a value or variable,
    the length of a dataset, and the results of incremental calculations. To quote
    Allen Downey in *Think Python*, “Time you spend building scaffolding can reduce
    the time you spend debugging.”
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的脚手架输出包括诸如值或变量的类型、数据集的长度以及增量计算的结果等内容。正如 Allen Downey 在《*Think Python*》中所说：“构建脚手架所花费的时间可以减少你调试所花费的时间。”
- en: The downside to using print() statements for debugging is that you have to go
    back and delete (or comment out) all these statements later, and you risk accidentally
    removing a print() statement that’s useful to the end user. Fortunately, there’s
    an alternative to scaffolding that lets you avoid these problems. It’s called
    the logging module.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 print() 语句进行调试的缺点是你需要在稍后回去删除（或注释掉）所有这些语句，而且你有可能不小心删除了对最终用户有用的 print() 语句。幸运的是，有一种替代脚手架的方法，可以让你避免这些问题。它被称为日志记录模块。
- en: '***Using the logging Module***'
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用日志记录模块***'
- en: 'The logging module is part of the Python Standard Library (*[https://docs.python.org/3/library/logging.html](https://docs.python.org/3/library/logging.html)*).
    With logging, you can get a customized report on what your program is doing at
    any location you choose. You can even write the reports to a permanent logfile.
    The following interactive shell example uses logging to check that a vowel-counting
    program is working correctly:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录模块是 Python 标准库的一部分 (*[https://docs.python.org/3/library/logging.html](https://docs.python.org/3/library/logging.html)*)。通过日志记录，你可以获得关于程序在任何你选择的位置正在做什么的定制报告。你甚至可以将报告写入永久的日志文件。以下交互式命令行示例使用日志记录检查一个音节计数程序是否正常工作：
- en: ➊ >>> import logging
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ >>> import logging
- en: ➋ >>> logging.basicConfig(level=logging.DEBUG,
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ >>> logging.basicConfig(level=logging.DEBUG,
- en: format='%(levelname)s - %(message)s')
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: format='%(levelname)s - %(message)s')
- en: '>>> word = ''scarecrow'''
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> word = ''scarecrow'''
- en: '>>> VOWELS = ''aeiouy'''
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> VOWELS = ''aeiouy'''
- en: '>>> num_vowels = 0'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> num_vowels = 0'
- en: '>>> for letter in word:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for letter in word:'
- en: 'if letter in VOWELS:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'if letter in VOWELS:'
- en: num_vowels += 1
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: num_vowels += 1
- en: ➌ logging.debug('letter & count = %s-%s', letter, num_vowels)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ logging.debug('字母 & 计数 = %s-%s', letter, num_vowels)
- en: DEBUG - letter & count = s-0
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = s-0
- en: DEBUG - letter & count = c-0
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = c-0
- en: DEBUG - letter & count = a-1
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = a-1
- en: DEBUG - letter & count = r-1
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = r-1
- en: DEBUG - letter & count = e-2
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = e-2
- en: DEBUG - letter & count = c-2
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = c-2
- en: DEBUG - letter & count = r-2
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = r-2
- en: DEBUG - letter & count = o-3
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = o-3
- en: DEBUG - letter & count = w-3
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DEBUG - 字母 & 计数 = w-3
- en: To use the logging module, first import it ➊. Then set up what debugging information
    you want to see and in what format ➋. The DEBUG level is the lowest level of information
    and is used for diagnosing the details. Note that the output uses string formatting
    with %s. You can include more information—for example, the date and time are shown
    using format='%(asctime)s'—but for this snippet of code, all you really need to
    check is whether the program is counting vowels correctly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 logging 模块，首先导入它 ➊。然后设置你想要查看的调试信息及其格式 ➋。DEBUG 级别是最低的消息级别，用于诊断详细信息。请注意，输出使用字符串格式化
    %s。你可以包括更多的信息，例如，日期和时间通过 format='%(asctime)s' 显示——但对于这段代码，你只需要检查程序是否正确计数元音即可。
- en: For each letter evaluated, enter the custom text message to display along with
    the variable values. Note that you have to convert nonstring objects, such as
    integers and lists, to strings ➌. The logging output follows. You can see the
    cumulative count, along with which letters actually change the count.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个评估的字母，输入自定义的文本消息以显示变量值。请注意，必须将非字符串对象（如整数和列表）转换为字符串 ➌。以下是 logging 输出。你可以看到累计计数，以及哪些字母实际上改变了计数。
- en: 'Like scaffolding, logging is for the developer, not the user. And like the
    print() function, logging can slow down your program. To disable the logging messages,
    simply insert the logging.disable(logging.CRITICAL) call after you import the
    module, as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就像脚手架一样，logging 是为开发者准备的，而不是用户。就像 print() 函数一样，logging 也会减慢程序的运行速度。要禁用日志消息，只需在导入模块后插入
    logging.disable(logging.CRITICAL) 调用，如下所示：
- en: '>>> import logging'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import logging'
- en: '>>> logging.disable(logging.CRITICAL)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> logging.disable(logging.CRITICAL)'
- en: Placing the disable call near the top of the program will allow you to find
    it easily and toggle messages on and off. The logging.disable() function will
    suppress all messages at the designated level or lower. Since CRITICAL is the
    highest level, passing it to the logging.disable() function turns all messages
    off. This is a far better solution than manually finding and commenting out print()
    statements!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将禁用调用放在程序顶部附近，可以轻松找到它，并切换消息的开关。logging.disable() 函数会抑制指定级别或更低级别的所有消息。由于 CRITICAL
    是最高级别，传递它给 logging.disable() 函数会关闭所有消息。这比手动查找并注释掉 print() 语句要好得多！
- en: '**The Code**'
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**代码**'
- en: The *markov_haiku.py* code in this section will take a training corpus called
    *train.txt*, prepare Markov models as dictionaries, and generate a haiku one word
    at a time. The *count_syllables.py* program and *missing_words.json* file from
    [Chapter 8](ch08.xhtml#ch08) will ensure *markov_haiku.py* uses the correct number
    of syllables for each line. You can download all these files from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    ([Chapter 9](ch09.xhtml#ch09) folder). Be sure to keep them together in the same
    directory.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的 *markov_haiku.py* 代码将处理名为 *train.txt* 的训练语料库，准备作为字典的马尔可夫模型，并一字一字生成俳句。*count_syllables.py*
    程序和 *missing_words.json* 文件来自 [第 8 章](ch08.xhtml#ch08)，将确保 *markov_haiku.py* 为每行使用正确的音节数。你可以从
    *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    下载所有这些文件（[第 9 章](ch09.xhtml#ch09) 文件夹）。务必将它们保存在同一目录中。
- en: '***Setting Up***'
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***设置***'
- en: '[Listing 9-1](ch09.xhtml#ch09list1) imports the necessary modules, then loads
    and prepares external files.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9-1](ch09.xhtml#ch09list1) 导入所需模块，然后加载并准备外部文件。'
- en: '*markov_haiku.py,* part 1'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*markov_haiku.py*，第一部分'
- en: ➊ import sys
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ import sys
- en: import logging
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: import logging
- en: import random
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: from collections import defaultdict
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: from collections import defaultdict
- en: from count_syllables import count_syllables
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: from count_syllables import count_syllables
- en: ➋ logging.disable(logging.CRITICAL)  # comment out to enable debugging messages
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ logging.disable(logging.CRITICAL)  # 注释掉以启用调试消息'
- en: logging.basicConfig(level=logging.DEBUG, format='%(message)s')
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: logging.basicConfig(level=logging.DEBUG, format='%(message)s')
- en: '➌ def load_training_file(file):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ def load_training_file(file):'
- en: '"""Return text file as a string."""'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '"""将文本文件作为字符串返回。"""'
- en: 'with open(file) as f:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(file) as f:'
- en: ➍ raw_haiku = f.read()
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ raw_haiku = f.read()
- en: return raw_haiku
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: return raw_haiku
- en: '➎ def prep_training(raw_haiku):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '➎ def prep_training(raw_haiku):'
- en: '"""Load string, remove newline, split words on spaces, and return list."""'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '"""加载字符串，去除换行符，按空格分割单词并返回列表。"""'
- en: corpus = raw_haiku.replace('\n', ' ').split()
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: corpus = raw_haiku.replace('\n', ' ').split()
- en: return corpus
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: return corpus
- en: '*Listing 9-1: Imports, loads, and prepares the training corpus*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9-1：导入、加载并准备训练语料库*'
- en: Start with the imports listed on separate lines ➊. You’ll need logging in order
    to receive debugging messages, and defaultdict will help you build a dictionary
    from a list by creating a new key automatically, rather than throwing an error.
    You also import the count_syllables function from the *count_syllables.py* program
    you wrote in [Chapter 8](ch08.xhtml#ch08). You should be familiar with the rest
    of these imports.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从单独的行开始列出导入语句 ➊。你需要使用日志记录以接收调试消息，而 defaultdict 会帮助你通过自动创建新键来从列表构建字典，而不是抛出错误。你还需要导入从*count_syllables.py*
    程序中编写的 count_syllables 函数，后者在[第8章](ch08.xhtml#ch08)中有介绍。你应该对这些导入语句中的其他内容已经熟悉。
- en: Put the statement to disable logging right after the imports so you can find
    it easily. To see logging messages, you need to comment out this statement ➋.
    The following statement configures what you will see, as described in the previous
    section. I chose to omit the level designation from the display.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入语句之后立即放置禁用日志记录的语句，这样你可以轻松找到它。要查看日志信息，你需要注释掉此语句 ➋。以下语句配置了你将看到的内容，如前一部分所述。我选择从显示中省略了级别标识。
- en: Next, define a function to load the training-corpus text file ➌. Use the built-in
    read() function to read the data as a string that the program can prepare before
    converting it to a list ➍. Return this string for use in the next function.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个函数来加载训练语料库文本文件 ➌。使用内置的 read() 函数将数据作为字符串读取，程序在转换为列表 ➍ 之前会对其进行准备。返回该字符串以供下一个函数使用。
- en: The prep_training() function ➎ takes the output from the load_training_file()
    function as an argument. It then replaces newline characters with spaces and splits
    the words into list items based on spaces. Finally, the function returns the corpus
    as a list.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: prep_training() 函数 ➎ 将 load_training_file() 函数的输出作为参数。然后，它将换行符替换为空格，并基于空格将单词拆分为列表项。最后，函数返回语料库作为列表。
- en: '***Building Markov Models***'
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***构建马尔可夫模型***'
- en: The Markov models are simply Python dictionaries that use a word or word pair
    as a key and the word that immediately follows them as a value. The statistical
    frequency of trailing words is captured by repetition of the trailing word in
    the list of values—similar to sets, dictionaries can’t have duplicate *keys*,
    but they can have duplicate *values*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫模型实际上是 Python 字典，使用单词或单词对作为键，紧随其后的单词作为值。通过重复值列表中的尾随单词来捕捉尾随单词的统计频率——类似于集合，字典不能有重复的*键*，但可以有重复的*值*。
- en: '[Listing 9-2](ch09.xhtml#ch09list2) defines two functions. Both functions take
    the corpus as an argument and return a Markov model.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9-2](ch09.xhtml#ch09list2) 定义了两个函数。两个函数都以语料库作为参数，并返回一个马尔可夫模型。'
- en: '*markov_haiku.py,* part 2'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*markov_haiku.py,* 第二部分'
- en: '➊ def map_word_to_word(corpus):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def map_word_to_word(corpus):'
- en: '"""Load list & use dictionary to map word to word that follows."""'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '"""加载列表并使用字典将单词映射到其后继单词。"""'
- en: ➋ limit = len(corpus) - 1
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ limit = len(corpus) - 1
- en: ➌ dict1_to_1 = defaultdict(list)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ dict1_to_1 = defaultdict(list)
- en: '➍ for index, word in enumerate(corpus):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ for index, word in enumerate(corpus):'
- en: 'if index < limit:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 'if index < limit:'
- en: ➎ suffix = corpus[index + 1]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ suffix = corpus[index + 1]
- en: dict1_to_1[word].append(suffix)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: dict1_to_1[word].append(suffix)
- en: ➏ logging.debug("map_word_to_word results for \"sake\" = %s\n",
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ logging.debug("map_word_to_word 结果对于 \"sake\" = %s\n",
- en: dict1_to_1['sake'])
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: dict1_to_1['sake'])
- en: ➐ return dict1_to_1
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ return dict1_to_1
- en: '➑ def map_2_words_to_word(corpus):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '➑ def map_2_words_to_word(corpus):'
- en: '"""Load list & use dictionary to map word-pair to trailing word."""'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '"""加载列表并使用字典将单词对映射到尾随单词。"""'
- en: ➒ limit = len(corpus) - 2
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ limit = len(corpus) - 2
- en: dict2_to_1 = defaultdict(list)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: dict2_to_1 = defaultdict(list)
- en: 'for index, word in enumerate(corpus):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'for index, word in enumerate(corpus):'
- en: 'if index < limit:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'if index < limit:'
- en: ➓ key = word + ' ' + corpus[index + 1]
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ key = word + ' ' + corpus[index + 1]
- en: suffix = corpus[index + 2]
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: suffix = corpus[index + 2]
- en: dict2_to_1[key].append(suffix)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: dict2_to_1[key].append(suffix)
- en: logging.debug("map_2_words_to_word results for \"sake jug\" = %s\n",
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: logging.debug("map_2_words_to_word 结果对于 \"sake jug\" = %s\n",
- en: dict2_to_1['sake jug'])
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: dict2_to_1['sake jug'])
- en: return dict2_to_1
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: return dict2_to_1
- en: '*Listing 9-2: Defines functions that build Markov models of order 1 and 2*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9-2：定义构建1阶和2阶马尔可夫模型的函数*'
- en: First, define a function to map every single word to its trailing word ➊. The
    program will use this function only to select the haiku’s second word from the
    seed word. Its only parameter is the corpus list that the prep_training() function
    returns.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义一个函数将每个单词映射到它的尾随单词 ➊。程序将仅使用此函数从种子单词选择俳句的第二个单词。它的唯一参数是 prep_training() 函数返回的语料库列表。
- en: Set a limit so you can’t pick the last word in the corpus ➋, because doing so
    would result in an index error. Now initialize a dictionary using defaultdict
    ➌. You want the dictionary values to be lists that hold all the suffixes you find,
    so use list as the argument.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个限制，以防无法选择语料库中的最后一个单词 ➋，因为这样会导致索引错误。然后使用 `defaultdict` 初始化一个字典 ➌。你希望字典的值是一个列表，用来存储你找到的所有后续单词，所以使用
    `list` 作为参数。
- en: Start looping through every word in the corpus, using enumerate to turn each
    word’s index into an object ➍. Use a conditional and the limit variable to prevent
    choosing the last word as a key. Assign a variable named suffix that will represent
    the trailing word ➎. The value will be the index location of the current word
    plus 1—the next word in the list. Append this variable to the dictionary as a
    value of the current word.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历语料库中的每个单词，使用 `enumerate` 将每个单词的索引转换为一个对象 ➍。使用条件语句和限制变量来防止选择最后一个单词作为关键字。定义一个名为
    `suffix` 的变量来表示后续单词 ➎。该值将是当前单词的索引位置加 1——即列表中的下一个单词。将此变量作为当前单词的值添加到字典中。
- en: To check that everything is working as planned, use logging to show the results
    *for a single key* ➏. There are thousands of words in the corpus, so you’re not
    going to want to print them all. Choose a word that you know is in the corpus,
    like *sake*. Note that you are using the old string formatting with %, as it fits
    the current design of the logger. Finish by returning the dictionary ➐.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查一切是否按计划工作，使用 `logging` 来显示 *单个关键字* 的结果 ➏。语料库中有成千上万的单词，因此你不希望打印出所有的单词。选择一个你知道在语料库中的单词，比如
    *sake*。注意，你正在使用旧的字符串格式化方法 `%`，因为它适合当前日志记录器的设计。最后返回字典 ➐。
- en: The next function, map_2_words_to_word(), is basically the same function, except
    it uses two consecutive words as the key and maps to trailing single words ➑.
    Important changes are to set the limit two words back from the end of the corpus
    ➒, make the key consist of two words with a space between them ➓, and add 2 to
    the index for the suffix.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数 `map_2_words_to_word()` 基本上是相同的函数，只不过它使用两个连续的单词作为键，映射到后续的单个单词 ➑。主要的变化是将限制设置为离语料库末尾两个单词的位置
    ➒，使得键由两个单词组成，中间用空格隔开 ➓，并且为后续单词的索引加 2。
- en: '***Choosing a Random Word***'
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***选择一个随机单词***'
- en: The program won’t be able to utilize a Markov model without a key, so either
    the user or the program must supply the first word in a simulated haiku. [Listing
    9-3](ch09.xhtml#ch09list3) defines a function that picks a first word at random,
    facilitating automated seeding.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 程序无法在没有键的情况下利用马尔可夫模型，因此用户或程序必须提供模拟俳句中的第一个单词。[列表 9-3](ch09.xhtml#ch09list3) 定义了一个随机选择第一个单词的函数，便于自动化种子选择。
- en: '*markov_haiku.py,* part 3'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*markov_haiku.py*，第 3 部分'
- en: '➊ def random_word(corpus):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def random_word(corpus):'
- en: '"""Return random word and syllable count from training corpus."""'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '"""从训练语料库中返回随机单词和音节数。"""'
- en: ➋ word = random.choice(corpus)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ word = random.choice(corpus)
- en: ➌ num_syls = count_syllables(word)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ num_syls = count_syllables(word)
- en: '➍ if num_syls > 4:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ if num_syls > 4:'
- en: random_word(corpus)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: random_word(corpus)
- en: 'else:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: ➎ logging.debug("random word & syllables = %s %s\n", word, num_syls)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ logging.debug("random word & syllables = %s %s\n", word, num_syls)
- en: return (word, num_syls)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: return (word, num_syls)
- en: '*Listing 9-3: Randomly chooses a seed word to initiate the haiku*'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 9-3：随机选择一个种子单词来启动俳句*'
- en: Define the function and pass it the corpus list ➊. Then assign a word variable
    and use the random choice() method to pick a word from the corpus ➋.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 定义函数并传递语料库列表 ➊。然后定义一个 `word` 变量，并使用 `random.choice()` 方法从语料库中随机选择一个单词 ➋。
- en: Use the count_syllables() function from the count_syllables module to count
    the syllables in the word; store the count in the num_syls variable ➌. I’m not
    a fan of single-word lines in haiku, so don’t allow the function to choose a word
    with more than four syllables (recall that the shortest haiku lines have five
    syllables). If this occurs, call the random_word() function recursively until
    you get an acceptable word ➍. Note that Python has a default maximum recursion
    depth of 1,000, but as long as you’re using a proper haiku-training corpus, there’s
    little chance you’ll exceed that prior to finding a suitable word. If that were
    not the case, you could address this condition later by calling the function using
    a while loop.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: If the word has fewer than five syllables, use logging to display the word and
    its syllable count ➎; then return the word and syllable count as a tuple.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '***Applying the Markov Models***'
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To choose the single word that follows the seed word, use the Markov model of
    order 1\. After that, the program should select all subsequent words using the
    order 2 model, which uses word pairs as keys. [Listing 9-4](ch09.xhtml#ch09list4)
    defines a separate function for each of these actions.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '*markov_haiku.py,* part 4'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '➊ def word_after_single(prefix, suffix_map_1, current_syls, target_syls):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '"""Return all acceptable words in a corpus that follow a single word."""'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: ➋ accepted_words = []
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: ➌ suffixes = suffix_map_1.get(prefix)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '➍ if suffixes != None:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '➎ for candidate in suffixes:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: num_syls = count_syllables(candidate)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'if current_syls + num_syls <= target_syls:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: ➏ accepted_words.append(candidate)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: ➐ logging.debug("accepted words after \"%s\" = %s\n",
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: prefix, set(accepted_words))
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: return accepted_words
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '➑ def word_after_double(prefix, suffix_map_2, current_syls, target_syls):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '"""Return all acceptable words in a corpus that follow a word pair."""'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: accepted_words = []
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: ➒ suffixes = suffix_map_2.get(prefix)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'if suffixes != None:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'for candidate in suffixes:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: num_syls = count_syllables(candidate)
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'if current_syls + num_syls <= target_syls:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: accepted_words.append(candidate)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: logging.debug("accepted words after \"%s\" = %s\n",
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: prefix, set(accepted_words))
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: ➓ return accepted_words
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9-4: Two functions that select a word given a prefix, Markov model,
    and syllable count*'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Define a function named word_after_single() to select the next word in the haiku
    based on the preceding single seed word. For arguments, this function takes the
    preceding word, the Markov order 1 model, the current syllable count, and the
    target syllable count ➊.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Start an empty list to hold the acceptable words, which are the words that both
    follow the prefix and whose syllable count doesn’t exceed the syllable target
    ➋. Call these trailing words suffixes and use the dictionary get() method, which
    returns a dictionary value given a key, to assign them to the variable ➌. Rather
    than raising a KeyError, the get() method will return None if you request a key
    that isn’t in the dictionary.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: There is the extremely rare chance that the prefix will be the last word in
    a corpus and that it will be unique. In that case, there will be no suffixes.
    Use an if statement to anticipate this ➍. If there are no suffixes, the function
    that calls word_after_single(), which you’ll define in the next section, chooses
    a new prefix.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Each of the suffixes represents a *candidate* word for the haiku, but the program
    hasn’t yet determined whether a candidate will “fit.” So use a for loop, the count_syllables
    module, and an if statement to determine whether adding the word to the line violates
    the target number of syllables per line ➎. If the target isn’t exceeded, append
    the word to the list of accepted words ➏. Display the acceptable words in a logging
    message and then return it ➐.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: The next function, word_after_double(), is like the previous function except
    that you pass it word pairs and the Markov order 2 model (suffix_map_2) ➑ and
    get the suffixes from this dictionary ➒. But just like the word_after_single()
    function, word_after_double() returns a list of acceptable words ➓.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '***Generating the Haiku Lines***'
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With all the helper functions ready, you can define the function that actually
    writes the lines of the haiku. The function can build either the whole haiku or
    just update lines two or three. There are two paths to take: one to use when the
    program has at most a one-word suffix to work with and another for every other
    situation.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the First Line**'
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 9-5](ch09.xhtml#ch09list5) defines the function that writes haiku
    lines and initiates the haiku’s first line.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '*markov_haiku.py,* part 5'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '➊ def haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line, target_syls):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '"""Build a haiku line from a training corpus and return it."""'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: ➋ line = '2/3'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: line_syls = 0
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: current_line = []
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ➌ if len(end_prev_line) == 0:  # build first line
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: ➍ line = '1'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: ➎ word, num_syls = random_word(corpus)
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: current_line.append(word)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: line_syls += num_syls
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: ➏ word_choices = word_after_single(word, suffix_map_1,
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: line_syls, target_syls)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '➐ while len(word_choices) == 0:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: prefix = random.choice(corpus)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: logging.debug("new random prefix = %s", prefix)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: word_choices = word_after_single(prefix, suffix_map_1,
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: line_syls, target_syls)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: ➑ word = random.choice(word_choices)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: num_syls = count_syllables(word)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: logging.debug("word & syllables = %s %s", word, num_syls)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: ➒ line_syls += num_syls
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: current_line.append(word)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '➓ if line_syls == target_syls:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: end_prev_line.extend(current_line[-2:])
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: return current_line, end_prev_line
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9-5: Defines the function that writes haiku lines and initiates the
    first line*'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that takes as arguments both Markov models, the training corpus,
    the last word pair from the end of the preceding line, and the target number of
    syllables for the current line ➊. Immediately use a variable to specify which
    haiku lines are being simulated ➋. Most of the processing will be for lines two
    and three (and possibly the last part of line one), where you will be working
    with an existing word-pair prefix, so let these represent the base case. After
    this, start a counter for the running total of syllables in the line and start
    an empty list to hold the words in the current line.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Use an if statement that’s True under the condition that the end_prev_line parameter’s
    length—the number of syllables in the previous line’s last two words—is 0, meaning
    there was no preceding line and you are on line one ➌. The first statement in
    that if block changes the line variable to 1 ➍.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Choose the initial seed word and get its syllable count by calling the random_word()
    function ➎. By assigning the word and num_syls variables together, you are “unpacking”
    the (word, num_sylls) tuple that the random_word() function returns. Functions
    end at return statements, so returning tuples is a great way to return multiple
    variables. In a more advanced version of this program, you could use generator
    functions with the yield keyword, as yield returns a value without relinquishing
    control of execution.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Next, append the word to current_line and add the num_syls to the running total.
    Now that you have a seed, collect all the seed’s possible suffixes with the word_after_single()
    function ➏.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: If there are no acceptable words, start a while loop to handle this situation.
    This loop will continue until a non-empty list of acceptable word choices has
    been returned ➐. The program will choose a new prefix—a ghost prefix—using the
    random module’s choice method. (Remember that this prefix will not become part
    of the haiku but is used only to reaccess the Markov model.) Inside the while
    loop, a logging message will let you know which ghost prefix has been chosen.
    Then the program will call word_after_single() function once more.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Once the list of acceptable words is built, use choice again to select a word
    from the word_choices list ➑. Because the list may include duplicate words, this
    is where you see the statistical impact of the Markov model. Follow by counting
    the syllables in the word and logging the results.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Add the syllable count to the line’s running total and append the word to the
    current_line list ➒.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: If the number of syllables in the first two words equals 5 ➓, name a variable
    end_prev_line and assign it the last two words of the previous line; this variable
    is the prefix for line two. Finally, return the whole line and the end_prev_line
    variable.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: If the target number of syllables for the first line hasn’t been reached, the
    program will jump to the while loop in the next section to complete the line.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the Remaining Lines**'
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In [Listing 9-6](ch09.xhtml#ch09list6), the last part of the haiku_line() function
    addresses the case where the haiku already contains a word-pair prefix that the
    program can use in the Markov order 2 model. The program uses it to complete line
    one—assuming the first two words didn’t already total five syllables—and to build
    lines two and three. The user will also be able to regenerate lines two or three,
    after a complete haiku has been written.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '*markov_haiku.py,* part 6'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '➊ else: # build lines 2 and 3'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ➋ current_line.extend(end_prev_line)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '➌ while True:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: logging.debug("line = %s\n", line)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ➍ prefix = current_line[-2] + ' ' + current_line[-1]
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ➎ word_choices = word_after_double(prefix, suffix_map_2,
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: line_syls, target_syls)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '➏ while len(word_choices) == 0:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: index = random.randint(0, len(corpus) - 2)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: prefix = corpus[index] + ' ' + corpus[index + 1]
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: logging.debug("new random prefix = %s", prefix)
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: word_choices = word_after_double(prefix, suffix_map_2,
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: line_syls, target_syls)
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: word = random.choice(word_choices)
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: num_syls = count_syllables(word)
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: logging.debug("word & syllables = %s %s", word, num_syls)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '➐ if line_syls + num_syls > target_syls:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'elif line_syls + num_syls < target_syls:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: current_line.append(word)
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: line_syls += num_syls
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'elif line_syls + num_syls == target_syls:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: current_line.append(word)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: break
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: ➑ end_prev_line = []
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: end_prev_line.extend(current_line[-2:])
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '➒ if line == ''1'':'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: final_line = current_line[:]
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: final_line = current_line[2:]
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: return final_line, end_prev_line
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9-6: Uses a Markov order 2 model to complete the function that writes
    haiku lines*'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Start with an else statement that’s executed if there is a suffix ➊. Since the
    last part of the haiku_line() function has to handle line one as well as lines
    two and three, use a trick where you add the end_prev_line list (built outside
    the conditional at step ➑) to the current_line list ➋. Later, when you add the
    finalized line to the haiku, you’ll discard this leading word pair.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a while loop that continues until the target syllable count for the line
    has been reached ➌. The start of each iteration begins with a debugging message
    that informs you of the path the loop is evaluating: ''1'' or ''2/3''.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: With the last two words of the previous line added to the start of the current
    line, the last two words of the current line will always be the prefix ➍.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Using the Markov order 2 model, create a list of acceptable words ➎. In the
    event the list is empty, the program uses the ghost prefix process ➏.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate what to do next using the syllable count ➐. If there are too many syllables,
    use a continue statement to restart the while loop. If there aren’t enough syllables,
    append the word and add its syllable count to the line’s syllable count. Otherwise,
    append the word and end the loop.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Assign the last two words in the line to the end_prev_line variable so the program
    can use it as a prefix for the next line ➑. If the current path is line '1', copy
    the current line to a variable called final_line; if the path is line '2/3', use
    index slicing to exclude the first two words before assigning to final_line ➒.
    This is how you remove the initial end_prev_line word pair from lines two or three.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '***Writing the User Interface***'
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Listing 9-7](ch09.xhtml#ch09list7) defines the *markov_haiku.py* program’s
    main() function, which runs the setup functions and the user interface. The interface
    presents the user with a menu of choices and displays the resulting haiku.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '*markov_haiku.py,* part 7'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '"""Give user choice of building a haiku or modifying an existing haiku."""'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: intro = """\n
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: A thousand monkeys at a thousand typewriters...
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: or one computer...can sometimes produce a haiku.\n"""
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: print("{}".format(intro))
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: ➊ raw_haiku = load_training_file("train.txt")
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: corpus = prep_training(raw_haiku)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: suffix_map_1 = map_word_to_word(corpus)
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: suffix_map_2 = map_2_words_to_word(corpus)
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: final = []
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: choice = None
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '➋ while choice != "0":'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: ➌ print(
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Japanese Haiku Generator
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 0 - Quit
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 1 - Generate a Haiku
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 2 - Regenerate Line 2
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 3 - Regenerate Line 3
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '➍ choice = input("Choice: ")'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: print()
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '# exit'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '➎ if choice == "0":'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: print("Sayonara.")
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: sys.exit()
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '# generate a full haiku'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '➏ elif choice == "1":'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: final = []
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: end_prev_line = []
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: first_line, end_prev_line1 = haiku_line(suffix_map_1, suffix_map_2,
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: corpus, end_prev_line, 5)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: final.append(first_line)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2,
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: corpus, end_prev_line1, 7)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: final.append(line)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: line, end_prev_line3 = haiku_line(suffix_map_1, suffix_map_2,
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: corpus, end_prev_line2, 5)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: final.append(line)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '# regenerate line 2'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '➐ elif choice == "2":'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'if not final:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: print("Please generate a full haiku first (Option 1).")
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2,
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: corpus, end_prev_line1, 7)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: final[1] = line
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '# regenerate line 3'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '➑ elif choice == "3":'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'if not final:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: print("Please generate a full haiku first (Option 1).")
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: line, end_prev_line3 = haiku_line(suffix_map_1, suffix_map_2,
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: corpus, end_prev_line2, 5)
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: final[2] = line
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '# some unknown choice'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '➒ else:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: print("\nSorry, but that isn't a valid choice.", file=sys.stderr)
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: continue
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '➓ # display results'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: print()
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: print("First line = ", end="")
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: print(' '.join(final[0]), file=sys.stderr)
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: print("Second line = ", end="")
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: print(" ".join(final[1]), file=sys.stderr)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: print("Third line = ", end="")
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: print(" ".join(final[2]), file=sys.stderr)
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: print()
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: input("\n\nPress the Enter key to exit.")
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'if __name__ == ''__main__'':'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: main()
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9-7: Starts the program and presents a user interface*'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: After the introduction message, load and prep the training corpus and build
    the two Markov models. Then create an empty list to hold the final haiku ➊. Next,
    name a choice variable and set it to None. Start a while loop that continues until
    the user chooses choice 0 ➋. By entering 0, the user has decided to quit the program.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Use a print() statement with triple quotes to display a menu ➌, and then get
    the user’s choice ➍. If the user chooses 0, exit and say goodbye ➎. If the user
    chooses 1, they want the program to generate a new haiku, so reinitialize the
    final list and the end_prev_line variable ➏. Then call the haiku_line() function
    for all three lines and pass it the correct arguments—including the target syllable
    count for each line. Note that the end_prev_line variable name changes with each
    line; for example, end_prev_line2 holds the final two words of line two. The last
    variable, end_prev_line3, is just a placeholder so you can reuse the function;
    in other words, it is never put to use. Each time the haiku_line() function is
    called, it returns a line that you need to append to the final list.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: If the user chooses 2, the program regenerates the second line ➐. There needs
    to be a full haiku before the program can rebuild a line, so use an if statement
    to handle the user jumping the gun. Then call the haiku_line() function, making
    sure to pass it the end_prev_line1 variable to link it to the preceding line,
    and set the syllable target to seven syllables. Insert the rebuilt line in the
    final list at index 1.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the process if the user chooses 3, only make the syllable target 5 and
    pass end_prev_line2 to the haiku_line() function ➑. Insert the line at index 2
    in final.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: If the user enters anything not on the menu, let them know and then continue
    the loop ➒. Finish by displaying the haiku. Use the join() method and file=sys.stderr
    for an attractive printout in the shell ➓.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: End the program with the standard code for running the program as a module or
    in stand-alone mode.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '**The Results**'
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate a poetry-writing program, you need a way to measure something subjective—whether
    or not poems are “good”—using objective criteria. For the *markov_haiku.py* program,
    I propose the following categories based on two criteria, originality and humanness:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplicate** Verbatim duplication of a haiku in the training corpus.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '**Good** A haiku that is—at least to some people—indistinguishable from a haiku
    written by a human poet. It should represent the initial result or the result
    of regenerating line two or three a few times.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '**Seed** A haiku that has merit but that many would suspect was written by
    a computer, or a haiku that you could transform into a good haiku by changing
    or repositioning no more than two words (described in more detail later). It may
    require multiple regenerations of lines two or three.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '**Rubbish** A haiku that is clearly a random amalgam of words and has no merit
    as a poem.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: If you use the program to generate a large number of haiku and place the results
    in these categories, you’ll probably end up with the distribution in [Figure 9-3](ch09.xhtml#ch09fig3).
    About 5 percent of the time, you’ll duplicate an existing haiku in the training
    corpus; 10 percent of the time, you’ll produce a good haiku; around 25 percent
    will be passable or fixable haiku; and the rest will be rubbish.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0181-01.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-3: Subjective results of generating 500 haiku using* markov_haiku.py'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Considering how simple the Markov process is, the results in [Figure 9-3](ch09.xhtml#ch09fig3)
    are impressive. To quote Charles Hartman once again, “Here is language creating
    itself out of nothing, out of mere statistical noise. . . . We can watch sense
    evolve and meaning stagger up onto its own miraculous feet.”
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '***Good Haiku***'
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following are some examples of simulated haiku classified as “good.” In
    the first example, the program has subtly—you might say “skillfully,” if you didn’t
    know an algorithm was responsible—altered my haiku from [Chapter 8](ch08.xhtml#ch08)
    to produce a new haiku with the same meaning.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Cloudbanks that I let
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Myself pretend are distant
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Mountains faraway
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next example, the program has managed to duplicate a common theme in
    traditional haiku: the juxtaposition of images or ideas.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: The mirror I stare
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Into shows my father’s face
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: An old silent pond
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: In this case, you find out that the mirror is really the surface of a still
    pond, though you may interpret the face itself as the pond.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the program is a little like panning for gold: sometimes you find a
    nugget. The haiku on the left was written by Ringai over 300 years ago. In the
    haiku on the right, the program has made subtle changes so that the poem now evokes
    images of a late spring freeze—a setback for the progress of the season.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '| In these dark watersDrawn up from my frozen wellGlittering of Spring                    —*Ringai*
    | Waters drawn up fromMy frozen well glitteringOf Spring standing still                         —*Python*
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: The following are examples of a few more “good” haiku. The first one is remarkable
    since it was built from three separate haiku in the training corpus, yet maintains
    a clear contextual thread throughout.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: As I walk the path
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Eleven brave knights canter
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Through the stormy woods
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: A line flip-flapping
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: Across the dark crimson sky
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: On this winter pond
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Such a thing alive
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: Rusted gate screeches open
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: Even things feel pain
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: The stone bridge! Sitting
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: Quietly doing nothing
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Yet Spring comes grass grows
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Dark sky oh! Autumn
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Snowflakes! A rotting pumpkin
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Collapsed and covered
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: Desolate moors fray
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Black cloudbank, broken, scatters
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: In the pines, the graves
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '***Seed Haiku***'
  id: totrans-506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The concept of computers helping humans write poetry has been around for some
    time. Poets often “prime the pump” by imitating earlier poems, and there’s no
    reason a computer can’t provide first drafts as part of a cyber-partnership. Even
    a fairly poor computer creation has the potential to “seed” the creative process
    and help a human overcome writer’s block.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: The following are three examples of seed haiku from the *markov_haiku.py* program.
    On the left is the not-quite-right computer-generated haiku. On the right is the
    version that I adjusted. I changed only a single word, highlighted in bold, in
    each.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '| My life must end likeAnother flower what aHungry wind **it** is | My life
    must end likeAnother flower what aHungry wind is **death** |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '|  |  |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| The dock floating inThe hot caressing night justBefore the dawn **old** |
    The dock floating inThe hot caressing night justBefore the dawn **rain** |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '|  |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| Moonrise on the graveAnd my old sadness a sharpShovel thrust **the** stars
    | Moonrise on the graveAnd my old sadness a sharpShovel thrust **of** stars |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: 'The last has cryptic meaning but seems to work, as it is filled with natural
    associations (moon and stars, grave and shovel, grave and sadness). At any rate,
    you shouldn’t fret too much over meaning. To paraphrase T.S. Eliot: meaning is
    like the meat the burglar throws the dog, to keep the mind diverted while the
    poem does its work!'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It took two chapters, but you now have a program that can simulate Japanese
    haiku written by the masters—or at least provide a useful starting point for a
    human poet. In addition, you applied the logging module to monitor what the program
    was doing at key steps.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Virtual Muse: Experiments in Computer Poetry* (Wesleyan University Press,
    1996) by Charles O. Hartman is an engaging look at the early collaboration between
    humans and computers to write poetry.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about Claude Shannon, check out *A Mind at Play: How
    Claude Shannon Invented the Information Age* (Simon & Schuster, 2017) by Jimmy
    Soni and Rod Goodman.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a digital version of *Japanese Haiku: Two Hundred Twenty Examples
    of Seventeen-Syllable Poems* (The Peter Pauper Press, 1955), translated by Peter
    Beilenson, online at Global Grey (*[https://www.globalgreyebooks.com/](https://www.globalgreyebooks.com/)*).'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'In the paper “Gaiku: Generating Haiku with Word Association Norms” (Association
    for Computational Linguistics, 2009), Yael Netzer and coauthors explore the use
    of word association norms (WANs) to generate haiku. You can build WAN corpora
    by submitting trigger words to people and recording their immediate responses
    (for example, *house* to *fly*, *arrest*, *keeper*, and so on). This results in
    the kind of tightly linked, intuitive relationships characteristic of human-generated
    haiku. You can find the paper online at *[http://www.cs.brandeis.edu/~marc/misc/proceedings/naacl-hlt-2009/CALC-09/pdf/CALC-0905.pdf](http://www.cs.brandeis.edu/~marc/misc/proceedings/naacl-hlt-2009/CALC-09/pdf/CALC-0905.pdf)*.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '*Automate the Boring Stuff with Python* (No Starch Press, 2015) by Al Sweigart
    has a useful overview chapter on debugging techniques, including logging.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Projects**'
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I’ve described some suggestions for spin-off projects in this section. As with
    all challenge projects, you’re on your own—no solutions are provided.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '***New Word Generator***'
  id: totrans-525
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In his award-winning 1961 sci-fi novel, *Stranger in a Strange Land*, author
    Robert A. Heinlein invented the word *grok* to represent deep, intuitive understanding.
    This word moved into the popular culture—especially the culture of computer programming—and
    is now in the *Oxford English Dictionary*.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with a new word that sounds legitimate is not easy, in part because
    humans are so anchored to the words we already know. But computers don’t suffer
    from this affliction. In *Virtual Muse*, Charles Hartman observed that his poetry-writing
    program would sometimes create intriguing letter combinations, such as *runkin*
    or *avatheformitor*, that could easily represent new words.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Write a program that recombines letters using Markov order 2, 3, and 4 models
    and use the program to generate interesting new words. Give them a definition
    and start applying them. Who knows—you may coin the next *frickin*, *frabjous*,
    *chortle*, or *trill*!
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '***Turing Test***'
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to Alan Turing, “A computer would deserve to be called intelligent
    if it could deceive a human into believing that it was human.” Use your friends
    to test haiku generated by the *markov_haiku.py* program. Mix the computer haiku
    with a few haiku written by the masters or yourself. Since computer haiku are
    often enjambed, be careful to choose human haiku that are also enjambed in order
    to deny your cleverer friends a free ride. Using lowercase letters and minimal
    punctuation for all the haiku also helps. I’ve provided an example, using Facebook,
    in [Figure 9-4](ch09.xhtml#ch09fig4).
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/f0185-01.jpg)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-4: Example Turing test experiment post on Facebook*'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '***Unbelievable! This Is Unbelievable! Unbelievable!***'
  id: totrans-533
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'President Trump is famous for speaking in short, simple sentences that use
    “the best words,” and short, simple sentences are great for haiku. In fact, the
    *Washington Post* published some inadvertent haiku found in some of his campaign
    speeches. Among these were:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: He’s a great, great guy.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: I saw him the other day.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: On television.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: They want to go out.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: They want to lead a good life.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: They want to work hard.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: We have to do it.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: And we need the right people.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: So Ford will come back.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Use online transcripts of Donald Trump’s speeches to build a new training corpus
    for the *markov_haiku.py* program. Remember that you’ll need to revisit [Chapter
    8](ch08.xhtml#ch08) and build a new “missing words” dictionary for any words not
    in the *Carnegie Mellon University Pronouncing Dictionary*. Then rerun the program
    and generate haiku that capture this moment in history. Save the best ones and
    revisit the Turing test challenge to see if your friends can separate your haiku
    from true Trump quotes.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '***To Haiku, or Not to Haiku***'
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: William Shakespeare wrote many famous phrases that fit the syllabic structure
    of haiku, such as “all our yesterdays,” “dagger of the mind,” and “parting is
    such sweet sorrow.” Use one or more of the Bard’s plays as a training corpus for
    the *markov_haiku.py* program. The big challenge here will be counting the syllables
    for all that olde English.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: '***Markov Music***'
  id: totrans-547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you’re musically inclined, perform an online search for “composing music
    with Markov chains.” You should find a wealth of material on using Markov chain
    analysis to compose music, using the notes from existing songs as a training corpus.
    The resulting “Markov music” is used like our seed haiku—as inspiration for human
    songwriters.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
