- en: '**4'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: STATISTICS**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Bad datasets lead to bad models. We’d like to understand our data before we
    build a model, and then use that understanding to create a useful dataset, one
    that leads to models that do what we expect them to do. Knowing basic statistics
    will enable us to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: A *statistic* is any number that’s calculated from a sample and used to characterize
    it in some way. In deep learning, when we talk about samples, we’re usually talking
    about datasets. Maybe the most basic statistic is the arithmetic mean, commonly
    known as the average. The mean of a dataset is a single-number summary of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll see many different statistics in this chapter. We’ll begin by learning
    about the types of data and characterizing a dataset with summary statistics.
    Next, we’ll learn about quantiles and plotting data to understand what it contains.
    After that comes a discussion of outliers and missing data. Datasets are seldom
    perfect, so we need to have some way of detecting bad data and dealing with missing
    data. We’ll follow our discussion of imperfect datasets with a discussion of the
    correlation between variables. Then we’ll close the chapter out by discussing
    hypothesis testing, where we attempt to answer questions like “How likely is it
    that the same parent process generated two datasets?” Hypothesis testing is widely
    used in science, including deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The four types of data are nominal, ordinal, interval, and ratio. Let’s look
    at each in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Nominal Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Nominal data*, sometimes called *categorical data*, is data that has no ordering
    between the different values. An example of this is eye color; there is no relationship
    between brown, blue, and green.'
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For *ordinal data*, the data has a ranking or order, though differences aren’t
    meaningful in a mathematical sense. For example, if a questionnaire asks you to
    select from “strongly disagree,” “disagree,” “neutral,” “agree,” and “strongly
    agree,” it’s pretty clear that there is an order. Still, it’s also clear that
    “agree” isn’t three more than “strongly disagree.” All we can say is that “strongly
    disagree” is to the left of “agree” (and “neutral” and “disagree”).
  prefs: []
  type: TYPE_NORMAL
- en: Another example of ordinal data is education level. If one person has a fourth-grade
    education and another has an eighth-grade education, we can say that the latter
    person is more educated than the former, but we can’t say that the latter person
    is twice as educated, because “twice as educated” has no fixed meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Interval Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Interval data* has meaningful differences. For example, if one cup of water
    is at 40 degrees Fahrenheit and another is at 80 degrees Fahrenheit, we can say
    that there is a 40-degree difference between the two cups of water. We can’t,
    however, say that there is twice as much heat in the second cup, because the zero
    for the Fahrenheit scale is arbitrary. Colloquially, we do say it’s twice as hot,
    but in reality, it isn’t. To see this, think about what happens if we change the
    temperature scale to another scale with an arbitrary, though more sensible, zero:
    the Celsius scale. We see that the first cup is at about 4.4 degrees Celsius,
    and the second is at 26.7 degrees Celsius. Clearly, the second cup doesn’t suddenly
    now have six times the heat of the first.'
  prefs: []
  type: TYPE_NORMAL
- en: Ratio Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Finally, *ratio data* is data where differences are meaningful, and there is
    a true zero point. Height is a ratio value because a height of zero is just that—no
    height at all. Similarly, age is also a ratio value because an age of zero means
    no age at all. If we were to adopt a new age scale and call a person zero when
    they reach, say, voting age, we’d then have an interval scale, not a ratio scale.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at temperature again. We said above that temperature is an interval
    quantity. This isn’t always the case. If we measure temperature in Fahrenheit
    or Celsius, then, yes, it is an interval quantity. However, if we measure temperature
    in Kelvin, the absolute temperature scale, then it becomes a ratio value. Why?
    Because a temperature of 0 Kelvin (or *K*) is just that, no temperature at all.
    If our first cup is at 40°F, 277.59 K, and the second is at 80°F, 299.82 K, then
    we can truthfully say that the second cup is 1.08 times hotter than the first,
    since (277.59)(1.08) ≈ 299.8.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-1](ch04.xhtml#ch04fig01) names the scales and shows their relationships
    to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-1: The four types of data*'
  prefs: []
  type: TYPE_NORMAL
- en: Each step in [Figure 4-1](ch04.xhtml#ch04fig01) from left to right adds something
    to the data that the type of data on the left is lacking. For nominal to ordinal,
    we add ordering. For ordinal to interval, we add meaningful differences. Lastly,
    moving from interval to ratio adds a true zero point.
  prefs: []
  type: TYPE_NORMAL
- en: In practical use, as far as statistics are concerned, we should be aware of
    the types of data so we don’t do something meaningless. If we have a questionnaire,
    and the mean value of question A on a 1-to-5 rating scale is 2, while for question
    B it’s 4, we can’t say that B is rated twice as high as A, only that B was rated
    higher than A. What “twice” means in this context is unclear and quite probably
    meaningless.
  prefs: []
  type: TYPE_NORMAL
- en: Interval and ratio data may be continuous (floating-points) or discrete (integers).
    From a deep learning perspective, models typically treat continuous and discrete
    data the same way, and we don’t need to do anything special for discrete data.
  prefs: []
  type: TYPE_NORMAL
- en: Using Nominal Data in Deep Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we have a nominal value, say a set of colors, such as red, green, and blue,
    and we want to pass that value into a deep network, we need to change the data
    before we can use it. As we just saw, nominal data has no order, so while it’s
    tempting to assign a value of 1 to red, 2 to green, and 3 to blue, it would be
    wrong to do so, since the network will interpret those numbers as interval data.
    In that case, to the network, blue = 3(red), which is of course nonsense. If we
    want to use nominal data with a deep network, we need to alter it so that the
    interval is meaningful. We do this with *one-hot encoding*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In one-hot encoding, we turn the single nominal variable into a vector, where
    each element of the vector corresponds to one of the nominal values. For the color
    example, the one nominal variable becomes a three-element vector with one element
    representing red, another green, and the last blue. Then, we set the value corresponding
    to the color to one and all the others to zero, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value** |  | **Vector** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| red | → | 1 0 0 |'
  prefs: []
  type: TYPE_TB
- en: '| green | → | 0 1 0 |'
  prefs: []
  type: TYPE_TB
- en: '| blue | → | 0 0 1 |'
  prefs: []
  type: TYPE_TB
- en: Now the vector values are meaningful because either it’s red (1) or it’s not
    (0), green (1) or it’s not (0), or blue (1) or it’s not (0). The interval between
    zero and one has mathematical meaning because the presence of the value, say red,
    is genuinely greater than its absence, and that works the same way for each color.
    The values are now interval, so the network can use them. In some toolkits, like
    Keras, class labels are one-hot encoded before passing them to the model. This
    is done so vector output operates nicely with the one-hot encoded class label
    when computing the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re given a dataset. How do we make sense of it? How should we characterize
    it to understand it better before we use it to build a model?
  prefs: []
  type: TYPE_NORMAL
- en: To answer these questions, we need to learn about *summary statistics*. Calculating
    summary statistics should be the first thing you do when handed a new dataset.
    Not looking at your dataset before building a model is like buying a used car
    without checking the tires, taking it for a test drive, and looking under the
    hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'People have different notions of what makes a good set of summary statistics.
    We’ll focus on the following: means; the median; and measures of variation, including
    variance, standard deviation, and standard error. The range and mode are also
    often mentioned. The *range* is the difference between the maximum and minimum
    of the dataset. The *mode* is the most frequent value in the dataset. We generally
    get a sense of the mode visually from the histogram, as the histogram shows us
    the shape of the distribution of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Means and Median
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most of us learned how to calculate the average of a set of numbers in elementary
    school: add the numbers and divide by how many there are. This is the *arithmetic
    mean*, or, more specifically, the *unweighted* arithmetic mean. If the dataset
    consists of a set of values, *{x[0]*, *x*[1], *x*[2], . . . , *x[n–1]*}, then
    the arithmetic mean is the sum of the data divided by the number of elements in
    the dataset (*n*). Notationally, we write this as the following.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ![image](Images/xbar.jpg) is the typical way to denote the mean of a sample.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 4.1](ch04.xhtml#ch04equ01) calculates the unweighted mean. Each value
    is given a weight of 1/*n*, where the sum of all the weights is 1.0\. Sometimes,
    we might want to weight elements of the dataset differently; in other words, not
    all of them should count equally. In that case, we calculate a weighted mean,'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/071equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *w[i]* is the weight given to *x[i]* and Σ*[i]w[i]* = 1\. The weights
    are not part of the dataset; they need to come from somewhere else. The grade
    point average (GPA) used by many universities is an example of a weighted mean.
    The grade for each course is multiplied by the number of course credits, and the
    sum is divided by the total number of credits. Algebraically, this is equivalent
    to multiplying each grade by a weight, *w[i]* = *c[i]*/Σ*[i]**c[i]*, with *c[i]*
    the number of credits for course *i* and Σ*[i]**c[i]* the total number of credits
    for the semester.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Mean
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The arithmetic mean is by far the most commonly used mean. However, there are
    others. The *geometric mean* of two positive numbers, *a* and *b*, is the square
    root of their product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/071equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In general, the geometric mean of *n* positive numbers is the *n*th root of
    their product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/071equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The geometric mean is used in finance to calculate average growth rates. In
    image processing, the geometric mean can be used as a filter to help reduce image
    noise. In deep learning, the geometric mean appears in the *Matthews correlation
    coefficient (MCC)*, one of the metrics we use to evaluate deep learning models.
    The MCC is the geometric mean of two other metrics, the informedness and the markedness.
  prefs: []
  type: TYPE_NORMAL
- en: Harmonic Mean
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The *harmonic mean* of two numbers, *a* and *b*, is the reciprocal of the arithmetic
    mean of their reciprocals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/071equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In general,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/072equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The harmonic mean shows up in deep learning as the F1 score. This is a frequently
    used metric for evaluating classifiers. The F1 score is the harmonic mean of the
    recall (sensitivity) and the precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/072equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Despite its frequent use, it’s not a good idea to use the F1 score to evaluate
    a deep learning model. To see this, consider the definitions of recall and precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/072equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, TP is the number of true positives, FN is the number of false negatives,
    and FP is the number of false positives. These values come from the test set used
    to evaluate the model. A fourth number that’s important for classifiers, TN, is
    the number of correctly classified true negatives (assuming a binary classifier).
    The F1 score ignores TN, but to understand how well the model performs, we need
    to consider both positive and negative classifications. Therefore, the F1 score
    is misleading and often too optimistic. Better metrics are the MCC mentioned above
    or Cohen’s κ (kappa), which is similar to MCC and usually tracks it closely.
  prefs: []
  type: TYPE_NORMAL
- en: Median
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Before moving on to measures of variation, there’s one more commonly used summary
    statistic we’ll mention here. It’ll show up again a little later in the chapter
    too. The *median* of a dataset is the middle value. It’s the value where, when
    the dataset is sorted numerically, half the values are below it and half are above
    it. Let’s use this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X* = {55,63,65,37,74,71,73,87,69,44}'
  prefs: []
  type: TYPE_NORMAL
- en: If we sort *X*, we get
  prefs: []
  type: TYPE_NORMAL
- en: '{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}'
  prefs: []
  type: TYPE_NORMAL
- en: We immediately see a potential problem. I said we need the middle value when
    the data is sorted. With 10 things in *X*, there is no middle value. The middle
    lies between 65 and 69\. When the number of elements in the data-set is even,
    the median is the arithmetic mean of the two middle numbers. Therefore, the median
    in this case is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/073equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The arithmetic mean of the data is 63.8\. What’s the difference between the
    mean and the median?
  prefs: []
  type: TYPE_NORMAL
- en: By design, the median tells us the value that splits the dataset, so the number
    of samples above equals the number below. It’s the number of samples that matters.
    For the mean, it’s a sum over the actual data values. Therefore, the mean is sensitive
    to the values themselves, while the median is sensitive to the ordering of the
    values.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at *X*, we see that most values are in the 60s and 70s, with one
    low value of 37\. It’s the low value of 37 that drags the mean down relative to
    the median. An excellent example of this effect is income. The current median
    annual family income in the United States is about $62,000\. A recent measure
    of the mean family income in the United States is closer to $72,000\. The difference
    is because of the small portion of the population who make significantly more
    money than everyone else. They pull the overall mean up. For income, then, the
    most meaningful statistic is the median.
  prefs: []
  type: TYPE_NORMAL
- en: Consider [Figure 4-2](ch04.xhtml#ch04fig02).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-2: The mean (solid) and median (dashed) plotted over the histogram
    of a sample dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-2](ch04.xhtml#ch04fig02) shows the histogram generated from 1,000
    samples of a simulated dataset. Also plotted are the mean (solid line) and median
    (dashed line). The two do not match; the long tail in the histogram drags the
    mean up. If we were to count, 500 samples would fall in the bins below the dashed
    line and 500 in the bins above.'
  prefs: []
  type: TYPE_NORMAL
- en: Are there times when the mean and median are the same? Yes. If the data distribution
    is completely symmetric, then the mean and median will be the same. The classic
    example of this situation is the normal distribution. [Figure 3-4](ch03.xhtml#ch03fig04)
    showed a normal distribution where the left-right symmetry was clear. The normal
    distribution is special. We’ll see it again throughout the chapter. For now, remember
    that the closer the distribution of the dataset is to a normal distribution, the
    closer the mean and median will be.
  prefs: []
  type: TYPE_NORMAL
- en: 'The opposite is also worth remembering: if the dataset’s distribution is far
    from normal, like in [Figure 4-2](ch04.xhtml#ch04fig02), then the median is likely
    the better statistic to consider when summarizing the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Measures of Variation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A beginning archer shoots 10 arrows at a target. Eight of the beginner’s arrows
    hit the target, two miss completely, and the eight that do hit the target are
    spread uniformly across it. An expert archer shoots 10 arrows at a target. All
    of the expert’s arrows hit within a few centimeters of the center. Think about
    the mean position of the arrows. For the expert, all of the arrows are near the
    center of the target, so we can see that the mean position of the arrows will
    be near the center. For the beginner, none of the arrows are near the center of
    the target, but they are scattered more or less equally to the left and right
    or above and below the center. Because of this, the average position will balance
    out and be near the center of the target as well.
  prefs: []
  type: TYPE_NORMAL
- en: However, the first archer’s arrows are scattered; their location varies greatly.
    The second archer’s arrows, on the other hand, are tightly clustered, and there
    is little variation in their position. One meaningful way to summarize and understand
    a dataset is to quantify its variation. Let’s see how we might do this.
  prefs: []
  type: TYPE_NORMAL
- en: Deviation vs. Variance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One way we might measure the variation of a dataset is to find the *range*,
    the difference between the largest and smallest values. However, the range is
    a crude measurement, as it pays no attention to most of the values in the dataset,
    only the extremes. We can do better by calculating the mean of the difference
    between the data values and the mean of the data. The formula is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Equation 4.2](ch04.xhtml#ch04equ02) is the *mean deviation*. It’s a natural
    measure and gives just what we want: an idea of how far, on average, each sample
    is from the mean. While there’s nothing wrong with calculating the mean deviation,
    you’ll find that it’s rarely used in practice. One reason has to do with algebra
    and calculus. The absolute value is annoying to deal with mathematically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of the natural measure of variation, let’s calculate this one using
    squared differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Equation 4.3](ch04.xhtml#ch04equ03) is known as the *biased sample variance*.
    It’s the mean of the squared difference between each value in the dataset and
    the mean. It’s an alternate way of characterizing the scatter in the dataset.
    Why it’s biased, we’ll discuss in a second. We’ll get into why it’s ![image](Images/ssub-bar.jpg)
    and not *s[n]* shortly after that.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do, it’s worth noting that you’ll often see a slightly different
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation is the *unbiased sample variance*. Using *n* – 1 in place of *n*
    is known as Bessel’s correction. It’s related to the number of degrees of freedom
    in the residuals, where the residuals are what’s left when the mean is subtracted
    from each of the values in the dataset. The sum of the residuals is zero, so if
    there are *n* values in the dataset, knowing *n* – 1 of the residuals allows the
    last residual to be calculated. This gives us the degrees of freedom for the residuals.
    We are “free” to calculate *n* – 1 of them knowing that we’ll get the last one
    from the fact that the residuals sum to zero. Dividing by *n*–1 gives a less biased
    estimate of the variance, assuming ![image](Images/ssub-bar.jpg) is biased in
    some way to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Why are we talking about biased variance and unbiased variance? Biased how?
    We should always remember that a dataset is a sample from some parent data-generating
    process, the population. The true population variance (σ²) is the scatter of the
    population around the true population mean (μ). However, we don’t know μ or σ²,
    so instead, we estimate them from the dataset we do have. The mean of the sample
    is ![image](Images/xbar.jpg). That’s our estimate for μ. It’s then natural to
    calculate the mean of the squared deviations around ![image](Images/xbar.jpg)
    and call that our estimate for σ². That’s ![image](Images/ssub-bar.jpg) ([Equation
    4.3](ch04.xhtml#ch04equ03)). The claim, which is true but beyond our scope to
    demonstrate, is that ![image](Images/ssub-bar.jpg) is biased and not the best
    estimate of σ², but if Bessel’s correction is applied, we’ll have a better estimate
    of the population variance. So we should use *s*² ([Equation 4.4](ch04.xhtml#ch04equ04))
    to characterize the variance of the dataset around the mean.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we should use ![image](Images/xbar.jpg) and *s*² to quantify the
    variance of the data-set. Now, why is it *s*²? The square root of the variance
    is the *standard deviation* denoted as σ for the population and *s* for the estimate
    of σ calculated from the dataset. Most often, we want to work with the standard
    deviation. Writing square roots becomes tiresome, so convention has adopted the
    σ or *s* notation for the standard deviation and uses the squared form when discussing
    the variance.
  prefs: []
  type: TYPE_NORMAL
- en: And, because life isn’t already ambiguous enough, you’ll often see σ used for
    *s*, and [Equation 4.3](ch04.xhtml#ch04equ03) used when it really should be [Equation
    4.4](ch04.xhtml#ch04equ04). Some toolkits, including our beloved NumPy, make it
    easy to use the wrong formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as the number of samples in our dataset increases, the difference
    between the biased and unbiased variance decreases because dividing by *n* or
    *n* – 1 matters less and less. A few lines of code illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> import numpy as np'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> n = 10'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.random.random(n)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> (1/n)*((a-a.mean())**2).sum()'
  prefs: []
  type: TYPE_NORMAL
- en: '0.08081748204006689'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> (1/(n-1))*((a-a.mean())**2).sum()'
  prefs: []
  type: TYPE_NORMAL
- en: '0.08979720226674098'
  prefs: []
  type: TYPE_NORMAL
- en: Here, a sample with only 10 values (a) shows a difference in the biased and
    unbiased variance in the third decimal. If we increase our dataset size from 10
    to 10,000, we get
  prefs: []
  type: TYPE_NORMAL
- en: '>>> n = 10000'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.random.random(n)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> (1/n)*((a-a.mean())**2).sum()'
  prefs: []
  type: TYPE_NORMAL
- en: '0.08304350577482553'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> (1/(n-1))*((a-a.mean())**2).sum()'
  prefs: []
  type: TYPE_NORMAL
- en: '0.08305181095592111'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the biased and unbiased estimate of the variance is now
    in the fifth decimal. Therefore, for the large datasets we typically work with
    in deep learning, it matters little in practice whether we use *s[n]* or *s* for
    the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '**MEDIAN ABSOLUTE DEVIATION**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard deviation is based on the mean. The mean, as we saw above, is
    sensitive to extreme values, and the standard deviation is doubly so because we
    square the deviation from the mean for each sample. A measure of variability that
    is insensitive to extreme values in the dataset is the *median absolute deviation
    (MAD)*. The MAD is defined as the median of the absolute values of the difference
    between the data and the median:'
  prefs: []
  type: TYPE_NORMAL
- en: MAD = median(|*X[i]* – median(*X*)|)
  prefs: []
  type: TYPE_NORMAL
- en: 'Procedurally, first calculate the median of the data, then subtract it from
    each data value, making the result positive, and report the median of that set.
    The implementation is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def MAD(x):'
  prefs: []
  type: TYPE_NORMAL
- en: return np.median(np.abs(x-np.median(x)))
  prefs: []
  type: TYPE_NORMAL
- en: The MAD is not often used, but its insensitivity to extreme values in the dataset
    argues toward more frequent use, especially for outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Error vs. Standard Deviation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We have one more measure of variance to discuss: the *standard error of the
    mean (SEM)*. The SEM is often simply called the *standard error (SE)*. We need
    to go back to the population to understand what the SE is and when to use it.
    If we select a sample from the population, a dataset, we can calculate the mean
    of the sample, ![image](Images/xbar.jpg). If we choose repeated samples and calculate
    those sample means, we’ll generate a dataset of means of the samples from the
    population. This might sound familiar; it’s the process we used to illustrate
    the central limit theorem in [Chapter 3](ch03.xhtml#ch03). The standard deviation
    of the set of means is the standard error.'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the standard error from the standard deviation is straightforward,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/077equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and is nothing more than a scaling of the sample standard deviation by the square
    root of the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: When should we use the standard deviation, and when should we use the standard
    error? Use the standard deviation to learn about the distribution of the samples
    around the mean. Use the standard error to say something about how good an estimate
    of the population mean a sample mean is. In a sense, the standard error is related
    to both the central limit theorem, as that affects the standard deviation of the
    means of multiple samples from the parent population, and the law of large numbers,
    since a larger dataset is more likely to give a better estimate of the population
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: From a deep learning point of view, we might use the standard deviation to describe
    the dataset used to train a model. If we train and test several models, remembering
    the stochastic nature of deep network initialization, we can calculate a mean
    over the models for some metric, say the accuracy. In that case, we might want
    to report the mean accuracy plus or minus the standard error. As we train more
    models and gain confidence that the mean accuracy represents the sort of accuracy
    the model architecture can provide, we should expect that the error in the mean
    accuracy over the models will decrease.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, in this section, we discussed different summary statistics, values
    we can use to start to understand a dataset. These include the various means (arithmetic,
    geometric, and harmonic), the median, the standard deviation, and, when appropriate,
    the standard error. For now, let’s see how we can use plots to help understand
    a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Quantiles and Box Plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To calculate the median, we need to find the middle value, the number splitting
    the dataset into two halves. Mathematically, we say that the median divides the
    dataset into two quantiles.
  prefs: []
  type: TYPE_NORMAL
- en: A *quantile* splits the dataset into fixed-sized groups where the fixed size
    is the number of data values in the quantile. Since the median splits the dataset
    into two equally sized groups, it’s a *2-quantile*. Sometimes you’ll see the median
    referred to as the *50th percentile*, meaning 50 percent of the data values are
    less than this value. By similar reasoning, then, the 95th percentile is the value
    that 95 percent of the dataset is less than. Researchers often calculate 4-quantiles
    and refer to them as *quartiles*, since they split the dataset into four groups
    such that 25 percent of the data values are in the first quartile, 50 percent
    are in the first and second, and 75 percent are in the first, second, and third,
    with the final 25 percent in the fourth quartile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work through an example to understand what we mean by quantiles. The
    example uses a synthetic exam dataset representing 1,000 test scores. See the
    file *exams.npy*. We’ll use NumPy to calculate the quartile values for us and
    then plot a histogram of the dataset with the quartile values marked. First, let’s
    calculate the quartile positions:'
  prefs: []
  type: TYPE_NORMAL
- en: d = np.load("exams.npy")
  prefs: []
  type: TYPE_NORMAL
- en: p = d[:,0].astype("uint32")
  prefs: []
  type: TYPE_NORMAL
- en: q = np.quantile(p, [0.0, 0.25, 0.5, 0.75, 1.0])
  prefs: []
  type: TYPE_NORMAL
- en: 'print("Quartiles: ", q)'
  prefs: []
  type: TYPE_NORMAL
- en: print("Counts by quartile:")
  prefs: []
  type: TYPE_NORMAL
- en: print("    %d" % ((q[0] <= p) & (p < q[1])).sum())
  prefs: []
  type: TYPE_NORMAL
- en: print("    %d" % ((q[1] <= p) & (p < q[2])).sum())
  prefs: []
  type: TYPE_NORMAL
- en: print("    %d" % ((q[2] <= p) & (p < q[3])).sum())
  prefs: []
  type: TYPE_NORMAL
- en: print("    %d" % ((q[3] <= p) & (p < q[4])).sum())
  prefs: []
  type: TYPE_NORMAL
- en: This code, along with code to generate the plot, is in the file *quantiles.py*.
  prefs: []
  type: TYPE_NORMAL
- en: First we load the synthetic exam data and keep the first exam scores (p). Note,
    we make p an integer array so we can use np.bincount later to make the histogram.
    (That code is not shown above.) We then use NumPy’s np.quantile function to calculate
    the quartile values. This function takes the source array and an array of quantile
    values in the range [0, 1]. The values are fractions of the distance from the
    minimum value of the array to its maximum. So, asking for the 0.5 quantile is
    asking for the value that is half the distance between the minimum of p and its
    maximum such that the number of values in each set is equal.
  prefs: []
  type: TYPE_NORMAL
- en: To get quartiles, we ask for the 0.25, 0.5, and 0.75 quantiles to get the values
    such that 25 percent, 50 percent, and 75 percent of the elements of p are less
    than the values. We also ask for the 0.0 and 1.0 quantiles, the minimum and maximum
    of p. We do this for convenience when we count the number of elements in each
    range. Note, we could have instead used the np.percentile function. It returns
    the same values as np.quantile but uses percentage values instead of fractions.
    In that case, the second argument would have been [0,25,50,75,100].
  prefs: []
  type: TYPE_NORMAL
- en: The returned quartile values are in q. We print them to get
  prefs: []
  type: TYPE_NORMAL
- en: 18.0, 56.75, 68.0, 78.0, 100.0
  prefs: []
  type: TYPE_NORMAL
- en: Here, 18 is the minimum, 100 is the maximum, and the three cutoff values for
    the quartiles are 56.75, 68, and 78\. Note that the cutoff for the second quartile
    is the median, 68.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining code counts the number of values in p in each range. With 1,000
    values, we’d expect to have 250 in each range, but because the math doesn’t always
    fall along existing data values, we get instead
  prefs: []
  type: TYPE_NORMAL
- en: 250, 237, 253, 248
  prefs: []
  type: TYPE_NORMAL
- en: meaning 250 elements of p are less than 56.75, 237 are in [56.75, 68], and so
    forth.
  prefs: []
  type: TYPE_NORMAL
- en: The code above uses a clever counting trick worth explaining. We want to count
    the number of values in p in some range. We can’t use NumPy’s np.where function,
    as it doesn’t like the compound conditional statement. However, if we use an expression
    like 10 <= p, we’ll be given an array the same size as p where each element is
    either True if the condition is true for that element or False if it is not. Therefore,
    asking for 10 <= p and p < 90 will return two Boolean arrays. To get the elements
    where both conditions are true, we need to logically AND them together (&). This
    gives us a final array the same size and shape as p, where all True elements represent
    values in p in [10, 90). To get the count, we apply the sum method that for a
    Boolean array treats True as one and False as zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-3](ch04.xhtml#ch04fig03) shows the histogram of the exam data with
    the quartiles marked.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-3: A histogram of 1,000 exam scores with the quartiles marked*'
  prefs: []
  type: TYPE_NORMAL
- en: The example above shows yet again how useful a histogram is for visualizing
    and understanding data. We should use histograms whenever possible to help understand
    what’s going on with a dataset. [Figure 4-3](ch04.xhtml#ch04fig03) superimposes
    the quartile values on the histogram. This helps us understand what the quartiles
    are and their relationship to the data values, but this is not a typical presentation
    style. More typical, and useful because it can show multiple features of a dataset,
    is the *box plot*. Let’s use it now for the exam scores above, but this time we’ll
    also include the two other sets of exam scores we ignored previously.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll show a box plot first, and then explain it. To see the box plot for the
    three exams in the *exams.npy* file, use
  prefs: []
  type: TYPE_NORMAL
- en: d = np.load("exams.npy")
  prefs: []
  type: TYPE_NORMAL
- en: plt.boxplot(d)
  prefs: []
  type: TYPE_NORMAL
- en: plt.xlabel("Test")
  prefs: []
  type: TYPE_NORMAL
- en: plt.ylabel("Scores")
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: where we’re loading the full set of exam scores and then using the Matplotlib
    boxplot function.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the output, shown in [Figure 4-4](ch04.xhtml#ch04fig04).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-4: Box plots for the three exams (top), and the box plot for the
    first exam with the components marked (bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: The top chart in [Figure 4-4](ch04.xhtml#ch04fig04) shows the box plot for the
    three sets of exam scores in *|*exams.npy|. The first of these is plotted again
    on the bottom of [Figure 4-4](ch04.xhtml#ch04fig04), along with labels describing
    the parts of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: A box plot shows us a visual summary of the data. The box in the bottom chart
    in [Figure 4-4](ch04.xhtml#ch04fig04) illustrates the range between the cutoffs
    for the first quartile (Q1) and the third quartile (Q3). The numerical difference
    between Q3 and Q1 is known as the *interquartile range (IQR)*. The larger the
    IQR, the more spread out the data is around the median. Notice that the score
    is on the y-axis this time. We could have easily made the plot horizontal, but
    vertical is the default. The median (Q2) is marked near the middle of the box.
    The mean is not shown in a box plot.
  prefs: []
  type: TYPE_NORMAL
- en: The box plot includes two additional lines, the *whiskers*, though Matplotlib
    calls them *fliers*. As indicated, they are 1.5 times the IQR above Q3 or below
    Q1\. Finally, there are some circles labeled “possible outliers.” By convention,
    values outside of the whiskers are considered *possible outliers*, meaning they
    might represent erroneous data, either entered incorrectly by hand or, more likely
    these days, received from faulty sensors. For example, bright or dead pixels on
    a CCD camera might be considered outliers. When evaluating a potential dataset,
    we should be sensitive to outliers and use our best judgment about what to do
    with them. Usually, there are only a few, and we can drop the samples from the
    dataset without harm. However, it’s also possible that the outliers are actually
    real and are highly indicative of a particular class. If that’s the case, we want
    to keep them in the dataset in the hopes that the model will use them effectively.
    Experience, intuition, and common sense must guide us here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s interpret the top chart in [Figure 4-4](ch04.xhtml#ch04fig04) showing
    the three sets of exam scores. The top of the whiskers is at 100 each time, which
    makes sense: a 100 is a perfect score, and there were 100s in the dataset. Notice
    that the box portion of the plot is not centered vertically in the whiskers. Recalling
    that 50 percent of the data values are between Q1 and Q3, with 25 percent above
    and below Q2 in the box, we see that the data is not rigorously normal; its distribution
    deviates from a normal curve. A glance back to the histogram in [Figure 4-3](ch04.xhtml#ch04fig03)
    confirms this for the first exam. Similarly, we see the second and third exams
    deviate from normality as well. So, a box plot can tell us how similar the distribution
    of the dataset is to a normal distribution. When we discuss hypothesis testing
    below, we’ll want to know if the data is normally distributed or not.'
  prefs: []
  type: TYPE_NORMAL
- en: What about possible outliers, the values below Q1 – 1.5 × IQR? We know the dataset
    represents test scores, so common sense tells us that these are not outliers but
    valid scores by particularly confused (or lazy) students. If the dataset contained
    values above 100 or below zero, those would be fair game to label outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes dropping samples with outliers is the right thing to do. However,
    if the outlier is caused by missing data, cutting the sample might not be an option.
    Let’s take a look at what we might do with missing data, and why we should generally
    avoid it like the plague.
  prefs: []
  type: TYPE_NORMAL
- en: Missing Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Missing data is just that, data we don’t have. If the dataset consists of samples
    representing feature vectors, missing data shows up as one or more features in
    a sample that were not measured for some reason. Often, missing data is encoded
    in some way. If the value is only positive, a missing feature might be marked
    with a –1 or, historically, –999\. If the feature is given to us as a string,
    the string might be empty. For floating-point values, a not a number (NaN) might
    be used. NumPy makes it easy for us to check for NaNs in an array by using np.isnan:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.arange(10, dtype="float64")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a[3] = np.nan'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.isnan(a[3])'
  prefs: []
  type: TYPE_NORMAL
- en: 'True'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a[3] == np.nan'
  prefs: []
  type: TYPE_NORMAL
- en: 'False'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a[3] is np.nan'
  prefs: []
  type: TYPE_NORMAL
- en: 'False'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that direct comparison to np.nan with either == or is doesn’t work; only
    testing with np.isnan works.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting missing data is dataset-specific. Assuming we’ve convinced ourselves
    there is missing data, how do we handle it?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate a small dataset with missing values and use our existing statistics
    knowledge to see how to handle them. The code for the following is in missing.py.
    First, we generate a dataset of 1,000 samples, each with four features:'
  prefs: []
  type: TYPE_NORMAL
- en: N = 1000
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed(73939133)
  prefs: []
  type: TYPE_NORMAL
- en: x = np.zeros((N,4))
  prefs: []
  type: TYPE_NORMAL
- en: x[:,0] = 5*np.random.random(N)
  prefs: []
  type: TYPE_NORMAL
- en: x[:,1] = np.random.normal(10,1,size=N)
  prefs: []
  type: TYPE_NORMAL
- en: x[:,2] = 3*np.random.beta(5,2,N)
  prefs: []
  type: TYPE_NORMAL
- en: x[:,3] = 0.3*np.random.lognormal(size=N)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is in x. We fix the random number seed to get a reproducible result.
    The first feature is uniformly distributed. The second is normally distributed,
    while the third follows a beta distribution and the fourth a lognormal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the moment, x has no missing values. Let’s add some by making random elements
    NaNs:'
  prefs: []
  type: TYPE_NORMAL
- en: i = np.random.randint(0,N, size=int(0.05*N))
  prefs: []
  type: TYPE_NORMAL
- en: x[i,0] = np.nan
  prefs: []
  type: TYPE_NORMAL
- en: i = np.random.randint(0,N, size=int(0.05*N))
  prefs: []
  type: TYPE_NORMAL
- en: x[i,1] = np.nan
  prefs: []
  type: TYPE_NORMAL
- en: i = np.random.randint(0,N, size=int(0.05*N))
  prefs: []
  type: TYPE_NORMAL
- en: x[i,2] = np.nan
  prefs: []
  type: TYPE_NORMAL
- en: i = np.random.randint(0,N, size=int(0.05*N))
  prefs: []
  type: TYPE_NORMAL
- en: x[i,3] = np.nan
  prefs: []
  type: TYPE_NORMAL
- en: The dataset now has NaNs across 5 percent of its values.
  prefs: []
  type: TYPE_NORMAL
- en: If a few samples in a large dataset have missing data, we can remove them from
    the dataset with little worry. However, if 5 percent of the samples have missing
    data, we probably don’t want to lose that much data. More worrisome still, what
    if there’s a correlation between the missing data and a particular class? Throwing
    the samples away might bias the dataset in some way that’ll make the model less
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: So, what can we do? We just spent many pages learning how to summarize a dataset
    with basic descriptive statistics. Can we use those? Of course. We can look at
    the distributions of the features, ignoring the missing values, and use those
    distributions to decide how we might want to replace the missing data. Naively,
    we’d use the mean of the data we do have, but looking at the distribution may
    or may not push us toward the median instead, depending on how far the distribution
    is from normal. This sounds like a job for a box plot. Fortunately for us, Matplotlib’s
    boxplot function is smart; it ignores the NaNs. Therefore, making the box plot
    is a straightforward call to boxplot(x).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-5](ch04.xhtml#ch04fig05) shows us the dataset with the NaNs ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-5: Box plot of the dataset ignoring missing values*'
  prefs: []
  type: TYPE_NORMAL
- en: The boxes in [Figure 4-5](ch04.xhtml#ch04fig05) make sense for the distributions
    of the features. Feature 1 is uniformly distributed, so we expect a symmetric
    box around the mean/median. (These are the same for the uniform distribution.)
    Feature 2 is normally distributed, so we get a similar box structure as Feature
    1, but, with only 1,000 samples, some asymmetry is evident. The beta distribution
    of Feature 3 is skewed toward the top of its range, which we see in the box plot.
    Finally, the lognormal distribution of Feature 4 should be skewed toward lower
    values, with a long tail visible as the many “outliers” above the whiskers, an
    object lesson against mindlessly calling such values outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have features that are highly not normally distributed, we’ll update
    missing values with the median instead of the mean. The code is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: good_idx = np.where(np.isnan(x[:,0]) == False)
  prefs: []
  type: TYPE_NORMAL
- en: m = np.median(x[good_idx,0])
  prefs: []
  type: TYPE_NORMAL
- en: bad_idx = np.where(np.isnan(x[:,0]) == True)
  prefs: []
  type: TYPE_NORMAL
- en: x[bad_idx,0] = m
  prefs: []
  type: TYPE_NORMAL
- en: Here, i first holds the indices of Feature 1 that are not NaNs. We use these
    to calculate the median (m). Next, we set i to the indices that are NaNs and replace
    them with the median. We can do the same for the other features, updating the
    entire dataset so we no longer have missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Did we cause much of a change from the earlier distributions? No, because we
    only updated 5 percent of the values. For example, for Feature 3, based on the
    beta distribution, the mean and standard deviations change like so:'
  prefs: []
  type: TYPE_NORMAL
- en: non-NaN mean, std = 2.169986, 0.474514
  prefs: []
  type: TYPE_NORMAL
- en: updated mean, std = 2.173269, 0.462957
  prefs: []
  type: TYPE_NORMAL
- en: The moral of the story is that if there’s enough missing data that the dataset
    might become biased by dropping it, the safest thing to do is replace the missing
    data with the mean or median. To decide whether to use the mean or median, consult
    descriptive statistics, a box plot, or a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, if the dataset is labeled, as a deep learning dataset would be,
    the process described above needs to be completed with the mean or median of samples
    grouped by each class. Otherwise, the calculated value might be inappropriate
    for the class.
  prefs: []
  type: TYPE_NORMAL
- en: With missing data eliminated, deep learning models can be trained on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At times, there is an association between the features in a dataset. If one
    goes up, the other might go up as well, though not necessarily in a simple linear
    way. Or, the other might go down—a negative association. The proper word for this
    type of association is *correlation*. A statistic that measures correlation is
    a handy way to understand how the features in a dataset are related.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it isn’t hard to see that the pixels of most images are highly
    correlated. This means if we select a pixel at random and then an adjacent pixel,
    there’s a good chance the second pixel will be similar to the first pixel. Images
    where this is not true look to us like random noise.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional machine learning, highly correlated features were undesirable,
    as they didn’t add any new information and only served to confuse the models.
    The entire art of feature selection was developed, in part, to remove this effect.
    For modern deep learning, where the network itself learns a new representation
    of the input data, it’s less critical to have uncorrelated inputs. This is, in
    part, why images work as inputs to deep networks when they usually fail to work
    at all with older machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Whether the learning is traditional or modern, as part of summarizing and exploring
    a dataset, correlations among the features are worth examining and understanding.
    In this section, we’ll discuss two types of correlations. Each type returns a
    single number that measures the strength of the correlation between two features
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson Correlation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Pearson correlation coefficient* returns a number, *r* ϵ [–1, +1], that
    indicates the strength of the *linear* correlation between two features. By *linear*
    we mean how strongly we can describe the correlation between the features by a
    line. If the correlation is such that one feature goes up exactly as the other
    feature goes up, the correlation coefficient is +1\. Conversely, if the second
    feature goes down exactly as the other goes up, the correlation is –1\. A correlation
    of zero means there is no association between the two features; they are (possibly)
    independent.
  prefs: []
  type: TYPE_NORMAL
- en: I slipped the word *possibly* in the sentence above because there are situations
    where a nonlinear dependence between two features might lead to a zero Pearson
    correlation coefficient. These situations are not common, however, and for our
    purposes, we can claim a correlation coefficient near zero indicates the two features
    are independent. The closer the correlation coefficient is to zero, either positive
    or negative, the weaker the correlation between the features.
  prefs: []
  type: TYPE_NORMAL
- en: The Pearson correlation is defined using the means of the two features or the
    means of products of the two features. The inputs are two features, two columns
    of the dataset. We’ll call these inputs *X* and *Y*, where the capital letter
    refers to a vector of data values. Note, since these are two features from the
    dataset, *X[i]* is paired with *Y[i]*, meaning they both come from the same feature
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the Pearson correlation coefficient is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We’ve introduced a new, but commonly used, notation. The mean of *X* is the
    *expectation* of *X*, denoted as E(*X*). Therefore, in [Equation 4.5](ch04.xhtml#ch04equ05),
    we see the mean of *X*, E(*X*), and the mean of *Y*, E(*Y*). As we might suspect,
    E(*XY*) is the mean of the product of *X* and *Y*, element by element. Similarly,
    E(*X*²) is the mean of the product of *X* with itself, and E(*X*)² is the square
    of the mean of *X*. With this notation in hand, we can easily write our own function
    to calculate the Pearson correlation of two vectors of features:'
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: 'def pearson(x,y):'
  prefs: []
  type: TYPE_NORMAL
- en: exy = (x*y).mean()
  prefs: []
  type: TYPE_NORMAL
- en: ex = x.mean()
  prefs: []
  type: TYPE_NORMAL
- en: ey = y.mean()
  prefs: []
  type: TYPE_NORMAL
- en: exx = (x*x).mean()
  prefs: []
  type: TYPE_NORMAL
- en: ex2 = x.mean()**2
  prefs: []
  type: TYPE_NORMAL
- en: eyy = (y*y).mean()
  prefs: []
  type: TYPE_NORMAL
- en: ey2 = y.mean()**2
  prefs: []
  type: TYPE_NORMAL
- en: return (exy - ex*ey)/(np.sqrt(exx-ex2)*np.sqrt(eyy-ey2))
  prefs: []
  type: TYPE_NORMAL
- en: The pearson function directly implements [Equation 4.5](ch04.xhtml#ch04equ05).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up a scenario where we can use pearson and compare it to what NumPy
    and SciPy provide. The code that follows, including the definition of pearson
    above, is in the file *correlation.py*.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll create three correlated vectors, x, y, and z. We imagine that these
    are features from a dataset so that x[0] is paired with y[0] and z[0]. The code
    we need is
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed(8675309)
  prefs: []
  type: TYPE_NORMAL
- en: N = 100
  prefs: []
  type: TYPE_NORMAL
- en: x = np.linspace(0,1,N) + (np.random.random(N)-0.5)
  prefs: []
  type: TYPE_NORMAL
- en: y = np.random.random(N)*x
  prefs: []
  type: TYPE_NORMAL
- en: z = -0.1*np.random.random(N)*x
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we’re again fixing the NumPy pseudorandom seed to make the output
    reproducible. The first feature, x, is a noisy line from zero to one. The second,
    y, tracks x but is also noisy because of the multiplication by a random value
    in [0, 1). Finally, z is negatively correlated to x because of the –0.1 coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: The top chart in [Figure 4-6](ch04.xhtml#ch04fig06) plots the three feature
    values sequentially to see how they track each other. The bottom chart shows the
    three as paired points, with one value on the x-axis and the other on the y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-6: Three features in sequence to show how they track (top), and a
    scatter plot of the features as pairs (bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The NumPy function to calculate the Pearson correlation is np.corrcoef. Unlike
    our version, this function returns a matrix showing the correlations between all
    pairs of variables passed to it. For example, using our pearson function, we get
    the following as the correlation coefficients between x, y, and z:'
  prefs: []
  type: TYPE_NORMAL
- en: pearson(x,y):  0.682852
  prefs: []
  type: TYPE_NORMAL
- en: 'pearson(x,z): -0.850475'
  prefs: []
  type: TYPE_NORMAL
- en: 'pearson(y,z): -0.565361'
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy returns the following, with x, y, and z stacked as a single 3 × 100 array:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = np.vstack((x,y,z))'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.corrcoef(d))'
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 1.          0.68285166 -0.85047468]'
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0.68285166  1.         -0.56536104]'
  prefs: []
  type: TYPE_NORMAL
- en: '[-0.85047468 -0.56536104  1.        ]]'
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal corresponds to the correlation with each feature and itself, which
    is naturally perfect and therefore 1.0\. The correlation between x and y is in
    element 0,1 and matches our pearson function value. Similarly, the correlation
    between x and z is in element 0,2, and the correlation between y and z is in element
    1,2\. Notice also that the matrix is symmetric, which we expect because corr(*X,
    Y*) = corr(*Y, X*).
  prefs: []
  type: TYPE_NORMAL
- en: SciPy’s correlation function is stats.pearsonr, which acts like ours but returns
    a *p*-value along with the *r* value. We’ll discuss *p*-values more later in the
    chapter. We use the returned *p*-value as the probability of an uncorrelated system
    producing the calculated correlation value. For our example features, the *p*-value
    is virtually identical to zero, implying there’s no reasonable likelihood that
    an uncorrelated system produced the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We stated earlier that for images, nearby pixels are usually highly correlated.
    Let’s see if this is actually true for a sample image. We’ll use the China image
    included with sklearn and treat specific rows of the green band as the paired
    vectors. We’ll calculate the correlation coefficient for two adjacent rows, a
    row further away, and a random vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from sklearn.datasets import load_sample_image'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> china = load_sample_image(''china.jpg'')'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = china[230,:,1].astype("float64")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = china[231,:,1].astype("float64")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> c = china[400,:,1].astype("float64")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = np.random.random(640)'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> pearson(a,b)'
  prefs: []
  type: TYPE_NORMAL
- en: '0.8979360'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> pearson(a,c)'
  prefs: []
  type: TYPE_NORMAL
- en: '-0.276082'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> pearson(a,d)'
  prefs: []
  type: TYPE_NORMAL
- en: '-0.038199'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing row 230 and row 231 shows that they are highly positively correlated.
    Comparing rows 230 and 400 shows a weaker and, in this case, negative correlation.
    Finally, as we might expect, correlation with a random vector gives a value approaching
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: The Pearson correlation coefficient is so widely used that you’ll often see
    it referred to as merely *the correlation coefficient*. Let’s now take a look
    at a second correlation function and see how it differs from the Pearson coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Spearman Correlation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second correlation measure we’ll explore is the *Spearman correlation coefficient*,
    ρ ϵ [–1, +1]. It’s a measure based on the ranks of the feature values instead
    of the values themselves.
  prefs: []
  type: TYPE_NORMAL
- en: To rank *X*, we replace each value in *X* with the index to that value in the
    sorted version of *X*. If *X* is
  prefs: []
  type: TYPE_NORMAL
- en: '[86, 62, 28, 43, 3, 92, 38, 87, 74, 11]'
  prefs: []
  type: TYPE_NORMAL
- en: then the ranks are
  prefs: []
  type: TYPE_NORMAL
- en: '[7, 5, 2, 4, 0, 9, 3, 8, 6, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: because when *X* is sorted, 86 goes in the eighth place (counting from zero),
    and 3 goes first.
  prefs: []
  type: TYPE_NORMAL
- en: The Pearson correlation looks for a linear relationship, whereas the Spearman
    looks for any monotonic association between the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: If we have the ranks for the feature values, then the Spearman coefficient is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *n* is the number of samples and *d* = rank(*X*) – rank(*Y*) is the difference
    of the rank of the paired *X* and *Y* values. Note how [Equation 4.6](ch04.xhtml#ch04equ06)
    is only valid if the rankings are unique (that is, there are no repeated values
    in *X* or *Y*).
  prefs: []
  type: TYPE_NORMAL
- en: To calculate *d* in [Equation 4.6](ch04.xhtml#ch04equ06), we need to rank *X*
    and *Y* and use the difference of the ranks. The Spearman correlation is the Pearson
    correlation of the ranks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example above points the way to an implementation of the Spearman correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: 'def spearman(x,y):'
  prefs: []
  type: TYPE_NORMAL
- en: n = len(x)
  prefs: []
  type: TYPE_NORMAL
- en: t = x[np.argsort(x)]
  prefs: []
  type: TYPE_NORMAL
- en: rx = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(n):'
  prefs: []
  type: TYPE_NORMAL
- en: rx.append(np.where(x[i] == t)[0][0])
  prefs: []
  type: TYPE_NORMAL
- en: rx = np.array(rx, dtype="float64")
  prefs: []
  type: TYPE_NORMAL
- en: t = y[np.argsort(y)]
  prefs: []
  type: TYPE_NORMAL
- en: ry = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(n):'
  prefs: []
  type: TYPE_NORMAL
- en: ry.append(np.where(y[i] == t)[0][0])
  prefs: []
  type: TYPE_NORMAL
- en: ry = np.array(ry, dtype="float64")
  prefs: []
  type: TYPE_NORMAL
- en: d = rx - ry
  prefs: []
  type: TYPE_NORMAL
- en: return 1.0 - (6.0/(n*(n*n-1)))*(d**2).sum()
  prefs: []
  type: TYPE_NORMAL
- en: To get the ranks, we need to first sort *X* (t). Then, for each value in *X*
    (x), we find where it occurs in t via np.where and take the first element, the
    first match. After building the rx list, we make it a floating-point NumPy array.
    We do the same for *Y* to get ry. With the ranks, d is set to their difference,
    and [Equation 4.6](ch04.xhtml#ch04equ06) is used to return the Spearman ρ value.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this version of the Spearman correlation is limited by [Equation
    4.6](ch04.xhtml#ch04equ06) and should be used when there are no duplicate values
    in *X* or *Y*. Our example in this section uses random floating-point values,
    so the probability of an exact duplicate is quite low.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll compare our spearman implementation to the SciPy version, stats .spearmanr.
    Like the SciPy version of the Pearson correlation, stats.spearmanr returns a *p*-value.
    We’ll ignore it. Let’s see how our function compares:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from scipy.stats import spearmanr'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(spearman(x,y), spearmanr(x,y)[0])'
  prefs: []
  type: TYPE_NORMAL
- en: 0.694017401740174 0.6940174017401739
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(spearman(x,z), spearmanr(x,z)[0])'
  prefs: []
  type: TYPE_NORMAL
- en: -0.8950855085508551 -0.895085508550855
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(spearman(y,z), spearmanr(y,z)[0])'
  prefs: []
  type: TYPE_NORMAL
- en: -0.6414041404140414 -0.6414041404140414
  prefs: []
  type: TYPE_NORMAL
- en: We have complete agreement with the SciPy function out to the last bit or so
    of the floating-point value.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s important to remember the fundamental difference between the Pearson and
    Spearman correlations. For example, consider the correlation between a linear
    ramp and the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: ramp = np.linspace(-20,20,1000)
  prefs: []
  type: TYPE_NORMAL
- en: sig = 1.0 / (1.0 + np.exp(-ramp))
  prefs: []
  type: TYPE_NORMAL
- en: print(pearson(ramp,sig))
  prefs: []
  type: TYPE_NORMAL
- en: print(spearman(ramp,sig))
  prefs: []
  type: TYPE_NORMAL
- en: Here, ramp increases linearly from –20 to 20 and sig follows a sigmoid shape
    (“S” curve). The Pearson correlation will be on the high side, since both are
    increasing as *x* becomes more positive, but the association is not purely linear.
    Running the example gives
  prefs: []
  type: TYPE_NORMAL
- en: '0.905328'
  prefs: []
  type: TYPE_NORMAL
- en: '1.0'
  prefs: []
  type: TYPE_NORMAL
- en: indicating a Pearson correlation of 0.9 but a perfect Spearman correlation of
    1.0, since for every increase in ramp there is an increase in sig and *only* an
    increase. The Spearman correlation has captured the nonlinear relationship between
    the arguments, while the Pearson correlation has only hinted at it. If we’re analyzing
    a dataset intended for a classical machine learning algorithm, the Spearman correlation
    might help us decide which features to keep and which to discard.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our examination of statistics for describing and understanding
    data. Let’s now learn how to use hypothesis testing to interpret experimental
    results and answer questions like “Are these two sets of data samples from the
    same parent distribution?”
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have two independent sets of 50 students studying cell biology. We have no
    reason to believe the groups differ in any significant way, as students from the
    larger population were assigned randomly. Group 1 attended the lectures and, in
    addition, worked through a structured set of computer exercises. Group 2 only
    attended the lectures. Both groups took the same final examination, leading to
    the test scores given in [Table 4-1](ch04.xhtml#ch04tab01). We want to know if
    asking the students to work through the computer exercises made a difference in
    their final test scores.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-1:** Group 1 and Group 2 Test Scores'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Group 1** | 81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88
    89 92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85
    85 78 94 100 |'
  prefs: []
  type: TYPE_TB
- en: '| **Group 2** | 92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86
    77 94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88
    91 83 85 73 |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 4-7](ch04.xhtml#ch04fig07) shows a box plot of [Table 4-1](ch04.xhtml#ch04tab01).'
  prefs: []
  type: TYPE_NORMAL
- en: To understand if there is a significant change in final test scores between
    the two groups, we need to test some hypotheses. The method we’ll use to test
    the hypotheses is known as *hypothesis testing*, and it’s a critical piece of
    modern science.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-7: Box plot for the data in [Table 4-1](ch04.xhtml#ch04tab01)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis testing is a broad topic, too extensive for us to provide more than
    a minimal introduction here. As this is a book on deep learning, we’ll focus on
    the scenario a deep learning researcher is likely to encounter. We’ll consider
    only two hypothesis tests: the t-test for unpaired samples of differing variance
    (a parametric test) and the Mann-Whitney U (a nonparametric test). As we progress,
    we’ll understand what these tests are and why we’re restricting ourselves to them,
    as well as the meaning of *parametric* and *nonparametric*.'
  prefs: []
  type: TYPE_NORMAL
- en: To be successful with hypothesis testing, we need to know what we mean by *hypothesis*,
    so we’ll address that first, along with our rationale for limiting the types of
    hypothesis testing we’ll consider. With the hypothesis concept in hand, we’ll
    discuss the t-test and the Mann-Whitney U test in turn, using the data in [Table
    4-1](ch04.xhtml#ch04tab01) as our example. Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Hypotheses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand if two sets of data are from the same parent distribution or not,
    we might look at summary statistics. [Figure 4-7](ch04.xhtml#ch04fig07) shows
    us the box plot for Group 1 and Group 2\. It appears that the two groups have
    different means and standard deviations. How do we know? The box plot shows us
    the location of the medians, and the whiskers tell us something about the variance.
    Both of these together hint that the means will be different because the medians
    are different, and both sets of data are reasonably symmetric around the median.
    The space between the whiskers hints at the standard deviation. So, let’s make
    hypotheses using the means of the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In hypothesis testing, we have two hypotheses. The first, known as the *null
    hypothesis* (*H*[0]), is that the two sets of data *are* from the same parent
    distribution, that there is nothing special to differentiate them. The second
    hypothesis, the *alternative hypothesis* (*H[a]*), is that the two groups are
    not from the same distribution. Since we’ll be using the means, *H*[0] is saying
    that the means, really the means of the parent population that generated the data,
    are the same. Similarly, if we reject *H*[0], we are implicitly accepting *H[a]*
    and claiming we have evidence that the means are different. We don’t have the
    true population means, so we’ll use the sample means and standard deviations instead.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothesis testing doesn’t tell us definitively whether *H*[0] is true. Instead,
    it gives us evidence in favor of rejecting or accepting the null hypothesis. It’s
    critical to remember this.
  prefs: []
  type: TYPE_NORMAL
- en: We’re testing two independent samples to see if we should think of them as coming
    from the same parent distribution. There are other ways to use hypothesis testing,
    but we rarely encounter them in deep learning. For the task at hand, we need the
    sample means and the sample standard deviations. Our tests will ask the question,
    “Is there a meaningful difference in the means of these two sets?”
  prefs: []
  type: TYPE_NORMAL
- en: We’re only interested in detecting whether the two groups of data are from the
    same parent distribution, so another simplification we’ll make is that all of
    our tests will be *two-sided*, or *two-tailed*. When we use a test, like the t-test
    we’ll describe next, we’re comparing our calculated test statistic (the t-value)
    to the distribution of the test statistic and asking questions about how likely
    our calculated t-value is. If we want to know about the test statistic being above
    or below some fraction of that distribution, we’re making a two-sided test. If
    instead we want to know about the likelihood of the test statistic being above
    a particular value without caring about it being below, or vice versa, then we’re
    making a one-sided test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s lay out our assumptions and approach:'
  prefs: []
  type: TYPE_NORMAL
- en: We have two independent sets of data we wish to compare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re making no assumption as to whether the standard deviations of the data
    are the same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our null hypothesis is that the means of the parent distributions of the datasets
    are the same, *H*[0] : μ[1] = μ[2]. We’ll use the sample means ![image](Images/094equ01a.jpg)
    and sample standard deviations (*s*[1], *s*[2]) to help us decide to accept or
    reject *H*[0].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hypothesis tests assume that the data is *independent and identically distributed
    (i.i.d.)*. We interpret this as a statement that the data is a fair random sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these assumptions understood, let’s start with the t-test, the most widely
    used hypothesis test.
  prefs: []
  type: TYPE_NORMAL
- en: The t-test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *t-test* depends on *t*, the test statistic. This statistic is compared
    to the t-distribution and used to generate a *p*-value, a probability we’ll use
    to reach a conclusion about *H*[0]. There’s a rich history behind the t-test and
    the related z-test that we’ll ignore here. I encourage you to dive more deeply
    into hypothesis testing when you have the chance or, at a minimum, review thoughtful
    articles about the proper way to do a hypothesis test and interpret its results.
  prefs: []
  type: TYPE_NORMAL
- en: The t-test is a *parametric* test. This means there are assumptions about the
    data and the distribution of the data. Specifically, the t-test assumes, beyond
    the data being i.i.d., that the distribution (histogram) of the data is normal.
    We’ve stated before that many physical processes do seem to follow a normal distribution,
    so there’s reason to think that data from actual measurements might do so.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to test if a dataset is normally distributed, but we’ll
    ignore them, as there’s some debate about the utility of such tests. Instead,
    I’ll (somewhat recklessly) suggest you use the t-test and the Mann-Whitney U test
    together to help make your decision about accepting or rejecting *H*[0]. Using
    both tests might lead to a situation where they disagree, where one test says
    there’s evidence against the null hypothesis and the other says there isn’t. In
    general, if the nonparametric test is claiming evidence against *H*[0], then one
    should probably accept that evidence regardless of the t-test result. If the t-test
    result is against *H*[0], but the Mann-Whitney U test isn’t, and you think the
    data is normal, then you might also accept the t-test result.
  prefs: []
  type: TYPE_NORMAL
- en: The t-test has different versions. We explicitly stated above that we’ll use
    a version designed for datasets of differing size and variance. The specific version
    of the t-test we’ll use is *Welch’s t-test*, which doesn’t assume the variance
    of the two datasets is the same.
  prefs: []
  type: TYPE_NORMAL
- en: The t-score for Welch’s t-test is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/095equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *n*[1] and *n*[2] are the size of the two groups.
  prefs: []
  type: TYPE_NORMAL
- en: The t-score, and an associated value known as the *degrees of freedom*, which
    is similar to but also different from the degrees of freedom mentioned above,
    generates the appropriate t-distribution curve. To get a *p*-value, we calculate
    the area under the curve, both above and below (positive and negative t-score),
    and return it. Since the integral of a probability distribution is 1, the total
    area under the tails from the positive and negative t-score value to positive
    and negative infinity will be the *p*-value. We’ll use the degrees of freedom
    below to help us calculate confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: What does the *p*-value tell us? It tells us the probability of seeing the difference
    between the two means we see, or larger, *if* the null hypothesis is true. Typically,
    if this probability is below some threshold we’ve chosen, we reject the null hypothesis
    and say we have evidence that the two groups have different means—that they come
    from different parent distributions. When we reject *H*[0], we say that the difference
    is *statistically significant*. The threshold for accepting/rejecting *H*[0] is
    called α, usually with α = 0.05 as a typical, if problematic, value. We’ll discuss
    why 0.05 is problematic below.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point to remember is that the *p*-value assumes the null hypothesis is
    true. It tells us the likelihood of a true *H*[0] giving us at least the difference
    we see, or greater, between the groups. If the *p*-value is small, that has two
    possible meanings: (1) the null hypothesis is false, or (2) a random sampling
    error has given us samples that fall outside what we might expect. Since the *p*-value
    assumes *H*[0] is true, a small *p*-value helps us believe less and less in (2)
    and boosts our confidence that (1) might be correct. However, the *p*-value alone
    cannot confirm (1); other knowledge needs to come into play.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I mentioned that using α = 0.05 is problematic. The main reason it’s problematic
    is that it’s too generous; it leads to too many rejections of a true null hypothesis.
    According to James Berger and Thomas Sellke in their article “Testing a Point
    Null Hypothesis: The Irreconcilability of *P* Values and Evidence” (*Journal of
    the American Statistical Association*, 1987), when α = 0.05, about 30 percent
    of true null hypotheses will be rejected. When we use something like α ≤ 0.001,
    the chance of falsely rejecting a true null hypothesis goes down to less than
    3 percent. The moral of the story is that *p* < 0.05 is not magic and, frankly,
    is unconvincing for a single study. Look for highly significant *p*-values of
    at least 0.001 or, preferably, much smaller. At *p*  = 0.05, all you have is a
    suggestion, and you should repeat the experiment. If repeated experiments all
    have a *p*-value around 0.05, then rejecting the null hypothesis begins to make
    sense.'
  prefs: []
  type: TYPE_NORMAL
- en: Confidence Intervals
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Along with a *p*-value, you’ll often see *confidence intervals (CIs)*. The confidence
    interval gives bounds within which we believe the true population difference in
    the means will lie, with a given confidence for repeated samples of the two datasets
    we’re comparing. Typically, we report 95 percent confidence intervals. Our hypothesis
    tests check for equality of means by asking if the difference of the sample means
    is zero or not. Therefore, any CI that includes zero signals to us that we cannot
    reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: For Welch’s t-test, the degrees of freedom is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: which we can use to calculate confidence intervals,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *t*[1–α/2,*df*] is the critical value, and the t-value for the given confidence
    level (α) and the degrees of freedom, *df*, come from [Equation 4.7](ch04.xhtml#ch04equ07).
  prefs: []
  type: TYPE_NORMAL
- en: 'How should we interpret the 95 percent confidence interval? There is a population
    value: the true difference between the group means. The 95 percent confidence
    interval is such that if we could draw repeated samples from the distribution
    that produced the two datasets, 95 percent of the calculated confidence intervals
    would contain the true difference between the means. It is *not* the range that
    includes the true difference in the means at 95 percent certainty.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond checking if zero is in the CI, the CI is useful because its width tells
    us something about the magnitude of the effect. Here, the effect is related to
    the difference between the means. We may have a statistically significant difference
    based on the *p*-value, but the effect might be practically meaningless. The CI
    will be narrow when the effect is large because small CIs imply a narrow range
    encompassing the true effect. We’ll see shortly how, when possible, to calculate
    another useful measure of effect.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a *p*-value less than α also will have a *CI*[α] that does not include
    *H*[0]. In other words, what the *p*-value tells us and what the confidence interval
    tells us track–they will not contradict each other.
  prefs: []
  type: TYPE_NORMAL
- en: Effect Size
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s one thing to have a statistically significant *p*-value. It’s another for
    the difference represented by that *p*-value to be meaningful in the real world.
    A popular measure of the size of an effect, the *effect size*, is *Cohen’s d*.
    For us, since we’re using Welch’s t-test, Cohen’s *d* is found by calculating
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cohen’s *d* is usually interpreted subjectively, though we should report the
    numeric value as well. Subjectively, the size of the effect could be
  prefs: []
  type: TYPE_NORMAL
- en: '| ***d*** | **Effect** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | Small |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | Medium |'
  prefs: []
  type: TYPE_TB
- en: '| 0.8 | Large |'
  prefs: []
  type: TYPE_TB
- en: Cohen’s *d* makes sense. The difference between the means is a natural way to
    think about the effect. Scaling it by the mean variance puts it in a consistent
    range. From [Equation 4.9](ch04.xhtml#ch04equ09), we see that a *p*-value corresponding
    to a statistically significant result might lead to a small effect that isn’t
    of any true practical importance.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Test Scores
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s put all of the above together to apply the t-test to our test data from
    [Table 4-1](ch04.xhtml#ch04tab01). You’ll find the code in the file *hypothesis.py*.
    We generate the data-sets first:'
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed(65535)
  prefs: []
  type: TYPE_NORMAL
- en: a = np.random.normal(85,6,50).astype("int32")
  prefs: []
  type: TYPE_NORMAL
- en: a[np.where(a > 100)] = 100
  prefs: []
  type: TYPE_NORMAL
- en: b = np.random.normal(82,7,50).astype("int32")
  prefs: []
  type: TYPE_NORMAL
- en: b[np.where(b > 100)] = 100
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we’re using a fixed NumPy pseudorandom number seed for repeatability.
    We make a a sample from a normal distribution with a mean of 85 and a standard
    deviation of 6.0\. We select b from a normal distribution with a mean of 82 and
    a standard deviation of 7.0\. For both, we cap any values over 100 to 100\. These
    are test scores, after all, without extra credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the t-test next:'
  prefs: []
  type: TYPE_NORMAL
- en: from scipy.stats import ttest_ind
  prefs: []
  type: TYPE_NORMAL
- en: t,p = ttest_ind(a,b, equal_var=False)
  prefs: []
  type: TYPE_NORMAL
- en: print("(t=%0.5f, p=%0.5f)" % (t,p))
  prefs: []
  type: TYPE_NORMAL
- en: We get *(t* = 2.40234, *p* = 0.01852). The *t* is the statistic, and *p* is
    the computed *p*-value. It’s 0.019, which is less than 0.05 but only by a factor
    of two. We have a weak result telling us we might want to reject the null hypothesis
    and believe that the two groups, a and b, come from different distributions. Of
    course, we know they do because we generated them, but it’s nice to see the test
    pointing in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the function we import from SciPy is ttest_ind. This is the function
    to use for independent samples, which are not paired. Also, notice that we added
    equal_var=False to the call. This is how to use Welch’s t-test, which doesn’t
    assume that the variance between the two datasets is equal. We know they’re not
    equal, since a uses a standard deviation of 6.0 while b uses 7.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the confidence intervals, we’ll write a CI function, since NumPy and
    SciPy don’t include one. The function directly implements [Equations 4.7](ch04.xhtml#ch04equ07)
    and [4.8](ch04.xhtml#ch04equ08):'
  prefs: []
  type: TYPE_NORMAL
- en: from scipy import stats
  prefs: []
  type: TYPE_NORMAL
- en: 'def CI(a, b, alpha=0.05):'
  prefs: []
  type: TYPE_NORMAL
- en: n1, n2 = len(a), len(b)
  prefs: []
  type: TYPE_NORMAL
- en: s1, s2 = np.std(a, ddof=1)**2, np.std(b, ddof=1)**2
  prefs: []
  type: TYPE_NORMAL
- en: df = (s1/n1 + s2/n2)**2 / ((s1/n1)**2/(n1-1) + (s2/n2)**2/(n2-1))
  prefs: []
  type: TYPE_NORMAL
- en: tc = stats.t.ppf(1 - alpha/2, df)
  prefs: []
  type: TYPE_NORMAL
- en: lo = (a.mean()-b.mean()) - tc*np.sqrt(s1/n1 + s2/n2)
  prefs: []
  type: TYPE_NORMAL
- en: hi = (a.mean()-b.mean()) + tc*np.sqrt(s1/n1 + s2/n2)
  prefs: []
  type: TYPE_NORMAL
- en: return lo, hi
  prefs: []
  type: TYPE_NORMAL
- en: The critical *t* value is given by calling stats.t.ppf, passing in the α/2 value
    and the proper degrees of freedom, *df*. The critical *t* value is the 97.5 percent
    percentile value, for α = 0.05, which is what the *percent point function (ppf)*
    returns. We divide by two to cover the tails of the t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our test example, the confidence interval is [0.56105, 5.95895]. Notice
    how this does not include zero, so the CI also indicates a statistically significant
    result. However, the range is rather large, so this is not a particularly robust
    result. The CI range can be difficult to interpret on its own, so, finally, let’s
    calculate Cohen’s *d* to see if it makes sense given the width of the confidence
    interval. In code, we implement [Equation 4.9](ch04.xhtml#ch04equ09):'
  prefs: []
  type: TYPE_NORMAL
- en: 'def Cohen_d(a,b):'
  prefs: []
  type: TYPE_NORMAL
- en: s1 = np.std(a, ddof=1)**2
  prefs: []
  type: TYPE_NORMAL
- en: s2 = np.std(b, ddof=1)**2
  prefs: []
  type: TYPE_NORMAL
- en: return (a.mean() - b.mean()) / np.sqrt(0.5*(s1+s2))
  prefs: []
  type: TYPE_NORMAL
- en: We get *d* = 0.48047, corresponding to a medium effect size.
  prefs: []
  type: TYPE_NORMAL
- en: The Mann-Whitney U Test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The t-test assumes the distribution of the source data is normal. If the data
    is not normally distributed, we should instead use a *nonparametric test*. Nonparametric
    tests make no assumptions about the underlying distribution of the data. The *Mann-Whitney
    U test*, sometimes called the *Wilcoxon rank-sum test*, is a nonparametric test
    to help decide if two different sets of data come from the same parent distribution.
    The Mann-Whitney U test does not rely directly on the values of the data, but
    instead uses the data’s ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 'The null hypothesis for this test is the following: the probability that a
    randomly selected value from Group 1 is larger than a randomly selected value
    from Group 2 is 0.5\. Let’s think a bit about that. If the data is from the same
    parent distribution, then we should expect any randomly selected pair of values
    from the two groups to show no preference as to which is larger than the other.'
  prefs: []
  type: TYPE_NORMAL
- en: The alternative hypothesis is that the probability of a randomly selected value
    from Group 1 being larger than a randomly selected value from Group 2 is not 0.5\.
    Notice, there is no statement as to the probability being greater or less than
    0.5, only that it isn’t 0.5; thus, the Mann-Whitney U test, as we’ll use it, is
    two-sided.
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis for the Mann-Whitney U test is not the same as the null
    hypothesis for the t-test. For the t-test, we’re asking whether the means between
    the two groups are the same. (Really, we’re asking if the difference in the means
    is zero.) However, if two sets of data *are* from different parent distributions,
    both null hypotheses are false, so we can use the Mann-Whitney U test in place
    of the t-test, especially when the underlying data is not normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: To generate *U*, the Mann-Whitney statistic, we first pool both sets of data
    and rank them. Ties are replaced with the mean between the tie value rank and
    the next rank value. We also keep track of the source group so we can separate
    the list of ranks again. The ranks, by group, are summed to give *R*[1] and *R*[2]
    (using the ranks from the pooled data). We calculate two values,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/100equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: with the smaller called *U*, the test statistic. It’s possible to generate a
    *p*-value from *U*, keeping in mind all the discussion above about the meaning
    and use of *p*-values. As before, *n*[1] and *n*[2] are the number of samples
    in the two groups. The Mann-Whitney U test requires the smaller of these two numbers
    to be at least 21 samples. If you don’t have that many, the results may not be
    reliable when using the SciPy mannwhitneyu function.
  prefs: []
  type: TYPE_NORMAL
- en: We can run the Mann-Whitney U test on our test data from [Table 4-1](ch04.xhtml#ch04tab01),
  prefs: []
  type: TYPE_NORMAL
- en: from scipy.stats import mannwhitneyu
  prefs: []
  type: TYPE_NORMAL
- en: u,p = mannwhitneyu(a,b)
  prefs: []
  type: TYPE_NORMAL
- en: print("(U=%0.5f, p=%0.5f)" % (u,p))
  prefs: []
  type: TYPE_NORMAL
- en: with a and b as we used above for the t-test. This gives us *(U* = 997.00000,
    *p* = 0.04058). The *p*-value is barely below the minimum threshold of 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: The means of a and b are 85 and 82, respectively. What happens to the *p*-values
    if we make the mean value of b 83 or 81? Changing the mean of b means changing
    the first argument to np.random.normal. Doing this gives us [Table 4-2](ch04.xhtml#ch04tab02),
    where I’ve included all results for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-2:** Mann-Whitney U Test and t-test Results for the Simulated Test
    Scores with Different Means (*n*[1]=*n*[2]=50)'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Means** | **Mann-Whitney U** | **t-test** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 85 vs. 83 | (*U*=1104.50000, *p*=0.15839) | (*t*=1.66543, *p*=0.09959) |'
  prefs: []
  type: TYPE_TB
- en: '| 85 vs. 82 | (*U*=997.00000, *p*=0.04058) | (*t*=2.40234, *p*=0.01852) |'
  prefs: []
  type: TYPE_TB
- en: '| 85 vs. 81 | (*U*=883.50000, *p*=0.00575) | (*t*=3.13925, *p*=0.00234) |'
  prefs: []
  type: TYPE_TB
- en: '[Table 4-2](ch04.xhtml#ch04tab02) should make sense to us. When the means are
    close, it’s harder to tell them apart, so we expect larger *p*-values. Recall
    how we have only 50 samples in each group. As the difference between the means
    increases, the *p*-values go down. A difference of three in the means leads to
    barely significant *p*-values. When the difference is larger still, the *p*-values
    become truly significant—again, as we expect.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis above begs the question: for a small difference in the means between
    the two groups, how do the *p*-values change as a function of the sample size?'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-8](ch04.xhtml#ch04fig08) shows the *p*-value (mean ± standard error)
    over 25 runs for both the Mann-Whitney U test and the t-test as a function of
    sample size for the case where the means are 85 and 84.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-8: Mean p-value as a function of sample size for a difference in
    the sample means of one, ![image](Images/101equ01.jpg)*'
  prefs: []
  type: TYPE_NORMAL
- en: Small datasets make it difficult to differentiate between cases when the difference
    in the means is small. We also see that larger sample sizes reveal the difference,
    regardless of the test. It is interesting that in [Figure 4-8](ch04.xhtml#ch04fig08),
    the Mann-Whitney U *p*-value is less than that of the t-test even though the underlying
    data is normally distributed. Conventional wisdom states that it’s usually the
    other way around.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-8](ch04.xhtml#ch04fig08) is an object lesson in the power of large-sample
    tests to detect real differences. When the sample size is large enough, a weak
    difference becomes significant. However, we need to balance this with the effect
    size. When we have 1,000 samples in each group, we have a statistically significant
    *p*-value, but we also have a Cohen’s *d* of about 0.13, signaling a weak effect.
    A large sample study might find a significant effect that is so weak as to be
    practically meaningless.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter touched on the key aspects of statistics you’ll encounter during
    your sojourn through the world of deep learning. Specifically, we learned about
    different types of data and how to ensure the data is useful for building models.
    We then learned about summary statistics and saw examples that used them to help
    us understand a dataset. Understanding our data is key to successful deep learning.
    We investigated the different types of means, learned about measures of variation,
    and saw the utility of visualizing the data via box plots.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data is a bane of deep learning. In this chapter, we investigated how
    to compensate for missing data. Next, we discussed correlation, how to detect
    and measure the relationships between elements of a dataset. Finally, we introduced
    hypothesis testing. Restricting ourselves to the most likely scenario we’ll encounter
    in deep learning, we learned how to apply both the t-test and the Mann-Whitney
    U test. Hypothesis testing introduced us to the *p*-value. We saw examples of
    it and discussed how to interpret it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we’ll leave statistics behind and dive headfirst into the
    world of linear algebra. Linear algebra is how we implement neural networks.
  prefs: []
  type: TYPE_NORMAL
