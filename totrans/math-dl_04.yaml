- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: STATISTICS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Bad datasets lead to bad models. We’d like to understand our data before we
    build a model, and then use that understanding to create a useful dataset, one
    that leads to models that do what we expect them to do. Knowing basic statistics
    will enable us to do just that.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 不良的数据集会导致糟糕的模型。在构建模型之前，我们希望能够理解我们的数据，然后利用这种理解来创建一个有用的数据集，一个能够生成符合我们预期模型的数据集。了解基本的统计学将使我们能够做到这一点。
- en: A *statistic* is any number that’s calculated from a sample and used to characterize
    it in some way. In deep learning, when we talk about samples, we’re usually talking
    about datasets. Maybe the most basic statistic is the arithmetic mean, commonly
    known as the average. The mean of a dataset is a single-number summary of the
    dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*统计量*是从样本中计算得出的任何数字，用来以某种方式描述样本。在深度学习中，当我们谈论样本时，我们通常是在谈论数据集。也许最基本的统计量是算术平均数，通常称为平均值。数据集的平均值是对数据集的单一数字总结。
- en: We’ll see many different statistics in this chapter. We’ll begin by learning
    about the types of data and characterizing a dataset with summary statistics.
    Next, we’ll learn about quantiles and plotting data to understand what it contains.
    After that comes a discussion of outliers and missing data. Datasets are seldom
    perfect, so we need to have some way of detecting bad data and dealing with missing
    data. We’ll follow our discussion of imperfect datasets with a discussion of the
    correlation between variables. Then we’ll close the chapter out by discussing
    hypothesis testing, where we attempt to answer questions like “How likely is it
    that the same parent process generated two datasets?” Hypothesis testing is widely
    used in science, including deep learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到许多不同的统计学内容。我们将从学习数据类型和通过摘要统计量来描述数据集开始。接下来，我们将学习分位数和绘制数据图表，以便了解数据包含的内容。之后，我们将讨论离群值和缺失数据。数据集很少是完美的，所以我们需要某种方法来检测不良数据并处理缺失数据。我们将在讨论完不完美的数据集后，讨论变量之间的相关性。最后，我们将通过讨论假设检验来结束本章，假设检验帮助我们回答诸如“同一父进程生成两个数据集的可能性有多大？”这样的问题。假设检验在科学中广泛应用，包括深度学习。
- en: Types of Data
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据类型
- en: The four types of data are nominal, ordinal, interval, and ratio. Let’s look
    at each in turn.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 四种数据类型分别是名义数据、序数数据、区间数据和比率数据。我们将逐一查看每种数据类型。
- en: Nominal Data
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 名义数据
- en: '*Nominal data*, sometimes called *categorical data*, is data that has no ordering
    between the different values. An example of this is eye color; there is no relationship
    between brown, blue, and green.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*名义数据*，有时称为*类别数据*，是指没有不同值之间顺序的数据。例如，眼睛颜色就是一个例子；棕色、蓝色和绿色之间没有任何关系。'
- en: Ordinal Data
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序数数据
- en: For *ordinal data*, the data has a ranking or order, though differences aren’t
    meaningful in a mathematical sense. For example, if a questionnaire asks you to
    select from “strongly disagree,” “disagree,” “neutral,” “agree,” and “strongly
    agree,” it’s pretty clear that there is an order. Still, it’s also clear that
    “agree” isn’t three more than “strongly disagree.” All we can say is that “strongly
    disagree” is to the left of “agree” (and “neutral” and “disagree”).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*序数数据*，数据具有排名或顺序，尽管在数学意义上，差异并不具有实际意义。例如，如果问卷要求你选择“强烈不同意”、“不同意”、“中立”、“同意”和“强烈同意”，很明显这是一种顺序。然而，也很明显，“同意”并不比“强烈不同意”多三倍。我们能说的仅仅是“强烈不同意”位于“同意”的左侧（以及“中立”和“不同意”）。
- en: Another example of ordinal data is education level. If one person has a fourth-grade
    education and another has an eighth-grade education, we can say that the latter
    person is more educated than the former, but we can’t say that the latter person
    is twice as educated, because “twice as educated” has no fixed meaning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个序数数据的例子是教育水平。如果一个人有四年级的教育水平，而另一个人有八年级的教育水平，我们可以说后者的教育水平更高，但不能说后者的教育水平是前者的两倍，因为“教育水平是两倍”没有固定的意义。
- en: Interval Data
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 区间数据
- en: '*Interval data* has meaningful differences. For example, if one cup of water
    is at 40 degrees Fahrenheit and another is at 80 degrees Fahrenheit, we can say
    that there is a 40-degree difference between the two cups of water. We can’t,
    however, say that there is twice as much heat in the second cup, because the zero
    for the Fahrenheit scale is arbitrary. Colloquially, we do say it’s twice as hot,
    but in reality, it isn’t. To see this, think about what happens if we change the
    temperature scale to another scale with an arbitrary, though more sensible, zero:
    the Celsius scale. We see that the first cup is at about 4.4 degrees Celsius,
    and the second is at 26.7 degrees Celsius. Clearly, the second cup doesn’t suddenly
    now have six times the heat of the first.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*区间数据*具有有意义的差异。例如，如果一杯水的温度是40华氏度，另一杯是80华氏度，我们可以说两杯水之间有40度的差异。然而，我们不能说第二杯水的热量是第一杯的两倍，因为华氏度标尺的零点是任意设定的。口语上，我们确实会说第二杯水热得是第一杯的两倍，但实际上并非如此。要看清这一点，可以想象如果我们将温度标尺改成另一个更合理的标尺——摄氏度标尺。我们会看到第一杯水的温度大约是4.4摄氏度，第二杯是26.7摄氏度。显然，第二杯水并不会突然变成第一杯的六倍热。'
- en: Ratio Data
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比率数据
- en: Finally, *ratio data* is data where differences are meaningful, and there is
    a true zero point. Height is a ratio value because a height of zero is just that—no
    height at all. Similarly, age is also a ratio value because an age of zero means
    no age at all. If we were to adopt a new age scale and call a person zero when
    they reach, say, voting age, we’d then have an interval scale, not a ratio scale.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*比率数据*是指差异有意义且存在真实零点的数据。身高就是比率值，因为零身高就意味着完全没有身高。同样，年龄也是比率值，因为零年龄意味着没有年龄。如果我们采用一种新的年龄尺度，并将某人定义为零年龄，比如当他们达到投票年龄时，那么我们就得到一个区间尺度，而非比率尺度。
- en: Let’s look at temperature again. We said above that temperature is an interval
    quantity. This isn’t always the case. If we measure temperature in Fahrenheit
    or Celsius, then, yes, it is an interval quantity. However, if we measure temperature
    in Kelvin, the absolute temperature scale, then it becomes a ratio value. Why?
    Because a temperature of 0 Kelvin (or *K*) is just that, no temperature at all.
    If our first cup is at 40°F, 277.59 K, and the second is at 80°F, 299.82 K, then
    we can truthfully say that the second cup is 1.08 times hotter than the first,
    since (277.59)(1.08) ≈ 299.8.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看温度。我们在上面说过，温度是区间量，这并不总是正确的。如果我们用华氏度或摄氏度来测量温度，那么它确实是区间量。然而，如果我们使用绝对温标——开尔文度来测量温度，那么它就变成了比率值。为什么？因为0开尔文（或*K*）表示的就是完全没有温度。如果我们的第一杯水温是40°F（277.59
    K），第二杯是80°F（299.82 K），那么我们可以真实地说第二杯水比第一杯热1.08倍，因为（277.59）（1.08）≈ 299.8。
- en: '[Figure 4-1](ch04.xhtml#ch04fig01) names the scales and shows their relationships
    to each other.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](ch04.xhtml#ch04fig01)列出了数据量表并展示了它们之间的关系。'
- en: '![image](Images/04fig01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig01.jpg)'
- en: '*Figure 4-1: The four types of data*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-1：四种数据类型*'
- en: Each step in [Figure 4-1](ch04.xhtml#ch04fig01) from left to right adds something
    to the data that the type of data on the left is lacking. For nominal to ordinal,
    we add ordering. For ordinal to interval, we add meaningful differences. Lastly,
    moving from interval to ratio adds a true zero point.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](ch04.xhtml#ch04fig01)从左到右的每一步都为数据添加了缺失的部分。对于从名义数据到有序数据，我们增加了顺序性。对于从有序数据到区间数据，我们增加了有意义的差异。最后，从区间数据到比率数据则增加了一个真实的零点。'
- en: In practical use, as far as statistics are concerned, we should be aware of
    the types of data so we don’t do something meaningless. If we have a questionnaire,
    and the mean value of question A on a 1-to-5 rating scale is 2, while for question
    B it’s 4, we can’t say that B is rated twice as high as A, only that B was rated
    higher than A. What “twice” means in this context is unclear and quite probably
    meaningless.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，统计学方面我们应该了解数据的类型，以免做出没有意义的操作。如果我们有一个问卷，问题A的平均值在1到5的评分尺度上是2，而问题B的平均值是4，我们不能说B的评分是A的两倍，只能说B的评分高于A。在这种情况下，“两倍”是什么意思并不明确，而且很可能是没有意义的。
- en: Interval and ratio data may be continuous (floating-points) or discrete (integers).
    From a deep learning perspective, models typically treat continuous and discrete
    data the same way, and we don’t need to do anything special for discrete data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 区间数据和比率数据可以是连续的（浮动点数）或离散的（整数）。从深度学习的角度来看，模型通常将连续数据和离散数据视为相同，不需要对离散数据做特别处理。
- en: Using Nominal Data in Deep Learning
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用名义数据进行深度学习
- en: If we have a nominal value, say a set of colors, such as red, green, and blue,
    and we want to pass that value into a deep network, we need to change the data
    before we can use it. As we just saw, nominal data has no order, so while it’s
    tempting to assign a value of 1 to red, 2 to green, and 3 to blue, it would be
    wrong to do so, since the network will interpret those numbers as interval data.
    In that case, to the network, blue = 3(red), which is of course nonsense. If we
    want to use nominal data with a deep network, we need to alter it so that the
    interval is meaningful. We do this with *one-hot encoding*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个名义值，例如一组颜色，如红色、绿色和蓝色，我们想将该值传递给深度网络，那么在使用之前，我们需要先修改数据。正如我们刚才看到的，名义数据没有顺序性，因此虽然将红色赋值为
    1，绿色为 2，蓝色为 3 看起来很有诱惑力，但这样做是错误的，因为网络会将这些数字解释为区间数据。在这种情况下，对网络来说，蓝色 = 3（红色），这显然是没有意义的。如果我们想要将名义数据用于深度网络，就需要改变它，使得间隔是有意义的。我们通过
    *独热编码* 来做到这一点。
- en: 'In one-hot encoding, we turn the single nominal variable into a vector, where
    each element of the vector corresponds to one of the nominal values. For the color
    example, the one nominal variable becomes a three-element vector with one element
    representing red, another green, and the last blue. Then, we set the value corresponding
    to the color to one and all the others to zero, like so:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在独热编码中，我们将单一的名义变量转换为一个向量，其中向量的每个元素对应名义值之一。以颜色为例，单一名义变量变成一个三元素的向量，表示红色的元素为一个，绿色为另一个，最后一个表示蓝色。然后，我们将对应颜色的值设置为
    1，其余的设置为 0，如下所示：
- en: '| **Value** |  | **Vector** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **值** |  | **向量** |'
- en: '| --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| red | → | 1 0 0 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 红色 | → | 1 0 0 |'
- en: '| green | → | 0 1 0 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 绿色 | → | 0 1 0 |'
- en: '| blue | → | 0 0 1 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 蓝色 | → | 0 0 1 |'
- en: Now the vector values are meaningful because either it’s red (1) or it’s not
    (0), green (1) or it’s not (0), or blue (1) or it’s not (0). The interval between
    zero and one has mathematical meaning because the presence of the value, say red,
    is genuinely greater than its absence, and that works the same way for each color.
    The values are now interval, so the network can use them. In some toolkits, like
    Keras, class labels are one-hot encoded before passing them to the model. This
    is done so vector output operates nicely with the one-hot encoded class label
    when computing the loss function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，向量的值是有意义的，因为它要么是红色（1），要么不是（0）；要么是绿色（1），要么不是（0）；要么是蓝色（1），要么不是（0）。零和一之间的间隔具有数学意义，因为例如红色的存在（1）确实大于其不存在（0），对每种颜色都是如此。现在这些值是区间数据，因此网络可以使用它们。在一些工具包中，比如
    Keras，类别标签在传递给模型之前会进行独热编码。这样做是为了让向量输出在计算损失函数时能与独热编码的类别标签良好配合。
- en: Summary Statistics
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汇总统计
- en: We’re given a dataset. How do we make sense of it? How should we characterize
    it to understand it better before we use it to build a model?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个数据集。我们如何理解它？在用它构建模型之前，应该如何描述它，以便更好地理解它？
- en: To answer these questions, we need to learn about *summary statistics*. Calculating
    summary statistics should be the first thing you do when handed a new dataset.
    Not looking at your dataset before building a model is like buying a used car
    without checking the tires, taking it for a test drive, and looking under the
    hood.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我们需要了解 *汇总统计*。计算汇总统计是当你得到一个新数据集时应该做的第一件事。在构建模型之前不查看数据集，就像买了一辆二手车却没有检查轮胎、没有试驾，也不看发动机一样。
- en: 'People have different notions of what makes a good set of summary statistics.
    We’ll focus on the following: means; the median; and measures of variation, including
    variance, standard deviation, and standard error. The range and mode are also
    often mentioned. The *range* is the difference between the maximum and minimum
    of the dataset. The *mode* is the most frequent value in the dataset. We generally
    get a sense of the mode visually from the histogram, as the histogram shows us
    the shape of the distribution of the data.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人对什么是好的汇总统计有不同的看法。我们将重点关注以下内容：均值；中位数；以及变化度量，包括方差、标准差和标准误差。范围和众数也常被提到。*范围*
    是数据集中的最大值与最小值之间的差值。*众数* 是数据集中出现频率最高的值。我们通常可以通过直方图从视觉上了解众数，因为直方图展示了数据的分布形状。
- en: Means and Median
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均值和中位数
- en: 'Most of us learned how to calculate the average of a set of numbers in elementary
    school: add the numbers and divide by how many there are. This is the *arithmetic
    mean*, or, more specifically, the *unweighted* arithmetic mean. If the dataset
    consists of a set of values, *{x[0]*, *x*[1], *x*[2], . . . , *x[n–1]*}, then
    the arithmetic mean is the sum of the data divided by the number of elements in
    the dataset (*n*). Notationally, we write this as the following.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人在小学时就学会了如何计算一组数字的平均值：将数字相加，再除以数字的个数。这就是 *算术均值*，更具体地说，是 *无权重* 算术均值。如果数据集由一组值组成，*{x[0]*,
    *x*[1], *x*[2], . . . , *x[n–1]*}，那么算术均值是数据的总和除以数据集中元素的个数 (*n*)。符号表示为：
- en: '![image](Images/04equ01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ01.jpg)'
- en: The ![image](Images/xbar.jpg) is the typical way to denote the mean of a sample.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![image](Images/xbar.jpg) 是表示样本均值的典型方式。'
- en: '[Equation 4.1](ch04.xhtml#ch04equ01) calculates the unweighted mean. Each value
    is given a weight of 1/*n*, where the sum of all the weights is 1.0\. Sometimes,
    we might want to weight elements of the dataset differently; in other words, not
    all of them should count equally. In that case, we calculate a weighted mean,'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 4.1](ch04.xhtml#ch04equ01) 计算的是无权重的均值。每个值都被赋予一个权重 1/*n*，其中所有权重的总和为 1.0。有时，我们可能希望对数据集的元素赋予不同的权重；换句话说，并非所有元素都应当平等计数。在这种情况下，我们计算加权均值，'
- en: '![image](Images/071equ01.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ01.jpg)'
- en: where *w[i]* is the weight given to *x[i]* and Σ*[i]w[i]* = 1\. The weights
    are not part of the dataset; they need to come from somewhere else. The grade
    point average (GPA) used by many universities is an example of a weighted mean.
    The grade for each course is multiplied by the number of course credits, and the
    sum is divided by the total number of credits. Algebraically, this is equivalent
    to multiplying each grade by a weight, *w[i]* = *c[i]*/Σ*[i]**c[i]*, with *c[i]*
    the number of credits for course *i* and Σ*[i]**c[i]* the total number of credits
    for the semester.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w[i]* 是赋给 *x[i]* 的权重，Σ*[i]w[i]* = 1。权重不是数据集的一部分；它们需要来自其他地方。许多大学使用的平均成绩点（GPA）就是一个加权均值的例子。每门课程的成绩乘以该课程的学分数，然后将所有结果相加，再除以总学分数。在代数上，这等同于将每个成绩乘以一个权重，*w[i]*
    = *c[i]* / Σ*[i]**c[i]*，其中 *c[i]* 是课程 *i* 的学分数，Σ*[i]**c[i]* 是该学期的总学分数。
- en: Geometric Mean
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 几何均值
- en: 'The arithmetic mean is by far the most commonly used mean. However, there are
    others. The *geometric mean* of two positive numbers, *a* and *b*, is the square
    root of their product:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 算术均值是最常用的均值。然而，还有其他类型的均值。两个正数 *a* 和 *b* 的 *几何均值* 是它们乘积的平方根：
- en: '![image](Images/071equ02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ02.jpg)'
- en: 'In general, the geometric mean of *n* positive numbers is the *n*th root of
    their product:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*n* 个正数的几何均值是它们乘积的 *n* 次方根：
- en: '![image](Images/071equ03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ03.jpg)'
- en: The geometric mean is used in finance to calculate average growth rates. In
    image processing, the geometric mean can be used as a filter to help reduce image
    noise. In deep learning, the geometric mean appears in the *Matthews correlation
    coefficient (MCC)*, one of the metrics we use to evaluate deep learning models.
    The MCC is the geometric mean of two other metrics, the informedness and the markedness.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 几何均值在金融学中用于计算平均增长率。在图像处理中，几何均值可以用作滤波器，帮助减少图像噪声。在深度学习中，几何均值出现在 *Matthews 相关系数（MCC）*
    中，这是我们用来评估深度学习模型的一个指标。MCC 是两个其他指标的几何均值，即信息度和标记度。
- en: Harmonic Mean
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调和均值
- en: 'The *harmonic mean* of two numbers, *a* and *b*, is the reciprocal of the arithmetic
    mean of their reciprocals:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数 *a* 和 *b* 的 *调和均值* 是它们倒数算术均值的倒数：
- en: '![image](Images/071equ04.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ04.jpg)'
- en: In general,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，
- en: '![image](Images/072equ01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/072equ01.jpg)'
- en: 'The harmonic mean shows up in deep learning as the F1 score. This is a frequently
    used metric for evaluating classifiers. The F1 score is the harmonic mean of the
    recall (sensitivity) and the precision:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调和均值出现在深度学习中作为 F1 分数。这是评估分类器时常用的一个指标。F1 分数是召回率（灵敏度）和精确度的调和均值：
- en: '![image](Images/072equ02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/072equ02.jpg)'
- en: 'Despite its frequent use, it’s not a good idea to use the F1 score to evaluate
    a deep learning model. To see this, consider the definitions of recall and precision:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 F1 分数被频繁使用，但不建议使用它来评估深度学习模型。要理解这一点，考虑召回率和精确度的定义：
- en: '![image](Images/072equ03.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/072equ03.jpg)'
- en: Here, TP is the number of true positives, FN is the number of false negatives,
    and FP is the number of false positives. These values come from the test set used
    to evaluate the model. A fourth number that’s important for classifiers, TN, is
    the number of correctly classified true negatives (assuming a binary classifier).
    The F1 score ignores TN, but to understand how well the model performs, we need
    to consider both positive and negative classifications. Therefore, the F1 score
    is misleading and often too optimistic. Better metrics are the MCC mentioned above
    or Cohen’s κ (kappa), which is similar to MCC and usually tracks it closely.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，TP是正确分类为正样本的数量，FN是错误分类为负样本的数量，FP是错误分类为正样本的数量。这些值来自用于评估模型的测试集。对于分类器来说，另一个重要的数字是TN，它是正确分类为负样本的数量（假设是二分类器）。F1得分忽略了TN，但为了理解模型的表现，我们需要同时考虑正负分类。因此，F1得分可能具有误导性，通常过于乐观。更好的度量标准是上面提到的MCC或Cohen’s
    κ（卡帕系数），它与MCC类似，通常会紧密跟踪MCC的变化。
- en: Median
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 中位数
- en: 'Before moving on to measures of variation, there’s one more commonly used summary
    statistic we’ll mention here. It’ll show up again a little later in the chapter
    too. The *median* of a dataset is the middle value. It’s the value where, when
    the dataset is sorted numerically, half the values are below it and half are above
    it. Let’s use this dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论变异度量之前，我们需要提到另一个常用的汇总统计量，它将在本章稍后再次出现。数据集的*中位数*是中间值。当数据集按数字顺序排序时，中位数是值位于其中一半数据点的上方和另一半数据点的下方的位置。我们使用这个数据集：
- en: '*X* = {55,63,65,37,74,71,73,87,69,44}'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* = {55,63,65,37,74,71,73,87,69,44}'
- en: If we sort *X*, we get
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对*X*进行排序，得到：
- en: '{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}'
- en: We immediately see a potential problem. I said we need the middle value when
    the data is sorted. With 10 things in *X*, there is no middle value. The middle
    lies between 65 and 69\. When the number of elements in the data-set is even,
    the median is the arithmetic mean of the two middle numbers. Therefore, the median
    in this case is
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立刻看到一个潜在的问题。我提到过，当数据排序时我们需要找到中间值。对于10个数据点的*X*，并没有一个明确的中间值。中间值位于65和69之间。当数据集中的元素数量为偶数时，中位数是两个中间数字的算术平均值。因此，在这种情况下，中位数是
- en: '![image](Images/073equ01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/073equ01.jpg)'
- en: The arithmetic mean of the data is 63.8\. What’s the difference between the
    mean and the median?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的算术均值是63.8。均值和中位数之间有什么区别？
- en: By design, the median tells us the value that splits the dataset, so the number
    of samples above equals the number below. It’s the number of samples that matters.
    For the mean, it’s a sum over the actual data values. Therefore, the mean is sensitive
    to the values themselves, while the median is sensitive to the ordering of the
    values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从设计上讲，中位数告诉我们将数据集分开的值，因此，上方的样本数量与下方的样本数量相等。这里关注的是样本数量。对于均值来说，它是所有实际数据值的总和。因此，均值对值本身非常敏感，而中位数则对值的排序更为敏感。
- en: If we look at *X*, we see that most values are in the 60s and 70s, with one
    low value of 37\. It’s the low value of 37 that drags the mean down relative to
    the median. An excellent example of this effect is income. The current median
    annual family income in the United States is about $62,000\. A recent measure
    of the mean family income in the United States is closer to $72,000\. The difference
    is because of the small portion of the population who make significantly more
    money than everyone else. They pull the overall mean up. For income, then, the
    most meaningful statistic is the median.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察*X*，会发现大部分值集中在60到70之间，只有一个较低的值为37。正是这个低值37将均值拉低，造成均值低于中位数。收入就是这个效应的一个典型例子。美国当前的年家庭收入中位数大约为62,000美元。最近测量的美国家庭收入的均值更接近72,000美元。这个差异源于一小部分收入远高于其他人的群体。他们将整体均值拉高。因此，对于收入而言，最有意义的统计量是中位数。
- en: Consider [Figure 4-2](ch04.xhtml#ch04fig02).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[图4-2](ch04.xhtml#ch04fig02)。
- en: '![image](Images/04fig02.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig02.jpg)'
- en: '*Figure 4-2: The mean (solid) and median (dashed) plotted over the histogram
    of a sample dataset*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-2：均值（实线）和中位数（虚线）绘制在样本数据集的直方图上*'
- en: '[Figure 4-2](ch04.xhtml#ch04fig02) shows the histogram generated from 1,000
    samples of a simulated dataset. Also plotted are the mean (solid line) and median
    (dashed line). The two do not match; the long tail in the histogram drags the
    mean up. If we were to count, 500 samples would fall in the bins below the dashed
    line and 500 in the bins above.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-2](ch04.xhtml#ch04fig02)显示了从1,000个模拟数据样本中生成的直方图。图中还绘制了均值（实线）和中位数（虚线）。这两者并不相同；直方图中的长尾将均值拉高。如果我们统计，500个样本会落在虚线以下的区间，而500个样本会落在虚线以上的区间。'
- en: Are there times when the mean and median are the same? Yes. If the data distribution
    is completely symmetric, then the mean and median will be the same. The classic
    example of this situation is the normal distribution. [Figure 3-4](ch03.xhtml#ch03fig04)
    showed a normal distribution where the left-right symmetry was clear. The normal
    distribution is special. We’ll see it again throughout the chapter. For now, remember
    that the closer the distribution of the dataset is to a normal distribution, the
    closer the mean and median will be.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 平均值和中位数有可能相同吗？是的。如果数据分布完全对称，那么均值和中位数将相同。这种情况的经典例子是正态分布。[图3-4](ch03.xhtml#ch03fig04)显示了一个正态分布，其中左右对称性非常明显。正态分布是特殊的。我们将在本章中多次看到它。目前，请记住，数据集的分布越接近正态分布，均值和中位数就越接近。
- en: 'The opposite is also worth remembering: if the dataset’s distribution is far
    from normal, like in [Figure 4-2](ch04.xhtml#ch04fig02), then the median is likely
    the better statistic to consider when summarizing the data.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 相反的情况也值得记住：如果数据集的分布远离正态分布，比如[图4-2](ch04.xhtml#ch04fig02)所示，那么中位数在总结数据时可能是更好的统计量。
- en: Measures of Variation
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变化度量
- en: A beginning archer shoots 10 arrows at a target. Eight of the beginner’s arrows
    hit the target, two miss completely, and the eight that do hit the target are
    spread uniformly across it. An expert archer shoots 10 arrows at a target. All
    of the expert’s arrows hit within a few centimeters of the center. Think about
    the mean position of the arrows. For the expert, all of the arrows are near the
    center of the target, so we can see that the mean position of the arrows will
    be near the center. For the beginner, none of the arrows are near the center of
    the target, but they are scattered more or less equally to the left and right
    or above and below the center. Because of this, the average position will balance
    out and be near the center of the target as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一位初学者射手向靶子射了10支箭。八支箭命中靶心，两支箭完全偏离，命中的八支箭均匀地分布在靶子上。一个专家射手向靶子射了10支箭。专家的所有箭都命中在离靶心几厘米的范围内。想想箭的平均位置。对于专家来说，所有的箭都靠近靶心，所以我们可以看出箭的均值位置会接近靶心。对于初学者来说，虽然没有箭命中靶心，但它们大致均匀地分布在靶心的左侧和右侧或上下。因此，平均位置会平衡并接近靶心。
- en: However, the first archer’s arrows are scattered; their location varies greatly.
    The second archer’s arrows, on the other hand, are tightly clustered, and there
    is little variation in their position. One meaningful way to summarize and understand
    a dataset is to quantify its variation. Let’s see how we might do this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，第一位射手的箭散落不齐；箭的位置变化很大。另一方面，第二位射手的箭紧密集中，位置变化很小。总结和理解数据集的一种有意义方式是量化其变化。让我们看看如何做到这一点。
- en: Deviation vs. Variance
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏差与方差
- en: One way we might measure the variation of a dataset is to find the *range*,
    the difference between the largest and smallest values. However, the range is
    a crude measurement, as it pays no attention to most of the values in the dataset,
    only the extremes. We can do better by calculating the mean of the difference
    between the data values and the mean of the data. The formula is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量数据集变化的一种方式是找出*范围*，即最大值与最小值之间的差异。然而，范围是一个粗略的度量，它只关注数据集中的极端值，而忽略了大部分值。我们可以通过计算数据值与数据均值之间差异的均值来做得更好。其公式为：
- en: '![image](Images/04equ02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ02.jpg)'
- en: '[Equation 4.2](ch04.xhtml#ch04equ02) is the *mean deviation*. It’s a natural
    measure and gives just what we want: an idea of how far, on average, each sample
    is from the mean. While there’s nothing wrong with calculating the mean deviation,
    you’ll find that it’s rarely used in practice. One reason has to do with algebra
    and calculus. The absolute value is annoying to deal with mathematically.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 4.2](ch04.xhtml#ch04equ02)是*均差*。它是一种自然的度量方法，正好符合我们的需求：提供一个关于每个样本平均偏离均值多少的概念。虽然计算均差没有问题，但你会发现它在实际中很少使用。一个原因与代数和微积分有关。绝对值在数学处理中是个麻烦。'
- en: 'Instead of the natural measure of variation, let’s calculate this one using
    squared differences:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与其使用自然的变异度量方法，不如用平方差来计算：
- en: '![image](Images/04equ03.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ03.jpg)'
- en: '[Equation 4.3](ch04.xhtml#ch04equ03) is known as the *biased sample variance*.
    It’s the mean of the squared difference between each value in the dataset and
    the mean. It’s an alternate way of characterizing the scatter in the dataset.
    Why it’s biased, we’ll discuss in a second. We’ll get into why it’s ![image](Images/ssub-bar.jpg)
    and not *s[n]* shortly after that.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 4.3](ch04.xhtml#ch04equ03)被称为*有偏样本方差*。它是数据集中每个值与均值之间平方差的均值。这是另一种描述数据集散布的方法。为什么它有偏，我们稍后会讨论。我们将很快讨论为什么它是![image](Images/ssub-bar.jpg)而不是*s[n]*。'
- en: 'Before we do, it’s worth noting that you’ll often see a slightly different
    equation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，值得注意的是，你会经常看到一个略有不同的方程：
- en: '![image](Images/04equ04.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ04.jpg)'
- en: This equation is the *unbiased sample variance*. Using *n* – 1 in place of *n*
    is known as Bessel’s correction. It’s related to the number of degrees of freedom
    in the residuals, where the residuals are what’s left when the mean is subtracted
    from each of the values in the dataset. The sum of the residuals is zero, so if
    there are *n* values in the dataset, knowing *n* – 1 of the residuals allows the
    last residual to be calculated. This gives us the degrees of freedom for the residuals.
    We are “free” to calculate *n* – 1 of them knowing that we’ll get the last one
    from the fact that the residuals sum to zero. Dividing by *n*–1 gives a less biased
    estimate of the variance, assuming ![image](Images/ssub-bar.jpg) is biased in
    some way to begin with.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是*无偏样本方差*。用*n* - 1代替*n* 被称为贝塞尔修正。它与残差的自由度数量有关，残差是指从数据集中的每个值减去均值后所剩下的部分。残差的总和为零，因此，如果数据集中有*n*个值，知道*n*
    - 1个残差就能计算出最后一个残差。这给我们提供了残差的自由度。我们可以“自由”地计算*n* - 1个残差，因为我们知道通过残差的总和为零可以得出最后一个残差。用*n*
    - 1来除可以得到一个较少偏差的方差估计，假设![image](Images/ssub-bar.jpg)一开始有某种偏差。
- en: Why are we talking about biased variance and unbiased variance? Biased how?
    We should always remember that a dataset is a sample from some parent data-generating
    process, the population. The true population variance (σ²) is the scatter of the
    population around the true population mean (μ). However, we don’t know μ or σ²,
    so instead, we estimate them from the dataset we do have. The mean of the sample
    is ![image](Images/xbar.jpg). That’s our estimate for μ. It’s then natural to
    calculate the mean of the squared deviations around ![image](Images/xbar.jpg)
    and call that our estimate for σ². That’s ![image](Images/ssub-bar.jpg) ([Equation
    4.3](ch04.xhtml#ch04equ03)). The claim, which is true but beyond our scope to
    demonstrate, is that ![image](Images/ssub-bar.jpg) is biased and not the best
    estimate of σ², but if Bessel’s correction is applied, we’ll have a better estimate
    of the population variance. So we should use *s*² ([Equation 4.4](ch04.xhtml#ch04equ04))
    to characterize the variance of the dataset around the mean.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要讨论偏差方差和无偏方差呢？偏差在哪里？我们应该始终记住，数据集是来自某个母体数据生成过程的一个样本，即母体。真正的母体方差(σ²)是母体数据围绕真实母体均值(μ)的散布。然而，我们并不知道μ或σ²，因此我们只能从已有的数据集中估算它们。样本的均值是![image](Images/xbar.jpg)，它是我们对μ的估计。然后，计算围绕![image](Images/xbar.jpg)的平方偏差的均值，并将其作为我们对σ²的估计。这就是![image](Images/ssub-bar.jpg)（[方程
    4.3](ch04.xhtml#ch04equ03)）。这个结论虽然是真的，但超出了我们演示的范围：![image](Images/ssub-bar.jpg)是有偏的，并不是σ²的最佳估计，但如果应用贝塞尔修正，我们将得到更好的母体方差估计。所以我们应该使用*s*²（[方程
    4.4](ch04.xhtml#ch04equ04)）来描述数据集围绕均值的方差。
- en: In summary, we should use ![image](Images/xbar.jpg) and *s*² to quantify the
    variance of the data-set. Now, why is it *s*²? The square root of the variance
    is the *standard deviation* denoted as σ for the population and *s* for the estimate
    of σ calculated from the dataset. Most often, we want to work with the standard
    deviation. Writing square roots becomes tiresome, so convention has adopted the
    σ or *s* notation for the standard deviation and uses the squared form when discussing
    the variance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们应该使用 ![image](Images/xbar.jpg) 和 *s*² 来量化数据集的方差。那么，为什么是 *s*² 呢？方差的平方根就是*标准差*，在总体中用
    σ 表示，而在从数据集计算的 σ 估计值中用 *s* 表示。通常，我们更希望使用标准差。写平方根变得很麻烦，因此约定采用 σ 或 *s* 表示标准差，而在讨论方差时使用平方形式。
- en: And, because life isn’t already ambiguous enough, you’ll often see σ used for
    *s*, and [Equation 4.3](ch04.xhtml#ch04equ03) used when it really should be [Equation
    4.4](ch04.xhtml#ch04equ04). Some toolkits, including our beloved NumPy, make it
    easy to use the wrong formula.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，由于生活本来就足够模糊，您经常会看到 σ 被用来表示 *s*，并且 [方程 4.3](ch04.xhtml#ch04equ03) 被用作实际应该使用
    [方程 4.4](ch04.xhtml#ch04equ04) 的情况。一些工具包，包括我们钟爱的 NumPy，使得使用错误公式变得轻而易举。
- en: 'However, as the number of samples in our dataset increases, the difference
    between the biased and unbiased variance decreases because dividing by *n* or
    *n* – 1 matters less and less. A few lines of code illustrate this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着数据集中样本数量的增加，偏差方差和无偏方差之间的差异逐渐减小，因为无论是除以 *n* 还是 *n* – 1，其影响越来越小。以下几行代码说明了这一点：
- en: '>>> import numpy as np'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import numpy as np'
- en: '>>> n = 10'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> n = 10'
- en: '>>> a = np.random.random(n)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.random.random(n)'
- en: '>>> (1/n)*((a-a.mean())**2).sum()'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> (1/n)*((a-a.mean())**2).sum()'
- en: '0.08081748204006689'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '0.08081748204006689'
- en: '>>> (1/(n-1))*((a-a.mean())**2).sum()'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> (1/(n-1))*((a-a.mean())**2).sum()'
- en: '0.08979720226674098'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '0.08979720226674098'
- en: Here, a sample with only 10 values (a) shows a difference in the biased and
    unbiased variance in the third decimal. If we increase our dataset size from 10
    to 10,000, we get
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，只有 10 个值的样本（a）显示了偏差方差和无偏方差之间在第三位小数上的差异。如果我们将数据集的大小从 10 增加到 10,000，则得到
- en: '>>> n = 10000'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> n = 10000'
- en: '>>> a = np.random.random(n)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.random.random(n)'
- en: '>>> (1/n)*((a-a.mean())**2).sum()'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> (1/n)*((a-a.mean())**2).sum()'
- en: '0.08304350577482553'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '0.08304350577482553'
- en: '>>> (1/(n-1))*((a-a.mean())**2).sum()'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> (1/(n-1))*((a-a.mean())**2).sum()'
- en: '0.08305181095592111'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '0.08305181095592111'
- en: The difference between the biased and unbiased estimate of the variance is now
    in the fifth decimal. Therefore, for the large datasets we typically work with
    in deep learning, it matters little in practice whether we use *s[n]* or *s* for
    the standard deviation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差估计和无偏估计之间的方差差异现在出现在第五位小数。因此，对于我们在深度学习中通常使用的大型数据集，实际上无论使用 *s[n]* 还是 *s* 来表示标准差几乎没有区别。
- en: '**MEDIAN ABSOLUTE DEVIATION**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**中位数绝对偏差**'
- en: 'The standard deviation is based on the mean. The mean, as we saw above, is
    sensitive to extreme values, and the standard deviation is doubly so because we
    square the deviation from the mean for each sample. A measure of variability that
    is insensitive to extreme values in the dataset is the *median absolute deviation
    (MAD)*. The MAD is defined as the median of the absolute values of the difference
    between the data and the median:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差是基于均值的。如上所示，均值对极端值敏感，而标准差则更加敏感，因为我们对每个样本的偏差进行平方。一个对数据集中的极端值不敏感的变异性度量是 *中位数绝对偏差
    (MAD)*。MAD 定义为数据与中位数之差的绝对值的中位数：
- en: MAD = median(|*X[i]* – median(*X*)|)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: MAD = 中位数(|*X[i]* – 中位数(*X*)|)
- en: 'Procedurally, first calculate the median of the data, then subtract it from
    each data value, making the result positive, and report the median of that set.
    The implementation is straightforward:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 操作步骤是，首先计算数据的中位数，然后从每个数据值中减去它，使结果变为正数，最后报告这个集合的中位数。实现起来很简单：
- en: 'def MAD(x):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'def MAD(x):'
- en: return np.median(np.abs(x-np.median(x)))
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: return np.median(np.abs(x-np.median(x)))
- en: The MAD is not often used, but its insensitivity to extreme values in the dataset
    argues toward more frequent use, especially for outlier detection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MAD（中位数绝对偏差）虽然不常用，但它对数据集中极端值的不敏感性使得它在检测异常值时具有较高的实用性，因此应更多使用。
- en: Standard Error vs. Standard Deviation
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标准误差与标准差
- en: 'We have one more measure of variance to discuss: the *standard error of the
    mean (SEM)*. The SEM is often simply called the *standard error (SE)*. We need
    to go back to the population to understand what the SE is and when to use it.
    If we select a sample from the population, a dataset, we can calculate the mean
    of the sample, ![image](Images/xbar.jpg). If we choose repeated samples and calculate
    those sample means, we’ll generate a dataset of means of the samples from the
    population. This might sound familiar; it’s the process we used to illustrate
    the central limit theorem in [Chapter 3](ch03.xhtml#ch03). The standard deviation
    of the set of means is the standard error.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个我们需要讨论的方差度量：*均值的标准误差（SEM）*。标准误差通常简称为*标准误差（SE）*。我们需要回到总体中去理解标准误差是什么以及何时使用它。如果我们从总体中选取一个样本数据集，我们可以计算该样本的均值，![image](Images/xbar.jpg)。如果我们选择多个重复的样本并计算这些样本的均值，我们就会得到一个来自总体的样本均值数据集。这可能听起来很熟悉；它是我们用来说明中心极限定理的过程，见[第3章](ch03.xhtml#ch03)。这些均值的标准差就是标准误差。
- en: The formula for the standard error from the standard deviation is straightforward,
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从标准差得到标准误差的公式是直接的，
- en: '![image](Images/077equ01.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/077equ01.jpg)'
- en: and is nothing more than a scaling of the sample standard deviation by the square
    root of the number of samples.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是将样本标准差按样本数量的平方根进行缩放。
- en: When should we use the standard deviation, and when should we use the standard
    error? Use the standard deviation to learn about the distribution of the samples
    around the mean. Use the standard error to say something about how good an estimate
    of the population mean a sample mean is. In a sense, the standard error is related
    to both the central limit theorem, as that affects the standard deviation of the
    means of multiple samples from the parent population, and the law of large numbers,
    since a larger dataset is more likely to give a better estimate of the population
    mean.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们什么时候使用标准差，什么时候使用标准误差？使用标准差来了解样本围绕均值的分布情况。使用标准误差来描述样本均值对总体均值估计的准确性。从某种意义上说，标准误差与中心极限定理有关，因为它影响从母体总体中提取多个样本均值的标准差，以及大数法则，因为较大的数据集更有可能给出总体均值的更好估计。
- en: From a deep learning point of view, we might use the standard deviation to describe
    the dataset used to train a model. If we train and test several models, remembering
    the stochastic nature of deep network initialization, we can calculate a mean
    over the models for some metric, say the accuracy. In that case, we might want
    to report the mean accuracy plus or minus the standard error. As we train more
    models and gain confidence that the mean accuracy represents the sort of accuracy
    the model architecture can provide, we should expect that the error in the mean
    accuracy over the models will decrease.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度学习的角度来看，我们可能会使用标准差来描述用于训练模型的数据集。如果我们训练并测试多个模型，考虑到深度网络初始化的随机性，我们可以计算多个模型的某个指标的均值，比如准确度。在这种情况下，我们可能会报告均值准确度加减标准误差。当我们训练更多的模型，并且逐渐确信均值准确度代表了模型架构能够提供的准确度时，我们应该预期模型的均值准确度误差会减少。
- en: To recap, in this section, we discussed different summary statistics, values
    we can use to start to understand a dataset. These include the various means (arithmetic,
    geometric, and harmonic), the median, the standard deviation, and, when appropriate,
    the standard error. For now, let’s see how we can use plots to help understand
    a dataset.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，在这一部分中，我们讨论了不同的总结性统计量，这些是我们用来开始理解数据集的值。这些统计量包括各种均值（算术均值、几何均值和调和均值）、中位数、标准差，以及在适当的情况下，标准误差。现在，让我们看看如何利用图表来帮助理解数据集。
- en: Quantiles and Box Plots
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分位数与箱形图
- en: To calculate the median, we need to find the middle value, the number splitting
    the dataset into two halves. Mathematically, we say that the median divides the
    dataset into two quantiles.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算中位数，我们需要找到中间值，即将数据集分成两半的数值。从数学上讲，我们说中位数将数据集分成了两个分位数。
- en: A *quantile* splits the dataset into fixed-sized groups where the fixed size
    is the number of data values in the quantile. Since the median splits the dataset
    into two equally sized groups, it’s a *2-quantile*. Sometimes you’ll see the median
    referred to as the *50th percentile*, meaning 50 percent of the data values are
    less than this value. By similar reasoning, then, the 95th percentile is the value
    that 95 percent of the dataset is less than. Researchers often calculate 4-quantiles
    and refer to them as *quartiles*, since they split the dataset into four groups
    such that 25 percent of the data values are in the first quartile, 50 percent
    are in the first and second, and 75 percent are in the first, second, and third,
    with the final 25 percent in the fourth quartile.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*四分位数*将数据集划分为固定大小的组，其中固定大小是四分位数中的数据值数量。由于中位数将数据集分为两个大小相等的组，因此它是一个*2-分位数*。有时你会看到中位数被称为*50百分位数*，意味着50%的数据值小于该值。类似地，95百分位数是数据集中95%的值小于它。研究人员通常计算4-分位数，并称它们为*四分位数*，因为它们将数据集分成四个组，使得25%的数据值位于第一个四分位数，50%的数据值位于第一个和第二个四分位数，75%的数据值位于第一个、第二个和第三个四分位数，最后25%的数据值位于第四个四分位数。
- en: 'Let’s work through an example to understand what we mean by quantiles. The
    example uses a synthetic exam dataset representing 1,000 test scores. See the
    file *exams.npy*. We’ll use NumPy to calculate the quartile values for us and
    then plot a histogram of the dataset with the quartile values marked. First, let’s
    calculate the quartile positions:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解四分位数的含义。该例子使用了一个合成的考试数据集，表示1,000个测试成绩。请参见文件*exams.npy*。我们将使用NumPy来计算四分位数值，并绘制数据集的直方图，并标记四分位数值。首先，让我们计算四分位数的位置：
- en: d = np.load("exams.npy")
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: d = np.load("exams.npy")
- en: p = d[:,0].astype("uint32")
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: p = d[:,0].astype("uint32")
- en: q = np.quantile(p, [0.0, 0.25, 0.5, 0.75, 1.0])
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: q = np.quantile(p, [0.0, 0.25, 0.5, 0.75, 1.0])
- en: 'print("Quartiles: ", q)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("四分位数: ", q)'
- en: print("Counts by quartile:")
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: print("按四分位数计数:")
- en: print("    %d" % ((q[0] <= p) & (p < q[1])).sum())
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: print("    %d" % ((q[0] <= p) & (p < q[1])).sum())
- en: print("    %d" % ((q[1] <= p) & (p < q[2])).sum())
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: print("    %d" % ((q[1] <= p) & (p < q[2])).sum())
- en: print("    %d" % ((q[2] <= p) & (p < q[3])).sum())
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: print("    %d" % ((q[2] <= p) & (p < q[3])).sum())
- en: print("    %d" % ((q[3] <= p) & (p < q[4])).sum())
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: print("    %d" % ((q[3] <= p) & (p < q[4])).sum())
- en: This code, along with code to generate the plot, is in the file *quantiles.py*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码以及生成图表的代码在文件*quantiles.py*中。
- en: First we load the synthetic exam data and keep the first exam scores (p). Note,
    we make p an integer array so we can use np.bincount later to make the histogram.
    (That code is not shown above.) We then use NumPy’s np.quantile function to calculate
    the quartile values. This function takes the source array and an array of quantile
    values in the range [0, 1]. The values are fractions of the distance from the
    minimum value of the array to its maximum. So, asking for the 0.5 quantile is
    asking for the value that is half the distance between the minimum of p and its
    maximum such that the number of values in each set is equal.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载合成考试数据并保留第一次考试的成绩（p）。请注意，我们将p设为整数数组，以便稍后可以使用np.bincount来生成直方图。（上面没有展示那段代码。）然后，我们使用NumPy的np.quantile函数计算四分位数值。该函数接受源数组和一个量化值数组，范围在[0,
    1]之间。这些值是数组的最小值到最大值之间的距离的分数。因此，要求0.5四分位数即是要求值，它是p的最小值和最大值之间距离的一半，使得每个组中的值数量相等。
- en: To get quartiles, we ask for the 0.25, 0.5, and 0.75 quantiles to get the values
    such that 25 percent, 50 percent, and 75 percent of the elements of p are less
    than the values. We also ask for the 0.0 and 1.0 quantiles, the minimum and maximum
    of p. We do this for convenience when we count the number of elements in each
    range. Note, we could have instead used the np.percentile function. It returns
    the same values as np.quantile but uses percentage values instead of fractions.
    In that case, the second argument would have been [0,25,50,75,100].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得四分位数，我们要求0.25、0.5和0.75四分位数，以获得使得25%、50%和75%的p元素小于该值的四分位数值。我们还要求0.0和1.0四分位数，即p的最小值和最大值。我们这样做是为了方便计算每个范围内的元素数量。请注意，我们本可以使用np.percentile函数来代替。它返回与np.quantile相同的值，但使用百分比值而不是分数。在这种情况下，第二个参数将是[0,25,50,75,100]。
- en: The returned quartile values are in q. We print them to get
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的四分位数值存储在q中。我们打印它们以获得
- en: 18.0, 56.75, 68.0, 78.0, 100.0
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 18.0, 56.75, 68.0, 78.0, 100.0
- en: Here, 18 is the minimum, 100 is the maximum, and the three cutoff values for
    the quartiles are 56.75, 68, and 78\. Note that the cutoff for the second quartile
    is the median, 68.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，18是最小值，100是最大值，三个四分位数的切分值分别是56.75、68和78。请注意，第二个四分位数的切分值是中位数，68。
- en: The remaining code counts the number of values in p in each range. With 1,000
    values, we’d expect to have 250 in each range, but because the math doesn’t always
    fall along existing data values, we get instead
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码用于统计 p 中每个范围内的值的数量。对于 1,000 个值，我们预计每个范围内会有 250 个值，但由于数学运算并不总是精确对齐到现有的数据值，因此我们实际上得到的是
- en: 250, 237, 253, 248
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 250, 237, 253, 248
- en: meaning 250 elements of p are less than 56.75, 237 are in [56.75, 68], and so
    forth.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，p 中有 250 个元素小于 56.75，237 个在 [56.75, 68] 之间，以此类推。
- en: The code above uses a clever counting trick worth explaining. We want to count
    the number of values in p in some range. We can’t use NumPy’s np.where function,
    as it doesn’t like the compound conditional statement. However, if we use an expression
    like 10 <= p, we’ll be given an array the same size as p where each element is
    either True if the condition is true for that element or False if it is not. Therefore,
    asking for 10 <= p and p < 90 will return two Boolean arrays. To get the elements
    where both conditions are true, we need to logically AND them together (&). This
    gives us a final array the same size and shape as p, where all True elements represent
    values in p in [10, 90). To get the count, we apply the sum method that for a
    Boolean array treats True as one and False as zero.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用了一个巧妙的计数技巧，值得解释一下。我们想要统计 p 中某个范围内的数值个数。我们不能使用 NumPy 的 np.where 函数，因为它不支持复合条件语句。然而，如果我们使用像
    10 <= p 这样的表达式，我们将得到一个与 p 同样大小的数组，其中每个元素如果条件为真则为 True，否则为 False。因此，要求 10 <= p
    和 p < 90 会返回两个布尔数组。为了获取同时满足两个条件的元素，我们需要对它们进行逻辑与运算（&）。这样我们就得到了一个与 p 大小和形状相同的数组，其中所有为
    True 的元素表示 p 中在 [10, 90) 范围内的值。为了获取计数，我们使用 sum 方法，这个方法对布尔数组处理时，将 True 视为 1，False
    视为 0。
- en: '[Figure 4-3](ch04.xhtml#ch04fig03) shows the histogram of the exam data with
    the quartiles marked.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](ch04.xhtml#ch04fig03)展示了标注四分位数的考试数据直方图。'
- en: '![image](Images/04fig03.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig03.jpg)'
- en: '*Figure 4-3: A histogram of 1,000 exam scores with the quartiles marked*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-3：标注四分位数的 1,000 个考试成绩直方图*'
- en: The example above shows yet again how useful a histogram is for visualizing
    and understanding data. We should use histograms whenever possible to help understand
    what’s going on with a dataset. [Figure 4-3](ch04.xhtml#ch04fig03) superimposes
    the quartile values on the histogram. This helps us understand what the quartiles
    are and their relationship to the data values, but this is not a typical presentation
    style. More typical, and useful because it can show multiple features of a dataset,
    is the *box plot*. Let’s use it now for the exam scores above, but this time we’ll
    also include the two other sets of exam scores we ignored previously.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子再次展示了直方图在可视化和理解数据方面的巨大作用。我们应该尽可能使用直方图，帮助我们理解数据集的情况。[图 4-3](ch04.xhtml#ch04fig03)将四分位数值叠加在直方图上。这有助于我们理解四分位数及其与数据值的关系，但这并不是一种典型的展示方式。更典型且实用的方式是
    *箱型图*，因为它能显示数据集的多个特征。现在我们就使用箱型图来展示上面的考试成绩，并且这次我们还会包括之前忽略的另外两个考试成绩数据集。
- en: We’ll show a box plot first, and then explain it. To see the box plot for the
    three exams in the *exams.npy* file, use
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示一个箱型图，然后再进行解释。要查看 *exams.npy* 文件中三个考试的箱型图，请使用
- en: d = np.load("exams.npy")
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: d = np.load("exams.npy")
- en: plt.boxplot(d)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: plt.boxplot(d)
- en: plt.xlabel("Test")
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel("测试")
- en: plt.ylabel("Scores")
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel("成绩")
- en: plt.show()
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: where we’re loading the full set of exam scores and then using the Matplotlib
    boxplot function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在加载完整的考试成绩数据集，并使用 Matplotlib 的 boxplot 函数。
- en: Take a look at the output, shown in [Figure 4-4](ch04.xhtml#ch04fig04).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请看一下输出结果，如[图 4-4](ch04.xhtml#ch04fig04)所示。
- en: '![image](Images/04fig04.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig04.jpg)'
- en: '*Figure 4-4: Box plots for the three exams (top), and the box plot for the
    first exam with the components marked (bottom)*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-4：三个考试的箱型图（顶部），以及第一个考试的箱型图和标注组件（底部）*'
- en: The top chart in [Figure 4-4](ch04.xhtml#ch04fig04) shows the box plot for the
    three sets of exam scores in *|*exams.npy|. The first of these is plotted again
    on the bottom of [Figure 4-4](ch04.xhtml#ch04fig04), along with labels describing
    the parts of the plot.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](ch04.xhtml#ch04fig04)中的顶部图表展示了 *exams.npy* 文件中三个考试成绩的箱型图。这些数据的第一个箱型图再次绘制在
    [图 4-4](ch04.xhtml#ch04fig04)的底部，并附有描述箱型图各部分的标签。'
- en: A box plot shows us a visual summary of the data. The box in the bottom chart
    in [Figure 4-4](ch04.xhtml#ch04fig04) illustrates the range between the cutoffs
    for the first quartile (Q1) and the third quartile (Q3). The numerical difference
    between Q3 and Q1 is known as the *interquartile range (IQR)*. The larger the
    IQR, the more spread out the data is around the median. Notice that the score
    is on the y-axis this time. We could have easily made the plot horizontal, but
    vertical is the default. The median (Q2) is marked near the middle of the box.
    The mean is not shown in a box plot.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图为我们提供了数据的可视化总结。[图 4-4](ch04.xhtml#ch04fig04)中底部图表的箱体展示了第一四分位数（Q1）和第三四分位数（Q3）之间的范围。Q3
    和 Q1 之间的数值差异被称为*四分位距（IQR）*。IQR 越大，数据围绕中位数的分布就越分散。注意，这次的分数在 y 轴上。我们本可以轻松地将图表做成水平的，但垂直是默认设置。中位数（Q2）标记在箱体的中间。箱线图中并不显示均值。
- en: The box plot includes two additional lines, the *whiskers*, though Matplotlib
    calls them *fliers*. As indicated, they are 1.5 times the IQR above Q3 or below
    Q1\. Finally, there are some circles labeled “possible outliers.” By convention,
    values outside of the whiskers are considered *possible outliers*, meaning they
    might represent erroneous data, either entered incorrectly by hand or, more likely
    these days, received from faulty sensors. For example, bright or dead pixels on
    a CCD camera might be considered outliers. When evaluating a potential dataset,
    we should be sensitive to outliers and use our best judgment about what to do
    with them. Usually, there are only a few, and we can drop the samples from the
    dataset without harm. However, it’s also possible that the outliers are actually
    real and are highly indicative of a particular class. If that’s the case, we want
    to keep them in the dataset in the hopes that the model will use them effectively.
    Experience, intuition, and common sense must guide us here.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图还包括两条附加的线，即*须弯*，不过 Matplotlib 称它们为*离群值线*。如图所示，须弯位于 Q3 以上或 Q1 以下 1.5 倍的 IQR
    处。最后，还有一些被标记为“可能的异常值”的圆点。根据约定，位于须弯之外的值被视为*可能的异常值*，这意味着它们可能代表错误的数据，这些数据可能是手动输入错误的，或者更常见的是来自故障传感器的数据。例如，CCD
    相机上的亮点或坏点可能被视为异常值。在评估潜在的数据集时，我们应该对异常值保持敏感，并运用我们的最佳判断来决定如何处理它们。通常，异常值只有几个，我们可以在不造成伤害的情况下将它们从数据集中剔除。然而，也有可能这些异常值实际上是真实的，且可能高度指示某一特定类别。如果是这种情况，我们希望将它们保留在数据集中，以期望模型能够有效地利用它们。经验、直觉和常识必须在这里引导我们。
- en: 'Let’s interpret the top chart in [Figure 4-4](ch04.xhtml#ch04fig04) showing
    the three sets of exam scores. The top of the whiskers is at 100 each time, which
    makes sense: a 100 is a perfect score, and there were 100s in the dataset. Notice
    that the box portion of the plot is not centered vertically in the whiskers. Recalling
    that 50 percent of the data values are between Q1 and Q3, with 25 percent above
    and below Q2 in the box, we see that the data is not rigorously normal; its distribution
    deviates from a normal curve. A glance back to the histogram in [Figure 4-3](ch04.xhtml#ch04fig03)
    confirms this for the first exam. Similarly, we see the second and third exams
    deviate from normality as well. So, a box plot can tell us how similar the distribution
    of the dataset is to a normal distribution. When we discuss hypothesis testing
    below, we’ll want to know if the data is normally distributed or not.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下[图 4-4](ch04.xhtml#ch04fig04)中显示的三个考试成绩的箱线图。每次须弯的顶部都在 100 处，这很有道理：100
    是满分，数据集中确实有 100 分的成绩。注意，图中的箱体部分并没有在须弯中垂直居中。回想一下，50% 的数据值位于 Q1 和 Q3 之间，箱体内 Q2 上下各有
    25% 的数据，因此我们可以看到数据并不严格符合正态分布；其分布偏离了正态曲线。回头看看[图 4-3](ch04.xhtml#ch04fig03)中的直方图，我们可以确认第一场考试的数据分布不符合正态性。同样，我们也可以看到第二场和第三场考试的数据分布也偏离正态性。因此，箱线图可以告诉我们数据集的分布与正态分布的相似程度。当我们下面讨论假设检验时，我们需要知道数据是否符合正态分布。
- en: What about possible outliers, the values below Q1 – 1.5 × IQR? We know the dataset
    represents test scores, so common sense tells us that these are not outliers but
    valid scores by particularly confused (or lazy) students. If the dataset contained
    values above 100 or below zero, those would be fair game to label outliers.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，低于 Q1 – 1.5 × IQR 的可能异常值呢？我们知道数据集代表的是考试成绩，因此常识告诉我们，这些并不是异常值，而是特别困惑（或懒惰）的学生所做出的有效成绩。如果数据集包含了超过
    100 或低于零的值，那些就可以被认为是异常值。
- en: Sometimes dropping samples with outliers is the right thing to do. However,
    if the outlier is caused by missing data, cutting the sample might not be an option.
    Let’s take a look at what we might do with missing data, and why we should generally
    avoid it like the plague.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有时丢弃包含异常值的样本是正确的做法。然而，如果异常值是由缺失数据引起的，删除样本可能不是一个合适的选择。让我们看看如何处理缺失数据，以及为什么我们通常应该尽量避免它。
- en: Missing Data
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失数据
- en: 'Missing data is just that, data we don’t have. If the dataset consists of samples
    representing feature vectors, missing data shows up as one or more features in
    a sample that were not measured for some reason. Often, missing data is encoded
    in some way. If the value is only positive, a missing feature might be marked
    with a –1 or, historically, –999\. If the feature is given to us as a string,
    the string might be empty. For floating-point values, a not a number (NaN) might
    be used. NumPy makes it easy for us to check for NaNs in an array by using np.isnan:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据就是没有的数据。如果数据集由代表特征向量的样本组成，缺失数据通常表现为某个样本中的一个或多个特征未被测量。缺失数据通常会以某种方式进行编码。如果值是正数，缺失的特征可能用–1或历史上用–999来标记。如果特征是以字符串形式给出的，字符串可能为空。对于浮点值，可能使用“不是一个数字”（NaN）来表示。NumPy提供了简便的方法来检查数组中的NaN值，使用np.isnan：
- en: '>>> a = np.arange(10, dtype="float64")'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a = np.arange(10, dtype="float64")'
- en: '>>> a[3] = np.nan'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a[3] = np.nan'
- en: '>>> np.isnan(a[3])'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.isnan(a[3])'
- en: 'True'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'True'
- en: '>>> a[3] == np.nan'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a[3] == np.nan'
- en: 'False'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'False'
- en: '>>> a[3] is np.nan'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> a[3] is np.nan'
- en: 'False'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'False'
- en: Notice that direct comparison to np.nan with either == or is doesn’t work; only
    testing with np.isnan works.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，直接用==或is与np.nan进行比较是行不通的；只有使用np.isnan来测试才有效。
- en: Detecting missing data is dataset-specific. Assuming we’ve convinced ourselves
    there is missing data, how do we handle it?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 检测缺失数据是数据集特定的。假设我们已经确认有缺失数据，接下来该如何处理呢？
- en: 'Let’s generate a small dataset with missing values and use our existing statistics
    knowledge to see how to handle them. The code for the following is in missing.py.
    First, we generate a dataset of 1,000 samples, each with four features:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一个包含缺失值的小数据集，并利用现有的统计学知识来看如何处理它们。以下代码在missing.py中。首先，我们生成一个包含1,000个样本的数据集，每个样本有四个特征：
- en: N = 1000
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: N = 1000
- en: np.random.seed(73939133)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: np.random.seed(73939133)
- en: x = np.zeros((N,4))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.zeros((N,4))
- en: x[:,0] = 5*np.random.random(N)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,0] = 5*np.random.random(N)
- en: x[:,1] = np.random.normal(10,1,size=N)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,1] = np.random.normal(10,1,size=N)
- en: x[:,2] = 3*np.random.beta(5,2,N)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,2] = 3*np.random.beta(5,2,N)
- en: x[:,3] = 0.3*np.random.lognormal(size=N)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: x[:,3] = 0.3*np.random.lognormal(size=N)
- en: The dataset is in x. We fix the random number seed to get a reproducible result.
    The first feature is uniformly distributed. The second is normally distributed,
    while the third follows a beta distribution and the fourth a lognormal distribution.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在x中。我们设置随机数种子以获得可重复的结果。第一个特征是均匀分布的，第二个特征是正态分布的，第三个特征遵循Beta分布，第四个特征遵循对数正态分布。
- en: 'At the moment, x has no missing values. Let’s add some by making random elements
    NaNs:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，x没有缺失值。我们通过将随机元素设为NaN来添加一些缺失值：
- en: i = np.random.randint(0,N, size=int(0.05*N))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.random.randint(0,N, size=int(0.05*N))
- en: x[i,0] = np.nan
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: x[i,0] = np.nan
- en: i = np.random.randint(0,N, size=int(0.05*N))
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.random.randint(0,N, size=int(0.05*N))
- en: x[i,1] = np.nan
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: x[i,1] = np.nan
- en: i = np.random.randint(0,N, size=int(0.05*N))
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.random.randint(0,N, size=int(0.05*N))
- en: x[i,2] = np.nan
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: x[i,2] = np.nan
- en: i = np.random.randint(0,N, size=int(0.05*N))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.random.randint(0,N, size=int(0.05*N))
- en: x[i,3] = np.nan
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: x[i,3] = np.nan
- en: The dataset now has NaNs across 5 percent of its values.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集的5%值为NaN。
- en: If a few samples in a large dataset have missing data, we can remove them from
    the dataset with little worry. However, if 5 percent of the samples have missing
    data, we probably don’t want to lose that much data. More worrisome still, what
    if there’s a correlation between the missing data and a particular class? Throwing
    the samples away might bias the dataset in some way that’ll make the model less
    useful.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个大数据集中的少量样本缺少数据，我们可以放心地将其从数据集中移除。然而，如果5%的样本缺失数据，我们可能不希望丢失这么多数据。更让人担心的是，如果缺失数据与某个特定类别相关呢？丢弃这些样本可能会导致数据集出现偏差，从而使得模型的效果变差。
- en: So, what can we do? We just spent many pages learning how to summarize a dataset
    with basic descriptive statistics. Can we use those? Of course. We can look at
    the distributions of the features, ignoring the missing values, and use those
    distributions to decide how we might want to replace the missing data. Naively,
    we’d use the mean of the data we do have, but looking at the distribution may
    or may not push us toward the median instead, depending on how far the distribution
    is from normal. This sounds like a job for a box plot. Fortunately for us, Matplotlib’s
    boxplot function is smart; it ignores the NaNs. Therefore, making the box plot
    is a straightforward call to boxplot(x).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该怎么做呢？我们刚刚花了很多页学习如何通过基本的描述性统计来概括一个数据集。我们可以使用这些方法吗？当然可以。我们可以查看特征的分布，忽略缺失值，然后用这些分布来决定如何替换缺失的数据。简单来说，我们可能会使用现有数据的均值，但查看分布可能会让我们更倾向于使用中位数，具体取决于分布是否偏离正态分布。这听起来像是箱型图的工作。幸运的是，Matplotlib
    的 boxplot 函数很聪明，它会忽略 NaN 值。因此，绘制箱型图只是一个简单的 boxplot(x) 调用。
- en: '[Figure 4-5](ch04.xhtml#ch04fig05) shows us the dataset with the NaNs ignored.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-5](ch04.xhtml#ch04fig05)展示了忽略 NaN 后的数据集。'
- en: '![image](Images/04fig05.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig05.jpg)'
- en: '*Figure 4-5: Box plot of the dataset ignoring missing values*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-5：忽略缺失值的数据集的箱型图*'
- en: The boxes in [Figure 4-5](ch04.xhtml#ch04fig05) make sense for the distributions
    of the features. Feature 1 is uniformly distributed, so we expect a symmetric
    box around the mean/median. (These are the same for the uniform distribution.)
    Feature 2 is normally distributed, so we get a similar box structure as Feature
    1, but, with only 1,000 samples, some asymmetry is evident. The beta distribution
    of Feature 3 is skewed toward the top of its range, which we see in the box plot.
    Finally, the lognormal distribution of Feature 4 should be skewed toward lower
    values, with a long tail visible as the many “outliers” above the whiskers, an
    object lesson against mindlessly calling such values outliers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-5](ch04.xhtml#ch04fig05)中的箱体图对于特征的分布是有意义的。特征 1 是均匀分布的，所以我们期望围绕均值/中位数有一个对称的箱体。（对于均匀分布，均值和中位数是相同的。）特征
    2 是正态分布的，因此我们得到的箱体结构与特征 1 类似，但由于只有 1,000 个样本，某些不对称现象是显而易见的。特征 3 的贝塔分布偏向其范围的上端，这在箱型图中得以体现。最后，特征
    4 的对数正态分布应该偏向较低的值，且上方有许多“离群值”出现在须状线以上，长尾显现出来，这为我们提供了一个反面教材，提醒我们不要盲目地将这些值称为离群值。'
- en: 'Because we have features that are highly not normally distributed, we’ll update
    missing values with the median instead of the mean. The code is straightforward:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有些特征的分布高度偏离正态分布，我们将用中位数而不是均值来更新缺失值。代码非常简单：
- en: good_idx = np.where(np.isnan(x[:,0]) == False)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: good_idx = np.where(np.isnan(x[:,0]) == False)
- en: m = np.median(x[good_idx,0])
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: m = np.median(x[good_idx,0])
- en: bad_idx = np.where(np.isnan(x[:,0]) == True)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: bad_idx = np.where(np.isnan(x[:,0]) == True)
- en: x[bad_idx,0] = m
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: x[bad_idx,0] = m
- en: Here, i first holds the indices of Feature 1 that are not NaNs. We use these
    to calculate the median (m). Next, we set i to the indices that are NaNs and replace
    them with the median. We can do the same for the other features, updating the
    entire dataset so we no longer have missing values.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，i 首先保存了特征 1 中不是 NaN 的索引。我们用这些索引来计算中位数（m）。接着，我们将 i 设置为 NaN 的索引，并用中位数替换它们。我们可以对其他特征做同样的操作，更新整个数据集，这样就不再有缺失值了。
- en: 'Did we cause much of a change from the earlier distributions? No, because we
    only updated 5 percent of the values. For example, for Feature 3, based on the
    beta distribution, the mean and standard deviations change like so:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否对早期的分布造成了较大变化？没有，因为我们只更新了 5% 的值。例如，对于特征 3，根据贝塔分布，均值和标准差的变化如下：
- en: non-NaN mean, std = 2.169986, 0.474514
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 非 NaN 均值，标准差 = 2.169986, 0.474514
- en: updated mean, std = 2.173269, 0.462957
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的均值，标准差 = 2.173269, 0.462957
- en: The moral of the story is that if there’s enough missing data that the dataset
    might become biased by dropping it, the safest thing to do is replace the missing
    data with the mean or median. To decide whether to use the mean or median, consult
    descriptive statistics, a box plot, or a histogram.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事的寓意是，如果缺失数据足够多，以至于删除它可能会让数据集产生偏差，最安全的做法是用均值或中位数来替换缺失数据。要决定使用均值还是中位数，可以参考描述性统计、箱型图或直方图。
- en: Additionally, if the dataset is labeled, as a deep learning dataset would be,
    the process described above needs to be completed with the mean or median of samples
    grouped by each class. Otherwise, the calculated value might be inappropriate
    for the class.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果数据集是有标签的，如同深度学习数据集那样，上述过程需要使用按类别分组的样本的均值或中位数完成。否则，计算出的值可能不适用于该类别。
- en: With missing data eliminated, deep learning models can be trained on the dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 去除缺失数据后，可以在数据集上训练深度学习模型。
- en: Correlation
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关性
- en: At times, there is an association between the features in a dataset. If one
    goes up, the other might go up as well, though not necessarily in a simple linear
    way. Or, the other might go down—a negative association. The proper word for this
    type of association is *correlation*. A statistic that measures correlation is
    a handy way to understand how the features in a dataset are related.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，数据集中的特征之间存在某种关联。如果一个特征增大，另一个特征也可能增大，但不一定是简单的线性关系。或者，另一个特征可能会减小——这是一种负相关。描述这种关联的恰当词是*相关性*。衡量相关性的统计量是理解数据集中各特征之间关系的一个便捷方式。
- en: For example, it isn’t hard to see that the pixels of most images are highly
    correlated. This means if we select a pixel at random and then an adjacent pixel,
    there’s a good chance the second pixel will be similar to the first pixel. Images
    where this is not true look to us like random noise.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，很容易看出大多数图像的像素是高度相关的。这意味着如果我们随机选择一个像素，然后选择一个相邻像素，第二个像素很有可能与第一个像素相似。图像中如果没有这种相关性，看起来就像是随机噪声。
- en: In traditional machine learning, highly correlated features were undesirable,
    as they didn’t add any new information and only served to confuse the models.
    The entire art of feature selection was developed, in part, to remove this effect.
    For modern deep learning, where the network itself learns a new representation
    of the input data, it’s less critical to have uncorrelated inputs. This is, in
    part, why images work as inputs to deep networks when they usually fail to work
    at all with older machine learning models.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习中，高度相关的特征是不受欢迎的，因为它们并未提供新的信息，反而容易混淆模型。特征选择的整个艺术部分就是为了解决这一问题。对于现代深度学习来说，网络本身会学习输入数据的新表示，因此输入不相关并不像传统机器学习那样关键。这也部分解释了为什么图像作为输入在深度网络中表现良好，而在旧的机器学习模型中通常会失败。
- en: Whether the learning is traditional or modern, as part of summarizing and exploring
    a dataset, correlations among the features are worth examining and understanding.
    In this section, we’ll discuss two types of correlations. Each type returns a
    single number that measures the strength of the correlation between two features
    in the dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 无论学习是传统的还是现代的，作为总结和探索数据集的一部分，检查和理解特征之间的相关性是值得的。在本节中，我们将讨论两种类型的相关性。每种类型都会返回一个数字，用来衡量数据集中两个特征之间的相关性强度。
- en: Pearson Correlation
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pearson相关性
- en: The *Pearson correlation coefficient* returns a number, *r* ϵ [–1, +1], that
    indicates the strength of the *linear* correlation between two features. By *linear*
    we mean how strongly we can describe the correlation between the features by a
    line. If the correlation is such that one feature goes up exactly as the other
    feature goes up, the correlation coefficient is +1\. Conversely, if the second
    feature goes down exactly as the other goes up, the correlation is –1\. A correlation
    of zero means there is no association between the two features; they are (possibly)
    independent.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*Pearson相关系数*返回一个数字，*r* ϵ [–1, +1]，表示两个特征之间*线性*相关性的强度。这里的*线性*是指我们能通过一条直线多强地描述两个特征之间的相关性。如果相关性使得一个特征的增大恰好伴随着另一个特征的增大，那么相关系数为+1。如果第二个特征恰好随着第一个特征的增大而减小，则相关系数为–1。零相关意味着两个特征之间没有关联，它们是（可能）独立的。'
- en: I slipped the word *possibly* in the sentence above because there are situations
    where a nonlinear dependence between two features might lead to a zero Pearson
    correlation coefficient. These situations are not common, however, and for our
    purposes, we can claim a correlation coefficient near zero indicates the two features
    are independent. The closer the correlation coefficient is to zero, either positive
    or negative, the weaker the correlation between the features.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上面的句子中加入了*可能*这个词，因为在某些情况下，两个特征之间的非线性依赖关系可能会导致Pearson相关系数为零。然而，这种情况并不常见，对于我们的目的来说，我们可以认为相关系数接近零意味着这两个特征是独立的。相关系数越接近零，无论是正数还是负数，特征之间的相关性就越弱。
- en: The Pearson correlation is defined using the means of the two features or the
    means of products of the two features. The inputs are two features, two columns
    of the dataset. We’ll call these inputs *X* and *Y*, where the capital letter
    refers to a vector of data values. Note, since these are two features from the
    dataset, *X[i]* is paired with *Y[i]*, meaning they both come from the same feature
    vector.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the Pearson correlation coefficient is
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ05.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'We’ve introduced a new, but commonly used, notation. The mean of *X* is the
    *expectation* of *X*, denoted as E(*X*). Therefore, in [Equation 4.5](ch04.xhtml#ch04equ05),
    we see the mean of *X*, E(*X*), and the mean of *Y*, E(*Y*). As we might suspect,
    E(*XY*) is the mean of the product of *X* and *Y*, element by element. Similarly,
    E(*X*²) is the mean of the product of *X* with itself, and E(*X*)² is the square
    of the mean of *X*. With this notation in hand, we can easily write our own function
    to calculate the Pearson correlation of two vectors of features:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'def pearson(x,y):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: exy = (x*y).mean()
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: ex = x.mean()
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ey = y.mean()
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: exx = (x*x).mean()
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: ex2 = x.mean()**2
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: eyy = (y*y).mean()
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: ey2 = y.mean()**2
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: return (exy - ex*ey)/(np.sqrt(exx-ex2)*np.sqrt(eyy-ey2))
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The pearson function directly implements [Equation 4.5](ch04.xhtml#ch04equ05).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up a scenario where we can use pearson and compare it to what NumPy
    and SciPy provide. The code that follows, including the definition of pearson
    above, is in the file *correlation.py*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll create three correlated vectors, x, y, and z. We imagine that these
    are features from a dataset so that x[0] is paired with y[0] and z[0]. The code
    we need is
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed(8675309)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: N = 100
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: x = np.linspace(0,1,N) + (np.random.random(N)-0.5)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: y = np.random.random(N)*x
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: z = -0.1*np.random.random(N)*x
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we’re again fixing the NumPy pseudorandom seed to make the output
    reproducible. The first feature, x, is a noisy line from zero to one. The second,
    y, tracks x but is also noisy because of the multiplication by a random value
    in [0, 1). Finally, z is negatively correlated to x because of the –0.1 coefficient.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The top chart in [Figure 4-6](ch04.xhtml#ch04fig06) plots the three feature
    values sequentially to see how they track each other. The bottom chart shows the
    three as paired points, with one value on the x-axis and the other on the y-axis.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig06.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-6: Three features in sequence to show how they track (top), and a
    scatter plot of the features as pairs (bottom)*'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'The NumPy function to calculate the Pearson correlation is np.corrcoef. Unlike
    our version, this function returns a matrix showing the correlations between all
    pairs of variables passed to it. For example, using our pearson function, we get
    the following as the correlation coefficients between x, y, and z:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: pearson(x,y):  0.682852
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'pearson(x,z): -0.850475'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'pearson(y,z): -0.565361'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy returns the following, with x, y, and z stacked as a single 3 × 100 array:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = np.vstack((x,y,z))'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.corrcoef(d))'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 1.          0.68285166 -0.85047468]'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0.68285166  1.         -0.56536104]'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[-0.85047468 -0.56536104  1.        ]]'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal corresponds to the correlation with each feature and itself, which
    is naturally perfect and therefore 1.0\. The correlation between x and y is in
    element 0,1 and matches our pearson function value. Similarly, the correlation
    between x and z is in element 0,2, and the correlation between y and z is in element
    1,2\. Notice also that the matrix is symmetric, which we expect because corr(*X,
    Y*) = corr(*Y, X*).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: SciPy’s correlation function is stats.pearsonr, which acts like ours but returns
    a *p*-value along with the *r* value. We’ll discuss *p*-values more later in the
    chapter. We use the returned *p*-value as the probability of an uncorrelated system
    producing the calculated correlation value. For our example features, the *p*-value
    is virtually identical to zero, implying there’s no reasonable likelihood that
    an uncorrelated system produced the features.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'We stated earlier that for images, nearby pixels are usually highly correlated.
    Let’s see if this is actually true for a sample image. We’ll use the China image
    included with sklearn and treat specific rows of the green band as the paired
    vectors. We’ll calculate the correlation coefficient for two adjacent rows, a
    row further away, and a random vector:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '>>> from sklearn.datasets import load_sample_image'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '>>> china = load_sample_image(''china.jpg'')'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = china[230,:,1].astype("float64")'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = china[231,:,1].astype("float64")'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '>>> c = china[400,:,1].astype("float64")'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = np.random.random(640)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '>>> pearson(a,b)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '0.8979360'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '>>> pearson(a,c)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '-0.276082'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '>>> pearson(a,d)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '-0.038199'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Comparing row 230 and row 231 shows that they are highly positively correlated.
    Comparing rows 230 and 400 shows a weaker and, in this case, negative correlation.
    Finally, as we might expect, correlation with a random vector gives a value approaching
    zero.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The Pearson correlation coefficient is so widely used that you’ll often see
    it referred to as merely *the correlation coefficient*. Let’s now take a look
    at a second correlation function and see how it differs from the Pearson coefficient.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Spearman Correlation
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second correlation measure we’ll explore is the *Spearman correlation coefficient*,
    ρ ϵ [–1, +1]. It’s a measure based on the ranks of the feature values instead
    of the values themselves.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: To rank *X*, we replace each value in *X* with the index to that value in the
    sorted version of *X*. If *X* is
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[86, 62, 28, 43, 3, 92, 38, 87, 74, 11]'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: then the ranks are
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[7, 5, 2, 4, 0, 9, 3, 8, 6, 1]'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: because when *X* is sorted, 86 goes in the eighth place (counting from zero),
    and 3 goes first.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The Pearson correlation looks for a linear relationship, whereas the Spearman
    looks for any monotonic association between the inputs.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: If we have the ranks for the feature values, then the Spearman coefficient is
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经获得了特征值的排名，那么 Spearman 系数就是
- en: '![image](Images/04equ06.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ06.jpg)'
- en: where *n* is the number of samples and *d* = rank(*X*) – rank(*Y*) is the difference
    of the rank of the paired *X* and *Y* values. Note how [Equation 4.6](ch04.xhtml#ch04equ06)
    is only valid if the rankings are unique (that is, there are no repeated values
    in *X* or *Y*).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是样本数量，*d* = rank(*X*) – rank(*Y*) 是配对 *X* 和 *Y* 值的排名差。注意，[公式 4.6](ch04.xhtml#ch04equ06)
    仅在排名唯一的情况下有效（即 *X* 或 *Y* 中没有重复值）。
- en: To calculate *d* in [Equation 4.6](ch04.xhtml#ch04equ06), we need to rank *X*
    and *Y* and use the difference of the ranks. The Spearman correlation is the Pearson
    correlation of the ranks.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在[公式 4.6](ch04.xhtml#ch04equ06)中计算 *d* 时，我们需要对 *X* 和 *Y* 进行排名，并使用排名的差值。Spearman
    相关系数是排名的 Pearson 相关系数。
- en: 'The example above points the way to an implementation of the Spearman correlation:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子展示了 Spearman 相关系数实现的方式：
- en: import numpy as np
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: 'def spearman(x,y):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 'def spearman(x,y):'
- en: n = len(x)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: n = len(x)
- en: t = x[np.argsort(x)]
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: t = x[np.argsort(x)]
- en: rx = []
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: rx = []
- en: 'for i in range(n):'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(n):'
- en: rx.append(np.where(x[i] == t)[0][0])
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: rx.append(np.where(x[i] == t)[0][0])
- en: rx = np.array(rx, dtype="float64")
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: rx = np.array(rx, dtype="float64")
- en: t = y[np.argsort(y)]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: t = y[np.argsort(y)]
- en: ry = []
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ry = []
- en: 'for i in range(n):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(n):'
- en: ry.append(np.where(y[i] == t)[0][0])
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ry.append(np.where(y[i] == t)[0][0])
- en: ry = np.array(ry, dtype="float64")
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ry = np.array(ry, dtype="float64")
- en: d = rx - ry
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: d = rx - ry
- en: return 1.0 - (6.0/(n*(n*n-1)))*(d**2).sum()
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: return 1.0 - (6.0/(n*(n*n-1)))*(d**2).sum()
- en: To get the ranks, we need to first sort *X* (t). Then, for each value in *X*
    (x), we find where it occurs in t via np.where and take the first element, the
    first match. After building the rx list, we make it a floating-point NumPy array.
    We do the same for *Y* to get ry. With the ranks, d is set to their difference,
    and [Equation 4.6](ch04.xhtml#ch04equ06) is used to return the Spearman ρ value.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取排名，我们首先需要对 *X* (t) 进行排序。然后，对于 *X* (x) 中的每个值，我们通过 np.where 找出它在 t 中的位置，并取第一个元素，即第一个匹配项。构建完
    rx 列表后，我们将其转化为浮点型的 NumPy 数组。对 *Y* 也进行同样的操作，得到 ry。通过这些排名，d 设置为它们的差值，然后使用[公式 4.6](ch04.xhtml#ch04equ06)来返回
    Spearman ρ 值。
- en: Please note that this version of the Spearman correlation is limited by [Equation
    4.6](ch04.xhtml#ch04equ06) and should be used when there are no duplicate values
    in *X* or *Y*. Our example in this section uses random floating-point values,
    so the probability of an exact duplicate is quite low.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个版本的 Spearman 相关系数受限于[公式 4.6](ch04.xhtml#ch04equ06)，当 *X* 或 *Y* 中没有重复值时使用。本节中的例子使用的是随机浮点数值，因此完全重复的概率非常低。
- en: 'We’ll compare our spearman implementation to the SciPy version, stats .spearmanr.
    Like the SciPy version of the Pearson correlation, stats.spearmanr returns a *p*-value.
    We’ll ignore it. Let’s see how our function compares:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们实现的 Spearman 相关系数与 SciPy 版本的 stats.spearmanr 进行比较。就像 SciPy 版本的 Pearson
    相关系数一样，stats.spearmanr 返回一个 *p* 值。我们将忽略它。让我们看看我们的函数和 SciPy 版本的比较：
- en: '>>> from scipy.stats import spearmanr'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from scipy.stats import spearmanr'
- en: '>>> print(spearman(x,y), spearmanr(x,y)[0])'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(spearman(x,y), spearmanr(x,y)[0])'
- en: 0.694017401740174 0.6940174017401739
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 0.694017401740174 0.6940174017401739
- en: '>>> print(spearman(x,z), spearmanr(x,z)[0])'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(spearman(x,z), spearmanr(x,z)[0])'
- en: -0.8950855085508551 -0.895085508550855
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: -0.8950855085508551 -0.895085508550855
- en: '>>> print(spearman(y,z), spearmanr(y,z)[0])'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(spearman(y,z), spearmanr(y,z)[0])'
- en: -0.6414041404140414 -0.6414041404140414
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: -0.6414041404140414 -0.6414041404140414
- en: We have complete agreement with the SciPy function out to the last bit or so
    of the floating-point value.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与 SciPy 函数的结果完全一致，直到浮点值的最后一位。
- en: 'It’s important to remember the fundamental difference between the Pearson and
    Spearman correlations. For example, consider the correlation between a linear
    ramp and the sigmoid function:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住 Pearson 相关系数和 Spearman 相关系数的根本区别。例如，考虑一个线性 ramp 和 sigmoid 函数之间的相关性：
- en: ramp = np.linspace(-20,20,1000)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ramp = np.linspace(-20,20,1000)
- en: sig = 1.0 / (1.0 + np.exp(-ramp))
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: sig = 1.0 / (1.0 + np.exp(-ramp))
- en: print(pearson(ramp,sig))
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: print(pearson(ramp,sig))
- en: print(spearman(ramp,sig))
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: print(spearman(ramp,sig))
- en: Here, ramp increases linearly from –20 to 20 and sig follows a sigmoid shape
    (“S” curve). The Pearson correlation will be on the high side, since both are
    increasing as *x* becomes more positive, but the association is not purely linear.
    Running the example gives
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ramp 从 -20 到 20 线性增加，而 sig 则遵循一个 S 型的 sigmoid 曲线。由于两者都在增加，所以 Pearson 相关系数会较高，因为
    *x* 越来越正，但关联关系并非完全线性。运行示例结果为：
- en: '0.905328'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '0.905328'
- en: '1.0'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: indicating a Pearson correlation of 0.9 but a perfect Spearman correlation of
    1.0, since for every increase in ramp there is an increase in sig and *only* an
    increase. The Spearman correlation has captured the nonlinear relationship between
    the arguments, while the Pearson correlation has only hinted at it. If we’re analyzing
    a dataset intended for a classical machine learning algorithm, the Spearman correlation
    might help us decide which features to keep and which to discard.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表明Pearson相关系数为0.9，但Spearman相关系数为1.0，因为每当坡道增加时，信号也随之增加，而且*仅仅*是增加。Spearman相关系数捕捉了两个变量之间的非线性关系，而Pearson相关系数只显示了它的线性趋势。如果我们正在分析一个经典机器学习算法的数据集，Spearman相关系数可能帮助我们决定保留哪些特征，丢弃哪些特征。
- en: This concludes our examination of statistics for describing and understanding
    data. Let’s now learn how to use hypothesis testing to interpret experimental
    results and answer questions like “Are these two sets of data samples from the
    same parent distribution?”
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对描述和理解数据的统计学考察的总结。现在让我们学习如何使用假设检验来解释实验结果，并回答诸如“这两组数据样本是否来自同一母体分布？”等问题。
- en: Hypothesis Testing
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假设检验
- en: We have two independent sets of 50 students studying cell biology. We have no
    reason to believe the groups differ in any significant way, as students from the
    larger population were assigned randomly. Group 1 attended the lectures and, in
    addition, worked through a structured set of computer exercises. Group 2 only
    attended the lectures. Both groups took the same final examination, leading to
    the test scores given in [Table 4-1](ch04.xhtml#ch04tab01). We want to know if
    asking the students to work through the computer exercises made a difference in
    their final test scores.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两组独立的50名细胞生物学的学生。我们没有理由认为这两组存在显著差异，因为这些学生是从更大的群体中随机分配的。第一组学生参加了讲座，并且完成了一组结构化的计算机练习。第二组学生仅参加了讲座。两组学生都参加了相同的期末考试，并得到了[表4-1](ch04.xhtml#ch04tab01)中的成绩。我们想知道要求学生完成计算机练习是否对他们的期末考试成绩产生了影响。
- en: '**Table 4-1:** Group 1 and Group 2 Test Scores'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-1：** 第一组和第二组的测试成绩'
- en: '| **Group 1** | 81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88
    89 92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85
    85 78 94 100 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| **第一组** | 81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88 89
    92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85 85
    78 94 100 |'
- en: '| **Group 2** | 92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86
    77 94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88
    91 83 85 73 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| **第二组** | 92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86 77
    94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88 91
    83 85 73 |'
- en: '[Figure 4-7](ch04.xhtml#ch04fig07) shows a box plot of [Table 4-1](ch04.xhtml#ch04tab01).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-7](ch04.xhtml#ch04fig07)展示了[表4-1](ch04.xhtml#ch04tab01)的箱线图。'
- en: To understand if there is a significant change in final test scores between
    the two groups, we need to test some hypotheses. The method we’ll use to test
    the hypotheses is known as *hypothesis testing*, and it’s a critical piece of
    modern science.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解两组之间的最终考试成绩是否有显著变化，我们需要进行假设检验。我们将使用的假设检验方法被称为*假设检验*，这是现代科学中的一个关键部分。
- en: '![image](Images/04fig07.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig07.jpg)'
- en: '*Figure 4-7: Box plot for the data in [Table 4-1](ch04.xhtml#ch04tab01)*'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-7：表4-1数据的箱线图*'
- en: 'Hypothesis testing is a broad topic, too extensive for us to provide more than
    a minimal introduction here. As this is a book on deep learning, we’ll focus on
    the scenario a deep learning researcher is likely to encounter. We’ll consider
    only two hypothesis tests: the t-test for unpaired samples of differing variance
    (a parametric test) and the Mann-Whitney U (a nonparametric test). As we progress,
    we’ll understand what these tests are and why we’re restricting ourselves to them,
    as well as the meaning of *parametric* and *nonparametric*.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验是一个广泛的主题，太过庞大，无法在此提供全面的介绍。由于这是一本关于深度学习的书，我们将专注于深度学习研究人员可能遇到的情景。我们将仅考虑两种假设检验：用于具有不同方差的未配对样本的t检验（参数检验）和Mann-Whitney
    U检验（非参数检验）。在接下来的讨论中，我们将了解这些检验是什么，为什么我们仅限于这两种检验，以及*参数检验*和*非参数检验*的含义。
- en: To be successful with hypothesis testing, we need to know what we mean by *hypothesis*,
    so we’ll address that first, along with our rationale for limiting the types of
    hypothesis testing we’ll consider. With the hypothesis concept in hand, we’ll
    discuss the t-test and the Mann-Whitney U test in turn, using the data in [Table
    4-1](ch04.xhtml#ch04tab01) as our example. Let’s get started.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功进行假设检验，我们需要理解*假设*的含义，因此我们首先会讨论这个概念，以及我们为何限制只考虑某些类型的假设检验。掌握了假设的概念后，我们将依次讨论
    t 检验和 Mann-Whitney U 检验，并使用[表 4-1](ch04.xhtml#ch04tab01)中的数据作为示例。让我们开始吧。
- en: Hypotheses
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 假设
- en: To understand if two sets of data are from the same parent distribution or not,
    we might look at summary statistics. [Figure 4-7](ch04.xhtml#ch04fig07) shows
    us the box plot for Group 1 and Group 2\. It appears that the two groups have
    different means and standard deviations. How do we know? The box plot shows us
    the location of the medians, and the whiskers tell us something about the variance.
    Both of these together hint that the means will be different because the medians
    are different, and both sets of data are reasonably symmetric around the median.
    The space between the whiskers hints at the standard deviation. So, let’s make
    hypotheses using the means of the datasets.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解两组数据是否来自相同的母体分布，我们可以查看汇总统计数据。[图 4-7](ch04.xhtml#ch04fig07)展示了第一组和第二组的箱型图。看起来这两组数据的均值和标准差不同。我们怎么知道呢？箱型图展示了中位数的位置，胡须部分则告诉我们一些关于方差的信息。这两者结合起来提示均值会有所不同，因为中位数不同，而且两组数据在中位数周围大致对称。胡须之间的空间则暗示了标准差。因此，我们可以利用数据集的均值来提出假设。
- en: In hypothesis testing, we have two hypotheses. The first, known as the *null
    hypothesis* (*H*[0]), is that the two sets of data *are* from the same parent
    distribution, that there is nothing special to differentiate them. The second
    hypothesis, the *alternative hypothesis* (*H[a]*), is that the two groups are
    not from the same distribution. Since we’ll be using the means, *H*[0] is saying
    that the means, really the means of the parent population that generated the data,
    are the same. Similarly, if we reject *H*[0], we are implicitly accepting *H[a]*
    and claiming we have evidence that the means are different. We don’t have the
    true population means, so we’ll use the sample means and standard deviations instead.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设检验中，我们有两个假设。第一个是假设被称为*原假设*（*H*[0]），即两组数据*来自*相同的母体分布，它们之间没有任何特别的区别。第二个假设是*备择假设*（*H[a]*），即这两组数据不来自相同的分布。由于我们将使用均值，*H*[0]
    表示均值，实际上是生成数据的母体的均值，是相同的。同样，如果我们拒绝*H*[0]，我们隐含地接受了*H[a]*，并声明我们有证据表明均值不同。由于我们没有真实的母体均值，因此我们将使用样本均值和标准差。
- en: Hypothesis testing doesn’t tell us definitively whether *H*[0] is true. Instead,
    it gives us evidence in favor of rejecting or accepting the null hypothesis. It’s
    critical to remember this.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验并不能明确告诉我们*H*[0]是否成立。相反，它为我们提供了接受或拒绝原假设的证据。记住这一点至关重要。
- en: We’re testing two independent samples to see if we should think of them as coming
    from the same parent distribution. There are other ways to use hypothesis testing,
    but we rarely encounter them in deep learning. For the task at hand, we need the
    sample means and the sample standard deviations. Our tests will ask the question,
    “Is there a meaningful difference in the means of these two sets?”
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在测试两个独立样本，看看是否应该认为它们来自相同的母体分布。还有其他使用假设检验的方法，但在深度学习中我们很少遇到它们。对于当前任务，我们需要样本的均值和样本的标准差。我们的检验会提出这样一个问题：“这两组数据的均值是否存在显著差异？”
- en: We’re only interested in detecting whether the two groups of data are from the
    same parent distribution, so another simplification we’ll make is that all of
    our tests will be *two-sided*, or *two-tailed*. When we use a test, like the t-test
    we’ll describe next, we’re comparing our calculated test statistic (the t-value)
    to the distribution of the test statistic and asking questions about how likely
    our calculated t-value is. If we want to know about the test statistic being above
    or below some fraction of that distribution, we’re making a two-sided test. If
    instead we want to know about the likelihood of the test statistic being above
    a particular value without caring about it being below, or vice versa, then we’re
    making a one-sided test.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s lay out our assumptions and approach:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: We have two independent sets of data we wish to compare.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re making no assumption as to whether the standard deviations of the data
    are the same.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our null hypothesis is that the means of the parent distributions of the datasets
    are the same, *H*[0] : μ[1] = μ[2]. We’ll use the sample means ![image](Images/094equ01a.jpg)
    and sample standard deviations (*s*[1], *s*[2]) to help us decide to accept or
    reject *H*[0].'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hypothesis tests assume that the data is *independent and identically distributed
    (i.i.d.)*. We interpret this as a statement that the data is a fair random sample.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these assumptions understood, let’s start with the t-test, the most widely
    used hypothesis test.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: The t-test
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *t-test* depends on *t*, the test statistic. This statistic is compared
    to the t-distribution and used to generate a *p*-value, a probability we’ll use
    to reach a conclusion about *H*[0]. There’s a rich history behind the t-test and
    the related z-test that we’ll ignore here. I encourage you to dive more deeply
    into hypothesis testing when you have the chance or, at a minimum, review thoughtful
    articles about the proper way to do a hypothesis test and interpret its results.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: The t-test is a *parametric* test. This means there are assumptions about the
    data and the distribution of the data. Specifically, the t-test assumes, beyond
    the data being i.i.d., that the distribution (histogram) of the data is normal.
    We’ve stated before that many physical processes do seem to follow a normal distribution,
    so there’s reason to think that data from actual measurements might do so.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to test if a dataset is normally distributed, but we’ll
    ignore them, as there’s some debate about the utility of such tests. Instead,
    I’ll (somewhat recklessly) suggest you use the t-test and the Mann-Whitney U test
    together to help make your decision about accepting or rejecting *H*[0]. Using
    both tests might lead to a situation where they disagree, where one test says
    there’s evidence against the null hypothesis and the other says there isn’t. In
    general, if the nonparametric test is claiming evidence against *H*[0], then one
    should probably accept that evidence regardless of the t-test result. If the t-test
    result is against *H*[0], but the Mann-Whitney U test isn’t, and you think the
    data is normal, then you might also accept the t-test result.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: The t-test has different versions. We explicitly stated above that we’ll use
    a version designed for datasets of differing size and variance. The specific version
    of the t-test we’ll use is *Welch’s t-test*, which doesn’t assume the variance
    of the two datasets is the same.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: The t-score for Welch’s t-test is
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/095equ01.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: where *n*[1] and *n*[2] are the size of the two groups.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The t-score, and an associated value known as the *degrees of freedom*, which
    is similar to but also different from the degrees of freedom mentioned above,
    generates the appropriate t-distribution curve. To get a *p*-value, we calculate
    the area under the curve, both above and below (positive and negative t-score),
    and return it. Since the integral of a probability distribution is 1, the total
    area under the tails from the positive and negative t-score value to positive
    and negative infinity will be the *p*-value. We’ll use the degrees of freedom
    below to help us calculate confidence intervals.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: What does the *p*-value tell us? It tells us the probability of seeing the difference
    between the two means we see, or larger, *if* the null hypothesis is true. Typically,
    if this probability is below some threshold we’ve chosen, we reject the null hypothesis
    and say we have evidence that the two groups have different means—that they come
    from different parent distributions. When we reject *H*[0], we say that the difference
    is *statistically significant*. The threshold for accepting/rejecting *H*[0] is
    called α, usually with α = 0.05 as a typical, if problematic, value. We’ll discuss
    why 0.05 is problematic below.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'The point to remember is that the *p*-value assumes the null hypothesis is
    true. It tells us the likelihood of a true *H*[0] giving us at least the difference
    we see, or greater, between the groups. If the *p*-value is small, that has two
    possible meanings: (1) the null hypothesis is false, or (2) a random sampling
    error has given us samples that fall outside what we might expect. Since the *p*-value
    assumes *H*[0] is true, a small *p*-value helps us believe less and less in (2)
    and boosts our confidence that (1) might be correct. However, the *p*-value alone
    cannot confirm (1); other knowledge needs to come into play.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'I mentioned that using α = 0.05 is problematic. The main reason it’s problematic
    is that it’s too generous; it leads to too many rejections of a true null hypothesis.
    According to James Berger and Thomas Sellke in their article “Testing a Point
    Null Hypothesis: The Irreconcilability of *P* Values and Evidence” (*Journal of
    the American Statistical Association*, 1987), when α = 0.05, about 30 percent
    of true null hypotheses will be rejected. When we use something like α ≤ 0.001,
    the chance of falsely rejecting a true null hypothesis goes down to less than
    3 percent. The moral of the story is that *p* < 0.05 is not magic and, frankly,
    is unconvincing for a single study. Look for highly significant *p*-values of
    at least 0.001 or, preferably, much smaller. At *p*  = 0.05, all you have is a
    suggestion, and you should repeat the experiment. If repeated experiments all
    have a *p*-value around 0.05, then rejecting the null hypothesis begins to make
    sense.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Confidence Intervals
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Along with a *p*-value, you’ll often see *confidence intervals (CIs)*. The confidence
    interval gives bounds within which we believe the true population difference in
    the means will lie, with a given confidence for repeated samples of the two datasets
    we’re comparing. Typically, we report 95 percent confidence intervals. Our hypothesis
    tests check for equality of means by asking if the difference of the sample means
    is zero or not. Therefore, any CI that includes zero signals to us that we cannot
    reject the null hypothesis.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: For Welch’s t-test, the degrees of freedom is
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ07.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: which we can use to calculate confidence intervals,
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ08.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: where *t*[1–α/2,*df*] is the critical value, and the t-value for the given confidence
    level (α) and the degrees of freedom, *df*, come from [Equation 4.7](ch04.xhtml#ch04equ07).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'How should we interpret the 95 percent confidence interval? There is a population
    value: the true difference between the group means. The 95 percent confidence
    interval is such that if we could draw repeated samples from the distribution
    that produced the two datasets, 95 percent of the calculated confidence intervals
    would contain the true difference between the means. It is *not* the range that
    includes the true difference in the means at 95 percent certainty.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Beyond checking if zero is in the CI, the CI is useful because its width tells
    us something about the magnitude of the effect. Here, the effect is related to
    the difference between the means. We may have a statistically significant difference
    based on the *p*-value, but the effect might be practically meaningless. The CI
    will be narrow when the effect is large because small CIs imply a narrow range
    encompassing the true effect. We’ll see shortly how, when possible, to calculate
    another useful measure of effect.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a *p*-value less than α also will have a *CI*[α] that does not include
    *H*[0]. In other words, what the *p*-value tells us and what the confidence interval
    tells us track–they will not contradict each other.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Effect Size
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It’s one thing to have a statistically significant *p*-value. It’s another for
    the difference represented by that *p*-value to be meaningful in the real world.
    A popular measure of the size of an effect, the *effect size*, is *Cohen’s d*.
    For us, since we’re using Welch’s t-test, Cohen’s *d* is found by calculating
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04equ09.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: Cohen’s *d* is usually interpreted subjectively, though we should report the
    numeric value as well. Subjectively, the size of the effect could be
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '| ***d*** | **Effect** |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | Small |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | Medium |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| 0.8 | Large |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: Cohen’s *d* makes sense. The difference between the means is a natural way to
    think about the effect. Scaling it by the mean variance puts it in a consistent
    range. From [Equation 4.9](ch04.xhtml#ch04equ09), we see that a *p*-value corresponding
    to a statistically significant result might lead to a small effect that isn’t
    of any true practical importance.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Test Scores
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s put all of the above together to apply the t-test to our test data from
    [Table 4-1](ch04.xhtml#ch04tab01). You’ll find the code in the file *hypothesis.py*.
    We generate the data-sets first:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: np.random.seed(65535)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: a = np.random.normal(85,6,50).astype("int32")
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: a[np.where(a > 100)] = 100
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: b = np.random.normal(82,7,50).astype("int32")
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: b[np.where(b > 100)] = 100
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we’re using a fixed NumPy pseudorandom number seed for repeatability.
    We make a a sample from a normal distribution with a mean of 85 and a standard
    deviation of 6.0\. We select b from a normal distribution with a mean of 82 and
    a standard deviation of 7.0\. For both, we cap any values over 100 to 100\. These
    are test scores, after all, without extra credit.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the t-test next:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: from scipy.stats import ttest_ind
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: t,p = ttest_ind(a,b, equal_var=False)
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: print("(t=%0.5f, p=%0.5f)" % (t,p))
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: We get *(t* = 2.40234, *p* = 0.01852). The *t* is the statistic, and *p* is
    the computed *p*-value. It’s 0.019, which is less than 0.05 but only by a factor
    of two. We have a weak result telling us we might want to reject the null hypothesis
    and believe that the two groups, a and b, come from different distributions. Of
    course, we know they do because we generated them, but it’s nice to see the test
    pointing in the right direction.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the function we import from SciPy is ttest_ind. This is the function
    to use for independent samples, which are not paired. Also, notice that we added
    equal_var=False to the call. This is how to use Welch’s t-test, which doesn’t
    assume that the variance between the two datasets is equal. We know they’re not
    equal, since a uses a standard deviation of 6.0 while b uses 7.0.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the confidence intervals, we’ll write a CI function, since NumPy and
    SciPy don’t include one. The function directly implements [Equations 4.7](ch04.xhtml#ch04equ07)
    and [4.8](ch04.xhtml#ch04equ08):'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: from scipy import stats
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'def CI(a, b, alpha=0.05):'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: n1, n2 = len(a), len(b)
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: s1, s2 = np.std(a, ddof=1)**2, np.std(b, ddof=1)**2
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: df = (s1/n1 + s2/n2)**2 / ((s1/n1)**2/(n1-1) + (s2/n2)**2/(n2-1))
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: tc = stats.t.ppf(1 - alpha/2, df)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: lo = (a.mean()-b.mean()) - tc*np.sqrt(s1/n1 + s2/n2)
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: hi = (a.mean()-b.mean()) + tc*np.sqrt(s1/n1 + s2/n2)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: return lo, hi
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: The critical *t* value is given by calling stats.t.ppf, passing in the α/2 value
    and the proper degrees of freedom, *df*. The critical *t* value is the 97.5 percent
    percentile value, for α = 0.05, which is what the *percent point function (ppf)*
    returns. We divide by two to cover the tails of the t-distribution.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'For our test example, the confidence interval is [0.56105, 5.95895]. Notice
    how this does not include zero, so the CI also indicates a statistically significant
    result. However, the range is rather large, so this is not a particularly robust
    result. The CI range can be difficult to interpret on its own, so, finally, let’s
    calculate Cohen’s *d* to see if it makes sense given the width of the confidence
    interval. In code, we implement [Equation 4.9](ch04.xhtml#ch04equ09):'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'def Cohen_d(a,b):'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: s1 = np.std(a, ddof=1)**2
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: s2 = np.std(b, ddof=1)**2
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: return (a.mean() - b.mean()) / np.sqrt(0.5*(s1+s2))
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: We get *d* = 0.48047, corresponding to a medium effect size.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: The Mann-Whitney U Test
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The t-test assumes the distribution of the source data is normal. If the data
    is not normally distributed, we should instead use a *nonparametric test*. Nonparametric
    tests make no assumptions about the underlying distribution of the data. The *Mann-Whitney
    U test*, sometimes called the *Wilcoxon rank-sum test*, is a nonparametric test
    to help decide if two different sets of data come from the same parent distribution.
    The Mann-Whitney U test does not rely directly on the values of the data, but
    instead uses the data’s ranking.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'The null hypothesis for this test is the following: the probability that a
    randomly selected value from Group 1 is larger than a randomly selected value
    from Group 2 is 0.5\. Let’s think a bit about that. If the data is from the same
    parent distribution, then we should expect any randomly selected pair of values
    from the two groups to show no preference as to which is larger than the other.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: The alternative hypothesis is that the probability of a randomly selected value
    from Group 1 being larger than a randomly selected value from Group 2 is not 0.5\.
    Notice, there is no statement as to the probability being greater or less than
    0.5, only that it isn’t 0.5; thus, the Mann-Whitney U test, as we’ll use it, is
    two-sided.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis for the Mann-Whitney U test is not the same as the null
    hypothesis for the t-test. For the t-test, we’re asking whether the means between
    the two groups are the same. (Really, we’re asking if the difference in the means
    is zero.) However, if two sets of data *are* from different parent distributions,
    both null hypotheses are false, so we can use the Mann-Whitney U test in place
    of the t-test, especially when the underlying data is not normally distributed.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: To generate *U*, the Mann-Whitney statistic, we first pool both sets of data
    and rank them. Ties are replaced with the mean between the tie value rank and
    the next rank value. We also keep track of the source group so we can separate
    the list of ranks again. The ranks, by group, are summed to give *R*[1] and *R*[2]
    (using the ranks from the pooled data). We calculate two values,
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/100equ01.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: with the smaller called *U*, the test statistic. It’s possible to generate a
    *p*-value from *U*, keeping in mind all the discussion above about the meaning
    and use of *p*-values. As before, *n*[1] and *n*[2] are the number of samples
    in the two groups. The Mann-Whitney U test requires the smaller of these two numbers
    to be at least 21 samples. If you don’t have that many, the results may not be
    reliable when using the SciPy mannwhitneyu function.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: We can run the Mann-Whitney U test on our test data from [Table 4-1](ch04.xhtml#ch04tab01),
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: from scipy.stats import mannwhitneyu
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: u,p = mannwhitneyu(a,b)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: print("(U=%0.5f, p=%0.5f)" % (u,p))
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: with a and b as we used above for the t-test. This gives us *(U* = 997.00000,
    *p* = 0.04058). The *p*-value is barely below the minimum threshold of 0.05.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: The means of a and b are 85 and 82, respectively. What happens to the *p*-values
    if we make the mean value of b 83 or 81? Changing the mean of b means changing
    the first argument to np.random.normal. Doing this gives us [Table 4-2](ch04.xhtml#ch04tab02),
    where I’ve included all results for completeness.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 4-2:** Mann-Whitney U Test and t-test Results for the Simulated Test
    Scores with Different Means (*n*[1]=*n*[2]=50)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '| **Means** | **Mann-Whitney U** | **t-test** |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| 85 vs. 83 | (*U*=1104.50000, *p*=0.15839) | (*t*=1.66543, *p*=0.09959) |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| 85 vs. 82 | (*U*=997.00000, *p*=0.04058) | (*t*=2.40234, *p*=0.01852) |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| 85 vs. 81 | (*U*=883.50000, *p*=0.00575) | (*t*=3.13925, *p*=0.00234) |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '[Table 4-2](ch04.xhtml#ch04tab02) should make sense to us. When the means are
    close, it’s harder to tell them apart, so we expect larger *p*-values. Recall
    how we have only 50 samples in each group. As the difference between the means
    increases, the *p*-values go down. A difference of three in the means leads to
    barely significant *p*-values. When the difference is larger still, the *p*-values
    become truly significant—again, as we expect.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis above begs the question: for a small difference in the means between
    the two groups, how do the *p*-values change as a function of the sample size?'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-8](ch04.xhtml#ch04fig08) shows the *p*-value (mean ± standard error)
    over 25 runs for both the Mann-Whitney U test and the t-test as a function of
    sample size for the case where the means are 85 and 84.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/04fig08.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4-8: Mean p-value as a function of sample size for a difference in
    the sample means of one, ![image](Images/101equ01.jpg)*'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Small datasets make it difficult to differentiate between cases when the difference
    in the means is small. We also see that larger sample sizes reveal the difference,
    regardless of the test. It is interesting that in [Figure 4-8](ch04.xhtml#ch04fig08),
    the Mann-Whitney U *p*-value is less than that of the t-test even though the underlying
    data is normally distributed. Conventional wisdom states that it’s usually the
    other way around.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-8](ch04.xhtml#ch04fig08) is an object lesson in the power of large-sample
    tests to detect real differences. When the sample size is large enough, a weak
    difference becomes significant. However, we need to balance this with the effect
    size. When we have 1,000 samples in each group, we have a statistically significant
    *p*-value, but we also have a Cohen’s *d* of about 0.13, signaling a weak effect.
    A large sample study might find a significant effect that is so weak as to be
    practically meaningless.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter touched on the key aspects of statistics you’ll encounter during
    your sojourn through the world of deep learning. Specifically, we learned about
    different types of data and how to ensure the data is useful for building models.
    We then learned about summary statistics and saw examples that used them to help
    us understand a dataset. Understanding our data is key to successful deep learning.
    We investigated the different types of means, learned about measures of variation,
    and saw the utility of visualizing the data via box plots.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Missing data is a bane of deep learning. In this chapter, we investigated how
    to compensate for missing data. Next, we discussed correlation, how to detect
    and measure the relationships between elements of a dataset. Finally, we introduced
    hypothesis testing. Restricting ourselves to the most likely scenario we’ll encounter
    in deep learning, we learned how to apply both the t-test and the Mann-Whitney
    U test. Hypothesis testing introduced us to the *p*-value. We saw examples of
    it and discussed how to interpret it correctly.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we’ll leave statistics behind and dive headfirst into the
    world of linear algebra. Linear algebra is how we implement neural networks.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
