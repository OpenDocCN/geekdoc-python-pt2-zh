- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: STATISTICS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Bad datasets lead to bad models. We’d like to understand our data before we
    build a model, and then use that understanding to create a useful dataset, one
    that leads to models that do what we expect them to do. Knowing basic statistics
    will enable us to do just that.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 糟糕的数据集会导致糟糕的模型。在构建模型之前，我们希望了解我们的数据，然后利用这些理解创建一个有用的数据集，进而生成符合预期的模型。掌握基本的统计学将帮助我们做到这一点。
- en: A *statistic* is any number that’s calculated from a sample and used to characterize
    it in some way. In deep learning, when we talk about samples, we’re usually talking
    about datasets. Maybe the most basic statistic is the arithmetic mean, commonly
    known as the average. The mean of a dataset is a single-number summary of the
    dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计量*是从样本中计算得出的任何数字，并以某种方式用来描述它。在深度学习中，当我们谈论样本时，通常是指数据集。也许最基本的统计量是算术平均数，通常称为平均值。数据集的平均值是数据集的一个单一数字摘要。'
- en: We’ll see many different statistics in this chapter. We’ll begin by learning
    about the types of data and characterizing a dataset with summary statistics.
    Next, we’ll learn about quantiles and plotting data to understand what it contains.
    After that comes a discussion of outliers and missing data. Datasets are seldom
    perfect, so we need to have some way of detecting bad data and dealing with missing
    data. We’ll follow our discussion of imperfect datasets with a discussion of the
    correlation between variables. Then we’ll close the chapter out by discussing
    hypothesis testing, where we attempt to answer questions like “How likely is it
    that the same parent process generated two datasets?” Hypothesis testing is widely
    used in science, including deep learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将看到许多不同的统计量。我们将从学习数据类型和用总结性统计量描述数据集开始。接下来，我们将学习分位数并绘制数据以了解其内容。之后将讨论异常值和缺失数据。数据集很少是完美的，因此我们需要有某种方式来检测不良数据并处理缺失数据。我们在讨论不完美的数据集之后，会讨论变量之间的相关性。最后，我们将通过讨论假设检验来结束本章，假设检验帮助我们回答诸如“相同的父进程生成两个数据集的可能性有多大？”这样的问题。假设检验在科学中被广泛应用，包括深度学习。
- en: Types of Data
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据类型
- en: The four types of data are nominal, ordinal, interval, and ratio. Let’s look
    at each in turn.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 四种数据类型是名义数据、顺序数据、间隔数据和比率数据。让我们逐一看一下它们。
- en: Nominal Data
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 名义数据
- en: '*Nominal data*, sometimes called *categorical data*, is data that has no ordering
    between the different values. An example of this is eye color; there is no relationship
    between brown, blue, and green.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*名义数据*，有时称为*类别数据*，是没有不同值之间顺序关系的数据。一个例子是眼睛颜色；棕色、蓝色和绿色之间没有关系。'
- en: Ordinal Data
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 顺序数据
- en: For *ordinal data*, the data has a ranking or order, though differences aren’t
    meaningful in a mathematical sense. For example, if a questionnaire asks you to
    select from “strongly disagree,” “disagree,” “neutral,” “agree,” and “strongly
    agree,” it’s pretty clear that there is an order. Still, it’s also clear that
    “agree” isn’t three more than “strongly disagree.” All we can say is that “strongly
    disagree” is to the left of “agree” (and “neutral” and “disagree”).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*顺序数据*，数据具有排名或顺序，尽管在数学意义上这些差异并不重要。例如，如果问卷要求你从“强烈反对”、“反对”、“中立”、“同意”和“强烈同意”中选择，显然是有顺序的。不过，仍然可以看出，“同意”并不比“强烈反对”大三倍。我们能说的只是“强烈反对”在“同意”的左侧（以及“中立”和“反对”）。
- en: Another example of ordinal data is education level. If one person has a fourth-grade
    education and another has an eighth-grade education, we can say that the latter
    person is more educated than the former, but we can’t say that the latter person
    is twice as educated, because “twice as educated” has no fixed meaning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个顺序数据的例子是教育水平。如果一个人的教育水平是四年级，另一个人的教育水平是八年级，我们可以说后者的教育水平高于前者，但我们不能说后者的教育水平是前者的两倍，因为“教育水平是两倍”没有固定的意义。
- en: Interval Data
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 间隔数据
- en: '*Interval data* has meaningful differences. For example, if one cup of water
    is at 40 degrees Fahrenheit and another is at 80 degrees Fahrenheit, we can say
    that there is a 40-degree difference between the two cups of water. We can’t,
    however, say that there is twice as much heat in the second cup, because the zero
    for the Fahrenheit scale is arbitrary. Colloquially, we do say it’s twice as hot,
    but in reality, it isn’t. To see this, think about what happens if we change the
    temperature scale to another scale with an arbitrary, though more sensible, zero:
    the Celsius scale. We see that the first cup is at about 4.4 degrees Celsius,
    and the second is at 26.7 degrees Celsius. Clearly, the second cup doesn’t suddenly
    now have six times the heat of the first.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*区间数据*具有有意义的差异。例如，如果一杯水的温度是40华氏度，另一杯是80华氏度，我们可以说这两杯水之间的温差是40度。然而，我们不能说第二杯水的热量是第一杯的两倍，因为华氏度的零点是任意的。口语中我们确实会说第二杯更热，但实际上并非如此。要验证这一点，可以想象如果我们将温度量表换成另一个具有任意零点但更合理的量表——摄氏度。我们看到第一杯水的温度大约是4.4°C，第二杯是26.7°C。显然，第二杯水并没有比第一杯多六倍的热量。'
- en: Ratio Data
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比率数据
- en: Finally, *ratio data* is data where differences are meaningful, and there is
    a true zero point. Height is a ratio value because a height of zero is just that—no
    height at all. Similarly, age is also a ratio value because an age of zero means
    no age at all. If we were to adopt a new age scale and call a person zero when
    they reach, say, voting age, we’d then have an interval scale, not a ratio scale.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*比率数据*是指差异有意义并且具有真实零点的数据。身高是比率值，因为零身高就是没有身高。同样，年龄也是比率值，因为零岁意味着没有年龄。如果我们采用一个新的年龄量表，并在一个人达到某个年龄段（例如投票年龄）时称其为零，那么我们就得到了一个区间量表，而不是比率量表。
- en: Let’s look at temperature again. We said above that temperature is an interval
    quantity. This isn’t always the case. If we measure temperature in Fahrenheit
    or Celsius, then, yes, it is an interval quantity. However, if we measure temperature
    in Kelvin, the absolute temperature scale, then it becomes a ratio value. Why?
    Because a temperature of 0 Kelvin (or *K*) is just that, no temperature at all.
    If our first cup is at 40°F, 277.59 K, and the second is at 80°F, 299.82 K, then
    we can truthfully say that the second cup is 1.08 times hotter than the first,
    since (277.59)(1.08) ≈ 299.8.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再来看一下温度。我们之前提到温度是一个区间量。这并非总是如此。如果我们用华氏度或摄氏度来测量温度，那么确实它是一个区间量。然而，如果我们用开尔文度（绝对温标）来测量温度，它就变成了比率值。为什么？因为0开尔文（或*K*）温度就是真正的零温度，意味着没有任何温度。如果我们的第一杯水是40°F，277.59
    K，第二杯是80°F，299.82 K，那么我们可以真实地说第二杯水比第一杯热1.08倍，因为（277.59）（1.08）≈299.8。
- en: '[Figure 4-1](ch04.xhtml#ch04fig01) names the scales and shows their relationships
    to each other.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-1](ch04.xhtml#ch04fig01)标明了各个量表，并展示了它们之间的关系。'
- en: '![image](Images/04fig01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig01.jpg)'
- en: '*Figure 4-1: The four types of data*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-1：四种数据类型*'
- en: Each step in [Figure 4-1](ch04.xhtml#ch04fig01) from left to right adds something
    to the data that the type of data on the left is lacking. For nominal to ordinal,
    we add ordering. For ordinal to interval, we add meaningful differences. Lastly,
    moving from interval to ratio adds a true zero point.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-1](ch04.xhtml#ch04fig01)中的每一步，从左到右，都为数据增加了左边数据类型所缺乏的内容。从名义数据到有序数据，我们增加了排序；从有序数据到区间数据，我们增加了有意义的差异；最后，从区间数据到比率数据增加了一个真正的零点。'
- en: In practical use, as far as statistics are concerned, we should be aware of
    the types of data so we don’t do something meaningless. If we have a questionnaire,
    and the mean value of question A on a 1-to-5 rating scale is 2, while for question
    B it’s 4, we can’t say that B is rated twice as high as A, only that B was rated
    higher than A. What “twice” means in this context is unclear and quite probably
    meaningless.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，就统计学而言，我们应该了解数据的类型，以免做出没有意义的分析。如果我们有一份问卷，问题A在1到5的评分量表上的平均值为2，而问题B的平均值为4，我们不能说B的评分是A的两倍，只能说B的评分高于A。在这个背景下，“两倍”是什么意思并不明确，很可能是没有意义的。
- en: Interval and ratio data may be continuous (floating-points) or discrete (integers).
    From a deep learning perspective, models typically treat continuous and discrete
    data the same way, and we don’t need to do anything special for discrete data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 区间数据和比率数据可以是连续的（浮动数值）或离散的（整数）。从深度学习的角度看，模型通常将连续数据和离散数据同等对待，我们不需要对离散数据做任何特殊处理。
- en: Using Nominal Data in Deep Learning
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在深度学习中使用名义数据
- en: If we have a nominal value, say a set of colors, such as red, green, and blue,
    and we want to pass that value into a deep network, we need to change the data
    before we can use it. As we just saw, nominal data has no order, so while it’s
    tempting to assign a value of 1 to red, 2 to green, and 3 to blue, it would be
    wrong to do so, since the network will interpret those numbers as interval data.
    In that case, to the network, blue = 3(red), which is of course nonsense. If we
    want to use nominal data with a deep network, we need to alter it so that the
    interval is meaningful. We do this with *one-hot encoding*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个名义值，比如一组颜色，像红色、绿色和蓝色，我们想将这些值传递到深度网络中，在使用之前我们需要先对数据进行转换。正如我们刚才看到的，名义数据是没有顺序的，所以虽然我们可能会想把红色赋值为1，绿色赋值为2，蓝色赋值为3，但这样做是错误的，因为网络会将这些数字解释为区间数据。在这种情况下，对网络来说，蓝色
    = 3（红色），这显然是无意义的。如果我们想将名义数据与深度网络一起使用，我们需要改变它，使得区间具有实际意义。我们通过*独热编码*来实现这一点。
- en: 'In one-hot encoding, we turn the single nominal variable into a vector, where
    each element of the vector corresponds to one of the nominal values. For the color
    example, the one nominal variable becomes a three-element vector with one element
    representing red, another green, and the last blue. Then, we set the value corresponding
    to the color to one and all the others to zero, like so:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在独热编码中，我们将单一的名义变量转化为一个向量，其中向量的每个元素对应于一个名义值。以颜色为例，单一的名义变量变成了一个三元素向量，一个元素代表红色，另一个代表绿色，最后一个代表蓝色。然后，我们将与颜色对应的值设置为1，其它值设置为0，如下所示：
- en: '| **Value** |  | **Vector** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **值** |  | **向量** |'
- en: '| --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| red | → | 1 0 0 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 红色 | → | 1 0 0 |'
- en: '| green | → | 0 1 0 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 绿色 | → | 0 1 0 |'
- en: '| blue | → | 0 0 1 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 蓝色 | → | 0 0 1 |'
- en: Now the vector values are meaningful because either it’s red (1) or it’s not
    (0), green (1) or it’s not (0), or blue (1) or it’s not (0). The interval between
    zero and one has mathematical meaning because the presence of the value, say red,
    is genuinely greater than its absence, and that works the same way for each color.
    The values are now interval, so the network can use them. In some toolkits, like
    Keras, class labels are one-hot encoded before passing them to the model. This
    is done so vector output operates nicely with the one-hot encoded class label
    when computing the loss function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，向量值变得有意义，因为它要么是红色（1），要么不是（0），要么是绿色（1），要么不是（0），要么是蓝色（1），要么不是（0）。零和一之间的区间具有数学意义，因为某个值的存在，比如红色，确实比它的缺失要“大”，对于每种颜色都一样。现在，这些值是区间型数据，网络可以使用它们。在一些工具包中，如Keras，类标签在传递给模型之前会进行独热编码。这样做是为了确保向量输出能够与独热编码后的类标签在计算损失函数时良好地配合。
- en: Summary Statistics
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汇总统计
- en: We’re given a dataset. How do we make sense of it? How should we characterize
    it to understand it better before we use it to build a model?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一份数据集。我们该如何理解它呢？在使用它来构建模型之前，我们应该如何描述它，以便更好地理解它？
- en: To answer these questions, we need to learn about *summary statistics*. Calculating
    summary statistics should be the first thing you do when handed a new dataset.
    Not looking at your dataset before building a model is like buying a used car
    without checking the tires, taking it for a test drive, and looking under the
    hood.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我们需要了解*汇总统计*。计算汇总统计应该是你拿到新数据集后的第一件事。没有在构建模型之前查看数据集，就像买了一辆二手车，却没有检查轮胎，试驾一圈，也没看引擎盖下的情况。
- en: 'People have different notions of what makes a good set of summary statistics.
    We’ll focus on the following: means; the median; and measures of variation, including
    variance, standard deviation, and standard error. The range and mode are also
    often mentioned. The *range* is the difference between the maximum and minimum
    of the dataset. The *mode* is the most frequent value in the dataset. We generally
    get a sense of the mode visually from the histogram, as the histogram shows us
    the shape of the distribution of the data.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的人对什么构成良好的汇总统计有不同的看法。我们将重点关注以下几项：均值；中位数；以及变异性度量，包括方差、标准差和标准误差。数据集的范围和众数也是常被提及的。*范围*是数据集中最大值和最小值之间的差异。*众数*是数据集中最频繁出现的值。我们通常通过直方图来视觉上感知众数，因为直方图展示了数据分布的形态。
- en: Means and Median
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均值与中位数
- en: 'Most of us learned how to calculate the average of a set of numbers in elementary
    school: add the numbers and divide by how many there are. This is the *arithmetic
    mean*, or, more specifically, the *unweighted* arithmetic mean. If the dataset
    consists of a set of values, *{x[0]*, *x*[1], *x*[2], . . . , *x[n–1]*}, then
    the arithmetic mean is the sum of the data divided by the number of elements in
    the dataset (*n*). Notationally, we write this as the following.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人在小学时学会了如何计算一组数字的平均值：将数字加起来，再除以数字的个数。这就是*算术平均数*，更具体地说，是*无权重*的算术平均数。如果数据集由一组值*{x[0]*,
    *x*[1], *x*[2], . . . , *x[n–1]*}组成，那么算术平均数是数据的总和除以数据集中元素的数量（*n*）。在符号上，我们可以这样表示：
- en: '![image](Images/04equ01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ01.jpg)'
- en: The ![image](Images/xbar.jpg) is the typical way to denote the mean of a sample.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![image](Images/xbar.jpg) 是表示样本均值的常见符号。'
- en: '[Equation 4.1](ch04.xhtml#ch04equ01) calculates the unweighted mean. Each value
    is given a weight of 1/*n*, where the sum of all the weights is 1.0\. Sometimes,
    we might want to weight elements of the dataset differently; in other words, not
    all of them should count equally. In that case, we calculate a weighted mean,'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 4.1](ch04.xhtml#ch04equ01) 计算的是无权重均值。每个值的权重为 1/*n*，所有权重的总和为 1.0。有时，我们可能需要对数据集中的元素赋予不同的权重；换句话说，并不是所有元素都应该平等计算。在这种情况下，我们计算加权均值，'
- en: '![image](Images/071equ01.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ01.jpg)'
- en: where *w[i]* is the weight given to *x[i]* and Σ*[i]w[i]* = 1\. The weights
    are not part of the dataset; they need to come from somewhere else. The grade
    point average (GPA) used by many universities is an example of a weighted mean.
    The grade for each course is multiplied by the number of course credits, and the
    sum is divided by the total number of credits. Algebraically, this is equivalent
    to multiplying each grade by a weight, *w[i]* = *c[i]*/Σ*[i]**c[i]*, with *c[i]*
    the number of credits for course *i* and Σ*[i]**c[i]* the total number of credits
    for the semester.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w[i]* 是赋予 *x[i]* 的权重，Σ*[i]w[i]* = 1。权重不是数据集的一部分，它们需要从其他地方获取。许多大学使用的学分绩点（GPA）就是加权均值的一个例子。每门课程的成绩乘以课程学分，然后将总和除以学分总数。从代数上讲，这相当于将每个成绩乘以一个权重，*w[i]*
    = *c[i]*/Σ*[i]**c[i]*，其中 *c[i]* 是课程 *i* 的学分，Σ*[i]**c[i]* 是学期的学分总数。
- en: Geometric Mean
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 几何均值
- en: 'The arithmetic mean is by far the most commonly used mean. However, there are
    others. The *geometric mean* of two positive numbers, *a* and *b*, is the square
    root of their product:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 算术均值是目前最常用的均值。然而，还有其他均值。两个正数 *a* 和 *b* 的*几何均值*是它们乘积的平方根：
- en: '![image](Images/071equ02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ02.jpg)'
- en: 'In general, the geometric mean of *n* positive numbers is the *n*th root of
    their product:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，*n* 个正数的几何均值是它们乘积的*n*次方根：
- en: '![image](Images/071equ03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ03.jpg)'
- en: The geometric mean is used in finance to calculate average growth rates. In
    image processing, the geometric mean can be used as a filter to help reduce image
    noise. In deep learning, the geometric mean appears in the *Matthews correlation
    coefficient (MCC)*, one of the metrics we use to evaluate deep learning models.
    The MCC is the geometric mean of two other metrics, the informedness and the markedness.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 几何均值通常用于金融中计算平均增长率。在图像处理中，几何均值可以作为滤波器来帮助减少图像噪声。在深度学习中，几何均值出现在*马修斯相关系数（MCC）*中，MCC
    是我们用来评估深度学习模型的指标之一。MCC 是另外两个指标——信息度和显著度的几何均值。
- en: Harmonic Mean
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调和均值
- en: 'The *harmonic mean* of two numbers, *a* and *b*, is the reciprocal of the arithmetic
    mean of their reciprocals:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数 *a* 和 *b* 的*调和均值*是它们倒数的算术均值的倒数：
- en: '![image](Images/071equ04.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/071equ04.jpg)'
- en: In general,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，
- en: '![image](Images/072equ01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/072equ01.jpg)'
- en: 'The harmonic mean shows up in deep learning as the F1 score. This is a frequently
    used metric for evaluating classifiers. The F1 score is the harmonic mean of the
    recall (sensitivity) and the precision:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调和均值在深度学习中表现为 F1 分数。这是一个常用的评估分类器的指标。F1 分数是召回率（灵敏度）和精确度的调和均值：
- en: '![image](Images/072equ02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/072equ02.jpg)'
- en: 'Despite its frequent use, it’s not a good idea to use the F1 score to evaluate
    a deep learning model. To see this, consider the definitions of recall and precision:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 F1 分数经常被使用，但并不建议用它来评估深度学习模型。为了说明这一点，考虑召回率和精确度的定义：
- en: '![image](Images/072equ03.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/072equ03.jpg)'
- en: Here, TP is the number of true positives, FN is the number of false negatives,
    and FP is the number of false positives. These values come from the test set used
    to evaluate the model. A fourth number that’s important for classifiers, TN, is
    the number of correctly classified true negatives (assuming a binary classifier).
    The F1 score ignores TN, but to understand how well the model performs, we need
    to consider both positive and negative classifications. Therefore, the F1 score
    is misleading and often too optimistic. Better metrics are the MCC mentioned above
    or Cohen’s κ (kappa), which is similar to MCC and usually tracks it closely.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，TP 是真正阳性（True Positive）的数量，FN 是假阴性（False Negative）的数量，FP 是假阳性（False Positive）的数量。这些值来自于用于评估模型的测试集。分类器的第四个重要数字是
    TN，即真正阴性（True Negative）的数量（假设为二分类器）。F1 分数忽略 TN，但为了了解模型的表现，我们需要考虑正类和负类的分类。因此，F1
    分数具有误导性，通常过于乐观。更好的度量标准是上面提到的 MCC 或 Cohen’s κ（Kappa），它与 MCC 相似，通常会紧密跟踪其变化。
- en: Median
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 中位数
- en: 'Before moving on to measures of variation, there’s one more commonly used summary
    statistic we’ll mention here. It’ll show up again a little later in the chapter
    too. The *median* of a dataset is the middle value. It’s the value where, when
    the dataset is sorted numerically, half the values are below it and half are above
    it. Let’s use this dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论变异度量之前，还有一个常用的总结性统计量我们在这里提到。它稍后会在本章中再次出现。数据集的*中位数*是中间值。它是数据按数值排序后，数据集一半的值位于它之下，一半的值位于它之上的那个值。我们使用这个数据集：
- en: '*X* = {55,63,65,37,74,71,73,87,69,44}'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* = {55,63,65,37,74,71,73,87,69,44}'
- en: If we sort *X*, we get
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对 *X* 进行排序，我们得到
- en: '{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '{37, 44, 55, 63, 65, 69, 71, 73, 74, 87}'
- en: We immediately see a potential problem. I said we need the middle value when
    the data is sorted. With 10 things in *X*, there is no middle value. The middle
    lies between 65 and 69\. When the number of elements in the data-set is even,
    the median is the arithmetic mean of the two middle numbers. Therefore, the median
    in this case is
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立刻看到一个潜在问题。我说过，我们需要在数据排序后得到中间值。对于 *X* 中的10个数据，没有真正的中间值。中间值位于65和69之间。当数据集中的元素数量是偶数时，中位数是两个中间数字的算术平均值。因此，在这种情况下，中位数是
- en: '![image](Images/073equ01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/073equ01.jpg)'
- en: The arithmetic mean of the data is 63.8\. What’s the difference between the
    mean and the median?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的算术平均值是63.8。均值和中位数之间有什么区别？
- en: By design, the median tells us the value that splits the dataset, so the number
    of samples above equals the number below. It’s the number of samples that matters.
    For the mean, it’s a sum over the actual data values. Therefore, the mean is sensitive
    to the values themselves, while the median is sensitive to the ordering of the
    values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从设计上看，中位数告诉我们将数据集分开的值，使得位于其上的样本数等于位于其下的样本数。重要的是样本的数量。对于均值，它是所有数据值的总和。因此，均值对值本身敏感，而中位数则对值的排序敏感。
- en: If we look at *X*, we see that most values are in the 60s and 70s, with one
    low value of 37\. It’s the low value of 37 that drags the mean down relative to
    the median. An excellent example of this effect is income. The current median
    annual family income in the United States is about $62,000\. A recent measure
    of the mean family income in the United States is closer to $72,000\. The difference
    is because of the small portion of the population who make significantly more
    money than everyone else. They pull the overall mean up. For income, then, the
    most meaningful statistic is the median.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察 *X*，会看到大多数值位于60到70之间，只有一个较低的值37。正是这个低值37使得均值相对于中位数较低。收入就是这种效应的一个典型例子。美国当前的家庭年收入中位数大约是62,000美元。最近的美国家庭收入均值接近72,000美元。二者之间的差异是因为少部分人群的收入远高于其他人，他们把整体均值拉高了。因此，对于收入而言，最有意义的统计量是中位数。
- en: Consider [Figure 4-2](ch04.xhtml#ch04fig02).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 [图 4-2](ch04.xhtml#ch04fig02)。
- en: '![image](Images/04fig02.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig02.jpg)'
- en: '*Figure 4-2: The mean (solid) and median (dashed) plotted over the histogram
    of a sample dataset*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-2：均值（实线）和中位数（虚线）在样本数据集直方图上的绘制*'
- en: '[Figure 4-2](ch04.xhtml#ch04fig02) shows the histogram generated from 1,000
    samples of a simulated dataset. Also plotted are the mean (solid line) and median
    (dashed line). The two do not match; the long tail in the histogram drags the
    mean up. If we were to count, 500 samples would fall in the bins below the dashed
    line and 500 in the bins above.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-2](ch04.xhtml#ch04fig02)展示了从1,000个模拟数据样本生成的直方图。图中还标出了均值（实线）和中位数（虚线）。两者并不重合；直方图中的长尾将均值拉高。如果我们进行计数，会发现500个样本落在虚线以下的区间，500个样本则落在虚线以上的区间。'
- en: Are there times when the mean and median are the same? Yes. If the data distribution
    is completely symmetric, then the mean and median will be the same. The classic
    example of this situation is the normal distribution. [Figure 3-4](ch03.xhtml#ch03fig04)
    showed a normal distribution where the left-right symmetry was clear. The normal
    distribution is special. We’ll see it again throughout the chapter. For now, remember
    that the closer the distribution of the dataset is to a normal distribution, the
    closer the mean and median will be.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有可能均值和中位数相同的情况？有的。如果数据的分布完全对称，那么均值和中位数将会相同。一个经典的例子就是正态分布。[图 3-4](ch03.xhtml#ch03fig04)展示了一个正态分布，左右对称性非常明显。正态分布是特别的，我们将在本章中再次遇到它。现在要记住的是，数据集的分布越接近正态分布，均值和中位数就越接近。
- en: 'The opposite is also worth remembering: if the dataset’s distribution is far
    from normal, like in [Figure 4-2](ch04.xhtml#ch04fig02), then the median is likely
    the better statistic to consider when summarizing the data.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得记住的情况是：如果数据集的分布与正态分布相差较远，如[图 4-2](ch04.xhtml#ch04fig02)所示，那么中位数可能是更好的统计量，适合用来总结数据。
- en: Measures of Variation
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变异度的度量
- en: A beginning archer shoots 10 arrows at a target. Eight of the beginner’s arrows
    hit the target, two miss completely, and the eight that do hit the target are
    spread uniformly across it. An expert archer shoots 10 arrows at a target. All
    of the expert’s arrows hit within a few centimeters of the center. Think about
    the mean position of the arrows. For the expert, all of the arrows are near the
    center of the target, so we can see that the mean position of the arrows will
    be near the center. For the beginner, none of the arrows are near the center of
    the target, but they are scattered more or less equally to the left and right
    or above and below the center. Because of this, the average position will balance
    out and be near the center of the target as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一名初学者射手向目标射击了10支箭。初学者的8支箭击中了目标，2支箭完全偏离，且那8支命中的箭均匀分布在目标上。另一位专家射手向目标射击了10支箭，所有箭都射中了距离中心几厘米以内的区域。想一想这些箭的均值位置。对于专家射手，所有的箭都集中在目标中心附近，因此均值位置也会靠近中心。对于初学者，虽然没有箭射中目标中心，但它们大致均匀分布在目标的左右或上下。由于这个原因，平均位置将会平衡，并接近目标的中心。
- en: However, the first archer’s arrows are scattered; their location varies greatly.
    The second archer’s arrows, on the other hand, are tightly clustered, and there
    is little variation in their position. One meaningful way to summarize and understand
    a dataset is to quantify its variation. Let’s see how we might do this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，第一位射手的箭散布得较广，它们的位置变化很大。另一方面，第二位射手的箭聚集得很紧，位置变化很小。总结和理解数据集的一种有意义的方法是量化它的变化。让我们看看如何实现这一点。
- en: Deviation vs. Variance
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏差与方差
- en: One way we might measure the variation of a dataset is to find the *range*,
    the difference between the largest and smallest values. However, the range is
    a crude measurement, as it pays no attention to most of the values in the dataset,
    only the extremes. We can do better by calculating the mean of the difference
    between the data values and the mean of the data. The formula is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们衡量数据集变化的一种方法是找出*极差*，即最大值和最小值之间的差异。然而，极差是一个粗略的度量，因为它忽略了数据集中的大多数值，仅关注极端值。我们可以通过计算数据值与数据均值之间差异的平均值来做得更好。公式如下：
- en: '![image](Images/04equ02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ02.jpg)'
- en: '[Equation 4.2](ch04.xhtml#ch04equ02) is the *mean deviation*. It’s a natural
    measure and gives just what we want: an idea of how far, on average, each sample
    is from the mean. While there’s nothing wrong with calculating the mean deviation,
    you’ll find that it’s rarely used in practice. One reason has to do with algebra
    and calculus. The absolute value is annoying to deal with mathematically.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 4.2](ch04.xhtml#ch04equ02)是*平均偏差*。它是一种自然的度量，正是我们想要的：给出每个样本平均偏离均值的程度。虽然计算平均偏差没有问题，但你会发现它在实践中很少被使用。一个原因与代数和微积分有关。绝对值在数学上很麻烦。'
- en: 'Instead of the natural measure of variation, let’s calculate this one using
    squared differences:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们不用自然的变异度量，而是通过平方差来计算：
- en: '![image](Images/04equ03.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ03.jpg)'
- en: '[Equation 4.3](ch04.xhtml#ch04equ03) is known as the *biased sample variance*.
    It’s the mean of the squared difference between each value in the dataset and
    the mean. It’s an alternate way of characterizing the scatter in the dataset.
    Why it’s biased, we’ll discuss in a second. We’ll get into why it’s ![image](Images/ssub-bar.jpg)
    and not *s[n]* shortly after that.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 4.3](ch04.xhtml#ch04equ03)被称为*偏差样本方差*。它是数据集中每个值与均值之间平方差的平均值。这是表征数据集散布的另一种方式。为什么它是偏差的，我们稍后会讨论。我们也会很快讨论为什么是![image](Images/ssub-bar.jpg)而不是*s[n]*。'
- en: 'Before we do, it’s worth noting that you’ll often see a slightly different
    equation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，值得注意的是，你经常会看到一个稍微不同的公式：
- en: '![image](Images/04equ04.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ04.jpg)'
- en: This equation is the *unbiased sample variance*. Using *n* – 1 in place of *n*
    is known as Bessel’s correction. It’s related to the number of degrees of freedom
    in the residuals, where the residuals are what’s left when the mean is subtracted
    from each of the values in the dataset. The sum of the residuals is zero, so if
    there are *n* values in the dataset, knowing *n* – 1 of the residuals allows the
    last residual to be calculated. This gives us the degrees of freedom for the residuals.
    We are “free” to calculate *n* – 1 of them knowing that we’ll get the last one
    from the fact that the residuals sum to zero. Dividing by *n*–1 gives a less biased
    estimate of the variance, assuming ![image](Images/ssub-bar.jpg) is biased in
    some way to begin with.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式是*无偏样本方差*。用*n* – 1代替*n*被称为贝塞尔修正。它与残差的自由度数相关，其中残差是从每个数据值中减去均值后剩余的部分。残差的总和为零，因此如果数据集中有*n*个值，知道*n*
    – 1个残差就可以计算出最后一个残差。这为残差提供了自由度。我们“自由”地计算出*n* – 1个残差，因为我们知道残差的总和为零，可以得出最后一个残差。除以*n*–1可以给出方差的一个更少偏差的估计，假设![image](Images/ssub-bar.jpg)本身一开始就有偏差。
- en: Why are we talking about biased variance and unbiased variance? Biased how?
    We should always remember that a dataset is a sample from some parent data-generating
    process, the population. The true population variance (σ²) is the scatter of the
    population around the true population mean (μ). However, we don’t know μ or σ²,
    so instead, we estimate them from the dataset we do have. The mean of the sample
    is ![image](Images/xbar.jpg). That’s our estimate for μ. It’s then natural to
    calculate the mean of the squared deviations around ![image](Images/xbar.jpg)
    and call that our estimate for σ². That’s ![image](Images/ssub-bar.jpg) ([Equation
    4.3](ch04.xhtml#ch04equ03)). The claim, which is true but beyond our scope to
    demonstrate, is that ![image](Images/ssub-bar.jpg) is biased and not the best
    estimate of σ², but if Bessel’s correction is applied, we’ll have a better estimate
    of the population variance. So we should use *s*² ([Equation 4.4](ch04.xhtml#ch04equ04))
    to characterize the variance of the dataset around the mean.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要讨论偏差方差和无偏方差？偏差是什么？我们应该始终记住，数据集是某个母体数据生成过程的一个样本，即母体。真实的母体方差（σ²）是母体围绕真实母体均值（μ）的散布。然而，我们不知道μ或σ²，因此我们从我们拥有的数据集中估计它们。样本的均值是![image](Images/xbar.jpg)。这是我们对μ的估计。然后，计算围绕![image](Images/xbar.jpg)的平方偏差的均值，并称之为我们对σ²的估计。这就是![image](Images/ssub-bar.jpg)（[公式
    4.3](ch04.xhtml#ch04equ03)）。这个结论虽然是正确的，但超出了我们展示的范围：![image](Images/ssub-bar.jpg)是有偏差的，并不是σ²的最佳估计，但如果应用贝塞尔修正，我们将获得更好的母体方差估计。因此，我们应该使用*s*²（[公式
    4.4](ch04.xhtml#ch04equ04)）来表征数据集围绕均值的方差。
- en: In summary, we should use ![image](Images/xbar.jpg) and *s*² to quantify the
    variance of the data-set. Now, why is it *s*²? The square root of the variance
    is the *standard deviation* denoted as σ for the population and *s* for the estimate
    of σ calculated from the dataset. Most often, we want to work with the standard
    deviation. Writing square roots becomes tiresome, so convention has adopted the
    σ or *s* notation for the standard deviation and uses the squared form when discussing
    the variance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们应该使用![image](Images/xbar.jpg)和 *s*² 来量化数据集的方差。那么，为什么是 *s*² 呢？方差的平方根是
    *标准差*，对于总体来说用σ表示，针对从数据集中计算出的σ估计值用 *s* 表示。我们通常需要处理的是标准差。写平方根会很麻烦，因此约定俗成地用σ或 *s*
    表示标准差，讨论方差时则使用其平方形式。
- en: And, because life isn’t already ambiguous enough, you’ll often see σ used for
    *s*, and [Equation 4.3](ch04.xhtml#ch04equ03) used when it really should be [Equation
    4.4](ch04.xhtml#ch04equ04). Some toolkits, including our beloved NumPy, make it
    easy to use the wrong formula.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，由于生活本身已经足够模糊，你经常会看到σ被用作 *s*，以及[方程式4.3](ch04.xhtml#ch04equ03)被使用，而实际上它应该使用[方程式4.4](ch04.xhtml#ch04equ04)。一些工具包，包括我们亲爱的NumPy，便于使用错误的公式。
- en: 'However, as the number of samples in our dataset increases, the difference
    between the biased and unbiased variance decreases because dividing by *n* or
    *n* – 1 matters less and less. A few lines of code illustrate this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着数据集中样本数量的增加，偏差方差和无偏方差之间的差异逐渐减小，因为除以 *n* 或 *n* – 1 的影响变得越来越小。以下几行代码展示了这一点：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, a sample with only 10 values (`a`) shows a difference in the biased and
    unbiased variance in the third decimal. If we increase our dataset size from 10
    to 10,000, we get
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，一个只有10个值的样本（`a`）显示了偏差方差和无偏方差在第三位小数上的差异。如果我们将数据集的大小从10增加到10,000，我们会得到
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The difference between the biased and unbiased estimate of the variance is now
    in the fifth decimal. Therefore, for the large datasets we typically work with
    in deep learning, it matters little in practice whether we use *s[n]* or *s* for
    the standard deviation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，偏差方差和无偏方差估计之间的差异已经在第五位小数。因此，对于我们通常在深度学习中使用的大数据集来说，实际上无论是使用 *s[n]* 还是 *s*
    作为标准差都没多大区别。
- en: '**MEDIAN ABSOLUTE DEVIATION**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**中位数绝对偏差**'
- en: 'The standard deviation is based on the mean. The mean, as we saw above, is
    sensitive to extreme values, and the standard deviation is doubly so because we
    square the deviation from the mean for each sample. A measure of variability that
    is insensitive to extreme values in the dataset is the *median absolute deviation
    (MAD)*. The MAD is defined as the median of the absolute values of the difference
    between the data and the median:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差基于均值。如上所述，均值对极端值敏感，而标准差更为敏感，因为我们对每个样本的偏差进行平方处理。一个对数据集中极端值不敏感的变异性度量是 *中位数绝对偏差（MAD）*。MAD定义为数据与中位数之间差异的绝对值的中位数：
- en: MAD = median(|*X[i]* – median(*X*)|)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: MAD = 中位数(|*X[i]* – 中位数(*X*)|)
- en: 'Procedurally, first calculate the median of the data, then subtract it from
    each data value, making the result positive, and report the median of that set.
    The implementation is straightforward:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 按步骤操作，首先计算数据的中位数，然后将其从每个数据值中减去，使结果为正，并报告该数据集的中位数。实现方法非常直接：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The MAD is not often used, but its insensitivity to extreme values in the dataset
    argues toward more frequent use, especially for outlier detection.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: MAD不常使用，但它对数据集中极端值的不敏感使得它在异常值检测中具有更频繁使用的价值。
- en: Standard Error vs. Standard Deviation
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标准误差与标准差
- en: 'We have one more measure of variance to discuss: the *standard error of the
    mean (SEM)*. The SEM is often simply called the *standard error (SE)*. We need
    to go back to the population to understand what the SE is and when to use it.
    If we select a sample from the population, a dataset, we can calculate the mean
    of the sample, ![image](Images/xbar.jpg). If we choose repeated samples and calculate
    those sample means, we’ll generate a dataset of means of the samples from the
    population. This might sound familiar; it’s the process we used to illustrate
    the central limit theorem in [Chapter 3](ch03.xhtml#ch03). The standard deviation
    of the set of means is the standard error.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个方差的度量需要讨论：*均值的标准误差（SEM）*。SEM通常简称为*标准误差（SE）*。为了理解SE是什么以及何时使用它，我们需要回到总体。如果我们从总体中选择一个样本，一个数据集，我们可以计算该样本的均值，
    ![image](Images/xbar.jpg)。如果我们选择多个样本并计算这些样本的均值，我们将生成一个包含来自总体样本均值的数据集。这听起来可能有些熟悉；它就是我们在[第3章](ch03.xhtml#ch03)中用来说明中心极限定理的过程。均值集合的标准差就是标准误差。
- en: The formula for the standard error from the standard deviation is straightforward,
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差的标准误差公式很简单，
- en: '![image](Images/077equ01.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/077equ01.jpg)'
- en: and is nothing more than a scaling of the sample standard deviation by the square
    root of the number of samples.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 只是将样本标准差按样本数量的平方根进行缩放。
- en: When should we use the standard deviation, and when should we use the standard
    error? Use the standard deviation to learn about the distribution of the samples
    around the mean. Use the standard error to say something about how good an estimate
    of the population mean a sample mean is. In a sense, the standard error is related
    to both the central limit theorem, as that affects the standard deviation of the
    means of multiple samples from the parent population, and the law of large numbers,
    since a larger dataset is more likely to give a better estimate of the population
    mean.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们什么时候使用标准差，什么时候使用标准误差？使用标准差来了解样本围绕均值的分布情况。使用标准误差来说明样本均值对总体均值的估计有多好。从某种意义上讲，标准误差与中心极限定理有关，因为它影响来自总体的多个样本均值的标准差，同时也与大数法则相关，因为较大的数据集更有可能给出总体均值的更好估计。
- en: From a deep learning point of view, we might use the standard deviation to describe
    the dataset used to train a model. If we train and test several models, remembering
    the stochastic nature of deep network initialization, we can calculate a mean
    over the models for some metric, say the accuracy. In that case, we might want
    to report the mean accuracy plus or minus the standard error. As we train more
    models and gain confidence that the mean accuracy represents the sort of accuracy
    the model architecture can provide, we should expect that the error in the mean
    accuracy over the models will decrease.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度学习的角度来看，我们可能使用标准差来描述用于训练模型的数据集。如果我们训练和测试多个模型，并且记住深度网络初始化的随机性，我们可以对某个指标（例如准确度）计算这些模型的均值。在这种情况下，我们可能会报告均值准确度加上或减去标准误差。随着我们训练更多模型，并且更加确信均值准确度代表了模型架构所能提供的准确度，我们应该期望模型的均值准确度的误差会减少。
- en: To recap, in this section, we discussed different summary statistics, values
    we can use to start to understand a dataset. These include the various means (arithmetic,
    geometric, and harmonic), the median, the standard deviation, and, when appropriate,
    the standard error. For now, let’s see how we can use plots to help understand
    a dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在本节中，我们讨论了不同的总结性统计量，这些值可以帮助我们开始理解数据集。包括各种均值（算术均值、几何均值和调和均值）、中位数、标准差，以及在适当情况下，标准误差。现在，让我们看看如何使用图表来帮助理解数据集。
- en: Quantiles and Box Plots
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分位数和箱线图
- en: To calculate the median, we need to find the middle value, the number splitting
    the dataset into two halves. Mathematically, we say that the median divides the
    dataset into two quantiles.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算中位数，我们需要找到中间值，即将数据集分成两个部分的数值。从数学角度讲，我们说中位数将数据集分为两个分位数。
- en: A *quantile* splits the dataset into fixed-sized groups where the fixed size
    is the number of data values in the quantile. Since the median splits the dataset
    into two equally sized groups, it’s a *2-quantile*. Sometimes you’ll see the median
    referred to as the *50th percentile*, meaning 50 percent of the data values are
    less than this value. By similar reasoning, then, the 95th percentile is the value
    that 95 percent of the dataset is less than. Researchers often calculate 4-quantiles
    and refer to them as *quartiles*, since they split the dataset into four groups
    such that 25 percent of the data values are in the first quartile, 50 percent
    are in the first and second, and 75 percent are in the first, second, and third,
    with the final 25 percent in the fourth quartile.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*分位数*将数据集划分为固定大小的组，其中固定大小是分位数中数据值的数量。由于中位数将数据集划分为两个大小相等的组，因此它是一个*2-分位数*。有时你会看到中位数被称为*第50百分位数*，意思是有50%的数据值小于这个值。通过类似的推理，第95百分位数是95%的数据集小于的值。研究人员通常计算4-分位数，并称其为*四分位数*，因为它将数据集划分为四个组，其中25%的数据值位于第一个四分位数，50%的数据值位于第一个和第二个四分位数，75%的数据值位于第一个、第二个和第三个四分位数，剩余的25%的数据值位于第四个四分位数。'
- en: 'Let’s work through an example to understand what we mean by quantiles. The
    example uses a synthetic exam dataset representing 1,000 test scores. See the
    file *exams.npy*. We’ll use NumPy to calculate the quartile values for us and
    then plot a histogram of the dataset with the quartile values marked. First, let’s
    calculate the quartile positions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解分位数的含义。这个例子使用了一个合成的考试数据集，代表了1,000个考试分数。请参阅文件*exams.npy*。我们将使用NumPy来为我们计算四分位数值，然后绘制该数据集的直方图，并标出四分位数值。首先，让我们计算四分位数的位置：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code, along with code to generate the plot, is in the file *quantiles.py*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码以及生成图表的代码都在文件*quantiles.py*中。
- en: First we load the synthetic exam data and keep the first exam scores (`p`).
    Note, we make `p` an integer array so we can use `np.bincount` later to make the
    histogram. (That code is not shown above.) We then use NumPy’s `np.quantile` function
    to calculate the quartile values. This function takes the source array and an
    array of quantile values in the range [0, 1]. The values are fractions of the
    distance from the minimum value of the array to its maximum. So, asking for the
    0.5 quantile is asking for the value that is half the distance between the minimum
    of `p` and its maximum such that the number of values in each set is equal.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载合成的考试数据并保留第一个考试分数（`p`）。注意，我们将`p`设置为整数数组，以便稍后使用`np.bincount`来制作直方图。（该代码未在上面显示。）然后，我们使用NumPy的`np.quantile`函数来计算四分位数值。该函数接受源数组和一个在[0,
    1]范围内的分位数数组。这些值是从数组的最小值到最大值的距离的分数。所以，要求0.5分位数实际上是要求位于`p`的最小值和最大值之间的距离的一半的值，使得每个集合中的值的数量相等。
- en: To get quartiles, we ask for the 0.25, 0.5, and 0.75 quantiles to get the values
    such that 25 percent, 50 percent, and 75 percent of the elements of `p` are less
    than the values. We also ask for the 0.0 and 1.0 quantiles, the minimum and maximum
    of `p`. We do this for convenience when we count the number of elements in each
    range. Note, we could have instead used the `np.percentile` function. It returns
    the same values as `np.quantile` but uses percentage values instead of fractions.
    In that case, the second argument would have been `[0,25,50,75,100]`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得四分位数，我们请求0.25、0.5和0.75分位数，以获取使得`p`中25%、50%和75%的元素小于这些值的四分位数值。我们还请求0.0和1.0分位数，即`p`的最小值和最大值。这样做是为了方便我们在计算每个范围内元素数量时使用。注意，我们本可以使用`np.percentile`函数来代替，它返回与`np.quantile`相同的值，只不过使用的是百分位数而不是分数。在这种情况下，第二个参数应该是`[0,25,50,75,100]`。
- en: The returned quartile values are in `q`. We print them to get
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的四分位数值在`q`中。我们打印它们以获取
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, 18 is the minimum, 100 is the maximum, and the three cutoff values for
    the quartiles are 56.75, 68, and 78\. Note that the cutoff for the second quartile
    is the median, 68.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，18是最小值，100是最大值，三个四分位数的截止值分别是56.75、68和78。请注意，第二个四分位数的截止值是中位数68。
- en: The remaining code counts the number of values in `p` in each range. With 1,000
    values, we’d expect to have 250 in each range, but because the math doesn’t always
    fall along existing data values, we get instead
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码计算`p`中每个范围内的值的数量。对于1,000个值，我们期望每个范围内有250个值，但因为数学运算不总是精确地落在现有数据值上，我们得到的结果是
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: meaning 250 elements of `p` are less than 56.75, 237 are in [56.75, 68], and
    so forth.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 意味着`p`中有250个元素小于56.75，237个元素在[56.75, 68]之间，依此类推。
- en: The code above uses a clever counting trick worth explaining. We want to count
    the number of values in `p` in some range. We can’t use NumPy’s `np.where` function,
    as it doesn’t like the compound conditional statement. However, if we use an expression
    like `10 <= p`, we’ll be given an array the same size as `p` where each element
    is either `True` if the condition is true for that element or `False` if it is
    not. Therefore, asking for `10 <= p` and `p < 90` will return two Boolean arrays.
    To get the elements where both conditions are true, we need to logically AND them
    together (`&`). This gives us a final array the same size and shape as `p`, where
    all `True` elements represent values in `p` in [10, 90). To get the count, we
    apply the `sum` method that for a Boolean array treats `True` as one and `False`
    as zero.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用了一种巧妙的计数技巧，值得解释一下。我们想要计算`p`中某个范围内的值的数量。我们不能使用NumPy的`np.where`函数，因为它不喜欢复合条件语句。但是，如果我们使用像`10
    <= p`这样的表达式，我们将得到一个与`p`大小相同的数组，其中每个元素如果满足条件，则为`True`，否则为`False`。因此，要求`10 <= p`且`p
    < 90`将返回两个布尔数组。要获得两个条件都为真的元素，我们需要将它们逻辑与（`&`）在一起。这样，我们得到一个与`p`大小和形状相同的最终数组，其中所有`True`元素表示`p`中位于[10,
    90)范围内的值。要计算数量，我们可以使用`sum`方法，布尔数组中的`True`作为1，`False`作为0。
- en: '[Figure 4-3](ch04.xhtml#ch04fig03) shows the histogram of the exam data with
    the quartiles marked.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](ch04.xhtml#ch04fig03)显示了带有四分位数标记的考试数据直方图。'
- en: '![image](Images/04fig03.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig03.jpg)'
- en: '*Figure 4-3: A histogram of 1,000 exam scores with the quartiles marked*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-3：带有四分位数标记的1,000个考试分数的直方图*'
- en: The example above shows yet again how useful a histogram is for visualizing
    and understanding data. We should use histograms whenever possible to help understand
    what’s going on with a dataset. [Figure 4-3](ch04.xhtml#ch04fig03) superimposes
    the quartile values on the histogram. This helps us understand what the quartiles
    are and their relationship to the data values, but this is not a typical presentation
    style. More typical, and useful because it can show multiple features of a dataset,
    is the *box plot*. Let’s use it now for the exam scores above, but this time we’ll
    also include the two other sets of exam scores we ignored previously.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例再次展示了直方图在可视化和理解数据时的强大作用。我们应尽可能使用直方图来帮助理解数据集的情况。[图 4-3](ch04.xhtml#ch04fig03)将四分位数值叠加在直方图上。这有助于我们理解四分位数是什么以及它们与数据值的关系，但这并不是一种典型的展示方式。更典型且更有用的展示方式是*箱形图*，因为它能够展示数据集的多个特征。现在让我们用它来展示上面的考试分数，这次我们还会包括之前忽略的另外两个考试分数集。
- en: We’ll show a box plot first, and then explain it. To see the box plot for the
    three exams in the *exams.npy* file, use
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将展示一个箱形图，然后解释它。要查看*exams.npy*文件中三个考试的箱形图，请使用
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: where we’re loading the full set of exam scores and then using the Matplotlib
    `boxplot` function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载完整的考试分数集，然后使用Matplotlib的`boxplot`函数。
- en: Take a look at the output, shown in [Figure 4-4](ch04.xhtml#ch04fig04).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下输出，见[图 4-4](ch04.xhtml#ch04fig04)。
- en: '![image](Images/04fig04.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig04.jpg)'
- en: '*Figure 4-4: Box plots for the three exams (top), and the box plot for the
    first exam with the components marked (bottom)*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-4：三个考试的箱形图（上），以及带有标记组件的第一个考试的箱形图（下）*'
- en: The top chart in [Figure 4-4](ch04.xhtml#ch04fig04) shows the box plot for the
    three sets of exam scores in *|*exams.npy|. The first of these is plotted again
    on the bottom of [Figure 4-4](ch04.xhtml#ch04fig04), along with labels describing
    the parts of the plot.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](ch04.xhtml#ch04fig04)中的上方图表显示了*|*exams.npy|中三个考试分数集的箱形图。这三个考试分数集的第一个在[图
    4-4](ch04.xhtml#ch04fig04)的下方再次绘制，并附有描述图表部分的标签。'
- en: A box plot shows us a visual summary of the data. The box in the bottom chart
    in [Figure 4-4](ch04.xhtml#ch04fig04) illustrates the range between the cutoffs
    for the first quartile (Q1) and the third quartile (Q3). The numerical difference
    between Q3 and Q1 is known as the *interquartile range (IQR)*. The larger the
    IQR, the more spread out the data is around the median. Notice that the score
    is on the y-axis this time. We could have easily made the plot horizontal, but
    vertical is the default. The median (Q2) is marked near the middle of the box.
    The mean is not shown in a box plot.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 箱形图为我们提供了数据的可视化摘要。[图 4-4](ch04.xhtml#ch04fig04)中下方的图表中的盒子表示第一四分位数（Q1）和第三四分位数（Q3）之间的范围。Q3和Q1之间的数值差异称为*四分位距（IQR）*。IQR越大，数据在中位数周围的分布越广泛。注意，这次分数位于y轴上。我们本可以轻松地将图表设置为水平，但垂直显示是默认设置。中位数（Q2）标记在盒子中间附近。箱形图中没有显示均值。
- en: The box plot includes two additional lines, the *whiskers*, though Matplotlib
    calls them *fliers*. As indicated, they are 1.5 times the IQR above Q3 or below
    Q1\. Finally, there are some circles labeled “possible outliers.” By convention,
    values outside of the whiskers are considered *possible outliers*, meaning they
    might represent erroneous data, either entered incorrectly by hand or, more likely
    these days, received from faulty sensors. For example, bright or dead pixels on
    a CCD camera might be considered outliers. When evaluating a potential dataset,
    we should be sensitive to outliers and use our best judgment about what to do
    with them. Usually, there are only a few, and we can drop the samples from the
    dataset without harm. However, it’s also possible that the outliers are actually
    real and are highly indicative of a particular class. If that’s the case, we want
    to keep them in the dataset in the hopes that the model will use them effectively.
    Experience, intuition, and common sense must guide us here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图包含了另外两条线，*胡须*，尽管 Matplotlib 称它们为 *飞行器*。如图所示，它们是 Q3 上方或 Q1 下方 1.5 倍 IQR 的值。最后，有一些圈圈标记为“可能的离群值”。根据惯例，位于胡须之外的值被认为是
    *可能的离群值*，这意味着它们可能代表错误数据，要么是手动输入错误，要么更有可能是现在从故障传感器收到的错误数据。例如，CCD 相机上的亮点或坏点像素可能会被视为离群值。在评估潜在数据集时，我们应该对离群值保持敏感，并根据最佳判断来处理它们。通常，离群值并不多，我们可以将这些样本从数据集中删除而不会造成伤害。然而，也有可能这些离群值实际上是有效的，并且高度指示某个特定类别。如果是这种情况，我们希望将它们保留在数据集中，以期模型能够有效利用它们。经验、直觉和常识必须在这里指导我们。
- en: 'Let’s interpret the top chart in [Figure 4-4](ch04.xhtml#ch04fig04) showing
    the three sets of exam scores. The top of the whiskers is at 100 each time, which
    makes sense: a 100 is a perfect score, and there were 100s in the dataset. Notice
    that the box portion of the plot is not centered vertically in the whiskers. Recalling
    that 50 percent of the data values are between Q1 and Q3, with 25 percent above
    and below Q2 in the box, we see that the data is not rigorously normal; its distribution
    deviates from a normal curve. A glance back to the histogram in [Figure 4-3](ch04.xhtml#ch04fig03)
    confirms this for the first exam. Similarly, we see the second and third exams
    deviate from normality as well. So, a box plot can tell us how similar the distribution
    of the dataset is to a normal distribution. When we discuss hypothesis testing
    below, we’ll want to know if the data is normally distributed or not.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下图 [Figure 4-4](ch04.xhtml#ch04fig04) 中显示的三组考试成绩的顶部图表。每次胡须的顶部都在 100，这很有道理：100
    是满分，数据集中确实有 100 分。注意到图中的箱子部分并没有在胡须中垂直居中。回想一下，50% 的数据值位于 Q1 和 Q3 之间，箱子中的 Q2 以上和以下各占
    25%，我们可以看到数据并不完全符合正态分布；它的分布偏离了正态曲线。回头看看图 [Figure 4-3](ch04.xhtml#ch04fig03) 中的直方图，第一场考试的数据也确认了这一点。类似地，我们看到第二次和第三次考试的数据也偏离了正态性。因此，箱线图可以告诉我们数据集的分布与正态分布的相似程度。当我们讨论假设检验时，我们需要知道数据是否符合正态分布。
- en: What about possible outliers, the values below Q1 – 1.5 × IQR? We know the dataset
    represents test scores, so common sense tells us that these are not outliers but
    valid scores by particularly confused (or lazy) students. If the dataset contained
    values above 100 or below zero, those would be fair game to label outliers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，可能的离群值呢，低于 Q1 – 1.5 × IQR 的值？我们知道数据集代表的是考试成绩，因此常识告诉我们，这些并不是离群值，而是特别迷茫（或懒散）的学生的有效成绩。如果数据集包含高于
    100 或低于 0 的值，那些才可以合理地标记为离群值。
- en: Sometimes dropping samples with outliers is the right thing to do. However,
    if the outlier is caused by missing data, cutting the sample might not be an option.
    Let’s take a look at what we might do with missing data, and why we should generally
    avoid it like the plague.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有时丢弃含有离群值的样本是正确的做法。然而，如果离群值是由缺失数据引起的，去除样本可能就不可行了。让我们看看对于缺失数据我们可以做些什么，为什么我们通常应该像对待瘟疫一样避免它。
- en: Missing Data
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失数据
- en: 'Missing data is just that, data we don’t have. If the dataset consists of samples
    representing feature vectors, missing data shows up as one or more features in
    a sample that were not measured for some reason. Often, missing data is encoded
    in some way. If the value is only positive, a missing feature might be marked
    with a –1 or, historically, –999\. If the feature is given to us as a string,
    the string might be empty. For floating-point values, a not a number (NaN) might
    be used. NumPy makes it easy for us to check for NaNs in an array by using `np.isnan`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据就是我们没有的数据。如果数据集由表示特征向量的样本组成，缺失数据表现为某些样本中一个或多个特征由于某种原因没有被测量。通常，缺失数据会以某种方式进行编码。如果值仅为正数，缺失的特征可能会用
    –1 或历史上用 –999 来标记。如果特征以字符串形式给出，字符串可能为空。对于浮点值，可以使用“不是一个数字”（NaN）。NumPy 使我们能够轻松检查数组中的
    NaN，通过使用`np.isnan`：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that direct comparison to `np.nan` with either `==` or `is` doesn’t work;
    only testing with `np.isnan` works.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，直接使用 `==` 或 `is` 来比较 `np.nan` 是行不通的；只有使用 `np.isnan` 测试才有效。
- en: Detecting missing data is dataset-specific. Assuming we’ve convinced ourselves
    there is missing data, how do we handle it?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 检测缺失数据是特定于数据集的。假设我们已经确信存在缺失数据，应该如何处理它？
- en: 'Let’s generate a small dataset with missing values and use our existing statistics
    knowledge to see how to handle them. The code for the following is in `missing.py`.
    First, we generate a dataset of 1,000 samples, each with four features:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一个带有缺失值的小数据集，并利用现有的统计学知识来看看如何处理它们。以下代码在 `missing.py` 文件中。首先，我们生成一个包含 1,000
    个样本的数据集，每个样本有四个特征：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The dataset is in `x`. We fix the random number seed to get a reproducible result.
    The first feature is uniformly distributed. The second is normally distributed,
    while the third follows a beta distribution and the fourth a lognormal distribution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集位于 `x` 中。我们固定随机数种子以获取可重复的结果。第一个特征是均匀分布的。第二个特征是正态分布的，第三个特征遵循 beta 分布，第四个特征遵循对数正态分布。
- en: 'At the moment, `x` has no missing values. Let’s add some by making random elements
    NaNs:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`x` 没有缺失值。让我们通过将随机元素设为 NaN 来添加一些缺失值：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dataset now has NaNs across 5 percent of its values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据集的 5% 的值是 NaN。
- en: If a few samples in a large dataset have missing data, we can remove them from
    the dataset with little worry. However, if 5 percent of the samples have missing
    data, we probably don’t want to lose that much data. More worrisome still, what
    if there’s a correlation between the missing data and a particular class? Throwing
    the samples away might bias the dataset in some way that’ll make the model less
    useful.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个大型数据集中的少数样本有缺失数据，我们可以删除这些样本而不必太担心。然而，如果 5% 的样本存在缺失数据，我们可能不想丢失这么多数据。更令人担忧的是，如果缺失数据与某个特定类别之间存在关联呢？丢弃这些样本可能会使数据集产生某种偏差，从而使得模型的效果变差。
- en: So, what can we do? We just spent many pages learning how to summarize a dataset
    with basic descriptive statistics. Can we use those? Of course. We can look at
    the distributions of the features, ignoring the missing values, and use those
    distributions to decide how we might want to replace the missing data. Naively,
    we’d use the mean of the data we do have, but looking at the distribution may
    or may not push us toward the median instead, depending on how far the distribution
    is from normal. This sounds like a job for a box plot. Fortunately for us, Matplotlib’s
    `boxplot` function is smart; it ignores the NaNs. Therefore, making the box plot
    is a straightforward call to `boxplot(x)`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该怎么做呢？我们刚刚花了很多篇幅学习如何用基本的描述性统计来概括数据集。我们可以使用这些方法吗？当然可以。我们可以查看特征的分布，忽略缺失值，并利用这些分布来决定如何替换缺失数据。我们可能会天真地使用已有数据的均值，但查看分布可能会引导我们使用中位数，具体取决于分布是否远离正态分布。这似乎是箱线图的工作。幸运的是，Matplotlib
    的 `boxplot` 函数很聪明；它会忽略 NaN。因此，生成箱线图只需要简单地调用 `boxplot(x)`。
- en: '[Figure 4-5](ch04.xhtml#ch04fig05) shows us the dataset with the NaNs ignored.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-5](ch04.xhtml#ch04fig05)展示了忽略 NaN 后的数据集。'
- en: '![image](Images/04fig05.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig05.jpg)'
- en: '*Figure 4-5: Box plot of the dataset ignoring missing values*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-5：忽略缺失值后的数据集箱线图*'
- en: The boxes in [Figure 4-5](ch04.xhtml#ch04fig05) make sense for the distributions
    of the features. Feature 1 is uniformly distributed, so we expect a symmetric
    box around the mean/median. (These are the same for the uniform distribution.)
    Feature 2 is normally distributed, so we get a similar box structure as Feature
    1, but, with only 1,000 samples, some asymmetry is evident. The beta distribution
    of Feature 3 is skewed toward the top of its range, which we see in the box plot.
    Finally, the lognormal distribution of Feature 4 should be skewed toward lower
    values, with a long tail visible as the many “outliers” above the whiskers, an
    object lesson against mindlessly calling such values outliers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-5](ch04.xhtml#ch04fig05)中的框图展示了特征分布的情况。特征 1 服从均匀分布，因此我们期望在均值/中位数周围有一个对称的框（均匀分布的均值和中位数是相同的）。特征
    2 服从正态分布，因此我们得到一个类似于特征 1 的框结构，但由于只有 1,000 个样本，出现了一些不对称性。特征 3 的贝塔分布向其范围的上端偏斜，这在箱型图中可以看到。最后，特征
    4 的对数正态分布应该向较低值偏斜，长尾部分显现为许多位于胡须上方的“异常值”，这是一个警示，告诉我们不要盲目地将这些值称为异常值。'
- en: 'Because we have features that are highly not normally distributed, we’ll update
    missing values with the median instead of the mean. The code is straightforward:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一些特征的分布高度不符合正态分布，我们将用中位数来替换缺失值，而不是用均值。代码非常简单：
- en: '[PRE10]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, `i` first holds the indices of Feature 1 that are not NaNs. We use these
    to calculate the median (`m`). Next, we set `i` to the indices that are NaNs and
    replace them with the median. We can do the same for the other features, updating
    the entire dataset so we no longer have missing values.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`i` 首先保存了特征 1 中非 NaN 的索引。我们使用这些索引来计算中位数（`m`）。接下来，我们将 `i` 设置为 NaN 的索引，并用中位数替换它们。我们可以对其他特征执行相同的操作，更新整个数据集，直到不再有缺失值。
- en: 'Did we cause much of a change from the earlier distributions? No, because we
    only updated 5 percent of the values. For example, for Feature 3, based on the
    beta distribution, the mean and standard deviations change like so:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否对早期的分布产生了很大变化？没有，因为我们只更新了 5% 的值。例如，对于特征 3，根据贝塔分布，均值和标准差变化如下：
- en: '[PRE11]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The moral of the story is that if there’s enough missing data that the dataset
    might become biased by dropping it, the safest thing to do is replace the missing
    data with the mean or median. To decide whether to use the mean or median, consult
    descriptive statistics, a box plot, or a histogram.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 故事的启示是，如果缺失数据足够多，以至于删除它会使数据集产生偏差，那么最安全的做法是用均值或中位数替换缺失值。决定使用均值还是中位数时，可以参考描述性统计、箱型图或直方图。
- en: Additionally, if the dataset is labeled, as a deep learning dataset would be,
    the process described above needs to be completed with the mean or median of samples
    grouped by each class. Otherwise, the calculated value might be inappropriate
    for the class.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果数据集是标注过的，比如深度学习数据集，那么上述过程需要使用每个类别的均值或中位数来完成。否则，计算出的值可能不适用于该类别。
- en: With missing data eliminated, deep learning models can be trained on the dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在剔除缺失数据后，可以在该数据集上训练深度学习模型。
- en: Correlation
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关性
- en: At times, there is an association between the features in a dataset. If one
    goes up, the other might go up as well, though not necessarily in a simple linear
    way. Or, the other might go down—a negative association. The proper word for this
    type of association is *correlation*. A statistic that measures correlation is
    a handy way to understand how the features in a dataset are related.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据集中的特征之间会存在某种关联。如果一个特征上升，另一个特征可能也会上升，尽管这种关系不一定是简单的线性关系。或者，另一个特征可能下降——这就是负相关。描述这种关联关系的正确术语是*相关性*。一种用于衡量相关性的统计量是理解数据集中特征之间关系的便捷工具。
- en: For example, it isn’t hard to see that the pixels of most images are highly
    correlated. This means if we select a pixel at random and then an adjacent pixel,
    there’s a good chance the second pixel will be similar to the first pixel. Images
    where this is not true look to us like random noise.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，不难看出大多数图像的像素之间高度相关。这意味着如果我们随机选择一个像素，再选择一个相邻的像素，那么第二个像素很可能与第一个像素相似。如果这一点不成立，图像就会显得像是随机噪声。
- en: In traditional machine learning, highly correlated features were undesirable,
    as they didn’t add any new information and only served to confuse the models.
    The entire art of feature selection was developed, in part, to remove this effect.
    For modern deep learning, where the network itself learns a new representation
    of the input data, it’s less critical to have uncorrelated inputs. This is, in
    part, why images work as inputs to deep networks when they usually fail to work
    at all with older machine learning models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的机器学习中，高度相关的特征是不受欢迎的，因为它们不会提供任何新的信息，只会混淆模型。特征选择的整个艺术部分是为了去除这种影响。对于现代深度学习来说，由于网络本身学习输入数据的新表示，是否存在不相关的输入变得不那么重要。这部分原因也解释了为什么图像可以作为深度网络的输入，而在旧的机器学习模型中通常完全无法使用。
- en: Whether the learning is traditional or modern, as part of summarizing and exploring
    a dataset, correlations among the features are worth examining and understanding.
    In this section, we’ll discuss two types of correlations. Each type returns a
    single number that measures the strength of the correlation between two features
    in the dataset.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是传统学习还是现代学习，作为总结和探索数据集的一部分，特征之间的相关性是值得检查和理解的。在本节中，我们将讨论两种类型的相关性。每种类型都会返回一个数值，衡量数据集中两个特征之间的相关性强度。
- en: Pearson Correlation
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 皮尔逊相关
- en: The *Pearson correlation coefficient* returns a number, *r* ϵ [–1, +1], that
    indicates the strength of the *linear* correlation between two features. By *linear*
    we mean how strongly we can describe the correlation between the features by a
    line. If the correlation is such that one feature goes up exactly as the other
    feature goes up, the correlation coefficient is +1\. Conversely, if the second
    feature goes down exactly as the other goes up, the correlation is –1\. A correlation
    of zero means there is no association between the two features; they are (possibly)
    independent.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*皮尔逊相关系数* 返回一个数值，*r* ϵ [–1, +1]，表示两个特征之间 *线性* 相关性的强度。这里的 *线性* 指的是我们能多强烈地用一条直线来描述特征之间的相关性。如果相关性是这样的，一个特征的增加正好与另一个特征的增加相对应，那么相关系数就是
    +1。相反，如果第二个特征的减少正好与另一个特征的增加相对应，那么相关系数是 –1。相关系数为零表示两个特征之间没有关联；它们是（可能）独立的。'
- en: I slipped the word *possibly* in the sentence above because there are situations
    where a nonlinear dependence between two features might lead to a zero Pearson
    correlation coefficient. These situations are not common, however, and for our
    purposes, we can claim a correlation coefficient near zero indicates the two features
    are independent. The closer the correlation coefficient is to zero, either positive
    or negative, the weaker the correlation between the features.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上面的句子中加了一个 *可能* 这个词，因为在某些情况下，两个特征之间的非线性依赖关系可能导致皮尔逊相关系数为零。然而，这种情况并不常见，对于我们的目的来说，我们可以认为接近零的相关系数表示两个特征是独立的。相关系数越接近零，无论是正还是负，特征之间的相关性越弱。
- en: The Pearson correlation is defined using the means of the two features or the
    means of products of the two features. The inputs are two features, two columns
    of the dataset. We’ll call these inputs *X* and *Y*, where the capital letter
    refers to a vector of data values. Note, since these are two features from the
    dataset, *X[i]* is paired with *Y[i]*, meaning they both come from the same feature
    vector.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关是通过两个特征的均值或两个特征乘积的均值来定义的。输入是两个特征，数据集中的两列。我们将这些输入称为 *X* 和 *Y*，其中大写字母表示数据值的向量。请注意，由于这两个特征来自数据集，*X[i]*
    和 *Y[i]* 是成对出现的，意味着它们都来自同一个特征向量。
- en: The formula for the Pearson correlation coefficient is
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数的公式是
- en: '![image](Images/04equ05.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ05.jpg)'
- en: 'We’ve introduced a new, but commonly used, notation. The mean of *X* is the
    *expectation* of *X*, denoted as E(*X*). Therefore, in [Equation 4.5](ch04.xhtml#ch04equ05),
    we see the mean of *X*, E(*X*), and the mean of *Y*, E(*Y*). As we might suspect,
    E(*XY*) is the mean of the product of *X* and *Y*, element by element. Similarly,
    E(*X*²) is the mean of the product of *X* with itself, and E(*X*)² is the square
    of the mean of *X*. With this notation in hand, we can easily write our own function
    to calculate the Pearson correlation of two vectors of features:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了一种新的但常用的符号表示法。*X* 的均值是 *X* 的 *期望*，记作 E(*X*)。因此，在[公式 4.5](ch04.xhtml#ch04equ05)中，我们看到
    *X* 的均值 E(*X*) 和 *Y* 的均值 E(*Y*)。正如我们所怀疑的那样，E(*XY*) 是 *X* 和 *Y* 的乘积的均值，逐元素计算。同样，E(*X*²)
    是 *X* 与其自身的乘积的均值，而 E(*X*)² 是 *X* 的均值的平方。有了这个符号表示法，我们可以轻松编写自己的函数来计算两个特征向量的皮尔逊相关系数：
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `pearson` function directly implements [Equation 4.5](ch04.xhtml#ch04equ05).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`pearson` 函数直接实现了 [公式 4.5](ch04.xhtml#ch04equ05)。'
- en: Let’s set up a scenario where we can use `pearson` and compare it to what NumPy
    and SciPy provide. The code that follows, including the definition of `pearson`
    above, is in the file *correlation.py*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一个情境，可以使用 `pearson` 并将其与 NumPy 和 SciPy 提供的函数进行比较。接下来的代码，包括上面定义的 `pearson`
    函数，都在 *correlation.py* 文件中。
- en: First, we’ll create three correlated vectors, `x`, `y`, and `z`. We imagine
    that these are features from a dataset so that `x[0]` is paired with `y[0]` and
    `z[0]`. The code we need is
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建三个相关的向量 `x`、`y` 和 `z`。我们假设这些是数据集中的特征，因此 `x[0]` 与 `y[0]` 和 `z[0]` 配对。我们需要的代码是
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that we’re again fixing the NumPy pseudorandom seed to make the output
    reproducible. The first feature, `x`, is a noisy line from zero to one. The second,
    `y`, tracks `x` but is also noisy because of the multiplication by a random value
    in [0, 1). Finally, `z` is negatively correlated to `x` because of the –0.1 coefficient.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们再次固定了 NumPy 的伪随机种子，以确保输出是可重复的。第一个特征 `x` 是从零到一的噪声线。第二个特征 `y` 跟踪 `x`，但由于与一个
    [0, 1) 之间的随机值相乘，它也是有噪声的。最后，`z` 与 `x` 是负相关的，因为它的系数是 -0.1。
- en: The top chart in [Figure 4-6](ch04.xhtml#ch04fig06) plots the three feature
    values sequentially to see how they track each other. The bottom chart shows the
    three as paired points, with one value on the x-axis and the other on the y-axis.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-6](ch04.xhtml#ch04fig06) 上方的图表顺序绘制了三个特征值，以观察它们如何相互跟踪。下方的图表则显示了三个特征作为配对点，其中一个值在
    x 轴上，另一个值在 y 轴上。'
- en: '![image](Images/04fig06.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig06.jpg)'
- en: '*Figure 4-6: Three features in sequence to show how they track (top), and a
    scatter plot of the features as pairs (bottom)*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-6：三个特征按顺序展示它们如何相互跟踪（上图），以及将特征作为配对的散点图（下图）*'
- en: 'The NumPy function to calculate the Pearson correlation is `np.corrcoef`. Unlike
    our version, this function returns a matrix showing the correlations between all
    pairs of variables passed to it. For example, using our `pearson` function, we
    get the following as the correlation coefficients between `x`, `y`, and `z`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 用于计算皮尔逊相关系数的函数是 `np.corrcoef`。与我们的版本不同，这个函数返回一个矩阵，显示传递给它的所有变量对之间的相关性。例如，使用我们的
    `pearson` 函数，我们得到如下的 `x`、`y` 和 `z` 之间的相关系数：
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'NumPy returns the following, with `x`, `y`, and `z` stacked as a single 3 ×
    100 array:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 返回以下结果，其中 `x`、`y` 和 `z` 被堆叠为一个 3 × 100 的数组：
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The diagonal corresponds to the correlation with each feature and itself, which
    is naturally perfect and therefore 1.0\. The correlation between `x` and `y` is
    in element 0,1 and matches our `pearson` function value. Similarly, the correlation
    between `x` and `z` is in element 0,2, and the correlation between `y` and `z`
    is in element 1,2\. Notice also that the matrix is symmetric, which we expect
    because corr(*X, Y*) = corr(*Y, X*).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线对应于每个特征与自身的相关性，显然是完美的，因此为 1.0。`x` 与 `y` 之间的相关性位于元素 0,1 处，并与我们的 `pearson`
    函数值匹配。同样，`x` 与 `z` 之间的相关性位于元素 0,2，`y` 与 `z` 之间的相关性位于元素 1,2。还要注意，矩阵是对称的，这与我们预期的相符，因为
    corr(*X, Y*) = corr(*Y, X*)。
- en: SciPy’s correlation function is `stats.pearsonr`, which acts like ours but returns
    a *p*-value along with the *r* value. We’ll discuss *p*-values more later in the
    chapter. We use the returned *p*-value as the probability of an uncorrelated system
    producing the calculated correlation value. For our example features, the *p*-value
    is virtually identical to zero, implying there’s no reasonable likelihood that
    an uncorrelated system produced the features.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy 的相关函数是 `stats.pearsonr`，其功能类似于我们的函数，但会返回一个 *p* 值和 *r* 值。我们将在本章后面讨论 *p*
    值。我们使用返回的 *p* 值作为一个无相关系统产生计算得到的相关值的概率。对于我们的示例特征，*p* 值几乎等于零，这意味着没有合理的可能性是一个无相关的系统生成了这些特征。
- en: 'We stated earlier that for images, nearby pixels are usually highly correlated.
    Let’s see if this is actually true for a sample image. We’ll use the China image
    included with `sklearn` and treat specific rows of the green band as the paired
    vectors. We’ll calculate the correlation coefficient for two adjacent rows, a
    row further away, and a random vector:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，对于图像来说，附近的像素通常是高度相关的。让我们看看这对于一张示例图像是否真的成立。我们将使用 `sklearn` 中提供的中国地图图像，并将绿色带的特定行作为配对向量。我们将计算两行相邻行、远离的行和一个随机向量的相关系数：
- en: '[PRE16]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Comparing row 230 and row 231 shows that they are highly positively correlated.
    Comparing rows 230 and 400 shows a weaker and, in this case, negative correlation.
    Finally, as we might expect, correlation with a random vector gives a value approaching
    zero.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 比较第230行和第231行可以看出它们高度正相关。比较第230行和第400行则显示出较弱的负相关性。最后，正如我们所预期的，与随机向量的相关性接近于零。
- en: The Pearson correlation coefficient is so widely used that you’ll often see
    it referred to as merely *the correlation coefficient*. Let’s now take a look
    at a second correlation function and see how it differs from the Pearson coefficient.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数使用广泛，因此你常常会看到它仅被称为*相关系数*。现在让我们来看第二个相关性函数，并看看它与皮尔逊系数有何不同。
- en: Spearman Correlation
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 斯皮尔曼相关性
- en: The second correlation measure we’ll explore is the *Spearman correlation coefficient*,
    ρ ϵ [–1, +1]. It’s a measure based on the ranks of the feature values instead
    of the values themselves.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探讨的第二个相关性度量是*斯皮尔曼相关系数*，ρ ϵ [–1, +1]。这是一个基于特征值排名的度量，而不是基于值本身。
- en: To rank *X*, we replace each value in *X* with the index to that value in the
    sorted version of *X*. If *X* is
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对*X*进行排名，我们用*X*中每个值在*X*的排序版本中的索引替代每个值。如果*X*是
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: then the ranks are
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，排名是
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: because when *X* is sorted, 86 goes in the eighth place (counting from zero),
    and 3 goes first.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因为当*X*排序时，86排在第八位（从零开始计数），而3排在第一位。
- en: The Pearson correlation looks for a linear relationship, whereas the Spearman
    looks for any monotonic association between the inputs.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关性寻找的是线性关系，而斯皮尔曼则寻找输入之间的任何单调关联。
- en: If we have the ranks for the feature values, then the Spearman coefficient is
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经有了特征值的排名，那么斯皮尔曼系数就是
- en: '![image](Images/04equ06.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ06.jpg)'
- en: where *n* is the number of samples and *d* = rank(*X*) – rank(*Y*) is the difference
    of the rank of the paired *X* and *Y* values. Note how [Equation 4.6](ch04.xhtml#ch04equ06)
    is only valid if the rankings are unique (that is, there are no repeated values
    in *X* or *Y*).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*n*是样本数量，*d* = 排名(*X*) – 排名(*Y*)是成对的*X*和*Y*值的排名差异。请注意，只有在排名唯一时，[方程式4.6](ch04.xhtml#ch04equ06)才有效（即*X*或*Y*中没有重复值）。
- en: To calculate *d* in [Equation 4.6](ch04.xhtml#ch04equ06), we need to rank *X*
    and *Y* and use the difference of the ranks. The Spearman correlation is the Pearson
    correlation of the ranks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算[方程式4.6](ch04.xhtml#ch04equ06)中的*d*，我们需要对*X*和*Y*进行排名，并使用排名的差异。斯皮尔曼相关性是排名的皮尔逊相关性。
- en: 'The example above points the way to an implementation of the Spearman correlation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例展示了Spearman相关性的实现方法：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To get the ranks, we need to first sort *X* (`t`). Then, for each value in *X*
    (`x`), we find where it occurs in `t` via `np.where` and take the first element,
    the first match. After building the `rx` list, we make it a floating-point NumPy
    array. We do the same for *Y* to get `ry`. With the ranks, `d` is set to their
    difference, and [Equation 4.6](ch04.xhtml#ch04equ06) is used to return the Spearman
    ρ value.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得排名，我们首先需要对*X*（`t`）进行排序。然后，对于*X*（`x`）中的每个值，我们通过`np.where`找到它在`t`中的位置，并取第一个元素，第一个匹配项。在构建了`rx`列表后，我们将其转换为浮点NumPy数组。对于*Y*，我们做同样的事情来获得`ry`。有了排名，`d`被设置为它们的差值，然后使用[方程式4.6](ch04.xhtml#ch04equ06)返回斯皮尔曼ρ值。
- en: Please note that this version of the Spearman correlation is limited by [Equation
    4.6](ch04.xhtml#ch04equ06) and should be used when there are no duplicate values
    in *X* or *Y*. Our example in this section uses random floating-point values,
    so the probability of an exact duplicate is quite low.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个版本的斯皮尔曼相关性受限于[方程式4.6](ch04.xhtml#ch04equ06)，当*X*或*Y*中没有重复值时应使用。在本节中的示例使用了随机浮点数值，因此完全重复的概率非常低。
- en: 'We’ll compare our `spearman` implementation to the SciPy version, `stats .spearmanr`.
    Like the SciPy version of the Pearson correlation, `stats.spearmanr` returns a
    *p*-value. We’ll ignore it. Let’s see how our function compares:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较我们的`spearman`实现与SciPy版本的`stats.spearmanr`。像SciPy版本的皮尔逊相关性一样，`stats.spearmanr`返回一个*p*-值。我们将忽略它。让我们看看我们的函数如何比较：
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have complete agreement with the SciPy function out to the last bit or so
    of the floating-point value.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与SciPy函数在浮点值的最后几位完全一致。
- en: 'It’s important to remember the fundamental difference between the Pearson and
    Spearman correlations. For example, consider the correlation between a linear
    ramp and the sigmoid function:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住皮尔逊相关性和斯皮尔曼相关性之间的根本区别。例如，考虑线性斜坡与Sigmoid函数之间的相关性：
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, `ramp` increases linearly from –20 to 20 and `sig` follows a sigmoid shape
    (“S” curve). The Pearson correlation will be on the high side, since both are
    increasing as *x* becomes more positive, but the association is not purely linear.
    Running the example gives
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`ramp` 从 -20 增加到 20，而 `sig` 则呈现出 S 型曲线（“S” 曲线）。皮尔逊相关系数会偏高，因为随着 *x* 的增加，`ramp`
    和 `sig` 都在增加，但这种关系并非完全线性。运行示例后得到的结果是
- en: '[PRE22]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: indicating a Pearson correlation of 0.9 but a perfect Spearman correlation of
    1.0, since for every increase in `ramp` there is an increase in `sig` and *only*
    an increase. The Spearman correlation has captured the nonlinear relationship
    between the arguments, while the Pearson correlation has only hinted at it. If
    we’re analyzing a dataset intended for a classical machine learning algorithm,
    the Spearman correlation might help us decide which features to keep and which
    to discard.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明皮尔逊相关系数为 0.9，但斯皮尔曼相关系数为 1.0，因为每次 `ramp` 增加时，`sig` 也只会增加，而没有其他变化。斯皮尔曼相关捕捉到了参数之间的非线性关系，而皮尔逊相关仅仅揭示了这一点。如果我们正在分析一个为传统机器学习算法准备的数据集，斯皮尔曼相关可能帮助我们决定保留哪些特征，丢弃哪些特征。
- en: This concludes our examination of statistics for describing and understanding
    data. Let’s now learn how to use hypothesis testing to interpret experimental
    results and answer questions like “Are these two sets of data samples from the
    same parent distribution?”
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对统计学在描述和理解数据中的应用的探讨结束。接下来，让我们学习如何通过假设检验来解释实验结果，并回答像“这两组数据样本是否来自同一母体分布？”这样的问题。
- en: Hypothesis Testing
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假设检验
- en: We have two independent sets of 50 students studying cell biology. We have no
    reason to believe the groups differ in any significant way, as students from the
    larger population were assigned randomly. Group 1 attended the lectures and, in
    addition, worked through a structured set of computer exercises. Group 2 only
    attended the lectures. Both groups took the same final examination, leading to
    the test scores given in [Table 4-1](ch04.xhtml#ch04tab01). We want to know if
    asking the students to work through the computer exercises made a difference in
    their final test scores.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两组各 50 名学生学习细胞生物学。由于学生是随机分配的，因此我们没有理由认为两组之间存在显著差异。组 1 参加了讲座，并额外完成了一系列结构化的计算机练习；组
    2 仅参加了讲座。两组都参加了相同的期末考试，得到了[表 4-1](ch04.xhtml#ch04tab01)中的测试成绩。我们想知道让学生进行计算机练习是否对他们的期末考试成绩有所影响。
- en: '**Table 4-1:** Group 1 and Group 2 Test Scores'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-1：** 组 1 和 组 2 的测试成绩'
- en: '| **Group 1** | 81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88
    89 92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85
    85 78 94 100 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| **组 1** | 81 80 85 87 83 87 87 90 79 83 88 75 87 92 78 80 83 91 82 88 89
    92 97 82 79 82 82 85 89 91 83 85 77 81 90 87 82 84 86 79 84 85 90 84 90 85 85
    78 94 100 |'
- en: '| **Group 2** | 92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86
    77 94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88
    91 83 85 73 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **组 2** | 92 82 78 74 86 69 83 67 85 82 81 91 79 82 82 88 80 63 85 86 77
    94 85 75 77 89 86 71 82 82 80 88 72 91 90 92 95 87 71 83 94 90 78 60 76 88 91
    83 85 73 |'
- en: '[Figure 4-7](ch04.xhtml#ch04fig07) shows a box plot of [Table 4-1](ch04.xhtml#ch04tab01).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](ch04.xhtml#ch04fig07)展示了[表 4-1](ch04.xhtml#ch04tab01)的数据箱线图。'
- en: To understand if there is a significant change in final test scores between
    the two groups, we need to test some hypotheses. The method we’ll use to test
    the hypotheses is known as *hypothesis testing*, and it’s a critical piece of
    modern science.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解两组之间的最终测试成绩是否有显著变化，我们需要进行假设检验。我们将使用的检验方法被称为 *假设检验*，这是现代科学中的一个关键环节。
- en: '![image](Images/04fig07.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig07.jpg)'
- en: '*Figure 4-7: Box plot for the data in [Table 4-1](ch04.xhtml#ch04tab01)*'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-7：来自[表 4-1](ch04.xhtml#ch04tab01)的数据箱线图*'
- en: 'Hypothesis testing is a broad topic, too extensive for us to provide more than
    a minimal introduction here. As this is a book on deep learning, we’ll focus on
    the scenario a deep learning researcher is likely to encounter. We’ll consider
    only two hypothesis tests: the t-test for unpaired samples of differing variance
    (a parametric test) and the Mann-Whitney U (a nonparametric test). As we progress,
    we’ll understand what these tests are and why we’re restricting ourselves to them,
    as well as the meaning of *parametric* and *nonparametric*.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验是一个广泛的主题，内容过于庞大，无法在这里提供全面介绍。由于这是一本关于深度学习的书，我们将重点讨论深度学习研究者可能遇到的情境。我们将仅考虑两种假设检验：针对方差不同的非配对样本的
    t 检验（一个参数检验）和 Mann-Whitney U 检验（一个非参数检验）。随着我们深入了解这些检验的含义，我们将理解为什么选择这两种检验，以及什么是
    *参数检验* 和 *非参数检验*。
- en: To be successful with hypothesis testing, we need to know what we mean by *hypothesis*,
    so we’ll address that first, along with our rationale for limiting the types of
    hypothesis testing we’ll consider. With the hypothesis concept in hand, we’ll
    discuss the t-test and the Mann-Whitney U test in turn, using the data in [Table
    4-1](ch04.xhtml#ch04tab01) as our example. Let’s get started.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功进行假设检验，我们需要明确什么是*假设*，因此我们首先会解决这个问题，并解释为何我们会限制考虑的假设检验类型。掌握了假设的概念后，我们将依次讨论t检验和Mann-Whitney
    U检验，并以[表4-1](ch04.xhtml#ch04tab01)中的数据作为示例。让我们开始吧。
- en: Hypotheses
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 假设
- en: To understand if two sets of data are from the same parent distribution or not,
    we might look at summary statistics. [Figure 4-7](ch04.xhtml#ch04fig07) shows
    us the box plot for Group 1 and Group 2\. It appears that the two groups have
    different means and standard deviations. How do we know? The box plot shows us
    the location of the medians, and the whiskers tell us something about the variance.
    Both of these together hint that the means will be different because the medians
    are different, and both sets of data are reasonably symmetric around the median.
    The space between the whiskers hints at the standard deviation. So, let’s make
    hypotheses using the means of the datasets.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解两组数据是否来自相同的母体分布，我们可以查看摘要统计量。[图4-7](ch04.xhtml#ch04fig07)显示了第1组和第2组的箱型图。看起来这两个组的均值和标准差不同。我们怎么知道的呢？箱型图显示了中位数的位置，而“须”部分告诉我们一些关于方差的信息。这两者结合起来表明均值可能不同，因为中位数不同，并且这两组数据都在中位数周围对称。须之间的距离暗示了标准差。那么，让我们用数据集的均值来提出假设。
- en: In hypothesis testing, we have two hypotheses. The first, known as the *null
    hypothesis* (*H*[0]), is that the two sets of data *are* from the same parent
    distribution, that there is nothing special to differentiate them. The second
    hypothesis, the *alternative hypothesis* (*H[a]*), is that the two groups are
    not from the same distribution. Since we’ll be using the means, *H*[0] is saying
    that the means, really the means of the parent population that generated the data,
    are the same. Similarly, if we reject *H*[0], we are implicitly accepting *H[a]*
    and claiming we have evidence that the means are different. We don’t have the
    true population means, so we’ll use the sample means and standard deviations instead.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设检验中，我们有两个假设。第一个称为*零假设*（*H*[0]），即这两个数据集*来自*相同的母体分布，意味着它们没有显著的区别。第二个假设是*备择假设*（*H[a]*），即这两个组来自不同的分布。由于我们将使用均值，*H*[0]的意思是，这些数据实际上是来自生成这些数据的母体的均值是相同的。类似地，如果我们拒绝了*H*[0]，我们就隐式接受了*H[a]*，并声明我们有证据表明均值存在差异。由于我们没有真正的总体均值，因此我们将使用样本均值和标准差来代替。
- en: Hypothesis testing doesn’t tell us definitively whether *H*[0] is true. Instead,
    it gives us evidence in favor of rejecting or accepting the null hypothesis. It’s
    critical to remember this.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验并不能明确告诉我们*H*[0]是否成立。相反，它为我们提供了拒绝或接受零假设的证据。记住这一点是至关重要的。
- en: We’re testing two independent samples to see if we should think of them as coming
    from the same parent distribution. There are other ways to use hypothesis testing,
    but we rarely encounter them in deep learning. For the task at hand, we need the
    sample means and the sample standard deviations. Our tests will ask the question,
    “Is there a meaningful difference in the means of these two sets?”
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在测试两个独立样本，看看它们是否可以认为来自相同的母体分布。虽然有其他方式可以使用假设检验，但在深度学习中我们很少遇到。对于眼前的任务，我们需要样本均值和样本标准差。我们的测试将提出这样一个问题：“这两个数据集的均值之间有显著差异吗？”
- en: We’re only interested in detecting whether the two groups of data are from the
    same parent distribution, so another simplification we’ll make is that all of
    our tests will be *two-sided*, or *two-tailed*. When we use a test, like the t-test
    we’ll describe next, we’re comparing our calculated test statistic (the t-value)
    to the distribution of the test statistic and asking questions about how likely
    our calculated t-value is. If we want to know about the test statistic being above
    or below some fraction of that distribution, we’re making a two-sided test. If
    instead we want to know about the likelihood of the test statistic being above
    a particular value without caring about it being below, or vice versa, then we’re
    making a one-sided test.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心检测这两组数据是否来自相同的母体分布，因此我们将做出另一个简化假设：我们所有的检验都将是*双侧*的，或者说是*双尾*的。当我们使用像接下来要描述的t检验时，我们将我们的计算检验统计量（t值）与检验统计量的分布进行比较，并询问我们计算得到的t值有多可能。如果我们想了解检验统计量高于或低于分布的某一部分，那么我们就是在进行双侧检验。如果我们只关心检验统计量是否超过某个特定值，而不关心它是否低于该值，或者反之，那么我们就是在进行单侧检验。
- en: 'Let’s lay out our assumptions and approach:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出我们的假设和方法：
- en: We have two independent sets of data we wish to compare.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有两组独立的数据需要进行比较。
- en: We’re making no assumption as to whether the standard deviations of the data
    are the same.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们并没有假设数据的标准差是否相同。
- en: 'Our null hypothesis is that the means of the parent distributions of the datasets
    are the same, *H*[0] : μ[1] = μ[2]. We’ll use the sample means ![image](Images/094equ01a.jpg)
    and sample standard deviations (*s*[1], *s*[2]) to help us decide to accept or
    reject *H*[0].'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们的原假设是数据集的母体分布的均值相同，*H*[0] : μ[1] = μ[2]。我们将使用样本均值 ![image](Images/094equ01a.jpg)
    和样本标准差（*s*[1]，*s*[2]）来帮助我们决定接受或拒绝 *H*[0]。'
- en: Hypothesis tests assume that the data is *independent and identically distributed
    (i.i.d.)*. We interpret this as a statement that the data is a fair random sample.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设检验假定数据是*独立同分布（i.i.d.）*的。我们将其解释为数据是一个公平的随机样本。
- en: With these assumptions understood, let’s start with the t-test, the most widely
    used hypothesis test.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了这些假设之后，让我们从t检验开始，这是最广泛使用的假设检验方法。
- en: The t-test
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: t检验
- en: The *t-test* depends on *t*, the test statistic. This statistic is compared
    to the t-distribution and used to generate a *p*-value, a probability we’ll use
    to reach a conclusion about *H*[0]. There’s a rich history behind the t-test and
    the related z-test that we’ll ignore here. I encourage you to dive more deeply
    into hypothesis testing when you have the chance or, at a minimum, review thoughtful
    articles about the proper way to do a hypothesis test and interpret its results.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*t检验*依赖于*t*，即检验统计量。这个统计量与t分布进行比较，并用来生成一个*p*-值，这个概率值将帮助我们得出关于 *H*[0] 的结论。t检验和相关的z检验有着丰富的历史背景，我们在这里将忽略这些背景。我鼓励你在有机会时深入了解假设检验，或者至少复习一些关于正确进行假设检验和解读其结果的有价值文章。'
- en: The t-test is a *parametric* test. This means there are assumptions about the
    data and the distribution of the data. Specifically, the t-test assumes, beyond
    the data being i.i.d., that the distribution (histogram) of the data is normal.
    We’ve stated before that many physical processes do seem to follow a normal distribution,
    so there’s reason to think that data from actual measurements might do so.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: t检验是一个*参数检验*。这意味着数据和数据分布有一些假设。具体来说，t检验假设除了数据是i.i.d.之外，数据的分布（直方图）是正态分布。我们之前已经提到，许多物理过程似乎遵循正态分布，因此有理由认为实际测量的数据也可能遵循正态分布。
- en: There are many ways to test if a dataset is normally distributed, but we’ll
    ignore them, as there’s some debate about the utility of such tests. Instead,
    I’ll (somewhat recklessly) suggest you use the t-test and the Mann-Whitney U test
    together to help make your decision about accepting or rejecting *H*[0]. Using
    both tests might lead to a situation where they disagree, where one test says
    there’s evidence against the null hypothesis and the other says there isn’t. In
    general, if the nonparametric test is claiming evidence against *H*[0], then one
    should probably accept that evidence regardless of the t-test result. If the t-test
    result is against *H*[0], but the Mann-Whitney U test isn’t, and you think the
    data is normal, then you might also accept the t-test result.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以测试数据集是否服从正态分布，但我们将忽略这些方法，因为关于此类测试的有效性存在一些争议。相反，我（有些鲁莽地）建议你同时使用 t 检验和
    Mann-Whitney U 检验来帮助决定是否接受或拒绝 *H*[0]。使用这两种检验可能会导致它们得出相反的结论，一种检验表明存在反对零假设的证据，而另一种则表明没有证据。通常情况下，如果非参数检验表明有证据反对
    *H*[0]，那么无论 t 检验的结果如何，应该接受这一证据。如果 t 检验结果反对 *H*[0]，但 Mann-Whitney U 检验结果没有，并且你认为数据是正态的，那么你可能也会接受
    t 检验结果。
- en: The t-test has different versions. We explicitly stated above that we’ll use
    a version designed for datasets of differing size and variance. The specific version
    of the t-test we’ll use is *Welch’s t-test*, which doesn’t assume the variance
    of the two datasets is the same.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: t 检验有不同的版本。我们在上面明确说明了将使用一个针对不同大小和方差的数据集设计的版本。我们将使用的 t 检验版本是 *Welch’s t-test*，它不假设两组数据的方差相同。
- en: The t-score for Welch’s t-test is
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Welch’s t-test 的 t 分数是
- en: '![image](Images/095equ01.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/095equ01.jpg)'
- en: where *n*[1] and *n*[2] are the size of the two groups.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[1] 和 *n*[2] 是两组的大小。'
- en: The t-score, and an associated value known as the *degrees of freedom*, which
    is similar to but also different from the degrees of freedom mentioned above,
    generates the appropriate t-distribution curve. To get a *p*-value, we calculate
    the area under the curve, both above and below (positive and negative t-score),
    and return it. Since the integral of a probability distribution is 1, the total
    area under the tails from the positive and negative t-score value to positive
    and negative infinity will be the *p*-value. We’ll use the degrees of freedom
    below to help us calculate confidence intervals.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: t 分数和一个称为 *自由度* 的相关值，类似于但又不同于上面提到的自由度，生成适当的 t 分布曲线。为了获得 *p*-值，我们计算曲线下的面积，包括正负
    t 分数下方的面积，并将其返回。由于概率分布的积分为 1，正负 t 分数值到正负无穷大的尾部的总面积即为 *p*-值。我们将使用下面的自由度来帮助我们计算置信区间。
- en: What does the *p*-value tell us? It tells us the probability of seeing the difference
    between the two means we see, or larger, *if* the null hypothesis is true. Typically,
    if this probability is below some threshold we’ve chosen, we reject the null hypothesis
    and say we have evidence that the two groups have different means—that they come
    from different parent distributions. When we reject *H*[0], we say that the difference
    is *statistically significant*. The threshold for accepting/rejecting *H*[0] is
    called α, usually with α = 0.05 as a typical, if problematic, value. We’ll discuss
    why 0.05 is problematic below.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*-值告诉我们什么？它告诉我们，在零假设成立的情况下，看到我们看到的两个均值之间的差异，或更大的差异的概率。通常，如果这个概率低于我们设定的某个阈值，我们就拒绝零假设，并说我们有证据表明这两组的均值不同——它们来自不同的母体分布。当我们拒绝
    *H*[0] 时，我们说这个差异是 *统计显著的*。接受/拒绝 *H*[0] 的阈值称为 α，通常 α = 0.05 是一个典型的（尽管有问题的）值。我们将在下面讨论为什么
    0.05 是一个有问题的值。'
- en: 'The point to remember is that the *p*-value assumes the null hypothesis is
    true. It tells us the likelihood of a true *H*[0] giving us at least the difference
    we see, or greater, between the groups. If the *p*-value is small, that has two
    possible meanings: (1) the null hypothesis is false, or (2) a random sampling
    error has given us samples that fall outside what we might expect. Since the *p*-value
    assumes *H*[0] is true, a small *p*-value helps us believe less and less in (2)
    and boosts our confidence that (1) might be correct. However, the *p*-value alone
    cannot confirm (1); other knowledge needs to come into play.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，*p* 值假设零假设为真。它告诉我们，在假设真实 *H*[0] 的情况下，至少能得到我们所观察到的样本间差异或更大的差异的概率。如果
    *p* 值较小，这有两种可能的含义：（1）零假设是错误的，或者（2）随机抽样误差导致我们得到了超出预期的样本。由于 *p* 值假设 *H*[0] 为真，一个较小的
    *p* 值有助于我们减少对（2）的信心，并增加对（1）可能正确的信心。然而，*p* 值本身不能确认（1）；需要其他知识的支持。
- en: 'I mentioned that using α = 0.05 is problematic. The main reason it’s problematic
    is that it’s too generous; it leads to too many rejections of a true null hypothesis.
    According to James Berger and Thomas Sellke in their article “Testing a Point
    Null Hypothesis: The Irreconcilability of *P* Values and Evidence” (*Journal of
    the American Statistical Association*, 1987), when α = 0.05, about 30 percent
    of true null hypotheses will be rejected. When we use something like α ≤ 0.001,
    the chance of falsely rejecting a true null hypothesis goes down to less than
    3 percent. The moral of the story is that *p* < 0.05 is not magic and, frankly,
    is unconvincing for a single study. Look for highly significant *p*-values of
    at least 0.001 or, preferably, much smaller. At *p*  = 0.05, all you have is a
    suggestion, and you should repeat the experiment. If repeated experiments all
    have a *p*-value around 0.05, then rejecting the null hypothesis begins to make
    sense.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到使用 α = 0.05 是有问题的。问题的主要原因是它过于宽松，导致过多地拒绝真实的零假设。根据 James Berger 和 Thomas Sellke
    在其文章《检验点零假设：*P* 值与证据的不可调和性》（*美国统计学会期刊*，1987年）中的论述，当 α = 0.05 时，大约 30% 的真实零假设会被拒绝。而当我们使用类似
    α ≤ 0.001 的值时，错误拒绝真实零假设的几率降到不到 3%。这个故事的寓意是，*p* < 0.05 不是魔法，坦率地说，对于单个研究来说是不可令人信服的。应该寻找至少为
    0.001，或者最好更小的高度显著的 *p* 值。在 *p* = 0.05 时，所得到的只是一个提示，应该重复实验。如果重复实验的 *p* 值都接近 0.05，那么拒绝零假设才开始显得有意义。
- en: Confidence Intervals
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 置信区间
- en: Along with a *p*-value, you’ll often see *confidence intervals (CIs)*. The confidence
    interval gives bounds within which we believe the true population difference in
    the means will lie, with a given confidence for repeated samples of the two datasets
    we’re comparing. Typically, we report 95 percent confidence intervals. Our hypothesis
    tests check for equality of means by asking if the difference of the sample means
    is zero or not. Therefore, any CI that includes zero signals to us that we cannot
    reject the null hypothesis.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与 *p* 值一起，你通常还会看到 *置信区间（CIs）*。置信区间提供了一个范围，在这个范围内我们认为群体均值的真实差异会存在，且具有对比两个数据集重复样本的给定置信度。通常，我们报告的是
    95% 置信区间。我们的假设检验通过检验样本均值的差异是否为零来检查均值是否相等。因此，任何包含零的置信区间都在告诉我们，不能拒绝零假设。
- en: For Welch’s t-test, the degrees of freedom is
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Welch 的 t 检验，自由度是
- en: '![image](Images/04equ07.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ07.jpg)'
- en: which we can use to calculate confidence intervals,
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用它来计算置信区间，
- en: '![image](Images/04equ08.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ08.jpg)'
- en: where *t*[1–α/2,*df*] is the critical value, and the t-value for the given confidence
    level (α) and the degrees of freedom, *df*, come from [Equation 4.7](ch04.xhtml#ch04equ07).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *t*[1–α/2,*df*] 是临界值，t 值是根据给定的置信水平 (α) 和自由度 *df* 从[公式 4.7](ch04.xhtml#ch04equ07)得出的。
- en: 'How should we interpret the 95 percent confidence interval? There is a population
    value: the true difference between the group means. The 95 percent confidence
    interval is such that if we could draw repeated samples from the distribution
    that produced the two datasets, 95 percent of the calculated confidence intervals
    would contain the true difference between the means. It is *not* the range that
    includes the true difference in the means at 95 percent certainty.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何解读 95% 置信区间？有一个总体值：群体均值之间的真实差异。95% 置信区间的含义是，如果我们能够从生成两个数据集的分布中反复抽取样本，95%
    计算出的置信区间将包含均值之间的真实差异。它*不是*指包含均值差异的范围，且在 95% 的置信度下。
- en: Beyond checking if zero is in the CI, the CI is useful because its width tells
    us something about the magnitude of the effect. Here, the effect is related to
    the difference between the means. We may have a statistically significant difference
    based on the *p*-value, but the effect might be practically meaningless. The CI
    will be narrow when the effect is large because small CIs imply a narrow range
    encompassing the true effect. We’ll see shortly how, when possible, to calculate
    another useful measure of effect.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检查零是否在置信区间（CI）内外，CI 还有其他用途，因为它的宽度能告诉我们效应的大小。在这里，效应与均值之间的差异有关。我们可能基于*p*-值获得统计显著的差异，但效应可能在实际中没有意义。当效应较大时，CI
    会很窄，因为小的 CI 表示一个包围真实效应的狭窄范围。我们稍后将看到，在可能的情况下，如何计算另一个有用的效应度量。
- en: Finally, a *p*-value less than α also will have a *CI*[α] that does not include
    *H*[0]. In other words, what the *p*-value tells us and what the confidence interval
    tells us track–they will not contradict each other.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个小于 α 的 *p*-值也会有一个不包含 *H*[0] 的 *CI*[α]。换句话说，*p*-值和置信区间告诉我们的信息是同步的——它们不会互相矛盾。
- en: Effect Size
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 效应大小
- en: It’s one thing to have a statistically significant *p*-value. It’s another for
    the difference represented by that *p*-value to be meaningful in the real world.
    A popular measure of the size of an effect, the *effect size*, is *Cohen’s d*.
    For us, since we’re using Welch’s t-test, Cohen’s *d* is found by calculating
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个统计上显著的 *p*-值是一回事。另一个问题是由该 *p*-值表示的差异在现实世界中是否有意义。一个流行的效应大小度量是 *Cohen 的 d*。对于我们来说，因为我们使用的是
    Welch 的 t 检验，Cohen 的 *d* 是通过计算得到的
- en: '![image](Images/04equ09.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04equ09.jpg)'
- en: Cohen’s *d* is usually interpreted subjectively, though we should report the
    numeric value as well. Subjectively, the size of the effect could be
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Cohen 的 *d* 通常是主观解释的，尽管我们也应该报告数值。主观上，效应的大小可以是
- en: '| ***d*** | **Effect** |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| ***d*** | **效应** |'
- en: '| --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.2 | Small |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | 小 |'
- en: '| 0.5 | Medium |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 中等 |'
- en: '| 0.8 | Large |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 | 大 |'
- en: Cohen’s *d* makes sense. The difference between the means is a natural way to
    think about the effect. Scaling it by the mean variance puts it in a consistent
    range. From [Equation 4.9](ch04.xhtml#ch04equ09), we see that a *p*-value corresponding
    to a statistically significant result might lead to a small effect that isn’t
    of any true practical importance.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Cohen 的 *d* 是合理的。均值之间的差异是思考效应的一种自然方式。通过均值方差进行缩放后，它被置于一个一致的范围内。从[公式 4.9](ch04.xhtml#ch04equ09)中我们可以看到，尽管与统计显著结果相关的
    *p*-值可能导致一个小的效应，但该效应在实际中可能没有真正的重要性。
- en: Evaluating the Test Scores
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估测试成绩
- en: 'Let’s put all of the above together to apply the t-test to our test data from
    [Table 4-1](ch04.xhtml#ch04tab01). You’ll find the code in the file *hypothesis.py*.
    We generate the data-sets first:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把以上所有内容结合起来，应用 t 检验到我们来自[表格 4-1](ch04.xhtml#ch04tab01)的测试数据。你可以在文件 *hypothesis.py*
    中找到代码。我们首先生成数据集：
- en: '[PRE23]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once again, we’re using a fixed NumPy pseudorandom number seed for repeatability.
    We make `a` a sample from a normal distribution with a mean of 85 and a standard
    deviation of 6.0\. We select `b` from a normal distribution with a mean of 82
    and a standard deviation of 7.0\. For both, we cap any values over 100 to 100\.
    These are test scores, after all, without extra credit.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们使用固定的 NumPy 伪随机数种子来确保可重复性。我们让 `a` 作为从均值为 85、标准差为 6.0 的正态分布中抽样的样本。我们从均值为
    82、标准差为 7.0 的正态分布中选择 `b`。对于两者，我们将超过 100 的值限制为 100。毕竟，这些是考试成绩，没有额外学分。
- en: 'We apply the t-test next:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应用 t 检验：
- en: '[PRE24]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We get *(t* = 2.40234, *p* = 0.01852). The *t* is the statistic, and *p* is
    the computed *p*-value. It’s 0.019, which is less than 0.05 but only by a factor
    of two. We have a weak result telling us we might want to reject the null hypothesis
    and believe that the two groups, `a` and `b`, come from different distributions.
    Of course, we know they do because we generated them, but it’s nice to see the
    test pointing in the right direction.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到 *(t* = 2.40234, *p* = 0.01852)。*t* 是统计量，*p* 是计算得出的 *p*-值。它是 0.019，小于 0.05，但仅小了一倍。我们得到了一个较弱的结果，这告诉我们可能想要拒绝零假设，并认为两组
    `a` 和 `b` 来自不同的分布。当然，我们知道它们确实来自不同的分布，因为我们是自己生成的，但看到检验结果朝正确的方向发展也让人放心。
- en: Notice that the function we import from SciPy is `ttest_ind`. This is the function
    to use for independent samples, which are not paired. Also, notice that we added
    `equal_var=False` to the call. This is how to use Welch’s t-test, which doesn’t
    assume that the variance between the two datasets is equal. We know they’re not
    equal, since `a` uses a standard deviation of 6.0 while `b` uses 7.0.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们从 SciPy 导入的函数是 `ttest_ind`。这是用于独立样本（即未配对样本）的函数。此外，请注意我们在调用时添加了 `equal_var=False`。这是使用
    Welch 的 t 检验的方法，它不假设两个数据集之间的方差相等。我们知道它们不相等，因为 `a` 的标准差为 6.0，而 `b` 的标准差为 7.0。
- en: 'To get the confidence intervals, we’ll write a CI function, since NumPy and
    SciPy don’t include one. The function directly implements [Equations 4.7](ch04.xhtml#ch04equ07)
    and [4.8](ch04.xhtml#ch04equ08):'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取置信区间，我们将编写一个 CI 函数，因为 NumPy 和 SciPy 没有包含此功能。该函数直接实现了 [方程 4.7](ch04.xhtml#ch04equ07)
    和 [4.8](ch04.xhtml#ch04equ08)：
- en: '[PRE25]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The critical *t* value is given by calling `stats.t.ppf`, passing in the α/2
    value and the proper degrees of freedom, *df*. The critical *t* value is the 97.5
    percent percentile value, for α = 0.05, which is what the *percent point function
    (ppf)* returns. We divide by two to cover the tails of the t-distribution.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 临界 *t* 值通过调用 `stats.t.ppf` 来获取，传入 α/2 值和正确的自由度 *df*。临界 *t* 值是 97.5 百分位值，对于 α
    = 0.05，这是 *百分位点函数 (ppf)* 返回的值。我们将其除以 2，以涵盖 t 分布的尾部。
- en: 'For our test example, the confidence interval is [0.56105, 5.95895]. Notice
    how this does not include zero, so the CI also indicates a statistically significant
    result. However, the range is rather large, so this is not a particularly robust
    result. The CI range can be difficult to interpret on its own, so, finally, let’s
    calculate Cohen’s *d* to see if it makes sense given the width of the confidence
    interval. In code, we implement [Equation 4.9](ch04.xhtml#ch04equ09):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的测试示例，置信区间为 [0.56105, 5.95895]。注意，这个区间不包含零，因此置信区间也表明结果具有统计学意义。然而，范围相当大，因此这不是一个特别稳健的结果。置信区间的范围可能单独解释起来较为困难，因此，最后，让我们计算
    Cohen 的 *d*，看看考虑到置信区间的宽度，这是否合理。在代码中，我们实现了 [方程 4.9](ch04.xhtml#ch04equ09)：
- en: '[PRE26]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We get *d* = 0.48047, corresponding to a medium effect size.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到 *d* = 0.48047，对应于中等效应大小。
- en: The Mann-Whitney U Test
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Mann-Whitney U 检验
- en: The t-test assumes the distribution of the source data is normal. If the data
    is not normally distributed, we should instead use a *nonparametric test*. Nonparametric
    tests make no assumptions about the underlying distribution of the data. The *Mann-Whitney
    U test*, sometimes called the *Wilcoxon rank-sum test*, is a nonparametric test
    to help decide if two different sets of data come from the same parent distribution.
    The Mann-Whitney U test does not rely directly on the values of the data, but
    instead uses the data’s ranking.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: t 检验假设源数据的分布是正态分布。如果数据不是正态分布，我们应当使用 *非参数检验*。非参数检验不对数据的基础分布做任何假设。*Mann-Whitney
    U 检验*，有时也称为 *Wilcoxon 秩和检验*，是一种非参数检验，用来帮助判断两组不同的数据是否来自相同的母体分布。Mann-Whitney U 检验不直接依赖于数据的值，而是使用数据的排名。
- en: 'The null hypothesis for this test is the following: the probability that a
    randomly selected value from Group 1 is larger than a randomly selected value
    from Group 2 is 0.5\. Let’s think a bit about that. If the data is from the same
    parent distribution, then we should expect any randomly selected pair of values
    from the two groups to show no preference as to which is larger than the other.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 该检验的零假设如下：从第 1 组随机选出的值大于从第 2 组随机选出的值的概率为 0.5。我们可以稍微思考一下这个假设。如果数据来自同一个母体分布，那么我们应当期望从这两组中随机选出的任意一对值在大小上没有偏好。
- en: The alternative hypothesis is that the probability of a randomly selected value
    from Group 1 being larger than a randomly selected value from Group 2 is not 0.5\.
    Notice, there is no statement as to the probability being greater or less than
    0.5, only that it isn’t 0.5; thus, the Mann-Whitney U test, as we’ll use it, is
    two-sided.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 备择假设是，随机从第 1 组选出的值大于随机从第 2 组选出的值的概率不为 0.5。请注意，这里并没有说明该概率是大于还是小于 0.5，仅仅是它不是 0.5；因此，我们将使用的
    Mann-Whitney U 检验是双尾的。
- en: The null hypothesis for the Mann-Whitney U test is not the same as the null
    hypothesis for the t-test. For the t-test, we’re asking whether the means between
    the two groups are the same. (Really, we’re asking if the difference in the means
    is zero.) However, if two sets of data *are* from different parent distributions,
    both null hypotheses are false, so we can use the Mann-Whitney U test in place
    of the t-test, especially when the underlying data is not normally distributed.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Mann-Whitney U 检验的零假设与 t 检验的零假设不同。对于 t 检验，我们在问两组之间的均值是否相同。（实际上，我们在问均值的差异是否为零。）然而，如果两组数据确实来自不同的母体分布，那么这两个零假设都是错误的，因此我们可以用
    Mann-Whitney U 检验替代 t 检验，特别是当数据不是正态分布时。
- en: To generate *U*, the Mann-Whitney statistic, we first pool both sets of data
    and rank them. Ties are replaced with the mean between the tie value rank and
    the next rank value. We also keep track of the source group so we can separate
    the list of ranks again. The ranks, by group, are summed to give *R*[1] and *R*[2]
    (using the ranks from the pooled data). We calculate two values,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成 *U*，即 Mann-Whitney 统计量，我们首先将两组数据合并并进行排名。相同的值会被替换为该值排名与下一个排名值之间的平均值。我们还会跟踪数据源组，以便稍后可以重新分组排名。按组求和后的排名得到
    *R*[1] 和 *R*[2]（使用合并数据的排名）。我们计算两个值，
- en: '![image](Images/100equ01.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/100equ01.jpg)'
- en: with the smaller called *U*, the test statistic. It’s possible to generate a
    *p*-value from *U*, keeping in mind all the discussion above about the meaning
    and use of *p*-values. As before, *n*[1] and *n*[2] are the number of samples
    in the two groups. The Mann-Whitney U test requires the smaller of these two numbers
    to be at least 21 samples. If you don’t have that many, the results may not be
    reliable when using the SciPy `mannwhitneyu` function.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 小的值称为 *U*，即检验统计量。可以从 *U* 生成 *p*-值，但需要记住上面关于 *p*-值含义和使用的讨论。与之前一样，*n*[1] 和 *n*[2]
    是两组中的样本数。Mann-Whitney U 检验要求这两个数字中的较小者至少为 21 个样本。如果样本数不足，使用 SciPy `mannwhitneyu`
    函数时，结果可能不可靠。
- en: We can run the Mann-Whitney U test on our test data from [Table 4-1](ch04.xhtml#ch04tab01),
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在来自[表 4-1](ch04.xhtml#ch04tab01)的测试数据上运行 Mann-Whitney U 检验，
- en: '[PRE27]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: with `a` and `b` as we used above for the t-test. This gives us *(U* = 997.00000,
    *p* = 0.04058). The *p*-value is barely below the minimum threshold of 0.05.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上面提到的 `a` 和 `b` 作为 t 检验的样本。这给出了 *(U* = 997.00000, *p* = 0.04058)。*p*-值刚好低于
    0.05 的最小阈值。
- en: The means of `a` and `b` are 85 and 82, respectively. What happens to the *p*-values
    if we make the mean value of `b` 83 or 81? Changing the mean of `b` means changing
    the first argument to `np.random.normal`. Doing this gives us [Table 4-2](ch04.xhtml#ch04tab02),
    where I’ve included all results for completeness.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`a` 和 `b` 的均值分别是 85 和 82。如果我们将 `b` 的均值设置为 83 或 81，*p*-值会发生什么变化？改变 `b` 的均值意味着改变
    `np.random.normal` 的第一个参数。这样做会得到[表 4-2](ch04.xhtml#ch04tab02)，在这里我已经包括了所有结果，以确保完整性。'
- en: '**Table 4-2:** Mann-Whitney U Test and t-test Results for the Simulated Test
    Scores with Different Means (*n*[1]=*n*[2]=50)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-2：** 不同均值的模拟测试得分的 Mann-Whitney U 检验和 t 检验结果 (*n*[1]=*n*[2]=50)'
- en: '| **Means** | **Mann-Whitney U** | **t-test** |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | **Mann-Whitney U** | **t 检验** |'
- en: '| --- | --- | --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 85 vs. 83 | (*U*=1104.50000, *p*=0.15839) | (*t*=1.66543, *p*=0.09959) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 85 对比 83 | (*U*=1104.50000, *p*=0.15839) | (*t*=1.66543, *p*=0.09959) |'
- en: '| 85 vs. 82 | (*U*=997.00000, *p*=0.04058) | (*t*=2.40234, *p*=0.01852) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 85 对比 82 | (*U*=997.00000, *p*=0.04058) | (*t*=2.40234, *p*=0.01852) |'
- en: '| 85 vs. 81 | (*U*=883.50000, *p*=0.00575) | (*t*=3.13925, *p*=0.00234) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 85 对比 81 | (*U*=883.50000, *p*=0.00575) | (*t*=3.13925, *p*=0.00234) |'
- en: '[Table 4-2](ch04.xhtml#ch04tab02) should make sense to us. When the means are
    close, it’s harder to tell them apart, so we expect larger *p*-values. Recall
    how we have only 50 samples in each group. As the difference between the means
    increases, the *p*-values go down. A difference of three in the means leads to
    barely significant *p*-values. When the difference is larger still, the *p*-values
    become truly significant—again, as we expect.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4-2](ch04.xhtml#ch04tab02) 对我们来说应该是有意义的。当均值接近时，更难区分它们，因此我们期望更大的 *p*-值。回想一下，我们每组只有
    50 个样本。当均值差异增大时，*p*-值会降低。均值差异为三时，*p*-值几乎达不到显著性。当差异更大时，*p*-值变得真正显著——这也是我们预期的结果。'
- en: 'The analysis above begs the question: for a small difference in the means between
    the two groups, how do the *p*-values change as a function of the sample size?'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分析引出了一个问题：对于两组之间均值差异较小的情况，*p*-值如何随样本量的变化而变化？
- en: '[Figure 4-8](ch04.xhtml#ch04fig08) shows the *p*-value (mean ± standard error)
    over 25 runs for both the Mann-Whitney U test and the t-test as a function of
    sample size for the case where the means are 85 and 84.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-8](ch04.xhtml#ch04fig08)展示了25次运行中，Mann-Whitney U检验和t检验在不同样本大小下的*p*-值（均值
    ± 标准误差），以样本均值为85和84的情况为例。'
- en: '![image](Images/04fig08.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig08.jpg)'
- en: '*Figure 4-8: Mean p-value as a function of sample size for a difference in
    the sample means of one, ![image](Images/101equ01.jpg)*'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-8：均值p值与样本大小的关系，样本均值差异为1，![image](Images/101equ01.jpg)*'
- en: Small datasets make it difficult to differentiate between cases when the difference
    in the means is small. We also see that larger sample sizes reveal the difference,
    regardless of the test. It is interesting that in [Figure 4-8](ch04.xhtml#ch04fig08),
    the Mann-Whitney U *p*-value is less than that of the t-test even though the underlying
    data is normally distributed. Conventional wisdom states that it’s usually the
    other way around.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 小型数据集使得当均值差异较小时，很难区分不同的情况。我们还发现，较大的样本量能够揭示出差异，无论使用哪种测试。有趣的是，在[图4-8](ch04.xhtml#ch04fig08)中，尽管底层数据是正态分布的，Mann-Whitney
    U检验的*p*-值却低于t检验。这与传统观念相反，通常情况下情况应该是相反的。
- en: '[Figure 4-8](ch04.xhtml#ch04fig08) is an object lesson in the power of large-sample
    tests to detect real differences. When the sample size is large enough, a weak
    difference becomes significant. However, we need to balance this with the effect
    size. When we have 1,000 samples in each group, we have a statistically significant
    *p*-value, but we also have a Cohen’s *d* of about 0.13, signaling a weak effect.
    A large sample study might find a significant effect that is so weak as to be
    practically meaningless.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-8](ch04.xhtml#ch04fig08)展示了大样本测试在检测实际差异方面的强大威力。当样本量足够大时，即使是微弱的差异也能变得显著。然而，我们需要与效应大小相平衡。当每组有1,000个样本时，我们得到一个统计学上显著的*p*-值，但Cohen’s
    *d*约为0.13，表示效应很弱。一项大样本研究可能会发现一个显著的效应，但其效应如此微弱，以至于在实践中几乎没有意义。'
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter touched on the key aspects of statistics you’ll encounter during
    your sojourn through the world of deep learning. Specifically, we learned about
    different types of data and how to ensure the data is useful for building models.
    We then learned about summary statistics and saw examples that used them to help
    us understand a dataset. Understanding our data is key to successful deep learning.
    We investigated the different types of means, learned about measures of variation,
    and saw the utility of visualizing the data via box plots.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及了你在深入学习深度学习过程中会遇到的统计学关键方面。具体来说，我们学习了不同类型的数据，以及如何确保数据对建立模型有用。接着，我们学习了总结性统计量，并通过示例帮助我们理解数据集。理解我们的数据是成功进行深度学习的关键。我们探讨了不同类型的均值，学习了变异性度量，并看到了通过箱型图可视化数据的实用性。
- en: Missing data is a bane of deep learning. In this chapter, we investigated how
    to compensate for missing data. Next, we discussed correlation, how to detect
    and measure the relationships between elements of a dataset. Finally, we introduced
    hypothesis testing. Restricting ourselves to the most likely scenario we’ll encounter
    in deep learning, we learned how to apply both the t-test and the Mann-Whitney
    U test. Hypothesis testing introduced us to the *p*-value. We saw examples of
    it and discussed how to interpret it correctly.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据是深度学习的一个痛点。在本章中，我们探讨了如何弥补缺失数据。接下来，我们讨论了相关性，如何检测和衡量数据集元素之间的关系。最后，我们介绍了假设检验。我们将讨论限制为在深度学习中最可能遇到的情景，学习了如何应用t检验和Mann-Whitney
    U检验。假设检验让我们接触到了*p*-值。我们看了它的示例，并讨论了如何正确地解释它。
- en: In the next chapter we’ll leave statistics behind and dive headfirst into the
    world of linear algebra. Linear algebra is how we implement neural networks.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将告别统计学，深入探索线性代数的世界。线性代数是我们实现神经网络的基础。
