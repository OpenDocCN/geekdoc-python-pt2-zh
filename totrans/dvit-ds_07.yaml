- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Unsupervised Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: We’ll start this chapter with an introduction to the concept of unsupervised
    learning, comparing it to supervised learning. Then we’ll generate data for clustering,
    the most common task associated with unsupervised learning. We’ll first focus
    on a sophisticated method called E-M clustering. Finally, we’ll round out the
    chapter by looking at how other clustering methods relate to the rest of unsupervised
    learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从介绍无监督学习的概念开始，并将其与有监督学习进行比较。接着，我们将生成用于聚类的数据，聚类是与无监督学习最相关的任务。我们将首先聚焦于一种叫做E-M聚类的复杂方法。最后，我们将通过研究其他聚类方法与无监督学习其他部分的关系来完善本章内容。
- en: Unsupervised Learning vs. Supervised Learning
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习与有监督学习的对比
- en: The easiest way to understand unsupervised learning is by comparing it to supervised
    learning. Remember from Chapter 6 that the supervised learning process is captured
    by [Figure 7-1](#figure7-1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理解无监督学习最简单的方法是与有监督学习进行比较。记住在第6章中提到的，有监督学习过程在[图 7-1](#figure7-1)中有所展示。
- en: '![](image_fi/502888c07/f07001.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07001.png)'
- en: 'Figure 7-1: The conceptual map of all supervised learning methods'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-1：所有有监督学习方法的概念图
- en: The *target* that [Figure 7-1](#figure7-1) refers to is the special variable
    in our dataset that we want to predict. The *features* are the variables in our
    dataset that we’ll use to predict the target. The *learned function* is a function
    that maps the features to the target. We can check the accuracy of the learned
    function by comparing our predictions to actual target values. If the predictions
    are very far from the target values, we know that we should try to find a better
    learned function. It’s like the target values are supervising our process by telling
    us how accurate our function is and enabling us to push toward the highest possible
    accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-1](#figure7-1)所提到的*目标*是我们数据集中我们希望预测的特殊变量。*特征*是我们数据集中用于预测目标的变量。*学习到的函数*是将特征映射到目标的函数。我们可以通过将我们的预测与实际目标值进行比较来检查学习到的函数的准确性。如果预测值与目标值相差很大，我们就知道应该尝试找到更好的学习函数。就像目标值通过告诉我们函数的准确度来监督我们的过程，帮助我们朝着尽可能高的准确度迈进。'
- en: Unsupervised learning doesn’t have this supervision because it doesn’t have
    a target variable. [Figure 7-2](#figure7-2) depicts the process of unsupervised
    learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习没有监督，因为它没有目标变量。[图 7-2](#figure7-2)展示了无监督学习的过程。
- en: '![](image_fi/502888c07/f07002.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07002.png)'
- en: 'Figure 7-2: The conceptual map of the unsupervised learning process'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-2：无监督学习过程的概念图
- en: Instead of trying to map features to a target variable, unsupervised learning
    is concerned with creating a model of the features themselves; it does this by
    finding relationships between observations and natural groups in the features.
    In general, it’s a way of exploring the features. Finding relationships among
    observations in our data can help us understand them better; it will also help
    us find anomalies and make the dataset a little less unwieldy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习并不像有监督学习那样试图将特征映射到目标变量，而是关注于创建特征本身的模型；它通过发现观察值与特征中的自然群体之间的关系来实现这一点。一般来说，它是一种探索特征的方式。找出我们数据中观察值之间的关系可以帮助我们更好地理解数据，它还可以帮助我们发现异常情况，并使数据集变得不那么繁琐。
- en: The arrow in [Figure 7-2](#figure7-2) connects the features to themselves. This
    arrow indicates that we are finding ways that features relate to one another,
    such as the natural groups they form; it does not indicate a cycle or repeating
    process. This probably sounds rather abstract, so let’s make it clearer by looking
    at a concrete example.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-2](#figure7-2)中的箭头将特征与自身连接。这个箭头表示我们正在寻找特征之间的关系，例如它们自然形成的群体；它并不表示一个循环或重复过程。这可能听起来有些抽象，因此我们通过一个具体的例子来使其更加清晰。'
- en: Generating and Exploring Data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成和探索数据
- en: Let’s start by looking at some data. Instead of reading in existing data as
    we’ve done in previous chapters, we’ll generate new data by using Python’s random
    number generation capabilities. Randomly generated data tends to be simpler and
    easier to work with than data from real life; this will help us as we’re trying
    to discuss the complexities of unsupervised learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先来看一些数据。不同于以往章节中读取现有数据的方法，这次我们将通过使用Python的随机数生成能力来生成新的数据。随机生成的数据比现实生活中的数据更简单、更易于处理；这有助于我们讨论无监督学习的复杂性。
- en: More importantly, one of the main goals of unsupervised learning is to understand
    how subsets of data relate to one another. Generating data ourselves will mean
    that we can judge whether our unsupervised learning methods have found the right
    relationships among subsets of our data, since we’ll know exactly where those
    subsets came from and how they relate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，无监督学习的一个主要目标是理解数据子集之间的关系。通过我们自己生成数据，意味着我们可以判断我们的无监督学习方法是否发现了数据子集之间正确的关系，因为我们会准确知道这些子集来自哪里以及它们如何关联。
- en: Rolling the Dice
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掷骰子
- en: 'We’ll start by generating simple example data with some dice rolls:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从生成一些简单的示例数据开始，进行几次骰子掷骰：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this snippet, we import the `choices()` and `seed()` functions from the `random`
    module. These are the functions we’ll use to do random number generation. We define
    a variable called `numberofrolls`, which is storing the value `1800`, the number
    of simulated dice rolls we want Python to generate for us. We call the `seed()`
    function, which isn’t necessary but will ensure you get the same results as the
    ones presented here in the book.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码段中，我们从`random`模块导入了`choices()`和`seed()`函数。这些是我们用来生成随机数的函数。我们定义了一个名为`numberofrolls`的变量，它存储了值`1800`，即我们希望Python为我们生成的模拟掷骰子次数。我们调用了`seed()`函数，虽然这个步骤并非必须，但它会确保你得到和书中展示的相同的结果。
- en: 'Next, we create two lists, `dice1` and `dice2`, using the `choices()` function.
    We pass two arguments to this function: the list `[1,2,3,4,5,6]`, which tells
    the `choices()` function that we want it to make random selections from the integers
    between 1 and 6, and `k=numberofrolls`, which tells the `choices()` function that
    we want it to make 1,800 such selections. The `dice1` list represents 1,800 rolls
    of one die, and the `dice2` variable likewise represents 1,800 rolls of a second,
    separate die.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`choices()`函数创建两个列表，`dice1`和`dice2`。我们传递给该函数两个参数：列表`[1,2,3,4,5,6]`，告诉`choices()`函数我们希望从1到6的整数中随机选择，以及`k=numberofrolls`，告诉`choices()`函数我们希望它进行1,800次随机选择。`dice1`列表表示1,800次掷骰子的结果，而`dice2`变量同样表示第二个骰子的1,800次掷骰子结果。
- en: 'You can look at the first 10 elements of `dice1` as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以如下查看`dice1`的前10个元素：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see the following output (if you ran `seed(9)` in the preceding
    snippet):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下输出（如果你在前面的代码段中运行了`seed(9)`）：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This list looks plausible as a record of 10 rolls of a fair die. After generating
    lists of 1,800 random rolls from two dice, we can find the sums of each of the
    1,800 rolls:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表看起来像是公平骰子10次掷骰子的记录。在生成了两颗骰子的1,800次随机掷骰子结果后，我们可以找出这1,800次掷骰子的和：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here we create the `dicesum` variable by using a list comprehension. The first
    element of `dicesum` is the sum of the first element of `dice1` and the first
    element of `dice2`, the second element of `dicesum` is the sum of the second element
    of `dice1` and the second element of `dice2`, and so on. All of this code simulates
    a common scenario: rolling two dice together and looking at the sum of the numbers
    that are face up after rolling. But instead of rolling the dice ourselves, we
    have Python simulate all 1,800 rolls for us.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们通过列表推导式创建了`dicesum`变量。`dicesum`的第一个元素是`dice1`的第一个元素和`dice2`的第一个元素之和，`dicesum`的第二个元素是`dice1`的第二个元素和`dice2`的第二个元素之和，依此类推。所有这些代码模拟了一个常见的场景：一起掷两个骰子并查看掷出后的点数和。但不同的是，我们不是自己掷骰子，而是让Python为我们模拟所有的1,800次掷骰子。
- en: 'Once we have the summed dice rolls, we can draw a histogram of all of them:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了掷骰子的和，我们可以绘制所有结果的直方图：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Figure 7-3](#figure7-3) shows the result.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-3](#figure7-3)展示了结果。'
- en: '![](image_fi/502888c07/f07003.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07003.png)'
- en: 'Figure 7-3: The outcomes of 1,800 simulated dice rolls'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-3：1,800次模拟掷骰子的结果
- en: This is a histogram just like the ones we’ve seen in Chapters 1 and 3. Each
    vertical bar represents a frequency of a particular dice outcome. For example,
    the leftmost bar indicates that of our 1,800 rolls, about 50 summed to 2\. The
    tallest bar in the middle indicates that around 300 of our dice rolls summed to
    7.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个直方图，类似于我们在第1章和第3章中看到的那些。每个垂直条形表示某一特定骰子结果的频率。例如，最左边的条形表示，在1,800次掷骰子中，大约50次的结果和为2。中间最高的条形表示，大约300次的掷骰子的和为7。
- en: 'Histograms like this one show us the *distribution* of our data—the relative
    frequency of different observations occurring. Our distribution shows that the
    highest and lowest values, like 2 and 12, are relatively uncommon, while the middle
    values, like 7, are much more common. We can also interpret a distribution in
    terms of probabilities: if we roll two fair dice, 7 is a highly likely outcome,
    and 2 and 12 are not likely outcomes. We can know the approximate likelihood of
    each outcome by looking at the height of each bar of our histogram.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的直方图向我们展示了数据的*分布*——不同观察值出现的相对频率。我们的分布显示，像2和12这样的极值是相对不常见的，而像7这样的中间值则更为常见。我们还可以通过概率来解读分布：如果我们掷两个公平的骰子，7是一个非常可能的结果，而2和12的结果则不太可能。通过观察每个直方图条形的高度，我们可以知道每个结果的大致可能性。
- en: 'We can see that this histogram takes a shape that resembles a bell. The more
    times we roll our dice, the more bell-like our histogram will become. For large
    numbers of dice rolls, the histogram of outcomes is closely approximated by a
    special distribution called the *normal distribution*, or *Gaussian distribution*.
    You also met this distribution in Chapter 3, although in that chapter we called
    it by one of its other names: the bell curve. The normal distribution is a common
    pattern we observe when we measure the relative frequencies of certain things,
    like differences between means in Chapter 3, or sums of dice rolls here.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个直方图呈现出类似钟形的形状。我们掷骰子的次数越多，直方图就越接近钟形。对于大量的骰子掷骰次数，结果的直方图会被一种特殊的分布——*正态分布*（或称*高斯分布*）所近似。你在第三章中也遇到过这种分布，尽管在那一章中我们称其为它的另一个名字：钟形曲线。正态分布是我们在测量某些事物的相对频率时常见的模式，例如第三章中的均值差异，或者此处的骰子掷骰和结果的和。
- en: 'Every bell curve is fully described by just two numbers: a *mean*, describing
    the center and highest point of the bell curve, and a *variance*, describing how
    widely spread out the bell curve is. The square root of the variance is the *standard
    deviation*, another measure of how widely spread out a bell curve is. We can calculate
    the mean and standard deviation of our dice roll data with the following simple
    function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个钟形曲线可以通过两个数字来完全描述：一个是*均值*，表示钟形曲线的中心和最高点；另一个是*方差*，表示钟形曲线的扩展范围。方差的平方根就是*标准差*，它是钟形曲线扩展程度的另一个度量。我们可以通过以下简单的函数来计算骰子掷骰数据的均值和标准差：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This function takes a list of observations as its input. It uses the `np.mean()`
    function to get the mean of the list and store it in the variable called `center`.
    Then, it uses the `np.cov()` method. This method’s name, `cov`, is short for *covariance*,
    another measurement of the way data varies. When we calculate a covariance of
    two separate lists of observations, it tells us how much those datasets vary together.
    When we calculate a covariance of one list of observations alone, it’s simply
    called the variance, and the square root of the variance is the standard deviation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受一个观察值列表作为输入。它使用`np.mean()`函数计算该列表的均值，并将其存储在名为`center`的变量中。然后，它使用`np.cov()`方法。这个方法的名字`cov`是*协方差*（covariance）的缩写，协方差是衡量数据变化的一种方式。当我们计算两个不同观察值列表的协方差时，它告诉我们这两个数据集是如何一起变化的。而当我们计算单一观察值列表的协方差时，它就是方差，方差的平方根就是标准差。
- en: 'If we run the preceding snippet, we should get the mean and standard deviation
    of our dice rolls:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行之前的代码段，我们应该能够得到骰子掷骰的均值和标准差：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This output tells us that the mean of our observed dice rolls is about 7 and
    the standard deviation is about 2.5\. Now that we know these numbers, we can plot
    a bell curve as an overlay on our histogram as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出告诉我们，观察到的骰子掷骰的均值大约是7，标准差大约是2.5。现在我们知道了这些数字，我们可以将钟形曲线叠加在直方图上，像下面这样绘制：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 7-4](#figure7-4) shows our output.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-4](#figure7-4)显示了我们的输出。'
- en: '![](image_fi/502888c07/f07004.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07004.png)'
- en: 'Figure 7-4: A bell curve overlaid on a histogram of dice rolls'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-4：钟形曲线叠加在骰子掷骰的直方图上
- en: 'You can see that the bell curve is a continuous curve that we’ve plotted over
    our histogram. Its value represents relative probability: since it has a relatively
    high value at 7, and a relatively low value at 2 and 12, we interpret that to
    mean that we are more likely to roll a 7 than to roll a 2 or 12\. We can see that
    these theoretical probabilities match our observed dice rolls pretty closely,
    since the height of the bell curve is close to the height of each histogram bar.
    We can easily check the number of rolls predicted by the bell curve as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，钟形曲线是我们在直方图上绘制的连续曲线。它的值代表相对概率：由于它在7处的值较高，在2和12处的值较低，我们可以解读为掷出7的可能性比掷出2或12的可能性更大。我们可以看到，这些理论概率与我们观察到的骰子掷出的结果非常接近，因为钟形曲线的高度接近每个直方图条形的高度。我们可以轻松检查钟形曲线预测的掷骰次数，方法如下：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we use the `stats.norm.pdf()` function to calculate the expected number
    of dice rolls for 2, 7, and 12\. This function is from the `stats` module, and
    its name, `norm.pdf`, is short for *normal probability density function*, which
    is yet another name for our familiar bell curve. The snippet uses `stats.norm.pdf()`
    to calculate how high the bell curve is at *x* = 2, *x* = 7, and *x* = 12 (in
    other words, how likely rolling a 2, rolling a 7, and rolling a 12 are based on
    the mean and standard deviation we calculated before). Then, it multiplies these
    likelihoods by the number of times we want to roll the dice (1,800 in this case)
    to get the total number of expected rolls of 2, 7, and 12, respectively.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`stats.norm.pdf()`函数来计算2、7和12的预期掷骰次数。这个函数来自`stats`模块，函数名`norm.pdf`是*正态概率密度函数*的缩写，这也是我们熟悉的钟形曲线的另一种名称。这个代码片段使用`stats.norm.pdf()`来计算在*x*
    = 2、*x* = 7和*x* = 12时钟形曲线的高度（换句话说，就是根据我们之前计算的均值和标准差，掷出2、掷出7和掷出12的可能性）。然后，它将这些可能性乘以我们希望掷骰的次数（在本例中为1,800），以得到2、7和12的预期掷骰总次数。
- en: Using Another Kind of Die
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用另一种类型的骰子
- en: We’ve calculated probabilities for the hypothetical scenario of rolling two
    6-sided dice together because dice rolls give us an easy, familiar way to think
    about important data science ideas like probabilities and distributions. But of
    course this is not the only possible type of data we could analyze, or even the
    only type of dice roll we could analyze.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经计算了掷两个6面骰子的假设情境的概率，因为掷骰子为我们提供了一种简单、熟悉的方式来思考概率和分布等重要数据科学概念。但当然，这并不是我们可以分析的唯一数据类型，甚至不是我们可以分析的唯一类型的骰子掷骰。
- en: 'Imagine rolling a pair of nonstandard 12-sided dice, whose sides are marked
    with the numbers 4, 5, 6, . . . , 14, 15\. When these dice are rolled together,
    their sum could be any integer between 8 and 30\. We can randomly generate 1,800
    hypothetical rolls again and draw a histogram of those rolls by using the same
    type of code we used before, with a few small changes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，掷一对非标准的12面骰子，这些骰子面的标记为4、5、6、...、14、15。当这对骰子一起掷出时，它们的和可能是8到30之间的任何整数。我们可以再次随机生成1,800次假设掷骰，并通过使用之前相同类型的代码，稍作修改，绘制这些掷骰的直方图：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Figure 7-5](#figure7-5) shows the resulting histogram.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-5](#figure7-5) 显示了结果的直方图。'
- en: '![](image_fi/502888c07/f07005.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07005.png)'
- en: 'Figure 7-5: A bell curve and histogram for dice rolls using a pair of custom
    12-sided dice'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-5：使用一对定制的12面骰子掷骰结果的钟形曲线和直方图
- en: The bell shape is roughly the same as in [Figure 7-4](#figure7-4), but in this
    case, 19 is the most likely outcome, not 7, and the range goes from 8 to 30 instead
    of from 2 to 12\. So we have a normal distribution or bell curve again, but with
    a different mean and standard deviation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 锥形曲线大致与[图 7-4](#figure7-4)中的相同，但在这种情况下，19是最可能的结果，而不是7，范围从8到30，而不是从2到12。所以我们再次得到一个正态分布或钟形曲线，但具有不同的均值和标准差。
- en: 'We can plot both of our histograms (Figures 7-4 and 7-5) together as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将两个直方图（图 7-4和图 7-5）一起绘制，方法如下：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Figure 7-6](#figure7-6) shows the result.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-6](#figure7-6) 显示了结果。'
- en: '![](image_fi/502888c07/f07006.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07006.png)'
- en: 'Figure 7-6: A combined histogram showing outcomes from 6-sided and 12-sided
    dice pairs'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-6：显示6面骰子和12面骰子配对结果的合并直方图
- en: This is technically one histogram, though we know that it was generated by combining
    the data from two separate histograms. Remember that for the pair of 6-sided dice,
    7 is the most common outcome, and for the pair of 12-sided dice, 19 is the most
    common outcome. We see this reflected in a local peak at 7 and another local peak
    at 19 in our histogram. These two local peaks are called *modes*. Since we have
    two modes, this is what we call a *bimodal* histogram.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这在技术上是一个直方图，尽管我们知道它是通过结合两个独立直方图的数据生成的。记住，对于 6 面骰子对，7 是最常见的结果，而对于 12 面骰子对，19
    是最常见的结果。我们可以在直方图中看到，7 处有一个局部峰值，19 处有另一个局部峰值。这两个局部峰值称为 *模态*。由于我们有两个模态，这就是我们所说的
    *双峰* 直方图。
- en: When you look at [Figure 7-6](#figure7-6), it should help you start to understand
    what the conceptual diagram in [Figure 7-2](#figure7-2) is trying to illustrate.
    We’re not predicting or classifying dice rolls as we’ve done in previous chapters
    on supervised learning. Instead, we’re making simple theoretical models—in this
    case, our bell curves—that express our understanding of the data and the way observations
    relate to one another. In the next section, we’ll use these bell curve models
    to reason about the data and understand it better.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看[图 7-6](#figure7-6)时，它应该帮助你开始理解[图 7-2](#figure7-2)中的概念图试图说明的内容。我们并不像在前面的监督学习章节中那样对掷骰子结果进行预测或分类。相反，我们在构建简单的理论模型——在这个例子中，就是我们的钟形曲线——来表达我们对数据的理解以及观察结果之间的关系。接下来的一节中，我们将使用这些钟形曲线模型来推理数据，并更好地理解它。
- en: The Origin of Observations with Clustering
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类观察的起源
- en: 'Imagine that we randomly select one dice roll out of all the rolls that we
    plotted in [Figure 7-6](#figure7-6):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从所有绘制的投骰子数据中随机选择一次掷骰子结果，如[图 7-6](#figure7-6)所示：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should see the output `[12]`, indicating that we randomly selected one
    instance in our data in which we rolled a sum of 12\. Without giving you any other
    information, imagine that I ask you to make an educated guess about which pair
    of dice was responsible for rolling this particular 12\. It could have been either
    pair: the 6-sided die could have come up as a pair of 6s, or the 12-sided die
    could have come up in various combinations, like 8 and 4\. How can you make an
    educated guess about which pair of dice is most likely to be the origin of this
    observation?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到输出 `[12]`，表示我们随机选择了一个数据点，其中我们掷出的和为 12。在没有任何其他信息的情况下，假设我要求你做出一个有根据的猜测，哪一对骰子最可能导致这个结果
    12。可能是任何一对骰子：6 面骰子可能是两颗 6，或者 12 面骰子可能是其他组合，例如 8 和 4。那么，如何做出一个有根据的猜测，判断哪一对骰子最有可能是这个观察结果的来源呢？
- en: You may have strong intuition already that the 12 was not likely to have been
    rolled by the 6-sided dice. After all, 12 is the least likely roll for 6-sided
    dice (tied with 2), but 12 is closer to the middle of [Figure 7-5](#figure7-5),
    indicating that it’s a more common roll for the 12-sided dice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经有强烈的直觉，认为 12 不太可能是由 6 面骰子掷出的。毕竟，12 是 6 面骰子中最不可能出现的结果（与 2 一起并列），但 12 更接近[图
    7-5](#figure7-5)的中间位置，表明它是 12 面骰子更常见的结果。
- en: Your educated guess need not be based merely on intuition. We can look at the
    heights of the histogram bars in Figures 7-4 and 7-5 to see that when we rolled
    both pairs of dice 1,800 times, we got about 50 instances of 12s from the 6-sided
    dice and more than 60 instances of 12s from the 12-sided dice. And from a theoretical
    perspective, the heights of the bell curves in [Figure 7-6](#figure7-6) enable
    us to directly compare the relative probabilities of each outcome for each pair
    of dice, since we roll both pairs equally often.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你的有根据的猜测不需要仅仅依赖直觉。我们可以通过观察[图 7-4](#figure7-4)和[图 7-5](#figure7-5)中直方图条形的高度来看到，当我们掷骰子
    1,800 次时，6 面骰子得到 12 的次数大约为 50 次，而 12 面骰子得到 12 的次数超过 60 次。从理论角度来看，[图 7-6](#figure7-6)中的钟形曲线高度使我们能够直接比较每对骰子每个结果的相对概率，因为我们对每对骰子投掷的次数是相同的。
- en: 'We can do this same type of reasoning to think about dice rolls other than
    12\. For example, we know that 8 is more likely for the 6-sided dice, not only
    because of our intuition but also because the left bell curve in [Figure 7-6](#figure7-6)
    is higher than the right bell curve when the *x* value is 8\. In case we don’t
    have [Figure 7-6](#figure7-6) in front of us, we can calculate the heights of
    each bell curve as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用同样的推理方法来思考其他点数的掷骰结果。例如，我们知道 6 面骰子掷出 8 的概率更大，不仅仅是因为直觉，而且因为在[图 7-6](#figure7-6)中，左侧的钟形曲线在
    *x* 值为 8 时高于右侧的钟形曲线。如果我们面前没有[图 7-6](#figure7-6)，我们可以按照以下方法计算每个钟形曲线的高度：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here we see that the 6-sided dice are more likely to be the origin of the observed
    8 roll: it’s expected about 266 times out of 1,800 rolls of the 6-sided dice,
    while we expect to roll 8 only about 11 or 12 times out of 1,800 rolls of the
    12-sided dice. We can follow exactly the same process to determine that the 12-sided
    pair is more likely to be the origin of the observed 12 roll:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们看到，6面骰子更可能是观察到的8点投掷的来源：它在1,800次6面骰子的投掷中大约会出现266次，而我们预计在1,800次12面骰子的投掷中，8点只会出现大约11或12次。我们可以完全按照相同的过程来判断12面骰子对更可能是观察到的12点投掷的来源：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we use this method of comparing the heights of bell curves, then for any
    observed dice roll, we can say which dice pair is most likely to be its origin.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用这种比较钟形曲线高度的方法，那么对于任何观察到的骰子投掷，我们都可以判断出最可能是哪个骰子对的来源。
- en: 'Now that we can make educated guesses about the origin of any dice rolls, we’re
    ready to tackle *clustering*, one of the most important, most common tasks in
    unsupervised learning. The goal of clustering is to answer a global version of
    the question we considered previously: Which pair of dice is the origin of every
    observation in our data?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对任何骰子投掷的来源做出有根据的猜测，我们已经准备好处理*聚类*，这是无监督学习中最重要、最常见的任务之一。聚类的目标是回答我们之前考虑过的一个全局版本的问题：哪一对骰子是我们数据中每个观测值的来源？
- en: 'Clustering begins with a reasoning process that’s similar to the reasoning
    in our previous section. But instead of reasoning about a single dice roll, we
    try to determine which dice pair is the origin of every observation in our data.
    This is a simple process that we can go through as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类开始时的推理过程类似于我们上一节中的推理。但不同的是，这次我们不是推理单次骰子投掷，而是尝试确定哪一对骰子是我们数据中每个观测值的来源。这是一个简单的过程，我们可以按如下步骤进行：
- en: For all rolls of 2, dice pair 1’s bell curve is higher than dice pair 2’s, so,
    without knowing anything else, we suppose that all rolls of 2 came from dice pair
    1.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有2点的投掷，骰子对1的钟形曲线高于骰子对2的曲线，因此，在不考虑其他因素的情况下，我们假设所有2点的投掷来自骰子对1。
- en: For all rolls of 3, dice pair 1’s bell curve is higher than dice pair 2’s, so,
    without knowing anything else, we suppose that all rolls of 3 came from dice pair
    1.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有3点的投掷，骰子对1的钟形曲线高于骰子对2的曲线，因此，在不考虑其他因素的情况下，我们假设所有3点的投掷来自骰子对1。
- en: . . .
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . .
- en: For all rolls of 12, dice pair 2’s bell curve is higher than dice pair 1’s,
    so, without knowing anything else, we suppose that all rolls of 12 came from dice
    pair 2.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有12点的投掷，骰子对2的钟形曲线高于骰子对1的曲线，因此，在不考虑其他因素的情况下，我们假设所有12点的投掷来自骰子对2。
- en: . . .
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: . . .
- en: For all rolls of 30, dice pair 2’s bell curve is higher than dice pair 1’s,
    so, without knowing anything else, we suppose that all rolls of 30 came from dice
    pair 2.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有30点的投掷，骰子对2的钟形曲线高于骰子对1的曲线，因此，在不考虑其他因素的情况下，我们假设所有30点的投掷来自骰子对2。
- en: 'By considering each of the 29 possible dice roll outcomes individually, we
    can make good guesses about the respective origins of every observation in our
    data. We can also write code to accomplish this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分别考虑29种可能的骰子投掷结果，我们可以对每个观测值的来源做出较好的猜测。我们也可以编写代码来完成这个过程：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s look at the `classify()` function. It takes three arguments. The first
    argument it requires is `allpts`, which represents a list of every observation
    in our data. The other two arguments the function requires are `allmns` and `allvar`.
    These two arguments represent the means and variances, respectively, of every
    group (that is, every dice pair) in our data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下`classify()`函数。它需要三个参数。第一个参数是`allpts`，表示我们数据中每个观测值的列表。函数需要的另外两个参数是`allmns`和`allvar`。这两个参数分别表示我们数据中每组（即每对骰子）的均值和方差。
- en: The function needs to accomplish what we did visually when we looked at [Figure
    7-6](#figure7-6) to find which dice pairs were the origin of each roll. We consider
    the bell curves for each dice pair, and whichever bell curve has a higher value
    for a particular dice roll is assumed to be the dice pair it came from. In our
    function, instead of looking visually at bell curves, we need to calculate the
    values of bell curves and see which one is higher. This is why we create a list
    called `vars`. This list starts out empty, but then we append our bell curves
    to it with the `multivariate_normal()` function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数需要完成我们在查看[图 7-6](#figure7-6)时所做的工作，即找出每次投掷的骰子对的来源。我们考虑每个骰子对的钟形曲线，并假设对于某次特定的骰子投掷，具有更高值的钟形曲线就是它来自的骰子对。在我们的函数中，
    instead of visually looking at bell curves，我们需要计算钟形曲线的值并查看哪一个更高。这就是我们创建一个名为`vars`的列表的原因。这个列表最初为空，但我们随后使用`multivariate_normal()`函数将我们的钟形曲线添加到该列表中。
- en: After we have our collection of bell curves, we consider every point in our
    data. If the first bell curve is higher than the other bell curves at that point,
    we say that the point is associated with the first dice pair. If the second bell
    curve is the highest at that point, we say the point belongs to the second dice
    pair. If we have more than two bell curves, we can compare all of them, classifying
    every point according to which bell curve is highest. We find the highest bell
    curve the same way we did when we were looking at [Figure 7-6](#figure7-6) previously,
    but now we’re doing it with code instead of with our eyes. Every time we classify
    a point, we append its dice pair number to a list called `classification`. When
    the function finishes running, it has filled up the list with a dice pair classification
    for every point in our data, and it returns this as its final value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们收集了钟形曲线后，我们考虑数据中的每个点。如果在某个点，第一条钟形曲线比其他钟形曲线更高，我们就说这个点与第一个骰子对相关联。如果第二条钟形曲线在这个点最高，我们就说这个点属于第二个骰子对。如果我们有超过两个钟形曲线，我们可以比较所有钟形曲线，并根据哪个钟形曲线更高来分类每个点。我们找到最高的钟形曲线的方法与我们之前查看[图
    7-6](#figure7-6)时一样，只不过现在我们是通过代码而非眼睛来完成。每次我们分类一个点时，我们都会将它的骰子对编号附加到一个名为`classification`的列表中。当函数运行完成时，它将列表填充为我们数据中每个点的骰子对分类，并返回该列表作为最终值。
- en: 'Let’s try out our new `classify()` function. First, let’s define some points,
    means, and variances:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试我们新的`classify()`函数。首先，定义一些点、均值和方差：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our `allpoints` list is a collection of hypothetical dice rolls that we want
    to classify. Our `allmeans` list consists of two numbers: 7, the mean dice roll
    expected from our 6-sided dice pair, and 19, the mean dice roll expected from
    our 12-sided dice pair. Our `allvar` list consists of the respective variances
    of the two dice pairs. Now that we have the three required arguments, we can call
    our `classify()` function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`allpoints`列表是一个包含我们想要分类的假设骰子投掷结果的集合。我们的`allmeans`列表由两个数字组成：7，即我们期望从6面骰子对中得到的平均投掷结果；19，即我们期望从12面骰子对中得到的平均投掷结果。我们的`allvar`列表包含两个骰子对的相应方差。现在我们有了三个必要的参数，我们可以调用`classify()`函数：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We see the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到以下输出：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This list is telling us that the first two dice rolls in our `allpoints` list,
    2 and 8, are more likely to be associated with the 6-sided dice pair. The other
    dice rolls in our `allpoints` list—12, 15, and 25—are more likely to be associated
    with the 12-sided dice pair.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表告诉我们，`allpoints`列表中的前两个骰子投掷结果，2和8，更可能与6面骰子对相关联。`allpoints`列表中的其他骰子投掷结果——12、15和25——更可能与12面骰子对相关联。
- en: What we’ve just done is take a list of very different dice rolls and classify
    them into two distinct groups. You might want to call this classification or grouping,
    but in the world of machine learning, it’s called *clustering*. If you look at
    [Figure 7-6](#figure7-6), you can begin to see why. Dice rolls from the 6-sided
    dice appear to cluster around their most common value, 7, while dice rolls from
    the 12-sided dice appear to cluster around their most common value, 19\. They
    form little mountains of observations, or groups, that we’re going to call clusters
    regardless of their shape or size.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚做的事情是将一个包含非常不同的骰子投掷结果的列表进行分类，分为两个不同的组。你可以称之为分类或分组，但在机器学习的世界里，这叫做*聚类*。如果你查看[图
    7-6](#figure7-6)，你可以开始理解原因。来自6面骰子的投掷结果似乎聚集在它们最常见的值7周围，而来自12面骰子的投掷结果则聚集在它们最常见的值19周围。它们形成了小山脉般的观察结果或组，我们将称这些为聚类，无论它们的形状或大小如何。
- en: It’s common in practice for data to have this type of clustered structure, in
    which a small number of subsets (clusters) are apparent, with most observations
    in each subset appearing close to the subset’s mean, and only a small minority
    of observations between subsets or far away from the mean. By forming a conclusion
    about the clusters that exist in our data, and assigning each observation to one
    of our clusters, we’ve accomplished a simple version of clustering, the main task
    of this chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，数据通常具有这种类型的聚类结构，其中少数几个子集（聚类）是显而易见的，每个子集中的大多数观测值都接近该子集的均值，而只有少数观测值位于子集之间或远离均值。通过对我们数据中存在的聚类形成结论，并将每个观测值分配到一个聚类中，我们完成了本章的主要任务——聚类的简单版本。
- en: Clustering in Business Applications
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类在商业应用中的作用
- en: Dice rolls have probabilities that are easy to understand and reason about.
    But not many situations in business require being directly interested in dice
    rolls. Nevertheless, clustering is commonly used in business, especially by marketers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 骰子掷出的结果具有容易理解和推理的概率，但在商业中，并非很多情境都需要直接关注骰子掷出的结果。尽管如此，聚类在商业中被广泛使用，尤其是营销人员。
- en: Imagine that [Figure 7-6](#figure7-6) is not a record of dice rolls but rather
    a record of transaction sizes at a retail store you’re running. The lower cluster
    around 7 indicates that people in one group are spending about $7 at your store,
    and the higher cluster around 19 indicates that people in another group are spending
    around $19 at your store. You can say that you have a cluster of low-spending
    customers and a cluster of high-spending customers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设[图 7-6](#figure7-6)不是骰子掷出的记录，而是你所经营的零售店的交易金额记录。围绕7的低聚类表明，一组人群在你的商店消费大约7美元，而围绕19的高聚类则表明，另一组人群在你的商店消费大约19美元。你可以说，你有一群低消费客户和一群高消费客户。
- en: Now that you know that you have two distinct groups of customers and you know
    who they are, you can act on this information. For example, instead of using the
    same advertising strategy for all your customers, you may want to advertise or
    market differently to each group. Maybe advertisements emphasizing bargains and
    utility are persuasive to low spenders, while advertisements emphasizing premium
    quality and social prestige are more appealing to high spenders. Once you have
    a firm grasp of the boundary between your two groups of customers, the size of
    each group, and their most common spending habits, you have most of what you need
    to enact a sophisticated two-pronged advertisement strategy.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道你有两个截然不同的客户群体，并且知道他们是谁，你可以基于这一信息采取行动。例如，你可能不再对所有客户使用相同的广告策略，而是根据每个群体的不同进行有针对性的广告或营销。也许，强调优惠和实用性的广告对低消费群体有吸引力，而强调高端质量和社会声望的广告则更吸引高消费群体。一旦你清楚了解了这两个客户群体之间的边界、每个群体的大小以及他们最常见的消费习惯，你就有了制定复杂的双管齐下广告策略所需的主要信息。
- en: On the other hand, after discovering the clusters in your data, you might want
    to eliminate them rather than cater to them. For example, you may believe that
    your low-spending customers are not budget conscious but rather simply unaware
    of some of your more expensive, useful products. You may focus on more aggressive
    and informative advertising strictly for them, to encourage all of your customers
    to be in the high-spending group. Your exact approach will depend on many other
    details of your business, your products, and your strategy. A cluster analysis
    can give you important input to your strategic decisions by showing you the salient
    groups of customers and their characteristics, but it won’t go all the way to
    providing a clear business strategy from scratch.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在发现数据中的聚类之后，你可能希望去除这些聚类，而不是迎合它们。例如，你可能认为低消费的客户并非预算紧张，而只是对你的一些价格较高但有用的产品不够了解。你可能会专门为他们推出更具攻击性和信息性的广告，以鼓励所有客户都成为高消费群体。你的具体方法将取决于许多其他的商业细节、你的产品以及你的战略。聚类分析能够通过显示客户群体及其特征为你的战略决策提供重要的输入，但它无法从零开始提供清晰的商业策略。
- en: 'Instead of transaction size, we could imagine that the x-axis of the histogram
    in [Figure 7-6](#figure7-6) refers to another variable, like customer age. Then,
    our clustering analysis would be telling us that two distinct groups patronize
    our business: a younger group and an older group. You could do clustering on any
    numeric variable that you measure related to your customers and potentially find
    interesting customer groups.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设想，直方图中[图7-6](#figure7-6)的x轴表示另一个变量，比如客户年龄，而不是交易金额。这样，我们的聚类分析就会告诉我们，来光顾我们业务的有两个不同的群体：年轻群体和年长群体。你可以对任何你测量到的与客户相关的数值变量进行聚类分析，从而可能发现一些有趣的客户群体。
- en: Corporate marketers had been splitting customers into groups for many years
    before the term *data science* was common or even before most of today’s clustering
    methods were invented. Before the age of data science and clustering, marketers
    called the practice of splitting customers into groups *customer segmentation*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 企业营销人员在“*数据科学*”这个术语普及之前，甚至在今天的大部分聚类方法发明之前，就已经在将客户分组。数据科学和聚类时代之前，营销人员通常称这种将客户分组的做法为*客户细分*。
- en: In practice, marketers often perform segmentation in a nonscientific way, not
    by finding clusters and boundaries from data but by picking round numbers from
    guesses or intuition. For example, a television producer might commission surveys
    of viewers and analyze the data in a way that seems natural, by looking at results
    from all viewers younger than age 30, then looking at viewers age 30 and up separately.
    Using the nice, round number 30 provides a potentially natural-seeming boundary
    between younger and older viewers. However, maybe the producer’s show has scarcely
    any viewers above age 30, so analyzing responses from this group separately would
    be a distraction from the much larger group of viewers under 30\. A simple cluster
    analysis might instead reveal a large cluster of viewers around age 18 and a large
    cluster around age 28, with a boundary between these groups at age 23\. Analyzing
    segments based on this clustering analysis, rather than the round-sounding but
    ultimately misguided under-30 and over-30 segments, would be more useful to understand
    the show’s viewers and their opinions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，营销人员常常以非科学的方式进行细分，他们并不是通过从数据中发现聚类和边界，而是通过猜测或直觉选择一些圆整的数字。例如，一位电视制片人可能会委托调查观众，并以一种看似自然的方式分析数据，先查看所有30岁以下观众的结果，再单独查看30岁及以上观众的结果。使用这个看似自然的圆整数字30可能提供一个潜在的边界，将年轻观众和年长观众区分开。然而，也许该制片人的节目30岁以上的观众极为稀少，所以单独分析这一组观众的反馈会分散注意力，反而忽略了30岁以下更大规模的观众群体。相反，简单的聚类分析可能会揭示出18岁左右的观众和28岁左右的观众各自有一个大的群体，这两个群体之间的边界在23岁。基于这个聚类分析来分析细分群体，而不是基于看似合理但最终误导性的“30岁以下”和“30岁以上”细分，将更有助于理解节目的观众及其观点。
- en: Segmentation predates clustering, but clustering is a great way to do segmentation
    because it enables us to find more accurate and useful segments, and precise boundaries
    between them. In this case, you could say that the clustering approach is giving
    us objective, data-driven insights, as compared to the intuition-based or experience-based
    approach associated with round-number segmentation. Improving from intuition to
    objective, data-driven insights is one of the main contributions of data science
    to business.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 细分早于聚类，但聚类是一种很好的细分方法，因为它使我们能够找到更准确、更有用的细分群体，并且能精确地划定它们之间的边界。在这种情况下，你可以说，聚类方法为我们提供了基于数据的客观洞察，而与之相对的是基于直觉或经验的圆整数字细分。由直觉转向客观、数据驱动的洞察，正是数据科学为商业带来的主要贡献之一。
- en: 'So far, we’ve discussed segmentation on only one variable at a time: dice rolls,
    spending, or age analyzed individually. Instead of clustering and segmenting on
    only one variable at a time, we can start thinking in multiple dimensions. For
    example, if we’re running a retail company in the United States, we might find
    a cluster of young, high spenders in the west; a group of older, low spenders
    in the southeast; and a cluster of middle-aged, moderately high spenders in the
    north. To discover this, we would have to perform clustering in multiple dimensions
    at once. Data science clustering methods have this capability, giving them another
    advantage over traditional segmentation methods.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅讨论了单个变量的细分：单独分析掷骰结果、消费或年龄等。我们可以开始思考多个维度，而不仅仅是在单个变量上进行聚类和细分。例如，如果我们在美国经营零售公司，我们可能会发现，在西部有一群年轻的高消费人群；在东南部有一群年长的低消费人群；在北部有一群中年、适度高消费的人群。为了发现这一点，我们需要在多个维度上同时进行聚类。数据科学中的聚类方法具备这种能力，给它们提供了相较于传统细分方法的另一优势。
- en: Analyzing Multiple Dimensions
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析多个维度
- en: 'In our dice roll data, each observation consists of just one number: the sum
    of the face-up numbers on the dice we rolled. We don’t record the temperature
    or color of the dice, the length or width of their edges, or anything else except
    for exactly one raw number per roll. Our dice roll dataset is *one-dimensional*.
    Here a *dimension* doesn’t necessarily refer to a dimension in space but rather
    to any measurement that can vary between low and high. Dice rolls can vary a great
    deal between a low roll like 2 and a high roll like 12 (or more, depending on
    the dice we’re using), but we measure only their highs and lows on one metric:
    the sum of the numbers that are face up after we roll them.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的掷骰数据中，每个观察值仅由一个数字组成：我们掷出的骰子正面朝上的数字之和。我们不记录骰子的温度或颜色，也不记录骰子的边长或宽度，或者除了每次掷骰的一个原始数字外的任何其他信息。我们的掷骰数据集是*一维*的。在这里，*维度*不一定指空间中的维度，而是指任何可以在低值和高值之间变化的测量。掷骰的结果可以有很大差异，从像2这样的低点到像12（或更多，取决于我们使用的骰子）这样的高点，但我们只测量它们在一个度量标准上的高低：掷出骰子后正面朝上的数字之和。
- en: In business scenarios, we’re almost always interested in more than one dimension.
    When we’re analyzing customer clusters, for example, we want to know customers’
    ages, locations, incomes, genders, years of education, and as much more as we
    can so we can successfully market to them. When we’re working with many dimensions,
    some things will look different. For example, the bell curves we’ve seen in Figures
    7-3 through 7-6 will gain an extra dimension, as in the right side of [Figure
    7-7](#figure7-7).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业场景中，我们几乎总是关心多个维度。例如，在分析客户群体时，我们想要了解客户的年龄、位置、收入、性别、教育年限等尽可能多的信息，以便能够成功地进行市场营销。当我们处理多个维度时，一些事物会看起来不同。例如，我们在图7-3到7-6中看到的钟形曲线将增加一个额外的维度，就像在[图7-7](#figure7-7)右侧所示的那样。
- en: '![](image_fi/502888c07/f07007.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07007.png)'
- en: 'Figure 7-7: A univariate bell curve (left) and a bivariate bell curve (right)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-7：单变量钟形曲线（左）和双变量钟形曲线（右）
- en: 'The left side of this diagram shows a *univariate bell curve*, called *univariate*
    because it shows relative probabilities for only one variable (the x-axis). The
    right side shows a *bivariate bell curve*, one that shows relative probabilities
    varying along two dimensions: the x- and y-axes. We can imagine that the x- and
    y-axes in the plot on the right side of [Figure 7-7](#figure7-7) could be age
    and average transaction size, for example.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该图左侧显示的是*单变量钟形曲线*，之所以叫*单变量*，是因为它仅展示一个变量（x轴）的相对概率。右侧显示的是*双变量钟形曲线*，它展示的是沿两个维度变化的相对概率：x轴和y轴。我们可以想象，在[图7-7](#figure7-7)右侧的图中，x轴和y轴可能分别表示年龄和平均交易金额。
- en: 'A univariate Gaussian curve has a mean that’s represented by just one number,
    like *x* = 0 in the left side of [Figure 7-7](#figure7-7). A bivariate Gaussian
    curve has a mean that’s represented by two numbers: an ordered pair consisting
    of an x-coordinate and a y-coordinate, like (0, 0). The number of dimensions increases,
    but the idea of using the mean of each dimension to find the highest point of
    the bell is the same. Finding the means of each dimension will tell us where to
    find the center and highest point of the bell, around which the other observations
    tend to cluster. In both the univariate and bivariate cases, we can interpret
    the height of the bell curve as a probability: points where the bell curve is
    higher correspond to observations that are more likely.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量高斯曲线的均值由一个数字表示，例如在[图 7-7](#figure7-7)的左侧，*x* = 0。双变量高斯曲线的均值由两个数字表示：由一个 x
    坐标和一个 y 坐标组成的有序对，如 (0, 0)。维度的数量增加，但使用每个维度的均值来找到钟形曲线的最高点的思路是相同的。找到每个维度的均值将告诉我们钟形曲线的中心和最高点的位置，其他观察值通常会围绕这一点聚集。在单变量和双变量的情况下，我们可以将钟形曲线的高度解释为概率：钟形曲线较高的点对应于更可能的观察值。
- en: Going from one to two dimensions also affects the way we express how spread
    out our bell curve is. In one dimension, we use the variance (or standard deviation)
    as a single number that expresses the degree of spread of our curve. In two or
    more dimensions, we use a matrix, or a rectangular array of numbers, to express
    the degree of the bell curve’s spread. The matrix we use, called a *covariance
    matrix*, records not only how spread out each dimension is on its own but also
    the extent to which different dimensions vary together. We don’t need to worry
    about the details of the covariance matrix; we mostly just need to calculate it
    with the `np.cov()` function and use it as an input in our clustering methods.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从一维到二维的转变还会影响我们表达钟形曲线分布范围的方式。在一维中，我们使用方差（或标准差）作为一个数字，表示曲线的分布程度。在二维或更高维度中，我们使用矩阵，或一组数字的矩形数组，来表示钟形曲线的分布范围。我们使用的矩阵称为*协方差矩阵*，它记录了每个维度的分布程度，以及不同维度之间的共同变化程度。我们不需要关心协方差矩阵的细节；我们主要只需要通过`np.cov()`函数来计算它，并将其作为输入应用于我们的聚类方法。
- en: 'When you increase the number of dimensions in your clustering analysis from
    two to three or more, the adjustment is straightforward. Instead of a univariate
    or bivariate bell curve, we’ll have a *multivariate bell curve*. In three dimensions,
    a mean will have three coordinates; in *n* dimensions, it will have *n* coordinates.
    The covariance matrix will also get bigger every time you increase the dimension
    of your problem. But no matter how many dimensions you have, the features of the
    bell curve are always the same: it has a mean, which most observations are near,
    and it has a measure of covariance, which shows how spread out the bell curve
    is.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在聚类分析中将维度从二维增加到三维或更多时，这一调整是直接的。我们将不再使用单变量或双变量的钟形曲线，而是使用*多变量钟形曲线*。在三维空间中，均值将有三个坐标；在*n*维空间中，均值将有*n*个坐标。每次增加问题的维度时，协方差矩阵也会变得更大。但是，无论你有多少维度，钟形曲线的特征始终相同：它有一个均值，大多数观察值会接近该均值，并且有一个协方差度量，显示钟形曲线的分布范围。
- en: In the rest of the chapter, we’ll look at a two-dimensional example, which will
    show the idea and process of clustering while still enabling us to draw simple,
    interpretable plots. This example will show all the essential features of clustering
    and unsupervised learning that you can apply in any number of dimensions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将看一个二维示例，该示例将展示聚类的概念和过程，同时仍能让我们绘制出简单、易于理解的图形。这个示例将展示聚类和无监督学习的所有基本特征，你可以将其应用于任意维度。
- en: E-M Clustering
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E-M 聚类
- en: We now have all the ingredients required to perform *E-M clustering*, a powerful
    unsupervised learning approach that enables us to intelligently find natural groups
    in multidimensional data. This technique is also called *Gaussian mixture modeling*,
    because it uses bell curves (Gaussian distributions) to model how groups mix together.
    Whatever you call it, it’s useful and relatively straightforward.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了执行*E-M 聚类*所需的所有要素，这是一种强大的无监督学习方法，可以帮助我们智能地发现多维数据中的自然分组。这种技术也被称为*高斯混合建模*，因为它使用钟形曲线（高斯分布）来建模群体是如何相互融合的。无论你怎么称呼它，它都非常有用且相对直接。
- en: 'We’ll start by looking at new two-dimensional data that we want to perform
    clustering on. We can read the data from its online home as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看我们想要进行聚类的新二维数据开始。我们可以按照以下方式从它的在线位置读取数据：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This snippet uses two modules: `ast` and `requests`. The `requests` package
    allows Python to request a file or dataset from a website—in this case, the website
    where the clustering data lives. The data is stored in a file as a Python list.
    Python reads *.txt* files as strings by default, but we want to read the data
    into a Python list instead of a string. The `ast` module contains a `literal_eval()`
    method that enables us to read list data from files that would otherwise be treated
    as strings. We read our list into a variable called `allpoints`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段使用了两个模块：`ast`和`requests`。`requests`包允许Python从网站请求文件或数据集——在这个例子中，网站是存放聚类数据的地方。数据以Python列表的形式存储在文件中。Python默认将*.txt*文件读取为字符串，但我们希望将数据读取为Python列表，而不是字符串。`ast`模块包含一个`literal_eval()`方法，使我们能够读取文件中的列表数据，否则它们会被当作字符串处理。我们将列表读取到一个名为`allpoints`的变量中。
- en: 'Now that we’ve read the data into Python, we can plot it to see what it looks
    like:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据读取到Python中，可以绘制图形来看数据长什么样子：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 7-8](#figure7-8) shows the results.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-8](#figure7-8)显示了结果。'
- en: '![](image_fi/502888c07/f07008.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07008.png)'
- en: 'Figure 7-8: A plot of our new two-dimensional data'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-8：我们新的二维数据的图示
- en: 'One thing you might notice is that these axes have no labels. This is not an
    accident: we’re going to work with this data as an unlabeled example and then
    talk about how it can be applied to many scenarios. You could imagine many possible
    labels for the axes in this example: maybe the points represent cities, the x-axis
    is percent population growth, and the y-axis is percent economic growth. If so,
    performing clustering will identify clusters of cities whose growth has been comparable
    recently. Maybe this could be useful if you’re a CEO and you’re trying to decide
    where to open a new franchise of your business. But the axes don’t have to represent
    the cities’ growth: they could represent anything at all, and our clustering algorithms
    will work in the same way regardless.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到这些轴没有标签。这不是偶然的：我们将把这些数据作为一个无标签的示例来处理，然后讨论它如何应用于多种场景。你可以想象在这个示例中，轴可能有许多不同的标签：也许这些点代表城市，x轴是人口增长百分比，y轴是经济增长百分比。如果是这样，执行聚类将会识别出那些增长趋势最近相似的城市聚类。也许如果你是一个CEO，正在考虑在哪里开设新分店，这个信息会很有用。但这些轴不一定代表城市的增长：它们可以代表任何东西，而我们的聚类算法将以相同的方式工作。
- en: A few other things are immediately apparent in [Figure 7-8](#figure7-8). Two
    particularly dense clusters of observations appear at the top and right of the
    plot, respectively. In the center of the plot, another cluster appears to be much
    less dense than the other two. We seem to have three clusters in different locations,
    with different sizes and densities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 7-8](#figure7-8)中，一些其他的现象也立即显现出来。两个特别密集的观测聚类分别出现在图的顶部和右侧。在图的中心，另一个聚类似乎比其他两个更为稀疏。我们似乎有三个位置不同、大小和密度不同的聚类。
- en: 'Instead of relying only on our eyes for this clustering exercise, let’s use
    a powerful clustering algorithm: the E-M algorithm. *E-M* is short for *expectation-maximization*.
    We can describe this algorithm in four steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与其仅仅依靠肉眼进行聚类练习，我们不如使用一个强大的聚类算法：E-M算法。*E-M*是*期望最大化*（expectation-maximization）的缩写。我们可以用四个步骤来描述这个算法：
- en: 'Guessing: Make guesses for the means and covariances of every cluster.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猜测：为每个聚类的均值和协方差做出猜测。
- en: 'Expectation: Classify every observation in our data according to which cluster
    it’s most likely to be a member of, according to the most recent estimates of
    means and covariances. (This is called the *E*, or *Expectation*, step because
    we’re classifying based on our expectation of how likely each point is to be in
    each cluster.)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 期望：根据最新的均值和协方差估计，按每个数据点最可能属于哪个聚类来对数据进行分类。（这被称为*E*，或*期望*，步骤，因为我们是根据每个点属于各个聚类的可能性进行分类。）
- en: 'Maximization: Use the classifications obtained in the Expectation step to calculate
    new estimates for the means and covariances of each cluster. (This is called the
    *M*, or *Maximization*, step because we find the means and variances that maximize
    the probability of matching our data.)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大化：利用期望步骤获得的分类结果来计算每个聚类的均值和协方差的新估计值。（这被称为*M*，或*最大化*，步骤，因为我们找到的均值和方差最大化了数据匹配的概率。）
- en: 'Convergence: Repeat the Expectation and Maximization steps until reaching a
    stopping condition.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛：重复执行期望步骤和最大化步骤，直到达到停止条件。
- en: If this algorithm seems intimidating, don’t worry; you’ve already done all the
    hard parts earlier in the chapter. Let’s proceed through each step in turn to
    understand them better.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个算法看起来让你感到害怕，不要担心；你已经在本章的早期部分做了所有的困难部分。让我们依次通过每个步骤，以更好地理解它们。
- en: The Guessing Step
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 猜测步骤
- en: 'The first step is the easiest, since we can make any guess whatsoever for the
    means and covariances of our clusters. Let’s make some initial guesses:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是最简单的，因为我们可以对聚类的均值和协方差做出任何猜测。让我们做一些初步猜测：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this snippet, we first make guesses for `mean1`, `mean2`, and `mean3`. These
    guesses are two-dimensional points that are supposed to be the respective centers
    of our three clusters. We then make guesses for the covariance of each cluster.
    We make the easiest-possible guess for covariance: we guess a special, simple
    matrix called the *identity matrix* as the covariance matrix for each cluster.
    (The details of the identity matrix aren’t important right now; we use it because
    it’s simple and tends to work well enough as an initial guess.) We can draw a
    plot to see what these guesses look like:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先对`mean1`、`mean2`和`mean3`进行猜测。这些猜测是二维点，应该是我们三个聚类的各自中心。然后，我们对每个聚类的协方差做出猜测。我们对协方差做出最简单的猜测：我们猜测一个特别简单的矩阵——*单位矩阵*作为每个聚类的协方差矩阵。（单位矩阵的细节现在不重要；我们之所以使用它，是因为它简单，并且作为初步猜测时往往足够有效。）我们可以绘制一个图来看看这些猜测是什么样的：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The plot looks like [Figure 7-9](#figure7-9).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图表类似于[图 7-9](#figure7-9)。
- en: '![](image_fi/502888c07/f07009.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07009.png)'
- en: 'Figure 7-9: Our data with some guesses at cluster centers shown as stars'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-9：我们的数据以及一些作为星星显示的聚类中心猜测
- en: 'You can see the points plotted again, with stars representing the guesses we
    made for cluster centers. (The stars will be red if you’re plotting at home.)
    Our guesses clearly were not very good. In particular, none of our guesses are
    in the center of the two dense clusters we see at the top and right of the plot,
    and none are close to the center of the main cloud of points. In this case, starting
    with inaccurate guesses is good, because it will enable us to see how powerful
    the E-M clustering algorithm is: it can find the right cluster centers even if
    our initial guesses in the Guessing step are quite poor.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以再次看到绘制的点，星星代表我们对聚类中心的猜测。（如果你在家里绘制，星星会是红色的。）显然，我们的猜测并不是很好。特别是，我们的猜测没有一个位于图表顶部和右侧的两个密集聚类的中心，也没有一个接近主要点云的中心。在这种情况下，从不准确的猜测开始是好的，因为它能让我们看到期望最大化（E-M）聚类算法的强大：即使我们的初步猜测在猜测步骤中相当糟糕，它依然能够找到正确的聚类中心。
- en: The Expectation Step
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 期望步骤
- en: 'We’ve completed the Guessing step of the algorithm. For the next step, we need
    to classify all of our points according to which cluster we believe they’re in.
    Luckily, we already have our `classify()` function for this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了算法的猜测步骤。在下一步中，我们需要根据我们认为每个点所属的聚类，对所有点进行分类。幸运的是，我们已经有了`classify()`函数来处理这个：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Remember what this function does. Earlier in the chapter, we used it to classify
    dice rolls. We took a set of dice roll observations and found which dice pair
    each dice roll was likely to come from by comparing the heights of two bell curves.
    Here, we will use the function for a similar task, but we’ll use our new unlabeled
    data instead of dice roll data. For each observation in our new data, this function
    finds which group it’s likely to belong to by comparing the heights of the bell
    curve associated with each group. Let’s call this function on our points, means,
    and variances:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个函数的作用。早些时候，我们使用它对骰子投掷结果进行分类。我们收集了一组骰子投掷的观察结果，通过比较两条钟形曲线的高度，找出每个骰子投掷结果最可能来自哪一对骰子。在这里，我们将使用这个函数来做一个类似的任务，但我们将使用新的未标记数据，而不是骰子投掷数据。对于我们新数据中的每个观察结果，这个函数通过比较与每个组相关联的钟形曲线的高度，来找出它最可能属于哪个组。我们将对我们的点、均值和方差调用这个函数：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we have a list called `theclass`, which contains a classification of every
    point in our data. We can look at the first 10 elements of `theclass` by running
    `print(theclass[:10])`. We see the following output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个名为`theclass`的列表，其中包含了我们数据中每个点的分类。我们可以通过运行`print(theclass[:10])`来查看`theclass`的前10个元素。我们看到以下输出：
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This output is telling us that the first point in our data seems to be in cluster
    1, the fifth point is in cluster 3, and so on. We’ve accomplished the Guessing
    step and the Expectation step: we have some values for means and variances of
    our clusters, and we’ve classified every data point into one of our clusters.
    Before we move on, let’s create a function that will plot our data and clusters:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出告诉我们，我们数据中的第一个点似乎在第1类，第五个点在第3类，以此类推。我们已经完成了猜测步骤和期望步骤：我们有了每个簇的均值和方差，并且我们已经将每个数据点归类到了其中一个簇中。在继续之前，让我们创建一个函数来绘制我们的数据和簇：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This function takes our data (`allpoints`), our cluster classifications (`theclass`),
    and our cluster means (`allmeans`) as inputs. Then it assigns colors to each cluster:
    points in the first cluster are black, points in the second cluster are green,
    and points in the third cluster are yellow. The `plt.scatter()` function draws
    all our points in their colors. Finally, it draws red stars for each of our cluster
    centers. Note this book is printed in black-and-white, so you will see these colors
    only if you try this code on your own computer.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将我们的数据（`allpoints`）、簇分类（`theclass`）和簇均值（`allmeans`）作为输入。然后它为每个簇分配颜色：第一个簇的点为黑色，第二个簇的点为绿色，第三个簇的点为黄色。`plt.scatter()`函数将所有的点绘制成相应的颜色。最后，它用红色星形标记绘制每个簇中心。请注意，本书是黑白印刷的，所以只有在你在自己的电脑上运行这段代码时，才能看到这些颜色。
- en: We can call this function by running `makeplot(allpoints,theclass,allmeans)`,
    and we should see [Figure 7-10](#figure7-10).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行`makeplot(allpoints,theclass,allmeans)`来调用这个函数，我们应该能看到[图7-10](#figure7-10)。
- en: '![](image_fi/502888c07/f07010.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07010.png)'
- en: 'Figure 7-10: Initial cluster classifications'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-10：初始簇分类
- en: This is a two-dimensional plot. But to understand how it performed classification
    into clusters, you should imagine three bivariate bell curves (like the one on
    the right side of [Figure 7-7](#figure7-7)) jutting out of the page, each one
    centered on one of the star-shaped cluster centers. The covariance we’ve estimated
    will determine how widely spread out each bell curve is. The cluster classifications
    are determined by which of these three bell curves is highest for each point in
    our data. You can imagine that if we moved the centers or changed the covariance
    estimates, our bivariate bell curves would look different, and we could get different
    classifications. (This will happen very soon.)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二维图。但是，为了理解它是如何将数据分类到簇中的，你可以想象有三个双变量的钟形曲线（就像在[图7-7](#figure7-7)右侧的那个），它们从页面中凸出，每个曲线的中心位于一个星形的簇中心上。我们估算的协方差将决定每个钟形曲线的分布范围。簇的分类是根据每个数据点在哪个钟形曲线最高来确定的。你可以想象，如果我们移动了中心点或改变了协方差估计值，我们的双变量钟形曲线会有所不同，分类结果也可能会变化。（这一点很快就会发生。）
- en: It’s clear from [Figure 7-10](#figure7-10) that we haven’t finished our clustering
    task. For one thing, the cluster shapes don’t match the shapes we think we see
    in [Figure 7-8](#figure7-8). But even more obviously, the points that we’ve called
    *cluster centers*, shown as stars on this plot, are clearly not in the center
    of their respective clusters; they’re more or less on the edge of their data.
    This is why we need to do the Maximization step of the E-M clustering algorithm,
    in which we’ll recalculate the means and variances of each cluster (and thereby
    move cluster centers to more appropriate locations).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图7-10](#figure7-10)可以清楚地看到，我们的聚类任务还没有完成。首先，簇的形状与我们在[图7-8](#figure7-8)中看到的形状不匹配。更明显的是，我们称为*簇中心*的点，在这个图中显示为星形标记，显然并不位于它们各自簇的中心位置；它们更像是位于数据的边缘。这就是我们需要执行E-M聚类算法中的最大化步骤的原因，在该步骤中，我们将重新计算每个簇的均值和方差（从而将簇中心移动到更合适的位置）。
- en: The Maximization Step
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大化步骤
- en: 'This step is pretty simple: we just need to take the points in each of our
    clusters and calculate their means and variances. We can update the `getcenters()`
    function we used previously to accomplish this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤很简单：我们只需要拿出每个簇中的点，计算它们的均值和方差。我们可以更新之前使用的`getcenters()`函数来完成这一任务：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our updated `getcenters()` function is simple. We pass a number `k` to the function
    as an argument; this number indicates the number of clusters in our data. We also
    pass the data and the cluster classifications to the function. The function calculates
    the mean and variance of every cluster and then returns a list of means (which
    we call `centers`) and a list of variances (which we call `thevars`).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新后的`getcenters()`函数很简单。我们将一个数字`k`作为参数传递给该函数；这个数字表示我们数据中的聚类数量。我们还将数据和聚类分类传递给该函数。函数计算每个聚类的均值和方差，然后返回一个均值列表（我们称之为`centers`）和一个方差列表（我们称之为`thevars`）。
- en: 'Let’s call our updated `getcenters()` function to find the actual means and
    variances of our three clusters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用更新后的`getcenters()`函数，找到我们三个聚类的实际均值和方差：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now that we have newly calculated means and variances, let’s plot our clusters
    again by running `makeplot(allpoints,theclass,allmeans)`. The result should look
    like [Figure 7-11](#figure7-11).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经重新计算了均值和方差，让我们通过运行`makeplot(allpoints,theclass,allmeans)`再次绘制聚类图。结果应该类似于[图7-11](#figure7-11)。
- en: '![](image_fi/502888c07/f07011.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07011.png)'
- en: 'Figure 7-11: Recalculated cluster centers'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-11：重新计算的聚类中心
- en: 'You can see that our cluster centers, the star shapes, have moved since we
    recalculated them in the Expectation step. But now that the cluster centers have
    moved, some of our previous cluster classifications are probably incorrect. If
    you run this on your computer, you’ll see some yellow points that are quite far
    from the center of the yellow cluster (the cluster at the top right of the plot),
    and quite close to the centers of the other clusters. Since the centers have moved
    and we’ve recalculated covariances, we need to rerun our classification function
    to reclassify all the points into their correct clusters (meaning, we need to
    run our Expectation step again):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们的聚类中心（星形）在重新计算后已经移动。由于聚类中心已移动，我们之前的一些聚类分类可能已经不正确。如果你在你的电脑上运行这个，你会看到一些黄色点，它们离黄色聚类的中心（图中的右上角聚类）相当远，而离其他聚类的中心则相当近。由于聚类中心已移动，并且我们重新计算了协方差，我们需要重新运行分类函数，将所有点重新分类到正确的聚类中（这意味着，我们需要再次运行期望步骤）：
- en: '[PRE28]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Again, to think about how this classification is accomplished, you can imagine
    three bivariate bell curves jutting out of the page, with bell curve centers determined
    by the positions of the stars, and widths determined by the bell curve covariances
    we’ve calculated. Whichever bell curve is highest at each point will determine
    that point’s cluster classification.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了理解这个分类是如何完成的，你可以想象三个双变量的钟形曲线从页面上突出出来，钟形曲线的中心由星形的位置决定，宽度由我们计算出的钟形曲线协方差决定。在每个点上，最高的钟形曲线将决定该点的聚类分类。
- en: Let’s make another plot that will reflect these newly recalculated cluster classifications
    by running `makeplot(allpoints,theclass,allmeans)` yet again. The result is [Figure
    7-12](#figure7-12).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再绘制一个图，反映这些新重新计算的聚类分类，通过再次运行`makeplot(allpoints,theclass,allmeans)`。结果是[图7-12](#figure7-12)。
- en: '![](image_fi/502888c07/f07012.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07012.png)'
- en: 'Figure 7-12: Reclassified cluster classifications'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-12：重新分类的聚类分类
- en: 'Here, you can see that the stars (cluster centers) are in the same locations
    as in [Figure 7-11](#figure7-11). But we’ve accomplished reclassification of the
    points: based on comparing the bell curves for each cluster, we’ve found the cluster
    that’s most likely to contain every point, and we’ve changed the coloring accordingly.
    You can compare this to [Figure 7-10](#figure7-10) to see the progress we’ve made
    since we started: we’ve changed our estimates of where the cluster centers are
    as well as our estimates of which points belong in which clusters.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到星形（聚类中心）的位置与[图7-11](#figure7-11)中的相同。但我们已经完成了点的重新分类：通过比较每个聚类的钟形曲线，我们找到了最有可能包含每个点的聚类，并据此改变了颜色。你可以将此与[图7-10](#figure7-10)进行对比，看看自我们开始以来取得的进展：我们已经改变了对聚类中心位置的估计，也改变了对每个点属于哪个聚类的估计。
- en: The Convergence Step
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收敛步骤
- en: 'You can see that two clusters have grown (the lower cluster and the cluster
    on the left), while one cluster has shrunk (the cluster at the top right). But
    now we’re in the same situation we were in before: after reclassifying the clusters,
    the centers are not correct, so we need to recalculate them too.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，两个聚类已经变大（下方的聚类和左侧的聚类），而一个聚类变小了（右上角的聚类）。但现在我们又回到了之前的情况：在重新分类聚类后，中心位置不再正确，因此我们也需要重新计算聚类中心。
- en: 'Hopefully, you can see the pattern in this process by now: every time we reclassify
    our clusters, we have to recalculate the clusters’ centers, but every time we
    recalculate the centers, we have to reclassify the clusters. Stated another way,
    every time we perform the Expectation step, we have to perform the Maximization
    step, but every time we perform the Maximization step, we have to perform the
    Expectation step again.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在你能看出这个过程的规律：每次我们重新分类簇时，都需要重新计算簇的中心，而每次重新计算中心时，我们又需要重新分类簇。换句话说，每次我们执行期望步骤时，都必须执行最大化步骤，而每次我们执行最大化步骤时，又必须重新执行期望步骤。
- en: 'That’s why the next, final step of E-M clustering is to repeat the Expectation
    and Maximization steps: both steps create a need for the other one. We can write
    a short loop that will accomplish this for us:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么 E-M 聚类的下一步、最后一步是重复执行期望和最大化步骤：这两个步骤相互依赖。我们可以写一个简短的循环来为我们完成这个任务：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The first line of the loop’s body (starting with `theclass=`) accomplishes the
    Expectation step, and the next line accomplishes the Maximization step. You may
    wonder whether we’ll get caught in an infinite loop, in which we have to constantly
    recalculate centers and reclassify clusters again and again forever, never reaching
    a final answer. We’re lucky because E-M clustering is mathematically guaranteed
    to *converge*, meaning that eventually we’ll reach a step where we recalculate
    the centers and find the same centers we calculated in the previous step, and
    we reclassify the clusters and find the same clusters we classified in the previous
    step. At that point, we can stop running our clustering because continuing will
    just give us a repetition of the same answers over and over.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 循环体的第一行（以`theclass=`开始）完成了期望步骤，下一行则完成了最大化步骤。你可能会想，我们会不会陷入无限循环，需要不断重新计算中心并重新分类簇，永远无法得出最终答案。幸运的是，E-M
    聚类在数学上是保证*收敛*的，这意味着最终我们会达到一个步骤，在这个步骤中我们重新计算中心并发现与上一步计算的中心相同，同时重新分类簇并发现与上一步分类的簇相同。此时，我们可以停止运行聚类，因为继续下去只是不断重复相同的结果。
- en: In the previous snippet, instead of checking for convergence, we set a limit
    at 100 iterations. For a dataset as small and simple as ours, this will certainly
    be more than enough iterations. If you have a complex dataset that doesn’t seem
    to converge after 100 iterations, you can increase to 1,000 or more until your
    E-M clustering reaches convergence.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们没有检查是否收敛，而是设置了一个迭代次数限制为 100 次。对于像我们这样的小而简单的数据集，这肯定足够了。如果你有一个复杂的数据集，在
    100 次迭代后似乎没有收敛，你可以增加到 1,000 次甚至更多，直到 E-M 聚类达到收敛。
- en: Let’s think about what we’ve done. We did the Guessing step, guessing means
    and variances of our clusters. We did the Expectation step, classifying clusters
    based on means and variances. We did the Maximization step, calculating means
    and variances based on clusters. And we did the Convergence step, repeating the
    Expectation and Maximization steps until reaching a stopping condition.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们做了什么。我们进行了猜测步骤，猜测了簇的均值和方差。我们进行了期望步骤，根据均值和方差对簇进行了分类。我们进行了最大化步骤，根据簇计算均值和方差。我们进行了收敛步骤，重复执行期望和最大化步骤直到达到停止条件。
- en: We’ve completed E-M clustering! Now that we’ve finished, let’s look at a plot
    of the final estimated clusters and centers by running `makeplot(allpoints,theclass,allmeans)`
    one final time; see [Figure 7-13](#figure7-13).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了 E-M 聚类！现在，我们完成了聚类，让我们通过再次运行`makeplot(allpoints,theclass,allmeans)`来看一下最终估计的簇和中心的图形；见[图
    7-13](#figure7-13)。
- en: When we look at this plot, we can see that our clustering succeeded. One of
    our cluster centers (stars) appears close to the center of the large, spread-out
    cluster. The other two cluster centers appear near the center of the smaller,
    more compact clusters. Importantly, we can see observations that are closer (in
    absolute distance) to the small clusters but are classified as being part of the
    large cluster. This is because E-M clustering takes variance into account; since
    it sees that the center cluster is more spread out, it assigns a higher variance
    to it, and therefore it’s able to include more points. Remember that we started
    with some atrocious guesses for cluster centers, but we got a result that exactly
    matches what looks perfect to us. Starting from our bad guesses in [Figure 7-10](#figure7-10),
    we’ve arrived at reasonable-looking results in [Figure 7-13](#figure7-13). This
    shows the strength of E-M clustering.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看这个图表时，我们可以看到我们的聚类成功了。我们的一个聚类中心（星号）出现在那个大而分散的聚类中心附近。其他两个聚类中心出现在较小、更紧凑的聚类中心附近。重要的是，我们可以看到一些观察点，它们与小聚类的距离更近（绝对距离），但它们被归类为大聚类的一部分。这是因为
    E-M 聚类考虑了方差；由于它看到中心聚类更分散，因此给它分配了更高的方差，从而能够包括更多的点。记住，我们开始时对聚类中心有一些非常糟糕的猜测，但最终得到了一个完全符合我们预期的结果。从[图
    7-10](#figure7-10)中的糟糕猜测出发，我们最终在[图 7-13](#figure7-13)中得到了一个看起来合理的结果。这展示了 E-M 聚类的强大功能。
- en: '![](image_fi/502888c07/f07013.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07013.png)'
- en: 'Figure 7-13: Final E-M clustering results'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-13：最终的 E-M 聚类结果
- en: Our E-M clustering process has identified clusters in our data. We’ve completed
    the clustering algorithm, but we haven’t applied it to any business scenario yet.
    The way we apply it to business will depend on what the data represents. This
    was just example data generated for the book, but we could perform exactly the
    same E-M clustering process on any other data from any field. For example, we
    might imagine, as we described before, that the points of [Figure 7-13](#figure7-13)
    represent cities, and the x- and y-axes represent types of urban growth. Or, the
    points of [Figure 7-13](#figure7-13) could represent customers, and the x- and
    y-axes represent customer attributes like total spending, age, location, or anything
    else.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 E-M 聚类过程已经识别出了数据中的聚类。我们已经完成了聚类算法，但还没有将其应用于任何业务场景。如何将其应用到业务中将取决于数据的具体内容。这只是为本书生成的示例数据，但我们可以在任何其他领域的数据上执行完全相同的
    E-M 聚类过程。例如，我们可以假设，如前所述，[图 7-13](#figure7-13)中的点代表城市，x 轴和 y 轴代表不同类型的城市发展。或者，[图
    7-13](#figure7-13)中的点可以代表客户，x 轴和 y 轴代表客户属性，如总消费、年龄、位置等。
- en: What you do with your clusters will depend on the data you’re working with and
    your goals. But in every situation, knowing the clusters that exist in your data
    can help you craft different marketing approaches or different products for different
    clusters, or different strategies for interacting with each of them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何处理你的聚类将取决于你所使用的数据和你的目标。但在任何情况下，了解数据中存在的聚类可以帮助你为不同的聚类制定不同的营销方法、不同的产品，或者与每个聚类互动的不同策略。
- en: Other Clustering Methods
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他聚类方法
- en: 'E-M clustering is a powerful clustering method, but it’s not the only one.
    Another method, *k-means clustering*, is more popular because it’s easier. If
    you can do E-M clustering, k-means clustering is easy after some straightforward
    changes to our code. The following are the steps of k-means clustering:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: E-M 聚类是一种强大的聚类方法，但它不是唯一的。另一种方法，*k-means 聚类*，更为流行，因为它更容易。如果你能做 E-M 聚类，那么通过一些简单的代码修改，k-means
    聚类就变得容易了。以下是 k-means 聚类的步骤：
- en: 'Guessing: Make guesses for the means of every cluster.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猜测：对每个聚类的均值进行猜测。
- en: 'Classification: Classify every observation in our data according to which cluster
    it’s most likely to be a member of, according to which mean it’s closest to.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类：根据每个观察点最有可能属于哪个聚类来分类我们的数据，依据的是它最接近哪个均值。
- en: 'Adjustment: Use the classifications obtained in the Classification step to
    calculate new estimates for the means of each cluster.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整：利用分类步骤得到的分类结果来计算每个聚类的均值的新估计。
- en: 'Convergence: Repeat the Classification and Adjustment steps until reaching
    a stopping condition.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收敛：重复分类和调整步骤，直到达到停止条件。
- en: 'You can see that k-means clustering consists of four steps, just like E-M clustering.
    The first and last steps (Guessing and Convergence) are identical: we make guesses
    at the beginning of both processes, and we repeat steps until convergence in both
    processes. The only differences are in the second and third steps.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，k-means聚类与E-M聚类一样，也由四个步骤组成。第一步和最后一步（猜测和收敛）是相同的：我们在两个过程的开始阶段都进行猜测，并且在两个过程中都重复步骤直到收敛。唯一的区别在于第二步和第三步。
- en: 'In both algorithms, the second step (Expectation for E-M clustering, Classification
    for k-means clustering) determines which observations belong to which clusters.
    The difference is in the way we determine which observations belong to which cluster.
    For E-M clustering, we determine an observation’s cluster based on comparing the
    heights of bell curves, as illustrated in [Figure 7-6](#figure7-6). With k-means
    clustering, we determine an observation’s cluster more simply: by measuring the
    distance between the observation and each cluster center, and finding which cluster
    center it’s closest to. So, when we see a dice roll equal to 12, E-M clustering
    will tell us that it was rolled by the 12-sided dice, because of the heights of
    bell curves in [Figure 7-6](#figure7-6). However, k-means clustering will tell
    us that it was rolled by the 6-sided dice, because 12 is closer to 7 (the mean
    roll of 6-sided dice) than it is to 19 (the mean roll of our 12-sided dice).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种算法中，第二步（E-M聚类中的期望步骤，k-means聚类中的分类步骤）决定了哪些观测值属于哪个聚类。区别在于我们如何确定哪些观测值属于哪个聚类。对于E-M聚类，我们通过比较钟形曲线的高度来确定一个观测值属于哪个聚类，如[图7-6](#figure7-6)所示。对于k-means聚类，我们通过更简单的方式来确定观测值属于哪个聚类：即通过测量观测值与每个聚类中心之间的距离，找到最接近的聚类中心。所以，当我们看到一个掷骰子结果为12时，E-M聚类会告诉我们这是12面骰子掷出的，因为在[图7-6](#figure7-6)中钟形曲线的高度决定了这一点。然而，k-means聚类会告诉我们这是6面骰子掷出的，因为12更接近7（6面骰子的平均点数），而不是19（12面骰子的平均点数）。
- en: The other difference between E-M clustering and k-means clustering is in the
    third step (Maximization for E-M clustering and Adjustment for k-means clustering).
    In E-M clustering, we need to calculate the means and covariance matrices for
    every cluster. But in k-means clustering, we need to calculate only the means
    of each cluster—we don’t use covariance estimates at all in k-means clustering.
    You can see that E-M clustering and k-means clustering both have the same general
    outline, and differ in only a few particulars of the way classification and adjustment
    are performed.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: E-M聚类和k-means聚类之间的另一个区别在于第三步（E-M聚类中的最大化步骤和k-means聚类中的调整步骤）。在E-M聚类中，我们需要计算每个聚类的均值和协方差矩阵。但是在k-means聚类中，我们只需要计算每个聚类的均值—在k-means聚类中完全不使用协方差估计。你可以看到，E-M聚类和k-means聚类有相同的基本框架，只有在分类和调整的具体步骤上有所不同。
- en: 'Actually, we can easily implement k-means clustering in Python if we import
    the right modules:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们只需导入正确的模块，就可以轻松地在Python中实现k-means聚类：
- en: '[PRE30]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we import `KMeans()` from the same sklearn module we’ve used before. Then,
    we create an object called `kmeans`; this is the object we can use to do k-means
    clustering on our data. You can see that when we call the `KMeans()` function,
    we need to specify a few important parameters, including the number of clusters
    we’re looking for (`n_clusters`). After we’ve created our `kmeans` object, we
    can call its `fit()` method to find the clusters in our `allpoints` data (the
    same data we used before). When we call the `fit()` method, this determines what
    cluster every point is in, and we can access the classification of each cluster
    in the `kmeans.labels_` object. We can also access the cluster centers in the
    `kmeans.cluster_centers_` object. Finally, we can call our `makeplot()` function,
    to plot our data and the clusters we found using k-means. [Figure 7-14](#figure7-14)
    has the result.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从之前使用过的相同sklearn模块中导入`KMeans()`。然后，我们创建一个名为`kmeans`的对象；这个对象将用于对我们的数据进行k-means聚类。你可以看到，当我们调用`KMeans()`函数时，需要指定一些重要参数，包括我们想要的聚类数量（`n_clusters`）。在创建了`kmeans`对象后，我们可以调用它的`fit()`方法，来找到我们`allpoints`数据中的聚类（也就是之前使用的数据）。当我们调用`fit()`方法时，它会决定每个数据点属于哪个聚类，我们可以通过`kmeans.labels_`对象访问每个聚类的分类。我们还可以通过`kmeans.cluster_centers_`对象访问聚类的中心。最后，我们可以调用我们的`makeplot()`函数，绘制我们的数据以及我们使用k-means找到的聚类。[图7-14](#figure7-14)展示了结果。
- en: '![](image_fi/502888c07/f07014.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c07/f07014.png)'
- en: 'Figure 7-14: The result of k-means clustering'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-14：k-means聚类的结果
- en: 'You can see in this plot that the results of k-means clustering are not very
    different from the results of E-M clustering: we’ve identified the two dense clusters
    at the top and right of the plot, and we’ve identified the looser cluster in the
    rest of the plot. One difference is that the cluster boundaries are not the same:
    in k-means clustering, the top and right clusters include some observations that
    look more like members of the less dense cluster. This is not a coincidence; k-means
    clustering is designed to find clusters that are approximately the same sizes,
    and it doesn’t have the flexibility that E-M clustering has to find different
    cluster sizes with different densities.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Many other clustering methods exist besides E-M and k-means, so many that they
    are too numerous to write about in detail here. Every clustering method is suited
    to a particular type of data and a particular application. For example, one powerful
    yet underappreciated clustering method is called *density-based spatial clustering
    of applications with noise* *(DBSCAN)*. Unlike E-M and k-means clustering, DBSCAN
    can detect clusters that have unique, nonspherical, non-bell-like shapes, like
    the shapes shown in [Figure 7-15](#figure7-15).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c07/f07015.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-15: The result of DBSCAN clustering, with nonspherical clusters'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: You can see two distinct groups, or clusters, of data. But since they swirl
    around each other, using bell curves to classify them wouldn’t work well. Bell
    curves can’t easily find the complex boundaries that these clusters have. DBSCAN
    doesn’t rely on bell curves, but rather relies on careful considerations of the
    distances between each of the points within and between clusters.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important kind of clustering is called *hierarchical clustering*. Instead
    of simply classifying observations into groups, hierarchical clustering yields
    a nested hierarchy that shows groups of observations in closely related, then
    successively more distant groups. Every type of clustering has different assumptions
    and methods associated with it. But all of them are accomplishing the same goal:
    classifying points into groups without any labels or supervision.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Other Unsupervised Learning Methods
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering is the most popular application of unsupervised learning, but a
    variety of algorithms besides clustering fall under the broad umbrella of unsupervised
    learning. Several unsupervised learning methods accomplish *anomaly detection*:
    finding observations that don’t fit with the general pattern of a dataset. Some
    anomaly detection methods are broadly similar to clustering methods, because they
    sometimes include identifying dense groups of near neighbors (like clusters) and
    measuring distances between observations and their closest clusters.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Another group of unsupervised learning methods are called *latent variable
    models*. These models try to express the observations in a dataset as a function
    of hypothetical hidden, or *latent*, variables. For example, a dataset may consist
    of student scores in eight classes. We might have a hypothesis that two main types
    of intelligence exist: analytic and creative. We can check whether students’ scores
    in quantitative, analytic classes like math and science tend to correlate, and
    whether students’ scores in more creative classes like language and music tend
    to correlate. In other words, we hypothesize that there are two hidden, or latent,
    variables that we haven’t directly measured, analytic intelligence and creative
    intelligence, and these two latent variables determine the values of all the variables
    we do observe, all the students’ grades, to a large extent.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t the only possible hypothesis. We could also hypothesize that student
    grades are determined by only one latent variable, general intelligence, or we
    could hypothesize that student grades are determined by three or any other number
    of latent variables that we could then try to measure and analyze.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The E-M clustering we accomplished in this chapter can also be thought of as
    a type of latent variable model. In the case of clustering dice rolls, the latent
    variables we’re interested in finding are the mean and standard deviations of
    the bell curves that indicate cluster locations and sizes. Many of these latent
    variable models rely on linear algebra and matrix algebra, so if you’re interested
    in unsupervised learning, you should study those topics diligently.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Remember that all of these methods are unsupervised, meaning we have no labels
    that we can rigorously test our hypotheses against. In the case of [Figure 7-13](#figure7-13)
    and [Figure 7-14](#figure7-14), we can see that the cluster classifications we’ve
    found look right and make sense in certain ways, but we can’t say with certainty
    whether they’re correct. Nor can we say whether E-M clustering (whose results
    are shown in [Figure 7-13](#figure7-13)) or k-means clustering (whose results
    are shown in [Figure 7-14](#figure7-14)) is more correct than the other—because
    there are no “ground truth” group labels that we can use to judge correctness.
    This is why unsupervised learning methods are often used for data exploration
    but aren’t often used to get final answers about predictions or classifications.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Since it’s not possible to definitively say whether any unsupervised learning
    method has delivered correct results, unsupervised learning requires good judgment
    to do well. Instead of giving us final answers, it tends to give us insight into
    data that in turn helps us get ideas for other analyses, including supervised
    learning. But that doesn’t mean it’s not worthwhile; unsupervised learning can
    provide invaluable insights and ideas.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we covered unsupervised learning, with a focus on E-M clustering.
    We discussed the concept of unsupervised learning, the details of E-M clustering,
    and the differences between E-M clustering and other clustering methods like k-means
    clustering. We finished with a discussion of other unsupervised learning methods.
    In the next chapter, we’ll discuss web scraping and how to get data quickly and
    easily from websites for analysis and business applications.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了无监督学习，重点讲解了E-M聚类。我们讨论了无监督学习的概念、E-M聚类的细节，以及E-M聚类与其他聚类方法（如k-means聚类）之间的区别。最后，我们讨论了其他无监督学习方法。在下一章中，我们将讨论网页抓取，以及如何从网站快速、轻松地获取数据进行分析和商业应用。
