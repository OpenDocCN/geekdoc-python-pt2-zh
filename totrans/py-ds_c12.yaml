- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning for Data Analysis
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Machine learning* is a method of data analysis where applications leverage
    existing data to discover patterns and make decisions, without being explicitly
    programmed to do so. In other words, the applications learn for themselves, independent
    of human interference. A robust data analysis technique, machine learning is used
    in many fields, including but not limited to classification, clustering, predictive
    analytics, learning associations, anomaly detection, image analysis, and natural
    language processing.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides an overview of some fundamental machine learning concepts,
    then explores two machine learning examples in depth. First we’ll perform a sentiment
    analysis, developing a model to predict the number of stars (from one to five)
    associated with a product review. After that, we’ll develop another model to predict
    changes in the price of a stock.
  prefs: []
  type: TYPE_NORMAL
- en: Why Machine Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning lets computers perform tasks that would be difficult, if not
    impossible, using conventional programming techniques. For instance, imagine you
    need to build an image-processing application that can distinguish between different
    types of animals based on submitted photos. In this hypothetical scenario, you
    already have a code library that can identify the edges of an object (such as
    an animal) in an image. In this way, you can transform the animal shown in a photo
    into a characteristic set of lines. But how can you programmatically distinguish
    between the lines representing two different animals—say, a cat and a dog?
  prefs: []
  type: TYPE_NORMAL
- en: A traditional programming approach would be to manually craft rules that map
    every characteristic line combination to an animal. Unfortunately, this solution
    would require a huge amount of code, and it could completely fail when a new photo
    is submitted whose edges don’t fit one of the manually defined rules. In contrast,
    applications built on machine learning algorithms don’t rely on predefined logic
    but instead hinge on the application’s ability to automatically learn from previously
    seen data. Thus, a machine learning–based photo-tagging application would look
    for patterns in the line combinations derived from previous photos and then make
    predictions about the animals in new photos based on probability statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data scientists distinguish between several types of machine learning. The two
    most common are supervised learning and unsupervised learning. In this chapter
    we’ll primarily be concerned with supervised learning, but this section provides
    a brief overview of both types.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Supervised learning* uses a labeled dataset (referred to as a *training set*)
    to teach a model to yield the desired output when given new, previously unseen
    data. Technically, supervised learning is the technique of inferring a function
    that maps an input to an output based on the training set. You’ve already seen
    an example of supervised learning in Chapter 3, where we used a set of example
    product reviews to train a model to predict whether new product reviews were positive
    or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: The input data for a supervised learning algorithm can represent characteristics
    of real-world objects or events. For example, you might use the characteristics
    of homes for sale (square footage, number of bedrooms and bathrooms, and so on)
    as input for an algorithm designed to predict home values. These values would
    be the output of the algorithm. You’d train the algorithm with a collection of
    input-output pairs, consisting of the characteristics of various homes and their
    associated values, then feed it the characteristics of new homes and receive those
    new homes’ estimated values as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other supervised learning algorithms are designed to work not with characteristics
    but with *observational data*: data gathered through observing an activity or
    behavior. As an example, consider a time series produced by the sensors monitoring
    noise levels at an airport. This observational noise level data might be submitted
    to a machine learning algorithm, along with information such as the time of day
    and day of the week, so that the algorithm can learn to predict the noise levels
    in the coming hours. In this example, the times and days of the week are the input,
    and the noise levels are the output. In other words, the algorithm would be designed
    to predict future observational data.'
  prefs: []
  type: TYPE_NORMAL
- en: Both the home value predictions and the noise level predictions are examples
    of *regression*, a common supervised learning technique for predicting continuous
    values. The other common supervised learning technique is *classification*, where
    the model assigns one of a finite number of class labels to each input. Distinguishing
    between favorable and unfavorable product reviews is an example of classification,
    as are other *sentiment analysis* applications, where text fragments are identified
    as being either positive or negative. We’ll explore an example of sentiment analysis
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Unsupervised learning* is a machine learning technique in which there’s no
    training stage. You only give the application input data, without any corresponding
    output values to learn from. In that sense, unsupervised machine learning models
    have to work on their own, discovering hidden patterns in the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: A great example of unsupervised learning is *association analysis*, where a
    machine learning application identifies items within a set that have an affinity
    for each other. In Chapter 11, you performed an association analysis on a set
    of transaction data, identifying items commonly purchased together. You used the
    Apriori algorithm, which doesn’t require example output data to learn from; instead,
    it takes all the transaction data as an input and searches the transactions for
    frequent itemsets, thus representing learning with no training.
  prefs: []
  type: TYPE_NORMAL
- en: How Machine Learning Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical machine learning pipeline relies on three major components:'
  prefs: []
  type: TYPE_NORMAL
- en: Data to learn from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A statistical model to apply to the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New, previously unseen data to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following sections take a closer look at each of these components.
  prefs: []
  type: TYPE_NORMAL
- en: Data to Learn From
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning is based on the idea that computer systems can learn, so any
    machine learning algorithm requires data to learn from. As we’ve already discussed,
    the nature of this data varies depending on whether the machine learning model
    is supervised or unsupervised. In the case of supervised machine learning, the
    data to learn from takes the form of input-output pairs that train the model to
    later predict outputs based on new inputs. In unsupervised learning, on the other
    hand, the model receives only input data, which it mines for patterns in order
    to produce output.
  prefs: []
  type: TYPE_NORMAL
- en: 'While all machine learning applications require data to learn from, the requisite
    format of this data may vary from algorithm to algorithm. Many algorithms learn
    from a dataset organized as a table, in which the rows represent various instances,
    such as perhaps individual objects or particular moments in time, and the columns
    represent attributes pertaining to those instances. A classic example is the Iris
    dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    It has 150 rows, each of which contains observations about a different specimen
    of iris. Here are the first four rows of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first four columns represent different attributes, or features, of the
    specimens. The fifth column contains a label for each instance: the iris’s exact
    species name. If you trained a classification model with this dataset, you’d use
    the values in the first four columns as the independent variables, or input, while
    the fifth column would be the dependent variable, or output. After learning from
    this data, ideally the model would be able to classify new iris specimens by species.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other machine learning algorithms learn from non-tabular data. For example,
    the Apriori algorithm used for association analysis, which we discussed in the
    previous chapter, takes a set of transactions (or baskets) of different sizes
    as input data. Here’s a simple example of such a transaction set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Beyond the question of how the machine learning data is structured, the type
    of data used also varies from algorithm to algorithm. As the previous examples
    illustrate, some machine learning algorithms work with numeric or textual data.
    There are also algorithms designed to work with photo, video, or audio data.
  prefs: []
  type: TYPE_NORMAL
- en: A Statistical Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Whatever data format the machine learning algorithm requires, the input data
    must be transformed in such a way that it can be analyzed to produce outputs.
    This is where a *statistical model* comes into play: statistics are used to create
    a representation of the data so the algorithm can identify relationships between
    variables, discover insights, make predictions about new data, generate recommendations,
    and so on. Statistical models lie at the heart of any machine learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the Apriori algorithm uses the support metric as a statistical
    model to find frequent itemsets. (As discussed in Chapter 11, support is the percentage
    of transactions that include an itemset.) In particular, the algorithm identifies
    every possible itemset and calculates the corresponding support metric, then selects
    only those itemsets with a sufficiently high support. Here’s a simple example
    illustrating how the algorithm works behind the scenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This example shows only two-item itemsets. In fact, after calculating the support
    for every possible two-item itemset, the Apriori algorithm moves on to analyzing
    every three-item itemset, four-item itemset, and so on. Then the algorithm uses
    the support values for the itemsets of all sizes to generate a list of frequent
    itemsets.
  prefs: []
  type: TYPE_NORMAL
- en: Previously Unseen Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In supervised machine learning, once you’ve trained a model on example data,
    you can apply it to new, previously unseen data. Before doing so, however, you
    might want to evaluate the model, which is why it’s common practice to split the
    initial dataset into training and testing sets. The former is the data the model
    learns from, and the latter becomes previously unseen data for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The testing data still has both input and output, but only the input is shown
    to the model. Then the real output is compared to the output suggested by the
    model to assess the accuracy of its predictions. Once you’ve made sure the model’s
    accuracy is acceptable, you can use fresh input data to do predictive analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of unsupervised learning, there is no distinction between data to
    learn from and previously unseen data. All the data is essentially previously
    unseen, and the model tries to learn from it by analyzing its underlying features.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Sentiment Analysis Example: Classifying Product Reviews'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve reviewed the basics of machine learning, you’re ready to conduct
    a sample sentiment analysis. As explained previously, this natural language processing
    technique allows you to programmatically determine whether a piece of writing
    is positive or negative. (More categories, such as neutral, very positive, or
    very negative, are also possibilities in some applications.) In essence, sentiment
    analysis is a form of classification, a supervised machine learning technique
    that sorts data into discrete categories.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 3, you used scikit-learn to perform a basic sentiment analysis on
    a set of product reviews from Amazon. You trained a model to identify whether
    reviews were good or bad. In this section, you’ll expand on the work you did in
    that chapter. You’ll obtain an actual set of product reviews directly from Amazon
    and use it to train a classification model. The goal of the model is to predict
    the star ratings of reviews, on a one to five scale. Thus, the model will sort
    reviews into five possible categories, rather than just two.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining Product Reviews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in building the model is to download a set of actual product
    reviews from Amazon. One easy way to do this is to use Amazon Reviews Exporter,
    a Google Chrome browser extension that downloads an Amazon product’s reviews as
    a CSV file. You can install this extension in your Chrome browser with one click
    from this page: [https://chrome.google.com/webstore/detail/amazon-reviews-exporter-c/njlppnciolcibljfdobcefcngiampidm](https://chrome.google.com/webstore/detail/amazon-reviews-exporter-c/njlppnciolcibljfdobcefcngiampidm).'
  prefs: []
  type: TYPE_NORMAL
- en: With the extension installed, open an Amazon product page in Chrome. For this
    example, we’ll use the Amazon page for No Starch Press’s *Python Crash Course*
    by Eric Matthes (https://www.amazon.com/Python-Crash-Course-2nd-Edition/dp/1593279280),
    which at the time of this writing has 445 reviews. To download the book’s reviews,
    find and click the Amazon Reviews Exporter button in your Chrome toolbar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the reviews in a CSV file, you can read them into a pandas DataFrame
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Before proceeding, you might want to look at the total number of reviews and
    the first few reviews loaded in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This view shows only the `title` and `rating` fields for each record. We’ll
    treat the review titles as the independent variable in the model (that is, the
    input) and the ratings as the dependent variable (the output). Notice that we’re
    ignoring the full text of each review and focusing just on the titles. This seems
    reasonable for the purposes of training a model for sentiment classification,
    since the title typically represents a summary of the reviewer’s feelings about
    the product. By contrast, the full review text often includes other nonemotional
    information, like a description of the book’s contents.
  prefs: []
  type: TYPE_NORMAL
- en: Cleansing the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you can process real-world data, it almost always requires cleansing.
    In this particular example, you’ll need to filter out reviews that aren’t written
    in English. For that, you’ll need a way to programmatically determine the language
    of each review. There are several Python libraries with language-detection capabilities;
    we’ll use google_trans_new.
  prefs: []
  type: TYPE_NORMAL
- en: Installing google_trans_new
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Use `pip` to install the google_trans_new library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Before going any further, make sure google_trans_new has fixed the known bug
    that raises a `JSONDecodeError` exception during language detection. For that,
    run the following test in a Python session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If this test runs without an error, you’re ready to proceed. If it raises a
    `JSONDecodeError` exception, you’ll need to make some small changes to the library’s
    source code in *google_trans_new.py*. Locate the file with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The command will show some basic information about the library, including the
    location of its source code on your local machine. Go to that location, and open
    *google_trans_new.py* in a text editor. Then find lines 151 and 233, which will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'and change them to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the changes, restart your Python session, and rerun the test. It should
    now correctly identify *good* as an English-language word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Removing Non-English Reviews
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now you’re ready to detect the language of each review and filter out the reviews
    that aren’t in English. In the following code, you use the `google_translator`
    module from google_trans_new to determine the language of each review title, and
    you store the language in a new column of the DataFrame. It may take a while to
    detect the language of a large number of samples, so be patient when you run the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You first create a `google_translator` object, then use a lambda expression
    to apply the object’s `detect()` method to each review title. You save the results
    to a new column called `lang`. Here you print that column, along with `title`
    and `rating`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Your next step is to filter the dataset, keeping only those reviews that are
    written in English:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This operation should reduce the total number of rows in the dataset. To verify
    that it worked, count the number of rows in the updated DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The row count should be less than it was originally because all the non-English
    reviews have been removed.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting and Transforming the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you go any further, you need to split the reviews into a training set
    for developing the model and a testing set for evaluating its accuracy. You also
    need to transform the natural language of the review titles into numerical data
    that the model can understand. As you saw in “Transforming Text into Numerical
    Feature Vectors” in Chapter 3, the bag of words (BoW) technique can be used for
    that purpose; to review how it works, refer back to that section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses scikit-learn to both split and transform the data.
    The code follows the same format used in Chapter 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To recap, scikit-learn’s `train_test_split()` function randomly splits the
    data into a training set and a testing set ❶, and the library’s `CountVectorizer`
    class has methods for transforming text data into numerical feature vectors ❷.
    The code generates the following structures, implementing the training and testing
    sets as NumPy arrays and their corresponding feature vectors as SciPy sparse matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reviews_train` An array containing the review titles chosen for training'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`reviews_test` An array containing the review titles chosen for testing'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`y_train` An array containing the star ratings corresponding to the reviews
    in `reviews_train`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`y_test` An array containing the star ratings corresponding to the reviews
    in `reviews_test`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`x_train` A matrix containing the set of feature vectors for the review titles
    found in the `reviews_train` array'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`x_test` A matrix containing the set of feature vectors for the review titles
    found in the `reviews_test` array'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We’re most interested in `x_train` and `x_test`, the numerical feature vectors
    scikit-learn has generated from the review titles using the BoW technique. Each
    of these matrices should include one row per review headline, with this row representing
    the headline’s numerical feature vector. To check the number of rows in the matrix
    generated from the `reviews_train` array, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting number should be 80 percent of the total number of English-language
    reviews, since you split the data into training and testing sets using the 80/20
    pattern. The `x_test` matrix should contain the other 20 percent of the feature
    vectors, which you can verify with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You might also want to check the length of the feature vectors in the training
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You print the length of just the first row in the matrix, but each row’s length
    is the same. The result may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This means that 442 unique words occur in the training set’s review titles.
    This collection of words is called the *vocabulary dictionary* of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re curious, here’s how to print the entire matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Each column of the matrix corresponds to one of the words in the dataset’s
    vocabulary dictionary, and the numbers tell you how many times each word appears
    in any given review title. As you can see, the matrix consists mostly of zeros.
    This is to be expected: the average review title in the example set consists of
    just 5 to 10 words, but the vocabulary dictionary of the entire set consists of
    442 words, meaning that in a typical row, only 5 to 10 elements out of 442 will
    be set to 1 or higher. Nevertheless, this representation of the example data is
    exactly what you need to train a classification model for sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now you’re ready to train your model. In particular, you need to train a *classifier*,
    a machine learning model that sorts data into categories, so that it can predict
    the number of stars of a review. For that, you can use scikit-learn’s `LogisticRegression`
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You import the `LogisticRegression` class and create a `classifier` object.
    Then you train the classifier by passing it the `x_train` matrix (the feature
    vectors of the review titles in the training set) and the `y_train` array (the
    corresponding star ratings).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the model has been trained, you can use the `x_test` matrix to evaluate
    its accuracy, comparing the model’s predicted ratings to the actual ratings in
    the `y_test` array. In Chapter 3, you used the `classifier` object’s `score()`
    method to evaluate its accuracy. Here you’ll use a different evaluation method,
    one that allows for more precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You use the classifier’s `predict()` method to predict ratings based on the
    `x_test` feature vectors ❶. Then you test for equivalency between the model’s
    predictions and the actual star ratings ❸. The result of this comparison is a
    Boolean array, where `True` and `False` indicate accurate and inaccurate predictions.
    By taking the arithmetic mean of the array ❷, you get an overall accuracy rating
    for the model. (For the purposes of calculating the mean, each `True` is treated
    as a `1` and each `False` as a `0`.) Printing the result should give you something
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This indicates that the model is 68 percent accurate, meaning that on average
    approximately 7 out of 10 predictions are correct. To get a more fine-grained
    understanding of the model’s accuracy, however, you need to use other scikit-learn
    features to examine more specific metrics. For example, you can study the model’s
    *confusion matrix*, a grid that compares predicted classifications with actual
    classifications. A confusion matrix can help reveal the model’s accuracy within
    each individual class, as well as show whether the model is likely to confuse
    two classes (mislabel one class as another). You can create the confusion matrix
    for your classification model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You import scikit-learn’s `metrics` module, then use the `confusion_matrix()`
    method to generate the matrix. You pass the method the actual ratings of the test
    set (`y_test`), the ratings predicted by your model (`predicted`), and the labels
    corresponding to those ratings. The matrix will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, the rows correspond to actual ratings, and the columns correspond to predicted
    ratings. For example, looking at the numbers in the first row tells you the test
    set contained eight actual one-star ratings, one of which was predicted to be
    a four-star rating and seven of which were predicted to be five-star ratings.
  prefs: []
  type: TYPE_NORMAL
- en: The main diagonal of the confusion matrix (top left to bottom right) shows the
    number of correct predictions for each rating level. Examining this diagonal,
    you can see that the model made 54 correct predictions for five-star reviews and
    only 1 correct prediction for four-star reviews. No one-, two-, or three-star
    reviews were correctly identified. Overall, out of a test set of 81 reviews, 55
    were correctly predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'This outcome raises a number of questions. For one, why does the model only
    work well for five-star reviews? The problem could be that the example dataset
    has only five-star reviews in sufficient quantity. To check if this is the case,
    you might count the rows in each rating group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You group the original DataFrame containing both the training and testing data
    by the `rating` column and use the `size()` method to get the number of entries
    in each group. The output might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this count confirms our hypothesis: there are far more five-star
    reviews than any other rating, suggesting the model didn’t have enough data to
    effectively learn the features of reviews with four stars or lower.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further explore the accuracy of the model, you might also want to look at
    its main classification metrics, comparing the `y_test` and `predicted` arrays.
    You can do this with the help of the `classification_report()` function found
    in scikit-learn’s `metrics` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated report will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This report shows the summary of the main classification metrics for each class
    of reviews. Here, we’ll focus on support and recall; for more information about
    the other metrics in the report, see [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report).
  prefs: []
  type: TYPE_NORMAL
- en: The support metric shows the number of reviews for each class of ratings. In
    particular, it reveals that the reviews are distributed extremely unevenly across
    rating groups, with the test set exhibiting the same tendency as the entire dataset.
    Of the total of 81 reviews, there are 57 five-star reviews and only 2 two-star
    reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Recall shows the ratio of correctly predicted reviews to all the reviews with
    a certain rating. For example, the recall metric for five-star reviews is 0.95,
    meaning the model was 95 percent accurate at predicting five-star reviews, while
    the same metric for four-star reviews is just 0.14\. Since the reviews with the
    other ratings don’t have any correct predictions, the weighted average recall
    for the whole test set is shown to be 0.68 at the bottom of the report. This is
    the same accuracy rating you got near the start of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Taking into consideration all these points, you can reasonably conclude that
    the problem is that the example set you’re using has a highly unequal number of
    reviews in each rating group.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Stock Trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To further explore how machine learning can be applied to data analysis, next
    we’ll create a model for predicting stock market trends. For simplicity, we’ll
    create another classification model: one that predicts whether the price of a
    stock tomorrow will be higher, lower, or the same as it is today. A more sophisticated
    model might instead use regression to predict the actual price of a stock from
    day to day.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to this chapter’s sentiment analysis example, our stock prediction
    model (and indeed, many models that involve nontextual data) raises a new question:
    how do we decide what data to use as the model’s features, or inputs? For the
    sentiment analysis model, you used the feature vectors generated from the text
    of the review headlines via the BoW technique. The content of such a vector firmly
    depends on the content of the corresponding text. In this sense, the content of
    the vector is predefined, being formed from the features extracted from the corresponding
    text in accordance with a certain rule.'
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, when your model involves nontextual data such as stock prices,
    it’s often up to you to decide on, and perhaps even calculate, the set of features
    to be used as the input data for the model. Percentage change in price since the
    previous day, average price over the past week, and total trading volume over
    the two preceding days? Perhaps. Percentage change in price since two days ago,
    average price over the past month, and change in volume since yesterday? Could
    be. Financial analysts use all sorts of metrics such as these, in different combinations,
    as input data for their stock prediction models.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 10, you learned how to derive metrics from stock market data by calculating
    percentage changes over time, rolling window averages, and the like. We’ll revisit
    some of these techniques later in this section to generate the features for our
    prediction model. But first, we need to obtain some data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train your model, you’ll need a year’s worth of data for an individual stock.
    For the purpose of this example, we’ll use Apple (AAPL). Here, you use the yfinance
    library to obtain the company’s stock data for the last year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You’ll use the resulting `hist` DataFrame to derive metrics about the stock,
    such as the day-to-day percentage change of the price, and feed those metrics
    to your model. However, you can reasonably assume that there are also external
    factors (that is, information that can’t be derived from the stock data itself)
    that influence Apple’s stock price. For example, the overall performance of the
    wider stock market might affect the performance of an individual stock. Thus,
    it would be interesting to also take into account data about a broader stock market
    index as part of your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most well-known stock market indexes is the S&P 500\. It measures
    the stock performance of 500 large companies. As you saw in Chapter 4, you can
    obtain S&P 500 data in Python via the pandas-datareader library. Here you use
    the library’s `get_data_stooq()` method to retrieve one year’s worth of S&P 500
    data from the Stooq website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Using Python’s `datetime` module, you define the start and end dates of your
    query relative to the current date ❶. Then you call the `get_data_stooq()` method,
    using `'^SPX'` to request S&P 500 data, and store the result in the `index_data`
    DataFrame ❷.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have both Apple stock figures and S&P 500 index figures for the
    same one-year time period, you can combine the data into a single DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The DataFrames being joined have columns with the same names. To avoid overlap,
    you use the `rsuffix` parameter. It instructs the `join()` method to add the suffix
    `'_idx'` to all the column names from the `index_data` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, you’ll only be interested in the daily closing prices and
    trading volumes for both Apple and the S&P 500\. Here you filter the DataFrame
    to just those columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If you now print the `df` DataFrame, you should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The DataFrame contains a continuous multivariate time series. The next step
    is to derive features from the data that can be used as input for the machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving Features from Continuous Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You want to train your model on information about the changes in price and
    volume from day to day. As you learned in Chapter 10, you calculate percentage
    changes in continuous time series data by shifting data points in time, bringing
    past data points in line with present data points for the purposes of comparison.
    In the following code, you use `shift(1)` to calculate the percentage change of
    each DataFrame column from one day to the next, saving the results in a new batch
    of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of the four columns, you divide each data point by the data point
    from the day before, then take the natural logarithm of the result. Remember,
    the natural logarithm provides a close approximation of the percentage change.
    You end up with several new columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`priceRise` The percentage change of Apple’s stock price from one day to the
    next'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`volumeRise` The percentage change of Apple’s trading volume from one day to
    the next'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`priceRise_idx` The percentage change of the S&P 500 index price from one day
    to the next'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`volumeRise_idx` The percentage change of trading volume for the S&P 500 from
    one day to the next'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can now filter the DataFrame again to only include the new columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents of the DataFrame will now look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: These columns will become the features, or independent variables, for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the Output Variable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to generate the output variable (also called the target or
    dependent variable) for the existing dataset. This variable should convey what
    happens with the stock’s price on the next day: does it go up, go down, or stay
    the same? You can find this out by looking at the next day’s `priceRise` column,
    which you access through `df[''priceRise''].shift(-1)`. The negative shift moves
    future values backward in time. Based on this shift, you can generate a new column
    with a `-1` if the price goes down, a `0` if the price stays the same, or a `1`
    if the price goes up. Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm implemented here assumes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A price increase of more than 1 percent in relation to the next day is regarded
    as a rise (`1`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A price decrease by more than 1 percent in relation to the next day is regarded
    as a fall (`-1`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rest is regarded as stagnation (`0`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To implement the algorithm, you define a `conditions` list that checks the data
    according to points 1 and 2 ❶ as well as a `choices` list with the values `1`
    and `-1` to indicate a rise or fall in price ❷. Then you feed the two lists to
    NumPy’s `select()` function ❸, which builds an array by selecting values from
    `choices` based on the values in `conditions`. If neither condition is satisfied,
    a default value of `0` is assigned, satisfying point 3\. You store the array in
    a new DataFrame column, `Pred`, that you can use as the output for training and
    testing your model. Essentially, `-1`, `0`, and `1` are now the possible classes
    that the model can choose from when classifying new data.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Evaluating the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train your model, scikit-learn requires that you present the input and output
    data in separate NumPy arrays. You generate the arrays from the `df` DataFrame
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `features` array now contains the four independent variables (the input),
    and the `target` array contains the one dependent variable (the output). Next,
    you can split the data into training and testing sets and train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as you did in the sentiment analysis example earlier in the chapter, you
    use scikit-learn’s `train_test_split()` function to divide the dataset according
    to the 80/20 pattern, and you use the `LogisticRegression` classifier to train
    the model. Next, you pass the testing portion of the dataset to the classifier’s
    `score()` method to evaluate its accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The result might look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This indicates the model accurately predicted the next day’s trajectory of Apple
    stock about 62 percent of the time. Of course, you may get a different figure.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter you learned how some data analysis tasks, such as classification,
    can be accomplished with machine learning, a method that enables computer systems
    to learn from historical data or past experience. In particular, you looked at
    how machine learning algorithms can be used for the NLP task of sentiment analysis.
    You converted text data from Amazon product reviews into machine-readable numerical
    feature vectors, then trained a model to classify reviews according to their star
    ratings. You also learned how to generate features based on numerical stock market
    data, and you used those features to train a model to predict changes in a stock’s
    price.
  prefs: []
  type: TYPE_NORMAL
- en: There are many possibilities for combining machine learning, statistical methods,
    public APIs, and the capabilities of data structures available in Python. This
    book has shown you some of those possibilities, covering a variety of topics,
    and hopefully has given you the inspiration to find many new innovative solutions.
  prefs: []
  type: TYPE_NORMAL
