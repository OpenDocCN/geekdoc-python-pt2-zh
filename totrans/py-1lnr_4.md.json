["```py\nfrom sklearn.linear_model import LinearRegression\n\nimport numpy as np\n\n## Data (Apple stock prices)\n\napple = np.array([155, 156, 157])\n\nn = len(apple)\n\n## One-liner\n\nmodel = LinearRegression().fit(np.arange(n).reshape((n,1)), apple)\n\n## Result & puzzle\n\nprint(model.predict([[3],[4]]))\n```", "```py\n[<training_data_1>,\n\n<training_data_2>,\n\n--snip--\n\n<training_data_n>]\n```", "```py\n<training_data> = [feature_1, feature_2, ..., feature_k]\n```", "```py\n[[0],\n\n [1],\n\n [2]]\n```", "```py\nprint(model.predict([[3],[4]]))\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\nimport numpy as np\n\n## Data (#cigarettes, cancer)\n\nX = np.array([[0, \"No\"],\n\n              [10, \"No\"],\n\n              [60, \"Yes\"],\n\n              [90, \"Yes\"]])\n\n## One-liner\n\nmodel = LogisticRegression().fit(X[:,0].reshape(n,1), X[:,1])\n\n## Result & puzzle\n\nprint(model.predict([[2],[12],[13],[40],[90]]))\n```", "```py\n[[0],\n\n [10],\n\n [60],\n\n [90]]\n```", "```py\n# ['No' 'No' 'Yes' 'Yes' 'Yes']\n```", "```py\nfor i in range(20):\n\n    print(\"x=\" + str(i) + \" --> \" + str(model.predict_proba([[i]])))\n```", "```py\nx=0 --> [[0.67240789 0.32759211]]\n\nx=1 --> [[0.65961501 0.34038499]]\n\nx=2 --> [[0.64658514 0.35341486]]\n\nx=3 --> [[0.63333374 0.36666626]]\n\nx=4 --> [[0.61987758 0.38012242]]\n\nx=5 --> [[0.60623463 0.39376537]]\n\nx=6 --> [[0.59242397 0.40757603]]\n\nx=7 --> [[0.57846573 0.42153427]]\n\nx=8 --> [[0.56438097 0.43561903]]\n\nx=9 --> [[0.55019154 0.44980846]]\n\nx=10 --> [[0.53591997 0.46408003]]\n\nx=11 --> [[0.52158933 0.47841067]]\n\nx=12 --> [[0.50722306 0.49277694]]\n\nx=13 --> [[0.49284485 0.50715515]]\n\nx=14 --> [[0.47847846 0.52152154]]\n\nx=15 --> [[0.46414759 0.53585241]]\n\nx=16 --> [[0.44987569 0.55012431]]\n\nx=17 --> [[0.43568582 0.56431418]]\n\nx=18 --> [[0.42160051 0.57839949]]\n\nx=19 --> [[0.40764163 0.59235837]]\n```", "```py\nInitialize random cluster centers (centroids).\n\nRepeat until convergence\n\n    Assign every data point to its closest cluster center.\n\nRecompute each cluster center as the centroid of all data points assigned to it.\n```", "```py\n## Dependencies\n\nfrom sklearn.cluster import KMeans\n\nimport numpy as np\n\n## Data (Work (h) / Salary ($))\n\nX = np.array([[35, 7000], [45, 6900], [70, 7100],\n\n              [20, 2000], [25, 2200], [15, 1800]])\n\n## One-liner\n\nkmeans = KMeans(n_clusters=2).fit(X)\n\n## Result & puzzle\n\ncc = kmeans.cluster_centers_\n\nprint(cc)\n```", "```py\n## One-liner\n\nkmeans = KMeans(n_clusters=2).fit(X)\n```", "```py\ncc = kmeans.cluster_centers_\n\nprint(cc)\n```", "```py\n## Result & puzzle\n\ncc = kmeans.cluster_centers_\n\nprint(cc)\n\n'''\n\n[[  50\\. 7000.]\n\n [  20\\. 2000.]]\n\n'''\n```", "```py\n## Dependencies\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport numpy as np\n\n## Data (House Size (square meters) / House Price ($))\n\nX = np.array([[35, 30000], [45, 45000], [40, 50000],\n\n              [35, 35000], [25, 32500], [40, 40000]])\n\n## One-liner\n\nKNN = KNeighborsRegressor(n_neighbors=3).fit(X[:,0].reshape(-1,1), X[:,1])\n\n## Result & puzzle\n\nres = KNN.predict([[30]])\n\nprint(res)\n```", "```py\nKNN = KNeighborsRegressor(n_neighbors=3).fit(X[:,0].reshape(-1,1), X[:,1])\n```", "```py\nprint(X[:,0])\n\n\"[35 45 40 35 25 40]\"\n\nprint(X[:,0].reshape(-1,1))\n\n\"\"\"\n\n[[35]\n\n [45]\n\n [40]\n\n [35]\n\n [25]\n\n [40]]\n\n\"\"\"\n```", "```py\n## Dependencies\n\nfrom sklearn.neural_network import MLPRegressor\n\nimport numpy as np\n\n## Questionaire data (WEEK, YEARS, BOOKS, PROJECTS, EARN, RATING)\n\nX = np.array(\n\n    [[20,  11,  20,  30,  4000,  3000], \n\n     [12,   4,   0,   0, 1000,  1500],\n\n     [2,   0,   1,  10,   0,  1400],\n\n     [35,   5,  10,  70,  6000,  3800],\n\n     [30,   1,   4,  65,   0,  3900],\n\n     [35,   1,   0,   0,   0, 100],\n\n     [15,   1,   2,  25,   0,  3700],\n\n     [40,   3,  -1,  60,  1000,  2000],\n\n     [40,   1,   2,  95,   0,  1000],\n\n     [10,   0,   0,   0,   0,  1400],\n\n     [30,   1,   0,  50,   0,  1700],\n\n     [1,   0,   0,  45,   0,  1762],\n\n     [10,  32,  10,   5,   0,  2400],\n\n     [5,  35,   4,   0, 13000,  3900],\n\n     [8,   9,  40,  30,  1000,  2625],\n\n     [1,   0,   1,   0,   0,  1900],\n\n     [1,  30,  10,   0,  1000,  1900],\n\n     [7,  16,   5,   0,   0,  3000]])\n\n## One-liner\n\nneural_net = MLPRegressor(max_iter=10000).fit(X[:,:-1], X[:,-1])\n\n## Result\n\nres = neural_net.predict([[0, 0, 0, 0, 0]])\n\nprint(res)\n```", "```py\n## Result\n\nres = neural_net.predict([[0, 0, 0, 0, 0]])\n\nprint(res)\n\n# [94.94925927]\n```", "```py\n## Result\n\nres = neural_net.predict([[20, 0, 0, 0, 0]])\n\nprint(res)\n\n# [440.40167562]\n```", "```py\n## Result\n\nres = neural_net.predict([[20, 0, 10, 0, 0]])\n\nprint(res)\n\n# [953.6317602]\n```", "```py\n## Result\n\nres = neural_net.predict([[20, 1, 10, 0, 0]])\n\nprint(res)\n\n# [999.94308353]\n```", "```py\n## Result\n\nres = neural_net.predict([[20, 1, 10, 50, 1000]])\n\nprint(res)\n\n# [1960.7595547]\n```", "```py\n## Dependencies\n\nfrom sklearn import tree\n\nimport numpy as np\n\n## Data: student scores in (math, language, creativity) --> study field\n\nX = np.array([[9, 5, 6, \"computer science\"],\n\n              [1, 8, 1, \"linguistics\"],\n\n              [5, 7, 9, \"art\"]])\n\n## One-liner\n\nTree = tree.DecisionTreeClassifier().fit(X[:,:-1], X[:,-1])\n\n## Result & puzzle\n\nstudent_0 = Tree.predict([[8, 6, 5]])\n\nprint(student_0)\n\nstudent_1 = Tree.predict([[3, 7, 9]])\n\nprint(student_1)\n```", "```py\n## Dependencies\n\nimport numpy as np\n\n## Data (rows: stocks / cols: stock prices)\n\nX = np.array([[25,27,29,30],\n\n              [1,5,3,2],\n\n              [12,11,8,3],\n\n              [1,1,2,2],\n\n              [2,6,2,2]])\n\n## One-liner\n\n# Find the stock with smallest variance\n\nmin_row = min([(i,np.var(X[i,:])) for i in range(len(X))], key=lambda x: x[1])\n\n## Result & puzzle\n\nprint(\"Row with minimum variance: \" + str(min_row[0]))\n\nprint(\"Variance: \" + str(min_row[1]))\n```", "```py\n\"\"\"\n\nRow with minimum variance: 3\n\nVariance: 0.25\n\n\"\"\"\n```", "```py\nvar = np.var(X, axis=1)\n\nmin_row = (np.where(var==min(var)), min(var))\n```", "```py\nimport numpy as np\n\nX = np.array([[1, 3, 5],\n\n              [1, 1, 1],\n\n              [0, 2, 4]])\n\nprint(np.average(X))\n\n# 2.0\n\nprint(np.var(X))\n\n# 2.4444444444444446\n\nprint(np.std(X))\n\n# 1.5634719199411433\n```", "```py\n## Dependencies\n\nimport numpy as np\n\n## Stock Price Data: 5 companies\n\n# (row=[price_day_1, price_day_2, ...])\n\nx = np.array([[8, 9, 11, 12],\n\n              [1, 2, 2, 1], \n\n              [2, 8, 9, 9],\n\n              [9, 6, 6, 3],\n\n              [3, 3, 3, 3]])\n\n## One-liner\n\navg, var, std = np.average(x, axis=1), np.var(x, axis=1), np.std(x, axis=1)\n\n## Result & puzzle\n\nprint(\"Averages: \" + str(avg))\n\nprint(\"Variances: \" + str(var))\n\nprint(\"Standard Deviations: \" + str(std))\n```", "```py\n\"\"\"\n\nAverages: [10.   1.5  7.   6.   3\\. ]\n\nVariances: [2.5  0.25 8.5  4.5  0.  ]\n\nStandard Deviations: [1.58113883 0.5   2.91547595 2.12132034 0.   ]\n\n\"\"\"\n```", "```py\nimport numpy as np\n\nx = np.array([[[1,2], [1,1]],\n\n              [[1,1], [2,1]],\n\n              [[1,0], [0,0]]])\n\nprint(np.average(x, axis=2))\n\nprint(np.var(x, axis=2))\n\nprint(np.std(x, axis=2))\n\n\"\"\"\n\n[[1.5 1\\. ]\n\n [1.  1.5]\n\n [0.5 0\\. ]]\n\n[[0.25 0.  ]\n\n [0.   0.25]\n\n [0.25 0.  ]]\n\n[[0.5 0\\. ]\n\n [0.  0.5]\n\n [0.5 0\\. ]]\n\n\"\"\"\n```", "```py\n## Dependencies\n\nfrom sklearn import svm\n\nimport numpy as np\n\n## Data: student scores in (math, language, creativity) --> study field\n\nX = np.array([[9, 5, 6, \"computer science\"],\n\n              [10, 1, 2, \"computer science\"],\n\n              [1, 8, 1, \"literature\"],\n\n              [4, 9, 3, \"literature\"],\n\n              [0, 1, 10, \"art\"],\n\n              [5, 7, 9, \"art\"]])\n\n## One-liner\n\nsvm = svm.SVC().fit(X[:,:-1], X[:,-1])\n\n## Result & puzzle\n\nstudent_0 = svm.predict([[3, 3, 6]])\n\nprint(student_0)\n\nstudent_1 = svm.predict([[8, 1, 1]])\n\nprint(student_1)\n```", "```py\n## Result & puzzle\n\nstudent_0 = svm.predict([[3, 3, 6]])\n\nprint(student_0)\n\n# ['art']\n\nstudent_1 = svm.predict([[8, 1, 1]])\n\nprint(student_1)\n\n## ['computer science']\n```", "```py\n## Dependencies\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Data: student scores in (math, language, creativity) --> study field\n\nX = np.array([[9, 5, 6, \"computer science\"],\n\n              [5, 1, 5, \"computer science\"],\n\n              [8, 8, 8, \"computer science\"],\n\n              [1, 10, 7, \"literature\"],\n\n              [1, 8, 1, \"literature\"],\n\n              [5, 7, 9, \"art\"],\n\n              [1, 1, 6, \"art\"]])\n\n## One-liner\n\nForest = RandomForestClassifier(n_estimators=10).fit(X[:,:-1], X[:,-1])\n\n## Result\n\nstudents = Forest.predict([[8, 6, 5],\n\n                           [3, 7, 9],\n\n                           [2, 2, 1]])\n\nprint(students)\n```", "```py\n## Result\n\nstudents = Forest.predict([[8, 6, 5],\n\n                           [3, 7, 9],\n\n                           [2, 2, 1]])\n\nprint(students)\n\n# ['computer science' 'art' 'art']\n```"]