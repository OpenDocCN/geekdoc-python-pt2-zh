["```py\n$ `dmesg -w`\n\n```", "```py\nimport os\nimport pathlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\nimport scipy.signal\nfrom scipy.io import wavfile\nimport glob\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import applications\n\n```", "```py\n# set seed for random functions\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\n```", "```py\nglob.glob(data_dir + '/*')\n\n['data/mini_speech_commands/up',\n\n 'data/mini_speech_commands/no',\n\n 'data/mini_speech_commands/README.md',\n\n 'data/mini_speech_commands/stop',\n\n 'data/mini_speech_commands/left',\n\n 'data/mini_speech_commands/right',\n\n 'data/mini_speech_commands/go',\n\n 'data/mini_speech_commands/down',\n\n 'data/mini_speech_commands/yes']\n\nYou pass `glob` the `'/*'` pattern to list all the first-level directories within the *data* directory (`*` is a wildcard character). The output shows you that the dataset comes with a *README.md* text file and eight subdirectories for the eight speech commands you’ll be training the model to identify. For convenience, you create a list of the commands:\n\n```", "```py\n\nIn your machine learning model, you’ll be matching audio samples to a `label_id` integer denoting one of the commands. These integers will correspond to the indices from the `commands` list. For example, a `label_id` of `0` indicates `'up'`, and a `label_id` of `6` indicates `'down'`.\nNow take a look at what’s in those subdirectories:\n\n```", "```py\nfilepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav'\nrate, data = wavfile.read(filepath)\n❶ plt.plot(data)\n\n```", "```py\n\n```", "```py\n❶ padded_data = np.zeros((16000,), dtype=np.int16)\n❷ padded_data[:data.shape[0]] = data\n❸ norm_data = np.array(padded_data/32768.0, dtype=np.float32)\nplt.plot(norm_data)\n\n```", "```py\nfilepath = 'data/mini_speech_commands/yes/00f0204f_nohash_0.wav'\nrate, data = wavfile.read(filepath)\n❶ f, t, spec = scipy.signal.stft(data, fs=16000, nperseg=255,\n                               noverlap = 124, nfft=256)\n❷ spec = np.abs(spec)\nprint(\"spec: min = {}, max = {}, shape = {}, dtype = {}\".format(np.min(spec),\n                                       np.max(spec), spec.shape, spec.dtype))\n❸ X = t * 129*124\n❹ plt.pcolormesh(X, f, spec)\n\nspec: min = 0.0, max = 2089.085693359375, shape = (129, 124), dtype = float32\n\n![](images/nsp-venkitachalam503045-g15003.jpg)\n\nYou pick an arbitrary WAV file from the *yes* subdirectory and extract its data using the `wavfile` module from `scipy`, as before. Then you use the `scipy.signal.stft()` function to compute the spectrogram of the data ❶. In this function call, `fs` is the sampling rate, `nperseg` is the length of each segment, and `noverlap` is the number of overlapping samples between consecutive segments. The `stft()` function returns a tuple comprising three members: `f`, an array of frequencies; `t`, an array of the time intervals mapped to the range [0.0, 1.0]; and `spec`, the STFT itself, a grid of 129×124 complex numbers (these dimensions are given as `shape` in the output). You use `np.abs()` to convert the complex numbers in `spec` into real numbers ❷. Then you print some information about the computed spectrogram. Next, you create an array `X` to hold the sample numbers corresponding to the time intervals ❸. You get these by multiplying `t` by the dimensions of the grid. Finally, you use the `pcolormesh()` method to plot the grid in `spec`, using the values in `X` as the grid’s x-axis and the values in `f` as the y-axis ❹.\nThe output shows the spectrogram. This 129×124 grid of values (an image), and many more like it, will be the input for the neural network. The bright spots around 1,000 Hz and lower, starting around 4,000 samples in, are where the frequency content is most prominent, while darker areas represent less prominent frequencies.\nNOTE Notice that the y-axis in the spectrogram images goes up to only about 8,000 Hz. This is a consequence of the *sampling theorem* in digital signal processing, which states that the maximum frequency that can be accurately measured in a digitally sampled signal is half the sampling rate. In this case, that maximum frequency works out to 16,000/2 = 8,000 Hz.\n\n```", "```py\ndef stft(x):\n  ❶ f, t, spec = scipy.signal.stft(x.numpy(), fs=16000, nperseg=255,\n                                   noverlap=124, nfft=256)\n  ❷ return tf.convert_to_tensor(np.abs(spec))\n\n```", "```py\ndef get_spec_label_pair(filepath):\n    # read WAV file\n    file_data = tf.io.read_file(filepath)\n    data, rate = tf.audio.decode_wav(file_data)\n    data = tf.squeeze(data, axis=-1)\n    # add zero padding for N < 16000\n  ❶ zero_padding = tf.zeros([16000] - tf.shape(data), dtype=tf.float32)\n    # combine data with zero padding\n  ❷ padded_data = tf.concat([data, zero_padding], 0)\n    # compute spectrogram\n  ❸ spec = tf.py_function(func=stft, inp=[padded_data], Tout=tf.float32)\n    spec.set_shape((129, 124))\n    spec = tf.expand_dims(spec, -1)\n    # get label\n  ❹ cmd = tf.strings.split(filepath, os.path.sep)[-2]\n  ❺ label_id = tf.argmax(tf.cast(cmd == commands, \"uint32\"))\n    # return tuple\n    return (spec, label_id)\n\n```", "```py\ntrain_files = wav_file_names[:6400]\nval_files = wav_file_names[6400:7200]\ntest_files = wav_file_names[7200:]\n\n```", "```py\ntrain_ds = tf.data.Dataset.from_tensor_slices(train_files)\nval_ds = tf.data.Dataset.from_tensor_slices(val_files)\ntest_ds = tf.data.Dataset.from_tensor_slices(test_files)\n\n```", "```py\ntrain_ds = train_ds.map(get_spec_label_pair)\nval_ds = val_ds.map(get_spec_label_pair)\ntest_ds = test_ds.map(get_spec_label_pair)\n\n```", "```py\nbatch_size = 64\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)\n\n```", "```py\n❶ input_shape = (129, 124, 1)\n❷ num_labels = 8\nnorm_layer = preprocessing.Normalization()\n❸ norm_layer.adapt(train_ds.map(lambda x, _: x))\n❹ model = `models`.Sequential([\n    layers.Input(shape=input_shape),\n    preprocessing.Resizing(32, 32),\n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n❺ model.summary()\n\nModel: \"sequential_3\"\n\n_____________________________________________________________________\n\n Layer (type)                      Output Shape            Param #\n\n=====================================================================\n\n resizing_3 (Resizing)            (None, 32, 32, 1)        0\n\n normalization_3 (Normalization)  (None, 32, 32, 1)        3\n\n conv2d_5 (Conv2D)                (None, 30, 30, 32)       320\n\n conv2d_6 (Conv2D)                (None, 28, 28, 64)       18496\n\n max_pooling2d_3 (MaxPooling2D)   (None, 14, 14, 64)       0\n\n dropout_6 (Dropout)              (None, 14, 14, 64)       0\n\n flatten_3 (Flatten)              (None, 12544)            0\n\n dense_6 (Dense)                  (None, 128)              1605760\n\n dropout_7 (Dropout)              (None, 128)              0\n\n dense_7 (Dense)                  (None, 8)                1032\n\n=====================================================================\n\nTotal params: 1,625,611\n\nTrainable params: 1,625,608\n\nNon-trainable params: 3\n\nYou set the shape of the input into the first layer of the model ❶ and then set the number of labels ❷, which will be the number of units in the model’s output layer. Next, you set up a normalization layer for the spectrogram data. This will scale and shift the data to a distribution centered on 1 with a standard deviation of 1\\. This is a common practice in ML that improves training. Don’t let the `lambda` scare you ❸. All it’s doing is defining an anonymous function that picks out just the spectrogram from each `(spec, label_id)` pair in the training dataset. The `x, _: x` is just saying to ignore the second element in the pair and return only the first element.\nYou next create the neural network model, one layer at a time ❹. The layers correspond to the architecture we viewed earlier in [Figure 15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1). Finally, you print out a summary of the model ❺, which is shown in the output. The summary tells you all the layers in the model, the shape of the output tensor at each stage, and the number of trainable parameters in each layer.\nNow you need to compile the model. The compilation step sets the optimizer, the loss function, and the data collection metrics for the model:\n\n```", "```py\n\nA *loss function* is a function used to measure how well a neural network is doing by comparing the output of the model to the known correct training data. An *optimizer* is a method used to adjust the trainable parameters in a model to reduce the losses. In this case, you’re using an optimizer of type `Adam` and a loss function of type `SparseCategoricalCrossentropy`. You also get set up to collect some accuracy metrics, which you’ll use to check how the training went.\nNext, you train the model:\n\nEPOCHS = 10\n\nhistory = model.fit(\n\n    train_ds,\n\n    validation_data=val_ds,\n\n    epochs=EPOCHS,\n\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=15), ❶\n\n)\n\nEpoch 1/10\n\n100/100 [==============================] - 38s 371ms/step - loss: 1.7219 - accuracy: 0.3700\n\n                                         - val_loss: 1.2672 - val_accuracy: 0.5763\n\nEpoch 2/10\n\n100/100 [==============================] - 37s 368ms/step - loss: 1.1791 - accuracy: 0.5756\n\n                                         - val_loss: 0.9616 - val_accuracy: 0.6650\n\n--snip--\n\nEpoch 10/10\n\n100/100 [==============================] - 39s 388ms/step - loss: 0.3897 - accuracy: 0.8639\n\n                                         - val_loss: 0.4766 - val_accuracy: 0.8450\n\nYou train the model by passing the training dataset `train_ds` into `model.fit()`. You also specify the validation dataset `val_ds`, which is used to evaluate how accurate the model is. The training takes place over 10 *epochs*. During each epoch, the complete set of training data is shown to the neural network. The data is randomly shuffled each time, so training across multiple epochs allows the model to learn better. You use the `callback` option ❶ to set up a function to exit the training if it turns out that the training loss is no longer decreasing with each epoch.\nRunning this Colab cell will take some time. The progress will be shown on the screen as the training is in process. Looking at the output, the `val_accuracy` listed under Epoch 10 shows that the model was about 85 percent accurate at running inference on the validation data by the end of the training. (The `val_accuracy` metric corresponds to the validation data, while `accuracy` corresponds to the training data.)\nNow you can try the model by running inference on the testing portion of the data:\n\n```", "```py\n\n```", "```py\nmodel.save('audioml.sav')\n\n```", "```py\n❶ converter = tf.lite.TFLiteConverter.from_saved_model('audioml.sav')\n❷ tflite_model = converter.convert()\n❸ with open('audioml.tflite', 'wb') as f:\n    f.write(tflite_model)\n\n```", "```py\nfrom google.colab import files\nfiles.download('audioml.tflite')\n\n```", "```py\nfrom scipy.io import wavfile\nfrom scipy import signal\nimport numpy as np\nimport argparse\nimport pyaudio\nimport wave\nimport time\nfrom tflite_runtime.interpreter import Interpreter\nfrom multiprocessing import Process, Queue\n\n```", "```py\nVERBOSE_DEBUG = False\nCHUNK = 4000\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nSAMPLE_RATE = 16000\nRECORD_SECONDS = 1\nNCHUNKS = int((SAMPLE_RATE * RECORD_SECONDS) / CHUNK)\nND = 2 * CHUNK * NCHUNKS\nNDH = ND // 2\n# device index of microphone\n❶ dev_index = -1\n\n```", "```py\ndef list_devices():\n    \"\"\"list pyaudio devices\"\"\"\n    # initialize pyaudio\n  ❶ p = pyaudio.PyAudio()\n    # get device list\n    index = None\n  ❷ nDevices = p.get_device_count()\n    print('\\naudioml.py:\\nFound the following input devices:')\n  ❸ for i in range(nDevices):\n        deviceInfo = p.get_device_info_by_index(i)\n        if deviceInfo['maxInputChannels'] > 0:\n            print(deviceInfo['index'], deviceInfo['name'],\n                  deviceInfo['defaultSampleRate'])\n    # clean up\n  ❹ p.terminate()\n\n```", "```py\naudioml.py:\nFound the following input devices:\n1 Mico: USB Audio (hw:3,0) 16000.0\n\n```", "```py\ndef get_live_input(interpreter):\n    # create a queue object\n  ❶ dataq = Queue()\n    # start inference process\n  ❷ proc = Process(target = inference_process, args=(dataq, interpreter))\n    proc.start()\n\n```", "```py\n    # initialize pyaudio\n  ❶ p = pyaudio.PyAudio()\n    print('opening stream...')\n  ❷ stream = p.open(format = FORMAT,\n                    channels = CHANNELS,\n                    rate = SAMPLE_RATE,\n                    input = True,\n                    frames_per_buffer = CHUNK,\n                    input_device_index = dev_index)\n    # discard first 1 second\n  ❸ for i in range(0, NCHUNKS):\n        data = stream.read(CHUNK, exception_on_overflow = False)\n\n```", "```py\n    # count for gathering two frames at a time\n  ❶ count = 0\n  ❷ inference_data = np.zeros((ND,), dtype=np.int16)\n    print(\"Listening...\")\n    try:\n      ❸ while True:\n            chunks = []\n          ❹ for i in range(0, NCHUNKS):\n                data = stream.read(CHUNK, exception_on_overflow = False)\n                chunks.append(data)\n            # process data\n            buffer = b''.join(chunks)\n          ❺ audio_data = np.frombuffer(buffer, dtype=np.int16)\n\n```", "```py\n            if count == 0:\n                # set first half\n              ❶ inference_data[:NDH] = audio_data\n                count += 1\n            elif count == 1:\n                # set second half\n              ❷ inference_data[NDH:] = audio_data\n                # add data to queue\n              ❸ dataq.put(inference_data)\n                count += 1\n            else:\n                # move second half to first half\n              ❹ inference_data[:NDH] = inference_data[NDH:]\n                # set second half\n              ❺ inference_data[NDH:] = audio_data\n                # add data to queue\n              ❻ dataq.put(inference_data)\n\n```", "```py\n    except KeyboardInterrupt:\n        print(\"exiting...\")\n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n```", "```py\ndef process_audio_data(waveform):\n    # compute peak to peak based on scaling by max 16-bit value\n  ❶ PTP = np.ptp(waveform / 32768.0)\n    # return None if too silent\n  ❷ if PTP < 0.3:\n        return []\n\n```", "```py\n    # normalize audio\n    wabs = np.abs(waveform)\n    wmax = np.max(wabs)\n  ❶ waveform = waveform / wmax\n    # compute peak to peak based on normalized waveform\n  ❷ PTP = np.ptp(waveform)\n    # scale and center\n  ❸ waveform = 2.0*(waveform - np.min(waveform))/PTP – 1\n\n```", "```py\n    # extract 16000 len (1 second) of data\n  ❶ max_index = np.argmax(waveform)\n  ❷ start_index = max(0, max_index-8000)\n  ❸ end_index = min(max_index+8000, waveform.shape[0])\n  ❹ waveform = waveform[start_index:end_index]\n    # padding for files with less than 16000 samples\n    waveform_padded = np.zeros((16000,))\n    waveform_padded[:waveform.shape[0]] = waveform\n    return waveform_padded\n\n```", "```py\ndef get_spectrogram(waveform):\n  ❶ waveform_padded = process_audio_data(waveform)\n  ❷ if not len(waveform_padded):\n        return []\n    # compute spectrogram\n  ❸ f, t, Zxx = signal.stft(waveform_padded, fs=16000, nperseg=255,\n        noverlap = 124, nfft=256)\n    # output is complex, so take abs value\n  ❹ spectrogram = np.abs(Zxx)\n    return spectrogram\n\n```", "```py\ndef inference_process(dataq, interpreter):\n    success = False\n    while True:\n      ❶ if not dataq.empty():\n            # get data from queue\n          ❷ inference_data = dataq.get()\n            # run inference only if previous one was not successful\n          ❸ if not success:\n                success = run_inference(inference_data, interpreter)\n            else:\n                # skipping, reset flag for next time\n              ❹ success = False\n\n```", "```py\ndef run_inference(waveform, interpreter):\n    # get spectrogram data\n  ❶ spectrogram = get_spectrogram(waveform)\n    if not len(spectrogram):\n        return False\n    # get input and output tensors details\n  ❷ input_details = interpreter.get_input_details()\n  ❸ output_details = interpreter.get_output_details()\n\n```", "```py\n    # set input\n  ❶ input_data = spectrogram.astype(np.float32)\n  ❷ interpreter.set_tensor(input_details[0]['index'], input_data)\n    # run interpreter\n    print(\"running inference...\")\n  ❸ interpreter.invoke()\n    # get output\n  ❹ output_data = interpreter.get_tensor(output_details[0]['index'])\n  ❺ yvals = output_data[0]\n    print(yvals)\n\n```", "```py\n    # Important! This should exactly match training labels/ids.\n    commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']\n    print(\">>> \" + commands[np.argmax(output_data[0])].upper())\n\n```", "```py\ndef main():\n    # globals set in this function\n    global VERBOSE_DEBUG\n    # create parser\n    descStr = \"This program does ML inference on audio data.\"\n    parser = argparse.ArgumentParser(description=descStr)\n    # add a mutually exclusive group\n  ❶ group = parser.add_mutually_exclusive_group(required=True)\n    # add mutually exclusive arguments\n  ❷ group.add_argument('--list', action='store_true', required=False)\n  ❸ group.add_argument('--input', dest='wavfile_name', required=False)\n  ❹ group.add_argument('--index', dest='index', required=False)\n    # add other arguments\n  ❺ parser.add_argument('--verbose', action='store_true', required=False)\n    # parse args\n    args = parser.parse_args()\n\n```", "```py\n    # load TF Lite model\n    interpreter = Interpreter('audioml.tflite')\n    interpreter.allocate_tensors()\n\n```", "```py\n    # check verbose flag\n    if args.verbose:\n        VERBOSE_DEBUG = True\n    # test WAV file\n    if args.wavfile_name:\n      ❶ wavfile_name = args.wavfile_name\n        # get audio data\n      ❷ rate, waveform = wavfile.read(wavfile_name)\n        # run inference\n      ❸ run_inference(waveform, interpreter)\n    elif args.list:\n        # list devices\n      ❹ list_devices()\n    else:\n        # store device index\n      ❺ dev_index = int(args.index)\n        # get live audio\n      ❻ get_live_input(interpreter)\n    print(\"done.\")\n\n```", "```py\n$ `sudo python audioml.py --input right.wav`\n\n```", "```py\nrunning inference...\n[  6.640185  -26.032831  -26.028618    8.746256   62.545185   -0.5698182\n -15.045679  -29.140179 ]\n❶ >>> RIGHT\nrun_inference: 0.029174549999879673s\ndone.\n\n```", "```py\n$ `sudo python audioml.py --list`\n\n```", "```py\naudioml.py:\nFound the following input devices:\n1 Mico: USB Audio (hw:3,0) 16000.0\ndone.\n\n```", "```py\n$ `sudo python audioml.py --index 1`\n--`snip`--\nopening stream...\nListening...\nrunning inference...\n[-2.647918    0.17592785 -3.3615346   6.6812882   4.472283   -3.7535028\n  1.2349942   1.8546474 ]\n❶ >>> LEFT\nrun_inference: 0.03520956500142347s\nrunning inference...\n[-2.7683923 -5.9614644 -8.532391   6.906795  19.197264  -4.0255833\n  1.7236844 -4.374415 ]\n❷ >>> RIGHT\nrun_inference: 0.03026762299850816s\n--`snip`--\n^C\nKeyboardInterrupt\nexiting...\ndone.\n\n```", "```py\n\"\"\"\n    simple_audio.py\n    This programs collects audio data from an I2S mic on the Raspberry Pi\n    and runs the TensorFlow Lite interpreter on a per-build model.\n    Author: Mahesh Venkitachalam\n\"\"\"\nfrom scipy.io import wavfile\nfrom scipy import signal\nimport numpy as np\nimport argparse\nimport pyaudio\nimport wave\nimport time\nfrom tflite_runtime.interpreter import Interpreter\nfrom multiprocessing import Process, Queue\nVERBOSE_DEBUG = False\nCHUNK = 4000                # choose a value divisible by SAMPLE_RATE\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nSAMPLE_RATE = 16000\nRECORD_SECONDS = 1\nNCHUNKS = int((SAMPLE_RATE * RECORD_SECONDS) / CHUNK)\nND = 2 * SAMPLE_RATE * RECORD_SECONDS\nNDH = ND // 2\n# device index of microphone\ndev_index = -1\ndef list_devices():\n    \"\"\"list pyaudio devices\"\"\"\n    # initialize pyaudio\n    p = pyaudio.PyAudio()\n    # get device list\n    index = None\n    nDevices = p.get_device_count()\n    print('\\naudioml.py:\\nFound the following input devices:')\n    for i in range(nDevices):\n        deviceInfo = p.get_device_info_by_index(i)\n        if deviceInfo['maxInputChannels'] > 0:\n            print(deviceInfo['index'], deviceInfo['name'], deviceInfo['defaultSampleRate'])\n    # clean up\n    p.terminate()\ndef inference_process(dataq, interpreter):\n    \"\"\"infererence process handler\"\"\"\n    success = False\n    while True:\n        if not dataq.empty():\n            # get data from queue\n            inference_data = dataq.get()\n            # run inference only if previous one was not success\n            # otherwise we will get duplicate results because of\n            # overlap in input data\n            if not success:\n                success = run_inference(inference_data, interpreter)\n            else:\n                # skipping, reset flag for next time\n                success = False\ndef process_audio_data(waveform):\n    \"\"\"Process audio input.\n    This function takes in raw audio data from a WAV file and does scaling\n    and padding to 16000 length.\n    \"\"\"\n    if VERBOSE_DEBUG:\n        print(\"waveform:\", waveform.shape, waveform.dtype, type(waveform))\n        print(waveform[:5])\n    # compute peak to peak based on scaling by max 16-bit value\n    PTP = np.ptp(waveform / 32768.0)\n    if VERBOSE_DEBUG:\n        print(\"peak-to-peak (16 bit scaling): {}\".format(PTP))\n    # return None if too silent\n    if PTP < 0.3:\n        return []\n    # normalize audio\n    wabs = np.abs(waveform)\n    wmax = np.max(wabs)\n    waveform = waveform / wmax\n    # compute peak to peak based on normalized waveform\n    PTP = np.ptp(waveform)\n    if VERBOSE_DEBUG:\n        print(\"peak-to-peak (after normalize): {}\".format(PTP))\n        print(\"After normalization:\")\n        print(\"waveform:\", waveform.shape, waveform.dtype, type(waveform))\n        print(waveform[:5])\n    # scale and center\n    waveform = 2.0*(waveform - np.min(waveform))/PTP - 1\n    # extract 16000 len (1 second) of data\n    max_index = np.argmax(waveform)\n    start_index = max(0, max_index-8000)\n    end_index = min(max_index+8000, waveform.shape[0])\n    waveform = waveform[start_index:end_index]\n    # padding for files with less than 16000 samples\n    if VERBOSE_DEBUG:\n        print(\"After padding:\")\n    waveform_padded = np.zeros((16000,))\n    waveform_padded[:waveform.shape[0]] = waveform\n    if VERBOSE_DEBUG:\n        print(\"waveform_padded:\", waveform_padded.shape,\n               waveform_padded.dtype, type(waveform_padded))\n        print(waveform_padded[:5])\n    return waveform_padded\ndef get_spectrogram(waveform):\n    \"\"\"computes spectrogram from audio data\"\"\"\n    waveform_padded = process_audio_data(waveform)\n    if not len(waveform_padded):\n        return []\n    # compute spectrogram\n    f, t, Zxx = signal.stft(waveform_padded, fs=16000, nperseg=255,\n        noverlap = 124, nfft=256)\n    # output is complex, so take abs value\n    spectrogram = np.abs(Zxx)\n    if VERBOSE_DEBUG:\n        print(\"spectrogram:\", spectrogram.shape, type(spectrogram))\n        print(spectrogram[0, 0])\n    return spectrogram\ndef run_inference(waveform, interpreter):\n    # start timing\n    start = time.perf_counter()\n    # get spectrogram data\n    spectrogram = get_spectrogram(waveform)\n    if not len(spectrogram):\n        if VERBOSE_DEBUG:\n            print(\"Too silent. Skipping...\")\n        return False\n    if VERBOSE_DEBUG:\n        print(\"spectrogram: %s, %s, %s\" % (type(spectrogram),\n               spectrogram.dtype, spectrogram.shape))\n    # get input and output tensors details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if VERBOSE_DEBUG:\n        print(\"input_details: {}\".format(input_details))\n        print(\"output_details: {}\".format(output_details))\n    # reshape spectrogram to match interpreter requirement\n    spectrogram = np.reshape(spectrogram, (-1, spectrogram.shape[0],\n                                           spectrogram.shape[1], 1))\n    # set input\n    input_data = spectrogram.astype(np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    # run interpreter\n    print(\"running inference...\")\n    interpreter.invoke()\n    # get output\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    yvals = output_data[0]\n    if VERBOSE_DEBUG:\n        print(output_data)\n    print(yvals)\n    # Important! This should exactly match training labels/ids.\n    commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']\n    print(\">>> \" + commands[np.argmax(output_data[0])].upper())\n    # stop timing\n    end = time.perf_counter()\n    print(\"run_inference: {}s\".format(end - start))\n    # return success\n    return True\ndef get_live_input(interpreter):\n    \"\"\"this function gets live input from the microphone\n    and runs inference on it\"\"\"\n    # create a queue object\n    dataq = Queue()\n    # start inference process\n    proc = Process(target = inference_process, args=(dataq, interpreter))\n    proc.start()\n    # initialize pyaudio\n    p = pyaudio.PyAudio()\n    print('opening stream...')\n    stream = p.open(format = FORMAT,\n                    channels = CHANNELS,\n                    rate = SAMPLE_RATE,\n                    input = True,\n                    frames_per_buffer = CHUNK,\n                    input_device_index = dev_index)\n    # discard first 1 second\n    for i in range(0, NCHUNKS):\n        data = stream.read(CHUNK, exception_on_overflow = False)\n    # count for gathering two frames at a time\n    count = 0\n    inference_data = np.zeros((ND,), dtype=np.int16)\n    print(\"Listening...\")\n    try:\n        while True:\n            # print(\"Listening...\")\n            chunks = []\n            for i in range(0, NCHUNKS):\n                data = stream.read(CHUNK, exception_on_overflow = False)\n                chunks.append(data)\n            # process data\n            buffer = b''.join(chunks)\n            audio_data = np.frombuffer(buffer, dtype=np.int16)\n            if count == 0:\n                # set first half\n                inference_data[:NDH] = audio_data\n                count += 1\n            elif count == 1:\n                # set second half\n                inference_data[NDH:] = audio_data\n                # add data to queue\n                dataq.put(inference_data)\n                count += 1\n            else:\n                # move second half to first half\n                inference_data[:NDH] = inference_data[NDH:]\n                # set second half\n                inference_data[NDH:] = audio_data\n                # add data to queue\n                dataq.put(inference_data)\n            # print(\"queue: {}\".format(dataq.qsize()))\n    except KeyboardInterrupt:\n        print(\"exiting...\")\n    stream.stop_stream()\n    stream.close()\n    p.terminate()\ndef main():\n    \"\"\"main function for the program\"\"\"\n    # globals set in this function\n    global VERBOSE_DEBUG\n    # create parser\n    descStr = \"This program does ML inference on audio data.\"\n    parser = argparse.ArgumentParser(description=descStr)\n    # add a mutually exclusive group\n    group = parser.add_mutually_exclusive_group(required=True)\n    # add mutually exclusive arguments\n    group.add_argument('--list', action='store_true', required=False)\n    group.add_argument('--input', dest='wavfile_name', required=False)\n    group.add_argument('--index', dest='index', required=False)\n    # add other arguments\n    parser.add_argument('--verbose', action='store_true', required=False)\n    # parse args\n    args = parser.parse_args()\n    # load TF Lite model\n    interpreter = Interpreter('audioml.tflite')\n    interpreter.allocate_tensors()\n    # check verbose flag\n    if args.verbose:\n        VERBOSE_DEBUG = True\n    # test WAV file\n    if args.wavfile_name:\n        wavfile_name = args.wavfile_name\n        # get audio data\n        rate, waveform = wavfile.read(wavfile_name)\n        # run inference\n        run_inference(waveform, interpreter)\n    elif args.list:\n        # list devices\n        list_devices()\n    else:\n        # store device index\n        dev_index = int(args.index)\n        # get live audio\n        get_live_input(interpreter)\n    print(\"done.\")\n# main method\nif __name__ == '__main__':\n    main()\n\n```"]