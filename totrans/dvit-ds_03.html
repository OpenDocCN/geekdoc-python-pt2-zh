<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_57" title="57"/>3</span><br/>
<span class="ChapterTitle">Group Comparisons</span></h1>
</header>
<figure class="opener">
<img alt="" height="200" src="image_fi/book_art/chapterart.png" width="200"/>
</figure>
<p class="ChapterIntro">In this chapter, we’ll discuss how to make intelligent comparisons between groups, using examples from business scenarios. We’ll start small by looking at one group alone. We’ll see which descriptive statistics most succinctly describe it, draw plots that capture its essence, and compare various samples from it. We’ll then be ready to reason about samples from two groups. We’ll conclude by looking at statistical significance tests: the t-test and the Mann-Whitney U test.</p>
<h2 id="h1-502888c03-0001"><span epub:type="pagebreak" id="Page_58" title="58"/>Reading Population Data</h2>
<p class="BodyFirst">Let’s start by reading in some data. This data records measurements of 1,034 professional baseball players, including their height, weight, and age at the time of measurement. You can download this data directly from <a class="LinkURL" href="https://bradfordtuckfield.com/mlb.csv">https://bradfordtuckfield.com/mlb.csv</a>. Its original source is the Statistics Online Computational Resource (SOCR) website (<a class="LinkURL" href="https://web.archive.org/web/20220629205951/https://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights">https://web.archive.org/web/20220629205951/https://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights</a>).</p>
<pre><code>import pandas as pd
mlb=pd.read_csv('mlb.csv')
print(mlb.head())
print(mlb.shape)</code></pre>
<p>In this snippet, we import pandas and use its <code>read_csv()</code> method to read in our data. This is all simple data ingestion, just as we did in <span class="xref" itemid="xref_target_Chapters 1 and 2">Chapters 1 and 2</span>. After you run this snippet, you should see the following output:</p>
<pre><code>              name team       position  height  weight    age
0    Adam_Donachie  BAL        Catcher      74   180.0  22.99
1        Paul_Bako  BAL        Catcher      74   215.0  34.69
2  Ramon_Hernandez  BAL        Catcher      72   210.0  30.78
3     Kevin_Millar  BAL  First_Baseman      72   210.0  35.43
4      Chris_Gomez  BAL  First_Baseman      73   188.0  35.71
(1034, 6)</code></pre>
<p>The last line of the output shows the <em>shape</em> of the data, the number of rows and columns in the dataset. We can see that our data has 1,034 rows and 6 columns. We have one row for each person who was measured, and one column for each fact recorded about each person. These 1,034 people are collectively called our <em>population</em>, which, in the world of statistics, means any set of similar items that are being studied to answer a particular question.</p>
<h3 id="h2-502888c03-0001">Summary Statistics</h3>
<p class="BodyFirst">Doing exploratory analysis every time we get a new dataset is useful. One thing we can do is run <code>print(mlb.describe())</code> to see our summary statistics all at once:</p>
<pre><code>            height       weight          age
count  1034.000000  1033.000000  1034.000000
mean     73.697292   201.689255    28.736712
std       2.305818    20.991491     4.320310
min      67.000000   150.000000    20.900000
25%      72.000000   187.000000    25.440000
50%      74.000000   200.000000    27.925000
75%      75.000000   215.000000    31.232500
max      83.000000   290.000000    48.520000</code></pre>
<p><span epub:type="pagebreak" id="Page_59" title="59"/>Plotting the data early and often is also a good idea in any data analysis effort. We’ll create a box plot with the following code:</p>
<pre><code>import matplotlib.pyplot as plt
fig1, ax1 = plt.subplots()
ax1.boxplot([mlb['height']])
ax1.set_ylabel('Height (Inches)')
plt.title('MLB Player Heights')
plt.xticks([1], ['Full Population'])
plt.show()</code></pre>
<p>Here, we import Matplotlib to create plots. We use its <code>boxplot()</code> command to create a box plot of all the heights in our population. You can see the results in <a href="#figure3-1" id="figureanchor3-1">Figure 3-1</a>.</p>
<figure>
<img alt="" class="" height="297" src="image_fi/502888c03/f03001.png" width="389"/>
<figcaption><p><a id="figure3-1">Figure 3-1</a>: A box plot showing the distribution of heights of our Major League Baseball (MLB) population</p></figcaption>
</figure>
<p>This box plot is similar to the box plots we looked at in <span class="xref" itemid="xref_target_Chapter 1">Chapter 1</span>. Remember that box plots show the range and distribution of data. Here, we can see that the minimum value of height in the data is around 67, and the maximum is around 83. The median (the horizontal line in the middle of the box) is around 74 inches. We can see that Matplotlib regards several of the points as outliers, and that’s why they’re drawn as circles beyond the range of the vertical lines extending from the top and bottom of the box. Box plots provide a simple way to explore our population and understand it better.</p>
<h3 id="h2-502888c03-0002">Random Samples</h3>
<p class="BodyFirst">In many common scenarios, we’re interested in studying a population, but we don’t have access to the full population, so we study a small part, or <em>sample</em>, instead. For example, medical researchers may want to create a drug that can cure a disease for all women over 50 years old. The researchers don’t have a way to contact all women in the world who are over 50, so instead they recruit a sample of that full population, maybe a few hundred <span epub:type="pagebreak" id="Page_60" title="60"/>people. They study the effect of their drug on this sample. They hope that their sample resembles the full population, so that if the drug works on the sample, it will also work on the full population.</p>
<p>Recruiting a sample is something you should do carefully, to make sure the sample resembles the full population as much as possible. For example, if you recruit participants at an Olympic training facility, your sample will contain people who are healthier than average, so you may create a drug that works for extremely healthy people but not for the general population. If you recruit participants at a Polish community festival, you might create a drug that works for Eastern Europeans but not for others. The best way to collect a sample that resembles the full population is to take a random sample. By selecting randomly from a full population, you expect to have equal likelihood of selecting each different type of person.</p>
<p>Let’s look at samples of our baseball player population, which we can create in Python as follows:</p>
<pre><code>sample1=mlb.sample(n=30,random_state=8675309)
sample2=mlb.sample(n=30,random_state=1729)</code></pre>
<p>Here, we use the convenient pandas <code>sample()</code> method. This method randomly selects 30 baseball players for both of our samples, <code>sample1</code> and <code>sample2</code>. Setting the <code>random_state</code> parameter is not necessary. But we set it here because it ensures that you’ll get the same results as we do when you run the same code.</p>
<p>You may wonder why we select 30 samples and not 20 or 40 or some other number. In fact, we could easily select any other number of samples by changing <code>n=30</code> to <code>n=20</code> or <code>n=40</code> or anything else we prefer. When we choose a large <code>n</code> for our random sample, we expect the sample to closely resemble the full population. But sometimes recruiting participants can be challenging, so we want to choose a small <code>n</code> to avoid the difficulty of recruiting. In the world of statistics, choosing <em>n</em> = 30 is a common convention; when we choose samples that have at least size 30, we feel reasonably confident that our samples are big enough to make our statistical calculations give us good results.</p>
<p>Let’s also create a third sample, which we’ll define manually as follows:</p>
<pre><code>sample3=[71, 72, 73, 74, 74, 76, 75, 75, 75, 76, 75, 77, 76, 75, 77, 76, 75,\ 
76, 76, 75, 75, 81,77, 75, 77, 75, 77, 77, 75, 75]</code></pre>
<p>We know that <code>sample1</code> and <code>sample2</code> are random samples from our baseball player population, since we created them using <code>sample()</code>. But it’s not yet clear where the measurements in <code>sample3</code> came from. Later, you’ll learn how to use statistical tests to reason about whether <code>sample3</code> is likely to be a random sample from our population of baseball players, or whether it is more likely to be associated with a different population, like basketball players or another group. Keep thinking about <code>sample3</code>, because reasoning about <span epub:type="pagebreak" id="Page_61" title="61"/>where <code>sample3</code> came from (and in general, whether two given samples come from the same population) will be the central goal of this chapter.</p>
<p>Let’s look at a plot of these samples to see whether they resemble each other and the full population:</p>
<pre><code>import numpy as np
fig1, ax1 = plt.subplots()
ax1.boxplot([mlb['height'],sample1['height'],sample2['height'],np.array(sample3)])
ax1.set_ylabel('Height (Inches)')
plt.title('MLB Player Heights')
plt.xticks([1,2,3,4], ['Full Population','Sample 1','Sample 2','Sample 3'])
plt.show()</code></pre>
<p>Here, we use the same box plot code we used before, but instead of plotting only one dataset, we plot four datasets: the distribution of heights of the full population and the distribution of heights of all three samples separately. We can see the results in <a href="#figure3-2" id="figureanchor3-2">Figure 3-2</a>.</p>
<figure>
<img alt="" class="" height="296" src="image_fi/502888c03/f03002.png" width="389"/>
<figcaption><p><a id="figure3-2">Figure 3-2</a>: Box plots of our full MLB population (far left), two samples from the population (middle), and a mystery sample that may or may not be from our population (right)</p></figcaption>
</figure>
<p>We can see that none of these box plots are quite identical, but they definitely bear some resemblance to one another. We see some similar median values and 75th percentile values, as well as some similar maxima. The similarity in the first three box plots should match your intuition: when we take large enough random samples from a population, the samples should resemble the population and should resemble one another. We can also check simple summary statistics related to each sample, like the mean:</p>
<pre><code>print(np.mean(sample1['height']))
print(np.mean(sample2['height']))
print(np.mean(sample3))</code></pre>
<p>Here, we check the mean height in all our samples. The mean height for <code>sample1</code> is 73.8, while the mean height for <code>sample2</code> is 74.4, and the mean <span epub:type="pagebreak" id="Page_62" title="62"/>height for <code>sample3</code> is 75.4. These means are relatively close to the mean height of the full population, 73.7. In this context, the mean height of the full population has a special name; it’s called the population’s <em>expected value</em>. If we take a random sample from our population, we expect that the mean height of our sample will be about the same as the population’s expected value for height, 73.7. At least two of our samples are random samples from the population, and we see that their means are indeed close to our expected value.</p>
<p>When we look at the box plot of <code>sample3</code>, we can see that it doesn’t seem to resemble the other three box plots as much as they resemble each other. We may interpret this as evidence that it isn’t a random sample from our population of baseball players. On the other hand, it doesn’t look different enough from the population or other samples that we can be immediately certain that it isn’t a random sample from our population. We need to learn more before we can feel certain about whether <code>sample3</code> is a random draw from our population or whether it comes from some other population.</p>
<p>So far, we’ve used vague and impressionistic language to talk about our samples: they <em>resemble</em> each other, and they have means that are <em>relatively close</em> or <em>about the same</em> as our expectations. If we want to make concrete, evidence-based decisions, we need to be more precise. In the next section, we’ll explore quantitative methods that statisticians have developed for reasoning about the differences between groups, including some easy-to-use tests that help us decide whether two groups come from the same population.</p>
<h3 id="h2-502888c03-0003">Differences Between Sample Data</h3>
<p class="BodyFirst">We saw a difference of about 0.6 inches between <code>sample1</code> and <code>sample2</code> and a difference of more than 1.6 inches between <code>sample1</code> and <code>sample3</code>. Here’s the important question we would like to answer: Do we believe that <code>sample3</code> is a random sample from the same population as <code>sample1</code> and <code>sample2</code>? We need a method more reliable than intuition to say, for example, that a difference of 0.6 inches between sample means is plausible or probable, while a difference of 1.6 inches between sample means makes it implausible that the samples come from the same population. How big of a difference between sample means would make it implausible that two samples come from the same population?</p>
<p>To answer this question, we need to understand the size differences we should expect between random samples from our population. So far, we’ve looked at only two random samples from our population. Instead of trying to generalize based on only two samples, let’s look at a large collection of samples and see how much they tend to differ from one another. This will help us understand which variations are plausible and which variations are implausible.</p>
<p><span epub:type="pagebreak" id="Page_63" title="63"/>Here’s some code to get a collection of 2,000 sample means and their differences:</p>
<pre><code>alldifferences=[]
for i in range(1000):
    newsample1=mlb.sample(n=30,random_state=i*2)
    newsample2=mlb.sample(n=30,random_state=i*2+1)
    alldifferences.append(newsample1['height'].mean()-newsample2['height'].mean())

print(alldifferences[0:10])</code></pre>
<p>In this snippet, we create the <code>alldifferences</code> variable as an empty list. Then we create a loop that goes through 1,000 iterations. In each iteration, we create two new samples and append the difference between their sample means to our <code>alldifferences</code> list. The final result is a completely filled-in <code>alldifferences</code>, a list of 1,000 differences between randomly selected samples. After running this snippet, you should see the following output:</p>
<pre><code>[0.8333333333333286, -0.30000000000001137, -0.10000000000000853,\
-0.1666666666666572, 0.06666666666667709, -0.9666666666666686,\
0.7999999999999972, 0.9333333333333371, -0.5333333333333314,\
-0.20000000000000284]</code></pre>
<p>You can see that the first two samples we checked have means that are about 0.83 inches apart. The second pair of samples has means that are about 0.3 inches apart. The sixth pair of samples has means that are almost a full inch apart from each other (about –0.97 inches), while the fifth pair of samples has means that are nearly identical, only about 0.07 inches apart. Looking at these 10 numbers, we can see that 0.6 is not an implausible difference between two samples from our population, since several of our first 10 differences are greater in magnitude than 0.6. However, none of the differences we’ve seen so far are greater than 1 inch in magnitude, so 1.6 inches is starting to seem more implausible.</p>
<p>We can see a fuller representation of our 1,000 differences by drawing a plot of the <code>alldifferences</code> list:</p>
<pre><code>import seaborn as sns
sns.set()
ax=sns.distplot(alldifferences).set_title("Differences Between Sample Means")
plt.xlabel('Difference Between Means (Inches)')
plt.ylabel('Relative Frequency')
plt.show()</code></pre>
<p>Here, we import the seaborn package because it can make beautiful plots. We use its <code>distplot()</code> method to plot the differences we find. You can see the results in <a href="#figure3-3" id="figureanchor3-3">Figure 3-3</a>.</p>
<span epub:type="pagebreak" id="Page_64" title="64"/><figure>
<img alt="" class="" height="309" src="image_fi/502888c03/f03003.png" width="389"/>
<figcaption><p><a id="figure3-3">Figure 3-3</a>: A histogram showing the distribution of differences between mean heights of random samples, creating an approximate bell curve pattern</p></figcaption>
</figure>
<p>In this histogram, each bar represents a relative frequency; it represents how likely each observation is compared to other observations. There’s a high bar at the point marked 0 on the x-axis. This indicates a relatively high number of differences in our <code>alldifferences</code> list that are very close to 0. A much lower bar appears at <em>x</em> = 1. This indicates relatively few cases in which the difference between the sample means is about 1. The full shape of the plot should make solid intuitive sense: it’s rare for our random samples to be very different from each other, because they’re samples from the same population and we expect their means to be roughly the same.</p>
<p>The shape that the bars make in <a href="#figure3-3">Figure 3-3</a> resembles a bell. You can see that we’ve drawn a line over the bars that shows this general bell shape. The curve that this plot approximates is called a <em>bell curve</em>. Approximate bell curves are found in many situations. One powerful theoretical result in statistics is called the <em>central limit theorem</em>, and it states that under a certain set of common conditions, differences between means of samples will be distributed in a shape that’s approximately a bell curve. The technical conditions that make this theorem true are that the random samples are independent and <em>identically distributed</em> (that is, random draws from the same population) and that the population has a finite expected value and a finite variance. The fact that we see approximate bell curves in so many domains provides evidence that these technical conditions are often met.</p>
<p>Once we know the shape and size of the bell curve in <a href="#figure3-3">Figure 3-3</a>, we can reason more accurately about difficult statistical questions. Let’s return to the question of <code>sample3</code>. Given what we know so far, do we believe that <code>sample3</code> is a random sample from our population of baseball players? We saw that the difference between the mean of <code>sample3</code> and the mean of <code>sample1</code> is about 1.6 inches. When we look at <a href="#figure3-3">Figure 3-3</a>, we can see that the bell curve is extremely low there, close to 0. This means that random samples from our population only rarely differ by as much as 1.6 inches. This makes it <span epub:type="pagebreak" id="Page_65" title="65"/>seem relatively implausible that <code>sample3</code> is a random sample from our baseball player population. We can find out how implausible it is by checking exactly how many of our differences have magnitude greater than or equal to 1.6 inches:</p>
<pre><code>largedifferences=[diff for diff in alldifferences if abs(diff)&gt;=1.6]
print(len(largedifferences))</code></pre>
<p>In this snippet, we create <code>largedifferences</code>, a list that contains all elements of <code>alldifferences</code> with magnitude greater than or equal to 1.6. We then check the length of the <code>largedifferences</code> list. We find that the list has only eight elements, meaning random samples from our <code>mlb</code> population have means that differ by 1.6 or more only about 8 in 1,000 times, or 0.8 percent of the time. This value, 0.8 percent or 0.008, is a calculated likelihood. We can think of it as our best estimate of the probability that the mean heights of two random samples from the <code>mlb</code> population differ by 1.6 inches or more. This probability is often called a <em>p-value</em>, where <em>p</em> is short for <em>probability</em>.</p>
<p>If we assume that <code>sample3</code> is a random sample from our <code>mlb</code> population, we have to believe that this rare difference, something whose extremity occurs less than 1 percent of the time, has happened naturally. The low likelihood of this event may convince us to reject the idea that <code>sample3</code> comes from the same population as <code>sample1</code>. In other words, the low <em>p</em>-value causes us to reject the notion that these two groups come from the same population. The lower the <em>p</em>-value, the more confident we feel about rejecting the notion that the groups come from the same population, because low <em>p</em>-values require us to believe in more and more unlikely coincidences. By contrast, consider how common it is that differences between sample means from our population are 0.6 inches or more:</p>
<pre><code>smalldifferences=[diff for diff in alldifferences if abs(diff)&gt;=0.6]
print(len(smalldifferences))</code></pre>
<p>Here, we create <code>smalldifferences</code>, a list containing every element of <code>alldifferences</code> that has magnitude greater than or equal to 0.6 inches. We can see that differences of this magnitude occur about 31.4 percent of the time. In this case, we would say that our <em>p</em>-value is 0.314. If <code>sample1</code> and <code>sample2</code> come from the same population, we would have to believe that this size of difference, which occurs about 31 percent of the time, occurred in our case. It’s not hard to believe that something with 31 percent probability occurred, so we conclude that the difference between <code>sample1</code> and <code>sample2</code> is plausible; we’re willing to accept that, though not identical, they’re random samples from the same population.</p>
<p>The <em>p</em>-values we’ve calculated here have led us to accept the notion that <code>sample1</code> and <code>sample2</code> come from the same population, and to reject the notion that <code>sample1</code> and <code>sample3</code> come from the same population. You can see how important the size of a <em>p</em>-value is in our efforts to compare groups.</p>
<h2 id="h1-502888c03-0002"><span epub:type="pagebreak" id="Page_66" title="66"/>Performing Hypothesis Testing</h2>
<p class="BodyFirst">We’ve outlined all the ingredients needed for a method of statistical reasoning called <em>hypothesis testing</em>. We can formalize this method of reasoning in more scientific terms. We’re trying to determine whether <code>sample3</code> is a random sample from the same population as <code>sample1</code>. In scientific terms, we can say that we’re considering two separate hypotheses:</p>
<p class="RunInPara"><span class="RunInHead">Hypothesis 0</span> <code>sample1</code> and <code>sample3</code> are random samples from the same population.</p>
<p class="RunInPara"><span class="RunInHead">Hypothesis 1</span> <code>sample1</code> and <code>sample3</code> are not random samples from the same population.</p>
<p>In the common statistical parlance, we call Hypothesis 0 the <em>null hypothesis</em>, and we call Hypothesis 1 the <em>alternative hypothesis</em>. The null hypothesis asserts that both samples are randomly drawn from one population (our baseball player dataset), with just one mean and one standard deviation. The alternative hypothesis asserts that the samples are randomly drawn from two totally different populations, each with its own mean, its own standard deviation, and all of its own unique characteristics. The way we choose between these two hypotheses is by following the same reasoning we followed previously:</p>
<ol class="decimal">
<li value="1">Assume that Hypothesis 0, the null hypothesis, is true.</li>
<li value="2">Find how likely we are to observe sample means that differ by as much as our observed sample means, assuming that Hypothesis 0 is true. The likelihood of this occurring is called the <em>p</em>-value.</li>
<li value="3">If the <em>p</em>-value is small enough, we reject Hypothesis 0, and we’re therefore willing to accept Hypothesis 1.</li>
</ol>
<p>Notice that step 3 is stated vaguely: it doesn’t specify how small the <em>p</em>-value should be to justify rejecting the null hypothesis. The reason for this vagueness is that there’s no mathematically dictated choice for how small the <em>p</em>-value needs to be. We can choose whatever level of smallness we think is appropriate to justify rejecting Hypothesis 0, based on our own judgment and intuitions. The <em>p</em>-value size that we believe justifies rejecting Hypothesis 0 is called the <em>significance level</em>.</p>
<p>The most common significance level used in empirical research is 5 percent, meaning that we consider the rejection of the null hypothesis justified if <em>p</em> &lt; 0.05. In the case of <code>sample1</code> and <code>sample3</code>, we can justify rejecting Hypothesis 0 at a significance level as low as 1 percent, because we found <em>p</em> &lt; 0.01. When we find a <em>p</em>-value that’s less than our chosen significance level, we say that the difference between our groups is <em>statistically significant</em>. The recommended practice is to choose the significance level that we want to use before we do any calculations; that way, we avoid the temptation to choose a significance level that confirms whichever hypothesis we want to be confirmed.</p>
<h3 id="h2-502888c03-0004"><span epub:type="pagebreak" id="Page_67" title="67"/>The t-Test</h3>
<p class="BodyFirst">We don’t have to go through the whole process of calculating means, creating a histogram, and manually calculating <em>p</em>-values every time we want to do hypothesis testing. Statisticians have discovered succinct equations that define how likely two groups are to come from the same population. They’ve created a relatively simple test called a <em>t-test</em> that does the process of hypothesis testing quickly and painlessly, without requiring a <code>for</code> loop or a histogram. We can do a t-test to test our Hypothesis 0 and Hypothesis 1. We’ll check whether <code>sample1</code> and <code>sample2</code> come from the same population as follows:</p>
<pre><code>import scipy.stats
scipy.stats.ttest_ind(sample1['height'],sample2['height'])</code></pre>
<p>Here, we import the <code>scipy.stats</code> module. The SciPy package that this module is a part of is a popular Python library that includes, among other things, many statistical tests that could be useful as you get more advanced in statistics and data science. After importing this module, we use its <code>ttest_ind</code> command to check for differences between our samples. Its output is the following:</p>
<pre><code>Ttest_indResult(statistic=-1.0839563860213952, pvalue=0.2828695892305152)</code></pre>
<p>Here, the <em>p</em>-value is relatively high (about 0.283), certainly higher than a 0.05 significance threshold. (It differs a little from the 0.314 <em>p</em>-value we calculated earlier because that <em>p</em>-value calculation method was an approximate method, and this one is more mathematically exact.) This high <em>p</em>-value indicates that it is plausible that these are samples from the same population. This is not surprising because we know that they are from the same population (we created them ourselves). In this case, we decide not to reject our null hypothesis, and we accept (until any other evidence convinces us otherwise) that <code>sample1</code> and <code>sample2</code> come from the same population. You can also run <code>scipy.stats.ttest_ind(sample1['height'],sample3)</code> to compare <code>sample1</code> and <code>sample3</code>, and if you do, you’ll find a low <em>p</em>-value (less than 0.05), which justifies rejecting the null hypothesis that <code>sample1</code> and <code>sample3</code> come from the same population.</p>
<p>Several types of t-tests exist, as well as other hypothesis tests besides the t-test. The <code>ttest_ind</code> command we’ve used so far has an <code>_ind</code> suffix to indicate that it’s meant to be used for independent samples. Here, <em>independent</em> means just what we would expect: no meaningful, consistent relationship exists between the individuals in one sample and the individuals in the other—the samples consist of different people who were randomly selected.</p>
<p>If we have <em>related</em> rather than independent samples, we can use another command, <code>scipy.stats.ttest_rel</code>, which performs another type of t-test that is mathematically a little different from <code>ttest_ind</code>. The <code>ttest_rel</code> command would be appropriate when observations in different samples have a meaningful relationship to each other—for example, if they’re two different exam scores for the same student, or two different medical test results for the same patient.</p>
<p><span epub:type="pagebreak" id="Page_68" title="68"/>Another type of t-test, the <em>Welch’s t-test</em>, is designed for comparing samples when we don’t want to assume that the samples have equal variance. You can implement Welch’s t-test in Python by adding <code>equal_var=False</code> to the t-test command.</p>
<p>The t-test is a <em>parametric test</em>, meaning that it relies on assumptions about the distribution of the data in our population. The t-test relies on several technical assumptions: first, that the groups being compared should have sample means that follow a bell curve; second, that the variances of the groups being compared should be identical (unless using Welch’s t-test); and third, that the two groups are independent of each other. If these assumptions are not met, the t-test is not completely accurate, though it’s rarely too far from the truth even if the assumptions are not met.</p>
<p>In some cases, we’d prefer to perform hypothesis testing with a test that didn’t make these strong assumptions that may not be true. If so, we can rely on a body of knowledge called <em>nonparametric statistics</em>, which provides tools for hypothesis testing and other statistical reasoning that make fewer assumptions about the distribution of our data (for example, we don’t need to work with populations whose sample means follow bell curves). One hypothesis test from nonparametric statistics is called the <em>Mann-Whitney U test</em> (or the <em>Wilcoxon rank-sum test</em>), and we can implement it easily in Python as follows:</p>
<pre><code>scipy.stats.mannwhitneyu(sample1['height'],sample2['height'])</code></pre>
<p>This test requires only one line, since the SciPy package includes an implementation of the Mann-Whitney U test. Just like the t-test, all we need to input is the data we’re comparing, and the code will output a <em>p</em>-value. If you want to have a deep understanding of the various kinds of hypothesis tests and exactly when to use them, you should read some advanced theoretical statistics textbooks. For now, the simple independent sample t-test we’ve used is quite robust and should work in most practical scenarios.</p>
<h3 id="h2-502888c03-0005">Nuances of Hypothesis Testing</h3>
<p class="BodyFirst">Hypothesis testing using a null hypothesis and a t-test is common enough to be called <em>popular</em>, but it’s not as beloved as most popular things are. Students tend to dislike it because it’s not intuitive to most people, and it requires some tortured reasoning to understand. Teachers sometimes dislike it because their students dislike it and struggle to learn it. Many methodological researchers find it irritating because it’s so common at all levels for people to misunderstand and misinterpret t-tests, <em>p</em>-values, and hypothesis tests in general. The antipathy toward hypothesis testing has even led some respected scientific journals to ban it from their publications, although this has been rare.</p>
<p>Most of the negative feelings toward hypothesis testing are the result of misunderstanding. Researchers misunderstand some nuances of hypothesis testing and misuse it, and then mistakes in research result, which methodological sticklers resent. Since these misunderstandings are common even <span epub:type="pagebreak" id="Page_69" title="69"/>among professionals, it’s worthwhile to mention some of them here and try to explain nuances that will help you avoid some of these same mistakes.</p>
<p>One important point to remember is what a <em>p</em>-value tells you: it tells you the likelihood of observing data, after assuming a null hypothesis to be true. People often think or wish it told them the converse: the likelihood of a hypothesis’s truth, given some observed data. Always remember that a <em>p</em>-value should not be interpreted directly as a probability of a hypothesis being true. So, when we see that the <em>p</em>-value for comparing the heights of <code>sample1</code> and <code>sample3</code> is <em>p</em> = 0.008, we can’t say, “These samples have only a 0.8 percent probability of coming from the same population,” nor can we say, “The null hypothesis has a 0.8 percent probability of being true.” We can say only, “If the null hypothesis is true, something with 0.8 percent probability occurred.” This enables us to decide whether to reject the null hypothesis, but it doesn’t enable us to say exactly how likely either hypothesis is to be true.</p>
<p>Another important nuance is the difference between accepting a hypothesis and failing to reject it. Hypothesis testing has only two possible outcomes: either we reject a null hypothesis, or we decide not to reject the null hypothesis. Failing to reject something is not quite the same as wholeheartedly accepting it, and just because a <em>p</em>-value is not below a significance threshold does not mean that two groups are certainly the same. Just because one t-test fails to lead to a rejection of the null hypothesis does not mean that the null hypothesis is certainly true.</p>
<p>Similarly, just because one <em>p</em>-value seems to justify a rejection of a null hypothesis does not mean that the null hypothesis is certainly false. This is especially true when we have limited data; difficult, noisy measurements; or a reason to doubt our measurements. Hypothesis testing does not let us take uncertain data and have perfect certainty about hypotheses. Rather, it provides one piece of evidence that we have to understand properly and then weigh together with a great deal of other evidence.</p>
<p>Another important concept to remember is the Anna Karenina principle. Leo Tolstoy wrote in <em>Anna Karenina</em> that “all happy families are alike; each unhappy family is unhappy in its own way.” Statistics has an analogous principle: all acceptances of the null hypothesis are alike, but each rejection of the null hypothesis happens for a different reason. The null hypothesis states that two samples are random draws from the same population. If we reject the null hypothesis, any one or more of a number of things could be true: our two samples could be random draws from different populations, or both could be samples from the same population but not randomly selected, or a source of sampling bias could be present, or blind luck might be occurring. Just because we are confident about rejecting the null hypothesis doesn’t mean we can be confident about which part of the null hypothesis is incorrect. As empirical researchers like to say, “Further research is needed.”</p>
<p>A final nuance to remember is the difference between <em>statistical significance</em> and <em>practical significance</em>. It’s possible for one sample of athletes to have mean height 73.11 and another sample of athletes to have mean height 73.12, and <span epub:type="pagebreak" id="Page_70" title="70"/>for these two means to have a statistically significant difference according to a t-test. We can justifiably conclude that these two groups are not random samples from the same population and treat them differently because of their different mean height. However, even if this difference of 0.01 inches is statistically significant, it’s not clear that this difference has practical significance. Members of these two groups should be able to wear the same clothes, sit on the same seats on airplanes, and reach the same high cupboards (on average). We have no reason to suppose that one group would be better than the other group at baseball in any practically important sense. In this case, we might wish to ignore the results of a t-test, since even though a statistically detectable difference exists, it’s not a difference that has any practical consequence. Practical significance is always an important thing to consider during the process of hypothesis testing.</p>
<p>Now that we’ve discussed hypothesis testing and its thorny theoretical nuances, let’s turn to a practical business example.</p>
<h2 id="h1-502888c03-0003">Comparing Groups in a Practical Context</h2>
<p class="BodyFirst">So far, this chapter has focused on statistical theory. But for data scientists, theoretical considerations always take place within a practical context. Let’s switch from our baseball example to a marketing example. Suppose you’re running a company that manufactures computers. To keep in touch with customers and increase sales, your company maintains email lists: interested customers can sign up for a topic they’re interested in and receive periodic emails from your company related to that topic. For now, you have only two email lists: the desktop list and the laptop list, designed for customers who are interested in your desktop computers and your laptop computers, respectively.</p>
<p>Until now, desktops and laptops have been the only products your company makes. But soon you’ll be releasing a new set of products that have been in development for several years: top-of-the-line web servers. It’s a natural product line for your company, since you already manufacture computer hardware and already have many tech clients who need server infrastructure. But since this product line is new, almost no one knows about it. Your marketing team is planning to email the subscribers on your email lists to tell them about your great new products and hopefully get started on the right foot with high sales of the new servers.</p>
<p>The marketing team members want to make this email campaign as effective as possible. They have a discussion with you about the campaign strategy. They could design one email and send it to every person on both email lists, or they could design one email message for the desktop subscribers and a different email message for the laptop subscribers. The experts on your marketing team know a lot about targeting: for example, they know that the email messages that extroverts react to most positively are different from the email messages that introverts react to most positively. Other personal characteristics—including age, income, and culture—also have strong effects on responses to advertising.</p>
<p><span epub:type="pagebreak" id="Page_71" title="71"/>We need to know whether the desktop subscribers have different characteristics than the laptop subscribers. If the two groups are essentially the same, we can save the marketing team members some time and have them send the same email to everyone. If the two groups are significantly different in ways we understand, we can craft better messages that appeal to each group and improve sales numbers.</p>
<p>We can start our investigation by reading in our data. We’ll read in two fabricated datasets (not based on real people or products, just created to illustrate the points in the chapter). You can download these two datasets from <a class="LinkURL" href="https://bradfordtuckfield.com/desktop.csv">https://bradfordtuckfield.com/desktop.csv</a> and <a class="LinkURL" href="https://bradfordtuckfield.com/laptop.csv">https://bradfordtuckfield.com/laptop.csv</a>, and then read them into Python as follows:</p>
<pre><code>desktop=pd.read_csv('desktop.csv')
laptop=pd.read_csv('laptop.csv')</code></pre>
<p>You can run <code>print(desktop.head())</code> and <code>print(laptop.head())</code> to see the first five rows of each dataset. You’ll notice that both datasets have four columns:</p>
<p class="RunInPara"><span class="RunInHead"><span class="LiteralBold"><code>userid</code></span></span>  Contains a unique number identifying a particular user</p>
<p class="RunInPara"><span class="RunInHead"><span class="LiteralBold"><code>spending</code></span></span>  Contains a record of how much that user has spent at your company’s website</p>
<p class="RunInPara"><span class="RunInHead"><span class="LiteralBold"><code>age</code></span></span>  Holds the user’s age, which you may have recorded during a separate survey</p>
<p class="RunInPara"><span class="RunInHead"><span class="LiteralBold"><code>visits</code></span> </span>  Holds the number of times the user has visited pages on your website</p>
<p>Our goal is to determine whether the users described in the <code>desktop</code> dataframe and the users described in the <code>laptop</code> dataframe differ significantly from each other. Let’s draw some plots and see whether any immediately apparent differences exist.</p>
<p>We can start with a plot of the amounts that subscribers to each list have spent on our company’s products. We’ll create a box plot with the following code:</p>
<pre><code>import matplotlib.pyplot as plt
sns.reset_orig()
fig1, ax1 = plt.subplots()
ax1.set_title('Spending by Desktop and Laptop Subscribers')
ax1.boxplot([desktop['spending'].values,laptop['spending'].values])
ax1.set_ylabel('Spending ($)')
plt.xticks([1,2], ['Desktop Subscribers','Laptop Subscribers'])
plt.show()</code></pre>
<p>Here, we import Matplotlib to create plots. We use its <code>boxplot()</code> command with data from <code>desktop</code>’s spending column and <code>laptop</code>’s spending column. You can see the results in <a href="#figure3-4" id="figureanchor3-4">Figure 3-4</a>.</p>
<span epub:type="pagebreak" id="Page_72" title="72"/><figure>
<img alt="" class="" height="287" src="image_fi/502888c03/f03004.png" width="389"/>
<figcaption><p><a id="figure3-4">Figure 3-4</a>: Box plots showing the spending levels of subscribers to the desktop email list (left) and subscribers of the laptop email list (right)</p></figcaption>
</figure>
<p>We can learn a few things by looking at these box plots. Both groups have minima at 0. Laptop subscribers have a higher 25th percentile, 50th percentile, and 75th percentile, as well as a high outlier that is higher than any observations in the desktop subscriber group. On the other hand, the distributions don’t seem terribly different either; desktop subscribers don’t seem totally different from laptop subscribers. We have groups that are different, but not too different. We should look more closely and see if some more precise quantitative metrics will help us judge how different they are.</p>
<p>Besides plotting, we can do simple calculations that get summary statistics for our data. In the following snippet, we’ll get some of these descriptive statistics:</p>
<pre><code>print(np.mean(desktop['age']))
print(np.mean(laptop['age']))
print(np.median(desktop['age']))
print(np.median(laptop['age']))
print(np.quantile(laptop['spending'],.25))
print(np.quantile(desktop['spending'],.75))
print(np.std(desktop['age']))</code></pre>
<p>In this snippet, we check the mean age of desktop subscribers and laptop subscribers. The results reveal that the mean desktop subscriber is about 35.8 years old, and the mean laptop subscriber is about 38.7 years old. We can conclude that these groups are different, in the sense that they’re not identical. But it’s not clear whether the groups are different enough that we should tell our marketing group to create two separate emails instead of one. To make that judgment, we need to use our hypothesis-testing framework. We can specify our null hypothesis and alternative hypothesis as follows:</p>
<p class="RunInPara"><span epub:type="pagebreak" id="Page_73" title="73"/><span class="RunInHead">Hypothesis 0</span>  The two email lists are random samples from the same population.</p>
<p class="RunInPara"><span class="RunInHead">Hypothesis 1</span>  The two email lists are not random samples from the same population.</p>
<p>Hypothesis 0, our null hypothesis, is describing a world in which there’s a population of people who are interested in computers, both laptops and desktops. People from this population sign up for your company’s email lists occasionally. But when they sign up for a list, they choose completely at random which of your two lists they sign up for. In this world, your lists have superficial differences, but they are truly two random samples from the same population and don’t differ in any essential ways that would warrant different treatment by your company.</p>
<p>Hypothesis 1, the alternative hypothesis, describes a world in which the null hypothesis is not true. This would mean that your subscribers’ membership on different email lists is the result at least partially of underlying differences in people who like desktops and people who like laptops. If Hypothesis 0 is true, it would be reasonable to send the same marketing email to both groups. If Hypothesis 1 is true, sending different marketing emails to each group makes more sense. A business decision now depends on the result of a statistical test.</p>
<p>Let’s run our t-test and see whether our two subscriber groups actually differ from each other. First, we should specify a significance level. Let’s use the 5 percent significance level that’s common in research. We can run our t-test with only one line of code:</p>
<pre><code>scipy.stats.ttest_ind(desktop['spending'],laptop['spending'])</code></pre>
<p>When you look at the results from our t-test, you can see that our <em>p</em>-value is about 0.04. Since we use the common 5 percent significance level, this <em>p</em>-value is low enough for us to conclude that the desktop and laptop groups are not random draws from the same population, so we can reject the null hypothesis. It appears that desktop and laptop email subscribers are at least slightly different in a detectable way.</p>
<p>After finding these differences, we can talk to our company’s marketing team and collectively make a decision about whether to design different email campaigns for the different groups. Suppose that the team decides to do so. We can feel proud of ourselves that our statistical analysis has led to a practical decision that we feel is justified. We didn’t only analyze data; we used the data to make a decision. This is common in data science: we use our data analysis for data-driven decision-making to improve business outcomes.</p>
<p>But what next? We have come so far in this chapter only to make one decision: a yes/no decision to send different emails to both of our subscriber lists. The next set of questions we need to ask is about how our emails to each group should differ: What should their content be, how should we design them, and how will we know whether we’re doing it right? In the next chapter, we’ll talk about A/B testing, a powerful framework for answering these difficult questions.</p>
<h2 id="h1-502888c03-0004"><span epub:type="pagebreak" id="Page_74" title="74"/>Summary</h2>
<p class="BodyFirst">In this chapter, we talked about populations and samples, as well as how samples that come from the same population should resemble each other. We introduced hypothesis testing, including the t-test, an easy, useful tool for detecting whether two groups are likely to be random draws from the same population. We discussed some business scenarios where the t-test would be useful, including a marketing scenario and a decision about whether to send different emails to different email lists.</p>
<p>The next chapter will build on the tools we introduced here. Instead of just comparing groups, we’ll discuss how to run experiments and then use group comparison tools to check for the differences between our experimental treatments.</p>
</section>
</div></body></html>