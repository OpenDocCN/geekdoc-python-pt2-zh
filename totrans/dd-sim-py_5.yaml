- en: Part V
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BEYOND THE CODE
  prefs: []
  type: TYPE_NORMAL
- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Packaging and Distribution
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: The best code in the world amounts to little if you never ship it. Once your
    project is functional, you should work out how you’ll package and distribute it,
    before continuing development. The trouble is, packaging in Python sometimes feels
    like beating yourself with a wet trout.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the difficulty doesn’t come from packaging your own code—that’s usually
    easy enough—but rather from handling your code’s dependencies, especially its
    non-Python dependencies. Distribution can be a snarly issue for even experienced
    programmers, due in part to the diverse scenarios in which Python is used. Still,
    if you understand how things are supposed to work, you’ll have a good foundation
    for pushing past the frustration and shipping working code.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I’ll break down the essentials of packaging and distributing
    a Python project, first via the Python Package Index, and then as an installable
    binary. To do this, I’ll walk you through packaging an actual application I wrote:
    Timecard. This project is well-suited as an example, because it has both Python
    and system dependencies, plus a few non-code resources, all of which need to be
    accounted for, somehow. The repository for this project is available on GitHub:
    [https://github.com/codemouse92/timecard/](https://github.com/codemouse92/timecard/).
    I’ve set up the `packaging_example` branch to contain only the project itself
    and none of the packaging files, so you can practice.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter functions as an introduction to the packaging process in general,
    regardless of the tool you plan to use. However, to avoid getting lost in the
    weeds explaining the myriad packaging tools out there, I’ll package Timecard using
    the popular setuptools package, from which we get the majority of modern packaging
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Along the way, I’ll touch on many other common tools, including a few popular
    third-party alternatives, but I won’t go into detail on most of them. In case
    you want to learn more about these tools, I will link to their official documentation
    for your reference. Also, if you want to go deeper into packaging in general,
    one of the best resources is the community-maintained *Python Packaging User Guide*
    at [https://packaging.python.org/](https://packaging.python.org/), which covers
    a number of more advanced topics, such as packaging a CPython binary extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this whole idea of packaging feel less threatening to you, I’d also
    like to mention that the mascot of Python packaging is a happy, purple platypus:
    an odd little creature that seems to be made up of many disparate parts, is cute
    and generally friendly, and can lay eggs. (That last part is a pun you’ll likely
    understand by the time this chapter is over.) If you’re feeling intimidated by
    packaging right now, go to [https://monotreme.club/](https://monotreme.club/)
    and revel in the cuteness that is the Python Packaging mascot. They have stickers.'
  prefs: []
  type: TYPE_NORMAL
- en: Planning Your Packaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you begin packaging, you need a solid idea of what you’re trying to accomplish,
    why, and how. Unfortunately, very few developers recognize this necessity and
    instead plunge headlong into writing their packaging scripts, with no real direction.
    These ad hoc packaging schemes can suffer from brittleness, unnecessary complexity,
    a lack of portability between systems, and poor or missing dependency installation.
  prefs: []
  type: TYPE_NORMAL
- en: The Dangers of Cargo Cult Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In an attempt to encourage the use of good packaging tools and practices, many
    well-meaning people will provide templates for the files *setup.py*, *setup.cfg*,
    or others used in packaging, with advice to copy and modify the templates. This
    practice, known as *cargo cult programming*, is widely employed in Python packaging,
    to the detriment of both project and ecosystem. Because configuration files are
    copied blind, errors, hacks, and anti-patterns propagate like plague-carrying
    rabbits.
  prefs: []
  type: TYPE_NORMAL
- en: A bug in packaging won’t always manifest with a failed installation or a helpful
    error message. For example, a mistake in packaging a library may instead manifest
    when that library is used as a dependency. Distribution is especially pesky in
    this regard, as many related bugs are platform specific. Sometimes, you’ll be
    able to install the package, but the program will fail in surprising ways! Issue
    trackers are rife with these sorts of tickets, many of which are unhelpfully closed
    with “Cannot reproduce,” perhaps because the bug only occurs on that one version
    of Linux with a particular version of that one system library.
  prefs: []
  type: TYPE_NORMAL
- en: All that is to say, do not give in to the temptation of cargo cult programming!
    While it is reasonable to start from a proven template, aim to understand every
    single line of code therein. Read through the documentation. Be certain you haven’t
    omitted a needed parameter the template may have overlooked, used some deprecated
    option, or even swapped the proper sequence of lines. (Yes, that last one is a
    thing.)
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, the *Python Packaging Authority* (*PyPA*) working group has done
    a lot to move the community away from this. The PyPA is a quasi-official group
    made up of Python community members who want to make Python packaging a better
    experience, and membership is open to anyone who maintains a project. They have
    extensively explained the whys and wherefores of each piece of their packaging
    templates and the Python Packaging User Guide they maintain.
  prefs: []
  type: TYPE_NORMAL
- en: A Note on Packaging Opinions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you’re about to discover, there are a plethora of ways to package and distribute
    Python projects. I’ll focus primarily on the techniques advised by the PyPA, but
    there are plenty of alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever packaging techniques you ultimately use, they must produce a reasonably
    portable, stable, “just works” package. Your end user should be able to run a
    predictable set of steps on any supported system and succeed at running your code.
    While it isn’t uncommon for there to be variations in installation instructions
    from one platform to the next, you want to minimize the number of steps that your
    end user needs to follow. The more steps, the more chances for errors! Keep things
    simple and try to respect the recommended packaging and distribution practices
    for each platform. If your end users consistently report problems or confusion
    when installing your project, *fix the packaging*.
  prefs: []
  type: TYPE_NORMAL
- en: Determining Your Packaging Goals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultimately, the goal of any packaging tool is to create a single *artifact*,
    usually a file, that can be installed on an end user’s environment, be it a personal
    computer, a server, a virtual machine, or another piece of hardware. There are
    a number of ways to package a project in Python for distribution. Selecting the
    right way all comes down to what your project is and who is going to be using
    it.
  prefs: []
  type: TYPE_NORMAL
- en: At the PyBay2017 conference, Mahmoud Hashemi presented a talk entitled “The
    Packaging Gradient” ([https://youtu.be/iLVNWfPWAC8](https://youtu.be/iLVNWfPWAC8)),
    in which he brought a lot of clarity to the Python packaging ecosystem. (I definitely
    recommend watching it.) In that talk, he introduced the concept of the *packaging
    gradient*, which visualizes the options for Python packaging and distribution
    like the layers of an onion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 1: Python Modules'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On the innermost layer of the packaging gradient is the *Python module*, which
    can be distributed by itself. If your entire project consists of a single Python
    module, such as some utility script, you may be able to simply distribute that.
    However, as you’ve probably noticed by now, this isn’t practical whenever there
    is more than one module involved in the project.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, many Python developers chicken out at this point, zip up their
    whole project in a compressed file (perhaps with a README file), and leave the
    hapless end user to figure out how to run the package on their particular system.
    Don’t do this to your users. It’s not a great experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 2: Python Distribution Packages'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So far in your Python journey, you’ve installed plenty of packages with pip.
    These are all provided by the *Python Package Index (PyPI)*, an online repository
    of Python packages. Each package in PyPI is in one or both of two formats: source
    distribution and built distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The *source distribution*, or *sdist*, contains one or more Python packages
    bundled into a compressed archive, such as a *.tar.gz* file. This is fine as long
    as your project’s code is solely in Python and only depends on Python packages.
    This is the second layer in the packaging gradient.
  prefs: []
  type: TYPE_NORMAL
- en: The third layer in the packaging gradient is the *built distribution*, or *bdist*,
    which contains precompiled Python bytecode, as well as binary files needed for
    the package to run. A built distribution is faster to install than a source distribution,
    and it can contain non-Python components.
  prefs: []
  type: TYPE_NORMAL
- en: A built distribution is packaged as a *wheel*, a standardized format defined
    in PEP 427\. The name *wheel* refers to a wheel of cheese, which is a reference
    to the “cheese shop,” the original code name for what is now PyPI. Prior to the
    2012 adoption of the wheel standard, Python unofficially used another format called
    *eggs*, which had a number of technical limitations that wheel overcame.
  prefs: []
  type: TYPE_NORMAL
- en: The source distribution, and by extension, the associated built distribution,
    is known as a *distribution package* once it has been bundled together and versioned.
  prefs: []
  type: TYPE_NORMAL
- en: PyPI can distribute both wheels and sdists, so it’s trivial (and thus recommended)
    to upload both. If you have to choose, favor sdists. If you were to upload only
    a *platform wheel*—a wheel built for a specific system—and omitted the sdist,
    users on other systems would not be able to install your package. Only publishing
    wheels also leaves out users in situations where auditing the source code is mandatory,
    such as in some corporations. Still, wheels are faster to install than sdists.
    Whenever you can, upload both.
  prefs: []
  type: TYPE_NORMAL
- en: 'Option 3: Application Distribution'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s just one problem so far: PyPI is intended for distributing to developers,
    not to end users. While it is possible to distribute an application on PyPI, it’s
    not well-suited for deploying to end users or production environments; pip is
    too brittle to be that reliable!'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter’s example project, Timecard, is certainly a good example of this.
    While providing a Python distribution package for my application doesn’t hurt,
    many end users will be at a loss if I just tell them to install from pip. I will
    need an additional layer in my packaging later.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the right distribution method for your application depends largely
    on your project dependencies, your target environments, and the needs of your
    end users. Later in this chapter, I’ll cover several good options for distributing
    an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Structure: src or src-less'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you start the process of packaging, you must decide whether to use a
    *src/* directory. Up to this point, all my examples in this book have used so-called
    *src-less* (“source-less”) project structures, where the main project package
    sits directly in the repository. I chose this technique in prior chapters because
    of the ease of running the package via `python3 -m` `packagenamehere`, without
    the need for installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative project structure involves placing all your project modules
    and scripts in a dedicated *src/* directory. Python developer Ionel Cristian Mărieș,
    one of the leading advocates of this approach, details several advantages of using
    a *src/* directory, which I can summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: It simplifies maintenance of your packaging scripts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It keeps your packaging scripts and your project source code clearly separated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It prevents several common packaging mistakes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It prevents you from making assumptions about the current working directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To test or run your package, you are forced to install your package, usually
    in a virtual environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That last item may seem like a peculiar benefit. Avoiding that situation is
    exactly why I didn’t introduce this technique back in Chapter 2, since it would
    have been impossible to properly introduce *setup.cfg* and *setup.py* then.
  prefs: []
  type: TYPE_NORMAL
- en: In production-grade development, however, forcing yourself to install the package
    is highly beneficial. It immediately exposes flaws in packaging, assumptions about
    current working directories, and a number of related problems and anti-patterns.
    I didn’t actually learn about this until researching this book, and I deeply wish
    I’d discovered it sooner, as it would have saved me many woes.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of that approach is that it keeps you from putting off questions
    of packaging until the last step in a project. I can say from personal experience
    that there is little more frustrating than building an entire project, only to
    discover you cannot package anywhere but on your own machine! Figure out packaging
    as early in the development process as you can.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this book, and in the Timecard project in particular, I’ll use
    the *src/* directory approach. Even if you don’t use a dedicated *src/* directory,
    you should install your package in a virtual environment when you want to test
    it. By the end of the next section, you’ll have done exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging a Distribution Package with setuptools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of interesting packaging tools out there for Python, each with
    its own vocal proponents, so it can be easy to get overwhelmed. When in doubt,
    I recommend starting with *setuptools*, the de facto standard tool for Python
    packaging. Even if you decide to use another packaging tool later, many concepts
    from setuptools are borrowed by most of the other packaging tools.
  prefs: []
  type: TYPE_NORMAL
- en: The setuptools library is a fork of a Python standard library package called
    *distutils*. In its heyday, distutils was the official standard packaging tool
    (ergo, its inclusion), but as of Python 3.10, it is now deprecated in favor of
    setuptools.
  prefs: []
  type: TYPE_NORMAL
- en: 'To package Timecard as a distribution package, I will use the `setuptools`
    and `wheel` modules, the latter of which is not installed by default. It’s good
    to ensure both are up-to-date in your environment, along with pip. You can do
    this with the following terminal command, *inside* your virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Remember to run that inside of the virtual environment you’re using, either
    by activating the virtual environment first or by directly invoking its captive
    pip binary (such as `venv/bin/pip`).
  prefs: []
  type: TYPE_NORMAL
- en: Project Files and Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here’s a quick overview of the files I’ll create for this project:'
  prefs: []
  type: TYPE_NORMAL
- en: '*README.md* is a Markdown file with project information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LICENSE* contains the project license.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pyproject.toml* specifies the build backend and lists build requirements for
    the package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*setup.cfg* contains distribution package metadata, options, and dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*setup.py* used to contain packaging instructions and dependencies; now it
    just ties things together in the source distribution package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MANIFEST.in* lists all the non-code files that should be included in the distribution
    package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*requirements.txt* lists dependencies (which are used differently from *setup.cfg*;
    it’s often useful to have both).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ll cover each of these files in the sections to come.
  prefs: []
  type: TYPE_NORMAL
- en: The recommendations contained herein are based partly on the latest version
    of PyPA’s sample project, which you can see at [https://github.com/pypa/sampleproject/](https://github.com/pypa/sampleproject/).
    The rest of the information comes from the PyCon 2021 talk “Packaging Python in
    2021” by Jeremiah Paige. Bernát Gábor, a member of PyPA, generously reviewed this
    chapter to make sure it was up-to-date.
  prefs: []
  type: TYPE_NORMAL
- en: Where Metadata Belongs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These days, the Python packaging ecosystem is evolving quickly, and standards
    are stuck in an odd limbo between yesterday, today, and tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, all the metadata for your project—its title, description, and
    so forth—belonged in a file called *setup.py*. This file also contained other
    build instructions, such as dependencies to install. Even today, most Python projects
    still use this convention.
  prefs: []
  type: TYPE_NORMAL
- en: The current convention is to move all of this data into a file called *setup.cfg*,
    which is easier to maintain by merit of being *declarative*, meaning it focuses
    on data rather than implementation. This technique is the one I’m using herein.
    The *setup.py* file still has an occasional role to play, but it’s mostly relegated
    to legacy builds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the near future, some packaging data, especially the metadata, will be moved
    to a third file: *pyproject.toml*. This will allow for a clear separation between
    the project metadata and options used by all packaging tools on the one hand,
    and the setuptools-specific configuration in *setup.cfg* on the other hand. As
    of the date of this writing, this new convention isn’t yet implemented by some
    Python packaging tools, but it is expected to be very soon. In the meantime, *pyproject.toml*
    still plays the invaluable role of specifying what packaging tools are used.'
  prefs: []
  type: TYPE_NORMAL
- en: The README.md and LICENSE Files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every good project should have a *README* file, which describes the project,
    its authors, and its basic usage. Nowadays, these are typically written as Markdown
    files (*.md*), which are rendered with nice formatting by most version control
    platforms, such as GitHub, GitLab, Bitbucket, and Phorge.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a significant packaging mistake to skimp on your README! I like to put
    a bit of thought and time into mine, and I include (at minimum) the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A project description, written to “sell” users on the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of authors and contributors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic installation instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic usage instructions, such as how to start the program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The technology stack I used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to contribute code or report an issue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, whether your code is open source or not, you should include a *LICENSE*
    file. In the case of free and open source software, this file should contain the
    complete text of the license. If you need help selecting an open source license
    for your project, check out [https://choosealicense.com/](https://choosealicense.com/)
    and [https://tldrlegal.com/](https://tldrlegal.com/). Otherwise, include the copyright
    information.
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer, you can also include the *.txt* file extension (*LICENSE.txt*)
    or use Markdown (*LICENSE.md*).
  prefs: []
  type: TYPE_NORMAL
- en: On occasion, I may also include such files as *BUILDING.md* or *INSTALL.md*,
    to describe building (for development) or installing the project. Whether you
    use these is up to you.
  prefs: []
  type: TYPE_NORMAL
- en: The setup.cfg File
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When creating a distribution package ultimately intended for production, one
    of the first files to create is *setup.cfg*, which is placed in the root of the
    repository. The *setup.cfg* file contains all the project metadata, dependencies,
    and options for setuptools, and it may be used by other packaging tools as well.
  prefs: []
  type: TYPE_NORMAL
- en: It may be tempting to grab a minimalist *setup.cfg* template, but I, like Mahmoud
    Hashemi, recommend you do not wait until the last phase of your project to start
    on packaging. Using a *src/* directory forces you to think about packaging early.
    This is one of those lessons I wish I’d learned years ago.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as you start work on a project, create your *setup.cfg* file. I’ll break
    down the one for Timecard in this section. This is the most important file in
    your packaging scripts, so I’ll be spending a lot of time on this one file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know about this file format, see the official documentation
    here: [https://setuptools.readthedocs.io/en/latest/userguide/declarative_config.html](https://setuptools.readthedocs.io/en/latest/userguide/declarative_config.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Project Metadata
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In writing the *setup.cfg* file for Timecard, I’ll start with the basic metadata.
    The *setup.cfg* file is divided into *sections*, indicating the tool or set of
    options the parameters to follow belong to. Each section is marked out with the
    section name in square brackets. For example, all the metadata belongs under the
    `[metadata]` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-1: *setup.cfg:1*'
  prefs: []
  type: TYPE_NORMAL
- en: All the data in a *setup.cfg* file is in key-value pairs. The first key here
    is called `name`, and I pass the string value `timecard-app`. Notice that I didn’t
    need to use quotes around the string value. (The documentation outlines the different
    types *setup.cfg* understands and what types it expects for each key.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Although my program is called Timecard, I gave the distribution package the
    name “Timecard-App,” to avoid confusion with the unrelated “Timecard” library
    published on PyPI. This is the name that will be used in the `pip install` command.
    PyPI further restricts this name: it must contain only ASCII letters and numbers,
    although you may include periods, underscores, and hyphens in the name, as long
    as they’re not at the start or end.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The version must be a string following the format outlined in PEP 440, as demonstrated
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-2: *setup.cfg:2*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, it must be made up of two or three integers, separated by dots: either
    in the format `major`.`minor` (`''3.0''`) or `major`.`minor`.`micro` (`''3.2.4''`).
    I strongly recommend employing semantic versioning. In this case, the version
    of Timecard is major version `2`, minor version `0`, and micro (or “patch”) version
    `5`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you need to indicate something more in the version, such as a release candidate,
    beta version, postrelease, or development version, this is permitted using a suffix.
    For example, `'3.1rc2'` would mean “3.1, release candidate 2.” See PEP 440 for
    more details on this convention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tools like *setuptools-scm* can handle version numbers for you, which can be
    helpful if you need to update them often: [https://pypi.org/project/setuptools-scm/](https://pypi.org/project/setuptools-scm/).
    I’ll stick to the manual method for this book, however.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `description` is a one-line description of the package, which I enter here
    explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-3: *setup.cfg:3*'
  prefs: []
  type: TYPE_NORMAL
- en: The `long_description` is a large multiline description, which I am deriving
    directly from the contents of the *README.md* file. The `file:` prefix indicates
    I’m reading from the file that follows. That file *must* exist in the same directory
    as *setup.cfg*, as paths are not supported here.
  prefs: []
  type: TYPE_NORMAL
- en: Since my README is a Markdown file, I also need to indicate that it needs to
    be processed as Markdown text with UTF-8 encoding, via the `long_description_content_type`
    keyword argument. If the README were instead written in reStructuredText (another
    markup language), I’d indicate that with the argument `'text/x-rst'`. Otherwise,
    if this keyword argument is omitted, it defaults to `'text/plain'`. If you view
    my project on PyPI ([https://pypi.org/project/Timecard-App/](https://pypi.org/project/Timecard-App/)),
    you’ll see that the *README.md* is used as the body of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'I also include the license information. There are three ways to do that: explicitly
    via a string value on the `license` key, via a single file with `license_file`,
    or via multiple files with `license_files`. Since I only have one license for
    the whole project, and that is in a *LICENSE* file, I’ll use the second option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-4: *setup.cfg:4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I’ll include more information about the project authorship:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-5: *setup.cfg:5*'
  prefs: []
  type: TYPE_NORMAL
- en: I indicate the `author` (myself) and the contact email for the author, `author_email`.
    In the case of this project, I’m also the project maintainer. If someone else
    were responsible for the packaging, their information would be included with the
    `maintainer` and `maintainer_email` keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: I also include the `url` for more information about the project. Optionally,
    you can include any other links by passing a dictionary to the `project_urls`
    argument. The keys are all strings with the link names, as they will be displayed
    on the PyPI project page. The values are the actual URLs as strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make finding my distribution package in PyPI easier, I include a space-delimited
    list of keywords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-6: *setup.cfg:6*'
  prefs: []
  type: TYPE_NORMAL
- en: If you find yourself translating a *setup.py* file to a *setup.cfg*, be aware
    that *setup.py* uses a comma-separated list instead. Be sure to revise it when
    moving it to *setup.cfg*.
  prefs: []
  type: TYPE_NORMAL
- en: Classifiers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PyPI makes use of *classifiers*, standardized strings that facilitate organizing
    and searching for packages on the index. The complete list of classifiers can
    be found at [https://pypi.org/classifiers/](https://pypi.org/classifiers/).
  prefs: []
  type: TYPE_NORMAL
- en: 'I include the relevant classifiers for Timecard in my *setup.cfg*, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-7: *setup.cfg:7*'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve included all of the relevant classifiers for my project above as a list
    of strings. Your project’s classifier list will likely be different. Browse the
    complete classifier list from PyPI and find the ones relevant to your project.
    A good rule of thumb is to pick at least one for each of the categories—the part
    of each classifier before the first double colon—in the list above. (If this task
    feels overwhelming, you can skip this part until you’re ready to distribute.)
  prefs: []
  type: TYPE_NORMAL
- en: Including Packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now I need to specify what files belong in my package. This is where the *src/*
    directory approach I used in structuring my repository really comes in handy.
    In a new section marked as `[options]`, I include the following keys and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-8: *setup.cfg:8a*'
  prefs: []
  type: TYPE_NORMAL
- en: The `package_dir` key informs setuptools where to find all my packages. It accepts
    a *dict*, which in *setup.cfg* is denoted as an indented, comma-separated list
    of key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Because I’m using the *src/* directory approach, I only need to tell setuptools
    that all packages, denoted by the empty string as the key, are in the directory
    `src`. This is recursive, so any nested packages will also be found.
  prefs: []
  type: TYPE_NORMAL
- en: This key does not actually tell setuptools *what* packages it will find. For
    that part, I need the `packages` key. Instead of manually listing all my packages,
    I can tell setuptools to use its special `find_packages()` function by passing
    the value `find:` ❶ (note the trailing colon!). This technique is especially helpful
    when your project consists of multiple top-level packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `find:` function can find all the packages in a given directory, but it
    has to know where to look first. I provide that information in a separate section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-9: *setup.cfg:9*'
  prefs: []
  type: TYPE_NORMAL
- en: On the key `where`, I provide the name of the directory to search for packages
    on, namely `src`.
  prefs: []
  type: TYPE_NORMAL
- en: Including Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Not everything that ships in a package is code. I need to include some non-code
    files as well. Returning to my `[options]` section, I indicate that setuptools
    will be including some non-code files like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-10: *setup.cfg:8b*'
  prefs: []
  type: TYPE_NORMAL
- en: In *setup.cfg*, the values `True` and `False` are interpreted as boolean values,
    not as strings.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to specify what non-code files to include. The approach I’m
    taking in this project is to use the *MANIFEST.in* file to list all non-code files
    I want included in the project. (I’ll cover this file shortly.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach for including non-code files, which I’m not showing here,
    is to use the `[options.package_data]` section. This gives you finer-grained control
    over what is and isn’t included, but it is likely overkill for your average project.
    The setuptools documentation has a good example with this approach: [https://setuptools.readthedocs.io/en/latest/userguide/declarative_config.html](https://setuptools.readthedocs.io/en/latest/userguide/declarative_config.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, I’ll define my project’s dependencies in the `[options]` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-11: *setup.cfg:8c*'
  prefs: []
  type: TYPE_NORMAL
- en: I can specify the version of Python that the project requires, using `python_requires`
    in place of a package name. Notice the key and value are still separated by an
    equal sign, even though the value may also start with an equal, greater-than,
    or less-than sign. Something like `python_requires = ==3.8` is completely valid,
    as the `==3.8` is a string.
  prefs: []
  type: TYPE_NORMAL
- en: I denote that Timecard requires Python 3.6 or later. I also assume it won’t
    work with the theoretical Python 4, since major releases of Python aren’t guaranteed
    to be backward compatible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I’ll list the packages that my project depends on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-12: *setup.cfg:8d*'
  prefs: []
  type: TYPE_NORMAL
- en: The `install_requires` key expects an indented list of values. Each value in
    the list specifies a Python package dependency. Timecard relies on the `PySide2`
    library and uses features introduced in version 5.15.0 of that library. Technically,
    I could have omitted the version and merely listed `PySide2`, but that is often
    a bad idea. It’s better to test the different versions and determine the oldest
    one your code will work with. You can always set this to the version you are currently
    using and change it later.
  prefs: []
  type: TYPE_NORMAL
- en: 'I could also require certain packages only for particular versions of Python.
    I don’t need this behavior in this example, but here’s what it might look like
    if I wanted to install PySide2 only for Python 3.7 and earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After a semicolon at the end of the dependency I want to limit, I include `python_requires`,
    a comparison operator, and the version *in quotes*. The quotes are necessary,
    or you’ll get a cryptic `Expected stringEnd` error.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it is seldom necessary to only install packages for certain versions
    of Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optionally, I can use the `[options.extras_require]` section to specify additional
    packages used for certain optional features. For example, if I wanted to allow
    installing this distribution package with tests, I’d need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-13: *setup.cfg:10*'
  prefs: []
  type: TYPE_NORMAL
- en: When I install Timecard via `pip install Timecard-App[test]`, it will install
    both the Timecard distribution package and everything I listed under `test` here.
    You can name the keys whatever you like, as long as you use alphanumeric characters
    and underscores. You can also include as many keys as you like.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Entry Points
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lastly, I need to specify the *entry points*, the means by which a user starts
    the program. Instead of having to write custom executable Python scripts to serve
    as entry points, I can let setuptools do this for me. It will even create these
    scripts as *.exe* files on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Entry points are specified in the section `[options.entry_points]`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-14: *setup.cfg:11*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two possible keys here: `''gui_scripts''`, for starting the program’s
    GUI, and `''console_scripts''`, for starting the command-line version of the program.
    The value of each is a list of strings containing assignment statements, which
    assign a particular function to a name that will be the name of the executable
    file or script. In this case, Timecard needs a single GUI script named *Timecard-App*,
    which calls the `main()` function in the module *timecard/__main__.py*.'
  prefs: []
  type: TYPE_NORMAL
- en: The setup.py File
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before *setup.cfg* became the convention, most projects used *setup.py* to store
    their project’s metadata and build instructions. All the information now provided
    to *setup.cfg* was instead passed directly to the `setuptools.setup()` function
    as keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The *setup.py* file is a Python module like any other, so using it for packaging
    configuration is considered *imperative*—it focuses on how packaging takes place.
    This stands in contrast to the data-centric (and more error-proof) *declarative*
    approach with *setup.cfg*.
  prefs: []
  type: TYPE_NORMAL
- en: 'One difficulty with this approach was that folks tended to get clever about
    their setup configurations. All manner of unrelated functionality crept in: scraping
    versions from files, creating git tags, publishing to PyPI, and so on. This ran
    the risk of introducing confusing bugs into the packaging process, which is already
    particularly hard to debug and could utterly block someone else’s packaging efforts.
    This has happened to me before!'
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, in many modern projects, *setup.py* is no longer needed at all;
    *setup.cfg* can be used for all setuptools configurations instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if your project needs good backward compatibility, uses C-extensions,
    or requires tools that depend on *setup.py*, you will want to include the following
    minimal *setup.py* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-15: *setup.py*'
  prefs: []
  type: TYPE_NORMAL
- en: That file merely imports the `setup()` function from the `setuptools` module
    and calls it. In the past, all the packaging data would have been passed to `setup()`
    as keyword arguments, but now, that all lives in *setup.cfg*.
  prefs: []
  type: TYPE_NORMAL
- en: No shebang line is necessary in this file; let the build tools find the interpreter
    they want to use.
  prefs: []
  type: TYPE_NORMAL
- en: The MANIFEST.in File
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *manifest template*, *MANIFEST.in*, provides a list of all the non-code
    files that should be included in the distribution package. These files can come
    from anywhere in your repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-16: *MANIFEST.in:1*'
  prefs: []
  type: TYPE_NORMAL
- en: Since my setup files are using the *README.md* and *LICENSE* files, I must include
    them here. I can list any number of files after an `include` directive, separating
    each with spaces. Manifest templates also support *glob patterns*, wherein I can
    use an asterisk (`*`) as a wildcard. For example, `*.md` matches all Markdown
    files, so any markdown files in the root of my repository, including *README.md*,
    are automatically included.
  prefs: []
  type: TYPE_NORMAL
- en: 'I also want to include all the files in the directories *src/timecard/resources/*
    and *distribution_resources/*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-17: *MANIFEST.in:2*'
  prefs: []
  type: TYPE_NORMAL
- en: The `graft` keyword includes all files that are in the specified directories
    and below. The *distribution_resources/* directory is where I keep OS-specific
    installation files for this particular project.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few more important directives, which I’m not using for Timecard.
    Here’s a more complex example with a different *MANIFEST.in* file (not belonging
    to my Timecard project):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this example, I add all files under the *stuff/* directory that have the
    *.ini* file extension. Next, I include the entire *data/* directory with the `graft`
    directive. The `prune` directive then goes back and excludes all files in the
    *data/temp/* subdirectory. I also exclude all files in *data/important/* that
    have the *.scary* file extension. None of the excluded files are deleted, but
    they are left out of the package.
  prefs: []
  type: TYPE_NORMAL
- en: The order of lines in the manifest template matters. Each subsequent directive
    adds to or removes from the list of files compiled by the prior lines. If you
    were to move the `prune` directive above the `graft` directive, the files in *data/temp/*
    would *not* be excluded!
  prefs: []
  type: TYPE_NORMAL
- en: Your manifest template will be used by setuptools to compile a *MANIFEST* file,
    which contains a complete list of all the non-code files being included in your
    distribution package. While you could theoretically write this *MANIFEST* file
    yourself, listing one file per line, this is strongly discouraged, as it’s easier
    to mess up the *MANIFEST* file than the *MANIFEST.in* file. Trust setuptools to
    follow the directives you give it in the manifest template.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few more directives and patterns that you can use in *MANIFEST.in*.
    Check out [https://packaging.python.org/guides/using-manifest-in/](https://packaging.python.org/guides/using-manifest-in/)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: The requirements.txt File
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may remember this file from Chapter 2. The *requirements.txt* file contains
    a list of Python packages that your project depends on. This probably sounds like
    its duplicating `install_requires` from *setup.cfg*, but I usually recommend that
    you use both, for reasons I’ll describe shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the *requirements.txt* for Timecard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-18: *requirements.txt*'
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder if there is a way to use the contents of *requirements.txt* in
    *setup.cfg*, but in fact, it’s best if you keep the two separate. In practice,
    they serve slightly different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: While the requirements listed in *setup.cfg* include everything you need to
    install your distribution package, the *requirements.txt* file is best thought
    of as a list of everything you need to re-create the complete development environment—all
    the optional packages and development tools needed to participate in developing
    your project, as opposed to merely using it.
  prefs: []
  type: TYPE_NORMAL
- en: Under some circumstances, you may omit *requirements.txt* if you list all your
    development dependencies as optional dependencies. The other benefit of using
    *requirements.txt*, however, is the ability to use a specific version of a library
    or tool—called *pinning*—for development, while enforcing looser version requirements
    for your users via *setup.cfg*. For example, your application might require `click
    >= 7.0`, but you’re developing the new version to use `click == 8.0.1` in particular.
    Pinning is mainly useful for application development. If you’re developing a library,
    it’s best to avoid pinning, since you can’t really make good assumptions about
    the package versions that a user of your library will need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another major benefit of having a *requirements.txt* file is that, as mentioned
    in Chapter 2, you can quickly install everything needed for development in one
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'I still recommend you keep *requirements.txt* to the things that your project
    truly needs, especially when versions matter. You generally wouldn’t include tools
    like `black` or `flake8`, which can be swapped out for other tools without breaking
    anything. Sometimes, I’ll create a separate *dev-requirements.txt* file (or *requirements.dev.txt*,
    if you prefer) with all of my optional development tools. Then, to set up my complete
    environment in a virtual environment, I’ll only need to run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have quite a number of packages installed in your working virtual environment,
    it can be a pain to try to turn this into a *requirements.txt* file. To aid you
    in this, you can use the command `pip freeze` to generate a complete list, with
    versions, of all packages installed in the environment. Then, you can redirect
    the contents into a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That command works the same on Windows, macOS, and Linux, exporting the complete
    list of packages installed within *venv/*, dependencies and all, to the file *requirements.txt*.
    Be sure to inspect that file for errors and *remove your own package from it*—it
    wouldn’t make sense for your package to rely on itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want both *requirements.txt* and *dev-requirements.txt*, you’ll want
    to `pip freeze` on two separate virtual environments: one that runs your package,
    and the other that contains your complete development environment. In both cases,
    inspect the output file and remove your own package from it!'
  prefs: []
  type: TYPE_NORMAL
- en: The pyproject.toml File
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *pyproject.toml* file serves a few purposes, but the most important is to
    specify the *build system* used to package your project, a standard introduced
    in PEP 517 and PEP 518.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 18-19: *pyproject.toml*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `[build-system]` section contains information about what packages are needed
    for building the distribution package: in this case, `setuptools` and `wheel`.
    These requirements are specified by assigning a list of strings to `requires`,
    with each string following the same convention as `install_requires` in *setup.py*.
    You’ll notice here that I can use any version of `wheel`, but I must use version
    40.8.0 or later of `setuptools`. The latter is necessary since it is the first
    version of `setuptools` that supports PEP 517 and PEP 518.'
  prefs: []
  type: TYPE_NORMAL
- en: The `build-backend` property specifies the scheme used for building the project,
    in this case, `setuptools.build_meta`. If you were using a different build tool,
    such as Poetry or Flit, you’d specify that here, according to the tool’s documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The *pyproject.toml* file is also one of the common files for storing Python
    tool configurations. Many of the major linters, autoformatters, and testing tools
    (albeit not all of them) support their configurations being stored in this file.
  prefs: []
  type: TYPE_NORMAL
- en: PEP 518 introduced the *pyproject.toml* file for project configuration, but
    it can increasingly be used to store tool configuration as well, thereby cutting
    down on the number of files in a project. There is heated debate about adding
    support for *pyproject.toml* to some prominent tools like Flake8\. It’s not as
    straightforward as it looks. If you really want Flake8 and *pyproject.toml* to
    play well together, check out *Flake9*, a fork of Flake8 that accomplishes just
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Setup Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If everything has been configured correctly, you’ll be able to install your
    project in a virtual environment. I’ll do that now, using the following command
    (assuming I have a fresh virtual environment called `venv`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: I particularly recommend testing this out in a fresh virtual environment the
    first time. The trailing dot installs whatever package is detailed by the *setup.cfg*
    file in the current directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Watch the output when you install and correct any warnings or errors. Once
    your package is installed successfully, try to run your project using the entry
    point(s) you specified in the *setup.cfg*, which should be installed as executables
    in the *bin/* directory of the virtual environment. I’d run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: If your project is a library, rather than an application, open the Python shell
    within the virtual environment (`venv/bin/python`) and import the library.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take the time to test out your project now. Does everything work as expected?
    Most packaging and distribution tools depend on this instance working correctly!
    If you encounter any new bugs or errors, go back and fix them. A few common causes
    of problems appearing only in a virtual-environment installation of your project
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing dependencies that need to be added to *setup.cfg*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data files not being included correctly via the *MANIFEST.in* or *setup.cfg*
    files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False assumptions in your code about the current working directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have your distribution package installed and working in a virtual environment,
    you’re ready to move on to the next step!
  prefs: []
  type: TYPE_NORMAL
- en: Installing as Editable
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In some circumstances, it can be useful to install your project in editable
    mode, so the virtual environment will directly use the source code files in *src/*.
    Any changes made to the code will immediately be reflected in the test installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install in editable mode, supply the `-e` flag to `pip`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Installing your package in editable mode makes testing and development a lot
    easier, as you don’t have to reinstall the package each time. However, it’s not
    without its drawbacks. Installing in editable mode can make it possible for the
    virtual environment to find external packages and modules that were not explicitly
    required in *setup.cfg*, thereby covering up packaging problems. Only use `-e`
    for testing your code, not your packaging.
  prefs: []
  type: TYPE_NORMAL
- en: Building Your Package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you’ve ensured that your *setup.cfg* and related files are configured correctly,
    it’s time to build your source distribution and wheel artifacts. Run the following
    commands in your terminal, from the root of your project’s repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The installation command ensures that `setuptools`, `wheel`, and `build` are
    installed and up-to-date in the current environment.
  prefs: []
  type: TYPE_NORMAL
- en: The next command builds according to whatever `build-backend` is specified in
    *pyproject.toml*. In this case, the project will be built with setuptools, applying
    the configuration in the *setup.cfg* file in the current directory. This command
    builds a source distribution, or sdist, and a built distribution or wheel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two artifacts are saved in the newly created *dist/* directory: the *.tar.gz*
    is your source distribution, and the *.whl* is the `build` distribution wheel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few other commands available for `build`, which you will find in
    the documentation: [https://pypa-build.readthedocs.io/en/latest/](https://pypa-build.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if you’re only using the default invocation of `build`, you can
    just run `pyproject-build`.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing on pip (Twine)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From here, you’re ready to publish your distribution package! In this section,
    I’ll do exactly that with Timecard.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading to Test PyPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you upload your project to the official PyPI index, test everything out
    one more time via Test PyPI, which is an altogether separate index specifically
    for testing out the tools. If you’re experimenting, you’re welcome to use Test
    PyPI. Packages and user accounts are periodically pruned, so don’t worry if you
    leave a bit of a mess.
  prefs: []
  type: TYPE_NORMAL
- en: To upload to the index, you must first create an account at [https://test.pypi.org/account/register/](https://test.pypi.org/account/register/).
    If you had one a while back but it’s not working now, don’t worry; as part of
    the regular pruning, old accounts are deleted. You can safely make a new one.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’re logged in, go to **Account settings** and scroll down to **API tokens**.
    Select **Add API Token**. You can name this token whatever you like, but be certain
    you set Scope to **Entire account** if you’re uploading a new project.
  prefs: []
  type: TYPE_NORMAL
- en: After creating the API token, you must save the entire token (including the
    leading `pypi-`) before leaving the page, as it will never be shown again. You’ll
    need this token in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, you can always delete tokens from the Account settings page. Do
    this any time you no longer remember the token or have no need of it anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll upload my Timecard-App distribution package to Test PyPI. (You’ll need
    to try uploading a package with a different name; Timecard-App will be taken by
    the time you read this.) For this step, I use a package called *twine*, which
    I install to my user environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'I use twine to upload the artifacts from my project’s *dist/* directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Notice I’ve explicitly specified that I’m uploading to the *testpypi* repository,
    which twine knows about by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'When prompted, enter the username *__token__* and use your API key from earlier
    as the password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Watch the terminal output carefully for any errors or warnings. If you need
    to make any corrections to your project or its packaging files, be certain to
    delete the *dist/* directory and cleanly rebuild the *sdist* and *bdist_wheel*
    artifacts before trying to upload again.
  prefs: []
  type: TYPE_NORMAL
- en: If all goes well, you will be given a URL where you can see your distribution
    package on Test PyPI. Make sure everything on that page looks correct.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Your Uploaded Package
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, make sure you can install your distribution package from Test PyPI in
    a *fresh* virtual environment. I’ll do that now with my Timecard-App distribution
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next step to work, I’ll need to manually install the dependencies for
    my package. This is one of the reasons I have a separate *requirements.txt* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I can install the package itself. Because this is a particularly long
    command, I’m splitting it up UNIX-style, with the backslash character (`\`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Because I’m testing the distribution package I uploaded to Test PyPI, rather
    than the regular PyPI, I have to explicitly tell pip to use that as the source
    repository, which I do with the `--index-url` argument. However, I don’t want
    to install any of the package dependencies from [test.pypi.org](http://test.pypi.org)—they
    might be missing, broken, malicious copycats, or otherwise wrong—so I pass the
    `--no-deps` flag. Finally, I specify that I’m installing the distribution package
    `Timecard-App`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If all went well, I should be able to invoke Timecard-App within that virtual
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: I try it out as before, to make sure everything works as expected, and it does!
    At this point, I’d want to try it out on other machines to ensure it functions
    as expected. I can use the `pip install` command from a moment ago to install
    this distribution package on any machine that’s connected to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading to PyPI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once I’m certain that Timecard-App is ready for prime time, I can repeat the
    entire upload process with PyPI at [https://pypi.org/](https://pypi.org/): create
    a user account (if necessary), log in, create an API key, and finally upload with
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The official PyPI is the default target, so once that upload completes, you’ll
    be given a new URL: the one for your project on the PyPI. Congratulations! You
    can now share this link exuberantly. You’ve shipped software!'
  prefs: []
  type: TYPE_NORMAL
- en: Alternative Packaging Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see, pip, setuptools, wheel, and twine do a pretty good job these
    days, but there are a lot of steps and details to using them. There are a couple
    of alternative tools you may benefit from learning on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Poetry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you only learn one of these alternative tools, make it this one!
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using four different tools, some Python developers prefer *Poetry*,
    which handles everything from dependency management to building and publishing
    your distribution package. Absolutely all of the packaging configuration, from
    dependencies to metadata, goes into *pyproject.toml*.
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to use Poetry is easy because it has excellent, succinct documentation,
    which is especially clear if you’re already somewhat familiar with setuptools.
    Information, installation instructions, and documentation can all be found at
    [https://python-poetry.org/](https://python-poetry.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Flit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Flit* is a tool that focuses on making the easy packaging scenarios easier—namely,
    by building and publishing pure Python distribution packages—and leaving the hard
    stuff for other, more complex tools to handle. It uses a few simple commands to
    handle building and publishing your distribution package. Many of the ideas from
    Flit have trickled into other tools and workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation is the best place to find more information about
    Flit: [https://flit.readthedocs.io/en/latest/index.html](https://flit.readthedocs.io/en/latest/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Distributing to End Users
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is still one leg of the journey to go before you reach your goal of being
    able to distribute software to end users. Installing via pip is not a good means
    of distributing your software to end users, for two reasons. First, most users
    don’t know anything about pip. Second, pip was never intended to deploy software
    in that manner. There are too many things that could go wrong when installing
    from PyPI, all of which would require the intervention of a Python developer well-versed
    in the ways of pip. To ship software to non-developers, you need a more robust,
    user-oriented solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mahmoud Hashemi describes layers of the packaging gradient for shipping to
    an end user. I’ve adapted them here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PEX:** Uses system-wide Python.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Freezers:** Includes Python.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Images and containers:** Includes most or all system dependencies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Virtual machines:** Includes *kernel*, the “heart” of the operating system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hardware:** Includes . . . well, everything!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at each of these options in a little more depth and consider which
    would be appropriate for Timecard.
  prefs: []
  type: TYPE_NORMAL
- en: PEX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The lowest-level option for distributing a stand-alone artifact—one that doesn’t
    need to be installed from pip—is a format called PEX (short for Python Executable).
    It allows you to package an entire virtual environment as a stand-alone file,
    which is essentially a neatly structured *.zip* file. This PEX file then relies
    on the Python interpreter provided by whatever system it’s being run on. Once
    you have a PEX, you can distribute it to anyone on Mac or Linux who has Python
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: PEX is far from intuitive, in terms of usage. It’s easy enough to turn a virtual
    environment into a PEX, but actually specifying a script to run on execution takes
    a bit more work. What’s more, PEX only works on Mac or Linux, so it’s not a viable
    option if you need to distribute on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Since PEX is oriented toward developers, it’s definitely not a good fit for
    distributing Timecard. If you want to learn more about PEX, read their documentation
    at [https://pex.readthedocs.io/](https://pex.readthedocs.io/). Alex Leonhardt
    also has an excellent article about PEX ([https://medium.com/ovni/pex-python-executables-c0ea39cee7f1](https://medium.com/ovni/pex-python-executables-c0ea39cee7f1))
    that is considerably easier to digest than the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Freezers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most portable means by far of packaging and distributing a Python application
    is with a *freezer*, which bundles the compiled Python code, the Python interpreter,
    and all the package dependencies into a single artifact. With some freezers, system
    dependencies are included as well. The benefit of all this is that you wind up
    with a single executable file in the target system’s preferred format, at the
    cost of an increased artifact size (usually by about 2–12 MB).
  prefs: []
  type: TYPE_NORMAL
- en: Far and away, this is one of the most common ways of distributing Python applications.
    It’s used by such programs as Dropbox and Eve Online.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of freezers in existence, but the three most common at present
    are *PyInstaller*, *cx_Freeze*, and *py2app*. If you’re building GUI-based applications
    with the Qt 5 toolkit, *fman Build System* is another great option. Yet another
    option is *py2exe*, although it is presently unmaintained.
  prefs: []
  type: TYPE_NORMAL
- en: PyInstaller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: My personal favorite freezer is PyInstaller. It has the particular advantage
    of working on all major operating systems. Although you will need to run PyInstaller
    separately on each of the targeted environments, you will seldom need to configure
    it more than once.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is quite a bit to learn about PyInstaller. You’ll find extensive guidance
    for its use, as well as how to handle various errors and tricky situations, in
    the official documentation: [https://pyinstaller.readthedocs.io/](https://pyinstaller.readthedocs.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: PyOxidizer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the newer kids on the block is PyOxidizer. It’s a promising-looking cross-platform
    tool for converting your project into a single executable file, bundled with the
    Python interpreter. The focus is on ensuring it remains easy to package, distribute,
    and install the end product.
  prefs: []
  type: TYPE_NORMAL
- en: Complete documentation, as well as a breakdown of what gives PyOxidizer an advantage
    over other tools, can be found at [https://pyoxidizer.readthedocs.io/en/stable/index.html](https://pyoxidizer.readthedocs.io/en/stable/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: py2app
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you only want to package your project for macOS, *py2app* is a great option.
    It works off your project’s *setup.py* file and freezes down into a single *.app*
    file.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about py2app, see the official documentation at [https://py2app.readthedocs.io/en/latest/tutorial.html](https://py2app.readthedocs.io/en/latest/tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: cx_Freeze
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another option for freezing your project is *cx_Freeze*, a cross-platform tool
    for building on Windows, Mac, and Linux. It’s quite a bit older than PyInstaller,
    but it still works well. If you’re having trouble with PyInstaller or py2app,
    try this one.
  prefs: []
  type: TYPE_NORMAL
- en: Information and documentation can be found at [https://cx_freeze.readthedocs.org/](https://cx_freeze.readthedocs.org/).
  prefs: []
  type: TYPE_NORMAL
- en: fman Build System
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you’re building a GUI-based application using the Qt 5 library—Timecard is
    one such application—you can build and package your project once for all operating
    systems with *fman Build System*. Unlike other tools, it even creates an executable
    installer on Windows, a *.dmg* on macOS, a *.deb* package on Debian-based Linux,
    an *.rpm* on Fedora-based Linux, and a *.tar.xz* for everyone else.
  prefs: []
  type: TYPE_NORMAL
- en: The fman freezer requires you to set up your project in a particular manner,
    so if you want to use it, you’ll have the best results if you use it from the
    start of your project. Otherwise, you’ll need to restructure it according to what
    fman Build System needs (which is why Timecard doesn’t use it).
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information, a great tutorial, and the complete documentation
    at [https://build-system.fman.io/](https://build-system.fman.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Nuitka
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I mentioned back in Chapter 1 that the *Nuitka* compiler allows you to transpile
    Python code to C and C++ and then assemble that down to machine code. Nuitka is
    practically a separate implementation, and the end-result executable is about
    two times faster than CPython.
  prefs: []
  type: TYPE_NORMAL
- en: As of the date of this writing, Nuitka has reached feature parity with Python
    3.8\. They’re working on adding 3.9+ features and further optimization. In any
    case, this is an exciting project to watch.
  prefs: []
  type: TYPE_NORMAL
- en: If you want truly “compiled” Python code, this is the tool you’re looking for.
    More information and documentation are available at [https://nuitka.net/pages/overview.html](https://nuitka.net/pages/overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: Images and Containers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All the packaging options I’ve covered up to this point are limited by a common
    factor: what system libraries are installed on the user’s machine. It’s possible
    to bundle some of these libraries, such as is seen with PyInstaller, but those
    are still subject to their *own* dependencies, some of which cannot be bundled.
    When you start getting into more complex applications, this can become a tricky
    problem to solve.'
  prefs: []
  type: TYPE_NORMAL
- en: This becomes particularly difficult when distributing on Linux. With so many
    Linux-based operating systems (each with multiple versions) and countless combinations
    of packages, building once for all can be a royal pain. The solution is found
    in *containers*, which are self-contained environments that bring all their own
    dependencies. Multiple applications can be installed on the system, each in its
    own container, and it won’t matter if they have different or even conflicting
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another advantage of using containers is *sandboxing*, which limits the containerized
    application’s access to the system. This provides transparency and control to
    users: they know what privileges any given container has, and in many cases, they
    can control those privileges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At present, there are four major containers: *Flatpak*, *Snapcraft*, *Appimage*,
    and *Docker*. Each one has unique advantages.'
  prefs: []
  type: TYPE_NORMAL
- en: Flatpak
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Flatpak* allows you to package an application as a standalone unit that can
    be installed on virtually any Linux environment, as well as Chrome OS. It is highly
    forward compatible, meaning your package will continue to work even on as-yet
    unreleased versions of operating systems that support Flatpak. It is not a container
    in the strictest sense, but it functions similarly. Even so, installed Flatpaks
    can share some dependencies they might have in common.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons I particularly like Flatpak is that you can select or build
    each of the dependencies or components you need. I can know that if my Flatpak
    works on my machine, it will work on others. The extra degree of control and predictability
    it provides makes it easier to work with snarly packaging scenarios in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flatpak also has its own app store, *Flathub*, which makes it easy for end
    users to browse and install applications on their Linux machines. For more information
    about and complete documentation on Flatpak, see their official website: [https://flatpak.org/](https://flatpak.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see how I packaged Timecard with Flatpak here: [https://github.com/flathub/com.codemouse92.timecard](https://github.com/flathub/com.codemouse92.timecard).'
  prefs: []
  type: TYPE_NORMAL
- en: Snapcraft
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *Snapcraft* format, maintained by Canonical (the company behind the Ubuntu
    operating system), packages your application into a dedicated container with its
    own filesystem. It is sandboxed from the rest of the system, accessing and sharing
    as little as possible. Because of its structure, you can build snaps from any
    development environment, including Windows and macOS, although you cannot install
    snaps in those environments. Snapcraft also has its own associated app store,
    the *Snap Store*.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the footprint of an installed snap can be quite large, as it
    brings everything *for each container*, excepting the kernel and a handful of
    core dependencies; it does not share dependencies between snaps. It also can be
    difficult to give a snap the correct permissions for many user applications to
    function. Due to these and other criticisms, some Linux environments have dropped
    their official support for Snapcraft.
  prefs: []
  type: TYPE_NORMAL
- en: Despite all this, Snapcraft is still a viable container format with a fairly
    loyal following. You can learn more about the format and find complete documentation
    at [https://snapcraft.io/](https://snapcraft.io/).
  prefs: []
  type: TYPE_NORMAL
- en: AppImage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *AppImage* format provides self-contained executables that don’t require
    anything, including themselves, to be installed. In many ways, an AppImage behaves
    like a macOS application. Unlike Flatpak and Snapcraft, AppImage requires no infrastructure
    on the target system, although the user may choose to use *appimaged* to automate
    registering AppImages with the system.
  prefs: []
  type: TYPE_NORMAL
- en: AppImage is intended to be decentralized, allowing you to provide your own download
    to the end user. You can even issue updates to your package by integrating *AppImageUpdate*.
    Technically, AppImage does have an app store of sorts, *AppImageHub*, where you
    can browse through many of the apps that are packaged in the format. New apps
    are added via a pull request against the store on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: The sole disadvantage of AppImage is that you need to test your package against
    every Linux distribution you plan to support. Your package *can* rely on existing
    system libraries, and in fact, it must do so for a few essentials like *libc*
    (the C language standard library, which is used by nearly everything). As a result,
    this can create a “works on my machine” scenario, where the AppImage may be implicitly
    relying on a system library and then may fail when run on another Linux system
    that lacks that library.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve that same aim, it is recommended you build your AppImage on the oldest
    environment you want to support, as it collects and bundles libraries from the
    current environment itself. AppImages are pretty decently forward compatible,
    but they’re not intended to be backward compatible.
  prefs: []
  type: TYPE_NORMAL
- en: Still, if you don’t mind working with some extra environments, AppImage can
    be a fantastic way to distribute your software to any Linux machine, without any
    other infrastructure being required. More information about the format and complete
    documentation can be found at [https://appimage.org/](https://appimage.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In modern software development parlance, *Docker* is usually the first thing
    that people think of when they hear “container.” It allows you to define a custom
    environment, bringing everything except the kernel. This is the one format out
    of the four I’m covering that will work on Windows and macOS, in addition to Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is primarily geared toward deploying on servers, rather than for user
    applications, as it requires quite a bit of setup on the target machine. Once
    Docker is configured, it’s relatively trivial to launch an image. This makes it
    ideal for distributing server applications.
  prefs: []
  type: TYPE_NORMAL
- en: Because a Docker image is a fully self-contained environment, it’s easy to create
    one for your project. You start by defining a *Dockerfile*, which outlines the
    steps for building the image. You start with a base image, such as one for a particular
    operating system, and you install all the packages and dependencies you need.
    You can even install via pip in the context of the Dockerfile. Docker converts
    the Dockerfile into an image, which can be uploaded to a registry like *Docker
    Hub* and then downloaded onto the client machine.
  prefs: []
  type: TYPE_NORMAL
- en: Complete information and exhaustive documentation can be found at [https://www.docker.com/](https://www.docker.com/).
  prefs: []
  type: TYPE_NORMAL
- en: A Note on Native Linux Packaging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linux users will notice that I haven’t touched native Debian or Fedora packaging
    at all. These packaging formats are still relevant, but decreasingly so as more
    portable formats like the preceding gain in adoption. Both Debian and Fedora packaging
    can be particularly difficult, while offering few, if any, advantages over portable
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: In case this sounds faddish, I assure you that I have been among the slowest
    to consider Flatpak, Snapcraft, and Appimage as viable alternatives to my beloved
    Debian packages. The difference in end user experience is slightly improved in
    these newer portable formats, but more importantly, the *developer* experience
    is significantly better. All three leverage varying degrees of sandboxing, in
    a manner similar to virtual environments and in stark contrast to native packaging
    formats, where one must be concerned with the exact versions of dependencies on
    each end user machine. What’s more, while portable packaging formats generally
    play well with virtual environments and PyPI, native packaging formats seldom
    do, especially when used in full compliance with the standards and policies of
    the distribution’s package repositories.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to package your Python project using Debian or Fedora packaging,
    you can certainly do so. Tools like *dh-virtualenv* can help! Still, be prepared
    for a battle if your project has any significant dependencies. Before you try
    to distribute your project in any native packaging format, be absolutely certain
    that portable packaging formats will not suffice. This is a decision only you
    can make.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every project needs documentation, and yes, that includes yours. The best code
    in the world means nothing if the end user doesn’t know how to install and use
    it!
  prefs: []
  type: TYPE_NORMAL
- en: In the case of particularly small projects, a single *README.md* may be sufficient,
    so long as it is easily discoverable by users. For anything more robust, you need
    a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: The historic answer to documentation in Python was the built-in module *pydoc*,
    but over the past several years, this has been utterly eclipsed by *Sphinx*. Nearly
    all documentation in the Python world, including the official documentation for
    Python itself, is built with Sphinx. In fact, while Sphinx was originally built
    for Python projects, its robust feature set and ease of use has led to its wide
    adoption across the entire programming industry.
  prefs: []
  type: TYPE_NORMAL
- en: Sphinx builds the documentation using a markup language called *reStructuredText*,
    abbreviated as *reST*. While a bit more complex and exacting than Markdown, reST
    is packed full of powerful features for even the most complicated technical writing.
    The end result can be exported to HTML, PDF, ePUB, Linux man pages, and many other
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your project’s documentation belongs in a separate directory, conventionally
    named *docs/*, in the project repository. If you have the `sphinx` package installed
    in your environment—usually, your development virtual environment—you can build
    the basic file structure and configuration by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You will be guided through several questions. For most things, I recommend using
    the defaults, displayed in square brackets (`[ ]`) at each prompt, until you know
    better.
  prefs: []
  type: TYPE_NORMAL
- en: For most of your documentation, you will write your own reStructuredText (*.rst*)
    files by hand, saving them in this *docs/* directory. There is no replacement
    for handwritten documentation! Expecting a user to learn your software purely
    from API documentation is like teaching someone to use a toaster by explaining
    the electrical specifications of the heating element.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, in some projects, especially libraries, it is useful to pull
    in the docstrings from your code. This is possible with Sphinx as well, using
    its *autodoc* feature.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to get started with Sphinx and reStructuredText, including the
    autodoc feature, is to read the official Quick Start Guide at [https://www.sphinx-doc.org/en/master/usage/quickstart.html](https://www.sphinx-doc.org/en/master/usage/quickstart.html).
    That website also provides the rest of the Sphinx documentation.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re getting ready to release your project, you will almost certainly
    want to publish your documentation online. For open source projects, one of the
    easiest ways to do this is to sign up for a free account on Read the Docs. That
    service specifically works with Sphinx and reStructuredText, and it can automatically
    update your documentation from your repository. For more information and to sign
    up, visit [https://readthedocs.org/](https://readthedocs.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re getting ready to start a project, consider how you want to handle
    packaging. There are a lot of options for packaging and distributing Python applications,
    so that leaves the question of which tools to use. As the developer of your own
    project, you’re the only person who can ultimately determine the best packaging
    scheme for your situation. If you’re completely lost, here’s my own opinion.
  prefs: []
  type: TYPE_NORMAL
- en: First, I strongly recommend using a *src/* directory for your code. It makes
    everything else easier. Then, get your project set up so you can install it in
    a virtual environment with pip. Personally, I use setuptools, although both Poetry
    and Flit are excellent options; use whichever you like more.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re developing a library or command-line tool for other Python developers
    to use, plan to publish it to the PyPI. If your project is an end user application
    or command-line program, I recommend packaging it into a stand-alone artifact
    using a tool like PyInstaller. For Linux distribution, I also strongly recommend
    creating a Flatpak. If you’re building a server application, on the other hand,
    I recommend packaging it into a Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: The last level of the packaging gradient is to deploy your project embedded
    on hardware. There are countless ways to do this, but some of the more popular
    options include single-board computers like *Arduino* and *Raspberry Pi*. This
    is a deep enough topic that entire books are dedicated to it. In Chapter 21, I’ll
    refer you to some resources for further study in this arena.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, these are my opinions, based on my own adventures in Python packaging.
    Regardless, remember that all these tools exist for a reason, and something that
    works well for my projects may not be suitable for yours. As I said at the start
    of this chapter, whatever packaging techniques you ultimately use, they should
    produce a reasonably portable, stable, “just works” package.
  prefs: []
  type: TYPE_NORMAL
- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Debugging and Logging
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Mistakes in code are inevitable. The bugs you’ll encounter can range from simple
    typos to malformed logic, from misunderstood usage to those strange errors that
    originate from deeper in the tech stack. As Lubarsky’s Law of Cybernetic Entomology
    observes, “There is always one more bug.” When things go horribly wrong in your
    code, you’re going to need the tools to find and fix the problem. In this chapter,
    you’ll learn all about those tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ll begin by covering three features of the Python language that you can add
    to your code to help you debug it later: warnings, logging, and assertions. These
    are improvements over using print statements for debugging purposes, as you are
    likely to forget where your “debugging” print statements are or may be tempted
    to leave them in place in production.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, I’ll guide you through the use of the Python debugger (`pdb`), which helps
    you step through the logic in your Python program, and `faulthandler`, which enables
    you to investigate undefined behavior in the C code behind Python. Finally, I’ll
    discuss using Bandit to check for security problems in your code.
  prefs: []
  type: TYPE_NORMAL
- en: Warnings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use a *warning* to notify the user of a problem that the program worked
    around or alert a developer to an upcoming breaking change in a later version
    of your library. Unlike an exception, which we discussed in Chapter 8, a warning
    won’t crash the program. This is preferable for problems that don’t interfere
    with the program’s normal function.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, warnings are more convenient than print statements because they’re
    output to the standard error stream by default. In a print statement like `print("My
    warning message", file=sys.stderr)`, I must explicitly output to the standard
    error stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python offers a warning module with a bevy of additional features and behaviors.
    To issue warnings, use the `warnings.warn()` function. For example, this rather
    theatrical (if silly) program aims to write some text to a file. If the value
    of `thumbs` is `"pricking"`, I issue a warning that something evil is coming.
    After the warning, I open a file, *locks.txt*, and write some text to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-1: *basic_warning.py:1a*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running that module outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The warning appears on the terminal via the standard error stream, but importantly,
    it does not crash the program. The file *locks.txt* is still created with the
    desired text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-2: *locks.txt*'
  prefs: []
  type: TYPE_NORMAL
- en: Types of Warnings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Warnings come in different types, each of which can be treated differently,
    as you see fit. For example, you should probably let your user know if the program
    had to work around a missing file on their system, but you may not want to bother
    them with warnings about weird syntax in the code; such warnings are things only
    a developer would need to know. Warning categories allow you to handle these different
    situations in a manner appropriate to your project.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 19-1](#table19-1) lists the various types of warnings, all of which
    inherit from the `Warning` base class. Just as you can create a custom `Exception`,
    you can create your own types of warnings by inheriting from the `Warning` class
    of any of its subclasses in [Table 19-1](#table19-1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 19-1: Warning Categories'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Usage** | **Ignored by default** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `UserWarning` | The default if no category is specified in `warn()` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `DeprecationWarning` | Warnings about deprecated features, intended for developers
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| `PendingDeprecationWarning` | Warnings about features that will be deprecated
    in the future | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| `FutureWarning` | Warnings about deprecated features, intended for users
    (Prior to 3.7, this instead referred to a feature’s behavior being changed.) |  |'
  prefs: []
  type: TYPE_TB
- en: '| `SyntaxWarning` | Warnings about potentially problematic syntax |  |'
  prefs: []
  type: TYPE_TB
- en: '| `RuntimeWarning` | Warnings about questionable runtime behavior |  |'
  prefs: []
  type: TYPE_TB
- en: '| `ImportWarning` | Warnings related to importing modules | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| `UnicodeWarning` | Warnings related to Unicode |  |'
  prefs: []
  type: TYPE_TB
- en: '| `BytesWarning` | Warnings related to bytes-like objects |  |'
  prefs: []
  type: TYPE_TB
- en: '| `ResourceWarning` | Warnings related to hardware resource usage | ✓ |'
  prefs: []
  type: TYPE_TB
- en: This table is up-to-date for Python 3.7 through at least Python 3.10\. Earlier
    versions of Python ignore different warnings by default, meaning you’ll have to
    explicitly enable those warnings to see them while running the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'To issue a particular type of warning, pass the desired `Warning` class as
    the second argument of `warn()`. For irony’s sake, I’ll revise my earlier example
    to issue a `FutureWarning`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-3: *basic_warning.py:1b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running that code produces the following output on the terminal, in addition
    to creating the same *locks.txt* file as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Filtering Warnings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *warnings filter* controls how warnings are displayed, and it can be passed
    as an argument to the Python interpreter when you run a module or package. For
    example, you can display a warning once or multiple times, hide it altogether,
    or even cause it to crash a program. (I’ll explain why you’d want to in the Converting
    Warnings to Exceptions section below.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning filters are composed of five optional fields, separated by colons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 19-2](#table19-2) explains what each of those fields does.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 19-2: Warning Filter Field'
  prefs: []
  type: TYPE_NORMAL
- en: '| `action` | How the warning should be displayed. There are six options for
    this field: `default`, `error`, `always`, `module`, `once`, and `ignore`. (See
    [Table 19-3](#table19-3).) |'
  prefs: []
  type: TYPE_TB
- en: '| `message` | A regular expression that the warning message must match to be
    filtered. |'
  prefs: []
  type: TYPE_TB
- en: '| `category` | The warning category to be filtered. |'
  prefs: []
  type: TYPE_TB
- en: '| `module` | The module the warning must occur in to be filtered. (Not to be
    confused with the `module` option for the `action` field.) |'
  prefs: []
  type: TYPE_TB
- en: '| `lineno` | The line number where the warning must occur to be filtered. |'
  prefs: []
  type: TYPE_TB
- en: You can omit any field, but you’d still need to include the appropriate number
    of delimiting colons between fields. If you were to specify ``action and `category`
    but omit `message`, you’d still need the separating colons between them:``
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice, however, that I did not need any trailing colons for the omitted `module`
    and `lineno` fields, as those came after the last field I specified: `category`.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `message` field, you can filter warnings that have a particular message.
    The `module` field can be used to filter warnings only in a particular module.
  prefs: []
  type: TYPE_NORMAL
- en: Hiding Duplicate Warnings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It isn’t uncommon for a warning to be raised multiple times, such as if it appears
    in a function that is called more than once. The `action` field controls this.
    [Table 19-3](#table19-3) shows the possible options you could pass to `action`
    `.`
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 19-3: Warning Filter Action Options'
  prefs: []
  type: TYPE_NORMAL
- en: '| `ignore` | Never show the warning. |'
  prefs: []
  type: TYPE_TB
- en: '| `once` | Only show the warning once, for the whole program. |'
  prefs: []
  type: TYPE_TB
- en: '| `module` | Only show the warning once per module. |'
  prefs: []
  type: TYPE_TB
- en: '| `default` | Show the warning once per module and line number. |'
  prefs: []
  type: TYPE_TB
- en: '| `always` | Always show the warning, no matter how often it occurs. |'
  prefs: []
  type: TYPE_TB
- en: '| `error` | Convert the warning to an exception. |'
  prefs: []
  type: TYPE_TB
- en: 'For example, if I only wanted to see the first instance of any particular warning
    in a module, I’d pass the string `module` to the `action` field, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: I pass the warnings filter to the warning filter flag, `-W`, followed by the
    warning filter itself (with no space in between the flag and the filter.) This
    flag must come *before* the module to run, as it’s an argument for Python itself.
    If it came at the end, it would be erroneously passed as an argument to the *basic_warning.py*
    module itself.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, you can alternatively print only the first occurrence of each warning
    in the entire program’s run with `-Wonce`.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring Warnings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can also use warning filters to hide an entire category of warnings from
    an end user by using the `ignore` action. Let’s say you don’t want the user to
    see all the deprecation warnings you plan to address in the next version of your
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `ignore` action hides warnings, and `DeprecationWarning` in the `category`
    field causes only deprecation warnings to be hidden while the Python module is
    running.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Warnings into Exceptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the `error` action, you can convert warnings into fatal exceptions to
    crash the program. This is possible because the base class `Warning` inherits
    from the `Exception` class. The following example turns all warnings into errors
    when running a particular module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Since no other fields are provided in the filter, this `error` action will affect
    all warnings. This might be helpful in continuous integration systems where a
    pull request should automatically be rejected if there are warnings.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason why you might raise warnings as exceptions is to ensure your
    program isn’t using any deprecated code. You can turn all `DeprecationWarning`
    warnings into errors with `-Werror::DeprecationWarning` and then resolve them
    one by one, until your program runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, turning errors into exceptions can have negative consequences because
    it also turns any warnings from within dependencies or the standard library into
    errors. To get around this, I need to limit the warning filter, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This warning filter will only convert warnings to errors in the module I’m directly
    executing, and it will handle any warnings from elsewhere according to the default
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'To convert warnings to errors across an entire package, such as my `timecard`
    package, without getting warnings from dependencies and the standard library,
    I’d use the following filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The regular expression `timecard[.*]` matches any module contained in the `timecard`
    package or any `subpackage` thereof.
  prefs: []
  type: TYPE_NORMAL
- en: There is quite a bit more to warning filters that’s outside the scope of this
    chapter. I recommend reading further at [https://docs.python.org/3/library/warnings.html](https://docs.python.org/3/library/warnings.html).
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Chapter 8, you learned how to log exceptions, rather than merely passing
    them to `print()`. This technique has a few advantages. It grants you control
    over whether messages are sent to the standard output or to a file, and it gives
    you the ability to filter messages based on their severity. The severity levels,
    in increasing order of severity, are as follows: `DEBUG`, `INFO`, `WARNING`, `ERROR`,
    and `CRITICAL`.'
  prefs: []
  type: TYPE_NORMAL
- en: The patterns I used in Chapter 9 were sufficient to get you this far, but logging
    in a production-grade project requires a bit more thought. You must consider which
    messages should be visible under which circumstances and how different types of
    messages should be logged. A critical warning might need to be displayed on the
    terminal and stored in a file, while an informative message about a normal operation
    may be hidden, except when the user runs the program in a provided “verbose” mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle all things logging, Python provides the `logging` module, which defines
    four components: `Logger`, `Handler`, `Filter`, and `Formatter`. I’ll break down
    each of these in turn.'
  prefs: []
  type: TYPE_NORMAL
- en: Logger Objects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A `Logger` object handles your logging messages. It accepts messages to be logged
    as `LogRecord` objects and passes them on to one or more `Handler` objects, based
    on the reported severity. I’ll come back to severity and `Handler` shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical project has one `Logger` per module. Never instantiate these `Logger`
    objects yourself. Instead, you must acquire `Logger` objects with `logger.getLogger()`,
    instead of instantiating them. This ensures that more than one logger with the
    same name is never created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `__name__` attribute is the name of the current module, as well as its parent
    packages (if any). If no `Logger` object yet exists by that name, it is created
    behind the scenes. In either case, the `Logger` object is bound to `logger` and
    becomes ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern works in most situations. In the entry module for a package, however,
    you must explicitly declare the name of the logger, using the name of the package.
    Using the `__name__` attribute here would be impractical, since it will always
    report the name of the entry module as `__main__`. This logger should have the
    package name, so it can serve as the primary logger for all the modules belonging
    to the package. All the other loggers will pass their messages up to this one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the use of `Logger`, I’ll create a `letter_counter` package for
    determining the most commonly occurring letters in a given passage of text. The
    package will use logging to handle warnings and informative messages. Here is
    the beginning of my `__main__.py` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-4: *letter_counter/__main__.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: I acquire the `Logger` object by passing the name of the package explicitly
    to the `logging.getLogger()` function ❶ and then binding that object to `logger`.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important that the name of this logger should match the package name, so
    it can serve as the primary logger for the `letter_counter` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'I must also acquire a `Logger` object for each module and subpackage in this
    package that needs to perform logging. Here, the *letter.py* module acquires a
    logger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-5: *letter_counter/**letters**.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: Because this `__name__` expression resolves to `letter_counter.letters`, the
    `letter_counter` logger created in [Listing 19-4](#listing19-4) automatically
    becomes the parent of this logger. As a result, all messages passed to the logger
    in *letters.py* will be passed in turn to the `letter_counter` logger.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, I can add a logger to the other file in my package, *common.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-6: *letter_counter/**common**.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: Handler Objects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Handler` is responsible for sending the `LogRecord` objects to the right
    place, whether that be the standard output, the standard error, a file, over a
    network, or some other location. The logging module contains a number of built-in
    `Handler` objects, all of which are thoroughly documented at [https://docs.python.org/3/library/logging.handlers.html](https://docs.python.org/3/library/logging.handlers.html).
    This chapter won’t cover many of these useful handlers, so the documentation is
    well worth a quick read. In most situations, however, you’ll wind up using one
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamHandler` sends logging output to streams, especially the standard output
    and standard error streams. You may pass the desired output stream to the `logging.StreamHandler`
    class initializer; otherwise, `sys.stderr` is used by default. Although you can
    use this handler for logging to a file, `FileHandler` will give you better results
    for that use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileHandler` sends logging output to files. You must pass the filename or
    `Path` to the target output file to the `logging.FileHandler` class initializer.
    (There are a number of further specialized `Handler` classes for dealing with
    rotating and system log files.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SocketHandler` sends logging output to a network socket over TCP. You would
    pass the host and port as arguments to the `logging.handlers.SocketHandler` class
    initializer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SMTPHandler` sends logging output to an email address via SMTP. The mail host,
    sender email address, recipient email address, subject, and login credentials
    all have to be passed to the `logging.handlers.SMTPHandler` class initializer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NullHandler` sends logging output into the black hole at the heart of a captive
    dark star, never to be seen or heard from again. Nothing has to be passed to the
    `logging.NullHandler` class initializer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I’ll continue my `letter_counter` package example by printing all the `LogRecord`
    objects in the `letter_counter` package to the terminal with `logging.StreamHandler()`,
    which sends logs to the standard-output or standard-error streams. I add this
    handler to my top-level `Logger` object like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-7: *letter_counter/__main__.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: Because I did not specify a stream with the `StreamHandler()` constructor, `stream_handler`
    will pass messages to the `sys.stderr` stream.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that I only need to add the `Handler` to the `letter_counter` logger.
    Since the loggers for `letter_counter.letters` and `letter_counter.common` are
    children, they will pass all their `LogRecord` objects up to their parent.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may attach as many handlers as you like, to any of the loggers. Child loggers
    will still relay their `LogRecord` objects to their parents in addition to employing
    their own handlers, unless you set the `propagate` attribute of a child logger
    to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In my example, I don’t need to add any handlers to the child loggers. I can
    let the loggers pass their `LogRecord` objects back up to the parent logger, which
    has the one `StreamHandler` attached.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Levels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My logging example is still incomplete because it doesn’t include the *severity*
    *level* of each message, which indicates the message’s relative importance.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with levels allows you to configure a logging system to display only
    messages that are at or above a given severity level. For example, you may want
    to see all `DEBUG` messages while developing, but end users should only see `WARNING`
    messages and above.
  prefs: []
  type: TYPE_NORMAL
- en: There are six built-in severity levels, as outlined in [Table 19-4](#table19-4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 19-4: Logging Severity Levels'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Level** | **Numeric value** | **Use** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `CRITICAL` | 50 | Messages related to horrible, terrible, no good, very bad,
    everything-is-broken situations |'
  prefs: []
  type: TYPE_TB
- en: '| `ERROR` | 40 | Messages related to an error that can probably be recovered
    from (meaning at least all is not lost) |'
  prefs: []
  type: TYPE_TB
- en: '| `WARNING` | 30 | Messages related to problems that are not (yet) errors but
    may require attention |'
  prefs: []
  type: TYPE_TB
- en: '| `INFO` | 20 | Informative and useful messages that are not related to actual
    problems |'
  prefs: []
  type: TYPE_TB
- en: '| `DEBUG` | 10 | Messages that are only of interest to developers, particularly
    when hunting for bugs |'
  prefs: []
  type: TYPE_TB
- en: '| `NOTSET` | 0 | Only used to specify that all messages should be displayed
    (never used as a message severity level) |'
  prefs: []
  type: TYPE_TB
- en: 'To assign a level to a message when logging it, use the `Logger` instance method
    corresponding to the level for the message, as I do in this function from *letter_counter/common.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-8: *letter_counter/**common**.py:2*'
  prefs: []
  type: TYPE_NORMAL
- en: This function converts a string to all lowercase and filters it down to contain
    only letters. The important part of this example is the `logger.debug()` method
    call, which passes a `LogRecord` to this module’s `logger` object ([Listing 19-6](#listing19-6))
    with level `DEBUG`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, over in *letter_counter/letters.py*, I have some `INFO` level messages
    I need to output under certain circumstances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-9: *letter_counter/**letters**.py:1*'
  prefs: []
  type: TYPE_NORMAL
- en: These functions count up the number of vowels and consonants in a given string,
    and they return the vowel or consonant that appears most frequently. The important
    parts here are the two calls to `logger.info()` ❶ ❷. Notice that these messages
    are logged only if an empty string is passed to `most_common_consonant()` or `most_common_vowel()`.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the Log Level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any given `Logger` object can be set to pick up `LogRecord` objects with a particular
    level or higher, using the `setLevel()` method. As with adding `Handler` objects,
    you only need to set the level on the top-level logger. While you *can* set it
    on child loggers if you see fit to do so, they will no longer delegate their `LogRecord`
    objects to their parents. By default, a `Logger` has a level of `NOTSET`, causing
    it to delegate its `LogRecord` objects up the hierarchy. As soon as a `Logger`
    with a level other than `NOTSET` is encountered in that hierarchy, the chain of
    delegation stops.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my example, I set the logging level on the top-level logger to `WARNING`
    by default, but I allow users to pass a `-v` argument on the invocation of my
    package to instead set the level to `INFO`. I’m using the built-in `argparse`
    module to handle command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-10: *letter_counter/__main__.py:3*'
  prefs: []
  type: TYPE_NORMAL
- en: 'I won’t go into the usage of `argparse` in much detail here, as it’s largely
    off-topic and the official `argparse` tutorial does a decent job of introducing
    it: [https://docs.python.org/3/howto/argparse.html](https://docs.python.org/3/howto/argparse.html).
    Suffice to say that I define two arguments: a `-v` flag for toggling verbose mode
    ❶ and the path to the file the program should read from ❷. That flag is the important
    part in this situation. If it was passed in the package invocation, I set the
    level of `logger` to `logging.INFO` ❸. Otherwise, I use `logging.WARNING`, thereby
    ignoring all messages logged with `logger.info()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the rest of my `__main__.py` module, which reads the file, calls the
    functions to count letters, and displays the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 19-11: *letter_counter/__main__.py:4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nothing much to see here in relation to logging, except for a couple more logged
    messages: one at `INFO` level ❶ and one at `WARNING` level ❷.'
  prefs: []
  type: TYPE_NORMAL
- en: Running the Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the logging system at work, I’ll invoke my package from the
    command line, passing it a path to a text file containing The Zen of Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Because I omitted the `-v` flag, the logging level is `WARNING`, meaning only
    messages at level `WARNING`, `ERROR`, or `CRITICAL` will be logged. Since *zen.txt*
    is a valid path, I see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, I’ll add that `-v` flag to my invocation, which should change the logger
    level according to the logic in [Listing 19-10](#listing19-10):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This causes the output to change a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'I now see the `INFO` level message from *letter_counter/__main__.py*. However,
    since the file exists and isn’t empty, I don’t see any of the `INFO` messages
    from the other modules. To see those, I’ll pass a path to an empty file instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output now contains additional messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll notice one other message that is absent: the letter count message from
    *letter_counter/common.py* ([Listing 19-8](#listing19-8)). As it was logged at
    level `DEBUG`, it is still being ignored with the logger set to level `INFO`.
    I would have to modify my code to see it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For one last test, I’ll drop the `-v` flag, thereby using the `WARNING` level
    on the logger, and I will pass an invalid file name in the invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, I see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: That output would be the same in this scenario whether I passed the `-v` flag
    to my program or not, as `WARNING` is higher priority than `INFO`.
  prefs: []
  type: TYPE_NORMAL
- en: Filter, Formatter, and Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two more components you can add to a logging system: a *Filter* and
    a *Formatter*.'
  prefs: []
  type: TYPE_NORMAL
- en: A `Filter` object further defines where to pick up `LogRecord` objects, and
    it can be applied to a `Logger` or `Handler` using the `addFilter()` method. You
    can also use any callable object as a filter.
  prefs: []
  type: TYPE_NORMAL
- en: A `Formatter` object is responsible for converting a `LogRecord` object to a
    string. These are usually defined by passing a special format string to the ``logging.Formatter()
    function. You can also add a single `Formatter` to a `Handler` object with the
    `setFormatter()` method.``
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67] THICKNESS = 0.125  # must be a positive number [PRE68] THICKNESS =
    0.125 **if __debug__:**  **if not THICKNESS > 0:**  **raise AssertionError("Vinyl
    must have a positive thickness!")** [PRE69] THICKNESS = 0.125 **assert THICKNESS
    > 0, "Vinyl must have a positive thickness!"** [PRE70] def fit_records(width,
    shelves):     records_per_shelf = width / THICKNESS     records = records_per_shelf
    * shelves     return int(records) [PRE71] def get_number(prompt):     while True:         value
    = input(prompt)         try:             assert value.isnumeric(), "You must enter
    a whole number"             value = int(value)             assert value > 0, "You
    must enter a positive number."         except AssertionError as e:             print(e)             continue         value
    = int(value)         return value [PRE72] def main():     width = get_number("What
    is the bookcase shelf width (in inches)? ")     print("How many shelves are...")     shelves_lp
    = get_number("    12+ inches high? ")     shelves_78 = get_number("    10-11.5
    inches high? ")     shelves_single = get_number("    7-9.5 inches high? ")      records_lp
    = fit_records(width, shelves_lp)     records_single = fit_records(width, shelves_single)     records_78
    = fit_records(width, shelves_78)      print(f"You can fit {records_lp} LPs, "           f"{records_single}
    singles, and "           f"{records_78} 78s.")   if __name__ == "__main__":     main()
    [PRE73] $ **python3 -O vinyl_collector.py**  What is the bookcase shelf width
    (in inches)? -4 How many shelves are...  12+ inches high? 0     10-11.5 inches
    high? 4     7-9.5 inches high? -4 You can fit 0 LPs, 128 singles, and -128 78s.
    [PRE74] def get_number(prompt):     while True:         value = input(prompt)         try:             value
    = int(value)         **except ValueError:**             **print(**"You must enter
    a whole number."**)**             continue          **if value <= 0:**             **print(**"You
    must enter a positive number."**)**             **continue**          return value
    [PRE75] THICKNESS = -0.125  **# HACK: this value is now wrong, for test purposes**
    assert THICKNESS > 0, "Vinyl must have a positive thickness!" [PRE76] python3
    vinyl_collector.py [PRE77] Traceback (most recent call last):   File "./vinyl_collector.py",
    line 2, in <module>     assert THICKNESS > 0, "Vinyl must have a positive thickness!"
    AssertionError: Vinyl must have a positive thickness! [PRE78] python3 -O vinyl_collector.py
    [PRE79] What is the bookcase shelf width (in inches)? 1 How many shelves are at
    least...     12 inches high? 1     10 inches high? 2     7 inches high? 3 You
    can fit -8 LPs, -24 singles, or -16 78s. [PRE80]`If you’re using an IDE, it may
    include features that integrate with `pdb` or else offer its own alternative debugger.
    While you should familiarize yourself with your IDE’s debugging tools, it is also
    helpful to know how to use `pdb` on the command line, especially for those cases
    where you don’t have access to your development environment of choice.    ###
    A Debugging Example    The best way to learn how to use the debugger is to try
    it out. [Listing 19-20](#listing19-20) is a complete module with a fairly pesky
    bug. If you take the time to read it, you’ll probably figure out the problem yourself,
    but production code is rarely this linear. So, even if you think you’ve identified
    the problem, enter this code into a file as is. I’ll work with this code quite
    a lot through the rest of the chapter.    [PRE81]    Listing 19-20: *train_timetable.py*    This
    module simulates grabbing live data about a particular train, perhaps from API
    transit data. It then processes that data to find particular information. (A more
    realistic example might work with data in the widely used *General Transit Feed
    Specification (GTFS)* format; this imaginary API merely serves up a list of dictionaries.)    If
    you run this code, it crashes with an exception that may seem indicative of a
    simple mistake in my code:    [PRE82]    “Oh, sure,” I might say to myself. “I
    must have forgotten my return statement.” But when I check the `arrives_at()`
    function, the logic makes sense. Besides, the station ID `''coon_rapids_fridley''`
    is *right there* in the test data! Something spooky is going on here.    Enter
    debugger, stage left.    ### Starting the Debugger    You’ll usually want to run
    the debugger directly on your code. One way is to invoke `pdb` from the command
    line when you run your program. For example, if I wanted to invoke the debugger
    on my *train_timetable.py* module, I could do this:    [PRE83]    Since Python
    3.7, the `pdb` module can also run a package with `-m` in the same manner as the
    interpreter. If I wanted to run my `timecard` package within the debugger, I’d
    do this:    [PRE84]    Either way, the module or package is started in the `pdb`
    shell. The debugger immediately stops the program, awaiting a command.    [PRE85]    At
    the `(Pdb)` prompt, I can enter debugger shell commands. I’ll come back to the
    usage of this debugger shell in a moment.    Another way of starting the debugger
    is by setting a breakpoint directly in your code. When you run the code normally,
    it will hit this breakpoint and hand off control to `pdb`.    From Python 3.7
    onward, you can set a breakpoint anywhere in your code with the following built-in
    function:    [PRE86]    Prior to 3.7, you could do the same with the following:    [PRE87]    Breakpoints
    will save you considerable time if you have an idea where the bug might originate
    from, or at least where the problematic execution stack begins. If you have a
    breakpoint set in your code, you only need to execute the program normally:    [PRE88]    This
    command starts the program normally, but as soon as it hits the breakpoint, the
    Python interpreter hands off execution control to `pdb`.    ### Debugger Shell
    Commands    The `pdb` tool has quite a few commands you can enter at the `(Pdb)`
    prompt to control and monitor the execution of the code.    In Appendix B, I document
    the most important `pdb` commands. These commands are also exhaustively documented
    at [https://docs.python.org/3/library/pdb.html](https://docs.python.org/3/library/pdb.html),
    although that page can be a bit hard to navigate when you’re looking for something
    in particular.    While I won’t be able to demonstrate many of the `pdb` commands,
    I’ll walk you through debugging this particular example from [Listing 19-20](#listing19-20).    ###
    Stepping Through the Code    For this example, I’ll start debugging at the top
    of my code by invoking `pdb` directly, like this:    [PRE89]    I’ll use the debugger
    as shown below. I strongly encourage you to follow along.    The `pdb` session
    starts at the top of the module, but the problem is farther down. I use the `next`
    (or `n`) command to move down to the beginning of the usage section of my module.    [PRE90]    When
    I reach the first function call ❶, I use the `list` (or `l`) command to see the
    nearby code. I’m currently stopped on line 34, as indicated by the `->` in the
    code.    ### Setting a Breakpoint and Stepping into a Function    I know I’m having
    a problem around line 39, so I’ll set a breakpoint there:    [PRE91]    The command
    `break 39` sets a breakpoint on line 39\. I then use the `continue` command to
    proceed to that breakpoint, since it is later in the execution flow than the present
    position.    Next, I use the `step` (or `s`) command to step into the `arrives_at()`
    function. Then, before checking that code, I use the `args` command to see what
    values were passed to the function:    [PRE92]    The station ID for Coon Rapids
    Fridley looks odd ❷, and it certainly doesn’t match the station ID I’m looking
    for ❶. The data in `timetable` is getting mutated somewhere, and this is probably
    part of the reason why my code is returning `None`. Still, I need to confirm my
    theory.    ### Moving Through the Execution Stack    Since I didn’t change the
    station ID in this part of the code, the problem must exist in an earlier part
    of the code. I *could* start over with a fresh debugging session, so I could stop
    earlier in the execution stack, but when debugging in a large, real-world program,
    that could be a royal pain. Thankfully, `pdb` provides an alternative.    First,
    I need to know where I am in the execution stack. The `where` command shows me
    the current stack trace, which is composed of four frames:    [PRE93]    I’m currently
    at the bottommost frame, as indicated by the `>` character ❶. I can move up one
    frame with the `up` command, which changes my focus to this line:    [PRE94]    Next,
    I’ll inspect the nearby code:    [PRE95]    I use the `l` command (an alias for
    `list`) to see the surrounding code. You can see that line 39 has a breakpoint,
    indicated by the `B`. The `->` also indicates that this is my current position.    I
    know from earlier that `timetable` is getting mutated somewhere unexpected. The
    first suspect is that `next_station()` function call on line 36, so I set a breakpoint
    there with `b 36` (the same as `break 36`).    I must move backward in the execution
    stack, which is one of the cool features of `pdb`. There are two ways to do this:
    either I could use the `restart` command from this point and then continue to
    the new breakpoint, or I could use `jump`. Because the latter is a bit trickier,
    I’ll show how to accomplish it here.    The difficulty with using `jump` comes
    from the fact that I cannot jump from any position other than the newest frame,
    and I’m one level removed from that. There are a few ways around this. I could
    set a breakpoint at a later line on the outer scope and then continue to it. My
    present situation is simple enough, so I can use the `next` command from this
    point to step out of the current function call:    [PRE96]    I’m now in a position
    to jump. However, an important distinction between `restart` and `jump` is that
    while the former starts afresh, the latter executes with the current state staying
    as it is. This means that if I want to get an accurate picture of what’s happening,
    I need to get a fresh value for `timetable` without changing anything else. The
    easiest way to do that in this code is to jump to line 34:    [PRE97]    Immediately
    after making the jump, `pdb` stays paused, awaiting further instructions. I run
    this line with `n` and then confirm that `timetable` is back to what it should
    be by pretty-printing it with `pp timetable`.    ### Inspecting the Source    Now,
    I’ll see how `timetable` is getting messed up. I don’t need to `continue` at this
    point, as I’m already sitting on line 36, where I’d wanted to check next. I know
    that the data in `timetable` is being mutated, so I inspect the code of `next_station()`
    with the `source next_station` command:    [PRE98]    Hmm . . . line 24 is intriguing,
    isn’t it? That’s the logic for changing from so-called *lower_snake_case* to *Title
    Case*. I don’t want to waste time stepping through that `for` loop, so I set a
    breakpoint on the suspect line with `b 24` and then continue execution with `c`.    Now
    I can check the before-and-after state of `station`:    [PRE99]    Aha! The first
    time I check station with `p station`, it has the correct station ID. After running
    that suspect line with `n`, I check the value again and find it has changed. That’s
    not so bad in itself, but if I look at the value `timetable` with `pp timetable`,
    I find the change was made there.    Although a tuple itself is immutable, this
    isn’t necessary true of its items, and a dictionary is most certainly mutable.
    By binding an item from the tuple to `station` and then mutating it, that change
    is visible in the tuple. The `next_station()` function has side effects. The `arrives_at()`
    function couldn’t find the station with the expected ID because the ID had been
    changed.    ### Checking a Solution    Once I have found the problem, I can quit
    the debugger and fix it. However, if I’m wrong, I have to start all over, and
    that could be a pain! Since I’m already located at the point of error, I’ll try
    changing the dictionary back to what it should be.    I can execute a Python statement
    at the current position, effectively before the line I’m stopped on, using the
    `!` command:    [PRE100]    After running the statement that fixes the value of
    `station`, I confirm that the fix worked with `pp timetable`. Sure enough, the
    `''coon_rapids_fridley''` entry changed to what it should be. Then, I move forward
    to the next line of code with the `n` command.    Just because I fixed a problem
    in the code doesn’t mean I fixed the only problem. I need to let the program finish
    running to be absolutely certain the problem is resolved. I list all the breakpoints
    with `b` (or `break`) without arguments. For this example, I’ll clear breakpoints
    1 and 2, as those are the ones that will get in the way when I continue:    [PRE101]    Now,
    I have removed the breakpoints I don’t need anymore. (I could also have cleared
    all breakpoints using `clear`.)    I continue from here using the `continue` command:    [PRE102]    While
    the output for “Next station is . . . ” isn’t what I want—I’ll have to work out
    a solution for that—the rest of the code functions without crashing. I’ve solved
    it! Finally, I can exit the `pdb` shell with `q` and then change my code based
    on what I discovered.    ### Postmortem Debugging    So far, you’ve seen that
    you can run a debugger from the top of a program or from a preset breakpoint.
    A third way to run a debugger is *postmortem*, meaning after a fatal crash has
    taken place. This is best thought of as a snapshot of the moment of the crash.
    You cannot move about in the code, set breakpoints, step into function calls,
    or jump around. However, you *can* inspect anything you want from the point of
    the crash.    There are a couple of ways to start postmortem debugging. The easiest
    way is to start the module in the interactive Python shell and allow it to crash,
    then invoke the postmortem debugger with `import pdb; pdb.pm()`:    [PRE103]    There’s
    not much I can do here apart from inspection, which means this technique isn’t
    terribly helpful in this particular scenario. That said, I could still check the
    value bound to the names `timetable` and `station`, which might grant me some
    initial insight:    [PRE104]    This is quite similar to print statement debugging,
    at least from the point of failure. From this, I could ascertain that the `timetable`
    itself might have been mutated unexpectedly, which is a useful insight. From here,
    I could start the module over in regular debugging mode and follow up that idea,
    as you saw me do earlier.    ## Using faulthandler    If you’ve ever worked with
    C or C++, you may be familiar with the concept of *undefined behavior*, or a situation
    in code with no formal definition for how it should be handled. Code with undefined
    behavior may do anything: it might appear to work, fail with an error, or even
    do something weird.    You may hear many Python developers say, “Python doesn’t
    have undefined behavior!” This is only partly true. Nothing in Python’s own language
    specification is marked as “undefined behavior.” Everything should either work
    in a defined fashion or fail with a specific error.    True as that may be, CPython
    (the default interpreter) and many common Python extensions and libraries are
    still built with C, and undefined behavior is a possibility in C. If you search
    for the term *undefined* in the Python documentation, you’ll find a few advanced
    situations where undefined behavior is possible.    When you believe you’re up
    against undefined behavior, particularly *segmentation faults*—fatal system errors
    resulting from a program trying to access computer memory it doesn’t have permission
    to access—the `faulthandler` module is one of the most helpful tools in your toolbox.    To
    demonstrate this tool’s use, consider the following brief segment of Python code
    with undefined behavior:    [PRE105]    Listing 19-21: *segfault.py:1a*    The
    undefined behavior here comes from the underlying C code: I attempt to set the
    memory at address `0`, the *null pointer*, to the value `254`. The behavior of
    accessing or modifying memory at the null pointer is undefined. While anything
    could happen, this particular action almost always results in a segmentation fault:    [PRE106]    It’s
    easy to spot this problem in a simple, two-line program, but imagine if this error
    occurred in a vast project with hundreds of lines.    This is where `faulthandler`
    comes in handy. It allows you to quickly locate the line in your code that contains
    the undefined behavior. There are two ways to run this tool for a project. The
    first way is to invoke it with `-X faulthandler` when you run the interpreter:    [PRE107]    Alternatively,
    you can enable `faulthandler` directly in your code:    [PRE108]    Listing 19-22:
    *segfault.py:1b*    Because the line enabling `faulthandler` is intended to be
    removed once the problem has been found, it’s acceptable to cram the import statement
    onto the same line as the call to `enable()`.    Regardless of how you enable
    `faulthandler`, the output is essentially the same. As soon as a segmentation
    fault or similar fatal error is encountered, you’ll see a complete stack trace:    [PRE109]    Based
    on this traceback, you can see that the problem is on line 5 of *segfault.py*
    (from [Listing 19-22](#listing19-22)), which contains the invalid call to `ctypes.memset`.    ##
    Evaluating Your Program’s Security with Bandit    As I’ve alluded to throughout
    this book, your choices about what modules and libraries you use and how you use
    them may introduce risks of a number of security concerns into your code. While
    you should try to stay informed about vulnerabilities, it’s not practical to memorize
    every single possible issue. Thankfully, you can employ a tool to help monitor
    your code’s security.    *Bandit* is a security-focused static analyzer. It checks
    your code for security issues by building and testing it as an *abstract syntax
    tree (AST)*, which is a tree data structure that represents the overall structure
    of the code. You can install the `bandit` package from pip and use it in the manner
    of most other static analyzers.    Consider the following very small program,
    which contains a significant security issue:    [PRE110]    Listing 19-23: *magic_calculator.py:1a*    Rather
    than point out the security problem here myself, I’ll run this program through
    Bandit to see what issues it finds:    [PRE111]    Here’s the output:    [PRE112]    The
    first warning ❶ complains about the `input()` built-in function being insecure
    in Python 2\. In looking at that warning, a developer might be tempted to say,
    “Oh, Bandit must be wrong. It’s complaining about Python 2, and I’m using Python
    3!” In fact, it’s not wrong here. I am assuming that the code will be run in Python
    3, but it might accidentally get executed by Python 2, where `input()` actually
    *is* insecure.    To fix this, I need to add a shebang to the top of my module,
    to ensure that the code will be executed by Python 3:    [PRE113]    Listing 19-24:
    *magic_calculator.py:1b*    Bandit’s second issue ❷ comes with a suggestion: switch
    to `ast.literal_eval()` instead of `eval()`, as the latter is vulnerable to code
    injection attacks. I’ll revise accordingly:    [PRE114]    Listing 19-25: *magic_calculator.py:1c*    Rerunning
    Bandit on this revised code shows no more issues. You can find the tool’s full
    documentation at [https://bandit.readthedocs.io/en/latest/](https://bandit.readthedocs.io/en/latest/).    The
    topic of security may feel irrelevant to your project, but you must remember that
    it doesn’t necessarily have anything to do with the data your program works with!
    Many security issues are related to an attacker using your code as a vector or
    tool in an oft-unrelated attack. Security flaws are like a screen door on a bank
    vault. It doesn’t matter how it got there, who is meant to use it, or how helpful
    it is to the authorized users. Sooner or later, someone is going to abuse it for
    illicit or unauthorized purposes.    ## Reporting Bugs to Python    Sometimes,
    the problem in your code isn’t your fault! Python, like all code, has bugs that
    crop up now and then. Once you’re quite certain the bug isn’t coming from your
    own code or a third-party module or package, you are strongly encouraged to report
    the bug to the Python developers.    Your first step should be to check whether
    the bug has already been reported. In any issue tracker, this can be tricky, so
    be patient with this step. All issues for Python are tracked at [https://bugs.python.org/](https://bugs.python.org/),
    so it’s worthwhile to have an account there. Try searching the site for different
    words, focusing on the parts of the language you’re using and any keywords in
    error messages you’ve received. Make sure to omit any words that are unique to
    your code, as they probably won’t appear in other bug reports. Also, don’t count
    out previously closed bugs as candidates. Regressions happen. If you find an existing
    bug that matches your situation, leave a comment with the information you have.    If
    you can’t find a matching issue, open a new bug report. Be prepared to put some
    time and effort into it. Since you’re the one facing the bug, you’re in a better
    position than anyone to pin it down. Provide as much information as possible to
    help the Python developers reproduce it. Be prepared to respond to further questions,
    as you’ll likely need to try some things out to prove your code isn’t the real
    issue.    The Python documentation has a helpful guide explaining how to report
    bugs effectively. Before reporting an issue, please read that guide at [https://docs.python.org/3/bugs.html](https://docs.python.org/3/bugs.html).    Security
    issues should be handled separately from normal bugs, as they need to be treated
    with confidentiality to minimize the risks to existing code. If you come across
    a security flaw in Python, please report it via email to [security@python.org](http://mailto:security@python.org),
    following the instructions at [https://www.python.org/dev/security/](https://www.python.org/dev/security/).    ##
    Wrapping Up    When it comes to combating bugs, the best defense is a good offense.
    Writing your code to make good use of exceptions, warnings, assertions, and logging
    will save you debugging work later.    When bugs do happen, don’t limit yourself
    to cramming `print()` statements into every part of your code. Logging and `assert`
    statements are helpful for manual debugging and catching problems while developing.
    Meanwhile, the Python debugger (`pdb`) is one of the most useful tools in your
    toolbox, and it is well worth learning to use, no matter how fancy the debugger
    in your IDE is.    Python’s default implementation, CPython, and many extensions
    besides, are built in C. This means that undefined behavior and other C-related
    bugs can creep into your Python code. When this happens, `faulthandler` is your
    best friend.    Finally, be prepared to check for and address security flaws in
    your code before anyone can take advantage of them. Bandit helps you get started
    with this, though true security testing will go beyond the scope of that tool.    Bugs
    may be inevitable, but the Python ecosystem provides excellent tools for catching,
    examining, and fixing them![PRE115]``  [PRE116]`# 20 Testing and Profiling  ![](Images/chapterart.png)  There
    are two important rules about code: untested code is broken code, and all claims
    of performance are mythical until proven otherwise. Thankfully, the Python ecosystem
    offers a wide variety of tools to test and profile your code.    *Testing* is
    one component of *quality assurance (QA)*, which in software development aims
    to improve the overall stability and maintainability of code. While many companies
    have dedicated QA teams, testing should be the shared responsibility of *every
    single developer* on a project. Similarly, *profiling* is a critical part of confirming
    that a project meets its performance goals. Even if some design pattern or algorithm
    looks faster on paper, profiling ensures that your implementation is meaningfully
    better performing than some alternative. In this chapter, I’ll cover the essentials
    of testing and profiling in Python, primarily with the pytest testing framework
    and with an eye toward production code.    Most experienced developers find it
    most effective to test their programs as part of the coding process, rather than
    testing the entire finished program after the fact. This approach allows developers
    the flexibility of catching and correcting issues early, when identifying and
    performing fixes is easier. I follow this process in this chapter. I’ll walk through
    the development of a complete (if small) multifile project, writing and expanding
    tests for each section before continuing with development. You’ll learn how to
    run basic unit tests, conditionally run tests, and correct flaky tests. You’ll
    additionally learn how to use fixtures, mocks, and parametrization. I’ll also
    touch on measuring test coverage, automating testing, benchmarking code, and profiling.    ##
    What About TDD?    The practice of testing should not be confused with the specific
    methodology of *test-driven development (TDD)*, wherein you write tests before
    writing the code and then write the code to make the tests pass. TDD is not mandatory,
    as you can just as effectively write the tests just after writing your code.    If
    you’re already a practitioner of TDD, I encourage you to continue applying it
    throughout this chapter, by writing your tests first. If you’re like me and prefer
    writing the code before the tests, you can stick with that. The important thing
    is to write tests, and the sooner, the better.    ## Test Frameworks    There
    are several frameworks for running tests in Python, many with unique features
    and use cases. The most popular testing framework for Python is *pytest*, a streamlined
    alternative to `unittest`-based frameworks with minimal boilerplate code. If you
    don’t know which framework to use, this is the one to pick up. You can find out
    more from the official documentation at [https://docs.pytest.org/](https://docs.pytest.org/),
    and you can install the `pytest` package from PyPI via pip.    This chapter also
    uses *tox*, which ensures that your packaging works across different Python environments
    and automatically runs your test suites in each of those environments. You can
    find the official tox documentation at [https://tox.readthedocs.io/](https://tox.readthedocs.io/),
    and you can install the `tox` package from PyPI via pip.    Before I dive into
    testing, I want to touch on a few of the other testing frameworks in regular use
    in Python projects.    Python’s standard library includes `unittest`, which has
    a long history of use. Python 2 had both `unittest` and `unittest2`, and the latter
    became just `unittest` in Python 3 ([https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)).
    The standard library also includes `doctest`, which allows you to write simple
    tests in docstrings ([https://docs.python.org/3/library/doctest.html](https://docs.python.org/3/library/doctest.html)).
    Both of these can be useful when you need to write tests without installing any
    packages.    The `unittest` module was further extended and improved by the now
    discontinued `nose` framework, which added support for plug-ins. This was in turn
    replaced by `nose2`. However, `nose2` is largely considered outdated, so it’s
    usually best to rewrite `nose2` tests to pytest or another modern framework when
    possible. The pytest documentation has a guide to this at [https://docs.pytest.org/en/stable/nose.html](https://docs.pytest.org/en/stable/nose.html).    There
    are newer testing libraries, many of which apply innovative ideas and offer simpler
    interfaces. One such example is *Hypothesis*, which automatically finds edge cases
    you may have overlooked simply by writing assertions describing what the code
    *should* do. More information is available in the documentation at [https://hypothesis.readthedocs.io/](https://hypothesis.readthedocs.io/).    My
    personal favorite testing library is *Ward*, which features improved test organization
    and clearer output. It also works with Hypothesis, and it is fully compatible
    with asynchrony. You can learn more at [https://wardpy.com/](https://wardpy.com/).    Finally,
    *RobotFramework* is a test automation framework that integrates with many other
    tools. It is better suited to large and complex systems that are harder to test,
    rather than small and compact stand-alone projects. You can learn more about RobotFramework
    at [https://robotframework.org/](https://robotframework.org/).    ## The Example
    Project    To demonstrate real-world Python testing, I’ll build a complete (but
    small) command-line program that performs a proofread check on a plaintext file.
    The program will accept a file path as input, and then it will check the contents
    of that file for spelling and grammar errors, using a free API. It will then prompt
    the user to correct the errors by allowing them to choose between suggested revisions.
    The corrected text will then be written out to another file.    The complete source
    code for this project can be found on my GitHub, at [https://github.com/codemouse92/textproof](https://github.com/codemouse92/textproof).
    However, I’ll be demonstrating good testing habits in this chapter by testing
    the program as I write it. I encourage you to follow along. The `example_starter`
    branch on that repository contains the initial folder structure and packaging
    scripts for this example.    To implement the actual spelling and grammar checking,
    I’ll use the free web API for *LanguageTool*, an open-source proofreading tool
    and service ([https://languagetool.org/](https://languagetool.org/)).    With
    the exception of the third-party modules `requests` and `click` and the LanguageTool
    API, this example only uses features and techniques you’ve already learned elsewhere
    in this book.    To use the LanguageTool API, you make a POST request with `requests`,
    to which you pass a plaintext string and some other necessary information packed
    in dictionaries that will be converted behind the scenes to JSON. The LanguageTool
    service will reply with a very large JSON object containing, among other things,
    all detected grammar and spelling errors and their suggested corrections. The
    `requests` module will return this as a Python dictionary. From there, my code
    will need to pick out whatever information it needs.    You can find more information
    about the API, as well as a web interface for trying it out, at [https://languagetool.org/http-api/swagger-ui/#!/default/post_check](https://languagetool.org/http-api/swagger-ui/#!/default/post_check).    The
    `click` module provides a more intuitive way to design a command-line interface
    than `argparse`, which I used in Chapter 19. I’ll use only the decorators `@click.command()`,
    `@click.argument()`, and `@click.option()`.    You can install the `requests`
    and `click` modules in your virtual environment via pip. The official documentation
    for `requests` can be found at [https://requests.readthedocs.io/](https://requests.readthedocs.io/),
    although I’ll only use the `requests.post()` method and the `requests.Response`
    object it returns. The `click` module is documented at [https://click.palletsprojects.com/](https://click.palletsprojects.com/).    If
    the LanguageTool API is offline, or if you’re otherwise unable to access it, rest
    assured that nearly all my tests run *without* access to the API. Thus, even without
    an internet connection, you should be able to work through most of the examples
    and ultimately prove that the code works correctly. This is the beauty of testing.    ##
    Testing and Project Structure    Before you start testing, it’s critical to get
    the project structure right. Back in Chapter 18, I introduced the recommended
    layout for a Python project, including the all-important *setup.cfg* file. I’ll
    expand on a similar structure for this example:    [PRE117]    Listing 20-1: Project
    directory tree for *textproof/*    The source code for the `textproof` package
    belongs in *src/textproof/*. As I mentioned back in Chapter 18, use of a *src/*
    directory is optional but strongly recommended. Not only does it make packaging
    easier, but it also simplifies configuration of testing tools. What’s more, it
    forces you to install your package directly before testing, exposing packaging
    flaws and any wrong assumptions about the current working directory in your code.    In
    this structure, the tests themselves will go in the *tests/* directory. This is
    known as *out-of-place testing*.    I’ll briefly review the setup-related files
    here, focusing primarily on their effect on the testing configuration. See Chapter
    18 for a full explanation of each.    *LICENSE* and *README.md* are fairly self-explanatory,
    so I won’t reproduce those here. Similarly, *setup.py* is the same as in Chapter
    18, so it’s omitted here.    The *setup.cfg* file is largely the same as the one
    for the Timecard project in Chapter 18, except for the metadata and the dependencies.
    I’ve omitted the metadata to save space:    [PRE118]    Listing 20-2: *setup.**cfg**:1a*    I’m
    using two libraries in this project: *requests*, for working with the API, and
    *click*, for creating the command-line interface. I’m also using pytest for testing;
    I’ll add some tools here later.    I don’t have any non-code data to include in
    the package this time, so my *MANIFEST.in* is pretty sparse:    [PRE119]    Listing
    20-3: *MANIFEST.in*    The most interesting setup-related file for this example
    is going to be *pyproject.toml*, which will ultimately store settings for some
    testing tools I’m using. For the moment, it looks like the one in Chapter 18:    [PRE120]    Listing
    20-4: *pyproject.toml:1a*    Under the project structure in [Listing 20-1](#listing20-1),
    my source code belongs in *src/textproof/*, and my tests belong in *tests/*.    ##
    Testing Basics    In this first part, I’ll write some initial code and a few basic
    tests, which I’ll be able to run even before the full program can be executed.    ###
    Starting the Example    The first thing my code needs to do is become able to
    load a text file and save it back out again. I’ll make that happen in my project
    with a `FileIO` class, which I’ll use for storing the file contents while I’m
    working with them:    [PRE121]    Listing 20-5: *src/textproof/**fileio.py:1a*    The
    `FileIO` class’s initializer accepts a path to a file to read and optionally a
    path for writing back out; if no `out_file` path is specified, it will write to
    the same file it reads. The `load()` instance method reads the specified file
    into a `data` instance attribute, and the `save()` instance method writes data
    out to a file.    ### Unit Testing    I test individual behaviors of my code so
    far with *unit tests*, so named because each one tests a single *unit*, such as
    a function, or in this case, a particular conditional path through a function.    Before
    I write any more code, I want to test the behaviors of this class so far. In my
    *tests/* directory, I create *test_fileio.py*. By default in pytest, all test
    modules must start with `test_` to be detected by the framework. If I named the
    file *tests/fileio.py*, none of these tests would run.    Each test is written
    as a function containing one or more `assert` statements:    [PRE122]    Listing
    20-6: *tests/test_fileio.py:1a*    Because all these tests relate to the same
    part of the code base, it is useful for organizational purposes to group them
    together in a class.    The first two tests check that the path string passed
    to the `FileIO` initializer is turned into a `pathlib.Path` object bound to the
    `in_file` and `out_file` attributes, respectively. The third test checks that,
    if only one path string is provided, that path will be used for both `in_file`
    and `out_file`.    Although these may seem like needlessly obvious things to check,
    these unit tests become invaluable as the code becomes more complex. If any change
    to the code causes the code to no longer behave in the manner these tests expect,
    I will be alerted by the failing tests, rather than by some sort of unexpected
    behavior that must be debugged.    Good testing practice demands that each unit
    test check only one behavior, which is why I wrote three individual tests, instead
    of one that checks all three things. This helps me zero in on a particular behavior
    that isn’t working, instead of having to pick through multiple assertions to find
    what’s broken.    I also didn’t create constants to hold the string literals I
    keep repeating. While this is contrary to the coding practice of DRY, it is often
    considered good practice in Python testing, so your tests never run the risk of
    false positives if a function under testing rebinds, mutates, or otherwise interacts
    with a variable in an odd way. Avoid using the same variable for both the input
    and the output.    Lastly, notice that pytest requires test functions to start
    with `test_`, and requires test classes to start with `Test`, in the same way
    the module must start with `test_`. If I named that first test only *in_path()*,
    it would not be run as a test. You can change this behavior in the settings for
    pytest: [https://docs.pytest.org/en/latest/example/pythoncollection.html](https://docs.pytest.org/en/latest/example/pythoncollection.html).
    Some other testing frameworks, like Ward, do not have this default convention.    ###
    Executing the Tests with pytest    To run these tests, I must first install my
    package in a virtual environment. In the example below, I already created a virtual
    environment, *venv/*. I will now install the package, along with its optional
    testing dependencies, by running the following in the command line from the root
    of the project:    [PRE123]    This installs the local package according to *setup.cfg*,
    including any packages needed for testing—namely `pytest`—which were specified
    in the `[options.extras_require]` section (see [Listing 20-2](#listing20-2)).
    You’ll notice that I wrap the `.[test]` in single quotes, to keep the command
    line from misinterpreting those square brackets as a glob pattern.    I’m also
    installing my package in *editable* mode via the `-e` argument, meaning the installation
    is directly using the files in *src/textproof/*, rather than copying them into
    the virtual environment. This is extremely useful if I need to run the code through
    a debugger!    To run my project’s tests with pytest, I issue the following command:    [PRE124]    This
    automatically scans the entire current directory for any modules starting with
    *test_*, any classes starting with *Test*, and any functions starting with *test_*.
    When pytest finds these test functions, it runs them, outputting the results onto
    the terminal in colorful, insightful detail, like this (sans color here in the
    book, unfortunately):    [PRE125]    All’s green and passing! The pytest tool
    found three tests in the module *tests/test_fileio.py*, and all three passed,
    as represented by the three dots after the module name.    ### Testing for Exceptions    One
    significant danger when testing is receiving false positives, wherein a test passes
    only due to a bug or logic error in the code. For example, have you noticed something
    odd about those passing tests? They all refer to a file called *tests/to_be.txt*,
    but that file does not exist in the project. If `FileIO` is passed a path to a
    file that doesn’t exist, it should raise a `FileNotFoundError` instead of proceeding
    quietly.    I’ll put that expectation into the form of a test:    [PRE126]    Listing
    20-7: *tests/test_fileio.py:2a*    To test that an exception is raised, I use
    the context manager `pytest.raises()` instead of an ordinary `assert` statement.
    In the suite of the `with` statement, I run the code that should raise the expected
    exception.    Re-running pytest shows the test is failing:    [PRE127]    The
    `F` after the module name indicates a failed test ❶. More details follow the `FAILURES`
    header, indicating that the expected exception was not raised ❷.    Remember that,
    in this case, failure is a good thing! It means the test has detected a mismatch
    between the expectations of the test and the behavior of the code.    Now I set
    about making the test pass, which, in this case, is as simple as adding some logic
    to the initializer of the `FileIO` object:    [PRE128]    Listing 20-8: *src/textproof/**fileio.py:1b*    Because
    I installed my local package as editable, I do not have to reinstall before running
    pytest again—the virtual environment directly uses the source code, so my changes
    are visible in that context immediately.    Running the tests now shows the first
    three tests failing and the fourth passing. Readers familiar with the practice
    of testing will recognize that this is a step forward, not backward: it reveals
    that the first three tests were originally passing erroneously!    ## Test Fixtures    Those
    tests failed not because of a flaw in the code, but due to their own logic. All
    three wrongly assumed the presence of a particular file, *tests/to_be.txt*. I
    could create that file myself, but it would be better to use the test framework
    to ensure that file is always there in advance. I can do so by creating a *software
    test fixture*, usually known as a *test fixture* or a *fixture*, which is a function
    or method that sets up anything a test might need, especially things that are
    shared by multiple tests. Fixtures can also perform *teardown*—tasks like closing
    a stream or database connection or deleting temporary files. By using a fixture,
    you cut down on errors in writing your tests and save time besides.    I’ll add
    a fixture to my `TestFileIO` test class, to create that demo file my tests are
    expecting.    [PRE129]    Listing 20-9: *tests**/test_**fileio.py:1c*    I define
    the contents of the demo file in the class attribute `demo_data`. This is acceptable
    for populating a fixture with example data, so long as I don’t also use the attribute
    in the test itself as part of an assertion.    The `demo_in_file()` and `demo_out_file()`
    functions are turned into fixtures via the `@pytest.fixture` decorator. Both fixtures
    have two important parameters. The `tmp_path` parameter is actually a fixture
    that is automatically provided by pytest via *dependency injection*, wherein an
    object receives the other objects it needs when it is created or called. In this
    case, merely *naming* the parameter `tmp_path` “magically” causes pytest to provide
    the `tmp_path` fixture object to this fixture. The `tmp_path` fixture will create
    a temporary directory on the filesystem, and it will automatically delete that
    directory and its contents when the fixture is torn down.    The `demo_in_file()`
    fixture itself writes `demo_data` to the file, and then it returns the path to
    that file. Whatever is returned by the fixture is provided directly to any test
    using said fixture. You can use `yield` in place of `return` in a fixture if you
    need to add teardown logic after that statement, such as closing a database connection.    The
    `demo_out_file()` fixture returns a path to an *out.txt* file (which doesn’t yet
    exist) in the temporary directory provided by `tmp_path`.    I use the fixtures
    in my tests like this:    [PRE130]    Listing 20-10: *tests**/test_**fileio.py:1c*
    (continued)    Like before, fixtures are added to tests via dependency injection.
    I need only add a parameter with the fixture’s name (`demo_in_file`), and pytest
    will inject the fixture. In the context of the test, `demo_in_file` will then
    refer to whatever value was returned or yielded by the fixture; in this case,
    that’s a string representation of the path to the demo file the fixture created.    I
    run pytest again and find that all four tests are passing.    Here are a few more
    unit tests, checking the read-write logic of my `FileIO` class:    [PRE131]    Listing
    20-11: *tests**/test_**fileio.py:3*    There’s not much to explain here. I test
    loading a file, saving a file, and ensuring that saving before loading raises
    a `RuntimeError`. These also pass on the first try.    The one thing worth noting
    is the name `test_save__no_load`. Some developers like using the naming convention
    `test_``subject__scenario`, using the double-underscore to separate the subject
    of the test from the description of the scenario under which the subject is being
    tested.    ### Continuing the Example: Using the API    Now that I have the basic
    file-reading and file-writing functionality built, I can add the next piece: communicating
    with the LanguageTool API. Here’s how I do that in my program:    [PRE132]    Listing
    20-12: *src/textproof/api.py*    I use the `requests` module to send a POST request
    to the public API endpoint at [https://languagetool.org/api/v2/check](https://languagetool.org/api/v2/check).
    The API will respond with JSON data, which `requests` will automatically convert
    to a Python dictionary and return from `requests.post()`; I bind this dictionary
    to `response`.    I check the status code of the POST request; if it’s not `200`,
    that indicates a problem communicating with or using the API, and I’d want to
    raise a `RuntimeError` with the details.    Otherwise, for a successful response,
    I print out the name and version of the API on the console for reference, as well
    as the language I’m checking against. Finally, I return the list of errors detected
    by LanguageTool. (I know about the keys and structure of the dictionary from trying
    it out at [https://languagetool.org/http-api/swagger-ui/#!/default/post_check](https://languagetool.org/http-api/swagger-ui/#!/default/post_check).)    This
    either works or doesn’t work, so I won’t test this function directly—although
    some may see testing it as justifiable. I *will* test the assumptions it makes
    about the API response later.    ### Sharing Data Between Test Modules    I need
    most of my tests to work without internet, for two reasons. First, I want them
    to work even if I’m disconnected or the API I’m using is temporarily unavailable.
    Second, I don’t want to send unnecessary API requests just to test my code. Instead,
    I want to use predetermined local data for most of my tests. This means I’ll need
    all my tests to have access to this data.    I also need to ensure my assumptions
    about the API, which are what my code and tests are based on, are correct. This
    looks like a job for testing!    In my *tests/* directory, I create a special
    *conftest.py* module. This module, *with this exact name*, is used by pytest to
    perform initial setup and share fixtures and the like between test modules. Here,
    I define the data I want my tests to use:    [PRE133]    Listing 20-13: *t**ests/conftest.py:1a*    The
    value bound to `example_api_response` is adapted directly from the `[''matches'']`
    value of the LanguageTool API server response for `example_text`, but I’ve removed
    all the fields I don’t use in my code. I’ll use this data for many other tests
    later. The string literal bound to `example_output` is the grammatically correct
    form of `example_text`, after applying the corrections suggested by LanguageTool.    To
    make these names available to all test modules in the *tests/* directory, I override
    the `pytest_configure()` function and add them as attributes of the `pytest` namespace.
    I can access them in any test module in the *tests/* directory as attributes on
    `pytest`.    ## Flaky Tests and Conditionally Skipping Tests    Sometimes, there
    are conditions under which you might want to skip a test, rather than have it
    fail. The pytest framework offers a function for doing exactly that.    For example,
    my API test is the only test dependent on having a working internet connection
    and the LanguageTool API’s availability. If I’m not careful how I write it, it
    could easily become a *flaky test*, which is a test that may fail unexpectedly
    or periodically for reasons other than a flaw in the code it’s testing. Deal with
    flaky tests as soon as you find them, lest you condition yourself to ignore false
    negatives. The pytest documentation has an entire section on flaky tests and how
    to mitigate them at [https://docs.pytest.org/en/stable/flaky.html](https://docs.pytest.org/en/stable/flaky.html).    In
    this case, I need to skip my API layout test when the public API server is unavailable
    or having other problems. In the following code, I do this with the `pytest.skip()`
    function:    [PRE134]    Listing 20-14: *t**ests/test_api.py*    The first part
    of the test is almost identical to my `textproof.api.api_query()` function, as
    I’m sending a POST request with the `example_text` and storing the response.    Next,
    I want to skip the test if `response.status_code` is any value other than `200`,
    thereby indicating some sort of problem with the API itself.    I skip a test
    with `pytest.skip()`. The pytest results will show that this test was skipped,
    rather than indicate a failure.    If the API request was successful, then I iterate
    over the values in the list bound to the `["matches"]` key in the dictionary representing
    the API response, and I iterate over the same in `pytest.example_api_response`
    as defined in *tests/conftest.py*. I create a set from each of those lists, and
    then I ensure that all the expected keys, as outlined in `pytest.example_api_response`,
    are also found in the API response.    ## Advanced Fixtures: Mocking and Parametrizing    One
    of the more challenging components of testing is replicating external inputs,
    such as user inputs or network responses. *Mocking* enables you to temporarily
    replace parts of the code with versions that will simulate inputs or other scenarios
    during testing.    *Parametrizing* expands a single test out into multiple tests,
    each one with the same logic but different data. This is especially helpful for
    testing how your code handles different input data.    ### Continuing the Example:
    Representing a Typo    Mocking and parametrizing are particularly useful for testing
    how code handles different user input. In the case of `textproof`, I’ll be using
    these concepts to test the command-line user interface, but I have to build that
    interface first.    In my `textproof` program, I want to represent a single error
    found by LanguageTool as an object:    [PRE135]    Listing 20-15: *src/textproof/typo.py:1*    In
    the initializer, I populate the instance attributes with data from the API response.
    In the `__str__()` special instance method, I convert the typo to a string representation
    by showing the original sentence, underlining the typo with caret symbols (`^`),
    and then describing the typo on the next line. The result would look something
    like this:    [PRE136]    Displaying a typo is one thing, but it won’t do much
    good unless the user can change it somehow. LanguageTool provides some suggestions,
    and I want to allow a user to choose between them.    Here’s the instance method
    for getting the user’s choice, where each suggested correction from LanguageTool
    is numbered from one onward, and where `0` is “skip”:    [PRE137]    Listing 20-16:
    *src/textproof/typo.py:2*    Finally, here’s the instance method for displaying
    all the suggestions, which will also call `get_choice()` and act on the user’s
    choice:    [PRE138]    Listing 20-17: *src/textproof/typo.py:3*    With that code
    in place, I move onward to tests!    ### Parametrizing    The first thing I want
    to test is that the `Typo` initializer is storing values where and how I expect
    them. It’s all too easy to mess up dictionary access, after all! I want to test
    on multiple scenarios, namely the three typos I have the example data for in my
    *conftest.py* module.    *Parametrization* allows you to generate multiple scenarios
    from the same test function. This is preferred over hardcoding all the scenarios
    in one test, so you can isolate which specific scenarios are failing. In pytest,
    this is accomplished with the `@pytest.mark.parametrize()` decorator:    [PRE139]    Listing
    20-18: *tests/test_typo.py:1a*    In this case, I want to run `test_create_typo()`
    three times: once for each of the three valid indices on `pytest.example_api_response`
    (defined in [Listing 20-14](#listing20-14)).    The `@pytest.mark.parametrize`
    decorator accepts two arguments. The first is the string representation of the
    name of the parameter to pass values to, which is `"index"` in this case. The
    second decorator argument is an iterable of values to pass to the named parameter.    The
    test itself must have a parameter of the same name, `index` here, which will receive
    the values from parametrization. I use that herein to access a particular item
    in the list `pytest.example_api_response`.    If I run this with pytest, I see
    the following:    [PRE140]    You’ll notice three dots next to *tests/test_typo.py*,
    indicating three tests were run. These were the three scenarios for `test_create_typo()`,
    as generated by parametrization.    If one were to fail, you’d see the value passed
    to the parameter, like this:    [PRE141]    The `[1]` after the test name indicates
    the parametrized value, from which you’d know that the problem occurred with the
    scenario where `index` was `1`.    ### Indirect Parametrization of Fixtures    Thinking
    forward to some other tests I’ll write later, I don’t want to have to directly
    access items in the `pytest.example_api_response` list every time, as this is
    going to be repetitive. Instead, I want to provide a fixture that returns part
    of the example API response. I want this fixture to be available to all tests,
    not just those defined in the *tests/test_typo.py* module, so it belongs in *conftest.py*.    For
    this new fixture to work, I will need it to work with parametrization of tests.
    This is possible via *indirect parametrization*, where parametrized values are
    relayed to the fixture.    Here’s my new fixture:    [PRE142]    Listing 20-19:
    *tests/conftest.py:2a*    To work with parametrization, this fixture *must* have
    a parameter named `request`, which correlates with pytest’s `request` fixture.
    Don’t confuse this with the `requests` module I’ve been using to work with the
    API. (Can you tell yet that pytest is extraordinarily picky about names?) This
    will be used to receive the indirect parametrization value; I access that value
    via `request.param`.    I’ll also add a similar fixture for generating a `Typo`
    object:    [PRE143]    Listing 20-20: *tests/conftest.py:3a*    Tests and fixtures
    are among the rare exceptions to the rule of placing `import` statements at the
    top of the module. I want to perform the import when the fixture is used and only
    make the imported names available in the context of the fixture. That way, these
    imports won’t leak into other fixtures, which is especially helpful if I need
    to import conflicting names from elsewhere in a different fixture.    Now I rewrite
    my test to use these fixtures:    [PRE144]    Listing 20-21: *tests/**test_typo**.py:1b*    The
    test itself doesn’t need to change much, since I was already using the names `example_typo`
    and `example_response` in the suite of the test function. (It’s almost like I
    planned this!) I add the new fixtures `example_typo` and `example_response` to
    the parameter list of the test function—these are provided by the special *conftest.py*
    module—and those names are locally bound to the values returned by those fixtures.    I
    need to parametrize on the fixtures, so I once again use the `@pytest.mark.parametrize`
    decorator. The first argument is a tuple of names (as strings) I’m parametrizing
    on. The second is an iterable of tuples, with each tuple representing the values
    passed to each name. The third argument, the `indirect=` keyword argument, is
    a tuple (or other iterable) of names that actually refer to fixtures that will
    receive the values. In this case, both names are fixtures, although that does
    not necessarily have to be the case.    Running pytest again shows three tests
    in *test_typo.py* as passing, indicating that the parametrization is working!    ###
    Mocking Inputs with Monkeypatch    The way to know that the `Typo.get_choice()`
    method is working is to give it some user input. Instead of bribing my four-year-old
    niece to hammer in some input on the keyboard every time I need to run the test—even
    though she would work for snacks—I’ll create a *mock* to temporarily replace Python’s
    built-in `input()` method and provide some inputs for the test. In pytest, mocking
    is performed by a tool called `monkeypatch`.    I’ll add a fixture to *conftest.py*
    for monkeypatching `input()`:    [PRE145]    Listing 20-22: *tests/conftest.py:4*    This
    fixture uses two other fixtures: `request` and `monkeypatch`. I intend to have
    this fixture receive an iterable via parametrization. I’ll use a closure, provided
    by `fake()`, to return each value in that iterable with each subsequent call to
    the closure.    I then temporarily replace the built-in `input()` method with
    this closure via `monkeypatch.setattr()`. Note that I am actually calling `fake()`
    here, as I want to monkeypatch the closure itself in place of `input()`.    Note
    that I return nothing from this fixture! Its sole purpose is to mock `input()`
    for the lifespan of the test using the fixture. The `monkeypatch` fixture will
    automatically undo itself during teardown.    Here’s the first version of my test
    for the `Typo.get_choice()` unit:    [PRE146]    Listing 20-23: *tests/**test_typo**.py:2a*    I
    parametrize on `fake_inputs`, creating three separate scenarios. The first scenario
    should act as the user inputting `-1`, `20`, and `3`; the first two inputs would
    prompt the user to try again. The second scenario would act as if the user had
    input `3` on the first try. Finally, the third scenario, my favorite, would involve
    two nonsense inputs: `fish` and `1.1`, followed by the valid input `3`. The `indirect=True`
    parameter indicates that the other parameters should be passed on to the `fake_inputs`
    fixture.    I’ve designed these inputs to be used only with the scenario presented
    in the third typo scenario; ergo, my explicitly fetching `pytest.example_api_response[2]`.    ###
    Marking    I want to be able to use my `example_typo` fixture, instead of manually
    accessing the `pytest.example_api_response` list in this test, but it’s rather
    overkill to parametrize the same value each time to the `example_response` fixture.
    Instead, I can pass a single parameter with *marking*, which is the application
    of metadata to tests and fixtures. (Parametrization is a type of marking.)    I’ll
    use my own custom mark called `typo_id` to specify a scenario number. I want this
    same mark to work on `example_response` and `example_typo`. Here’s the adjusted
    `example_response` fixture:    [PRE147]    Listing 20-24: *tests/conftest:2b*    In
    short, I try to get the value passed to the `typo_id` mark, but if it’s not provided,
    I default to using the value provided by parametrization. If a value is not provided
    to the fixture by either means, an `AttributeError` will be raised from trying
    to access the then-undefined `request.param`.    While I’m here, I’ll modify the
    `example_typo` fixture in the same way:    [PRE148]    Listing 20-25: *tests/conftest:3b*    I
    can now rewrite my `test_choice` test to use the `example_typo` fixture with marking:    [PRE149]    Listing
    20-26: *tests/**test_typo**.py:2b*    I use the `@pytest.mark.typo_id` decorator
    to pass a value to the `typo_id` mark, and that is used by the `example_typo`
    fixture.    Running pytest again shows this is successful, with one small hiccup:    [PRE150]    There
    is now a warning about using an unknown mark. To fix this, I need to register
    the mark with pytest. There are two primary ways I can do this. The first way
    is to use a configuration file named *pytest.ini*; the second is to add the setting
    (using slightly different syntax) to *pyproject.toml*. Of the two, the latter
    is preferred, as it allows you to collect nearly all the configuration settings
    for various Python tools into one *pyproject.toml* file. I’ll use that approach
    in this example:    [PRE151]    Listing 20-27: *pyproject.toml:1b*    Below the
    section `[tool.pytest.ini_options]`, I assign to `markers` a list of all custom
    mark names as strings. The part of the string after the colon is the mark’s optional
    description, not part of the mark name.    Alternatively, I could register the
    mark from the `pytest_configure()` function in *conftest.py*, like this:    [PRE152]    However,
    I’ll stick with the *pyproject.toml* approach in [Listing 20-27](#listing20-27)
    instead.    Whichever way you register the mark, running pytest again shows that
    the warning is resolved.    ### Capturing from Standard Streams    If I want to
    test `Typo.select_fix()`, I need to not only be able to provide input, but also
    verify the output. By default, pytest captures everything sent to the standard
    output and standard error streams, including everything sent from print statements.
    This is why you cannot use `print()` directly in a test and see the output during
    the run, unless you invoke pytest with the `-s` argument to shut off standard
    output and standard error capture. Because pytest captures output, that output
    can be accessed directly using the `capsys` fixture.    Before continuing, I must
    add the expected outputs to *conftest.py*:    [PRE153]    Listing 20-28: *tests/conftest.py:1b*    I’ll
    also add a fixture for accessing these prompts using parametrization or the `typo_id`
    mark:    [PRE154]    Listing 20-29: *tests/conftest.py:5*    Here’s the test for
    `Typo.select_fix()`:    [PRE155]    Listing 20-30: *tests/**test_typo**.py:3*    I
    indirectly parametrize on the fixtures `example_typo` and `example_prompt`. I
    monkeypatch `input()` to always simulate the user entering `0` at choice prompts.
    *After* running the `example_typo.select_fix()` method, I retrieve the captured
    output and ensure it matches the expected output as defined in `example_prompts`
    from *conftest.py*.    ### GUI Testing    Mocking `input()` and capturing from
    the standard output stream is all well and good for command-line applications,
    but what about GUI-based and web-based applications? Although testing a user interface
    is considerably more complicated, there are a number of libraries that make this
    easier.    *PyAutoGUI* is one such tool, allowing you to control the mouse and
    keyboard from Python. It’s compatible with any Python test framework, and it works
    on Windows, macOS, and Linux (but not on mobile). More information is available
    in the official documentation: [https://pyautogui.readthedocs.io/](https://pyautogui.readthedocs.io/).    If
    you’re using the Qt GUI framework (PyQt5, PyQt6, PySide2, or PySide6), consider
    *pytest-qt*, which is designed specifically for testing Qt 5 applications. As
    the name suggests, this is a plug-in for the pytest framework. Check out their
    official documentation at [https://pytest-qt.readthedocs.io/](https://pytest-qt.readthedocs.io/).    If
    you work with web development, you may already be familiar with *Selenium*, a
    browser automation tool for testing web applications. Selenium has official Python
    bindings, which are available on pip simply as `selenium`. You can learn more
    about Selenium at [https://www.selenium.dev/](https://www.selenium.dev/) or by
    reading the unofficial documentation, *Selenium with Python* by Baiju Muthukadan,
    at [https://selenium-python.readthedocs.io/](https://selenium-python.readthedocs.io/).    For
    mobile development, *Appium* is one of the leading test automation frameworks.
    It borrows some concepts and specifications from Selenium, as the name implies.
    *Appium-Python-Client* is the official Appium client for Python, and it is available
    through pip. For more information about Appium, see [https://appium.io/](https://appium.io/)
    and [https://github.com/appium/python-client](https://github.com/appium/python-client).    ###
    Continuing the Example: Connecting the API to Typo    In my program, I now need
    to connect the API request logic and the `Typo` class. I’ll create a `CheckedText`
    class to store the text being edited, alongside the typos detected in it.    [PRE156]    Listing
    20-31: *src/textproof/checked_text.py*    I’ll let you read through the logic
    yourself, using what you know. In short, the initializer creates a `CheckedText`
    object by running a provided string of plaintext through the API and then initializing
    `Typo` objects for each typo reported by the API.    The `fix_typos()` instance
    method will iterate over each `Typo`, prompting the user to select what to do
    about each via the `Typo.select_fix()` instance method. Then, the method will
    make the selected correction directly in a copy of the text, bound to `self.revised`.
    In this logic, I had to work out how to deal with a correction having a different
    length from the original text being replaced, then factor that into future edits.
    One of the upcoming tests will confirm this logic worked.    ### Autouse Fixtures    All
    my tests up to this point, except one, have been able to sidestep use of the API.
    I need to start tying together all the logic in my `textproof` program, so my
    upcoming tests will need to monkeypatch the API call. In tests, I *always* want
    a call to `textproof.api.api_query()` to return `example_api_response`, rather
    than send a request to the public API. I don’t want to leave it to my (infamously
    bad) memory to include the fixture on each test that might have such a call. To
    get around this, I’ll make an *autouse fixture*, which is automatically applied
    to all tests.    I add the following fixture to *conftest.py*:    [PRE157]    Listing
    20-32: *tests/conftest.py:6a*    The `autouse=True` argument passed to the `@pytest.fixture`
    decorator causes this fixture to be used by *all* tests.    In this fixture, I
    have a callable that can be called in the same way as `textproof.api.api_query`,
    accepting one argument, which I ignore. The callable returns `example_api_response`.
    I also print “FAKING IT” to the screen, instead of the public API information
    that `textproof.api.api_query()` prints. This is ordinarily invisible, since pytest
    captures all output, but if I invoke the test with `pytest -s`, I can confirm
    that the monkeypatched function is being used instead of the real thing.    There’s
    one surprising problem with this fixture: it won’t actually monkeypatch the `api_query()`
    function in the context of the *src/textproof/checked_text.py* module. This is
    because of this import line:    [PRE158]    Monkeypatching occurs *after* the
    modules have performed all their imports, so replacing `textproof.api.api_query`
    doesn’t shadow the function that was already imported into this module as `api_query`.
    In other words, the `import` statement bound the function in question to a second
    fully qualified name: `textproof.checked_text.api_query`.    Instead, I need to
    monkeypatch each fully qualified name that the function may be bound to:    [PRE159]    Listing
    20-33: *tests/conftest.py:6b*    If I import `api_query` elsewhere in my program,
    I’ll need to add any other fully qualified names to this fixture.    Once this
    fixture is in place, there’s nothing else I need to do to use it. Because it’s
    an autouse fixture, all the tests in this project will automatically use it. I
    can now safely proceed with testing *src/textproof/checked_text.py*, knowing that
    no actual API requests will take place in the process:    [PRE160]    Listing
    20-34: *tests/test_checked_text.py:1*    That new fixture and test employ the
    concepts I’ve already introduced, so I won’t rehash them.    ### Mixed Parametrization    It
    is possible to mix direct and indirect parametrization in the same test. For example,
    to test different outcomes with the `CheckedText` object, I will need to use the
    `fake_inputs` fixture while directly providing the expected outcome. I can do
    that like so:    [PRE161]    Listing 20-35: *tests/test_checked_text.py:2*    The
    trick here is that, although I’ve specified two arguments to parametrize, I’ve
    only made one of them—`fake_inputs`—indirect. I can then run the `example_checked.fix_typos()`
    method, which will use the monkeypatched `input()` function provided by the `fake_inputs`
    fixture, and then compare `example_checked.revised` to the expected result parametrized
    on `expected`.    I should point out that although this test corresponds to a
    unit, `CheckedText.fix_typos()`, it is also an *integration test*, because it
    demonstrates several other units working together correctly. Integration tests
    are just as important as unit tests, as it’s perfectly possible to have multiple
    working units that simply don’t interact correctly.    ### Fuzzing    In the program
    I built in this chapter, I provided explicit input values for all my tests. However,
    passing tests may conceal many bugs, because it’s all too easy to overlook edge
    cases. *Fuzzing* is a technique that can help catch these edge cases, generating
    random inputs in tests to find ones that fail in unexpected ways.    The *pythonfuzz*
    tool, currently maintained by GitLab, is designed to conduct fuzz testing on Python.
    It works independently of any other testing framework. To learn more about pythonfuzz,
    check out the README and examples in the official repository: [https://gitlab.com/gitlab-org/security-products/analyzers/fuzzers/pythonfuzz](https://gitlab.com/gitlab-org/security-products/analyzers/fuzzers/pythonfuzz).    ###
    Wrapping Up the Example    Have you noticed that I haven’t even run the `textproof`
    package directly yet? It isn’t a complete or valid program, but even now, I know
    that all the pieces will work as expected. This is the beauty of testing while
    coding. I can confirm my work on each part as I go, even if the whole is not complete.    Still,
    this example would feel wrong if it didn’t result in a complete program, so here’s
    the last needed module: *src/textproof/__main__.py*:    [PRE162]    Listing 20-36:
    *src/textproof/__main__.py:1a*    This module defines my program’s command-line
    interface, using the popular `click` package, which is easier to use than the
    similar built-in `argparse` module. On the command line, I accept one required
    parameter, *path*, where I want to read the text from. The optional `--output`
    flag accepts a path depicting where I want to write the revised text to.    I
    define the `FileIO` object with these paths, read in the text, and instantiate
    a `CheckedText` object from that text. As you will remember, in the process of
    instantiating the `CheckedText` object, a request is sent to the LanguageTool
    public API, and the suggested revisions are sent back.    The call to `check.fix_typos()`
    will walk the user through each suggestion, prompting them to select a fix, which
    will be immediately applied. The revised text is given back to the `FileIO` object
    file and saved to the file.    That’s it! Now I can try this out. First, I’ll
    create a file containing text to revise, which for this example, I’ll just save
    in the root of the *textproof/* project directory, next to *setup.cfg*:    [PRE163]    Listing
    20-37: *fixme.txt*    Finally, I’ll invoke my `textproof` program in the virtual
    environment, like this:    [PRE164]    Assuming I have an internet connection
    with access to the LanguageTool public API, the program will display the first
    error and prompt me to select a fix. I’ll omit the full output here, since it’s
    quite long, but I encourage you to try out the program yourself if you’ve been
    building along with me.    ## Code Coverage    When you start talking to developers
    about testing, you’ll likely hear the term *code coverage* a lot. Code coverage
    refers to the percentage of lines of code in your project that your tests execute,
    or *cover*. Good code coverage is important because any uncovered code is likewise
    not tested and could be harboring bugs or other undesirable behavior.    Python
    offers two built-in modules that track which statements are executed: `trace`
    and `ctrace`. Instead of using these directly, most Python developers use the
    third-party tool *coverage.py* (`coverage` in pip), which employs `trace` and
    `ctrace` behind the scenes to generate code coverage reports.    I’ll test my
    code coverage now. If you’re using pytest specifically, you can use *pytest-cov*,
    a plug-in that allows you to invoke coverage.py from pytest. I won’t use that
    plug-in here, to keep this example as framework agnostic as possible. Instead,
    I’ve adapted and expanded a technique from developer Will Price ([https://www.willprice.dev/2019/01/03/python-code-coverage.html](https://www.willprice.dev/2019/01/03/python-code-coverage.html)).    First,
    I want to add the `coverage` package to my testing dependencies in *setup.cfg*,
    like this:    [PRE165]    Listing 20-38: *setup.cfg:1b*    Next, I’ll ensure that
    the package is installed in the virtual environment by issuing the following in
    the command line:    [PRE166]    Code coverage will be assessed the same, whether
    you install your package as editable (with `-e`) or not.    I also need to tell
    coverage.py what files to scan and tell it about any replication of those files.
    This is especially important with an `src`-based project configuration, where
    tests may be running code installed in a virtual environment. To inform coverage.py
    what to scan, I add two new sections to the *pyproject.toml* file:    [PRE167]    Listing
    20-39: *pyproject.toml:2*    In the first section, `[tool.coverage.run]`, I specify
    a list of packages I am testing. In the second section, `[tool.coverage.paths]`,
    I indicate the path to the original source code and where the source code can
    be found inside a virtual environment. These paths will be considered equivalent,
    as far as coverage.py is concerned; the tool will recognize *src/textproof/api.py*
    and *venv/lib64/python3.9/site-packages/textproof/api.py* as the same module,
    in terms of results.    Finally, I can invoke coverage.py from the command line,
    like this:    [PRE168]    The first command invokes pytest in the context of coverage.py.
    Although I don’t pass any arguments to pytest here, you can. If you’re using a
    different test suite, you can invoke that instead of pytest here.    Next, I combine
    reports for the same files in different locations, following the guidance I provided
    in the `[tool.coverage.paths]` section of *pyproject.toml*. Depending on your
    circumstances, this command may not have anything to combine, but it never hurts
    to check.    Finally, I display the coverage report:    [PRE169]    Seventy-two
    percent isn’t too bad for a first attempt! I could go back and add more tests
    if I wished, pushing this number ever closer to 100 percent.    Code coverage
    is a useful metric to have, so long as you remember that it is part of a larger
    picture. In his article, “Flaws in coverage measurement” ([https://nedbatchelder.com/blog/200710/flaws_in_coverage_measurement.html](https://nedbatchelder.com/blog/200710/flaws_in_coverage_measurement.html)),
    coverage.py developer Ned Batchelder points out that 100 percent coverage can
    create a false sense of security:    > There are dozens of ways your code or your
    tests could still [be] broken, but now you aren’t getting any directions. The
    measurement coverage.py provides is more accurately called statement coverage,
    because it tells you which statements were executed. Statement coverage testing
    has taken you to the end of its road, and the bad news is, you aren’t at your
    destination, but you’ve run out of road.    Similarly, in a 2000 paper entitled
    “How to Misuse Code Coverage,” Brian Marick makes this observation:    > If a
    part of your test suite is weak in a way that coverage can detect, it’s likely
    also weak in a way coverage can’t detect.    That 72-percent code coverage I achieved
    tells me that *at least* 28 percent of the code is not being tested, but the true
    percentage of untested code is almost certainly more. Code coverage can point
    out areas where additional testing will be helpful, but it cannot issue any guarantees
    that additional testing isn’t needed elsewhere.    You can learn more about coverage.py
    from the official documentation: [https://coverage.readthedocs.io/](https://coverage.readthedocs.io/).    ##
    Automating Testing with tox    Up to this point, I’ve been testing on one virtual
    environment, which in my case is running Python 3.9\. I also like to believe that
    said virtual environment only contains the packages demanded explicitly by *setup.cfg*,
    but I may have forgotten about something I manually installed or something I’d
    previously specified as a requirement that I’ve since dropped but forgotten to
    uninstall.    The *tox* tool is a fairly essential part of a testing system, because
    it automates installing and testing your package in fresh virtual environments
    for multiple versions of Python. In this section, I’ll demonstrate this tool’s
    use within my Timecard project.    I should first add `tox` to my *setup.cfg*:    [PRE170]    Listing
    20-40: *setup.cfg:1c*    Traditionally, all of the configuration for tox belongs
    in a *tox.ini* file. More recently, the trend is shifting toward use of *pyproject.toml*
    instead, for as much as possible. As I write, however, native support for *pyproject.toml*
    syntax is still forthcoming. You’ll need to embed the *tox.ini* file contents
    directly in *pyproject.toml*, like this:    [PRE171]    Listing 20-41: *pyproject.toml:3*    Whatever
    I would have saved in *tox.ini* now belongs in the multiline string assigned to
    `legacy_tox_ini`, under the `[tool.tox]` section.    Within the *tox.ini*-style
    data itself, I have two sections. Under `[tox]`, I use `isolated_build` to specify
    that tox should create fresh, isolated virtual environments for its tests. The
    field `envlist` is a comma-separated list of Python environments I want to test
    against. The tox tool supports Python 2.7 (`py27`), Python 3.4 (`py34`) through
    the latest release (`py310`, at the moment), Pypy’s latest releases (`pypy27`
    and `pypy35`), and Jython (`jython`). (See Chapter 21 to learn more about Pypy
    and Jython.)    Under the `[testenv]` section, I list the testing dependencies
    with `deps`. You’ll notice I omitted `coverage` here, since there’s no need to
    run coverage in all these different environments. I set `commands` to the command
    I use to invoke tests: in this case, that’s just `pytest`. This command will be
    run directly in each virtual environment, so I don’t need to worry about the `venv/bin/`
    prefix, which would be wrong anyway.    I ensure tox is installed in my primary
    virtual environment via the following, as usual:    [PRE172]    Finally, I can
    invoke tox:    [PRE173]    It may take several minutes to run. As it does, you’ll
    notice the `textproof` package and its dependencies (but not the optional `[test]`
    dependencies) being installed in each virtual environment and the tests being
    run.    After everything has run, you’ll see a summary report:    [PRE174]    I
    know that my package installs and my tests work on Python 3.8, Python 3.9, and
    Python 3.10, so I’m also reasonably confident `textproof` could run in any of
    those environments on other machines.    You can learn more about tox from the
    official documentation: [https://tox.readthedocs.io/](https://tox.readthedocs.io/).    ##
    Benchmarking and Profiling    As programmers, we’re often very interested in making
    our code run faster. You will likely make many decisions about your code, based
    primarily on the notion that one technique will run faster than another. As I
    mentioned at the top of the chapter, all claims of performance are mythical until
    proven otherwise.    *Benchmarking* is how you establish that one piece of code
    is faster than another. The closely related technique of *profiling* is how you
    find areas where existing code can be optimized, by locating performance bottlenecks
    and common inefficiencies.    Python offers four built-in tools that are useful
    for these tasks: `timeit`, `cProfile`, `profile`, and `tracemalloc`. I’ll cover
    each briefly.    ### Benchmarking with timeit    When you need to quickly verify
    that one chunk of code is faster than another, `timeit` is an excellent tool.
    For example, you may encounter a claim online that multiple assignment in Python
    is faster than the ordinary single assignment I’ve used throughout the book. You
    can verify that claim using `timeit`, like this:    [PRE175]    Listing 20-42:
    *profiling_with_timeit.py*    Each statement I’m measuring should be in a function
    or other callable object. I also must determine how many times to evaluate and
    run each statement. This needs to be a large number of times for the results to
    be meaningful, and the larger the number is, the more accurate the results will
    be. I bound this value, `10_000_000` (10 million), to `count` and passed it to
    the optional `number=` keyword argument of `timeit`, rather than risk entering
    different numbers on the two function calls, which would skew the results.    The
    number of seconds that elapsed while running the statement repeatedly is returned
    by `timeit`. I bind the results to `time_multiple_assign` and `time_single_assign`.
    Finally, I print out the results. I use a tab separator in the print statement
    to line up the two numbers.    Running the code, here are the results:    [PRE176]    You’ll
    get different results each time because your computer manages processes via pre-emptive
    multitasking (recall Chapter 16), meaning the Python process may get suspended
    at any time to allow another process to work for a few milliseconds. One profiling
    result is not conclusive; instead, look for trends among a large sample of results.    There’s
    not a profound difference between the two, but it’s fairly clear that multiple
    assignment is *not* faster; it is rather slightly slower, at least on Python 3.10
    on my environment.    Alternatively, I could pass a string literal containing
    the code to be timed. In this case, that code must be able to run by itself and
    not depend on anything else in the module. If I needed to perform some setup,
    I could pass any amount of code as a string to the `setup=` keyword argument of
    `timeit`.    The `timeit` module also can be used from the command line. Here,
    I’ll benchmark the exact same code as in [Listing 20-42](#listing20-42), but in
    a UNIX terminal. The responses are inline:    [PRE177]    The `-n` argument is
    where I specify how many times the code is executed. The last argument is the
    required one: the Python code to run, as a string. (If you’re using Bash, remember
    to wrap the string in single quotes, rather than double quotes, to prevent Bash
    from trying to interpret anything in the Python code.)    ### Profiling with cProfile
    or profile    While benchmarking produces a single measurement for each code snippet
    measured, profiling generates a table of measurements, allowing you to see what
    parts of the code take the most time to run.    Python offers two tools for conducting
    in-depth code profiling: `cProfile` and `profile`. These both have exactly the
    same interface, but while `cProfile` is written as a C extension, thereby minimizing
    overhead and bypassing the GIL, the `profile` module is written purely in Python
    and has considerably more overhead as a result. For this reason, I use `cProfile`
    whenever possible and only use `profile` when `cProfile` is not available, such
    as when I’m using an alternative Python implementation.    Unlike `timeit`, the
    `cProfile` and `profile` modules are aware of their own surroundings and can call
    any functions or methods available in the current namespace.    You *can* perform
    benchmarks with `cProfile` or `profile`, merely by running the two competing statements
    or function calls in separate calls to `run()`. However, `timeit` is usually better
    suited for this purpose. I’m instead going to use `cProfile` for the purpose it’s
    best suited to: identifying possible performance bottlenecks.    I’ll call `cProfile`
    on the `main()` function from the `textproof` package’s default entry point, like
    this:    [PRE178]    Listing 20-43: *src/textproof/__main__.py:1b*    Since this
    is temporary code, I import `cProfile` right here, instead of at the top of the
    file. Both `cProfile` and `profile` provide identical methods, including `run()`.
    If you use `profile` instead of `cProfile`, everything else in my examples is
    the same.    I pass a string containing the Python statement to `profile`. In
    this case, I want to profile the entire program by calling the `main()` function.    Now
    I can install and invoke my program. I must not use the usual entry point provided
    by `textproof`, as that will bypass this whole if `__name__` clause. Instead,
    I need to execute the package directly:    [PRE179]    The program will start
    as normal, and I can interact with it.    After I finish using the program and
    it exits, `cProfile` displays a report on the terminal. However, this report is
    huge, difficult to navigate, and unhelpfully sorted by name. I need to sort on
    something more useful, such as, say, the number of calls.    The class `cProfile.Profile()`
    provides a bit more control. Ordinarily, I can use it like this, although there’s
    one critical problem particular to my code that I’ll come back to:    [PRE180]    Listing
    20-44: *src/textproof/__main__.py:1c*    I create a new `cProfile.Profile` object
    and bind it to the name `pr`. I enable it with `pr.enable()`, after which I have
    the code I want to profile. When I’m done, I disable the profiler in the same
    manner, with `pr.disable()`.    To sort the profiling results, I create a `pstats.Stats()`
    object. I strip out the path information with `strip_dirs()`, so I see only module
    names. Then I sort by the *cumulative runtime* of each function with `sort_stats()`,
    meaning the total time the program spent running that function.    Finally, I
    print out the stats with `print_stats()`, specifying that I only want to see the
    first 10 lines of output, instead of the hundreds that would be displayed. I could
    also pass a floating-point number here, representing a percentage of lines to
    display.    As of Python 3.8, `cProfile.Profile` is also a context manager, so
    I can use this syntax instead of manually enabling and disabling:    [PRE181]    Listing
    20-45: *src/textproof/__main__.py:1d*    If you try to run the code from either
    [Listing 20-44](#listing20-44) or [Listing 20-45](#listing20-45), you’ll notice
    that *there is no output*. I spent about half an hour scratching my head over
    this one. I finally realized that, because I decorated `main()` with `@click.command()`,
    Click causes the program to exit immediately at the end of `main()`, instead of
    returning here to finish up. This sort of problem isn’t exclusive to Click. In
    real-world applications, there are many situations that will cause the program
    to terminate without returning from `main()` or another function normally. Perhaps
    the user closes a window or clicks the Quit button.    In this case, I can get
    the best results by moving the logic right into my `main()` function:    [PRE182]   **Listing
    20-46: *src/textproof/__main__.py:1e*    Executing that will *finally* give me
    some useful output:    [PRE183]    The columns here are the number of calls (`ncalls`),
    the total time spent in the function itself (`tottime`), the average time spent
    in the function (`percall`), the total time spent in the function and anything
    it calls (`cumtime`), and the average thereof (`percall`). The most insightful
    of these is `cumtime`, which I sorted on.    I might have expected the API call
    to take the longest, but in fact, it’s sixth on this list, with a cumulative runtime
    of `0.827` seconds. The method `fix_typos()` from *checked_text.py* is the winner,
    at `2.693` seconds, but on closer examination, I can see that virtually all this
    time was spent in the `input()` function. The program’s runtime is IO-bound, but
    since it feels perfectly responsive, it needs no further attention.    I could
    increase the number of results displayed and continue to work my way through it,
    looking for possible bottlenecks, but you get the idea.    You can also invoke
    `cProfile` or `profile` from the command line. By itself, this does not provide
    a means of showing only a segment of results, which makes seeing the results decidedly
    non-trivial. Instead, you can view the results graphically with the tool *SnakeViz*,
    installable from pip as `snakeviz`. Then, I use it like this:    [PRE184]    I
    invoke cProfile directly on the command line, specifying that the profiling results
    will be saved in the file *profile_out* ❶. Then, I open *profile_out* with `snakeviz`,
    which will open an interactive graph of the results in your default web browser.    You
    can learn more about SnakeViz at [https://jiffyclub.github.io/snakeviz/](https://jiffyclub.github.io/snakeviz/).
    There’s quite a lot more to profiling on Python. The official documentation does
    an excellent job of demonstrating how to perform effective profiling and the various
    considerations that go into it: [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html).    ###
    tracemalloc    If `cProfile` or `profile` gives you a picture of time complexity
    in your code, what about space complexity? If you’re using CPython, you can use
    *tracemalloc* to examine how memory is allocated on the system and see what parts
    of your code are using the most memory.    Bearing in mind the logistical issues
    I mentioned with `cProfile`, the documentation is more than sufficient to demonstrate
    how this works: [https://docs.python.org/3/library/tracemalloc.html](https://docs.python.org/3/library/tracemalloc.html).    ##
    Wrapping Up    Testing is a critical component of any production-grade project.
    The Python ecosystem offers many tools for testing code, as well as tools for
    checking code coverage and automating testing in different environments. In practice,
    it takes a bit of work to get all these components to work together seamlessly,
    although using an `src`-based project structure helps. Once your test system is
    working smoothly, it becomes easier to continually verify that each change you
    make to the code is a step in the right direction.    The project structure I’ve
    demonstrated also works well with *continuous integration* tools, like GitHub
    Actions, Travis CI, CircleCI, and Jenkins, which automatically run tests on repository
    commits or pull requests.    In addition to this, you can gain insights on the
    performance of your code by benchmarking with `timeit`, profiling with `cProfile`
    or `profile`, and checking memory allocation with `tracemalloc`.    I’ve also
    got some incredible news for you: if you’ve been following me since Chapter 1,
    you’ve now seen, learned, and practiced nearly every essential component of the
    core language, a good chunk of the standard library, and much of the Python ecosystem
    as a whole. We now have just one more stop to go on our tour.**  **# 21 The Parting
    of the Ways  ![](Images/chapterart.png)  You’ve reached an important milestone
    on your programming journey: you now know Python! You’ve become familiar with
    the syntax and patterns, and you’ve learned how to structure, design, and ship
    production-quality software in the Python language. Equipped with this foundational
    knowledge, you’ll be able to understand the official documentation and even participate
    in the pedantic discussions so common among Python developers.    However, there
    is a profound difference between knowing a language and mastering it. Only by
    writing real-world code can you truly *think* in Python. If you’ve been working
    on an actual project while reading this book, you may already have reached this
    milestone. Otherwise, your next step is quite simple and yet incredibly complicated:
    go build something!    “Yes, but what?” you might say. “I know how Python works
    now, but what can I really make with it?”    You’re at a crossroads. From here,
    you can go in many directions. In this chapter, I’ll point out several of the
    best-traveled roads and suggest further resources for the next leg of your journey,
    whatever that may be. Finally, I’ll show you how to get plugged into the Python
    community as a whole.    ## About the Future    Python is under perpetual development.
    Each version brings new features, and each new feature typically starts life as
    a PEP, which can stem from conversations anywhere in the community. From there,
    the PEP may be debated, adjusted, reworked, and ultimately either accepted or
    rejected.    Due to the nature of software development, not all changes to the
    language are smooth. When a package is under consideration for addition to the
    standard library, it may be marked as a *provisional package* or *provisional
    API*, meaning it may change at any time, without regard to backward compatibility.
    The documentation will warn you of provisional packages, according to the rules
    outlined in PEP 411.    On occasion, when a feature is slated to be released in
    a later version of Python but the core developers want to allow users of the language
    to test a preview version in live code in advance, the feature will be added to
    a special module called `__future__`. The upcoming feature can be imported from
    that module and used as if it had already been made part of the language. As of
    the date of this writing, there’s only one upcoming feature in `__future__`: postponed
    evaluation of annotations (PEP 563), which is a feature of Python 3.10.    If
    you want an insider’s view of possible new features and the future of the language,
    sign up for the official forums at [https://discuss.python.org/](https://discuss.python.org/)
    and subscribe to the `python-dev` mailing list at [https://mail.python.org/mailman3/lists/python-dev.python.org/](https://mail.python.org/mailman3/lists/python-dev.python.org/).    ##
    Where Do You Go from Here?    Python’s versatility is a key reason it remains
    one of the most popular programming languages. Yet it’s essential to remember:
    you cannot learn everything, nor should you attempt it! Programming is not like
    riding a bike. Knowledge, once gained, must be regularly practiced or it will
    be lost.    The better route is to find a problem that you care about solving
    and build a solution for it. The purpose of this section is to give you a sampling
    of the sorts of problems Python is commonly used to solve.    The future is in
    your hands. What will you build?    ### Application Development in Python    I
    personally enjoy building GUI-based user applications in Python. Whether you’re
    developing for the desktop or mobile, Python is a great language for application
    development because of its intuitive syntax and wide selection of frameworks.    Even
    in this internet age, desktop and mobile applications still have a firm place
    in the market. Services like Spotify and Dropbox provide client applications (both
    written in Python!) with additional device integration. Desktop applications are
    still reliable workhorses in many fields and workflows, from graphics design to
    data visualization. They are also in a position to fully utilize system resources
    and hardware in ways that may be more challenging in the browser.    There are
    a number of GUI frameworks available for Python, including *Tkinter*, the Python
    binding for the Tk framework. It’s one of the easiest GUI frameworks to pick up,
    but its default graphics style is noticeably outdated. Tkinter is included in
    the Python standard library, although some Linux distributions distribute it as
    a separate package.    One of the most prominent GUI frameworks is Qt (officially
    pronounced “cute”), which provides everything you need to build clean, modern
    applications across a spectacular array of environments and devices. There are
    two Python bindings for the Qt framework: *PySide2* (Qt 5) and *PySide6* (Qt 6),
    the official bindings maintained by The Qt Company; and *PyQt5* or *PyQt6*, which
    are maintained by Riverbank Computing.    Another popular GUI framework is *GTK*,
    a mature and robust framework that is particularly prominent on Linux. *PyGObject*
    is the Python binding for GTK3 and GTK4.    *Kivy* is a GUI toolkit that works
    across major desktop and mobile operating systems. It’s especially geared toward
    touch screen devices (although it supports keyboard and mouse) and is particularly
    well-suited for game development. As of this writing, it’s particularly difficult
    (and not fully supported) to package a Kivy application for Linux. Some improvements
    made in Kivy 2.0 are promising on this front, but I still strongly recommend that
    you figure out your packaging *before* you start building.    There are quite
    a few more GUI frameworks, such as *wxPython* and *Flexx*, but there are far too
    many to enumerate here. You can find a fairly up-to-date list here: [https://wiki.python.org/moin/GuiProgramming](https://wiki.python.org/moin/GuiProgramming).    If
    you don’t know where to start with GUI applications, I recommend you start with
    Qt. If you prefer a guided approach, check out *Create GUI Applications with Python
    & Qt5* by Martin Fitzpatrick. He has editions of the book for both PySide2 and
    PyQt5\. Visit his website at [https://www.learnpyqt.com/](https://www.learnpyqt.com/)
    for more information, plus tutorials and examples.    ### Game Development in
    Python    Although it’s not as robust as many game engines, Python lends itself
    to fairly streamlined game development. Prominent games like *Civilization IV*,
    *EVE Online*, *Frets on Fire*, and *Toontown Online* were all built with Python.
    Depending on the game, you may be able to get by with one of the general-purpose
    GUI frameworks, but for best results, you’ll often want to pick up a dedicated
    game development library.    *PyGame* is one of the oldest and most frequently
    cited Python game development libraries. It’s primarily a wrapper around the *Simple
    DirectMedia Layer (SDL2)*, which provides cross-platform access to the hardware
    for working with graphics, sound, and devices. It also interfaces with other graphics
    APIs, like *OpenGL*, *Direct3D*, and *Vulkan*.    There are more options beyond
    PyGame, depending on the sort of game you want to build. *Wasabi2D* and *pyglet*
    both work with OpenGL, which is the underpinning of most major game engines. *Panda3D*
    and *Ogre* are two popular options for creating real-time 3D games. There are
    many other libraries besides.    Whichever game development framework you want
    to use, its documentation is the best place to start. Alternatively, if you’d
    prefer a shallow learning curve with plenty of guided examples, check out Al Sweigart’s
    *Invent Your Own Computer Games with Python,* 4th Edition (No Starch Press, 2016),
    which will get you acclimated to PyGame and, more importantly, the different concepts
    associated with game development.    ### Web Development in Python    Python excels
    as a server-side language, especially for rapidly developing web applications
    and APIs. There are three libraries in the spotlight at present for this: *Django*,
    *Flask*, and *FastAPI*.    *Django* is the batteries-included option. It employs
    a Model View Template (MVT) architecture, and it includes database integration,
    an object-relational mapper (ORM), and just about everything you could need to
    build a web application or API in Python. Django is used by BitBucket, Instagram,
    the Public Broadcasting Service (PBS), and the *Washington Times,* among others.
    To get started with Django, visit their website at [https://www.djangoproject.com/](https://www.djangoproject.com/).
    Django Girls also has a particularly excellent tutorial at [https://tutorial.djangogirls.org/](https://tutorial.djangogirls.org/).    *Flask*,
    by contrast, is the minimalist option. It’s lightweight, providing the bare-minimum
    framework and leaving it to the developer to choose what tools and components
    to use. The Flask community provides a wide variety of extensions for adding functionality
    on par with Django, all of which are selected and installed separately. Flask
    emphasizes leaving as much control in the hands of the developer as possible.
    Websites like Pinterest and LinkedIn are built in Flask. If you want to learn
    Flask, their documentation will guide you through the entire process of setting
    up, getting started, and working with every part of the framework: [https://flask.palletsprojects.com/](https://flask.palletsprojects.com/).    *FastAPI*
    is a framework aimed at web API design specifically. It is designed for performance
    and stability, and it is fully compliant with the OpenAPI (Swagger) and JSON Schema.
    See the documentation at [https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/).    ###
    Client-Side Python    You may wonder if it is possible to run Python in the browser,
    client-side. At present, there are a few implementations of Python attempting
    exactly that. Here are just a few.    *Brython*, which is short for *Browser Python*,
    is the more mature of the two options. It works by transpiling Python to JavaScript.
    Brython is designed to work with the W3C Document Object Model (DOM). More information
    and full documentation are available at [https://www.brython.info/](https://www.brython.info/)
    and on the project’s GitHub at [https://github.com/brython-dev/brython](https://github.com/brython-dev/brython).    *Skulpt*
    is a newer solution; it’s intended to be a complete implementation of the Python
    language in JavaScript, as opposed to a transpiler. As of this writing, Skulpt
    is still missing a lot of core language features. More information is at [https://skulpt.org/](https://skulpt.org/).    *Pyodide*
    is a third option. It’s a port of CPython to WebAssembly and Emscripten, and has
    support for many C extensions. More information and documentation can be found
    at [https://pyodide.org/en/stable/](https://pyodide.org/en/stable/).    ### Data
    Science in Python    Python is one of the leading languages in the field of *data
    science*, which focuses on gaining insights and extracting information by aggregating
    and analyzing data. Data science is considered applied statistics and related
    to but distinct from computer science. The task of distilling information out
    of a data set is sometimes referred to as *data mining*. The term *big data* refers
    to work with particularly large data sets.    There is a vast ecosystem geared
    especially toward data science, although many of these tools are also useful in
    software development.    #### The Data Science Environment    *Jupyter Notebook*
    is perhaps the single most important tool in the data science ecosystem. It is
    a complete interactive-development environment and is particularly well-suited
    to data science and scientific computing, as it combines executable Python code
    with Markdown-formatted text, mathematics equations, live charts and graphs, and
    other rich media. A *notebook document* is a single Jupyter Notebook file, ending
    in the file extension *.ipynb* (for IPython Notebook, the former name of Jupyter
    Notebook). In addition to supporting Python, Jupyter works with the other two
    most popular languages in data science, namely *Julia* and *R*. You can learn
    more about Jupyter Notebook and its related projects at [https://jupyter.org/](https://jupyter.org/).    *Anaconda*
    is a distinct Python distribution geared specifically toward data science and
    scientific computing. It ships with over 250 of the most common data science libraries
    and tools preinstalled, including Jupyter Notebook. It also comes with its own
    integrated development environment, *Anaconda Navigator*, and a dedicated package
    manager, *conda*. In addition to all this, Anaconda offers a data science–oriented
    cloud service with both free and paid plans. More information can be found at
    [https://anaconda.org/](https://anaconda.org/).    #### Data Science Packages    There
    are hundreds of packages for data science in Python, but there are a handful of
    particularly notable ones, many of which consistently lead the pack:    1.  *Pandas*
    is considered essential for processing data. It supplies *dataframes*, which allow
    you to select, merge, reshape, and process data from databases, spreadsheets,
    tables, CSV files, and more, similar to the R language. Learn more at [https://pandas.pydata.org/](https://pandas.pydata.org/).
    2.  *NumPy* is the leading package for handling numeric computing, everything
    from mathematics to statistical analysis, and even performing advanced processing
    of lists and arrays. Learn more at [https://numpy.org/about](https://numpy.org/about)*/*.
    3.  *SciPy* (the library) expands on NumPy, offering additional numeric routines
    for scientific computing, including linear algebra and numerical optimization.
    It is part of the *SciPy ecosystem*, along with NumPy, pandas, and several other
    tenants of this list. Learn more at [https://scipy.org/](https://scipy.org/).
    4.  *Matplotlib* is one of the most popular libraries for generating plots, charts,
    graphs, and other data visualizations. Learn more at [https://matplotlib.org/.](https://matplotlib.org/.)
    5.  *Seaborn* expands on Matplotlib and is integrated with pandas, to provide
    more advanced data visualization tools (with all the pretty colors!) Learn more
    at [https://seaborn.pydata.org/.](https://seaborn.pydata.org/.) 6.  *Bokeh* is
    another popular visualization library, independent of Matplotlib, that allows
    you to create interactive data visualizations that can be embedded in web pages
    and Jupyter notebooks. Learn more at [https://bokeh.org/.](https://bokeh.org/.)
    7.  *Dask* is a Python parallelism library that is built specifically for working
    with major packages in data science and related fields. It allows you to speed
    up the execution time, especially when working with large data sets or CPU-intensive
    analysis. Learn more at [https://dask.org/.](https://dask.org/.) 8.  *Kedro* is
    relatively new to the party, but it fulfills an important role in data science:
    the need for a framework to keep data pipelines scalable, easily maintainable,
    and production ready. Learn more at [https://kedro.readthedocs.io/.](https://kedro.readthedocs.io/.)    There
    are plenty of subtopics in the realm of data science, including *geographic information
    systems (GIS)* and the many branches of *scientific computing*. These fields often
    have their own common libraries and tools.    Also worth a mention is *Numba*,
    a just-in-time (JIT) compiler for Python and NumPy, which allows you to compile
    specific, selected parts of your Python program to machine code.    One way to
    learn more about the Anaconda ecosystem, including many of the libraries listed
    here, is to read *Doing Science with Python* by Lee Vaughan (No Starch Press,
    2022).    ### Machine Learning in Python    Another popular topic in Python is
    *machine learning*, which is at the heart of artificial intelligence. Machine
    learning is the process by which an algorithm can be made to improve automatically
    over time, based on data and feedback provided to it. This is a process known
    as *training*. For example, your smartphone uses machine learning to improve its
    autocompletion suggestions while you type. The more you use your smartphone, the
    better that algorithm gets at suggesting the word you’re typing.    Machine learning
    works using *neural networks*, which are data structures that replicate the structure
    and behavior of biological neural networks, such as the brain you are using right
    now. When you layer neural networks together, you get into the topic of *deep
    learning*.    The structure of a neural network isn’t particularly difficult to
    comprehend, but there is a fair bit of advanced mathematics involved, including
    linear algebra, multivariate calculus, and probability. If you’re interested in
    machine learning, invest some time in understanding this math well. Don’t worry
    about doing the math on paper; focus on mastering the concepts and let the computer
    do the number crunching for you.    Machine learning is often closely associated
    with data science. (The data used to train the neural network has to come from
    somewhere!) Therefore, you’ll find many of the same packages used across both
    specialties.    Here are the five most popular machine learning packages:    1.  *TensorFlow*
    is a C++ and Python symbolic math library behind some of today’s larger machine
    learning projects, including artificial intelligence work at Google, where it
    got its start. It’s harder to learn than many of the other options, but it’s widely
    used, especially due to its speed. Learn more at [https://www.tensorflow.org/](https://www.tensorflow.org/).
    2.  *Keras* is a deep- learning API that expands on TensorFlow, and it is generally
    considered easier to use besides. If you’re looking for a place to start, this
    is an excellent option. Learn more at [https://keras.io/](https://keras.io/).
    3.  *Scikit-learn* is a simpler machine learning library built atop NumPy. It
    is particularly useful for predictive data analysis and other data science applications.
    Learn more at [https://scikit-learn.org/](https://scikit-learn.org/). 4.  *PyTorch*
    is based on Facebook’s Torch framework. It independently brings the same functionality
    you’d get out of NumPy, SciPy, and Scikit-learn. PyTorch offers acceleration via
    the GPU, and it can work with deep neural networks. Learn more at [https://pytorch.org/](https://pytorch.org/).
    5.  *Aesera* is a Python-only machine learning library that tightly integrates
    with NumPy and focuses primarily on some of the mathematics involved. Although
    it can be used by itself, it’s most often employed alongside other options, like
    Keras. Aesera is a continuation of the *Theano* library. Learn more at [https://aesara.readthedocs.io/en/latest/](https://aesara.readthedocs.io/en/latest/).    Within
    the arena of machine learning are a number of other specialties, including natural
    language processing and computer vision. Once you understand the basics of machine
    learning, you can branch out into whatever subtopics you find interesting.    If
    you’re interested in this field, two books to get you started are *Deep Learning:
    A Visual Approach* by Andrew Glassner (No Starch Press, 2021) and *Practical Deep
    Learning* by Ronald T. Kneusel (No Starch Press, 2021). If you prefer to learn
    as you go, start with Keras or Scikit-learn.    ### Security    Python is increasingly
    popular in the field of *information security*, or *infosec*, which focuses on
    ensuring data, software, and computer systems are safe and secure.    A word of
    caution is prudent here. The techniques used to find flaws in security that need
    to be shored up are the same techniques used to exploit those flaws. The entire
    field of infosec operates on a fine legal and ethical line, which separates ethical
    (“white hat”) hackers from criminal (“black hat”) hackers. Both sides know how
    to construct and deploy malware, reverse engineer software, and hack into systems:
    the difference is that the ethical hackers use these techniques to find and report
    or close security holes before criminal hackers can exploit them. For example,
    the infamous Heartbleed and Spectre bugs were discovered and reported by ethical
    hackers before they could be exploited.    Two of the best books about Python
    and infosec are *Black Hat Python*, 2nd Edition, by Justin Seitz and Tim Arnold
    (No Starch Press, 2021) and *Gray Hat Python*, also by Justin Seitz (No Starch
    Press, 2009). (The titles are deliberately ironic.) If you’re interested in this
    field, those books provide the best place to start.    But once again, let me
    remind you: it is your responsibility to use your powers for good. Using computers
    to commit crimes or cause trouble is *never* okay, and doing so will earn you
    the derision of the entire Python community. Keep your hacking ethical.    ###
    Embedded Development in Python    If maker culture is more your speed, you’ll
    be pleased to know that Python is currently the fastest-growing language in *embedded
    development*, wherein code is shipped directly on the hardware it controls. This
    means you can use Python for robotics, Internet-of-Things devices, and many other
    hardware projects.    Python works with Raspberry Pi, which comes with the *Thonny*
    Python IDE preinstalled. More information can be found at [https://www.raspberrypi.com/documentation/computers/os.html](https://www.raspberrypi.com/documentation/computers/os.html).    Python
    can be used to program the *Arduino* microcontroller via *pyserial*: [https://pythonhosted.org/pyserial/](https://pythonhosted.org/pyserial/).    *MicroPython*
    is a separate implementation of Python that is geared specifically toward embedded
    development. It works best with the *pyboard* microcontroller. You can learn more
    about MicroPython and pyboard at [https://micropython.org/](https://micropython.org/).    *CircuitPython*
    is another Python implementation. It is based on MicroPython but geared primarily
    toward *Adafruit* microcontrollers. It can also be used on a number of Raspberry
    Pi and Ardunio microcontrollers, as well as hardware from many other brands. Device-specific
    downloads and links to documentation, tutorials, and guides can be found at [https://circuitpython.org/](https://circuitpython.org/).    ###
    Scripting    With all these vast and impressive uses of Python, it’s easy to forget
    that one of the reasons this language exists is to facilitate automation and scripting.
    Countless libraries exist to allow Python to interact with all sorts of software,
    operating systems, and hardware. If you have a repetitive task that needs a clean
    solution, there’s a good chance that Python can help with that.    Two excellent
    books for learning how to automate with Python are *Real-World Python* by Lee
    Vaughan (No Starch Press, 2020) and *Automate the Boring Stuff* *with Python*,
    2nd Edition, by Al Sweigart (No Starch Press, 2019).    ## Python Flavors    As
    you’ve probably gathered, the default implementation of Python, known formally
    as *CPython*, isn’t the only implementation out there. Quite a few others exist,
    most of them with special uses.    I have to start by mentioning a highly specialized
    implementation, *RPython*, which is geared toward building interpreted languages.
    It’s a restricted subset of the Python language with a *just-in-time (JIT) compiler*,
    meaning the language is built to machine code immediately before execution, instead
    of being interpreted during execution by an interpreter. RPython is extensively
    documented at [https://rpython.readthedocs.io/](https://rpython.readthedocs.io/).    *PyPy*
    is another implementation, and it is notable for being quite a bit faster than
    CPython. It is implemented on RPython, instead of C. It owes its speed to the
    fact that it is JIT compiled, instead of interpreted, allowing it to reach performance
    comparable to C++ or Java. PyPy is always several versions behind CPython—as of
    this writing, it is up to Python 3.6—but for projects where performance matters,
    this is an acceptable compromise. Additionally, because PyPy does not rely on
    C, it typically doesn’t work with binary extensions, except for some built to
    that purpose in CFFI.    *Stackless Python* is another peculiar implementation
    of Python that offers some unique tools for improved concurrency and code structure.
    Stackless is its own beast in many ways, and it must be learned as such. The best
    place to start is with its wiki, which has links to further reading and resources:
    [https://github.com/stackless-dev/stackless/wiki](https://github.com/stackless-dev/stackless/wiki).    Earlier,
    I mentioned *Brython* ([https://www.brython.info/](https://www.brython.info/)),
    *Skulpt* ([https://skulpt.org/](https://skulpt.org/)), and *Pyodide* ([https://pyodide.org/en/stable/](https://pyodide.org/en/stable/)),
    which are in-browser implementations of Python geared toward web development.
    I also mentioned *MicroPython* ([https://micropython.org/](https://micropython.org/))
    and *CircuitPython* ([https://circuitpython.org/](https://circuitpython.org/)),
    which are implementations for embedded development.    Besides these, there are
    a handful of implementations of the Python interpreter built in different languages.
    The four most notable of these are RustPython, implemented in Rust; IronPython
    and *Python.NET*, both of which integrate tightly with the .NET framework; and
    *Jython*, which is written in Java for integration with the *Java Virtual Machine
    (JVM)*. As of this writing, RustPython supports up to Python 3.9, and Python.NET
    supports up to Python 3.8, while IronPython is only on Python 3.4, and Jython
    is still in line with Python 2.7\. You can learn more about RustPython at [https://rustpython.github.io/](https://rustpython.github.io/),
    IronPython at [https://ironpython.net/](https://ironpython.net/), Python.NET at
    [https://pythonnet.github.io/](https://pythonnet.github.io/), and Jython at [https://www.jython.org/](https://www.jython.org/).    No
    matter what implementation you use, remember that the official implementation,
    CPython, serves as the baseline for all of them. Even if you plan to spend most
    of your time in another implementation, it’s important to know how to use CPython
    well.    ## Developing for Python    The Python ecosystem itself is maintained
    by thousands of developers around the world. Some write libraries to answer specific
    needs, while others extend and improve the Python language in all its different
    implementations.    If you’d like to become involved in the development of Python,
    it would be helpful for you to have some experience or interest in other branches
    of development, so you’ll understand what needs exist and how to best address
    them. Very few developers set out to work on Python; rather, they drift into it
    after recognizing an area of need in their own work. Even so, when you’re able
    to contribute to the Python ecosystem, it’s a great feeling indeed.    ### Developing
    Python Packages and Tools    The techniques you’ve learned in this book have fully
    prepared you to build and ship production-quality packages, libraries, and development
    tools. Maybe you have some ideas already, or maybe you’re still pondering what
    you could build.    In either case, I strongly recommend learning more about the
    existing tools and packages you use on a regular basis. A large majority of projects
    are maintained by a handful of volunteers, often thanklessly, and issues sometimes
    come in faster than the volunteers can resolve them. Before you set out to build
    a brand-new thing, consider if you could instead improve an existing solution.
    Contributing to open source projects is an incredible way to build your skills
    and make new professional connections. Your contributions don’t even have to be
    massive. Whether you’re performing code reviews on backlogged pull requests, fixing
    minor bugs, doing small “housekeeping” tasks, tidying up the documentation, or
    polishing the packaging, it all helps!    There are also a number of projects
    that are abandoned or otherwise unmaintained and are in need of a new maintainer
    to step up and take over. When you adopt an abandoned project, you get the benefits
    of a working code base (for some definitions of “working”) and an existing user
    base. Often, projects are abandoned because they need to be ported to Python 3\.
    Also, working with legacy code can be a very rewarding experience.    Nearly all
    packages on the Python Package Index (PyPI) have links to the official website,
    source code, and issue tracker. Other packages list an email address for the present
    maintainer. When you find a package that you use regularly, you should seriously
    consider getting involved in its development and maintenance.    As for new projects,
    if you find yourself building a tool or library for solving a problem you’re having
    in your development efforts, consider publishing it for the rest of the world
    to use!    ### Developing Python Extensions    *Binary extension modules*, often
    just called *extension modules* or *extensions*, add new functionality atop the
    CPython interpreter and allow you to integrate CPython with C and C++ code.    *Wrapper
    modules* expose C libraries to Python. PyGObject is an example of this, as it
    wraps the GTK C library and several others besides, and it makes them available
    to Python. Extensions can also wrap C++ and FORTRAN libraries, among others.    Another
    common use case for extensions is to provide low-level access to the operating
    system, hardware, or the CPython runtime.    *Accelerator modules* offer equivalent
    behavior to a pure Python module, but they are actually written in C. One particular
    advantage of accelerator modules is that they can be written to bypass the Global
    Interpreter Lock, since they run as compiled machine code. These modules should
    provide a pure Python fallback module for situations where the extension may not
    work.    There are quite a few ways to develop extensions. Traditionally, you
    can include the *Python.h* header file in your C code and build from there. Details
    and documentation about this can be found at [https://docs.python.org/3/extending/index.html](https://docs.python.org/3/extending/index.html).    However,
    working directly with *Python.h* is no longer considered the best approach. Aside
    from this technique being quite clunky and error-prone, extensions built this
    way often have significant difficulty working with PyPy and other implementations
    of Python. Instead, there are a number of third-party tools for building extensions,
    which are far simpler and more obvious in their usage.    The *C Foreign Function
    Interface* (*CFFI*) is one of the more popular options. Unlike some other tools,
    CFFI doesn’t require you to learn an additional specialty language. Instead, it
    uses purely C and Python. It works with both CPython and PyPy. You can learn more
    about CFFI from its extensive official documentation: [https://cffi.readthedocs.io/](https://cffi.readthedocs.io/).    CFFI
    does not work with C++, so if you need C++ and Python interoperability, check
    out *cppyy*. Official documentation lives at [https://cppyy.readthedocs.io/](https://cppyy.readthedocs.io/).    Cython
    is a separate programming language that is a superset of Python and provides direct
    access to C and C++. You compile your Cython code up front, as you would with
    C. More information and official documentation can be found at [https://cython.org/](https://cython.org/).    *Simplified
    Wrapper and Interface Generator (SWIG)* is a tool for interoperability between
    over a dozen programming languages, including Python, C, C++, Java, C#, Perl,
    JavaScript, and Ruby. It can be used to create Python binary extensions. Information,
    documentation, and tutorials can be found on its official website at [http://www.swig.org/](http://www.swig.org/).    Development
    of Python binary extensions is a particularly deep topic, especially as it involves
    the C language. The Python Packaging Authority has an excellent guide that explores
    binary extension development, particularly from a packaging standpoint: [https://packaging.python.org/guides/packaging-binary-extensions/](https://packaging.python.org/guides/packaging-binary-extensions/).    ###
    Contributing to Python    Python is an open source project with a rich community
    and a well-maintained development pipeline. If you’re passionate about Python,
    the language itself always welcomes new contributors! Your contributions could
    include fixing bugs, testing patches, implementing new features, and updating
    documentation. Even if you don’t have incredible C-coding skills, there’s plenty
    for you to do. If you want to get started contributing to Python, read through
    the official Python Developer’s Guide at [https://devguide.python.org/](https://devguide.python.org/).    If
    Java or .NET is more your speed, or if you’re fascinated with RPython, you can
    instead contribute to Jython, RustPython, Python.NET, IronPython, or PyPy. These
    are all considered important implementations in the Python ecosystem, and there’s
    always more to do.    #### Changing the Language    Because Python is built by
    the community, you can propose changes to the Python language or its standard
    library. This will involve a considerable amount of work on your part, along with
    quite a bit of discussion, diplomacy, debate, and testing. Somewhere along the
    way, you will need to create a PEP outlining your proposed changes and all the
    discussion that’s gone into it so far.    Don’t embark on this process lightly!
    Even if your idea seems obvious to you, you are likely underestimating the depth
    or merit of other viewpoints. We all want Python to be the best it can be, and
    that means factoring in the wildly divergent needs and perspectives of our diverse
    user base. That’s why proposing a PEP is a significant time investment.    If
    you’re certain you’re up for the challenge, see the official guides: [https://devguide.python.org/langchanges/](https://devguide.python.org/langchanges/)
    for language changes and [https://devguide.python.org/stdlibchanges/](https://devguide.python.org/stdlibchanges/)
    for standard library changes.    #### Becoming a Core Developer    Once you’ve
    been making quality contributions to CPython for some time, you can apply to become
    a *core developer*, which brings with it additional authority and responsibility.
    Core developers are involved in leading Python development, and their opinions
    regarding language direction and proposed changes bear much weight.    If you
    want to become a Python core developer, start by contributing *patches*, which
    consist of code fixing a bug or implementing an approved feature, to CPython.
    Keep at this until a core developer offers you commit privileges. They’ll keep
    an eye on your work after that and help mentor you in the Python development process.
    Eventually, if you do this well, you may be offered an official opportunity to
    become a core developer.    The entire process and all the responsibilities and
    steps involved are outlined in the guide at [https://devguide.python.org/coredev/](https://devguide.python.org/coredev/),
    which itself is based on the Python Language Governance policies outlined in PEP
    13.    ## Getting Involved with Python    Wherever you go from here, I strongly
    recommend getting involved in the Python community! You will learn a lot from
    your fellow Python developers, and you can greatly benefit from helping and mentoring
    others. There are a number of official and unofficial communities around the internet,
    including the following:    *   The DEV Community: [https://dev.to/t/python](https://dev.to/t/python)
    *   Discord: [https://pythondiscord.com/](https://pythondiscord.com/) *   Forums
    (Official): [https://www.python.org/community/forums/](https://www.python.org/community/forums/)
    *   Libera.Chat IRC (Official): [https://www.python.org/community/irc/](https://www.python.org/community/irc/)
    *   Mailing Lists/Newsgroups (Official): [https://www.python.org/community/lists/](https://www.python.org/community/lists/)
    *   Reddit: [https://www.reddit.com/r/learnpython/](https://www.reddit.com/r/learnpython/)
    *   Slack: [https://pyslackers.com/web](https://pyslackers.com/web)    Of the
    ones on this list, my personal favorites are DEV, where this book got its start,
    and Libera.Chat IRC, where I met most of my technical editors.    ### Asking Questions    The
    primary reason developers first join a community is to ask questions. This can
    feel intimidating at first, no matter what platform you’re on! Here are a few
    principles for asking questions and getting help in the Python community. These
    guidelines are true across most programming communities, but especially in Python!    First,
    do some research and experimentation yourself. *Read the documentation*. Try out
    some possible solutions and take note of what doesn’t work. We in the Python community
    are happy to help you, but we want to see you bring your own efforts to the table,
    too.    Second, be specific. The more information you can provide, the better
    we can help you. When possible, give us code we can inspect and run, the exact
    text of error messages or wrong output, details about your environment (operating
    system, Python version, and library versions), and insight into the outcomes of
    your own experimentation. When providing all this information, be careful to follow
    community rules regarding large pastes. Many platforms, including Libera.Chat
    IRC, ask that you use a paste-sharing tool like *bpaste.net*, instead of dumping
    pastes into chat, where they will clog up the backlog. Never paste multiple lines
    of code or output directly into chat!    Third, be prepared for unexpected threads
    of feedback. If a bug is precipitating from poor design, incorrect assumptions,
    or nonidiomatic practice, we’d much rather help you fix the design than the bug.
    We aren’t interested in *working* code, so much as *idiomatic* code. You will
    likely be asked things like “What is your goal, why are you doing it this way,
    and is there a reason you aren’t doing X?” Stay calm and work with us.    Fourth,
    be polite and patient. All the online communities I’ve mentioned are staffed by
    volunteers who give freely of their time and effort to help others. It may take
    time to get an answer. Just ask your question outright in the public space—never
    “ask to ask” or inquire after an expert in such and such, as it only wastes people’s
    time—and then wait for someone to respond. In the case of IRC, *stay logged in*
    or you’ll miss our response! In any chat-based medium, if your message gets completely
    buried in backlog (I’m talking three or more desktop screen pages), calmly repost
    it. On forum-like platforms, resist the temptation to “bump” the thread.    When
    you get a response, read it thoroughly and answer thoughtfully. We will probably
    have many more questions for you as we try to home in on the solution.    ###
    Answering Questions    Having made it through *Dead Simple Python*, you now know
    quite a lot about Python! As a result, you will encounter questions from other
    community members that you may be able to answer. This is a great way to give
    back to the community and build relationships with other developers. You’ll learn
    quite a bit in the process, too. Even so, when you’re getting started, it can
    feel intimidating. Here are a few tips.    First, don’t be afraid of making mistakes.
    If you feel like you can answer a question, give it your best shot. This is a
    benefit of answering questions in the community: if you get something wrong, someone
    else will often be there to correct you. Python developers love pedantic correctness!
    (Seriously; you should see some comments I’ve gotten from my technical editors.)
    In the worst-case scenario, you’ll walk away from the encounter having learned
    something. No one worthwhile will think less of you for it, I promise.    Second,
    more than half of the process of answering a question is *asking more questions*.
    Like The Zen of Python says, “In the face of ambiguity, refuse the temptation
    to guess.” Ultimately, your goal should be to guide the asker toward the most
    Pythonic solution to the problem they’re trying to solve.    Third, it’s important
    to be kind. The only things up for critique are *code* and *practice*, never people.
    Resist the temptation to deploy put-downs or shutdowns, no matter how clever or
    “funny” they may be.    On that note, be careful of issuing “read the documentation”
    admonishments. Documentation can be infamously obtuse and difficult to parse,
    especially for a beginner. It’s okay to share a link to the relevant page or section,
    or to any other helpful resource, but you should *never* shame anyone for not
    having previously read or understood the documentation or other material. The
    same goes for web search results—it can sometimes take real skill to determine
    which keywords are the best or which results are relevant. In any case, be prepared
    in case they say that the link doesn’t solve their problem; you may need to ask
    more questions to better understand what issue they’re stuck on.    ### User Groups    Online
    communities are excellent resources, but nothing beats in-person networking and
    collaboration! There are over 1,600 Python user groups worldwide, providing opportunities
    for developers of all backgrounds and skill levels to exchange knowledge, especially
    through events like social meetings, speaking presentations, hack sessions, and
    even local conferences.    A complete list of Python user groups is maintained
    on the official Python wiki: [https://wiki.python.org/moin/LocalUserGroups](https://wiki.python.org/moin/LocalUserGroups).
    Consider connecting with one in your area. If you don’t have a nearby user group,
    you may consider starting one yourself! The wiki has a guide for that, too: [https://wiki.python.org/moin/StartingYourUsersGroup](https://wiki.python.org/moin/StartingYourUsersGroup).    ###
    PyLadies    Of particular note is *PyLadies*, a group organized by the Python
    Software Foundation, which focuses on supporting and mentoring female developers
    in the Python community.    PyLadies organizes meetups and other events, and it
    provides resources. In addition to the international online community, there are
    a number of local PyLadies groups all over the world. If there isn’t one near
    you, perhaps you could organize one. More information about PyLadies is available
    at [https://pyladies.com/](https://pyladies.com/). The official list of PyLadies
    groups is maintained here: [https://pyladies.com/locations/](https://pyladies.com/locations/).    ###
    Conferences    Attending conferences is an incredible way to learn and grow as
    a developer, while connecting with the larger Python community. There are a number
    of fantastic conferences in the Python world, including PyCon US (the official
    conference), Pyjamas, SciPy, Python Pizza, and PyData. There are also versions
    of PyCon in many countries.    The first thing that comes to mind when you think
    of a conference is probably all the lectures. Talks are certainly among the highlights,
    especially keynote presentations from important people in the Python community,
    but they’re not the only thing going on. *Workshops* present opportunities to
    get hands-on with a new topic. *Lightning talk* sessions are a lot of fun, too:
    they let anyone present a 5- or 10-minute talk, sometimes improvised right on
    the spot. Some presentations are amazingly insightful, and others are downright
    hilarious. Many conferences also have *sprints*, collaborative coding sessions
    (think “hackathon”) where attendees can freely join teams to build or improve
    code.    While attending a conference can be a lot of fun, getting involved can
    be even more exciting! If you want to present a talk at a conference, be on the
    lookout for the *Call for Proposals (CFP)*, when you can submit a proposal for
    a talk or workshop. If you’re brand-new to speaking, you may consider presenting
    to your local Python User Group or giving a lightning talk. You could also submit
    a project for a sprint, which can be an excellent way to get new contributors
    and users while improving your code.    Most conferences rely pretty heavily on
    volunteers, so consider contacting any conference you’re thinking about attending,
    and then see how you can help out. You’ll get to meet a lot of great people that
    way, and volunteering is really fun.    If you want to attend a conference but
    can’t afford to go, you can check if there are scholarships or other financial
    aid available, especially if you’re a volunteer. If you’re employed as a programmer,
    your employer may also be willing to help with some of your conference expenses.
    After all, when you get smarter, they benefit!    A list of Python conferences
    is maintained on the Python wiki: [https://www.python.org/community/workshops/](https://www.python.org/community/workshops/)*.*    ###
    Joining the Python Software Foundation    The *Python Software Foundation (PSF)*
    is the nonprofit organization that officially manages the Python language. They’re
    responsible for all the major decisions, and they help to grow the community worldwide.    The
    Python Software Foundation is an open-membership organization, meaning anyone
    can join for free. Basic members need only sign up and agree to the PSF Code of
    Conduct, which is the set of guidelines that the entire Python community operates
    on. Members are subscribed to the Python Software Foundation newsletter.    There
    are three special types of members, all of whom get to vote in the PSF Board of
    Directors elections. *Supporting members* donate to the PSF annually. *Managing
    members* volunteer at least five hours a month in the Python community, including
    helping to organize events and user groups or volunteering on Python Software
    Foundation projects. *Contributing members* volunteer at least five hours a month
    on free, publicly available open source projects that advance the mission of the
    PSF. You can learn more about PSF membership at [https://www.python.org/psf/membership/](https://www.python.org/psf/membership/).    ##
    And the Road Goes Ever On . . .    This, dear reader, is where I leave you. The
    direction you take from here is up to you! I am certain there are many adventures
    ahead of you. I send you forward into the brave frontier beyond with three final
    pieces of advice. If you’ve been coding for a while, you may already know these
    things, but they are always worth hearing again.    First, *it is dangerous to
    go alone*. Shiny new solutions call to you from the shadows, tempting you to stray
    from time-tested development paths. Clever solutions lure you away as they steal
    time and sanity from your future self and your colleagues. Bugs lurk in the dark
    recesses of your code, waiting for midnight on a Friday to leap out at you with
    bared teeth and inexplicable log files. The comradeship of your fellow Python
    developers is the surest defense against these and many more dangers! There is
    safety in numbers. I could never have written *Dead Simple Python* without the
    all the support, insight, debate, and encouragement from the Python community.
    We need each other to thrive.    Second, *embrace the adventure of making mistakes!*
    You will always learn more from solving a bug, working through a difficult problem,
    or making a mistake, than you will ever learn from writing or reading the best
    code in the world. I love how my colleague Wilfrantz Dede puts it whenever he
    embarks on a new task: “I’m going to go write some bugs.” Mistakes are an inevitable
    part of the learning process. Learn to expect them, embrace them, conquer them,
    and laugh at them! Share your coding mistakes with your colleagues. I promise
    you that most will think more of you for it, not less, and anyone who *would*
    look down on you is merely covering up their own egregious errors.    Third and
    finally, *you are a real programmer*. Never again question that! No matter how
    long you’re in this industry, you will always have more to learn. There are even
    expert developers who have been programming for decades but still feel the call
    of knowledge not yet gained.    The journey is never really over. There is always
    one more crest of the hill, one more bend in the road, and one more vast plain
    stretching ahead! Embrace every step of the adventure. It never gets old.**[PRE185]``'
  prefs: []
  type: TYPE_NORMAL
