- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11**'
- en: GRADIENT DESCENT**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: 'In this final chapter, we’ll slow down a bit and consider gradient descent
    afresh. We’ll begin by reviewing the idea of gradient descent using illustrations,
    discussing what it is and how it works. Next, we’ll explore the meaning of *stochastic*
    in *stochastic gradient descent*. Gradient descent is a simple algorithm that
    invites tweaking, so after we explore stochastic gradient descent, we’ll consider
    a useful and commonly used tweak: momentum. We’ll conclude the chapter by discussing
    more advanced, adaptive gradient descent algorithms, specifically RMSprop, Adagrad,
    and Adam.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们将稍微放慢节奏，重新审视梯度下降。我们将通过图示回顾梯度下降的概念，讨论它是什么以及如何运作。接下来，我们将探讨*随机*在*随机梯度下降*中的含义。梯度下降是一个简单的算法，允许进行调整，因此在我们探讨完随机梯度下降后，我们将考虑一个有用且常用的调整方法：动量。最后，我们将通过讨论更先进的自适应梯度下降算法来结束本章，具体包括RMSprop、Adagrad和Adam。
- en: This is a math book, but gradient descent is very much applied math, so we’ll
    learn by experimentation. The equations are straightforward, and the math we saw
    in previous chapters is relevant as background. Therefore, consider this chapter
    an opportunity to apply what we’ve learned so far.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一本数学书，但梯度下降非常贴近应用数学，因此我们将通过实验学习。这些方程是直接的，我们在前几章看到的数学知识作为背景是相关的。因此，可以将本章视为应用我们迄今为止所学内容的机会。
- en: The Basic Idea
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本概念
- en: 'We’ve encountered gradient descent several times already. We know the form
    of the basic gradient descent update equations from [Equation 10.14](ch10.xhtml#ch10equ14):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到过几次梯度下降。我们知道基本的梯度下降更新方程的形式，来自[方程 10.14](ch10.xhtml#ch10equ14)：
- en: '![Image](Images/11equ01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ01.jpg)'
- en: Here, **Δ*W*** and **Δ*b*** are errors based on the partial derivatives of the
    weights and biases, respectively; η (eta) is a step size or learning rate, a value
    we use to adjust how we move.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**Δ*W*** 和 **Δ*b*** 分别是基于权重和偏差的偏导数计算出的误差；η（eta）是步长或学习率，这是我们用来调整移动方式的值。
- en: '[Equation 11.1](ch11.xhtml#ch11equ01) isn’t specific to machine learning. We
    can use the same form to implement gradient descent on arbitrary functions. Let’s
    discuss gradient descent using 1D and 2D examples to lay a foundation for how
    it operates. We’ll use an unmodified form of gradient descent known as *vanilla
    gradient descent*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.1](ch11.xhtml#ch11equ01)并不特定于机器学习。我们可以使用相同的形式在任意函数上实现梯度下降。让我们通过一维和二维示例讨论梯度下降，为其操作打下基础。我们将使用一种未经修改的梯度下降形式，称为*普通梯度下降*。'
- en: Gradient Descent in One Dimension
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一维梯度下降
- en: 'Let’s begin with a scalar function of *x*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个标量函数 *x* 开始：
- en: '![Image](Images/11equ02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ02.jpg)'
- en: '[Equation 11.2](ch11.xhtml#ch11equ02) is a parabola facing upward. Therefore,
    it has a minimum. Let’s find the minimum analytically by setting the derivative
    to zero and solving for *x*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.2](ch11.xhtml#ch11equ02)是一个向上的抛物线。因此，它有一个最小值。让我们通过将导数设为零并解出*x*来解析地找到最小值：'
- en: '![Image](Images/272equ01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/272equ01.jpg)'
- en: The minimum of the parabola is at *x* = 1\. Now, let’s instead use gradient
    descent to find the minimum of [Equation 11.2](ch11.xhtml#ch11equ02). How should
    we begin?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 抛物线的最小值在 *x* = 1。现在，让我们改用梯度下降来找到[方程 11.2](ch11.xhtml#ch11equ02)的最小值。我们应该如何开始？
- en: First, we need to write the proper update equation, the form of [Equation 11.1](ch11.xhtml#ch11equ01)
    that applies in this case. We need the gradient, which for a 1D function is simply
    the derivative, *f*′(*x*) = 12*x* − 12\. With the derivative, gradient descent
    becomes
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要写出适当的更新方程，这是[方程 11.1](ch11.xhtml#ch11equ01)在这种情况下的形式。我们需要梯度，对于一维函数，它就是导数，*f*′(*x*)
    = 12*x* − 12。通过导数，梯度下降变为：
- en: '![Image](Images/11equ03.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ03.jpg)'
- en: Notice that we subtract η (12*x* − 12). This is why the algorithm is called
    gradient *descent*. Recall that the gradient points in the direction of maximum
    change in the function’s value. We’re interested in minimizing the function, not
    maximizing it, so we move in the direction opposite to the gradient toward smaller
    function values; therefore, we subtract.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们减去了η (12*x* − 12)。这就是算法被称为梯度*下降*的原因。回想一下，梯度指向函数值变化最大的方向。我们关心的是最小化函数，而不是最大化它，因此我们沿着与梯度相反的方向前进，朝着更小的函数值移动；因此，我们进行减法。
- en: '[Equation 11.3](ch11.xhtml#ch11equ03) is one gradient descent step. It moves
    from an initial position, *x*, to a new position based on the value of the slope
    at the current position. Again η, the learning rate, governs how far we move.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.3](ch11.xhtml#ch11equ03)是一次梯度下降步骤。它从初始位置 *x* 移动到一个新位置，基于当前点的斜率值。同样，学习率
    *η* 决定了我们移动的距离。'
- en: Now that we have the equation, let’s implement gradient descent. We’ll plot
    [Equation 11.2](ch11.xhtml#ch11equ02), pick a starting position, say *x* = −0.9,
    and iterate [Equation 11.3](ch11.xhtml#ch11equ03), plotting the function value
    at each new position of *x*. If we do this, we should see a series of points on
    the function that move ever closer to the minimum position at *x* = 1\. Let’s
    write some code.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经得到了方程，让我们实现梯度下降法。我们将绘制[方程 11.2](ch11.xhtml#ch11equ02)，选择一个起始位置，例如 *x*
    = −0.9，并迭代[方程 11.3](ch11.xhtml#ch11equ03)，每次在 *x* 的新位置绘制函数值。如果我们这样做，我们应该会看到一系列的点在函数上，逐渐逼近最小值位置
    *x* = 1。现在让我们写些代码。
- en: 'First, we implement [Equation 11.2](ch11.xhtml#ch11equ02) and its derivative:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们实现[方程 11.2](ch11.xhtml#ch11equ02)及其导数：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we plot the function, and then we iterate [Equation 11.3](ch11.xhtml#ch11equ03),
    plotting the new pair, (*x*, *f*(*x*)), each time:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制函数图像，然后迭代[方程 11.3](ch11.xhtml#ch11equ03)，每次绘制新的位置对（*x*，*f*(*x*)）：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s walk through the code. After importing NumPy and Matplotlib, we plot [Equation
    11.2](ch11.xhtml#ch11equ02) ❶. Next, we set our initial *x* position ❷ and take
    15 gradient descent steps ❸. We plot before stepping, so we see the initial *x*
    but do not plot the last step, which is fine in this case.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步讲解代码。导入 NumPy 和 Matplotlib 后，我们绘制[方程 11.2](ch11.xhtml#ch11equ02) ❶。接下来，我们设置初始的
    *x* 位置 ❷，并进行 15 步梯度下降 ❸。我们在每一步之前进行绘图，因此我们看到的是初始的 *x*，但不会绘制最后一步，这在这种情况下是可以的。
- en: The final line ❹ is key. It implements [Equation 11.3](ch11.xhtml#ch11equ03).
    We update the current *x* position by multiplying the derivative’s value at *x*
    by η = 0.03 as the step size. The code above is in the file *gd_1d.py*. If we
    run it, we get [Figure 11-1](ch11.xhtml#ch11fig01).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行 ❹ 是关键。它实现了[方程 11.3](ch11.xhtml#ch11equ03)。我们通过将导数在 *x* 处的值乘以步长 *η* = 0.03
    来更新当前的 *x* 位置。上面的代码位于 *gd_1d.py* 文件中。如果我们运行它，我们将得到[图 11-1](ch11.xhtml#ch11fig01)。
- en: '![image](Images/11fig01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig01.jpg)'
- en: '*Figure 11-1: Gradient descent in one dimension with small steps (η = 0.03)*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：使用较小步长（η = 0.03）的单维梯度下降*'
- en: Our initial position, which we can think of as an initial guess at the location
    of the minimum, is *x* = −0.9\. Clearly, this isn’t the minimum. As we take gradient
    descent steps, we move successively closer to the minimum, as the sequence of
    circles moving toward it shows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始位置可以看作是最小值位置的初步猜测，*x* = −0.9。显然，这并不是最小值。随着我们进行梯度下降，每一步都会让我们逐渐接近最小值，正如一系列逐步朝向最小值的圆圈所示。
- en: 'Notice two things here. First, we do get closer and closer to the minimum.
    After 14 steps, we are, for all intents and purposes, at the minimum: *x* = 0.997648\.
    Second, each gradient descent step leads to smaller and smaller changes in *x*.
    The learning rate is constant at *η* = 0.03, so the source of the smaller updates
    to *x* must be smaller and smaller values of the derivative at each *x* position.
    This makes sense if we think about it. As we approach the minimum position, the
    derivative gets smaller and smaller, until it reaches zero at the minimum, so
    the update using the derivative gets successively smaller as well.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两点需要注意。首先，我们确实越来越接近最小值。经过14步后，从实际角度看，我们已经接近最小值：*x* = 0.997648。其次，每一步梯度下降都会导致
    *x* 的变化越来越小。学习率保持在 *η* = 0.03，因此 *x* 的小更新源自于每个 *x* 位置处的导数值逐渐减小。如果我们思考一下，就能理解。随着我们接近最小值位置，导数会越来越小，直到最小值处导数为零，因此使用导数进行更新时，更新量也会逐渐减小。
- en: We selected the step size for [Figure 11-1](ch11.xhtml#ch11fig01) to move smoothly
    toward the minimum of the parabola. What if we change the step size? Further along
    in *gd_1d.py*, the code repeats the steps above, starting at *x* = 0.75 and setting
    *η* = 0.15 to take steps that are five times larger than those plotted in [Figure
    11-1](ch11.xhtml#ch11fig01). The result is [Figure 11-2](ch11.xhtml#ch11fig02).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了[图 11-1](ch11.xhtml#ch11fig01)中的步长，使其平滑地朝着抛物线的最小值移动。如果我们改变步长会怎么样呢？在 *gd_1d.py*
    文件中，代码重复了上述步骤，从 *x* = 0.75 开始，设置 *η* = 0.15，步长是[图 11-1](ch11.xhtml#ch11fig01)中绘制的步长的五倍。结果是[图
    11-2](ch11.xhtml#ch11fig02)。
- en: '![image](Images/11fig02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig02.jpg)'
- en: '*Figure 11-2: Gradient descent in one dimension with large steps (η = 0.15)*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-2：使用较大步长（η = 0.15）的单维梯度下降*'
- en: In this case, the steps overshoot the minimum. The new *x* positions oscillate,
    bouncing back and forth over the true minimum position. The dashed lines connect
    successive *x* positions. The overall search still approaches the minimum but
    takes longer to reach it, as the large step size makes each update to *x* tend
    to move past the minimum.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，步骤会超越最小值。新的 *x* 位置会振荡，来回越过真正的最小值位置。虚线连接了连续的 *x* 位置。整体搜索仍然朝最小值逼近，但由于步长较大，每次更新
    *x* 时可能会越过最小值，导致需要更长时间才能到达。
- en: Small gradient descent steps move short distances along the function, whereas
    large steps move large distances. If the learning rate is too small, many gradient
    descent steps are necessary. If the learning rate is too large, the search overshoots
    and oscillates around the minimum position. The proper learning rate is not immediately
    obvious, so intuition and experience come into play when selecting it. Additionally,
    these examples fixed *η*. There’s no reason why *η* has to be a constant. In many
    deep learning applications, the learning rate is not constant but evolves as training
    progresses, effectively making *η* a function of the number of gradient descent
    steps taken.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 小的梯度下降步长沿函数走小距离，而大的步长则走大距离。如果学习率过小，可能需要很多梯度下降步骤。如果学习率过大，搜索就会超越并在最小值位置附近振荡。适当的学习率并不容易确定，因此直觉和经验在选择时非常重要。此外，这些例子中假设
    *η* 是常数。事实上，*η* 不必是常数。在许多深度学习应用中，学习率不是恒定的，而是随着训练进展而变化，实际上使得 *η* 成为梯度下降步数的函数。
- en: Gradient Descent in Two Dimensions
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二维梯度下降
- en: Gradient descent in one dimension is straightforward enough. Let’s move to two
    dimensions to increase our intuition about the algorithm. The code referenced
    below is in the file *gd_2d.py*. We’ll first consider the case where the function
    has a single minimum, then look at cases with multiple minima.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一维梯度下降比较简单。接下来，我们转向二维，以增强我们对算法的直觉。下面引用的代码在文件 *gd_2d.py* 中。我们将首先考虑函数有一个最小值的情况，然后再讨论多个最小值的情况。
- en: Gradient Descent with a Single Minimum
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单一最小值的梯度下降
- en: To work in two dimensions, we need a scalar function of a vector, *f*(***x***)
    = *f*(*x*, *y*), where, to make it easier to follow, we separate the vector into
    its components, ***x*** = (*x*, *y*).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在二维空间中工作，我们需要一个向量的标量函数，*f*(***x***) = *f*(*x*, *y*)，为了便于理解，我们将向量分解成其分量，***x***
    = (*x*, *y*)。
- en: The first function we’ll work with is
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的第一个函数是：
- en: '*f*(*x*, *y*) = 6*x*² + 9*y*² − 12*x* − 14*y* + 3'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*, *y*) = 6*x*² + 9*y*² − 12*x* − 14*y* + 3'
- en: 'To implement gradient descent, we need the partial derivatives as well:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现梯度下降，我们还需要偏导数：
- en: '![Image](Images/276equ01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/276equ01.jpg)'
- en: Our update equations become
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的更新方程变为：
- en: '![Image](Images/276equ02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/276equ02.jpg)'
- en: 'In code, we define the function and partial derivatives:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们定义了函数和偏导数：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since the partial derivatives are independent of the other variable, we get
    away with passing only *x* or *y*. We’ll see an example later in this section
    where that’s not the case.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于偏导数与另一个变量无关，我们只需要传递 *x* 或 *y*。稍后在本节中，我们将看到一个例子，情况并非如此。
- en: 'Gradient descent follows the same pattern as before: select an initial position,
    this time a vector, iterate for some number of steps, and plot the path. The function
    is 2D, so we first plot it using contours, as shown next.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降遵循与之前相同的模式：选择一个初始位置，这次是一个向量，进行若干步迭代，并绘制路径。该函数是二维的，因此我们首先使用等高线来绘制它，如下所示。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code requires some explanation. To plot contours, we need a representation
    of the function over a grid of (*x*, *y*) pairs. To generate the grid, we use
    NumPy, specifically `np.meshgrid`. The arguments to `np.meshgrid` are the *x*
    and *y* points, here provided by `np.linspace`, which itself generates a vector
    from −1 to 3 of *N* = 100 evenly spaced values. The `np.meshgrid` function returns
    two 100 × 100 matrices. The first contains the *x* values over the given range,
    and the second contains the *y* values. All possible (*x*, *y*) pairs are represented
    in the return value to form a grid of points covering the region of −1 . . . 3
    in both *x* and *y*. Passing these points to the function then returns `z`, a
    100 × 100 matrix of the function value at each (*x*, *y*) pair.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要一些解释。为了绘制等高线，我们需要在 (*x*, *y*) 对的网格上表示函数。为了生成网格，我们使用 NumPy，特别是 `np.meshgrid`。`np.meshgrid`
    的参数是 *x* 和 *y* 的点，这里由 `np.linspace` 提供，后者生成从 −1 到 3 的 *N* = 100 个均匀分布的值。`np.meshgrid`
    函数返回两个 100 × 100 的矩阵。第一个矩阵包含给定范围内的 *x* 值，第二个矩阵包含 *y* 值。返回值表示所有可能的 (*x*, *y*) 对，形成覆盖
    *x* 和 *y* 范围为 −1 到 3 的点的网格。将这些点传递给函数后，返回 `z`，即一个 100 × 100 的矩阵，表示每个 (*x*, *y*)
    对的函数值。
- en: We could plot the function in 3D, but that’s difficult to see and unnecessary
    in this case. Instead, we’ll use the function values in *x*, *y*, and *z* to generate
    contour plots. Contour plots show 3D information as a series of lines of equal
    *z* value. Think of lines around a hill on a topographic map, where each line
    is at the same altitude. As the hill gets higher, the lines enclose successively
    smaller regions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制三维函数图，但那样不容易观察且在此情况下不必要。相反，我们将使用 *x*、*y* 和 *z* 的函数值来生成等高线图。等高线图以相同 *z*
    值的一系列线条来展示三维信息。可以想象成地形图上环绕山丘的线条，每条线表示相同的海拔高度。随着山丘的升高，线条将围绕越来越小的区域。
- en: Contour plots come in two varieties, as either lines of equal function value
    or shading over ranges of the function. We’ll plot both varieties using a grayscale
    map. That’s the net result of calling Matplotlib’s `plt.contourf` and `plt.contour`
    functions. The remaining `plt.plot` calls show the axes and mark the function
    minimum with a plus sign. The contour plots are such that lighter shades imply
    lower function values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 等高线图有两种类型：一种是相同函数值的线条，另一种是表示函数范围的阴影。我们将使用灰度图同时绘制这两种类型。这是调用 Matplotlib 的 `plt.contourf`
    和 `plt.contour` 函数的最终结果。其余的 `plt.plot` 调用显示了坐标轴，并用加号标记了函数的最小值。等高线图中的较浅阴影表示较低的函数值。
- en: We’re now ready to plot the sequence of gradient descent steps. We’ll plot each
    position in the sequence and connect them with a dashed line to make the path
    clear (see [Listing 11-1](ch11.xhtml#ch11ex01)). In code, that’s
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备绘制梯度下降步骤的序列。我们将绘制序列中的每个位置，并用虚线将它们连接起来，以使路径更加清晰（见 [清单 11-1](ch11.xhtml#ch11ex01)）。在代码中，这就是
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 11-1: Gradient descent in two dimensions*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-1：二维梯度下降*'
- en: We begin at (*x*, *y*) = (−0.5, 2.9) and take 12 gradient descent steps. To
    connect the last position to the new position using a dashed line, we track both
    the current position in *x* and *y* and the previous position, (*x*[old], *y*[old]).
    The gradient descent step updates both *x* and *y* using *η* = 0.02 and calling
    the respective partial derivative functions, `dx` and `dy`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 (*x*, *y*) = (−0.5, 2.9) 开始，进行 12 步梯度下降。为了用虚线连接最后一个位置到新位置，我们跟踪当前的 *x* 和
    *y* 位置以及先前的位置，(*x*[old], *y*[old])。梯度下降步骤更新 *x* 和 *y*，使用 *η* = 0.02，并调用各自的偏导数函数
    `dx` 和 `dy`。
- en: '[Figure 11-3](ch11.xhtml#ch11fig03) shows the gradient descent path that [Listing
    11-1](ch11.xhtml#ch11ex01) follows (circles) along with two other paths starting
    at (1.5, −0.8) (squares) and (2.7, 2.3) (triangles).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-3](ch11.xhtml#ch11fig03) 显示了 [清单 11-1](ch11.xhtml#ch11ex01) 所遵循的梯度下降路径（圆形），以及从
    (1.5, −0.8)（方形）和 (2.7, 2.3)（三角形）出发的另外两条路径。'
- en: '![image](Images/11fig03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig03.jpg)'
- en: '*Figure 11-3: Gradient descent in two dimensions for small steps*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-3：二维梯度下降（小步长）*'
- en: All three gradient descent paths converge toward the minimum of the function.
    This isn’t surprising, as the function has only one minimum. If the function has
    a single minimum, then gradient descent will eventually find it. If the step size
    is too small, many steps might be necessary, but they will ultimately converge
    on the minimum. If the step size is too large, gradient descent may oscillate
    around the minimum but continually step over it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三条梯度下降路径都汇聚到函数的最小值。这并不令人惊讶，因为该函数只有一个最小值。如果函数只有一个最小值，那么梯度下降最终会找到它。如果步长太小，可能需要很多步骤，但它们最终会收敛到最小值。如果步长太大，梯度下降可能会在最小值附近振荡，但会不断越过它。
- en: 'Let’s change our function a bit to stretch it in the *x* direction relative
    to the *y* direction:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们稍微修改一下函数，将其在*x*方向上相对于*y*方向进行拉伸：
- en: '*f*(*x*, *y*) = 6*x*² + 40*y*² − 12*x* − 30*y* + 3'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*, *y*) = 6*x*² + 40*y*² − 12*x* − 30*y* + 3'
- en: This function has partials ∂*f*/∂*x* = 12*x* − 12 and ∂*f*/∂*y* = 80*y* − 30.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的偏导数为∂*f*/∂*x* = 12*x* − 12 和 ∂*f*/∂*y* = 80*y* − 30。
- en: Additionally, let’s pick two starting locations, (−0.5, 2.3) and (2.3, 2.3),
    and generate a sequence of gradient descent steps with *η* = 0.02 and *η* = 0.01,
    respectively. [Figure 11-4](ch11.xhtml#ch11fig04) shows the resulting paths.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们选择两个起始位置（−0.5, 2.3）和（2.3, 2.3），并分别使用*η* = 0.02和*η* = 0.01生成一系列梯度下降步骤。[图11-4](ch11.xhtml#ch11fig04)显示了结果路径。
- en: '![image](Images/11fig04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig04.jpg)'
- en: '*Figure 11-4: Gradient descent in 2D with larger steps and a slightly different
    function*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-4：具有较大步长和稍有不同函数的二维梯度下降*'
- en: Consider the *η* = 0.02 (circle) path first. The new function is like a canyon,
    narrow in *y* but long in *x*. The larger step size oscillates up and down in
    *y* as it moves toward the minimum in *x*. Bouncing off the canyon walls aside,
    we still find the minimum.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑*η* = 0.02（圆形）路径。新函数像一个峡谷，*y*方向很窄，但*x*方向很长。较大的步长使得在*y*方向上上下振荡，同时朝着*x*方向的最小值移动。尽管在峡谷壁上反弹，我们仍然能找到最小值。
- en: Now, take a look at the *η* = 0.01 (square) path. It quickly falls into the
    canyon and then moves slowly over the flat region along the canyon floor toward
    the minimum position. The component of the vector gradient (the *x* and *y* partial
    derivative values) along the *x* direction is small in the canyon, so motion along
    *x* is proportionately slow. There is no motion in the *y* direction—the canyon
    is steep, and the relatively small learning rate has already located the canyon
    floor, where the gradient is primarily along *x*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看*η* = 0.01（方形）路径。它很快掉入峡谷，然后沿峡谷底部缓慢移动，朝着最小值位置前进。在峡谷中，沿*x*方向的梯度分量（*x*和*y*的偏导数值）很小，因此沿*x*方向的运动相对较慢。在*y*方向上没有运动——峡谷很陡峭，相对较小的学习率已经定位到了峡谷底部，那里梯度主要沿*x*方向。
- en: 'What’s the lesson here? Again, the step size matters. However, the shape of
    the function matters even more. The minimum of the function lies at the bottom
    of a long, narrow canyon. The gradient along the canyon is tiny; the canyon floor
    is flat in the *x* direction, so motion is slow because it depends on the gradient
    value. We frequently encounter this effect in deep learning: if the gradient is
    small, learning is slow. This is why the rectified linear unit has come to dominate
    deep learning; the gradient is a constant one for positive inputs. For a sigmoid
    or hyperbolic tangent, the gradient approaches zero when inputs are far from zero.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的教训是什么？同样，步长很重要。然而，函数的形状更为重要。该函数的最小值位于一个狭长峡谷的底部。峡谷中的梯度非常小；峡谷底部在*x*方向上是平坦的，因此运动较慢，因为它依赖于梯度值。我们在深度学习中经常遇到这种情况：如果梯度很小，学习就很慢。这也是为什么修正线性单元（ReLU）在深度学习中占主导地位的原因；对于正输入，梯度是恒定的。对于Sigmoid或双曲正切函数，当输入远离零时，梯度接近零。
- en: Gradient Descent with Multiple Minima
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 具有多个最小值的梯度下降
- en: 'The functions we’ve examined so far have a single minimum value. What if that
    isn’t the case? Let’s see what happens to gradient descent when the function has
    more than one minimum. Consider this function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所研究的函数都有一个单一的最小值。如果情况不是这样呢？让我们来看一下当函数有多个最小值时，梯度下降会发生什么。考虑这个函数：
- en: '![Image](Images/11equ04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11equ04.jpg)'
- en: '[Equation 11.4](ch11.xhtml#ch11equ04) is the sum of two inverted Gaussians,
    one with a minimum value of −2 at (−1, 1) and the other with a minimum of −1 at
    (1, −1). If gradient descent is to find the global minimum, it should find it
    at (−1, 1). The code for this example is in *gd_multiple.py*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.4](ch11.xhtml#ch11equ04)是两个反向高斯分布的和，一个在 (−1, 1) 处的最小值为 −2，另一个在 (1, −1)
    处的最小值为 −1。如果梯度下降法要找到全局最小值，它应该会在 (−1, 1) 处找到。此示例的代码在 *gd_multiple.py* 中。'
- en: The partial derivatives are
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数为：
- en: '![Image](Images/280equ01.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/280equ01.jpg)'
- en: 'which translates into the following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这转换为以下代码：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice, in this case, the partial derivatives do depend on both *x* and *y*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，偏导数确实依赖于 *x* 和 *y*。
- en: The code for the gradient descent portion of *gd_multiple.py* is as before.
    Let’s run the cases in [Table 11-1](ch11.xhtml#ch11tab01).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*gd_multiple.py* 中的梯度下降部分代码与之前相同。让我们运行[表 11-1](ch11.xhtml#ch11tab01)中的案例。'
- en: '**Table 11-1:** Different Starting Positions and Number of Gradient Descent
    Steps Taken'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 11-1：** 不同起始位置和梯度下降步数'
- en: '| **Starting point** | **Steps** | **Symbol** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **起始点** | **步数** | **符号** |'
- en: '| --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| (–1.5,1.2) | 9 | circle |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| (–1.5,1.2) | 9 | 圆形 |'
- en: '| (1.5,–1.8) | 9 | square |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (1.5,–1.8) | 9 | 正方形 |'
- en: '| (0,0) | 20 | plus |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| (0,0) | 20 | 加号 |'
- en: '| (0.7,–0.2) | 20 | triangle |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| (0.7,–0.2) | 20 | 三角形 |'
- en: '| (1.5,1.5) | 30 | asterisk |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (1.5,1.5) | 30 | 星号 |'
- en: The Symbol column refers to the plot symbol used in [Figure 11-5](ch11.xhtml#ch11fig05).
    For all cases, *η* = 0.4.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: “符号”列指的是在[图 11-5](ch11.xhtml#ch11fig05)中使用的图形符号。对于所有情况，*η* = 0.4。
- en: '![image](Images/11fig05.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig05.jpg)'
- en: '*Figure 11-5: Gradient descent for a function with two minima*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-5：具有两个极小值的函数的梯度下降*'
- en: The gradient descent paths indicated in [Figure 11-5](ch11.xhtml#ch11fig05)
    make sense. In three of the five cases, the path does move into the well that
    the deeper of the two minima defines—a successful search. However, for the triangle
    and the square, gradient descent fell into the wrong minimum. Clearly, how successful
    gradient descent is, in this case, depends on where we start the process. Once
    the path moves downhill to a deeper position, gradient descent has no way to escape
    upward to find a potentially better minimum.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-5](ch11.xhtml#ch11fig05)中所示的梯度下降路径是合理的。在五个案例中，三条路径确实进入了由两个极小值中较深的一个定义的“深谷”——这是一次成功的搜索。然而，对于三角形和正方形，梯度下降却陷入了错误的最小值。显然，在这种情况下，梯度下降的成功与否取决于我们从哪里开始。一旦路径向下移动到一个更深的位置，梯度下降就没有办法反向上升以找到一个可能更好的最小值。'
- en: Current thinking is that the loss landscape for a deep learning model contains
    many minima. It’s also currently believed that in most cases, the minima are pretty
    similar, which partially explains the success of deep learning models—to train
    them, you don’t need to find the one, magic, global minimum of the loss, only
    one of the (probably) many that are (probably) about as good as any of the others.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的看法是，深度学习模型的损失景观包含许多局部最小值。现在也普遍认为，在大多数情况下，这些最小值是相当相似的，这部分解释了深度学习模型的成功——训练这些模型时，你不需要找到损失的唯一“魔法”全局最小值，只需要找到其中一个（可能）与其他最小值差不多好的最小值。
- en: 'I selected the initial positions used for the examples in this section intentionally
    based on knowledge of the function’s form. For a deep learning model, picking
    the starting point means random initialization of the weights and biases. In general,
    we don’t know the form of the loss function, so initialization is a shot in the
    dark. Most of the time, or at least much of the time, gradient descent produces
    a well-performing model. Sometimes, however, it doesn’t; it fails miserably. In
    those cases, it’s possible the initial position was like the square in [Figure
    11-5](ch11.xhtml#ch11fig05): it fell into an inferior local minimum because it
    started in a bad place.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了本节示例中使用的初始位置，基于对函数形式的了解有意识地进行了选择。对于深度学习模型，选择起始点意味着对权重和偏置的随机初始化。通常，我们不知道损失函数的具体形式，因此初始化是一种盲目的尝试。大多数时候，或者至少很多时候，梯度下降会产生一个表现良好的模型。然而，有时它并不会如此成功；它可能会彻底失败。在这种情况下，可能是因为初始位置像[图
    11-5](ch11.xhtml#ch11fig05)中的正方形那样：它陷入了一个较差的局部最小值，因为它从一个不好的位置开始。
- en: Now that we have a handle on gradient descent, what it is, and how it works,
    let’s investigate how we can apply it in deep learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了梯度下降法的基本概念，它是什么以及如何运作，让我们来探讨如何将其应用于深度学习中。
- en: Stochastic Gradient Descent
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Training a neural network is primarily the act of minimizing the loss function
    while preserving generalizability via various forms of regularization. In [Chapter
    10](ch10.xhtml#ch10), we wrote the loss as *L*(**θ**; ***x***, *y*) for a vector
    of the weights and biases, **θ** (theta), and training instances (***x***, *y*),
    where ***x*** is the input vectors and y is the known labels. Note how here, ***x***
    is a stand-in for *all* training data, not just a single sample.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的主要目的是最小化损失函数，同时通过各种形式的正则化保持泛化能力。在[第10章](ch10.xhtml#ch10)中，我们将损失表示为*L*(**θ**;
    ***x***, *y*)，其中**θ**（theta）是权重和偏置的向量，训练实例（***x***, *y*）是输入向量***x***和已知标签y。请注意，这里，***x***代表的是*所有*训练数据，而不仅仅是单个样本。
- en: 'Gradient descent needs ∂*L*/∂**θ**, which we get via backpropagation. The expression
    ∂*L*/∂**θ** is a concise way of referring to all the individual weight and bias
    error terms backpropagation gives us. We get ∂*L*/∂**θ** by averaging the error
    over the training data. This begs the question: Do we average over all of the
    training data or only some of the training data?'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降需要∂*L*/∂**θ**，我们通过反向传播得到这个值。∂*L*/∂**θ**是指所有单独的权重和偏置误差项的简洁表示，它们是反向传播给出的。我们通过对训练数据上的误差进行平均来得到∂*L*/∂**θ**。这就引出了一个问题：我们是对所有训练数据进行平均，还是仅对部分训练数据进行平均？
- en: Passing all the training data through the model before taking a gradient descent
    step is called batch training. At first blush, batch training seems sensible.
    After all, if our training set is a good sample from the parent distribution that
    generates the sort of data our model intends to work with, then why not use all
    of that sample to do gradient descent?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行梯度下降步骤之前，将所有训练数据传递给模型的过程称为批量训练。乍一看，批量训练似乎是合理的。毕竟，如果我们的训练集是来自于生成我们模型所需数据的父分布的良好样本，那么为什么不使用所有这些样本来进行梯度下降呢？
- en: When datasets were small, batch training was the natural thing to do. However,
    models got bigger, as did datasets, and suddenly the computational burden of passing
    *all* the training data through the model for each gradient descent step became
    too much. This chapter’s examples already hint that many gradient descent steps
    might be necessary to find a good minimum position, especially for tiny learning
    rates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集较小时，批量训练是自然的选择。然而，随着模型和数据集的增大，每次梯度下降步骤中将*所有*训练数据传递给模型的计算负担变得过于沉重。本章的示例已经暗示，为了找到一个好的最小值位置，可能需要进行许多梯度下降步骤，尤其是在使用非常小的学习率时。
- en: Therefore, practitioners began to use subsets of the training data for each
    gradient descent step—the *minibatch*. Minibatch training was probably initially
    viewed as a compromise, as the gradient calculated over the minibatch was “wrong”
    because it wasn’t based on the performance of the full training set.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实践者开始在每次梯度下降步骤中使用训练数据的子集——即*小批量*。小批量训练最初可能被视为一种折衷，因为在小批量上计算的梯度是“错误的”，因为它并非基于完整训练集的表现。
- en: Of course, the difference between *batch* and *minibatch* is just an agreed-upon
    fiction. In truth, it’s a continuum from a minibatch of one to a minibatch of
    all available samples. With that in mind, all the gradients computed during network
    training are “wrong,” or at least incomplete, as they are based on incomplete
    knowledge of the data generator and the full set of data it could generate.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，*批量*和*小批量*之间的区别只是一个约定俗成的虚构。实际上，它是从一个样本的小批量到包含所有可用样本的小批量的一个连续体。考虑到这一点，所有在网络训练过程中计算的梯度都是“错误的”，或者至少是不完整的，因为它们是基于对数据生成器及其能够生成的完整数据集的不完全了解。
- en: Rather than a concession, then, minibatch training is reasonable. The gradient
    over a small minibatch is noisy compared to that computed over a larger minibatch,
    in the sense that the small minibatch gradient is a coarser estimate of the “real”
    gradient. When things are noisy or random, the word *stochastic* tends to show
    up, as it does here. Gradient descent with minibatches is *stochastic gradient
    descent (SGD)*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，小批量训练并不是一种妥协，而是合理的选择。与在较大小批量上计算的梯度相比，小批量的梯度噪声较大，从某种意义上说，小批量的梯度是对“真实”梯度的粗略估计。当情况是噪声较大或随机时，*随机*这个词通常会出现，就像这里一样。使用小批量的梯度下降就是*随机梯度下降（SGD）*。
- en: In practice, gradient descent using smaller minibatches often leads to models
    that perform better than those trained with larger minibatches. The rationale
    generally given is that the noisy gradient of the smaller minibatch helps gradient
    descent avoid falling into poor local minima of the loss landscape. We saw this
    effect in [Figure 11-5](ch11.xhtml#ch11fig05), where the triangle and the square
    both fell into the wrong minimum.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用较小的小批量进行梯度下降往往会导致表现优于使用较大小批量训练的模型。一般给出的理论是，较小小批量的噪声梯度帮助梯度下降避免陷入损失景观中的差局部最小值。我们在[图
    11-5](ch11.xhtml#ch11fig05)中看到了这个效果，那里三角形和方形都陷入了错误的最小值。
- en: Again, we find ourselves strangely fortunate. Before, we were fortunate because
    first-order gradient descent succeeded in training models that shouldn’t train
    due to nonlinear loss landscapes, and now we get a boost by intentionally using
    small amounts of data to estimate gradients, thereby skipping a computational
    burden likely to make the entire enterprise of deep learning too cumbersome to
    implement in many cases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 再次地，我们发现自己非常幸运。以前，我们的幸运在于一阶梯度下降成功地训练了那些由于非线性损失景观而无法训练的模型，而现在，通过故意使用少量数据来估计梯度，我们获得了一个提升，从而避免了一个计算负担，这个负担可能使得深度学习的整个工作变得过于繁琐，在很多情况下难以实施。
- en: How large should our minibatch be? Minibatch size is a *hyperparameter*, something
    we need to select to train the model, but is not part of the model itself. The
    proper minibatch size is dataset-dependent. For example, in the extreme, we could
    take a gradient descent step for each sample, which sometimes works well. This
    case is often referred to as *online learning*. However, especially if we use
    layers like batch normalization, we need a minibatch large enough to make the
    calculated means and standard deviations reasonable estimates. Again, as with
    most everything else in deep learning at present, it’s empirical, and you need
    to both have intuition and try many variations to optimize the training of the
    model. This is why people work on *AutoML* systems, systems that seek to do all
    the hyperparameter tuning for you.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小批量应该多大？小批量大小是一个*超参数*，是我们需要选择的参数来训练模型，但它不是模型本身的一部分。适当的小批量大小取决于数据集。例如，在极端情况下，我们可以为每个样本进行一次梯度下降步骤，这有时效果很好。这个情况通常被称为*在线学习*。然而，特别是如果我们使用像批量归一化这样的层时，我们需要一个足够大的小批量，以便使计算出的均值和标准差成为合理的估计。同样，就像目前深度学习中的大多数事情一样，这些都是经验性的，你既需要有直觉，又需要尝试许多变种来优化模型的训练。这也是为什么人们研究*AutoML*系统，这些系统旨在为你进行所有的超参数调优。
- en: 'Another good question: What should be in the minibatch? That is, what small
    subset of the full dataset should we use? Typically, the order of the samples
    in the training set is randomized, and minibatches are pulled from the set as
    successive chunks of samples until all samples have been used. Using all the samples
    in the dataset defines one epoch, so the number of samples in the training set
    divided by the minibatch size determines the number of minibatches per epoch.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好问题是：小批量中应该包含什么？也就是说，我们应该使用完整数据集中的哪个小子集？通常，训练集中的样本顺序是随机化的，随后从数据集中抽取小批量，直到所有样本都被使用。使用数据集中的所有样本定义一个轮次，因此训练集中样本的数量除以小批量大小，决定了每个轮次的小批量数量。
- en: Alternatively, as we did for *NN.py*, a minibatch might genuinely be a random
    sampling from the available data. It’s possible that a particular training sample
    is never used while another is used many times, but on the whole, the majority
    of the dataset is used during training.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，就像我们在*NN.py*中做的那样，一个小批量可能是真正从可用数据中随机抽取的。某些训练样本可能永远不会被使用，而其他样本可能被多次使用，但总体而言，大部分数据集在训练过程中都会被使用。
- en: Some toolkits train for a specified number of minibatches. Both *NN.py* and
    Caffe operate this way. Other toolkits, like Keras and `sklearn`, use epochs.
    Gradient descent steps happen after a minibatch is processed. Larger minibatches
    result in fewer gradient descent steps per epoch. To compensate, practitioners
    using toolkits that use epochs need to ensure that the number of gradient descent
    steps increases as minibatch size increases—larger minibatches require more epochs
    to train well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工具包会为指定数量的小批量进行训练。*NN.py*和Caffe就是采用这种方式。其他工具包，如Keras和`sklearn`，则使用轮次（epochs）。梯度下降步骤在处理完一个小批量后进行。较大的小批量会导致每轮次的梯度下降步骤较少。为了弥补这一点，使用轮次的工具包的实践者需要确保随着小批量大小的增加，梯度下降步骤的数量也增加——较大的小批量需要更多的轮次才能训练得好。
- en: 'To recap, deep learning does not use full batch training for at least the following
    reasons:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，深度学习至少因为以下原因不使用全批量训练：
- en: The computational burden is too great to pass the entire training set through
    the model for each gradient descent step.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算负担太重，无法在每个梯度下降步骤中将整个训练集通过模型。
- en: The gradient computed from the average loss over a minibatch is a noisy but
    reasonable estimate of the true, and ultimately unknowable, gradient.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从小批量的平均损失计算出的梯度是一个噪声较大但合理的估计，接近真实的梯度，尽管这个真实的梯度是不可知的。
- en: The noisy gradient points in a slightly wrong direction in the loss landscape,
    thereby possibly avoiding bad minima.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 含噪声的梯度在损失曲线中指向一个略有偏差的方向，从而可能避开不好的最小值。
- en: Minibatch training simply works better in practice for many datasets.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小批量训练在实践中对于许多数据集更有效。
- en: 'Reason #4 should not be underestimated: many practices in deep learning are
    employed initially because they simply work better. Only later are they justified
    by theory, if at all.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第4个原因不容小觑：深度学习中的许多实践最初采用，是因为它们工作得更好。只有后来，才会用理论来证明其合理性，如果有的话。
- en: As we already implemented SGD in [Chapter 10](ch10.xhtml#ch10) (see *NN.py*),
    we won’t reimplement it here, but in the next section, we’ll add momentum to see
    how that affects neural network training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在[第10章](ch10.xhtml#ch10)实现了SGD（参见*NN.py*），所以这里不再重新实现，但在接下来的部分，我们将加入动量，看看它如何影响神经网络训练。
- en: Momentum
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: Vanilla gradient descent relies solely on the value of the partial derivative
    multiplied by the learning rate. If the loss landscape has many local minima,
    especially if they’re steep, vanilla gradient descent might fall into one of the
    minima and be unable to recover. To compensate, we can modify vanilla gradient
    descent to include a *momentum* term, a term that uses a fraction of the previous
    step’s update. Including this momentum in gradient descent adds inertia to the
    algorithm’s motion through the loss landscape, thereby potentially allowing gradient
    descent to move past bad local minima.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 原始梯度下降仅依赖于偏导数的值与学习率的乘积。如果损失曲线有许多局部最小值，特别是这些最小值很陡峭，原始梯度下降可能会陷入其中的一个最小值，无法恢复。为了解决这个问题，我们可以修改原始梯度下降，加入*动量*项，这个项使用前一步更新的一个比例。将这个动量项包含在梯度下降中，为算法在损失曲线中的运动增加了惯性，从而有可能让梯度下降跳过不好的局部最小值。
- en: Let’s define and then experiment with momentum using 1D and 2D examples, as
    we did earlier. After that, we’ll update our *NN.py* toolkit to use momentum to
    see how that affects models trained on more complex datasets.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义并尝试使用动量，先用一维和二维示例，如我们之前所做的那样。之后，我们将更新我们的*NN.py*工具包，使用动量来观察它如何影响在更复杂数据集上训练的模型。
- en: What Is Momentum?
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是动量？
- en: In physics, the momentum of a moving object is defined as the mass times the
    velocity, ***p*** = *m**v***. However, velocity itself is the first derivative
    of the position, ***v*** = *d**x***/*dt*, so momentum is mass times how fast the
    position of the object is changing in time.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理学中，运动物体的动量定义为质量乘以速度，***p*** = *m**v***。然而，速度本身是位置的导数，***v*** = *d**x***/*dt*，所以动量是质量乘以物体位置随时间变化的速率。
- en: For gradient descent, *position* is the function value, and *time* is the argument
    to the function. The *velocity*, then, is how fast the function value changes
    with a change in the argument, ∂*f*/∂***x***. Therefore, we can think of *momentum*
    as a scaled velocity term. In physics, the scale factor is the mass. For gradient
    descent, the scale factor is *μ* (mu), a number between zero and one.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降，*位置*是函数值，*时间*是函数的自变量。然后，*速度*是函数值随着自变量变化的变化速率，∂*f*/∂***x***。因此，我们可以将*动量*看作是一个缩放的速度项。在物理学中，缩放因子是质量。对于梯度下降，缩放因子是*μ*（mu），一个介于零和一之间的数字。
- en: If we call the gradient including the momentum term ***v***, then the gradient
    descent update equation that was
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将包含动量项的梯度称为***v***，那么之前的梯度下降更新方程将是
- en: '![Image](Images/285equ01.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/285equ01.jpg)'
- en: becomes
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 变成
- en: '![Image](Images/11equ05.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ05.jpg)'
- en: for some initial velocity, ***v*** = 0, and the “mass,” *μ*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些初始速度，***v*** = 0，以及“质量”*μ*。
- en: Let’s walk through [Equation 11.5](ch11.xhtml#ch11equ05) to understand what
    it means. The two-step update, first ***v*** and then ***x***, makes it easy to
    iterate, as we know we must do for gradient descent. If we substitute ***v***
    into the update equation for ***x***, we get
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 [公式 11.5](ch11.xhtml#ch11equ05) 来理解它的含义。这个两步更新，先更新***v***再更新***x***，使得迭代变得容易，因为我们知道这是梯度下降中必须做的。如果我们将***v***代入更新方程中的***x***，我们得到
- en: '![Image](Images/285equ02.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/285equ02.jpg)'
- en: This makes it clear that the update includes the gradient step we had previously
    but adds back in a fraction of the previous step size. It’s a fraction because
    we restrict *μ* to [0, 1]. If *μ* = 0, we’re back to vanilla gradient descent.
    It might be helpful to think of *μ* as a scale factor, the fraction of the previous
    velocity to keep along with the current gradient value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得更新过程变得清晰，它包含了之前的梯度步长，但会加入上一轮步长的一部分。之所以是部分，是因为我们将 *μ* 限制在 [0, 1] 范围内。如果 *μ*
    = 0，我们就回到常规的梯度下降方法。可以将 *μ* 理解为一个比例因子，表示保留多少上一轮速度并与当前梯度值一起更新。
- en: The momentum term tends to keep motion through the loss landscape heading in
    its previous direction. The value of *μ* determines the strength of that tendency.
    Deep learning practitioners typically use *μ* = 0.9, so most of the previous update
    direction is maintained in the next step, with the current gradient providing
    a small adjustment. Again, like many things in deep learning, this number was
    chosen empirically.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 动量项倾向于使运动在损失景观中沿着其先前的方向继续。*μ* 的值决定了这一倾向的强度。深度学习从业者通常使用 *μ* = 0.9，这样大部分先前的更新方向在下一步中得以保留，当前的梯度提供了一个小的调整。同样，像深度学习中的许多事物一样，这个数字是通过经验选择的。
- en: Newton’s first law of motion states that an object in motion remains in motion
    unless acted upon by an outside force. Resistance to an external force is related
    to the object’s mass and is called *inertia*. So, we might also view the *μ**v***
    term as inertia, which might have been a better name for it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿的第一运动定律表明，除非外力作用，否则物体会保持运动状态。对外力的抗拒与物体的质量有关，这种抗拒被称为 *惯性*。因此，我们也可以将 *μ**v***
    项视为惯性，这或许是一个更合适的名称。
- en: Regardless of the name, now that we have it, let’s see what it does to the 1D
    and 2D examples we worked through earlier using vanilla gradient descent.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不管名字如何，现在我们已经有了它，让我们看看它对我们之前使用常规梯度下降处理的 1D 和 2D 示例有什么影响。
- en: Momentum in 1D
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1D 中的动量
- en: Let’s modify the 1D and 2D examples above to use a momentum term. We’ll start
    with the 1D case. The updated code is in the file *gd_1d_momentum.py* and appears
    here as [Listing 11-2](ch11.xhtml#ch11ex02).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改上面的 1D 和 2D 示例，使用动量项。我们从 1D 情况开始。更新后的代码位于文件 *gd_1d_momentum.py* 中，并在此作为[清单
    11-2](ch11.xhtml#ch11ex02)展示。
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 11-2: Gradient descent in one dimension with momentum*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-2：具有动量的一维梯度下降*'
- en: '[Listing 11-2](ch11.xhtml#ch11ex02) is a bit dense, so let’s parse it out.
    First, we are plotting, so we include Matplotlib. Next, we define the function,
    `f(x)`, and its derivative, `d(x)`, as we did before. To configure plotting, we
    define a collection of markers ❶ and then plot the function itself. As before,
    we begin at *x* = 0.75 ❷ and set the step size (`eta`), momentum (`mu`), and initial
    velocity (`v`).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11-2](ch11.xhtml#ch11ex02)有点密集，所以让我们逐步解析。首先，我们进行绘图，因此需要引入 Matplotlib。接下来，我们定义函数
    `f(x)` 及其导数 `d(x)`，与之前一样。为了配置绘图，我们定义了一组标记 ❶，然后绘制函数本身。如同之前一样，我们从 *x* = 0.75 ❷ 开始，并设置步长（`eta`）、动量（`mu`）和初始速度（`v`）。'
- en: We’re now ready to iterate. We’ll use two gradient descent loops. The first
    plots each step ❸ and the second continues gradient descent to demonstrate that
    we do eventually locate the minimum, which we mark with an `'X'` ❹. For each step,
    we calculate the new velocity by mimicking [Equation 11.5](ch11.xhtml#ch11equ05),
    and then we add the velocity to the current position to get the next position.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始迭代。我们将使用两个梯度下降循环。第一个绘制每一步 ❸，第二个继续梯度下降以展示我们最终确实能找到最小值，并用 `'X'` ❹ 标记它。对于每一步，我们通过模仿[公式
    11.5](ch11.xhtml#ch11equ05)计算新的速度，然后将速度加到当前的位置，得到下一个位置。
- en: '[Figure 11-6](ch11.xhtml#ch11fig06) shows the output of *gd_1d_momentum.py*.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-6](ch11.xhtml#ch11fig06) 显示了 *gd_1d_momentum.py* 的输出。'
- en: '![image](Images/11fig06.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig06.jpg)'
- en: '*Figure 11-6: Gradient descent in one dimension with momentum*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-6：具有动量的一维梯度下降*'
- en: Note that we intentionally used a large step size (*η*), so we overshoot the
    minimum. The momentum term tends to overshoot minima as well. If you follow the
    dashed line and the sequence of plot markers, you can walk through the first 10
    gradient descent steps. There is oscillation, but the oscillation is damped and
    eventually settles at the minimum, as marked. Adding momentum enhanced the overshoot
    due to the large step size. However, even with the momentum term, which isn’t
    advantageous here, because there’s only one minimum, with enough gradient descent
    steps, we find the minimum in the end.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们故意使用了较大的步长(*η*)，因此我们超越了最小值。动量项也倾向于超越最小值。如果你沿着虚线和图表标记的序列走，你可以通过前10步的梯度下降过程。虽然有振荡，但振荡是衰减的，并最终在标记的最小值处稳定下来。添加动量增强了由于较大步长导致的超越。然而，即使有了动量项，在这里并没有什么优势，因为这里只有一个最小值，经过足够的梯度下降步骤，我们最终还是找到了最小值。
- en: Momentum in 2D
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二维中的动量
- en: 'Now, let’s update our 2D example. We’re working with the code in *gd _momentum.py*.
    Recall that, for the 2D example, the function is the sum of two inverted Gaussians.
    Including momentum updates the code slightly, as shown in [Listing 11-3](ch11.xhtml#ch11ex03):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新我们的二维示例。我们正在使用*gd_momentum.py*中的代码。回想一下，在二维示例中，函数是两个反向高斯函数的和。包括动量后，代码会稍作更新，如[示例
    11-3](ch11.xhtml#ch11ex03)所示：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 11-3: Gradient descent in two dimensions with momentum*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11-3：带动量的二维梯度下降*'
- en: Here, we have the new function, `gd`, which performs gradient descent with momentum
    beginning at `(x,y)`, using the given *μ* and *η*, and runs for `steps` iterations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个新的函数`gd`，它执行带动量的梯度下降，从`(x, y)`开始，使用给定的*μ*和*η*，并运行`steps`次迭代。
- en: The initial velocity is set ❶, and the loop begins. The velocity update of [Equation
    11.5](ch11.xhtml#ch11equ05) becomes `vx = mu*vx - eta * dx(x,y)` ❷, and the position
    update becomes `x = x + vx` ❸. As before, a line is plotted between the last position
    and the current one to track motion through the function landscape.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 初始速度被设置为❶，然后循环开始。[公式 11.5](ch11.xhtml#ch11equ05)中的速度更新变为`vx = mu*vx - eta *
    dx(x,y)`❷，位置更新变为`x = x + vx`❸。和之前一样，绘制一条线连接最后的位置和当前的位置，以追踪通过函数空间的运动轨迹。
- en: The code in *gd_momentum.py* traces the motion starting at two of the points
    we used before, (0.7, −0.2) and (1.5, 1.5) ❹. Note the number of steps and learning
    rate vary by point to keep the plot from becoming too cluttered. The output of
    *gd_momentum.py* is [Figure 11-7](ch11.xhtml#ch11fig07).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*gd_momentum.py*中的代码追踪了从之前使用的两个点开始的运动，分别是(0.7, −0.2)和(1.5, 1.5)❹。请注意，不同的点使用的步数和学习率不同，以避免图形过于杂乱。*gd_momentum.py*的输出为[图
    11-7](ch11.xhtml#ch11fig07)。'
- en: '![image](Images/11fig07.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig07.jpg)'
- en: '*Figure 11-7: Gradient descent in two dimensions with momentum*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-7：带动量的二维梯度下降*'
- en: Compare the paths in [Figure 11-7](ch11.xhtml#ch11fig07) with those in [Figure
    11-5](ch11.xhtml#ch11fig05). Adding momentum has pushed the paths, so they tend
    to keep moving in the same direction. Notice how the path beginning at (1.5, 1.5)
    spirals toward the minimum, while the other path curves toward the shallower minimum,
    passes it, and backtracks toward it again.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 比较[图 11-7](ch11.xhtml#ch11fig07)中的路径与[图 11-5](ch11.xhtml#ch11fig05)中的路径。添加动量后，路径发生了偏移，因此它们趋向于朝同一方向持续运动。注意，从(1.5,
    1.5)开始的路径朝着最小值螺旋移动，而另一条路径则朝着较浅的最小值弯曲，经过它后又返回到它附近。
- en: The momentum term alters the dynamics of motion through the function space.
    However, it’s not immediately evident that momentum adds anything helpful. After
    all, the (1.5, 1.5) starting position using vanilla gradient descent moved directly
    to the minimum position without spiraling.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 动量项改变了在函数空间中运动的动态。然而，并不立刻能看出动量是否有帮助。毕竟，使用普通梯度下降的(1.5, 1.5)起始位置直接移动到了最小值位置，没有出现螺旋形运动。
- en: Let’s add momentum to our *NN.py* toolkit and see if it buys us anything when
    training real neural networks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将动量添加到我们的*NN.py*工具集中，看看在训练真实神经网络时它是否能带来好处。
- en: Training Models with Momentum
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用动量训练模型
- en: 'To support momentum in *NN.py*, we need to tweak the `FullyConnectedLayer`
    method in two places. First, as shown in [Listing 11-4](ch11.xhtml#ch11ex04),
    we modify the constructor to allow a `momentum` keyword:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在*NN.py*中支持动量，我们需要在`FullyConnectedLayer`方法的两个地方进行调整。首先，如[示例 11-4](ch11.xhtml#ch11ex04)所示，我们修改构造函数以允许`momentum`关键字：
- en: '[PRE8]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 11-4: Adding the momentum keyword*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11-4：添加动量关键字*'
- en: Here, we add a `momentum` keyword, with a default of zero, into the argument
    list. Then, we define initial velocities for the weights (`vw`) and biases (`vb`)
    ❶. These are matrices of the proper shape initialized to zero. We also keep the
    momentum argument for later use.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们向参数列表中添加了一个 `momentum` 关键字，默认值为零。然后，我们定义了权重（`vw`）和偏置（`vb`）的初始速度 ❶。这些是正确形状的矩阵，初始化为零。我们还保留了动量参数以供后续使用。
- en: 'The second modification is to the `step` method, as [Listing 11-5](ch11.xhtml#ch11ex05)
    shows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个修改是针对 `step` 方法的，正如[列表 11-5](ch11.xhtml#ch11ex05)所示：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 11-5: Updating the step to include momentum*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-5：更新 step 以包含动量*'
- en: 'We implement [Equation 11.5](ch11.xhtml#ch11equ05), first for the weights ❶,
    then for the biases in the line after. We multiply the momentum (*μ*) by the previous
    velocity, then subtract the average error over the minibatch, multiplied by the
    learning rate. We then move the weights and biases by adding the velocity ❷. That’s
    all we need to do to incorporate momentum. Then, to use it, we add the momentum
    keyword to each fully connected layer when building the network, as shown in [Listing
    11-6](ch11.xhtml#ch11ex06):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了[公式 11.5](ch11.xhtml#ch11equ05)，首先是针对权重 ❶，然后是下一行的偏置。我们将动量（*μ*）乘以前一个速度，然后减去每个小批量的平均误差，再乘以学习率。接着，我们通过加上速度
    ❷ 来更新权重和偏置。这就是我们引入动量所需要做的全部。然后，要使用它，我们在构建网络时为每个全连接层添加动量关键字，如[列表 11-6](ch11.xhtml#ch11ex06)所示：
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Listing 11-6: Specifying momentum when building the network*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-6：在构建网络时指定动量*'
- en: Adding momentum per layer opens up the possibility of using layer-specific momentum
    values. While I’m unaware of any research doing so, it seems a fairly obvious
    thing to try, so by now, someone has likely experimented with it. For our purposes,
    we’ll set the momentum of all layers to 0.9 and move on.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 每层添加动量使得可以使用特定于每层的动量值。尽管我不知道是否有研究这样做，但这似乎是一个相当明显的尝试，因此现在可能已经有人尝试过了。对于我们的目的，我们将所有层的动量设置为
    0.9，并继续进行。
- en: 'How should we test our new momentum? We could use the MNIST dataset we used
    above, but it’s not a good candidate, because it’s too easy. Even a simple fully
    connected network achieves better than 97 percent accuracy. Therefore, we’ll replace
    the MNIST digits dataset with another, similar dataset that’s known to be more
    of a challenge: the Fashion-MNIST dataset. (See “Fashion-MNIST: A Novel Image
    Dataset for Benchmarking Machine Learning Algorithms” by Han Xiao et al., arXiv:1708.07747
    [2017].)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '我们应该如何测试我们新的动量？我们可以使用上面提到的 MNIST 数据集，但它并不是一个合适的候选者，因为它太简单了。即使是一个简单的全连接网络也能达到超过
    97% 的准确率。因此，我们将用另一个已知更具挑战性的数据集替换 MNIST 数字数据集：Fashion-MNIST 数据集。（请参见 Han Xiao 等人的《Fashion-MNIST:
    A Novel Image Dataset for Benchmarking Machine Learning Algorithms》，arXiv:1708.07747
    [2017]。）'
- en: 'The *Fashion-MNIST dataset (FMNIST)* is a drop-in replacement for the existing
    MNIST dataset. It contains images from 10 classes of clothing, all 28×28-pixel
    grayscale. For our purposes, we’ll do as we did for MNIST and reduce the 28×28-pixel
    images to 14 ×14 pixels. The images are in the `dataset` directory as NumPy arrays.
    Let’s train a model using them. The code for the model is similar to that of [Listing
    10-7](ch10.xhtml#ch10ex07), except in [Listing 11-7](ch11.xhtml#ch11ex07) we replace
    the MNIST dataset with FMNIST:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fashion-MNIST 数据集（FMNIST）* 是现有 MNIST 数据集的替代品。它包含来自 10 个服装类别的图像，均为 28×28 像素的灰度图像。为了我们的目的，我们将像
    MNIST 那样将 28×28 像素的图像缩减为 14×14 像素。图像存储在 `dataset` 目录中，以 NumPy 数组的形式。让我们使用这些数据训练一个模型。模型的代码与[列表
    10-7](ch10.xhtml#ch10ex07)类似，唯一不同的是在[列表 11-7](ch11.xhtml#ch11ex07)中，我们将 MNIST
    数据集替换为 FMNIST：'
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 11-7: Loading the Fashion-MNIST dataset*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 11-7：加载 Fashion-MNIST 数据集*'
- en: We also include code to calculate the Matthews correlation coefficient (MCC)
    on the test data. We first encountered the MCC in [Chapter 4](ch04.xhtml#ch04),
    where we learned that it’s a better measure of a model’s performance than the
    accuracy is. The code to run is in *fmnist.py*. Taking around 18 minutes on an
    older Intel i5 box, one run of it produced
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还包括了计算测试数据集的 Matthews 相关系数（MCC）的代码。我们在[第 4 章](ch04.xhtml#ch04)中首次接触到 MCC，并了解到它比准确率更能衡量模型的表现。要运行的代码在*fmnist.py*中。在一台较旧的
    Intel i5 计算机上，大约 18 分钟完成一次运行，输出结果为：
- en: '[PRE12]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The confusion matrix, still 10 × 10 because of the 10 classes in FMNIST, is
    quite noisy compared to the very clean confusion matrix we saw with MNIST proper.
    This is a challenging dataset for fully connected models. Recall that the MCC
    is a measure where the closer it is to one, the better the model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵，依然是10×10，因为FMNIST有10个类别，和我们在MNIST中看到的非常干净的混淆矩阵相比，它相当嘈杂。对于全连接模型来说，这是一个具有挑战性的数据集。回想一下，MCC是一个衡量标准，其值越接近1，模型越好。
- en: The confusion matrix above is for a model trained without momentum. The learning
    rate was 1.0, and it was trained for 40,000 minibatches of 64 samples. What happens
    if we add momentum of 0.9 to each fully connected layer and reduce the learning
    rate to 0.2? When we add momentum, it makes sense to reduce the learning rate
    so we aren’t taking large steps compounded by the momentum already moving in a
    particular direction. Do explore what happens if you run *fmnist.py* with a learning
    rate of 0.2 and no momentum.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的混淆矩阵是用于没有动量训练的模型。学习率为1.0，训练了40,000个包含64个样本的小批量。如果我们对每个全连接层添加0.9的动量，并将学习率降低到0.2，会发生什么呢？当我们添加动量时，降低学习率是有道理的，这样我们就不会因为动量已经朝某个方向移动而使步伐变得过大。请尝试在没有动量的情况下，以学习率0.2运行*fmnist.py*，看看会发生什么。
- en: The version of the code with momentum is in *fmnist_momentum.py*. After about
    20 minutes, one run of this code produced
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 含有动量的代码版本在*fmnist_momentum.py*中。大约20分钟后，这段代码运行一次产生了
- en: '[PRE13]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: giving us a slightly higher MCC. Does that mean momentum helped? Maybe. As we
    well understand by now, training neural networks is a stochastic process. So,
    we can’t rely on results from a single training of the models. We need to train
    the models many times and perform statistical tests on the results. Excellent!
    This gives us a chance to put the hypothesis testing knowledge we gained in [Chapter
    4](ch04.xhtml#ch04) to good use.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了略微更高的MCC。这是否意味着动量起到了帮助作用？也许吧。正如我们现在所理解的那样，训练神经网络是一个随机过程。所以，我们不能依赖单次训练模型的结果。我们需要多次训练模型并对结果进行统计测试。太棒了！这给了我们一个机会，能将我们在[第4章](ch04.xhtml#ch04)中学到的假设检验知识付诸实践。
- en: Instead of running *fmnist.py* and *fmnist_momentum.py* one time each, let’s
    run them 22 times each. This takes the better part of a day on my old Intel i5
    system, but patience is a virtue. The net result is 22 MCC values for the model
    with momentum and 22 for the model without momentum. There’s nothing magical about
    22 samples, but we intend to use the Mann-Whitney U test, and the rule of thumb
    for that test is to have at least 20 samples in each dataset.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与其分别运行*fmnist.py*和*fmnist_momentum.py*各一次，不如每个运行22次。这在我的旧Intel i5系统上大概需要一天的时间，但耐心是美德。最终结果是动量模型有22个MCC值，没有动量的模型也有22个。22个样本没有什么神奇之处，但我们打算使用Mann-Whitney
    U检验，而该检验的经验法则是每个数据集至少要有20个样本。
- en: '[Figure 11-8](ch11.xhtml#ch11fig08) displays histograms of the results.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-8](ch11.xhtml#ch11fig08)显示了结果的直方图。'
- en: '![image](Images/11fig08.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig08.jpg)'
- en: '*Figure 11-8: Histograms showing the distribution of MCC for models trained
    with momentum (light gray) and without (dark gray)*'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-8：显示带有动量（浅灰色）和不带动量（深灰色）模型的MCC分布的直方图*'
- en: The darker gray bars are the no-momentum MCC values, and the lighter bars are
    those with momentum. Visually, the two are largely distinct from each other. The
    code producing [Figure 11-8](ch11.xhtml#ch11fig08) is in the file *fmnist_analyze.py*.
    Do take a look at the code. It uses SciPy’s `ttest_ind` and `mannwhitneyu` along
    with the implementation we gave in [Chapter 4](ch04.xhtml#ch04) of Cohen’s *d*
    to calculate the effect size. The MCC values themselves are in the NumPy files
    listed in the code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 深灰色条表示没有动量的MCC值，浅灰色条表示有动量的MCC值。从视觉上看，这两者有很大的区别。生成[图11-8](ch11.xhtml#ch11fig08)的代码在*fmnist_analyze.py*文件中。请一定查看这段代码，它使用了SciPy的`ttest_ind`和`mannwhitneyu`，并结合我们在[第4章](ch04.xhtml#ch04)中提供的Cohen's
    *d*实现来计算效应大小。MCC值本身保存在代码中列出的NumPy文件中。
- en: 'Along with the graph, *fmnist_analyze.py* produces the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图表，*fmnist_analyze.py*还生成了以下输出：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: where the top two lines are the mean and the standard error of the mean. The
    t-test results are (*t*, *p*), the *t*-test statistic and associated *p*-value.
    Similarly, the Mann-Whitney U test results are (*U*, *p*), the *U* statistic and
    its *p*-value. Recall how the Mann-Whitney U test is a nonparametric test assuming
    nothing about the shape of the distribution of MCC values. The t-test assumes
    they are normally distributed. As we have only 22 samples each, we really can’t
    make any definitive statement about whether the results are normally distributed;
    the histograms don’t look much like Gaussian curves. That’s why we included the
    Mann-Whitney U test results.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，前两条线分别是均值和均值的标准误差。t检验的结果是（*t*，*p*），即*t*统计量及其关联的*p*值。同样，Mann-Whitney U检验的结果是（*U*，*p*），即*U*统计量及其*p*值。回想一下，Mann-Whitney
    U检验是一种非参数检验，假设MCC值的分布形态没有任何假定。而t检验假设它们服从正态分布。由于我们每组样本只有22个，实际上我们不能对结果是否服从正态分布做出任何明确的结论；这些直方图看起来并不像高斯曲线。这就是我们包含Mann-Whitney
    U检验结果的原因。
- en: A glance at the respective *p*-values tells us that the difference in means
    between the MCC values with and without momentum is highly statistically significant
    in favor of the with-momentum results. The *t*-value is positive, and the with-momentum
    result was the first argument. What of Cohen’s *d*-value? It’s a bit above 2.0,
    indicating a (very) large effect size.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下各自的*p*值，我们可以得出结论，带动量和不带动量的MCC值均值差异在统计上是高度显著的，且带动量的结果更具优势。*t*值为正，且带动量的结果是第一个参数。那么Cohen的*d*值呢？它略高于2.0，表示效应大小（非常）大。
- en: Can we *now* say that momentum helps in this case? Probably. It produced better
    performing models given the hyperparameters we used. The stochastic nature of
    training neural networks makes it possible that we could tweak the hyperparameters
    of both models to eliminate the difference we see in the data we have. The architecture
    between the two is fixed, but nothing says the learning rate and minibatch size
    are optimized for either model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们*现在*可以说动量在这个案例中有帮助吗？大概可以。它在我们使用的超参数下，产生了更好的模型表现。训练神经网络的随机性使得我们有可能调整两个模型的超参数，以消除我们在现有数据中看到的差异。两者之间的架构是固定的，但并没有规定学习率和小批量大小已针对任何一个模型进行了优化。
- en: A punctilious researcher would feel compelled to run an optimization process
    over the hyperparameters and, once satisfied that they’d found the very best model
    for both approaches, make a more definite statement after repeating the experiment.
    We, thankfully, are not punctilious researchers. Instead, we’ll use the evidence
    we have, along with the several decades of wisdom acquired by the world’s machine
    learning researchers regarding the utility of momentum in gradient descent, to
    state that, yes, momentum helps models learn, and you should use it in most cases.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一位严谨的研究人员可能会感到有必要对超参数进行优化处理，并且一旦确信自己找到了适用于这两种方法的最佳模型，便会在重复实验后做出更明确的结论。幸运的是，我们并不是严谨的研究人员。相反，我们将利用现有的证据，以及全球机器学习研究人员数十年来关于动量在梯度下降法中作用的智慧，来声明：是的，动量确实有助于模型学习，在大多数情况下你应该使用它。
- en: However, the normality question is begging for further investigation. We are,
    after all, seeking to improve our mathematical *and* practical intuition regarding
    deep learning. Therefore, let’s train the with-momentum model for FMNIST, not
    22 times but 100 times. As a concession, we’ll reduce the number of minibatches
    from 40,000 to 10,000\. Still, expect to spend the better part of a day waiting
    for the program to finish. The code, which we won’t walk through here, is in *fmnist_repeat.py*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正态性问题仍然需要进一步调查。毕竟，我们是在寻求提升自己在深度学习方面的数学*和*实践直觉。因此，接下来我们将对带动量的模型进行FMNIST训练，不是训练22次，而是训练100次。作为妥协，我们将把小批量的数量从40,000减少到10,000。尽管如此，预计仍然需要大部分时间来等待程序完成。代码，虽然我们在这里不逐步讲解，位于*fmnist_repeat.py*中。
- en: '[Figure 11-9](ch11.xhtml#ch11fig09) presents a histogram of the results.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-9](ch11.xhtml#ch11fig09)展示了结果的直方图。'
- en: Clearly, this distribution does not look at all like a normal curve. The output
    of *fmnist_repeat.py* includes the result of SciPy’s `normaltest` function. This
    function performs a statistical test on a set of data under the null hypothesis
    that the data *is* normally distributed. Therefore, a *p*-value below, say, 0.05
    or 0.01, indicates data that is not normally distributed. Our *p*-value is virtually
    zero.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这个分布看起来完全不像一个正态曲线。*fmnist_repeat.py* 的输出包括了 SciPy 的 `normaltest` 函数的结果。该函数对一组数据进行统计检验，零假设是数据*服从*正态分布。因此，*p*-值低于
    0.05 或 0.01 表示数据不服从正态分布。我们的 *p*-值几乎为零。
- en: 'What to make of [Figure 11-9](ch11.xhtml#ch11fig09)? First, as the results
    are certainly not normal, we aren’t justified in using a t-test. However, we also
    used the nonparametric Mann-Whitney U test and found highly statistically significant
    results, so our claims above are still valid. Second, the long tail of the distribution
    in [Figure 11-9](ch11.xhtml#ch11fig09) is to the left. We might even make an argument
    that the result is possibly bimodal: that there are two peaks, one near 0.83 and
    the other, smaller one near an MCC of 0.75.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解读 [图 11-9](ch11.xhtml#ch11fig09)？首先，由于结果显然不是正态分布，我们不能使用 t 检验。然而，我们也使用了非参数的
    Mann-Whitney U 检验，并得到了高度统计显著的结果，因此我们上述的结论仍然有效。其次，[图 11-9](ch11.xhtml#ch11fig09)中的长尾在左侧。我们甚至可以提出一个论点，认为结果可能是双峰的：有两个峰，一个接近
    0.83，另一个较小，接近 MCC 为 0.75。
- en: '![image](Images/11fig09.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig09.jpg)'
- en: '*Figure 11-9: Distribution of MCC values for 100 trainings of the FMNIST model*'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-9：FMNIST 模型 100 次训练的 MCC 值分布*'
- en: Most models trained to a relatively consistent level of performance, with an
    MCC near 0.83\. However, the long tail indicates that when the model wasn’t reasonably
    good, it was just plain horrid.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型训练后表现相对一致，MCC 值接近 0.83。然而，长尾表明，当模型的表现不理想时，它的表现非常糟糕。
- en: Intuitively, [Figure 11-9](ch11.xhtml#ch11fig09) seems reasonable to me. We
    know stochastic gradient descent is susceptible to improper initialization, and
    our little toolkit is using old-school small random value initialization. It seems
    likely that we have an increased chance of starting at a poor location in the
    loss landscape and are doomed after that to poor performance.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 直观来看，[图 11-9](ch11.xhtml#ch11fig09)对我来说似乎是合理的。我们知道随机梯度下降对初始化不当敏感，而我们的工具包使用的是传统的小随机值初始化。看起来我们更有可能从一个较差的位置开始，之后就注定会得到较差的表现。
- en: What if the tail were on the right? What might that indicate? A long tail on
    the right would mean most model performance is mediocre to poor, but, on occasion,
    an especially “bright” model comes along. Such a scenario would mean that better
    models are out there, but that our training and/or initialization strategy isn’t
    particularly good at finding them. I think the tail on the left is preferable—most
    models find reasonably good local minima, so most trainings, unless horrid, end
    up in pretty much the same place in terms of performance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尾部出现在右侧呢？那可能意味着什么？右侧的长尾意味着大多数模型表现中等或较差，但偶尔会有一个特别“优秀”的模型出现。这样的情景意味着更好的模型是存在的，但我们的训练和/或初始化策略并不是特别擅长找到它们。我认为左侧的尾部更可取——大多数模型能找到合理的局部最小值，因此大多数训练，除非非常糟糕，最终会在表现上基本达到相同的水平。
- en: Now, let’s examine a common variant of momentum, one that you’ll no doubt run
    across during your sojourn through deep learning.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探讨一个常见的动量变种，你在深入学习中无疑会遇到它。
- en: Nesterov Momentum
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Nesterov 动量
- en: Many deep learning toolkits include the option to use *Nesterov momentum* during
    gradient descent. Nesterov momentum is a modification of gradient descent widely
    used in the optimization community. The version typically implemented in deep
    learning updates standard momentum from
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习工具包都包含了在梯度下降中使用 *Nesterov 动量* 的选项。Nesterov 动量是梯度下降的一种改进，广泛应用于优化领域。深度学习中通常实现的版本是从标准动量更新而来的：
- en: '![Image](Images/295equ01.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/295equ01.jpg)'
- en: to
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '![Image](Images/11equ06.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ06.jpg)'
- en: where we’re using gradient notation instead of partials of a loss function to
    indicate that the technique is general and applies to any function, *f*(***x***).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用梯度符号，而不是损失函数的偏导数，以表明该技术是通用的，适用于任何函数，*f*(***x***).
- en: The difference between standard momentum and deep learning Nesterov momentum
    is subtle, just a term that’s added to the argument of the gradient. The idea
    is to use the existing momentum to calculate the gradient, not at the current
    position, ***x***, but the position gradient descent would be at if it continued
    further using the current momentum, ***x*** + *μ **v***. We then use the gradient’s
    value at that position to update the current position, as before.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 标准动量和深度学习 Nesterov 动量之间的差异很微妙，只是在梯度的参数中增加了一个项。其思想是利用现有的动量来计算梯度，而不是在当前位置***x***上，而是在如果继续使用当前动量，梯度下降将会达到的位置，即
    ***x*** + *μ **v***。然后，我们使用该位置的梯度值来更新当前位置，如之前所做的那样。
- en: The claim, well demonstrated for optimization in general, is that this tweak
    leads to faster convergence, meaning gradient descent will find the minimum in
    fewer steps. However, even though toolkits implement it, there is reason to believe
    the noise that stochastic gradient descent with minibatches introduces offsets
    the adjustment to the point where it’s unlikely Nesterov momentum is any more
    useful for training deep learning models than regular momentum. (For more on this,
    see the comment on page 292 of *Deep Learning* by Ian Goodfellow et al.)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这一声明在优化中得到了很好的证明：这个调整导致了更快的收敛，这意味着梯度下降将在更少的步骤中找到最小值。然而，尽管工具包已经实现了它，但有理由相信，随机梯度下降和小批量引入的噪声抵消了这个调整，使得
    Nesterov 动量在训练深度学习模型时可能不比常规动量更有用。（有关更多信息，请参见 Ian Goodfellow 等人著作《深度学习》第 292 页上的评论。）
- en: However, the 2D example in this chapter uses the actual function to calculate
    gradients, so we might expect Nesterov momentum to be effective in that case.
    Let’s update the 2D example, minimizing the sum of two inverted Gaussians, and
    see if Nesterov momentum improves convergence, as claimed. The code we’ll run
    is in *gd_nesterov.py* and is virtually identical to the code in *gd_momentum.py*.
    Additionally, I tweaked both files a tiny bit to return the final position after
    gradient descent is complete. That way, we can see how close we are to the known
    minima.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本章中的二维示例使用了实际的函数来计算梯度，因此我们可以预期在这种情况下 Nesterov 动量会有效。让我们更新二维示例，最小化两个反向高斯分布的和，看看
    Nesterov 动量是否如所声称的那样改善了收敛。我们将运行的代码在 *gd_nesterov.py* 中，它与 *gd_momentum.py* 中的代码几乎相同。此外，我稍微修改了这两个文件，使它们在梯度下降完成后返回最终位置。这样，我们就可以看到我们接近已知最小值的程度。
- en: Implementing [Equation 11.6](ch11.xhtml#ch11equ06) is straightforward and affects
    only the velocity update, causing
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 [方程 11.6](ch11.xhtml#ch11equ06) 很简单，只会影响速度更新，导致
- en: vx = mu*vx - eta * dx(x,y)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: vx = mu*vx - eta * dx(x,y)
- en: vy = mu*vy - eta * dy(x,y)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: vy = mu*vy - eta * dy(x,y)
- en: to become
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 变为
- en: vx = mu * vx - eta * dx(x + mu * vx,y)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: vx = mu * vx - eta * dx(x + mu * vx,y)
- en: vy = mu * vy - eta * dy(x,y + mu * vy)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: vy = mu * vy - eta * dy(x,y + mu * vy)
- en: to add the momentum for each component, *x* and *y*. Everything else remains
    the same.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个分量 *x* 和 *y* 添加动量。其他一切保持不变。
- en: '[Figure 11-10](ch11.xhtml#ch11fig010) compares standard momentum (top, from
    [Figure 11-7](ch11.xhtml#ch11fig07)) and Nesterov momentum (bottom).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-10](ch11.xhtml#ch11fig010) 比较了标准动量（上，来自 [图 11-7](ch11.xhtml#ch11fig07)）和
    Nesterov 动量（下）。'
- en: '![image](Images/11fig10.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig10.jpg)'
- en: '*Figure 11-10: Standard momentum (top) and Nesterov momentum (bottom)*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-10：标准动量（上）和 Nesterov 动量（下）*'
- en: Visually, Nesterov momentum shows less of an overshoot, especially for the spiral
    marking the path beginning at (1.5, 1.5). What about the final location that each
    approach returns? We get [Table 11-2](ch11.xhtml#ch11tab02).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉上，Nesterov 动量显示出较少的超调，特别是在从 (1.5, 1.5) 开始的螺旋路径上。那每种方法返回的最终位置如何呢？我们可以看到 [表
    11-2](ch11.xhtml#ch11tab02)。
- en: '**Table 11-2**: Final Location for Gradient Descent With and Without Nesterov
    Momentum'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 11-2**：使用和不使用 Nesterov 动量的梯度下降最终位置'
- en: '| **Initial point** | **Standard** | **Nesterov** | **Minimum** |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **初始点** | **标准** | **Nesterov** | **最小值** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| (1.5,1.5) | (–0.9496, 0.9809) | (–0.9718, 0.9813) | (–1,1) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| (1.5,1.5) | (–0.9496, 0.9809) | (–0.9718, 0.9813) | (–1,1) |'
- en: '| (0.7,–0.2) | (0.8807, –0.9063) | (0.9128, –0.9181) | (1,–1) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (0.7,–0.2) | (0.8807, –0.9063) | (0.9128, –0.9181) | (1,–1) |'
- en: The Nesterov momentum results are closer to the known minima than the standard
    momentum results after the same number of gradient descent steps.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的梯度下降步骤数后，Nesterov 动量的结果比标准动量的结果更接近已知最小值。
- en: Adaptive Gradient Descent
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应梯度下降
- en: 'The gradient descent algorithm is almost trivial, which invites adaptation.
    In this section, we’ll walk through the math behind three variants of gradient
    descent popular with the deep learning community: RMSprop, Adagrad, and Adam.
    Of the three, Adam is the most popular by far, but the others are well worth understanding,
    as they build in succession leading up to Adam. All three of these algorithms
    adapt the learning rate on the fly in some manner.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法几乎是微不足道的，这也促使了它的适应性。在本节中，我们将深入探讨深度学习社区中流行的三种梯度下降变体：RMSprop、Adagrad 和 Adam。三者中，Adam
    至今最为流行，但其他两者也值得了解，因为它们是逐步演变至 Adam 的基础。这三种算法都以某种方式动态调整学习率。
- en: RMSprop
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RMSprop
- en: Geoffrey Hinton introduced *RMSprop*, which stands for *root mean square propagation*,
    in his 2012 Coursera lecture series. Much like momentum (with which it can be
    combined), RMSprop is gradient descent that tracks the value of the gradient as
    it changes and uses that value to modify the step taken.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey Hinton 在2012年 Coursera 讲座系列中介绍了 *RMSprop*，它代表了 *均方根传播*（root mean square
    propagation）。类似于动量（可以与其结合使用），RMSprop 是一种梯度下降法，它跟踪梯度值的变化，并利用这些值来修改步长。
- en: RMSprop uses a *decay term*, γ (gamma), to calculate a running average of the
    gradients as the algorithm progresses. In his lecture, Hinton uses γ = 0.9.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 使用 *衰减项* γ（gamma）来计算算法执行过程中的梯度运行平均值。在他的讲座中，Hinton 使用 γ = 0.9。
- en: The gradient descent update becomes
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新公式变为
- en: '![Image](Images/11equ07.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ07.jpg)'
- en: First, we update *m*, the running average of the squares of the gradients, weighted
    by γ, the decay term. Next comes the velocity term, which is almost the same as
    in vanilla gradient descent, but we divide the learning rate by the running average’s
    square root, hence the RMS part of RMSprop. We then subtract the scaled velocity
    from the current position to take the step. We’re writing the step as an addition,
    similar to the momentum equations above ([Equations 11.5](ch11.xhtml#ch11equ05)
    and [11.6](ch11.xhtml#ch11equ06)); note the minus sign before the velocity update.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们更新 *m*，即梯度平方的加权运行平均值，权重是 γ，即衰减项。接下来是速度项，它与普通梯度下降几乎相同，但我们将学习率除以运行平均值的平方根，这就是
    RMSprop 中的 RMS 部分。然后我们从当前的位置中减去缩放后的速度，以便迈出步伐。我们将这一步写成加法，类似于上面的动量方程（[方程 11.5](ch11.xhtml#ch11equ05)
    和 [11.6](ch11.xhtml#ch11equ06)）；注意速度更新前的减号。
- en: 'RMSprop works with momentum as well. For example, extending RMSprop with Nesterov
    momentum is straightforward:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 也可以与动量一起使用。例如，扩展 RMSprop 与 Nesterov 动量结合是很直接的：
- en: '![Image](Images/11equ08.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ08.jpg)'
- en: with *μ* the momentum factor, as before.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *μ* 是动量因子，和之前一样。
- en: It’s claimed that RMSprop is a robust classifier. We’ll see below how it fared
    on one test. We’re considering it an adaptive technique because the learning rate
    (*η*) is scaled by the square root of the running gradient mean; therefore, the
    effective learning rate is adjusted based on the history of the descent—it isn’t
    fixed once and for all.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有人声称 RMSprop 是一种稳健的分类器。我们将在下面看到它在一个测试中的表现。我们将其视为一种自适应技术，因为学习率（*η*）是通过梯度的均值的平方根来缩放的；因此，有效学习率是根据下降过程的历史进行调整的——它不是一成不变的。
- en: RMSprop is often used in reinforcement learning, the branch of machine learning
    that attempts to learn how to act. For example, playing Atari video games uses
    reinforcement learning. RMSprop is believed to be robust when the optimization
    process is *nonstationary*, meaning the statistics change in time. Conversely,
    a *stationary* process is one where the statistics do not change in time. Training
    classifiers using supervised learning is stationary, as the training set is, typically,
    fixed and not changing, as should be the data fed to the classifier over time,
    though that is harder to enforce. In reinforcement learning, time is a factor,
    and the statistics of the dataset might change over time; therefore, reinforcement
    learning might involve nonstationary optimization.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 常用于强化学习，这是机器学习的一个分支，旨在学习如何采取行动。例如，玩雅达利电子游戏就使用了强化学习。当优化过程是 *非平稳*（nonstationary）时，RMSprop
    被认为是稳健的，这意味着统计数据会随着时间发生变化。相反，*平稳*（stationary）过程是指统计数据不会随时间变化。使用监督学习训练分类器是平稳的，因为训练集通常是固定的，并且不会发生变化，就像输入给分类器的数据应该是固定的一样，尽管这更难强制执行。在强化学习中，时间是一个因素，数据集的统计特性可能会随着时间的推移而变化；因此，强化学习可能涉及非平稳的优化过程。
- en: Adagrad and Adadelta
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Adagrad 和 Adadelta
- en: '*Adagrad* appeared in 2011 (see “Adaptive Subgradient Methods for Online Learning
    and Stochastic Optimization” by John Duchi et al., *Journal of Machine Learning
    Research* 12[7], [2011]). At first glance, it looks quite similar to RMSprop,
    though there are important differences.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adagrad* 出现于2011年（参见 John Duchi 等人的“自适应子梯度方法用于在线学习与随机优化”，*机器学习研究杂志* 12[7]，
    [2011]）。乍一看，它与RMSprop非常相似，尽管存在重要的差异。'
- en: We can write the basic update rule for Adagrad as
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将Adagrad的基本更新规则写作：
- en: '![Image](Images/11equ09.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ09.jpg)'
- en: This requires some explanation.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一些解释。
- en: First, notice the *i* subscript on the velocity update, both on the velocity,
    ***v***, and the gradient, *▽f **(x***). Here, *i* refers to a component of the
    velocity, meaning the update must be applied per component. The top of [Equation
    11.9](ch11.xhtml#ch11equ09) repeats for all the components of the system. For
    a deep neural network, this means all the weights and biases.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意速度更新中的*i*下标，既在速度***v***上，也在梯度*▽f **(x***上。这里的*i*指的是速度的一个组件，意味着更新必须逐组件应用。[方程11.9](ch11.xhtml#ch11equ09)的顶部对系统的所有组件都重复。对于深度神经网络来说，这意味着所有的权重和偏置。
- en: Next, look at the sum in the denominator of the per-component velocity update.
    Here, τ (tau) is a counter over *all* the gradient steps taken during the optimization
    process, meaning for each component of the system, Adagrad tracks the sum of the
    square of the gradient calculated at each step. If we’re using [Equation 11.9](ch11.xhtml#ch11equ09)
    for the 11th gradient descent step, then the sum in the denominator will have
    11 terms, and so on. As before, *η* is a learning rate, which here is global to
    all components.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看看分母中每个组件的速度更新的和。这里，τ（tau）是优化过程中进行的*所有*梯度步骤的计数器，意味着对于系统的每个组件，Adagrad跟踪在每个步骤中计算的梯度平方和。如果我们使用[方程11.9](ch11.xhtml#ch11equ09)进行第11步梯度下降，那么分母中的和将有11项，依此类推。如前所述，*η*是学习率，在这里是全局的，适用于所有组件。
- en: 'A variant of Adagrad is also in widespread use: *Adadelta*. (See “Adadelta:
    An Adaptive Learning Rate Method” by Matthew Zeiler, [2012].) Adadelta replaces
    the square root of the sum over all steps in the velocity update with a running
    average of the last few steps, much like the running average of RMSprop. Adadelta
    also replaces the manually selected global learning rate, *η*, with a running
    average of the previous few velocity updates. This eliminates the selection of
    an appropriate *η* but introduces a new parameter, γ, to set the window’s size,
    as was done for RMSprop. It’s likely that γ is less sensitive to the properties
    of the dataset than *η* is. Note how in the original Adadelta paper, γ is written
    as *ρ* (rho).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad的一个变种也在广泛使用中：*Adadelta*。（参见Matthew Zeiler的“Adadelta：一种自适应学习率方法”，[2012]）。Adadelta将速度更新中所有步长的平方和的平方根，替换为最近几步的滑动平均，类似于RMSprop的滑动平均。Adadelta还将手动选择的全局学习率*η*，替换为前几次速度更新的滑动平均。这消除了选择合适的*η*，但引入了一个新参数γ，用于设置窗口大小，类似于RMSprop的做法。γ可能对数据集的特性不如*η*敏感。注意，在原始的Adadelta论文中，γ被写作*ρ*（rho）。
- en: Adam
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Adam
- en: 'Kingma and Ba published *Adam*, from “adaptive moment estimation,” in 2015,
    and it has been cited over 66,000 times as of this writing. Adam uses the square
    of the gradient, as RMSprop and Adagrad do, but also tracks a momentum-like term.
    Let’s present the update equations and then walk through them:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Kingma和Ba于2015年发布了*Adam*，即“自适应矩估计”，截至目前已被引用超过66,000次。Adam使用梯度的平方，就像RMSprop和Adagrad一样，但还跟踪类似动量的项。让我们先展示更新方程，然后逐步解析：
- en: '![Image](Images/11equ10.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ10.jpg)'
- en: The first two lines of [Equation 11.10](ch11.xhtml#ch11equ10) define ***m***
    and ***v*** as running averages of the first and second moments. The first moment
    is the mean; the second moment is akin to the variance, which is the second moment
    of the difference between a data point and the mean. Note the squaring of the
    gradient value in the definition of ***v***. The running moments are weighted
    by two scalar parameters, *β*[1] and *β*[2].
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程11.10](ch11.xhtml#ch11equ10)的前两行定义了***m***和***v***作为一阶和二阶矩的滑动平均。一阶矩是均值；二阶矩类似于方差，是数据点与均值之间差异的二阶矩。注意在***v***的定义中有对梯度值的平方。滑动矩由两个标量参数*β*[1]和*β*[2]加权。'
- en: The next two lines define ![Image](Images/300equ01.jpg) and ![Image](Images/300equ02.jpg).
    These are bias correction terms to make ***m*** and ***v*** better estimates of
    the first and second moments. Here, *t*, an integer starting at zero, is the timestep.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行定义了![Image](Images/300equ01.jpg)和![Image](Images/300equ02.jpg)。这些是偏差修正项，用来使***m***和***v***成为第一和第二矩的更好估计值。这里的*t*是一个从零开始的整数，表示时间步长。
- en: The actual step updates ***x*** by subtracting the bias-corrected first moment,
    ![Image](Images/300equ01.jpg), scaled by the ratio of the global learning rate,
    *η*, and the square root of the bias-corrected second moment, ![Image](Images/300equ02.jpg).
    The ∊ term is a constant to avoid division by zero.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 实际步骤通过减去偏差修正的第一矩，![Image](Images/300equ01.jpg)，再乘以全局学习率的比率，*η*，以及偏差修正的第二矩的平方根，![Image](Images/300equ02.jpg)，来更新***x***。∊项是一个常数，用于避免除以零。
- en: '[Equation 11.10](ch11.xhtml#ch11equ10) has four parameters, which seems excessive,
    but three of them are straightforward to set and are seldom changed. The original
    paper suggests *β*[1] = 0.9, *β*[2] = 0.999, and ∊ = 10^(−8). Therefore, as with
    vanilla gradient descent, the user is left to select *η*. For example, Keras defaults
    to *η* = 0.001, which works well in many cases.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.10](ch11.xhtml#ch11equ10)有四个参数，虽然看起来过多，但其中三个参数设置简单且很少更改。原始论文建议*β*[1]
    = 0.9，*β*[2] = 0.999，∊ = 10^(−8)。因此，就像普通的梯度下降一样，用户需要选择*η*。例如，Keras默认的*η* = 0.001，这在许多情况下效果良好。'
- en: The Kingma and Ba paper shows via experiment that Adam generally outperforms
    SGD with Nesterov momentum, RMSprop, Adagrad, and Adadelta. This is likely why
    Adam is currently the go-to optimizer for many deep learning tasks.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Kingma和Ba的论文通过实验表明，Adam通常优于带Nesterov动量的SGD、RMSprop、Adagrad和Adadelta。这很可能是为什么Adam目前成为许多深度学习任务的首选优化器。
- en: Some Thoughts About Optimizers
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于优化器的一些思考
- en: Which optimization algorithm to use and when depends on the dataset. As mentioned,
    Adam is currently favored for many tasks, though properly tuned SGD can be quite
    effective as well, and some swear by it. While it’s not possible to make a blanket
    statement about which is the best algorithm, for there is no such thing, we can
    conduct a little experiment and discuss the results.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪个优化算法以及何时使用取决于数据集。如前所述，Adam目前在许多任务中是首选，尽管经过适当调优的SGD也能非常有效，甚至有人对此深信不疑。虽然无法做出“最佳算法”的绝对判断，因为并不存在所谓的“最佳”，但我们可以进行一个小实验，并讨论结果。
- en: 'This experiment, for which I’ll present only the results, trained a small convolutional
    neural network on MNIST using 16,384 random samples for the training set, a minibatch
    of 128, and 12 epochs. The results show the mean and standard error of the mean
    for five runs of each optimizer: SGD, RMSprop, Adagrad, and Adam. Of interest
    is the accuracy of the test set and the training clock time. I trained all models
    on the same machine, so relative timing is what we should look at. No GPU was
    used.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验，我将仅展示结果，使用16,384个随机样本对MNIST上的小型卷积神经网络进行训练，训练集大小为128的迷你批次，训练12个周期。结果展示了每个优化器（SGD、RMSprop、Adagrad和Adam）五次运行的均值和标准误差。感兴趣的点是测试集的准确率和训练时钟时间。我在同一台机器上训练了所有模型，因此我们应关注相对时间。没有使用GPU。
- en: '[Figure 11-11](ch11.xhtml#ch11fig011) shows the overall test set accuracy (top)
    and the training time (bottom) by optimizer.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-11](ch11.xhtml#ch11fig011)展示了按优化器分类的总体测试集准确率（上）和训练时间（下）。'
- en: On average, SGD and RMSprop were about 0.5 percent less accurate than the other
    optimizers, with RMSprop varying widely but never matching Adagrad or Adam. Arguably,
    Adam performed the best in terms of accuracy. For training time, SGD was the fastest
    and Adam the slowest, as we might expect, given the multiple per-step calculations
    Adam performs relative to the simplicity of SGD. Overall, the results support
    the community’s intuition that Adam is a good optimizer.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 平均来说，SGD和RMSprop的准确率比其他优化器低约0.5%，而RMSprop的准确率波动较大，但始终未能超过Adagrad或Adam。可以说，Adam在准确率方面表现最好。至于训练时间，SGD最快，而Adam最慢，正如我们所料，Adam相比SGD需要执行更多的每步计算。总体而言，结果支持社区的直觉：Adam是一个不错的优化器。
- en: '![image](Images/11fig11.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig11.jpg)'
- en: '*Figure 11-11: MNIST model accuracy (top) and training time (bottom) by optimizer*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-11：按优化器分类的MNIST模型准确率（上）和训练时间（下）*'
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented gradient descent, working through the basic form, vanilla
    gradient descent, with 1D and 2D examples. We followed by introducing stochastic
    gradient descent and justified its use in deep learning.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了梯度下降，从基本形式的原始梯度下降开始，配合1D和2D的例子。接着，我们介绍了随机梯度下降，并证明了它在深度学习中的应用。
- en: We discussed momentum next, both standard and Nesterov. With standard momentum,
    we demonstrated that it does help in training deep models (well, relatively “deep”).
    We showed the effect of Nesterov momentum visually using a 2D example and discussed
    why Nesterov momentum and stochastic gradient descent might counteract each other.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们讨论了动量，包括标准动量和Nesterov动量。通过标准动量，我们展示了它在训练深度模型（好吧，相对“深”）中的帮助。我们用一个2D例子直观展示了Nesterov动量的效果，并讨论了为什么Nesterov动量和随机梯度下降可能会互相抵消。
- en: The chapter concluded with a look at the gradient descent update equations for
    advanced algorithms, thereby illustrating how vanilla gradient descent invites
    modification. A simple experiment gave us insight into how the algorithms perform
    and appeared to justify the deep learning community’s belief in Adam’s general
    suitability over SGD.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后，我们回顾了先进算法的梯度下降更新方程，展示了原始梯度下降如何引入修改。一个简单的实验让我们深入了解了这些算法的表现，并似乎证明了深度学习社区普遍认为Adam在一般情况下比SGD更合适的观点。
- en: And, with this chapter, our exploration of the mathematics of deep learning
    draws to a close. All that remains is a final appendix that points you to places
    where you can go to learn more.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的结束，我们对深度学习数学的探索也画上了句号。剩下的只是最后一个附录，指引你去了解更多的资源。
- en: Epilogue
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结语
- en: As the great computer scientist Edsger W. Dijkstra said, “There should be no
    such thing as boring mathematics.” I sincerely hope you didn’t find this book
    boring. I’d hate to offend Dijkstra’s ghost. If you’re still reading at this point,
    I suspect you did find something of merit. Good! Thanks for sticking with it.
    Math should never be boring.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 正如伟大的计算机科学家艾兹格尔·W·戴克斯特拉（Edsger W. Dijkstra）所说：“不应该有无聊的数学。”我真心希望你没有觉得这本书无聊。我可不想得罪戴克斯特拉的幽灵。如果你此刻还在继续阅读，我怀疑你确实找到了些有价值的内容。好极了！感谢你坚持下来。数学永远不应该无聊。
- en: 'We’ve covered the basics of what you need to understand and work with deep
    learning. Don’t stop here, however: use the references in the Appendix and continue
    your mathematical explorations. You should never be satisfied with your knowledge
    base—always seek to broaden it.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了理解和使用深度学习所需的基础知识。然而，别停在这里：请使用附录中的参考资料，继续你的数学探索。你永远不应该对自己的知识基础感到满足——总是寻求扩展它。
- en: If you have questions or comments, please do reach out to me at *[mathfordeeplearning@gmail.com](mailto:mathfordeeplearning@gmail.com)*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或意见，请通过*【mathfordeeplearning@gmail.com】(mailto:mathfordeeplearning@gmail.com)*与我联系。
