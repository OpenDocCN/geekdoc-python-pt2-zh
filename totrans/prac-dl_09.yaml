- en: '**9'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TRAINING A NEURAL NETWORK**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we’ll discuss how to train a neural network. We’ll look at
    the standard approaches and tricks being used in the field today. There will be
    some math, some hand-waving, and a whole host of new terms and concepts. But you
    don’t need to follow the math at a deep level: we’ll gloss over things as needed
    to get the main point across.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is perhaps the most challenging in the book, at least conceptually.
    It certainly is mathematically. While it’s crucially important to building intuition
    and understanding, sometimes we get impatient and like to dive into things first
    to test the waters. Thanks to preexisting libraries, we can do that here. If you
    want to play around with neural networks before learning how they work, jump to
    [Chapter 10](ch10.xhtml#ch10) before coming back here to fill in the theory. But
    do come back.
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to learn to use powerful toolkits like sklearn and Keras without
    understanding how they work. That approach should not satisfy anyone, though the
    temptation is real. Understanding how these algorithms work is well worth your
    time.
  prefs: []
  type: TYPE_NORMAL
- en: A High-Level Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin this chapter with an overview of the concepts we’ll discuss. Read
    it, but don’t fret if the concepts are unclear. Instead, try to get a feel for
    the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in training a neural network is selecting intelligent initial
    values for the weights and biases. We then use *gradient descent* to modify these
    weights and biases so that we reduce the error over the training set. We’ll use
    the average value of the loss function to measure the error, which tells us how
    wrong the network currently is. We know if the network is right or wrong because
    we have the expected output for each input sample in the training set (the class
    label).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is an algorithm that requires gradients. For now, think of
    gradients as measures of steepness. The larger the gradient, the steeper the function
    is at that point. To use gradient descent to search for the smallest value of
    the loss function, we need to be able to find gradients. For that, we’ll use *backpropagation*.
    This is the fundamental algorithm of neural networks, the one that allows them
    to learn successfully. It gives us the gradients we need by starting at the output
    of the network and moving back through the network toward the input. Along the
    way, it calculates the gradient value for each weight and bias.
  prefs: []
  type: TYPE_NORMAL
- en: With the gradient values, we can use the gradient descent algorithm to update
    the weights and biases so that the next time we pass the training samples through
    the network, the average of the loss function will be less than it was before.
    In other words, our network will be less wrong. This is the goal of training,
    and we hope it results in a network that has learned general features of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Learning general features of the dataset requires *regularization*. There are
    many approaches to regularization, and we’ll discuss the main ones. Without regularization,
    the training process is in danger of overfitting, and we could end up with a network
    that doesn’t generalize. But with regularization, we can be successful and get
    a useful model.
  prefs: []
  type: TYPE_NORMAL
- en: So, the following sections introduce gradient descent, backpropagation, loss
    functions, weight initialization, and, finally, regularization. These are the
    main components of successful neural network training. We don’t need to understand
    these in all their gory mathematical details; instead, we need to understand them
    conceptually so we can build an intuitive approach to what it means to train a
    neural network. With this intuition, we’ll be able to make meaningful use of the
    parameters that sklearn and Keras give us for training.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The standard way to train a neural network is to use gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s parse the phrase *gradient descent*. We already know what the word *descent*
    means. It means to go down from somewhere higher up. What about *gradient*? The
    short answer is that a gradient indicates how quickly something changes with respect
    to how fast something else changes. Measuring how much one thing changes as another
    changes is something we’re all familiar with. We all know about speed, which is
    how position changes as time changes. We even say it in words: *miles per hour*
    or *kilometers per hour*.'
  prefs: []
  type: TYPE_NORMAL
- en: You’re probably already familiar with the gradient in another context. Consider
    the equation of a line
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *mx* + *b*'
  prefs: []
  type: TYPE_NORMAL
- en: where *m* is the slope and *b* is the y-axis intercept. The slope is how quickly
    the line’s *y* position changes with each change in the *x* position. If we know
    two points that are on the line, (*x*[0],*y*[0]) and (*x*[1],*y*[1]), then we
    can calculate the slope as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/191equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'which, in words, we might say as “*y*’s per *x*.” It’s a measure of how steep
    or shallow the line is: its gradient. In mathematics, we often talk about a change
    in a variable, and the notation for that is to put a Δ (delta) in front. So, we
    might write the slope of a line as'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/191equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: to drive home the point that the slope is the change in *y* for each change
    in *x*. Fortunately for us, it turns out that not only do lines have a slope at
    each point, but also most functions have a slope at each point. However, except
    for straight lines, this slope changes from point to point. A picture will help
    here. Consider [Figure 9-1](ch09.xhtml#ch9fig1).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-1: A function with several tangent lines indicated*'
  prefs: []
  type: TYPE_NORMAL
- en: The graph in [Figure 9-1](ch09.xhtml#ch9fig1) is of a polynomial. Notice the
    lines drawn on the figure that are just touching the function. These are *tangent*
    lines. And as lines, they have a slope we can see in the plot. Now imagine moving
    one of the lines over the function so that it continues to touch the function
    at only one point; imagine how the slope of the line changes as it moves.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that how the slope changes over the function is itself a function,
    and it’s called the *derivative*. Given a function and *x* value, the derivative
    tells us the slope of the function at that point, *x*. The fact that functions
    have derivatives is a fundamental insight of calculus, and of fundamental importance
    to us.
  prefs: []
  type: TYPE_NORMAL
- en: The notion of a derivative is essential because for single variable functions,
    the derivative at the point *x* is the gradient at *x*; it’s the direction in
    which the function is changing. If we want to find the minimum of the function,
    the *x* that gives us the smallest *y*, we want to move in the direction *opposite*
    to the gradient as that will move us in the direction of the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: The derivative is written in many different ways, but the way that echoes the
    idea of the slope, how *y* changes for a change in *x*, is
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/192equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We’ll return to this form next when discussing the backpropagation algorithm.
    That’s it for the gradient; now let’s take a closer look at descent.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Minimums
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since we want a model that makes few mistakes, we need to find the set of parameters
    that lead to a small value for the loss function. In other words, we need to find
    a *minimum* of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look again at [Figure 9-1](ch09.xhtml#ch9fig1). The minimum is on the right,
    where tangent line C is. We can see it’s the minimum, and notice that the gradient
    is 0 there. This tells us we’re at a minimum (or maximum). If we start at B, we
    see that the slope of the tangent line is negative (down and to the right). Therefore,
    we need to move to an *x* value in the positive direction because this is opposite
    to the sign of the gradient. Doing this will take us closer to the minimum at
    C. Similarly, if we start at D, the slope of the tangent line is positive (up
    and to the right) meaning we need to move in the negative *x* direction, again
    toward C, to move closer to the minimum. All of this hints at an algorithm for
    finding the minimum of a function: pick a starting point (an *x* value) and use
    the gradient to move to a lower point.'
  prefs: []
  type: TYPE_NORMAL
- en: For simple functions of just *x*, like those of [Figure 9-1](ch09.xhtml#ch9fig1),
    this approach will work nicely, assuming we start in a good place like B or D.
    When we move to more than one dimension, it turns out that this approach will
    still work nicely provided we start in a good place with our initial guess.
  prefs: []
  type: TYPE_NORMAL
- en: Working still with [Figure 9-1](ch09.xhtml#ch9fig1) and assuming we’re starting
    at B, we see that the gradient tells us to move to the right, toward C. But how
    do we select the next *x* value to consider, to move us closer to C? This is the
    step size, and it tells us how big a jump we make from one *x* position to the
    next. Step size is a parameter we have to choose, and in practice this value,
    called the *learning rate*, is often fluid and gets smaller and smaller as we
    move, under the assumption that as we move, we get closer and closer to the minimum
    value and therefore need smaller and smaller steps.
  prefs: []
  type: TYPE_NORMAL
- en: This is all well and good, even intuitive, but we have a small problem. What
    if instead of starting at B or D, we start at A? The gradient at A is pointing
    us to the left, not the right. In this case, our simple algorithm will fail—it
    will move us to the left, and we’ll never reach C. The figure shows only one minimum,
    at C, but we can easily imagine a second minimum, say to the left of A, that doesn’t
    go as low (doesn’t have as small a *y* value) as C. If we start at A, we’ll move
    toward this minimum, and not the one at C. Our algorithm will fall into a *local
    minimum*. Once in, our algorithm can’t get us out, and we won’t be able to find
    the global minimum at C. We’ll see that this is a genuine issue for neural networks,
    but one that for modern deep networks is, almost magically, not much of an issue
    after all.
  prefs: []
  type: TYPE_NORMAL
- en: So how does all of this help us train a neural network? The gradient tells us
    how a small change in *x* changes *y*. If *x* is one of the parameters of our
    network and *y* is the error given by the loss function, then the gradient tells
    us how much a change in that parameter affects the overall error of the network.
    Once we know that, we’re in a position to modify the parameter by an amount based
    on the gradient, and we know that this will move us toward a minimum error. When
    the error over the training set is at a minimum, we can claim that the network
    has been trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s talk a bit more about the gradients and parameters. All of our discussion
    to this point, based on [Figure 9-1](ch09.xhtml#ch9fig1), has been rather one-dimensional;
    our functions are functions of *x* only. We talked about changing one thing, the
    position along the x-axis, to see how it affects the *y* position. In reality,
    we’re not working with just one dimension. Every weight and bias in our network
    is a parameter, and the loss function value depends upon all of them. For the
    simple network in [Figure 8-1](ch08.xhtml#ch8fig1) alone, there are 20 parameters,
    meaning that the loss function is a 20-dimensional function. Regardless, our approach
    remains much the same: if we know the gradient for each parameter, we can still
    apply our algorithm in an attempt to locate a set of parameters minimizing the
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Updating the Weights
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’ll talk about how to get gradient values in a bit, but for the time being
    let’s assume we have them already. We’ll say that we have a set of numbers that
    tells us how, given the current configuration of the network, a change in any
    weight or bias value changes the loss. With that knowledge, we can apply gradient
    descent: we adjust the weight or bias by some fraction of that gradient value
    to move us, collectively, toward a minimum of the entire loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we update each weight and bias using a simple rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w* ← *w* –Δ*w*'
  prefs: []
  type: TYPE_NORMAL
- en: Here *w* is one of the weights (or biases), *η* (eta) is the learning rate (the
    step size), and *Δw* is the gradient value.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9-1](ch09.xhtml#ch9lis1) gives an algorithm for training a neural
    network using gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Pick some intelligent starting values for the weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Run the training set through the network using its current weights and
  prefs: []
  type: TYPE_NORMAL
- en: biases and calculate the average loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Use this loss to get the gradient for each weight and bias.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Update the weight or bias value by the step size times the gradient value.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Repeat from step 2 until the loss is low enough.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9-1: Gradient descent in five (deceptively) simple steps*'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm appears simple, but as they say, the devil is in the details.
    We have to make choices at every step, and every choice we make will prompt further
    questions. For example, step 1 says to “Pick some intelligent starting values.”
    What should they be? It turns out that successfully training a neural network
    depends critically on choosing good initial values. We already saw how this might
    be so in our preceding example using [Figure 9-1](ch09.xhtml#ch9fig1) where if
    we start at A, we won’t find the minimum at C. Much research has been conducted
    over the years related to step 1.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 is straightforward; it’s the forward-pass through the network. We haven’t
    talked in detail about the loss function itself; for now, just think of it as
    a function measuring the effectiveness of the network on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 is a black box for the time being. We’ll explore how to do it shortly.
    For now, assume we can find the gradient values for each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 follows the form of the previous equation that moves the parameter from
    its current value to one that will reduce the overall loss. In practice, the simple
    form of this equation is not sufficient; there are other terms, like momentum,
    that preserve some fraction of the previous weight change for the next iteration
    (next pass of the training data through the network) so that parameters do not
    change too wildly. We’ll revisit momentum later. For now, let’s look at a variation
    of gradient descent, the one that is actually used to train deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous steps describe gradient descent training of a neural network. As
    we might expect, in practice there are many different flavors of this basic idea.
    One that’s in widespread use and works well empirically is called *stochastic
    gradient descent (SGD)*. The word *stochastic* refers to a random process. We’ll
    see next why the word *stochastic* goes before *gradient descent* in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Batches and Minibatches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Step 2 of [Listing 9-1](ch09.xhtml#ch9lis1) says to run the complete training
    set through the network using the current values of the weights and biases. This
    approach is called *batch training*, so named because we use all of the training
    data to estimate the gradients. Intuitively, this is a reasonable thing to do:
    we’ve carefully constructed the training set to be a fair representation of the
    unknown parent process that generates the data, and it’s this parent process we
    want the network to successfully model for us.'
  prefs: []
  type: TYPE_NORMAL
- en: If our dataset is small, like the original iris dataset of [Chapter 5](ch05.xhtml#ch05),
    then batch training makes sense. But what if our training dataset isn’t small?
    What if it’s hundreds of thousands or even millions of samples? We’ll be facing
    longer and longer training times.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve run into a problem. We want a large training set as that will (hopefully)
    better represent the unknown parent process that we want to model. But the larger
    the training set, the longer it takes to pass each sample through the network,
    get an average value for the loss, and update the weights and biases. We call
    passing the entire training set through the network an *epoch*, and we’ll need
    many dozens to hundreds of epochs to train the network. Doing a better job of
    representing the thing we want to model means longer and longer computation times
    because of all the samples that must be passed through the network.
  prefs: []
  type: TYPE_NORMAL
- en: This is where SGD comes into play. Instead of using all the training data on
    each pass, let’s alternatively select a small subset of the training data and
    use the average loss calculated from it to update the parameters. We’ll calculate
    an “incorrect” gradient value because we’re estimating the loss over the full
    training set using only a small sample, but we’ll save a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how this sampling plays out with a simple example. We’ll define a
    vector of 100 random bytes using NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = np.random.normal(128,20,size=100).astype("uint8")'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d'
  prefs: []
  type: TYPE_NORMAL
- en: 130, 141,  99, 106, 135, 119,  98, 147, 152, 163, 118, 149, 122,
  prefs: []
  type: TYPE_NORMAL
- en: 133, 115, 128, 176, 132, 173, 145, 152,  79, 124, 133, 158, 111,
  prefs: []
  type: TYPE_NORMAL
- en: 139, 140, 126, 117, 175, 123, 154, 115, 130, 108, 139, 129, 113,
  prefs: []
  type: TYPE_NORMAL
- en: 129, 123, 135, 112, 146, 125, 134, 141, 136, 155, 152, 101, 149,
  prefs: []
  type: TYPE_NORMAL
- en: 137, 119, 143, 136, 118, 161, 138, 112, 124,  86, 135, 161, 112,
  prefs: []
  type: TYPE_NORMAL
- en: 117, 145, 140, 123, 110, 163, 122, 105, 135, 132, 145, 121,  92,
  prefs: []
  type: TYPE_NORMAL
- en: 118, 125, 154, 148,  92, 142, 118, 128, 128, 129, 125, 121, 139,
  prefs: []
  type: TYPE_NORMAL
- en: 152, 122, 128, 126, 126, 157, 124, 120, 152
  prefs: []
  type: TYPE_NORMAL
- en: Here the byte values are normally distributed around a mean of 128\. The actual
    mean of the 100 values is 130.9\. Selecting subsets of these values, 10 at a time,
    gives us an estimate of the actual mean value
  prefs: []
  type: TYPE_NORMAL
- en: '>>> i = np.argsort(np.random.random(100))'
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d[i[:10]].mean()'
  prefs: []
  type: TYPE_NORMAL
- en: '138.9'
  prefs: []
  type: TYPE_NORMAL
- en: with repeated subsets leading to estimated means of 135.7, 131.7, 134.2, 128.1,
    and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: None of the estimated means are the actual mean, but they are all close to it.
    If we can estimate the mean from a random subset of the full dataset, we can see
    by analogy that we should be able to estimate the gradients of the loss function
    with a subset of the full training set. Since the sample is randomly selected,
    the resulting gradient values are randomly varying estimates. This is why we add
    the word *stochastic* in front of *gradient descent*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because passing the full training set through the network on each weight and
    bias update step is known as *batch training*, passing a subset through is known
    as *minibatch training*. You will hear people use the term *minibatch* quite frequently.
    A minibatch is the subset of the training data used for each stochastic gradient
    descent step. Training is usually some number of epochs, where the relationship
    between epochs and minibatches is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/196equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, we don’t really want to select the minibatches at random from
    the full training set. If we do that, we run the risk of not using all the samples:
    some might never be selected, and others might be selected too often. Typically,
    we randomize the order of the training samples and select fixed-size blocks of
    samples sequentially whenever a minibatch is required. When all available training
    samples are used, we can shuffle the full training set order and repeat the process.
    Some deep learning toolkits don’t even do this; they instead cycle through the
    same set of minibatches again.'
  prefs: []
  type: TYPE_NORMAL
- en: Convex vs. Nonconvex Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGD sounds like a concession to practicality. In theory, it seems that we’d
    never want to use it, and we might expect that our training results will suffer
    because of it. However, the opposite is generally true. In some sense, gradient
    descent training of a neural network shouldn’t work at all because we’re applying
    an algorithm meant for convex functions to one that is nonconvex. [Figure 9-2](ch09.xhtml#ch9fig2)
    illustrates the difference between a convex function and a nonconvex function.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-2: A convex function of* x *(left). A nonconvex function of* x *(right)*.'
  prefs: []
  type: TYPE_NORMAL
- en: A convex function is such that a line segment between any two points on the
    function does not cross the function at any other point. The black line on the
    left of [Figure 9-2](ch09.xhtml#ch9fig2) is one example, and any such segment
    will not cross the function at any other point, indicating that this is a convex
    function. However, the same can’t be said of the curve on the right of [Figure
    9-2](ch09.xhtml#ch9fig2). This is the curve from [Figure 9-1](ch09.xhtml#ch9fig1).
    Here the black line does cross the function.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent is designed to find the minimum when the function is convex,
    and because it relies only on the gradient, the first derivative, it’s sometimes
    known as a *first-order* optimization method. Gradient descent should not work,
    in general, with nonconvex functions because it runs the risk of getting trapped
    in a local minimum instead of finding the global minimum. Again, we saw this with
    the example in [Figure 9-1](ch09.xhtml#ch9fig1).
  prefs: []
  type: TYPE_NORMAL
- en: Here’s where stochastic gradient descent helps. In multiple dimensions, the
    gradient will point in a direction that isn’t necessarily toward the nearest minimum
    of the loss function. This means that our step will be in a slightly wrong direction,
    but that somewhat wrong direction might help us avoid getting trapped somewhere
    we don’t want to be.
  prefs: []
  type: TYPE_NORMAL
- en: The situation is more complicated, of course, and more mysterious. The machine
    learning community has been struggling with the contradiction between the obvious
    success of using first-order optimization on the nonconvex loss function and the
    fact that it shouldn’t work at all.
  prefs: []
  type: TYPE_NORMAL
- en: Two ideas are emerging. The first is what we just stated, that stochastic gradient
    descent helps by actually moving us in a slightly wrong direction. The second
    idea, which seems to be pretty much proven now, is that, for the loss functions
    used in deep learning, it turns out that there are many, many local minimums and
    that these are all basically the same, so that landing in almost any one of them
    will result in a network that performs well.
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers argue that most gradient descent learning winds up on a *saddle
    point*; this is a place that looks like a minimum but isn’t. Imagine a saddle
    for a horse and place a marble in the middle. The marble will sit in place, but
    you could push the marble in a certain direction and have it roll off the saddle.
    The argument, not without some justification, is that most training ends on a
    saddle point, and better results are possible with a better algorithm. Again,
    however, even the saddle point, if it is one, is still for practical purposes
    a good place to be, so the model is successful regardless.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, then, we should use stochastic gradient descent because it leads
    to better overall learning and reduces the training time by not requiring full
    batches. It does introduce a new hyperparameter, the minibatch size, that we must
    select at some point before training.
  prefs: []
  type: TYPE_NORMAL
- en: Ending Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We haven’t yet discussed a critical question: when should we stop training?
    Remember that in [Chapter 5](ch05.xhtml#ch05), we went through some effort to
    create training sets, validation sets, and test sets. This is where we’ll use
    the validation sets. While training, we can use the accuracy, or some other metric,
    on the validation set to decide when to stop. If using SGD, we typically run the
    validation set through the network for each minibatch or set of minibatches to
    compute the accuracy. By tracking the accuracy on the validation set, we can decide
    when to stop training.'
  prefs: []
  type: TYPE_NORMAL
- en: If we train for a long time, eventually two things usually happen. The first
    is that the error on the training set goes toward zero; we get better and better
    on the training set. The second is that the error on the validation set goes down
    and then, eventually, starts to go back up.
  prefs: []
  type: TYPE_NORMAL
- en: These effects are due to overfitting. The training error goes down and down
    as the model learns more and more to represent the parent distribution that generated
    the dataset. But, eventually, it will stop learning general things about the training
    set. At this point, we’re overfitting, and we want to stop training because the
    model is no longer learning general features and is instead learning minutiae
    about the particular training set we’re using. We can watch for this by using
    the validation set while training. Since we don’t use the samples in the validation
    set to update the weights and biases of the network, it should give us a fair
    test of the current state of the network. When overfitting starts, the error on
    the validation set will begin to go up from a minimum value. What we can do then
    is to keep the weights and biases that produced the minimum value on the validation
    set and claim that those represent the best model.
  prefs: []
  type: TYPE_NORMAL
- en: We don’t want to use any data that has influenced training to measure the final
    effectiveness of our network. We use the validation set to decide when to stop
    training, so characteristics of the samples in the validation set have also influenced
    the final model; this means we can’t strongly rely on the validation set to give
    us an idea of how the model will behave on new data. It’s only the held-out test
    set, unused until we declare victory over training, that gives us some idea of
    how we might expect the model to perform on data in the wild. So, just as it is
    anathema to report training set accuracy as a measure of how good the model is,
    it’s also anathema to report the validation set accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the Learning Rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our generic update equation for changing the weights and biases based on
    the gradient, we introduced a hyperparameter, *η* (eta), the learning rate or
    step size. It’s a scale factor indicating how much we should update the weight
    or bias based on the gradient value.
  prefs: []
  type: TYPE_NORMAL
- en: We previously stated that the learning rate doesn’t need to be fixed and that
    it could, and even should, get smaller and smaller as we train under the assumption
    that we need smaller and smaller steps to get to the actual minimum value of the
    loss function. We didn’t state how we should actually update the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more than one way to update the step size, but some are more helpful
    than others. The MLPClassifier class of sklearn, which uses SGD solvers, has three
    options. The first is to never change the learning rate—just leave *η* at its
    initial value, *η*[0]. The second is to scale *η* so that it decreases with epochs
    (minibatches) according to
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/199equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *η*[0] is set by the user, *t* is the iteration (epoch, minibatch), and
    *p* is an exponent on *t*, also picked by the user. The sklearn default *p* is
    0.5—that is, scale by ![Image](Images/199equ02.jpg), which seems a reasonable
    default.
  prefs: []
  type: TYPE_NORMAL
- en: The third option is to adapt the learning rate by watching the loss function
    value. As long as the loss is decreasing, leave the learning rate where it is.
    When the loss stops decreasing for a set number of minibatches, divide the learning
    rate by some value like 5, the sklearn default. If we never change the learning
    rate, and it’s too large, we might end up moving around the minimum without ever
    being able to reach it because we’re consistently stepping over it. It’s a good
    idea then to decrease the learning rate when using SGD. Later in the book, we’ll
    encounter other optimization approaches that automatically adjust the learning
    rate for us.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There’s one last wrinkle in SGD we have to cover. As we saw previously, the
    weight update equation for both gradient descent and SGD is
  prefs: []
  type: TYPE_NORMAL
- en: '*w* ← *w* – *η*Δ*w*'
  prefs: []
  type: TYPE_NORMAL
- en: We update the weight by the learning rate (*η*) times the gradient, which we
    are representing here as Δ*w*.
  prefs: []
  type: TYPE_NORMAL
- en: A common and powerful trick is to introduce a *momentum* term that adds back
    some fraction of the previous Δ*w*, the update of the prior minibatch. The momentum
    term prevents the *w* parameter from changing too quickly in response to a particular
    minibatch. Adding in this term gives us
  prefs: []
  type: TYPE_NORMAL
- en: '*w*[*i*+1] ← *w*[*i*] – *η*Δ*w*[*i*] + *μ*Δ*w*[*i–1*]'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve added subscripts to indicate the next pass through the network (*i* +
    1), the current pass (*i*), and the previous pass (*i –* 1). The previous pass
    Δ*w* is the one we need to use. A typical value for *μ* (mu), the momentum, is
    around 0.9\. Virtually all toolkits implement momentum in some form, including
    sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve been operating under the assumption that we know the gradient value for
    each parameter. Let’s discuss how the backpropagation algorithm gives us these
    magic numbers. The backpropagation algorithm is perhaps the single most important
    development in the history of neural networks as it enables the training of large
    networks with hundreds, thousands, millions, and even billions of parameters.
    This is especially true of the convolutional networks we’ll work with in [Chapter
    12](ch12.xhtml#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation algorithm itself was published by Rumelhart, Hinton, and
    Williams in 1986 in their paper “Learning Representations by Back-propagating
    Errors.” It’s a careful application of the chain rule for derivatives. The algorithm
    is called *backpropagation* because it works backward from the output layer of
    the network toward the input layer, propagating the error from the loss function
    down to each parameter of the network. Colloquially, the algorithm is known as
    *backprop*; we’ll use that term here, so we sound more like native machine learning
    experts.
  prefs: []
  type: TYPE_NORMAL
- en: Adding backprop into the training algorithm for gradient descent, and tailoring
    it to SGD, gives us the algorithm in [Listing 9-2](ch09.xhtml#ch9lis2).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Pick some intelligent starting values for the weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Run a minibatch through the network using its current weights and biases
  prefs: []
  type: TYPE_NORMAL
- en: and calculate the average loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Use this loss and backprop to get the gradient for each weight and bias.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Update the weight or bias value by the step size times the gradient value.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Repeat from step 2 until the loss is low enough.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9-2: Stochastic gradient descent with backprop*'
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 of [Listing 9-2](ch09.xhtml#ch9lis2) is referred to as the *forward pass*;
    step 3 is the *backward pass*. The forward pass is also how we’ll use the network
    after it’s finally trained. The backward pass is backprop calculating the gradients
    for us so that we can update the parameters in step 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll describe backprop twice. First, we’ll do so with a simple example and
    work with the actual derivatives. Second, we’ll work with a more abstract notation
    to see how backprop applies to actual neural networks in a general sense. There
    is no way to sugarcoat this: this section involves derivatives, but we already
    have a good intuitive sense of what those are from our discussion of gradient
    descent, so we should be in good shape to proceed.'
  prefs: []
  type: TYPE_NORMAL
- en: Backprop, Take 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have two functions, *z* = *f* (*y*) and *y* = *g*(*x*), meaning
    *z* = *f* (*g*(*x*)). We know that the derivative of the function *g* gives us
    *dy*/*dx*, which tells us how *y* changes when *x* changes. Similarly, we know
    that the derivative of the function *f* will give us *dz*/*dy*. The value of *z*
    depends upon the composition of *f* and *g*, meaning the output of *g* is the
    input to *f*, so if we want to find an expression for *dz*/*dx*, how *z* changes
    with *x*, we need a way to link through the composed functions. This is what the
    chain rule for derivatives gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/201equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This notation is especially nice because we can imagine the *dy* “term” canceling
    just as it would if these were actual fractions.
  prefs: []
  type: TYPE_NORMAL
- en: How does this help us? In a neural network, the output of one layer is the input
    to the next, which is composition, so we can see intuitively that the chain rule
    might apply. Remember that we want the values that tell us how the loss function
    changes with respect to the weights and biases. Let’s call the loss function *L*
    and any given weight or bias *w*. We want to calculate *∂*ℒ/*∂w* for all the weights
    and biases.
  prefs: []
  type: TYPE_NORMAL
- en: Alarm bells should be going off in your head. The previous paragraph slipped
    in new notation. So far, we’ve been writing derivatives as *dy*/*dx*, but the
    derivative for the loss with respect to a weight was written as *∂*ℒ/*∂w*. What
    is this fancy *∂*?
  prefs: []
  type: TYPE_NORMAL
- en: When we had a function of one variable, just *x*, there was only one slope at
    a point to talk about. As soon as we have a function with more than one variable,
    the idea of the slope at a point becomes ambiguous. There are an infinite number
    of lines tangent to the function at any point. So we need the idea of the *partial
    derivative*, which is the slope of the line in the direction of the variable we’re
    considering when all other variables are treated as fixed. This tells us how the
    output will change as we change only the one variable. To note that we are using
    a partial derivative, we shift from *d* to *∂*, which is just a script *d*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up a straightforward network so that we can see how the chain rule
    leads directly to the expressions we want. We’re looking at the network in [Figure
    9-3](ch09.xhtml#ch9fig3), which consists of an input, two hidden layers of a single
    node each, and an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-3: A simple network to illustrate the chain rule*'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we’ll ignore any bias values. Additionally, let’s define the
    activation function to be the identity function, *h*(*x*) = *x*. This simplification
    removes the derivative of the activation function to make things more transparent.
  prefs: []
  type: TYPE_NORMAL
- en: For this network, the forward pass computes
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[1] = *w*[1]*x*'
  prefs: []
  type: TYPE_NORMAL
- en: '*h*[2] = *w*[2]*h*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *w*[3]*h*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: which follows the form we’ve used previously, chaining things together by making
    the output of one layer the input to the next. This gives us the output of the
    network, *y*, for input *x*. If we’re looking to train the network, we’ll have
    a training set, a set of pairs, (*x*[*i*], *ŷ*), *i* = 0, 1, …, that are examples
    of what the output should be for a given input. Note that the forward pass moved
    from the input, *x*, to the output, *y*. We’ll next see why the backward pass
    moves from the output to the input.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s define the loss function, *ℒ*, to be the squared error between *y*,
    the network output for a given input *x*, and *ŷ*, the output we should get. Functionally,
    the loss looks like the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/202equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For simplicity, we’re ignoring the fact that the loss is a mean over the training
    set or some minibatch drawn from it. The factor of ![Image](Images/1by2.jpg) is
    not strictly necessary but it’s commonly used to make the derivative a bit nicer.
    Since we’re looking to minimize the loss for a particular set of weights, it doesn’t
    matter that we’re always multiplying the loss by a constant factor of ![Image](Images/1by2.jpg)—the
    smallest loss will still be the smallest loss regardless of its actual numeric
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use gradient descent, we need to find how the loss changes with the weights.
    In this simple network, that means we need to find three gradient values, one
    each for *w*[1], *w*[2], and *w*[3]. This is where the chain rule comes into play.
    We’ll write the equations first and then talk about them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/202equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The order of these equations shows why this algorithm is called *backpropagation*.
    To get the partial derivative for the output layer parameter, we need only the
    output and the loss, *y* and *ℒ*. To get the partial derivative for the middle
    layer weight, we need the following two partial derivatives from the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/203equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, to get the partial derivative for the input layer weight, we need partial
    derivatives from the output and middle layer. In effect, we have moved backward
    through the network propagating values from later layers.
  prefs: []
  type: TYPE_NORMAL
- en: For each of these equations, the right-hand side matches the left-hand side
    if we imagine the “terms” canceling like fractions. Since we selected a particularly
    simple form for the network, we can calculate the actual gradients by hand. We
    need the following gradients, from the right-hand side of the preceding equations.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/203equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *∂ℒ*/*∂y* comes from the form we selected for the loss and the rules of
    differentiation from calculus.
  prefs: []
  type: TYPE_NORMAL
- en: Putting these back into the equations for the gradients of the weights gives
    us
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/204equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After a forward pass, we have numeric values for all the quantities on the right-hand
    side of these equations. Therefore, we know the numeric value of the gradients.
    The update rule from gradient descent then tells us to change the weights like
    the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/204equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *η* is the learning rate parameter defining how large a step to take when
    updating.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we need to use the chain rule, the heart of the backprop algorithm,
    to find the gradients we need to update the weights during training. For our simple
    network, we were able to work out the value of these gradients explicitly by moving
    backward through the network from the output toward the input. Of course, this
    is just a toy network. Let’s now take a second look at how to use backprop in
    a more general sense to calculate the necessary gradients for any network.
  prefs: []
  type: TYPE_NORMAL
- en: Backprop, Take 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s begin by revisiting the loss function and introducing some new notation.
    The loss function is a function of all the parameters in the network, meaning
    that every weight and bias value contributes to it. For example, the loss for
    the network of [Figure 8-1](ch08.xhtml#ch8fig1), which has 20 weights and biases,
    could be written as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/205equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note we’ve introduced a new notation for the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/205equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This represents the weight that links the *j*-th input, an output of the *i
    –* 1 layer, to the *k*-th node of layer *i*. We also have
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/205equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: to represent the bias value for the *k*-th node of the *i*-th layer. Here layer
    0 is the input layer itself. The parentheses on the exponent are a label, the
    layer number; they should not be interpreted as actual exponents. Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/205equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is the weight from the third output of the first layer to the first node of
    the second layer. This is the highlighted weight in [Figure 9-4](ch09.xhtml#ch9fig4).
    Remember that we always number nodes top to bottom, starting with 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-4: The network of [Figure 8-1](ch08.xhtml#ch8fig1) with weight w^((2))[20]
    marked with a bold line*'
  prefs: []
  type: TYPE_NORMAL
- en: This notation is a bit daunting, but it will let us reference any weight or
    bias of the network precisely. The number we need to use backprop is the partial
    derivative of the loss with respect to each weight or bias. Therefore, what we
    want to find ultimately is written, in all its glorious mathematical notation,
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/206equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us the slope: the amount the loss will change for a change in the
    weight linking the *k*-th node of the *i*-th layer to the *j*-th output of the
    *i –* 1 layer. A similar equation gives us the partial derivatives of the biases.'
  prefs: []
  type: TYPE_NORMAL
- en: We can simplify this cumbersome notation by dealing only with the layer number
    understanding that buried in the notation is a vector (biases, activations) or
    matrix (weights) so that we want to find
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/206equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These correspond to a matrix for all the weights linking layer *i –* 1 to *i*,
    and a vector for all the biases of layer *i*, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll protect our notational sanity by looking at things in terms of vectors
    and matrices. Let’s start with the output layer and see what that buys us. We
    know that the activations of the output layer, layer *L*, are found via
  prefs: []
  type: TYPE_NORMAL
- en: '*a*^((*L*)) = *h*(*W*^((*L*))*a*^((*L*–1)) + *b*^((*L*)))'
  prefs: []
  type: TYPE_NORMAL
- en: with *a* being the activations from layer *L –* 1, *b* the bias vector for layer
    *L*, and *W* the weight matrix between layers *L –* 1 and *L*. The activation
    function is *h*.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ll define the argument to *h* to be *z*^((*L*))
  prefs: []
  type: TYPE_NORMAL
- en: '*z*^((*L*)) ≡ *W*^((*L*))*a*^((*L*–1)) + *b*^((*L*))'
  prefs: []
  type: TYPE_NORMAL
- en: and call *∂L*/*∂z*^((*l*)) the *error*, the contribution to the loss from the
    inputs to layer *l*. Next, we define
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/206equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: so that we can work with *δ* (delta) from now on.
  prefs: []
  type: TYPE_NORMAL
- en: For the output layer, we can write *δ* as
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/207equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The notation *h*′(*z*^((*L*))) is another way to write the derivative of *h*
    (with respect to *z*) evaluated at *z*^((*L*)). The ⋅ represents elementwise multiplication.
    This is the way NumPy works when multiplying two arrays of the same size so that
    if *C* = *A* ⋅ *B*, then *C*[*ij*] = *A*[*ij*]*B*[*ij*]. Technically, this product
    is called the *Hadamard product*, named for the French mathematician Jacques Hadamard.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding means that to use backpropagation, we need a loss function that
    can be differentiated—a loss function for which a derivative exists at every point.
    This isn’t too much of a burden; the loss functions we’ll examine in the next
    section meet this criterion. We also need an activation function that can be differentiated
    so we can find *h*(*z*). Again, the activation functions we have considered so
    far are essentially all differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note** *I say “essentially” because the derivative of the ReLU is undefined
    at* x = 0*. The derivative from the left is 0 while the derivative from the right
    is 1\. In practice, implementations choose a particular value to return should
    the argument to the derivative of the ReLU be exactly 0\. For example, TensorFlow
    simply asks if the argument is less than or equal to 0 and, if it is, returns
    0 as the derivative. Otherwise, it returns 1\. This works because, numerically,
    there is so much rounding off happening to floating-point values during calculations
    that it’s unlikely the value passed to the derivative of the ReLU function was
    actually meant to be identically 0.*'
  prefs: []
  type: TYPE_NORMAL
- en: The equation for *δ* tells us the error due to the inputs to a particular layer.
    We’ll see next how to use this to get the error from each weight of a layer.
  prefs: []
  type: TYPE_NORMAL
- en: With *δ*^((*L*)) in hand, we can propagate the error down to the next layer
    via
  prefs: []
  type: TYPE_NORMAL
- en: '*δ*^((*l*)) = ((*W*^((*l*+1)))^(*Tδ*^(*l*+1))) · *h*′(*z*^((*l*)))'
  prefs: []
  type: TYPE_NORMAL
- en: where, for the next-to-last layer, *l* + 1 = *L*. The *T* represents matrix
    transpose. This is a standard matrix operation that involves a reflection across
    the diagonal so that if
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/207equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/207equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We need the transpose of the weight matrix because we are going in the opposite
    direction from the forward pass. If there are three nodes in layer *l* and two
    in layer *l* + 1, then the weight matrix between them, *W*, is a 2 × 3 matrix,
    so *Wx* is a two-element vector. In backprop, we are going from layer *l* + 1
    to layer *l*, so we transpose the weight matrix to map the two-element vector,
    here *δ*, to a three-element vector for layer *l*.
  prefs: []
  type: TYPE_NORMAL
- en: The *δ*^((*l*)) equation is used for every layer moving backward through the
    network. The output layer values are given by *δ*^((*L*)), which starts the process.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the errors per layer, we can finally find the gradient values we
    need. For the biases, the values are the elements of *δ* for that layer
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/208equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for the *j*-th element of the bias for the *l*-th layer. For the weights, we
    need
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/208equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: linking the *k*-th output of the previous layer to the *j*-th error for the
    current layer, *l*.
  prefs: []
  type: TYPE_NORMAL
- en: Using the preceding equations for each layer of the network gives us the set
    of weight and bias gradient values needed to continue applying gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: As I hope you can see from this rather dense section, we can use a convenient
    mathematical definition of the error to set up an iterative process that moves
    the error from the output of the network back through the layers of the network
    to the input layer. We cannot calculate the errors for a layer without already
    knowing the errors for the layer after it, so we end up propagating the error
    backward through the network, hence the name *backpropagation*.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *loss function* is used during training to measure how poorly the network
    is doing. The goal of training is to make this value as small as possible, while
    still generalizing to the true characteristics of the data. In theory, we can
    create any loss function we want if we feel it’s relevant to the problem at hand.
    If you read the deep learning literature, you’ll see papers do this all the time.
    Still, most research falls back on a few standard loss functions that, empirically,
    do a good job most of the time. We’ll discuss three of those here: absolute loss
    (sometimes called *L*[1] loss), mean squared error (sometimes called *L*[2] loss),
    and cross-entropy loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Absolute and Mean Squared Error Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s start with the absolute and mean squared error loss functions. We’ll discuss
    them together because they’re very similar mathematically.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen mean squared error already in our discussion of backprop. Absolute
    loss is new. Mathematically, the two equations are
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/209equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we’ve labeled them *abs* for *absolute value* and *MSE* for *mean squared
    error*, respectively. Note that we’ll always use *y* for the network output, the
    output from the forward pass with input *x*. We’ll always use *ŷ* for the known
    training class label, which is always an integer label starting with 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we’re writing the loss functions in a simple form, we need to remember
    that when used, the value is really the mean of the loss over the training set
    or minibatch. This is also the origin of *mean* in *mean squared error*. Therefore,
    we really should be writing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/209equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here we’re finding the average of the squared error loss over the *N* values
    in the training set (or minibatch).
  prefs: []
  type: TYPE_NORMAL
- en: Both of these loss functions are reasonable if we consider what they are measuring.
    We want the network to output a value that matches the expected value, the sample
    label. The difference between these two is an indication of how wrong the network
    output is. For the absolute loss, we find the difference and drop the sign, which
    is what the absolute value does. For the MSE loss, we find the difference and
    then square it. This also makes the difference positive because multiplying a
    negative number by itself always results in a positive number. As mentioned in
    the “Backpropagation” section on [page 200](#lev1_56), the ![Image](Images/1by2.jpg)
    factor on the MSE loss simplifies the derivative of the loss function but does
    not change how it works.
  prefs: []
  type: TYPE_NORMAL
- en: The absolute loss and MSE are different, however. The MSE is more sensitive
    to outliers. This is because we’re squaring the difference, and a plot of *y*
    = *x*² grows quickly as *x*, the difference, gets larger. For the absolute loss,
    this effect is minimized because there is no squaring; the difference is merely
    the difference.
  prefs: []
  type: TYPE_NORMAL
- en: In truth, neither of these loss functions are commonly used for neural networks
    when the goal of the network is *classification*, which is our implicit assumption
    in this book. It’s more common to use the cross-entropy loss, presented next.
    We want the network output to lead to the correct class label for the input. However,
    it’s entirely possible to train a network to output a continuous real value instead.
    This is called *regression*, and both of these loss functions are quite useful
    in that context.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Entropy Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even though we can use the absolute and MSE loss functions in training a neural
    network for classification, the most commonly used loss is the *cross-entropy
    loss* (closely related to the log-loss). This loss function assumes the output
    of the network is a softmax (vector) for the multiclass case or a sigmoid (logistic,
    scalar) for the binary case. Mathematically, it looks like this for *M* classes
    in the multiclass case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/210equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'What is the cross-entropy doing that often makes it a better choice for training
    a neural network for classification? Let’s think about the multiclass case with
    softmax outputs. The definition of *softmax* means that the network outputs can
    be thought of as probability estimates of the likelihood that the input represents
    each of the possible classes. If we have three classes, we might get a softmax
    output that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = (0.03, 0.87, 0.10)'
  prefs: []
  type: TYPE_NORMAL
- en: This output roughly means that the network thinks there is a 3 percent chance
    the input is of class 0, an 87 percent chance it is of class 1, and a 10 percent
    chance it is of class 2\. This is the output vector, *y*. We compute the loss
    by supplying the actual label via a vector where 0 means *not this class* and
    1 means *this class*. So, the *ŷ* vector associated with the input that led to
    this *y* would be
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ* = (0,1,0)'
  prefs: []
  type: TYPE_NORMAL
- en: for an overall loss value of
  prefs: []
  type: TYPE_NORMAL
- en: '*ℒ*[ent] = –(0(log 0.03) + 1(log 0.87) + 0(log 0.10)) = 0.139262'
  prefs: []
  type: TYPE_NORMAL
- en: The three predictions of the network can be thought of together as a probability
    distribution, just like the one we get when we sum together the likelihoods of
    different outcomes for throwing two dice. We also have a known probability distribution
    from the class label. For the preceding example, the actual class is class 1,
    so we made a probability distribution that assigns no chance to classes 0 and
    2, and 100 percent probability to class 1, the actual class. As the network trains,
    we expect the output distribution to be closer and closer to (0,1,0), the distribution
    for the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing the cross-entropy drives the network toward better and better predictions
    of the probability distribution for the different classes we want the network
    to learn about. Ideally, these output distributions will look like the training
    labels: 0 for all classes except the actual class, which has an output of 1.'
  prefs: []
  type: TYPE_NORMAL
- en: For classification tasks, we usually use the cross-entropy loss. The sklearn
    MLPClassifier class uses cross-entropy. Keras supports cross-entropy loss as well,
    but provides many others, including absolute and mean squared error.
  prefs: []
  type: TYPE_NORMAL
- en: Weight Initialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can train a neural network, we need to initialize the weights and
    biases. Step 1 of [Listing 9-1](ch09.xhtml#ch9lis1) on gradient descent says to
    “Pick some intelligent starting values for the weights and biases.”
  prefs: []
  type: TYPE_NORMAL
- en: The initialization techniques examined here all depend upon selecting random
    numbers in some range. More than that, the random numbers need to be either uniform
    over that range or normally distributed. *Uniformly distributed* means that all
    the values in the range are equally likely to be selected. This is what you get
    for each number, 1 through 6, if you roll a fair die many times over. Normally
    distributed values were introduced in [Chapter 4](ch04.xhtml#ch04). These are
    values with a particular mean, the most likely value returned, and a range around
    the mean over which the likelihood of a value being selected falls off gradually
    toward 0 according to a parameter known as the *standard deviation*. This is the
    classic bell curve shape. Either distribution can be used. The main point is that
    the initial weights are not all the same value (like 0) because if they are, all
    gradients will be the same during backprop, and each weight will change in the
    same way. The initial weights need to be different to break this symmetry and
    allow individual weights to adapt themselves to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: In the early days of neural networks, people initialized the weights and biases
    by choosing values in [0,1) uniformly (*U*(0,1)) or by drawing them from the standard
    normal distribution, *N*(0,1), with a mean of 0 and a standard deviation of 1\.
    These values were often multiplied by some small constant, like 0.01\. In many
    cases, this approach works, at least for simple networks. However, as networks
    became more complex, this simple approach fell apart. Networks initialized in
    this way had trouble learning, and many failed to learn at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fast-forward several decades and a great deal of research later. Researchers
    realized that precisely how the weights of a particular layer should be initialized
    depended primarily on a few things: the type of activation function used and the
    number of weights coming into the layer (*f*[*in*]) and, possibly, going out (*f*[*out*]).
    These realizations led to the main initialization approaches in use today.'
  prefs: []
  type: TYPE_NORMAL
- en: The sklearn MLPClassifier class uses *Glorot initialization*. This is also sometimes
    called *Xavier initialization*, though some toolkits mean something different
    when they use that term.^([1](ch09.xhtml#ch09fn1)) (Note *Xavier* and *Glorot*
    actually refer to the same person.) Let’s see how sklearn uses Glorot initialization.
    The key method in MLPClassifier for initializing the weights is _init_coef. This
    method uses a uniform distribution and sets the range for it so that the weights
    are in
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/212equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the bracket notation indicates the smallest possible value selected (left)
    to the largest possible value (right). As the distribution is uniform, every value
    in that range is equally likely to be selected.
  prefs: []
  type: TYPE_NORMAL
- en: We did not yet specify what *A* is. This value depends upon the activation function
    used. According to the literature, if the activation function is a sigmoid (logistic),
    then *A* = 2 is suggested. Otherwise, *A* = 6 is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Now to confuse things. Some toolkits, like Caffe, use an alternate form of Xavier
    initialization by which they mean a multiplier on samples from a standard normal
    distribution. In that case, we initialize the weights with draws from
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/212equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To add even more confusion, the introduction of the rectified linear unit (ReLU)
    resulted in a further recommended change. This is known now as *He initialization*
    and it replaces the 1 in Xavier initialization with a 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/212equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For more on this, see “Delving Deep into Rectifiers: Surpassing Human-Level
    Performance on ImageNet Classification” by Kaiming He et al.'
  prefs: []
  type: TYPE_NORMAL
- en: The key point with these initialization schemes is that the old-school “small
    random value” is replaced by a more principled set of values that take the network
    architecture into account via *f*[*in*] and *f*[*out*].
  prefs: []
  type: TYPE_NORMAL
- en: The preceding discussion ignored bias values. This was intentional. While it
    might be okay to initialize the bias values instead of leaving them all 0, prevailing
    wisdom, which is fickle and fluid, currently says it’s best to initialize them
    all to 0\. That said, sklearn MLPClassifier initializes the bias values in the
    same way as the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of training a model is for it to learn essential, general features
    of the parent distribution the dataset is sampled from. That way, when the model
    encounters new inputs, it’s prepared to interpret them correctly. As we’ve seen
    in this chapter, the primary method for training a neural network involves optimization—looking
    for the “best” set of parameters so that the network makes as few errors as possible
    on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s not enough to simply look for the best set of values that minimizes
    the training error. If we make no mistakes when classifying the training data,
    it’s often the case that we’ve overfitted and haven’t actually learned general
    features of the data. This is more likely the situation with traditional models,
    neural network or classical, and less so with deep models like the convolutional
    networks of [Chapter 12](ch12.xhtml#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Overfitting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve mentioned overfitting from time to time before now but have not gained
    any good intuition as to what it is. One way to think of overfitting is to consider
    a separate problem, the problem of fitting a function to a set of points. This
    is known as *curve fitting*, and one approach to it is to optimize some measure
    of error over the points by finding parameters to the function that minimize the
    error. This should sound familiar. It’s exactly what we do when training a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of curve fitting, consider the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *x* | *y* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.00 | 50.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.61 | –17.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.22 | 74.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.83 | 29.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.44 | 114.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.06 | 55.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3.67 | 66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.28 | 89.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.89 | 128.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 5.51 | 180.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 6.12 | 229.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 6.73 | 229.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 7.34 | 227.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 7.95 | 354.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 8.57 | 477.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9.18 | 435.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 9.79 | 470.1 |'
  prefs: []
  type: TYPE_TB
- en: We want to find a function, *y* = *f* (*x*), that describes these points—a function
    that might have been the parent function these points were measured from, albeit
    noisily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, when curve fitting, we already know the form of the function; it’s
    the parameters we’re looking for. But what if we didn’t know the exact form of
    the function, only that it was some kind of polynomial? In general, a polynomial
    looks like this for some maximum exponent, *n*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*² + *a*[3]*x*³ + … + *a[n]x^n*'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of fitting a polynomial to a dataset is to find the parameters, *a*[0],*a*[1],*a*[2],…,*a*[*n*].
    The method for doing this usually minimizes the squared difference between *y*,
    a given output for a given *x* position, and *f* (*x*), the function output at
    the same *x* for the current set of parameters. This should sound very familiar,
    as we discussed using precisely this type of loss function for training a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: How does this relate to overfitting? Let’s plot the previous dataset, along
    with the result of fitting two different functions to it. The first function is
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*²'
  prefs: []
  type: TYPE_NORMAL
- en: which is a quadratic function, the type of function you may have learned to
    hate as a beginning algebra student. The second function is
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*² + *a*[3]*x*³ + … + *a*[14]*x*^(14) +
    *a*[15]*x*^(15)'
  prefs: []
  type: TYPE_NORMAL
- en: which is a 15th-degree polynomial. The results are shown in [Figure 9-5](ch09.xhtml#ch9fig5).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-5: A dataset and two functions fit to it: a quadratic (dashed) and
    a 15th-degree polynomial (solid)*'
  prefs: []
  type: TYPE_NORMAL
- en: Which function does a better job of capturing the general trend of the dataset?
    The quadratic clearly follows the general trend of the data, while the 15th-degree
    polynomial is all over the place. Look again at [Figure 9-5](ch09.xhtml#ch9fig5).
    If all we use to decide that we have fit the data well is the distance between
    the data points and the corresponding function value, we’d say that the 15th-degree
    polynomial is the better fit; it passes through nearly all the data points, after
    all. This is analogous to training a neural network and achieving perfection on
    the training set. The cost of that perfection might well be a poor ability to
    generalize to new inputs. The quadratic fit of [Figure 9-5](ch09.xhtml#ch9fig5)
    did not hit the data points, but it did capture the general trend of the data,
    making it more useful should we want to make predictions about the *y* values
    we’d expect to get for a new *x* value.
  prefs: []
  type: TYPE_NORMAL
- en: When a human wants to fit a curve to something like our sample dataset, they
    usually look at the data and, noticing the general trend, select the function
    to fit. It might also be the case that the expected functional form is already
    known from theory. If we want to be analogous to neural networks, however, we’ll
    find ourselves in a situation where we don’t know the proper function to fit,
    and need to find a “best” one from the space of functions of *x* along with its
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this example drives home the idea that training a neural network
    is not an optimization problem like other optimization problems—we need something
    to push the function the network is learning in a direction that captures the
    essence of the data without falling into the trap of paying too much attention
    to specific features of the training data. That something is regularization, and
    you need it, especially for large networks that have a huge capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Regularization* is anything that pushes the network to learn the relevant
    features of the parent distribution and not the details of the training set. The
    best form of regularization is increasing the size and representative nature of
    the training set. The larger the dataset and the better it represents all the
    types of samples the network will encounter in the wild, the better it will learn.
    Of course, we’re typically forced to work with a finite training set. The machine
    learning community has spent, and is spending, untold time and energy learning
    how to get more from smaller datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.xhtml#ch05), we encountered perhaps the second-best way
    to regularize a model, data augmentation. This is a proxy for having a larger
    dataset, where we use the data we do have to generate new training samples that
    are plausibly from the parent distribution. For example, we considered increasing
    a limited set of training images by simple rotations, flips, and shifts of the
    images already in the training set. Data augmentation is powerful, and you should
    use it when possible. It’s particularly easy to apply when working with images
    as inputs, though in [Chapter 5](ch05.xhtml#ch05) we also saw a way to augment
    a dataset consisting of continuously valued vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have two tricks in our regularization toolbox: more data and data augmentation.
    These are the best tricks to know, but there others that you should use when available.
    Let’s look at two more: L2 regularization and dropout. The former is now standard
    and widely supported by the toolkits, including sklearn and Keras. The latter
    is powerful and was a game changer when it appeared in 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A model with a few weights that have large values is somehow less simple than
    a model that has smaller weights. Therefore, keeping the weights small will hopefully
    allow the network to implement a simpler function better suited to the task we
    want it to learn.
  prefs: []
  type: TYPE_NORMAL
- en: We can encourage the weights to be small by using L2 regularization. *L2 regularization*
    adds a term to the loss function so that the loss becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/216equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the first term is whatever loss we’re already using, and the second term
    is the new L2 regularization term. Notice that the loss is a function of the input
    (*x*), the label (*y*), the weights (*w*), and the biases (*b*), where we mean
    all the weights and all the biases of the network. The regularization term is
    a sum over all the weights in the network and only the weights. The “L2” label
    is what causes us to square the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here *L2* refers to the type of norm or distance. You might be familiar with
    the equation for the distance between two points on the plane: *d*² = (*x*[2]
    *– x*[1])² + (*y*[2] *– y*[1])². This is the *Euclidean distance*, also known
    as the *L2 distance*, because the values are squared. This is why the regularization
    term is called *L2* and the weight values are squared. It’s also possible to use
    an L1 loss term, where instead of squaring the weights, one uses the absolute
    value. In practice, L2 regularization is more common and, at least empirically,
    seems to work better for neural network classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: The *λ* (lambda) multiplier sets the importance of this term; the larger it
    is, the more it dominates the overall loss used to train the network. Typical
    values of *λ* are around 0.0005\. We’ll see in a little bit why the multiplier
    is *λ*/2 and not just *λ*.
  prefs: []
  type: TYPE_NORMAL
- en: What is the L2 term doing? Recall that the loss is the thing we want to minimize
    while training. The new L2 term sums the squares of the weights of the network.
    If weights are large, the loss is large, and that’s something we don’t want while
    training. Smaller weights make the L2 term smaller, so gradient descent will favor
    small weights, whether they are positive or negative, since we square the weight
    value. If all the weights of the network are relatively small, and none strongly
    dominate, then the network will use all of the weights to represent the data,
    and this is a good thing when it comes to preventing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization is also known as *weight decay* because of what the L2 term
    does during backprop. Backprop gives us the partial derivative of the loss function
    with respect to *w*[*i*]. Adding L2 regularization means that the partial derivative
    of the total loss now adds in the partial derivative of the L2 term itself with
    respect to any particular weight, *w*[*i*]. The derivative of ![Image](Images/217equ01.jpg)
    is *λw*; the ![Image](Images/1by2.jpg) cancels the factor of 2 that would otherwise
    be there. Also, since we want the partial derivative with respect to a specific
    weight, *w*[*i*], all the other parts of the L2 term go to 0\. The net effect
    is that the update for weight *w*[*i*] during gradient descent becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/217equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *η* (eta) is the learning rate, and we are ignoring any additional momentum
    term. The *ηλw*[*i*] term is new. It is due to L2 regularization, and we can see
    that it’s pushing the weights toward 0 as training progresses because both *η*
    and *λ* are < 1, so on each minibatch, we’re subtracting some small fraction of
    the weight value. The weight can still increase, but to do so, the gradient of
    the original loss must be large.
  prefs: []
  type: TYPE_NORMAL
- en: We previously stated that the form of the loss function is up to us, the developer
    of the network. A regularization term isn’t the only kind of term we can add to
    the loss function. As we did with the L2 term, we can create and add terms to
    change the behavior of the network during training and help it learn what we want
    it to learn. This is a powerful technique that can be used to customize various
    aspects of what a neural network learns.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dropout took the machine learning community by storm when it appeared in 2012,
    see “Imagenet Classification with Deep Convolutional Neural Networks” by Alex
    Krizhevsky et al. As of Fall 2020, this paper has been cited over 70,000 times,
    and as one well-known machine learning researcher told me privately at the time,
    “If we had had dropout in the 1980s, this would be a different world now.” So,
    what is dropout, and why was everyone so excited by it?
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer that question, we need to review the concept of ensembles of models.
    We talked about them a bit in [Chapter 6](ch06.xhtml#ch06). An ensemble is a group
    of models, all slightly different and all trained on the same dataset or a slightly
    different version of the dataset. The idea is straightforward: since training
    most models involves randomness, training multiple similar models should result
    in a set that is mutually reinforcing—one where the set of outputs can be combined
    to produce a result that is better than any one model alone. Ensembles are useful,
    and we use them often, but they come at a price in terms of runtime. If it takes
    *x* milliseconds to run a sample through a neural network, and we have an ensemble
    of 20 networks, then our evaluation time (inference time) has jumped to 20*x*
    milliseconds, ignoring the possibility of parallel execution. In some situations,
    that is unacceptable (to say nothing of the storage and power requirements for
    20 big networks versus 1). Since the net result of an ensemble of models is better
    overall performance, we can say that an ensemble is a kind of regularizer as well
    since it embodies the “wisdom of the crowd.”'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dropout* takes the ensemble idea to an extreme but does so only during training
    and without creating a second network so that in the end, we still have one model
    to deal with. Like many good ideas in statistics, this one requires randomness.
    Right now, when we train the network, we do a forward pass using the current weights
    and biases. What if, during that forward pass, we randomly assign a 0 or a 1 to
    each node of the network so that nodes with a 1 are used in the next layer while
    nodes with a 0 are dropped out? We’d effectively be running the training samples
    through a different neural network configuration each time. For example, see [Figure
    9-6](ch09.xhtml#ch9fig6).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/09fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 9-6: Possible networks used when applying dropout during training*'
  prefs: []
  type: TYPE_NORMAL
- en: Here we show the network of [Figure 8-1](ch08.xhtml#ch8fig1) but with a 0 or
    1 for each of the hidden nodes. This 0 or 1 determines whether the output is used
    or not. The heavy lines in the network show the connections that are still valid.
    In other words, the heavy lines show the network that was actually used to create
    the output accumulated for backprop. If we do this for each training sample, we
    can readily see that we’ll be training a vast number of neural networks, each
    trained on a single sample. Moreover, since the weights and biases persist between
    forward passes, all the networks will share those weights in the hope that the
    process will reinforce good weight values that represent the essence of the dataset.
    As we’ve mentioned several times in this chapter, learning the essence of the
    data is the goal of training; we want to generalize well to new data from the
    same virtual parent distribution that generated the training set in the first
    place. Dropout is serious regularization.
  prefs: []
  type: TYPE_NORMAL
- en: I previously said that we “randomly assign a 0 or a 1” to the nodes. Do we assign
    them equally? The probability with which we drop nodes in a layer is something
    we get to specify. Let’s call it *p*. Typically, *p* = 0.5, meaning about 50 percent
    of the nodes in a layer will be dropped for each training sample. Setting *p*
    = 0.8 would drop 80 percent of the nodes, while *p* = 0.1 would drop only 10 percent.
    Sometimes a different probability is used for different layers of the network,
    especially the first input layer, which should use a smaller probability than
    the hidden nodes. If we drop too many of the inputs, we’ll lose the source of
    the signal we’re trying to get the network to recognize. Dropout applied to the
    input layer can be thought of as a form of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, dropout is training a large set of networks that share weights.
    The output of each of these networks can be combined with the others via a geometric
    mean, assuming we use a softmax output. The geometric mean of two numbers is the
    square root of their product. The geometric mean of *n* numbers is the *n*th root
    of their product. In the case of dropout, it turns out that this can be approximated
    by using the entire network with all the weights multiplied by the probability
    that they would be included. Given we said *p* is the probability that a node
    is dropped, the weights need to be multiplied by 1 *– p*, as that is the probability
    the node would not be dropped. So, if we fix *p* = 0.5 and use it for all the
    nodes, then the final network is the one where all the weights are divided by
    2.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, sklearn’s MLPClassifier class does not support dropout,
    but Keras most certainly does, so we’ll see dropout again in [Chapter 12](ch12.xhtml#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because this is an important chapter, let’s review what we’ve learned in a
    little more depth. In this chapter, we described how to train a neural network
    using gradient descent and backpropagation. The overall sequence of steps is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the architecture of the model. This means the number of layers, their
    sizes, and the type of activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the weights and biases of the network using intelligently selected
    initial values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a minibatch of training samples through the network and compute the mean
    loss over the minibatch. We discussed common loss functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using backpropagation, calculate the contribution of each weight and bias to
    the overall loss for the minibatch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using gradient descent, update the weight and bias values of the model based
    on the contributions found via backpropagation. We discussed stochastic gradient
    descent and its relationship to the concept of minibatches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 3 until the desired number of epochs or minibatches have been
    processed, or the loss has dropped below some threshold, or stopped changing much,
    or when the score on a validation set of samples has reached its minimum value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the network isn’t learning well, apply regularization and train again. We
    looked at L2 regularization and dropout in this chapter. Data augmentation, or
    increasing the size or representativeness of the training set, can also be thought
    of as regularization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal of training a neural network is to learn the parameters of a model
    that generalizes well to unseen inputs. This is the goal of all supervised machine
    learning. For a neural network, we know it’s able to approximate any function,
    with enough capacity and enough training data. Naïvely, we may think that we are
    doing nothing more than ordinary optimization, but, in an important sense, we
    are not. Perfection on the training set is often not a good thing; it’s often
    a sign of overfitting. Instead, we want the model to learn a function that captures
    the essential nature of the function implied by the training set. We use the test
    data to give us confidence that we’ve learned a useful function.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll get real and explore traditional neural networks
    through a series of experiments using sklearn.
  prefs: []
  type: TYPE_NORMAL
- en: '[1.](ch09.xhtml#Rch09fn1) For more on these, see Glorot, Xavier, and Yoshua
    Bengio. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.”'
  prefs: []
  type: TYPE_NORMAL
