- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9'
- en: TRAINING A NEURAL NETWORK**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: 'In this chapter, we’ll discuss how to train a neural network. We’ll look at
    the standard approaches and tricks being used in the field today. There will be
    some math, some hand-waving, and a whole host of new terms and concepts. But you
    don’t need to follow the math at a deep level: we’ll gloss over things as needed
    to get the main point across.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论如何训练神经网络。我们将研究当前领域中使用的标准方法和技巧。会有一些数学、一些手势表达，还有一大堆新术语和概念。但你不需要深入跟进数学部分：我们会适当地跳过一些内容，以便传达主要观点。
- en: This chapter is perhaps the most challenging in the book, at least conceptually.
    It certainly is mathematically. While it’s crucially important to building intuition
    and understanding, sometimes we get impatient and like to dive into things first
    to test the waters. Thanks to preexisting libraries, we can do that here. If you
    want to play around with neural networks before learning how they work, jump to
    [Chapter 10](ch10.xhtml#ch10) before coming back here to fill in the theory. But
    do come back.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章可能是本书中最具挑战性的一章，至少在概念上是如此。从数学上来说，绝对是。虽然这对建立直觉和理解至关重要，但有时我们会感到不耐烦，喜欢先跳进实操中试探一下。幸运的是，由于已有的库，我们可以在这里做到这一点。如果你想在学习它们如何工作之前先玩玩神经网络，可以跳到[第十章](ch10.xhtml#ch10)，然后再回来填补理论。但一定要回来。
- en: It’s possible to learn to use powerful toolkits like sklearn and Keras without
    understanding how they work. That approach should not satisfy anyone, though the
    temptation is real. Understanding how these algorithms work is well worth your
    time.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能在不理解它们如何工作的情况下学习使用像 sklearn 和 Keras 这样的强大工具包。然而，这种方法应该不会满足任何人，尽管诱惑很大。理解这些算法如何工作是非常值得花时间的。
- en: A High-Level Overview
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级概览
- en: Let’s begin this chapter with an overview of the concepts we’ll discuss. Read
    it, but don’t fret if the concepts are unclear. Instead, try to get a feel for
    the overall process.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从概述我们将讨论的概念开始这一章的内容。读一读它，但如果这些概念不清楚，不用担心。相反，尽量感受一下整个过程。
- en: The first step in training a neural network is selecting intelligent initial
    values for the weights and biases. We then use *gradient descent* to modify these
    weights and biases so that we reduce the error over the training set. We’ll use
    the average value of the loss function to measure the error, which tells us how
    wrong the network currently is. We know if the network is right or wrong because
    we have the expected output for each input sample in the training set (the class
    label).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的第一步是为权重和偏置选择智能的初始值。然后，我们使用*梯度下降*来调整这些权重和偏置，从而减少训练集上的误差。我们将使用损失函数的平均值来衡量误差，这告诉我们网络当前的错误程度。我们知道网络是否正确，因为我们有每个输入样本在训练集中的预期输出（即类标签）。
- en: Gradient descent is an algorithm that requires gradients. For now, think of
    gradients as measures of steepness. The larger the gradient, the steeper the function
    is at that point. To use gradient descent to search for the smallest value of
    the loss function, we need to be able to find gradients. For that, we’ll use *backpropagation*.
    This is the fundamental algorithm of neural networks, the one that allows them
    to learn successfully. It gives us the gradients we need by starting at the output
    of the network and moving back through the network toward the input. Along the
    way, it calculates the gradient value for each weight and bias.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种需要梯度的算法。现在，可以把梯度看作是陡峭度的度量。梯度越大，函数在该点的陡峭度就越大。为了使用梯度下降搜索损失函数的最小值，我们需要能够找到梯度。为此，我们将使用*反向传播*。这是神经网络的基本算法，它使得神经网络能够成功学习。它通过从网络的输出开始，向输入方向反向传播，通过计算每个权重和偏置的梯度值来为我们提供所需的梯度。
- en: With the gradient values, we can use the gradient descent algorithm to update
    the weights and biases so that the next time we pass the training samples through
    the network, the average of the loss function will be less than it was before.
    In other words, our network will be less wrong. This is the goal of training,
    and we hope it results in a network that has learned general features of the data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过梯度值，我们可以使用梯度下降算法更新权重和偏置，这样下次我们将训练样本传入网络时，损失函数的平均值将比之前小。换句话说，我们的网络会变得更少出错。这就是训练的目标，我们希望它能带来一个已经学会数据一般特征的网络。
- en: Learning general features of the dataset requires *regularization*. There are
    many approaches to regularization, and we’ll discuss the main ones. Without regularization,
    the training process is in danger of overfitting, and we could end up with a network
    that doesn’t generalize. But with regularization, we can be successful and get
    a useful model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 学习数据集的通用特征需要进行*正则化*。正则化有许多方法，我们将讨论其中的主要几种。如果没有正则化，训练过程就可能面临过拟合的风险，最终可能得到一个无法泛化的网络。而通过正则化，我们可以取得成功，并得到一个有用的模型。
- en: So, the following sections introduce gradient descent, backpropagation, loss
    functions, weight initialization, and, finally, regularization. These are the
    main components of successful neural network training. We don’t need to understand
    these in all their gory mathematical details; instead, we need to understand them
    conceptually so we can build an intuitive approach to what it means to train a
    neural network. With this intuition, we’ll be able to make meaningful use of the
    parameters that sklearn and Keras give us for training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍梯度下降、反向传播、损失函数、权重初始化以及最后的正则化。这些是成功的神经网络训练的主要组成部分。我们不需要了解它们所有的复杂数学细节；相反，我们需要从概念上理解它们，这样我们才能建立起一种直观的理解，知道训练神经网络的含义。有了这种直觉，我们就能有意义地使用sklearn和Keras提供的参数来进行训练。
- en: Gradient Descent
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: The standard way to train a neural network is to use gradient descent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的标准方法是使用梯度下降。
- en: 'Let’s parse the phrase *gradient descent*. We already know what the word *descent*
    means. It means to go down from somewhere higher up. What about *gradient*? The
    short answer is that a gradient indicates how quickly something changes with respect
    to how fast something else changes. Measuring how much one thing changes as another
    changes is something we’re all familiar with. We all know about speed, which is
    how position changes as time changes. We even say it in words: *miles per hour*
    or *kilometers per hour*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解析一下“*梯度下降*”这个短语。我们已经知道“*下降*”意味着什么，它表示从更高的地方向下走。那么“*梯度*”呢？简短的答案是，梯度表示某事物相对于另一样事物的变化速度。测量一件事随着另一件事变化的程度是我们都熟悉的事情。我们都知道速度，它是位置随着时间变化的速度。我们甚至会用语言表达出来：*每小时多少英里*或*每小时多少公里*。
- en: You’re probably already familiar with the gradient in another context. Consider
    the equation of a line
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经在其他上下文中熟悉了梯度。考虑直线的方程
- en: '*y* = *mx* + *b*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *mx* + *b*'
- en: where *m* is the slope and *b* is the y-axis intercept. The slope is how quickly
    the line’s *y* position changes with each change in the *x* position. If we know
    two points that are on the line, (*x*[0],*y*[0]) and (*x*[1],*y*[1]), then we
    can calculate the slope as
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m* 是斜率，*b* 是 y 轴截距。斜率表示直线的 *y* 位置随着 *x* 位置变化的速度。如果我们知道直线上的两个点，(*x*[0], *y*[0])
    和 (*x*[1], *y*[1])，那么我们可以通过以下公式计算斜率：
- en: '![image](Images/191equ01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/191equ01.jpg)'
- en: 'which, in words, we might say as “*y*’s per *x*.” It’s a measure of how steep
    or shallow the line is: its gradient. In mathematics, we often talk about a change
    in a variable, and the notation for that is to put a Δ (delta) in front. So, we
    might write the slope of a line as'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用语言描述的话，我们可以说是“*y* 对 *x* 的变化”。它是衡量直线陡峭或平缓的程度：即它的梯度。在数学中，我们经常讨论变量的变化，表示这种变化的符号是在前面加一个Δ（delta）。所以，我们可能会写出直线的斜率为
- en: '![image](Images/191equ02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/191equ02.jpg)'
- en: to drive home the point that the slope is the change in *y* for each change
    in *x*. Fortunately for us, it turns out that not only do lines have a slope at
    each point, but also most functions have a slope at each point. However, except
    for straight lines, this slope changes from point to point. A picture will help
    here. Consider [Figure 9-1](ch09.xhtml#ch9fig1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调斜率是每次 *x* 变化时 *y* 的变化量。幸运的是，事实证明，不仅直线在每个点都有斜率，大多数函数在每个点也有斜率。然而，除了直线外，这个斜率会随着点的位置而变化。这里用一张图来帮助理解。请参考[图
    9-1](ch09.xhtml#ch9fig1)。
- en: '![image](Images/09fig01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig01.jpg)'
- en: '*Figure 9-1: A function with several tangent lines indicated*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-1：一个具有多个切线的函数*'
- en: The graph in [Figure 9-1](ch09.xhtml#ch9fig1) is of a polynomial. Notice the
    lines drawn on the figure that are just touching the function. These are *tangent*
    lines. And as lines, they have a slope we can see in the plot. Now imagine moving
    one of the lines over the function so that it continues to touch the function
    at only one point; imagine how the slope of the line changes as it moves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1](ch09.xhtml#ch9fig1)中的图是一个多项式图。注意图中那些与函数仅有一个接触点的线条，这些是*切线*。作为直线，它们的斜率可以在图中看到。现在，想象将其中一条线移动到函数上，使其继续仅在一个点上接触函数；想象当这条线移动时，斜率是如何变化的。'
- en: It turns out that how the slope changes over the function is itself a function,
    and it’s called the *derivative*. Given a function and *x* value, the derivative
    tells us the slope of the function at that point, *x*. The fact that functions
    have derivatives is a fundamental insight of calculus, and of fundamental importance
    to us.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，斜率如何随着函数变化而变化本身也是一个函数，这个函数叫做*导数*。给定一个函数和 *x* 值，导数告诉我们该点 *x* 处的函数斜率。函数有导数这一事实是微积分的一个基本洞察，对我们来说非常重要。
- en: The notion of a derivative is essential because for single variable functions,
    the derivative at the point *x* is the gradient at *x*; it’s the direction in
    which the function is changing. If we want to find the minimum of the function,
    the *x* that gives us the smallest *y*, we want to move in the direction *opposite*
    to the gradient as that will move us in the direction of the minimum.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 导数的概念至关重要，因为对于单变量函数，点 *x* 处的导数就是该点的梯度；它代表了函数变化的方向。如果我们想找到函数的最小值，即给我们最小 *y* 的
    *x*，我们需要朝着梯度的*相反方向*移动，因为这会把我们带向最小值的方向。
- en: The derivative is written in many different ways, but the way that echoes the
    idea of the slope, how *y* changes for a change in *x*, is
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 导数有许多不同的写法，但最能体现斜率概念，即 *y* 如何随 *x* 的变化而变化的写法是：
- en: '![image](Images/192equ01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/192equ01.jpg)'
- en: We’ll return to this form next when discussing the backpropagation algorithm.
    That’s it for the gradient; now let’s take a closer look at descent.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下文讨论反向传播算法时回到这种形式。梯度部分讲解完毕，现在让我们更深入地了解下降法。
- en: Finding Minimums
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 寻找最小值
- en: Since we want a model that makes few mistakes, we need to find the set of parameters
    that lead to a small value for the loss function. In other words, we need to find
    a *minimum* of the loss function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要一个能够尽量减少错误的模型，我们需要找到一组参数，使得损失函数的值尽可能小。换句话说，我们需要找到损失函数的*最小值*。
- en: 'Look again at [Figure 9-1](ch09.xhtml#ch9fig1). The minimum is on the right,
    where tangent line C is. We can see it’s the minimum, and notice that the gradient
    is 0 there. This tells us we’re at a minimum (or maximum). If we start at B, we
    see that the slope of the tangent line is negative (down and to the right). Therefore,
    we need to move to an *x* value in the positive direction because this is opposite
    to the sign of the gradient. Doing this will take us closer to the minimum at
    C. Similarly, if we start at D, the slope of the tangent line is positive (up
    and to the right) meaning we need to move in the negative *x* direction, again
    toward C, to move closer to the minimum. All of this hints at an algorithm for
    finding the minimum of a function: pick a starting point (an *x* value) and use
    the gradient to move to a lower point.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看一下[图 9-1](ch09.xhtml#ch9fig1)。最小值位于右侧，切线 C 所在的位置。我们可以看到这是最小值，并且注意到此处的梯度为
    0。这告诉我们我们处于一个最小值（或最大值）处。如果我们从 B 开始，我们看到切线的斜率是负的（向下且朝右）。因此，我们需要朝着正方向的 *x* 值移动，因为这与梯度的符号相反。这样做会使我们更接近
    C 处的最小值。类似地，如果我们从 D 开始，切线的斜率是正的（向上且朝右），意味着我们需要朝着负 *x* 方向移动，再次朝着 C 移动，以便接近最小值。所有这些都暗示着一种寻找函数最小值的算法：选择一个起点（*x*
    值），然后使用梯度移动到更低的点。
- en: For simple functions of just *x*, like those of [Figure 9-1](ch09.xhtml#ch9fig1),
    this approach will work nicely, assuming we start in a good place like B or D.
    When we move to more than one dimension, it turns out that this approach will
    still work nicely provided we start in a good place with our initial guess.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像[图 9-1](ch09.xhtml#ch9fig1)中那样的简单 *x* 函数，这种方法可以很好地工作，前提是我们从一个好的位置开始，比如 B
    或 D。当我们扩展到多维空间时，事实证明，只要我们从一个合适的初始猜测开始，这种方法仍然有效。
- en: Working still with [Figure 9-1](ch09.xhtml#ch9fig1) and assuming we’re starting
    at B, we see that the gradient tells us to move to the right, toward C. But how
    do we select the next *x* value to consider, to move us closer to C? This is the
    step size, and it tells us how big a jump we make from one *x* position to the
    next. Step size is a parameter we have to choose, and in practice this value,
    called the *learning rate*, is often fluid and gets smaller and smaller as we
    move, under the assumption that as we move, we get closer and closer to the minimum
    value and therefore need smaller and smaller steps.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用[图9-1](ch09.xhtml#ch9fig1)，假设我们从B开始，我们可以看到梯度告诉我们向右移动，朝C方向前进。但我们如何选择下一个 *x*
    值，以便将我们带得更接近C呢？这就是步长，它告诉我们从一个 *x* 位置到下一个 *x* 位置该跳多远。步长是我们必须选择的参数，实际上，这个值，称为 *学习率*，通常是流动的，随着我们的移动，它会越来越小，假设随着我们接近最小值，我们需要越来越小的步伐。
- en: This is all well and good, even intuitive, but we have a small problem. What
    if instead of starting at B or D, we start at A? The gradient at A is pointing
    us to the left, not the right. In this case, our simple algorithm will fail—it
    will move us to the left, and we’ll never reach C. The figure shows only one minimum,
    at C, but we can easily imagine a second minimum, say to the left of A, that doesn’t
    go as low (doesn’t have as small a *y* value) as C. If we start at A, we’ll move
    toward this minimum, and not the one at C. Our algorithm will fall into a *local
    minimum*. Once in, our algorithm can’t get us out, and we won’t be able to find
    the global minimum at C. We’ll see that this is a genuine issue for neural networks,
    but one that for modern deep networks is, almost magically, not much of an issue
    after all.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切看起来都不错，甚至是直观的，但我们有一个小问题。如果我们不是从B或D开始，而是从A开始呢？A点的梯度是指向左边的，而不是右边。在这种情况下，我们的简单算法将失败——它会将我们推向左边，而我们永远也无法到达C。图中只显示了一个最小值，位于C处，但我们很容易想象在A的左侧有第二个最小值，虽然它没有C那么低（即没有那么小的
    *y* 值）。如果我们从A开始，我们会朝这个最小值前进，而不是C处的最小值。我们的算法会陷入 *局部最小值*。一旦陷入局部最小值，我们的算法就无法摆脱它，我们将无法找到位于C的全局最小值。我们会看到，这是神经网络中的一个真实问题，但对于现代深度网络来说，这个问题几乎是神奇地不再是问题了。
- en: So how does all of this help us train a neural network? The gradient tells us
    how a small change in *x* changes *y*. If *x* is one of the parameters of our
    network and *y* is the error given by the loss function, then the gradient tells
    us how much a change in that parameter affects the overall error of the network.
    Once we know that, we’re in a position to modify the parameter by an amount based
    on the gradient, and we know that this will move us toward a minimum error. When
    the error over the training set is at a minimum, we can claim that the network
    has been trained.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这一切如何帮助我们训练神经网络呢？梯度告诉我们 *x* 的小变化如何影响 *y*。如果 *x* 是我们网络中的一个参数，而 *y* 是损失函数给出的误差，那么梯度告诉我们这个参数的变化对网络整体误差的影响。了解这一点后，我们就可以根据梯度修改参数，并且知道这样做会使我们向最小误差靠近。当训练集上的误差达到最小值时，我们就可以宣称网络已完成训练。
- en: 'Let’s talk a bit more about the gradients and parameters. All of our discussion
    to this point, based on [Figure 9-1](ch09.xhtml#ch9fig1), has been rather one-dimensional;
    our functions are functions of *x* only. We talked about changing one thing, the
    position along the x-axis, to see how it affects the *y* position. In reality,
    we’re not working with just one dimension. Every weight and bias in our network
    is a parameter, and the loss function value depends upon all of them. For the
    simple network in [Figure 8-1](ch08.xhtml#ch8fig1) alone, there are 20 parameters,
    meaning that the loss function is a 20-dimensional function. Regardless, our approach
    remains much the same: if we know the gradient for each parameter, we can still
    apply our algorithm in an attempt to locate a set of parameters minimizing the
    loss.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再多谈一谈梯度和参数。到目前为止，我们所有的讨论，基于[图9-1](ch09.xhtml#ch9fig1)，都相当单维；我们的函数仅仅是 *x*
    的函数。我们讨论了改变一个因素，即沿着x轴的位置，来观察它如何影响 *y* 位置。实际上，我们并不仅仅在处理一个维度。网络中的每个权重和偏置都是一个参数，而损失函数的值依赖于所有这些参数。仅在[图8-1](ch08.xhtml#ch8fig1)中，简单网络就有20个参数，这意味着损失函数是一个20维的函数。尽管如此，我们的方法仍然大致相同：如果我们知道每个参数的梯度，我们仍然可以应用我们的算法，试图找到一组最小化损失的参数。
- en: Updating the Weights
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新权重
- en: 'We’ll talk about how to get gradient values in a bit, but for the time being
    let’s assume we have them already. We’ll say that we have a set of numbers that
    tells us how, given the current configuration of the network, a change in any
    weight or bias value changes the loss. With that knowledge, we can apply gradient
    descent: we adjust the weight or bias by some fraction of that gradient value
    to move us, collectively, toward a minimum of the entire loss function.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讲解如何获得梯度值，但暂时假设我们已经得到了这些值。我们可以说，我们已经有了一组数字，它告诉我们在当前网络配置下，任何权重或偏置值的变化是如何影响损失的。借助这些信息，我们可以应用梯度下降：我们通过梯度值的一部分来调整权重或偏置，从而将整体网络推向损失函数的最小值。
- en: 'Mathematically, we update each weight and bias using a simple rule:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，我们使用一个简单的规则来更新每个权重和偏置：
- en: '*w* ← *w* –Δ*w*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*w* ← *w* –Δ*w*'
- en: Here *w* is one of the weights (or biases), *η* (eta) is the learning rate (the
    step size), and *Δw* is the gradient value.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w*是权重（或偏置）之一，*η*（eta）是学习率（步长），*Δw*是梯度值。
- en: '[Listing 9-1](ch09.xhtml#ch9lis1) gives an algorithm for training a neural
    network using gradient descent.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单9-1](ch09.xhtml#ch9lis1)给出了使用梯度下降训练神经网络的算法。'
- en: 1\. Pick some intelligent starting values for the weights and biases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 为权重和偏置选择一些智能的初始值。
- en: 2\. Run the training set through the network using its current weights and
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用当前的权重和偏置将训练集传入网络。
- en: biases and calculate the average loss.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置并计算平均损失。
- en: 3\. Use this loss to get the gradient for each weight and bias.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 使用该损失来计算每个权重和偏置的梯度。
- en: 4\. Update the weight or bias value by the step size times the gradient value.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 根据步长和梯度值更新权重或偏置值。
- en: 5\. Repeat from step 2 until the loss is low enough.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 从第2步开始重复，直到损失足够低。
- en: '*Listing 9-1: Gradient descent in five (deceptively) simple steps*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单9-1：五个（看似）简单的梯度下降步骤*'
- en: The algorithm appears simple, but as they say, the devil is in the details.
    We have to make choices at every step, and every choice we make will prompt further
    questions. For example, step 1 says to “Pick some intelligent starting values.”
    What should they be? It turns out that successfully training a neural network
    depends critically on choosing good initial values. We already saw how this might
    be so in our preceding example using [Figure 9-1](ch09.xhtml#ch9fig1) where if
    we start at A, we won’t find the minimum at C. Much research has been conducted
    over the years related to step 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法看起来很简单，但正如人们所说，关键在于细节。我们在每个步骤都需要做出选择，而我们做出的每个选择都会引发进一步的问题。例如，第1步要求“选择一些智能的初始值。”这些初始值应该是什么呢？事实证明，成功训练神经网络关键在于选择好的初始值。我们已经在前面的例子中看到了这一点，使用[图9-1](ch09.xhtml#ch9fig1)时，如果我们从A开始，我们就无法找到C的最小值。多年来，关于第1步的研究已经积累了大量的成果。
- en: Step 2 is straightforward; it’s the forward-pass through the network. We haven’t
    talked in detail about the loss function itself; for now, just think of it as
    a function measuring the effectiveness of the network on the training set.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步很直接，就是通过网络进行前向传播。我们还没有详细讨论损失函数本身；目前，只需要将它视为一个衡量网络在训练集上有效性的函数。
- en: Step 3 is a black box for the time being. We’ll explore how to do it shortly.
    For now, assume we can find the gradient values for each parameter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步目前还是一个黑盒。我们很快会探讨如何实现它。现在，假设我们能够找到每个参数的梯度值。
- en: Step 4 follows the form of the previous equation that moves the parameter from
    its current value to one that will reduce the overall loss. In practice, the simple
    form of this equation is not sufficient; there are other terms, like momentum,
    that preserve some fraction of the previous weight change for the next iteration
    (next pass of the training data through the network) so that parameters do not
    change too wildly. We’ll revisit momentum later. For now, let’s look at a variation
    of gradient descent, the one that is actually used to train deep networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步遵循前一个方程的形式，将参数从当前值移动到一个能够减少整体损失的值。实际上，这个方程的简单形式不足够；还有其他项，例如动量，它保留一些前一次权重变化的分量，供下一次迭代（即下一次训练数据通过网络）使用，从而防止参数变化过于剧烈。稍后我们会再讨论动量。现在，先来看一种梯度下降的变种，这种变种实际上被用来训练深度网络。
- en: Stochastic Gradient Descent
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: The previous steps describe gradient descent training of a neural network. As
    we might expect, in practice there are many different flavors of this basic idea.
    One that’s in widespread use and works well empirically is called *stochastic
    gradient descent (SGD)*. The word *stochastic* refers to a random process. We’ll
    see next why the word *stochastic* goes before *gradient descent* in this case.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的步骤描述了神经网络的梯度下降训练。正如我们可能预期的那样，实际上这种基本思想有很多不同的变种。一个广泛使用且在实践中表现良好的变种叫做*随机梯度下降（SGD）*。其中的*随机*一词指的是一种随机过程。接下来我们将看到，为什么在这种情况下，*随机*这个词要放在*梯度下降*前面。
- en: Batches and Minibatches
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批次与小批次
- en: 'Step 2 of [Listing 9-1](ch09.xhtml#ch9lis1) says to run the complete training
    set through the network using the current values of the weights and biases. This
    approach is called *batch training*, so named because we use all of the training
    data to estimate the gradients. Intuitively, this is a reasonable thing to do:
    we’ve carefully constructed the training set to be a fair representation of the
    unknown parent process that generates the data, and it’s this parent process we
    want the network to successfully model for us.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9-1](ch09.xhtml#ch9lis1)的第 2 步提到，使用当前的权重和偏置值将完整的训练集输入到网络中。这种方法被称为*批量训练*，之所以这样命名，是因为我们使用所有的训练数据来估计梯度。直观来看，这是一种合理的做法：我们已经精心构建了训练集，使其能公平地代表生成数据的未知母过程，而我们希望网络成功地为我们建模的正是这个母过程。'
- en: If our dataset is small, like the original iris dataset of [Chapter 5](ch05.xhtml#ch05),
    then batch training makes sense. But what if our training dataset isn’t small?
    What if it’s hundreds of thousands or even millions of samples? We’ll be facing
    longer and longer training times.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据集很小，比如[第 5 章](ch05.xhtml#ch05)中的原始鸢尾花数据集，那么批量训练是合适的。但如果我们的训练数据集很大呢？如果它有成百上千甚至数百万个样本呢？那样的话，我们将面临越来越长的训练时间。
- en: We’ve run into a problem. We want a large training set as that will (hopefully)
    better represent the unknown parent process that we want to model. But the larger
    the training set, the longer it takes to pass each sample through the network,
    get an average value for the loss, and update the weights and biases. We call
    passing the entire training set through the network an *epoch*, and we’ll need
    many dozens to hundreds of epochs to train the network. Doing a better job of
    representing the thing we want to model means longer and longer computation times
    because of all the samples that must be passed through the network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到了一个问题。我们希望拥有一个大的训练集，因为这将（希望）更好地代表我们想要建模的未知母过程。但是训练集越大，每次通过网络处理每个样本，计算损失的平均值，并更新权重和偏置的时间就越长。我们称将整个训练集通过网络一次的过程为*一个时期*，我们将需要几十到几百个时期来训练网络。更好地代表我们想要建模的对象意味着更长的计算时间，因为必须通过网络处理所有样本。
- en: This is where SGD comes into play. Instead of using all the training data on
    each pass, let’s alternatively select a small subset of the training data and
    use the average loss calculated from it to update the parameters. We’ll calculate
    an “incorrect” gradient value because we’re estimating the loss over the full
    training set using only a small sample, but we’ll save a lot of time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 SGD 发挥作用的地方。我们不再在每次迭代时使用所有的训练数据，而是交替选择一个小的训练数据子集，并利用从中计算出的平均损失来更新参数。由于我们只是使用一个小样本来估算整个训练集上的损失，计算出来的梯度值可能是“错误的”，但我们将节省大量的时间。
- en: 'Let’s see how this sampling plays out with a simple example. We’ll define a
    vector of 100 random bytes using NumPy:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来看看这个采样是如何运作的。我们将使用 NumPy 定义一个包含 100 个随机字节的向量：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here the byte values are normally distributed around a mean of 128\. The actual
    mean of the 100 values is 130.9\. Selecting subsets of these values, 10 at a time,
    gives us an estimate of the actual mean value
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，字节值呈正态分布，均值为 128。100 个值的实际均值是 130.9。从这些值中每次选择 10 个子集，我们可以估算出实际的均值。
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: with repeated subsets leading to estimated means of 135.7, 131.7, 134.2, 128.1,
    and so forth.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重复的子集，估计得到的均值分别为 135.7、131.7、134.2、128.1 等等。
- en: None of the estimated means are the actual mean, but they are all close to it.
    If we can estimate the mean from a random subset of the full dataset, we can see
    by analogy that we should be able to estimate the gradients of the loss function
    with a subset of the full training set. Since the sample is randomly selected,
    the resulting gradient values are randomly varying estimates. This is why we add
    the word *stochastic* in front of *gradient descent*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有估计的均值都不是实际的均值，但它们都非常接近。如果我们可以从完整数据集的随机子集估计均值，我们可以通过类比看出，我们应该能够通过完整训练集的一个子集来估计损失函数的梯度。由于样本是随机选择的，因此得到的梯度值是随机波动的估计值。这就是为什么我们在*梯度下降*前面加上*随机*一词的原因。
- en: 'Because passing the full training set through the network on each weight and
    bias update step is known as *batch training*, passing a subset through is known
    as *minibatch training*. You will hear people use the term *minibatch* quite frequently.
    A minibatch is the subset of the training data used for each stochastic gradient
    descent step. Training is usually some number of epochs, where the relationship
    between epochs and minibatches is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在每次更新权重和偏置时将完整训练集传递通过网络被称为*批训练*，而将一个子集传递通过网络则称为*小批量训练*。你会经常听到人们使用*小批量*这个术语。小批量是用于每次随机梯度下降步骤的训练数据子集。训练通常包含若干个周期（epoch），其中周期和小批量之间的关系如下：
- en: '![image](Images/196equ01.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/196equ01.jpg)'
- en: 'In practice, we don’t really want to select the minibatches at random from
    the full training set. If we do that, we run the risk of not using all the samples:
    some might never be selected, and others might be selected too often. Typically,
    we randomize the order of the training samples and select fixed-size blocks of
    samples sequentially whenever a minibatch is required. When all available training
    samples are used, we can shuffle the full training set order and repeat the process.
    Some deep learning toolkits don’t even do this; they instead cycle through the
    same set of minibatches again.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们并不希望从完整的训练集中随机选择小批量。如果我们这样做，可能会面临没有使用到所有样本的风险：有些样本可能永远不会被选中，而有些样本可能被选中得太频繁。通常，我们会随机化训练样本的顺序，并在需要小批量时按顺序选择固定大小的样本块。当所有可用的训练样本都被使用完时，我们可以打乱整个训练集的顺序，并重复这一过程。有些深度学习工具包甚至不这样做，它们会再次循环使用相同的小批量。
- en: Convex vs. Nonconvex Functions
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 凸函数与非凸函数
- en: SGD sounds like a concession to practicality. In theory, it seems that we’d
    never want to use it, and we might expect that our training results will suffer
    because of it. However, the opposite is generally true. In some sense, gradient
    descent training of a neural network shouldn’t work at all because we’re applying
    an algorithm meant for convex functions to one that is nonconvex. [Figure 9-2](ch09.xhtml#ch9fig2)
    illustrates the difference between a convex function and a nonconvex function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）听起来像是对实用性的让步。从理论上讲，我们似乎永远不想使用它，并且可能会预期我们的训练结果因此而受损。然而，事实恰恰相反。在某种意义上，神经网络的梯度下降训练本不应当有效，因为我们把一个用于凸函数的算法应用于非凸函数。[图9-2](ch09.xhtml#ch9fig2)说明了凸函数和非凸函数之间的区别。
- en: '![image](Images/09fig02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig02.jpg)'
- en: '*Figure 9-2: A convex function of* x *(left). A nonconvex function of* x *(right)*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-2：* x *的凸函数（左）。* x *的非凸函数（右）*。'
- en: A convex function is such that a line segment between any two points on the
    function does not cross the function at any other point. The black line on the
    left of [Figure 9-2](ch09.xhtml#ch9fig2) is one example, and any such segment
    will not cross the function at any other point, indicating that this is a convex
    function. However, the same can’t be said of the curve on the right of [Figure
    9-2](ch09.xhtml#ch9fig2). This is the curve from [Figure 9-1](ch09.xhtml#ch9fig1).
    Here the black line does cross the function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 凸函数是指在函数上的任意两点之间的线段不会在其他点与函数交叉。[图9-2](ch09.xhtml#ch9fig2)左侧的黑线就是一个例子，任何这样的线段都不会在其他点与函数交叉，表明这是一个凸函数。然而，[图9-2](ch09.xhtml#ch9fig2)右侧的曲线就不能这样说了。这就是[图9-1](ch09.xhtml#ch9fig1)中的曲线。在这里，黑线确实会与函数交叉。
- en: Gradient descent is designed to find the minimum when the function is convex,
    and because it relies only on the gradient, the first derivative, it’s sometimes
    known as a *first-order* optimization method. Gradient descent should not work,
    in general, with nonconvex functions because it runs the risk of getting trapped
    in a local minimum instead of finding the global minimum. Again, we saw this with
    the example in [Figure 9-1](ch09.xhtml#ch9fig1).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法在函数是凸的情况下被设计用来找到最小值，因为它仅依赖于梯度，即一阶导数，因此有时被称为*一阶*优化方法。一般来说，梯度下降法不应在非凸函数上起作用，因为它有可能陷入局部最小值，而不是找到全局最小值。我们在[图9-1](ch09.xhtml#ch9fig1)的例子中也看到了这一点。
- en: Here’s where stochastic gradient descent helps. In multiple dimensions, the
    gradient will point in a direction that isn’t necessarily toward the nearest minimum
    of the loss function. This means that our step will be in a slightly wrong direction,
    but that somewhat wrong direction might help us avoid getting trapped somewhere
    we don’t want to be.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是随机梯度下降的帮助所在。在多维空间中，梯度指向的方向不一定是损失函数的最近最小值方向。这意味着我们的步伐会稍微朝错误的方向前进，但这个略微错误的方向可能帮助我们避免被困在不想去的地方。
- en: The situation is more complicated, of course, and more mysterious. The machine
    learning community has been struggling with the contradiction between the obvious
    success of using first-order optimization on the nonconvex loss function and the
    fact that it shouldn’t work at all.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 情况当然更加复杂，也更加神秘。机器学习社区一直在挣扎于使用一阶优化方法在非凸损失函数上取得明显成功与其实际上不应该起作用之间的矛盾。
- en: Two ideas are emerging. The first is what we just stated, that stochastic gradient
    descent helps by actually moving us in a slightly wrong direction. The second
    idea, which seems to be pretty much proven now, is that, for the loss functions
    used in deep learning, it turns out that there are many, many local minimums and
    that these are all basically the same, so that landing in almost any one of them
    will result in a network that performs well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个思想正在浮现。第一个是我们刚刚提到的，随机梯度下降通过实际让我们朝稍微错误的方向移动来起作用。第二个思想，现在似乎已经得到充分验证的是，对于深度学习中使用的损失函数，实际上存在着许多局部最小值，而这些最小值基本上是相同的，因此几乎落在任何一个局部最小值上都能得到表现良好的网络。
- en: Some researchers argue that most gradient descent learning winds up on a *saddle
    point*; this is a place that looks like a minimum but isn’t. Imagine a saddle
    for a horse and place a marble in the middle. The marble will sit in place, but
    you could push the marble in a certain direction and have it roll off the saddle.
    The argument, not without some justification, is that most training ends on a
    saddle point, and better results are possible with a better algorithm. Again,
    however, even the saddle point, if it is one, is still for practical purposes
    a good place to be, so the model is successful regardless.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员认为，大多数梯度下降学习最终会停留在一个*鞍点*；这个地方看起来像一个最小值，但并不是。想象一下马鞍，将一个小球放在中间。小球会保持在原地，但你可以把它推向某个方向，使它从鞍点滚下来。这个论点，尽管有一定的合理性，认为大多数训练最终停留在鞍点，且通过更好的算法可以获得更好的结果。然而，即使这个点是鞍点，若它真的是鞍点，它仍然在实际操作中是一个不错的位置，因此模型无论如何都能取得成功。
- en: In practice, then, we should use stochastic gradient descent because it leads
    to better overall learning and reduces the training time by not requiring full
    batches. It does introduce a new hyperparameter, the minibatch size, that we must
    select at some point before training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实际操作中我们应该使用随机梯度下降，因为它能够带来更好的整体学习效果，并通过不要求全批次的数据来减少训练时间。它确实引入了一个新的超参数——小批量大小，我们必须在训练之前的某个时刻选择它。
- en: Ending Training
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结束训练
- en: 'We haven’t yet discussed a critical question: when should we stop training?
    Remember that in [Chapter 5](ch05.xhtml#ch05), we went through some effort to
    create training sets, validation sets, and test sets. This is where we’ll use
    the validation sets. While training, we can use the accuracy, or some other metric,
    on the validation set to decide when to stop. If using SGD, we typically run the
    validation set through the network for each minibatch or set of minibatches to
    compute the accuracy. By tracking the accuracy on the validation set, we can decide
    when to stop training.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有讨论一个关键问题：我们应该何时停止训练？记得在[第5章](ch05.xhtml#ch05)中，我们花了一些功夫来创建训练集、验证集和测试集。现在就是我们使用验证集的地方。在训练过程中，我们可以使用验证集上的准确度或其他一些指标来决定何时停止。如果使用SGD，我们通常会在每个小批量或一组小批量上运行验证集，通过网络计算准确度。通过跟踪验证集上的准确度，我们可以决定何时停止训练。
- en: If we train for a long time, eventually two things usually happen. The first
    is that the error on the training set goes toward zero; we get better and better
    on the training set. The second is that the error on the validation set goes down
    and then, eventually, starts to go back up.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练很长时间，通常会发生两件事。第一，训练集上的误差趋近于零；我们在训练集上的表现越来越好。第二，验证集上的误差先下降，然后最终开始回升。
- en: These effects are due to overfitting. The training error goes down and down
    as the model learns more and more to represent the parent distribution that generated
    the dataset. But, eventually, it will stop learning general things about the training
    set. At this point, we’re overfitting, and we want to stop training because the
    model is no longer learning general features and is instead learning minutiae
    about the particular training set we’re using. We can watch for this by using
    the validation set while training. Since we don’t use the samples in the validation
    set to update the weights and biases of the network, it should give us a fair
    test of the current state of the network. When overfitting starts, the error on
    the validation set will begin to go up from a minimum value. What we can do then
    is to keep the weights and biases that produced the minimum value on the validation
    set and claim that those represent the best model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些效应是由于过拟合导致的。随着模型不断学习以表示生成数据集的母体分布，训练误差会不断下降。然而，最终，它将停止学习训练集的通用特征。在这个时候，我们发生了过拟合，应该停止训练，因为模型不再学习通用特征，而是学习我们所用特定训练集的细节。我们可以通过在训练过程中使用验证集来监控这一点。由于我们不使用验证集中的样本来更新网络的权重和偏置，它应该能公平地测试网络当前的状态。当过拟合开始时，验证集上的误差将从最小值开始上升。此时，我们可以保留在验证集上产生最小误差的权重和偏置，并认为这些代表了最佳模型。
- en: We don’t want to use any data that has influenced training to measure the final
    effectiveness of our network. We use the validation set to decide when to stop
    training, so characteristics of the samples in the validation set have also influenced
    the final model; this means we can’t strongly rely on the validation set to give
    us an idea of how the model will behave on new data. It’s only the held-out test
    set, unused until we declare victory over training, that gives us some idea of
    how we might expect the model to perform on data in the wild. So, just as it is
    anathema to report training set accuracy as a measure of how good the model is,
    it’s also anathema to report the validation set accuracy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望使用任何已影响训练的数据来衡量网络的最终效果。我们使用验证集来决定何时停止训练，因此验证集中样本的特征也已影响最终模型；这意味着我们不能过度依赖验证集来判断模型在新数据上的表现。只有在我们宣布训练胜利之前未使用的测试集，才能让我们对模型在实际数据中的表现有所了解。因此，正如报告训练集准确度作为模型好坏的衡量标准是错误的，报告验证集准确度同样也是错误的。
- en: Updating the Learning Rate
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新学习率
- en: In our generic update equation for changing the weights and biases based on
    the gradient, we introduced a hyperparameter, *η* (eta), the learning rate or
    step size. It’s a scale factor indicating how much we should update the weight
    or bias based on the gradient value.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们基于梯度更新权重和偏置的通用更新方程中，我们引入了一个超参数，*η*（eta），即学习率或步长。它是一个比例因子，表示我们应根据梯度值更新权重或偏置的程度。
- en: We previously stated that the learning rate doesn’t need to be fixed and that
    it could, and even should, get smaller and smaller as we train under the assumption
    that we need smaller and smaller steps to get to the actual minimum value of the
    loss function. We didn’t state how we should actually update the learning rate.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，学习率不需要固定，并且它可以，甚至应该，随着训练逐渐变小，因为我们假设需要越来越小的步伐才能达到损失函数的实际最小值。我们没有说明应该如何实际更新学习率。
- en: There’s more than one way to update the step size, but some are more helpful
    than others. The `MLPClassifier` class of sklearn, which uses SGD solvers, has
    three options. The first is to never change the learning rate—just leave *η* at
    its initial value, *η*[0]. The second is to scale *η* so that it decreases with
    epochs (minibatches) according to
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更新步长有不止一种方法，但有些方法比其他方法更有帮助。sklearn的`MLPClassifier`类使用SGD求解器，提供了三种选项。第一种是永远不改变学习率——只需保持*η*为初始值*η*[0]。第二种是按周期（小批量）缩放*η*，使其随着周期数的增加而减小，具体如下：
- en: '![image](Images/199equ01.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/199equ01.jpg)'
- en: where *η*[0] is set by the user, *t* is the iteration (epoch, minibatch), and
    *p* is an exponent on *t*, also picked by the user. The sklearn default *p* is
    0.5—that is, scale by ![Image](Images/199equ02.jpg), which seems a reasonable
    default.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*η*[0]由用户设置，*t*是迭代次数（周期，小批量），*p*是*t*的指数，用户也可以指定。sklearn的默认值*p*为0.5，也就是按![Image](Images/199equ02.jpg)进行缩放，这似乎是一个合理的默认值。
- en: The third option is to adapt the learning rate by watching the loss function
    value. As long as the loss is decreasing, leave the learning rate where it is.
    When the loss stops decreasing for a set number of minibatches, divide the learning
    rate by some value like 5, the sklearn default. If we never change the learning
    rate, and it’s too large, we might end up moving around the minimum without ever
    being able to reach it because we’re consistently stepping over it. It’s a good
    idea then to decrease the learning rate when using SGD. Later in the book, we’ll
    encounter other optimization approaches that automatically adjust the learning
    rate for us.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选项是通过观察损失函数值来调整学习率。只要损失在下降，就保持学习率不变。当损失在设置的几个小批量后不再下降时，将学习率除以一个值，比如5，这是sklearn的默认值。如果我们从不改变学习率，并且它太大，我们可能会一直围绕最小值移动，而无法到达它，因为我们总是跨越它。那时，降低学习率是使用SGD的一个好主意。本书后面我们将遇到其他优化方法，它们会自动为我们调整学习率。
- en: Momentum
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动量
- en: There’s one last wrinkle in SGD we have to cover. As we saw previously, the
    weight update equation for both gradient descent and SGD is
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SGD中还有一个最后需要注意的问题。如我们之前所见，梯度下降和SGD的权重更新方程式是：
- en: '*w* ← *w* – *η*Δ*w*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*w* ← *w* – *η*Δ*w*'
- en: We update the weight by the learning rate (*η*) times the gradient, which we
    are representing here as Δ*w*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习率（*η*）乘以梯度来更新权重，我们在这里表示为Δ*w*。
- en: A common and powerful trick is to introduce a *momentum* term that adds back
    some fraction of the previous Δ*w*, the update of the prior minibatch. The momentum
    term prevents the *w* parameter from changing too quickly in response to a particular
    minibatch. Adding in this term gives us
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见且强大的技巧是引入一个*动量*项，将前一次Δ*w*的某个分数加回来，即前一个小批量的更新。动量项可以防止*w*参数在面对某个特定小批量时变化过快。加入此项后，我们得到：
- en: '*w*[*i*+1] ← *w*[*i*] – *η*Δ*w*[*i*] + *μ*Δ*w*[*i–1*]'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[*i*+1] ← *w*[*i*] – *η*Δ*w*[*i*] + *μ*Δ*w*[*i–1*]'
- en: We’ve added subscripts to indicate the next pass through the network (*i* +
    1), the current pass (*i*), and the previous pass (*i –* 1). The previous pass
    Δ*w* is the one we need to use. A typical value for *μ* (mu), the momentum, is
    around 0.9\. Virtually all toolkits implement momentum in some form, including
    sklearn.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了下标来表示网络的下一次传递（*i* + 1），当前传递（*i*）和上一次传递（*i –* 1）。我们需要使用的是上一次传递的Δ*w*。*μ*（动量）的典型值约为0.9。几乎所有工具包都以某种形式实现了动量，包括sklearn。
- en: Backpropagation
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: We’ve been operating under the assumption that we know the gradient value for
    each parameter. Let’s discuss how the backpropagation algorithm gives us these
    magic numbers. The backpropagation algorithm is perhaps the single most important
    development in the history of neural networks as it enables the training of large
    networks with hundreds, thousands, millions, and even billions of parameters.
    This is especially true of the convolutional networks we’ll work with in [Chapter
    12](ch12.xhtml#ch12).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直假设已经知道每个参数的梯度值。让我们讨论一下反向传播算法是如何为我们提供这些“魔法数字”的。反向传播算法可能是神经网络历史上最重要的进展，它使得训练包含成百上千甚至数百万、数十亿个参数的大型网络成为可能。特别是在我们将在
    [第 12 章](ch12.xhtml#ch12) 中研究的卷积网络中，这一点尤为重要。
- en: The backpropagation algorithm itself was published by Rumelhart, Hinton, and
    Williams in 1986 in their paper “Learning Representations by Back-propagating
    Errors.” It’s a careful application of the chain rule for derivatives. The algorithm
    is called *backpropagation* because it works backward from the output layer of
    the network toward the input layer, propagating the error from the loss function
    down to each parameter of the network. Colloquially, the algorithm is known as
    *backprop*; we’ll use that term here, so we sound more like native machine learning
    experts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法本身由 Rumelhart、Hinton 和 Williams 在 1986 年的论文《通过反向传播误差来学习表示》中发布。这是链式法则在导数中的一种巧妙应用。该算法被称为
    *反向传播*，因为它从网络的输出层向输入层反向工作，将损失函数中的误差传播到网络的每个参数。口语中，这个算法被称为 *反向传播*（*backprop*）；我们在这里也使用这个术语，这样听起来更像是机器学习领域的专家。
- en: Adding backprop into the training algorithm for gradient descent, and tailoring
    it to SGD, gives us the algorithm in [Listing 9-2](ch09.xhtml#ch9lis2).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将反向传播加入梯度下降的训练算法，并根据随机梯度下降（SGD）进行调整，给出了 [列表 9-2](ch09.xhtml#ch9lis2) 中的算法。
- en: 1\. Pick some intelligent starting values for the weights and biases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 为权重和偏置选择一些合理的初始值。
- en: 2\. Run a minibatch through the network using its current weights and biases
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用当前的权重和偏置将一个小批量数据通过网络运行。
- en: and calculate the average loss.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 并计算平均损失。
- en: 3\. Use this loss and backprop to get the gradient for each weight and bias.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 使用此损失和反向传播来获取每个权重和偏置的梯度。
- en: 4\. Update the weight or bias value by the step size times the gradient value.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 通过步长乘以梯度值来更新权重或偏置值。
- en: 5\. Repeat from step 2 until the loss is low enough.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 从步骤 2 开始重复，直到损失足够小。
- en: '*Listing 9-2: Stochastic gradient descent with backprop*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 9-2：带有反向传播的随机梯度下降*'
- en: Step 2 of [Listing 9-2](ch09.xhtml#ch9lis2) is referred to as the *forward pass*;
    step 3 is the *backward pass*. The forward pass is also how we’ll use the network
    after it’s finally trained. The backward pass is backprop calculating the gradients
    for us so that we can update the parameters in step 4.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9-2](ch09.xhtml#ch9lis2)的步骤 2 被称为 *前向传播*；步骤 3 是 *反向传播*。前向传播也是我们在网络训练完成后使用网络的方式。反向传播是反向传播算法，它计算梯度，供我们在步骤
    4 中更新参数。'
- en: 'We’ll describe backprop twice. First, we’ll do so with a simple example and
    work with the actual derivatives. Second, we’ll work with a more abstract notation
    to see how backprop applies to actual neural networks in a general sense. There
    is no way to sugarcoat this: this section involves derivatives, but we already
    have a good intuitive sense of what those are from our discussion of gradient
    descent, so we should be in good shape to proceed.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述两次反向传播。首先，我们将通过一个简单的例子并运用实际的导数进行讲解。其次，我们将使用更抽象的符号，看看反向传播如何在一般意义上应用于实际的神经网络。不能回避这一点：这一部分涉及导数，但我们已经通过梯度下降的讨论对这些有了直观的理解，因此应该能顺利进行。
- en: Backprop, Take 1
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向传播，第 1 部分
- en: 'Suppose we have two functions, *z* = *f* (*y*) and *y* = *g*(*x*), meaning
    *z* = *f* (*g*(*x*)). We know that the derivative of the function *g* gives us
    *dy*/*dx*, which tells us how *y* changes when *x* changes. Similarly, we know
    that the derivative of the function *f* will give us *dz*/*dy*. The value of *z*
    depends upon the composition of *f* and *g*, meaning the output of *g* is the
    input to *f*, so if we want to find an expression for *dz*/*dx*, how *z* changes
    with *x*, we need a way to link through the composed functions. This is what the
    chain rule for derivatives gives us:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个函数，*z* = *f* (*y*) 和 *y* = *g*(*x*)，也就是说 *z* = *f* (*g*(*x*)）。我们知道函数
    *g* 的导数给出了 *dy*/*dx*，它告诉我们当 *x* 改变时 *y* 如何变化。类似地，我们知道函数 *f* 的导数会给出 *dz*/*dy*。*z*
    的值依赖于 *f* 和 *g* 的组合，即 *g* 的输出是 *f* 的输入，因此，如果我们想找到 *dz*/*dx* 的表达式，表示 *z* 随着 *x*
    的变化，我们需要通过组合函数进行链接。这就是导数链式法则给我们的结果：
- en: '![image](Images/201equ01.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/201equ01.jpg)'
- en: This notation is especially nice because we can imagine the *dy* “term” canceling
    just as it would if these were actual fractions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个符号特别好，因为我们可以想象 *dy* 这个“项”就像实际的分数一样被消去。
- en: How does this help us? In a neural network, the output of one layer is the input
    to the next, which is composition, so we can see intuitively that the chain rule
    might apply. Remember that we want the values that tell us how the loss function
    changes with respect to the weights and biases. Let’s call the loss function *L*
    and any given weight or bias *w*. We want to calculate *∂*ℒ/*∂w* for all the weights
    and biases.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们有什么帮助呢？在神经网络中，一层的输出是下一层的输入，这就是组合，所以我们可以直观地看到链式法则可能适用。记住，我们想要的是告诉我们损失函数如何相对于权重和偏置变化的值。我们把损失函数叫做
    *L*，任何给定的权重或偏置叫做 *w*。我们想计算所有权重和偏置的 *∂*ℒ/*∂w*。
- en: Alarm bells should be going off in your head. The previous paragraph slipped
    in new notation. So far, we’ve been writing derivatives as *dy*/*dx*, but the
    derivative for the loss with respect to a weight was written as *∂*ℒ/*∂w*. What
    is this fancy *∂*?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 警钟应该在你脑中响起了。前一段中引入了新的符号。到目前为止，我们一直写导数为 *dy*/*dx*，但损失函数对权重的导数却写作 *∂*ℒ/*∂w*。这个花哨的
    *∂* 是什么？
- en: When we had a function of one variable, just *x*, there was only one slope at
    a point to talk about. As soon as we have a function with more than one variable,
    the idea of the slope at a point becomes ambiguous. There are an infinite number
    of lines tangent to the function at any point. So we need the idea of the *partial
    derivative*, which is the slope of the line in the direction of the variable we’re
    considering when all other variables are treated as fixed. This tells us how the
    output will change as we change only the one variable. To note that we are using
    a partial derivative, we shift from *d* to *∂*, which is just a script *d*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们只有一个变量函数，即只有 *x* 时，讨论一个点的斜率是没有问题的。但一旦我们有了多个变量的函数，点的斜率的概念就变得模糊了。在任何点上，都有无数条切线。因此，我们需要使用*偏导数*的概念，它是我们所考虑的变量的方向上的斜率，而将其他变量视为固定值。这告诉我们，当我们只改变一个变量时，输出会如何变化。为了表示我们使用的是偏导数，我们将
    *d* 换成 *∂*，这只是 *d* 的一种变体。
- en: Let’s set up a straightforward network so that we can see how the chain rule
    leads directly to the expressions we want. We’re looking at the network in [Figure
    9-3](ch09.xhtml#ch9fig3), which consists of an input, two hidden layers of a single
    node each, and an output layer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们建立一个简单的网络，这样我们可以看到链式法则是如何直接得到我们想要的表达式的。我们正在查看 [图 9-3](ch09.xhtml#ch9fig3)
    中的网络，它由一个输入、两个每个只有一个节点的隐藏层和一个输出层组成。
- en: '![image](Images/09fig03.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig03.jpg)'
- en: '*Figure 9-3: A simple network to illustrate the chain rule*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-3：一个简单的网络，用于说明链式法则*'
- en: For simplicity, we’ll ignore any bias values. Additionally, let’s define the
    activation function to be the identity function, *h*(*x*) = *x*. This simplification
    removes the derivative of the activation function to make things more transparent.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们将忽略任何偏置值。此外，假设激活函数是恒等函数，*h*(*x*) = *x*。这个简化去除了激活函数的导数，使得问题更为清晰。
- en: For this network, the forward pass computes
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个网络，前向传播计算
- en: '*h*[1] = *w*[1]*x*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *w*[1]*x*'
- en: '*h*[2] = *w*[2]*h*[1]'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[2] = *w*[2]*h*[1]'
- en: '*y* = *w*[3]*h*[2]'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *w*[3]*h*[2]'
- en: which follows the form we’ve used previously, chaining things together by making
    the output of one layer the input to the next. This gives us the output of the
    network, *y*, for input *x*. If we’re looking to train the network, we’ll have
    a training set, a set of pairs, (*x*[*i*], *ŷ*), *i* = 0, 1, …, that are examples
    of what the output should be for a given input. Note that the forward pass moved
    from the input, *x*, to the output, *y*. We’ll next see why the backward pass
    moves from the output to the input.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这遵循了我们之前使用的形式，通过将一层的输出作为下一层的输入将各个部分链接在一起。这给出了网络的输出 *y*，对应输入 *x*。如果我们希望训练网络，就会有一个训练集，一组对
    (*x*[*i*], *ŷ*)，*i* = 0, 1, …，表示对于给定输入应该得到的输出。请注意，前向传播是从输入 *x* 到输出 *y*。接下来我们将看到为什么反向传播是从输出到输入。
- en: Now let’s define the loss function, *ℒ*, to be the squared error between *y*,
    the network output for a given input *x*, and *ŷ*, the output we should get. Functionally,
    the loss looks like the following.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义损失函数 *ℒ* 为 *y*（给定输入 *x* 时的网络输出）和 *ŷ*（我们应该得到的输出）之间的平方误差。函数上看，损失像下面这样：
- en: '![image](Images/202equ02.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/202equ02.jpg)'
- en: For simplicity, we’re ignoring the fact that the loss is a mean over the training
    set or some minibatch drawn from it. The factor of ![Image](Images/1by2.jpg) is
    not strictly necessary but it’s commonly used to make the derivative a bit nicer.
    Since we’re looking to minimize the loss for a particular set of weights, it doesn’t
    matter that we’re always multiplying the loss by a constant factor of ![Image](Images/1by2.jpg)—the
    smallest loss will still be the smallest loss regardless of its actual numeric
    value.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们忽略了损失是训练集或从中抽取的某个小批次的平均值这一事实。![image](Images/1by2.jpg)因子不是绝对必要的，但它通常用于让导数更简洁一些。因为我们希望最小化特定权重集的损失，所以不管我们是否总是将损失乘以一个常数因子![image](Images/1by2.jpg)，这并不重要——最小的损失仍然是最小的损失，无论其实际数值如何。
- en: 'To use gradient descent, we need to find how the loss changes with the weights.
    In this simple network, that means we need to find three gradient values, one
    each for *w*[1], *w*[2], and *w*[3]. This is where the chain rule comes into play.
    We’ll write the equations first and then talk about them:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用梯度下降，我们需要找出损失函数随权重变化的情况。在这个简单的网络中，这意味着我们需要找到三个梯度值，分别是 *w*[1]、*w*[2] 和 *w*[3]。这就是链式法则发挥作用的地方。我们先写出方程，然后再讨论它们：
- en: '![image](Images/202equ03.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/202equ03.jpg)'
- en: 'The order of these equations shows why this algorithm is called *backpropagation*.
    To get the partial derivative for the output layer parameter, we need only the
    output and the loss, *y* and *ℒ*. To get the partial derivative for the middle
    layer weight, we need the following two partial derivatives from the output layer:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程的顺序展示了为什么这个算法被称为 *反向传播*。为了得到输出层参数的偏导数，我们只需要输出 *y* 和损失 *ℒ*。为了得到中间层权重的偏导数，我们需要以下两个来自输出层的偏导数：
- en: '![image](Images/203equ01.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/203equ01.jpg)'
- en: Finally, to get the partial derivative for the input layer weight, we need partial
    derivatives from the output and middle layer. In effect, we have moved backward
    through the network propagating values from later layers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了得到输入层权重的偏导数，我们需要输出层和中间层的偏导数。实际上，我们已经通过网络反向传播，从后面的层传播值。
- en: For each of these equations, the right-hand side matches the left-hand side
    if we imagine the “terms” canceling like fractions. Since we selected a particularly
    simple form for the network, we can calculate the actual gradients by hand. We
    need the following gradients, from the right-hand side of the preceding equations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些方程中的每一个，如果我们想象“项”像分数一样相互抵消，那么右侧与左侧是匹配的。由于我们为网络选择了一个特别简单的形式，因此我们可以手动计算实际的梯度。我们需要以下梯度，来自前面方程的右侧。
- en: '![image](Images/203equ02.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/203equ02.jpg)'
- en: The *∂ℒ*/*∂y* comes from the form we selected for the loss and the rules of
    differentiation from calculus.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*∂ℒ*/*∂y* 来自我们为损失选择的形式以及微积分中的求导规则。'
- en: Putting these back into the equations for the gradients of the weights gives
    us
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些代回到权重梯度的方程中，得到：
- en: '![image](Images/204equ01.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/204equ01.jpg)'
- en: After a forward pass, we have numeric values for all the quantities on the right-hand
    side of these equations. Therefore, we know the numeric value of the gradients.
    The update rule from gradient descent then tells us to change the weights like
    the following.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播后，我们已经得到了这些方程右侧所有量的数值。因此，我们知道了梯度的数值。梯度下降的更新规则告诉我们像下面这样改变权重。
- en: '![image](Images/204equ02.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/204equ02.jpg)'
- en: where *η* is the learning rate parameter defining how large a step to take when
    updating.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *η* 是学习率参数，定义了在更新时应该采取的步长。
- en: To recap, we need to use the chain rule, the heart of the backprop algorithm,
    to find the gradients we need to update the weights during training. For our simple
    network, we were able to work out the value of these gradients explicitly by moving
    backward through the network from the output toward the input. Of course, this
    is just a toy network. Let’s now take a second look at how to use backprop in
    a more general sense to calculate the necessary gradients for any network.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们需要使用链式法则，这是反向传播算法的核心，以找到在训练过程中更新权重所需的梯度。对于我们的简单网络，我们通过从输出反向推导到输入，明确地计算出了这些梯度的值。当然，这只是一个简单的玩具网络。现在让我们再看一下如何在更一般的意义上使用反向传播来计算任何网络所需的梯度。
- en: Backprop, Take 2
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向传播，第二部分
- en: Let’s begin by revisiting the loss function and introducing some new notation.
    The loss function is a function of all the parameters in the network, meaning
    that every weight and bias value contributes to it. For example, the loss for
    the network of [Figure 8-1](ch08.xhtml#ch8fig1), which has 20 weights and biases,
    could be written as
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从重新审视损失函数并引入一些新的符号开始。损失函数是网络中所有参数的函数，意味着每个权重和偏置的值都会对其产生影响。例如，具有 20 个权重和偏置的[图
    8-1](ch08.xhtml#ch8fig1)网络的损失可以写成
- en: '![image](Images/205equ01.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/205equ01.jpg)'
- en: 'Note we’ve introduced a new notation for the parameters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们引入了一种新的参数符号：
- en: '![image](Images/205equ02.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/205equ02.jpg)'
- en: This represents the weight that links the *j*-th input, an output of the *i
    –* 1 layer, to the *k*-th node of layer *i*. We also have
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示将第*i – 1*层的第*j*个输入（即输出）连接到第*i*层的第*k*个节点的权重。我们还可以有
- en: '![image](Images/205equ03.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/205equ03.jpg)'
- en: to represent the bias value for the *k*-th node of the *i*-th layer. Here layer
    0 is the input layer itself. The parentheses on the exponent are a label, the
    layer number; they should not be interpreted as actual exponents. Therefore,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表示第*i*层的第*k*个节点的偏置值。这里，层 0 就是输入层本身。指数上的括号是一个标签，表示层号；它们不应被理解为实际的指数。因此，
- en: '![image](Images/205equ04.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/205equ04.jpg)'
- en: is the weight from the third output of the first layer to the first node of
    the second layer. This is the highlighted weight in [Figure 9-4](ch09.xhtml#ch9fig4).
    Remember that we always number nodes top to bottom, starting with 0.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从第一层的第三个输出到第二层的第一个节点的权重。这是[图 9-4](ch09.xhtml#ch9fig4)中突出显示的权重。记住，我们总是从上到下编号节点，从
    0 开始。
- en: '![image](Images/09fig04.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig04.jpg)'
- en: '*Figure 9-4: The network of [Figure 8-1](ch08.xhtml#ch8fig1) with weight w^((2))[20]
    marked with a bold line*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-4：带有加粗线标记的[图 8-1](ch08.xhtml#ch8fig1)网络，权重 w^((2))[20]*'
- en: This notation is a bit daunting, but it will let us reference any weight or
    bias of the network precisely. The number we need to use backprop is the partial
    derivative of the loss with respect to each weight or bias. Therefore, what we
    want to find ultimately is written, in all its glorious mathematical notation,
    as
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个符号有些令人生畏，但它能让我们精确地引用网络中的任何权重或偏置。我们需要用来反向传播的数字是损失函数对每个权重或偏置的偏导数。因此，我们最终要找到的是，用所有华丽的数学符号表示的
- en: '![image](Images/206equ01.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/206equ01.jpg)'
- en: 'This gives us the slope: the amount the loss will change for a change in the
    weight linking the *k*-th node of the *i*-th layer to the *j*-th output of the
    *i –* 1 layer. A similar equation gives us the partial derivatives of the biases.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了斜率：当连接第*i*层第*k*个节点到第*i – 1*层第*j*个输出的权重发生变化时，损失的变化量。类似的方程给出了偏置的偏导数。
- en: We can simplify this cumbersome notation by dealing only with the layer number
    understanding that buried in the notation is a vector (biases, activations) or
    matrix (weights) so that we want to find
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过仅处理层号来简化这种繁琐的符号，因为在符号中包含了一个向量（偏置、激活）或矩阵（权重），因此我们想要找到的是
- en: '![image](Images/206equ02.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/206equ02.jpg)'
- en: These correspond to a matrix for all the weights linking layer *i –* 1 to *i*,
    and a vector for all the biases of layer *i*, respectively.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分别对应于从第*i – 1*层到第*i*层的所有权重的矩阵，以及第*i*层所有偏置的向量。
- en: We’ll protect our notational sanity by looking at things in terms of vectors
    and matrices. Let’s start with the output layer and see what that buys us. We
    know that the activations of the output layer, layer *L*, are found via
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将问题转换为向量和矩阵的形式来保护我们的符号一致性。让我们从输出层开始，看看这样做能带来什么好处。我们知道输出层（层*L*）的激活值是通过以下方式找到的：
- en: '*a*^((*L*)) = *h*(*W*^((*L*))*a*^((*L*–1)) + *b*^((*L*)))'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*^((*L*)) = *h*(*W*^((*L*))*a*^((*L*–1)) + *b*^((*L*)))'
- en: with *a* being the activations from layer *L –* 1, *b* the bias vector for layer
    *L*, and *W* the weight matrix between layers *L –* 1 and *L*. The activation
    function is *h*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a*是来自层*L –* 1的激活值，*b*是层*L*的偏置向量，*W*是层*L –* 1和*L*之间的权重矩阵。激活函数是*h*。
- en: Additionally, we’ll define the argument to *h* to be *z*^((*L*))
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将定义* h *的参数为* z *^((*L*))
- en: '*z*^((*L*)) ≡ *W*^((*L*))*a*^((*L*–1)) + *b*^((*L*))'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*^((*L*)) ≡ *W*^((*L*))*a*^((*L*–1)) + *b*^((*L*))'
- en: and call *∂L*/*∂z*^((*l*)) the *error*, the contribution to the loss from the
    inputs to layer *l*. Next, we define
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 并称*∂L*/*∂z*^((*l*))为*误差*，即来自层*l*输入对损失的贡献。接下来，我们定义
- en: '![image](Images/206equ03.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/206equ03.jpg)'
- en: so that we can work with *δ* (delta) from now on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以从现在开始处理*δ*（delta）。
- en: For the output layer, we can write *δ* as
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出层，我们可以将*δ*写为
- en: '![image](Images/207equ01.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/207equ01.jpg)'
- en: The notation *h*′(*z*^((*L*))) is another way to write the derivative of *h*
    (with respect to *z*) evaluated at *z*^((*L*)). The ⋅ represents elementwise multiplication.
    This is the way NumPy works when multiplying two arrays of the same size so that
    if *C* = *A* ⋅ *B*, then *C*[*ij*] = *A*[*ij*]*B*[*ij*]. Technically, this product
    is called the *Hadamard product*, named for the French mathematician Jacques Hadamard.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 符号*h*′(*z*^((*L*)))是另一种表示*h*对*z*的导数的方式，评估点为*z*^((*L*))。⋅表示逐元素相乘。这是NumPy在乘两个相同大小的数组时的工作方式，假设*C*
    = *A* ⋅ *B*，那么*C*[*ij*] = *A*[*ij*] * B*[*ij*]。技术上，这个乘积被称为*Hadamard积*，以法国数学家Jacques
    Hadamard命名。
- en: The preceding means that to use backpropagation, we need a loss function that
    can be differentiated—a loss function for which a derivative exists at every point.
    This isn’t too much of a burden; the loss functions we’ll examine in the next
    section meet this criterion. We also need an activation function that can be differentiated
    so we can find *h*(*z*). Again, the activation functions we have considered so
    far are essentially all differentiable.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述意味着，要使用反向传播，我们需要一个可以微分的损失函数——即在每个点都有导数的损失函数。这并不是太大的负担；我们将在下一节中考察的损失函数都满足这个条件。我们还需要一个可以微分的激活函数，这样我们才能找到*h*(*z*)。同样，到目前为止我们考虑的激活函数基本上都是可微的。
- en: '**Note** *I say “essentially” because the derivative of the ReLU is undefined
    at* x = 0*. The derivative from the left is 0 while the derivative from the right
    is 1\. In practice, implementations choose a particular value to return should
    the argument to the derivative of the ReLU be exactly 0\. For example, TensorFlow
    simply asks if the argument is less than or equal to 0 and, if it is, returns
    0 as the derivative. Otherwise, it returns 1\. This works because, numerically,
    there is so much rounding off happening to floating-point values during calculations
    that it’s unlikely the value passed to the derivative of the ReLU function was
    actually meant to be identically 0.*'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** *我说“基本上”是因为ReLU的导数在* x = 0*时是未定义的。左侧的导数是0，而右侧的导数是1。在实际应用中，当ReLU导数的参数恰好为0时，通常会选择返回一个特定的值。例如，TensorFlow会检查参数是否小于或等于0，如果是，则返回0作为导数，否则返回1。这是因为在计算过程中，浮点值会进行大量的舍入，因此传递给ReLU导数函数的值不太可能恰好是0。*'
- en: The equation for *δ* tells us the error due to the inputs to a particular layer.
    We’ll see next how to use this to get the error from each weight of a layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 方程*δ*告诉我们特定层输入导致的误差。接下来我们将看到如何利用这个误差来计算每个层的权重误差。
- en: With *δ*^((*L*)) in hand, we can propagate the error down to the next layer
    via
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有了*δ*^((*L*))，我们可以通过以下方式将误差传播到下一层：
- en: '*δ*^((*l*)) = ((*W*^((*l*+1)))^(*Tδ*^(*l*+1))) · *h*′(*z*^((*l*)))'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*δ*^((*l*)) = ((*W*^((*l*+1)))^(*Tδ*^(*l*+1))) · *h*′(*z*^((*l*)))'
- en: where, for the next-to-last layer, *l* + 1 = *L*. The *T* represents matrix
    transpose. This is a standard matrix operation that involves a reflection across
    the diagonal so that if
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于倒数第二层，*l* + 1 = *L*。*T*表示矩阵转置。这是一个标准的矩阵操作，涉及到沿对角线的反射，因此如果
- en: '![image](Images/207equ02.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/207equ02.jpg)'
- en: then
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![image](Images/207equ03.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/207equ03.jpg)'
- en: We need the transpose of the weight matrix because we are going in the opposite
    direction from the forward pass. If there are three nodes in layer *l* and two
    in layer *l* + 1, then the weight matrix between them, *W*, is a 2 × 3 matrix,
    so *Wx* is a two-element vector. In backprop, we are going from layer *l* + 1
    to layer *l*, so we transpose the weight matrix to map the two-element vector,
    here *δ*, to a three-element vector for layer *l*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要权重矩阵的转置，因为我们正朝着与前向传播相反的方向进行。如果 *l* 层有三个节点，而 *l* + 1 层有两个节点，那么它们之间的权重矩阵 *W*
    是一个 2 × 3 矩阵，因此 *Wx* 是一个包含两个元素的向量。在反向传播中，我们是从 *l* + 1 层到 *l* 层，所以我们将权重矩阵转置，以便将这个包含两个元素的向量（此处为
    *δ*）映射为 *l* 层的三元素向量。
- en: The *δ*^((*l*)) equation is used for every layer moving backward through the
    network. The output layer values are given by *δ*^((*L*)), which starts the process.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*δ*^((*l*)) 方程用于通过网络向后移动的每一层。输出层的值由 *δ*^((*L*)) 给出，它启动了这个过程。'
- en: Once we have the errors per layer, we can finally find the gradient values we
    need. For the biases, the values are the elements of *δ* for that layer
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了每层的误差，就可以最终找到所需的梯度值。对于偏置，值就是该层的 *δ* 元素。
- en: '![image](Images/208equ01.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/208equ01.jpg)'
- en: for the *j*-th element of the bias for the *l*-th layer. For the weights, we
    need
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *l* 层的 *j* 号偏置项。对于权重，我们需要
- en: '![image](Images/208equ02.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/208equ02.jpg)'
- en: linking the *k*-th output of the previous layer to the *j*-th error for the
    current layer, *l*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将前一层的 *k* 号输出与当前层 *l* 的 *j* 号误差连接起来。
- en: Using the preceding equations for each layer of the network gives us the set
    of weight and bias gradient values needed to continue applying gradient descent.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的每一层的方程，可以得到继续应用梯度下降所需的权重和偏置梯度值集合。
- en: As I hope you can see from this rather dense section, we can use a convenient
    mathematical definition of the error to set up an iterative process that moves
    the error from the output of the network back through the layers of the network
    to the input layer. We cannot calculate the errors for a layer without already
    knowing the errors for the layer after it, so we end up propagating the error
    backward through the network, hence the name *backpropagation*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能从这一段密集的内容中看到的那样，我们可以使用一个方便的数学定义来设定一个迭代过程，将误差从网络的输出通过各层向输入层传递。我们无法计算某一层的误差，除非已经知道后一层的误差，因此我们最终通过网络反向传播误差，这也就是“反向传播”（*backpropagation*）的由来。
- en: Loss Functions
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The *loss function* is used during training to measure how poorly the network
    is doing. The goal of training is to make this value as small as possible, while
    still generalizing to the true characteristics of the data. In theory, we can
    create any loss function we want if we feel it’s relevant to the problem at hand.
    If you read the deep learning literature, you’ll see papers do this all the time.
    Still, most research falls back on a few standard loss functions that, empirically,
    do a good job most of the time. We’ll discuss three of those here: absolute loss
    (sometimes called *L*[1] loss), mean squared error (sometimes called *L*[2] loss),
    and cross-entropy loss.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数* 在训练过程中用来衡量网络的表现有多差。训练的目标是将这个值尽可能地小，同时仍然能很好地概括数据的真实特征。理论上，如果我们觉得某个损失函数与当前问题相关，我们可以创建任何损失函数。如果你阅读深度学习文献，会发现很多论文都在这么做。尽管如此，大多数研究还是依赖于一些标准的损失函数，这些损失函数经验上大多数情况下表现得不错。我们将在这里讨论三种：绝对误差（有时称为
    *L*[1] 损失）、均方误差（有时称为 *L*[2] 损失）和交叉熵损失。'
- en: Absolute and Mean Squared Error Loss
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 绝对误差和均方误差损失
- en: Let’s start with the absolute and mean squared error loss functions. We’ll discuss
    them together because they’re very similar mathematically.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从绝对误差和均方误差损失函数开始。我们将一起讨论它们，因为它们在数学上非常相似。
- en: We’ve seen mean squared error already in our discussion of backprop. Absolute
    loss is new. Mathematically, the two equations are
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在讨论反向传播时已经看过均方误差。绝对误差是新内容。从数学角度看，这两个方程是
- en: '![image](Images/209equ01.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/209equ01.jpg)'
- en: where we’ve labeled them *abs* for *absolute value* and *MSE* for *mean squared
    error*, respectively. Note that we’ll always use *y* for the network output, the
    output from the forward pass with input *x*. We’ll always use *ŷ* for the known
    training class label, which is always an integer label starting with 0.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将它们分别标记为 *abs*（绝对值）和 *MSE*（均方误差）。请注意，我们总是使用 *y* 来表示网络的输出，即通过输入 *x* 进行前向传播得到的输出。我们总是使用
    *ŷ* 来表示已知的训练类标签，通常是从 0 开始的整数标签。
- en: 'Even though we’re writing the loss functions in a simple form, we need to remember
    that when used, the value is really the mean of the loss over the training set
    or minibatch. This is also the origin of *mean* in *mean squared error*. Therefore,
    we really should be writing this:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们将损失函数写成简单的形式，我们需要记住，当使用时，实际值是训练集或小批量上损失的均值。这也是*均值*在*均方误差*中的来源。因此，我们实际上应该这样写：
- en: '![image](Images/209equ02.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/209equ02.jpg)'
- en: Here we’re finding the average of the squared error loss over the *N* values
    in the training set (or minibatch).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算训练集（或小批量）中*N*个值的平方误差损失的平均值。
- en: Both of these loss functions are reasonable if we consider what they are measuring.
    We want the network to output a value that matches the expected value, the sample
    label. The difference between these two is an indication of how wrong the network
    output is. For the absolute loss, we find the difference and drop the sign, which
    is what the absolute value does. For the MSE loss, we find the difference and
    then square it. This also makes the difference positive because multiplying a
    negative number by itself always results in a positive number. As mentioned in
    the “Backpropagation” section on [page 200](#lev1_56), the ![Image](Images/1by2.jpg)
    factor on the MSE loss simplifies the derivative of the loss function but does
    not change how it works.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑它们测量的内容，这两种损失函数都是合理的。我们希望网络输出一个值，与期望的值（即样本标签）匹配。这两者之间的差异表示网络输出有多错误。对于绝对损失，我们找到差异并去掉符号，这就是绝对值的作用。对于MSE损失，我们找到差异然后将其平方。这也使得差异变为正数，因为将负数与其自身相乘总是会得到正数。如[第200页](#lev1_56)“反向传播”部分中所提到的，MSE损失中的![Image](Images/1by2.jpg)因子简化了损失函数的导数，但并没有改变它的工作方式。
- en: The absolute loss and MSE are different, however. The MSE is more sensitive
    to outliers. This is because we’re squaring the difference, and a plot of *y*
    = *x*² grows quickly as *x*, the difference, gets larger. For the absolute loss,
    this effect is minimized because there is no squaring; the difference is merely
    the difference.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，绝对损失和MSE是不同的。MSE对异常值更加敏感。这是因为我们在平方差异，而*y* = *x*²的图像随着*x*（差异）增大而迅速增长。对于绝对损失，这种影响被最小化，因为没有平方，差异只是差异。
- en: In truth, neither of these loss functions are commonly used for neural networks
    when the goal of the network is *classification*, which is our implicit assumption
    in this book. It’s more common to use the cross-entropy loss, presented next.
    We want the network output to lead to the correct class label for the input. However,
    it’s entirely possible to train a network to output a continuous real value instead.
    This is called *regression*, and both of these loss functions are quite useful
    in that context.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当网络的目标是*分类*时，这两种损失函数并不是常用的，这也是本书中的隐含假设。更常见的是使用接下来介绍的交叉熵损失。我们希望网络输出能导致输入的正确类别标签。然而，完全有可能训练一个网络输出一个连续的实值，这叫做*回归*，在这种情况下，这两种损失函数都是非常有用的。
- en: Cross-Entropy Loss
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: 'Even though we can use the absolute and MSE loss functions in training a neural
    network for classification, the most commonly used loss is the *cross-entropy
    loss* (closely related to the log-loss). This loss function assumes the output
    of the network is a softmax (vector) for the multiclass case or a sigmoid (logistic,
    scalar) for the binary case. Mathematically, it looks like this for *M* classes
    in the multiclass case:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们可以使用绝对损失和MSE损失来训练神经网络进行分类，最常用的损失函数是*交叉熵损失*（与对数损失密切相关）。这个损失函数假设网络的输出是多类情况下的softmax（向量）或二类情况下的sigmoid（逻辑斯蒂，标量）。在数学上，对于多类情况中的*M*类，它看起来是这样的：
- en: '![image](Images/210equ01.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/210equ01.jpg)'
- en: 'What is the cross-entropy doing that often makes it a better choice for training
    a neural network for classification? Let’s think about the multiclass case with
    softmax outputs. The definition of *softmax* means that the network outputs can
    be thought of as probability estimates of the likelihood that the input represents
    each of the possible classes. If we have three classes, we might get a softmax
    output that looks like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵到底在做什么，为什么它通常是训练神经网络进行分类时的更好选择？我们来思考一下带有softmax输出的多类情况。*softmax*的定义意味着网络的输出可以被看作是每个可能类别的输入表示的概率估计。如果我们有三个类别，softmax的输出可能看起来是这样的：
- en: '*y* = (0.03, 0.87, 0.10)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = (0.03, 0.87, 0.10)'
- en: This output roughly means that the network thinks there is a 3 percent chance
    the input is of class 0, an 87 percent chance it is of class 1, and a 10 percent
    chance it is of class 2\. This is the output vector, *y*. We compute the loss
    by supplying the actual label via a vector where 0 means *not this class* and
    1 means *this class*. So, the *ŷ* vector associated with the input that led to
    this *y* would be
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出大致意味着网络认为输入属于类别 0 的概率是 3%，属于类别 1 的概率是 87%，属于类别 2 的概率是 10%。这就是输出向量 *y*。我们通过提供实际标签来计算损失，标签向量中
    0 表示*不是这个类别*，1 表示*是这个类别*。因此，与此 *y* 相关联的 *ŷ* 向量就是
- en: '*ŷ* = (0,1,0)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = (0,1,0)'
- en: for an overall loss value of
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 用于整体损失值的
- en: '*ℒ*[ent] = –(0(log 0.03) + 1(log 0.87) + 0(log 0.10)) = 0.139262'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*ℒ*[ent] = –(0(log 0.03) + 1(log 0.87) + 0(log 0.10)) = 0.139262'
- en: The three predictions of the network can be thought of together as a probability
    distribution, just like the one we get when we sum together the likelihoods of
    different outcomes for throwing two dice. We also have a known probability distribution
    from the class label. For the preceding example, the actual class is class 1,
    so we made a probability distribution that assigns no chance to classes 0 and
    2, and 100 percent probability to class 1, the actual class. As the network trains,
    we expect the output distribution to be closer and closer to (0,1,0), the distribution
    for the label.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的三个预测可以被看作是一个概率分布，就像我们在掷两个骰子时，通过加总不同结果的可能性得到的概率分布一样。我们也有一个已知的类标签的概率分布。在前面的例子中，实际类别是类别
    1，因此我们构建了一个概率分布，将类别 0 和类别 2 的概率设为 0，将类别 1 的概率设为 100%，即实际类别。随着网络训练的进行，我们期望输出分布会越来越接近
    (0,1,0)，即标签的分布。
- en: 'Minimizing the cross-entropy drives the network toward better and better predictions
    of the probability distribution for the different classes we want the network
    to learn about. Ideally, these output distributions will look like the training
    labels: 0 for all classes except the actual class, which has an output of 1.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化交叉熵会推动网络不断改进对不同类别概率分布的预测，这些类别是我们希望网络学习的。理想情况下，这些输出分布将与训练标签相似：除了实际类别外，所有类别的输出为
    0，实际类别的输出为 1。
- en: For classification tasks, we usually use the cross-entropy loss. The sklearn
    `MLPClassifier` class uses cross-entropy. Keras supports cross-entropy loss as
    well, but provides many others, including absolute and mean squared error.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，我们通常使用交叉熵损失。sklearn 的 `MLPClassifier` 类使用交叉熵。Keras 也支持交叉熵损失，但还提供了其他许多选项，包括绝对误差和均方误差。
- en: Weight Initialization
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Before we can train a neural network, we need to initialize the weights and
    biases. Step 1 of [Listing 9-1](ch09.xhtml#ch9lis1) on gradient descent says to
    “Pick some intelligent starting values for the weights and biases.”
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练神经网络之前，需要初始化权重和偏差。[列表 9-1](ch09.xhtml#ch9lis1) 中的第 1 步提到“为权重和偏差选择一些智能的初始值”。
- en: The initialization techniques examined here all depend upon selecting random
    numbers in some range. More than that, the random numbers need to be either uniform
    over that range or normally distributed. *Uniformly distributed* means that all
    the values in the range are equally likely to be selected. This is what you get
    for each number, 1 through 6, if you roll a fair die many times over. Normally
    distributed values were introduced in [Chapter 4](ch04.xhtml#ch04). These are
    values with a particular mean, the most likely value returned, and a range around
    the mean over which the likelihood of a value being selected falls off gradually
    toward 0 according to a parameter known as the *standard deviation*. This is the
    classic bell curve shape. Either distribution can be used. The main point is that
    the initial weights are not all the same value (like 0) because if they are, all
    gradients will be the same during backprop, and each weight will change in the
    same way. The initial weights need to be different to break this symmetry and
    allow individual weights to adapt themselves to the training data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这里考察的初始化技术都依赖于在某个范围内选择随机数。更重要的是，这些随机数需要在该范围内均匀分布或符合正态分布。*均匀分布*意味着该范围内的所有值被选中的概率是相等的。这正是你在多次投掷公平的骰子时，每个数字（1到6）出现的概率。正态分布值在[第4章](ch04.xhtml#ch04)中介绍过。正态分布的值具有特定的均值，即最可能的返回值，并且围绕均值有一个范围，在这个范围内，随着偏离均值的增大，选中某个值的可能性逐渐减小，直到接近0，这个过程由一个叫做*标准差*的参数控制。这就是经典的钟形曲线形状。两种分布都可以使用。关键点是初始权重不是所有值都相同（比如0），因为如果它们相同，反向传播过程中所有的梯度将会相同，每个权重将会以相同的方式变化。初始权重需要有所不同，以打破这种对称性，并使得每个权重能够根据训练数据自我调整。
- en: In the early days of neural networks, people initialized the weights and biases
    by choosing values in [0,1) uniformly (*U*(0,1)) or by drawing them from the standard
    normal distribution, *N*(0,1), with a mean of 0 and a standard deviation of 1\.
    These values were often multiplied by some small constant, like 0.01\. In many
    cases, this approach works, at least for simple networks. However, as networks
    became more complex, this simple approach fell apart. Networks initialized in
    this way had trouble learning, and many failed to learn at all.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的早期，人们通过选择[0,1)范围内均匀分布的值（*U*(0,1)）或从标准正态分布中抽取值（*N*(0,1)，均值为0，标准差为1）来初始化权重和偏差。这些值通常会乘以一些小常数，比如0.01。在许多情况下，这种方法有效，至少对于简单的网络来说。然而，随着网络变得更加复杂，这种简单的方法逐渐失败了。以这种方式初始化的网络在学习时遇到困难，许多网络根本无法学习。
- en: 'Let’s fast-forward several decades and a great deal of research later. Researchers
    realized that precisely how the weights of a particular layer should be initialized
    depended primarily on a few things: the type of activation function used and the
    number of weights coming into the layer (*f*[*in*]) and, possibly, going out (*f*[*out*]).
    These realizations led to the main initialization approaches in use today.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快进到几十年后，经过大量研究。研究人员意识到，特定层的权重应该如何初始化，主要取决于几件事：使用的激活函数类型、进入该层的权重数量（*f*[*in*]）以及可能，出去的权重数量（*f*[*out*]）。这些认识促成了今天使用的主要初始化方法。
- en: The sklearn `MLPClassifier` class uses *Glorot initialization*. This is also
    sometimes called *Xavier initialization*, though some toolkits mean something
    different when they use that term.^([1](ch09.xhtml#ch09fn1)) (Note *Xavier* and
    *Glorot* actually refer to the same person.) Let’s see how sklearn uses Glorot
    initialization. The key method in `MLPClassifier` for initializing the weights
    is `_init_coef`. This method uses a uniform distribution and sets the range for
    it so that the weights are in
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn的`MLPClassifier`类使用*Glorot初始化*。这也有时被称为*Xavier初始化*，不过一些工具包使用该术语时表示的含义有所不同。^([1](ch09.xhtml#ch09fn1))（注意，*Xavier*和*Glorot*实际上指的是同一个人。）我们来看看sklearn是如何使用Glorot初始化的。`MLPClassifier`中初始化权重的关键方法是`_init_coef`。该方法使用均匀分布，并为其设置范围，使得权重在
- en: '![image](Images/212equ01.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/212equ01.jpg)'
- en: where the bracket notation indicates the smallest possible value selected (left)
    to the largest possible value (right). As the distribution is uniform, every value
    in that range is equally likely to be selected.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号表示从最小可能值（左）到最大可能值（右）所选的范围。由于分布是均匀的，这个范围内的每个值被选中的概率是相同的。
- en: We did not yet specify what *A* is. This value depends upon the activation function
    used. According to the literature, if the activation function is a sigmoid (logistic),
    then *A* = 2 is suggested. Otherwise, *A* = 6 is recommended.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有具体说明*A*是什么。这个值取决于所使用的激活函数。根据文献，如果激活函数是sigmoid（逻辑函数），则建议*A* = 2。否则，推荐*A*
    = 6。
- en: Now to confuse things. Some toolkits, like Caffe, use an alternate form of Xavier
    initialization by which they mean a multiplier on samples from a standard normal
    distribution. In that case, we initialize the weights with draws from
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，事情变得更加复杂了。一些工具包，比如Caffe，使用Xavier初始化的另一种形式，即通过标准正态分布中的样本来进行乘法初始化。在这种情况下，我们通过从中抽取样本来初始化权重：
- en: '![image](Images/212equ02.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/212equ02.jpg)'
- en: 'To add even more confusion, the introduction of the rectified linear unit (ReLU)
    resulted in a further recommended change. This is known now as *He initialization*
    and it replaces the 1 in Xavier initialization with a 2:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步增加混淆，修正线性单元（ReLU）的引入导致了一个进一步推荐的变化。现在，这被称为*He初始化*，它用2替代了Xavier初始化中的1：
- en: '![image](Images/212equ03.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/212equ03.jpg)'
- en: 'For more on this, see “Delving Deep into Rectifiers: Surpassing Human-Level
    Performance on ImageNet Classification” by Kaiming He et al.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多内容，请参阅Kaiming He等人的《深入探讨修正线性单元：超越人类级别的ImageNet分类性能》。
- en: The key point with these initialization schemes is that the old-school “small
    random value” is replaced by a more principled set of values that take the network
    architecture into account via *f*[*in*] and *f*[*out*].
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这些初始化方案的关键在于，传统的“随机小值”被一组更加有原则的值所取代，这些值通过*f*[*in*]和*f*[*out*]考虑了网络架构。
- en: The preceding discussion ignored bias values. This was intentional. While it
    might be okay to initialize the bias values instead of leaving them all 0, prevailing
    wisdom, which is fickle and fluid, currently says it’s best to initialize them
    all to 0\. That said, sklearn `MLPClassifier` initializes the bias values in the
    same way as the weights.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的讨论忽略了偏差值。这是故意的。虽然初始化偏差值而不是将其全部设为0可能是可以接受的，但当前的主流观点（尽管这个观点常常变化）认为最好将其全部初始化为0。话虽如此，sklearn
    的 `MLPClassifier` 以与权重相同的方式初始化偏差值。
- en: Overfitting and Regularization
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合与正则化
- en: The goal of training a model is for it to learn essential, general features
    of the parent distribution the dataset is sampled from. That way, when the model
    encounters new inputs, it’s prepared to interpret them correctly. As we’ve seen
    in this chapter, the primary method for training a neural network involves optimization—looking
    for the “best” set of parameters so that the network makes as few errors as possible
    on the training set.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的目标是让它学习数据集所采样的父分布的基本、通用特征。这样，当模型遇到新的输入时，它就能正确地进行解释。正如我们在本章中所看到的，训练神经网络的主要方法是优化——寻找“最佳”的参数集，使得网络在训练集上的错误尽可能少。
- en: However, it’s not enough to simply look for the best set of values that minimizes
    the training error. If we make no mistakes when classifying the training data,
    it’s often the case that we’ve overfitted and haven’t actually learned general
    features of the data. This is more likely the situation with traditional models,
    neural network or classical, and less so with deep models like the convolutional
    networks of [Chapter 12](ch12.xhtml#ch12).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅寻找能最小化训练误差的最佳值集是不够的。如果我们在分类训练数据时没有犯错，通常意味着我们已经过拟合，并没有真正学到数据的一般特征。这种情况在传统模型中更为常见，无论是神经网络还是经典模型，而对于像[第12章](ch12.xhtml#ch12)中提到的卷积网络这样的深度模型来说，情况要少一些。
- en: Understanding Overfitting
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解过拟合
- en: We’ve mentioned overfitting from time to time before now but have not gained
    any good intuition as to what it is. One way to think of overfitting is to consider
    a separate problem, the problem of fitting a function to a set of points. This
    is known as *curve fitting*, and one approach to it is to optimize some measure
    of error over the points by finding parameters to the function that minimize the
    error. This should sound familiar. It’s exactly what we do when training a neural
    network.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经不时提到过拟合，但还没有对它形成清晰的直觉。思考过拟合的一种方式是考虑一个独立的问题——将一个函数拟合到一组点上。这被称为*曲线拟合*，而其一种方法是通过找到能够最小化误差的函数参数，从而优化这些点上的误差度量。这应该听起来很熟悉，这正是我们在训练神经网络时所做的事情。
- en: 'As an example of curve fitting, consider the following points:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 作为曲线拟合的一个例子，考虑以下几点：
- en: '| *x* | *y* |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *y* |'
- en: '| --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.00 | 50.0 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 0.00 | 50.0 |'
- en: '| 0.61 | –17.8 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 0.61 | –17.8 |'
- en: '| 1.22 | 74.1 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 1.22 | 74.1 |'
- en: '| 1.83 | 29.9 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 1.83 | 29.9 |'
- en: '| 2.44 | 114.8 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 2.44 | 114.8 |'
- en: '| 3.06 | 55.3 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 3.06 | 55.3 |'
- en: '| 3.67 | 66.0 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 3.67 | 66.0 |'
- en: '| 4.28 | 89.1 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 4.28 | 89.1 |'
- en: '| 4.89 | 128.3 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 4.89 | 128.3 |'
- en: '| 5.51 | 180.8 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 5.51 | 180.8 |'
- en: '| 6.12 | 229.7 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 6.12 | 229.7 |'
- en: '| 6.73 | 229.3 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 6.73 | 229.3 |'
- en: '| 7.34 | 227.7 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 7.34 | 227.7 |'
- en: '| 7.95 | 354.9 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 7.95 | 354.9 |'
- en: '| 8.57 | 477.1 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 8.57 | 477.1 |'
- en: '| 9.18 | 435.4 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 9.18 | 435.4 |'
- en: '| 9.79 | 470.1 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 9.79 | 470.1 |'
- en: We want to find a function, *y* = *f* (*x*), that describes these points—a function
    that might have been the parent function these points were measured from, albeit
    noisily.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到一个函数，*y* = *f* (*x*)，描述这些点——一个函数，可能是这些点从中测量出来的父函数，尽管有噪声。
- en: 'Typically, when curve fitting, we already know the form of the function; it’s
    the parameters we’re looking for. But what if we didn’t know the exact form of
    the function, only that it was some kind of polynomial? In general, a polynomial
    looks like this for some maximum exponent, *n*:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在曲线拟合时，我们已经知道函数的形式；我们要找的是参数。但如果我们不知道函数的确切形式，只知道它是某种多项式，该怎么办？一般来说，多项式的形式如下，对于某个最大指数*n*：
- en: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*² + *a*[3]*x*³ + … + *a[n]x^n*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*² + *a*[3]*x*³ + … + *a[n]*x^n*'
- en: The goal of fitting a polynomial to a dataset is to find the parameters, *a*[0],*a*[1],*a*[2],…,*a*[*n*].
    The method for doing this usually minimizes the squared difference between *y*,
    a given output for a given *x* position, and *f* (*x*), the function output at
    the same *x* for the current set of parameters. This should sound very familiar,
    as we discussed using precisely this type of loss function for training a neural
    network.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合多项式到数据集的目标是找到参数，*a*[0]，*a*[1]，*a*[2]，…，*a*[*n*]。通常的方法是最小化 *y*，即给定 *x* 位置的输出，与
    *f* (*x*)，即当前参数集合下的函数输出之间的平方差。这应该听起来很熟悉，因为我们讨论过在神经网络训练中使用这种类型的损失函数。
- en: How does this relate to overfitting? Let’s plot the previous dataset, along
    with the result of fitting two different functions to it. The first function is
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这与过拟合有何关联？让我们将前面的数据集绘制出来，以及拟合两个不同函数的结果。第一个函数是
- en: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*²'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*²'
- en: which is a quadratic function, the type of function you may have learned to
    hate as a beginning algebra student. The second function is
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二次函数，这是你可能作为初学代数学生学会讨厌的类型函数。第二个函数是
- en: '*y* = *a*[0] + *a*[1]*x* + *a*[2]*x*² + *a*[3]*x*³ + … + *a*[14]*x*^(14) +
    *a*[15]*x*^(15)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *a*[0] + *a*[1]*x* + *a*[2]**x*² + *a*[3]*x*³ + … + *a*[14]*x*^(14) +
    *a*[15]*x*^(15)'
- en: which is a 15th-degree polynomial. The results are shown in [Figure 9-5](ch09.xhtml#ch9fig5).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个15次多项式。结果如[图9-5](ch09.xhtml#ch9fig5)所示。
- en: '![image](Images/09fig05.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig05.jpg)'
- en: '*Figure 9-5: A dataset and two functions fit to it: a quadratic (dashed) and
    a 15th-degree polynomial (solid)*'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-5：数据集和拟合到它的两个函数：一个二次函数（虚线）和一个15次多项式（实线）*'
- en: Which function does a better job of capturing the general trend of the dataset?
    The quadratic clearly follows the general trend of the data, while the 15th-degree
    polynomial is all over the place. Look again at [Figure 9-5](ch09.xhtml#ch9fig5).
    If all we use to decide that we have fit the data well is the distance between
    the data points and the corresponding function value, we’d say that the 15th-degree
    polynomial is the better fit; it passes through nearly all the data points, after
    all. This is analogous to training a neural network and achieving perfection on
    the training set. The cost of that perfection might well be a poor ability to
    generalize to new inputs. The quadratic fit of [Figure 9-5](ch09.xhtml#ch9fig5)
    did not hit the data points, but it did capture the general trend of the data,
    making it more useful should we want to make predictions about the *y* values
    we’d expect to get for a new *x* value.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个函数更好地捕捉了数据集的一般趋势？明显是二次函数更符合数据的一般趋势，而15次多项式则到处都是。再看一下[图9-5](ch09.xhtml#ch9fig5)。如果我们决定我们已经很好地拟合了数据的唯一依据是数据点与相应函数值之间的距离，我们可能会说15次多项式是更好的拟合；毕竟它几乎通过了所有数据点。这类似于训练神经网络并在训练集上达到完美。然而，这种完美的代价很可能是无法很好地推广到新的输入。二次拟合[图9-5](ch09.xhtml#ch9fig5)未命中数据点，但却捕捉了数据的一般趋势，这使其更有用，如果我们想预测我们将得到的*y*值的新*x*值。
- en: When a human wants to fit a curve to something like our sample dataset, they
    usually look at the data and, noticing the general trend, select the function
    to fit. It might also be the case that the expected functional form is already
    known from theory. If we want to be analogous to neural networks, however, we’ll
    find ourselves in a situation where we don’t know the proper function to fit,
    and need to find a “best” one from the space of functions of *x* along with its
    parameters.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类想要拟合一条曲线到类似我们样本数据集的东西时，他们通常会观察数据，注意到一般趋势，然后选择一个合适的函数进行拟合。也可能在理论上已经知道期望的函数形式。然而，如果我们想要与神经网络类比，我们会发现自己处于一种情况——我们不知道应该拟合哪个函数，而是需要从
    *x* 的函数空间中找到一个“最佳”函数及其参数。
- en: Hopefully, this example drives home the idea that training a neural network
    is not an optimization problem like other optimization problems—we need something
    to push the function the network is learning in a direction that captures the
    essence of the data without falling into the trap of paying too much attention
    to specific features of the training data. That something is regularization, and
    you need it, especially for large networks that have a huge capacity.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这个例子能帮助你理解，训练神经网络并不是一个像其他优化问题那样的优化问题——我们需要某种方式将网络学习的函数推向一个方向，使其能够捕捉数据的本质，而不是过度关注训练数据中的特定特征。这种方式就是正则化，而且它是必需的，尤其是对于那些具有巨大容量的大型网络。
- en: Understanding Regularization
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解正则化
- en: '*Regularization* is anything that pushes the network to learn the relevant
    features of the parent distribution and not the details of the training set. The
    best form of regularization is increasing the size and representative nature of
    the training set. The larger the dataset and the better it represents all the
    types of samples the network will encounter in the wild, the better it will learn.
    Of course, we’re typically forced to work with a finite training set. The machine
    learning community has spent, and is spending, untold time and energy learning
    how to get more from smaller datasets.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化* 是指任何能够推动网络学习父分布相关特征，而不是训练集细节的手段。最好的正则化方式是增加训练集的大小和代表性。数据集越大，越能代表网络在实际应用中遇到的所有样本类型，网络学习效果就越好。当然，我们通常需要在有限的训练集上工作。机器学习社区花费了大量的时间和精力，研究如何从较小的数据集中获得更多信息。'
- en: In [Chapter 5](ch05.xhtml#ch05), we encountered perhaps the second-best way
    to regularize a model, data augmentation. This is a proxy for having a larger
    dataset, where we use the data we do have to generate new training samples that
    are plausibly from the parent distribution. For example, we considered increasing
    a limited set of training images by simple rotations, flips, and shifts of the
    images already in the training set. Data augmentation is powerful, and you should
    use it when possible. It’s particularly easy to apply when working with images
    as inputs, though in [Chapter 5](ch05.xhtml#ch05) we also saw a way to augment
    a dataset consisting of continuously valued vectors.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.xhtml#ch05)中，我们遇到了或许是第二好的正则化方法——数据增强。这是通过使用现有数据生成新的训练样本来模拟更大数据集的一种方式，这些新样本看起来是来自父分布的。例如，我们考虑通过对训练集中已有的图像进行简单的旋转、翻转和位移，来增加有限的训练图像数据集。数据增强非常强大，应该在可能的情况下使用。尤其是在使用图像作为输入时，这种方法特别容易应用，尽管在[第5章](ch05.xhtml#ch05)中我们也看到了增强一个包含连续值向量的数据集的方法。
- en: 'We now have two tricks in our regularization toolbox: more data and data augmentation.
    These are the best tricks to know, but there others that you should use when available.
    Let’s look at two more: L2 regularization and dropout. The former is now standard
    and widely supported by the toolkits, including sklearn and Keras. The latter
    is powerful and was a game changer when it appeared in 2012.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在正则化工具箱里有了两种技巧：更多的数据和数据增强。这些是最好的技巧，但还有其他技巧，应该在可能时使用。让我们来看两个更多的技巧：L2 正则化和
    dropout。前者现在已经成为标准，并且被各大工具包广泛支持，包括 sklearn 和 Keras。后者则非常强大，2012年出现时堪称改变游戏规则的技术。
- en: L2 Regularization
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: L2 正则化
- en: A model with a few weights that have large values is somehow less simple than
    a model that has smaller weights. Therefore, keeping the weights small will hopefully
    allow the network to implement a simpler function better suited to the task we
    want it to learn.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有几个大权重的模型，其复杂度往往高于一个具有较小权重的模型。因此，保持权重较小有助于网络实现一个更简单的函数，更适合我们希望其学习的任务。
- en: We can encourage the weights to be small by using L2 regularization. *L2 regularization*
    adds a term to the loss function so that the loss becomes
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用L2正则化来鼓励权重变小。*L2正则化*向损失函数添加一个项，使得损失变为
- en: '![image](Images/216equ01.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/216equ01.jpg)'
- en: where the first term is whatever loss we’re already using, and the second term
    is the new L2 regularization term. Notice that the loss is a function of the input
    (*x*), the label (*y*), the weights (*w*), and the biases (*b*), where we mean
    all the weights and all the biases of the network. The regularization term is
    a sum over all the weights in the network and only the weights. The “L2” label
    is what causes us to square the weights.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一个项是我们已经使用的任何损失，第二个项是新的L2正则化项。注意，损失是输入（*x*）、标签（*y*）、权重（*w*）和偏置（*b*）的函数，其中我们指的是网络的所有权重和所有偏置。正则化项是对网络中所有权重的求和，并且仅涉及权重。“L2”标签就是让我们对权重进行平方的原因。
- en: 'Here *L2* refers to the type of norm or distance. You might be familiar with
    the equation for the distance between two points on the plane: *d*² = (*x*[2]
    *– x*[1])² + (*y*[2] *– y*[1])². This is the *Euclidean distance*, also known
    as the *L2 distance*, because the values are squared. This is why the regularization
    term is called *L2* and the weight values are squared. It’s also possible to use
    an L1 loss term, where instead of squaring the weights, one uses the absolute
    value. In practice, L2 regularization is more common and, at least empirically,
    seems to work better for neural network classifiers.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*L2*指的是一种范数或距离类型。你可能熟悉平面上两点之间距离的公式：*d*² = (*x*[2] – *x*[1])² + (*y*[2] –
    *y*[1])²。这是*欧几里得距离*，也称为*L2距离*，因为数值是平方的。这就是为什么正则化项被称为*L2*，并且权重值被平方的原因。也可以使用L1损失项，在这种情况下，不是对权重进行平方，而是使用其绝对值。在实践中，L2正则化更为常见，至少从经验上看，它似乎对于神经网络分类器效果更好。
- en: The *λ* (lambda) multiplier sets the importance of this term; the larger it
    is, the more it dominates the overall loss used to train the network. Typical
    values of *λ* are around 0.0005\. We’ll see in a little bit why the multiplier
    is *λ*/2 and not just *λ*.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ*（lambda）乘数设置了这个项的重要性；它越大，越会主导训练网络时使用的整体损失。*λ*的典型值大约是0.0005。稍后我们将看到，为什么乘数是*λ*/2，而不仅仅是*λ*。'
- en: What is the L2 term doing? Recall that the loss is the thing we want to minimize
    while training. The new L2 term sums the squares of the weights of the network.
    If weights are large, the loss is large, and that’s something we don’t want while
    training. Smaller weights make the L2 term smaller, so gradient descent will favor
    small weights, whether they are positive or negative, since we square the weight
    value. If all the weights of the network are relatively small, and none strongly
    dominate, then the network will use all of the weights to represent the data,
    and this is a good thing when it comes to preventing overfitting.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: L2项是做什么的？回顾一下，损失是我们在训练时想要最小化的东西。新的L2项对网络的权重进行平方求和。如果权重很大，损失就很大，而这是我们在训练时不想要的。较小的权重会使L2项变小，因此梯度下降会偏向小权重，无论它们是正数还是负数，因为我们对权重值进行平方。如果网络的所有权重都相对较小，并且没有任何权重特别占主导地位，那么网络会利用所有权重来表示数据，这在防止过拟合方面是一个好现象。
- en: L2 regularization is also known as *weight decay* because of what the L2 term
    does during backprop. Backprop gives us the partial derivative of the loss function
    with respect to *w*[*i*]. Adding L2 regularization means that the partial derivative
    of the total loss now adds in the partial derivative of the L2 term itself with
    respect to any particular weight, *w*[*i*]. The derivative of ![Image](Images/217equ01.jpg)
    is *λw*; the ![Image](Images/1by2.jpg) cancels the factor of 2 that would otherwise
    be there. Also, since we want the partial derivative with respect to a specific
    weight, *w*[*i*], all the other parts of the L2 term go to 0\. The net effect
    is that the update for weight *w*[*i*] during gradient descent becomes
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化也被称为*权重衰减*，因为L2项在反向传播过程中所起的作用。反向传播给出了损失函数关于*W*[*i*]的偏导数。添加L2正则化意味着，总损失的偏导数现在还包含L2项本身关于某个特定权重*W*[*i*]的偏导数。![Image](Images/217equ01.jpg)的导数是*λW*；![Image](Images/1by2.jpg)抵消了原本会出现的2的因子。此外，由于我们想要得到关于特定权重*W*[*i*]的偏导数，L2项的其他部分会变为0。最终效果是，梯度下降过程中对于权重*W*[*i*]的更新变为
- en: '![image](Images/217equ02.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/217equ02.jpg)'
- en: where *η* (eta) is the learning rate, and we are ignoring any additional momentum
    term. The *ηλw*[*i*] term is new. It is due to L2 regularization, and we can see
    that it’s pushing the weights toward 0 as training progresses because both *η*
    and *λ* are < 1, so on each minibatch, we’re subtracting some small fraction of
    the weight value. The weight can still increase, but to do so, the gradient of
    the original loss must be large.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*η*（eta）是学习率，我们忽略任何额外的动量项。*ηλw*[*i*]项是新的，它是由于L2正则化的影响，我们可以看到随着训练的进行，它在推动权重趋向0，因为*η*和*λ*都小于1，因此在每个小批次上，我们都会从权重值中减去一小部分。权重仍然可以增加，但要做到这一点，原始损失的梯度必须很大。
- en: We previously stated that the form of the loss function is up to us, the developer
    of the network. A regularization term isn’t the only kind of term we can add to
    the loss function. As we did with the L2 term, we can create and add terms to
    change the behavior of the network during training and help it learn what we want
    it to learn. This is a powerful technique that can be used to customize various
    aspects of what a neural network learns.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，损失函数的形式是由我们作为网络的开发者决定的。正则化项不是唯一可以添加到损失函数中的项。正如我们使用L2项一样，我们可以创建并添加其他项，以改变网络在训练过程中的行为，帮助它学习我们希望它学习的内容。这是一种强大的技术，可以用来定制神经网络学习的各个方面。
- en: Dropout
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）
- en: Dropout took the machine learning community by storm when it appeared in 2012,
    see “Imagenet Classification with Deep Convolutional Neural Networks” by Alex
    Krizhevsky et al. As of Fall 2020, this paper has been cited over 70,000 times,
    and as one well-known machine learning researcher told me privately at the time,
    “If we had had dropout in the 1980s, this would be a different world now.” So,
    what is dropout, and why was everyone so excited by it?
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）在2012年出现时震撼了机器学习社区，参见Alex Krizhevsky等人发表的论文《使用深度卷积神经网络进行Imagenet分类》。截至2020年秋季，该论文已被引用超过70,000次，正如当时一位著名的机器学习研究人员私下告诉我：“如果我们在1980年代就有了dropout，今天的世界将完全不同。”那么，什么是dropout，为什么大家如此兴奋？
- en: 'To answer that question, we need to review the concept of ensembles of models.
    We talked about them a bit in [Chapter 6](ch06.xhtml#ch06). An ensemble is a group
    of models, all slightly different and all trained on the same dataset or a slightly
    different version of the dataset. The idea is straightforward: since training
    most models involves randomness, training multiple similar models should result
    in a set that is mutually reinforcing—one where the set of outputs can be combined
    to produce a result that is better than any one model alone. Ensembles are useful,
    and we use them often, but they come at a price in terms of runtime. If it takes
    *x* milliseconds to run a sample through a neural network, and we have an ensemble
    of 20 networks, then our evaluation time (inference time) has jumped to 20*x*
    milliseconds, ignoring the possibility of parallel execution. In some situations,
    that is unacceptable (to say nothing of the storage and power requirements for
    20 big networks versus 1). Since the net result of an ensemble of models is better
    overall performance, we can say that an ensemble is a kind of regularizer as well
    since it embodies the “wisdom of the crowd.”'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们需要回顾一下模型集成的概念。我们在[第6章](ch06.xhtml#ch06)中稍微讨论过它。集成是一组模型，这些模型稍有不同，并且都在同一数据集或该数据集的略微不同版本上进行训练。这个想法很简单：由于训练大多数模型都涉及随机性，训练多个相似的模型应该会产生一个互相强化的结果——一个可以将输出结果结合起来，从而产生比任何单个模型更好的结果的集合。集成是有用的，我们经常使用它们，但它们在运行时间上有一定的代价。如果通过一个神经网络运行一个样本需要*x*毫秒，而我们有一个由20个网络组成的集成，那么我们的评估时间（推理时间）将跳到20*x*毫秒，忽略并行执行的可能性。在某些情况下，这是无法接受的（更不用说20个大网络与1个网络在存储和功耗方面的差异了）。由于模型集成的最终结果是更好的整体性能，我们可以说，集成本质上是一种正则化方法，因为它体现了“集体智慧”。
- en: '*Dropout* takes the ensemble idea to an extreme but does so only during training
    and without creating a second network so that in the end, we still have one model
    to deal with. Like many good ideas in statistics, this one requires randomness.
    Right now, when we train the network, we do a forward pass using the current weights
    and biases. What if, during that forward pass, we randomly assign a 0 or a 1 to
    each node of the network so that nodes with a 1 are used in the next layer while
    nodes with a 0 are dropped out? We’d effectively be running the training samples
    through a different neural network configuration each time. For example, see [Figure
    9-6](ch09.xhtml#ch9fig6).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout* 将集成思想推向极致，但它仅在训练期间进行，而且不创建第二个网络，因此最终我们仍然只需要处理一个模型。就像许多统计学中的好想法一样，这个方法也需要随机性。现在，当我们训练网络时，我们使用当前的权重和偏置进行前向传递。如果在前向传递过程中，我们随机为网络的每个节点分配一个0或1，那么值为1的节点会在下一层被使用，而值为0的节点则会被丢弃？实际上，我们每次都会让训练样本通过一个不同的神经网络配置。例如，见[图
    9-6](ch09.xhtml#ch9fig6)。'
- en: '![image](Images/09fig06.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig06.jpg)'
- en: '*Figure 9-6: Possible networks used when applying dropout during training*'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-6：在训练期间应用 dropout 时可能使用的网络*'
- en: Here we show the network of [Figure 8-1](ch08.xhtml#ch8fig1) but with a 0 or
    1 for each of the hidden nodes. This 0 or 1 determines whether the output is used
    or not. The heavy lines in the network show the connections that are still valid.
    In other words, the heavy lines show the network that was actually used to create
    the output accumulated for backprop. If we do this for each training sample, we
    can readily see that we’ll be training a vast number of neural networks, each
    trained on a single sample. Moreover, since the weights and biases persist between
    forward passes, all the networks will share those weights in the hope that the
    process will reinforce good weight values that represent the essence of the dataset.
    As we’ve mentioned several times in this chapter, learning the essence of the
    data is the goal of training; we want to generalize well to new data from the
    same virtual parent distribution that generated the training set in the first
    place. Dropout is serious regularization.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们展示了[图 8-1](ch08.xhtml#ch8fig1)的网络，但每个隐藏节点的值为0或1。这个0或1决定了输出是否被使用。网络中的粗线表示仍然有效的连接。换句话说，粗线显示的是实际用于生成反向传播累计输出的网络。如果我们对每个训练样本都这样做，我们很容易看到，我们将训练大量的神经网络，每个网络都只训练一个单独的样本。此外，由于权重和偏置在前向传递之间是持续存在的，所有网络将共享这些权重，期望这一过程能够强化那些表示数据集本质的良好权重值。正如我们在本章中多次提到的，学习数据的本质是训练的目标；我们希望能很好地对来自相同虚拟父分布的新数据进行泛化，这个父分布最初生成了训练集。Dropout
    是一种严密的正则化方法。
- en: I previously said that we “randomly assign a 0 or a 1” to the nodes. Do we assign
    them equally? The probability with which we drop nodes in a layer is something
    we get to specify. Let’s call it *p*. Typically, *p* = 0.5, meaning about 50 percent
    of the nodes in a layer will be dropped for each training sample. Setting *p*
    = 0.8 would drop 80 percent of the nodes, while *p* = 0.1 would drop only 10 percent.
    Sometimes a different probability is used for different layers of the network,
    especially the first input layer, which should use a smaller probability than
    the hidden nodes. If we drop too many of the inputs, we’ll lose the source of
    the signal we’re trying to get the network to recognize. Dropout applied to the
    input layer can be thought of as a form of data augmentation.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前说过，我们“随机地为节点分配0或1”。我们是否是等概率分配的呢？我们丢弃一个层中的节点的概率是我们可以指定的。我们称之为 *p*。通常，*p* =
    0.5，这意味着每个训练样本中，该层大约50%的节点会被丢弃。如果设置 *p* = 0.8，则80%的节点会被丢弃，而 *p* = 0.1 则只有10%的节点会被丢弃。有时，不同的网络层会使用不同的概率，尤其是输入层，它的丢弃概率通常比隐藏层小。如果我们丢弃了太多输入节点，我们就会失去我们试图让网络识别的信号来源。对输入层应用的
    dropout 可以看作是一种数据增强的形式。
- en: Conceptually, dropout is training a large set of networks that share weights.
    The output of each of these networks can be combined with the others via a geometric
    mean, assuming we use a softmax output. The geometric mean of two numbers is the
    square root of their product. The geometric mean of *n* numbers is the *n*th root
    of their product. In the case of dropout, it turns out that this can be approximated
    by using the entire network with all the weights multiplied by the probability
    that they would be included. Given we said *p* is the probability that a node
    is dropped, the weights need to be multiplied by 1 *– p*, as that is the probability
    the node would not be dropped. So, if we fix *p* = 0.5 and use it for all the
    nodes, then the final network is the one where all the weights are divided by
    2.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，dropout 就是训练一大组共享权重的网络。每个网络的输出可以通过几何平均与其他网络的输出结合，前提是我们使用 softmax 输出。两个数的几何平均是它们乘积的平方根。*n*
    个数的几何平均是它们乘积的 *n* 次根。在 dropout 的情况下，事实证明，这可以通过使用整个网络并将所有权重乘以它们被包含的概率来近似。假设我们说
    *p* 是节点被丢弃的概率，那么权重需要乘以 1 *– p*，因为这是节点不会被丢弃的概率。所以，如果我们将 *p* = 0.5 并将其用于所有节点，那么最终的网络就是所有权重都除以
    2 的网络。
- en: As of this writing, sklearn’s `MLPClassifier` class does not support dropout,
    but Keras most certainly does, so we’ll see dropout again in [Chapter 12](ch12.xhtml#ch12).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，sklearn 的 `MLPClassifier` 类不支持 dropout，但 Keras 肯定支持，因此我们将在[第 12 章](ch12.xhtml#ch12)中再次看到
    dropout。
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Because this is an important chapter, let’s review what we’ve learned in a
    little more depth. In this chapter, we described how to train a neural network
    using gradient descent and backpropagation. The overall sequence of steps is as
    follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个重要的章节，让我们更深入地复习一下我们所学的内容。在本章中，我们描述了如何使用梯度下降和反向传播来训练神经网络。整体步骤的顺序如下：
- en: Select the architecture of the model. This means the number of layers, their
    sizes, and the type of activation function.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型的架构。这意味着选择层数、层的大小以及激活函数的类型。
- en: Initialize the weights and biases of the network using intelligently selected
    initial values.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用智能选择的初始值初始化网络的权重和偏置。
- en: Run a minibatch of training samples through the network and compute the mean
    loss over the minibatch. We discussed common loss functions.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一小批训练样本通过网络，并计算小批量的平均损失。我们讨论了常见的损失函数。
- en: Using backpropagation, calculate the contribution of each weight and bias to
    the overall loss for the minibatch.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播，计算每个权重和偏置对小批量整体损失的贡献。
- en: Using gradient descent, update the weight and bias values of the model based
    on the contributions found via backpropagation. We discussed stochastic gradient
    descent and its relationship to the concept of minibatches.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降，根据反向传播找到的贡献来更新模型的权重和偏置值。我们讨论了随机梯度下降及其与小批量概念的关系。
- en: Repeat from step 3 until the desired number of epochs or minibatches have been
    processed, or the loss has dropped below some threshold, or stopped changing much,
    or when the score on a validation set of samples has reached its minimum value.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤 3 开始重复，直到处理完所需的周期或小批量，或者损失降到某个阈值以下，或者停止变化，或者当验证集样本的分数达到最小值时。
- en: If the network isn’t learning well, apply regularization and train again. We
    looked at L2 regularization and dropout in this chapter. Data augmentation, or
    increasing the size or representativeness of the training set, can also be thought
    of as regularization.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果网络学习不好，应用正则化并重新训练。我们在本章中讨论了 L2 正则化和 dropout。数据增强，即增加训练集的大小或代表性，也可以视为一种正则化方法。
- en: The goal of training a neural network is to learn the parameters of a model
    that generalizes well to unseen inputs. This is the goal of all supervised machine
    learning. For a neural network, we know it’s able to approximate any function,
    with enough capacity and enough training data. Naïvely, we may think that we are
    doing nothing more than ordinary optimization, but, in an important sense, we
    are not. Perfection on the training set is often not a good thing; it’s often
    a sign of overfitting. Instead, we want the model to learn a function that captures
    the essential nature of the function implied by the training set. We use the test
    data to give us confidence that we’ve learned a useful function.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的目标是学习一个能很好地对未见输入进行泛化的模型参数。这是所有监督学习的目标。对于神经网络，我们知道它在具备足够容量和足够训练数据的情况下能够逼近任何函数。天真地看，我们可能会认为这不过是普通的优化过程，但从一个重要角度来看，实际上并非如此。训练集上的完美表现往往不是一件好事，它通常是过拟合的表现。相反，我们希望模型学习到一个能捕捉训练集所暗示的函数本质的函数。我们使用测试数据来确保我们已经学到一个有用的函数。
- en: In the next chapter, we’ll get real and explore traditional neural networks
    through a series of experiments using sklearn.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将通过一系列使用 sklearn 的实验，深入探索传统神经网络。
- en: '[1.](ch09.xhtml#Rch09fn1) For more on these, see Glorot, Xavier, and Yoshua
    Bengio. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.”'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.](ch09.xhtml#Rch09fn1) 关于这些内容，参见 Glorot, Xavier 和 Yoshua Bengio. “理解训练深度前馈神经网络的难度。”'
