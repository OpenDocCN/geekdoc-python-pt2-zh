<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch05"><span epub:type="pagebreak" id="page_103"/><strong><span class="big">5</span><br/>LINEAR ALGEBRA</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">Formally, linear algebra is the study of linear equations, in which the highest power of the variable is one. However, for our purposes, <em>linear algebra</em> refers to multidimensional mathematical objects—like vectors and matrices—and operations on them. This is how linear algebra is typically applied in deep learning, and how data is manipulated in programs that implement deep learning algorithms. By making this distinction, we are throwing away a massive amount of fascinating mathematics, but as our goal is to understand the mathematics used and applied in deep learning, we can hopefully be forgiven.</p>&#13;
<p class="indent">In this chapter, I’ll introduce the objects used in deep learning, specifically scalars, vectors, matrices, and tensors. As we’ll see, all of these objects are actually tensors of various orders. We’ll discuss tensors from a mathematical, notational perspective and then experiment with them using NumPy. NumPy was explicitly designed to add multidimensional arrays <span epub:type="pagebreak" id="page_104"/>to Python, and they are good, though incomplete, analogues for the mathematical objects we’ll work with in this chapter.</p>&#13;
<p class="indent">We’ll spend the bulk of the chapter learning how to do arithmetic with tensors, which is of fundamental importance in deep learning. Most of the effort in implementing highly performant deep learning toolkits involves finding ways to do arithmetic with tensors as efficiently as possible.</p>&#13;
<h3 class="h3" id="ch05lev1_1">Scalars, Vectors, Matrices, and Tensors</h3>&#13;
<p class="noindent">Let’s introduce our cast of characters. I’ll relate them to Python variables and NumPy arrays to show how we’ll implement these objects in code. Then I’ll present a handy conceptual mapping between tensors and geometry.</p>&#13;
<h4 class="h4" id="ch05lev2_1">Scalars</h4>&#13;
<p class="noindent">Even if you’re not familiar with the word, you’ve known what a scalar is since the day you first learned to count. A <em>scalar</em> is just a number, like 7, 42, or π. In expressions, we’ll use <em>x</em> to mean a scalar, that is, the ordinary notation used for variables. To a computer, a scalar is a simple numeric variable:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">s = 66</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">s</span><br/>&#13;
66</p>&#13;
<h4 class="h4" id="ch05lev2_2">Vectors</h4>&#13;
<p class="noindent">A <em>vector</em> is a 1D array of numbers. Mathematically, a vector has an orientation, either horizontal or vertical. If horizontal, it’s a <em>row vector</em>. For example,</p>&#13;
<div class="imagec" id="ch05equ01"><img src="Images/05equ01.jpg" alt="Image" width="404" height="21"/></div>&#13;
<p class="noindent">is a row vector of three elements or components. Note, we’ll use <em>x</em>, a lowercase letter in bold, to mean a vector.</p>&#13;
<p class="indent">Mathematically, vectors are usually assumed to be <em>column vectors</em>,</p>&#13;
<div class="imagec" id="ch05equ02"><img src="Images/05equ02.jpg" alt="Image" width="393" height="94"/></div>&#13;
<p class="noindent">where <em><strong>y</strong></em> has four components, making it a four-dimensional (4D) vector. Notice that in <a href="ch05.xhtml#ch05equ01">Equation 5.1</a> we used square brackets, whereas in <a href="ch05.xhtml#ch05equ02">Equation 5.2</a> we used parentheses. Either notation is acceptable.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_105"/>In code, we usually implement vectors as 1D arrays:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">import numpy as np</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">x = np.array([1,2,3])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(x)</span><br/>&#13;
[1 2 3]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(x.reshape((3,1)))</span><br/>&#13;
[[1]<br/>&#13;
 [2]<br/>&#13;
 [3]]</p>&#13;
<p class="noindent">Here, we’ve used <span class="literal">reshape</span> to turn the three-element row vector into a column vector of three rows and one column.</p>&#13;
<p class="indent">The components of a vector are often interpreted as lengths along a set of coordinate axes. For example, a three-component vector might be used to represent a point in 3D space. In this vector,</p>&#13;
<p class="center"><em><strong>x</strong></em> = [<em>x</em>, <em>y</em>, <em>z</em>]</p>&#13;
<p class="noindent"><em>x</em> could be the length along the x-axis, <em>y</em> the length along the y-axis, and <em>z</em> the length along the z-axis. These are the Cartesian coordinates and serve to uniquely identify all points in 3D space.</p>&#13;
<p class="indent">However, in deep learning, and machine learning in general, the components of a vector are often unrelated to each other in any strict geometric sense. Rather, they’re used to represent <em>features</em>, qualities of some sample that the model will use to attempt to arrive at a useful output, like a class label, or a regression value. That said, the vector representing the collection of features, called the <em>feature vector</em>, is sometimes thought about geometrically. For example, some machine learning models, like <em>k</em>-nearest neighbors, interpret the vector as representing some coordinate in geometric space.</p>&#13;
<p class="indent">You’ll often hear deep learning people discuss the <em>feature space</em> of a problem. The feature space refers to the set of possible inputs. The training set for a model needs to accurately represent the feature space of the possible inputs the model will encounter when used. In this sense, the feature vector is a point, a location in this <em>n</em>-dimensional space where <em>n</em> is the number of features in the feature vector.</p>&#13;
<h4 class="h4" id="ch05lev2_3">Matrices</h4>&#13;
<p class="noindent">A <em>matrix</em> is a 2D array of numbers:</p>&#13;
<div class="imagec"><img src="Images/105equ01.jpg" alt="Image" width="239" height="70"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_106"/>The elements of <em><strong>A</strong></em> are subscripted by the row number and column number. The matrix <em><strong>A</strong></em> has three rows and four columns, so we say that it’s a 3 × 4 matrix, where 3 × 4 is the <em>order</em> of the matrix. Notice that <em><strong>A</strong></em> uses subscripts starting with 0. Math texts often begin with 1, but increasingly, they’re using 0 so that there isn’t an offset between the math notation and the computer representation of the matrix. Note, also, that we’ll use <em><strong>A</strong></em>, an uppercase letter in bold, to mean a matrix.</p>&#13;
<p class="indent">In code, matrices are represented as 2D arrays:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">A = np.array([[1,2,3],[4,5,6],[7,8,9]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(A)</span><br/>&#13;
[[1 2 3]<br/>&#13;
 [4 5 6]<br/>&#13;
 [7 8 9]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.arange(12).reshape((3,4)))</span><br/>&#13;
[[ 0 1 2 3]<br/>&#13;
 [ 4 5 6 7]<br/>&#13;
 [ 8 9 10 11]]</p>&#13;
<p class="indent">To get element <em>a</em><sub>12</sub> of <em><strong>A</strong></em> in Python, we write <span class="literal">A[1,2]</span>. Notice that when we printed the arrays, there was an extra <span class="literal">[</span> and <span class="literal">]</span> around them. NumPy uses these brackets to indicate that the 2D array can be thought of as a row vector in which each element is itself a vector. In Python-speak, this means that a matrix can be thought of as a list of sublists in which each sublist is of the same length. Of course, this is exactly how we defined <span class="literal">A</span> to begin with.</p>&#13;
<p class="indent">We can think of vectors as matrices with a single row or column. A column vector with three elements is a 3 × 1 matrix: it has three rows and one column. Similarly, a row vector of four elements acts like a 1 × 4 matrix: it has one row and four columns. We’ll make use of this observation later.</p>&#13;
<h4 class="h4" id="ch05lev2_4">Tensors</h4>&#13;
<p class="noindent">A scalar has no dimensions, a vector has one, and a matrix has two. As you might suspect, we don’t need to stop there. A mathematical object with more than two dimensions is colloquially referred to as a <em>tensor</em>. When necessary, we’ll represent tensors like this: <span class="literal">T</span>, as a sans serif capital letter.</p>&#13;
<p class="indent">The number of dimensions a tensor has defines its <em>order</em>, which is not to be confused with the order of a matrix. A 3D tensor has order 3. A matrix is a tensor of order 2. A vector is an order-1 tensor, and a scalar is an order-0 tensor. When we discuss the flow of data through a deep neural network in <a href="ch09.xhtml#ch09">Chapter 9</a>, we’ll see that many toolkits use tensors of order 4 (or more).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_107"/>In Python, NumPy arrays with three or more dimensions are used to implement tensors. For example, we can define an order-3 tensor in Python as shown below:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">t = np.arange(36).reshape((3,3,4))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(t)</span><br/>&#13;
[[[ 0 1 2 3]<br/>&#13;
  [ 4 5 6 7]<br/>&#13;
  [ 8 9 10 11]]<br/>&#13;
<br/>&#13;
 [[12 13 14 15]<br/>&#13;
  [16 17 18 19]<br/>&#13;
  [20 21 22 23]]<br/>&#13;
<br/>&#13;
 [[24 25 26 27]<br/>&#13;
  [28 29 30 31]<br/>&#13;
  [32 33 34 35]]]</p>&#13;
<p class="indent">Here, we use <span class="literal">np.arange</span> to define <span class="literal">t</span> to be a vector of 36 elements holding the numbers 0 . . . 35. Then, we immediately <span class="literal">reshape</span> the vector into a tensor of 3 × 3 × 4 elements (3 × 3 × 4 = 36). One way to think of a 3 × 3 × 4 tensor is that it contains a stack of three 3 × 4 images. If we keep this in mind, the following statements make sense:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">print(t[0])</span><br/>&#13;
[[ 0 1 2 3]<br/>&#13;
 [ 4 5 6 7]<br/>&#13;
 [ 8 9 10 11]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(t[0,1])</span><br/>&#13;
[4 5 6 7]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(t[0,1,2])</span><br/>&#13;
6</p>&#13;
<p class="indent">Asking for <span class="literal">t[0]</span> will return the first 3 × 4 <em>image</em> in the stack. Asking for <span class="literal">t[0,1]</span>, then, should return the second row of the first image, which it does. Finally, we get to an individual element of <span class="literal">t</span> by asking for the image number (<span class="literal">0</span>), the row number (<span class="literal">1</span>), and the element of that row (<span class="literal">2</span>).</p>&#13;
<p class="indent">Assigning the dimensions of a tensor to successively smaller collections of something is a handy way to keep the meaning of the dimensions in mind. For example, we can define an order-5 tensor like so:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">w = np.zeros((9,9,9,9,9))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">w[4,1,2,0,1]</span><br/>&#13;
0.0</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_108"/>But, what does asking for <span class="literal">w[4,1,2,0,1]</span> mean? The exact meaning depends on the application. For example, we might think of <span class="literal">w</span> as representing a bookcase. The first index selects the shelf, and the second selects the book on the shelf. Then, the third index selects the page within the book, and the fourth selects the line on the page. The final index selects the word on the line. Therefore, <span class="literal">w[4,1,2,0,1]</span> is asking for the second word of the first line of the third page of the second book on the fifth shelf of the bookcase, understood by reading the indices from right to left.</p>&#13;
<p class="indent">The bookcase analogy does have its limitations. NumPy arrays have fixed dimensions, meaning that if <span class="literal">w</span> is a bookcase, there are nine shelves, and each shelf has <em>exactly</em> nine books. Likewise, each book has exactly nine pages, and each page has nine lines. Finally, each line has precisely nine words. NumPy arrays ordinarily use contiguous memory in the computer, so the size of each dimension is fixed when the array is defined. Doing so, and selecting the specific data type, like unsigned integer, makes locating an element of the array an indexing operation using a simple formula to compute an offset from a base memory address. This is what makes NumPy arrays so much faster than Python lists.</p>&#13;
<p class="indent">Any tensor of less than order <em>n</em> can be represented as an order-<em>n</em> tensor by supplying the missing dimensions of length one. We saw an example of this above when I said that an <em>m</em>-component vector could be thought of as a 1 × <em>m</em> or an <em>m</em> × 1 matrix. The order-1 tensor (the vector) is turned into an order-2 tensor (matrix) by adding a missing dimension of length one.</p>&#13;
<p class="indent">As an extreme example, we can treat a scalar (order-0 tensor) as an order-5 tensor, like this:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">t = np.array(42).reshape((1,1,1,1,1))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(t)</span><br/>&#13;
[[[[[42]]]]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">t.shape</span><br/>&#13;
(1, 1, 1, 1, 1)<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">t[0,0,0,0,0]</span><br/>&#13;
42</p>&#13;
<p class="noindent">Here, we <span class="literal">reshape</span> the scalar <span class="literal">42</span> into an order-5 tensor (a five-dimensional [5D] array) with length one on each axis. Notice that NumPy tells us that the tensor <span class="literal">t</span> has five dimensions with the <span class="literal">[[[[[</span> and <span class="literal">]]]]]</span> around <span class="literal">42</span>. Asking for the shape of <span class="literal">t</span> confirms that it is a 5D tensor. Finally, as a tensor, we can get the value of the single element it contains by specifying all the dimensions with <span class="literal">t[0,0,0,0,0]</span>. We’ll often use this trick of adding new dimensions of length one. In fact, in NumPy, there is a way to do this directly, which you’ll see when using deep learning toolkits:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">t = np.array([[1,2,3],[4,5,6]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(t)</span><br/>&#13;
[[1 2 3]<br/>&#13;
 [4 5 6]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">w = t[np.newaxis,:,:]</span><br/>&#13;
<span epub:type="pagebreak" id="page_109"/>&#13;
&gt;&gt;&gt; <span class="codestrong1">w.shape</span><br/>&#13;
(1, 2, 3)<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(w)</span><br/>&#13;
[[[1 2 3]<br/>&#13;
  [4 5 6]]]</p>&#13;
<p class="indent">Here, we’ve turned <span class="literal">t</span>, an order-2 tensor (a matrix), into an order-3 tensor by using <span class="literal">np.newaxis</span> to create a new axis of length one. That’s why <span class="literal">w.shape</span> returns <span class="literal">(1,2,3)</span> and not <span class="literal">(2,3)</span>, as it would for <span class="literal">t</span>.</p>&#13;
<p class="indent">There are analogues between tensors up to order-3 and geometry that are helpful in visualizing the relationships between the different orders:</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Order (dimensions)</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Tensor name</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Geometric name</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Scalar</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Point</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Vector</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Line</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Matrix</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Plane</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Tensor</p></td>&#13;
<td style="vertical-align: top"><p class="tab">Volume</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Notice, I used <em>tensor</em> in its common sense in the table. There seems to be no standardized name for an order-3 tensor.</p>&#13;
<p class="indent">In this section, we defined the mathematical objects of deep learning in relation to multidimensional arrays, since that’s how they are implemented in code. We’ve thrown away a lot of mathematics by doing this, but we’ve preserved what we need to understand deep learning. Let’s move on now and see how to use tensors in expressions.</p>&#13;
<h3 class="h3" id="ch05lev1_2">Arithmetic with Tensors</h3>&#13;
<p class="noindent">The purpose of this section is to detail operations on tensors, with special emphasis on tensors of order-1 (vectors) and order-2 (matrices). We’ll assume operations with scalars are well in hand at this point.</p>&#13;
<p class="indent">We’ll start with what I’m calling <em>array operations</em>, by which I mean the element-wise operations that toolkits like NumPy perform on arrays of all dimensions. Then we’ll move on to operations particular to vectors. This sets the stage for the critical topic of matrix multiplication. Finally, we’ll discuss block matrices.</p>&#13;
<h4 class="h4" id="ch05lev2_5">Array Operations</h4>&#13;
<p class="noindent">The way we’ve used the NumPy toolkit so far has shown us that all the normal scalar arithmetic operations translate directly into the world of multidimensional arrays. This includes standard operations like addition, subtraction, multiplication, division, and exponentiation, as well as the application of functions to an array. In all of these cases, the scalar operation is applied element-wise to each element of the array. The examples here will set the tone for the rest of this section and will also let us explore some NumPy broadcasting rules that we haven’t called out yet.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_110"/>Let’s first define some arrays to work with:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">a = np.array([[1,2,3],[4,5,6]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.array([[7,8,9],[10,11,12]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">c = np.array([10,100,1000])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">d = np.array([10,11])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a)</span><br/>&#13;
[[1 2 3]<br/>&#13;
 [4 5 6]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(b)</span><br/>&#13;
[[ 7 8 9]<br/>&#13;
 [10 11 12]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(c)</span><br/>&#13;
[  10 100 1000]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(d)</span><br/>&#13;
[10 11]</p>&#13;
<p class="indent">Element-wise arithmetic is straightforward for arrays with dimensions that match:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">print(a+b)</span><br/>&#13;
[[ 8 10 12]<br/>&#13;
 [14 16 18]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a-b)</span><br/>&#13;
[[-6 -6 -6]<br/>&#13;
 [-6 -6 -6]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a*b)</span><br/>&#13;
[[ 7 16 27]<br/>&#13;
 [40 55 72]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a/b)</span><br/>&#13;
[[0.14285714 0.25       0.33333333]<br/>&#13;
 [0.4        0.45454545 0.5       ]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(b**a)</span><br/>&#13;
[[       7       64      729]<br/>&#13;
 [   10000   161051  2985984]]</p>&#13;
<p class="indent">These results are all easy enough to interpret; NumPy applies the desired operation to the corresponding elements of each array. Element-wise multiplication on two matrices (<span class="literal">a</span> and <span class="literal">b</span>) is often known as the <em>Hadamard product</em>. (You’ll encounter this term from time to time in the deep learning literature.)</p>&#13;
<p class="indent">The NumPy toolkit extends the idea of element-wise operations into what it calls <em>broadcasting</em>. When broadcasting, NumPy applies rules, which we’ll see via examples, where one array is passed over another to produce a meaningful output.</p>&#13;
<p class="indent">We’ve already encountered a form of broadcasting when operating on an array with a scalar. In that case, the scalar value was broadcast to every value of the array.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_111"/>For our first example, even though <span class="literal">a</span> is a 2 × 3 matrix, NumPy allows operations on it with <span class="literal">c</span>, a three-component vector, by applying broadcasting:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">print(a+c)</span><br/>&#13;
[[  11  102 1003]<br/>&#13;
 [  14  105 1006]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(c*a)</span><br/>&#13;
[[  10 200 3000]<br/>&#13;
 [  40 500 6000]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a/c)</span><br/>&#13;
[[0.1  0.02  0.003]<br/>&#13;
 [0.4  0.05  0.006]]</p>&#13;
<p class="noindent">Here, the three-component vector, <span class="literal">c</span>, has been broadcast over the rows of the 2 × 3 matrix, <span class="literal">a</span>. NumPy recognized that the last dimensions of <span class="literal">a</span> and <span class="literal">c</span> were both three, so the vector could be passed over the matrix to produce the given output. When looking at deep learning code, much of which is in Python, you’ll see situations like this. At times, some thought is necessary, along with some experimentation at the Python prompt, to understand what’s happening.</p>&#13;
<p class="indent">Can we broadcast <span class="literal">d</span>, a two-component vector, over <span class="literal">a</span>, a 2 × 3 matrix? If we try to do so the same way we broadcast <span class="literal">c</span> over <span class="literal">a</span>, we’ll fail:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">print(a+d)</span><br/>&#13;
Traceback (most recent call last):<br/>&#13;
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;<br/>&#13;
ValueError: operands could not be broadcast together with shapes (2,3) (2,)</p>&#13;
<p class="indent">However, the broadcasting rules for NumPy accommodate dimensions of length one. The shape of <span class="literal">d</span> is 2; it’s a two-element vector. If we reshape <span class="literal">d</span> so that it’s a 2D array with shape 2 × 1, we’ll give NumPy what it needs:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">d = d.reshape((2,1))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">d.shape</span><br/>&#13;
(2, 1)<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(a+d)</span><br/>&#13;
[[11 12 13]<br/>&#13;
 [15 16 17]]</p>&#13;
<p class="noindent">We now see that Numpy has added <span class="literal">d</span> across the columns of <span class="literal">a</span>.</p>&#13;
<p class="indent">Let’s return to the world of mathematics and look at operations on vectors.</p>&#13;
<h4 class="h4" id="ch05lev2_6">Vector Operations</h4>&#13;
<p class="noindent">Vectors are represented in code as a collection of numbers that can be interpreted as values along a set of coordinate axes. Here, we’ll define several operations that are unique to vectors.</p>&#13;
<h5 class="h5" id="ch05lev3_1"><span epub:type="pagebreak" id="page_112"/>Magnitude</h5>&#13;
<p class="noindent">Geometrically, we can understand vectors as having a direction and a length. They’re often drawn as arrows, and we’ll see an example of a vector plot in <a href="ch06.xhtml#ch06">Chapter 6</a>. People speak of the length of a vector as its <em>magnitude</em>. Therefore, the first vector operation we’ll consider is calculating its magnitude. For a vector, <em><strong>x</strong></em>, with <em>n</em> components, the formula for its magnitude is</p>&#13;
<div class="imagec" id="ch05equ03"><img src="Images/05equ03.jpg" alt="Image" width="465" height="38"/></div>&#13;
<p class="indent">In <a href="ch05.xhtml#ch05equ03">Equation 5.3</a>, the double vertical bars around the vector represent its magnitude. You’ll often see people use single bars here as well. Single bars are also used for absolute value; we usually rely on context to tell the difference between the two.</p>&#13;
<p class="indent">Where did <a href="ch05.xhtml#ch05equ03">Equation 5.3</a> come from? Consider a vector in 2D, <em><strong>x</strong></em> = (<em>x</em>, <em>y</em>). If <em>x</em> and <em>y</em> are lengths along the x-axis and y-axis, respectively, we see that <em>x</em> and <em>y</em> form the sides of a right triangle. The length of the hypotenuse of this right triangle is the length of the vector. Therefore, according to Pythagoras, and the Babylonians long before him, this length is <img src="Images/112equ01.jpg" alt="Image" width="81" height="25"/>, which, generalized to <em>n</em> dimensions, becomes <a href="ch05.xhtml#ch05equ03">Equation 5.3</a>.</p>&#13;
<h5 class="h5" id="ch05lev3_2">Unit Vectors</h5>&#13;
<p class="noindent">Now that we can calculate the magnitude of a vector, we can introduce a useful form of a vector known as a <em>unit vector</em>. If we divide the components of a vector by its magnitude, we’re left with a vector that points in the same direction as the original vector but has a magnitude of one. This is the unit vector. For a vector, <em><strong>v</strong></em>, the unit vector in the same direction is</p>&#13;
<div class="imagec"><img src="Images/112equ02.jpg" alt="Image" width="66" height="43"/></div>&#13;
<p class="noindent">where the hat over the vector serves to identify it as a unit vector. Let’s see a concrete example. Our example vector is <em><strong>v</strong></em> = (2, –4,3). Therefore, the unit vector in the same direction as <em><strong>v</strong></em> is</p>&#13;
<div class="imagec"><img src="Images/112equ03.jpg" alt="Image" width="660" height="54"/></div>&#13;
<p class="indent">In code, we calculate the unit vector as the following:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">v = np.array((2, -4, 3))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">u = v / np.sqrt((v*v).sum())</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(u)</span><br/>&#13;
[ 0.37139068 -0.74278135 0.55708601 ]</p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_113"/>Here, we make use of the fact that to square each element of <span class="literal">v</span>, we multiply it by itself, element-wise, and then add the components together by calling <span class="literal">sum</span> to get the magnitude squared.</p>&#13;
<h5 class="h5" id="ch05lev3_3">Vector Transpose</h5>&#13;
<p class="noindent">We mentioned earlier that row vectors can be thought of as 1 × <em>n</em> matrices, while column vectors are <em>n</em> × 1 matrices. The act of changing a row vector into a column vector and vice versa is known as taking the <em>transpose</em>. We’ll see in <a href="ch06.xhtml#ch06">Chapter 6</a> that the transpose also applies to matrices. Notationally, we denote the vector transpose of <em><strong>y</strong></em> as <em><strong>y</strong></em><sup>⊤</sup>. Therefore, we have</p>&#13;
<div class="imagec"><img src="Images/113equ01.jpg" alt="Image" width="150" height="376"/></div>&#13;
<p class="noindent">Of course, we’re not limited to just three components.</p>&#13;
<p class="indent">In code, we transpose vectors in several ways. As we saw above, we can use <span class="literal">reshape</span> to reshape the vector into a 1 × <em>n</em> or <em>n</em> × 1 matrix. We can also call the <span class="literal">transpose</span> method on the vector, with some care, or use the transpose shorthand. Let’s see examples of all of these approaches. First, let’s define a NumPy vector and see how <span class="literal">reshape</span> turns it into a 3 × 1 column vector and a 1 × 3 row vector, as opposed to a plain vector of three elements:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">v = np.array([1,2,3])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(v)</span><br/>&#13;
[1 2 3]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(v.reshape((3,1)))</span><br/>&#13;
[[1]<br/>&#13;
 [2]<br/>&#13;
 [3]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(v.reshape((1,3)))</span><br/>&#13;
[[1 2 3]]</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_114"/>Notice the difference between the first <span class="literal">print(v)</span> and the last after calling <span class="literal">reshape((1,3))</span>. The output now has an extra set of brackets around it to indicate the leading dimension of one.</p>&#13;
<p class="indent">Next, we apply the transpose operation on <span class="literal">v</span>:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">print(v.transpose())</span><br/>&#13;
[1 2 3]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(v.T)</span><br/>&#13;
[1 2 3]</p>&#13;
<p class="noindent">Here, we see that calling <span class="literal">transpose</span> or <span class="literal">T</span> changes nothing about <span class="literal">v</span>. This is because the shape of <span class="literal">v</span> is simply <span class="literal">3</span>, not <span class="literal">(1,3)</span> or <span class="literal">(3,1)</span>. If we explicitly alter <span class="literal">v</span> to be a 1 × 3 matrix, we see that <span class="literal">transpose</span> and <span class="literal">T</span> have the desired effect:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">v = v.reshape((1,3))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(v.transpose())</span><br/>&#13;
[[1]<br/>&#13;
 [2]<br/>&#13;
 [3]]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(v.T)</span><br/>&#13;
[[1]<br/>&#13;
 [2]<br/>&#13;
 [3]]</p>&#13;
<p class="noindent">Here, <span class="literal">v</span> goes from being a row vector to a column vector, as we expect. The lesson, then, is to be careful about the actual dimensionality of vectors in NumPy code. Most of the time, we can be sloppy, but sometimes we need to be explicit and care about the distinction between plain vectors, row vectors, and column vectors.</p>&#13;
<h5 class="h5" id="ch05lev3_4">Inner Product</h5>&#13;
<p class="noindent">Perhaps the most common vector operation is the <em>inner product</em>, or, as it is frequently called, the <em>dot product</em>. Notationally, the inner product between two vectors is written as</p>&#13;
<div class="imagec" id="ch05equ04"><img src="Images/05equ04.jpg" alt="Image" width="465" height="124"/></div>&#13;
<div class="imagec" id="ch05equ05"><img src="Images/05equ05.jpg" alt="Image" width="349" height="22"/></div>&#13;
<p class="noindent">Here, <em>θ</em> is the angle between the two vectors if they’re interpreted geometrically. The result of the inner product is a scalar. The 〈<em><strong>a</strong></em>, <em><strong>b</strong></em>〉 notation is seen frequently, though the <em><strong>a</strong></em> • <em><strong>b</strong></em> dot notation seems more common in the deep learning literature. The <em><strong>a</strong></em><sup>⊤</sup><em><strong>b</strong></em> matrix multiplication notation explicitly calls out how to calculate the inner product, but we’ll wait until we discuss matrix <span epub:type="pagebreak" id="page_115"/>multiplication to explain its meaning. For the present, the summation tells us what we need to know: the inner product of two vectors of length <em>n</em> is the sum of the products of the <em>n</em> components.</p>&#13;
<p class="indent">The inner product of a vector with itself is the magnitude squared:</p>&#13;
<p class="center"><em><strong>a</strong></em> • <em><strong>a</strong></em> = ||<em><strong>a</strong></em>||<sup>2</sup></p>&#13;
<p class="indent">The inner product is commutative,</p>&#13;
<p class="center"><em><strong>a</strong></em> • <em><strong>b</strong></em> = <em><strong>b</strong></em> • <em><strong>a</strong></em></p>&#13;
<p class="noindent">and distributive,</p>&#13;
<p class="center"><em><strong>a</strong></em> • (<em><strong>b</strong></em> + <em><strong>c</strong></em>) = <em><strong>a</strong></em> • <em><strong>b</strong></em> + <em><strong>a</strong></em> • <em><strong>c</strong></em></p>&#13;
<p class="noindent">but not associative, as the output of the first inner product is a scalar, not a vector, and multiplying a vector by a scalar is not an inner product.</p>&#13;
<p class="indent">Finally, notice that the inner product is zero when the angle between the vectors is 90 degrees; this is because cos <em>θ</em> is zero (<a href="ch05.xhtml#ch05equ05">Equation 5.5</a>). This means the two vectors are perpendicular, or <em>orthogonal</em>, to each other.</p>&#13;
<p class="indent">Let’s look at some examples of the inner product. First, we’ll be literal and implement <a href="ch05.xhtml#ch05equ04">Equation 5.4</a> explicitly:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">a = np.array([1,2,3,4])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.array([5,6,7,8])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">def inner(a,b):</span><br/>&#13;
<span class="codestrong1">...   s = 0.0</span><br/>&#13;
<span class="codestrong1">...   for i in range(len(a)):</span><br/>&#13;
<span class="codestrong1">...     s += a[i]*b[i]</span><br/>&#13;
<span class="codestrong1">...   return s</span><br/>&#13;
<span class="codestrong1">...</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">inner(a,b)</span><br/>&#13;
70.0</p>&#13;
<p class="noindent">However, since <span class="literal">a</span> and <span class="literal">b</span> are NumPy arrays, we know we can be more efficient:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">(a*b).sum()</span><br/>&#13;
70</p>&#13;
<p class="noindent">Or, probably most efficient of all, we’ll let NumPy do it for us by using <span class="literal">np.dot</span>:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">np.dot(a,b)</span><br/>&#13;
70</p>&#13;
<p class="noindent">You’ll see <span class="literal">np.dot</span> frequently in deep learning code. It can do more than calculate the inner product, as we’ll see below.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch05equ05">Equation 5.5</a> tells us that the angle between two vectors is</p>&#13;
<div class="imagec"><img src="Images/115equ01.jpg" alt="Image" width="148" height="48"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_116"/>In the code, this could be calculated as</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">A = np.sqrt(np.dot(a,a))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">B = np.sqrt(np.dot(b,b))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">t = np.arccos(np.dot(a,b)/(A*B))</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">t*(180/np.pi)</span><br/>&#13;
14.335170291600924</p>&#13;
<p class="noindent">This tells us that the angle between <em><strong>a</strong></em> and <em><strong>b</strong></em> is approximately 14° after converting <span class="literal">t</span> from radians.</p>&#13;
<p class="indent">If we consider vectors in 3D space, we see that the dot product between orthogonal vectors is zero, implying that the angle between them is 90°:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">a = np.array([1,0,0])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.array([0,1,0])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.dot(a,b)</span><br/>&#13;
0<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">t = np.arccos(0)</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">t*(180/np.pi)</span><br/>&#13;
90.0</p>&#13;
<p class="noindent">This is true because <span class="literal">a</span> is a unit vector along the x-axis, <span class="literal">b</span> is a unit vector along the y-axis, and we know there’s a right angle between them.</p>&#13;
<p class="indent">With the inner product in our toolkit, let’s see how we can use it to project one vector onto another.</p>&#13;
<h5 class="h5" id="ch05lev3_5">Projection</h5>&#13;
<p class="noindent">The projection of one vector onto another calculates the amount of the first vector that’s in the direction of the second. The projection of <em><strong>a</strong></em> onto <em><strong>b</strong></em> is</p>&#13;
<div class="imagec"><img src="Images/116equ01.jpg" alt="Image" width="136" height="48"/></div>&#13;
<p class="indent"><a href="ch05.xhtml#ch05fig01">Figure 5-1</a> shows graphically what projection means for 2D vectors.</p>&#13;
<div class="image" id="ch05fig01"><img src="Images/05fig01.jpg" alt="image" width="316" height="302"/></div>&#13;
<p class="figcap"><em>Figure 5-1: A graphical representation of the projection of <strong>a</strong> onto <strong>b</strong> in 2D</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_117"/>Projection finds the component of <em><strong>a</strong></em> in the direction of <em><strong>b</strong></em>. Note the projection of <em><strong>a</strong></em> onto <em><strong>b</strong></em> is not the same as the projection of <em><strong>b</strong></em> onto <em><strong>a</strong></em>.</p>&#13;
<p class="indent">Because we use an inner product in the numerator, we can see that the projection of a vector onto another vector that’s orthogonal to it is zero. No component of the first vector is in the direction of the second. Think again of the x-axis and y-axis. The entire reason we use Cartesian coordinates is because the two axes, or three in 3D space, are all mutually orthogonal; no part of one is in the direction of the others. This lets us specify any point, and the vector from the origin to that point, by specifying the components along these axes. We’ll see this breaking up of an object into mutually orthogonal components later when we discuss eigenvectors and PCA in <a href="ch06.xhtml#ch06">Chapter 6</a>.</p>&#13;
<p class="indent">In code, calculating the projection is straightforward:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">a = np.array([1,1])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.array([1,0])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">p = (np.dot(a,b)/np.dot(b,b))*b</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(p)</span><br/>&#13;
[1. 0.]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">c = np.array([-1,1])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">p = (np.dot(c,b)/np.dot(b,b))*b</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(p)</span><br/>&#13;
[-1. -0.]</p>&#13;
<p class="indent">In the first example, <em><strong>a</strong></em> points in the direction 45° up from the x-axis, while <em><strong>b</strong></em> points along the x-axis. We’d then expect the projection of <em><strong>a</strong></em> to be along the x-axis, which it is (<span class="literal">p</span>). In the second example, <em><strong>c</strong></em> points in the direction 135° = 90° + 45° from the x-axis. Therefore, we’d expect the component of <em><strong>c</strong></em> along <em><strong>b</strong></em> to be along the x-axis but in the opposite direction from <em><strong>b</strong></em>, which it is.</p>&#13;
<div class="note">&#13;
<p class="notet"><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>Projecting <strong>c</strong> along <strong>b</strong> returned a y-axis component of <span class="literal">–0</span>. The negative sign is a quirk of the IEEE 754 representation used for floating-point numbers. The significand (mantissa) of the internal representation is zero, but the sign can still be specified, leading to an output of negative zero from time to time. For a detailed explanation of computer number formats, including floating-point, please see my book,</em> Numbers and Computers <em>(Springer-Verlag, 2017).</em></p>&#13;
</div>&#13;
<p class="indent">Let’s move on now to consider the outer product of two vectors.</p>&#13;
<h5 class="h5" id="ch05lev3_6">Outer Product</h5>&#13;
<p class="noindent">The inner product of two vectors returned a scalar value. The <em>outer product</em> of two vectors instead returns a matrix. Note that unlike the inner product, the outer product does not require the two vectors to have the same number of components. Specifically, for vectors <em><strong>a</strong></em> of <em>m</em> components and <em><strong>b</strong></em> of <em>n</em> components, the outer product is the matrix formed by multiplying each element of <em><strong>a</strong></em> by each element of <em><strong>b</strong></em>, as shown next.</p>&#13;
<span epub:type="pagebreak" id="page_118"/>&#13;
<div class="imagec"><img src="Images/118equ01.jpg" alt="Image" width="516" height="95"/></div>&#13;
<p class="noindent">The <em><strong>ab</strong></em><sup>⊤</sup> notation is how to calculate the outer product via matrix multiplication. Notice that this notation is not the same as the inner product, <em><strong>a</strong></em><sup>⊤</sup><em><strong>b</strong></em>, and that it assumes <em><strong>a</strong></em> and <em><strong>b</strong></em> to be column vectors. No operator symbol is consistently used for the outer product, primarily because it’s so easily specified via matrix multiplication and because it’s less common than the dot product. However, ⊗ seems the most commonly used when the outer product is presented with a binary operator.</p>&#13;
<p class="indent">In code, NumPy has kindly provided an outer product function for us:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">a = np.array([1,2,3,4])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.array([5,6,7,8])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.dot(a,b)</span><br/>&#13;
70<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">np.outer(a,b)</span><br/>&#13;
array([[ 5, 6, 7, 8],<br/>&#13;
       [10, 12, 14, 16],<br/>&#13;
       [15, 18, 21, 24],<br/>&#13;
       [20, 24, 28, 32]])</p>&#13;
<p class="indent">We used <span class="literal">a</span> and <span class="literal">b</span> above when discussing the inner product. As expected, <span class="literal">np.dot</span> gives us a scalar output for <em><strong>a</strong></em>•<em><strong>b</strong></em>. However, the <span class="literal">np.outer</span> function returns a 4 × 4 matrix, where we see that each row is vector <span class="literal">b</span> multiplied successively by each element of vector <span class="literal">a</span>, first <span class="literal">1</span>, then <span class="literal">2</span>, then <span class="literal">3</span>, and finally <span class="literal">4</span>. Therefore, each element of <span class="literal">a</span> has multiplied each element of <span class="literal">b</span>. The resulting matrix is 4 × 4 because both <span class="literal">a</span> and <span class="literal">b</span> have four elements.</p>&#13;
<div class="sidebar">&#13;
<p class="sb-title">THE CARTESIAN PRODUCT</p>&#13;
<p class="sb-noindent">There is a direct analogue between the outer product of two vectors and the Cartesian product of two sets, <em>A</em> and <em>B</em>. The <em>Cartesian product</em> is a new set, each element of which is one of the possible pairings of elements from <em>A</em> and <em>B</em>. So, if <em>A</em>={1,2,3,4} and <em>B</em>={5,6,7,8}, the Cartesian product can be written as</p>&#13;
<div class="imagec"><img src="Images/118equ02.jpg" alt="Image" width="500" height="176"/></div>&#13;
<p class="sb-noindent"><span epub:type="pagebreak" id="page_119"/>Here, we see that if we replace each entry with the product of the pair, we get the corresponding vector product we saw above with NumPy <span class="literal">np.outer</span>. Also, note that × is typically used for the Cartesian product when working with sets.</p>&#13;
</div>&#13;
<p class="indent">The ability of the outer product to mix all combinations of its inputs has been used in deep learning for neural collaborative filtering and visual question answering applications. These functions are performed by advanced networks that make recommendations or answer text questions about an image. The outer product appears as a mixing of two different embedding vectors. <em>Embeddings</em> are the vectors generated by lower layers of a network, for example, the next to last fully connected layer before the softmax layer’s output of a traditional convolutional neural network (CNN). The embedding layer is usually viewed as having learned a new representation of the network input. It can be thought of as mapping complex inputs, like images, to a reduced space of several hundred to several thousands of dimensions.</p>&#13;
<h5 class="h5" id="ch05lev3_7">Cross Product</h5>&#13;
<p class="noindent">Our final vector-vector operator is the <em>cross product</em>. This operator is only defined for 3D space (ℝ<sup>3</sup>). The cross product of <em><strong>a</strong></em> and <em><strong>b</strong></em> is a new vector that is perpendicular to the plane containing <em><strong>a</strong></em> and <em><strong>b</strong></em>. Note, this does not imply that <em><strong>a</strong></em> and <em><strong>b</strong></em> are themselves perpendicular. The cross product is defined as</p>&#13;
<div class="imagec" id="ch05equ06"><img src="Images/05equ06.jpg" alt="Image" width="553" height="84"/></div>&#13;
<p class="noindent">where <img src="Images/ncap.jpg" alt="Image" width="12" height="16"/> is a unit vector and <em>θ</em> is the angle between <em><strong>a</strong></em> and <em><strong>b</strong></em>. The direction of <img src="Images/ncap.jpg" alt="Image" width="12" height="16"/> is given by the <em>right-hand rule</em>. With your right hand, point your index finger in the direction of <em><strong>a</strong></em> and your middle finger in the direction of <em><strong>b</strong></em>. Then, your thumb will be pointing in the direction of <img src="Images/ncap.jpg" alt="Image" width="12" height="16"/>. <a href="ch05.xhtml#ch05equ06">Equation 5.6</a> gives the actual ℝ<sup>3</sup> components of the cross product vector.</p>&#13;
<p class="indent">NumPy implements the cross product via <span class="literal">np.cross</span>:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">a = np.array([1,0,0])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">b = np.array([0,1,0])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.cross(a,b))</span><br/>&#13;
[0 0 1]<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">c = np.array([1,1,0])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">print(np.cross(a,c))</span><br/>&#13;
[0 0 1]</p>&#13;
<p class="indent">In the first example, <span class="literal">a</span> points along the x-axis and <span class="literal">b</span> along the y-axis. Therefore, we expect the cross product to be perpendicular to these axes, and it is: the cross product points along the z-axis. The second example shows that it doesn’t matter if <span class="literal">a</span> and <span class="literal">b</span> are perpendicular to each other. Here, <span epub:type="pagebreak" id="page_120"/><span class="literal">c</span> is at a 45° angle to the x-axis, but <span class="literal">a</span> and <span class="literal">c</span> are still in the xy-plane. Therefore, the cross product is still along the z-axis.</p>&#13;
<p class="indent">The definition of the cross product involves sin <em>θ</em>, while the inner product uses cos <em>θ</em>. The inner product is zero when the two vectors are orthogonal to each other. The cross product, on the other hand, is zero when the two vectors are in the same direction and is maximized when the vectors are perpendicular. The second NumPy example above works out because the magnitude of <span class="literal">c</span> is <img src="Images/120equ01.jpg" alt="Image" width="26" height="22"/> and sin <img src="Images/120equ02.jpg" alt="Image" width="185" height="23"/>. As a result, the <img src="Images/120equ01.jpg" alt="Image" width="26" height="22"/> factors cancel out to leave a magnitude of 1 for the cross product because <span class="literal">a</span> is a unit vector.</p>&#13;
<p class="indent">The cross product is widely used in physics and other sciences but is less often used in deep learning because of its restriction to 3D space. Nontheless, you should be familiar with it if you’re going to tackle the deep learning literature.</p>&#13;
<p class="indent">This concludes our look at vector-vector operations. Let’s leave the 1D world and move on to consider the most important operation for all deep learning: matrix multiplication.</p>&#13;
<h4 class="h4" id="ch05lev2_7">Matrix Multiplication</h4>&#13;
<p class="noindent">In the previous section, we saw how to multiply two vectors in various ways: Hadamard product, inner (dot) product, outer product, and cross product. In this section, we’ll investigate multiplication of matrices, recalling that row and column vectors are themselves matrices with one row or column.</p>&#13;
<h5 class="h5" id="ch05lev3_8">Properties of Matrix Multiplication</h5>&#13;
<p class="noindent">We’ll define the matrix product operation shortly, but, before we do, let’s look at the properties of matrix multiplication. Let <em><strong>A</strong></em>, <em><strong>B</strong></em>, and <em><strong>C</strong></em> be matrices. Then, following the algebra convention of multiplying symbols by placing them next to each other,</p>&#13;
<p class="center"><em><strong>(AB)C</strong></em> = <em><strong>A</strong></em>(<em><strong>BC</strong></em>)</p>&#13;
<p class="noindent">meaning matrix multiplication is associative. Second, matrix multiplication is distributive:</p>&#13;
<div class="imagec" id="ch05equ07"><img src="Images/05equ07.jpg" alt="Image" width="443" height="22"/></div>&#13;
<div class="imagec" id="ch05equ08"><img src="Images/05equ08.jpg" alt="Image" width="440" height="22"/></div>&#13;
<p class="indent">However, in general, matrix multiplication is <em>not</em> commutative:</p>&#13;
<p class="center"><em><strong>AB</strong></em> ≠ <em><strong>BA</strong></em></p>&#13;
<p class="indent">As you can see in <a href="ch05.xhtml#ch05equ08">Equation 5.8</a>, matrix multiplication over addition from the right produces a different result than matrix multiplication over addition from the left, as shown in <a href="ch05.xhtml#ch05equ07">Equation 5.7</a>. This explains why we showed both <a href="ch05.xhtml#ch05equ07">Equation 5.7</a> and <a href="ch05.xhtml#ch05equ08">Equation 5.8</a>; matrix multiplication can be performed from the left or the right, and the result will be different.</p>&#13;
<h5 class="h5" id="ch05lev3_9"><span epub:type="pagebreak" id="page_121"/>How to Multiply Two Matrices</h5>&#13;
<p class="noindent">To calculate <em><strong>AB</strong></em>, knowing that <em><strong>A</strong></em> must be on the left of <em><strong>B</strong></em>, we first need to verify that the matrices are compatible. It’s only possible to multiply two matrices if the number of columns in <em><strong>A</strong></em> is the same as the number of rows in <em><strong>B</strong></em>. Therefore, if <em><strong>A</strong></em> is an <em>n × m</em> matrix and <em><strong>B</strong></em> is an <em>m × k</em> matrix, then the product, <em><strong>AB</strong></em>, can be found and will be a new <em>n</em> × <em>k</em> matrix.</p>&#13;
<p class="indent">To calculate the product, we perform a series of inner product multiplications between the row vectors of <em><strong>A</strong></em> and the column vectors of <em><strong>B</strong></em>. <a href="ch05.xhtml#ch05fig02">Figure 5-2</a> illustrates the process for a 3 × 3 matrix <em><strong>A</strong></em> and a 3 × 2 matrix <em><strong>B</strong></em>.</p>&#13;
<div class="image" id="ch05fig02"><img src="Images/05fig02.jpg" alt="image" width="530" height="330"/></div>&#13;
<p class="figcap"><em>Figure 5-2: Multiplying a 3</em> × <em>3 matrix by a 3</em> × <em>2 matrix</em></p>&#13;
<p class="indent">In <a href="ch05.xhtml#ch05fig02">Figure 5-2</a>, the first row of the output matrix is found by computing the inner product of the first row of <em><strong>A</strong></em> with each of the columns of <em><strong>B</strong></em>. The first element of the output matrix is shown where the first row of <em><strong>A</strong></em> is multiplied by the first column of <em><strong>B</strong></em>. The remaining first row of the output matrix is found by repeating the dot product of the first row of <em><strong>A</strong></em> by the remaining column of <em><strong>B</strong></em>.</p>&#13;
<p class="indent">Let’s present a worked example with actual numbers for the matrices in <a href="ch05.xhtml#ch05fig02">Figure 5-2</a>:</p>&#13;
<div class="imagec"><img src="Images/121equ01.jpg" alt="Image" width="484" height="297"/></div>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_122"/>Notice, <em><strong>AB</strong></em> is defined, but <em><strong>BA</strong></em> is not, because we can’t multiply a 3 × 2 matrix by a 3 × 3 matrix. The number of columns in <em><strong>B</strong></em> needs to be the same as the number of rows in <em><strong>A</strong></em>.</p>&#13;
<p class="indent">Another way to think of matrix multiplication is by considering what goes into making up each of the output matrix elements. For example, if <em><strong>A</strong></em> is <em>n</em> × <em>m</em> and <em><strong>B</strong></em> is <em>m</em> × <em>p</em>, we know that the matrix product exists as an <em>n</em> × <em>p</em> matrix, <em><strong>C</strong></em>. We find the output elements by computing</p>&#13;
<div class="imagec" id="ch05equ09"><img src="Images/05equ09.jpg" alt="Image" width="407" height="58"/></div>&#13;
<p class="noindent">for <em>i</em> = 0, . . . , <em>n</em> − 1 and <em>j</em> = 0, . . . , <em>p</em> − 1. In the example above, we find <em>c</em><sub>21</sub> by summing the products <em>a</em><sub>20</sub><em>b</em><sub>01</sub> + <em>a</em><sub>21</sub><em>b</em><sub>11</sub> + <em>a</em><sub>22</sub><em>b</em><sub>21</sub>, which fits <a href="ch05.xhtml#ch05equ09">Equation 5.9</a> with <em>i</em> = 2, <em>j</em> = 1 and <em>k</em> = 0, 1, 2.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch05equ09">Equation 5.9</a> tells us how to find a single output matrix element. If we loop over <em>i</em> and <em>j</em>, we can find the entire output matrix. This implies a straightforward implementation of matrix multiplication:</p>&#13;
<p class="programs">def matrixmul(A,B):<br/>&#13;
    I,K = A.shape<br/>&#13;
    J = B.shape[1]<br/>&#13;
    C = np.zeros((I,J), dtype=A.dtype)<br/>&#13;
    for i in range(I):<br/>&#13;
        for j in range(J):<br/>&#13;
            for k in range(K):<br/>&#13;
                C[i,j] += A[i,k]*B[k,j]<br/>&#13;
    return C</p>&#13;
<p class="indent">We’ll assume the arguments, <em><strong>A</strong></em> and <em><strong>B</strong></em>, are compatible matrices. We set the number of rows (<span class="literal">I</span>) and columns (<span class="literal">J</span>) of the output matrix, <em><strong>C</strong></em>, and use them as the loop limits for the elements of <em><strong>C</strong></em>. We create the output matrix, <span class="literal">C</span>, and give it the same data type as <span class="literal">A</span>. Then starts a triple loop. The loop over <span class="literal">i</span> covers all the rows of the output. The next loop, over <span class="literal">j</span>, covers the columns of the current row, and the innermost loop, over <span class="literal">k</span>, covers the combining of elements from <span class="literal">A</span> and <span class="literal">B</span>, as in <a href="ch05.xhtml#ch05equ09">Equation 5.9</a>. When all loops finish, we return the matrix product, <span class="literal">C</span>.</p>&#13;
<p class="indent">The function <span class="literal">matrixmul</span> works. It finds the matrix product. However, in terms of implementation, it’s quite naive. Advanced algorithms exist, as do many optimizations of the naive approach when using compiled code. As we’ll see below, NumPy supports matrix multiplication and internally uses highly optimized compiled code libraries that far outstrip the performance of the simple code above.</p>&#13;
<h5 class="h5" id="ch05lev3_10">Matrix Notation for Inner and Outer Products</h5>&#13;
<p class="noindent">We are now in a position to understand the matrix notation above for the inner product, <em><strong>a</strong></em><sup>⊤</sup><em><strong>b</strong></em>, and the outer product, <em><strong>ab</strong></em><sup>⊤</sup>, of two vectors. In the first case, we have a 1 × <em>n</em> row vector, because of the transpose, and an <em>n</em> × 1 column vector. The algorithm says to form the inner product of the row vector <span epub:type="pagebreak" id="page_123"/>and the column vector to arrive at an output matrix that is 1 × 1, that is, a single scalar number. Notice that there must be <em>n</em> components in both <em><strong>a</strong></em> and <em><strong>b</strong></em>.</p>&#13;
<p class="indent">For the outer product, we have an <em>n</em> × 1 column vector on the left and a 1 × <em>m</em> row vector on the right. Therefore, we know the output matrix is <em>n</em> × <em>m</em>. If <em>m</em> = <em>n</em>, we’ll have an output matrix that’s <em>n</em> × <em>n</em>. A matrix with as many rows as it has columns is a <em>square matrix</em>. These have special properties, some of which we’ll see in <a href="ch06.xhtml#ch06">Chapter 6</a>.</p>&#13;
<p class="indent">To find the outer product of two vectors by matrix multiplication, we multiply each element of the rows of <em><strong>a</strong></em> by each of the columns of <em><strong>b</strong> as a row vector</em>,</p>&#13;
<div class="imagec"><img src="Images/123equ01.jpg" alt="Image" width="239" height="153"/></div>&#13;
<p class="noindent">where each column of <em><strong>b</strong></em><sup>⊤</sup>, a single scalar number, is passed down the rows of <em><strong>a</strong></em>, thereby forming each possible product between the elements of the two vectors.</p>&#13;
<p class="indent">We’ve seen how to perform matrix multiplication manually. Let’s take a look now at how NumPy supports matrix multiplication.</p>&#13;
<h5 class="h5" id="ch05lev3_11">Matrix Multiplication in NumPy</h5>&#13;
<p class="noindent">NumPy provides two different functions that we can use for matrix multiplication. The first, we’ve seen already, <span class="literal">np.dot</span>, though we’ve only used it so far to compute inner products of vectors. The second is <span class="literal">np.matmul</span>, which is also called when using the <span class="literal">@</span> binary operator available in Python 3.5 and later. Matrix multiplication with either function works as we expect. However, NumPy sometimes treats 1D arrays differently from row or column vectors.</p>&#13;
<p class="indent">We can use <span class="literal">shape</span> to decide if a NumPy array is a 1D array, a row vector, or a column vector, as shown in <a href="ch05.xhtml#ch05ex01">Listing 5-1</a>:</p>&#13;
<p class="programs">&gt;&gt;&gt; <span class="codestrong1">av = np.array([1,2,3])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">ar = np.array([[1,2,3]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">ac = np.array([[1],[2],[3]])</span><br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">av.shape</span><br/>&#13;
(3,)<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">ar.shape</span><br/>&#13;
(1, 3)<br/>&#13;
&gt;&gt;&gt; <span class="codestrong1">ac.shape</span><br/>&#13;
(3, 1)</p>&#13;
<p class="ex-caption" id="ch05ex01"><em>Listing 5-1: NumPy vectors</em></p>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_124"/>Here, we see that a 1D array with three elements, <span class="literal">av</span>, has a shape different from a row vector with three components, <span class="literal">ar</span>, or a column vector of three components, <span class="literal">ac</span>. However, each of these arrays contains the same three integers: 1, 2, and 3.</p>&#13;
<p class="indent">Let’s run an experiment to help us understand how NumPy implements matrix multiplication. We’ll test <span class="literal">np.dot</span>, but the results are the same if we use <span class="literal">np.matmul</span> or the <span class="literal">@</span> operator. We need a collection of vectors and matrices to work with. We’ll then apply combinations of them to <span class="literal">np.dot</span> and consider the output, which may very well be an error if the operation is undefined for that combination of arguments.</p>&#13;
<p class="indent">Let’s create the arrays, vectors, and matrices we’ll need:</p>&#13;
<p class="programs">a1 = np.array([1,2,3])<br/>&#13;
ar = np.array([[1,2,3]])<br/>&#13;
ac = np.array([[1],[2],[3]])<br/>&#13;
b1 = np.array([1,2,3])<br/>&#13;
br = np.array([[1,2,3]])<br/>&#13;
bc = np.array([[1],[2],[3]])<br/>&#13;
A = np.array([[1,2,3],[4,5,6],[7,8,9]])<br/>&#13;
B = np.array([[9,8,7],[6,5,4],[3,2,1]])</p>&#13;
<p class="noindent">The shape of the objects should be discernible from the definition, if we keep the results of <a href="ch05.xhtml#ch05ex01">Listing 5-1</a> in mind. We’ll also define two 3 × 3 matrices, <span class="literal">A</span> and <span class="literal">B</span>.</p>&#13;
<p class="indent">Next, we’ll define a helper function to wrap the call to NumPy so we can trap any errors:</p>&#13;
<p class="programs">def dot(a,b):<br/>&#13;
    try:<br/>&#13;
        return np.dot(a,b)<br/>&#13;
    except:<br/>&#13;
        return "fails"</p>&#13;
<p class="noindent">This function calls <span class="literal">np.dot</span> and returns the word <span class="literal">fails</span> if the call doesn’t succeed. <a href="ch05.xhtml#ch05tab01">Table 5-1</a> shows the output of <span class="literal">dot</span> for the given combinations of the inputs defined above.</p>&#13;
<p class="indent"><a href="ch05.xhtml#ch05tab01">Table 5-1</a> illustrates how NumPy sometimes treats 1D arrays differently from row or column vectors. See the difference in <a href="ch05.xhtml#ch05tab01">Table 5-1</a> for <span class="literal">a1,A</span> versus <span class="literal">ar,A</span> and <span class="literal">A,ac</span>. The output of <span class="literal">A,ac</span> is what we’d expect to see mathematically, with the column vector <em><strong>a<sub>c</sub></strong></em> multiplied on the left by <em><strong>A</strong></em>.</p>&#13;
<p class="indent">Is there any real difference between <span class="literal">np.dot</span> and <span class="literal">np.matmul</span>? Yes, some. For 1D and 2D arrays, there is no difference. However, there is a difference between how each function handles arrays greater than two dimensions, although we won’t work with those here. Also, <span class="literal">np.dot</span> allows one of its arguments to be a scalar and multiplies each element of the other argument by it. Multiplying by a scalar with <span class="literal">np.matmul</span> throws an error.</p>&#13;
<p class="tabcap" id="ch05tab01"><span epub:type="pagebreak" id="page_125"/><strong>Table 5-1:</strong> Results of Applying <span class="literal">dot</span> or <span class="literal">matmul</span> to Different Types of Arguments</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Arguments</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Result of <span class="literal">np.dot</span> or <span class="literal">np.matmul</span></strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">a1,b1</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">14 (scalar)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">a1,br</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">fails</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">a1,bc</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">[14] (1 vector)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ar,b1</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">[14] (1 vector)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ar,br</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">fails</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ar,bc</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">[14] (1 × 1 matrix)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ac,b1</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">fails</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ac,br</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><img src="Images/124equ01.jpg" alt="Image" width="186" height="51"/></p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ac,bc</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">fails</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">A,a1</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">[14 32 50] (3 vector)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">A,ar</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">fails</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">A,ac</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><img src="Images/124equ02.jpg" alt="Image" width="30" height="52"/></p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">a1,A</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">[30 36 42] (3 vector)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ar,A</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">[30 36 42] (1 × 3 matrix)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">ac,A</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab">fails</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><span class="literal">A,B</span></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><img src="Images/124equ03.jpg" alt="Image" width="116" height="52"/></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<h4 class="h4" id="ch05lev2_8">Kronecker Product</h4>&#13;
<p class="noindent">The final form of matrix multiplication we’ll discuss is the <em>Kronecker product</em> or <em>matrix direct product</em> of two matrices. When computing the matrix product, we mixed individual elements of the matrices, multiplying them together. For the Kronecker product, we multiply the elements of one matrix by an entire matrix to produce an output matrix that is larger than the input matrices. The Kronecker product is also a convenient place to introduce the idea of a <em>block matrix</em>, or a matrix constructed from smaller matrices (the blocks).</p>&#13;
<p class="indent">For example, if we have three matrices</p>&#13;
<div class="imagec" id="ch05equ10"><img src="Images/05equ10.jpg" alt="Image" width="548" height="68"/></div>&#13;
<p class="noindent">we can define a block matrix, <em><strong>M</strong></em>, as the following.</p>&#13;
<span epub:type="pagebreak" id="page_126"/>&#13;
<div class="imagec"><img src="Images/126equ01.jpg" alt="Image" width="451" height="141"/></div>&#13;
<p class="noindent">where each element of <em><strong>M</strong></em> is a smaller matrix stacked on top of each other.</p>&#13;
<p class="indent">We can most easily define the Kronecker product using a visual example involving a block matrix. The Kronecker product of <em><strong>A</strong></em> and <em><strong>B</strong></em>, typically written as <em><strong>A</strong></em> ⊗ <em><strong>B</strong></em>, is</p>&#13;
<div class="imagec"><img src="Images/126equ02.jpg" alt="Image" width="406" height="98"/></div>&#13;
<p class="noindent">for <em><strong>A</strong></em>, an <em>m</em> × <em>n</em> matrix. This is a block matrix because of <em><strong>B</strong></em>, so, when written out completely, the Kronecker product results in a matrix larger than either <em><strong>A</strong></em> or <em><strong>B</strong></em>. Note, unlike matrix multiplication, the Kronecker product is defined for arbitrarily sized <em><strong>A</strong></em> and <em><strong>B</strong></em> matrices. For example, using <em><strong>A</strong></em> and <em><strong>B</strong></em> from <a href="ch05.xhtml#ch05equ10">Equation 5.10</a>, the Kronecker product is</p>&#13;
<div class="imagec"><img src="Images/126equ03.jpg" alt="Image" width="612" height="217"/></div>&#13;
<p class="indent">Notice above that we used ⊗ for the Kronecker product. This is the convention, though the symbol ⊗ is sometimes abused and is used for other things too. We used it for the outer product of two vectors, for example. NumPy supports the Kronecker product via <span class="literal">np.kron</span>.</p>&#13;
<h3 class="h3" id="ch05lev1_3">Summary</h3>&#13;
<p class="noindent">In this chapter, we introduced the mathematical objects used in deep learning: scalars, vector, matrices, and tensors. We then explored arithmetic with tensors, in particular with vectors and matrices. We saw how to perform operations on these objects, both mathematically and in code via NumPy.</p>&#13;
<p class="indent">Our exploration of linear algebra is not complete, however. In the next chapter, we’ll dive deeper into matrices and their properties to discuss just a handful of the important things that we can do with or know about them.</p>&#13;
</div></body></html>