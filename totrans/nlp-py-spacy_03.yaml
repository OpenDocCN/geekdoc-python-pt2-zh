- en: '**3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: WORKING WITH CONTAINER OBJECTS AND CUSTOMIZING SPACY**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: 'You can divide the main objects composing the spaCy API into two categories:
    containers (such as Tokens and Doc objects) and processing pipeline components
    (such as the part-of-speech tagger and named entity recognizer). This chapter
    explores container objects further. Using container objects and their methods,
    you can access the linguistic annotations that spaCy assigns to each token in
    a text.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also learn how to customize the pipeline components to suit your needs
    and use Cython code to speed up time-consuming NLP tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '**spaCy’s Container Objects**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A *container object* groups multiple elements into a single unit. It can be
    a collection of objects, like tokens or sentences, or a set of annotations related
    to a single object. For example, spaCy’s Token object is a container for a set
    of annotations related to a single token in a text, such as that token’s part
    of speech. Container objects in spaCy mimic the structure of natural language
    texts: a text is composed of sentences, and each sentence contains tokens.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Token, Span, and Doc, the most widely used container objects in spaCy from a
    user’s standpoint, represent a token, a phrase or sentence, and a text, respectively.
    A container can contain other containers—for example, a Doc contains Tokens. In
    this section, we’ll explore working with these container objects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '***Getting the Index of a Token in a Doc Object***'
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Doc object contains a collection of the Token objects generated as a result of
    the tokenization performed on a submitted text. These tokens have indices, allowing
    you to access them based on their positions in the text, as shown in [Figure 3-1](../Text/ch03.xhtml#ch03fig01).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig3-1.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-1: The tokens in a Doc object*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The tokens are indexed starting with 0, which makes the length of the document
    minus 1 the index of the end position. To shred the Doc instance into tokens,
    you derive the tokens into a Python list by iterating over the Doc from the start
    token to the end token:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It’s worth noting that we can create a Doc object using its constructor explicitly,
    as illustrated in the following example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We invoke the Doc’s constructor, passing it the following two parameters: a
    *vocab object* ➊—which is a storage container that provides vocabulary data, such
    as lexical types (adjective, verb, noun, and so on)—and a list of tokens to add
    to the Doc object being created ➋.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '***Iterating over a Token’s Syntactic Children***'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we need to find the leftward children of a token in the syntactic dependency
    parse of a sentence. For example, we can apply this operation to a noun to obtain
    its adjectives, if any. We might need to do this if we want to know what adjectives
    are able to modify a given noun. As an example, consider the following sentence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The diagram in [Figure 3-2](../Text/ch03.xhtml#ch03fig02) highlights the syntactic
    dependencies of interest.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig3-2.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-2: An example of leftward syntactic dependencies*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the leftward syntactic children of the word “apple” in this sample
    sentence programmatically, we might use the following code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this script, we simply iterate through the apple’s children, outputting them
    in a list.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s interesting to note that in this example, the leftward syntactic children
    of the word “apple” represent the entire sequence of the token’s syntactic children.
    In practice, this means that we might replace `Token.lefts` with `Token.children`,
    which finds all of a token’s syntactic children:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result list will remain the same.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also use `Token.rights` to get a token’s rightward syntactic children:
    in this example, the word “apple” is a rightward child of the word “want,” as
    shown in [Figure 3-1](../Text/ch03.xhtml#ch03fig01).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '***The doc.sents Container***'
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, the linguistic annotations assigned to a token make sense only in
    the context of the sentence in which the token occurs. For example, information
    about whether the word is a noun or a verb might apply only to the sentence in
    which this word is located (like the word “count,” discussed in previous chapters).
    In such cases, it would be useful to have the ability to access the tokens in
    the document with sentence-level indices.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'The Doc object’s `doc.sents` property lets us separate a text into its individual
    sentences, as illustrated in the following example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We iterate over the sentences in the `doc` ➊, creating a separate list of tokens
    for each sentence ➋.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, we can still refer to the tokens in a multi-sentence text
    using the global, or document-level, indices, as shown here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The ability to refer to the Token objects in a document by their sentence-level
    indices can be useful if, for example, we need to check whether the first word
    in the second sentence of the text being processed is a pronoun (say we want to
    figure out the connection between two sentences: the first of which contains a
    noun and the second of which contains a pronoun that refers to the noun):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, we use an enumerator in the `for` loop to distinguish the sentences
    by index. This allows us to filter out sentences that we’re not interested in
    and check only the second sentence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the first word in a sentence is a breeze, because its index is always
    0\. But what about the last one? For example, what if we need to find out how
    many sentences in the text end with a verb—(not counting any periods, of course)?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Although the lengths of sentences vary, we can easily determine the length
    of a given sentence using the `len()` function. We reduce the value of `len(sent)`
    by 2 for the following reasons: first, the indices always start at 0 and end at
    size-1\. Second, the last token in both sentences in the sample is a period, which
    we need to ignore.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '***The doc.noun_chunks Container***'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Doc object’s `doc.noun_chunks` property allows us to iterate over the noun
    chunks in the document. A *noun chunk* is a phrase that has a noun as its head.
    For example, the previous sentence contains the following noun chunks:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Doc对象的`doc.noun_chunks`属性允许我们迭代文档中的名词短语。*名词短语*是以名词为核心的短语。例如，前一句包含以下名词短语：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With `doc.noun_chunks`, we can extract them as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`doc.noun_chunks`，我们可以按如下方式提取它们：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Alternatively, we might extract noun chunks by iterating over the nouns in
    the sentence and finding the syntactic children for each noun to form a chunk.
    Earlier in “[Iterating over a Token’s Syntactic Children](../Text/ch03.xhtml#lev28)”
    on [page 33](../Text/ch03.xhtml#page_33), you saw an example of how to extract
    a phrase based on the syntactic dependency parse. Now let’s apply this technique
    to the sample sentence in this example to compose noun chunks manually:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过迭代句子中的名词并找到每个名词的句法子节点来提取名词短语。如在“[迭代标记的句法子节点](../Text/ch03.xhtml#lev28)”中所示，您看到过如何基于句法依赖分析提取短语的示例。现在让我们将这一技术应用到本示例中的样本句子，手动组成名词短语：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Iterating over the tokens, we pick up only nouns ➊. Next, in the inner loop,
    we iterate over a noun’s children ➋, picking up only the tokens that are either
    determiners or adjectives for the noun chunk (noun chunks can also include some
    other parts of speech, say, adverbs) ➌. Then we append the noun to the chunk ➍.
    As a result, the output of the script should be the same as in the previous example.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代标记时，我们只选择名词 ➊。接下来，在内层循环中，我们迭代名词的子节点 ➋，只选择那些是限定词或形容词的标记来构成名词短语（名词短语还可以包含其他词性，比如副词）
    ➌。然后我们将名词添加到短语中 ➍。因此，脚本的输出应与前一个示例相同。
- en: '***Try This***'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: Notice that the words used to modify a noun (determiners and adjectives) are
    always the leftward syntactic children of the noun. This makes it possible to
    replace `Token.children` with `Token.lefts` in the previous code and then remove
    the check for the children to be either a determiner or an adjective, as necessary.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用于修饰名词的词（限定词和形容词）始终是名词的左侧句法子节点。这使得我们能够在之前的代码中将`Token.children`替换为`Token.lefts`，然后根据需要移除对子节点是限定词或形容词的检查。
- en: Rewrite the previous snippet, incorporating the changes suggested here. The
    resulting set of noun chunks should remain the same in your script.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重写之前的代码片段，整合此处建议的修改。最终生成的名词短语集应该与您的脚本中的保持一致。
- en: '***The Span Object***'
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***Span对象***'
- en: The Span object is a slice from a Doc object. In the previous sections, you
    saw how to use it as a container for a sentence and a noun chunk, derived from
    `doc.sents` and `doc.noun_chunks`, respectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Span对象是Doc对象的一个切片。在前面的部分中，您已经看到了如何将它作为句子和名词短语的容器，分别来源于`doc.sents`和`doc.noun_chunks`。
- en: 'The Span object’s usage isn’t limited to being a container for sentences or
    noun chunks only. We can use it to contain an arbitrary set of neighboring tokens
    in the document by specifying a range of indices, as in the following example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Span对象的使用不仅限于作为句子或名词短语的容器。我们还可以通过指定索引范围，将文档中相邻的多个标记包含在内，如以下示例所示：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Span object contains several methods, one of the most interesting of which
    is `span.merge()`, which allows us to merge the span into a single token, retokenizing
    the document. This can be useful when the text contains names consisting of several
    words.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Span对象包含几个方法，其中最有趣的一个是`span.merge()`，它允许我们将Span合并为单一标记，从而重新标记文档。当文本包含由多个单词组成的名称时，这非常有用。
- en: 'The sample sentence in the following example contains two place names consisting
    of several words (“Golden Gate Bridge” and “San Francisco”) that we might want
    to group together. The default tokenization won’t recognize these multi-word place
    names as single tokens. Look at what happens when we list the text’s tokens:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例中的句子包含了两个由多个单词组成的地名（“Golden Gate Bridge”和“San Francisco”），我们可能希望将它们归为一类。默认的标记化方式不会将这些多词地名识别为单一标记。查看当我们列出文本的标记时会发生什么：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each word and punctuation mark is its own token.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词和标点符号都是它自己的标记。
- en: 'With the `span.merge()` method, we can change this default behavior:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`span.merge()`方法，我们可以改变这种默认行为：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this example, we create a lemma for the “Golden Gate Bridge” span, and then
    pass the lemma to `span.merge()` as a parameter. (To be precise, we pass on the
    lemma’s id obtained through the `doc.vocab.string` attribute.)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们为“Golden Gate Bridge”span创建一个词形，然后将该词形作为参数传递给`span.merge()`。（准确地说，我们传递了通过`doc.vocab.string`属性获取的词形ID。）
- en: Note that the `span.merge()` method doesn’t merge the corresponding lemmas by
    default. When called without parameters, it sets the lemma of the merged token
    to the lemma of the first token of the span being merged. To specify the lemma
    we want to assign to the merged token, we pass it to `span.merge()` as the lemma
    parameter, as illustrated here.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`span.merge()`方法默认不会合并相应的词形。当没有参数时，它将合并的词元的词形设置为被合并的span中第一个词元的词形。为了指定我们希望分配给合并词元的词形，我们将其作为词形参数传递给`span.merge()`，如下面所示。
- en: 'Let’s check whether the lemmatizer, part-of-speech tagger, and dependency parser
    can handle the newly created lemma correctly:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下词形还原器、词性标注器和依存解析器是否能够正确处理新创建的词形：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This should produce the following output:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: All the attributes shown in the listing have been assigned to the “Golden Gate
    Bridge” token correctly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中显示的所有属性都已正确分配给“Golden Gate Bridge”词元。
- en: '***Try This***'
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: The sentence in the preceding example also contains San Francisco, another multi-word
    place name that you might want to merge into a single token. To achieve this,
    perform the same operations as listed in the previous code snippets for the “Golden
    Gate Bridge” span.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例中的句子还包含“San Francisco”，这是另一个多词地名，你可能希望将其合并为单个词元。为了实现这一点，请执行与前面的代码片段中“Golden
    Gate Bridge”span相同的操作。
- en: When determining the start and end positions for the “San Francisco” span in
    the document, don’t forget that the indices of the tokens located to the right
    of the newly created “Golden Gate Bridge” token have been shifted downward respectively.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定文档中“San Francisco”span的起始和结束位置时，别忘了，位于新创建的“Golden Gate Bridge”词元右侧的词元索引已经相应地向下移动了。
- en: '**Customizing the Text-Processing Pipeline**'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自定义文本处理流水线**'
- en: In the previous sections, you learned how spaCy’s container objects represent
    linguistic units, such as a text and an individual token, allowing you to extract
    linguistic features associated with them. Let’s now look at the objects in the
    spaCy API that create those containers and fill them with relevant data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你了解了spaCy的容器对象如何表示语言单元，如文本和单个词元，从而使你能够提取与它们相关的语言特征。现在，让我们来看看spaCy API中创建这些容器并将相关数据填充到其中的对象。
- en: 'These objects are referred to as processing pipeline components. As you’ve
    already learned, a pipeline set includes—by default—a part-of-speech tagger, a
    dependency parser, and an entity recognizer. You can check what pipeline components
    are available for your nlp object like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象被称为处理流水线组件。正如你已经了解的，默认情况下，流水线设置包括词性标注器、依存解析器和实体识别器。你可以像这样检查你的nlp对象可用的流水线组件：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As discussed in the following sections, spaCy allows you to customize the components
    in your pipeline to best suit your needs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如下文所讨论的，spaCy允许你自定义流水线中的组件，以最适合你的需求。
- en: '***Disabling Pipeline Components***'
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***禁用流水线组件***'
- en: 'spaCy allows you to load a selected set of pipeline components, disabling those
    that aren’t necessary. You can do this when creating an nlp object by setting
    the `disable` parameter:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy允许你加载选定的流水线组件，并禁用不必要的组件。你可以通过设置`disable`参数来做到这一点：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this example, we create a processing pipeline without a dependency parser.
    If we call this nlp instance on a text, the tokens won’t receive dependency labels.
    The following example illustrates this point clearly:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建了一个没有依存解析器的处理流水线。如果我们在文本上调用这个nlp实例，词元将不会收到依存标签。以下示例清楚地说明了这一点：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We try to print out the following information for each token from the sample
    sentence: the text content ➊, a part-of-speech tag ➋, and a dependency label ➌.
    But the dependency labels don’t appear.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试为每个来自示例句子的词元打印出以下信息：文本内容➊、词性标记➋和依存标签➌。但依存标签没有出现。
- en: '***Loading a Model Step by Step***'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***逐步加载模型***'
- en: 'You can perform several operations in one step with `spacy.load()`, which loads
    a model. For example, when you make this call:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`spacy.load()`一次性执行多个操作来加载模型。例如，当你进行以下调用时：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'spaCy performs the following steps behind the scenes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy在幕后执行以下步骤：
- en: Looking at the name of the model to be loaded, spaCy identifies what Language
    class it should initialize. In this example, spaCy creates an English class instance
    with shared vocabulary and other language data.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: spaCy iterates over the processing pipeline names, creates corresponding components,
    and adds them to the processing pipeline.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: spaCy loads the model data from disk and makes it available to the Language
    class instance.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These implementation details are hidden by `spacy.load()`, which in most cases
    saves you effort and time. But sometimes, you might need to implement these steps
    explicitly to have fine-grained control over the process. For example, you might
    need to load a custom component to the processing pipeline. The component could
    print some information about the Doc object in the pipeline, such as the number
    of tokens or the presence or absence of certain parts of speech.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: As usual, more fine-grained control requires you to provide more information.
    First, rather than specifying a shortcut, you’ll need to obtain the actual model
    name so you can get the path to the model package.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'You can identify the full name of the model as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `nlp.meta` attribute used in this code is a dictionary that contains the
    metadata of the loaded model. What you need in this example is the model’s language
    and the model’s name.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know the model’s name, you can find its location in your system
    by using the `get_package_path` utility function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The path specified in this code might be different on your machine, depending
    on your Python installation directory. Regardless, this is not the full path.
    You’ll need to append one more folder to it. The name of this folder is composed
    of the model name and the model version appended to it. (This is where the model
    package is located.) You can determine its name as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You might also want to look at the list of pipeline components used with the
    model. (It’s important to know what components are supported in the context of
    the model and therefore can be loaded to the pipeline.) You can obtain this information
    via the `nlp.meta` attribute’s `''pipeline''` field, as shown here (or via the
    `nlp.pipe_names` attribute introduced in the beginning of “[Customizing the Text-Processing
    Pipeline](../Text/ch03.xhtml#lev34)” on [page 37](../Text/ch03.xhtml#page_37)):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With this information, we can create a script that implements the steps provided
    at the beginning of this section:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this script, we use `spacy.util.get_lang_class()` ➊ to load a Language class.
    Which class we load depends on the two-letter language code specified as the parameter.
    In this example, we load English. Next, in a loop ➋, we create ➌ and add ➍ the
    pipeline components to the processing pipeline. Then we load a model from disk,
    specifying the path to it used on your machine ➎.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the code in this script, it might seem that the pipeline components
    become functional once we’ve added them to the processing pipeline. Actually,
    we can’t use them until we load the model data, so if we omit the last line of
    code in the script, we won’t even be able to create a Doc object using this nlp
    instance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '***Customizing the Pipeline Components***'
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By customizing pipeline components, you can best meet the needs of your application.
    For example, suppose you want your model’s named entity recognizer system to recognize
    the word Festy as a city district. By default, it recognizes it as an organization,
    as illustrated in the following example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The label `ORG` stands for companies, agencies, and other institutions. But
    you want to make the entity recognizer classify it as an entity of type `DISTRICT`
    instead.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The entity recognizer component is implemented in the spaCy API as the `EntityRecognizer`
    class. Using this class’s methods, you can initialize an instance of `ner` and
    then apply it to a text. In most cases, you don’t need to perform these operations
    explicitly; spaCy does it for you under the hood when you create an nlp object
    and then create a Doc object, respectively.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: But when you want to update the named entity recognition system of an existing
    model with your own examples, you’ll need to work with some of the `ner` object’s
    methods explicitly.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, you’ll first have to add a new label called `DISTRICT`
    to the list of supported entity types. Then you need to create a training example,
    which is what you’ll show the entity recognizer so it will learn what to apply
    the `DISTRICT` label to. The simplest implementation of the preparation steps
    might look as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: For simplicity, this training set contains just two training samples (typically,
    you need to provide many more). Each training sample includes a sentence that
    might or might not contain an entity (or entities) to which the new entity label
    should be assigned ➊. If there is an entity in the sample, you specify its start
    and end position ➋. The second sentence in the training set doesn’t contain the
    word Festy at all ➌. This is due to the way the training process is organized.
    [Chapter 10](../Text/ch10.xhtml#ch10) covers the details of this process in more
    depth.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Your next step is to add a new entity label `DISTRICT` to the entity recognizer:
    but before you can do this, you must get the instance of the `ner` pipeline component.
    You can do this as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once you have a `ner` object, you can add a new label to it using the `ner.add_label()`
    method, as shown here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Another action you need to take before you can start training the entity recognizer
    is to disable the other pipes to make sure that only the entity recognizer will
    be updated during the training process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then you can start training the entity recognizer using the training samples
    in the `TRAIN_DATA` list created earlier in this section:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: During training, the sample examples are shown to the model in a loop, in random
    order, to efficiently update the underlying model’s data and avoid any generalizations
    based on the order of training examples. The execution will take a while.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the preceding code has successfully completed, you can test how the updated
    optimizer recognizes the token Festy:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: According to the output, it works correctly.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that the updates you just made will be lost when you close this
    Python interpreter session. To address this problem, the `Pipe` class—the parent
    of the `EntityRecognizer` class and other pipeline components classes—has the
    `to_disk()` method, which allows you to serialize the pipe to disk:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now you can load the updated component to a new session with the `from_disk()`
    method. To make sure it works, close your current interpreter session, start a
    new one, and then run the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You load the model, disabling its default `ner` component ➊. Next, you create
    a new `ner` instance ➋ and then load it with the data from disk ➌. Then you add
    the `ner` component to the processing pipeline ➍.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can test it, like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you can see, the entity recognizer labels the name Festy correctly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Although I’ve shown you how to customize the named entity recognizer only, you
    can also customize the other pipeline components in a similar way.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Using spaCy’s C-Level Data Structures**'
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even with spaCy, NLP operations that involve processing large amounts of text
    can be very time-consuming. For example, you might need to compose a list of the
    adjectives most appropriate for a certain noun, and to do this, you’ll have to
    examine a large amount of text. If processing speed is critical to your application,
    spaCy allows you to take advantage of Cython’s C-level data structures and interfaces.
    Cython is one of the languages in which spaCy is written (the other one is Python).
    Because it’s a superset of Python, Cython considers almost all Python code valid
    Cython code. In addition to Python’s functionality, Cython allows you to natively
    call C functions and declare fast C types, enabling the compiler to generate very
    efficient code. You might want to use Cython to speed up time-consuming text processing
    operations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: spaCy’s core data structures are implemented as Cython objects, and spaCy’s
    public API allows you to access those structures. For details, refer to the Cython
    Architecture page in the documentation at *[https://spacy.io/api/cython/](https://spacy.io/api/cython/)*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '***How It Works***'
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To use Cython code with spaCy, you must turn it into a Python extension module
    that you can then import into your program, as illustrated in [Figure 3-3](../Text/ch03.xhtml#ch03fig03).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/fig3-3.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3-3: Building a Python extension module from a Cython script*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: You can do this by saving Cython code in a *.pyx* file and then running a *setup.py*
    Python script that first converts Cython code into corresponding C or C++ code
    and then invokes a C or C++ compiler. The script generates the Python extension
    module.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '***Preparing Your Working Environment and Getting Text Files***'
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before you can start building Cython code, you need to install Cython on your
    machine and obtain a large text file to work with.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Cython on your machine using `pip`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Next, to simulate a time-consuming task and measure performance, you’ll need
    a large text file. For this, you can use a *Wikipedia dump file*, which contains
    a set of pages wrapped in XML. Wikipedia dump files are available for download
    at *[https://dumps.wikimedia.org/enwiki/latest/](https://dumps.wikimedia.org/enwiki/latest/)*.
    Scroll down to the *enwiki-latest-pages-articles*.xml-*.bz2* files and choose
    one that is large enough for your tests. But don’t choose one that is too large
    unless you want to spend hours waiting for your machine to complete your test
    code. A dump file of 10–100MB should be appropriate.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve downloaded the file, extract raw text from it with a tool like *gensim.corpora.wikicorpus*
    (*[https://radimrehurek.com/gensim/corpora/wikicorpus.html](https://radimrehurek.com/gensim/corpora/wikicorpus.html)*),
    which is designed specifically for constructing a text corpus from a Wikipedia
    database dump.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '***Your Cython Script***'
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now let’s write a Cython script that analyzes the text file. For simplicity,
    suppose all you want to do is count the number of personal pronouns in the submitted
    text. That means you need to count the number of tokens with the `PRP` part-of-speech
    tag assigned to them.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '*As stated in the documentation, C-level methods intended for use from Cython
    are designed for speed over safety. Mistakes in the code might cause the execution
    to crash abruptly.*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'In a directory in your local filesystem, create a file called *spacytext.pyx*
    and insert the following code into it:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We start with a set of `cimport` statements to import necessary Cython modules,
    mostly from the spaCy library.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Then we define the Cython struct `DocStruct` as the container for the text being
    processed and the `TokenC*` variable ➊, a pointer to a `TokenC` struct used in
    spaCy as the data container for the Token object.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a Cython function `counter` ➋ that counts the number of personal
    pronouns in the text.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '*The* `cdef` *functions won’t be available in the Python code that imports
    the module. If you want to create a function that will be visible to Python and
    to take advantage of C-level data structures and interfaces at the same time,
    you need to declare that function as* `cpdef`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we define a `cpdef` Cython/Python main function ➌ that we can use in
    Python.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '***Building a Cython Module***'
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unlike Python, you must compile Cython code. You can do this in several ways,
    the best of which is to write a distutils/setuptools *setup.py* Python script.
    Create a *setup.py* file in the same directory as your Cython script. Your *setup.py*
    file should include the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This is a regular distutils/setuptools *setup.py* script except for two additions
    related to the example we’re working with. First, we import `numpy` ➊ and then
    explicitly specify where to find the *.h* files of the library ➌. We do this to
    avoid the *numpy/arrayobject.h* compilation error that occurs in some systems.
    We use the other setup option, `language = "c++"` ➋ to instruct the setup process
    to employ a C++ compiler rather than performing C compilation, which is the default.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the setup script, you can build your Cython code. You can
    do this from within a system terminal, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A bunch of messages will display during the compilation process. Some of them
    might be warnings, but they’re rarely critical. For example, you might see this
    message, which is not critical for the process:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '***Testing the Module***'
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After the compilation completes successfully, the `spacytext` module will be
    added to your Python environment. To test the newly created module, open a Python
    session and run the following command:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If it displays no errors, you can enter the following commands (this assumes
    your text data is in a *test.txt* file):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You open the file in which you have text data for this example in binary mode
    to obtain a bytes object ➊. If the file is too big, you can pick up only part
    of its content when creating a Doc object ➋. Once you’ve created the Doc object,
    you can test the `spacytext` module you just created with Cython, invoking its
    `main()` function ➌.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The first figure in the output generated by the `spacytext.main()` function
    shows the total number of tokens found in the submitted text. The second figure
    is the number of personal pronouns found in this same text.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you looked at the most important of spaCy’s container objects.
    You also learned how to customize your text-processing pipeline and use spaCy’s
    C-level data structures and interfaces from Cython.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
