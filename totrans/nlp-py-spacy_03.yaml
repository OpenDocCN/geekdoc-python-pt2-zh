- en: '**3'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3'
- en: WORKING WITH CONTAINER OBJECTS AND CUSTOMIZING SPACY**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 处理容器对象和自定义 spaCy**
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: 'You can divide the main objects composing the spaCy API into two categories:
    containers (such as Tokens and Doc objects) and processing pipeline components
    (such as the part-of-speech tagger and named entity recognizer). This chapter
    explores container objects further. Using container objects and their methods,
    you can access the linguistic annotations that spaCy assigns to each token in
    a text.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将构成 spaCy API 的主要对象分为两类：容器（如 Tokens 和 Doc 对象）和处理管道组件（如词性标注器和命名实体识别器）。本章进一步探讨容器对象。通过使用容器对象及其方法，你可以访问
    spaCy 为文本中每个标记分配的语言学注释。
- en: You’ll also learn how to customize the pipeline components to suit your needs
    and use Cython code to speed up time-consuming NLP tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将学习如何自定义管道组件，以满足你的需求，并使用 Cython 代码加速耗时的自然语言处理任务。
- en: '**spaCy’s Container Objects**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**spaCy 的容器对象**'
- en: 'A *container object* groups multiple elements into a single unit. It can be
    a collection of objects, like tokens or sentences, or a set of annotations related
    to a single object. For example, spaCy’s Token object is a container for a set
    of annotations related to a single token in a text, such as that token’s part
    of speech. Container objects in spaCy mimic the structure of natural language
    texts: a text is composed of sentences, and each sentence contains tokens.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *容器对象* 将多个元素组合成一个单一单元。它可以是对象的集合，如标记或句子，或者与单个对象相关的一组注释。例如，spaCy 的 Token 对象是一个容器，包含与文本中单个标记相关的一组注释，例如该标记的词性。spaCy
    中的容器对象模仿了自然语言文本的结构：文本由句子组成，每个句子包含标记。
- en: Token, Span, and Doc, the most widely used container objects in spaCy from a
    user’s standpoint, represent a token, a phrase or sentence, and a text, respectively.
    A container can contain other containers—for example, a Doc contains Tokens. In
    this section, we’ll explore working with these container objects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Token、Span 和 Doc 是 spaCy 中最常用的容器对象，分别表示标记、短语或句子和文本。容器可以包含其他容器——例如，一个 Doc 包含
    Tokens。在本节中，我们将探讨如何使用这些容器对象。
- en: '***Getting the Index of a Token in a Doc Object***'
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***获取 Doc 对象中标记的索引***'
- en: A Doc object contains a collection of the Token objects generated as a result of
    the tokenization performed on a submitted text. These tokens have indices, allowing
    you to access them based on their positions in the text, as shown in [Figure 3-1](../Text/ch03.xhtml#ch03fig01).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Doc 对象包含由提交文本的标记化生成的一组 Token 对象。这些标记有索引，允许你根据它们在文本中的位置访问它们，如 [图 3-1](../Text/ch03.xhtml#ch03fig01)
    所示。
- en: '![image](../Images/fig3-1.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig3-1.jpg)'
- en: '*Figure 3-1: The tokens in a Doc object*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-1：Doc 对象中的标记*'
- en: 'The tokens are indexed starting with 0, which makes the length of the document
    minus 1 the index of the end position. To shred the Doc instance into tokens,
    you derive the tokens into a Python list by iterating over the Doc from the start
    token to the end token:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的索引从 0 开始，这使得文档长度减去 1 就是结束位置的索引。要将 Doc 实例拆分为标记，你可以通过迭代 Doc，从开始标记到结束标记，将标记提取为
    Python 列表：
- en: '>>> [doc[i] for i in range(len(doc))]'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> [doc[i] for i in range(len(doc))]'
- en: '[A, severe, storm, hit, the, beach, .]'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[A, severe, storm, hit, the, beach, .]'
- en: 'It’s worth noting that we can create a Doc object using its constructor explicitly,
    as illustrated in the following example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们可以显式使用构造函数创建一个 Doc 对象，如下例所示：
- en: '>>> from spacy.tokens.doc import Doc'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from spacy.tokens.doc import Doc'
- en: '>>> from spacy.vocab import Vocab'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from spacy.vocab import Vocab'
- en: '>>> doc = Doc(➊Vocab(), ➋words=[u''Hi'', u''there''])'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = Doc(➊Vocab(), ➋words=[u''Hi'', u''there''])'
- en: doc
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: doc
- en: Hi there
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你好
- en: 'We invoke the Doc’s constructor, passing it the following two parameters: a
    *vocab object* ➊—which is a storage container that provides vocabulary data, such
    as lexical types (adjective, verb, noun, and so on)—and a list of tokens to add
    to the Doc object being created ➋.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用 Doc 的构造函数，传入以下两个参数：一个 *词汇对象* ➊——它是一个存储容器，提供词汇数据，如词汇类型（形容词、动词、名词等）——以及一个要添加到正在创建的
    Doc 对象中的标记列表 ➋。
- en: '***Iterating over a Token’s Syntactic Children***'
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***迭代标记的句法子节点***'
- en: 'Suppose we need to find the leftward children of a token in the syntactic dependency
    parse of a sentence. For example, we can apply this operation to a noun to obtain
    its adjectives, if any. We might need to do this if we want to know what adjectives
    are able to modify a given noun. As an example, consider the following sentence:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要查找句法依赖分析中一个词的左侧子节点。例如，我们可以将这个操作应用于名词，从而获得其形容词（如果有的话）。如果我们想知道哪些形容词能够修饰给定的名词，就可能需要这样做。举个例子，考虑以下句子：
- en: I want a green apple.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: I want a green apple.
- en: The diagram in [Figure 3-2](../Text/ch03.xhtml#ch03fig02) highlights the syntactic
    dependencies of interest.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](../Text/ch03.xhtml#ch03fig02)中的图表突出显示了感兴趣的句法依赖关系。'
- en: '![image](../Images/fig3-2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig3-2.jpg)'
- en: '*Figure 3-2: An example of leftward syntactic dependencies*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-2：一个左侧句法依赖关系的示例*'
- en: 'To obtain the leftward syntactic children of the word “apple” in this sample
    sentence programmatically, we might use the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要在程序中获取这个示例句子中“apple”的左侧句法子节点，我们可以使用以下代码：
- en: '>>> doc = nlp(u''I want a green apple.'')'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''I want a green apple.'')'
- en: '>>> [w for w in doc[4].lefts]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> [w for w in doc[4].lefts]'
- en: '[a, green]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[a, green]'
- en: In this script, we simply iterate through the apple’s children, outputting them
    in a list.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们简单地遍历“apple”的子节点，并将它们输出为一个列表。
- en: 'It’s interesting to note that in this example, the leftward syntactic children
    of the word “apple” represent the entire sequence of the token’s syntactic children.
    In practice, this means that we might replace Token.lefts with Token.children,
    which finds all of a token’s syntactic children:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在这个例子中，单词“apple”的左侧句法子节点表示该词所有句法子节点的整个序列。实际上，这意味着我们可以将Token.lefts替换为Token.children，从而找到一个词的所有句法子节点：
- en: '>>> [w for w in doc[4].children]'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> [w for w in doc[4].children]'
- en: The result list will remain the same.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果列表将保持不变。
- en: 'We could also use Token.rights to get a token’s rightward syntactic children:
    in this example, the word “apple” is a rightward child of the word “want,” as
    shown in [Figure 3-1](../Text/ch03.xhtml#ch03fig01).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Token.rights来获取一个词的右侧句法子节点：在这个例子中，单词“apple”是单词“want”的右侧子节点，如[图 3-1](../Text/ch03.xhtml#ch03fig01)所示。
- en: '***The doc.sents Container***'
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***doc.sents 容器***'
- en: Typically, the linguistic annotations assigned to a token make sense only in
    the context of the sentence in which the token occurs. For example, information
    about whether the word is a noun or a verb might apply only to the sentence in
    which this word is located (like the word “count,” discussed in previous chapters).
    In such cases, it would be useful to have the ability to access the tokens in
    the document with sentence-level indices.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分配给词汇的语言注释只有在词汇所在的句子上下文中才有意义。例如，关于某个词是名词还是动词的信息可能只适用于该词所在的句子（就像前面章节中讨论的“count”一词）。在这种情况下，能够使用句子级索引访问文档中的词汇将会非常有用。
- en: 'The Doc object’s doc.sents property lets us separate a text into its individual
    sentences, as illustrated in the following example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Doc对象的doc.sents属性允许我们将文本分割成单独的句子，如下例所示：
- en: '>>> doc = nlp(u''A severe storm hit the beach. It started to rain.'')'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''A severe storm hit the beach. It started to rain.'')'
- en: '➊ >>> for sent in doc.sents:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ >>> for sent in doc.sents:'
- en: ➋ ...   [sent[i] for i in range(len(sent))]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ ...   [sent[i] for i in range(len(sent))]
- en: '...'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: '[A, severe, storm, hit, the, beach, .]'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[A, severe, storm, hit, the, beach, .]'
- en: '[It, started, to, rain, .]'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[It, started, to, rain, .]'
- en: '>>>'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '>>>'
- en: We iterate over the sentences in the doc ➊, creating a separate list of tokens
    for each sentence ➋.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历文档中的句子 ➊，为每个句子创建一个独立的词汇列表 ➋。
- en: 'At the same time, we can still refer to the tokens in a multi-sentence text
    using the global, or document-level, indices, as shown here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们仍然可以使用全局（文档级）索引引用多句文本中的词汇，如下所示：
- en: '>>> [doc[i] for i in range(len(doc))]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> [doc[i] for i in range(len(doc))]'
- en: '[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]'
- en: 'The ability to refer to the Token objects in a document by their sentence-level
    indices can be useful if, for example, we need to check whether the first word
    in the second sentence of the text being processed is a pronoun (say we want to
    figure out the connection between two sentences: the first of which contains a
    noun and the second of which contains a pronoun that refers to the noun):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过句子级索引引用文档中的Token对象非常有用，举个例子，我们可能需要检查正在处理的文本中第二个句子的第一个单词是否是代词（例如，我们想弄清楚两个句子之间的关系，第一个句子包含一个名词，而第二个句子包含一个指代该名词的代词）：
- en: '>>> for i,sent in enumerate(doc.sents):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for i,sent in enumerate(doc.sents):'
- en: '...   if i==1 and sent[0].pos_== ''PRON'':'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '...   if i==1 and sent[0].pos_== ''PRON'':'
- en: '...     print(''The second sentence begins with a pronoun.'')'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '...     print(''第二个句子以代词开头。'')'
- en: The second sentence begins with a pronoun.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个句子以代词开头。
- en: In this example, we use an enumerator in the for loop to distinguish the sentences
    by index. This allows us to filter out sentences that we’re not interested in
    and check only the second sentence.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们在 for 循环中使用枚举器，通过索引区分句子。这使我们能够筛选出不感兴趣的句子，只检查第二个句子。
- en: Identifying the first word in a sentence is a breeze, because its index is always
    0\. But what about the last one? For example, what if we need to find out how
    many sentences in the text end with a verb—(not counting any periods, of course)?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 确定句子中的第一个词非常简单，因为它的索引总是 0。但是最后一个词呢？例如，如果我们需要找出文本中有多少个句子以动词结尾——（当然不包括句号）怎么办？
- en: '>>> counter = 0'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> counter = 0'
- en: '>>> for sent in doc.sents:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for sent in doc.sents:'
- en: '...   if sent[len(sent)-2].pos_ == ''VERB'':'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '...     if sent[len(sent)-2].pos_ == ''VERB'':'
- en: '...     counter+=1'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '...     counter+=1'
- en: '>>> print(counter)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(counter)'
- en: '1'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '1'
- en: 'Although the lengths of sentences vary, we can easily determine the length
    of a given sentence using the len() function. We reduce the value of len(sent)
    by 2 for the following reasons: first, the indices always start at 0 and end at
    size-1\. Second, the last token in both sentences in the sample is a period, which
    we need to ignore.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管句子的长度各异，我们可以轻松地使用 len() 函数确定给定句子的长度。我们将 len(sent) 的值减去 2，原因如下：首先，索引总是从 0 开始，到
    size-1 结束。其次，示例中两个句子的最后一个词都是句号，我们需要忽略它。
- en: '***The doc.noun_chunks Container***'
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***doc.noun_chunks 容器***'
- en: 'A Doc object’s doc.noun_chunks property allows us to iterate over the noun
    chunks in the document. A *noun chunk* is a phrase that has a noun as its head.
    For example, the previous sentence contains the following noun chunks:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Doc 对象的 doc.noun_chunks 属性允许我们遍历文档中的名词短语。一个 *名词短语* 是以名词为核心的短语。例如，前一句包含以下名词短语：
- en: A noun chunk
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名词短语
- en: a phrase
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个短语
- en: a noun
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名词
- en: its head
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它的核心
- en: 'With doc.noun_chunks, we can extract them as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 doc.noun_chunks，我们可以通过以下方式提取名词短语：
- en: '>>> doc = nlp(u''A noun chunk is a phrase that has a noun as its head.'')'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''一个名词短语是以名词为核心的短语。'')'
- en: '>>> for chunk in doc.noun_chunks:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for chunk in doc.noun_chunks:'
- en: '...   print(chunk)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '...   print(chunk)'
- en: 'Alternatively, we might extract noun chunks by iterating over the nouns in
    the sentence and finding the syntactic children for each noun to form a chunk.
    Earlier in “[Iterating over a Token’s Syntactic Children](../Text/ch03.xhtml#lev28)”
    on [page 33](../Text/ch03.xhtml#page_33), you saw an example of how to extract
    a phrase based on the syntactic dependency parse. Now let’s apply this technique
    to the sample sentence in this example to compose noun chunks manually:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过遍历句子中的名词，并找到每个名词的句法子项来提取名词短语。早些时候，在 “[遍历 Token 的句法子项](../Text/ch03.xhtml#lev28)”
    [第33页](../Text/ch03.xhtml#page_33) 你看到了如何基于句法依赖解析来提取短语的示例。现在，让我们将这个技术应用于本例中的示例句子，手动组成名词短语：
- en: 'for token in doc:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'for token in doc:'
- en: '➊ if token.pos_==''NOUN'':'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ if token.pos_==''NOUN'':'
- en: chunk = ''
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: chunk = ''
- en: '➋ for w in token.children:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ for w in token.children:'
- en: '➌ if w.pos_ == ''DET'' or w.pos_ == ''ADJ'':'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ if w.pos_ == ''DET'' or w.pos_ == ''ADJ'':'
- en: chunk = chunk + w.text + ' '
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: chunk = chunk + w.text + ' '
- en: ➍ chunk = chunk + token.text
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ chunk = chunk + token.text
- en: print(chunk)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: print(chunk)
- en: Iterating over the tokens, we pick up only nouns ➊. Next, in the inner loop,
    we iterate over a noun’s children ➋, picking up only the tokens that are either
    determiners or adjectives for the noun chunk (noun chunks can also include some
    other parts of speech, say, adverbs) ➌. Then we append the noun to the chunk ➍.
    As a result, the output of the script should be the same as in the previous example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历词元时，我们只拾取名词 ➊。接下来，在内层循环中，我们遍历名词的子项 ➋，只拾取作为名词短语的限定词或形容词（名词短语还可以包含其他词性，如副词）
    ➌。然后，我们将名词添加到短语中 ➍。因此，脚本的输出应该与前面的示例相同。
- en: '***Try This***'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试这个***'
- en: Notice that the words used to modify a noun (determiners and adjectives) are
    always the leftward syntactic children of the noun. This makes it possible to
    replace Token.children with Token.lefts in the previous code and then remove the
    check for the children to be either a determiner or an adjective, as necessary.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，用于修饰名词的词（限定词和形容词）总是名词的左侧句法子项。这使得在前面的代码中可以将 Token.children 替换为 Token.lefts，并在需要时移除检查子项是否为限定词或形容词。
- en: Rewrite the previous snippet, incorporating the changes suggested here. The
    resulting set of noun chunks should remain the same in your script.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '***The Span Object***'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Span object is a slice from a Doc object. In the previous sections, you
    saw how to use it as a container for a sentence and a noun chunk, derived from
    doc.sents and doc.noun_chunks, respectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The Span object’s usage isn’t limited to being a container for sentences or
    noun chunks only. We can use it to contain an arbitrary set of neighboring tokens
    in the document by specifying a range of indices, as in the following example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '>>> doc=nlp(''I want a green apple.'')'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '>>> doc[2:5]'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: a green apple
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The Span object contains several methods, one of the most interesting of which
    is span.merge(), which allows us to merge the span into a single token, retokenizing
    the document. This can be useful when the text contains names consisting of several
    words.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample sentence in the following example contains two place names consisting
    of several words (“Golden Gate Bridge” and “San Francisco”) that we might want
    to group together. The default tokenization won’t recognize these multi-word place
    names as single tokens. Look at what happens when we list the text’s tokens:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '>>> doc = nlp(u''The Golden Gate Bridge is an iconic landmark in San Francisco.'')'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '>>> [doc[i] for i in range(len(doc))]'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Each word and punctuation mark is its own token.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'With the span.merge() method, we can change this default behavior:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '>>> span = doc[1:4]'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '>>> lem_id = doc.vocab.strings[span.text]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '>>> span.merge(lemma = lem_id)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Golden Gate Bridge
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we create a lemma for the “Golden Gate Bridge” span, and then
    pass the lemma to span.merge() as a parameter. (To be precise, we pass on the
    lemma’s id obtained through the doc.vocab.string attribute.)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note that the span.merge() method doesn’t merge the corresponding lemmas by
    default. When called without parameters, it sets the lemma of the merged token
    to the lemma of the first token of the span being merged. To specify the lemma
    we want to assign to the merged token, we pass it to span.merge() as the lemma
    parameter, as illustrated here.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check whether the lemmatizer, part-of-speech tagger, and dependency parser
    can handle the newly created lemma correctly:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '>>> for token in doc:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: print(token.text, token.lemma_, token.pos_, token.dep_)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'This should produce the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The                the                DET   det
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Golden Gate Bridge Golden Gate Bridge PROPN nsubj
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: is                 be                 VERB  ROOT
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: an                 an                 DET   det
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: iconic             iconic             ADJ   amod
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: landmark           landmark           NOUN  attr
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: in                 in                 ADP   prep
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: San                san                PROPN compound
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Francisco          francisco          PROPN pobj
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: .                  .                  PUNCT punct
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: All the attributes shown in the listing have been assigned to the “Golden Gate
    Bridge” token correctly.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所有列出的属性已正确分配给“Golden Gate Bridge”标记。
- en: '***Try This***'
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: The sentence in the preceding example also contains San Francisco, another multi-word
    place name that you might want to merge into a single token. To achieve this,
    perform the same operations as listed in the previous code snippets for the “Golden
    Gate Bridge” span.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例中的句子还包含San Francisco，这是另一个你可能希望合并为单个标记的多词地名。为了实现这一点，按照前面代码片段中列出的操作，对“Golden
    Gate Bridge”跨度执行相同的操作。
- en: When determining the start and end positions for the “San Francisco” span in
    the document, don’t forget that the indices of the tokens located to the right
    of the newly created “Golden Gate Bridge” token have been shifted downward respectively.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定文档中“San Francisco”跨度的开始和结束位置时，别忘了新创建的“Golden Gate Bridge”标记右侧的标记索引已分别向下移动。
- en: '**Customizing the Text-Processing Pipeline**'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**自定义文本处理管道**'
- en: In the previous sections, you learned how spaCy’s container objects represent
    linguistic units, such as a text and an individual token, allowing you to extract
    linguistic features associated with them. Let’s now look at the objects in the
    spaCy API that create those containers and fill them with relevant data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你了解了spaCy的容器对象如何表示语言单元，如文本和单个标记，从而使你能够提取与它们相关的语言特征。现在，让我们来看一下spaCy API中创建这些容器并填充相关数据的对象。
- en: 'These objects are referred to as processing pipeline components. As you’ve
    already learned, a pipeline set includes—by default—a part-of-speech tagger, a
    dependency parser, and an entity recognizer. You can check what pipeline components
    are available for your nlp object like this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象被称为处理管道组件。如你已了解，默认情况下，管道集包括词性标注器、依赖解析器和实体识别器。你可以通过以下方式检查哪些管道组件可用于你的nlp对象：
- en: '>>> nlp.pipe_names'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> nlp.pipe_names'
- en: '[''tagger'', ''parser'', ''ner'']'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[''tagger'', ''parser'', ''ner'']'
- en: As discussed in the following sections, spaCy allows you to customize the components
    in your pipeline to best suit your needs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如下文所述，spaCy允许你定制管道中的组件，以最适应你的需求。
- en: '***Disabling Pipeline Components***'
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***禁用管道组件***'
- en: 'spaCy allows you to load a selected set of pipeline components, disabling those
    that aren’t necessary. You can do this when creating an nlp object by setting
    the disable parameter:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy允许你加载一组选定的管道组件，禁用那些不必要的组件。你可以在创建nlp对象时通过设置disable参数来做到这一点：
- en: nlp = spacy.load('en', disable=['parser'])
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en', disable=['parser'])
- en: 'In this example, we create a processing pipeline without a dependency parser.
    If we call this nlp instance on a text, the tokens won’t receive dependency labels.
    The following example illustrates this point clearly:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建了一个没有依赖解析器的处理管道。如果我们在文本上调用此nlp实例，标记将不会得到依赖标签。以下示例清楚地说明了这一点：
- en: '>>> doc = nlp(u''I want a green apple.'')'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''I want a green apple.'')'
- en: '>>> for token in doc:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for token in doc:'
- en: '...   print(➊token.text, ➋token.pos_, ➌token.dep_)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '...   print(➊token.text, ➋token.pos_, ➌token.dep_)'
- en: I     PRON
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: I     代词
- en: want  VERB
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: want 动词
- en: a     DET
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: a     限定词
- en: green ADJ
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: green 形容词
- en: apple NOUN
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: apple 名词
- en: .     PUNCT
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: .     标点符号
- en: 'We try to print out the following information for each token from the sample
    sentence: the text content ➊, a part-of-speech tag ➋, and a dependency label ➌.
    But the dependency labels don’t appear.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试打印出样本文本中每个标记的以下信息：文本内容 ➊、词性标注 ➋、和依赖标签 ➌。但是，依赖标签没有出现。
- en: '***Loading a Model Step by Step***'
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***逐步加载模型***'
- en: 'You can perform several operations in one step with spacy.load(), which loads
    a model. For example, when you make this call:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过spacy.load()一次性执行多个操作，该函数加载一个模型。例如，当你调用以下代码时：
- en: nlp = spacy.load('en')
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: 'spaCy performs the following steps behind the scenes:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy在幕后执行以下步骤：
- en: Looking at the name of the model to be loaded, spaCy identifies what Language
    class it should initialize. In this example, spaCy creates an English class instance
    with shared vocabulary and other language data.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看要加载的模型名称时，spaCy会识别应该初始化哪个Language类。在此示例中，spaCy创建了一个包含共享词汇和其他语言数据的英语类实例。
- en: spaCy iterates over the processing pipeline names, creates corresponding components,
    and adds them to the processing pipeline.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spaCy遍历处理管道的名称，创建相应的组件，并将其添加到处理管道中。
- en: spaCy loads the model data from disk and makes it available to the Language
    class instance.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spaCy从磁盘加载模型数据，并将其提供给Language类实例。
- en: These implementation details are hidden by spacy.load(), which in most cases
    saves you effort and time. But sometimes, you might need to implement these steps
    explicitly to have fine-grained control over the process. For example, you might
    need to load a custom component to the processing pipeline. The component could
    print some information about the Doc object in the pipeline, such as the number
    of tokens or the presence or absence of certain parts of speech.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实现细节被spacy.load()隐藏，在大多数情况下，它为你节省了精力和时间。但有时，你可能需要显式地实现这些步骤，以便更精细地控制过程。例如，你可能需要将自定义组件加载到处理管道中。该组件可以打印关于管道中Doc对象的一些信息，如标记数或某些词性是否存在。
- en: As usual, more fine-grained control requires you to provide more information.
    First, rather than specifying a shortcut, you’ll need to obtain the actual model
    name so you can get the path to the model package.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，更多的精细控制要求你提供更多的信息。首先，不是指定快捷方式，而是需要获取实际的模型名称，这样你才能获取到模型包的路径。
- en: 'You can identify the full name of the model as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式识别模型的完整名称：
- en: '>>> print(nlp.meta[''lang''] + ''_'' + nlp.meta[''name''])'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(nlp.meta[''lang''] + ''_'' + nlp.meta[''name''])'
- en: en_core_web_sm
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: en_core_web_sm
- en: The nlp.meta attribute used in this code is a dictionary that contains the metadata
    of the loaded model. What you need in this example is the model’s language and
    the model’s name.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码中使用的nlp.meta属性是一个字典，包含加载的模型的元数据。在这个示例中，你需要的是模型的语言和模型的名称。
- en: 'Now that you know the model’s name, you can find its location in your system
    by using the get_package_path utility function:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了模型的名称，你可以通过使用get_package_path工具函数来查找它在系统中的位置：
- en: '>>> from spacy import util'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from spacy import util'
- en: '>>> util.get_package_path(''en_core_web_sm'')'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> util.get_package_path(''en_core_web_sm'')'
- en: PosixPath('/usr/local/lib/python3.5/site-packages/en_core_web_sm')
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: PosixPath('/usr/local/lib/python3.5/site-packages/en_core_web_sm')
- en: 'The path specified in this code might be different on your machine, depending
    on your Python installation directory. Regardless, this is not the full path.
    You’ll need to append one more folder to it. The name of this folder is composed
    of the model name and the model version appended to it. (This is where the model
    package is located.) You can determine its name as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中指定的路径在你的机器上可能不同，具体取决于你的Python安装目录。不管怎样，这不是完整路径，你需要在其后追加一个文件夹。该文件夹的名称由模型名称和模型版本组成。（这是模型包所在的位置。）你可以按以下方式确定其名称：
- en: '>>> print(nlp.meta[''lang''] + ''_'' + nlp.meta[''name''] + ''-'' + nlp.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(nlp.meta[''lang''] + ''_'' + nlp.meta[''name''] + ''-'' + nlp.'
- en: meta['version'])
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: meta['version'])
- en: en_core_web_sm-2.0.0
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: en_core_web_sm-2.0.0
- en: 'You might also want to look at the list of pipeline components used with the
    model. (It’s important to know what components are supported in the context of
    the model and therefore can be loaded to the pipeline.) You can obtain this information
    via the nlp.meta attribute’s ''pipeline'' field, as shown here (or via the nlp.pipe_names
    attribute introduced in the beginning of “[Customizing the Text-Processing Pipeline](../Text/ch03.xhtml#lev34)”
    on [page 37](../Text/ch03.xhtml#page_37)):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可能想查看与模型一起使用的管道组件列表。（了解模型支持哪些组件非常重要，因此可以将它们加载到管道中。）你可以通过nlp.meta属性的'pipeline'字段获取这些信息，如下所示（或者通过nlp.pipe_names属性，它在“[定制文本处理管道](../Text/ch03.xhtml#lev34)”一节的[第37页](../Text/ch03.xhtml#page_37)中介绍）：
- en: '>>> nlp.meta[''pipeline'']'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> nlp.meta[''pipeline'']'
- en: '[''tagger'', ''parser'', ''ner'']'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[''tagger'', ''parser'', ''ner'']'
- en: 'With this information, we can create a script that implements the steps provided
    at the beginning of this section:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，我们可以创建一个脚本，按照本节开始时提供的步骤进行操作：
- en: '>>> lang = ''en'''
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> lang = ''en'''
- en: '>>> pipeline = [''tagger'', ''parser'', ''ner'']'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> pipeline = [''tagger'', ''parser'', ''ner'']'
- en: '>>> model_data_path = ''/usr/local/lib/python3.5/site-packages/en_core_web_sm/'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> model_data_path = ''/usr/local/lib/python3.5/site-packages/en_core_web_sm/'
- en: en_core_web_sm-2.0.0'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: en_core_web_sm-2.0.0'
- en: ➊ >>> lang_cls = spacy.util.get_lang_class(lang)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ >>> lang_cls = spacy.util.get_lang_class(lang)
- en: '>>> nlp = lang_cls()'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> nlp = lang_cls()'
- en: '➋ >>> for name in pipeline:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ >>> for name in pipeline:'
- en: ➌ ...   component = nlp.create_pipe(name)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ ...   component = nlp.create_pipe(name)
- en: ➍ ...   nlp.add_pipe(component)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ ...   nlp.add_pipe(component)
- en: ➎ >>> nlp.from_disk(model_data_path)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ >>> nlp.from_disk(model_data_path)
- en: In this script, we use spacy.util.get_lang_class() ➊ to load a Language class.
    Which class we load depends on the two-letter language code specified as the parameter.
    In this example, we load English. Next, in a loop ➋, we create ➌ and add ➍ the
    pipeline components to the processing pipeline. Then we load a model from disk,
    specifying the path to it used on your machine ➎.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们使用 spacy.util.get_lang_class() ➊ 来加载一个 Language 类。我们加载哪个类取决于作为参数指定的两位字母语言代码。在这个例子中，我们加载英语。接下来，在一个循环
    ➋ 中，我们创建 ➌ 并添加 ➍ 处理管道组件到处理管道中。然后我们从磁盘加载一个模型，指定用于你机器上的路径 ➎。
- en: Looking at the code in this script, it might seem that the pipeline components
    become functional once we’ve added them to the processing pipeline. Actually,
    we can’t use them until we load the model data, so if we omit the last line of
    code in the script, we won’t even be able to create a Doc object using this nlp
    instance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此脚本中的代码，可能会觉得一旦我们将管道组件添加到处理管道中，它们就会变得可用。实际上，在加载模型数据之前，我们无法使用它们，因此，如果省略脚本中的最后一行代码，我们甚至无法使用这个
    nlp 实例创建一个 Doc 对象。
- en: '***Customizing the Pipeline Components***'
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***自定义处理管道组件***'
- en: 'By customizing pipeline components, you can best meet the needs of your application.
    For example, suppose you want your model’s named entity recognizer system to recognize
    the word Festy as a city district. By default, it recognizes it as an organization,
    as illustrated in the following example:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自定义管道组件，你可以最好地满足应用程序的需求。例如，假设你希望你的模型的命名实体识别系统将“Festy”识别为一个市区。默认情况下，它会将其识别为一个组织，如下所示：
- en: '>>> doc = nlp(u''I need a taxi to Festy.'')'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''I need a taxi to Festy.'')'
- en: '>>> for ent in doc.ents:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for ent in doc.ents:'
- en: '...  print(ent.text, ent.label_)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '...  print(ent.text, ent.label_)'
- en: Festy ORG
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Festy ORG
- en: The label ORG stands for companies, agencies, and other institutions. But you
    want to make the entity recognizer classify it as an entity of type DISTRICT instead.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 标签 ORG 代表公司、机构和其他组织。但你希望让实体识别器将其分类为 DISTRICT 类型的实体。
- en: The entity recognizer component is implemented in the spaCy API as the EntityRecognizer
    class. Using this class’s methods, you can initialize an instance of ner and then
    apply it to a text. In most cases, you don’t need to perform these operations
    explicitly; spaCy does it for you under the hood when you create an nlp object
    and then create a Doc object, respectively.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 实体识别组件在 spaCy API 中实现为 EntityRecognizer 类。使用该类的方法，你可以初始化一个 ner 实例，然后将其应用于文本。在大多数情况下，你不需要显式地执行这些操作；当你创建一个
    nlp 对象并分别创建一个 Doc 对象时，spaCy 会自动为你处理这些操作。
- en: But when you want to update the named entity recognition system of an existing
    model with your own examples, you’ll need to work with some of the ner object’s
    methods explicitly.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但当你希望用自己的示例更新现有模型的命名实体识别系统时，你需要显式地使用一些 ner 对象的方法。
- en: 'In the following example, you’ll first have to add a new label called DISTRICT
    to the list of supported entity types. Then you need to create a training example,
    which is what you’ll show the entity recognizer so it will learn what to apply
    the DISTRICT label to. The simplest implementation of the preparation steps might
    look as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，你首先需要将一个名为 DISTRICT 的新标签添加到支持的实体类型列表中。然后你需要创建一个训练示例，实体识别器将通过它学习将 DISTRICT
    标签应用于哪些内容。准备步骤的最简单实现可能如下所示：
- en: LABEL = 'DISTRICT'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: LABEL = 'DISTRICT'
- en: TRAIN_DATA = [
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: TRAIN_DATA = [
- en: ➊ ('We need to deliver it to Festy.', {
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ ('We need to deliver it to Festy.', {
- en: '➋ ''entities'': [(25, 30, ''DISTRICT'')]'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ ''entities'': [(25, 30, ''DISTRICT'')]'
- en: '}),'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '}),'
- en: ➌ ('I like red oranges', {
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ ('I like red oranges', {
- en: '''entities'': []'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '''entities'': []'
- en: '})'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: ']'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: For simplicity, this training set contains just two training samples (typically,
    you need to provide many more). Each training sample includes a sentence that
    might or might not contain an entity (or entities) to which the new entity label
    should be assigned ➊. If there is an entity in the sample, you specify its start
    and end position ➋. The second sentence in the training set doesn’t contain the
    word Festy at all ➌. This is due to the way the training process is organized.
    [Chapter 10](../Text/ch10.xhtml#ch10) covers the details of this process in more
    depth.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，这个训练集仅包含两个训练样本（通常，你需要提供更多样本）。每个训练样本包括一个句子，该句子可能包含或不包含需要分配新实体标签的实体（或多个实体）➊。如果样本中有实体，你需要指定它的起始和结束位置➋。训练集中的第二个句子根本不包含“Festy”这个词➌。这是由于训练过程的组织方式。[第10章](../Text/ch10.xhtml#ch10)更详细地介绍了这个过程的细节。
- en: 'Your next step is to add a new entity label DISTRICT to the entity recognizer:
    but before you can do this, you must get the instance of the ner pipeline component.
    You can do this as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是向实体识别器添加一个新的实体标签 DISTRICT：但在此之前，你必须获取 ner 管道组件的实例。你可以通过以下方式实现：
- en: ner = nlp.get_pipe('ner')
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ner = nlp.get_pipe('ner')
- en: 'Once you have a ner object, you can add a new label to it using the ner.add_label()
    method, as shown here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有一个 ner 对象，你可以使用 ner.add_label() 方法向其中添加一个新的标签，如下所示：
- en: ner.add_label(LABEL)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ner.add_label(LABEL)
- en: 'Another action you need to take before you can start training the entity recognizer
    is to disable the other pipes to make sure that only the entity recognizer will
    be updated during the training process:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练实体识别器之前，你还需要采取一个动作，即禁用其他管道，以确保在训练过程中只有实体识别器会被更新：
- en: nlp.disable_pipes('tagger')
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: nlp.disable_pipes('tagger')
- en: nlp.disable_pipes('parser')
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: nlp.disable_pipes('parser')
- en: 'Then you can start training the entity recognizer using the training samples
    in the TRAIN_DATA list created earlier in this section:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用之前在本节中创建的 TRAIN_DATA 列表中的训练样本开始训练实体识别器：
- en: optimizer = nlp.entity.create_optimizer()
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer = nlp.entity.create_optimizer()
- en: import random
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: import random
- en: 'for i in range(25):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(25):'
- en: random.shuffle(TRAIN_DATA)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: random.shuffle(TRAIN_DATA)
- en: 'for text, annotations in TRAIN_DATA:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'for text, annotations in TRAIN_DATA:'
- en: nlp.update([text], [annotations], sgd=optimizer)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: nlp.update([text], [annotations], sgd=optimizer)
- en: During training, the sample examples are shown to the model in a loop, in random
    order, to efficiently update the underlying model’s data and avoid any generalizations
    based on the order of training examples. The execution will take a while.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，示例会以循环、随机顺序展示给模型，以有效更新底层模型的数据，并避免根据训练示例的顺序进行任何概括。执行过程可能需要一些时间。
- en: 'Once the preceding code has successfully completed, you can test how the updated
    optimizer recognizes the token Festy:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦前面的代码成功完成，你就可以测试更新后的优化器如何识别 token Festy：
- en: '>>> doc = nlp(u''I need a taxi to Festy.'')'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''我需要一辆去 Festy 的出租车。'')'
- en: '>>> for ent in doc.ents:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for ent in doc.ents:'
- en: '... print(ent.text, ent.label_)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '... print(ent.text, ent.label_)'
- en: '...'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: Festy DISTRICT
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Festy DISTRICT
- en: According to the output, it works correctly.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出，它正常工作。
- en: 'Keep in mind that the updates you just made will be lost when you close this
    Python interpreter session. To address this problem, the Pipe class—the parent
    of the EntityRecognizer class and other pipeline components classes—has the to_disk()
    method, which allows you to serialize the pipe to disk:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当你关闭这个 Python 解释器会话时，刚才所做的更新将会丢失。为了解决这个问题，Pipe 类——EntityRecognizer 类及其他管道组件类的父类——提供了
    to_disk() 方法，允许你将管道序列化到磁盘：
- en: '>>> ner.to_disk(''/usr/to/ner'')'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> ner.to_disk(''/usr/to/ner'')'
- en: 'Now you can load the updated component to a new session with the from_disk()
    method. To make sure it works, close your current interpreter session, start a
    new one, and then run the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用 from_disk() 方法将更新后的组件加载到新会话中。为了确保它正常工作，关闭当前的解释器会话，启动一个新会话，然后运行以下代码：
- en: '>>> import spacy'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import spacy'
- en: '>>> from spacy.pipeline import EntityRecognizer'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from spacy.pipeline import EntityRecognizer'
- en: ➊ >>> nlp = spacy.load('en', disable=['ner'])
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ >>> nlp = spacy.load('en', disable=['ner'])
- en: ➋ >>> ner = EntityRecognizer(nlp.vocab)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ >>> ner = EntityRecognizer(nlp.vocab)
- en: ➌ >>> ner.from_disk('/usr/to/ner')
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ >>> ner.from_disk('/usr/to/ner')
- en: ➍ >>> nlp.add_pipe(ner)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ >>> nlp.add_pipe(ner)
- en: You load the model, disabling its default ner component ➊. Next, you create
    a new ner instance ➋ and then load it with the data from disk ➌. Then you add
    the ner component to the processing pipeline ➍.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你加载了模型，禁用了它的默认 ner 组件 ➊。接着，你创建一个新的 ner 实例 ➋，然后将其从磁盘加载数据 ➌。然后你将 ner 组件添加到处理管道中
    ➍。
- en: 'Now you can test it, like this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以像这样测试它：
- en: '>>> doc = nlp(u''We need to deliver it to Festy.'')'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u''我们需要把它送到 Festy。'')'
- en: '>>> for ent in doc.ents:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for ent in doc.ents:'
- en: '... print(ent.text, ent.label_)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '... print(ent.text, ent.label_)'
- en: Festy DISTRICT
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Festy DISTRICT
- en: As you can see, the entity recognizer labels the name Festy correctly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，实体识别器正确地标记了名字 Festy。
- en: Although I’ve shown you how to customize the named entity recognizer only, you
    can also customize the other pipeline components in a similar way.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我只向你展示了如何定制命名实体识别器，但你也可以以类似的方式定制其他管道组件。
- en: '**Using spaCy’s C-Level Data Structures**'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用 spaCy 的 C 层数据结构**'
- en: Even with spaCy, NLP operations that involve processing large amounts of text
    can be very time-consuming. For example, you might need to compose a list of the
    adjectives most appropriate for a certain noun, and to do this, you’ll have to
    examine a large amount of text. If processing speed is critical to your application,
    spaCy allows you to take advantage of Cython’s C-level data structures and interfaces.
    Cython is one of the languages in which spaCy is written (the other one is Python).
    Because it’s a superset of Python, Cython considers almost all Python code valid
    Cython code. In addition to Python’s functionality, Cython allows you to natively
    call C functions and declare fast C types, enabling the compiler to generate very
    efficient code. You might want to use Cython to speed up time-consuming text processing
    operations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用 spaCy，涉及处理大量文本的 NLP 操作也可能非常耗时。例如，你可能需要编写一个最适合某个名词的形容词列表，为此你需要检查大量的文本。如果处理速度对你的应用至关重要，spaCy
    允许你利用 Cython 的 C 级数据结构和接口。Cython 是 spaCy 编写的语言之一（另一个是 Python）。由于它是 Python 的超集，Cython
    几乎认为所有 Python 代码都是有效的 Cython 代码。除了 Python 的功能，Cython 还允许你原生调用 C 函数并声明快速的 C 类型，从而使编译器能够生成非常高效的代码。你可能希望使用
    Cython 来加速耗时的文本处理操作。
- en: spaCy’s core data structures are implemented as Cython objects, and spaCy’s
    public API allows you to access those structures. For details, refer to the Cython
    Architecture page in the documentation at *[https://spacy.io/api/cython/](https://spacy.io/api/cython/)*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的核心数据结构是作为 Cython 对象实现的，而 spaCy 的公共 API 允许你访问这些结构。有关详细信息，请参考文档中的 Cython
    架构页面，链接为 *[https://spacy.io/api/cython/](https://spacy.io/api/cython/)*。
- en: '***How It Works***'
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***它是如何工作的***'
- en: To use Cython code with spaCy, you must turn it into a Python extension module
    that you can then import into your program, as illustrated in [Figure 3-3](../Text/ch03.xhtml#ch03fig03).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Cython 代码与 spaCy 一起使用，你必须将其转换为 Python 扩展模块，然后可以将其导入到程序中，如 [图 3-3](../Text/ch03.xhtml#ch03fig03)
    所示。
- en: '![image](../Images/fig3-3.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig3-3.jpg)'
- en: '*Figure 3-3: Building a Python extension module from a Cython script*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3-3：从 Cython 脚本构建 Python 扩展模块*'
- en: You can do this by saving Cython code in a *.pyx* file and then running a *setup.py*
    Python script that first converts Cython code into corresponding C or C++ code
    and then invokes a C or C++ compiler. The script generates the Python extension
    module.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将 Cython 代码保存为 *.pyx* 文件，然后运行 *setup.py* Python 脚本来完成这项工作，该脚本首先将 Cython
    代码转换为相应的 C 或 C++ 代码，然后调用 C 或 C++ 编译器。该脚本生成 Python 扩展模块。
- en: '***Preparing Your Working Environment and Getting Text Files***'
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***准备你的工作环境并获取文本文件***'
- en: Before you can start building Cython code, you need to install Cython on your
    machine and obtain a large text file to work with.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建 Cython 代码之前，你需要在你的机器上安装 Cython 并获得一个大型文本文件来进行处理。
- en: 'Install Cython on your machine using pip:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pip 在你的机器上安装 Cython：
- en: pip install Cython
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: pip install Cython
- en: Next, to simulate a time-consuming task and measure performance, you’ll need
    a large text file. For this, you can use a *Wikipedia dump file*, which contains
    a set of pages wrapped in XML. Wikipedia dump files are available for download
    at *[https://dumps.wikimedia.org/enwiki/latest/](https://dumps.wikimedia.org/enwiki/latest/)*.
    Scroll down to the *enwiki-latest-pages-articles*.xml-*.bz2* files and choose
    one that is large enough for your tests. But don’t choose one that is too large
    unless you want to spend hours waiting for your machine to complete your test
    code. A dump file of 10–100MB should be appropriate.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了模拟一个耗时的任务并测量性能，你需要一个大型文本文件。你可以使用 *Wikipedia dump file*，它包含一组以 XML 格式封装的页面。Wikipedia
    dump 文件可以从 *[https://dumps.wikimedia.org/enwiki/latest/](https://dumps.wikimedia.org/enwiki/latest/)*
    下载。向下滚动找到 *enwiki-latest-pages-articles*.xml-*.bz2* 文件，并选择一个足够大的文件进行测试。但不要选择太大的文件，除非你想花几个小时等待机器完成测试代码。一个
    10–100MB 的 dump 文件应该合适。
- en: Once you’ve downloaded the file, extract raw text from it with a tool like *gensim.corpora.wikicorpus*
    (*[https://radimrehurek.com/gensim/corpora/wikicorpus.html](https://radimrehurek.com/gensim/corpora/wikicorpus.html)*),
    which is designed specifically for constructing a text corpus from a Wikipedia
    database dump.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，你可以使用 *gensim.corpora.wikicorpus*（*[https://radimrehurek.com/gensim/corpora/wikicorpus.html](https://radimrehurek.com/gensim/corpora/wikicorpus.html)*）等工具从中提取原始文本，它专门用于从
    Wikipedia 数据库 dump 中构建文本语料库。
- en: '***Your Cython Script***'
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***你的 Cython 脚本***'
- en: Now let’s write a Cython script that analyzes the text file. For simplicity,
    suppose all you want to do is count the number of personal pronouns in the submitted
    text. That means you need to count the number of tokens with the PRP part-of-speech
    tag assigned to them.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写一个分析文本文件的 Cython 脚本。为了简化，假设您只想计算提交文本中个人代词的数量。这意味着您需要计算那些被分配了 PRP 词性标签的
    token 数量。
- en: '**WARNING**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**'
- en: '*As stated in the documentation, C-level methods intended for use from Cython
    are designed for speed over safety. Mistakes in the code might cause the execution
    to crash abruptly.*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*如文档中所述，面向 Cython 的 C 层方法优先考虑速度而非安全性。代码中的错误可能导致执行突然崩溃。*'
- en: 'In a directory in your local filesystem, create a file called *spacytext.pyx*
    and insert the following code into it:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在您本地文件系统中的一个目录中，创建一个名为 *spacytext.pyx* 的文件，并插入以下代码：
- en: from cymem.cymem cimport Pool
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从 cymem.cymem cimport Pool
- en: from spacy.tokens.doc cimport Doc
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从 spacy.tokens.doc cimport Doc
- en: from spacy.structs cimport TokenC
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从 spacy.structs cimport TokenC
- en: from spacy.typedefs cimport hash_t
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从 spacy.typedefs cimport hash_t
- en: '➊ cdef struct DocStruct:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ cdef struct DocStruct：
- en: TokenC* c
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: TokenC* c
- en: int length
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: int length
- en: '➋ cdef int counter(DocStruct* doc, hash_t tag):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ cdef int counter(DocStruct* doc, hash_t tag)：
- en: cdef int cnt = 0
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: cdef int cnt = 0
- en: 'for c in doc.c[:doc.length]:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 c 在 doc.c[:doc.length] 中：
- en: 'if c.tag == tag:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 c.tag == tag：
- en: cnt += 1
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: cnt += 1
- en: return cnt
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 cnt
- en: '➌ cpdef main(Doc mydoc):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ cpdef main(Doc mydoc)：
- en: cdef int cnt
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: cdef int cnt
- en: cdef Pool mem = Pool()
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: cdef Pool mem = Pool()
- en: cdef DocStruct* doc_ptr = <DocStruct*>mem.alloc(1, sizeof(DocStruct))
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: cdef DocStruct* doc_ptr = <DocStruct*>mem.alloc(1, sizeof(DocStruct))
- en: doc_ptr.c = mydoc.c
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: doc_ptr.c = mydoc.c
- en: doc_ptr.length = mydoc.length
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: doc_ptr.length = mydoc.length
- en: tag = mydoc.vocab.strings.add('PRP')
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: tag = mydoc.vocab.strings.add('PRP')
- en: cnt = counter(doc_ptr, tag)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: cnt = counter(doc_ptr, tag)
- en: print(doc_ptr.length)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 打印(doc_ptr.length)
- en: print(cnt)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 打印(cnt)
- en: We start with a set of cimport statements to import necessary Cython modules,
    mostly from the spaCy library.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一组 cimport 语句开始，以导入所需的 Cython 模块，主要来自 spaCy 库。
- en: Then we define the Cython struct DocStruct as the container for the text being
    processed and the TokenC* variable ➊, a pointer to a TokenC struct used in spaCy
    as the data container for the Token object.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义 Cython 结构体 DocStruct，作为处理文本的容器，以及 TokenC* 变量 ➊，它是指向 TokenC 结构体的指针，spaCy
    使用它作为 Token 对象的数据容器。
- en: Next, we define a Cython function counter ➋ that counts the number of personal
    pronouns in the text.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个 Cython 函数 counter ➋，用于计算文本中个人代词的数量。
- en: '**NOTE**'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*The* cdef *functions won’t be available in the Python code that imports the
    module. If you want to create a function that will be visible to Python and to
    take advantage of C-level data structures and interfaces at the same time, you
    need to declare that function as* cpdef.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cdef* 函数 *在导入该模块的 Python 代码中不可用。如果您想创建一个同时对 Python 可见并能利用 C 层数据结构和接口的函数，您需要将该函数声明为*
    cpdef。'
- en: Finally, we define a cpdef Cython/Python main function ➌ that we can use in
    Python.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义一个 cpdef Cython/Python 主函数 ➌，可以在 Python 中使用。
- en: '***Building a Cython Module***'
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***构建 Cython 模块***'
- en: 'Unlike Python, you must compile Cython code. You can do this in several ways,
    the best of which is to write a distutils/setuptools *setup.py* Python script.
    Create a *setup.py* file in the same directory as your Cython script. Your *setup.py*
    file should include the following code:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Python 不同，您必须编译 Cython 代码。您可以通过几种方式进行编译，最好的方法是编写一个 distutils/setuptools *setup.py*
    Python 脚本。创建一个与 Cython 脚本位于同一目录下的 *setup.py* 文件。您的 *setup.py* 文件应包含以下代码：
- en: from distutils.core import setup
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 从 distutils.core 导入 setup
- en: from Cython.Build import cythonize
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Cython.Build 导入 cythonize
- en: ➊ import numpy
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ import numpy
- en: setup(name='spacy text app',
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: setup(name='spacy text app'，
- en: ➋ ext_modules=cythonize("spacytext.pyx", language="c++"),
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ ext_modules=cythonize("spacytext.pyx", language="c++")，
- en: ➌ include_dirs=[numpy.get_include()]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ include_dirs=[numpy.get_include()]
- en: )
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: This is a regular distutils/setuptools *setup.py* script except for two additions
    related to the example we’re working with. First, we import numpy ➊ and then explicitly
    specify where to find the *.h* files of the library ➌. We do this to avoid the
    *numpy/arrayobject.h* compilation error that occurs in some systems. We use the
    other setup option, language = "c++" ➋ to instruct the setup process to employ
    a C++ compiler rather than performing C compilation, which is the default.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常规的 distutils/setuptools *setup.py* 脚本，除了与我们正在处理的示例相关的两个补充项。首先，我们导入 numpy
    ➊，然后显式指定在哪里找到库的 *.h* 文件 ➌。这样做是为了避免在某些系统中出现 *numpy/arrayobject.h* 编译错误。我们使用另一个
    setup 选项，language = "c++" ➋，指示 setup 过程使用 C++ 编译器，而不是执行默认的 C 编译。
- en: 'Now that we have the setup script, you can build your Cython code. You can
    do this from within a system terminal, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了设置脚本，你可以开始构建你的 Cython 代码。你可以通过系统终端来完成此操作，方法如下：
- en: python setup.py build_ext --inplace
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: python setup.py build_ext --inplace
- en: 'A bunch of messages will display during the compilation process. Some of them
    might be warnings, but they’re rarely critical. For example, you might see this
    message, which is not critical for the process:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译过程中，会显示一堆信息。其中一些可能是警告，但它们通常不是关键性的。例如，你可能会看到这样的信息，但这对过程并不关键：
- en: '#warning "Using deprecated NumPy API ...'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#warning "使用已弃用的 NumPy API ..."'
- en: '***Testing the Module***'
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***测试模块***'
- en: 'After the compilation completes successfully, the spacytext module will be
    added to your Python environment. To test the newly created module, open a Python
    session and run the following command:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 编译成功后，spacytext 模块将被添加到你的 Python 环境中。要测试新创建的模块，打开一个 Python 会话并运行以下命令：
- en: '>>> from spacytext import main'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from spacytext import main'
- en: 'If it displays no errors, you can enter the following commands (this assumes
    your text data is in a *test.txt* file):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有显示错误，你可以输入以下命令（假设你的文本数据位于 *test.txt* 文件中）：
- en: '>>> import spacy'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import spacy'
- en: '>>> nlp = spacy.load(''en'')'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> nlp = spacy.load(''en'')'
- en: ➊ >>> f= open("test.txt","rb")
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ >>> f= open("test.txt","rb")
- en: '>>> contents =f.read()'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> contents =f.read()'
- en: ➋ >>> doc = nlp(contents[:100000].decode('utf8'))
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ >>> doc = nlp(contents[:100000].decode('utf8'))
- en: ➌ >>> main(doc)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ >>> main(doc)
- en: '21498'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '21498'
- en: '216'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '216'
- en: You open the file in which you have text data for this example in binary mode
    to obtain a bytes object ➊. If the file is too big, you can pick up only part
    of its content when creating a Doc object ➋. Once you’ve created the Doc object,
    you can test the spacytext module you just created with Cython, invoking its main()
    function ➌.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 你以二进制模式打开包含文本数据的文件，以获取一个字节对象 ➊。如果文件过大，你可以在创建 Doc 对象 ➋ 时只提取部分内容。一旦创建了 Doc 对象，你可以测试刚刚用
    Cython 创建的 spacytext 模块，调用其 main() 函数 ➌。
- en: The first figure in the output generated by the spacytext.main() function shows
    the total number of tokens found in the submitted text. The second figure is the
    number of personal pronouns found in this same text.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: spacytext.main() 函数生成的输出中的第一个数字表示提交文本中找到的总标记数。第二个数字表示在同一文本中找到的个人代词的数量。
- en: '**Summary**'
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you looked at the most important of spaCy’s container objects.
    You also learned how to customize your text-processing pipeline and use spaCy’s
    C-level data structures and interfaces from Cython.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了 spaCy 中最重要的容器对象。你还学习了如何自定义文本处理管道，并使用 spaCy 的 C 级数据结构和接口进行 Cython 开发。
