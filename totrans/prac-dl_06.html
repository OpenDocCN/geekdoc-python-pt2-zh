<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch06"><span epub:type="pagebreak" id="page_107"/><strong><span class="big">6</span><br/>CLASSICAL MACHINE LEARNING</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">It’s satisfying to be able to write “Classical Machine Learning” as it implies that there is something newer that makes older techniques “classical.” Of course, we know by now that there is—deep learning—and we’ll get to it in the chapters that follow. But first, we need to build our intuition by examining older techniques that will help cement concepts for us and, frankly, because the older techniques are still useful when the situation warrants.</p>&#13;
<p class="indent">It’s tempting to include some sort of history here. To keep to the practical nature of this book, we won’t, but a full history of machine learning is needed, and as of this writing, I have not found one. Historians reading this, please take note. I will say that machine learning is not new; the techniques of this chapter go back decades and have had considerable success on their own.</p>&#13;
<p class="indent">However, the successes were always limited in a way that deep learning has now largely overcome. Still, owning a hammer doesn’t make everything a nail. You will encounter problems that are well suited to these older techniques. This might be because there’s too little data available to train a deep <span epub:type="pagebreak" id="page_108"/>model, because the problem is simple and easily solved by a classical technique, or because the operating environment is not conducive to a large, deep model (think microcontroller). Besides, many of these techniques are easier to understand, conceptually, than a deep model is, and all the comments of earlier chapters about building datasets, as well as the comments in <a href="ch11.xhtml#ch11">Chapter 11</a> about evaluating models, still apply.</p>&#13;
<p class="indent">The following sections will introduce several popular classical models, not in great detail, but in essence. All of these models are supported by sklearn. In <a href="ch07.xhtml#ch07">Chapter 7</a>, we’ll apply the models to some of the datasets we developed in <a href="ch05.xhtml#ch05">Chapter 5</a>. This will give us an idea of the relative performance of the models when compared to each other as well as giving us a baseline for comparing the performance of deep models in subsequent chapters.</p>&#13;
<p class="indent">We’ll examine six classical models. The order in which we discuss them roughly tracks with the complexity of the type of model. The first three, Nearest Centroid, <em>k</em>-Nearest Neighbors, and Naïve Bayes, are quite simple to understand and implement. The last three, Decision Trees, Random Forests, and Support Vector Machines, are harder, but we’ll do our best to explain what’s going on.</p>&#13;
<h3 class="h3" id="lev1_38">Nearest Centroid</h3>&#13;
<p class="noindent">Assume we want to build a classifier and that we have a properly designed dataset of <em>n</em> classes (see <a href="ch04.xhtml#ch04">Chapter 4</a>). For simplicity, we’ll assume that we have <em>m</em> samples of each of the <em>n</em> classes. This isn’t necessary but saves us from adding many subscripts to things. Since our dataset is properly designed, we have training samples and test samples. We don’t need validation samples in this case, so we can throw them into the training set. Our goal is to have a model that uses the training set to learn so we can apply the model to the test set to see how it will do with new, unknown samples. Here the sample is a feature vector of floating-point values.</p>&#13;
<p class="indent">The goal of selecting components for a feature vector is to end up with a feature vector that makes the different classes distinct in the feature space. Let’s say that the feature vector has <em>w</em> features. This means we can think of the feature vector as the coordinates of a point in a <em>w</em>-dimensional space. If <em>w</em> = 2 or <em>w</em> = 3, we can graph the feature vectors. However, mathematically, there’s no reason for us to restrict <em>w</em> to 2 or 3; all of what we describe here works in 100, 500, or 1000 dimensions. Note it won’t work equally well: the dreaded curse of dimensionality will creep in and eventually require an exponentially large training dataset, but we’ll ignore this elephant in the room for now.</p>&#13;
<p class="indent">If the features are well chosen, we might expect a plot of the points in the <em>w</em>-dimensional space to group the classes so that all of the samples from class 0 are near each other, and all of the samples from class 1 are near each other but distinct from class 0, and so forth. If this is our expectation, then <span epub:type="pagebreak" id="page_109"/>how might we use this knowledge to assign a new, unknown sample to a particular class? Of course, this is the goal of classification, but in this case, given our assumption that the classes are well separated in the feature space, what is something simple we could do?</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch6fig1">Figure 6-1</a> shows a hypothetical 2D feature space with four distinct classes. The different classes are clearly separated in this toy example. A new, unknown feature vector will fall into this space as a point. The goal is to assign a class label to the new point, either square, star, circle, or triangle.</p>&#13;
<div class="image" id="ch6fig1"><img src="Images/06fig01.jpg" alt="image" width="670" height="501"/></div>&#13;
<p class="figcap"><em>Figure 6-1: A hypothetical 2D feature space with four distinct classes</em></p>&#13;
<p class="indent">Since the points of <a href="ch06.xhtml#ch6fig1">Figure 6-1</a> are so well grouped, we might think that we could represent each group by an average position in the feature space. Instead of the 10 square points, we’d use a single point to represent the squares. This seems an entirely reasonable thing to do.</p>&#13;
<p class="indent">It turns out, the average point of a group of points has a name: the <em>centroid</em>, the center point. We know how to compute the average of a set of numbers: add them up and divide by how many we added. To find the centroid of a set of points in 2D space, we first find the average of all the x-axis coordinates and then the average of all the y-axis coordinates. If we have three dimensions, we’ll do this for the x-, y-, and z-axes. If we have <em>w</em> dimensions, we’ll do it for each of the dimensions. In the end, we’ll have a single point that we can use to represent the entire group. If we do this for our toy example, we get <a href="ch06.xhtml#ch6fig2">Figure 6-2</a>, where the centroid is shown as the large marker.</p>&#13;
<div class="image" id="ch6fig2"><span epub:type="pagebreak" id="page_110"/><img src="Images/06fig02.jpg" alt="image" width="670" height="501"/></div>&#13;
<p class="figcap"><em>Figure 6-2: A hypothetical 2D feature space with four distinct classes and their centroids</em></p>&#13;
<p class="indent">How is the centroid helpful to us? Well, if a new, unknown sample is given to us, it will be a point in the feature space as mentioned previously. We can then measure the distance between this point and each of the centroids and assign the class label of the closest centroid. The idea of <em>distance</em> is somewhat ambiguous; there are many different ways to define distance. One obvious way is to draw straight line between the two points; this distance is known as the <em>Euclidean distance</em>, and it’s easy enough to compute. If we have two points, (<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>) and (<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>) then the Euclidean distance between them is simply the following:</p>&#13;
<div class="imagec"><img src="Images/110equ01.jpg" alt="image" width="234" height="38"/></div>&#13;
<p class="noindent">If we have three dimensions, the distance between two points becomes</p>&#13;
<div class="imagec"><img src="Images/110equ02.jpg" alt="image" width="333" height="38"/></div>&#13;
<p class="noindent">which can be generalized to <em>w</em> dimensions for two points, <em>x</em><sub>0</sub> and <em>x</em><sub>1</sub>, as</p>&#13;
<div class="imagec"><img src="Images/110equ03.jpg" alt="image" width="166" height="75"/></div>&#13;
<p class="noindent">where <span class="middle"><img src="Images/110equ04.jpg" alt="Image" width="18" height="27"/></span> is the <em>i</em>-th component of the point <em>x</em><sub>0</sub>. This means, component by component, find the difference between the two points, square it, and add <span epub:type="pagebreak" id="page_111"/>it to the squared difference of all the other components. Then, take the square root.</p>&#13;
<p class="indent"><a href="ch06.xhtml#ch6fig3">Figure 6-3</a> shows a sample point in the feature space as well as the distances to the centroids. The shortest distance is to the circle class, so we’d assign the new sample to that class.</p>&#13;
<div class="image" id="ch6fig3"><img src="Images/06fig03.jpg" alt="image" width="675" height="505"/></div>&#13;
<p class="figcap"><em>Figure 6-3: A hypothetical 2D feature space with four distinct classes, their centroids, and a new, unknown sample</em></p>&#13;
<p class="indent">The process we just implemented is known as a <em>Nearest Centroid</em> classifier. It’s also sometimes called <em>template matching</em>. The centroids of the classes learned from the training data are used as a proxy for the class as a whole. Then, new samples use those centroids to decide on a label.</p>&#13;
<p class="indent">This seems so simple and perhaps even somewhat obvious, so why isn’t this classifier used more? Well, there are several reasons. One has already been mentioned, the curse of dimensionality. As the number of features increases, the space gets larger and larger, and we need exponentially more training data to get a good idea of where the centroids should be. So, a large feature space implies that this might not be the right approach.</p>&#13;
<p class="indent">There’s a more severe problem, however. Our toy example had very tight groups. What if the groups are more diffuse, even overlapping? Then the selection of the Nearest Centroid becomes problematic: how would we know whether the closest centroid represents class A or class B?</p>&#13;
<p class="indent">Still more severe is that a particular class might fall into <em>two</em> or more distinct groups. If we calculate the centroid of only the class as a whole, the centroid will be between the groups for the class and not represent either cluster well. We’d need to know that the class is split between groups and <span epub:type="pagebreak" id="page_112"/>use multiple centroids for the class. If the feature space is small, we can plot it and see that the class is divided between groups. However, if the feature space is larger, there’s no easy way for us to decide that the class is divided between multiple groups and that multiple centroids are required. Still, for elementary problems, this approach might be ideal. Not every application deals with difficult data. We might be building an automated system that needs to make simple, easy decisions on new inputs. In that case, this simple classifier might be a perfect fit.</p>&#13;
<h3 class="h3" id="lev1_39">k-Nearest Neighbors</h3>&#13;
<p class="noindent">As we saw earlier, one problem with a centroid approach is that the classes might be divided among multiple groups in the feature space. As the number of groups increases, so would the number of centroids necessary to specify the class. This implies another approach. Instead of computing per class centroids, what if we used the training data as is and selected the class label for a new input sample by finding the closest member of the training set and using its label?</p>&#13;
<p class="indent">This type of classifier is called a <em>Nearest Neighbor</em> classifier. If we look at only the closest sample in the training set, we are using one neighbor, so we call the classifier a <em>1-Nearest Neighbor</em> or <em>1-NN classifier</em>. But we don’t need to look at only the nearest training point. We might want to look at several and then vote to assign a new sample the most common class label. In the event of a tie, we can select one of the class labels at random. If we use three nearest neighbors, we have a 3-NN classifier, and if we use <em>k</em> neighbors, we have a <em>k</em>-NN classifier.</p>&#13;
<p class="indent">Let’s revisit the hypothetical dataset of <a href="ch06.xhtml#ch6fig1">Figure 6-1</a> but generate a new version where the tight clusters are more spread out. We still have two features and four classes with 10 examples each. Let’s set <em>k</em> = 3, a typical value. To assign a label to a new sample, we plot the sample in the feature space and then find the three closest training data points to it. <a href="ch06.xhtml#ch6fig4">Figure 6-4</a> shows the three nearest neighbors for three unknown samples.</p>&#13;
<p class="indent">The three training data points closest to Sample A are square, square, and star. Therefore, by majority vote, we assign Sample A to the class square. Similarly, the three closest training data points for Sample B are circle, triangle, and triangle. Therefore, we declare Sample B to be of class triangle. Things are more interesting with Sample C. In this case, the three closest training samples are each from a different class: circle, star, and triangle. So, voting is a tie.</p>&#13;
<p class="indent">When this happens, the <em>k</em>-NN implementation has to make a choice. The simplest thing to do is select the class label at random since one might argue that any of the three are equally as likely. Alternatively, one might believe a little more strongly in the value of the distance between the unknown sample and the training data and select the one with the shortest distance. In this case, we’d label Sample C with class star, since that’s the training sample closest to it.</p>&#13;
<div class="image" id="ch6fig4"><span epub:type="pagebreak" id="page_113"/><img src="Images/06fig04.jpg" alt="image" width="654" height="491"/></div>&#13;
<p class="figcap"><em>Figure 6-4: Applying <em>k</em>-NN for <em>k</em> =3 to three unknown samples A, B, and C</em></p>&#13;
<p class="indent">The beauty of a <em>k</em>-NN classifier is that the training data <em>is</em> the model—no training step is necessary. Of course, the training data must be carried around with the model and, depending upon the size of the training set, finding the <em>k</em> nearest neighbors for a new input sample might be computationally very expensive. People have worked for decades to try to speed up the neighbor search or store the training data more efficiently, but in the end, the curse of dimensionality is still there and still an issue.</p>&#13;
<p class="indent">However, some <em>k</em>-NN classifiers have performed very well: if the dimensionality of the feature space is small enough, <em>k</em>-NN might be attractive. There needs to be a balance between training data size, which leads to better performance but more storage and more laborious searching for neighbors, and the dimensionality of the feature space. The same sort of scenario that might make Nearest Centroid a good fit will also make <em>k</em>-NN a good fit. However, <em>k</em>-NN is perhaps more robust to diffuse and somewhat overlapping class groups than Nearest Centroid is. If the samples for a class are split between several groups, <em>k</em>-NN will be superior to Nearest Centroid.</p>&#13;
<h3 class="h3" id="lev1_40">Naïve Bayes</h3>&#13;
<p class="noindent">Widely used in natural language processing research, the <em>Naïve Bayes</em> classifier is simple to implement and straightforward to understand, though we’ll have to include some math to do it. However, I promise, the description of what’s happening will make the math understandable even if the notation isn’t so familiar.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_114"/>The technique uses Bayes’ theorem (see Thomas Bayes’ “An Essay Towards Solving a Problem in the Doctrine of Chances” published in 1763). The theorem relates probabilities, and its modern formulation is</p>&#13;
<div class="imagec"><img src="Images/114equ01.jpg" alt="image" width="193" height="49"/></div>&#13;
<p class="noindent">which uses some mathematical notation from probability theory that we need to describe to understand how we’ll use this theorem to implement a classifier.</p>&#13;
<p class="indent">The expression <em>P</em>(<em>A|B</em>) represents the probability that event A has occurred, given event B has already occurred. In this context, it’s called the <em>posterior probability</em>. Similarly, <em>P</em>(<em>B|A</em>) represents the probability that event B has occurred, given event A has occurred. We call <em>P</em>(<em>B|A</em>) the <em>likelihood</em> of B, given A. Finally, <em>P</em>(<em>A</em>) and <em>P</em>(<em>B</em>) represent, respectively, the probability that event A has occurred, regardless of event B, and the probability that event B has occurred, regardless of event A. We call <em>P</em>(<em>A</em>) the <em>prior probability</em> of A. <em>P</em>(<em>B</em>) is the probability of <em>B</em> happening regardless of <em>A</em>.</p>&#13;
<p class="indent">Bayes’ theorem gives us the probability of something happening (event A) given that we already know something else has happened (event B). So how does this help us classify? We want to know whether a feature vector belongs to a given class. We know the feature vector, but we don’t know the class. So if we have a dataset of <em>m</em> feature vectors, where each feature vector has <em>n</em> features, <em>x</em> = <em>{x</em><sub>1</sub>,<em>x</em><sub>2</sub>,<em>x</em><sub>3</sub>,…,<em>x</em><sub><em>n</em></sub><em>}</em>, then we can replace the <em>B</em> in Bayes’ theorem with each of the features in the feature vector. We can also replace <em>A</em> with <em>y</em>, the class label we want to assign to a new, unknown feature vector <em>x</em>. The theorem now looks like this:</p>&#13;
<div class="imagec"><img src="Images/114equ02.jpg" alt="image" width="429" height="50"/></div>&#13;
<p class="noindent">Let’s explain things a bit. Bayes’ theorem states that if we know the likelihood of having <em>x</em> be our feature vector given that <em>y</em> is the class, and we know how often class <em>y</em> shows up (this is <em>P</em>(<em>y</em>), the prior probability of <em>y</em>), then we can calculate the probability that the class of the feature vector <em>x</em> is <em>y</em>. If we are able to do this for all the possible classes, all the different <em>y</em> values, we can select the highest probability and label the input feature vector <em>x</em> as belonging to that class, <em>y</em>.</p>&#13;
<p class="indent">Recall that a training dataset is a set of pairs, (<em>x</em><sup><em>i</em></sup>,<em>y</em><sup><em>i</em></sup>), for a known feature vector, <em>x</em><sup><em>i</em></sup>, and a known class it belongs to, <em>y</em><sup><em>i</em></sup>. Here the <em>i</em> superscript is counting the feature vector and label pairs in the training dataset. Now, given a dataset like this, we can calculate <em>P</em>(<em>y</em>) by making a histogram of how often each class label shows up in the training set. We believe that the training set fairly represents the parent distribution of possible feature vectors so that we can use the training data to calculate the values we need to make use of Bayes’ theorem. (See <a href="ch04.xhtml#ch04">Chapter 4</a> for techniques to ensure that the dataset is a good one.)</p>&#13;
<p class="indent">Once we have <em>P</em>(<em>y</em>), we need to know the likelihood, <em>P</em>(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,<em>x</em><sub>3</sub>,…,<em>x</em><sub><em>n</em></sub><em>|y</em>). Unfortunately, we can’t calculate this directly. But all is not lost: we’ll make <span epub:type="pagebreak" id="page_115"/>an assumption that will let us move ahead. We’ll assume that each of the features in <em>x</em> is <em>statistically independent</em>. This means that the fact that we measure a particular <em>x</em><sub>1</sub> has nothing whatsoever to do with the values of any of the other <em>n –</em> 1 features. This isn’t true always, or even most of the time, but in practice, it turns out that this assumption is often close enough to true that we can get by. This is why it’s called <em>Naïve Bayes</em>, as it’s naïve to assume the features are independent of each other. That assumption is most definitely not true, for example, when our input is an image. The pixels of an image are highly dependent upon each other. Pick one at random, and the pixels next to it are almost certainly within a few values of it.</p>&#13;
<p class="indent">When two events are independent, their <em>joint probability</em>, the probability that they both happen, is simply the product of their individual probabilities. The independence assumption lets us change the likelihood portion of Bayes’ theorem like so:</p>&#13;
<div class="imagec"><img src="Images/115equ01.jpg" alt="image" width="303" height="61"/></div>&#13;
<p class="noindent">The <span class="ent">∏</span> symbol means <em>multiplied together</em>, much like the <span class="ent">∑</span> symbol means <em>added together</em>. The right side of the equation is saying that if we know the probability of measuring a particular value of a feature, say feature <em>x</em><sub><em>i</em></sub>, given that the class label is <em>y</em>, we can get the likelihood of the entire feature vector <em>x</em>, given class label <em>y</em>, by multiplying each of the per feature probabilities together.</p>&#13;
<p class="indent">If our dataset consists of categorical values, or discrete values like integers (for example, age), then we can use the dataset to calculate the <em>P</em>(<em>x</em><sub><em>i</em></sub><em>|y</em>) values by building a histogram for each feature for each class. For example, if feature <em>x</em><sub>2</sub> for class 1 has the following values</p>&#13;
<pre>7, 4, 3, 1, 6, 5, 2, 8, 5, 4, 4, 2, 7, 1, 3, 1, 1, 3, 3, 3, 0, 3,<br/>&#13;
4, 4, 2, 3, 4, 5, 2, 4, 2, 3, 2, 4, 4, 1, 3, 3, 3, 2, 2, 4, 6, 5,<br/>&#13;
2, 6, 5, 2, 6, 6, 3, 5, 2, 4, 2, 4, 5, 4, 5, 5, 2, 5, 3, 4, 3, 1,<br/>&#13;
6, 6, 5, 3, 4, 3, 3, 4, 1, 1, 3, 5, 4, 4, 7, 0, 6, 2, 4, 7, 4, 3,<br/>&#13;
4, 3, 5, 4, 6, 2, 5, 4, 4, 5, 6, 5</pre>&#13;
<p class="noindent">then each value occurs with the following probability</p>&#13;
<pre>0: 0.02<br/>&#13;
1: 0.08<br/>&#13;
2: 0.15<br/>&#13;
3: 0.20<br/>&#13;
4: 0.24<br/>&#13;
5: 0.16<br/>&#13;
6: 0.10<br/>&#13;
7: 0.04<br/>&#13;
8: 0.01</pre>&#13;
<p class="noindent">which comes from the number of times each value occurs divided by 100, the total number of values in the dataset.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_116"/>This histogram is exactly what we need to find <em>P</em>(<em>x</em><sub>2</sub><em>|y</em> = 1), the probability for feature 2 when the class label is 1. For example, we can expect a new feature vector of class 1 to have <em>x</em><sub>2</sub> = 4 about 24 percent of the time and to have <em>x</em><sub>2</sub> = 1 about 8 percent of the time.</p>&#13;
<p class="indent">By building tables like this for each feature and each class label, we can complete our classifier for the categorical and discrete cases. For a new feature vector, we use the tables to find the probability that each feature would have that value. We multiply each of those probabilities together and then multiply by the prior probability of that class. This, repeated for each of the <em>m</em> classes in the dataset, will give us a set of <em>m</em> posterior probabilities. To classify the new feature vector, select the largest of these <em>m</em> values, and assign the corresponding class label.</p>&#13;
<p class="indent">How do we calculate <em>P</em>(<em>x</em><sub><em>i</em></sub><em>|y</em>) if the feature values are continuous? One way would be to bin the continuous values and then make tables as in the discrete case. Another is to make one more assumption. We need to make an assumption about the distribution of possible <em>x</em><sub><em>i</em></sub> feature values that we could measure. Most natural phenomena seem to follow a normal distribution. We discussed the normal distribution in <a href="ch01.xhtml#ch01">Chapter 1</a>. Let’s assume, then, that the features all follow normal distributions. A normal distribution is defined by its mean value (<em>μ</em>, mu) and a standard deviation (<em>σ</em>, sigma). The mean value is just the average value we’d expect if we drew samples from the distribution repeatedly. The standard deviation is a measure of how wide the distribution is—how spread out it is around the mean value.</p>&#13;
<p class="indent">Mathematically, what we want to do is replace each <em>P</em>(<em>x</em><sub><em>i</em></sub><em>|y</em>) like so</p>&#13;
<p class="center"><em>P</em>(<em>x</em><sub><em>i</em></sub>|<em>y</em>) ≈ <em>N</em>(<em>μ</em><sub><em>i</em></sub>, <em>σ</em><sub><em>i</em></sub>)</p>&#13;
<p class="noindent">for each feature in our feature vector. Here N(<em>μ</em><sub><em>i</em></sub>,<em>σ</em><sub><em>i</em></sub>) is notation meaning a normal distribution centered around some mean value (<em>μ</em>) and defined by a spread (<em>σ</em>).</p>&#13;
<p class="indent">We don’t really know the exact <em>μ</em> and <em>σ</em> values, but we can approximate them from the training data. For example, assume the training data consists of 25 samples, where the class label is 0. Further, assume that the following are the values of feature 3, that is, <em>x</em><sub>3</sub>, in those cases:</p>&#13;
<p class="program1">0.21457111,  4.3311102,   5.50481251,  0.80293956,  2.5051598,<br/>&#13;
2.37655204,  2.4296739,   2.84224169, -0.11890662,  3.18819152,<br/>&#13;
1.6843311,   4.05982237,  4.14488722,  4.29148855,  3.22658406,<br/>&#13;
6.45507675,  0.40046778,  1.81796124,  0.2732696,   2.91498336,<br/>&#13;
1.42561983,  2.73483704,  1.68382843,  3.80387653,  1.53431146</p>&#13;
<p class="indent">Then we’d use <em>μ</em><sub>3</sub> = 2.58 and <em>σ</em><sub>3</sub> = 1.64 when setting up the normal distribution for feature 3 for class 0 since the average of these values is 2.58, and the standard deviation, the spread around the mean value, is 1.64.</p>&#13;
<p class="indent">When a new unknown sample is given to the classifier, we would compute the probability of the given <em>x</em><sub>3</sub> happening if the actual class was class 0 by using the following equation.</p>&#13;
<div class="imagec"><span epub:type="pagebreak" id="page_117"/><img src="Images/117equ01.jpg" alt="image" width="287" height="65"/></div>&#13;
<p class="indent">This equation comes from the definition of a normal distribution with mean <em>μ</em> and standard deviation <em>σ</em>. It says that the likelihood of a particular feature value, given the class is <em>y</em>, is distributed around the mean value we measured from the training data according to the normal distribution. This is an assumption we are making on top of the independence assumption between features.</p>&#13;
<p class="indent">We use this equation for each of the features in the unknown feature vector. We’d then multiply the resulting probabilities together, and multiply that value by the prior probability of class 0 happening. We’d repeat this process for each of the classes. In the end, we’ll have <em>m</em> numbers, the probabilities of the feature vector belonging to each of the <em>m</em> classes. To make a final decision, we’d do what we did before: select the largest of these probabilities and label the input as being of the corresponding class.</p>&#13;
<p class="indent">Some readers may complain that we ignored the denominator of Bayes’ theorem. We did that because it’s a constant across all the calculations, and since we always select the largest posterior probability, we really don’t care whether we divide each value by a constant. We’ll select the same class label, regardless.</p>&#13;
<p class="indent">Also, for the discrete case, it’s possible that our training set does not have any instances of a value that rarely shows up. We ignored that, too, but it is a problem since if the value never shows up, the <em>P</em>(<em>x</em><sub><em>i</em></sub><em>|y</em>) we use would be 0, making the entire posterior probability 0. This often happens in natural language processing, where a particular word is rarely used. A technique called <em>Laplace smoothing</em> gets around this, but for our purposes, we claim that a “good” training set will represent <em>all</em> possible values for the features and simply press ahead. The sklearn <code>MultinomialNB</code> Naïve Bayes classifier for discrete data uses Laplace smoothing by default.</p>&#13;
<h3 class="h3" id="lev1_41">Decision Trees and Random Forests</h3>&#13;
<p class="noindent">The left side of <a href="ch06.xhtml#ch6fig5">Figure 6-5</a> shows an x-ray image of a puppy with a malformed right hip socket. Since the puppy is on its back in the x-ray, the right hip socket is on the left side of the image. The right side of <a href="ch06.xhtml#ch6fig5">Figure 6-5</a> shows the corresponding histogram of the pixel intensities (8-bit values, [0,255]). There are two modes to this histogram, corresponding to the dark background and the lighter-intensity x-ray data. If we want to classify each pixel of the image into either background or x-ray, we can do so with the following rule: “If the pixel intensity is less than 11, call the pixel background.”</p>&#13;
<div class="image" id="ch6fig5"><span epub:type="pagebreak" id="page_118"/><img src="Images/06fig05.jpg" alt="image" width="668" height="340"/></div>&#13;
<p class="figcap"><em>Figure 6-5: An x-ray image of a puppy (left). The corresponding histogram of 8-bit pixel values [0,255] (right).</em></p>&#13;
<p class="indent">This rule implements a decision made about the data based on one of the features, in this case, the pixel intensity value. Simple decisions like this are at the heart of <em>Decision Trees</em>, the classification algorithm we’ll explore in this section. For completeness, if we apply the decision rule to each pixel in the image and output 0 or 255 (maximum pixel value) for background versus x-ray data, we get a mask showing us which pixels are part of the image. See <a href="ch06.xhtml#ch6fig6">Figure 6-6</a>.</p>&#13;
<div class="image" id="ch6fig6"><img src="Images/06fig06.jpg" alt="image" width="416" height="329"/></div>&#13;
<p class="figcap"><em>Figure 6-6: An x-ray image of a puppy (left). The corresponding pixel mask generated by the decision rule. White pixels are part of the x-ray image (right).</em></p>&#13;
<p class="indent">A Decision Tree is a set of nodes. The nodes either define a condition and branch based on the truth or falsehood of the condition, or select a particular class. Nodes that do not branch are called <em>leaf nodes</em>. Decision Trees are called <em>trees</em> because, especially for the binary case we’ll consider here, <span epub:type="pagebreak" id="page_119"/>they branch like trees. <a href="ch06.xhtml#ch6fig7">Figure 6-7</a> shows a Decision Tree learned by the sklearn <code>DecisionTreeClassifier</code> class for the full iris dataset using the first three features. See <a href="ch05.xhtml#ch05">Chapter 5</a>.</p>&#13;
<div class="image" id="ch6fig7"><img src="Images/06fig07.jpg" alt="image" width="798" height="992"/></div>&#13;
<p class="figcap"><em>Figure 6-7: A Decision Tree classifier for the iris dataset</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_120"/>By convention, the first node in the tree, the <em>root</em>, is drawn at the top. For this tree, the root node asks the question, “Is the petal length ≤ 2.45?” If it is, the left branch is taken, and the tree immediately reaches a leaf node and assigns a label of “virginica” (class 0). We’ll discuss the other information in the nodes shortly. If the petal length is not ≤ 2.45, the right branch is taken, leading to a new node that asks, “Is the petal length ≤ 4.75?” If so, we move to a node that asks a question about the sepal length. If not, we move to the right node and consider the petal length again. This process continues until a leaf is reached, which determines the class label.</p>&#13;
<p class="indent">The process just described is exactly how a Decision Tree is used after it’s created. For any new feature vector, the series of questions is asked starting with the root node, and the tree is traversed until a leaf node is reached to decide the class label. This is a human-friendly way to move through the classification process, which is why Decision Trees are handy in situations where the “why” of the class assignment is as important to know as the class assignment itself. A Decision Tree can explain itself.</p>&#13;
<p class="indent">Using a Decision Tree is simple enough, but how is the tree created in the first place? Unlike the simple algorithms in the previous sections, the tree-building process is more involved, but not so involved that we can’t follow through the main steps to build some intuition as to what goes into defining the tree.</p>&#13;
<h4 class="h4" id="lev2_46">Recursion Primer</h4>&#13;
<p class="noindent">Before we talk about the Decision Tree algorithm, however, we need to discuss the concept of <em>recursion</em>. If you’re familiar with computer science, you probably already know that tree-like data structures and recursion go hand in hand. If not, don’t worry; recursion is a straightforward but powerful concept. The essence of a recursive algorithm is that the algorithm repeats itself at different levels. When implemented as a function in a programming language, this generally means that the function calls itself on a smaller version of the problem. Naturally, if the function calls itself indefinitely, we’ll have an infinite loop, so the recursion needs a stopping condition—something that says we no longer need to recurse.</p>&#13;
<p class="indent">Let’s introduce the idea of recursion mathematically. The factorial of an integer, <em>n</em>, denoted <em>n</em>!, is defined to be</p>&#13;
<p class="center"><em>n</em>! = <em>n</em>(<em>n</em> – 1)(<em>n</em> – 2)(<em>n</em> – 3) . . . (<em>n</em> – <em>n</em> + 1)</p>&#13;
<p class="noindent">which just means multiply together all the integers from 1 to <em>n</em>. By definition, 0! = 1. Therefore, the factorial of 5 is 120 because</p>&#13;
<p class="center">5! = 5 × 4 × 3 × 2 × 1 = 120</p>&#13;
<p class="indent">If we look at 5! we see that it is nothing more than 5 × 4! or, in general, that, <em>n</em>! = <em>n</em> × (<em>n –</em> 1)!. Now, let’s write a Python function to calculate factorials recursively using this insight. The code is simple, also a hallmark of many recursive functions, as <a href="ch06.xhtml#ch6lis1">Listing 6-1</a> shows.</p>&#13;
<p class="programs" id="ch6lis1"><span epub:type="pagebreak" id="page_121"/>def fact(n):<br/>&#13;
    if (n &lt;= 1):<br/>&#13;
     return 1<br/>&#13;
   else:<br/>&#13;
     return n*fact(n-1)</p>&#13;
<p class="figcap"><em>Listing 6-1: Calculating the factorial</em></p>&#13;
<p class="indent">The code is a direct implementation of the rule that the factorial of <em>n</em> is <em>n</em> times the factorial of <em>n –</em> 1. To find the factorial of <code>n</code>, we first ask if <code>n</code> is 1. If it is, we know the factorial is 1 so we return 1—this is our stopping condition. If <code>n</code> is not 1, we know that the factorial of <code>n</code> is simply <code>n</code> times the factorial of <code>n-1</code>, which we find by calling <code>fact</code> with <code>n-1</code> as the argument.</p>&#13;
<h4 class="h4" id="lev2_47">Building Decision Trees</h4>&#13;
<p class="noindent">The algorithm to build a Decision Tree is also recursive. Let’s walk through what happens at a high level. The algorithm starts with the root node, determines the proper rule for that node, and then calls itself on the left and right branches. The call to the left branch will start again as if the left branch is the root node. This will continue until a stopping condition is met.</p>&#13;
<p class="indent">For a Decision Tree, the stopping condition is a leaf node (we’ll discuss how a Decision Tree knows whether to create a leaf node next). Once a leaf node is created, the recursion terminates, and the algorithm returns to that leaf’s parent node and calls itself on the right branch. The algorithm then starts again as if the right branch were the root node. Once both recursive calls terminate, and a node’s left and right subtrees are created, the algorithm returns to that node’s parent, and so on and so forth until the entire tree is constructed.</p>&#13;
<p class="indent">Now to get a little more specific. How is the training data used to build the tree? When the root node is defined, all the training data is present—say, all <em>n</em> samples. This is the set of samples used to pick the rule the root node implements. Once that rule has been selected and applied to the training samples, we have two new sets of samples: one for the left side (the true side) and one for the right side (the false side).</p>&#13;
<p class="indent">The recursion then works with these nodes, using their respective set of training samples, to define the rule for the left and right branches. Every time a branch node is created, the training set for that branch node gets split into samples that meet the rule and samples that don’t meet the rule. A leaf node is declared when the set of training samples is either too small, of a sufficiently high proportion of one class, or the maximum tree depth has been reached.</p>&#13;
<p class="indent">By now you’re probably wondering, “How do we select the rule for a branch node?” The rule relates a single input feature, like the petal length, to a particular value. The Decision Tree is a <em>greedy</em> algorithm; this means that at every node it selects the best rule for the current set of information available to it. In this case, this is the current set of training samples that are available to the node. The best rule is the one that best separates the <span epub:type="pagebreak" id="page_122"/>classes into two groups. This implies that we need a way to select possible candidate rules and that we have a way to determine that a candidate rule is “best.” The Decision Tree algorithm uses brute force to locate candidate rules. It runs through all possible combinations of features and values, making continuous values discrete by binning, and evaluates the purity of the left and right training sets after the rule is applied. The best-performing rule is the one kept at that node.</p>&#13;
<p class="indent">“Best performing" is determined by the <em>purity</em> of the split into left and right training sample subsets. One way to measure purity is to use the <em>Gini index</em>. This is the metric sklearn uses. The Gini index of each node in the iris example of <a href="ch06.xhtml#ch6fig7">Figure 6-7</a> is shown. It’s calculated as</p>&#13;
<div class="imagec"><img src="Images/122equ01.jpg" alt="image" width="312" height="52"/></div>&#13;
<p class="noindent">where <em>P</em>(<em>y</em><sub><em>i</em></sub>) is the fraction of training examples in the subset for the current node that are of class <em>i</em>. A perfect split between classes, all of one class and none of the other, will result in a Gini index of 0. A 50-50 split has a Gini index of 0.5. The algorithm seeks to minimize the Gini index at each node by selecting the candidate rule that results in the smallest Gini index.</p>&#13;
<p class="indent">For example, in <a href="ch06.xhtml#ch6fig7">Figure 6-7</a> the right-hand node below the root has a Gini index of 0.5. This means that the rule for <em>the node above</em>, the root, will result in a subset of the training data that has petal length &gt; 2.45, and that subset will be evenly divided between classes 1 and 2. This is the meaning of the “value” line in the node text. It shows the number of training samples in the subset that defined the node. The “class” line is the class that would be assigned if the tree were stopped at that node. It’s simply the class label of the class that has the largest number of training samples in the node’s subset. When the tree is used on new, unknown samples, it’s run from root to a leaf, always.</p>&#13;
<h4 class="h4" id="lev2_48">Random Forests</h4>&#13;
<p class="noindent">Decision Trees are useful when the data is discrete or categorical or has missing values. Continuous data needs to be binned first (sklearn does this for you). Decision Trees have a bad habit, however, of <em>overfitting</em> the training data. This means that they are likely to learn meaningless statistical nuances of the training data that you happened to use, instead of learning meaningful general features of the data that are useful when applied to unknown data samples. Decision Trees also grow very large, as the number of features grows, unless managed by depth parameters.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_123"/>Decision Tree overfitting can be mitigated by using <em>Random Forests</em>. In fact, unless your problem is simple, you probably want to look at using a Random Forest from the start. The following three concepts lead from a Decision Tree to a Random Forest:</p>&#13;
<ul>&#13;
<li class="noindent">Ensembles of classifiers and voting between them</li>&#13;
<li class="noindent">Resampling of the training set by selecting samples <em>with replacement</em></li>&#13;
<li class="noindent">Selection of random feature subsets</li>&#13;
</ul>&#13;
<p class="indent">If we have a set of classifiers, each trained on different data or of a different type, like a <em>k</em>-NN and a Naïve Bayes, we can use their outputs to vote on the actual category to assign to any particular unknown sample. This is called an <em>ensemble</em> and, with diminishing returns as the number of classifiers increases, it will, in general, improve the performance over that of any individual classifier. We can employ a similar idea and imagine an ensemble, or <em>forest</em>, of Decision Trees, but unless we do something more with the training set, we’ll have a forest of <em>exactly</em> the same tree because a particular set of training examples will always lead to the exact same Decision Tree. The algorithm to create a Decision Tree is deterministic—it always returns the same results.</p>&#13;
<p class="indent">A way to deal with the particular statistical nuances of the training set you have to work with is to select a new training set from the original training set but allow the same training set sample to be selected more than once. This is selection with replacement. Think of it as choosing colored marbles from a bag, but before you select the next marble, put the one you just selected back in the bag so you might pick it again. A new dataset selected in this way is known as a <em>bootstrap sample</em>. Building a collection of new datasets in this way is known as <em>bagging</em>, and it is models built from this collection of resampled datasets that build the Random Forest.</p>&#13;
<p class="indent">If we train multiple trees, each with a resampled training set with replacement, we’ll get a forest of trees, each one slightly different from the others. This alone, along with ensemble voting, will probably improve things. However, there’s one issue. If some of the features are highly predictive, they will dominate, and the resulting forest of trees will be very similar to one another and therefore suffer from very similar weaknesses. This is where the <em>random</em> of <em>Random Forest</em> comes into play.</p>&#13;
<p class="indent">Instead of just bagging, which changes the distribution of samples in the per tree training set but not the set of features examined, what if we also randomly selected, for each tree in the forest, a subset of the <em>features</em> themselves and trained on only those features? Doing this would break the correlation between the trees and increase the overall robustness of the forest. In practice, if there are <em>n</em> features per feature vector, each tree will randomly select <span class="middle"><img src="Images/nsqt.jpg" alt="Image" width="28" height="21"/></span> of them over which to build the tree. Random Forests are supported in sklearn as well.</p>&#13;
<h3 class="h3" id="lev1_42"><span epub:type="pagebreak" id="page_124"/>Support Vector Machines</h3>&#13;
<p class="noindent">Our final classical machine learning model is the one that held neural networks at bay for most of the 1990s, the <em>Support Vector Machine (SVM)</em>. If the neural network is a highly data-driven, empirical approach to generating a model, the SVM is a highly elegant, mathematically founded, approach. We’ll discuss the performance of an SVM at a conceptual level as the mathematics involved is beyond what we want to introduce here. If you’re so inclined, the classic reference is “Support-Vector Networks” by Cortes and Vapnik (1995).</p>&#13;
<p class="indent">We can summarize what a Support Vector Machine is doing by gaining intuition about the concepts of margins, support vectors, optimization, and kernels. Let’s look at each concept in turn.</p>&#13;
<h4 class="h4" id="lev2_49">Margins</h4>&#13;
<p class="noindent"><a href="ch06.xhtml#ch6fig8">Figure 6-8</a> shows a two-class dataset with two features. We’ve plotted each sample in the dataset with feature 1 along the x-axis and feature 2 along the y-axis. Class 0 is shown as circles, class 1 as diamonds. This is obviously a contrived dataset, one that’s easily separated by plotting a line between the circles and the diamonds.</p>&#13;
<div class="image" id="ch6fig8"><img src="Images/06fig08.jpg" alt="image" width="532" height="445"/></div>&#13;
<p class="figcap"><em>Figure 6-8: A toy dataset with two classes, circles and diamonds, and two features, x-axis and y-axis</em></p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_125"/>A classifier can be thought of as locating one or more <em>planes</em> that split the space of the training data into homogeneous groups. In the case of <a href="ch06.xhtml#ch6fig8">Figure 6-8</a> the separating “plane” is a line. If we had three features, the separating plane would be a 2D plane. With four features, the separating plane would be three-dimensional, and for <em>n</em> dimensions the separating plane is <em>n –</em> 1 dimensional. Since the plane is multidimensional, we refer to it as a <em>hyperplane</em> and say that the goal of the classifier is to separate the training feature space into groups using hyperplanes.</p>&#13;
<p class="indent">If we look again at <a href="ch06.xhtml#ch6fig8">Figure 6-8</a>, we can imagine an infinite set of lines that separate the training data into two groups, with all of class 0 on one side and all of class 1 on the other. Which one do we want to use? Well, let’s think a bit about what the position of a line separating the two classes implies. If we draw a line more to the right side, just before any of the diamonds, we’ll have separated the training data, but only barely so. Recall, we’re using the training data as a surrogate for the true distribution of samples of each class. The more training data we have, the more faithfully we’ll know that true distribution. However, we don’t really know it.</p>&#13;
<p class="indent">A new, unknown sample, which must be of class 0 or class 1, will fall somewhere on the graph. It’s reasonable to believe that there are class 1 (diamond) samples in the wild that will fall even closer to the circles than any of the samples in the training set. If the separating line is too close to the diamonds, we run the risk of calling valid class 1 samples <em>class 0</em> because the separating line is too far to the right. We can make a similar claim if we place the separating line very close to the class 0 points (circles). Then we run the risk of mislabeling class 0 samples as <em>class 1</em> (diamonds).</p>&#13;
<p class="indent">Therefore, in the absence of more training data, it seems most reasonable to choose the separating line that is as far from both classes as possible. This is the line that is farthest from the rightmost circles while still being as far to the left of the diamonds as possible. This line is the maximal margin location, where the <em>margin</em> is defined as the distance from the closest sample points. <a href="ch06.xhtml#ch6fig9">Figure 6-9</a> shows the maximal margin location as the heavy line with the maximum margin indicated by the two dotted lines. The goal of an SVM is to locate the maximum margin position, as this is the location where we can be most certain to not misclassify new samples, given the knowledge gained from the training set.</p>&#13;
<div class="image" id="ch6fig9"><span epub:type="pagebreak" id="page_126"/><img src="Images/06fig09.jpg" alt="image" width="531" height="445"/></div>&#13;
<p class="figcap"><em>Figure 6-9: The toy dataset of <a href="ch06.xhtml#ch6fig8">Figure 6-8</a> with the maximal margin separating line (heavy) and the maximum margins (dotted)</em></p>&#13;
<h4 class="h4" id="lev2_50">Support Vectors</h4>&#13;
<p class="noindent">Look again at <a href="ch06.xhtml#ch6fig9">Figure 6-9</a>. Notice the four training data points on the margin? These are the training samples that define the margin, or, in other words, support the margin; hence they are <em>support vectors</em>. This is the origin of the name <em>Support Vector Machine</em>. The support vectors define the margin, but how can we use them to locate the margin position? Here is where we’ll simplify things a bit to avoid a large amount of complex vector mathematics that will only muddy the waters for us. For a more mathematical treatment, see “A Tutorial on Support Vector Machines for Pattern Recognition” by Christopher Burges (1998).</p>&#13;
<h4 class="h4" id="lev2_51">Optimization</h4>&#13;
<p class="noindent">Mathematically, we can find the maximum margin hyperplane by solving an optimization problem. Recall that in an optimization problem, we have a quantity that depends on certain parameters, and we want to find the set of parameter values that makes the quantity as small or as large as possible.</p>&#13;
<p class="indent">In the SVM case, the orientation of the hyperplane can be specified by a vector, <span class="middle"><img src="Images/wbar.jpg" alt="Image"/></span>. There is also an offset, <em>b</em>, which we must find. Finally, before we can do the optimization, we need to change the way we specify the class labels. Instead of using 0 or 1 for <em>y</em><sub><em>i</em></sub>, the label of the <em>i</em>-th training sample, <em>x</em><sub><em>i</em></sub>, we’ll use –1 or +1. This will let us define the condition of the optimization problem more simply.</p>&#13;
<p class="indent">So, mathematically, what we want is to find <span class="middle"><img src="Images/wbar.jpg" alt="Image" width="14" height="15"/></span> and <em>b</em> so that the quantity <span class="middle"><img src="Images/126equ01.jpg" alt="Image" width="75" height="29"/></span> is as small as possible, given that <span class="middle"><img src="Images/126equ02.jpg" alt="Image" width="149" height="21"/></span> for all <em>y</em><sub><em>i</em></sub> labels and <em>x</em><sub><em>i</em></sub> training vectors in the dataset. This sort of optimization problem is <span epub:type="pagebreak" id="page_127"/>readily solved via a technique called <em>quadratic programming</em>. (We’re ignoring another important mathematical step here: the actual optimization problem solved uses a Lagrangian to solve the dual form, but again, we’ll try to avoid muddying the waters too much.)</p>&#13;
<p class="indent">The preceding formulation is for a case where the dataset, assumed to have only two classes, can be separated by a hyperplane. This is the linearly separable case. In reality, as we well appreciate by now, not every dataset can be separated this way. So, the full form of the optimization problem includes a fudge factor, <em>C</em>, which affects the size of the margin found. This factor shows up in the sklearn SVM class and needs to be specified to some level. From a practical point of view, <em>C</em> is a <em>hyperparameter</em> of the SVM, a value that we need to set to get the SVM to train properly. The right value of <em>C</em> is problem dependent. In general, any parameter of a model that is not learned by the model but must be set to use the model, like <em>C</em> for an SVM or <em>k</em> for <em>k</em>-NN, is a hyperparameter.</p>&#13;
<h4 class="h4" id="lev2_52">Kernels</h4>&#13;
<p class="noindent">There’s one more mathematical concept we need to introduce, with suitable hand waving. The preceding description is for a linear SVM and uses the training data directly (the <span class="middle"><img src="Images/127equ01.jpg" alt="Image" width="23" height="16"/></span>). The nonlinear case maps the training data to another space by passing it through a function, typically called <span class="middle"><img src="Images/127equ02.jpg" alt="Image" width="35" height="21"/></span>, that produces a new version of the training data vector, <span class="middle"><img src="Images/xbar1.jpg" alt="Image" width="11" height="16"/></span>. The SVM algorithm uses inner products, <span class="middle"><img src="Images/127equ03.jpg" alt="Image" width="32" height="19"/></span>, which means that the mapped version will use <span class="middle"><img src="Images/127equ04.jpg" alt="Image" width="81" height="23"/></span>. In this notation, vectors are thought of as a column of numbers so that <em>T</em>, the transpose, produces a row vector. Then normal matrix multiplication of a 1 × <em>n</em> row vector and an <em>n</em> × 1 column vector will result in a 1 × 1 output, which is a scalar. The inner product is typically written as</p>&#13;
<div class="imagec"><img src="Images/127equ05.jpg" alt="image" width="159" height="25"/></div>&#13;
<p class="noindent">and the function <span class="middle"><img src="Images/127equ06.jpg" alt="Image" width="54" height="20"/></span> is called a <em>kernel</em>. The linear kernel is simply <span class="middle"><img src="Images/127equ07.jpg" alt="Image" width="32" height="20"/></span>, but other kernels are possible. The <em>Gaussian kernel</em> is a popular one, also known as a <em>radial basis function (RBF)</em> kernel. In practical use, this kernel introduces a new parameter, apart from <em>C</em>, which is <em>γ</em>. This parameter relates to how spread out the Gaussian kernel is around a particular training point, with smaller values extending the range of influence of the training sample. Typically, one uses a grid search over <em>C</em> and, if using the RBF kernel, <em>γ</em>, to locate the best performing model.</p>&#13;
<p class="indent">To summarize, then, a Support Vector Machine uses the training data, mapped through a kernel function, to optimize the orientation and location of a hyperplane that produces the maximum margin between the hyperplane and the support vectors of the training data. The user needs to select the kernel function and associated parameters like <em>C</em> and <em>γ</em> so that the model best fits the training data.</p>&#13;
<p class="indent">Support Vector Machines dominated machine learning in the 1990s and early 2000s, before the advent of deep learning. This is because they’re trained efficiently and don’t need extensive computational resources to be <span epub:type="pagebreak" id="page_128"/>successful. Since the arrival of deep learning, however, SVMs have fallen somewhat by the wayside because powerful computers have enabled neural networks to do what previously was not possible with more limited computing resources. Still, SVMs have a place at the table. One popular approach uses a large neural network trained on a particular dataset as a preprocessor for a different dataset with an SVM trained on the output of the neural network (minus the top layers).</p>&#13;
<h3 class="h3" id="lev1_43">Summary</h3>&#13;
<p class="noindent">In this chapter, we introduced six of the most common classic machine learning models: Nearest Centroid, <em>k</em>-NN, Näive Bayes, Decision Tree, Random Forest, and SVMs. These models are classic because they have been used for decades. They are also still relevant if the conditions they support best are present. At times, the classic model is still the correct choice. An experienced machine learning practitioner will know when to fall back to the classics.</p>&#13;
<p class="indent">In the next chapter, we’ll use each of these models, via sklearn, to perform a number of experiments that will build our intuition of how the models work and when to use them.</p>&#13;
</div></body></html>