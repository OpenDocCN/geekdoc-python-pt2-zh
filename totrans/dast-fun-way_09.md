# 9

空间树

![](img/chapterart.png)

上一章展示了如何通过最近邻搜索来找到附近或相近的数据点，扩展了我们回答与咖啡相关问题的能力，例如找到最近的物理位置或找到具有相似属性的项目。本章我们将在基于树的数据结构和空间划分的基础上，进一步提升我们的最近邻搜索能力。

第八章讨论了如何将寻找特定值的算法概念适应到更一般的寻找最近邻问题中。我们还看到，随着从单一维度过渡到多维度，操作变得更加复杂。也许我们想在二维空间中寻找附近的咖啡馆，或寻找相似的朋友（基于共情、倾听意愿以及永恒重要的酷感因子）。网格取代了简单的数组，单一维度排序已经不再足够。

本章介绍了两种基于树的新数据结构：均匀四叉树和 k-d 树。术语*四叉树*通常用来描述一个二维数据结构的整个类，基于计算机科学家 Raphael Finkel 和 Jon Bentley 提出的原始四叉树，该结构将每个二维节点在每个级别分割成四个子区域。我们关注的是均匀四叉树，如研究员和发明家 David P. Anderson 提出的结构。该结构具有与网格结构相似的等大小子区域，从而延续了我们在上一章中的讨论。相比之下，*k-d 树*由 Jon Bentley 发明，使用一种更灵活的二进制分割方案，可以进一步适应数据并允许我们扩展到更高的维度。通过研究四叉树和 k-d 树，我们学会了如何概括和修改基于树的数据结构，并通过与城市规划项目的比较来审视这些数据结构。

## 四叉树

虽然网格提供了一种方便的二维数据存储结构，但它们也带来了许多复杂性。正如我们在上一章所看到的，网格的开销和实用性在很大程度上取决于我们如何细分空间。使用大量网格单元（通过创建细粒度的网格）需要大量内存，并可能要求我们搜索许多单元。另一方面，粗粒度的分割可能导致每个单元中有大量的点，如图 9-1 所示，类似于对大量单独点的简单线性扫描。

![一个包含 11 个数据点和一个目标点的二维网格](img/f09001.png)

图 9-1：一个具有少量格子的网格

我们可以把网格想象成不同的家居整理方法。如果我们把所有厨房用具丢进一个巨大的抽屉里，要找某个物品可得花费很长时间。这相当于使用一个大的网格箱。为了改善这一点，假设我们根据物品的用途将餐具分成不同的抽屉，把炊具分放到不同的橱柜里。甚至把麦片放在一层架子上，把香料放在另一层。这相当于使用更精细的网格箱。突然间情况变得好转——我们不再需要在煎锅下翻找肉桂粉了。多一点结构就让我们的烹饪变得更加轻松。

然而，我们有时会过度应用这一思路。也许我们有太多的餐具，寻找一个已满的抽屉需要很长时间。我们可以通过将餐具分为专门放置勺子用具的抽屉和放置非勺子用具的抽屉来提高效率。但想象一下将每个餐具存放在自己的抽屉中的开销——更糟的是，为每个可能购买的餐具分配一个抽屉。很快，我们会看到一整面抽屉墙，心里想着，得检查多少个抽屉才能找到打蛋器。这就是当我们使用过于精细的网格划分时会发生的情况。

解决这个难题的方法是相对于数据动态地划分空间。只有在需要时，我们才引入额外的结构及其相应的开销。我们从一个粗粒度的空间划分开始。也许我们等到有至少五个铲子时才分配一个单独的抽屉。否则我们可以把它们和勺子、打蛋器放在一起。当我们需要更细的粒度时，我们进一步对子空间进行划分。为了实现这种动态性，我们可以使用*均匀四叉树*。

均匀四叉树将树的分支结构引入网格。树中的每个节点代表一个空间区域。根节点代表四叉树覆盖的整个空间以及该空间内的所有点。每个节点被划分为四个大小相等的象限，每个非空象限都有一个子节点。术语*均匀*指的是节点被划分为大小相等的空间区域，从而在节点内均匀地划分空间。图 9-2 展示了四叉树的划分。

![四叉树的根节点位于顶部，覆盖了整个空间并包含 11 个点。每个子节点包含根节点空间的均匀区域，以及所有落在该区域内的点。](img/f09002.png)

图 9-2：一个四叉树节点最多可以有四个子节点，分别代表该空间的四个大小相等的象限。

在讨论四个子树时，通常会标记它们为西北（NorthWest）、东北（NorthEast）、西南（SouthWest）和东南（SouthEast），以反映它们在原始区域中的空间位置。

内部四叉树节点存储最多四个子节点的指针，以及与之关联的元数据，如分支中点的数量或该区域的空间边界（节点在 x 轴和 y 轴上的最小值和最大值）。叶子节点存储该区域内的点列表，以及任何所需的元数据。我们可以通过同时保持指向子节点的 2×2 指针网格（对于内部节点）和点数组（对于叶子节点），使用一个统一的数据结构来表示内部节点和叶子节点。我们可以在叶子节点中将子节点条目设置为 `null`，或在内部节点中使用空数组。

以下是一个 `QuadTreeNode` 的复合数据结构示例：

```py
QuadTreeNode {
    Boolean: is_leaf
    Integer: num_points
    Float: x_min
    Float: x_max
    Float: y_min
    Float: y_max
    Matrix of QuadTreeNodes: children
    Array of Points: points
}
```

我们为点使用了一个简单的复合数据结构：

```py
Point {
    Float: x
    Float: y
}
```

正如前一章所述，我们也可以选择将点存储在数组或有序元组中。

从技术上讲，我们不需要显式地存储节点的空间边界。我们可以通过从根节点的边界和分割序列中推导出每个节点的边界，因为每个节点都会沿每个维度的中线分割其点，将空间划分为四个可预测大小的子区域。给定根节点的原始边界和一系列分支，我们可以精确计算出任何子节点的边界。然而，预计算并存储边界有一个明显的优势：在任何给定的节点，我们可以直接查找边界，而不是推导它们，这使得我们实现搜索算法变得更加容易。我发现，存储这种额外的空间信息的价值往往超过了额外的内存开销。

四叉树的强大之处在于每一级的分支（当有足够的点时）有效地创建了一个自适应的分层网格。图 9-3 显示了一个四叉树空间分区的示例。

![一个四层四叉树的可视化。线条表示每个级别的分割。](img/f09003.png)

图 9-3：四叉树创建的空间分区

想象一下四叉树的连续分区，以及我们如何将其作为科幻惊悚片中的互动地理搜索软件进行搜索。主角们挤满了指挥室，凝视着代表整个城市的大屏幕。紧张的音乐播放着。随着新信息的不断涌入，操作员从屏幕中选择一个象限。有人说：“在那里放大并增强”，操作员照做了。无论对话如何，这一操作等同于在四叉树中下降一级。瞬间，指挥室的屏幕上显示了城市的一个子集。显示范围的整个区域是上一层级的一个象限。每个人都专注地盯着新的地理子集，随后他们选择进一步的子象限并再次放大。搜索在我们的英雄找到了最接近目标点的发射器时结束。

和其他树一样，我们可以在统一的四叉树外部添加一个包装数据结构，以简化账务管理：

```py
QuadTree {
    QuadTreeNode: root
}
```

根节点在树创建时作为一个空节点与正确的尺寸一起创建，因此我们不需要担心它会是 null。

### 构建统一的四叉树

我们通过递归地将分配的空间划分为越来越小的子区域来构建这些神奇的四叉树。由于数据可能包含非常接近或甚至重复的点，我们需要额外的逻辑来确定何时停止细分，并将包含多个点的节点标记为叶节点。在每个层级，我们检查是否需要将当前节点设为内部节点（具有子节点）或叶节点（包含点的列表）。我们可以使用不同的机制来进行此测试，但以下是最常见的几种：

1.  是否有足够的点来进行划分？如果点太少，检查到子节点的距离成本高于逐一检查每个点的成本。这样做不值得浪费开销。

1.  空间边界是否足够大，以致于可以进行划分？如果我们有 10 个点正好位于同一个位置呢？我们可以一直划分，却永远无法分割这些点。这将浪费时间和内存。

1.  我们是否达到了最大深度？这提供了一种替代检查，防止我们在过度细分时浪费时间和内存，通过限制树的深度来避免这种情况。

我们可以将这个过程想象为一个非常规城市规划师的尝试，他们将土地划分为每个地块都有建筑物的方式。由于不熟悉现代地理分区技术，规划师总是将区域分为四个相等大小的象限。每当他们查看一块土地时，他们会问：“这块土地上的建筑物是不是太多了？”以及“这块土地足够大，能够进一步细分吗？我不能卖一个 2 英尺乘 2 英尺的地块。人们会笑话我。”第三个标准（最大深度）表示规划师在放弃之前愿意进行多少次细分。经过四个层级，规划师可能会说“差不多了”然后继续。如果停止细分的标准没有达到，规划师会叹气，嘀咕一句“真的吗？又来？”然后继续细分这块地。

在划分一个层级时，我们将当前空间划分为四个相等的象限，根据象限划分点，并递归地检查每个象限。如果我们将最小点数设置为 1，最大深度设置为 4（包括根节点），我们将构建图 9-4 中的树。如图所示，我们可以通过只存储每个节点的非空子节点来节省内存。如果某个象限没有子节点，我们可以将其指针设置为`null`。

![图示展示了一个具有四个层级的四叉树。每个叶节点包含一个单独的点。](img/f09004.png)

图 9-4：一个具有四个层级的示例四叉树

用于批量构建四叉树的代码与下一节添加点的代码非常相似。事实上，逐步向一个空的四叉树中添加点是构建四叉树的好方法。

### 添加点

由于四叉树是动态数据结构，我们可以在保持树结构的同时高效地添加点。我们从`QuadTree`数据结构的包装函数开始：

```py
QuadTreeInsert(QuadTree: tree, Float: x, Float: y):
    IF x < tree.root.x_min OR x > tree.root.x_max:
        return False
    IF y < tree.root.y_min OR y > tree.root.y_max:
        return False
    QuadTreeNodeInsert(tree.root, x, y)
    return True
```

这个包装器保证我们始终调用`QuadTreeNodeInsert`时传入的是非空节点。代码还会检查插入的点是否落在四叉树的边界内。这是至关重要的，因为均匀四叉树使用的是相同大小的容器，且无法动态调整大小。所有点必须落在根节点的空间边界内。如果点超出范围，代码返回`False`，但是根据实现方式，你可能想使用其他机制，比如返回错误或抛出异常。

如以下代码所示，向节点添加点的过程是遍历树来找到新点的位置。这个搜索可以通过两种方式结束：在叶子节点或内部死胡同。如果我们在叶子节点结束搜索，就可以在该节点添加新点。根据我们的分割标准（空间边界、最大深度和点的数量），我们可能需要将节点分割成子节点。如果我们结束于内部死胡同，那就说明我们找到了一个先前没有包含任何点的路径。我们可以在此创建适当的节点。

```py
QuadTreeNodeInsert(QuadTreeNode: node, Float: x, Float: y):
  ❶ node.num_points = node.num_points + 1

    # Determine into which child bin the point should go.
  ❷ Float: x_bin_size = (node.x_max - node.x_min) / 2.0
    Float: y_bin_size = (node.y_max - node.y_min) / 2.0
    Integer: xbin = Floor((x - node.x_min) / x_bin_size)
    Integer: ybin = Floor((y - node.y_min) / y_bin_size)

    # Add the point to the correct child.
  ❸ IF NOT node.is_leaf:
      ❹ IF node.children[xbin][ybin] == null:
            node.children[xbin][ybin] = QuadTreeNode(
                  node.x_min + xbin * x_bin_size,
                  node.x_min + (xbin + 1) * x_bin_size,
                  node.y_min + ybin * y_bin_size,
                  node.y_min + (ybin + 1) * y_bin_size)
        QuadTreeNodeInsert(node.children[xbin][ybin], x, y)
        return

 # Add the point to a leaf node and split if needed.
  ❺ node.points.append(Point(x, y))
  ❻ IF we satisfy the conditions to split:
        node.is_leaf = False
      ❼ FOR EACH pt IN node.points:
            QuadTreeNodeInsert(node, pt.x, pt.y)
      ❽ node.num_points = (node.num_points -
                           length(node.points))
        node.points = []
```

代码首先通过增加`num_points`来表示新点❶。然后，函数通过计算容器的大小，并使用这些信息将 x 和 y 索引映射到`0`或`1`来确定新点属于四个容器中的哪一个❷。如果节点不是叶子节点，代码需要递归地将点添加到正确的子节点❸。它首先检查子节点是否存在。如果不存在（即子节点的指针为`null`），则创建子节点❹。我们利用`xbin`和`ybin`都只能是`0`或`1`这一事实来简化逻辑。我们不需要列举所有四种情况，而是可以通过算术运算计算子节点的边界。最后，在叶子节点中，代码直接将点插入到节点中❺。

不过，我们还没完成。代码需要检查是否满足分割条件❻；如果满足，就会分割当前的叶子节点。幸运的是，我们可以重用插入函数来进行分割，就像我们插入一个点时一样。代码将节点标记为非叶子节点（`node.is_leaf = False`），并使用`FOR`循环逐一重新插入点❼。因为当前节点不再是叶子节点，重新插入的点现在会落到正确的子节点，并根据需要创建新的子节点。然而，由于我们为每个点调用了两次这个函数，我们需要修正`num_points`，以避免重复计数重新插入的点❽（由于❶处的计数器增加）。代码还会清空新内部节点的点列表。

图 9-5 显示了如果我们向树中添加两个点会发生什么。插入的空心圆点在触发最大深度条件之前导致一个节点分裂。结果的叶节点包含两个点。插入的空心方形点为其父节点的西北象限添加了一个新的子节点。由于最小点数条件，代码没有再次分裂。

如前一部分所述，我们可以使用这种方法从一组点构建一个统一的四叉树。首先，我们创建一个空的根节点，并设置必要的空间边界。然后我们逐步将每个点添加到树中。由于分裂始终基于每个维度的中间点，树的结构不会因我们插入点的顺序而改变。

![一幅图，显示将两个点添加到统一四叉树的过程。空心圆插入到根节点的西北子节点及该节点的西南子节点。之前的叶节点现在有两个点并再次分裂。两个点位于同一节点中，该节点由于深度限制而成为叶节点。](img/f09005.png)

图 9-5：将两个点（表示为阴影圆圈和阴影方块）添加到四叉树的示例

### 删除节点

从节点中删除点的过程与插入点类似，但更为复杂。我们首先从叶节点中的点列表中删除该点。然后，我们可以沿着树返回，删除那些不再符合我们标准的分裂。这可能涉及递归地从每个节点的子节点中提取点（这些子节点可能本身也有子节点），并将它们合并到下一个叶节点的单一列表中。

另一个困难是确定删除哪个点。与网格一样，用户可能会插入任意接近的点，甚至是重复的点。在下面的代码中，我们删除叶节点列表中的第一个匹配点。由于浮动点误差（由于浮动点变量的精度限制而导致的舍入误差），我们也不能使用直接的相等测试。因此，我们使用一个辅助函数来找到足够接近的点。我们可以复用在清单 8-3 中的`approx_equal`函数来进行此测试。

我们还使用一个辅助函数来合并那些不再符合分裂标准的节点。代码会合并一个有子节点的节点，并返回包含整个子树所有点的数组：

```py
QuadTreeNodeCollapse(QuadTreeNode: node):
  ❶ IF node.is_leaf:
        return node.points

  ❷ FOR i IN [0, 1]:
        FOR j IN [0, 1]:
            IF node.children[i][j] != null:
                Array: sub_pts = QuadTreeNodeCollapse(node.children[i][j])
                FOR EACH pt IN sub_pts:
                    node.points.append(pt)
                node.children[i][j] = null
  ❸ node.is_leaf = True
  ❹ return node.points
```

代码首先检查当前节点是否已经是叶节点，如果是，则直接返回点的数组 ❶。否则，该节点是内部节点，代码需要从每个子节点中汇总单独的数据点。代码循环遍历四个子节点，检查它们是否为`null`，如果不是，则递归调用`QuadTreeNodeCollapse`来汇总点 ❷。函数最后通过将当前节点设置为叶节点 ❸，并返回点 ❹。

使用这个辅助函数后，我们可以进入删除函数。我们从包装器开始。

```py
QuadTreeDelete(QuadTree: tree, Float: x, Float: y):
    IF x < tree.root.x_min OR x > tree.root.x_max:
        return False
    IF y < tree.root.y_min OR y > tree.root.y_max:
        return False
    return QuadTreeNodeDelete(tree.root, x, y)
```

包装函数首先检查点是否位于树的边界内，因此可能在树中。如果是，它会调用递归删除函数。

递归删除代码沿树向下推进，仿佛在搜索目标点。当到达叶子节点时，如果该点存在，则删除该点。然后，代码向上返回树，必要时压缩节点。如果删除了一个点，函数返回 `True`。

```py
QuadTreeNodeDelete(QuadTreeNode: node, Float: x, Float: y):
  ❶ IF node.is_leaf:
        Integer: i = 0
      ❷ WHILE i < length(node.points):
          ❸ IF approx_equal(node.points[i].x, node.points[i].y, x, y):
                remove point i from node.points
                node.num_points = node.num_points - 1
                return True
 i = i + 1
        return False

    # Determine into which child bin the point to be removed would go.
  ❹ Float: x_bin_size = (node.x_max - node.x_min) / 2.0
    Float: y_bin_size = (node.y_max - node.y_min) / 2.0
    Integer: xbin = Floor((x - node.x_min) / x_bin_size)
    Integer: ybin = Floor((y - node.y_min) / y_bin_size)

  ❺ IF node.children[xbin][ybin] == null:
        return False

  ❻ IF QuadTreeNodeDelete(node.children[xbin][ybin], x, y):
        node.num_points = node.num_points - 1

      ❼ IF node.children[xbin][ybin].num_points == 0:
            node.children[xbin][ybin] = null

      ❽ IF node no longer meets the split conditions
            node.points = QuadTreeNodeCollapse(node)
        return True
  ❾ return False
```

代码首先检查递归是否到达叶子节点 ❶。如果是，它会遍历点数组 ❷，检查每个点是否与目标点匹配 ❸。如果找到匹配的点，它会将其从数组中移除，减少该节点中的点数，并返回 `True`，表示删除成功。请注意，只有一个匹配点会被删除。如果代码在叶子节点没有找到匹配点，它会返回 `False`。

搜索随后进入目标点应该被分配的桶 ❹。代码检查是否存在正确的子节点，如果不存在，则返回 `False`，表示该点不在树中 ❺。否则，代码递归调用 `QuadTreeNodeDelete` 来处理子节点 ❻。

如果对 `QuadTreeNodeDelete` 的递归调用返回 `True`，则说明代码已从其子节点中删除了一个点。它会更新点的计数，并检查该子节点是否为空 ❼。如果为空，它会删除该子节点。然后，代码检查当前节点是否继续满足作为内部节点的标准 ❽。如果不满足，则会压缩该节点。代码返回 `True`，表示删除成功。如果递归调用未返回 `True`，则说明没有删除任何点。该函数最终返回 `False` ❾。

### 搜索均匀四叉树

我们从四叉树的根节点开始搜索。在每个节点中，我们首先询问该节点是否可能包含比当前候选点更接近的点。如果是这样，对于内部节点，我们递归地探索其子节点；对于叶子节点，我们直接测试这些点。然而，如果我们确定当前节点不可能包含我们最近的邻居，我们可以通过忽略该节点及其整个子树来剪枝搜索。

我们使用与第八章中对网格单元应用的相同测试来检查节点内点的兼容性。我们可以使用相似的信息：点的位置和区域的边界框。如前一章所述，距离计算为：

![g09001](img/g09001.png)

其中

如果 *x* < *x*[min]，则 *x*[dist] = *x*[min] – *x*

如果 *x*[min] ≤ *x* ≤ *x*[max]，则 *x*[dist] = 0

如果 *x* > *x*[max]，则 *x*[dist] = *x* – *x*[max]

和

如果 *y < y*[min]，则 *y*[dist] *= y*[min] *– y*

如果 *y*[min] ≤ *y* ≤ *y*[max]，则 *y*[dist] = 0

如果 *y* > *y*[max]，则 *y*[dist] = *y* – *y*[max]

简而言之，我们正在检查从搜索目标到节点空间区域内最接近点的距离。回顾上一章的例子，我们再次在问：我们的懒惰邻居需要多远扔一个球才能勉强把球扔回我们的院子？这个检查只测试在此距离下是否有点可能存在于节点中。我们需要探索节点，以查看有哪些点存在。

考虑搜索图 9-6 中标记为 X 的点的最近邻。该图展示了与我们在图 8-3 中咖啡店位置地图相同的点分布，其中 X 代表我们的当前位置，其他点则是附近的咖啡馆。

我们从根节点开始，设置一个无限远的虚拟候选点。我们还没有看到任何候选邻居，所以需要一个起始点来开始搜索。使用一个无限远的虚拟点可以让算法接受它找到的第一个点作为新的候选点，而无需特殊的逻辑。我们找到的任何点都会比无限远的点要近，而且每个区域都会包含更近的点。在搜索咖啡馆的情况下，这相当于从一个虚拟的、无限远的咖啡馆开始。我们列表中的任何一个真实咖啡馆都一定比这个虚拟点要近。

![一组二维点。图的左上方有 11 个数据点和 1 个查询点。](img/f09006.png)

图 9-6：最近邻搜索的示例数据集

由于我们的（虚拟）候选点距离无限远，因此根节点的兼容性测试通过。根节点中的至少一个点*可能*会比无限远的距离更近。虽然这个测试在数学上与我们用于网格单元的测试相同，但它有一个很大的实际区别：我们在每一层树上测试的单元大小是不同的。在较高层次上，每个节点覆盖较大的空间；随着我们向下移动，空间边界会逐渐收紧。

我们根据子节点与查询点的接近程度来优先搜索哪个子节点。毕竟，我们最终是想找到最近的点并尽可能地修剪搜索。因此，我们考虑 x 和 y 的划分，并问：“我们的目标点会落入四个象限中的哪个？”在这种情况下，我们的查询点位于西北象限，如图 9-7 所示，因此我们从这里开始。

我们沿着指针进入到西北子节点，发现自己聚焦于一个子集，该子集包括空间和点，如图 9-8 所示。灰色节点表示尚未探索的节点，灰色点表示尚未检查的点。

![该图显示了根节点，并通过线条表示其划分为四个象限。查询点以 X 表示，位于西北象限。](img/f09007.png)

图 9-7：查询点位于四叉树根节点的西北象限内。

![该图显示了四叉树的前三个级别。表示根节点西北象限对应的子节点的圈，表明搜索正在探索该节点。](img/f09008.png)

图 9-8：示例中最近邻搜索从根节点对应的西北象限子树开始，如图 9-6 所示。

再次，我们的空间兼容性测试表明这个节点*可能*包含我们最近的邻居。该节点中任何点的最小距离都小于我们当前的（虚拟）候选点的距离。我们在另一个内部节点，这意味着空间进一步被划分为四个子区域。

我们的搜索继续到相关内部节点的西南子节点，如图 9-9 所示。我们选择这个节点，因为它离我们的查询点最近。第三次，兼容性测试通过。由于我们已经到达了叶节点，因此我们明确检查该节点中每个点的距离。在这种情况下，只有一个点，并且它比我们的虚拟候选点更近。

![该图显示了四叉树的前三个级别。三个节点是实心的：根节点、根节点的西北子节点以及该节点的西南子节点。当前正在搜索的节点被圈起来。](img/f09009.png)

图 9-9：在四叉树的第二级，我们的搜索从离目标点最近的象限——西南象限开始。

我们找到了第一个真正的候选最近邻！它的距离成为了我们迄今为止的最小距离。我们可以对所有未来的点要求更高的标准。在我们寻找附近咖啡店的示例中，第一个邻居代表了目前为止找到的最近咖啡店。到这个点的距离是我们获得一杯咖啡所需的最大距离。我们可能会在之后的搜索中找到更近的咖啡店，但至少我们不必走无穷远的路。这种解脱感是显而易见的。

一旦我们测试了叶节点中的所有点，我们返回到（内部）父节点并检查剩余的子节点。现在我们有了一个真正的候选点和距离，我们的剪枝测试就有了意义。我们检查所有剩余子象限的兼容性：西北、东北和东南。我们的距离测试显示，我们可以跳过西北象限：如图 9-10 所示，它的空间范围内最近的点比我们已有的候选点更远。它不可能包含更好的邻居。

![该图显示了根节点西北子节点中的四个象限。图中从查询点到目前为止最佳节点的线表示阈值距离。当前节点西北子节点的最小距离也通过一条较长的线表示。](img/f09010.png)

图 9-10：显示到目前为止最佳候选点与当前节点的西北象限之间的相对距离

我们还可以跳过空的东北和东南象限。由于它们没有任何点，它们也不能有更好的邻居。我们的搜索可以丢弃这两个象限，而无需进行距离测试，因为它们的指针将是`null`，表示不存在这样的子节点。如图 9-11 所示，我们成功地修剪了该节点的四个象限中的三个。西北象限中的两个数据点也保持灰显，因为我们从未测试过它们。

![搜索已返回到根节点的西北子节点。该节点的三个象限已被灰显，表示它们已被从搜索中修剪掉。](img/f09011.png)

图 9-11：最近邻搜索能够跳过该节点的四个象限中的三个。

一旦完成检查内部节点的象限，我们返回到其父节点并重复此过程。下一个最接近的象限是西南象限，我们的修剪测试确认它足够接近，可能包含更好的邻居，如图 9-12 所示。

每当我们找到一个子节点，按照我们的简单距离测试，它*可能*包含一个更近的点时，我们就沿着该路径继续前进，检查是否确实存在更近的邻居。在这种情况下，我们的搜索向下遍历到了西南象限。

![根节点修剪示意图。查询点以 X 表示，一条线表示到目前为止最佳邻居的距离。第二条线显示到根节点的西南子节点的（较小的）距离。](img/f09012.png)

图 9-12：修剪测试，其中候选象限可能包含比当前最佳点更接近的邻居

在下一级，四个潜在象限再次争夺我们的注意力。我们有了一个真实的候选点及其对应的距离，可以积极地修剪搜索，如图 9-13 所示。我们检查西北象限（以及它的唯一一个点），因为它在我们的距离阈值内。我们可以跳过其他三个象限；东北和西南象限都是空的，因此它们的子指针为 null，我们使用距离测试确认东南象限距离太远，无法包含更好的邻居。

这次当我们返回根节点时，我们可以修剪掉剩余的两个子节点，如图 9-14 所示。

东北和东南象限的远端显著超出了我们的距离阈值。

![搜索已遍历到与根节点的西南子节点对应的整个分支。该子节点的四个象限中有三个已被灰显。](img/f09013.png)

图 9-13：在检查根节点的西南象限时，最近邻搜索可以再次跳过四个子象限中的三个。

![搜索已返回到根节点。根节点的两个象限被灰色标出。11 个数据点中的 9 个保持为灰色。](img/f09014.png)

图 9-14：返回四叉树的根节点后，搜索可以跳过该节点的两个象限。

### 最近邻搜索代码

为了简化最近邻搜索代码的实现，我们首先定义一个辅助函数来计算目标点（*x*，*y*）到某个节点的距离。检查最小距离的代码与上一章中介绍的网格最小距离代码类似：

```py
MinDist(QuadTreeNode: node, Float: x, Float: y):
    Float: x_dist = 0.0
    IF x < node.x_min:
        x_dist = node.x_min – x
    IF x > node.x_max:
        x_dist = x – node.x_max

    Float: y_dist = 0.0
    IF y < node.y_min:
        y_dist = node.y_min – y
    IF y > node.y_max:
        y_dist = y – node.y_max

    return sqrt(x_dist*x_dist + y_dist*y_dist)
```

然而，在这种情况下，代码不需要计算节点的最小和最大边界，因为每个节点都明确地存储了这些边界。

主要的搜索算法使用与其他基于树的方法相同的递归公式。我们对该搜索算法的实现包括一个参数`best_dist`，表示目前为止的最短距离。通过将`best_dist`传递给我们的搜索函数，我们可以简化剪枝逻辑。如果当前节点的最小距离大于目前为止的最佳距离，我们可以终止该分支的搜索。然后，如果找到更近的点，函数会返回一个*更近*的点，否则返回`null`。需要注意的是，在这个实现中，返回`null`意味着当前节点内没有比`best_dist`更近的点。

我们使用一个简单的包装器，传递根节点和初始的无限距离：

```py
QuadTreeNearestNeighbor(QuadTree: tree, Float: x, Float: y):
    return QuadTreeNodeNearestNeighbor(tree.root, x, y, Inf)
```

我们的最近邻搜索包装函数没有检查目标点是否落在四叉树的边界内。这使我们能够使用代码找到树内邻居，即使目标点在树的边界之外，从而提高代码的实用性。

下面是递归搜索节点的代码：

```py
QuadTreeNodeNearestNeighbor(QuadTreeNode: node, Float: x,
                            Float: y, Float: best_dist):
    # Prune if the node is too far away. 
  ❶ IF MinDist(node, x, y) >= best_dist:
        return null
    Point: best_candidate = null

 # If we are in a leaf, search the points.
  ❷ IF node.is_leaf:
        FOR EACH current IN node.points:
            Float: dist = euclidean_dist(x, y, current.x, current.y)

            IF dist < best_dist:
                best_dist = dist
                best_candidate = current
        return best_candidate

    # Recursively check all 4 children starting with the closest.
  ❸ Float: x_bin_size = (node.x_max - node.x_min) / 2.0 
    Float: y_bin_size = (node.y_max - node.y_min) / 2.0 
    Integer: xbin = Floor((x - node.x_min) / x_bin_size)
    IF xbin < 0:
        xbin = 0
    IF xbin > 1:
        xbin = 1

    Integer: ybin = Floor((y - node.y_min) / y_bin_size)
    IF ybin < 0:
        ybin = 0
    IF ybin > 1:
        ybin = 1

  ❹ FOR EACH i IN [xbin, (xbin + 1) % 2]:
        FOR EACH j IN [ybin, (ybin + 1) % 2]:
            IF node.children[i][j] != null:
                Point: quad_best = QuadTreeNodeNearestNeighbor(
                                       node.children[i][j], 
                                       x, y, best_dist)
              ❺ IF quad_best != null:
                    best_candidate = quad_best
                    best_dist = euclidean_dist(x, y, quad_best.x, 
                                               quad_best.y)
    return best_candidate
```

函数首先进行剪枝测试，如果没有任何点比`best_dist`更近，则跳过该节点并返回`null` ❶。然后，代码检查是否到达叶节点 ❷。如果已到达叶节点，它会使用`FOR`循环检查每个点是否比`best_dist`更近，如果是，则更新`best_dist`和`best_candidate`。在这个循环结束时，我们返回`best_candidate`，如果没有找到更近的点，它的值将是`null`。

接下来的代码块处理内部节点的逻辑。只有当节点不是叶节点时，我们才会到达这里，因此它没有任何候选点。一些基本的数值测试和整数操作控制着代码搜索子节点的顺序，允许代码先搜索最接近的子节点，然后再扩展到其他子节点。代码首先计算候选点应该落入哪个 x 和 y 的区域 ❸，调整值，使得`xbin`和`ybin`都落在[0, 1]范围内，从而指示出最接近的子节点。这一调整是必要的，因为对于许多内部节点，我们的目标点可能完全位于当前节点表示的 2×2 网格之外。

然后，我们使用一对嵌套的`FOR`循环递归地探索非空的子节点，遍历这些对子 ❹。每次我们检查是否找到更接近的点（由`quad_best != null`表示），如果找到了，就更新`best_candidate`和`best_dist` ❺。函数结束时，我们返回`best_candidate`。与叶节点的情况一样，如果我们没有找到比原始`best_dist`更近的点，`best_candidate`可能会是`null`。

## k-d 树

我们已经解决了两维的动态划分问题，现在是时候将注意力转向使用 k-d 树在三维或更多维度中查找点或最近邻的任务了。我们已经看到，当我们没有考虑到咖啡中的所有相关属性时，可能会带来失望。在数据集中寻找相似点时，高维问题很常见，例如在天气数据集中寻找相似的条件（温度、压力、湿度）。

从理论上讲，我们可以通过简单地沿着更多的维度进行划分来扩展四叉树。例如，*八叉树*是三维版本，每一层都划分为八个子节点。八向划分看起来似乎还不算太糟，但这种方法显然无法随着维度数量的增加而优雅扩展。如果我们要构建一个覆盖*D*维数据的树结构，我们需要一次性沿所有*D*维度进行划分，这样每个内部节点就会有 2^(*D*)个子节点。如果我们在处理包含温度、压力、湿度、降水量和风速的天气数据时，我们将使用五维点，每一层就需要划分 32 个子树！这种巨大的开销是我们在将网格扩展到更高维度时遇到的相同问题。

为了有效地扩展到更高的维度，我们需要控制分支因子。有多种强大的数据结构设计用于高维空间中的高效近邻搜索。一个例子是 k-d 树，它基于与四叉树类似的概念构建。

### k-d 树结构

*k-d 树*是一种空间数据结构，它结合了四叉树的空间划分和二叉搜索树的二叉分支因子，兼具两者的优点。k-d 树不是在每一层沿每个维度进行分割，而是选择一个维度并沿该维度分割数据。因此，每个内部节点将数据分割成两个子节点：左子节点的点小于（或等于）分割值，右子节点的点大于分割值。

在使用 k-d 树时，我们失去了均匀四叉树的规则网格结构，但作为交换，我们回到了我们熟悉并喜爱的二叉搜索树的二叉分支因子。这种沿单一维度分割的能力使得 k-d 树能够扩展到更高维度的数据集。我们不再需要在每一层分割为 2^(*D*) 个子节点。图 9-15 展示了一个 k-d 树的示例。

![k-d 树的根节点沿 y 维度进行分割。每个产生的子节点沿 x 维度进行分割。](img/f09015.png)

图 9-15：k-d 树在每一层沿单一维度进行分割

与四叉树相比，k-d 树可以更灵活地适应数据的结构。我们不再局限于在每个维度的中点进行分割。与将空间划分为 2^(*D*) 个大小相等的网格单元不同，我们可以选择一个分割维度和适合当前节点的数据的分割值。因此，每个内部节点都存储着我们正在进行分割的维度（`split_dim`）和分割值（`split_val`），如果点在该维度上的值小于或等于节点的分割值，则将其分配给左子节点：

```py
pt[split_dim] <= split_val
```

我们不再像在图 9-15 中那样沿交替的轴进行分割，这样的分割方式类似于四叉树。我们可以通过根据当前节点内数据点的组成来选择分割维度，从而定制我们的树结构，使分割更有利于未来的搜索。我们可能根据节点的整体边界来选择分割值，比如在最宽的维度的中间进行分割。或者我们可以根据数据点的分布来选择分割值，比如在分割维度上选择中位数或范围的中点。这两种选择之间的差异在图 9-16 中有所展示。

![左边的框显示了沿 x 维度中点分割的点，空间被一分为二。右边的框显示了沿 x 维度的中位数值分割的点，产生了两个具有大致相等数量的点但大小不同的框。](img/f09016b.png)

图 9-16：一个节点分别在中点（左）和中位数（右）进行分割

k-d 树的灵活结构意味着我们在处理节点的空间边界时需要额外小心。与统一的四叉树的方形网格单元不同，k-d 树的节点覆盖了整体搜索空间的多维矩形。每个维度的宽度可能完全不同。一些节点可能是接近方形的，而其他的则可能是长而窄的。我们通过显式追踪定义其空间边界的多维矩形来表示一个节点的区域——每个维度的最小值和最大值。由于我们可以有任意数量的维度，因此我们将边界存储在两个数组中，`x_min`和`x_max`，其中`x_min[d]`表示当前节点内维度`d`的最小值，`x_max[d]`表示最大值。所有在该节点内的点都满足：

```py
x_min[d] <= pt[d] <= x_max[d] FOR ALL d
```

由于其复杂性，每个 k-d 树节点都存储了大量信息。虽然这看起来可能是昂贵的开销，但正如我们在本节中将看到的，成本被树本身的强大功能和灵活性所抵消。与本书中的其他数据结构一样，我们在内存、数据结构复杂性和后续算法效率之间做出了显式的权衡。

这是`KDTreeNode`的一个示例复合数据结构：

```py
KDTreeNode {
    Boolean: is_leaf
    Integer: num_dimensions
    Integer: num_points
    Array of Floats: x_min
    Array of Floats: x_max
    Integer: split_dim
    Float: split_val
    KDTreeNode: left
    KDTreeNode: right
    Array of Arrays: points
}
```

在这种情况下，我们使用数组表示每个点，从而使其具有任意维度。

和本书中的其他树一样，我们可以将节点封装在外部数据结构中，如`KDTree`。

```py
KDTree {
    Integer: num_dimensions
    KDTreeNode: root
}
```

在这个封装数据结构中存储维度的数量有助于在执行插入、删除或搜索等操作时检查一致性。`num_dimensions`的值在创建 k-d 树时设置，并且对于该树保持不变。

选择分割节点策略的灵活性展示了 k-d 树的真正优势：我们在四叉树的空间划分基础上进行了增强，进一步适应数据。如果我们的点集是聚集的，我们会选择那些能够提供最多信息的分割方式，重点关注这些区域。图 9-17 展示了这种动态划分。

![k-d 树的图示，水平或垂直线表示每个层次上的不同分割，所有子区划分的维度不同。](img/f09017.png)

图 9-17：k-d 树创建的空间划分

假设我们正在进行定位附近咖啡店的任务。如果我们沿着 95 号州际公路从佛罗里达到缅因州进行自驾游，我们可以预处理数据，仅存储离高速公路 50 英里以内的咖啡店。图 9-18 展示了这种形状的分布示例。

这种预过滤帮助将搜索空间限制为仅靠近高速公路的咖啡店，但我们可以通过将这些位置存储在空间数据结构中，使搜索更加高效。我们还希望将搜索范围缩小到高速公路上的适当区域。毕竟，当我们还在南卡罗来纳州时，没有必要检查马萨诸塞州的咖啡店。我们很快发现，均匀的分区方案远非理想：我们的旅程涵盖了超过 1500 英里的大部分北向行驶。由于我们已经过滤到仅沿高速公路的商店，因此从东西方向均匀分区所带来的剪枝效果不大。通过在南北方向上偏向分区，我们可以增加剪枝的数量，从而减少搜索成本。

再举个例子，如果四叉树是一个城市规划者按照严格的规定划分地图的日常工作，那么 k-d 树就像一个拥有更多工具的城市规划者。他们不再局限于将每个地块划分为完美的正方形。相反，他们可以根据实际点的分布灵活地划分空间。我们的城市规划者选择最大的维度进行分割，以尽量减少狭长的区域。他们还使用该维度上的中位点来提供平衡的树结构。结果可能不像四叉树的正方形那样整齐，但它可能更有效。

![形状类似于 95 号公路的点散点图。](img/f09018.png)

图 9-18：咖啡店在主要高速公路上的分布示例

### 更紧密的空间边界

我们通常可以通过跟踪落在给定节点内所有点的边界框，而不是节点的总空间边界，进一步提高空间树的剪枝能力。这将我们的剪枝问题从“在节点覆盖的空间中是否可能存在最近邻候选点？”转变为“在节点内实际点的边界框中是否可能存在最近邻候选点？”虽然这看起来是一个小变化，但根据我们用于分割节点的逻辑，可能会看到显著的节省，如图 9-19 所示。

![k-d 树节点的紧密边界框显示为一个虚线框，框住节点内的点。](img/f09019.png)

图 9-19：k-d 树节点中点的边界框

如果我们在节点构建过程中使用这些更紧密的边界框，我们可以更好地适应数据。我们不再是基于整个可能的空间区域进行划分，而是基于实际点所占据的区域进行划分。结果的 k-d 树结构如图 9-20 所示。黑色框表示每个节点的紧密边界框。灰色点和线则显示节点的边界框与数据集其余部分的关系。

![图中的每个 k-d 树节点显示为一组点，紧密边界框覆盖了灰色区域的完整数据集。](img/f09020.png)

图 9-20：一个三层的 k-d 树及每个节点的边界框

在搜索过程中，我们使用黑盒（紧密边界框）来进行剪枝。正如图中所示，得到的区域可以显著缩小，从而使我们能够更积极地进行剪枝。由于紧密边界框较小，它通常与查询点的最小距离更大。

我们可以使用一个简单的辅助函数来计算由数组表示的一组点的紧密边界框（列表 9-1）：

```py
ComputeBoundingBox(Array of Arrays: pts):
  ❶ Integer: num_points = length(pts)
    IF num_points == 0:
        return Error
    Integer: num_dims = length(pts[0])

  ❷ Array: L = Array of length num_dims
    Array: H = Array of length num_dims
    Integer: d = 0
  ❸ WHILE d < num_dims:
        L[d] = pts[0][d]
        H[d] = pts[0][d]
        d = d + 1

    Integer: i = 1
  ❹ WHILE i < num_points:
        d = 0
        WHILE d < num_dims:
            IF L[d] > pts[i][d]:
                L[d] = pts[i][d]
            IF H[d] < pts[i][d]:
                H[d] = pts[i][d]
            d = d + 1
        i = i + 1
  ❺ return (L, H)
```

列表 9-1：计算节点中点的紧密边界框的辅助函数

该代码从输入数据中提取点的数量和维度的数量 ❶。然后，它创建新的数组 `L` 和 `H` 来分别存储低值和高值边界 ❷，并用数组中第一个点的坐标初始化它们 ❸。接着，代码遍历数组中剩余的点，并检查是否有点超出了边界 ❹。如果有，它会扩展边界。代码最后返回这两个数组 ❺。

这个辅助函数还展示了在 `KDTree` 的包装函数中预先检查所有点是否包含正确的维度数量的好处。这个检查确保我们不会尝试访问数据点中无效的数组条目。

当然，当我们开始处理动态变化时，跟踪这些额外的边界会增加复杂性。向 k-d 树中添加点可能会增加给定节点的边界。同样，删除节点可能会缩小边界。

### 构建 k-d 树

构建 k-d 树使用一个递归过程，类似于二叉搜索树的构建过程，但有一些主要的区别。我们从所有数据点开始，并通过选择一个分割维度和分割值将它们分成两个子集。这个过程在每一层重复，直到满足终止条件：节点中剩余的点数最小、宽度最小或深度最大。通常，我们使用两个测试：最小点数和最小宽度，但至少使用最后两个测试中的一个是必要的，以避免在有重复数据点时发生无限递归。

我们从一个包装函数开始，检查数据是否有效：

```py
BuildKDTree(KDTree: tree, Array of Arrays: pts):
  FOR EACH pt IN pts:
      IF length(pt) != tree.num_dimensions:
          Return an error.
  IF length(pts) > 0:
      tree.root = KDTreeNode()
      RecursiveBuildKDTree(tree.root, tree.num_dimensions, pts)
  ELSE:
      tree.root = null
```

代码首先检查所有点是否具有正确的维度。然后，它检查是否有点可以用来构建树。如果有，代码会分配一个新的根节点（覆盖先前的树），并使用下面的函数递归地构建 k-d 树。否则，它将根节点设置为 `null`，表示空树。

k-d 树与四叉树构建的主要区别在于，k-d 树要求我们在每一层选择一个单独的分割维度。如果我们使用紧密边界框，我们还需要计算 *D* 维度上的边界框。尽管这些变化使得代码略显冗长（增加了对 *D* 的额外循环），但并没有增加任何实际的复杂性。构建 k-d 树的代码会递归地将我们的点集划分到子节点中，直到满足终止条件。

```py
RecursiveBuildKDTree(KDTreeNode: node, Integer: num_dims,
                     Array of Arrays: pts):
  ❶ node.num_points = length(pts)
    node.num_dimensions = num_dims
    node.left = null
    node.right = null
    node.points = empty array
    node.split_dim = -1
    node.split_val = 0.0
    node.is_leaf = True

    # Compute the bounding box of the points.
  ❷ (node.x_min, node.x_max) = ComputeBoundingBox(pts)

    # Compute the width of the widest dimension.
  ❸ Float: max_width = 0.0
    Integer: d = 0
    WHILE d < node.num_dimensions:
        IF node.x_max[d] - node.x_min[d] > max_width:
            max_width = node.x_max[d] - node.x_min[d]
        d = d + 1
 # If we meet the conditions for a leaf, append the
    # remaining points to the node's point list.
  ❹ IF we do not satisfy the conditions to split:
        FOR EACH pt IN pts:
            node.points.append(pt)
        return

    # Choose split dimension and value.
  ❺ node.split_dim = chosen split dimension
    node.split_val = chosen split value along node.split_dim
    node.is_leaf = False

    # Partition the points into two sets based on
    # the split dimension and value.
    Array of Arrays: left_pts = []
    Array of Arrays: right_pts = []
  ❻ FOR EACH pt IN pts:
        IF pt[node.split_dim] <= node.split_val:
            left_pts.append(pt)
        ELSE:
            right_pts.append(pt)

    # Recursively build the child nodes.
  ❼ node.left = KDTreeNode()
    RecursiveBuildKDTree(node.left, num_dims, left_pts)

    node.right = KDTreeNode()
    RecursiveBuildKDTree(node.right, num_dims, right_pts)
```

这段代码从记账开始，填充每个节点所需的信息。我们记录一些必要的细节，比如当前点的数量和维度数量 ❶。然后，函数遍历所有点，使用清单 9-1 中的辅助函数计算该节点的紧密边界框 ❷。

一旦获得边界，代码会遍历每个维度，找出最宽的维度 ❸。我们使用这个循环作为递归的一个停止条件（不再对过小的节点进行分割）。如果节点不符合继续分割的条件，代码会将所有的点存储在叶节点的一个列表中 ❹。否则，代码会为该节点选择一个分割维度和分割值 ❺。代码会遍历当前点集，将其根据分割维度和分割值划分成两个数组，`left_pts` 和 `right_pts` ❻。这两个数组将用于递归地构建两个子节点 ❼。

选择 `split_dim` 和 `split_val` 的一种方法是沿着最宽维度的中间进行分割。这段代码相对简单，而且大部分可以被并入到找到最宽维度 ❸ 的初始代码块中。

```py
 Float: max_width = 0.0
    Integer: split_dim = 0
    Integer: d = 0
 WHILE d < node.num_dimensions:
        IF node.x_max[d] - node.x_min[d] > max_width:
            max_width = node.x_max[d] - node.x_min[d]
            split_dim = d
        d = d + 1
```

然后，在 ❺ 处设置分割维度和值：

```py
 node.split_dim = split_dim
    node.split_val = (node.x_min[node.split_dim] + 
                      node.x_max[node.split_dim]) / 2.0
```

批量构建 k-d 树相较于动态插入和删除点有着显著的优势。通过在构建过程中考虑所有数据点，我们可以更好地调整树的结构以适应数据。我们选择的分割是基于所有数据点的，而不是目前为止插入的子集。

### k-d 树操作

插入点、删除点和搜索 k-d 树的基本操作与四叉树的操作方法相同。我们从树的顶部（根节点）开始所有操作，并使用分割值来沿着相应的分支导航。主要的区别是，选择分割的不是四个象限中的一个，而是使用`split_dim`并根据`split_val`来选择两个子节点之一。由于高层概念与四叉树中介绍的类似，我们不会对每一项操作的代码进行详细讲解。相反，让我们来看看一些区别。

1.  插入 当向 k-d 树节点插入点时，我们使用 `split_dim` 和 `split_val` 来决定应该选择哪一分支。如果叶节点满足分割条件，我们将其分割，方法与批量构建时使用的方式相同。最后，如果我们正在跟踪每个节点的紧密边界框，则需要更新边界以考虑新加入的点。由于我们在添加点，这个更新总会增加边界框的大小。

    ```py
     Integer: d = 0
        WHILE d < node.num_dimensions:
            IF x[d] < node.x_min[d]:
                node.x_min[d] = x[d]
            IF x[d] > node.x_max[d]:
                node.x_max[d] = x[d]
            d = d + 1
    ```

    这段代码遍历新点的每个维度，检查其是否超出了边界框，如果超出，则更新边界。

1.  删除 在删除 k-d 树中的点时，我们使用 `split_dim` 和 `split_val` 来决定在查找过程中应该选择哪一分支。删除节点后，我们回到树的根节点。在沿途的每个节点上，我们检查是否可以缩小边界（通过叶节点中的点或内部节点的两个子节点的边界框）。我们还会检查内部节点是否可以被合并。

1.  查找 四叉树与 k-d 树在查找操作中的关键区别在于是否可以修剪节点。例如，我们可以通过扩展公式 ![i09001](img/i09001.png) 来计算点 *x* 与节点（非均匀的，*D* 维度）边界框内最接近点的欧几里得距离，这个公式是我们在四叉树和网格中使用过的。我们首先计算点到节点空间边界在每个维度上的最小距离：

如果 *x*[*d*] < *x*[min][*d*] 则 *dist*[d] = *x*[min][*d*] − *x*[*d*]

如果 *x*[min][*d*] ≤ *x* ≤ *x*[max][*d*] 则 *dist*[d] = 0

如果 *x*[*d*] > *x*[max][*d*] 则 *dist*[d] = *x*[*d*] − *x*[max][*d*]

1.  其中 *x*[*d*] 表示查询点的 *d* 维度，*x*[min][*d*] 和 *x*[max][*d*] 表示节点在 *d* 维度上的低和高边界。然后，我们计算每个维度上平方距离的总和并取平方根。我们可以将整个计算过程实现为一个在维度上进行的 `WHILE` 循环：

    ```py
    KDTreeNodeMinDist(KDTreeNode: node, Point: pt):
        Float: dist_sum = 0.0
        Integer: d = 0
        WHILE d < node.num_dimensions:
            Float: diff = 0.0
            IF pt[d] < node.x_min[d]:
                diff = node.x_min[d] - pt[d]
            IF pt[d] > node.x_max[d]:
                diff = pt[d] - node.x_max[d]
            dist_sum = dist_sum + diff * diff
            d = d + 1
        return sqrt(dist_sum)
    ```

请注意，k-d 树对增加和删除的敏感度可能高于四叉树。虽然两者都可能由于分割规则和点的分布而变得不平衡，但 k-d 树的分割是基于当时数据选择的。如果我们显著改变点的分布，原来的分割值可能不再合适。在批量构建过程中，我们可以根据当前数据适应分割，考虑诸如树的深度、是否平衡以及节点空间边界的紧凑性等因素。这展示了数据结构中的另一种权衡——随着数据的变化，结构的性能可能会下降。

## 为什么这很重要

四叉树和 k-d 树是我们如何将动态数据结构的强大功能与空间结构结合应用于搜索问题的例子。通过在多个维度上同时分支，四叉树使我们能够根据局部区域内数据点的密度来调整网格的分辨率。高密度区域会导致更深的分支，从而生成更细粒度的网格。与此同时，保持规则网格结构会在更高维度上引入新的成本。考察四叉树、八叉树及其变体在不同数据集上的扩展性，为我们思考如何利用空间结构提供了一个重要的视角。

k-d 树代表了我们在过去几章中构建的概念的结合，旨在解决最近邻搜索问题。它通过回归到二叉搜索树的核心概念，选择一个单一维度在每个节点进行分割，从而解决了高分支因子的难题。此外，它还允许更多的灵活性来适应数据的结构，从而增强了剪枝能力。

四叉树和 k-d 树并不是唯一能够促进最近邻搜索的数据结构。还有许多其他基于树的和非树的方法。空间数据结构这一主题可以填满好几本书。和计算机科学中的几乎所有问题一样，每种方法都有其在程序复杂性、计算成本和内存使用方面的权衡。在本书中，四叉树和 k-d 树作为如何将最近邻搜索的空间剪枝与基于树的结构相结合的绝佳例子，从而使空间树能够根据手头的数据进行适应。
