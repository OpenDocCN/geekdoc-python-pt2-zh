- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Computer scientists use the term *supervised learning* to refer to a broad
    range of quantitative methods that predict and classify. In fact, you’ve already
    done supervised learning: the linear regression you did in Chapter 2 and the LPMs
    and logistic regression from Chapter 5 are all instances of supervised learning.
    By learning those methods, you’ve already become familiar with the basic ideas
    of supervised learning. This chapter introduces some advanced supervised learning
    methods and discusses the idea of supervised learning in general. We’re dwelling
    on this topic so much because it’s such a crucial component of data science.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by introducing yet another business challenge and describing how
    supervised learning can help us resolve it. We’ll talk about linear regression
    as an imperfect solution and discuss supervised learning in general. Then we’ll
    introduce k-NN, a simple but elegant supervised learning method. We’ll also briefly
    introduce decision trees, random forests, and neural networks, and discuss how
    to use them for prediction and classification. We’ll close with a discussion of
    how to measure accuracy and what unites each of these disparate methods.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Website Traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine that you’re running a website. Your website’s business model is simple:
    you post articles on interesting topics, and you earn money from the people who
    view your website’s articles. Whether your revenue comes from ad sales, subscriptions,
    or donations, you earn money in proportion to the number of people who visit your
    site: the more visitors, the higher your revenue.'
  prefs: []
  type: TYPE_NORMAL
- en: Amateur writers submit articles to you with the hope that you’ll publish them
    on your site. You receive an enormous number of submissions and can’t possibly
    read, much less publish, everything you receive. So you have to do some curation.
    You may consider many factors as you’re deciding what to publish. Of course, you’ll
    try to consider the quality of submitted articles. You’ll also want to consider
    which articles fit with the “brand” of your site. But in the end, you’re trying
    to run a business, and maximizing your site’s revenue will be crucial to ensuring
    your business’s long-term survival. Since you earn revenue in proportion to the
    number of visitors to your site, maximizing revenue will depend on selecting articles
    to publish that are likely to get many visitors.
  prefs: []
  type: TYPE_NORMAL
- en: You could try to rely on intuition to decide which articles are likely to receive
    many visitors. This would require either you or your team to read every submission
    and make difficult judgments about which articles are likely to attract visitors.
    This would be extremely time-consuming, and even after spending all that time
    reading articles, it’s far from certain that your team would have the right judgment
    about which articles will attract the most visitors.
  prefs: []
  type: TYPE_NORMAL
- en: 'A faster and potentially more accurate approach to this problem is through
    supervised learning. Imagine that you could write code to read articles for you
    as soon as they arrived in your inbox and could then use information that the
    code gleans from each submitted article to accurately predict the number of visitors
    it will attract, before you publish it. If you had code like that, you could even
    fully automate your publishing process: a bot could read submissions from emails,
    predict the likely revenue expected from every submitted article, and publish
    every article that had an expected revenue above a particular threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: The hardest part of that process would be predicting an article’s expected revenue;
    that’s the part we need to rely on supervised learning to accomplish. In the rest
    of the chapter, we’ll go through the steps required for the supervised learning
    that would enable this kind of automated system to predict the number of visitors
    a given article will attract.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and Plotting News Article Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like most data science scenarios, supervised learning requires us to read in
    data. We’ll read in a dataset that’s available for free from the University of
    California, Irvine (UCI) Machine Learning Repository ([https://archive-beta.ics.uci.edu/](https://archive-beta.ics.uci.edu/)).
    This repository contains hundreds of datasets that machine learning researchers
    and enthusiasts can use for research and fun.
  prefs: []
  type: TYPE_NORMAL
- en: The particular dataset we’ll use contains detailed information about news articles
    published on Mashable ([https://mashable.com](https://mashable.com)) in 2013 and
    2014\. This Online News Popularity dataset has a web page at [https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity](https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity)
    that presents more information about the data, including its source, the information
    it contains, and papers that have been published containing analyses of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can obtain a ZIP file of the data from [https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip).
    After you download the ZIP archive, you must extract it on your computer. You’ll
    then see the *OnlineNewsPopularity.csv* file, which is the dataset itself. After
    extracting that *.csv*, you can read it into your Python session as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We import our old friend the pandas package and read the news dataset into a
    variable called `news`. Each row of `news` contains detailed information about
    one particular article published on Mashable. The first column, `url`, contains
    the URL of the original article. If you visit the URL of a particular article,
    you can see the text and images associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: In total, our `news` dataset has 61 columns. Each column after the first contains
    a numeric measurement of something about the article. For example, the third column
    is called `n_tokens_title`. This is a count of the *tokens* in the title, which
    in this case just means the number of words in the title. Many of the columns
    in the `news` dataset have names that refer to advanced methods in *natural language
    processing (NLP)*. NLP is a relatively new field concerned with using computer
    science and mathematical algorithms to analyze, generate, and translate natural
    human language in a way that’s quick and automatic and doesn’t require human effort.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the 46th column, `global_sentiment_polarity`. This column contains
    a measure of each article’s overall *sentiment*, ranging from –1 (highly negative)
    to 0 (neutral) to 1 (highly positive). The ability to automatically measure the
    sentiment of text written in natural human language is one of the recent, exciting
    developments in the world of NLP. The most advanced sentiment analysis algorithms
    are able to closely match humans’ sentiment ratings, so an article about death,
    horror, and sadness will be ranked by both humans and NLP algorithms as having
    a highly negative sentiment (close to –1), while an article about joy, freedom,
    and data analysis will be universally agreed to have a highly positive sentiment
    (close to 1). The creators of our dataset have already run a sentiment analysis
    algorithm to measure the sentiment of each article in the dataset, and the result
    is stored in `global_sentiment_polarity`. Other columns have other measurements,
    including simple things like article length as well as other advanced NLP results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final column, `shares`, records the number of times each article was shared
    on social media platforms. Our true goal is to increase revenue by increasing
    the number of visitors. But our dataset doesn’t contain any direct measurement
    of either revenue or visitors! This is a common occurrence in the practice of
    data science: we want to analyze something, but our data contains only other things.
    In this case, it’s reasonable to suppose that the number of social media shares
    is correlated with the number of visitors to an article, both because highly visited
    articles will be shared often and because highly shared articles will be visited
    often. And, as we mentioned before, our revenue is directly related to the number
    of website visits. So, we can reasonably suppose that the number of social media
    shares of an article is closely related to the revenue obtained from the article.
    This means that we’ll use shares as a *proxy* for visits and revenue.'
  prefs: []
  type: TYPE_NORMAL
- en: It will help our analysis if we can determine which features of an article are
    positively related to shares. For example, we might guess that articles with high
    sentiment scores will also get shared frequently, if we believe that people like
    to share happy things. If that’s true, knowing the sentiment of an article will
    help us predict the number of times an article will be shared. By learning how
    to predict shares, we suppose that we’ll be simultaneously learning how to predict
    both visitors and revenue as well. And, if we know the features of a highly shared
    article, we’ll know how to design future articles to maximize our revenue.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ve done before (especially in Chapter 1), we can start with simple exploration.
    We’ll start by drawing a graph. Let’s consider a graph of the relationship between
    sentiment and shares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that when we access our dataset’s columns in this Python snippet,
    we put a space at the beginning of every column name. For example, we write `news['
    shares']` instead of `news['shares']` to refer to the column recording the number
    of shares. We do this because that’s the way the column names are recorded in
    the original data file. For whatever reason, that file contains a space before
    every column name instead of the column name alone, so we need to include that
    space when we tell Python to access each column by name. You’ll see these spaces
    throughout the chapter; every dataset has its own quirks, and part of being a
    successful data scientist is being able to understand and adapt to quirks like
    this one.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-1](#figure6-1) shows the relationship between sentiment polarity
    and shares.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-1: The relationship between sentiment and shares for every article
    in our dataset'
  prefs: []
  type: TYPE_NORMAL
- en: One thing we can notice about this plot is that, at least to the naked eye,
    no clear linear relationship exists between polarity and shares. High-sentiment
    articles don’t seem to be shared much more than low-sentiment articles, or vice
    versa. If anything, articles close to the middle of the polarity scale (articles
    that have close to neutral sentiment) seem to earn the most shares.
  prefs: []
  type: TYPE_NORMAL
- en: Using Linear Regression as a Prediction Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can do a more rigorous test for this (lack of a) linear relationship by
    performing a linear regression, just as we did in Chapters 2 and 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet performs a linear regression predicting shares using sentiment
    polarity. It does so in the same way we outlined in Chapter 2. We start by importing
    from the module `sklearn.linear_model`, which contains the `LinearRegression()`
    function we want to use. Then, we reshape the data so that the module we’re importing
    can work with it. We create a variable called `regressor`, and we fit the regressor
    to our data. Finally, we print out the coefficient and intercept obtained by fitting
    the regression: 499.3 and 3,335.8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll remember from Chapter 2 that we can interpret these numbers as the slope
    and intercept of the regression line, respectively. In other words, our linear
    regression estimates the relationship between sentiment and shares as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*shares* = 3335.8 + 499.3 · *sentiment*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot this regression line together with our data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output should look like [Figure 6-2](#figure6-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-2: A regression line showing the estimated relationship between sentiment
    and shares'
  prefs: []
  type: TYPE_NORMAL
- en: Our regression line, which should be red if you create the plot at home, appears
    quite flat, showing only a weak relationship between sentiment and shares. Using
    this regression line to predict shares probably wouldn’t help us much, since it
    predicts nearly identical numbers of shares for every sentiment value. We’ll want
    to explore other supervised learning methods that can lead to better, more accurate
    predictions. But first, let’s think about supervised learning in general, including
    what it is about linear regression that makes it a type of supervised learning,
    and what other types of supervised learning could also be applicable to our business
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The linear regression we just did is an example of supervised learning. We’ve
    mentioned supervised learning several times in this chapter, without precisely
    defining it. We can define it as the process of learning a function that maps
    feature variables to target variables. This may not sound immediately obvious
    or clear. To understand what we mean, consider [Figure 6-3](#figure6-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-3: The supervised learning process'
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about how this figure applies to the linear regression we completed earlier
    in the chapter. We used sentiment as our only feature (the oval on the left).
    Our target variable was shares (the oval on the right). The following equation
    shows our *learned function* (the arrow in the middle):'
  prefs: []
  type: TYPE_NORMAL
- en: '*shares* = 3,335.8 + 499.3 · *sentiment*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This function does what every learned function is supposed to do in supervised
    learning: it takes a feature (or multiple features) as its input, and it outputs
    a prediction of the value of a target variable. In our code, we imported capabilities
    from the sklearn module that determined the coefficients, or learned the function,
    for us. (For its part, sklearn learned the function by relying on linear algebra
    equations that are guaranteed to find the coefficients that minimize the mean
    squared error on our target variable, as we discussed in Chapter 2.)'
  prefs: []
  type: TYPE_NORMAL
- en: The term *supervised learning* refers to the process of determining (learning)
    this function. The target variable is what supervises the process, because as
    we’re determining the learned function, we check whether it leads to accurate
    predictions of the target. Without a target variable, we would have no way to
    learn the function, because we’d have no way to determine which coefficients led
    to high accuracy and which led to low accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Every supervised learning method you’ll ever use can be described by [Figure
    6-3](#figure6-3). In some cases, we can do *feature engineering*, in which we
    carefully select which variables in our dataset will lead to the most accurate
    possible predictions. In other cases, we’ll adjust our target variable—for example,
    by using a proxy or a transformation of the original variable. But the most important
    part of any supervised learning method is the learned function that maps the features
    to the target. Mastering new supervised learning methods consists of mastering
    new ways to determine these learned functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use linear regression as our chosen supervised learning method, the
    learned function we get is always in the form shown in [Equation 6-1](#equation6-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '*target* = *intercept* + *coefficient*[1] · *feature*[1] + *coefficient*[2]
    · *feature*[2] + … + *coefficient*[*n*] · *feature*[*n*]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 6-1: The general form of every linear regression’s learned function'
  prefs: []
  type: TYPE_NORMAL
- en: For someone who has taken lots of algebra classes, this may seem like a natural
    form for a function to take. Coefficients are multiplied by features and added
    up. When we do this in two dimensions, we get a line, like the line in [Figure
    6-2](#figure6-2).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this is not the only possible form for a learned function. If we think
    more deeply about this form, we can realize that the function for linear regression
    is implicitly expressing an assumed view, or *model*, of the world. In particular,
    linear regression is implicitly assuming that the world can be described by lines:
    that anytime we have two variables *x* and *y*, there’s a way to relate them accurately
    as the line *y* = *a* + *bx*, for some *a* and *b*. Many things in the world can
    be described by lines, but not everything. The universe is a big place, and there
    are many models of the world, many learned functions, and many supervised learning
    methods that can give us more accurate predictions by abandoning this assumption
    of linearity.'
  prefs: []
  type: TYPE_NORMAL
- en: If the world isn’t described by lines and linear relationships, what model of
    the world is the correct one, or the most accurate or useful one? Many answers
    are possible. For example, instead of a world made up of lines, we could think
    of the world as composed of unique little neighborhoods around points. Instead
    of using a line to make predictions, we could measure characteristics of neighborhoods
    around points, and use those neighborhoods to make predictions. (This approach
    will become clearer in the next section.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything we observe in the world is related by lines and linear relationships,
    linear regression is the right model for studying it. If the world is instead
    made up of neighborhoods, another supervised learning model is more suitable:
    k-nearest neighbors. We’ll examine this method next.'
  prefs: []
  type: TYPE_NORMAL
- en: k-Nearest Neighbors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that you have an intern who has never studied statistics, linear regression,
    supervised learning, or data science at any level. You just received a new article
    from an author who wants to be published on your website. You give the intern
    the newly submitted article as well as the `news` dataset and some NLP software.
    You assign the intern to predict the number of times the new article will be shared.
    If your intern predicts a high number of shares, you’ll publish the article. Otherwise,
    you won’t.
  prefs: []
  type: TYPE_NORMAL
- en: Your intern uses NLP software to determine that this article has `global_sentiment_polarity`
    equal to 0.42\. Your intern doesn’t know how to do the linear regression that
    we did at the beginning of the chapter. Instead, they have a simple idea of how
    they’ll predict shares. Their simple idea is to look through the `news` dataset
    until they find an article that closely resembles this new article. If an existing
    article in the dataset closely resembles the newly submitted article, it’s reasonable
    to suppose that the new article’s number of shares will resemble the existing
    article’s number of shares.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose they find an existing article in the dataset that has `global_sentiment_polarity`
    equal to 0.4199\. They’ll conclude, reasonably, that the existing article is similar
    to our new article, because their sentiment ratings are nearly identical. If the
    existing article achieved 1,200 shares, we can expect that our new article, with
    a nearly identical `global_sentiment_polarity`, should have a similar number of
    shares. “Similar articles get similar numbers of shares” is one way to sum up
    this simple thought process. In the context of supervised learning, we can rephrase
    this as “similar feature values lead to similar target values,” though of course
    your intern has never heard of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re working with numeric data, we don’t need to speak merely qualitatively
    about articles *resembling* each other. We can directly measure the *distance*
    between any two observations in our dataset. The existing article that resembles
    our new article has `global_sentiment_polarity` equal to 0.4199, which is 0.0001
    different from our new article’s `global_sentiment_polarity` of 0.42\. Since `global_sentiment_polarity`
    is the only variable we’ve considered so far, we can say that these two articles
    have a *distance* of 0.0001 between them.
  prefs: []
  type: TYPE_NORMAL
- en: You may think that distance is something that has one nonnegotiable definition.
    But in data science and machine learning, we often find ourselves measuring distances
    that don’t match what we mean by the term in everyday life. In this example, we’re
    using a difference between sentiment scores as our distance, even though it’s
    not a distance that can be walked or measured with a ruler. In other cases, we
    may find ourselves expressing a distance between true and false values, especially
    if we’re doing a classification as in Chapter 5. When we talk about distance,
    we’re often using the term as a loose analogy rather than a literal physical measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observations that have a small distance between them can be called *neighbors*,
    and in this case we’ve found two close neighbors. Another article with sentiment
    0.41 would have distance 0.1 from our new article: still a neighbor, but a little
    further down the “street.” For any two articles, we can measure the distance between
    them on all variables that interest us and use this as a measurement of the extent
    to which any two articles are neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of considering just one neighbor article, we can consider the entire
    neighborhood surrounding the new article we want to make predictions about. We
    might find the 15 nearest neighbors to our new article—the 15 points in our dataset
    with `global_sentiment_polarity` closest to 0.42\. We can consider the number
    of shares associated with each of those 15 articles. The mean of the number of
    shares achieved by these 15 nearest neighbors is a reasonable prediction for the
    number of shares we can expect our new article to get.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your intern didn’t think their prediction method was anything special. It just
    seemed like a natural, simple way to make a prediction without using any calculus
    or computer science. However, their simple process is actually a powerful supervised
    learning algorithm called *k-nearest neighbors (k-NN)*. We can describe the whole
    method in four simple steps; truly, it is simplicity itself:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a point *p* you want to make a prediction about for a target variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a natural number, *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the *k* nearest neighbors to point *p* in your dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mean target value of the *k* nearest neighbors is the prediction for the
    target value of *p*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You may have noticed that the k-NN process doesn’t require any matrix multiplication
    or calculus or really any math at all. Though it’s usually taught in only postgraduate-level
    computer science classes, k-NN is nothing more than a simple idea that children
    and even interns already intuitively grasp: that if things resemble each other
    in some ways, they’re likely to resemble each other in other ways. If things live
    in the same neighborhood, they might be similar to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing k-NN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Writing code for k-NN supervised learning is straightforward. We’ll start by
    defining `k`, the number of neighbors we’ll look at, and `newsentiment`, which
    will hold the `global_sentiment_polarity` of the hypothetical new article we want
    to make a prediction about. In this case, let’s suppose that we receive another
    new article, and this one has a sentiment score of 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we’ll be predicting the number of shares that will be achieved by a new
    article with a sentiment score of 0.5\. We’ll look at the 15 nearest neighbors
    of our new article to make these predictions. It will be convenient to convert
    our polarity and shares data to lists, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can calculate the distance between every article in our dataset and
    the hypothetical new article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This snippet uses a list comprehension to calculate the absolute value of the
    difference between the sentiment of each existing article and the sentiment of
    our new article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have all these distances, we need to find which are the smallest.
    Remember, the articles with the smallest distance to the new article are the nearest
    neighbors, and we’ll use them to make our final prediction. A useful function
    in Python’s NumPy package enables us to easily find the nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this snippet, we import NumPy and then define a variable called `idx`, which
    is short for *index*. If you run `print(idx[0:k])`, you can see what this variable
    consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These 15 numbers are the index numbers of the nearest neighbors. The 30,230th
    article in our dataset has the `global_sentiment_polarity` that is closest to
    0.5 out of all articles in the data. The 30,670th article has the `global_sentiment_polarity`
    that’s second closest, and so on. The `argsort()` method we use is a convenient
    method that sorts the distances list from smallest to largest, then provides the
    indices of the `k` smallest distances (the indices of the nearest neighbors) to
    us.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we know the indices of the nearest neighbors, we can create a list of
    the number of shares associated with each neighbor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final prediction is just the mean of this list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You should get the output 7344.466666666666, indicating that past articles with
    sentiment equal to about 0.5 get about 7,344 social media shares, on average.
    If we trust the logic of k-NN, we should expect that any future article that has
    sentiment about equal to 0.5 will also get about 7,344 social media shares.
  prefs: []
  type: TYPE_NORMAL
- en: Performing k-NN with Python’s sklearn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We don’t have to go through that whole process every time we want to use k-NN
    for prediction. Certain Python packages can perform k-NN for us, including the
    sklearn package, whose relevant module we can import into Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You may be surprised that the module we import here is called `KNeighborsRegressor`.
    We just finished describing how k-NN is very different from linear regression,
    so why would a k-NN module be using the word *regressor* just like a linear regression
    module does?
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-NN method is certainly not linear regression, and it doesn’t use any
    of the matrix algebra that linear regression relies on, and it doesn’t output
    regression lines like linear regression. However, since it’s a supervised learning
    method, it’s accomplishing the same goal as linear regression: determining a function
    that maps features to targets. Since regression was the dominant supervised learning
    method for well over a century, people began to think of *regression* as synonymous
    with *supervised learning*. So people started to call k-NN functions *k-NN regressors*
    because they accomplish the same goal as regression, though without doing any
    actual linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, the words *regression* and *regressors* are used for all supervised learning
    methods that make predictions about continuous, numeric target variables, regardless
    of whether they’re actually related to linear regression. Since supervised learning
    and data science are relatively new fields (compared to mathematics, which has
    been around for millennia), many instances of confusing or redundant terminology
    like these remain that haven’t been cleaned up; part of learning data science
    is getting used to these confusing names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we’ve done with linear regression, we need to reshape our sentiment
    list so that it’s in the format this package expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, instead of calculating distances and indices, we can simply create a “regressor”
    and fit it to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can find the prediction our classifier makes for any sentiment, as long
    as it’s properly reshaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This k-NN regressor has predicted that the new article will receive 7,344.46666667
    shares. This exactly matches the number we got before, when doing the k-NN process
    manually. You should be pleased that the numbers match: it means that you know
    how to write code for k-NN at least as well as the authors of the respected and
    popular sklearn package.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned a new supervised learning method, think about how it’s
    similar to and different from linear regression. Both linear regression and k-NN
    rely on feature variables and a target variable, as shown in [Figure 6-3](#figure6-3).
    Both create a learned function that maps feature variables to the target variable.
    In the case of linear regression, the learned function is a linear sum of variables
    multiplied by coefficients, in the form shown in [Equation 6-1](#equation6-1).
    In the case of k-NN, the learned function is a function that finds the mean target
    value for *k* nearest neighbors in the relevant dataset.
  prefs: []
  type: TYPE_NORMAL
- en: While linear regression implicitly expresses a model of the world in which all
    variables can be related to each other by lines, k-NN implicitly expresses a model
    of the world in which neighborhoods of points are all similar to each other. These
    models of the world, and the learned functions they imply, are quite different.
    Because the learned functions are different, they could make different predictions
    about numbers of article shares or anything else we want to predict. But the goal
    of accurately predicting a target variable is the same in both cases, and so both
    are commonly used supervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Using Other Supervised Learning Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear regression and k-NN are only two of many supervised learning algorithms
    that can be used for our prediction scenario. The same sklearn package that allowed
    us to easily do k-NN regression can also enable us to use these other supervised
    learning algorithms. [Listing 6-1](#listing6-1) shows how to do supervised learning
    with five methods, each using the same features and target variables, but with
    different supervised learning algorithms (different learned functions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 6-1: A collection of five supervised learning methods'
  prefs: []
  type: TYPE_NORMAL
- en: 'This snippet contains five sections of four code lines each. The first two
    sections are for linear regression and k-NN; they’re the same code we ran previously
    to use sklearn’s prebuilt packages to easily get linear regression and k-NN predictions.
    The other three sections have the exact same structure as the first two sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a “regressor.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the regressor to our data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fitted regressor to print a prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The difference is that each of the five sections uses a different kind of regressor.
    The third section uses a decision tree regressor, the fourth uses a random forest
    regressor, and the fifth uses a neural network regressor. You may not know what
    any of these types of regressors are, but you can think of that as a convenient
    thing: supervised learning is so easy that you can write code to build models
    and make predictions before you even know what the models are! (That’s not to
    say this is a good practice—it’s always better to have a solid theoretical understanding
    of every algorithm you use.)'
  prefs: []
  type: TYPE_NORMAL
- en: Describing every detail of all these supervised learning algorithms goes beyond
    the scope of this book. But we can provide a sketch of the main ideas. Each approach
    accomplishes the same goal (prediction of a target variable), but does it using
    different learned functions. In turn, these learned functions implicitly express
    different assumptions and different math or, in other words, different models
    of the world.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s begin by looking at decision trees, the first type of model in our code
    after our k-NN section. Instead of assuming that variables are related by lines
    (like linear regression) or by membership in neighborhoods (like k-NN), *decision
    trees* assume that the relationships among variables can be best expressed as
    a tree that consists of binary splits. Don’t worry if that description doesn’t
    sound immediately clear; we’ll use sklearn’s decision tree–plotting function to
    create a plot of the decision tree regressor called `dtregressor` that was created
    by the code in [Listing 6-1](#listing6-1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see the result in [Figure 6-4](#figure6-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4: A decision tree for predicting article shares based on sentiment'
  prefs: []
  type: TYPE_NORMAL
- en: We can follow this flowchart to make predictions about shares, given any `global_sentiment_polarity`.
    Since the flowchart has a branching structure that resembles a tree’s branches,
    and since it enables decision-making, we call it a *decision tree*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start at the box at the top of the tree. The first line of the box expresses
    a condition: `X[0] <= 0.259`. Here, `X[0]` is referring to the `global_sentiment_polarity`
    variable, which is the only feature in our dataset. If that condition is true,
    we proceed along the leftward arrow to a box on the next lowest level. Otherwise,
    we proceed along the rightward arrow to the other side of the tree. We continue
    to check the conditions in each box until we arrive at a box that specifies no
    condition and has no arrows pointing to other, lower boxes. We then check the
    value specified there and use that as our prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sentiment value we’ve been working with in our example (0.5), we go
    right from the first box because 0.5 > 0.259, then we go right at the second box
    for the same reason, and then we go right yet again at our third box because 0.5
    > 0.263\. Finally, we arrive at the fourth box, which doesn’t have any condition
    to check, and we get our prediction: about 3,979 shares for an article with sentiment
    polarity 0.5.'
  prefs: []
  type: TYPE_NORMAL
- en: If you create this decision tree at home, you’ll see that some of the boxes
    are shaded or colored. This shading is done automatically, and the level of shading
    applied is proportional to the value predicted by the decision tree. For example,
    you can see that one box in [Figure 6-4](#figure6-4) indicates a prediction of
    57,100 shares, and it has the darkest shading. Boxes that predict lower numbers
    of shares will have lighter shading or no shading at all. This automatic shading
    is done to highlight especially high predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the details of how sklearn creates the decision tree in [Figure
    6-4](#figure6-4) in advanced machine learning textbooks. For most standard business
    use cases, the details and math of optimizing decision trees is not as important
    as the much easier task of writing a few simple Python lines to create one and
    then read its plot.
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree in [Figure 6-4](#figure6-4) can be generated with only a few
    lines of code and can be interpreted without any special training. This means
    that decision trees are well suited to business applications. You can quickly
    generate a decision tree and show it to clients or company leaders, and explain
    it without needing to go into any math, computer science, or other difficult topics.
    Because of this, data scientists often say that decision trees are *interpretable
    models*, in contrast to other models like neural networks that are more opaque
    and difficult to quickly understand or explain. A decision tree can be a natural,
    quick addition to any presentation or report that can provide visual interest
    and can help others understand a dataset or prediction problem. These are important
    advantages of decision trees in business applications. On the other hand, decision
    trees tend to have lower accuracy than other, more complex methods like random
    forests (see the next section).
  prefs: []
  type: TYPE_NORMAL
- en: Just like linear regression and k-NN, a decision tree uses a feature of data
    (in this case, sentiment) to make a prediction about a target (in this case, shares).
    The difference is that decision trees don’t rely on an assumption that the variables
    are related by a line (the assumption of linear regression) or that the variables
    are related by small neighborhoods around points (the assumption of k-NN). Instead,
    decision trees are built with the assumption that the branching structure shown
    in [Figure 6-4](#figure6-4) is the appropriate model of the world.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fourth section of [Listing 6-1](#listing6-1) uses *random forests* for prediction.
    Random forests are a type of *ensemble method*. Ensemble methods got their name
    because they consist of a collection of many simpler methods. As you might surmise
    from the name, random forests consist of a collection of simpler decision trees.
    Every time you use a random forest regressor for prediction, the sklearn code
    creates many decision tree regressors. Each of the individual decision tree regressors
    is created with a different subset of the training data and a different subset
    of the training features. The final random forest prediction is the mean of the
    predictions made by each of the many individual decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of [Figure 6-3](#figure6-3), random forests learn a complicated
    function: one that consists of a mean of many learned functions from multiple
    randomly selected decision trees. Nevertheless, because random forests learn a
    function that maps features to a target variable, they are a standard supervised
    learning method, just like linear regression, k-NN, and all the rest.'
  prefs: []
  type: TYPE_NORMAL
- en: Random forests have become popular because their code is relatively easy to
    write and they often have much better accuracy than decision trees or linear regressions.
    These are their main advantages. On the other hand, while we can draw an easily
    interpretable representation of a decision tree, like [Figure 6-4](#figure6-4),
    random forests often consist of hundreds of unique decision trees averaged together,
    and it’s not easy to draw a representation of a random forest in a way a human
    can understand. Choosing a random forest as your supervised learning method will
    probably increase your accuracy, but at the cost of interpretability and explainability.
    Every supervised learning method has advantages and disadvantages, and choosing
    the right trade-offs that are appropriate for your situation is important for
    any data scientist who wants to succeed at supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Neural networks* have become extremely popular in recent years as our computer
    hardware has matured to the point of being able to handle their computational
    complexity. The complexity of neural networks also makes them hard to describe
    succinctly, except to say that we can use them for supervised learning. We can
    start by showing a diagram of one particular neural network ([Figure 6-5](#figure6-5)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-5: A diagram of a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: This plot is a representation of a neural network’s learned function. In this
    plot, you can see a column of 13 circles, called *nodes*, on the left side. These
    13 nodes are collectively called the *input layer* of the neural network. Each
    node of the input layer represents one feature of the training data. The single
    node on the far right represents the neural network’s final prediction of a target
    variable. All of the lines and nodes between the left and right represent a complex
    learned function that maps feature inputs to the final target prediction.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can see that the topmost node in the leftmost column (labeled
    *A*) has an arrow pointing to another node (labeled *B*), with the number 4.52768
    written near it. This number is a *weight*, and we’re supposed to multiply this
    weight by the value of the feature corresponding to node A. We then add the result
    of that multiplication to a running total that corresponds to node B. You can
    see that node B has 13 arrows pointing to it, one for each node in the input layer.
    Each feature will be multiplied by a different weight, and the product of the
    feature value and the weight will be added to the running total for node B. Then,
    the number –0.14254 will be added to the result; this is the number drawn on an
    arrow between a blue node with a 1 inside it and node B. (This blue node is also
    called a *bias node*.)
  prefs: []
  type: TYPE_NORMAL
- en: 'After all this multiplication and addition, we’ll have a running total for
    node B, and we’ll apply a new function called an *activation function* to it.
    Many possible activation functions exist, one of which is the logistic function
    you met in Chapter 5. After we apply our activation function, we’ll have a final
    numeric value for node B. We’ve only barely begun the process of calculating the
    neural network’s learned function. You can see that node B has four arrows emanating
    from it, each pointing to other nodes further to the right. For each of those
    arrows, we’ll have to follow the same steps of multiplying weights by node values,
    adding to running totals for every node, and applying activation functions. After
    we do this for all the nodes and all the arrows in the diagram, we’ll have a final
    value for the rightmost node: this will be our prediction of the target value.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are designed in such a way that this whole process, including
    repeated multiplication and addition and activation functions, should give us
    a highly accurate prediction as its final output. The complexity of neural networks
    can be a challenge, but it’s also what enables them to accurately model our complex
    nonlinear world.
  prefs: []
  type: TYPE_NORMAL
- en: These networks are called *neural* because the nodes and arrows in [Figure 6-5](#figure6-5)
    resemble the neurons and synapses in a brain. This resemblance is mostly superficial.
    You could depict neural networks in a way that didn’t look like a brain, or you
    could write down other methods like linear regression in a way that did look like
    a brain.
  prefs: []
  type: TYPE_NORMAL
- en: To really master neural networks, you need to learn a lot more. Some of the
    interesting advances in neural networks have come from experimenting with different
    structures, or *architectures*, of the nodes. For example, *deep neural networks*
    have many layers between the leftmost input nodes and the rightmost output. *Convolutional
    neural networks* add an extra type of layer that performs a special operation
    called *convolution* to the network structure. *Recurrent neural networks* allow
    connections to flow in multiple directions, instead of just from left to right.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have found remarkable applications for neural networks in computer
    vision (like recognizing a dog or a cat or a car or a person), language processing
    (like machine translation and speech recognition), and much more. On the other
    hand, neural networks are hard to interpret, hard to understand, and hard to train
    properly, and they sometimes require specialized hardware. These downsides sometimes
    make neural networks an unattractive option in business applications despite their
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Prediction Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whichever supervised learning model we choose, after we fit it, we’ll want
    to measure its prediction accuracy. Here is how we do it for our scenario of predicting
    shares of articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple snippet calculates the MAE, as we’ve done before. In the first
    line, we use our regressor’s `predict()` method to predict the number of shares
    for each article in our dataset. (Remember, this `regressor` is the linear regression
    model we created near the beginning of the chapter. If you’d like, you can replace
    `regressor` with `rfregressor` or `nnregressor` to measure the accuracy of our
    random forest or our neural network, respectively.) In the second line, we calculate
    the prediction error of these predictions: this is simply the absolute value of
    the difference between predicted and actual. The mean of our prediction error,
    calculated in the third line, is a measurement of how well our particular supervised
    learning method performed, where 0 is the best-possible value, and higher values
    are worse. We can use this process to calculate prediction accuracy for many supervised
    learning algorithms, and then choose the algorithm that leads to the highest accuracy
    (the lowest mean absolute error) as the best method for our scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: The only problem with this approach is that it doesn’t resemble a true prediction
    scenario. In real life, we’d have to make predictions for articles that were not
    in our training dataset—articles that our regressor had never seen during its
    training process. In contrast, we’ve taken a dataset of articles from 2013 and
    2014, fit a regressor to that whole dataset, and then judged our accuracy based
    on the same 2013–14 dataset that was used to fit our regressor. Since we judged
    our accuracy based on the same data that was used to fit our regressor, what we’ve
    done isn’t truly prediction. It’s *postdiction—*saying what happened after it
    happened instead of before. When we do postdiction, we’re liable to be guilty
    of overfitting, the dastardly peril that we already encountered in Chapter 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the problems of postdiction and overfitting, we can take the same
    approach we took in Chapter 2: split our dataset into two mutually exclusive subsets,
    a training set and a test set. We use the training set to train the data or, in
    other words, to allow our supervised learning model to learn its learned function.
    After we train the data using only the training dataset, we test it using the
    test data. The test data, since it wasn’t used in the training process, is “as
    if” from the future, since our regressor hasn’t used it for learning, even if
    it’s actually from the past.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sklearn package has a convenient function we can use to split our data
    into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The four outputs of this snippet are `trainingx` and `trainingy`—the *x* and
    *y* components of our training data—and `testx` and `testy`—the *x* and *y* components
    of our test data. Let’s check the length of each of these outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that our training data consists of `trainingx` (the sentiment scores
    of the training examples) and `trainingy` (the share statistics of the training
    examples). Both of these training datasets consist of 29,733 observations, or
    75 percent of the data. The test datasets (`testx` and `testy`) consist of 9,911
    observations, or the other 25 percent. This type of split follows the same approach
    that we took in Chapter 2: training our model with the majority of the data and
    testing our model with a smaller minority of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One important difference between the training/test split we did here and the
    training/test split we did in Chapter 2 is that, in Chapter 2, we used earlier
    data (the first years of our dataset) as training data and later data (the last
    years of our dataset) as test data. Here, we don’t do a before/after split for
    our training and test data. Instead, the `train_test_split()` function we used
    performs a random split: randomly choosing training and test sets, instead of
    neatly selecting from earlier and later times. This is an important distinction
    to remember: for time-series data (data recorded at regular, ordered intervals),
    we choose training and test sets based on a split between earlier and later data,
    but for all other datasets, we select training and test sets randomly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to train our models by using these training sets, and we need
    to calculate prediction error by using these test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You can see in this snippet that we fit the regressor by using only the training
    data. Then we calculate the prediction error by using only the test data. Even
    though all of our data comes from the past, by making predictions for data that
    weren’t included in our training, we’re making sure that our process resembles
    a true prediction process instead of postdiction.
  prefs: []
  type: TYPE_NORMAL
- en: We can see the error on the test set by running `print(np.mean(predictionerror))`.
    You’ll see that the mean prediction error on our test set is about 3,816 when
    using our random forest regressor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also do the same with our other regressors. For example, this is how
    we can check the prediction error of our k-NN regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Again, we can use `print(np.mean(predictionerror))` to find out whether this
    method seems to perform better than our other supervised learning methods. When
    we do, we find that our k-NN regressor has a mean prediction error equal to about
    3,292 on the test set. In this case, k-NN has better performance than random forests,
    as measured by prediction error on the test set. When we want to choose the best
    supervised learning method for a particular scenario, the simplest way to do it
    is to choose the one with the lowest prediction error *on a test set*.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Multivariate Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in this chapter, we’ve worked with only univariate supervised learning,
    meaning that we’ve used only one feature (sentiment) to predict shares. Once you
    know how to do univariate supervised learning, jumping to *multivariate supervised
    learning*, where we use multiple features to predict a target, is completely straightforward.
    All we need to do is specify more features in our *x* variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we specify an `x` variable that contains not only the sentiment of an
    article but also two other features from other columns in our dataset. After that,
    the process is the same as we’ve followed before: splitting into a training and
    test set, creating and fitting a regressor using a training set, and calculating
    prediction error on a test set. When we run `print(np.mean(predictionerror))`
    now, we see that our multivariate model has a mean prediction error equal to about
    3,474, indicating that our multivariate random forest model performs better than
    our univariate random forest model on our test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Classification Instead of Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, this whole chapter has presented various ways to predict shares, given
    different features of an article. The `shares` variable can take any integer value
    from 0 to infinity. For data like that (continuous, numeric variables), it’s appropriate
    to use regression to predict the values it will take. We used linear regression,
    k-NN regression, decision tree regression, random forest regression, and neural
    network regression: five supervised learning methods, all of them used to predict
    targets that can take a wide range of values.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of doing prediction and regression, we may want to do categorical classification,
    as we did in Chapter 5. In our business scenario, we might not be interested in
    predicting a precise number of shares. Instead, we may be interested only in whether
    an article will reach a number of shares that’s higher than the median number.
    Deciding whether something is above or below a median is a classification scenario,
    since it consists of deciding true/false to a question with only two possible
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a variable that enables us to do classification as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we create a `themedian` variable that represents the median value of
    shares in our dataset. Then we add a new column to the `news` dataset called `abovemedianshares`.
    This new column is 1 when an article’s share count is above the median, and it’s
    0 otherwise. This new measurement is derived from a numeric measurement (number
    of shares), but we can think of it as a categorical measurement: one that expresses
    a true/false proposition of whether an article is in the high-share category.
    Since our business goal is to publish high-share articles and not publish low-share
    articles, being able to accurately classify new articles as likely high-share
    articles or likely low-share articles would be useful to us.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform classification instead of regression, we need to change our supervised
    learning code. But luckily, the changes we have to make are minor. In the following
    snippet, we use classifiers instead of regressors for our new categorical target
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the difference between the regression we were doing before
    and the classification we’re doing here is quite minor. The only changes are shown
    in bold. In particular, instead of importing the `KNeighborsRegressor` module,
    we import the `KNeighborsClassifier` module. Both modules use k-NN, but one is
    designed for regression and the other for classification. We name our variable
    `knnclassifier` instead of `knnregressor`, but beyond that, the supervised learning
    process is just the same: importing a supervised learning module, splitting data
    into training and test sets, fitting the model to a training dataset, and finally
    using the fit model for predictions on a test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should remember from Chapter 5 that we usually measure accuracy differently
    in classification scenarios than we do in regression scenarios. The following
    snippet creates a confusion matrix, just like the ones we made in Chapter 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that the output of this code is a confusion matrix that shows the
    number of true positives, true negatives, false positives, and false negatives
    on our test set. The confusion matrix looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that every confusion matrix has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'So, when we look at our confusion matrix, we find that our model made 2,703
    true-positive classifications: our model predicted above-median shares for 2,703
    articles, and those articles did have above-median shares. We have 2,280 false
    positives: predictions of above-median shares for articles that instead had below-median
    shares. We have 2,370 false negatives: predictions for below-median shares for
    articles that instead had above-median shares. Finally, we have 2,558 true negatives:
    predictions of below-median shares for articles that did have below-median shares.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate our precision and recall as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see that our precision is equal to about 0.53, and our recall is equal
    to about 0.52\. These are not extremely encouraging values; precision and recall
    are supposed to be as close to 1 as possible. One reason these values are so low
    is that we’re trying to make difficult predictions. It’s inherently hard to know
    the number of shares an article will get, no matter how good your algorithms are.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that even though supervised learning is a sophisticated
    set of methods based on ingenious ideas and executed on powerful hardware, it’s
    not magic. Many things in the universe are inherently difficult to predict, even
    when using the best-possible methods. But just because perfect prediction may
    be impossible doesn’t mean we shouldn’t try to make predictions at all. In this
    case, a model that helps us even a little bit is better than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored supervised learning. We started with a business
    scenario related to prediction. We reviewed linear regression, including its shortcomings.
    We then talked about supervised learning in general and introduced several other
    supervised learning methods. We went on to discuss some finer points of supervised
    learning, including multivariate supervised learning and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll discuss supervised learning’s less popular younger
    sibling: unsupervised learning. Unsupervised learning gives us powerful ways to
    explore and understand hidden relationships in data, without even using a target
    variable for supervision. Supervised learning and unsupervised learning together
    make up the bulk of machine learning, one of the most essential data science skills.'
  prefs: []
  type: TYPE_NORMAL
