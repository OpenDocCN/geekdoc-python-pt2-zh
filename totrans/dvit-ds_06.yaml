- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Supervised Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: 'Computer scientists use the term *supervised learning* to refer to a broad
    range of quantitative methods that predict and classify. In fact, you’ve already
    done supervised learning: the linear regression you did in Chapter 2 and the LPMs
    and logistic regression from Chapter 5 are all instances of supervised learning.
    By learning those methods, you’ve already become familiar with the basic ideas
    of supervised learning. This chapter introduces some advanced supervised learning
    methods and discusses the idea of supervised learning in general. We’re dwelling
    on this topic so much because it’s such a crucial component of data science.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学家使用术语*监督学习*来指代一系列预测和分类的定量方法。事实上，你已经做过监督学习：你在第二章做的线性回归以及第五章中的LPM和逻辑回归都属于监督学习的实例。通过学习这些方法，你已经熟悉了监督学习的基本概念。本章介绍了一些先进的监督学习方法，并讨论了监督学习的一般概念。我们如此详细地探讨这一话题，因为它是数据科学中至关重要的组成部分。
- en: We’ll start by introducing yet another business challenge and describing how
    supervised learning can help us resolve it. We’ll talk about linear regression
    as an imperfect solution and discuss supervised learning in general. Then we’ll
    introduce k-NN, a simple but elegant supervised learning method. We’ll also briefly
    introduce decision trees, random forests, and neural networks, and discuss how
    to use them for prediction and classification. We’ll close with a discussion of
    how to measure accuracy and what unites each of these disparate methods.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍另一个商业挑战，描述监督学习如何帮助我们解决这个问题来开始。我们将讨论线性回归作为一个不完美的解决方案，并一般性地讨论监督学习。接下来，我们将介绍k-NN，这是一种简单而优雅的监督学习方法。我们还将简要介绍决策树、随机森林和神经网络，并讨论如何使用它们进行预测和分类。最后，我们将讨论如何衡量准确性以及这些不同方法的共同点。
- en: Predicting Website Traffic
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测网站流量
- en: 'Imagine that you’re running a website. Your website’s business model is simple:
    you post articles on interesting topics, and you earn money from the people who
    view your website’s articles. Whether your revenue comes from ad sales, subscriptions,
    or donations, you earn money in proportion to the number of people who visit your
    site: the more visitors, the higher your revenue.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你经营一个网站。你的网站商业模式很简单：你发布有趣话题的文章，通过访问你网站文章的人赚取收入。无论收入来自广告销售、订阅还是捐赠，你的收入与访问你网站的人数成正比：访问者越多，收入越高。
- en: Amateur writers submit articles to you with the hope that you’ll publish them
    on your site. You receive an enormous number of submissions and can’t possibly
    read, much less publish, everything you receive. So you have to do some curation.
    You may consider many factors as you’re deciding what to publish. Of course, you’ll
    try to consider the quality of submitted articles. You’ll also want to consider
    which articles fit with the “brand” of your site. But in the end, you’re trying
    to run a business, and maximizing your site’s revenue will be crucial to ensuring
    your business’s long-term survival. Since you earn revenue in proportion to the
    number of visitors to your site, maximizing revenue will depend on selecting articles
    to publish that are likely to get many visitors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 非专业作家将文章提交给你，希望你能在网站上发布它们。你收到大量的投稿，根本不可能阅读，更不用说发布所有收到的文章。因此，你必须进行一些筛选。在决定发布哪些文章时，你可能会考虑许多因素。当然，你会尽量考虑提交文章的质量。你还会考虑哪些文章与网站的“品牌”相符。但最终，你的目标是经营一家成功的企业，最大化网站的收入对确保企业的长期生存至关重要。由于你的收入与访问你网站的人数成正比，最大化收入将取决于选择那些可能会吸引大量访问者的文章进行发布。
- en: You could try to rely on intuition to decide which articles are likely to receive
    many visitors. This would require either you or your team to read every submission
    and make difficult judgments about which articles are likely to attract visitors.
    This would be extremely time-consuming, and even after spending all that time
    reading articles, it’s far from certain that your team would have the right judgment
    about which articles will attract the most visitors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试依靠直觉来决定哪些文章可能会获得更多的访问者。这需要你或你的团队阅读每一篇提交的文章，并作出关于哪些文章可能吸引访问者的艰难判断。这将非常耗时，即使在花费大量时间阅读文章之后，也不能完全确定你的团队会做出正确的判断，知道哪些文章会吸引最多的访问者。
- en: 'A faster and potentially more accurate approach to this problem is through
    supervised learning. Imagine that you could write code to read articles for you
    as soon as they arrived in your inbox and could then use information that the
    code gleans from each submitted article to accurately predict the number of visitors
    it will attract, before you publish it. If you had code like that, you could even
    fully automate your publishing process: a bot could read submissions from emails,
    predict the likely revenue expected from every submitted article, and publish
    every article that had an expected revenue above a particular threshold.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个更快速且可能更准确的方法是通过监督学习。想象一下，你能够编写代码，在文章到达你的邮箱后立即阅读它们，并利用代码从每篇提交的文章中获取的信息，准确预测它将吸引的访客数量，在发布之前。如果你有这样的代码，你甚至可以完全自动化你的发布过程：一个机器人可以从邮件中读取提交内容，预测每篇提交文章的预期收入，并发布每篇预期收入超过特定阈值的文章。
- en: The hardest part of that process would be predicting an article’s expected revenue;
    that’s the part we need to rely on supervised learning to accomplish. In the rest
    of the chapter, we’ll go through the steps required for the supervised learning
    that would enable this kind of automated system to predict the number of visitors
    a given article will attract.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程最难的部分是预测文章的预期收入；这是我们需要依赖监督学习来完成的部分。在本章的其余部分，我们将介绍实现这种自动化系统所需的监督学习步骤，以预测某篇文章将吸引的访客数量。
- en: Reading and Plotting News Article Data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和绘制新闻文章数据
- en: Like most data science scenarios, supervised learning requires us to read in
    data. We’ll read in a dataset that’s available for free from the University of
    California, Irvine (UCI) Machine Learning Repository ([https://archive-beta.ics.uci.edu/](https://archive-beta.ics.uci.edu/)).
    This repository contains hundreds of datasets that machine learning researchers
    and enthusiasts can use for research and fun.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数数据科学场景一样，监督学习要求我们读取数据。我们将读取一个可以免费获得的数据集，来自加利福尼亚大学欧文分校（UCI）机器学习库（[https://archive-beta.ics.uci.edu/](https://archive-beta.ics.uci.edu/)）。该库包含了数百个数据集，供机器学习研究人员和爱好者用于研究和娱乐。
- en: The particular dataset we’ll use contains detailed information about news articles
    published on Mashable ([https://mashable.com](https://mashable.com)) in 2013 and
    2014\. This Online News Popularity dataset has a web page at [https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity](https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity)
    that presents more information about the data, including its source, the information
    it contains, and papers that have been published containing analyses of it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的特定数据集包含有关2013年和2014年在 Mashable 网站上发布的新闻文章的详细信息（[https://mashable.com](https://mashable.com)）。这个在线新闻流行度数据集有一个网页
    [https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity](https://archive-beta.ics.uci.edu/dataset/332/online+news+popularity)，提供了更多关于数据的信息，包括数据的来源、包含的信息，以及已有分析的论文。
- en: 'You can obtain a ZIP file of the data from [https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip).
    After you download the ZIP archive, you must extract it on your computer. You’ll
    then see the *OnlineNewsPopularity.csv* file, which is the dataset itself. After
    extracting that *.csv*, you can read it into your Python session as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 [https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip)
    获取该数据的 ZIP 文件。下载 ZIP 压缩包后，你必须将其解压到计算机中。然后你会看到 *OnlineNewsPopularity.csv* 文件，这就是数据集本身。在解压该
    *.csv* 文件后，你可以按如下方式将其读取到 Python 会话中：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We import our old friend the pandas package and read the news dataset into a
    variable called `news`. Each row of `news` contains detailed information about
    one particular article published on Mashable. The first column, `url`, contains
    the URL of the original article. If you visit the URL of a particular article,
    you can see the text and images associated with it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入我们老朋友 pandas 包，并将新闻数据集读取到一个名为 `news` 的变量中。`news` 的每一行包含关于某篇特定文章的详细信息，这些文章都发布在
    Mashable 上。第一列 `url` 包含该文章的原始 URL。如果你访问特定文章的 URL，你可以看到与之相关的文本和图像。
- en: In total, our `news` dataset has 61 columns. Each column after the first contains
    a numeric measurement of something about the article. For example, the third column
    is called `n_tokens_title`. This is a count of the *tokens* in the title, which
    in this case just means the number of words in the title. Many of the columns
    in the `news` dataset have names that refer to advanced methods in *natural language
    processing (NLP)*. NLP is a relatively new field concerned with using computer
    science and mathematical algorithms to analyze, generate, and translate natural
    human language in a way that’s quick and automatic and doesn’t require human effort.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们的 `news` 数据集有61列。每一列从第二列开始，都包含关于文章的某些数值度量。例如，第三列名为 `n_tokens_title`。这是标题中的*token*数量，在这种情况下，指的是标题中的单词数。`news`
    数据集中的许多列名称都涉及到自然语言处理（NLP）中的高级方法。NLP 是一个相对较新的领域，旨在利用计算机科学和数学算法，以快速、自动的方式分析、生成和翻译自然人类语言，而不需要人工干预。
- en: Consider the 46th column, `global_sentiment_polarity`. This column contains
    a measure of each article’s overall *sentiment*, ranging from –1 (highly negative)
    to 0 (neutral) to 1 (highly positive). The ability to automatically measure the
    sentiment of text written in natural human language is one of the recent, exciting
    developments in the world of NLP. The most advanced sentiment analysis algorithms
    are able to closely match humans’ sentiment ratings, so an article about death,
    horror, and sadness will be ranked by both humans and NLP algorithms as having
    a highly negative sentiment (close to –1), while an article about joy, freedom,
    and data analysis will be universally agreed to have a highly positive sentiment
    (close to 1). The creators of our dataset have already run a sentiment analysis
    algorithm to measure the sentiment of each article in the dataset, and the result
    is stored in `global_sentiment_polarity`. Other columns have other measurements,
    including simple things like article length as well as other advanced NLP results.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑第46列，`global_sentiment_polarity`。这一列包含了每篇文章的整体*情感*度量，从 -1（高度负面）到 0（中性）再到
    1（高度正面）。能够自动衡量用自然人类语言写成的文本的情感是自然语言处理（NLP）领域最近令人兴奋的进展之一。最先进的情感分析算法能够与人类的情感评分紧密匹配，因此，一篇关于死亡、恐怖和悲伤的文章，无论是人类还是NLP算法都会将其评定为高度负面的情感（接近
    -1），而一篇关于快乐、自由和数据分析的文章则普遍被认为具有高度正面的情感（接近 1）。我们的数据集创建者已经运行了情感分析算法来衡量数据集中每篇文章的情感，结果存储在
    `global_sentiment_polarity` 中。其他列则包含了其他度量值，包括文章长度等简单的指标以及其他高级的NLP结果。
- en: 'The final column, `shares`, records the number of times each article was shared
    on social media platforms. Our true goal is to increase revenue by increasing
    the number of visitors. But our dataset doesn’t contain any direct measurement
    of either revenue or visitors! This is a common occurrence in the practice of
    data science: we want to analyze something, but our data contains only other things.
    In this case, it’s reasonable to suppose that the number of social media shares
    is correlated with the number of visitors to an article, both because highly visited
    articles will be shared often and because highly shared articles will be visited
    often. And, as we mentioned before, our revenue is directly related to the number
    of website visits. So, we can reasonably suppose that the number of social media
    shares of an article is closely related to the revenue obtained from the article.
    This means that we’ll use shares as a *proxy* for visits and revenue.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一列，`shares`，记录了每篇文章在社交媒体平台上被分享的次数。我们的真正目标是通过增加访问量来增加收入。但我们的数据集中并没有直接测量收入或访问量！这是数据科学实践中的一种常见情况：我们想分析某些内容，但数据中只有其他信息。在这种情况下，合理的推测是，社交媒体分享的次数与文章的访问量相关，因为高访问量的文章会经常被分享，而高度分享的文章也会频繁被访问。正如我们之前提到的，收入直接与网站访问量相关。因此，我们可以合理地假设，文章的社交媒体分享次数与该文章所获得的收入紧密相关。这意味着我们将使用分享次数作为访问量和收入的*代理*。
- en: It will help our analysis if we can determine which features of an article are
    positively related to shares. For example, we might guess that articles with high
    sentiment scores will also get shared frequently, if we believe that people like
    to share happy things. If that’s true, knowing the sentiment of an article will
    help us predict the number of times an article will be shared. By learning how
    to predict shares, we suppose that we’ll be simultaneously learning how to predict
    both visitors and revenue as well. And, if we know the features of a highly shared
    article, we’ll know how to design future articles to maximize our revenue.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能确定哪些文章的特征与分享数呈正相关，那将有助于我们的分析。例如，我们可能会猜测，情感分数较高的文章也会被更频繁地分享，如果我们相信人们喜欢分享愉快的事情的话。如果这是正确的，了解一篇文章的情感倾向将有助于我们预测它的分享次数。通过学习如何预测分享次数，我们假设我们也将同时学会如何预测访客数和收入。而且，如果我们知道一篇文章的高分享特征，我们将知道如何设计未来的文章以最大化我们的收入。
- en: 'As we’ve done before (especially in Chapter 1), we can start with simple exploration.
    We’ll start by drawing a graph. Let’s consider a graph of the relationship between
    sentiment and shares:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们以前做过的那样（特别是在第一章中），我们可以从简单的探索开始。我们将从绘制图表开始。让我们考虑一个情感与分享数之间关系的图表：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You may notice that when we access our dataset’s columns in this Python snippet,
    we put a space at the beginning of every column name. For example, we write `news['
    shares']` instead of `news['shares']` to refer to the column recording the number
    of shares. We do this because that’s the way the column names are recorded in
    the original data file. For whatever reason, that file contains a space before
    every column name instead of the column name alone, so we need to include that
    space when we tell Python to access each column by name. You’ll see these spaces
    throughout the chapter; every dataset has its own quirks, and part of being a
    successful data scientist is being able to understand and adapt to quirks like
    this one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，当我们在这段 Python 代码中访问数据集的列时，我们在每个列名的开头加了一个空格。例如，我们写 `news[' shares']`
    而不是 `news['shares']` 来引用记录分享次数的列。我们这样做是因为原始数据文件中的列名本身就包含了一个空格。无论是什么原因，这个文件中的每个列名前都带有一个空格，而不是只有列名本身，因此我们在告诉
    Python 访问每一列时，需要包含这个空格。你将在本章中看到这些空格；每个数据集都有它自己的特殊之处，而成为一名成功的数据科学家的一部分，就是能够理解并适应这些特殊之处。
- en: '[Figure 6-1](#figure6-1) shows the relationship between sentiment polarity
    and shares.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](#figure6-1)显示了情感极性与分享数之间的关系。'
- en: '![](image_fi/502888c06/f06001.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c06/f06001.png)'
- en: 'Figure 6-1: The relationship between sentiment and shares for every article
    in our dataset'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-1：我们数据集中每篇文章的情感与分享数之间的关系
- en: One thing we can notice about this plot is that, at least to the naked eye,
    no clear linear relationship exists between polarity and shares. High-sentiment
    articles don’t seem to be shared much more than low-sentiment articles, or vice
    versa. If anything, articles close to the middle of the polarity scale (articles
    that have close to neutral sentiment) seem to earn the most shares.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个图中注意到的一点是，至少从肉眼来看，情感极性与分享数之间似乎不存在明显的线性关系。高情感的文章似乎并没有比低情感的文章更频繁地被分享，反之亦然。事实上，情感接近中立的文章（情感较为中性的文章）似乎获得了最多的分享。
- en: Using Linear Regression as a Prediction Method
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性回归作为预测方法
- en: 'We can do a more rigorous test for this (lack of a) linear relationship by
    performing a linear regression, just as we did in Chapters 2 and 5:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行线性回归来对这种（缺乏）线性关系进行更严格的检验，就像我们在第二章和第五章中做的那样：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This snippet performs a linear regression predicting shares using sentiment
    polarity. It does so in the same way we outlined in Chapter 2. We start by importing
    from the module `sklearn.linear_model`, which contains the `LinearRegression()`
    function we want to use. Then, we reshape the data so that the module we’re importing
    can work with it. We create a variable called `regressor`, and we fit the regressor
    to our data. Finally, we print out the coefficient and intercept obtained by fitting
    the regression: 499.3 and 3,335.8.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码执行了一个使用情感极性预测分享数的线性回归。它的执行方式和我们在第二章中概述的相同。我们首先从 `sklearn.linear_model` 模块导入所需的
    `LinearRegression()` 函数。接着，我们对数据进行重塑，以便导入的模块能够与之配合使用。我们创建一个名为 `regressor` 的变量，并将回归器拟合到我们的数据上。最后，我们打印出拟合回归后得到的系数和截距：499.3
    和 3,335.8。
- en: 'You’ll remember from Chapter 2 that we can interpret these numbers as the slope
    and intercept of the regression line, respectively. In other words, our linear
    regression estimates the relationship between sentiment and shares as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得在第 2 章中，我们可以将这些数字解释为回归线的斜率和截距，分别对应于回归方程的系数。换句话说，我们的线性回归估计了情感与股票数之间的关系，如下所示：
- en: '*shares* = 3335.8 + 499.3 · *sentiment*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*shares* = 3335.8 + 499.3 · *sentiment*'
- en: 'We can plot this regression line together with our data as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这条回归线与我们的数据一起绘制，如下所示：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output should look like [Figure 6-2](#figure6-2).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于[图 6-2](#figure6-2)。
- en: '![](image_fi/502888c06/f06002.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c06/f06002.png)'
- en: 'Figure 6-2: A regression line showing the estimated relationship between sentiment
    and shares'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-2：显示情感与股票数之间估计关系的回归线
- en: Our regression line, which should be red if you create the plot at home, appears
    quite flat, showing only a weak relationship between sentiment and shares. Using
    this regression line to predict shares probably wouldn’t help us much, since it
    predicts nearly identical numbers of shares for every sentiment value. We’ll want
    to explore other supervised learning methods that can lead to better, more accurate
    predictions. But first, let’s think about supervised learning in general, including
    what it is about linear regression that makes it a type of supervised learning,
    and what other types of supervised learning could also be applicable to our business
    scenario.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的回归线，如果你在家创建图表的话应该是红色的，看起来非常平坦，显示情感与股票数之间的关系较弱。使用这条回归线来预测股票数可能不会有太大帮助，因为它为每个情感值预测几乎相同的股票数。我们将需要探索其他监督学习方法，以便得到更好、更准确的预测。但首先，让我们先思考一下监督学习的一般概念，包括线性回归为什么是一种监督学习方法，以及其他哪些监督学习方法可能适用于我们的商业场景。
- en: Understanding Supervised Learning
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解监督学习
- en: The linear regression we just did is an example of supervised learning. We’ve
    mentioned supervised learning several times in this chapter, without precisely
    defining it. We can define it as the process of learning a function that maps
    feature variables to target variables. This may not sound immediately obvious
    or clear. To understand what we mean, consider [Figure 6-3](#figure6-3).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才做的线性回归是监督学习的一个例子。在本章中，我们多次提到了监督学习，但没有准确地定义它。我们可以将其定义为学习一个将特征变量映射到目标变量的函数的过程。听起来可能不太直观或清晰。为了理解我们的意思，请参考[图
    6-3](#figure6-3)。
- en: '![](image_fi/502888c06/f06003.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/502888c06/f06003.png)'
- en: 'Figure 6-3: The supervised learning process'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6-3：监督学习过程
- en: 'Think about how this figure applies to the linear regression we completed earlier
    in the chapter. We used sentiment as our only feature (the oval on the left).
    Our target variable was shares (the oval on the right). The following equation
    shows our *learned function* (the arrow in the middle):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想这个图如何应用于我们在本章早些时候完成的线性回归。我们使用情感作为唯一的特征（左侧的椭圆）。我们的目标变量是股票数（右侧的椭圆）。以下方程展示了我们的*学习函数*（中间的箭头）：
- en: '*shares* = 3,335.8 + 499.3 · *sentiment*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*shares* = 3,335.8 + 499.3 · *sentiment*'
- en: 'This function does what every learned function is supposed to do in supervised
    learning: it takes a feature (or multiple features) as its input, and it outputs
    a prediction of the value of a target variable. In our code, we imported capabilities
    from the sklearn module that determined the coefficients, or learned the function,
    for us. (For its part, sklearn learned the function by relying on linear algebra
    equations that are guaranteed to find the coefficients that minimize the mean
    squared error on our target variable, as we discussed in Chapter 2.)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数做了监督学习中每个学习函数应该做的事情：它接受一个（或多个）特征作为输入，并输出目标变量值的预测。在我们的代码中，我们从 sklearn 模块中导入了功能，这些功能为我们确定了系数，或者说学习了这个函数。（在这方面，sklearn
    通过依赖线性代数方程来学习这个函数，这些方程可以保证找到最小化目标变量均方误差的系数，正如我们在第 2 章中讨论的那样。）
- en: The term *supervised learning* refers to the process of determining (learning)
    this function. The target variable is what supervises the process, because as
    we’re determining the learned function, we check whether it leads to accurate
    predictions of the target. Without a target variable, we would have no way to
    learn the function, because we’d have no way to determine which coefficients led
    to high accuracy and which led to low accuracy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习*一词指的是确定（学习）这个函数的过程。目标变量是监督这个过程的关键，因为在我们确定学习到的函数时，我们会检查它是否能准确预测目标值。如果没有目标变量，我们就无法学习函数，因为我们无法判断哪些系数导致了高准确度，哪些导致了低准确度。'
- en: Every supervised learning method you’ll ever use can be described by [Figure
    6-3](#figure6-3). In some cases, we can do *feature engineering*, in which we
    carefully select which variables in our dataset will lead to the most accurate
    possible predictions. In other cases, we’ll adjust our target variable—for example,
    by using a proxy or a transformation of the original variable. But the most important
    part of any supervised learning method is the learned function that maps the features
    to the target. Mastering new supervised learning methods consists of mastering
    new ways to determine these learned functions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用的每一个监督学习方法都可以通过[图6-3](#figure6-3)来描述。在某些情况下，我们可以进行*特征工程*，仔细选择数据集中哪些变量能够带来最准确的预测。在其他情况下，我们会调整目标变量——例如，使用代理变量或对原始变量进行转换。但任何监督学习方法中最重要的部分是学习到的函数，它将特征映射到目标。掌握新的监督学习方法就意味着掌握确定这些学习函数的新方法。
- en: 'When we use linear regression as our chosen supervised learning method, the
    learned function we get is always in the form shown in [Equation 6-1](#equation6-1):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们选择线性回归作为监督学习方法时，得到的学习函数总是呈[方程6-1](#equation6-1)所示的形式：
- en: '*target* = *intercept* + *coefficient*[1] · *feature*[1] + *coefficient*[2]
    · *feature*[2] + … + *coefficient*[*n*] · *feature*[*n*]'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标* = *截距* + *系数*[1] · *特征*[1] + *系数*[2] · *特征*[2] + … + *系数*[*n*] · *特征*[*n*]'
- en: 'Equation 6-1: The general form of every linear regression’s learned function'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6-1：每个线性回归的学习函数的一般形式
- en: For someone who has taken lots of algebra classes, this may seem like a natural
    form for a function to take. Coefficients are multiplied by features and added
    up. When we do this in two dimensions, we get a line, like the line in [Figure
    6-2](#figure6-2).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些上过许多代数课的人来说，这可能是函数采取的自然形式。系数与特征相乘并加总。当我们在二维空间中进行这种操作时，我们得到一条直线，就像[图6-2](#figure6-2)中的直线。
- en: 'However, this is not the only possible form for a learned function. If we think
    more deeply about this form, we can realize that the function for linear regression
    is implicitly expressing an assumed view, or *model*, of the world. In particular,
    linear regression is implicitly assuming that the world can be described by lines:
    that anytime we have two variables *x* and *y*, there’s a way to relate them accurately
    as the line *y* = *a* + *bx*, for some *a* and *b*. Many things in the world can
    be described by lines, but not everything. The universe is a big place, and there
    are many models of the world, many learned functions, and many supervised learning
    methods that can give us more accurate predictions by abandoning this assumption
    of linearity.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是学习到的函数唯一可能的形式。如果我们更深入地思考这种形式，我们会意识到，线性回归的函数隐含地表达了一种假设的世界观或*模型*。特别地，线性回归隐含地假设世界可以用直线来描述：每当我们有两个变量*x*和*y*时，就存在一种准确地将它们关联起来的方式，即直线*y*
    = *a* + *bx*，其中*a*和*b*是某些常数。世界上许多事物可以用直线来描述，但并非所有事物都如此。宇宙是一个庞大的地方，存在许多世界模型、许多学习到的函数，以及许多监督学习方法，通过放弃这种线性假设，我们可以获得更准确的预测。
- en: If the world isn’t described by lines and linear relationships, what model of
    the world is the correct one, or the most accurate or useful one? Many answers
    are possible. For example, instead of a world made up of lines, we could think
    of the world as composed of unique little neighborhoods around points. Instead
    of using a line to make predictions, we could measure characteristics of neighborhoods
    around points, and use those neighborhoods to make predictions. (This approach
    will become clearer in the next section.)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果世界不是通过直线和线性关系来描述的，那么哪种世界模型是正确的，或者最准确或最有用的呢？有许多可能的答案。例如，我们可以将世界视为由围绕点的小区组成，而不是由直线构成。我们可以不用直线来进行预测，而是测量点周围小区的特征，并利用这些小区来进行预测。（这种方法将在下一节中更加清晰。）
- en: 'If everything we observe in the world is related by lines and linear relationships,
    linear regression is the right model for studying it. If the world is instead
    made up of neighborhoods, another supervised learning model is more suitable:
    k-nearest neighbors. We’ll examine this method next.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: k-Nearest Neighbors
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that you have an intern who has never studied statistics, linear regression,
    supervised learning, or data science at any level. You just received a new article
    from an author who wants to be published on your website. You give the intern
    the newly submitted article as well as the `news` dataset and some NLP software.
    You assign the intern to predict the number of times the new article will be shared.
    If your intern predicts a high number of shares, you’ll publish the article. Otherwise,
    you won’t.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Your intern uses NLP software to determine that this article has `global_sentiment_polarity`
    equal to 0.42\. Your intern doesn’t know how to do the linear regression that
    we did at the beginning of the chapter. Instead, they have a simple idea of how
    they’ll predict shares. Their simple idea is to look through the `news` dataset
    until they find an article that closely resembles this new article. If an existing
    article in the dataset closely resembles the newly submitted article, it’s reasonable
    to suppose that the new article’s number of shares will resemble the existing
    article’s number of shares.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose they find an existing article in the dataset that has `global_sentiment_polarity`
    equal to 0.4199\. They’ll conclude, reasonably, that the existing article is similar
    to our new article, because their sentiment ratings are nearly identical. If the
    existing article achieved 1,200 shares, we can expect that our new article, with
    a nearly identical `global_sentiment_polarity`, should have a similar number of
    shares. “Similar articles get similar numbers of shares” is one way to sum up
    this simple thought process. In the context of supervised learning, we can rephrase
    this as “similar feature values lead to similar target values,” though of course
    your intern has never heard of supervised learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re working with numeric data, we don’t need to speak merely qualitatively
    about articles *resembling* each other. We can directly measure the *distance*
    between any two observations in our dataset. The existing article that resembles
    our new article has `global_sentiment_polarity` equal to 0.4199, which is 0.0001
    different from our new article’s `global_sentiment_polarity` of 0.42\. Since `global_sentiment_polarity`
    is the only variable we’ve considered so far, we can say that these two articles
    have a *distance* of 0.0001 between them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: You may think that distance is something that has one nonnegotiable definition.
    But in data science and machine learning, we often find ourselves measuring distances
    that don’t match what we mean by the term in everyday life. In this example, we’re
    using a difference between sentiment scores as our distance, even though it’s
    not a distance that can be walked or measured with a ruler. In other cases, we
    may find ourselves expressing a distance between true and false values, especially
    if we’re doing a classification as in Chapter 5. When we talk about distance,
    we’re often using the term as a loose analogy rather than a literal physical measurement.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为距离是一个有着明确且不可改变定义的概念。但在数据科学和机器学习中，我们常常发现自己测量的距离并不符合我们日常生活中的定义。在这个例子中，我们使用的是情感评分的差异作为我们的距离，尽管这不是一种可以步行或用尺子测量的距离。在其他情况下，我们可能会发现自己在表达真假值之间的距离，特别是当我们在进行分类时，正如第五章所示。我们谈论距离时，通常是将其作为一种宽松的类比，而不是字面上的物理测量。
- en: 'Observations that have a small distance between them can be called *neighbors*,
    and in this case we’ve found two close neighbors. Another article with sentiment
    0.41 would have distance 0.1 from our new article: still a neighbor, but a little
    further down the “street.” For any two articles, we can measure the distance between
    them on all variables that interest us and use this as a measurement of the extent
    to which any two articles are neighbors.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 具有小距离的观测值可以被称为 *邻居*，在这个例子中，我们找到了两个相近的邻居。另一篇情感得分为0.41的文章，与我们新文章的距离是0.1：仍然是一个邻居，但距离“街道”稍微远一些。对于任何两篇文章，我们都可以在所有感兴趣的变量上衡量它们之间的距离，并将此作为衡量它们是邻居的程度。
- en: Instead of considering just one neighbor article, we can consider the entire
    neighborhood surrounding the new article we want to make predictions about. We
    might find the 15 nearest neighbors to our new article—the 15 points in our dataset
    with `global_sentiment_polarity` closest to 0.42\. We can consider the number
    of shares associated with each of those 15 articles. The mean of the number of
    shares achieved by these 15 nearest neighbors is a reasonable prediction for the
    number of shares we can expect our new article to get.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅仅考虑一个邻居文章，而是考虑围绕我们想要预测的新文章的整个邻里。我们可能会找到离新文章最近的15个邻居——数据集中 `global_sentiment_polarity`
    最接近0.42的15个点。我们可以考虑这些15篇文章的分享数量。这15个最近邻的分享数量的平均值，是我们可以合理预测新文章将获得的分享数量。
- en: 'Your intern didn’t think their prediction method was anything special. It just
    seemed like a natural, simple way to make a prediction without using any calculus
    or computer science. However, their simple process is actually a powerful supervised
    learning algorithm called *k-nearest neighbors (k-NN)*. We can describe the whole
    method in four simple steps; truly, it is simplicity itself:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你的实习生认为他们的预测方法没什么特别的，似乎只是一个简单自然的预测方式，没有使用任何微积分或计算机科学的知识。然而，他们的简单过程实际上是一个强大的监督学习算法，叫做
    *k-最近邻（k-NN）*。我们可以用四个简单的步骤来描述整个方法；实际上，它本身就是简单：
- en: Choose a point *p* you want to make a prediction about for a target variable.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个你想要对目标变量进行预测的点 *p*。
- en: Choose a natural number, *k*.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个自然数，*k*。
- en: Find the *k* nearest neighbors to point *p* in your dataset.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到数据集中与点 *p* 最近的 *k* 个邻居。
- en: The mean target value of the *k* nearest neighbors is the prediction for the
    target value of *p*.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*k* 个最近邻的目标值的平均值就是 *p* 的目标值预测。'
- en: 'You may have noticed that the k-NN process doesn’t require any matrix multiplication
    or calculus or really any math at all. Though it’s usually taught in only postgraduate-level
    computer science classes, k-NN is nothing more than a simple idea that children
    and even interns already intuitively grasp: that if things resemble each other
    in some ways, they’re likely to resemble each other in other ways. If things live
    in the same neighborhood, they might be similar to each other.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，k-NN过程不需要任何矩阵乘法、微积分，甚至不需要任何数学。尽管它通常只在研究生级别的计算机科学课程中教授，k-NN其实只是一个简单的思想，孩子甚至实习生都能直观地理解：如果事物在某些方面相似，那么它们在其他方面也很可能相似。如果事物处于同一个邻里，它们可能是相似的。
- en: Implementing k-NN
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现k-NN
- en: 'Writing code for k-NN supervised learning is straightforward. We’ll start by
    defining `k`, the number of neighbors we’ll look at, and `newsentiment`, which
    will hold the `global_sentiment_polarity` of the hypothetical new article we want
    to make a prediction about. In this case, let’s suppose that we receive another
    new article, and this one has a sentiment score of 0.5:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 k-NN 监督学习的代码很直接。我们将首先定义 `k`，即我们将查看的邻居数量，以及 `newsentiment`，它将保存我们想要预测的假设新文章的
    `global_sentiment_polarity`。在这种情况下，假设我们收到另一篇新文章，这篇文章的情感评分为 0.5：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So, we’ll be predicting the number of shares that will be achieved by a new
    article with a sentiment score of 0.5\. We’ll look at the 15 nearest neighbors
    of our new article to make these predictions. It will be convenient to convert
    our polarity and shares data to lists, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将预测一个情感评分为 0.5 的新文章将获得的分享数量。我们将查看与新文章最接近的 15 个邻居来进行这些预测。将极性和分享数据转换为列表会更为方便，如下所示：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we can calculate the distance between every article in our dataset and
    the hypothetical new article:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以计算数据集中每篇文章与假设的新文章之间的距离：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This snippet uses a list comprehension to calculate the absolute value of the
    difference between the sentiment of each existing article and the sentiment of
    our new article.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用了列表推导式来计算每篇现有文章的情感与新文章情感之间差值的绝对值。
- en: 'Now that we have all these distances, we need to find which are the smallest.
    Remember, the articles with the smallest distance to the new article are the nearest
    neighbors, and we’ll use them to make our final prediction. A useful function
    in Python’s NumPy package enables us to easily find the nearest neighbors:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有这些距离，我们需要找出哪些是最小的。记住，距离新文章最近的文章就是最邻近的邻居，我们将利用它们来做最终预测。Python 的 NumPy
    包中的一个有用函数使我们可以轻松找到最近的邻居：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this snippet, we import NumPy and then define a variable called `idx`, which
    is short for *index*. If you run `print(idx[0:k])`, you can see what this variable
    consists of:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们导入了 NumPy，然后定义了一个名为 `idx` 的变量，它是 *index*（索引）的缩写。如果你运行 `print(idx[0:k])`，你可以看到这个变量的内容：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These 15 numbers are the index numbers of the nearest neighbors. The 30,230th
    article in our dataset has the `global_sentiment_polarity` that is closest to
    0.5 out of all articles in the data. The 30,670th article has the `global_sentiment_polarity`
    that’s second closest, and so on. The `argsort()` method we use is a convenient
    method that sorts the distances list from smallest to largest, then provides the
    indices of the `k` smallest distances (the indices of the nearest neighbors) to
    us.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这 15 个数字是最邻近的邻居的索引编号。我们数据集中第 30,230 篇文章的 `global_sentiment_polarity` 是最接近 0.5
    的。第 30,670 篇文章的 `global_sentiment_polarity` 排名第二，依此类推。我们使用的 `argsort()` 方法是一个便捷的方法，它将距离列表从小到大排序，然后提供
    `k` 个最小距离（最邻近的邻居）的索引。
- en: 'After we know the indices of the nearest neighbors, we can create a list of
    the number of shares associated with each neighbor:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们知道了最近邻的索引后，我们可以创建一个包含每个邻居对应分享次数的列表：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our final prediction is just the mean of this list:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终预测只是这个列表的平均值：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You should get the output 7344.466666666666, indicating that past articles with
    sentiment equal to about 0.5 get about 7,344 social media shares, on average.
    If we trust the logic of k-NN, we should expect that any future article that has
    sentiment about equal to 0.5 will also get about 7,344 social media shares.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到输出 7344.466666666666，这表明情感接近 0.5 的过去文章平均获得大约 7,344 次社交媒体分享。如果我们相信 k-NN
    的逻辑，那么我们应该预期，任何未来情感接近 0.5 的文章也将获得大约 7,344 次社交媒体分享。
- en: Performing k-NN with Python’s sklearn
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 的 sklearn 执行 k-NN
- en: 'We don’t have to go through that whole process every time we want to use k-NN
    for prediction. Certain Python packages can perform k-NN for us, including the
    sklearn package, whose relevant module we can import into Python as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要每次想要使用 k-NN 进行预测时都经过那个整个过程。某些 Python 包可以为我们执行 k-NN，包括 sklearn 包，我们可以通过以下方式将其相关模块导入到
    Python 中：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You may be surprised that the module we import here is called `KNeighborsRegressor`.
    We just finished describing how k-NN is very different from linear regression,
    so why would a k-NN module be using the word *regressor* just like a linear regression
    module does?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会惊讶于我们这里导入的模块叫做 `KNeighborsRegressor`。我们刚刚描述了 k-NN 与线性回归非常不同，那么为什么一个 k-NN
    模块会像线性回归模块一样使用 *regressor* 这个词呢？
- en: 'The k-NN method is certainly not linear regression, and it doesn’t use any
    of the matrix algebra that linear regression relies on, and it doesn’t output
    regression lines like linear regression. However, since it’s a supervised learning
    method, it’s accomplishing the same goal as linear regression: determining a function
    that maps features to targets. Since regression was the dominant supervised learning
    method for well over a century, people began to think of *regression* as synonymous
    with *supervised learning*. So people started to call k-NN functions *k-NN regressors*
    because they accomplish the same goal as regression, though without doing any
    actual linear regression.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN方法显然不是线性回归，它也不使用线性回归依赖的任何矩阵代数，也不会像线性回归那样输出回归线。然而，既然它是一个监督学习方法，它实现的目标与线性回归相同：确定一个将特征映射到目标的函数。由于回归是主导了一个多世纪的监督学习方法，人们开始将*回归*视为*监督学习*的同义词。因此，人们开始将k-NN函数称为*k-NN回归器*，因为它们实现的目标与回归相同，尽管没有进行实际的线性回归。
- en: Today, the words *regression* and *regressors* are used for all supervised learning
    methods that make predictions about continuous, numeric target variables, regardless
    of whether they’re actually related to linear regression. Since supervised learning
    and data science are relatively new fields (compared to mathematics, which has
    been around for millennia), many instances of confusing or redundant terminology
    like these remain that haven’t been cleaned up; part of learning data science
    is getting used to these confusing names.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，*回归*和*回归器*这两个词被用于所有关于连续数值型目标变量的监督学习方法，无论它们是否与线性回归直接相关。由于监督学习和数据科学是相对较新的领域（与数学相比，数学已有几千年的历史），许多混淆或冗余的术语仍然存在并未被清理；学习数据科学的一部分就是习惯这些令人困惑的名称。
- en: 'Just as we’ve done with linear regression, we need to reshape our sentiment
    list so that it’s in the format this package expects:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对线性回归所做的那样，我们需要调整情感列表的形状，以便它符合这个包的预期格式：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, instead of calculating distances and indices, we can simply create a “regressor”
    and fit it to our data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再计算距离和索引，而是可以简单地创建一个“回归器”并将其拟合到我们的数据中：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can find the prediction our classifier makes for any sentiment, as long
    as it’s properly reshaped:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只要我们正确地调整了形状，就可以找到分类器对任何情感所做的预测：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This k-NN regressor has predicted that the new article will receive 7,344.46666667
    shares. This exactly matches the number we got before, when doing the k-NN process
    manually. You should be pleased that the numbers match: it means that you know
    how to write code for k-NN at least as well as the authors of the respected and
    popular sklearn package.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个k-NN回归器预测新文章将获得7,344.46666667次分享。这与我们之前在手动进行k-NN过程时得到的结果完全一致。你应该为结果匹配而感到高兴：这意味着你至少和受尊敬的、流行的sklearn包的作者一样，知道如何编写k-NN的代码。
- en: Now that you’ve learned a new supervised learning method, think about how it’s
    similar to and different from linear regression. Both linear regression and k-NN
    rely on feature variables and a target variable, as shown in [Figure 6-3](#figure6-3).
    Both create a learned function that maps feature variables to the target variable.
    In the case of linear regression, the learned function is a linear sum of variables
    multiplied by coefficients, in the form shown in [Equation 6-1](#equation6-1).
    In the case of k-NN, the learned function is a function that finds the mean target
    value for *k* nearest neighbors in the relevant dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学习了一种新的监督学习方法，试着思考它与线性回归有何相似之处与不同之处。线性回归和k-NN都依赖特征变量和目标变量，如[图6-3](#figure6-3)所示。它们都创建了一个学习到的函数，将特征变量映射到目标变量。在线性回归的情况下，学习到的函数是变量与系数相乘后的线性和，形式如[方程6-1](#equation6-1)所示。在k-NN的情况下，学习到的函数是一个寻找相关数据集中*k*个最近邻居的均值目标值的函数。
- en: While linear regression implicitly expresses a model of the world in which all
    variables can be related to each other by lines, k-NN implicitly expresses a model
    of the world in which neighborhoods of points are all similar to each other. These
    models of the world, and the learned functions they imply, are quite different.
    Because the learned functions are different, they could make different predictions
    about numbers of article shares or anything else we want to predict. But the goal
    of accurately predicting a target variable is the same in both cases, and so both
    are commonly used supervised learning methods.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然线性回归隐式地表达了一个模型，其中所有变量可以通过直线相互关联，但k-NN隐式地表达了一个模型，其中点的邻域彼此相似。这些世界模型及其所暗示的学习函数是截然不同的。由于学习到的函数不同，它们可能会对文章分享次数或我们想要预测的其他任何内容做出不同的预测。但准确预测目标变量的目标在两者中是相同的，因此这两者都是常用的监督学习方法。
- en: Using Other Supervised Learning Algorithms
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用其他监督学习算法
- en: 'Linear regression and k-NN are only two of many supervised learning algorithms
    that can be used for our prediction scenario. The same sklearn package that allowed
    us to easily do k-NN regression can also enable us to use these other supervised
    learning algorithms. [Listing 6-1](#listing6-1) shows how to do supervised learning
    with five methods, each using the same features and target variables, but with
    different supervised learning algorithms (different learned functions):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和k-NN只是我们预测场景中可以使用的众多监督学习算法中的两种。允许我们轻松进行k-NN回归的相同sklearn包，也可以使我们使用这些其他监督学习算法。[列表6-1](#listing6-1)展示了如何使用五种方法进行监督学习，每种方法使用相同的特征和目标变量，但使用不同的监督学习算法（不同的学习函数）：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Listing 6-1: A collection of five supervised learning methods'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6-1：五种监督学习方法的集合
- en: 'This snippet contains five sections of four code lines each. The first two
    sections are for linear regression and k-NN; they’re the same code we ran previously
    to use sklearn’s prebuilt packages to easily get linear regression and k-NN predictions.
    The other three sections have the exact same structure as the first two sections:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段包含五个部分，每个部分有四行代码。前两个部分用于线性回归和k-NN；它们是我们之前运行的相同代码，用于使用sklearn的预构建包轻松获得线性回归和k-NN预测。其他三个部分与前两个部分具有完全相同的结构：
- en: Import the package.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包。
- en: Define a “regressor.”
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个“回归器”。
- en: Fit the regressor to our data.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回归器拟合到我们的数据。
- en: Use the fitted regressor to print a prediction.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用拟合的回归器打印预测结果。
- en: 'The difference is that each of the five sections uses a different kind of regressor.
    The third section uses a decision tree regressor, the fourth uses a random forest
    regressor, and the fifth uses a neural network regressor. You may not know what
    any of these types of regressors are, but you can think of that as a convenient
    thing: supervised learning is so easy that you can write code to build models
    and make predictions before you even know what the models are! (That’s not to
    say this is a good practice—it’s always better to have a solid theoretical understanding
    of every algorithm you use.)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于五个部分中的每一个都使用不同类型的回归器。第三部分使用决策树回归器，第四部分使用随机森林回归器，第五部分使用神经网络回归器。你可能不知道这些回归器的具体类型，但你可以将其看作一种便利的事情：监督学习如此简单，你甚至可以在不知道模型是什么的情况下编写代码来构建模型并做出预测！(这并不是说这是一个好的实践——通常最好对你使用的每个算法有一个扎实的理论理解。)
- en: Describing every detail of all these supervised learning algorithms goes beyond
    the scope of this book. But we can provide a sketch of the main ideas. Each approach
    accomplishes the same goal (prediction of a target variable), but does it using
    different learned functions. In turn, these learned functions implicitly express
    different assumptions and different math or, in other words, different models
    of the world.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 描述所有这些监督学习算法的每个细节超出了本书的范围。但我们可以提供主要思想的概述。每种方法实现了相同的目标（预测目标变量），但使用不同的学习函数。反过来，这些学习函数隐式地表达了不同的假设和不同的数学，换句话说，不同的世界模型。
- en: Decision Trees
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: 'Let’s begin by looking at decision trees, the first type of model in our code
    after our k-NN section. Instead of assuming that variables are related by lines
    (like linear regression) or by membership in neighborhoods (like k-NN), *decision
    trees* assume that the relationships among variables can be best expressed as
    a tree that consists of binary splits. Don’t worry if that description doesn’t
    sound immediately clear; we’ll use sklearn’s decision tree–plotting function to
    create a plot of the decision tree regressor called `dtregressor` that was created
    by the code in [Listing 6-1](#listing6-1):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see the result in [Figure 6-4](#figure6-4).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06004.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4: A decision tree for predicting article shares based on sentiment'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: We can follow this flowchart to make predictions about shares, given any `global_sentiment_polarity`.
    Since the flowchart has a branching structure that resembles a tree’s branches,
    and since it enables decision-making, we call it a *decision tree*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'We start at the box at the top of the tree. The first line of the box expresses
    a condition: `X[0] <= 0.259`. Here, `X[0]` is referring to the `global_sentiment_polarity`
    variable, which is the only feature in our dataset. If that condition is true,
    we proceed along the leftward arrow to a box on the next lowest level. Otherwise,
    we proceed along the rightward arrow to the other side of the tree. We continue
    to check the conditions in each box until we arrive at a box that specifies no
    condition and has no arrows pointing to other, lower boxes. We then check the
    value specified there and use that as our prediction.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sentiment value we’ve been working with in our example (0.5), we go
    right from the first box because 0.5 > 0.259, then we go right at the second box
    for the same reason, and then we go right yet again at our third box because 0.5
    > 0.263\. Finally, we arrive at the fourth box, which doesn’t have any condition
    to check, and we get our prediction: about 3,979 shares for an article with sentiment
    polarity 0.5.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: If you create this decision tree at home, you’ll see that some of the boxes
    are shaded or colored. This shading is done automatically, and the level of shading
    applied is proportional to the value predicted by the decision tree. For example,
    you can see that one box in [Figure 6-4](#figure6-4) indicates a prediction of
    57,100 shares, and it has the darkest shading. Boxes that predict lower numbers
    of shares will have lighter shading or no shading at all. This automatic shading
    is done to highlight especially high predicted values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: You can find the details of how sklearn creates the decision tree in [Figure
    6-4](#figure6-4) in advanced machine learning textbooks. For most standard business
    use cases, the details and math of optimizing decision trees is not as important
    as the much easier task of writing a few simple Python lines to create one and
    then read its plot.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree in [Figure 6-4](#figure6-4) can be generated with only a few
    lines of code and can be interpreted without any special training. This means
    that decision trees are well suited to business applications. You can quickly
    generate a decision tree and show it to clients or company leaders, and explain
    it without needing to go into any math, computer science, or other difficult topics.
    Because of this, data scientists often say that decision trees are *interpretable
    models*, in contrast to other models like neural networks that are more opaque
    and difficult to quickly understand or explain. A decision tree can be a natural,
    quick addition to any presentation or report that can provide visual interest
    and can help others understand a dataset or prediction problem. These are important
    advantages of decision trees in business applications. On the other hand, decision
    trees tend to have lower accuracy than other, more complex methods like random
    forests (see the next section).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Just like linear regression and k-NN, a decision tree uses a feature of data
    (in this case, sentiment) to make a prediction about a target (in this case, shares).
    The difference is that decision trees don’t rely on an assumption that the variables
    are related by a line (the assumption of linear regression) or that the variables
    are related by small neighborhoods around points (the assumption of k-NN). Instead,
    decision trees are built with the assumption that the branching structure shown
    in [Figure 6-4](#figure6-4) is the appropriate model of the world.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fourth section of [Listing 6-1](#listing6-1) uses *random forests* for prediction.
    Random forests are a type of *ensemble method*. Ensemble methods got their name
    because they consist of a collection of many simpler methods. As you might surmise
    from the name, random forests consist of a collection of simpler decision trees.
    Every time you use a random forest regressor for prediction, the sklearn code
    creates many decision tree regressors. Each of the individual decision tree regressors
    is created with a different subset of the training data and a different subset
    of the training features. The final random forest prediction is the mean of the
    predictions made by each of the many individual decision trees.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of [Figure 6-3](#figure6-3), random forests learn a complicated
    function: one that consists of a mean of many learned functions from multiple
    randomly selected decision trees. Nevertheless, because random forests learn a
    function that maps features to a target variable, they are a standard supervised
    learning method, just like linear regression, k-NN, and all the rest.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Random forests have become popular because their code is relatively easy to
    write and they often have much better accuracy than decision trees or linear regressions.
    These are their main advantages. On the other hand, while we can draw an easily
    interpretable representation of a decision tree, like [Figure 6-4](#figure6-4),
    random forests often consist of hundreds of unique decision trees averaged together,
    and it’s not easy to draw a representation of a random forest in a way a human
    can understand. Choosing a random forest as your supervised learning method will
    probably increase your accuracy, but at the cost of interpretability and explainability.
    Every supervised learning method has advantages and disadvantages, and choosing
    the right trade-offs that are appropriate for your situation is important for
    any data scientist who wants to succeed at supervised learning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Neural networks* have become extremely popular in recent years as our computer
    hardware has matured to the point of being able to handle their computational
    complexity. The complexity of neural networks also makes them hard to describe
    succinctly, except to say that we can use them for supervised learning. We can
    start by showing a diagram of one particular neural network ([Figure 6-5](#figure6-5)).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/502888c06/f06005.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-5: A diagram of a neural network'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: This plot is a representation of a neural network’s learned function. In this
    plot, you can see a column of 13 circles, called *nodes*, on the left side. These
    13 nodes are collectively called the *input layer* of the neural network. Each
    node of the input layer represents one feature of the training data. The single
    node on the far right represents the neural network’s final prediction of a target
    variable. All of the lines and nodes between the left and right represent a complex
    learned function that maps feature inputs to the final target prediction.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can see that the topmost node in the leftmost column (labeled
    *A*) has an arrow pointing to another node (labeled *B*), with the number 4.52768
    written near it. This number is a *weight*, and we’re supposed to multiply this
    weight by the value of the feature corresponding to node A. We then add the result
    of that multiplication to a running total that corresponds to node B. You can
    see that node B has 13 arrows pointing to it, one for each node in the input layer.
    Each feature will be multiplied by a different weight, and the product of the
    feature value and the weight will be added to the running total for node B. Then,
    the number –0.14254 will be added to the result; this is the number drawn on an
    arrow between a blue node with a 1 inside it and node B. (This blue node is also
    called a *bias node*.)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'After all this multiplication and addition, we’ll have a running total for
    node B, and we’ll apply a new function called an *activation function* to it.
    Many possible activation functions exist, one of which is the logistic function
    you met in Chapter 5. After we apply our activation function, we’ll have a final
    numeric value for node B. We’ve only barely begun the process of calculating the
    neural network’s learned function. You can see that node B has four arrows emanating
    from it, each pointing to other nodes further to the right. For each of those
    arrows, we’ll have to follow the same steps of multiplying weights by node values,
    adding to running totals for every node, and applying activation functions. After
    we do this for all the nodes and all the arrows in the diagram, we’ll have a final
    value for the rightmost node: this will be our prediction of the target value.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are designed in such a way that this whole process, including
    repeated multiplication and addition and activation functions, should give us
    a highly accurate prediction as its final output. The complexity of neural networks
    can be a challenge, but it’s also what enables them to accurately model our complex
    nonlinear world.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: These networks are called *neural* because the nodes and arrows in [Figure 6-5](#figure6-5)
    resemble the neurons and synapses in a brain. This resemblance is mostly superficial.
    You could depict neural networks in a way that didn’t look like a brain, or you
    could write down other methods like linear regression in a way that did look like
    a brain.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: To really master neural networks, you need to learn a lot more. Some of the
    interesting advances in neural networks have come from experimenting with different
    structures, or *architectures*, of the nodes. For example, *deep neural networks*
    have many layers between the leftmost input nodes and the rightmost output. *Convolutional
    neural networks* add an extra type of layer that performs a special operation
    called *convolution* to the network structure. *Recurrent neural networks* allow
    connections to flow in multiple directions, instead of just from left to right.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have found remarkable applications for neural networks in computer
    vision (like recognizing a dog or a cat or a car or a person), language processing
    (like machine translation and speech recognition), and much more. On the other
    hand, neural networks are hard to interpret, hard to understand, and hard to train
    properly, and they sometimes require specialized hardware. These downsides sometimes
    make neural networks an unattractive option in business applications despite their
    power.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Prediction Accuracy
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whichever supervised learning model we choose, after we fit it, we’ll want
    to measure its prediction accuracy. Here is how we do it for our scenario of predicting
    shares of articles:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This simple snippet calculates the MAE, as we’ve done before. In the first
    line, we use our regressor’s `predict()` method to predict the number of shares
    for each article in our dataset. (Remember, this `regressor` is the linear regression
    model we created near the beginning of the chapter. If you’d like, you can replace
    `regressor` with `rfregressor` or `nnregressor` to measure the accuracy of our
    random forest or our neural network, respectively.) In the second line, we calculate
    the prediction error of these predictions: this is simply the absolute value of
    the difference between predicted and actual. The mean of our prediction error,
    calculated in the third line, is a measurement of how well our particular supervised
    learning method performed, where 0 is the best-possible value, and higher values
    are worse. We can use this process to calculate prediction accuracy for many supervised
    learning algorithms, and then choose the algorithm that leads to the highest accuracy
    (the lowest mean absolute error) as the best method for our scenario.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The only problem with this approach is that it doesn’t resemble a true prediction
    scenario. In real life, we’d have to make predictions for articles that were not
    in our training dataset—articles that our regressor had never seen during its
    training process. In contrast, we’ve taken a dataset of articles from 2013 and
    2014, fit a regressor to that whole dataset, and then judged our accuracy based
    on the same 2013–14 dataset that was used to fit our regressor. Since we judged
    our accuracy based on the same data that was used to fit our regressor, what we’ve
    done isn’t truly prediction. It’s *postdiction—*saying what happened after it
    happened instead of before. When we do postdiction, we’re liable to be guilty
    of overfitting, the dastardly peril that we already encountered in Chapter 2.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the problems of postdiction and overfitting, we can take the same
    approach we took in Chapter 2: split our dataset into two mutually exclusive subsets,
    a training set and a test set. We use the training set to train the data or, in
    other words, to allow our supervised learning model to learn its learned function.
    After we train the data using only the training dataset, we test it using the
    test data. The test data, since it wasn’t used in the training process, is “as
    if” from the future, since our regressor hasn’t used it for learning, even if
    it’s actually from the past.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'The sklearn package has a convenient function we can use to split our data
    into training and test sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The four outputs of this snippet are `trainingx` and `trainingy`—the *x* and
    *y* components of our training data—and `testx` and `testy`—the *x* and *y* components
    of our test data. Let’s check the length of each of these outputs:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can see that our training data consists of `trainingx` (the sentiment scores
    of the training examples) and `trainingy` (the share statistics of the training
    examples). Both of these training datasets consist of 29,733 observations, or
    75 percent of the data. The test datasets (`testx` and `testy`) consist of 9,911
    observations, or the other 25 percent. This type of split follows the same approach
    that we took in Chapter 2: training our model with the majority of the data and
    testing our model with a smaller minority of the data.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'One important difference between the training/test split we did here and the
    training/test split we did in Chapter 2 is that, in Chapter 2, we used earlier
    data (the first years of our dataset) as training data and later data (the last
    years of our dataset) as test data. Here, we don’t do a before/after split for
    our training and test data. Instead, the `train_test_split()` function we used
    performs a random split: randomly choosing training and test sets, instead of
    neatly selecting from earlier and later times. This is an important distinction
    to remember: for time-series data (data recorded at regular, ordered intervals),
    we choose training and test sets based on a split between earlier and later data,
    but for all other datasets, we select training and test sets randomly.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to train our models by using these training sets, and we need
    to calculate prediction error by using these test sets:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You can see in this snippet that we fit the regressor by using only the training
    data. Then we calculate the prediction error by using only the test data. Even
    though all of our data comes from the past, by making predictions for data that
    weren’t included in our training, we’re making sure that our process resembles
    a true prediction process instead of postdiction.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: We can see the error on the test set by running `print(np.mean(predictionerror))`.
    You’ll see that the mean prediction error on our test set is about 3,816 when
    using our random forest regressor.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also do the same with our other regressors. For example, this is how
    we can check the prediction error of our k-NN regressor:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Again, we can use `print(np.mean(predictionerror))` to find out whether this
    method seems to perform better than our other supervised learning methods. When
    we do, we find that our k-NN regressor has a mean prediction error equal to about
    3,292 on the test set. In this case, k-NN has better performance than random forests,
    as measured by prediction error on the test set. When we want to choose the best
    supervised learning method for a particular scenario, the simplest way to do it
    is to choose the one with the lowest prediction error *on a test set*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Working with Multivariate Models
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in this chapter, we’ve worked with only univariate supervised learning,
    meaning that we’ve used only one feature (sentiment) to predict shares. Once you
    know how to do univariate supervised learning, jumping to *multivariate supervised
    learning*, where we use multiple features to predict a target, is completely straightforward.
    All we need to do is specify more features in our *x* variable, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we specify an `x` variable that contains not only the sentiment of an
    article but also two other features from other columns in our dataset. After that,
    the process is the same as we’ve followed before: splitting into a training and
    test set, creating and fitting a regressor using a training set, and calculating
    prediction error on a test set. When we run `print(np.mean(predictionerror))`
    now, we see that our multivariate model has a mean prediction error equal to about
    3,474, indicating that our multivariate random forest model performs better than
    our univariate random forest model on our test set.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Using Classification Instead of Regression
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, this whole chapter has presented various ways to predict shares, given
    different features of an article. The `shares` variable can take any integer value
    from 0 to infinity. For data like that (continuous, numeric variables), it’s appropriate
    to use regression to predict the values it will take. We used linear regression,
    k-NN regression, decision tree regression, random forest regression, and neural
    network regression: five supervised learning methods, all of them used to predict
    targets that can take a wide range of values.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Instead of doing prediction and regression, we may want to do categorical classification,
    as we did in Chapter 5. In our business scenario, we might not be interested in
    predicting a precise number of shares. Instead, we may be interested only in whether
    an article will reach a number of shares that’s higher than the median number.
    Deciding whether something is above or below a median is a classification scenario,
    since it consists of deciding true/false to a question with only two possible
    answers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a variable that enables us to do classification as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, we create a `themedian` variable that represents the median value of
    shares in our dataset. Then we add a new column to the `news` dataset called `abovemedianshares`.
    This new column is 1 when an article’s share count is above the median, and it’s
    0 otherwise. This new measurement is derived from a numeric measurement (number
    of shares), but we can think of it as a categorical measurement: one that expresses
    a true/false proposition of whether an article is in the high-share category.
    Since our business goal is to publish high-share articles and not publish low-share
    articles, being able to accurately classify new articles as likely high-share
    articles or likely low-share articles would be useful to us.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform classification instead of regression, we need to change our supervised
    learning code. But luckily, the changes we have to make are minor. In the following
    snippet, we use classifiers instead of regressors for our new categorical target
    variable:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can see that the difference between the regression we were doing before
    and the classification we’re doing here is quite minor. The only changes are shown
    in bold. In particular, instead of importing the `KNeighborsRegressor` module,
    we import the `KNeighborsClassifier` module. Both modules use k-NN, but one is
    designed for regression and the other for classification. We name our variable
    `knnclassifier` instead of `knnregressor`, but beyond that, the supervised learning
    process is just the same: importing a supervised learning module, splitting data
    into training and test sets, fitting the model to a training dataset, and finally
    using the fit model for predictions on a test set.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'You should remember from Chapter 5 that we usually measure accuracy differently
    in classification scenarios than we do in regression scenarios. The following
    snippet creates a confusion matrix, just like the ones we made in Chapter 5:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Remember that the output of this code is a confusion matrix that shows the
    number of true positives, true negatives, false positives, and false negatives
    on our test set. The confusion matrix looks like this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Remember that every confusion matrix has the following structure:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'So, when we look at our confusion matrix, we find that our model made 2,703
    true-positive classifications: our model predicted above-median shares for 2,703
    articles, and those articles did have above-median shares. We have 2,280 false
    positives: predictions of above-median shares for articles that instead had below-median
    shares. We have 2,370 false negatives: predictions for below-median shares for
    articles that instead had above-median shares. Finally, we have 2,558 true negatives:
    predictions of below-median shares for articles that did have below-median shares.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate our precision and recall as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You’ll see that our precision is equal to about 0.53, and our recall is equal
    to about 0.52\. These are not extremely encouraging values; precision and recall
    are supposed to be as close to 1 as possible. One reason these values are so low
    is that we’re trying to make difficult predictions. It’s inherently hard to know
    the number of shares an article will get, no matter how good your algorithms are.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that even though supervised learning is a sophisticated
    set of methods based on ingenious ideas and executed on powerful hardware, it’s
    not magic. Many things in the universe are inherently difficult to predict, even
    when using the best-possible methods. But just because perfect prediction may
    be impossible doesn’t mean we shouldn’t try to make predictions at all. In this
    case, a model that helps us even a little bit is better than nothing.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored supervised learning. We started with a business
    scenario related to prediction. We reviewed linear regression, including its shortcomings.
    We then talked about supervised learning in general and introduced several other
    supervised learning methods. We went on to discuss some finer points of supervised
    learning, including multivariate supervised learning and classification.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll discuss supervised learning’s less popular younger
    sibling: unsupervised learning. Unsupervised learning gives us powerful ways to
    explore and understand hidden relationships in data, without even using a target
    variable for supervision. Supervised learning and unsupervised learning together
    make up the bulk of machine learning, one of the most essential data science skills.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
