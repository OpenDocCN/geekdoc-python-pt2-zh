<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch11"><span epub:type="pagebreak" id="page_251"/><strong><span class="big">11</span><br/>EVALUATING MODELS</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">So far, we’ve evaluated models by looking at their accuracy on a held-out test set. This is natural and intuitive, but as we’ll learn in this chapter, it’s not all that we can, or should, do to evaluate a model.</p>&#13;
<p class="indent">We’ll begin this chapter by defining metrics and delineating some basic assumptions. Then we’ll look at why we need more than just accuracy. We’ll introduce the concept of a confusion matrix and spend time discussing the metrics we can derive from it. From there, we’ll jump to performance curves, which are the best way to compare different models together. Finally, we’ll extend the idea of a confusion matrix to the multiclass case. We won’t say all there is to say about performance metrics, as this area is still somewhat evolving. However, by the end of this chapter, you’ll be familiar with the sorts of numbers that people involved in machine learning will throw around and have a good understanding of what they mean.</p>&#13;
<h3 class="h3" id="lev1_72">Definitions and Assumptions</h3>&#13;
<p class="noindent">There are many other metrics besides accuracy that we can use to help us evaluate how well a model is performing. These allow us to reasonably compare models. Let’s start by defining the word <em>metric</em>. For us, a metric is a <span epub:type="pagebreak" id="page_252"/>number or set of numbers that represents something about how well the model is doing.</p>&#13;
<p class="indent">The value of the metric increases or decreases as the performance of the model increases or decreases, or possibly vice versa. At times, we’ll be a bit sloppy and refer to graphs as metrics as well since we use them to judge the performance of a model.</p>&#13;
<p class="indent">We’re concerned with evaluating a model for which we have a single held-out test set. We’ll assume that we followed the advice of <a href="ch04.xhtml#ch04">Chapter 4</a> and built three datasets: a training set to teach the model, a validation set to decide when the model was done training, and a held-out test set to evaluate the trained model. We’ve now trained our model, thereby utilizing the training and validation sets, and want to know how well we’ve done.</p>&#13;
<p class="indent">We have another, implicit assumption in this chapter. It’s a crucial one: we assume that the held-out test set is a good representation of the parent distribution that generated the data. Put another way, the held-out test set must represent the sort of data the model will encounter in the wild in as many ways as possible. For example, the frequency with which particular classes appear in the test set should match, as far as is practical, the expected rates that will be encountered when the model is used.</p>&#13;
<p class="indent">This is necessary because the training set is conditioning the model to expect a particular distribution, a particular set of characteristics, and if the data given to the model when it’s used has different characteristics, the model won’t perform well. A difference in distribution between the training set and the set of data presented to the model when it’s used is one of the most common reasons deployed machine learning models fail in actual use.</p>&#13;
<h3 class="h3" id="lev1_73">Why Accuracy Is Not Enough</h3>&#13;
<p class="noindentb">A binary classifier outputs a single decision for a particular input: class 0 or class 1. Let’s define the following,</p>&#13;
<p class="block1"><em>N</em><sub><em>c</em></sub>, the number of test examples the model correctly classified</p>&#13;
<p class="block1"><em>N</em><sub><em>w</em></sub>, the number of test examples the model got wrong</p>&#13;
<p class="noindenta">Then, the overall accuracy of this model, a number between 0 and 1, is</p>&#13;
<div class="imagec"><img src="Images/252equ01.jpg" alt="image" width="139" height="46"/></div>&#13;
<p class="noindent">This is the accuracy as we have been using it throughout the book. Note, in this chapter, we will use <em>ACC</em> when we mean the overall accuracy.</p>&#13;
<p class="indent">This seems like a pretty reasonable metric, but there are a couple of good reasons not to trust this number too much. For example, <em>N</em><sub><em>c</em></sub> and <em>N</em><sub><em>w</em></sub> tell us nothing about the relative frequency of each class. What if one class is rare? Let’s see how that might affect things.</p>&#13;
<p class="indent">If the model is 95 percent accurate (ACC = 0.95), we might be happy. However, let’s say the frequency (read <em>prior probability</em>) of class 1 is only 5 percent, meaning that on average, if we draw 100 samples from the test set, <span epub:type="pagebreak" id="page_253"/>about 5 of them will be of class 1 and the other 95 will be of class 0. We see that a model that predicts all inputs are of class 0 will be right 95 percent of the time. But consider this: our model might be returning only class 0 for all inputs. If we stick with the overall accuracy, we might think we have a good model when, in fact, we have a terrible model that we could implement in two lines of Python as</p>&#13;
<p class="programs">def predict(x):<br/>&#13;
    return 0</p>&#13;
<p class="indent">In this code, we say that the class is 0 regardless of the input feature vector, <em>x</em>. No one would be satisfied with such a model.</p>&#13;
<p class="indentb">The prior probabilities of the classes affect how we should think about the overall accuracy. However, if we know the following</p>&#13;
<p class="block2"><em>N</em><sub>0</sub>, the number of class 0 instances in our test set</p>&#13;
<p class="block2"><em>N</em><sub>1</sub>, the number of class 1 instances in our test set</p>&#13;
<p class="block2"><em>C</em><sub>0</sub>, the number of class 0 instances our model found</p>&#13;
<p class="block2"><em>C</em><sub>1</sub>, the number of class 1 instances our model found</p>&#13;
<p class="noindenta">we can easily compute the accuracy per class:</p>&#13;
<div class="imagec"><img src="Images/253equ01.jpg" alt="image" width="149" height="227"/></div>&#13;
<p class="noindent">The final expression is just another way to compute the overall accuracy because it tallies all of the correct classifications divided by the number of samples tested.</p>&#13;
<p class="indent">The per class accuracy is better than the overall accuracy because it accounts for any imbalance in the frequency of the respective classes in the test set. For our previous hypothetical test set with the frequency of class 1 at 5 percent, if the classifier were predicting class 0 for all inputs, we would detect it because our per class accuracies would be ACC<sub>0</sub> = 1.0 and ACC<sub>1</sub> = 0.0. This makes sense. We’d get every class 0 sample correct and every class 1 sample wrong (we’d call them class 0 anyway). Per class accuracies will show up again when we consider evaluating multiclass models.</p>&#13;
<p class="indent">A more subtle reason to not just use the overall accuracy is that being wrong might bring a much higher cost than being right. This introduces something outside just the test set: it introduces the meaning we assign to class 0 and class 1. For example, if our model is testing for breast cancer, <span epub:type="pagebreak" id="page_254"/>perhaps using the dataset we created in <a href="ch05.xhtml#ch05">Chapter 5</a>, reporting class 1 (malignant) when, in fact, the sample does not represent a malignant case might cause anxiety for the woman waiting for her test results. With further testing, however, she’ll be shown to not have breast cancer after all. But consider the other case. A benign result that is actually malignant might mean that she will not receive treatment, or receive it too late, which could very well be fatal. The relative cost of one class versus another isn’t the same and might literally mean the difference between life and death. The same could be said of a self-driving car that thinks the child playing in the middle of the road is an empty soda can, or any number of other real-world examples.</p>&#13;
<p class="indent">We use models in the real world, so their outputs are connected to the real world, and sometimes the cost associated with an output is significant. Using just the overall accuracy of a model can be misleading because it does not take the cost of an error into account.</p>&#13;
<h3 class="h3" id="lev1_74">The 2 × 2 Confusion Matrix</h3>&#13;
<p class="noindent">The models we’ve worked with so far have all ultimately assigned each input a class label. For example, a neural network with a logistic output is interpreted as a probability of membership of class 1. Using a typical threshold of 0.5 lets us assign a class label: if the output is &lt; 0.5, call the input class 0; otherwise, call it class 1. For other model types, the decision rule is different (for example, voting in <em>k</em>-NN), but the effect is the same: we get a class assignment for the input.</p>&#13;
<p class="indent">If we run our entire test set through our model and apply the decision rule, we get the assigned class label along with the true class label for each sample. Again, thinking only of the binary classifier case, we have four possible outcomes for each input sample in regards to the assigned class and the true class (see <a href="ch11.xhtml#ch11tab1">Table 11-1</a>).</p>&#13;
<p class="tabcap" id="ch11tab1"><strong>Table 11-1:</strong> Possible Relationships Between the True Class Label and the Assigned Class Label for a Binary Classifier</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:40%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Assigned class</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>True class</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Case</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">True negative (TN)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">False negative (FN)</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">False positive (FP)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">True positive (TP)</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">The <em>Case</em> label defines how we’ll talk about these situations. If the actual class of the input is class 0 <span epub:type="pagebreak" id="page_255"/>and the model assigns class 0, we have a correctly identified negative case, so we have a <em>true negative</em>, or <em>TN</em>. If the actual class is class 1 and the model assigns class 1, we have a correctly identified positive case, so we have a <em>true positive</em>, or <em>TP</em>. However, if the actual class is class 1 and the model assigns class 0, we have a positive case wrongly called a negative case, so we have a <em>false negative</em>, or <em>FN</em>. Finally, if the actual class is 0 and the model assigns class 1, we have a negative case wrongly called a positive case, so we have a <em>false positive</em>, or <em>FP</em>.</p>&#13;
<p class="indent">We can place each of the inputs in our test set into one, and only one, of these cases. Doing this lets us tally the number of times each case appears in the test set, which we can present nicely as a table (see <a href="ch11.xhtml#ch11tab2">Table 11-2</a>).</p>&#13;
<p class="tabcap" id="ch11tab2"><strong>Table 11-2:</strong> Definition of the Class Labels in the 2 × 2 Table</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:60%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Actual class 1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Actual class 0</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Model assigns class 1</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">TP</p></td>&#13;
<td style="vertical-align: top"><p class="tab">FP</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>Model assigns class 0</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">FN</p></td>&#13;
<td style="vertical-align: top"><p class="tab">TN</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">I have placed the case labels (TP, FP, and so forth) in the location where the actual tally counts would go for each case.</p>&#13;
<p class="indent">This table is called a 2 × 2 <em>confusion matrix</em> (or 2 × 2 <em>contingency table</em>). It is 2 × 2 because there are two rows and two columns. It is a confusion matrix because it shows us at a glance how the classifier is performing and, especially, where it is confused. The classifier is confused when it assigns an instance of one class to the other class. In the 2 × 2 table, this confusion shows up as counts that are not along the main diagonal of the table (upper left to lower right). These are the FP and FN entries. A model that performs flawlessly on the test set will have FP = 0 and FN = 0; it will make no mistakes in assigning class labels.</p>&#13;
<p class="indent">In <a href="ch07.xhtml#ch07">Chapter 7</a>, we experimented with the breast cancer dataset built in <a href="ch05.xhtml#ch05">Chapter 5</a>. We reported the performance of classic models against this dataset by looking at their overall accuracy. This is what the sklearn <span class="literal">score</span> method returns. Let’s now instead look at some 2 × 2 tables generated from the test set for these models.</p>&#13;
<p class="indent">The code we are looking at is in the file <em>bc_experiments.py</em>. This code trains multiple classic model types. Instead of using the overall accuracy, however, let’s introduce a new function that computes the entries in the 2 × 2 table (<a href="ch11.xhtml#ch11lis1">Listing 11-1</a>):</p>&#13;
<p class="programs" id="ch11lis1">def tally_predictions(clf, x, y):<br/>&#13;
    p = clf.predict(x)<br/>&#13;
    score = clf.score(x,y)<br/>&#13;
    tp = tn = fp = fn = 0<br/>&#13;
    for i in range(len(y)):<br/>&#13;
        if (p[i] == 0) and (y[i] == 0):<br/>&#13;
            tn += 1<br/>&#13;
<span epub:type="pagebreak" id="page_256"/>        elif (p[i] == 0) and (y[i] == 1):<br/>&#13;
            fn += 1<br/>&#13;
            (*\pagebreak*)<br/>&#13;
        elif (p[i] == 1) and (y[i] == 0):<br/>&#13;
            fp += 1<br/>&#13;
        else:<br/>&#13;
            tp += 1<br/>&#13;
    return [tp, tn, fp, fn, score]</p>&#13;
<p class="figcap"><em>Listing 11-1: Generating tally counts</em></p>&#13;
<p class="indent">This function accepts a trained sklearn model object (<span class="literal">clf</span>), the test samples (<span class="literal">x</span>), and the corresponding actual test labels (<span class="literal">y</span>). The first thing this function does is use the sklearn model to predict a class label for each of the test samples; the result is stored in <span class="literal">p</span>. It then calculates the overall score, and loops over each of the test samples and compares the predicted class label (<span class="literal">p</span>) to the actual known class label (<span class="literal">y</span>) to see if that sample is a true positive, true negative, false positive, or false negative. When done, all of these values are returned.</p>&#13;
<p class="indent">Applying <span class="literal">tally_predictions</span> to the output of <em>bc_experiments.py</em> gives us <a href="ch11.xhtml#ch11tab3">Table 11-3</a>. Here, the sklearn model type is given.</p>&#13;
<p class="tabcap" id="ch11tab3"><strong>Table 11-3:</strong> 2 × 2 Tables for the Breast Cancer Test Set</p>&#13;
<div class="image"><img src="Images/table11-3.jpg" alt="image" width="800" height="266"/></div>&#13;
<p class="indent">In <a href="ch11.xhtml#ch11tab3">Table 11-3</a>, we see four 2 × 2 tables corresponding to the test set applied to the respective models: Nearest Centroid, 3-NN, Decision Tree, and linear SVM. From the tables alone, we see that the best-performing model was the 3-NN as it had only one false positive and no false negatives. This means that the model never called a true malignant case benign and only once called a benign case malignant. Given our discussion in the previous section, we see that this is an encouraging result.</p>&#13;
<p class="indent">Look now at the results for the Nearest Centroid and the Decision Tree. The overall accuracies for these models are 94.7 percent and 93.9 percent, respectively. From the accuracy alone, we might be tempted to say that the Nearest Centroid model is better. However, if we look at the 2 × 2 tables, we see that even though the Decision Tree had more false positives (6), it had only one false negative, while the Nearest Centroid had two false negatives. Again, in this case, a false negative means a missed cancer detection with potentially serious consequences. So, for this dataset, we want to minimize false negatives even if that means we need to tolerate a small increase <span epub:type="pagebreak" id="page_257"/>in false positives. Therefore, we’ll select the Decision Tree over the Nearest Centroid model.</p>&#13;
<h3 class="h3" id="lev1_75">Metrics Derived from the 2 × 2 Confusion Matrix</h3>&#13;
<p class="noindent">Looking at the raw 2 × 2 table is helpful, but even more helpful are the metrics derived from it. Let’s look at several of these in this section to see how they can help us interpret the information in the 2 × 2 table. Before we start, however, we should keep in mind that the metrics we’ll discuss are sometimes a bit controversial. There is still healthy academic debate as to which are best to use when. Our intention here is to introduce them via examples, and to describe what it is that they are measuring. As a machine learning practitioner, you’ll encounter virtually all of these from time to time, so it’s wise to at least be familiar with them.</p>&#13;
<h4 class="h4" id="lev2_98">Deriving Metrics from the 2 × 2 Table</h4>&#13;
<p class="noindent">The first metrics are derived directly from the values in the 2 × 2 table: TP, TN, FP, FN. Think of these as the bread-and-butter metrics. They’re easy to compute and easy to understand. Recall the general form of the 2 × 2 table from <a href="ch11.xhtml#ch11tab2">Table 11-2</a>. We’ll now define two other quantities:</p>&#13;
<div class="imagec"><img src="Images/257equ01.jpg" alt="image" width="328" height="129"/></div>&#13;
<p class="indent">The <em>true positive rate (TPR)</em> is the probability that an actual instance of class 1 will be correctly identified by the model. The TPR is frequently known by other names: <em>sensitivity</em>, <em>recall,</em> and <em>hit rate</em>. You will likely see it referred to as <em>sensitivity</em> in medical literature.</p>&#13;
<p class="indent">The <em>true negative rate (TNR)</em> is the probability that an actual instance of class 0 will be correctly identified by the model. The TNR is also known as the <em>specificity,</em> again, particularly so in medical literature. Both of these quantities, as probabilities, have a value between 0 and 1; higher is better. A perfect classifier will have TPR = TNR = 1.0; this happens when it makes no mistakes so that FP = FN = 0, always.</p>&#13;
<p class="indent">The TPR and TNR need to be understood together to assess a model. For example, we previously mentioned that if class 1 is rare and the model always predicts class 0, it will have high accuracy. If we look at TPR and TNR in that case, we’ll see that the TNR is 1 because the model never assigns an instance of class 0 to class 1 (FP = 0). However, the TPR is 0 for the very same reason, all actual instances of class 1 will be misidentified as false negatives; they get assigned to class 0. Therefore, the two metrics together immediately indicate that the model is not a good one.</p>&#13;
<p class="indent">What about the breast cancer case where a false negative might be fatal? How do we want the TPR and TNR to look in this case? Ideally, of course, <span epub:type="pagebreak" id="page_258"/>we want them to both be as high as possible, but we might be willing to use the model anyway if the TPR is very high while the TNR might be lower. In that situation, we know that actual breast cancers, when presented, are detected almost always. Why? Because the false negative count (FN) is virtually 0, so the denominator of the TPR is about TP, which implies a TPR of about 1.0. If, on the other hand, we tolerate false positives (actual negative instances called malignant by the model), we see that the TNR might be well below 1.0 because the denominator of the TNR includes the FP counts.</p>&#13;
<p class="indent">The TPR and TNR tell us something about the likelihood that the model will pick up actual class 1 and class 0 instances. What it does not tell us, however, is how much faith we should put into the output of the model. For example, if the model says “class 1,” should we believe it? To make that assessment, we need two other metrics derived directly from the 2 × 2 table:</p>&#13;
<div class="imagec"><img src="Images/258equ01.jpg" alt="image" width="392" height="128"/></div>&#13;
<p class="indent">The <em>positive predictive value (PPV)</em> is most often known as the <em>precision</em>. It’s the probability that when the model says the instance is of class 1, it is of class 1. Similarly, the <em>negative predictive value (NPV)</em> is the probability that the model is correct when it claims an instance is of class 0. Both of these values are also numbers between 0 and 1, where higher is better.</p>&#13;
<p class="indent">The only difference between the TPR and the PPV is whether we consider false negatives or false positives in the denominator. By including the false positives, the instances the model says are of class 1 when they are really of class 0; we get the probability that the model output is correct.</p>&#13;
<p class="indent">For the case of a model that always predicts class 0, the PPV is undefined because both the TP and FP are zero. All of the class 1 instances are pushed into the FN count, and the TN count includes all the actual class 0 instances. For the case where TPR is high, but TNR is not, we have a nonzero FP count so that the PPV goes down. Let’s make up an example to see why this is so and how we might understand it.</p>&#13;
<p class="indent">Let’s say that our breast cancer model has produced the following 2 × 2 table (<a href="ch11.xhtml#ch11tab4">Table 11-4</a>).</p>&#13;
<p class="tabcap" id="ch11tab4"><strong>Table 11-4:</strong> A Hypothetical 2 × 2 Table for a Breast Cancer Dataset</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:40%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Actual 1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Actual 0</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Model assigns 1</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">312</p></td>&#13;
<td style="vertical-align: top"><p class="tab">133</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>Model assigns 0</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">645</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_259"/>In this example, the metrics we have covered so far are</p>&#13;
<p class="center"><em>TPR</em> = 0.9811</p>&#13;
<p class="center"><em>TNR</em> = 0.8398</p>&#13;
<p class="center"><em>PPV</em> = 0.7011</p>&#13;
<p class="center"><em>NPV</em> = 0.9908</p>&#13;
<p class="noindent">This means a truly malignant case will be called malignant by the model 98 percent of the time, but a benign case will be called benign only 84 percent of the time. The PPV of 70 percent implies that when the model says “malignant,” there is only a 70 percent chance that the case is malignant; however, because of the high TPR, we know that buried in the “malignant” outputs are virtually all of the actual breast cancer cases. Notice also that this implies a high NPV, so when the model says “benign,” we have very high confidence that the instance is not breast cancer. This is what makes the model useful even if the PPV is less than 100 percent. In a clinical setting, this model will warrant further testing when it says “malignant” but in general, no further testing will likely be needed if it says “benign.” Of course, what acceptable levels of these metrics are depends upon the use case for the model. Some might call an NPV of only 99.1 percent too low given the potentially very high cost of missing a cancer detection. Thoughts like these likely also motivate the recommended frequency of screening.</p>&#13;
<p class="indent">There are two additional basic metrics we can easily derive from the 2 × 2 table:</p>&#13;
<div class="imagec"><img src="Images/259equ01.jpg" alt="image" width="327" height="129"/></div>&#13;
<p class="noindent">These metrics tell us the likelihood that a sample will be a false positive if the actual class is class 0 or a false negative if the actual class is class 1, respectively. The FPR will show up again later when we talk about using curves to assess models. Notice that FPR = 1 – TNR and FNR = 1 – TPR.</p>&#13;
<p class="indent">Calculating these basic metrics is straightforward, especially if we use the output of the <span class="literal">tally_predictions</span> function defined previously as the input (<a href="ch11.xhtml#ch11lis2">Listing 11-2</a>):</p>&#13;
<p class="programs" id="ch11lis2">def basic_metrics(tally):<br/>&#13;
    tp, tn, fp, fn, _ = tally<br/>&#13;
    return {<br/>&#13;
        "TPR": tp / (tp + fn),<br/>&#13;
        "TNR": tn / (tn + fp),<br/>&#13;
        "PPV": tp / (tp + fp),<br/>&#13;
        "NPV": tn / (tn + fn),<br/>&#13;
        (*\pagebreak*)<br/>&#13;
<span epub:type="pagebreak" id="page_260"/>        "FPR": fp / (fp + tn),<br/>&#13;
        "FNR": fn / (fn + tp)<br/>&#13;
    }</p>&#13;
<p class="figcap"><em>Listing 11-2: Calculating basic metrics</em></p>&#13;
<p class="indent">We break up the list returned by <span class="literal">tally_predictions</span>, disregarding the accuracy, and then build and return a dictionary containing each of the six basic metrics we described. Of course, robust code would check for pathological cases where the denominators are zero, but we’ve ignored that code here to preserve clarity in the presentation.</p>&#13;
<h4 class="h4" id="lev2_99">Using Our Metrics to Interpret Models</h4>&#13;
<p class="noindent">Let’s use <span class="literal">tally_predictions</span> and <span class="literal">basic_metrics</span> to interpret some models. We’ll work with the vector form of the MNIST data but keep only digits 3 and 5 so that we have a binary classifier. The code is similar to that found in <em>mnist_experiments.py</em>, which we used in <a href="ch07.xhtml#ch07">Chapter 7</a>.</p>&#13;
<p class="indent">Keeping only digits 3 and 5 leaves us with 11,552 training samples (6,131 3s; 5,421 5s) and 1,902 test samples of which 1,010 are 3s and 892 are 5s. The actual code is in <em>mnist_2x2_tables.py</em> with selected output in <a href="ch11.xhtml#ch11tab5">Table 11-5</a>.</p>&#13;
<p class="tabcap" id="ch11tab5"><strong>Table 11-5:</strong> Selected Output from MNIST 3 vs. 5 Models and Corresponding Basic Metrics</p>&#13;
 <table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>TP</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>TN</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>FP</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>FN</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Nearest Centroid</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">760</p></td>&#13;
<td style="vertical-align: top"><p class="tab">909</p></td>&#13;
<td style="vertical-align: top"><p class="tab">101</p></td>&#13;
<td style="vertical-align: top"><p class="tab">132</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>3-NN</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">878</p></td>&#13;
<td style="vertical-align: top"><p class="tab">994</p></td>&#13;
<td style="vertical-align: top"><p class="tab">16</p></td>&#13;
<td style="vertical-align: top"><p class="tab">14</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Naïve Bayes</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">612</p></td>&#13;
<td style="vertical-align: top"><p class="tab">976</p></td>&#13;
<td style="vertical-align: top"><p class="tab">34</p></td>&#13;
<td style="vertical-align: top"><p class="tab">280</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>RF 500</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">884</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1,003</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>LinearSVM</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">853</p></td>&#13;
<td style="vertical-align: top"><p class="tab">986</p></td>&#13;
<td style="vertical-align: top"><p class="tab">24</p></td>&#13;
<td style="vertical-align: top"><p class="tab">39</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<td style="vertical-align: top"><p class="tab"><strong>Model</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>TPR</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>TNR</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>PPV</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>NPV</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>FPR</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>FNR</strong></p></td>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Nearest Centroid</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8520</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8827</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8732</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1000</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1480</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>3-NN</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9843</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9842</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9821</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9861</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0158</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0157</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Naïve Bayes</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6851</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9663</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9474</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7771</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0337</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3139</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>RF 500</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9910</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9931</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9921</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9921</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0069</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0090</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>LinearSVM</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9563</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9762</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9726</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9620</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0238</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0437</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">In <a href="ch11.xhtml#ch11tab5">Table 11-5</a>, we see the raw counts at the top and the metrics defined in this section at the bottom. Lots of numbers! Let’s parse things a bit to see what’s going on. We’ll concentrate on the metrics at the bottom of the table. The first two columns show the true positive rate (sensitivity, recall) and the true negative rate (specificity). These values should be examined together.</p>&#13;
<p class="indent">If we look at the Nearest Centroid results, we see TPR = 0.8520 and TNR = 0.9000. Here class 1 is a five, and class 0 is a three. So, the Nearest Centroid classifier will call 85 percent of the fives it sees “five.” Similarly, it will call 90 percent of the threes it sees “three.” While not too shabby, we <span epub:type="pagebreak" id="page_261"/>should not be impressed. Looking down the columns, we see that two models performed very well for these metrics: 3-NN and the Random Forest with 500 trees. In both cases, the TPR and TNR were nearly identical and quite close to 1.0. This is a sign of the model performing well. Absolute perfection would be TPR = TNR = PPV = NPV = 1.0 and FPR = FNR = 0.0. The closer we get to perfection, the better. If attempting to pick the best model for this classifier, we would likely choose the Random Forest because it was the closest to perfection on the test set.</p>&#13;
<p class="indent">Let’s look briefly at the Naïve Bayes results. The TNR (specificity) is reasonably high, about 97 percent. However, the TPR (sensitivity) of 68.5 percent is pathetic. Roughly speaking, only two out of every three 5’s presented to this model will be correctly classified. If we examine the next two columns, the positive and negative predictive values, we see a PPV of 94.7 percent, meaning when the model does happen to say the input is a five, we can be somewhat confident that it is a five. However, the negative predictive value isn’t so good at 77.7 percent. Looking at the top portion of <a href="ch11.xhtml#ch11tab5">Table 11-5</a> shows us what is happening in this case. The FP count is only 34 out of 1010 threes in the test set, but the FN count is high: 280 of the fives were labeled “three.” This is the source of the low NPV for this model.</p>&#13;
<p class="indent">Here is a good rule of thumb for these metrics: a well-performing model has TPR, TNR, PPV, and NPV very close to 1.0, and FPR and FNR very close to 0.0.</p>&#13;
<p class="indent">Look again at <a href="ch11.xhtml#ch11tab5">Table 11-5</a>, particularly the lower metrics for the Random Forest. As their names suggest, the FPR and FNR values are rates. We can use them to estimate how often FP and FN will occur when using the model. For example, if we present the model with <em>N</em> = 1,000 cases that are threes (class 0), we can use the FPR to estimate how many of them the model will call fives (class 1):</p>&#13;
<p class="center">estimated number of FP = FPR × N = 0.0069(1000) ≈ 7</p>&#13;
<p class="indent">A similar calculation gives us the estimated number of FN for <em>N</em> = 1000: instances that are really 5’s:</p>&#13;
<p class="center">estimated number of FN = FNR × N = 0.0090(1000) = 9</p>&#13;
<p class="indent">The same holds for the TPR and TNR, which also have “rate” in their names (<em>N</em> = 1000 each for actual threes and fives):</p>&#13;
<p class="center">estimated number of TP = TPR × N = 0.9910(1000) = 991</p>&#13;
<p class="center">estimated number of FN = FNR × N = 0.9931(1000) = 993</p>&#13;
<p class="noindent">These calculations show how well this model performs on the test data.</p>&#13;
<h3 class="h3" id="lev1_76"><span epub:type="pagebreak" id="page_262"/>More Advanced Metrics</h3>&#13;
<p class="noindent">Let’s look in this section at what I’m arbitrarily calling <em>more advanced metrics</em>. I say they are more advanced because instead of using the 2 × 2 table entries directly, they are built from values calculated from the table itself. In particular, we’ll examine five advanced metrics: informedness, markedness, F1 score, Cohen’s kappa, and the Matthews correlation coefficient (MCC).</p>&#13;
<h4 class="h4" id="lev2_100">Informedness and Markedness</h4>&#13;
<p class="noindent"><em>Informedness</em> and <em>markedness</em> go together. They are somewhat less well known than other metrics in this section, but they will hopefully be better known in the future. I said earlier that TPR (sensitivity) and TNR (specificity) should be interpreted together. The informedness (also called Youden’s <em>J</em> statistic) does just that:</p>&#13;
<p class="center">Informedness = TPR + TNR − 1</p>&#13;
<p class="noindent">Informedness is a number in [<em>–</em>1,+1] that combines both the TPR and TNR. The higher the informedness is, the better. An informedness of 0 implies random guessing, while an informedness of 1 implies perfection (on the test set). An informedness of less than 0 might suggest a model that is worse than random guessing. An informedness of –1 implies that all true positive instances were called negatives, and vice versa. In that case, we could swap the label the model wants to assign to each input and get a quite good model. Only pathological models lead to negative informedness values.</p>&#13;
<p class="indent">The markedness combines the positive and negative predictive values in the same way that informedness combines TPR and TNR:</p>&#13;
<p class="center">Markedness = PPV + NPV − 1</p>&#13;
<p class="noindent">We see that it has the same range as informedness. The informedness says something about how well the model is doing at correctly labeling inputs from each class. The markedness says something about how well the model is doing at being correct when it does claim a particular label for a particular input, be it class 0 or class 1. Random guessing will give a markedness near 0 and perfection a markedness near 1.</p>&#13;
<p class="indent">I like that the informedness and markedness each capture essential aspects of the model’s performance in a single number. Some claim that these metrics are unbiased by the prior probabilities of the particular classes. This means if class 1 is significantly less common than class 0, the informedness and markedness are not affected. For in-depth details, see “Evaluation: From Precision, Recall and F-measure to ROC, Informedness, Markedness, and Correlation” by David Martin Powers.</p>&#13;
<h4 class="h4" id="lev2_101"><span epub:type="pagebreak" id="page_263"/>F1 Score</h4>&#13;
<p class="noindent">The <em>F1 score</em>, rightly or wrongly, is widely used, and we should be familiar with it. The F1 score combines two basic metrics into one. Its definition is straightforward in terms of precision (PPV) and recall (TPR):</p>&#13;
<div class="imagec"><img src="Images/263equ01.jpg" alt="image" width="369" height="49"/></div>&#13;
<p class="indent">The F1 score is a number in [0,1], where higher is better. Where does this formula come from? It’s not obvious in this form, but the the F1 score is the harmonic mean of the precision and recall. A <em>harmonic mean</em> is the reciprocal of the arithmetic mean of the reciprocals. Like this,</p>&#13;
<div class="imagec"><img src="Images/263equ02.jpg" alt="image" width="304" height="246"/></div>&#13;
<p class="indent">One criticism of the F1 score is that it does not take the true negatives into account as informedness does (via the TNR). If we look at the definition of PPV and TPR, we see that both of these quantities depend entirely on the TP, FP, and FN counts from the 2 × 2 table, but not the TN count. Additionally, the F1 score places equal weight on the precision and the recall. Precision is affected by false positives, while recall is affected by false negatives. From the previous breast cancer model, we saw that the human cost of a false negative is substantially higher than a false positive. Some argue that this must be taken into account when evaluating model performance, and indeed it should. However, if the relative costs of a false positive and a false negative are the same, the F1 score will have more meaning.</p>&#13;
<h4 class="h4" id="lev2_102">Cohen’s Kappa</h4>&#13;
<p class="noindent"><em>Cohen’s kappa</em> is another statistic commonly found in machine learning. It attempts to account for the possibility that the model might put the input into the correct class by accident. Mathematically, the metric is defined as</p>&#13;
<div class="imagec"><img src="Images/263equ03.jpg" alt="image" width="92" height="47"/></div>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_264"/>where <em>p</em><sub><em>o</em></sub> is the observed accuracy and <em>p</em><sub><em>e</em></sub> is the accuracy expected by chance. For a 2 × 2 table, these values are defined to be</p>&#13;
<div class="imagec"><img src="Images/264equ01.jpg" alt="image" width="420" height="108"/></div>&#13;
<p class="noindent">with N being the total number of samples in the test set.</p>&#13;
<p class="indent">Cohen’s kappa is generally between 0 and 1. 0 means a complete disagreement between the assigned class labels and the given class labels. A negative value indicates worse than chance agreement. A value near 1 indicates strong agreement.</p>&#13;
<h4 class="h4" id="lev2_103">Matthews Correlation Coefficient</h4>&#13;
<p class="noindent">Our final metric is <em>Matthews correlation coefficient (MCC)</em>. It is the geometric mean of the informedness and markedness. In that sense, it is, like the F1 score, a combination of two metrics into one.</p>&#13;
<p class="indent">The MCC is defined as</p>&#13;
<div class="imagec"><img src="Images/264equ02.jpg" alt="image" width="444" height="52"/></div>&#13;
<p class="noindent">which, mathematically, works out to the geometric mean of the informedness and markedness:</p>&#13;
<div class="imagec"><img src="Images/264equ03.jpg" alt="image" width="337" height="21"/></div>&#13;
<p class="indent">The MCC is favored by many because it takes the full 2 × 2 table into account, including the relative frequency of the two classes (the class prior probabilities). This is something that the F1 score does not do because it ignores the true negatives.</p>&#13;
<p class="indent">The MCC is a number between 0 and 1, with higher being better. If considering only one value as a metric for evaluating a binary model, make it the MCC. Note, there are four sums in the denominator of the MCC. If one of these sums is 0, the entire denominator will be 0, which is a problem since we cannot divide by 0. Fortunately, in that case, the denominator can be replaced with 1 to give a still meaningful result. A well-performing model has MCC close to 1.0.</p>&#13;
<h4 class="h4" id="lev2_104">Implementing Our Metrics</h4>&#13;
<p class="noindent">Let’s write a function to construct these metrics from a given 2 × 2 table. The code is shown in <a href="ch11.xhtml#ch11lis3">Listing 11-3</a>:</p>&#13;
<p class="programs" id="ch11lis3">from math import sqrt<br/>&#13;
def advanced_metrics(tally, m):<br/>&#13;
<span epub:type="pagebreak" id="page_265"/>    tp, tn, fp, fn, _ = tally<br/>&#13;
    n = tp+tn+fp+fn<br/>&#13;
    po = (tp+tn)/n<br/>&#13;
    pe = (tp+fn)*(tp+fp)/n**2 + (tn+fp)*(tn+fn)/n**2<br/>&#13;
<br/>&#13;
    return {<br/>&#13;
        "F1": 2.0*m["PPV"]*m["TPR"] / (m["PPV"] + m["TPR"]),<br/>&#13;
        "MCC": (tp*tn - fp*fn) / sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)),<br/>&#13;
        "kappa": (po - pe) / (1.0 - pe),<br/>&#13;
        "informedness": m["TPR"] + m["TNR"] - 1.0,<br/>&#13;
        "markedness": m["PPV"] + m["NPV"] - 1.0<br/>&#13;
    }</p>&#13;
<p class="figcap"><em>Listing 11-3: Calculating advanced metrics</em></p>&#13;
<p class="indent">For the sake of simplicity, we’re not checking if the MCC denominator is 0 as a full implementation would.</p>&#13;
<p class="indent">This code takes the tallies and basic metrics as arguments and returns a new dictionary with the more advanced metrics. Let’s see how our MNIST example from <a href="ch11.xhtml#ch11tab5">Table 11-5</a> looks when we calculate the advanced metrics.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11tab6">Table 11-6</a> shows the metrics of this section for the MNIST 3 versus 5 models. A few things are worth noticing. First, the F1 score is always higher than the MCC or Cohen’s kappa. In a way, the F1 score is overly optimistic. As previously noted, the F1 score does not take the true negatives into account, while both the MCC and Cohen’s kappa do.</p>&#13;
<p class="tabcap" id="ch11tab6"><strong>Table 11-6:</strong> Selected Output from MNIST 3 vs. 5 Models and Corresponding Advanced Metrics</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:20%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
<col style="width:15%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Model</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>F1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>MCC</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Cohen’s <em>κ</em></strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Informedness</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Markedness</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Nearest Centroid</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8671</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7540</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7535</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7520</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7559</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>3-NN</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9832</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9683</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9683</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9685</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9682</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>Naïve Bayes</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7958</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6875</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6631</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6524</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7244</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><em>RF 500</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9916</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9842</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9842</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9841</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9842</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><em>LinearSVM</em></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9644</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9335</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9334</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9325</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9346</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Another thing to note is that well-performing models, like 3-NN and the Random Forest, score highly in all of these metrics. When the model performs well, the difference between the F1 score and MCC is smaller than when the model is doing poorly (Naïve Bayes, for example). Notice also that the MCC is always between the informedness and markedness, as a geometric mean will be. Finally, from the values in <a href="ch11.xhtml#ch11tab5">Table 11-5</a> and <a href="ch11.xhtml#ch11tab6">Table 11-6</a>, we see that the best-performing model is the Random Forest, based on the MCC of 0.9842.</p>&#13;
<p class="indent">In this section, and the two before it, we looked at quite a few metrics and saw how they could be calculated and interpreted. A well-performing model will score highly on all of these metrics. This is the hallmark of a <span epub:type="pagebreak" id="page_266"/>good model. It’s when the models we’re evaluating are less than sterling that the relative differences between the metrics, and the meaning of the metrics, really comes into play. That’s when we need to consider specific metric values and the cost associated with the mistakes the models are making (FP and FN). In those cases, we have to use our judgment and problem-specific factors to decide which model is ultimately selected.</p>&#13;
<p class="indent">Now, let’s shift gears and take a look at a graphical way of evaluating model performance.</p>&#13;
<h3 class="h3" id="lev1_77">The Receiver Operating Characteristics Curve</h3>&#13;
<p class="noindent">They say that a picture is worth a thousand words. In this section, we’ll learn that a picture—more accurately, a curve—can be worth upward of a dozen numbers. That is, we’ll learn how to turn the output of a model into a curve that captures more of the performance than the metrics of the previous sections can. Specifically, we’ll learn about the widely used <em>receiver operating characteristics (ROC) curve</em>: what it is, how to plot it, and how to use sklearn to plot it for us.</p>&#13;
<h4 class="h4" id="lev2_105">Gathering Our Models</h4>&#13;
<p class="noindent">To make the curve, we need a model that outputs a probability of belonging to class 1. In the previous sections, we used models that output a class label so that we could tally the TP, TN, FP, and FN counts. For our ROC curves, we still need these counts, but instead of the class label as model output, we need the probability of class 1 membership. We’ll apply different thresholds to these probabilities to decide what label to give the input.</p>&#13;
<p class="indent">Fortunately for us, traditional neural networks (and the deep networks we will see in <a href="ch12.xhtml#ch12">Chapter 12</a>) output the necessary probability. If we’re using sklearn, other classical models can also be made to output a probability estimate, but we’ll ignore that fact here to keep things simple.</p>&#13;
<p class="indent">Our test case is a series of neural networks trained to decide between even MNIST digits (class 0) and odd MNIST digits (class 1). Our inputs are the vector form of the digits that we’ve been using up to this point in the book. We can use the training and test data we created in <a href="ch05.xhtml#ch05">Chapter 5</a>—we only need to recode the labels so that digits 0, 2, 4, 6, and 8 are class 0, while digits 1, 3, 5, 7, and 9 are class 1. That is easily accomplished with a few lines of code:</p>&#13;
<p class="programs">old = np.load("mnist_train_labels.npy")<br/>&#13;
new = np.zeros(len(old), dtype="uint8")<br/>&#13;
new[np.where((old % 2) == 0)] = 0<br/>&#13;
new[np.where((old % 2) == 1)] = 1<br/>&#13;
np.save("mnist_train_even_odd_labels.npy", new)<br/>&#13;
<br/>&#13;
old = np.load("mnist_test_labels.npy")<br/>&#13;
new = np.zeros(len(old), dtype="uint8")<br/>&#13;
new[np.where((old % 2) == 0)] = 0<br/>&#13;
<span epub:type="pagebreak" id="page_267"/>new[np.where((old % 2) == 1)] = 1<br/>&#13;
np.save("mnist_test_even_odd_labels.npy", new)</p>&#13;
<p class="indent">The directory paths point to the same place the other MNIST data is stored. We use the fact that the remainder when an even number is divided by 2 is always 0 or 1 depending on whether the number is even or odd.</p>&#13;
<p class="indent">What models will we test? To emphasize the difference between the respective models, we’ll intentionally train models that we know are far from ideal. In particular, we’ll use the following code to generate the models and produce the probability estimates:</p>&#13;
<p class="programs">import numpy as np<br/>&#13;
from sklearn.neural_network import MLPClassifier<br/>&#13;
<br/>&#13;
def run(x_train, y_train, x_test, y_test, clf):<br/>&#13;
    clf.fit(x_train, y_train)<br/>&#13;
    return clf.predict_proba(x_test)<br/>&#13;
<br/>&#13;
def nn(layers):<br/>&#13;
    return MLPClassifier(solver="sgd", verbose=False, tol=1e-8,<br/>&#13;
            nesterovs_momentum=False, early_stopping=False, batch_size=64,<br/>&#13;
            learning_rate_init=0.001, momentum=0.9, max_iter=200,<br/>&#13;
            hidden_layer_sizes=layers, activation="relu")<br/>&#13;
<br/>&#13;
def main():<br/>&#13;
    x_train = np.load("mnist_train_vectors.npy").astype("float64")/256.0<br/>&#13;
    y_train = np.load("mnist_train_even_odd_labels.npy")<br/>&#13;
    x_test = np.load("mnist_test_vectors.npy").astype("float64")/256.0<br/>&#13;
    y_test = np.load("mnist_test_even_odd_labels.npy")<br/>&#13;
    x_train = x_train[:1000]<br/>&#13;
    y_train = y_train[:1000]<br/>&#13;
    layers = [(2,), (100,), (100,50), (500,250)]<br/>&#13;
    mlayers = ["2", "100", "100x50", "500x250"]<br/>&#13;
    for i,layer in enumerate(layers):<br/>&#13;
        prob = run(x_train, y_train, x_test, y_test, nn(layer))<br/>&#13;
        np.save("mnist_even_odd_probs_%s.npy" % mlayers[i], prob)</p>&#13;
<p class="indent">The code can be found in the file <em>mnist_even_odd.py</em>. The <span class="literal">run</span> and <span class="literal">nn</span> functions should be familiar. We used virtually identical versions in <a href="ch10.xhtml#ch10">Chapter 10</a>, where <span class="literal">nn</span> returns a configured <span class="literal">MLPClassifier</span> object and <span class="literal">run</span> trains the classifier and returns the prediction probabilities on the test set. The <span class="literal">main</span> function loads the train and test sets, limits the training set to the first 1,000 samples (about 500 even and 500 odd), then loops over the hidden layer sizes we will train. The first two are single hidden layer networks with 2 and 100 nodes, respectively. The last two are two hidden layer networks with 100 × 50 and 500 × 250 nodes per layer.</p>&#13;
<h4 class="h4" id="lev2_106"><span epub:type="pagebreak" id="page_268"/>Plotting Our Metrics</h4>&#13;
<p class="noindent">The output of <span class="literal">clf.predict_proba</span> is a matrix with as many rows as there are test samples (ten thousand in this case). The matrix has as many columns as there are classes; since we’re dealing with a binary classifier, there are two columns per sample. The first is the probability that the sample is even (class 0), and the second is the probability of the sample being odd (class 1). For example, the first 10 outputs for one of the models are shown in <a href="ch11.xhtml#ch11tab7">Table 11-7</a>.</p>&#13;
<p class="tabcap" id="ch11tab7"><strong>Table 11-7:</strong> Example Model Output Showing the Assigned per Class Probabilities Along with the Actual Original Class Label</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:40%"/>&#13;
<col style="width:30%"/>&#13;
<col style="width:30%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Class 0</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Class 1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Actual label</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.009678</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.990322</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.000318</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.999682</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.001531</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.998469</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.007464</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.992536</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.011103</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.988897</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.186362</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.813638</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.037229</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.962771</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.999412</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.000588</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0.883890</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.116110</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">0.999981</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.000019</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">The first column is the probability of being even, and the second is the probability of being odd. The third column is the actual class label for the sample showing that the predictions are spot on. The odd digits have high class 1 probabilities and low class 0 probabilities, while the opposite is true for the even samples.</p>&#13;
<p class="indent">When we build a 2 × 2 table from the performance of a model on a held-out test set, we get a collection of TP, TN, FP, and FN numbers from which we can calculate all the metrics of the previous sections. This includes the true positive rate (TPR, sensitivity) and the false positive rate (FPR, equal to 1 – specificity). Implicit in the table is the threshold we used to decide when the model output should be considered class 1 or class 0. In the previous sections, this threshold was 0.5. If the output is ≥ 0.5, we assign the sample to class 1; otherwise, we assign it to class 0. Sometimes you’ll see this threshold added as a subscript like TPR<sub>0.5</sub> or FPR<sub>0.5</sub>.</p>&#13;
<p class="indent">Mathematically, we can consider the TPR and FPR calculated from a 2 × 2 table to be a point on the FPR (x-axis) versus TPR (y-axis) plane, specifically, the point (FPR, TPR). Since both FPR and TPR range from 0 to 1, the point (FPR, TPR) will lie somewhere within a square of length 1 with the lower-left corner of the square at the point (0,0) and the upper-right corner at the point (1,1). Every time we change our decision threshold, we get a new 2 × 2 table leading to a new point on the FPR versus TPR plane. For example, if we change our decision threshold from 0.5 to 0.3 so that each output <span epub:type="pagebreak" id="page_269"/>class 1 probability of 0.3 or higher is called class 1, we’ll get a new 2 × 2 table and a new point, (FPR<sub>0.3</sub>,TPR<sub>0.3</sub>), on the plane. As we systematically change the decision threshold from high to low, we generate a sequence of points that we can connect to form a curve.</p>&#13;
<p class="indent">Curves generated by changing a parameter in this way are called <em>parametric curves</em>. The points are functions of the threshold. Let’s call the threshold value <em>θ</em> (theta) and vary it from near 1 to near 0. Doing so lets us calculate a set of points, (FPR<sub><em>θ</em></sub>,TPR<sub><em>θ</em></sub>), which, when plotted, lead to a curve in the FPR versus TPR plane. As noted earlier, this curve has a name: the receiver operating characteristics (ROC) curve. Let’s look at an ROC curve and explore what such a curve can tell us.</p>&#13;
<h4 class="h4" id="lev2_107">Exploring the ROC Curve</h4>&#13;
<p class="noindent"><a href="ch11.xhtml#ch11fig1">Figure 11-1</a> shows the ROC curve for the MNIST even-versus-odd model with a single hidden layer of 100 nodes.</p>&#13;
<div class="image" id="ch11fig1"><img src="Images/11fig01.jpg" alt="image" width="666" height="500"/></div>&#13;
<p class="figcap"><em>Figure 11-1: An ROC curve with key elements marked</em></p>&#13;
<p class="indent">The labeled points represent the FPR and TPR for the given threshold values. The dashed line is the diagonal from (0,0) to (1,1). This dashed line represents a classifier that guesses its output randomly. The closer our curve is to this dashed line, the less powerful the model is. If your curve lies on top of the line, you might as well flip a coin and assign the label that way. Any curve below the dashed line is performing <em>worse</em> than random guessing. If the model were entirely wrong, meaning it calls all class 1 instances class 0, and <em>vice versa</em>, a curious thing happens: we can turn the entirely wrong <span epub:type="pagebreak" id="page_270"/>model into a perfectly correct model by changing all class 1 output to class 0 and all class 0, output to class 1. It’s unlikely that you will run across a model this bad.</p>&#13;
<p class="indent">The ROC curve in <a href="ch11.xhtml#ch11fig1">Figure 11-1</a> has a single point labeled <em>perfection</em> in the upper-left corner of the graph. This is the ideal we are striving for. We want our ROC curve to move up and to the left toward this point. The closer we get the curve to this point, the better the model is performing against our test set. A perfect model will have an ROC curve that jumps up vertically to this point and then horizontally to the point (1,1). The ROC curve in <a href="ch11.xhtml#ch11fig1">Figure 11-1</a> is going in the right direction and represents a reasonably well-performing model.</p>&#13;
<p class="indent">Notice the labeled <em>θ</em> values. We can select a level of performance from the model by adjusting <em>θ</em>. In this case, the typical default value of 0.5 gives us the best performance because that threshold value returns a TPR and FPR with the best balance, the point closest to the upper left of the graph. However, there are reasons we might want to use a different <em>θ</em> value. If we make <em>θ</em> small, say 0.1, we move along the curve toward the right. Two things happen. First, the TPR goes up to about 0.99, meaning we correctly assign about 99 percent of the real class 1 instances handed to the model to class 1. Second, the FPR also goes up, to about 0.32, meaning we will simultaneously call about 32 percent of the true negatives (class 0) class 1 as well. If our problem is such that we can tolerate calling some negative instances “positive,” knowing that we now have a meager chance of doing the opposite, calling a positive case “negative,” we might choose to change the threshold to 0.1. Think of the previous breast cancer example: we never want to call a positive case “negative,” so we tolerate more false positives to know we are not mislabeling any actual positives.</p>&#13;
<p class="indent">What does it mean to move the threshold (<em>θ</em>) to 0.9? In this case, we’ve moved along the curve to the left, to a point with a very low false-positive rate. We might do this if we want to know with a high degree of confidence that when the model says “class 1,” it is an instance of class 1. This means we want a high positive predictive value (PPV, precision). Recall the definition of the PPV:</p>&#13;
<div class="imagec"><img src="Images/270equ01.jpg" alt="image" width="134" height="42"/></div>&#13;
<p class="noindent">The PPV is high if FP is low. Setting <em>θ</em> to 0.9 makes the FP low for any given test set. For the ROC curve of <a href="ch11.xhtml#ch11fig1">Figure 11-1</a>, moving to <em>θ</em> = 0.9 implies an FPR of about 0.02 and a TPR of about 0.71 for a PPV of about 0.97. At <em>θ</em> = 0.9, when the model outputs “class 1,” there is a 97 percent chance that the model is correct. In contrast, at <em>θ</em> = 0.1, the PPV is about 76 percent. A high threshold can be used in a situation where we are interested in definitely locating an example of class 1 without caring that we might not detect all class 1 instances.</p>&#13;
<p class="indent">Changing the threshold <em>θ</em> moves us along the ROC curve. As we do so, we should expect the metrics of the previous section to also change as a function of <em>θ</em>. <a href="ch11.xhtml#ch11fig2">Figure 11-2</a> shows us how the MCC and PPV change with <em>θ</em>.</p>&#13;
<div class="image" id="ch11fig2"><span epub:type="pagebreak" id="page_271"/><img src="Images/11fig02.jpg" alt="image" width="673" height="502"/></div>&#13;
<p class="figcap"><em>Figure 11-2: How MCC (circles) and PPV (squares) change as the decision threshold (<em>θ</em>) changes for the MNIST even/odd model of <a href="ch11.xhtml#ch11fig1">Figure 11-1</a></em></p> &#13;
<p class="indent">In the figure, we see that as the threshold goes up, so does the PPV. The model becomes more confident when it declares an input a member of class 1. However, this is tempered by the change in MCC, which, as we previously saw, is an excellent single metric measure of overall model performance. In this case, the highest MCC is at <em>θ</em> = 0.5, with MCC falling off as the threshold increases or decreases.</p>&#13;
<h4 class="h4" id="lev2_108">Comparing Models with ROC Analysis</h4>&#13;
<p class="noindent">The ROC curve gives us a significant amount of information. It’s also handy for comparing models, even if those models are radically different from each other in architecture or approach. However, care must be taken when making the comparison so that the test sets used to generate the curves are ideally the same or very nearly the same.</p>&#13;
<p class="indent">Let’s use ROC analysis to compare the different MNIST even/odd digit models that we trained previously. We’ll see if this helps us to choose between them.</p>&#13;
<p class="indent"><a href="ch11.xhtml#ch11fig3">Figure 11-3</a> shows the ROC curves for these models with an inset expanding the upper-left corner of the graph to make it easier to distinguish one model from the other. The number of nodes in each hidden layer is indicated to identify the models.</p>&#13;
<div class="image" id="ch11fig3"><span epub:type="pagebreak" id="page_272"/><img src="Images/11fig03.jpg" alt="image" width="675" height="504"/></div>&#13;
<p class="figcap"><em>Figure 11-3: ROC curves for the MNIST even/odd models. The model hidden layer sizes are indicated.</em></p>&#13;
<p class="indent">We immediately see that one ROC curve is significantly different from the other three. This is the ROC curve for the model with a single hidden layer with two nodes. All the other ROC curves are above this one. As a general rule, if one ROC curve is entirely above another, then the model that generated the curve can be considered superior. All of the larger MNIST even/odd models are superior to the model with only two nodes in its hidden layer.</p>&#13;
<p class="indent">The other three models are quite close to each other, so how do we choose one? The decision isn’t always clear-cut. Following our rule of thumb about ROC curves, we should select the two-layer model with 500 and 250 nodes, respectively, as its ROC curve is above the others. However, we might hesitate depending upon our use case. This model has over 500,000 parameters. Running it requires use of all of those parameters. The 100 × 50 model contains slightly more than 80,000 parameters. That’s less than one-fifth the number of the larger model. We might decide that processing speed considerations eclipse the small improvement in the overall performance of the larger model and select the smaller model. The ROC analysis showed us that doing so involves only a minor performance penalty.</p>&#13;
<p class="indent">Another factor to consider when comparing ROC curves visually is the slope of the curve when the FPR is small. A perfect model has a vertical slope since it jumps immediately from the point (0,0) to (0,1). Therefore, the better model will have an ROC curve that has a steeper slope in the low FPR region.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_273"/>A commonly used metric derived from the ROC curve is the area under it. This area is usually abbreviated as <em>AUC</em> or, in medical circles, <em>Az</em>. A perfect ROC curve has an AUC of 1.0 since the curve jumps from (0,0) to (0,1) and then over to (1,1), forming a square of side 1 with an area of 1. A model that guesses randomly (the diagonal line in the ROC plot) has an AUC of 0.5, the area of the triangle formed by the dashed diagonal line. To calculate the area under an arbitrary ROC curve, one needs to perform numerical integration. Fortunately for us, sklearn knows how to do this, so we don’t need to. We’ll see this shortly.</p>&#13;
<p class="indent">People often report the AUC, but as time goes by, I’m less and less in favor of it. The main reason is that AUC replaces the highly informative graph with a single number, but different ROC curves can lead to the same AUC. If the AUC of two curves is the same, but one leans far to the right while the other has a steep slope in the low FPR region, we might be tempted to think the models are roughly equivalent in terms of performance, when, in reality, the model with the steeper slope is likely the one we want because it will reach a reasonable TPR without too many false positives.</p>&#13;
<p class="indent">Another caution when using the AUC is that the AUC changes only a small amount for even fairly significant changes in other parameters. This makes it difficult for humans to judge well based on AUC values that are only slightly different from each other. For example, the AUC of the MNIST even/odd model with two nodes in its hidden layer is 0.9373, while the AUC of the model with 100 nodes is 0.9722. Both are well above 0.9 out of a possible 1.0, so, are they both about the same? We know that they are not, since the ROC curves clearly show the two-node model to be well below the other.</p>&#13;
<h4 class="h4" id="lev2_109">Generating an ROC Curve</h4>&#13;
<p class="noindent">We are now ready to learn how to create an ROC curve. The easy way to get the ROC curve, and AUC, is to use <span class="literal">sklearn</span>:</p>&#13;
<p class="programs">import os<br/>&#13;
import sys<br/>&#13;
import numpy as np<br/>&#13;
import matplotlib.pylab as plt<br/>&#13;
from sklearn.metrics import roc_auc_score, roc_curve<br/>&#13;
<br/>&#13;
def main():<br/>&#13;
    labels = np.load(sys.argv[1])<br/>&#13;
    probs = np.load(sys.argv[2])<br/>&#13;
    pname = sys.argv[3]<br/>&#13;
<br/>&#13;
    auc = roc_auc_score(labels, probs[:,1])<br/>&#13;
    roc = roc_curve(labels, probs[:,1])<br/>&#13;
    print("AUC = %0.6f" % auc)<br/>&#13;
<br/>&#13;
    plt.plot(roc[0], roc[1], color='r')<br/>&#13;
    plt.plot([0,1],[0,1], color='k', linestyle=':')<br/>&#13;
<span epub:type="pagebreak" id="page_274"/>    plt.xlabel("FPR")<br/>&#13;
    plt.ylabel("TPR")<br/>&#13;
    plt.tight_layout(pad=0, w_pad=0, h_pad=0)<br/>&#13;
    plt.savefig(pname, dpi=300)<br/>&#13;
    plt.show()</p>&#13;
<p class="indent">This routine reads a set of labels and the associated per class probabilities, such as the output generated by the code in the previous section. It then calls the sklearn functions <span class="literal">roc_auc_score</span> and <span class="literal">roc_curve</span> to return the AUC and the ROC points, respectively. The ROC curve is plotted, saved to disk, and displayed.</p>&#13;
<p class="indent">We need not use sklearn as a black box. We can generate the ROC curve points ourselves quickly enough. We load the same inputs, the labels, and the per class probabilities, but instead of calling a library function, we loop over the threshold values of interest and calculate TP, TN, FP, and FN for each threshold. From these, we can directly calculate the FPR and TPR, which gives us the set of points we need to plot. The code to do this is straightforward:</p>&#13;
<p class="programs">def table(labels, probs, t):<br/>&#13;
    tp = tn = fp = fn = 0<br/>&#13;
    for i,l in enumerate(labels):<br/>&#13;
        c = 1 if (probs[i,1] &gt;= t) else 0<br/>&#13;
        if (l == 0) and (c == 0):<br/>&#13;
            tn += 1<br/>&#13;
        if (l == 0) and (c == 1):<br/>&#13;
            fp += 1<br/>&#13;
        if (l == 1) and (c == 0):<br/>&#13;
            fn += 1<br/>&#13;
        if (l == 1) and (c == 1):<br/>&#13;
            tp += 1<br/>&#13;
    return [tp, tn, fp, fn]<br/>&#13;
<br/>&#13;
def main():<br/>&#13;
    labels = np.load(sys.argv[1])<br/>&#13;
    probs = np.load(sys.argv[2])<br/>&#13;
    pname = sys.argv[3]<br/>&#13;
<br/>&#13;
    th = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]<br/>&#13;
    roc = []<br/>&#13;
    for t in th:<br/>&#13;
        tp, tn, fp, fn = table(labels, probs, t)<br/>&#13;
        tpr = tp / (tp + fn)<br/>&#13;
        fpr = fp / (tn + fp)<br/>&#13;
        roc.append([fpr, tpr])<br/>&#13;
    roc = np.array(roc)<br/>&#13;
<br/>&#13;
<span epub:type="pagebreak" id="page_275"/>    xy = np.zeros((roc.shape[0]+2, roc.shape[1]))<br/>&#13;
    xy[1:-1,:] = roc<br/>&#13;
    xy[0,:] = [0,0]<br/>&#13;
    xy[-1,:] = [1,1]<br/>&#13;
    plt.plot(xy[:,0], xy[:,1], color='r', marker='o')<br/>&#13;
    plt.plot([0,1],[0,1], color='k', linestyle=':')<br/>&#13;
    plt.xlabel("FPR")<br/>&#13;
    plt.ylabel("TPR")<br/>&#13;
    plt.savefig(pname)<br/>&#13;
    plt.show()</p>&#13;
<p class="indent">The <span class="literal">main</span> function loads the labels and probabilities. The loop over <span class="literal">th</span> applies the different threshold values, accumulating the ROC points in <span class="literal">roc</span> by calling the <span class="literal">table</span> function, which calculates the TP, TN, FP, and FN for the current threshold.</p>&#13;
<p class="indent">The <span class="literal">table</span> function loops over all the per class probabilities assigning a class label of 1 if the class 1 probability is greater than or equal to the current threshold value. This class assignment is then compared to the actual class label, and the appropriate tally counter is incremented.</p>&#13;
<p class="indent">Once the ROC points are calculated, the plot is made by adding the point (0,0) to the beginning of the point list and the point (1,1) to the end of the list. Doing this ensures that the plot extends the full range of FPR values. The points are plotted and saved to disk.</p>&#13;
<h4 class="h4" id="lev2_110">The Precision–Recall Curve</h4>&#13;
<p class="noindent">Before leaving this section, we should mention one other evaluation curve that you will run across from time to time in machine learning. This is the <em>precision-recall (PR) curve</em>. As the name suggests, it plots the PPV (precision) and TPR (recall, sensitivity) as the decision threshold varies, just like an ROC curve. A good PR curve moves toward the upper right instead of the upper left as a good ROC curve does. The points of this curve are easily generated in sklearn using the <span class="literal">precision_recall_curve</span> function in the <span class="literal">metrics</span> module.</p>&#13;
<p class="indent">We’re not spending time with this curve because it does not take the true negatives into account. Consider the definition of the PPV and TPR to see that this is so. My bias against the PR curve stems from the same concern as my bias against the F1 score. By not taking the true negatives into account, the PR curve and F1 score give an incomplete picture of the quality of the classifier. The PR curve does have utility when the true positive class is rare or when the true negative performance is not essential. However, in general, for evaluating classifier performance, I claim it is best to stick to the ROC curve and the metrics we have defined.</p>&#13;
<h3 class="h3" id="lev1_78"><span epub:type="pagebreak" id="page_276"/>Handling Multiple Classes</h3>&#13;
<p class="noindent">All of the metrics we’ve discussed so far apply to binary classifiers only. Of course, we know that many classifiers are multiclass: they output multiple labels, not just 0 or 1. To evaluate these models, we’ll extend our idea of the confusion matrix to the multiclass case and see that we can also extend some of the metrics we’re already familiar with as well.</p>&#13;
<p class="indent">We need some multiclass model results to work with. Thankfully, the MNIST data is already multiclass. Recall, we went to the trouble of recoding the labels to make the dataset binary. Here we’ll train models with the same architectures, but this time we’ll leave the labels as they are so that the model will output one of ten labels: the digit it assigned to the test input, the output of the <span class="literal">predict</span> method of the <span class="literal">MLPClassifier</span> class. We won’t show the code as it’s identical to the code in the previous section except that <span class="literal">predict</span> is called in place of <span class="literal">predict_proba</span>.</p>&#13;
<h4 class="h4" id="lev2_111">Extending the Confusion Matrix</h4>&#13;
<p class="noindent">The basis for our binary metrics was the 2 × 2 confusion matrix. The confusion matrix is readily extended to the multiclass case. To do that, we let the rows of the matrix represent the actual class labels, while the columns of the matrix represent the model’s predictions. The matrix is square with as many rows and columns as there are classes in the dataset. For MNIST, then, we arrive at a 10 × 10 confusion matrix since there are 10 digits.</p>&#13;
<p class="indent">We calculate the confusion matrix from the actual known test labels and the predicted labels from the model. There is a function in the <span class="literal">metrics</span> module of sklearn, <span class="literal">confusion_matrix</span>, which we can use, but it’s straightforward enough to calculate it ourselves:</p>&#13;
<p class="programs">def confusion_matrix(y_test, y_predict, n=10):<br/>&#13;
    cmat = np.zeros((n,n), dtype="uint32")<br/>&#13;
    for i,y in enumerate(y_test):<br/>&#13;
        cmat[y, y_predict[i]] += 1<br/>&#13;
    return cmat</p>&#13;
<p class="indent">Here <span class="literal">n</span> is the number of classes, fixed at 10 for MNIST. If needed, we could instead determine it from the supplied test labels.</p>&#13;
<p class="indent">The code is straightforward. The inputs are vectors of the actual labels (<span class="literal">y_test</span>) and the predicted labels (<span class="literal">y_predict</span>), and the confusion matrix (<span class="literal">cmat</span>) is filled in by incrementing each possible index formed from the actual label and the predicted label. For example, if the actual label is 3 and the predicted label is 8, then we add one to <span class="literal">cmat[3,8]</span>.</p>&#13;
<p class="indent">Let’s look at the confusion matrix for a model with one hidden layer of 100 nodes (<a href="ch11.xhtml#ch11tab8">Table 11-8</a>).</p>&#13;
<p class="tabcap" id="ch11tab8"><span epub:type="pagebreak" id="page_277"/><strong>Table 11-8:</strong> Confusion Matrix for the Model with a Single Hidden Layer of 100 Nodes</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top"><p class="tab"><strong>0</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>2</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>3</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>4</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>5</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>6</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>7</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>8</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>9</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">943</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1102</p></td>&#13;
<td style="vertical-align: top"><p class="tab">14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">16</p></td>&#13;
<td style="vertical-align: top"><p class="tab">15</p></td>&#13;
<td style="vertical-align: top"><p class="tab">862</p></td>&#13;
<td style="vertical-align: top"><p class="tab">36</p></td>&#13;
<td style="vertical-align: top"><p class="tab">18</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">17</p></td>&#13;
<td style="vertical-align: top"><p class="tab">24</p></td>&#13;
<td style="vertical-align: top"><p class="tab">41</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">937</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">20</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">13</p></td>&#13;
<td style="vertical-align: top"><p class="tab">17</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">879</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">66</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">19</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">53</p></td>&#13;
<td style="vertical-align: top"><p class="tab">13</p></td>&#13;
<td style="vertical-align: top"><p class="tab">719</p></td>&#13;
<td style="vertical-align: top"><p class="tab">17</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">44</p></td>&#13;
<td style="vertical-align: top"><p class="tab">18</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">21</p></td>&#13;
<td style="vertical-align: top"><p class="tab">15</p></td>&#13;
<td style="vertical-align: top"><p class="tab">894</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">21</p></td>&#13;
<td style="vertical-align: top"><p class="tab">32</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">902</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">51</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">17</p></td>&#13;
<td style="vertical-align: top"><p class="tab">14</p></td>&#13;
<td style="vertical-align: top"><p class="tab">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab">72</p></td>&#13;
<td style="vertical-align: top"><p class="tab">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab">46</p></td>&#13;
<td style="vertical-align: top"><p class="tab">21</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">749</p></td>&#13;
<td style="vertical-align: top"><p class="tab">24</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">11</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">13</p></td>&#13;
<td style="vertical-align: top"><p class="tab">42</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">31</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10</p></td>&#13;
<td style="vertical-align: top"><p class="tab">884</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">The rows represent the actual test sample label, [0,9]. The columns are the label assigned by the model. If the model is perfect, there will be a one-to-one match between the actual label and the predicted label. This is the main diagonal of the confusion matrix. Therefore, a perfect model will have entries along the main diagonal, and all other elements will be 0. <a href="ch11.xhtml#ch11tab8">Table 11-8</a> is not perfect, but the largest counts are along the main diagonal.</p>&#13;
<p class="indent">Look at row 4 and column 4. The place where the row and column meet has the value 879. This means that there were 879 times when the actual class was 4 and the model correctly predicted “4” as the label. If we look along row 4, we see other numbers that are not zero. Each of these represents a case where an actual 4 was called another digit by the model. For example, there were 66 times when a 4 was called a “9” but only one case of a 4 being labeled a “7”.</p>&#13;
<p class="indent">Column 4 represents the cases when the model called the input a “4”. As we saw, it was correct 879 times. However, there were other digits that the model accidentally labeled as “4”, like the 21 times a 6 was called a “4” or the one time a 1 was mistaken for a “4”. There were no cases of a 3 being labeled a “4”.</p>&#13;
<p class="indent">The confusion matrix tells us at a glance how well the model is doing on the test set. We can quickly see if the matrix is primarily diagonal. If it is, the model is doing a good job on the test set. If not, we need to take a closer look to see what classes are being confused with other classes. A simple adjustment to the matrix can help. Instead of the raw counts, which require us to remember how many examples of each class are in the test set, we can divide the values of each row by the sum of the row. Doing so converts the entries from counts to fractions. We can then multiply the entries by 100 to convert to percents. This transforms the confusion matrix into what we’ll call an <em>accuracy matrix</em>. The conversion is straightforward:</p>&#13;
<p class="programs">acc = 100.0*(cmat / cmat.sum(axis=1))</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_278"/>Here <span class="literal">cmat</span> is the confusion matrix. This produces an accuracy matrix, <a href="ch11.xhtml#ch11tab9">Table 11-9</a>.</p>&#13;
<p class="tabcap" id="ch11tab9"><strong>Table 11-9:</strong> A Confusion Matrix Presented as per Class Accuracies</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:5%"/>&#13;
<col style="width:5%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top"><p class="tab"><strong>0</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>2</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>3</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>4</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>5</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>6</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>7</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>8</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>9</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>96.2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>97.1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>83.5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>92.8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>89.5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.5</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>80.6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>93.3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>87.7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.1</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>76.9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>87.6</strong></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">The diagonal shows the per class accuracies. The worst performing class is 8 with an accuracy of 76.9 percent, and the best performing class is 1 with an accuracy of 97.1 percent. The non-diagonal elements are the percentage of the actual class labeled as a different class by the model. For class 0, the model called a true zero class “5” 1.1 percent of the time. The row percentages sum to 100 percent (within rounding error).</p>&#13;
<p class="indent">Why did class 8 do so poorly? Looking across the row for class 8, we see that the model mistook 7.1 percent of the actual 8 instances for a “3” and 5.2 percent of the instances for a “5”. Confusing an 8 with a “3” was the biggest single mistake the model made, though 6.5 percent of 4 instances were labeled “9” as well. A moment’s reflection makes sense of the errors. How often do people confuse 8 and 3 or 4 and 9? This model is making errors similar to those humans make.</p>&#13;
<p class="indent">The confusion matrix can reveal pathological performance as well. Consider the MNIST model in <a href="ch11.xhtml#ch11fig3">Figure 11-3</a>, with a single hidden layer of only two nodes. The accuracy matrix it produces is shown in <a href="ch11.xhtml#ch11tab10">Table 11-10</a>.</p>&#13;
<p class="indent">We can immediately see that this is an inferior model. Column 5 is entirely zero, meaning the model never outputs “5” for any input. Much the same is true for output labels “8” and “9”. On the other hand, the model likes to call inputs “0”, “1”, “2”, or “3” as those columns are densely populated for all manner of input digits. Looking at the diagonal, we see that only 1 and 3 stand a reasonable chance of being correctly identified, though many of these will be called “7”. Class 8 is rarely correctly labeled (1.3 percent). A poorly performing model will have a confusion matrix like this, with oddball outputs and large off-diagonal values.</p>&#13;
<p class="tabcap" id="ch11tab10"><span epub:type="pagebreak" id="page_279"/><strong>Table 11-10:</strong> Accuracy Matrix for the Model with Only Two Nodes in Its&#13;
Hidden Layer</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
<col style="width:10%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"/>&#13;
<th style="vertical-align: top"><p class="tab"><strong>0</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>1</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>2</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>3</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>4</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>5</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>6</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>7</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>8</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>9</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>51.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">10.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">34.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>88.3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>75.2</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">6.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">3.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>79.4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">13.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>4</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">28.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">31.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">7.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9.7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
<td style="vertical-align: top"><p class="tab">13.6</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.5</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">11.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">42.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>0.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">16.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>6</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">35.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">1.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>55.0</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>7</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">66.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>25.5</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab"><strong>8</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">10.5</p></td>&#13;
<td style="vertical-align: top"><p class="tab">41.9</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">22.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>1.3</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab"><strong>9</strong></p></td>&#13;
<td style="vertical-align: top"><p class="tab">4.7</p></td>&#13;
<td style="vertical-align: top"><p class="tab">9.1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">26.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">5.8</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">41.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">2.2</p></td>&#13;
<td style="vertical-align: top"><p class="tab"><strong>3.1</strong></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<h4 class="h4" id="lev2_112">Calculating Weighted Accuracy</h4>&#13;
<p class="noindent">The diagonal elements of an accuracy matrix tell us the per class accuracies for the model. We can calculate an overall accuracy by averaging these values. However, this could be misleading if one or more classes is far more prevalent in the test data than the others. Instead of a simple average, we should use a weighted average. The weights are based on the total number of test samples from each class divided by the total number of test samples presented to the model. Say we have three classes and their frequency and per class accuracies in our test set are as in <a href="ch11.xhtml#ch11tab11">Table 11-11</a>:</p>&#13;
<p class="tabcap" id="ch11tab11"><strong>Table 11-11:</strong> Hypothetical per Class Accuracies for a Model with Three Classes</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:20%"/>&#13;
<col style="width:40%"/>&#13;
<col style="width:40%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Class</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Frequency</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Accuracy</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4,004</p></td>&#13;
<td style="vertical-align: top"><p class="tab">88.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,502</p></td>&#13;
<td style="vertical-align: top"><p class="tab">76.6</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8,080</p></td>&#13;
<td style="vertical-align: top"><p class="tab">65.2</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="indent">Here we have <em>N</em> = 4,004 + 6,502 + 8,080 = 18586 test samples. Then, the per class weights are shown in <a href="ch11.xhtml#ch11tab12">Table 11-12</a>.</p>&#13;
<p class="tabcap" id="ch11tab12"><strong>Table 11-12:</strong> Example per-class weights</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab">Class</p></th>&#13;
<th style="vertical-align: top"><p class="tab">Weight</p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">0</p></td>&#13;
<td style="vertical-align: top"><p class="tab">4,004 / 18,586 = 0.2154</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">1</p></td>&#13;
<td style="vertical-align: top"><p class="tab">6,502 / 18,586 = 0.3498</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">8,080 / 18,586 = 0.4347</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent"><span epub:type="pagebreak" id="page_280"/>The average accuracy can be calculated to be</p>&#13;
<p class="center">ACC = 0.2154 × 88.1 + 0.3498 × 76.6 + 0.4347 × 65.2 = 74.1</p>&#13;
<p class="indent">Philosophically, we should replace the weights with the actual per class prior probabilities, if we know them. These probabilities are the true likelihood of the class appearing in the wild. However, if we assume that the test set is fairly constructed, we’re likely safe using only the per class frequencies. We claim that a properly built test set will represent the true prior class probabilities reasonably well.</p>&#13;
<p class="indent">In code, the weighted mean accuracy can be calculated succinctly from the confusion matrix:</p>&#13;
<p class="programs">def weighted_mean_acc(cmat):<br/>&#13;
    N = cmat.sum()<br/>&#13;
    C = cmat.sum(axis=1)<br/>&#13;
    return ((C/N)*(100*np.diag(cmat)/C)).sum()</p>&#13;
<p class="indent"><em>N</em> is the total number of samples that were tested, which is just the sum of the entries in the confusion matrix since every sample in the test set falls somewhere in the matrix, and <em>C</em> is a vector of the number of samples per class. This is just the sum of the rows of the confusion matrix. The per class accuracy, as a percentage, is calculated from the diagonal elements of the confusion matrix (<span class="literal">np.diag(cmat)</span>) divided by the number of times each class shows up in the test set, <em>C</em>. Multiply by 100 to make these percent accuracies.</p>&#13;
<p class="indent">If we summed these per class and divided by the number of classes, we would have the (potentially misleading) unweighted mean accuracy. Instead, we first multiply by <em>C</em>/<em>N</em>, the fraction of all test samples that were of each class (recall, <em>C</em> is a vector), and then sum to get the weighted accuracy. This code works for any size confusion matrix.</p>&#13;
<p class="indent">For the MNIST models of the previous section, we calculate weighted mean accuracies to be those in <a href="ch11.xhtml#ch11tab13">Table 11-13</a>.</p>&#13;
<p class="tabcap" id="ch11tab13"><strong>Table 11-13:</strong> Weighted Mean Accuracies for the MNIST Models</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Architecture</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>Weighted mean accuracy</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">40.08%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">88.71%</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">100 × 50</p></td>&#13;
<td style="vertical-align: top"><p class="tab">88.94%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">500 × 250</p></td>&#13;
<td style="vertical-align: top"><p class="tab">89.63%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent"><a href="ch11.xhtml#ch11tab13">Table 11-13</a> shows the sort of diminishing returns we’ve seen previously as the model size increases. The single hidden layer of 100 nodes is virtually identical to the two hidden layer model with 100 and 50 nodes and only 1 percent worse than the much larger model with 500 nodes and 250 nodes in its hidden layers. The model with only two nodes in the hidden layer performs poorly. Since there are 10 classes, random guessing would tend to <span epub:type="pagebreak" id="page_281"/>have an accuracy of 1/10 = 0.1 = 10 percent, so even this very strange model that maps 784 input values (28×28 pixels) to only two and then to ten output nodes is still four times more accurate than random guessing. However, this is misleading on its own because, as we just saw in <a href="ch11.xhtml#ch11tab10">Table 11-10</a>, the confusion matrix for this model is quite strange. We certainly would not want to use this model. Nothing beats careful consideration of the confusion matrix.</p>&#13;
<h4 class="h4" id="lev2_113">Multiclass Matthews Correlation Coefficient</h4>&#13;
<p class="noindent">The 2 × 2 confusion matrix led to many possible metrics. While it’s possible to extend several of those metrics to the multiclass case, we’ll consider only the main metric here: the Matthews correlation coefficient (MCC). For the binary case, we saw that the MCC was</p>&#13;
<div class="imagec"><img src="Images/281equ01.jpg" alt="image" width="444" height="52"/></div>&#13;
<p class="noindent">This can be extended to the multiclass case by using terms from the confusion matrix like so</p>&#13;
<div class="imagec"><img src="Images/281equ02.jpg" alt="image" width="332" height="71"/></div>&#13;
<p class="noindent">where</p>&#13;
<div class="imagec"><img src="Images/281equ03.jpg" alt="image" width="134" height="399"/></div>&#13;
<p class="noindent">Here, <em>K</em> is the number of classes, and <em>C</em> is the confusion matrix. This notation is from the sklearn website’s description of the MCC, giving us a direct view of how it’s implemented. We don’t need to follow the equations in detail; we need to know only that the MCC is built from the confusion matrix in the multiclass case as in the binary case. Intuitively, this makes sense. The binary MCC is a value in the range [<em>–</em>1,+1]. The multiclass case changes the <span epub:type="pagebreak" id="page_282"/>lower bound based on the number of classes, but the upper bound remains 1.0, so the closer the MCC is to 1.0, the better the model is doing.</p>&#13;
<p class="indent">Calculating the MCC for the MNIST models, as we did for the weighted mean accuracy, gives <a href="ch11.xhtml#ch11tab14">Table 11-14</a>.</p>&#13;
<p class="tabcap" id="ch11tab14"><strong>Table 11-14:</strong> The MCC for the MNIST Models</p>&#13;
<table class="bordertb">&#13;
<colgroup>&#13;
<col style="width:50%"/>&#13;
<col style="width:50%"/>&#13;
</colgroup>&#13;
<thead>&#13;
<tr class="borderb">&#13;
<th style="vertical-align: top"><p class="tab"><strong>Architecture</strong></p></th>&#13;
<th style="vertical-align: top"><p class="tab"><strong>MCC</strong></p></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">2</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.3440</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">100</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8747</p></td>&#13;
</tr>&#13;
<tr class="bg-g">&#13;
<td style="vertical-align: top"><p class="tab">100 × 50</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8773</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td style="vertical-align: top"><p class="tab">500 × 250</p></td>&#13;
<td style="vertical-align: top"><p class="tab">0.8849</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="noindent">Again, this shows us that the smallest model is inferior while the other three models are all quite similar in terms of performance. The time to make predictions on the 10,000 test samples, however, varies quite a bit by model. The single hidden layer model with 100 nodes takes 0.052 seconds, while the largest model needs 0.283 seconds, over five times longer. If speed is essential, the smaller model might be preferable. Many factors come into play when deciding on a model to use. The metrics discussed in this chapter are guides, but they should not be followed blindly. In the end, only you know what makes sense for the problem you are trying to solve.</p>&#13;
<h3 class="h3" id="lev1_79">Summary</h3>&#13;
<p class="noindent">In this chapter, we learned why accuracy is not a sufficient measure of the performance of a model. We learned how to generate the 2 × 2 confusion matrix for a binary classifier, and what this matrix tells us about the model’s performance on the held-out test set. We derived basic metrics from the 2 × 2 confusion matrix and used those basic metrics to derive more advanced metrics. We discussed the utility of the various metrics to build our intuition as to how and when to use them. We then learned about the receiver operating characteristics (ROC) curve, including what it illustrates about the model and how to interpret it to compare models against each other. Finally, we introduced the multiclass confusion matrix, giving examples of how to interpret it and how to extend some of the binary classifier metrics to the multiclass case.</p>&#13;
<p class="indent">In the next chapter, we’ll reach the pinnacle of our machine learning models: convolutional neural networks (CNNs). The next chapter introduces the basic ideas behind the CNN; later chapters will conduct many experiments using this deep learning architecture.</p>&#13;
</div></body></html>