<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch01"><span epub:type="pagebreak" id="page_1"/><strong><span class="big">1</span><br/>GETTING STARTED</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">This chapter introduces our operating environment and details how to set it up. It also includes a primer on some of the math we will encounter. We’ll end with a brief note about graphics processors, or GPUs, which you may have heard are essential for deep learning. For our purposes, they’re not, so don’t worry—this book won’t suddenly cost you a lot of money.</p>&#13;
<h3 class="h3" id="lev1_4">The Operating Environment</h3>&#13;
<p class="noindent">In this section, we’ll detail the environment we’ll assume throughout the remainder of the book. Our underlying assumption is that we’re using a 64-bit Linux system. The exact distribution is not critical, but to make things simpler in the presentation, we’ll also assume that we’re using Ubuntu 20.04.<a id="dx1-10002"/> Given the excellent support behind the Ubuntu distribution, we trust that any newer distributions will also work similarly. The Python language is our <em>lingua franca</em>, the common language of machine learning. Specifically, we’ll use Python 3.8.2; that’s the version used by Ubuntu 20.04.</p>&#13;
<p class="indent">Let’s look at a quick overview of the Python toolkits that we’ll be using.</p>&#13;
<h4 class="h4" id="lev2_1"><span epub:type="pagebreak" id="page_2"/>NumPy</h4>&#13;
<p class="noindent"><em>NumPy</em> is a Python library that adds array processing abilities to Python. While Python lists can be used like one-dimensional arrays, in practice they are too slow and inflexible. The NumPy library adds the array features missing from Python—features that are necessary for many scientific applications. NumPy is a base library required by all the other libraries we’ll use.</p>&#13;
<h4 class="h4" id="lev2_2">scikit-learn</h4>&#13;
<p class="noindent">All of the traditional machine learning models we’ll explore in this book are found in the superb <span class="literal">scikit-learn</span> library, or <em>sklearn</em>, as it’s usually called when loaded into Python. Note also that we’re writing <em>scikit-learn</em> without caps as this is how the authors consistently refer to it in their documentation. This library uses NumPy arrays. It implements a standardized interface to many different machine learning models as well as an entire host of other functionality that we won’t even have time to touch. I strongly encourage you to review the official sklearn documentation (<a href="https://scikit-learn.org/stable/documentation.html">https://scikit-learn.org/stable/documentation.html</a>) as you become more and more familiar with machine learning and the tools behind it.</p>&#13;
<h4 class="h4" id="lev2_3">Keras with TensorFlow</h4>&#13;
<p class="noindent">Deep learning is hard enough to understand, let alone implement efficiently and correctly, so instead of attempting to write convolutional neural networks from scratch, we’ll use one of the popular toolkits already in active development. From its inception, the deep learning community has supported the development of toolkits to make deep networks easier to use and has made the toolkits open source with very generous licenses. At the time of this writing, there are many popular toolkits we could have used in Python. Among many others, these include the following:</p>&#13;
<ul>&#13;
<li class="noindent">Keras</li>&#13;
<li class="noindent">PyTorch</li>&#13;
<li class="noindent">Caffe</li>&#13;
<li class="noindent">Caffe2</li>&#13;
<li class="noindent">Apache MXnet</li>&#13;
</ul>&#13;
<p class="noindent">Some of these toolkits are waxing, and others appear to be waning. But the one that has probably the most active following at present is Keras with the TensorFlow backend, so that’s the one we’ll use here.</p>&#13;
<p class="indent"><em>Keras</em> (<em><a href="https://keras.io/">https://keras.io/</a></em>) is a Python deep learning toolkit that uses the TensorFlow toolkit (<em><a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></em>) under the hood. <em>TensorFlow</em> is an open source Google product that implements the core functionality of deep neural networks for many different platforms. We selected Keras not only because it’s popular and in active development, but also because it’s straightforward to use. Our goal is to become familiar with deep learning to <span epub:type="pagebreak" id="page_3"/>the point where we can implement models and use them with a minimum of programming overhead.</p>&#13;
<h3 class="h3" id="lev1_5">Installing the Toolkits</h3>&#13;
<p class="noindent">We can’t reasonably give an exhaustive guide for installing the toolkits on all systems and hardware. Instead, we’ll provide step-by-step instructions for the specific operating system we’ll use as the reference system. These steps, along with the minimum version numbers of the libraries, should be enough for most readers to get a working system in place.</p>&#13;
<p class="indent">Remember, we’re assuming that we’re working in a Linux environment, specifically Ubuntu 20.04. Ubuntu is a widely used Linux distribution, and  it runs on almost any modern computer system. Other Linux distributions will work, as will macOS, but the instructions here are specific to Ubuntu. For the most part, the machine learning community has left the Windows operating system. Still, individuals have ported toolkits to Windows; therefore, an adventurous reader might give Windows a try.</p>&#13;
<p class="indent">A freshly installed Ubuntu 20.04 base desktop system gives us Python 3.8.2 for free. To install the remaining packages, we need to go into a shell and execute the sequence of steps below in the order given:</p>&#13;
<p class="programs"><span class="codestrong1">$ sudo apt - get update</span><br/>&#13;
<span class="codestrong1">$ sudo apt - get install python3 - pip</span><br/>&#13;
<span class="codestrong1">$ sudo apt - get install build - essential python3 - dev</span><br/>&#13;
<span class="codestrong1">$ sudo apt - get install python3 - setuptools python3 - numpy</span><br/>&#13;
<span class="codestrong1">$ sudo apt - get install python3 - scipy libatlas - base - dev</span><br/>&#13;
<span class="codestrong1">$ sudo apt - get install python3 - matplotlib</span><br/>&#13;
<span class="codestrong1">$ pip3 install scikit - learn</span><br/>&#13;
<span class="codestrong1">$ pip3 install tensorflow</span><br/>&#13;
<span class="codestrong1">$ pip3 install pillow</span><br/>&#13;
<span class="codestrong1">$ pip3 install h5py</span><br/>&#13;
<span class="codestrong1">$ pip3 install keras</span></p>&#13;
<p class="indent">Once the installation is complete, we’ll have installed the following versions of the libraries and toolkits:</p>&#13;
<p class="programs">NumPy 1.17.4<br/>&#13;
sklearn 0.23.2<br/>&#13;
keras 2.4.3<br/>&#13;
tensorflow 2.2.0<br/>&#13;
pillow 7.0.0<br/>&#13;
h5py 2.10.0<br/>&#13;
matplotlib 3.1.2</p>&#13;
<p class="indent">The <span class="literal">pillow</span> library is an image processing library, <span class="literal">h5py</span> is a library for working with HDF5 format data files, and <span class="literal">matplotlib</span> is for plotting. <em>HDF5</em> is a generic, hierarchical file format for storing scientific data. Keras uses it to store model parameters.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_4"/>The following two sections are light introductions to some of the math that will creep into the book.</p>&#13;
<h3 class="h3" id="lev1_6">Basic Linear Algebra</h3>&#13;
<p class="noindent">We’re about to look at vectors and matrices. The math that deals with these concepts falls under the general heading of <em>linear algebra</em>, or matrix theory. As you might imagine, linear algebra is a complex field. All we need to know for this book is what a vector is, what a matrix is, and how we can multiply two vectors, or two matrices, or vectors and matrices together. We’ll see later on that this gives us a powerful way to implement specific models, particularly neural networks.</p>&#13;
<p class="indent">Let’s begin by looking at vectors.</p>&#13;
<h4 class="h4" id="lev2_4">Vectors</h4>&#13;
<p class="noindent">A <em>vector</em> is a one-dimensional list of numbers. Mathematically, a vector might appear as</p>&#13;
<p class="center"><em>a</em> = [0, 1, 2, 3, 4]</p>&#13;
<p class="noindent">with the third element given as <em>a</em><sub>2</sub> = 2. Notice we’re following the programming convention of indexing from zero, so <em>a</em><sub>2</sub> gives us the third element in the vector.</p>&#13;
<p class="indent">The vector above was written horizontally and therefore is known as a <em>row vector</em>. When used in mathematical expressions, however, vectors are usually assumed to be written vertically:</p>&#13;
<div class="imagec"><img src="Images/004equ02.jpg" alt="Image" width="65" height="125"/></div>&#13;
<p class="indent">When written vertically, a vector is known as a <em>column vector</em>. This vector has five elements and is denoted as a five-element column vector. In this book, we’ll typically use vectors to represent one <em>sample</em>: one set of features that we’ll input to a model.</p>&#13;
<p class="indent">Mathematically, vectors are used to represent points in space. If we’re talking about the two-dimensional (2D) Cartesian plane, we locate a point with a vector of two numbers, (<em>x</em>,<em>y</em>), where <em>x</em> is the distance along the x-axis and <em>y</em> is the distance along the y-axis. That vector represents a point in two dimensions, even though the vector itself has only one dimension. If we have three dimensions, we need a vector with three elements, (<em>x</em>,<em>y</em>,<em>z</em>).</p>&#13;
<p class="indent">In machine learning, since we often use vectors to represent the inputs to our models, we’ll be working with dozens to hundreds of dimensions. Of course, we can’t plot them as points in a space, but mathematically, that’s what they are. As we’ll see, some models, such as the <em>k</em>-Nearest Neighbors <span epub:type="pagebreak" id="page_5"/>model, use the feature vectors as just that—points in a high-dimensional space.</p>&#13;
<h4 class="h4" id="lev2_5">Matrices</h4>&#13;
<p class="noindent">A <em>matrix</em> is a two-dimensional array of numbers where we index a particular entry by its row number and column number. For example, this is a matrix:</p>&#13;
<div class="imagec"><img src="Images/005equ01.jpg" alt="Image" width="128" height="75"/></div>&#13;
<p class="noindent">If we want to refer to the 6, we write <em>a</em><sub>1,2</sub> = 6. Again, we’re indexing from zero. Because this matrix <em>a</em> has three rows and three columns, we call it a 3 × 3 matrix.</p>&#13;
<h4 class="h4" id="lev2_6">Multiplying Vectors and Matrices</h4>&#13;
<p class="noindent">The simplest way to think of multiplying two vectors together is to multiply their corresponding elements. For example:</p>&#13;
<p class="center">[1, 2, 3] × [4, 5, 6] = [4, 10, 18]</p>&#13;
<p class="indent">This is the most common way to multiply an array when using a toolkit like NumPy, and we’ll make heavy use of this in the chapters that follow. However, in mathematics, this is seldom actually done.</p>&#13;
<p class="indent">When multiplying vectors together mathematically, we need to know if they are row or column vectors. We’ll work with two vectors, <em>A</em> = (<em>a</em>,<em>b</em>,<em>c</em>), and <em>B</em> = (<em>d</em>,<em>e</em>,<em>f</em> ), which, following mathematical convention, are assumed to be column vectors. Adding a superscript <em>T</em> turns a column vector into a row vector. The mathematically allowed ways to multiply <em>A</em> and <em>B</em> are</p>&#13;
<div class="imagec"><img src="Images/005equ02.jpg" alt="Image" width="326" height="75"/></div>&#13;
<p class="indent">which is called the <em>outer product</em> and</p>&#13;
<div class="imagec"><img src="Images/005equ03.jpg" alt="Image" width="305" height="75"/></div>&#13;
<p class="noindent">which is called the <em>inner product</em>, or <em>dot product</em>. Notice that the outer product becomes a matrix, and the inner product becomes a single number, a <em>scalar</em>.</p>&#13;
<p class="indent">When multiplying a matrix and a vector, the vector is typically on the right side of the matrix. The multiplication can proceed if the number of columns in the matrix matches the number of elements in the vector, again assumed to be a column vector. The result is also a vector with as many elements as there are rows in the matrix (read <em>ax</em> + <em>by</em> + <em>cz</em> as a single element).</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_6"/>For example:</p>&#13;
<div class="imagec"><img src="Images/006equ01.jpg" alt="Image" width="256" height="76"/></div>&#13;
<p class="noindent">Here we’ve multiplied a 2 × 3 matrix by a 3 × 1 column vector to get a 2 × 1 output vector. Notice that the number of columns of the matrix and the number of rows of the vector match. If they do not, then the multiplication is not defined. Also, notice that the values in the output vector are sums of products of the matrix and vector. This same rule applies when multiplying two matrices:</p>&#13;
<div class="imagec"><img src="Images/006equ02.jpg" alt="Image" width="438" height="75"/></div>&#13;
<p class="noindent">Here multiplying a 2 × 3 matrix by a 3 × 2 matrix has given us a 2 × 2 answer.</p>&#13;
<p class="indent">When we get to convolutional neural networks, we’ll work with arrays that have three and even four dimensions. Generically, these are referred to as <em>tensors</em>. If we imagine a stack of matrices, all the same size, we get a three-dimensional tensor, and we can use the first index to refer to any one of the matrices and the remaining two indices to refer to a particular element of that matrix. Similarly, if we have a stack of three-dimensional tensors, we have a four-dimensional tensor, and we can use the first index of that to refer to any one of the three-dimensional tensors.</p>&#13;
<p class="indent">The main points of this section are that vectors have one dimension, matrices have two dimensions, there are rules for multiplying these objects together, and our toolkits will work with four-dimensional tensors in the end. We’ll review some of these points as we encounter them later in the book.</p>&#13;
<h3 class="h3" id="lev1_7">Statistics and Probability</h3>&#13;
<p class="noindent">The topics of statistics and probability are so broad that often it’s better to either say almost nothing or to write a book or two. Therefore, I’ll mention only key ideas that we’ll use throughout the book and leave the rest to you to pick up as you see fit. I’ll assume you know some basic things about probability from flipping coins and rolling dice.</p>&#13;
<h4 class="h4" id="lev2_7">Descriptive Statistics</h4>&#13;
<p class="noindent">When we do experiments, we need to report the results in some meaningful way. Typically, for us, we’ll report results as the mean (arithmetic average) plus or minus a quantity known as the <em>standard error of the mean (SE)</em>. Let’s define the standard error of the mean through an example.</p>&#13;
<p class="indent">If we have many measurements <em>x</em>, say the length of a part of a flower, then we can calculate the mean (<span class="middle"><img src="Images/xbar.jpg" alt="Image"/></span>) by adding all the values together and <span epub:type="pagebreak" id="page_7"/>dividing by the number of values we added. Then, once we have the mean, we can calculate the average spread of the individual values around the mean by subtracting each value from the mean, squaring the result, and adding all these squared values together before dividing by the number of values we added minus one. This number is the <em>variance</em>. If we take the square root of this value, we get the <em>standard deviation</em> (<em>σ</em>), which we’ll see again below. With the standard deviation, we can calculate the standard error of the mean as <span class="middle"><img src="Images/007equ01.jpg" alt="Image" width="111" height="21"/></span>, where <em>n</em> is the number of values that we used to calculate the mean. The smaller the SE is, the more tightly the values are clustered around the mean. We can interpret this value as the uncertainty we have about the mean value. This means we expect the actual mean, which we don’t really know, to be between <span class="middle"><img src="Images/007equ02.jpg" alt="Image" width="53" height="15"/></span> and <span class="middle"><img src="Images/007equ03.jpg" alt="Image" width="53" height="15"/></span>.</p>&#13;
<p class="indent">Sometimes, we’ll talk about the median instead of the mean. The <em>median</em> is the middle value, the value that half of our samples are below and half are above. To find the median for a set of values, we first sort the values numerically and then find the middle value. This is the exact middle value if we have an odd number of samples, or the mean of the two middle values if we have an even number of samples. The median is sometimes more useful than the mean if the samples do not have a good, even spread around the mean. The classic example is income. A few very rich people move the mean income up to the point where it does not have much meaning. Instead, the median, the value where half the people make less and half make more, is more representative.</p>&#13;
<p class="indent">In later chapters, we’ll talk about <em>descriptive statistics</em>. These are values derived from a dataset that can be used to understand the dataset. We just mentioned three of them: the mean, the median, and the standard deviation. We’ll see how to use these and how they can be plotted to help us understand a dataset.</p>&#13;
<h4 class="h4" id="lev2_8">Probability Distributions</h4>&#13;
<p class="noindent">In this book, we’ll talk about something known as a <em>probability distribution</em>. You can think of it as an oracle of sorts—something that, when asked, will give us a number or set of numbers. For example, when we train a model, we use numbers, or sets of numbers, that we measure; we can think of those numbers as coming from a probability distribution. We’ll refer to that distribution as the <em>parent distribution</em>. Think of it as the thing that generates the data we’ll feed our model; another, more Platonic, way to think about it is as the ideal set of data that our data is approximating.</p>&#13;
<p class="indent">Probability distributions come in many different forms; some even have names. The two that we’ll encounter are the two most common: uniform and normal distributions. You’ve already encountered a uniform distribution: it’s what we get if we roll a fair die. If the die has six sides, we know that the likelihood of getting any value, 1 through 6, is the same. If we roll the die 100 times and tally the numbers that come up, we know that the tally will be roughly equal for each number and that in the long run, we can easily convince ourselves that the number will even out.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_8"/>A <em>uniform distribution</em> is an oracle that is equally likely to give us any of its allowed responses. Mathematically, we’ll write uniform distributions as <em>U</em>(<em>a</em>,<em>b</em>) where <em>U</em> means uniform and <em>a</em> and <em>b</em> are the range of values it will use to bracket its response. Unless we specify the distribution gives only integers, any real number is allowed as the response. Notationally, we write <em>x</em> ~ <em>U</em>(0,1) to mean that <em>x</em> is a value returned by the oracle that gives real numbers in the range (0,1) with equal likelihood. Also, note that using “(" and “)" to bracket a range excludes the associated bound, while using “[" and “]" includes it. Thus <em>U</em>[0,1) returns values from 0 to 1, including 0 but excluding 1.</p>&#13;
<p class="indent">A <em>normal distribution</em>, also called a <em>Gaussian</em> distribution, is visually a bell curve—a shape where one value is most likely, and then the likelihood of the other values decreases as one gets further from the most likely value. The most likely value is the mean, <span class="middle"><img src="Images/xbar.jpg" alt="Image"/></span>, and the parameter that controls how quickly the likelihood drops to zero (without ever really reaching it) is the standard deviation, <em>σ</em> (sigma). For our purposes, if we want a sample from a normal distribution, we’ll write <span class="middle"><img src="Images/008equ01.jpg" alt="Image" width="97" height="20"/></span> to mean <em>x</em> is drawn from a normal distribution with a mean of <span class="middle"><img src="Images/xbar.jpg" alt="Image" width="10" height="14"/></span> and a standard deviation of <em>σ</em>.</p>&#13;
<h4 class="h4" id="lev2_9">Statistical Tests</h4>&#13;
<p class="noindent">Another topic that will pop up from time to time is the idea of a <em>statistical test</em>, a measurement used to decide if a particular hypothesis is likely true or not. Typically, the hypothesis relates to two sets of measurements, and the hypothesis is that the two sets of measurements came from the same parent distribution. If the statistic calculated by the test is outside of a certain range, we reject the hypothesis and claim we have evidence that the two sets of measurements are <em>not</em> from the same parent distribution.</p>&#13;
<p class="indent">Here, we’ll usually use the <em>t-test</em>, a common statistical test that assumes our data is normally distributed. Because we assumed that our data is normally distributed, which may or may not be true, the t-test is known as a <em>parametric test</em>.</p>&#13;
<p class="indent">Sometimes, we’ll use another test, the <em>Mann–Whitney U test</em>, which is like a t-test in that it helps us decide if two samples are from the same parent distribution, but it makes no assumption about how the data values in the sample are themselves distributed. Tests like these are known as <em>nonparametric tests</em>.</p>&#13;
<p class="indent">Whether the test is parametric or nonparametric, the value we ultimately get from the test is called a <em>p-value</em>. It represents the probability that we would see the test statistic value we calculated if the hypothesis that the samples come from the same parent distribution is true. If the <em>p</em>-value is low, we have evidence that the hypothesis is not true.</p>&#13;
<p class="indent">The usual <em>p</em>-value cutoff is 0.05, indicating a 1 in 20 chance that we’d measure the test statistic value (t-test or Mann–Whitney U) even if the samples came from the same parent distribution. However, in recent years, it has become clear that this threshold is too generous. When <em>p</em>-values are near 0.05, but not above, we begin to think there is some evidence against the hypothesis. If the <em>p</em>-value is, say, 0.001 or even less, then we have strong <span epub:type="pagebreak" id="page_9"/>evidence that the samples are not from the same parent distribution. In this case, we say that the difference is <em>statistically significant</em>.</p>&#13;
<h3 class="h3" id="lev1_8">Graphics Processing Units</h3>&#13;
<p class="noindent">One of the enabling technologies for modern deep learning was the development of powerful <em>graphics processing units (GPUs)</em>. These are co-computers implemented on graphics cards. Originally designed for video gaming, the highly parallel nature of GPUs has been adapted to the extreme computational demands of deep neural network models. Many of the advances of recent years would not have been possible without the supercomputer-like abilities GPUs provide to even basic desktop computers. NVIDIA is the leader in the creation of GPUs for deep learning, and via its Compute Unified Device Architecture (CUDA), NVIDIA has been foundational to the success of deep learning. It’s not an understatement to say that without GPUs, deep learning would not have happened, or at least not been so widely used.</p>&#13;
<p class="indent">That said, we’re not expecting GPUs to be present for the models we’ll work with in this book. We’ll use small enough datasets and models so that we can train in a reasonable amount of time using just a CPU. We’ve already enforced this decision in the packages we’ve installed, since the version of TensorFlow we installed is a CPU-only version.</p>&#13;
<p class="indent">If you do have a CUDA-capable GPU and you want to use it for the deep learning portion of this book, please do so, but don’t think that you need to purchase one to run the examples. If you’re using a GPU, be sure to have CUDA properly installed before installing the packages indicated previously and be sure to install a GPU-enabled version of TensorFlow. The sklearn toolkit is CPU only.</p>&#13;
<h3 class="h3" id="lev1_9">Summary</h3>&#13;
<p class="noindent">In this chapter, we summarized our operating environment. Next, we described the essential Python toolkits we’ll use throughout the book and gave detailed instructions for installing the toolkits assuming an Ubuntu 20.04 Linux distribution. As mentioned, the toolkits will work just as nicely on many other Linux distributions as well as macOS. We then briefly reviewed some of the math we’ll encounter later and ended with an explanation of why we do not need GPUs for our models.</p>&#13;
<p class="indent">In the next chapter, we’ll review the fundamentals of Python.<span epub:type="pagebreak" id="page_10"/></p>&#13;
</div></body></html>