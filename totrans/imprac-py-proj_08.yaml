- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8'
- en: COUNTING SYLLABLES FOR HAIKU POETRY**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**为俳句诗歌计算音节**'
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Poetry may be the supreme form of literature. It is, as Coleridge put it, “the
    best words in the best order.” The poet must—with great brevity—tell a story,
    promote an idea, describe a scene, or evoke intensity of feeling, all while obeying
    strict rules on rhythm and rhyme, style and structure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 诗歌可能是文学的最高形式。正如柯尔律治所说：“最好的词汇排列在最佳的顺序中。”诗人必须以极其简练的方式讲述一个故事，提出一个观点，描述一个场景，或激发强烈的情感，同时遵循严格的节奏和韵律、风格和结构的规则。
- en: 'Computers love rules and structure and even have the potential to evoke emotions.
    In his 1996 book *Virtual Muse: Experiments in Computer Poetry*, author Charles
    Hartman describes early attempts to write algorithms that could mimic human poetry.
    To quote Hartman, “The complexity of poetic interaction, the tricky dance among
    poet and text and reader, causes a game of hesitation. In this game, a properly
    programmed computer has a chance to slip in some interesting moves.”'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机喜欢规则和结构，甚至有潜力唤起情感。在他1996年的书《虚拟缪斯：计算机诗歌实验》中，作者查尔斯·哈特曼描述了早期编写能够模仿人类诗歌的算法尝试。引用哈特曼的话：“诗歌互动的复杂性，诗人、文本和读者之间微妙的舞蹈，造成了一种犹豫不决的游戏。在这个游戏中，经过恰当编程的计算机有机会插入一些有趣的动作。”
- en: The early programs Hartman describes could, at best, produce bad beatnik poetry.
    The goal at the time was to “introduce calculated bits of mechanized anarchy into
    the language, put the results back into the contingent world where language lives,
    and see how the dust settles.” As we have touched on in several chapters, context
    is the weak link in programming things like proper-name anagrams and null ciphers.
    To write computer poems that pass the “supreme” test of literature, you cannot
    ignore context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 哈特曼描述的早期程序最多只能生成糟糕的“嬉皮”诗歌。那个时候的目标是“将计算出的机械化无序引入语言，将结果重新放回语言生活的偶然世界，看看灰尘如何沉淀。”正如我们在几章中提到的，编程中像是人名字谜和空白密码之类的情境问题是薄弱环节。要写出符合文学“至高”标准的计算机诗歌，你不能忽视情境。
- en: Getting a computer to simulate this most human of human endeavors is an intriguing
    challenge—and certainly not one we can pass up. In this chapter and the next,
    you’ll teach your computer how to generate a traditional form of Japanese poetry
    called *haiku*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让计算机模拟这种最具人类特征的行为是一个令人着迷的挑战——而且显然是我们不能放弃的。在本章和下一章中，你将教会你的计算机如何生成一种传统的日本诗歌形式，称为*俳句*。
- en: '**Japanese Haiku**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**日本俳句**'
- en: Haiku consist of three lines of five, seven, and five syllables, respectively.
    The poems rarely rhyme, and the subject matter usually addresses the natural world—mainly
    the seasons—either directly or indirectly. If done properly, a haiku can immerse
    you in the scene, as if evoking a memory.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 俳句由三行组成，分别为五、七、五个音节。诗歌很少押韵，主题通常直接或间接地涉及自然界——主要是四季。如果做得恰当，俳句能让你沉浸在场景中，仿佛唤起了记忆。
- en: I’ve provided three example haiku here. The first is by the master Buson (1715–1783),
    the second by the master Issa (1763–1828), and the third by yours truly, based
    on memories of childhood road trips.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里提供了三个俳句的例子。第一个是大师芭蕉（1715–1783）的作品，第二个是大师一茶（1763–1828）的作品，第三个则是我自己创作的，基于童年旅行的回忆。
- en: Standing still at dusk
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 傍晚时分站立
- en: Listen . . . in far distances
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 听吧……在遥远的地方
- en: The song of froglings!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 小青蛙的歌声！
- en: '*—Buson*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*—芭蕉*'
- en: Good friend grasshopper
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好朋友蝗虫
- en: Will you play the caretaker
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你会当那守墓人吗
- en: For my little grave?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为我的小坟墓？
- en: '*—Issa*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*—一茶*'
- en: Faraway cloudbanks
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 远方的云层
- en: That I let myself pretend
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我让自己假装
- en: Are distant mountains
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 远方的山脉
- en: '*—Vaughan*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*—沃恩*'
- en: 'Because of its evocative nature, every haiku has a built-in “exploitable gap”
    for the programmer. This is summed up nicely by Peter Beilenson in his 1955 book
    *Japanese Haiku*: “the haiku is not expected to be always a complete or even a
    clear statement. The reader is supposed to add to the words his own associations
    and imagery, and thus to become a co-creator of his own pleasure in the poem.”
    Hartman adds, “The reader’s mind works most actively on sparse materials. We draw
    the clearest constellations from the fewest stars. So, the nonsense factor is
    low for a tiny collocation of words that can be imbued with imagistic significance.”
    To put it simply, it’s harder to mess up a short poem. Readers always assume the
    poet had a point and will make one up themselves if they can’t find it.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其唤起性的特质，每首俳句都有一个内建的“可利用空白”供程序员使用。彼得·贝伦森在他1955年出版的《*日本俳句*》一书中很好地总结了这一点：“俳句并不要求总是一个完整甚至清晰的陈述。读者应该将自己的联想和形象加到这些词语中，从而成为自己在诗歌中享受的共同创作者。”哈特曼补充道：“读者的大脑在稀缺材料上工作最为活跃。我们从最少的星星中画出最清晰的星座。因此，对于一小段能够赋予形象意义的词语，无意义的因素很少。”简单来说，短诗更难出错。读者总是假设诗人有意图，如果找不到，他们会自己编造一个。
- en: Despite this advantage, training your computer to write poetry is no mean feat,
    and you’ll need two whole chapters to get it done. In this chapter, you’ll write
    a program that counts the number of syllables in words and phrases so that you
    can honor the syllabic structure of the haiku. In [Chapter 9](ch09.xhtml#ch09),
    you’ll use a technique called *Markov chain analysis* to capture the essence of
    haiku—the elusive evocative component—and transform existing poems into something
    new and, occasionally, arguably better.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，将计算机训练成能够写诗并非易事，你将需要两个完整的章节来完成。在本章中，你将编写一个程序来计算单词和短语中的音节数，以便尊重俳句的音节结构。在[第9章](ch09.xhtml#ch09)中，你将使用一种叫做*马尔可夫链分析*的技术来捕捉俳句的精髓——那种难以捉摸、引人入胜的成分——并将现有的诗歌转变成新的东西，偶尔，甚至可以说更好。
- en: '**Project #15: Counting Syllables**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #15：音节计数**'
- en: Counting syllables in English is difficult. The problem lies in, as Charles
    Hartman put it, the quirky spelling and tangled linguistic history of English.
    For example, a word like *aged* may be one syllable or two depending on whether
    it describes a man or a cheese. How can a program count syllables accurately without
    degenerating into an endless list of special cases?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用英语计算音节是困难的。问题在于，正如查尔斯·哈特曼所说，英语的拼写奇特且语言历史错综复杂。例如，像*aged*这样的词语，根据是描述一个人还是描述奶酪，可能是一音节或两音节。那么，如何写一个程序，在不陷入无休止的特殊情况列表的情况下准确地计算音节呢？
- en: 'The answer is that it can’t, at least not without a “cheat sheet.” Fortunately,
    these cheat sheets exist, thanks to a branch of science known as *natural language
    processing* (*NLP*), which deals with interactions between the precise and structured
    language of computers and the nuanced, frequently ambiguous “natural” language
    used by humans. Example uses for NLP include machine translations, spam detection,
    comprehension of search engine questions, and predictive text recognition for
    cell phone users. The biggest impact of NLP is yet to come: the mining of vast
    volumes of previously unusable, poorly structured data and engaging in seamless
    conversations with our computer overlords.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是它做不到，至少没有一个“备忘单”。幸运的是，正因为有一种叫做*自然语言处理*（*NLP*）的科学分支，这些备忘单才得以存在。自然语言处理研究计算机精确且结构化的语言与人类使用的充满细微差别和经常模糊的“自然”语言之间的互动。自然语言处理的应用示例包括机器翻译、垃圾邮件检测、搜索引擎问题的理解以及手机用户的预测文本识别。自然语言处理的最大影响还在未来：它将开采大量以前无法使用的、结构不良的数据，并与我们的计算机“统治者”进行无缝对话。
- en: 'In this chapter, you’ll use an NLP dataset to help count syllables in words
    or phrases. You’ll also write code that finds words that are missing from this
    dataset and then helps you build a supporting dictionary. Finally, you’ll write
    a program to help you check your syllable-counting code. In [Chapter 9](ch09.xhtml#ch09),
    you’ll use this syllable-counting algorithm as a module in a program that helps
    you computationally produce the highest achievement in literature: poetry.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用一个自然语言处理数据集来帮助计算单词或短语中的音节数。你还将编写代码，找到这个数据集中缺失的单词，并帮助你构建一个支持字典。最后，你将编写一个程序，帮助你检查音节计数代码。在[第9章](ch09.xhtml#ch09)中，你将把这个音节计数算法作为一个模块，应用到一个帮助你计算机化创作文学最高成就——诗歌——的程序中。
- en: '**THE OBJECTIVE**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**'
- en: Write a Python program that counts the number of syllables in an English word
    or phrase.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个 Python 程序，计算英语单词或短语中的音节数。
- en: '**The Strategy**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**策略**'
- en: For you and me, counting syllables is easy. Place the back of your hand just
    below your chin and start talking. Every time your chin hits your hand you’ve
    spoken a syllable. Computers don’t have hands or chins, but every vowel sound
    represents a syllable—and computers can count vowel sounds. It’s not easy, however,
    as there isn’t a simple rule for doing this. Some vowels in written language are
    silent, such as the *e* in *like*, and some combine to make a single sound, such
    as the *oo* in *moo*. Luckily, the number of words in the English language isn’t
    infinite. Fairly exhaustive lists are available that include much of the information
    you need.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对你和我来说，计算音节很简单。把手背放在下巴下面，然后开始说话。每次下巴碰到手背时，就说明你说出了一个音节。计算机没有手和下巴，但每个元音音素代表一个音节——而计算机可以计算元音音素。然而，这并不简单，因为没有简单的规则可以做到这一点。一些书面语言中的元音是静音的，比如
    *like* 中的 *e*，而有些元音组合发出一个音，比如 *moo* 中的 *oo*。幸运的是，英语语言中的单词数量并不是无限的。可以找到相当全面的列表，涵盖了你需要的大部分信息。
- en: A *corpus* is a fancy name for a body of text. In [Chapter 9](ch09.xhtml#ch09),
    you’ll use a *training corpus*—composed of haiku—that teaches Python how to write
    new haiku. In this chapter, you’ll use this same corpus to extract syllable counts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*语料库* 是一组文本的高级名称。在 [第 9 章](ch09.xhtml#ch09) 中，你将使用一个 *训练语料库*——由俳句组成——教 Python
    如何创作新的俳句。在这一章中，你将使用同一个语料库来提取音节计数。'
- en: Your syllable counter should evaluate both phrases and individual words, since
    you will ultimately use it to count the syllables in entire *lines* in a haiku.
    The program will take some text as an input, count the number of syllables in
    each word, and return the total syllable count. You’ll also have to deal with
    things like punctuation, whitespace, and missing words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你的音节计数器应评估短语和单个单词，因为你最终将用它来计算整行 *俳句* 的音节数。该程序将接受文本输入，计算每个单词的音节数，并返回总的音节数。你还需要处理标点符号、空格和缺失单词等问题。
- en: 'The primary steps you need to follow are:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要遵循的主要步骤是：
- en: Download a large corpus with syllable-count information.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一个包含音节计数信息的大型语料库。
- en: Compare the syllable-count corpus to the haiku-training corpus and identify
    all the words missing from the syllable-count corpus.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将音节计数语料库与俳句训练语料库进行比较，找出音节计数语料库中缺失的所有单词。
- en: Build a dictionary of the missing words and their syllable counts.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个字典，包含缺失单词及其音节计数。
- en: Write a program that uses both the syllable-count corpus and the missing-words
    dictionary to count syllables in the training corpus.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个程序，利用音节计数语料库和缺失单词字典来计算训练语料库中的音节数。
- en: Write a program that checks the syllable-counting program against updates of
    the training corpus.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个程序，检查音节计数程序与训练语料库的更新是否一致。
- en: '***Using a Corpus***'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用语料库***'
- en: The Natural Language Toolkit (NLTK) is a popular suite of programs and libraries
    for working with human language data in Python. It was created in 2001 as part
    of a computational linguistics course in the Department of Computer and Information
    Science at the University of Pennsylvania. Development and expansion have continued
    with the help of dozens of contributors. To learn more, check out the official
    NLTK website at *[http://www.nltk.org/](http://www.nltk.org/)*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言工具包（NLTK）是一个流行的程序和库套件，用于在 Python 中处理人类语言数据。它于 2001 年作为宾夕法尼亚大学计算机与信息科学系的一门计算语言学课程的一部分创建。经过众多贡献者的帮助，开发和扩展持续进行。如果你想了解更多，可以访问官方
    NLTK 网站 *[http://www.nltk.org/](http://www.nltk.org/)*。
- en: For this project, you will use NLTK to access the Carnegie Mellon University
    Pronouncing Dictionary (CMUdict). This corpus contains almost 125,000 words mapped
    to their pronunciations. It is machine readable and useful for tasks such as speech
    recognition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用 NLTK 访问卡内基梅隆大学发音字典（CMUdict）。这个语料库包含了近 125,000 个单词及其对应的发音。它是机器可读的，适用于语音识别等任务。
- en: '***Installing NLTK***'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***安装 NLTK***'
- en: 'You can find instructions for installing NLTK on Unix, Windows, and macOS at
    *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*. If you
    are using Windows, I suggest you start by opening Windows Command Prompt or PowerShell
    and trying to install with pip:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*
    找到在 Unix、Windows 和 macOS 上安装 NLTK 的说明。如果你使用的是 Windows，我建议你首先打开 Windows 命令提示符或
    PowerShell，并尝试使用 pip 安装：
- en: python -m pip install nltk
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: python -m pip install nltk
- en: 'You can check the installation by opening the Python interactive shell and
    typing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过打开 Python 交互式 shell 并输入以下命令来检查安装情况：
- en: '>>> import nltk'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import nltk'
- en: '>>>'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '>>>'
- en: If you don’t get an error, you’re good to go. Otherwise, follow the instructions
    on the website just cited.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有遇到错误，你就可以开始使用了。否则，请按照刚才引用的网页上的说明进行操作。
- en: '***Downloading CMUdict***'
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***正在下载 CMUdict***'
- en: 'To get access to CMUdict (or any of the other NLTK corpora), you have to download
    it. You can do this using the handy NLTK Downloader. Once you’ve installed NLTK,
    enter the following into the Python shell:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 CMUdict（或其他任何 NLTK 语料库），你需要先下载它。你可以使用方便的 NLTK 下载器来实现这一点。一旦你安装了 NLTK，在 Python
    shell 中输入以下内容：
- en: '>>> import nltk'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import nltk'
- en: '>>> nltk.download()'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> nltk.download()'
- en: The NLTK Downloader window ([Figure 8-1](ch08.xhtml#ch08fig1)) should now be
    open. Click the **Corpora** tab near the top, then click **cmudict** in the Identifier
    column. Next, scroll to the bottom of the window and set the Download Directory;
    I used the default, *C:\nltk_data*. Finally, click the **Download** button to
    load CMUdict.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该打开 NLTK 下载器窗口（[图 8-1](ch08.xhtml#ch08fig1)）。点击顶部附近的**Corpora**标签，然后在标识符列中点击**cmudict**。接下来，滚动到窗口底部并设置下载目录；我使用了默认目录，*C:\nltk_data*。最后，点击**Download**按钮加载
    CMUdict。
- en: '![image](../images/f0149-01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0149-01.jpg)'
- en: '*Figure 8-1: The NLTK Downloader window with cmudict selected for download*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-1：带有选中 cmudict 下载项的 NLTK 下载器窗口*'
- en: 'When CMUdict has finished downloading, exit the Downloader and enter the following
    into the Python interactive shell:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CMUdict 下载完成后，退出下载器并在 Python 交互式 shell 中输入以下内容：
- en: '>>> from nltk.corpus import cmudict'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from nltk.corpus import cmudict'
- en: '>>>'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '>>>'
- en: If you don’t encounter an error, then the corpus has been successfully downloaded.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有遇到错误，那么语料库已经成功下载。
- en: '***Counting Sounds Instead of Syllables***'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***按音素计数而非音节计数***'
- en: The CMUdict corpus breaks words into sets of *phonemes*—perceptually distinct
    *units* of sound in a specified language—and marks vowels for lexical stress using
    numbers (0, 1, and 2). The CMUdict corpus marks every vowel with one, and only
    one, of these numbers, so you can use the numbers to identify the vowels in a
    word.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: CMUdict 语料库将单词分解为一组*音素*——在指定语言中感知上不同的*声音单位*——并使用数字（0、1 和 2）标记元音的词汇重音。CMUdict
    语料库将每个元音标记为其中一个数字，而且只有一个数字，因此你可以使用这些数字来识别单词中的元音。
- en: 'Looking at words as a set of phonemes will help you sidestep a few problems.
    For one, CMUdict will not include vowels in the written word that are unpronounced.
    For example, here’s how CMUdict sees the word *scarecrow*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词视为一组音素将帮助你避免一些问题。首先，CMUdict 不会在书写的单词中包含不发音的元音。例如，CMUdict 如何处理单词*scarecrow*：
- en: '[[''S'', ''K'', ''AE1'', ''R'', ''K'', ''R'', ''OW0'']]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[[''S'', ''K'', ''AE1'', ''R'', ''K'', ''R'', ''OW0'']]'
- en: Each item with a numerical suffix represents a pronounced vowel. Note that the
    silent *e* at the end of *scare* is correctly omitted.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个带有数字后缀的项目代表一个发音的元音。请注意，*scare* 结尾的静音*e*被正确省略了。
- en: 'Second, sometimes multiple and consecutive written vowels are pronounced as
    just a single phoneme. For example, this is how CMUdict represents *house*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，有时多个连续的书写元音会被发音为单一的音素。例如，CMUdict 如何表示*house*：
- en: '[[''HH'', ''AW1'', ''S'']]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[[''HH'', ''AW1'', ''S'']]'
- en: Note how the corpus treats the written double vowels *ou* as a single vowel,
    'AW1', for pronunciation purposes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意语料库如何将书写中的双元音*ou*当作一个元音，'AW1'，来处理发音。
- en: '***Handling Words with Multiple Pronunciations***'
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***处理具有多种发音的单词***'
- en: 'As I mention in the introduction, some words have multiple distinct pronunciations;
    *aged* and *learned* are just two examples:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在介绍中提到的，一些单词有多种不同的发音；*aged* 和 *learned* 只是两个例子：
- en: '[[''EY1'', ''JH'', ''D''], [''EY1'', ''JH'', ''IH0'', ''D'']]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[[''EY1'', ''JH'', ''D''], [''EY1'', ''JH'', ''IH0'', ''D'']]'
- en: '[[''L'', ''ER1'', ''N'', ''D''], [''L'', ''ER1'', ''N'', ''IH0'', ''D'']]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[[''L'', ''ER1'', ''N'', ''D''], [''L'', ''ER1'', ''N'', ''IH0'', ''D'']]'
- en: Note the nested lists. The corpus recognizes that both words can be pronounced
    with one or two syllables. This means it will return more than one syllable count
    for certain words, something you will have to account for in your code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意嵌套的列表。语料库识别到这两个单词可以发音为一个或两个音节。这意味着它会为某些单词返回多个音节计数，代码中需要考虑这一点。
- en: '**Managing Missing Words**'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**管理缺失的单词**'
- en: CMUdict is very useful, but with a corpus, a word is either there or not. It
    took only seconds to find more than 50 words—like *dewdrop*, *bathwater*, *dusky*,
    *ridgeline*, *storks*, *dragonfly*, *beggar*, and *archways*—missing from CMUdict
    in a 1,500-word test case. So, one of your strategies should be to check CMUdict
    for missing words and then address any omissions by making a corpus for your corpus!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CMUdict非常有用，但对于一个语料库，单词要么在，要么不在。仅仅在一个1500字的测试案例中，就花了几秒钟找到了50多个缺失的单词——如*dewdrop*、*bathwater*、*dusky*、*ridgeline*、*storks*、*dragonfly*、*beggar*和*archways*——这些单词在CMUdict中找不到。所以，你的策略之一应该是检查CMUdict中缺失的单词，然后通过为你的语料库制作一个语料库来弥补这些遗漏！
- en: '***The Training Corpus***'
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***训练语料库***'
- en: In [Chapter 9](ch09.xhtml#ch09), you’ll use a training corpus of several hundred
    haiku to “teach” your program how to write new ones. But you can’t count on CMUdict
    to contain all the words in this corpus because some will be Japanese words, like
    *sake*. And as you already saw, even some common English words are missing from
    CMUdict.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)中，你将使用包含几百首俳句的训练语料库来“教”你的程序如何生成新的俳句。但是你不能指望CMUdict包含这个语料库中的所有单词，因为其中有些是日语单词，比如*sake*。正如你之前所看到的，即便是一些常见的英语单词，也没有出现在CMUdict中。
- en: So the first order of business is to check all the words in the training corpus
    for membership in CMUdict. To do this, you’ll need to download the training corpus,
    called *train.txt*, from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*.
    Keep it in the same folder as all the Python programs from this chapter. The file
    contains slightly under 300 haiku that have been randomly duplicated around 20
    times to ensure a robust training set.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先要做的就是检查训练语料库中所有单词是否在CMUdict中。为此，你需要下载名为*train.txt*的训练语料库，从*[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*获取。把它和本章所有的Python程序放在同一文件夹中。该文件包含不到300首俳句，这些俳句被随机复制了大约20次，以确保训练集的多样性。
- en: Once you find words that aren’t in CMUdict, you’ll write a script to help you
    prepare a Python dictionary that uses words as keys and syllable counts as values;
    then you’ll save this dictionary to a file that can support CMUdict in the syllable-counting
    program.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你发现CMUdict中没有的单词，你就需要编写一个脚本，帮助你准备一个以单词为键，音节数为值的Python字典；然后你将把这个字典保存到一个支持CMUdict的文件中，以便在音节计数程序中使用。
- en: '***The Missing Words Code***'
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***缺失单词代码***'
- en: The code in this section will find words missing from CMUdict, help you prepare
    a dictionary of the words and their syllable counts, and save the dictionary to
    a file. You can download the code from *[https://nostarch.com/impracticalpython/](https://nostarch.com/impracticalpython/)*
    as *missing_words_finder.py*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码将找到CMUdict中缺失的单词，帮助你准备一个包含这些单词及其音节数的字典，并将字典保存到文件中。你可以从*[https://nostarch.com/impracticalpython/](https://nostarch.com/impracticalpython/)*下载代码，文件名为*missing_words_finder.py*。
- en: '**Importing Modules, Loading CMUdict, and Defining the main() Function**'
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入模块，加载CMUdict，并定义main()函数**'
- en: '[Listing 8-1](ch08.xhtml#ch08list1) imports modules, loads CMUdict, and defines
    the main() function that runs the program.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码清单8-1](ch08.xhtml#ch08list1)导入模块，加载CMUdict，并定义运行程序的main()函数。'
- en: '*missing_words_finder.py,* part 1'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py,* 第1部分'
- en: import sys
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: from string import punctuation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: from string import punctuation
- en: ➊ import pprint
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ import pprint
- en: import json
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: from nltk.corpus import cmudict
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: from nltk.corpus import cmudict
- en: ➋ cmudict = cmudict.dict()  # Carnegie Mellon University Pronouncing Dictionary
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ cmudict = cmudict.dict()  # 卡内基梅隆大学发音词典'
- en: '➌ def main():'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ def main():'
- en: ➍ haiku = load_haiku('train.txt')
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ haiku = load_haiku('train.txt')
- en: ➎ exceptions = cmudict_missing(haiku)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ exceptions = cmudict_missing(haiku)
- en: ➏ build_dict = input("\nManually build an exceptions dictionary (y/n)? \n")
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ build_dict = input("\n是否手动构建异常字典（y/n）？\n")
- en: 'if build_dict.lower() == ''n'':'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果build_dict.lower() == 'n'：
- en: sys.exit()
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: sys.exit()
- en: 'else:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: ➐ missing_words_dict = make_exceptions_dict(exceptions)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ missing_words_dict = make_exceptions_dict(exceptions)
- en: save_exceptions(missing_words_dict)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: save_exceptions(missing_words_dict)
- en: '*Listing 8-1: Imports modules, loads CMUdict, and defines* main()'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码清单8-1：导入模块，加载CMUdict，并定义* main()'
- en: You start with some familiar imports and a few new ones. The pprint module lets
    you “pretty print” your dictionary of missing words in an easy-to-read format
    ➊. You’ll write out this same dictionary as persistent data using JavaScript Object
    Notation (json), a text-based way for computers to exchange data that works well
    with Python data structures; it’s part of the standard library, standardized across
    multiple languages, and the data is secure and human readable. Finish by importing
    the CMUdict corpus.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你从一些熟悉的导入开始，并增加了几个新的。pprint模块允许你以易于阅读的格式“漂亮地打印”缺失词汇的字典 ➊。你将使用JavaScript对象表示法（json）将这个字典写出为持久化数据，这是一种基于文本的计算机数据交换格式，非常适合Python数据结构；它是标准库的一部分，在多种语言中都有标准化，且数据既安全又易于人类阅读。最后，导入CMUdict语料库。
- en: Next, call the cmudict module’s dict() method to turn the corpus into a dictionary
    with the words as keys and their phonemes as values ➋.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用cmudict模块的dict()方法，将语料库转换为一个字典，词汇作为键，音标作为值 ➋。
- en: Define the main() function that will call functions to load the training corpus,
    find missing words in CMUdict, build a dictionary with the words and their syllable
    counts, and save the results ➌. You’ll define these functions after defining main().
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 定义main()函数，调用函数加载训练语料库，查找CMUdict中缺失的词汇，构建一个包含词汇及其音节计数的字典，并保存结果 ➌。你将在定义main()之后定义这些函数。
- en: Call the function to load the haiku-training corpus and assign the returned
    set to a variable named haiku ➍. Then call the function that will find the missing
    words and return them as a set ➎. Using sets removes duplicate words that you
    don’t need. The cmudict_missing() function will also display the number of missing
    words and some other statistics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 调用函数加载俳句训练语料库，并将返回的集合赋值给名为haiku的变量 ➍。然后调用另一个函数，找到缺失的词汇并将其返回为一个集合 ➎。使用集合可以去除不需要的重复词汇。cmudict_missing()函数还会显示缺失词汇的数量以及其他一些统计信息。
- en: Now, ask the user if they want to manually build a dictionary to address the
    missing words and assign their input to the build_dict variable ➏. If they want
    to stop, exit the program; otherwise, call a function to build the dictionary
    ➐ and then another one to save the dictionary. Note that the user isn’t restricted
    to pressing y if they want to continue, though that’s the prompt.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，询问用户是否希望手动构建一个字典以解决缺失的词汇，并将他们的输入赋值给build_dict变量 ➏。如果他们想停止，退出程序；否则，调用一个函数来构建字典
    ➐，然后再调用另一个函数来保存字典。请注意，用户并不限于按y键继续，尽管这是提示信息。
- en: '**Loading the Training Corpus and Finding Missing Words**'
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**加载训练语料库并查找缺失的词汇**'
- en: '[Listing 8-2](ch08.xhtml#ch08list2) loads and prepares the training corpus,
    compares its contents to CMUdict, and keeps track of the differences. These tasks
    are divided between two functions.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-2](ch08.xhtml#ch08list2)加载并准备训练语料库，将其内容与CMUdict进行比较，并跟踪差异。这些任务分为两个函数。'
- en: '*missing_words_finder.py,* part 2'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py,* 第2部分'
- en: '➊ def load_haiku(filename):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def load_haiku(filename):'
- en: '"""Open and return training corpus of haiku as a set."""'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '"""打开并返回作为集合的俳句训练语料库。"""'
- en: 'with open(filename) as in_file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(filename) as in_file:'
- en: ➋ haiku = set(in_file.read().replace('-', ' ').split())
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ haiku = set(in_file.read().replace('-', ' ').split())
- en: ➌ return haiku
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ return haiku
- en: 'def cmudict_missing(word_set):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'def cmudict_missing(word_set):'
- en: '"""Find and return words in word set missing from cmudict."""'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '"""查找并返回在词汇集内但缺失于cmudict的词汇。"""'
- en: ➍ exceptions = set()
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ exceptions = set()
- en: 'for word in word_set:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'for word in word_set:'
- en: word = word.lower().strip(punctuation)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: word = word.lower().strip(punctuation)
- en: 'if word.endswith("''s") or word.endswith("’s"):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'if word.endswith("''s") or word.endswith("’s"):'
- en: word = word[:-2]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: word = word[:-2]
- en: '➎ if word not in cmudict:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '➎ if word not in cmudict:'
- en: exceptions.add(word)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: exceptions.add(word)
- en: print("\nexceptions:")
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: print("\n异常词汇:")
- en: print(*exceptions, sep='\n')
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: print(*exceptions, sep='\n')
- en: ➏ print("\nNumber of unique words in haiku corpus = {}"
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ print("\n俳句语料库中的唯一词汇数量 = {}"
- en: .format(len(word_set)))
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: .format(len(word_set)))
- en: print("Number of words in corpus not in cmudict = {}"
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: print("语料库中不在cmudict中的词汇数量 = {}"
- en: .format(len(exceptions)))
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: .format(len(exceptions)))
- en: membership = (1 - (len(exceptions) / len(word_set))) * 100
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: membership = (1 - (len(exceptions) / len(word_set))) * 100
- en: ➐ print("cmudict membership = {:.1f}{}".format(membership, '%'))
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ print("cmudict匹配度 = {:.1f}{}".format(membership, '%'))
- en: return exceptions
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: return exceptions
- en: '*Listing 8-2: Defines functions to load the corpus and finds words missing
    from CMUdict*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-2：定义了加载语料库的函数，并找到缺失的词汇*'
- en: Define a function to read in the words from the haiku-training corpus ➊. The
    haiku in *train.txt* have been duplicated many times, plus the original haiku
    contain duplicate words, like *moon*, *mountain*, and *the*. There’s no point
    in evaluating a word more than once, so load the words as a set to remove repeats
    ➋. You also need to replace hyphens with spaces. Hyphens are popular in haiku,
    but you need to separate the words on either side in order to check for them in
    CMUdict. End the function by returning the haiku set ➌.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个函数来读取俳句训练语料库中的单词➊。*train.txt*中的俳句被多次重复，而且原始俳句中有重复的词汇，如*moon*、*mountain*和*the*。没有必要重复评估一个词汇，因此将这些词汇作为集合加载，以去除重复项➋。你还需要将连字符替换为空格。连字符在俳句中很常见，但你需要将连字符两侧的词分开，以便在CMUdict中进行查找。函数结束时返回俳句集合➌。
- en: It’s now time to find missing words. Define a function, cmudict_missing(), that
    takes as an argument a sequence—in this case, the set of words returned by the
    load_haiku() function. Start an empty set called exceptions to hold any missing
    words ➍. Loop through each word in the haiku set, converting it to lowercase and
    stripping any leading or trailing punctuation. Note that you don’t want to remove
    interior punctuation other than hyphens because CMUdict recognizes words like
    *wouldn’t*. Possessive words typically aren’t in a corpus, so remove the trailing
    *’s*, since this won’t affect the syllable count.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候查找缺失的词汇了。定义一个函数cmudict_missing()，该函数以一个序列作为参数——在本例中是由load_haiku()函数返回的词汇集合。创建一个空集合exceptions，用于保存任何缺失的词汇
    ➍。遍历俳句集合中的每个词，将其转换为小写，并去除任何前后的标点符号。请注意，不要去除除了连字符以外的内部标点，因为CMUdict会识别像*wouldn’t*这样的词汇。所有格的词通常不在语料库中，因此需要去掉末尾的*’s*，因为这不会影响音节数。
- en: '**NOTE**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Be careful of curly apostrophes (’) produced by word-processing software.
    These are different from the straight apostrophes (*''*) used in simple text editors
    and shells and may not be recognized by CMUdict. If you add new words to either
    the training or JSON files, be sure to use a straight apostrophe for contractions
    or possessive nouns.*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*小心词处理软件产生的弯引号（’）。这些与简单文本编辑器和Shell中使用的直引号（*''*）不同，可能不会被CMUdict识别。如果你在训练文件或JSON文件中添加新词汇，请确保使用直引号来表示缩写或所有格名词。*'
- en: If the word isn’t found in CMUdict, add it to exceptions ➎. Print these words
    as a check, along with some basic information ➏, like how many unique words, how
    many missing words, and what percentage of the training corpus are members of
    CMUdict. Set the percent value precision to one decimal place ➐. End the function
    by returning the set of exceptions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在CMUdict中找不到该词，请将其添加到异常词汇表➎。打印这些词作为检查，并附上基本信息➏，例如有多少个唯一词汇，有多少个缺失的词汇，以及CMUdict中包含的训练语料库的百分比。将百分比的精度设置为小数点后一位➐。函数的结束返回异常词汇表。
- en: '**Building a Dictionary of Missing Words**'
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**构建缺失词汇的字典**'
- en: '[Listing 8-3](ch08.xhtml#ch08list3) continues the *missing_words_finder.py*
    code, now supplementing CMUdict by assigning syllable counts to the missing words
    as values in a Python dictionary. Since the number of missing words should be
    relatively small, the user can assign the counts manually, so write the code to
    help them interact with the program.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 8-3](ch08.xhtml#ch08list3)继续了*missing_words_finder.py*的代码，现在通过将音节数作为值分配给缺失的词汇，补充了CMUdict。由于缺失词汇的数量应该相对较少，用户可以手动分配音节数，因此编写代码帮助他们与程序交互。'
- en: '*missing_words_finder.py,* part 3'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py,* 第三部分'
- en: '➊ def make_exceptions_dict(exceptions_set):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def make_exceptions_dict(exceptions_set):'
- en: '"""Return dictionary of words & syllable counts from a set of words."""'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '"""返回一个包含词汇和音节数的字典，来自一个词汇集合。"""'
- en: ➋ missing_words = {}
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ missing_words = {}
- en: 'print("Input # syllables in word. Mistakes can be corrected at end. \n")'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: print("输入词汇的音节数。错误可以在最后更正。\n")
- en: 'for word in exceptions_set:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'for word in exceptions_set:'
- en: 'while True:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'while True:'
- en: '➌ num_sylls = input("Enter number syllables in {}: ".format(word))'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ num_sylls = input("请输入{}的音节数：".format(word))
- en: '➍ if num_sylls.isdigit():'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ if num_sylls.isdigit():'
- en: break
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: 'else:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: print("                 Not a valid answer!", file=sys.stderr)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: print("                 无效的答案！", file=sys.stderr)
- en: ➎ missing_words[word] = int(num_sylls)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ missing_words[word] = int(num_sylls)
- en: print()
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: ➏ pprint.pprint(missing_words, width=1)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ pprint.pprint(missing_words, width=1)
- en: ➐ print("\nMake Changes to Dictionary Before Saving?")
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ print("\n在保存之前是否要修改字典？")
- en: print("""
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: print("""
- en: 0 - Exit & Save
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 0 - 退出并保存
- en: 1 – Add a Word or Change a Syllable Count
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 1 – 添加词汇或更改音节数
- en: 2 - Remove a Word
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 2 - 删除词汇
- en: '""")'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '""")'
- en: '➑ while True:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '➑ while True:'
- en: 'choice = input("\nEnter choice: ")'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: choice = input("\n请输入选项：")
- en: 'if choice == ''0'':'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'if choice == ''0'':'
- en: break
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: 'elif choice == ''1'':'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif choice == ''1'':'
- en: 'word = input("\nWord to add or change: ")'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: word = input("\n要添加或更改的单词：")
- en: 'missing_words[word] = int(input("Enter number syllables in {}: "'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: missing_words[word] = int(input("请输入 {} 的音节数："
- en: .format(word)))
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: .format(word)))
- en: 'elif choice == ''2'':'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif choice == ''2'':'
- en: 'word = input("\nEnter word to delete: ")'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: word = input("\n请输入要删除的单词：")
- en: ➒ missing_words.pop(word, None)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ missing_words.pop(word, None)
- en: print("\nNew words or syllable changes:")
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: print("\n新增单词或音节更改：")
- en: ➓ pprint.pprint(missing_words, width=1)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ➓ pprint.pprint(missing_words, width=1)
- en: return missing_words
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: return missing_words
- en: '*Listing 8-3: Allows the user to manually count syllables and builds a dictionary*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-3：允许用户手动计数音节并构建字典*'
- en: Start by defining a function that takes the set of exceptions returned by the
    cmudict_missing() function as an argument ➊. Immediately assign an empty dictionary
    to a variable named missing_words ➋. Let the user know that if they make a mistake,
    they’ll have a chance to fix it later; then, use a for and while loop to go through
    the set of missing words and present each word to the user, asking for the number
    of syllables as input. The word will be the dictionary key, and the num_sylls
    variable will become its value ➌. If the input is a digit ➍, break out of the
    loop. Otherwise, warn the user and let the while loop request input again. If
    the input passes, add the value to the dictionary as an integer ➎.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义一个函数，接受由 cmudict_missing() 函数返回的异常集合作为参数 ➊。立即将一个空字典分配给名为 missing_words 的变量
    ➋。告知用户如果他们犯了错，他们稍后会有机会修正；然后，使用 for 和 while 循环遍历缺失单词集，并展示每个单词给用户，要求输入音节数。单词将作为字典的键，而
    num_sylls 变量将成为其值 ➌。如果输入是数字 ➍，则退出循环。否则，警告用户并让 while 循环再次请求输入。如果输入通过，则将值作为整数添加到字典中
    ➎。
- en: Use pprint to display each key/value pair on a separate line, as a check. The
    width parameter acts as a newline argument ➏.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pprint 将每个键/值对显示在单独的一行，以进行检查。width 参数充当换行符的作用 ➏。
- en: Give the user the opportunity to make last-minute changes to the missing_words
    dictionary before saving it as a file ➐. Use triple quotes to present the options
    menu, followed by a while loop to keep the options active until the user is ready
    to save ➑. The three options are exiting, which invokes the break command; adding
    a new word or changing the syllable count for an existing word, which requires
    the word and syllable count as input; and removing an entry, which uses the dictionary
    pop() function ➒. Adding the None argument to pop() means the program won’t raise
    a KeyError if the user enters a word that’s not in the dictionary.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 给用户提供在保存缺失单词字典为文件之前进行最后修改的机会 ➐。使用三引号展示选项菜单，然后通过 while 循环保持选项活动，直到用户准备好保存 ➑。这三个选项分别是退出，触发
    break 命令；添加新单词或更改现有单词的音节数，要求输入单词和音节数；以及删除条目，使用字典的 pop() 函数 ➒。向 pop() 添加 None 参数意味着如果用户输入的单词不在字典中，程序不会引发
    KeyError。
- en: Finish by giving the user a last look at the dictionary, in the event changes
    were made ➓, and then return it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，给用户一个最后查看字典的机会，以防有更改 ➓，然后返回字典。
- en: '**Saving the Missing Words Dictionary**'
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**保存缺失单词字典**'
- en: '*Persistent data* is data that is preserved after a program terminates. To
    make the missing_words dictionary available for use in the *count_syllables.py*
    program you’ll write later in this chapter, you need to save it to a file. [Listing
    8-4](ch08.xhtml#ch08list4) does just that.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*持久数据*是指程序终止后仍然保留的数据。为了让缺失单词字典能够在你稍后编写的 *count_syllables.py* 程序中使用，你需要将其保存到一个文件中。[清单
    8-4](ch08.xhtml#ch08list4) 正是完成了这一操作。'
- en: '*missing_words_finder.py,* part 4'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py,* 第四部分'
- en: '➊ def save_exceptions(missing_words):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ def save_exceptions(missing_words):'
- en: '"""Save exceptions dictionary as json file."""'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '"""将异常字典保存为 json 文件。"""'
- en: ➋ json_string = json.dumps(missing_words)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ json_string = json.dumps(missing_words)
- en: ➌ f = open('missing_words.json', 'w')
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ f = open('missing_words.json', 'w')
- en: f.write(json_string)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(json_string)
- en: f.close()
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: f.close()
- en: ➍ print("\nFile saved as missing_words.json")
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ print("\n文件已保存为 missing_words.json")
- en: '➎ if __name__ == ''__main__'':'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '➎ if __name__ == ''__main__'':'
- en: main()
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: main()
- en: '*Listing 8-4: Saves missing-words dictionary to a file and calls* main()'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-4：将缺失单词字典保存为文件并调用* main()'
- en: Use json to save the dictionary. Define a new function that takes the set of
    missing words as an argument ➊. Assign the missing_words dictionary to a new variable
    named json_string ➋; then, open a file with a *.json* extension ➌, write the json
    variable, and close the file. Display the name of the file as a reminder to the
    user ➍. End with the code that lets the program be run as a module or in stand-alone
    mode ➎.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用json保存字典。定义一个新函数，以缺失单词的集合作为参数 ➊。将missing_words字典分配给一个名为json_string的新变量 ➋；然后，打开一个*.json*扩展名的文件
    ➌，写入json变量，并关闭文件。以文件名提醒用户 ➍。最后，使用代码让程序可以作为模块或独立模式运行 ➎。
- en: 'The json.dumps() method serializes the missing_words dictionary into a string.
    *Serialization* is the process of converting data into a more transmittable or
    storable format. For example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: json.dumps()方法将missing_words字典序列化为字符串。*序列化*是将数据转换为更易传输或存储的格式的过程。例如：
- en: '>>> import json'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import json'
- en: '>>> d = {''scarecrow'': 2, ''moon'': 1, ''sake'': 2}'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> d = {''scarecrow'': 2, ''moon'': 1, ''sake'': 2}'
- en: '>>> json.dumps(d)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> json.dumps(d)'
- en: '''{"sake": 2, "scarecrow": 2, "moon": 1}'''
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '''{"sake": 2, "scarecrow": 2, "moon": 1}'''
- en: Note that the serialized dictionary is bound by single quotes, making it a string.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，序列化字典由单引号括起来，这使其成为一个字符串。
- en: I’ve provided a partial output from *missing_words_finder.py* here. The list
    of missing words at the top and the manual syllable counts at the bottom have
    both been shortened for brevity.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里提供了*missing_words_finder.py*的部分输出。顶部是缺失单词列表，底部是手动填写的音节数，因简洁起见都已被缩短。
- en: --snip--
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: --省略--
- en: froglings
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 小蛙
- en: scatters
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 散布
- en: paperweights
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 文件重
- en: hibiscus
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 木槿花
- en: cumulus
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 云状云
- en: nightingales
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 夜莺
- en: Number of unique words in haiku corpus = 1523
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 俳句语料库中唯一单词的数量 = 1523
- en: Number of words in corpus not in cmudict = 58
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中不在cmudict中的单词数量 = 58
- en: cmudict membership = 96.2%
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: cmudict成员身份 = 96.2%
- en: Manually build an exceptions dictionary (y/n)?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建异常字典（y/n）？
- en: y
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: y
- en: 'Enter number syllables in woodcutter: 3'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在樵夫中输入音节数：3
- en: 'Enter number syllables in morningglory: 4'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在牵牛花中输入音节数：4
- en: 'Enter number syllables in cumulus: 3'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在云状云中输入音节数：3
- en: --snip--
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: --省略--
- en: Don’t worry—you won’t have to assign all the syllable counts. The *missing_words.json*
    file is complete and ready for download when you need it.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——你不需要为所有音节数分配值。*missing_words.json*文件已经完成，并且在需要时可以下载。
- en: '**NOTE**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*For words that have multiple pronunciations, like* jagged *or* our*, you can
    force the program to use the one you prefer by manually opening the* missing_words.json
    *file and adding the key/value pair at any location. I did this with the word*
    sake *so that it uses the two-syllable Japanese pronunciation. Because word membership
    is checked in this file first, it will override the CMUdict value.*'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于具有多种发音的单词，例如* jagged *或* our*，你可以通过手动打开* missing_words.json *文件并在任何位置添加键值对来强制程序使用你偏好的发音。我对单词*sake*做了这个处理，以便它使用两个音节的日语发音。因为首先在这个文件中检查单词的成员身份，它将覆盖CMUdict中的值。*'
- en: Now that you’ve addressed the holes in CMUdict, you’re ready to write the code
    that counts syllables. In [Chapter 9](ch09.xhtml#ch09), you’ll use this code as
    a module in the *markov_haiku.py* program.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经解决了CMUdict中的空白，准备编写计算音节数的代码。在[第9章](ch09.xhtml#ch09)，你将把这段代码作为模块在*markov_haiku.py*程序中使用。
- en: '**The Count Syllables Code**'
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**音节计数代码**'
- en: This section contains the code for the *count_syllables.py* program. You’ll
    also need the *missing_words.json* file you created in the previous section. You
    can download both from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*.
    Keep them together in the same folder.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含*count_syllables.py*程序的代码。你还需要在上一节中创建的*missing_words.json*文件。你可以从*[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*下载它们。将它们保存在同一文件夹中。
- en: '***Prepping, Loading, and Counting***'
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***准备、加载和计数***'
- en: '[Listing 8-5](ch08.xhtml#ch08list5) imports the necessary modules, loads the
    CMUdict and missing-words dictionaries, and defines a function that will count
    the syllables in a given word or phrase.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-5](ch08.xhtml#ch08list5)导入所需的模块，加载CMUdict和缺失单词字典，并定义一个函数，用于计算给定单词或短语的音节数。'
- en: '*count_syllables.py,* part 1'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*count_syllables.py，* 第1部分'
- en: import sys
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: from string import punctuation
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: from string import punctuation
- en: import json
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: from nltk.corpus import cmudict
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: from nltk.corpus import cmudict
- en: '# load dictionary of words in haiku corpus but not in cmudict'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '# 加载不在cmudict中的俳句语料库单词字典'
- en: 'with open(''missing_words.json'') as f:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''missing_words.json'') as f:'
- en: missing_words = json.load(f)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: missing_words = json.load(f)
- en: ➊ cmudict = cmudict.dict()
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ cmudict = cmudict.dict()
- en: '➋ def count_syllables(words):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ def count_syllables(words):'
- en: '"""Use corpora to count syllables in English word or phrase."""'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '"""使用语料库计算英语单词或短语中的音节数。"""'
- en: '# prep words for cmudict corpus'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '# 为 cmudict 语料库准备单词'
- en: words = words.replace('-', ' ')
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: words = words.replace('-', ' ')
- en: words = words.lower().split()
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: words = words.lower().split()
- en: ➌ num_sylls = 0
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ num_sylls = 0
- en: '➍ for word in words:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 对于 words 中的每个单词：
- en: word = word.strip(punctuation)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: word = word.strip(punctuation)
- en: 'if word.endswith("''s") or word.endswith("’s"):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单词以 "'s" 或 "’s" 结尾：
- en: word = word[:-2]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: word = word[:-2]
- en: '➎ if word in missing_words:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ 如果单词在 missing_words 中：
- en: num_sylls += missing_words[word]
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: num_sylls += missing_words[word]
- en: 'else:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 否则：
- en: '➏ for phonemes in cmudict[word][0]:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ 对于 cmudict[word][0] 中的音素：
- en: 'for phoneme in phonemes:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个音素：
- en: '➐ if phoneme[-1].isdigit():'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ 如果音素[-1]是数字：
- en: num_sylls += 1
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: num_sylls += 1
- en: ➑ return num_sylls
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ 返回 num_sylls
- en: '*Listing 8-5: Imports modules, loads dictionaries, and counts syllables*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-5：导入模块，加载字典，并计算音节数*'
- en: After some familiar imports, load the *missing_words.json* file that contains
    all the words and syllable counts missing from CMUdict. Using json.load() restores
    the dictionary that was stored as a string. Next, turn the CMUdict corpus into
    a dictionary using the dict() method ➊.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些常见的导入后，加载包含所有在 CMUdict 中缺失的单词和音节计数的 *missing_words.json* 文件。使用 json.load()
    恢复存储为字符串的字典。接着，使用 dict() 方法将 CMUdict 语料库转换为字典 ➊。
- en: Define a function called count_syllables() to count syllables. It should take
    both words *and* phrases, because you’ll ultimately want to pass it lines from
    a haiku. Prep the words as you did previously in the *missing_words_finder.py*
    program ➋.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为 count_syllables() 的函数来计算音节数。它应该同时接受单词 *和* 短语，因为最终你希望将其用于处理俳句中的行。像之前在
    *missing_words_finder.py* 程序中那样准备单词 ➋。
- en: 'Assign a num_sylls variable to hold the syllable count and set it to 0 ➌. Now
    start looping through the input words, stripping punctuation and *’s* from the
    ends. Note that you can get tripped up by the format of the apostrophe, so two
    versions are supplied: one with a straight apostrophe and one with a curly apostrophe
    ➍. Next, check whether the word is a member of the small dictionary of missing
    words. If the word is found, add the dictionary value for the word to num_sylls
    ➎. Otherwise, start looking through the phonemes, which represent a value in CMUdict;
    for each phoneme, look through the strings that make it up ➏. If you find a digit
    at the end of the string, then you know that phoneme is a vowel. To illustrate
    using the word *aged*, only the first string (highlighted in gray here) ends with
    a digit, so the word contains one vowel:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 num_sylls 变量来存储音节数，并将其初始化为 0 ➌。现在开始循环遍历输入的单词，去掉标点符号和 *’s*。请注意，可能会被撇号的格式所困扰，因此提供了两种格式：一种是直引号，另一种是弯引号
    ➍。接下来，检查单词是否属于缺失单词的小字典。如果找到该单词，则将该字典中的值加到 num_sylls ➎ 中。否则，开始检查音素，它们表示 CMUdict
    中的值；对于每个音素，查看组成它的字符串 ➏。 如果发现字符串末尾有数字，那么这个音素就是元音。以单词 *aged* 为例，只有第一个字符串（在此处以灰色突出显示）末尾带有数字，因此该单词包含一个元音：
- en: '[[''EY1'', ''JH'', ''D''], [''EY1'', ''JH'', ''IH0'', ''D'']]'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[[''EY1'', ''JH'', ''D''], [''EY1'', ''JH'', ''IH0'', ''D'']]'
- en: Note that you use the first value ([0]) in case there are multiple pronunciations;
    remember that CMUdict represents each pronunciation in a nested list. This may
    result in the occasional error, as the proper choice will depend on context.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你使用第一个值（[0]），以防有多个发音；记住，CMUdict 将每种发音表示为一个嵌套列表。这可能会导致偶尔的错误，因为正确的选择将取决于上下文。
- en: Check whether the end of the phoneme has a digit, and if it does, add 1 to num_sylls
    ➐. Finally, return the total syllable count for the word or phrase ➑.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 检查音素的结尾是否包含数字，如果包含，则将num_sylls ➐加1。最后，返回该单词或短语的总音节数 ➑。
- en: '***Defining the main() Function***'
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***定义 main() 函数***'
- en: Completing the program, [Listing 8-6](ch08.xhtml#ch08list6) defines and runs
    the main() function. The program will call this function when the program is run
    in stand-alone mode—for example, to spot-check a word or phrase—but it won’t be
    called if you import syllable_counter as a module.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 完成程序后，[列表 8-6](ch08.xhtml#ch08list6) 定义并运行了 main() 函数。当程序以独立模式运行时，它将调用此函数——例如，检查某个单词或短语——但如果你将
    syllable_counter 作为模块导入，它将不会被调用。
- en: '*count_syllables.py,* part 2'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*count_syllables.py,* 第 2 部分'
- en: 'def main():'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: '➊ while True:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ while True:'
- en: print("Syllable Counter")
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: print("音节计数器")
- en: '➋ word = input("Enter word or phrase; else press Enter to Exit: ")'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ word = input("请输入单词或短语；否则按 Enter 键退出：")
- en: '➌ if word == '''':'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 如果单词为空：
- en: sys.exit()
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: sys.exit()
- en: '➍ try:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ try:'
- en: num_syllables = count_syllables(word)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: num_syllables = count_syllables(word)
- en: 'print("number of syllables in {} is: {}"'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: print("单词 {} 的音节数为：{}"
- en: .format(word, num_syllables))
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: .format(word, num_syllables))
- en: print()
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'except KeyError:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 'except KeyError:'
- en: print("Word not found.  Try again.\n", file=sys.stderr)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: print("未找到单词。请再试一次。\n", file=sys.stderr)
- en: '➎ if __name__ == ''__main__'':'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '➎ if __name__ == ''__main__'':'
- en: main()
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: main()
- en: '*Listing 8-6: Defines and calls the* main() *function*'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '*列出 8-6：定义并调用* main() *函数*'
- en: Define the main() function and then start a while loop ➊. Ask the user to input
    a word or phrase ➋. If the user presses ENTER with no input, the program exits
    ➌. Otherwise, start a try-except block so the program won’t crash if a user enters
    a word not found in either dictionary ➍. An exception should be raised only in
    stand-alone mode, as you have already prepared the program to run on the haiku-training
    corpus with no exceptions. Within this block, the count_syllables() function is
    called and passed the input, and then the results are displayed in the interactive
    shell. End with the standard code that lets the program run stand-alone or as
    a module in another program ➎.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 main() 函数并启动一个 while 循环 ➊。要求用户输入一个单词或短语 ➋。如果用户按下 ENTER 键而没有输入内容，则程序退出 ➌。否则，启动一个
    try-except 块，这样如果用户输入一个字典中没有的单词，程序就不会崩溃 ➍。只有在独立模式下才应抛出异常，因为你已经为该程序准备好了在 haiku
    训练语料库中运行，无需任何异常。在这个块中，调用 count_syllables() 函数并传入输入，然后在交互式 shell 中显示结果。最后以标准代码结束，允许程序在独立模式或作为另一个程序的模块运行
    ➎。
- en: '**A Program to Check Your Program**'
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**检查程序的程序**'
- en: You have carefully tailored the syllable-counting program to ensure it will
    work with the training corpus. As you continue with the haiku program, you may
    want to add a poem or two to this corpus, but adding new haiku might introduce
    a new word that isn’t in either the CMUdict or your exceptions dictionary. Before
    you go back and rebuild the exceptions dictionary, check whether you really need
    to do so.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你已仔细调整音节计数程序，确保它能够与训练语料库配合使用。在继续进行 haiku 程序时，你可能想要向此语料库添加一两首诗，但添加新的 haiku 可能会引入一个在
    CMUdict 或你的例外字典中都没有的新单词。在你回去重新构建例外字典之前，请检查是否真的需要这么做。
- en: '[Listing 8-7](ch08.xhtml#ch08list7) will automatically count the syllables
    in each word in your training corpus and display any word(s) on which it failed.
    You can download this program from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    as *test_count_syllables_w_full_corpus.py*. Keep it in the same folder as *count_syllables.py*,
    *train.txt*, and *missing_words.json*.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[列出 8-7](ch08.xhtml#ch08list7) 将自动计算训练语料库中每个单词的音节数，并显示所有计算失败的单词。你可以从 *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    下载此程序，命名为 *test_count_syllables_w_full_corpus.py*。将其保存在与 *count_syllables.py*、*train.txt*
    和 *missing_words.json* 相同的文件夹中。'
- en: '*test_count_syllables_w_full_corpus.py*'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*test_count_syllables_w_full_corpus.py*'
- en: import sys
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: import count_syllables
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: import count_syllables
- en: 'with open(''train.txt.'') as in_file:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''train.txt.'') as in_file:'
- en: ➊ words = set(in_file.read().split())
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ words = set(in_file.read().split())
- en: ➋ missing = []
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ missing = []
- en: '➌ for word in words:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ for word in words:'
- en: 'try:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 'try:'
- en: num_syllables = count_syllables.count_syllables(word)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: num_syllables = count_syllables.count_syllables(word)
- en: '##print(word, num_syllables, end=''\n'') # uncomment to see word counts'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '##print(word, num_syllables, end=''\n'') # 取消注释以查看单词计数'
- en: '➍ except KeyError:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ except KeyError:'
- en: missing.append(word)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: missing.append(word)
- en: ➎ print("Missing words:", missing, file=sys.stderr)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ print("缺失的单词:", missing, file=sys.stderr)
- en: '*Listing 8-7: Attempts to count syllables in words in a training corpus and
    lists all failures*'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*列出 8-7：尝试计算训练语料库中单词的音节并列出所有失败项*'
- en: Open your updated *train.txt* training corpus and load it as a set to remove
    duplicates ➊. Start an empty list, called missing, to hold any new words for which
    syllables can’t be counted ➋. Words in missing won’t be in CMUdict or in your
    missing_words dictionary.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你的更新后的 *train.txt* 训练语料库，并将其加载为一个集合，以去除重复项 ➊。启动一个空的列表，命名为 missing，用来存储任何无法计算音节的单词
    ➋。missing 中的单词不会出现在 CMUdict 或你的 missing_words 字典中。
- en: Loop through the words in the new training corpus ➌ and use a try-except block
    to handle the KeyError that will be raised if *count_syllables.py* can’t find
    the word ➍. Append this word to the missing list and then display the list ➎.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 循环遍历新训练语料库中的单词 ➌，并使用 try-except 块来处理当 *count_syllables.py* 找不到单词时抛出的 KeyError
    ➍。将这个单词添加到 missing 列表中，然后显示该列表 ➎。
- en: If the program displays an empty list, then all the words in the new haiku are
    already present in either CMUdict or *missing_words.json*, so you don’t need to
    make any adjustments. Otherwise, you have the choice of manually adding the words
    to the *missing_words.json* file or rerunning *missing_words_finder.py* to rebuild
    *missing_words.json*.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果程序显示一个空列表，则表示新俳句中的所有单词已经出现在CMUdict或*missing_words.json*中，因此你不需要做任何调整。否则，你可以选择手动将单词添加到*missing_words.json*文件中，或者重新运行*missing_words_finder.py*来重建*missing_words.json*。
- en: '**Summary**'
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you’ve learned how to download NLTK and use one of its datasets,
    the Carnegie Mellon Pronouncing Dictionary (CMUdict). You checked the CMUdict
    dataset against a training corpus of haiku and built a supporting Python dictionary
    for any missing words. You saved this Python dictionary as persistent data using
    JavaScript Object Notation (JSON). Finally, you wrote a program that can count
    syllables. In [Chapter 9](ch09.xhtml#ch09), you’ll use your syllable-counting
    program to help you generate novel haiku.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经学会了如何下载NLTK并使用其数据集之一——卡内基梅隆发音词典（CMUdict）。你将CMUdict数据集与俳句训练语料库进行比对，并为任何缺失的单词构建了一个支持性的Python词典。你将该Python词典保存为持久数据，使用了JavaScript对象表示法（JSON）。最后，你编写了一个可以计算音节数的程序。在[第9章](ch09.xhtml#ch09)，你将使用你的音节计数程序来帮助你生成新的俳句。
- en: '**Further Reading**'
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: '*Virtual Muse: Experiments in Computer Poetry* (Wesleyan University Press,
    1996) by Charles O. Hartman is an engaging look at the early collaboration between
    humans and computers to write poetry.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 查尔斯·O·哈特曼（Charles O. Hartman）的《虚拟缪斯：计算机诗歌实验》（Wesleyan University Press，1996年）深入探讨了人类与计算机在创作诗歌过程中的早期合作。
- en: '*Natural Language Processing with Python: Analyzing Text with the Natural Language
    Toolkit* (O’Reilly, 2009) by Steven Bird, Ewan Klein, and Edward Loper is an accessible
    introduction to NLP using Python, with lots of exercises and useful integration
    with the NLTK website. A new version of the book, updated for Python 3 and NLTK
    3, is available online at *[http://www.nltk.org/book/](http://www.nltk.org/book/)*.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python自然语言处理：使用自然语言工具包分析文本*（O’Reilly，2009年）由Steven Bird、Ewan Klein和Edward
    Loper编写，是一本易于理解的关于使用Python进行NLP的入门书籍，包含大量练习并与NLTK网站有良好的集成。此书的新版本已更新为支持Python 3和NLTK
    3，可在线阅读，地址为* [http://www.nltk.org/book/](http://www.nltk.org/book/)*。'
- en: “The Growing Importance of Natural Language Processing” by Stephen F. DeAngelis
    is a *Wired* magazine article on the expanding role of NLP in big data. An online
    version is available at *[https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/](https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/)*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 斯蒂芬·F·迪安杰利斯（Stephen F. DeAngelis）的《自然语言处理日益重要性》是*Wired*杂志的一篇文章，讨论了NLP在大数据中的日益重要的角色。在线版可在*
    [https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/](https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/)*上查看。
- en: '**Practice Project: Syllable Counter vs. Dictionary File**'
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**实践项目：音节计数器与词典文件**'
- en: 'Write a Python program that lets you test *count_syllables.py* (or any other
    syllable-counting Python code) against a dictionary file. After allowing the user
    to specify how many words to check, choose the words at random and display a listing
    of each word and its syllable count on separate lines. The output should look
    similar to this printout:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个Python程序，允许你测试*count_syllables.py*（或任何其他音节计数Python代码）与词典文件的匹配。在允许用户指定要检查的单词数量后，随机选择单词并显示每个单词及其音节数的清单，每个单词和音节数应该显示在不同的行中。输出应类似于以下打印输出：
- en: ululation 4
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ululation 4
- en: intimated 4
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: intimated 4
- en: sand 1
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: sand 1
- en: worms 1
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: worms 1
- en: leatherneck 3
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: leatherneck 3
- en: contenting 3
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: contenting 3
- en: scandals 2
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: scandals 2
- en: livelihoods 3
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: livelihoods 3
- en: intertwining 4
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: intertwining 4
- en: beaming 2
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: beaming 2
- en: untruthful 3
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: untruthful 3
- en: advice 2
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: advice 2
- en: accompanying 5
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: accompanying 5
- en: deathly 2
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: deathly 2
- en: hallos 2
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: hallos 2
- en: Downloadable dictionary files are listed in [Table 2-1](ch02.xhtml#ch02tab1)
    on [page 20](ch02.xhtml#page_20). You can find a solution in the appendix that
    can be downloaded from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    as *test_count_syllables_w_dict.py*.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 可下载的词典文件在[表2-1](ch02.xhtml#ch02tab1)中列出，位于[第20页](ch02.xhtml#page_20)。你可以在附录中找到一个解决方案，附录文件可以从*
    [https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*下载，文件名为*test_count_syllables_w_dict.py*。
