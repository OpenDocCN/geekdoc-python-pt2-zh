<html><head></head><body><div id="sbo-rt-content"><h2 class="h2e" id="introduction"><span epub:type="pagebreak" id="page_xxiii"/>INTRODUCTION</h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="Image" width="189" height="189"/></div>&#13;
<p class="noindents">Math is essential to the modern world. Deep learning is also rapidly becoming essential. From the promise of self-driving cars to medical systems detecting fractures better than all but the very best physicians, to say nothing of increasingly capable, and possibly worrisome, voice-controlled assistants, deep learning is everywhere.</p>&#13;
<p class="indent">This book covers the essential math for making deep learning comprehensible. It’s true that you can learn the toolkits, set up the configuration files or Python code, format some data, and train a model, all without understanding <em>what</em> you’re doing, let alone the math behind it. And, because of the power of deep learning, you’ll often be successful. However, you won’t <em>understand</em>, and you shouldn’t be satisfied. To understand, you need some math. Not a lot of math, but some specific math. In particular, you’ll need working knowledge of topics in probability, statistics, linear algebra, and differential calculus. Fortunately, those are the very topics this book happens to address.</p>&#13;
<h3 class="h3" id="lev1_1"><span epub:type="pagebreak" id="page_xxiv"/>Who Is This Book For?</h3>&#13;
<p class="noindent">This is not an introductory deep learning book. It will not teach you the basics of deep learning. Instead, it’s meant as an adjunct to such a book. (See my book <em>Practical Deep Learning: A Python-Based Introduction</em> [No Starch Press, 2021].) I expect you to be familiar with deep learning, at least conceptually, though I’ll explain things along the way.</p>&#13;
<p class="indent">Additionally, I expect you to bring certain knowledge to the table. I expect you to know high school mathematics, in particular algebra. I also expect you to be familiar with programming using Python, R, or a similar language. We’ll be using Python 3.<em>x</em> and some of its popular toolkits, such as NumPy, SciPy, and scikit-learn.</p>&#13;
<p class="indent">I’ve attempted to keep other expectations to a minimum. After all, the point of the book is to give <em>you</em> what you need to be successful in deep learning.</p>&#13;
<h3 class="h3" id="lev1_2">About This Book</h3>&#13;
<p class="noindent">At its core, this is a math book. But instead of proofs and practice exercises, we’ll use code to illustrate the concepts. Deep learning is an applied discipline that you need to do to be able to understand. Therefore, we’ll use code to bridge the gap between pure mathematical knowledge and practice.</p>&#13;
<p class="indent">The chapters build one upon the other, with foundational chapters followed by more advanced math topics and, ultimately, deep learning algorithms that make use of everything covered in the earlier chapters. I recommend reading the book straight through and, if you wish, skipping topics you’re already familiar with as you encounter them.</p>&#13;
<div class="block">&#13;
<p class="noindent5"><strong><a href="ch01.xhtml#ch01">Chapter 1: Setting the Stage</a></strong> This chapter configures our working environment and the toolkits we’ll use, which are those used most often in deep learning.</p>&#13;
<p class="noindent5"><strong><a href="ch02.xhtml#ch02">Chapter 2: Probability</a></strong> Probability affects almost all aspects of deep learning and is essential to understanding how neural networks learn. This chapter, the first of two on this subject, introduces fundamental topics in probability.</p>&#13;
<p class="noindent5"><strong><a href="ch03.xhtml#ch03">Chapter 3: More Probability</a></strong> Probability is so important that one chapter isn’t enough. This chapter continues our exploration and includes key deep learning topics, like probability distributions and Bayes’ theorem.</p>&#13;
<p class="noindent5"><strong><a href="ch04.xhtml#ch04">Chapter 4: Statistics</a></strong> Statistics make sense of data and are crucial for evaluating models. Statistics go hand in hand with probability, so we need to understand statistics to understand deep learning.</p>&#13;
<p class="noindent5"><strong><a href="ch05.xhtml#ch05">Chapter 5: Linear Algebra</a></strong> Linear algebra is the world of vectors and matrices. Deep learning is, at its core, linear algebra–focused. Implementing neural networks is an exercise in vector and matrix mathematics, so it is essential to understand what these concepts represent and how to work with them.</p>&#13;
<p class="noindent5"><span epub:type="pagebreak" id="page_xxv"/><strong><a href="ch06.xhtml#ch06">Chapter 6: More Linear Algebra</a></strong> This chapter continues our exploration of linear algebra, focusing on important topics concerning matrices.</p>&#13;
<p class="noindent5"><strong><a href="ch07.xhtml#ch07">Chapter 7: Differential Calculus</a></strong> Perhaps the most fundamental concept behind the training of neural networks is the gradient. To understand the gradient, what it is and how to use it, we must know how to work with derivatives of functions. This chapter builds the foundation necessary to understand derivatives and gradients.</p>&#13;
<p class="noindent5"><strong><a href="ch08.xhtml#ch08">Chapter 8: Matrix Calculus</a></strong> Deep learning manipulates derivatives of vectors and matrices. Therefore, in this chapter we generalize the concept of a derivative to these objects.</p>&#13;
<p class="noindent5"><strong><a href="ch09.xhtml#ch09">Chapter 9: Data Flow in Neural Networks</a></strong> To understand how neural networks manipulate vectors and matrices, we need to understand how data flows through the network. That’s the subject of this chapter.</p>&#13;
<p class="noindent5"><strong><a href="ch10.xhtml#ch10">Chapter 10: Backpropagation</a></strong> Successful training of neural networks usually involves two algorithms that go hand in hand: backpropagation and gradient descent. In this chapter, we work through backpropagation in detail to see how the math we learned earlier in the book applies to the training of actual neural networks.</p>&#13;
<p class="noindent5"><strong><a href="ch11.xhtml#ch11">Chapter 11: Gradient Descent</a></strong> Gradient descent uses the gradients that the backpropagation algorithm provides to train a neural network. This chapter explores gradient descent, beginning with 1D examples and progressing through to fully connected neural networks. It also describes and compares common variants of gradient descent.</p>&#13;
<p class="noindent5"><strong><a href="app01.xhtml">Appendix: Going Further</a></strong> We must, of necessity, gloss over many topics in probability, statistics, linear algebra, and calculus. This appendix points you toward resources that will aid you in going further with the mathematics behind deep learning.</p>&#13;
</div>&#13;
<p class="indent">You can download all the code from the book here: <em><a href="https://github.com/rkneusel9/MathForDeepLearning/">https://github.com/rkneusel9/MathForDeepLearning/</a></em>. And please look at <em><a href="https://nostarch.com/math-deep-learning/">https://nostarch.com/math-deep-learning/</a></em> for future errata. Let’s get started.<span epub:type="pagebreak" id="page_xxvi"/></p>&#13;
</div></body></html>