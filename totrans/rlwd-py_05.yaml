- en: '5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FINDING PLUTO
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: According to Woody Allen, 80 percent of success is just showing up. This certainly
    describes the success of Clyde Tombaugh, an untrained Kansas farm boy growing
    up in the 1920s. With a passion for astronomy but no money for college, he took
    a stab in the dark and mailed his best astronomical sketches to Lowell Observatory.
    To his great surprise, he was hired as an assistant. A year later, he had discovered
    Pluto and gained eternal glory!
  prefs: []
  type: TYPE_NORMAL
- en: Percival Lowell, the famous astronomer and founder of Lowell Observatory, had
    postulated the presence of Pluto based on perturbations in the orbit of Neptune.
    His calculations were wrong, but by pure coincidence, he correctly predicted Pluto’s
    orbital path. Between 1906 and his death in 1916, he had photographed Pluto twice.
    Both times, his team failed to notice it. Tombaugh, on the other hand, photographed
    *and* recognized Pluto in January 1930, after only a year of searching ([Figure
    5-1](ch05.xhtml#ch05fig1)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1: Discovery plates for Pluto, indicated by the arrow'
  prefs: []
  type: TYPE_NORMAL
- en: What Tombaugh accomplished was extraordinary. Without computers, the methodology
    he followed was impractical, tedious, and demanding. He had to photograph and
    re-photograph small parts of the sky night after night, usually in a freezing
    cold dome shaken by icy winds. He then developed and sifted through all the negatives,
    searching for the faintest signs of movement within crowded star fields.
  prefs: []
  type: TYPE_NORMAL
- en: Although he lacked a computer, he did have a state-of-the-art device, known
    as a *blink comparator*, that let him rapidly switch between negatives from successive
    nights. As viewed through the blink comparator, the stars remained stationary,
    but Pluto, a moving object, flashed on and off like a beacon.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll first write a Python program that replicates an early
    20th-century blink comparator. Then you’ll move into the 21st century and write
    a program that automates the detection of moving objects using modern computer
    vision techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In 2006, the International Astronomical Union reclassified Pluto as a dwarf
    planet. This was based on the discovery of other Pluto-sized bodies in the Kuiper
    Belt, including one—Eris—that is volumetrically smaller but 27 percent more massive
    than Pluto.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Project #7: Replicating a Blink Comparator**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pluto may have been photographed with a telescope, but it was found with a microscope.
    The blink comparator ([Figure 5-2](ch05.xhtml#ch05fig2)), also called the *blink
    microscope*, lets the user mount two photographic plates and rapidly switch from
    looking at one to the other. During this “blinking,” any object that changes position
    between photographs will appear to jump back and forth.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-2: A blink comparator'
  prefs: []
  type: TYPE_NORMAL
- en: For this technique to work, the photos need to be taken with the same exposure
    and under similar viewing conditions. Most importantly, the stars in the two images
    must line up perfectly. In Tombaugh’s day, technicians achieved this through painstaking
    manual labor; they carefully guided the telescope during the hour-long exposures,
    developed the photographic plates, and then shifted them in the blink comparator
    to fine-tune the alignment. Because of this exacting work, it would sometimes
    take Tombaugh a week to examine a single pair of plates.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, you’ll digitally duplicate the process of aligning the plates
    and blinking them on and off. You’ll work with bright and dim objects, see the
    impact of different exposures between photos, and compare the use of positive
    images to the negative ones that Tombaugh used.
  prefs: []
  type: TYPE_NORMAL
- en: THE OBJECTIVE
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that aligns two nearly identical images and displays
    each one in rapid succession in the same window.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Strategy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The photos for this project are already taken, so all you need to do is align
    them and flash them on and off. Aligning images is often referred to as image
    *registration*. This involves making a combination of vertical, horizontal, or
    rotational transformations to one of the images. If you’ve ever taken a panorama
    with a digital camera, you’ve seen registration at work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image registration follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate distinctive features in each image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Numerically describe each feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the numerical descriptors to match identical features in each image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Warp one image so that matched features share the same pixel locations in both
    images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this to work well, the images should be the same size and cover close to
    the same area.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the OpenCV Python package ships with algorithms that perform these
    steps. If you skipped [Chapter 1](ch01.xhtml), you can read about OpenCV on [page
    6](ch01.xhtml#page_6).
  prefs: []
  type: TYPE_NORMAL
- en: Once the images are registered, you’ll need to display them in the same window
    so that they overlay exactly and then loop through the display a set number of
    times. Again, you can easily accomplish this with the help of OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Data***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The images you’ll need are in the *Chapter_5* folder in the book’s supporting
    files, downloadable from *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*.
    The folder structure should look like [Figure 5-3](ch05.xhtml#ch05fig3). After
    downloading the folders, don’t change this organizational structure or the folder
    contents and names.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3: The folder structure for Project 7'
  prefs: []
  type: TYPE_NORMAL
- en: The *night_1* and *night_2* folders contain the input images you’ll use to get
    started. In theory, these would be images of the same region of space taken on
    different nights. The ones used here are the same star field image to which I’ve
    added an artificial *transient*. A transient, short for *transient astronomical
    event*, is a celestial object whose motion is detectable over relatively short
    time frames. Comets, asteroids, and planets can all be considered transients,
    as their movement is easily detected against the more static background of the
    galaxy.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-1](ch05.xhtml#ch05table1) briefly describes the contents of the *night_1*
    folder. This folder contains files with *left* in their filenames, which means
    they should go on the left side of a blink comparator. The images in the *night_2*
    folder contain *right* in the filenames and should go on the other side.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Files in the *night_1* folder'
  prefs: []
  type: TYPE_NORMAL
- en: '| Filename | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *1_bright_transient_left.png* | Contains a large, bright transient |'
  prefs: []
  type: TYPE_TB
- en: '| *2_dim_transient_left.png* | Contains a dim transient a single pixel in diameter
    |'
  prefs: []
  type: TYPE_TB
- en: '| *3_diff_exposures_left.png* | Contains a dim transient with an overexposed
    background |'
  prefs: []
  type: TYPE_TB
- en: '| *4_single_transient_left.png* | Contains a bright transient in left image
    only |'
  prefs: []
  type: TYPE_TB
- en: '| *5_no_transient_left.png* | Star field with no transient |'
  prefs: []
  type: TYPE_TB
- en: '| *6_bright_transient_neg_left.png* | A negative of the first file to show
    the type of image Tombaugh used |'
  prefs: []
  type: TYPE_TB
- en: '[Figure 5-4](ch05.xhtml#ch05fig4) is an example of one of the images. The arrow
    points to the transient (but isn’t part of the image file).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-4: 1_bright_transient_left.png with an arrow indicating the transient'
  prefs: []
  type: TYPE_NORMAL
- en: To duplicate the difficulty in perfectly aligning a telescope from night to
    night, I’ve slightly shifted the images in the *night_2* folder with respect to
    those in *night_1*. You’ll need to loop through the contents of the two folders,
    registering and comparing each pair of photos. For this reason, the number of
    files in each folder should be the same, and the naming convention should ensure
    that the photos are properly paired.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Blink Comparator Code***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following *blink_comparator.py* code will digitally duplicate a blink comparator.
    Find this program in the *Chapter_5* folder from the website. You’ll also need
    the folders described in the previous section. Keep the code in the folder above
    the *night_1* and *night_2* folders.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing Modules and Assigning a Constant**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-1](ch05.xhtml#ch05list1) imports the modules you’ll need to run
    the program and assigns a constant for the minimum number of keypoint matches
    to accept. Also called interest points, *keypoints* are interesting features in
    an image that you can use to characterize the image. They’re usually associated
    with sharp changes in intensity, such as corners or, in this case, stars.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-1: Importing modules and assigning a constant for keypoint matches'
  prefs: []
  type: TYPE_NORMAL
- en: Start by importing the operating system module, which you’ll use to list the
    contents of folders. Then import pathlib, a handy module that simplifies working
    with files and folders. Finish by importing NumPy and cv (OpenCV) for working
    with images. If you skipped [Chapter 1](ch01.xhtml), you can find installation
    instructions for NumPy on [page 8](ch01.xhtml#page_8).
  prefs: []
  type: TYPE_NORMAL
- en: Assign a constant variable for the minimum number of keypoint matches to accept.
    For efficiency, you ideally want the smallest value that will yield an acceptable
    registration result. In this project, the algorithm runs so quickly that you can
    increase this value without a significant cost.
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining the main() Function**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-2](ch05.xhtml#ch05list2) defines the first part of the main() function,
    used to run the program. These initial steps create lists and directory paths
    used to access the various image files.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-2: Defining the first part of main(), used to manipulate files and
    folders'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by defining main() and then use the os module’s listdir() method to create
    a list of the filenames in the *night_1* and *night_2* folders. For the *night_1*
    folder, listdir() returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that os.listdir() does not impose an order on the files when they’re returned.
    The underlying operating system determines the order, meaning macOS will return
    a different list than Windows! To ensure that the lists are consistent and the
    files are paired correctly, wrap os.listdir() with the built-in sorted() function.
    This function will return the files in numerical order, based on the first character
    in the filename.
  prefs: []
  type: TYPE_NORMAL
- en: Next, assign path names to variables using the pathlib Path class. The first
    two variables will point to the two input folders, and the third will point to
    an output folder to hold the registered images.
  prefs: []
  type: TYPE_NORMAL
- en: The pathlib module, introduced in Python 3.4, is an alternative to os.path for
    handling file paths. The os module treats paths as strings, which can be cumbersome
    and requires you to use functionality from across the Standard Library. Instead,
    the pathlib module treats paths as objects and gathers the necessary functionality
    in one place. The official documentation for pathlib is at *[https://docs.python.org/3/library/pathlib.html](https://docs.python.org/3/library/pathlib.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: For the first part of the directory path, use the cwd() class method to get
    the current working directory. If you have at least one Path object, you can use
    a mix of objects and strings in the path designation. You can join the string,
    representing the folder name, with the / symbol. This is similar to using os.path.join(),
    if you’re familiar with the os module.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you will need to execute the program from within the project directory.
    If you call it from elsewhere in the filesystem, it will fail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Looping in main()**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-3](ch05.xhtml#ch05list3), still in the main() function, runs the
    program with a big for loop. This loop will take a file from each of the two “night”
    folders, load them as grayscale images, find matching keypoints in each image,
    use the keypoints to warp (or *register*) the first image to match the second,
    save the registered image, and then compare (or *blink*) the registered first
    image with the original second image. I’ve also included a few optional quality
    control steps that you can comment out once you’re satisfied with the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-3: Running the program loop in main()'
  prefs: []
  type: TYPE_NORMAL
- en: Begin the loop by enumerating the night1_files list. The enumerate() built-in
    function adds a counter to each item in the list and returns this counter along
    with the item. Since you only need the counter, use a single underscore (_) for
    the list item. By convention, the single underscore indicates a temporary or insignificant
    variable. It also keeps code-checking programs, such as Pylint, happy. Were you
    to use a variable name here, such as infile, Pylint would complain about an *unused
    variable*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, load the image, along with its pair from the night2_files list, using
    OpenCV. Note that you have to convert the path to a string for the imread() method.
    You’ll also want to convert the image to grayscale. This way, you’ll need to work
    with only a single channel, which represents intensity. To keep track of what’s
    going on during the loop, print a message to the shell indicating which files
    are being compared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, find the keypoints and their best matches ➊. The find_best_matches() function,
    which you’ll define later, will return these values as three variables: kp1, which
    represents the keypoints for the first loaded image; kp2, which represents the
    keypoints for the second; and best_matches, which represents a list of the matching
    keypoints.'
  prefs: []
  type: TYPE_NORMAL
- en: So you can visually check the matches, draw them on img1 and img2 using OpenCV’s
    drawMatches() method. As arguments, this method takes each image with its keypoints,
    the list of best matching keypoints, and an output image. In this case, the output
    image argument is set to None, as you’re just going to look at the output, not
    save it to a file.
  prefs: []
  type: TYPE_NORMAL
- en: To distinguish between the two images, draw a vertical white line down the right
    side of img1. First get the height and width of the image using shape. Next, call
    OpenCV’s line() method and pass it the image on which you want to draw, the start
    and end coordinates, the line color, and the thickness. Note that this is a color
    image, so to represent white, you need the full BGR tuple (255, 255, 255) rather
    than the single intensity value (255) used in grayscale images.
  prefs: []
  type: TYPE_NORMAL
- en: Now, call the quality control function—which you’ll define later—to display
    the matches ➋. [Figure 5-5](ch05.xhtml#ch05fig5) shows an example output. You
    may want to comment out this line after you confirm the program is behaving correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-5: Example output of the QC_best_matches() function'
  prefs: []
  type: TYPE_NORMAL
- en: With the best keypoint matches found and checked, it’s time to register the
    first image to the second. Do this with a function you’ll write later. Pass the
    function the two images, the keypoints, and the list of best matches.
  prefs: []
  type: TYPE_NORMAL
- en: The blink comparator, named blink(), is another function that you’ll write later.
    Call it here to see the effect of the registration process on the first image.
    Pass it the original and registered images, a name for the display window, and
    the number of blinks you want to perform ➌. The function will flash between the
    two images. The amount of “wiggle” you see will depend on the amount of warping
    needed to match img2. This is another line you may want to comment out after you’ve
    confirmed that the program runs as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Next, save the registered image into a folder named *night_1_registered*, which
    the path3 variable points to. Start by assigning a filename variable that references
    the original filename, with *_registered.png* appended to the end. So you don’t
    repeat the file extension in the name, use index slicing ([:-4]) to remove it
    before adding the new ending. Finish by using imwrite() to save the file. Note
    that this will overwrite existing files with the same name without warning.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll want an uncluttered view when you start looking for transients, so call
    the method to destroy all the current OpenCV windows. Then call the blink() function
    again, passing it the registered image, the second image, a window name, and the
    number of times to loop through the images. The first images are shown side by
    side in [Figure 5-6](ch05.xhtml#ch05fig6). Can you find the transient?
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-6: Blink Comparator windows for first image in night_1_registered
    and night_2 folders'
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the Best Keypoint Matches**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now it’s time to define the functions used in main(). [Listing 5-4](ch05.xhtml#ch05list4)
    defines the function that finds the best keypoint matches between each pair of
    images extracted from the *night_1* and *night_2* folders. It should locate, describe,
    and match keypoints, generate a list of the matches, and then truncate that list
    by the constant for the minimum number of acceptable keypoints. The function returns
    the list of keypoints for each image and the list of best matches.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-4: Defining the function to find the best keypoint matches'
  prefs: []
  type: TYPE_NORMAL
- en: Start by defining the function, which takes two images as arguments. The main()
    function will pick these images from the input folders with each run of the for
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create an orb object using OpenCV’s ORB_create() method. ORB is an acronym
    of nested acronyms: *O*riented FAST and *R*otated *B*RIEF.'
  prefs: []
  type: TYPE_NORMAL
- en: FAST, short for *F*eatures from *A*ccelerated *S*egment *T*est, is a fast, efficient,
    and free algorithm for *detecting* keypoints. To *describe* the keypoints so that
    you can compare them across different images, you need BRIEF. Short for *B*inary
    *R*obust *I*ndependent *E*lementary *F*eatures, BRIEF is also fast, compact, and
    open source.
  prefs: []
  type: TYPE_NORMAL
- en: ORB combines FAST and BRIEF into a matching algorithm that works by first detecting
    distinctive regions in an image, where pixel values change sharply, and then recording
    the position of these distinctive regions as *keypoints*. Next, ORB describes
    the feature found at the keypoint using numerical arrays, or *descriptors*, by
    defining a small area, called a *patch*, around a keypoint. Within the image patch,
    the algorithm uses a pattern template to take regular samples of intensity. It
    then compares preselected pairs of samples and converts them into binary strings
    called *feature vectors* ([Figure 5-7](ch05.xhtml#ch05fig7)).
  prefs: []
  type: TYPE_NORMAL
- en: A *vector* is a series of numbers. A *matrix* is a rectangular array of numbers
    in rows and columns that’s treated as a single entity and manipulated according
    to rules. A *feature vector* is a matrix with one row and multiple columns. To
    build one, the algorithm converts the sample pairs into a binary series by concatenating
    a 1 to the end of the vector if the first sample has the largest intensity and
    a 0 if the reverse is true.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-7: Cartoon example of how ORB generates keypoint descriptors'
  prefs: []
  type: TYPE_NORMAL
- en: Some example feature vectors are shown next. I’ve shortened the list of vectors,
    because ORB usually compares and records 512 pairs of samples!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These descriptors act as digital fingerprints for features. OpenCV uses additional
    code to compensate for rotation and scale changes. This allows it to match similar
    features even if the feature sizes and orientations are different (see [Figure
    5-8](ch05.xhtml#ch05fig8)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-8: OpenCV can match keypoints despite differences in scale and orientation.'
  prefs: []
  type: TYPE_NORMAL
- en: When you create the ORB object, you can specify the number of keypoints to examine.
    The method defaults to 500, but 100 will be more than enough for the image registration
    needed in this project.
  prefs: []
  type: TYPE_NORMAL
- en: Next, using the orb.detectAndCompute() method ➊, find the keypoints and their
    descriptors. Pass it img1 and then repeat the code for img2.
  prefs: []
  type: TYPE_NORMAL
- en: With the keypoints located and described, the next step is to find the keypoints
    common to both images. Start this process by creating a BFMatcher object that
    includes a distance measurement. The brute-force matcher takes the descriptor
    of one feature in the first image and compares it to all the features in the second
    image using the Hamming distance. It returns the closest feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'For two strings of equal length, the *Hamming distance* is the number of positions,
    or indexes, at which the corresponding values are different. For the following
    feature vectors, the positions that don’t match are shown in bold, and the Hamming
    distance is 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The bf variable will be a BFMatcher object. Call the match() method and pass
    it the descriptors for the two images ➋. Assign the returned list of DMatch objects
    to a variable named matches.
  prefs: []
  type: TYPE_NORMAL
- en: The best matches will have the lowest Hamming distance, so sort the objects
    in ascending order to move these to the start of the list. Note that you use a
    lambda function along with the object’s distance attribute. A *lambda function*
    is a small, one-off, unnamed function defined on the fly. Words and characters
    that directly follow lambda are parameters. Expressions come after the colon,
    and returns are automatic.
  prefs: []
  type: TYPE_NORMAL
- en: Since you only need the minimum number of keypoint matches defined at the start
    of the program, create a new list by slicing the matches list. The best matches
    are at the start, so slice from the start of matches up to the value specified
    in MIN_NUM_KEYPOINT_MATCHES.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you’re still dealing with arcane objects, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, OpenCV knows how to handle these. Complete the function by returning
    the two sets of keypoints and the list of best matching objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Checking the Best Matches**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-5](ch05.xhtml#ch05list5) defines a short function to let you visually
    check the keypoint matches. You saw the results of this function in [Figure 5-5](ch05.xhtml#ch05fig5).
    By encapsulating these tasks in a function, you can reduce the clutter in main()
    and allow the user to turn off the functionality by commenting out a single line.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-5: Defining a function to check the best keypoint matches'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the function with one parameter: the matched image. This image was generated
    by the main() function in [Listing 5-3](ch05.xhtml#ch05list3). It consists of
    the left and right images with the keypoints drawn as colored circles and with
    colored lines connecting corresponding keypoints.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, call OpenCV’s imshow() method to display the window. You can use the format()
    method when naming the window. Pass it the constant for the number of minimum
    keypoint matches.
  prefs: []
  type: TYPE_NORMAL
- en: Complete the function by giving the user 2.5 seconds to view the window. Note
    that the waitKey() method doesn’t destroy the window; it just suspends the program
    for the allocated amount of time. After the wait period, new windows will appear
    as the program resumes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Registering Images**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-6](ch05.xhtml#ch05list6) defines the function to register the first
    image to the second image.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-6: Defining a function to register one image to another'
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that takes the two input images, their keypoint lists, and
    the list of DMatch objects returned from the find_best_matches() function as arguments.
    Next, load the location of the best matches into NumPy arrays. Start with a conditional
    to check that the list of best matches equals or exceeds the MIN_NUM_KEYPOINT_MATCHES
    constant. If it does, then initialize two NumPy arrays with as many rows as there
    are best matches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The np.zeros() NumPy method returns a new array of a given shape and data type,
    filled with zeros. For example, the following snippet produces a zero-filled array
    three rows tall and two columns wide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the actual code, the arrays will be at least 50×2, since you stipulated a
    minimum of 50 matches.
  prefs: []
  type: TYPE_NORMAL
- en: Now, enumerate the matches list and start populating the arrays with actual
    data ➊. For the source points, use the queryIdx.pt attribute to get the index
    of the descriptor in the list of descriptors for kp1. Repeat this for the next
    set of points, but use the trainIdx.pt attribute. The query/train terminology
    is a bit confusing but basically refers to the first and second images, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to apply *homography*. Homography is a transformation, using
    a 3×3 matrix, that maps points in one image to corresponding points in another
    image. Two images can be related by a homography if both are viewing the same
    plane from a different angle or if both images are taken from the same camera
    rotated around its optical axis with no shift. To run correctly, homography needs
    at least four corresponding points in two images.
  prefs: []
  type: TYPE_NORMAL
- en: Homography assumes that the matching points really are corresponding points.
    But if you look carefully at [Figures 5-5](ch05.xhtml#ch05fig5) and [5-8](ch05.xhtml#ch05fig8),
    you’ll see that the feature matching isn’t perfect. In [Figure 5-8](ch05.xhtml#ch05fig8),
    around 30 percent of the matches are incorrect!
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, OpenCV includes a findHomography() method with an outlier detector
    called *random sample consensus* (RANSAC). RANSAC takes random samples of the
    matching points, finds a mathematical model that explains their distribution,
    and favors the model that predicts the most points. It then discards outliers.
    For example, consider the points in the “Raw data” box in [Figure 5-9](ch05.xhtml#ch05fig9).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-9: Example line fit using RANSAC to ignore outliers'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you want to fit a line through the true data points (called
    the *inliers*) and ignore the smaller number of spurious points (the *outliers*).
    Using RANSAC, you randomly sample a subset of the raw data points, fit a line
    to these, and then repeat this process a set number of times. Each line-fit equation
    would then be applied to all the points. The line that passes through the most
    points is used for the final line fit. In [Figure 5-9](ch05.xhtml#ch05fig9), this
    would be the line in the rightmost box.
  prefs: []
  type: TYPE_NORMAL
- en: To run findHomography(), pass it the source and destination points and call
    the RANSAC method. This returns a NumPy array and a mask. The mask specifies the
    inlier and outlier points or the good matches and bad matches, respectively. You
    can use it to do tasks like draw only the good matches.
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to warp the first image so that it perfectly aligns with the
    second. You’ll need the dimensions of the second image, so use shape() to get
    the height and width of img2 ➋. Pass this information, along with img1 and the
    homography h_array, to the warpPerspective() method. Return the registered image,
    which will be a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: If the number of keypoint matches is less than the minimum number you stipulated
    at the start of the program, the image *may not* be properly aligned. So, print
    a warning and return the original, nonregistered image. This will allow the main()
    function to continue looping through the folder images uninterrupted. If the registration
    is poor, the user will be aware something is wrong as the problem pair of images
    won’t be properly aligned in the blink comparator window. An error message will
    also appear in the shell.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Building the Blink Comparator**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-7](ch05.xhtml#ch05list7) defines a function to run the blink comparator
    and then calls main() if the program is run in stand-alone mode. The blink() function
    loops through a specified range, showing first the registered image and then the
    second image, both in the same window. It shows each image for only one-third
    of a second, Clyde Tombaugh’s preferred frequency when using a blink comparator.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-7: Defining a function to blink images on and off'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the blink() function with four parameters: two image files, a window
    name, and the number of blinks to perform. Start a for loop with a range set to
    the number of blinks. Since you don’t need access to the running index, use a
    single underscore (_) to indicate the use of an insignificant variable. As mentioned
    previously in this chapter, this will prevent code-checking programs from raising
    an “unused variable” warning.'
  prefs: []
  type: TYPE_NORMAL
- en: Now call OpenCV’s imshow() method and pass it the window name and the first
    image. This will be the *registered* first image. Then pause the program for 330
    milliseconds, the amount of time recommended by Clyde Tombaugh himself.
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the previous two lines of code for the second image. Because the two
    images are aligned, the only thing that will change in the window are transients.
    If only one image contains a transient, it will appear to blink on and off. If
    both images capture the transient, it will appear to dance back and forth.
  prefs: []
  type: TYPE_NORMAL
- en: End the program with the standard code that lets it run in stand-alone mode
    or be imported as a module.
  prefs: []
  type: TYPE_NORMAL
- en: '***Using the Blink Comparator***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before you run *blink_comparator.py*, dim your room lights to simulate looking
    through the device’s eyepieces. Then launch the program. You should first see
    two obvious bright dots flashing near the center of the image. In the next pair
    of images, the same dots will become very small—only a pixel across—but you should
    still be able to detect them.
  prefs: []
  type: TYPE_NORMAL
- en: The third loop will show the same small transient, only this time the second
    image will be brighter overall than the first. You should still be able to find
    the transient, but it will be much more difficult. This is why Tombaugh had to
    carefully take and develop the images to a consistent exposure.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth loop contains a single transient, shown in the left image. It should
    blink on and off rather than dance back and forth as in the previous images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fifth image pair represents control images with no transients. This is
    what the astronomer would see almost all the time: disappointing static star fields.'
  prefs: []
  type: TYPE_NORMAL
- en: The final loop uses negative versions of the first image pair. The bright transient
    appears as flashing black dots. This is the type of image Clyde Tombaugh used,
    as it saved time. Since a black dot is as easy to spot as a white one, he felt
    no need to print positive images for each negative.
  prefs: []
  type: TYPE_NORMAL
- en: If you look along the left side of the registered negative image, you’ll see
    a black stripe that represents the amount of translation needed to align the images
    ([Figure 5-10](ch05.xhtml#ch05fig10)). You won’t notice this on the positive images
    because it blends in with the black background.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-10: The negative image, 6_bright_transient_neg_left_registered.png'
  prefs: []
  type: TYPE_NORMAL
- en: 'In all the loops, you may notice a dim star blinking in the upper-left corner
    of each image pair. This is not a transient but a false positive caused by an
    *edge artifact*. An edge artifact is a change to an image caused by image misalignment.
    An experienced astronomer would ignore this dim star because: it occurs very close
    to the edge of the image, and the possible transient doesn’t move between images
    but just dims.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the cause of this false positive in [Figure 5-11](ch05.xhtml#ch05fig11).
    Because only part of a star is captured in the first frame, its brightness is
    reduced relative to the same star in the second image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-11: Registering a truncated star in Image 1 results in a noticeably
    dimmer star than in Image 2'
  prefs: []
  type: TYPE_NORMAL
- en: Humans can handle edge effects intuitively, but computers require explicit rules.
    In the next project, you’ll address this issue by excluding the edges of images
    when searching for transients.
  prefs: []
  type: TYPE_NORMAL
- en: '**Project #8: Detecting Astronomical Transients with Image Differencing**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Blink comparators, once considered as important as telescopes, now sit idly
    gathering dust in museums. Astronomers no longer need them, as modern image-differencing
    techniques are much better at detecting moving objects than human eyes. Today,
    every part of Clyde Tombaugh’s work would be done by computers.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, let’s pretend you’re a summer intern at an observatory. Your
    job is to produce a digital workflow for an ancient astronomer still clinging
    to his rusty blink comparator.
  prefs: []
  type: TYPE_NORMAL
- en: THE OBJECTIVE
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that takes two registered images and highlights any differences
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Strategy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of an algorithm that blinks the images, you now want one that automatically
    finds the transients. This process will still require registered images, but for
    convenience, just use the ones already produced in Project 7.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting differences between images is a common enough practice that OpenCV
    ships with an absolute difference method, absdiff(), dedicated to this purpose.
    It takes the per-element difference between two arrays. But just detecting the
    differences isn’t enough. Your program will need to recognize that a difference
    exists and show the user only the images containing transients. After all, astronomers
    have more important things to do, like demoting planets!
  prefs: []
  type: TYPE_NORMAL
- en: Because the objects you’re looking for rest on a black background and matching
    bright objects are removed, any bright object remaining after differencing is
    worth noting. And since the odds of having more than one transient in a star field
    are astronomically low, flagging one or two differences should be enough to get
    an astronomer’s attention.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Transient Detector Code***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following *transient_detector.py* code will automate the process of detecting
    transients in astronomical images. Find it in the *Chapter_5* folder from the
    website. To avoid duplicating code, the program uses the images already registered
    by *blink_comparator.py*, so you’ll need the *night_1_registered_transients* and
    *night_2* folders in the directory for this project (see [Figure 5-3](ch05.xhtml#ch05fig3)).
    As in the previous project, keep the Python code in the folder *above* these two
    folders.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing Modules and Assigning a Constant**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-8](ch05.xhtml#ch05list8) imports the modules needed to run the program
    and assigns a pad constant to manage edge artifacts (see [Figure 5-11](ch05.xhtml#ch05fig11)).
    The pad represents a small distance, measured perpendicular to the image’s edges,
    that you want to exclude from the analysis. Any objects detected between the edge
    of the image and the pad will be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-8: Importing modules and assigning a constant to manage edge effects'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need all the modules used in the previous project except for NumPy, so
    import them here. Set the pad distance to 5 pixels. This value may change slightly
    with different datasets. Later, you’ll draw a rectangle around the edge space
    within the image so you can see how much area this parameter is excluding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Detecting and Circling Transients**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-9](ch05.xhtml#ch05list9) defines a function you’ll use to find and
    circle up to two transients in each image pair. It will ignore transients in the
    padded area.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-9: Defining a function to detect and circle transients'
  prefs: []
  type: TYPE_NORMAL
- en: 'The find_transient() function has three parameters: the input image, an image
    representing the difference between the first and second input images (representing
    the *difference map*), and the PAD constant. The function will find the location
    of the brightest pixel in the difference map, draw a circle around it, and return
    the location along with a Boolean indicating that an object was found.'
  prefs: []
  type: TYPE_NORMAL
- en: Begin the function by setting a variable, named transient, to False. You’ll
    use this variable to indicate whether a transient has been discovered. As transients
    are rare in real life, its base state should be False.
  prefs: []
  type: TYPE_NORMAL
- en: To apply the PAD constant and exclude the area near the edge of the image, you’ll
    need the limits of the image. Get these with the shape attribute, which returns
    a tuple of the image’s height and width.
  prefs: []
  type: TYPE_NORMAL
- en: Use the height and width variables and the PAD constant to draw a white rectangle
    on the image variable using OpenCV’s rectangle() method. Later, this will show
    the user which parts of the image were ignored.
  prefs: []
  type: TYPE_NORMAL
- en: The diff_image variable is a NumPy array representing pixels. The background
    is black, and any “stars” that changed position (or appeared out of nowhere) between
    the two input images will be gray or white (see [Figure 5-12](ch05.xhtml#ch05fig12)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-12: Difference image derived from the “bright transient” input images'
  prefs: []
  type: TYPE_NORMAL
- en: To locate the brightest transient present, use OpenCV’s minMaxLoc() method,
    which returns the minimum and maximum pixel values in the image, along with their
    location tuple. Note that I’m naming the variables to be consistent with OpenCV’s
    mixed-case naming scheme (evident in names such as maxLoc). If you want to use
    something more acceptable to Python’s PEP8 style guide (*[https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/)*),
    feel free to use names like max_loc in place of maxLoc.
  prefs: []
  type: TYPE_NORMAL
- en: You may have found a maximum value near the edge of the image, so run a conditional
    to exclude this case by ignoring values found in the area delimited by the PAD
    constant ➊. If the location passes, circle it on the image variable. Use a white
    circle with a radius of 10 pixels and a line width of 0.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve drawn a circle, then you’ve found a transient, so set the transient
    variable to True. This will trigger additional activity later in the program.
  prefs: []
  type: TYPE_NORMAL
- en: End the function by returning the transient and maxLoc variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The minMaxLoc() method is susceptible to noise, such as false positives, as
    it works on individual pixels. Normally, you would first run a preprocessing step,
    like blurring, to remove spurious pixels. This can cause you to miss dim astronomical
    objects, however, which can be indistinguishable from noise in a single image.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing Files and Folders**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-10](ch05.xhtml#ch05list10) defines the main() function, creates
    lists of the filenames in the input folders, and assigns the folder paths to variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-10: Defining main(), listing the folder contents, and assigning path
    variables'
  prefs: []
  type: TYPE_NORMAL
- en: Define the main() function. Then, just as you did in [Listing 5-2](ch05.xhtml#ch05list2)
    on [page 100](ch05.xhtml#page_100), list the contents of the folders containing
    the input images and assign their paths to variables. You’ll use an existing folder
    to hold images containing identified transients.
  prefs: []
  type: TYPE_NORMAL
- en: '**Looping Through Images and Calculating Absolute Difference**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-11](ch05.xhtml#ch05list11) starts the for loop through the image
    pairs. The function reads corresponding image pairs as grayscale arrays, calculates
    the difference between the images, and shows the result in a window. It then calls
    the find_transient() function on the difference image.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-11: Looping through the images and finding the transients'
  prefs: []
  type: TYPE_NORMAL
- en: Start a for loop that iterates through the images in the *night1_files* list.
    The program is designed to work on *positive* images, so use image slicing ([:-1])
    to exclude the negative image. Use enumerate() to get a counter; name it i, rather
    than _, since you’ll use it as an index later.
  prefs: []
  type: TYPE_NORMAL
- en: To find the differences between images, just call the cv.absdiff() method and
    pass it the variables for the two images. Show the results for two seconds before
    continuing the program.
  prefs: []
  type: TYPE_NORMAL
- en: Since you’re going to blank out the brightest transient, first make a copy of
    diff_imgs1_2. Name this copy temp, for temporary. Now, call the find_transient()
    function you wrote earlier. Pass it the first input image, the difference image,
    and the PAD constant. Use the results to update the transient variable and to
    create a new variable, transient_loc1, that records the location of the brightest
    pixel in the difference image.
  prefs: []
  type: TYPE_NORMAL
- en: The transient may or may not have been captured in both images taken on successive
    nights. To see if it was, obliterate the bright spot you just found by covering
    it with a black circle. Do this on the temp image by using black as the color
    and a line width of –1, which tells OpenCV to fill the circle. Continue to use
    a radius of 10, though you can reduce this if you’re concerned the two transients
    will be very close together.
  prefs: []
  type: TYPE_NORMAL
- en: Call the find_transient() function again but use a single underscore for the
    location variable, as you won’t be using it again. It’s unlikely there’ll be more
    than two transients present, and finding even one will be enough to open the images
    up to further scrutiny, so don’t bother looking for more.
  prefs: []
  type: TYPE_NORMAL
- en: '**Revealing the Transient and Saving the Image**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 5-12](ch05.xhtml#ch05list12), still in the for loop of the main()
    function, displays the first input image with any transients circled, posts the
    names of the image files involved, and saves the image with a new filename. You’ll
    also print a log of the results for each image pair in the interpreter window.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5-12: Showing the circled transients, logging the results, and saving
    the results'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a conditional that checks whether a transient was found. If this evaluates
    to True, print a message in the shell. For the four images evaluated by the for
    loop, you should get this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Posting a negative outcome shows that the program is working as expected and
    leaves no doubt that the images were compared.
  prefs: []
  type: TYPE_NORMAL
- en: Next, post the names of the two images with a positive response on the img1
    array. Start by assigning a font variable for OpenCV ➊. For a listing of available
    fonts, search for *HersheyFonts* at *[https://docs.opencv.org/4.3.0/](https://docs.opencv.org/4.3.0/)*.
  prefs: []
  type: TYPE_NORMAL
- en: Now call OpenCV’s putText() method and pass it the first input image, the filename
    of the image, a position, the font variable, a size, a color (white), a thickness,
    and a line type. The LINE_AA attribute creates an anti-aliased line. Repeat this
    code for the second image.
  prefs: []
  type: TYPE_NORMAL
- en: If you found two transients, you can show them both on the same image using
    OpenCV’s addWeighted() method. This method calculates the weighted sum of two
    arrays. The arguments are the first image and a weight, the second image and a
    weight, and a scalar that’s added to each sum. Use the first input image and the
    difference image, set the weights to 1 so that each image is used fully, and set
    the scalar to 0. Assign the result to a variable named blended.
  prefs: []
  type: TYPE_NORMAL
- en: Show the blended image in a window named Surveyed. [Figure 5-13](ch05.xhtml#ch05fig13)
    shows an example outcome for the “bright” transient.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-13: Example output window of transient_detector.py with the pad rectangle
    indicated by the arrow'
  prefs: []
  type: TYPE_NORMAL
- en: Note the white rectangle near the edges of the image. This represents the PAD
    distance. Any transients outside this rectangle were ignored by the program.
  prefs: []
  type: TYPE_NORMAL
- en: Save the blended image using the filename of the current input image plus “DETECTED”
    ➋. The dim transient in [Figure 5-13](ch05.xhtml#ch05fig13) would be saved as
    *1_bright_transient_left_registered_DECTECTED.png*. Write it to the *night_1_2_transients*
    folder, using the path3 variable.
  prefs: []
  type: TYPE_NORMAL
- en: If no transients were found, document the result in the shell window. Then end
    the program with the code to run it as a module or in stand-alone mode.
  prefs: []
  type: TYPE_NORMAL
- en: '***Using the Transient Detector***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine how happy Clyde Tombaugh would’ve been with your transient detector.
    It’s truly set-it-and-forget-it. Even the changing brightness between the third
    pair of images, so problematic with the blink comparator, is no challenge for
    this program.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, you replicated an old-time blink comparator device and then
    updated the process using modern computer vision techniques. Along the way, you
    used the pathLib module to simplify working with directory paths, and you used
    a single underscore for insignificant, unused variable names. You also used OpenCV
    to find, describe, and match interesting features in images, align the features
    with homography, blend the images together, and write the result to a file.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Out of the Darkness: The Planet Pluto* (Stackpole Books, 2017), by Clyde Tombaugh
    and Patrick Moore, is the standard reference on the discovery of Pluto, told in
    the discoverer’s own words.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chasing New Horizons: Inside the Epic First Mission to Pluto* (Picador, 2018),
    by Alan Stern and David Grinspoon, records the monumental effort to finally send
    a spacecraft—which, incidentally, contained Clyde Tombaugh’s ashes—to Pluto.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Practice Project: Plotting the Orbital Path**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Edit the *transient_detector.py* program so that if the transient is present
    in both input image pairs, OpenCV draws a line connecting the two transients.
    This will reveal the transient’s orbital path against the background stars.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of information was key to the discovery of Pluto. Clyde Tombaugh used
    the distance Pluto traveled in the two discovery plates, along with the time between
    exposures, to verify that the planet was near Lowell’s predicted path and not
    just some asteroid orbiting closer to Earth.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a solution, *practice_orbital_path.py*, in the appendix and in
    the *Chapter_5* folder.
  prefs: []
  type: TYPE_NORMAL
- en: '**Practice Project: What’s the Difference?**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The feature matching you did in this chapter has broad-reaching applications
    beyond astronomy. For example, marine biologists use similar techniques to identify
    whale sharks by their spots. This improves the accuracy of the scientists’ population
    counts.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-14](ch05.xhtml#ch05fig14), something has changed between the left
    and right photos. Can you spot it? Even better, can you write Python programs
    that align and compare the two images and circle the change?
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-14: Spot the difference between the left and right images.'
  prefs: []
  type: TYPE_NORMAL
- en: The starting images can be found in the *montages* folder in the *Chapter_5*
    folder, downloadable from the book’s website. These are color images that you’ll
    need to convert to grayscale and align prior to object detection. You can find
    solutions, *practice_montage_aligner.py* and *practice_montage_difference_finder.py*,
    in the appendix and in the *montages* folder.
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Project: Counting Stars**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to *Sky and Telescope* magazine, there are 9,096 stars visible to
    the naked eye from both hemispheres (*[https://www.skyandtelescope.com/astronomy-resources/how-many-stars-night-sky-09172014/](https://www.skyandtelescope.com/astronomy-resources/how-many-stars-night-sky-09172014/)*).
    That’s a lot on its own, but if you look through a telescope, the number increases
    exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate large numbers of stars, astronomers survey small regions of the
    sky, use a computer program to count the stars, and then extrapolate the results
    to larger areas. For this challenge project, pretend you’re an assistant at Lowell
    Observatory and you’re on a survey team. Write a Python program that counts the
    number of stars in the image *5_no_transient_left.png*, used in Projects 7 and
    8.
  prefs: []
  type: TYPE_NORMAL
- en: For hints, search online for *how to count dots in an image with Python and
    OpenCV*. For a solution using Python and SciPy, see *[http://prancer.physics.louisville.edu/astrowiki/index.php/Image_processing_with_Python_and_SciPy](http://prancer.physics.louisville.edu/astrowiki/index.php/Image_processing_with_Python_and_SciPy)*.
    You may find your results improve if you divide the image into smaller parts.
  prefs: []
  type: TYPE_NORMAL
