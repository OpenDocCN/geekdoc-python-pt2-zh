<html><head></head><body><div id="sbo-rt-content"><h2 class="h2" id="ch16"><span epub:type="pagebreak" id="page_411"/><strong><span class="big">16</span><br/>GOING FURTHER</strong></h2>&#13;
<div class="imagec"><img src="Images/common.jpg" alt="image" width="189" height="189"/></div>&#13;
<p class="noindents">You now have what I feel is a good introduction to modern machine learning. We have covered building datasets, classical models, model evaluation, and introductory deep learning, from traditional neural networks to convolutional neural networks. This short chapter is intended to help you go further.</p>&#13;
<p class="indent">We’ll look at both short-term “what’s next” sorts of things as well as longer-term forks in the road you may wish to explore. We’ll also include online resources where you will find the latest and greatest (always cognizant that anything online is ephemeral). After that comes a necessarily subjective list of conferences you may wish to attend. We’ll close the chapter and book with a thank you and a goodbye.</p>&#13;
<h3 class="h3" id="lev1_107">Going Further with CNNs</h3>&#13;
<p class="noindent">Even after four chapters’ worth of material, we’ve barely scratched the surface of what convolutional neural networks can do. In part, we limited ourselves so you could grasp the fundamentals. And, in part, we were limited because we made a conscious decision not to require a GPU. Training complex models with a GPU is, in general, 20 to 25 times faster than using a <span epub:type="pagebreak" id="page_412"/>CPU. With a GPU in your system, preferably designed for deep learning applications, the possibilities increase dramatically.</p>&#13;
<p class="indent">The models we developed were small, reminiscent of the original LeNet models LeCun developed in the 1990s. They get the point across, but they will not go too far in many cases. Modern CNNs come in a variety of flavors and now “standard” architectures. With a GPU, you can explore these larger architectures.</p>&#13;
<p class="indent">These architectures should be on your list of what to look at next:</p>&#13;
<ul>&#13;
<li class="noindent">ResNet</li>&#13;
<li class="noindent">U-Net</li>&#13;
<li class="noindent">VGG</li>&#13;
<li class="noindent">DenseNet</li>&#13;
<li class="noindent">Inception</li>&#13;
<li class="noindent">AlexNet</li>&#13;
<li class="noindent">YOLO</li>&#13;
</ul>&#13;
<p class="indent">Fortunately, the Keras toolkit we introduced (but also barely explored) supports all of these architectures. The two that seem especially useful to me are ResNet and U-Net. The latter is for semantic segmentation of inputs and has been widely successful, especially in medical imaging. To successfully train any of these architectures before your computer’s power supply or hard drive has failed, to say nothing of your heart, you do need a GPU. Medium to higher-end gaming GPUs (from NVIDIA, for example) will support new enough versions of CUDA that you can get going with a card for under 500 USD. The real trick is ensuring that your computer will support the card. The power requirements are high, typically requiring a power supply of 600W or more, and a slot that supports a double-wide PCIe card. Go for RAM over performance; the more RAM the GPU has, the larger a model it will support.</p>&#13;
<p class="indent">Even if you don’t upgrade your system with a GPU, it’s worth your time to study the aforementioned architectures to see what makes them special and to understand how additional layers work. Check out the Keras documentation for more details: <a href="http://keras.io">keras.io</a>.</p>&#13;
<h3 class="h3" id="lev1_108">Reinforcement Learning and Unsupervised Learning</h3>&#13;
<p class="noindent">This book has dealt exclusively with supervised learning. Of the three main branches of machine learning, supervised learning is probably the most widely used. Recalling the Marx brothers, supervised learning is like Groucho, the one everyone remembers. That isn’t an insult to the memory of Harpo and Chico, nor is it an insult to the other two branches of machine learning: reinforcement learning and unsupervised learning.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_413"/><em>Reinforcement learning</em> is goal-oriented; it encourages models to learn how to behave and act to maximize a reward. Instead of learning how to take an input and map it to a specific output class, as in supervised learning, reinforcement learning learns how to act in the current situation to maximize an overall goal, like winning a game. Many of the impressive news stories related to machine learning have involved reinforcement learning. These include the first Atari 2600 game-playing systems capable of beating the best humans, as well as the fall of the world Go champion to AlphaGo, and the even more impressive achievement of AlphaGo Zero, which mastered Go from scratch without learning from millions of games played by humans. Any self-driving car system is likely extremely complex, but it’s a sure bet that reinforcement learning is a key part of that system.</p>&#13;
<p class="indent"><em>Unsupervised learning</em> refers to systems that learn on their own from unlabeled input data. Historically, this meant clustering, algorithms like <em>k</em>-means that take unlabeled feature vectors and attempt to group them by some similarity metric. Currently, one might argue that unsupervised learning is viewed as somewhat unimportant, given the insane amount of work being done with supervised learning and reinforcement learning. This is only half true; a lot of supervised learning is attempting to use unlabeled data (search for <em>domain adaptation</em>). How much of our own learning is unsupervised? An autonomous system set loose on an alien world will likely be more successful if it can learn things its creators didn’t know it would need to know. This suggests the importance of unsupervised learning.</p>&#13;
<h3 class="h3" id="lev1_109">Generative Adversarial Networks</h3>&#13;
<p class="noindent"><em>Generative adversarial networks (GANs)</em> burst on the scene in 2014, the brainchild of deep learning researcher Ian Goodfellow. GANs were quickly heralded as the most significant advance in machine learning in 20 years (Yann LeCun, spoken at NIPS 2016, Barcelona).</p>&#13;
<p class="indent">Recent news about models that can generate an infinite number of photo-quality human faces use GANs. So do models that create simulated scenes and convert images of one style (say, a painting) to another (like a photograph). GANs wed a network that generates outputs, often based on some random setting of its input, to a discriminative network that tries to learn how to tell the difference between real inputs and inputs that came from the generative part. The two networks are trained together so that the generative network gets better and better at fooling the discriminative network. In contrast, the discriminative network gets better and better at learning how to tell the difference. The result is a generative network that is pretty good at outputting what you want it to output.</p>&#13;
<p class="indent">A proper study of GANs would require a book, but they are well worth a look and some of your time, at least to develop an intuitive sense of what is going on. A good place to start is with the particularly popular GAN architecture, CycleGAN, which has, in turn, spawned a small army of similar models.</p>&#13;
<h3 class="h3" id="lev1_110"><span epub:type="pagebreak" id="page_414"/>Recurrent Neural Networks</h3>&#13;
<p class="noindent">A major topic entirely ignored by this book is <em>recurrent neural networks (RNNs)</em>. These are networks with feedback loops, and they work well for processing sequences like a time series of measurements—think sound samples or video frames. The most common form is the LSTM, the long short-term memory network. Recurrent networks are widely used in neural translation models like Google Translate that have made it possible to do real-time translation between dozens of languages.</p>&#13;
<h3 class="h3" id="lev1_111">Online Resources</h3>&#13;
<p class="noindent">The online resources for machine learning are legion and growing daily. Here are a few places that I find helpful and that are likely to stand the test of time. In no particular order:</p>&#13;
<p class="block"><strong>Reddit Machine Learning (<em><a href="http://www.reddit.com/r/MachineLearning/">www.reddit.com/r/MachineLearning/</a></em>)</strong> Look here for up-to-the-minute news and discussions of the latest papers and research.</p>&#13;
<p class="block"><strong>Arxiv (<em><a href="https://arxiv.org/">https://arxiv.org/</a></em>)</strong> Machine learning progresses too quickly for most papers to go through the lengthy peer-review process print journals require. Instead, almost without exception, researchers and many conferences place all their papers on this preprint server, providing free access to the very latest in machine learning research. It can be daunting to sift through. Personally, I use the Arxiv app for my phone and several times a week peruse the following categories: Computer Vision and Pattern Recognition, Artificial Intelligence, Neural and Evolutionary Computing, and Machine Learning. The number of papers appearing in just these categories per week is impressive and a good indication of how active this field really is. To address the insane quantity of papers, deep learning researcher Andrej Karpathy created the useful Arxiv Sanity site at <em><a href="http://www.arxiv-sanity.com/">http://www.arxiv-sanity.com/</a></em>.</p>&#13;
<p class="block"><strong>GitHub (<em><a href="https://github.com/">https://github.com/</a></em>)</strong> This is a place where people can host software projects. Go to the site directly and search for machine learning projects or use a standard search engine and add the keyword <em>github</em> to the search. With the explosion of machine learning projects, a beautiful thing has happened. The vast majority of the projects are freely available, even for commercial use. This typically includes full source code and datasets. If you read about something in a paper on Arxiv, you’ll likely find an implementation of it on Github.</p>&#13;
<p class="block"><strong>Coursera (<em><a href="https://www.coursera.org/">https://www.coursera.org/</a></em>)</strong> Coursera is a premier site for online courses, the vast majority of which can be audited for free. There are other sites, but Coursera was co-founded by Andrew Ng, and his machine learning course is very popular.</p>&#13;
<p class="block"><span epub:type="pagebreak" id="page_415"/><strong>YouTube (<em><a href="https://www.youtube.com/">https://www.youtube.com/</a></em>)</strong> YouTube is a force of nature at this point, but it is chock-full of machine learning videos. Let the viewer beware, but with some digging and judicious selection, you’ll find a lot here, including demonstrations of the latest and greatest. Search for “Neural Networks for Machine Learning” taught by Geoffrey Hinton.</p>&#13;
<p class="block"><strong>Kaggle (<em><a href="https://www.kaggle.com/">https://www.kaggle.com/</a></em>)</strong> Kaggle hosts machine learning competitions and is a good resource for datasets. Winners detail their models and training processes, providing ample opportunity to learn the art.</p>&#13;
<h3 class="h3" id="lev1_112">Conferences</h3>&#13;
<p class="noindent">One of the best ways to learn a new language is to immerse yourself in a culture that speaks the language. The same is true for machine learning. The way to immerse yourself in the culture of machine learning is to attend conferences. This can be expensive, but many schools and companies view it as important, so you might be able to get support for attending.</p>&#13;
<p class="indent">The massive explosion of interest in machine learning has caused a new phenomenon, one that I haven’t seen happen in other academic disciplines: conferences selling out. This is true of the biggest conferences, but it might be happening to other conferences as well. If you want to attend, be aware that timing matters. Again, in no particular order, and missing many good but smaller conferences, consider the following:</p>&#13;
<p class="block"><strong>NeurIPS (formerly NIPS)</strong> Short for <em>Neural Information Processing Systems</em>, this is likely the biggest machine learning conference. At this academic conference, you can expect to see the latest research presented. NeurIPS has sold out quickly in recent years, in under 12 minutes in 2018 (!), and has now switched to a lottery system, so unless you are a presenter of some kind, getting the golden ticket email allowing you to register is not assured. It’s usually held in Canada.</p>&#13;
<p class="block"><strong>ICML</strong> Short for <em>International Conference on Machine Learning</em>, this is perhaps the second largest annual conference. This academic conference has several tracks and workshops and is typically held in Europe or North America.</p>&#13;
<p class="block"><strong>ICLR</strong> The International Conference on Learning Representations is a deep learning–focused academic conference. If you want in-the-weeds technical presentations on deep learning, this is the place to be.</p>&#13;
<p class="block"><strong>CVPR</strong> Computer Vision and Pattern Recognition is another large conference that’s perhaps slightly less academic than ICLR. CVPR is popular and not exclusively machine learning–oriented.</p>&#13;
<p class="block"><strong>GTC</strong> The GPU Technology Conference, sponsored by NVIDIA, is a technical conference as opposed to an academic conference. The annual presentation of new NVIDIA hardware happens here, along with a large expo, in San Jose, California.</p>&#13;
<h3 class="h3" id="lev1_113"><span epub:type="pagebreak" id="page_416"/>The Book</h3>&#13;
<p class="noindent">Saying there are a few machine learning books out there is like saying there are a few fish in the sea. However, as far as deep learning is concerned, one stands head-and-shoulders above the rest: <em>Deep Learning</em> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press, 2016). See <em><a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a></em>.</p>&#13;
<p class="indent"><em>Deep Learning</em> is the book you should go to if you want to get serious about being a machine learning researcher. Even if you don’t, it covers the key topics in depth and with mathematical rigor. The book is not for those looking to get better at using one toolkit or another, but for those who want to see the theory behind machine learning and the math that goes with it. In essence, it’s an advanced undergraduate—if not graduate-level text, but that shouldn’t put you off. At some point, you will want to take a look at this book, so keep it in the back of your mind—or on your bookshelf.</p>&#13;
<h3 class="h3" id="lev1_114">So Long and Thanks for All the Fish</h3>&#13;
<p class="noindent">We’ve reached the end of the book. There’s no monster here, only ourselves, and the knowledge and intuition we’ve gained by working through the preceding chapters. Thank you for persevering. It’s been fun for me to write; I genuinely hope it’s been fun for you to read and contemplate. Don’t stop now—take what we’ve developed and run with it. If you’re like me, you’ll see applications for machine learning everywhere. Go forth and classify!</p>&#13;
</div></body></html>