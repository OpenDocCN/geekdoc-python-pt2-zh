- en: '**7'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DIFFERENTIAL CALCULUS**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The discovery of “the calculus” by Sir Issac Newton, and separately by Gottfried
    Wilhelm Leibniz, was one of the greatest achievements in the history of mathematics.
    Calculus is typically split into two main parts: differential and integral. Differential
    calculus talks about rates of change and their relationships, embodied in the
    notion of the derivative. Integral calculus is concerned with things like the
    area under a curve.'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t need integral calculus for deep learning, but we’ll use differential
    calculus often. For example, we use differential calculus to train neural networks;
    we adjust the weights of a neural network using gradient descent, which relies
    on derivatives calculated via the backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The derivative will be the star of this chapter. We’ll begin by introducing
    the idea of slope and seeing how it leads to the notion of a derivative. We’ll
    then formally define the derivative and learn how to calculate derivatives of
    functions of one variable. After that, we’ll learn how to use derivatives to find
    the minima and maxima of functions. Next come partial derivatives, the derivatives
    with respect to a single variable for functions of more than one variable. We’ll
    use partial derivatives extensively in the backpropagation algorithm. We’ll conclude
    with gradients, which will introduce us to matrix calculus, the subject of [Chapter
    8](ch08.xhtml#ch08).
  prefs: []
  type: TYPE_NORMAL
- en: Slope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In algebra class, we learned all about lines. One way to define a line is the
    slope-intercept form,
  prefs: []
  type: TYPE_NORMAL
- en: '*y* = *mx* + *b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where *m* is the slope and *b* is the *y* intercept, the place where the line
    crosses the y-axis. We’re interested in the slope. If we know two points on the
    line, (*x*[1], *y*[1]) and (*x*[0], *y*[0]), we know the slope of the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/164equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *slope* tells us how much of a change in *y* we will get for any given change
    in *x* position. If the slope is positive, then a positive change in *x* leads
    to a positive change in *y*. On the other hand, a negative slope means that a
    positive change in *x* leads to a negative change in *y*.
  prefs: []
  type: TYPE_NORMAL
- en: The slope-intercept form of the line tells us that the slope is the proportionality
    constant between *x* and *y*. The intercept, *b*, is a constant offset. This means
    a change in *x* position from *x*[1] to *x*[0] leads to an *m*(*x*[1] − *x*[0])
    change in *y*. Slopes relate two things, telling us how changing one affects the
    other. We’ll get back to this idea several times in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s visualize this with some examples. [Figure 7-1](ch07.xhtml#ch07fig01)
    shows the plot of a curve and some lines that intersect it.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-1: A curve with a secant line (*A*) and a tangent line (*B*)*'
  prefs: []
  type: TYPE_NORMAL
- en: The line labeled *A* crosses the curve at two points, *x*[1] and *x*[0]. A line
    passing between two points on a curve is called a *secant* line. The other line,
    *B*, just touches the curve at the point *x**[t]*. Lines that touch a curve at
    one point are called *tangent* lines. We’ll get back to secant lines in the next
    section, but, for now, notice that the tangent line has a particular slope at
    *x**[t]* and that a secant line becomes a tangent line as the distance between
    the points *x*[1] and *x*[0] goes toward zero.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we move the point *x**[t]* from place to place along the curve;
    we can see that the slope of the tangent line at *x**[t]* would change with it.
    As we approach the minimum point of the curve, near *x* = 0.3, we see that the
    slope becomes more and more shallow. If we approach from the left, the slope is
    negative and becomes less and less negative. If we approach from the right, the
    slope is positive but becomes smaller and smaller. At the actual minimum point,
    near *x* = 0.3, the tangent line is horizontal, with a slope of zero. Similarly,
    if we approach the maximum of the curve, near *x* = −0.8, the slope also approaches
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the tangent line tells us how the curve is changing at a point.
    As we’ll see later in the chapter, the fact that the slope of a tangent line is
    zero at the minima and maxima of a curve points us toward a method for finding
    these points. The points where the slope of the tangent line is zero are known
    as *stationary points*.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, to take advantage of the slope of the tangent line, we need to be
    able to find its value for any *x* on the curve. The next section will show us
    how.
  prefs: []
  type: TYPE_NORMAL
- en: Derivatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous section introduced the idea of secant and tangent lines and hinted
    that knowing the slope of the tangent line at any point on a curve is a potentially
    useful thing. The slope of the tangent line at a point *x* is known as the *derivative*
    at *x*. It tells us how the curve (function) is changing at the point *x*, that
    is, how the function value changes with an infinitesimal change in *x*. In this
    section, we’ll formally define the derivative and learn shortcut rules for calculating
    derivatives of functions of a single variable, *x*.
  prefs: []
  type: TYPE_NORMAL
- en: A Formal Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A typical first-semester calculus course introduces you to derivatives through
    studying limits. I mentioned above how the slope of the secant line between two
    points on a curve becomes a tangent line when the points collapse on top of each
    other, and that’s one place where limits come into play.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if *y* = *f*(*x*) is a curve, and we have two points on the curve,
    *x*[0] and *x*[1], then the slope, Δ*y*/Δ*x*, between these points is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the *rise over run* you may remember learning in school. The *rise*,
    *Δy* = *y*[1] − *y*[0] = *f*(*x*[1]) − *f*(*x*[0]), is divided by the *run*, Δ*x*
    = *x*[1] − *x*[0]. We typically use Δ as a prefix to mean the change in some variable.
  prefs: []
  type: TYPE_NORMAL
- en: If we define *h* = *x*[1] − *x*[0], we can rewrite [Equation 7.1](ch07.xhtml#ch07equ01)
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/166equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: since *x*[1] = *x*[0] + *h*.
  prefs: []
  type: TYPE_NORMAL
- en: In this new form, we can find the slope of the tangent line at *x*[0] by letting
    *h* get closer and closer to zero, *h* → 0\. Letting a value approach another
    value is a *limit*. Letting *h* → 0 moves the two points we’re calculating the
    slope between closer and closer. This leads directly to the definition of the
    derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *dy*/*dx* or *f′*(*x*) is used to represent the derivative of *f*(*x*).
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into what the derivative means, let’s take a minute to discuss
    notation. Using *f*′(*x*) for the derivative follows Joseph-Louis Lagrange. Leibniz
    used *dy*/*dx* to mirror the notation for slope with Δ → *d*. If Δ*y* is a change
    in *y* between two points, *dy* is the infinitesimal change in *y* at a single
    point. Newton used yet another notation, ![Image](Images/166equ02.jpg), with the
    dot representing the derivative of *f*. Physicists often use Newton’s notation
    for the specific case of derivatives with respect to time. For example, if *f*(*t*)
    is the position of a particle as a function of time, *t*, then ![Image](Images/166equ03.jpg)
    is the derivative with respect to *t*, that is, how the position is changing in
    time. How a position changes in time is the speed (velocity if using vectors).
    You’ll see all of these notations in books. My preference is to preserve ![Image](Images/166equ02.jpg)
    for functions of time and use Lagrange’s *f*′(*x*) and Leibniz’s *dy*/*dx* interchangeably
    elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Although [Equation 7.2](ch07.xhtml#ch07equ02) above is quite tedious and the
    bane of many beginning calculus students, at least until they hit integration,
    you could work with it if you had to. However, we won’t discuss integration at
    all in this book, so you can take a deep breath and relax.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the struggle with limits, calculus students are let in on a secret: a
    small set of rules will allow you to calculate virtually all derivatives *without*
    using limits. We’ll start by introducing these rules, one at a time with examples,
    and then, at the end of this section, we’ll put them all together in a form suitable
    for a T-shirt.'
  prefs: []
  type: TYPE_NORMAL
- en: However, before we dive into the rules, let’s spend a little more time discussing
    *what* the derivative is telling us. Above, I mentioned that how a position changes
    in time is given by the derivative. This is true of all derivatives; they tell
    us how something is changing with respect to how something else is changing. We
    even see this in Leibniz’s notation, *dy*/*dx*, how *dy* changes for a change
    in *dx*. The derivative at *x* tells us how the function is changing at *x*. As
    we’ll see, the derivative of *f*(*x*) is itself a new function of *x*. If we pick
    a specific *x*[0], then we know that *f*(*x*[0]) is the value of the function
    at *x*[0].
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if we know the derivative, then *f*′(*x*[0]) is how quickly, and
    in which direction, the function, *f*(*x*), is changing at *x*[0]. Consider the
    definition of speed as how the position changes with time. We even say it in words:
    my current speed is 30 mph—*miles per hour*—a change in position with respect
    to a change in time.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use derivatives like rates and see how changing one thing affects another.
    In the end, for deep learning, we want to know how changing the value of a parameter
    in a network will ultimately change the loss function, the error between what
    the network should have output and what it actually output.
  prefs: []
  type: TYPE_NORMAL
- en: If *f*′(*x*) is a function of *x*, then we should be able to take the derivative
    of it. We call *f*′(*x*) the *first derivative*. Its derivative, which we denote
    as *f*′′(*x*), is the *second derivative*. In Leibniz’s notation, we write *d*²*y*/*dx*².
    The second derivative tells us how the first derivative is changing with respect
    to *x*. Physics helps here. The first derivative of the position as a function
    of time (*f*) is the *velocity*, ![Image](Images/167equ01.jpg)—how the position
    is changing with time. Therefore, the second derivative of the position, ![Image](Images/167equ02.jpg),
    which is the first derivative of the velocity, is how the velocity is changing
    with time. We call this the *acceleration*.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, there is no end to how many derivatives we can calculate. In reality,
    many functions ultimately end up with a derivative that is a constant value. Since
    a constant value doesn’t change, its derivative is zero.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, the derivative of *f*(*x*) is another function, *f*′(*x*) or *dy*/*dx*,
    that tells us the slope of the line tangent to *f*(*x*) at every point. And, since
    *f*′(*x*) is a function of *x*, it also has a derivative, *f*′′(*x*) or *d*²*y*/*dx*²,
    the second derivative, telling us how *f*′(*x*) changes at each *x*, and so on.
    We’ll see below how to make use of first and second derivatives. For now, let’s
    learn the rules of *differentiation*, the act of calculating a derivative.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Rules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We mentioned one rule in the previous section: that the derivative of a constant,
    *c*, is zero. So, we write'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/167equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where we’re using Leibniz’s notation in operator form: *d*/*dx*. Think of *d*/*dx*
    as something operating on what follows; it does this the same way that negation
    does: to negate *c*, we write *−c*; to differentiate *c*, we write ![Image](Images/167equ04.jpg).
    If our expression has no *x*, then we will treat it as a constant, and the derivative
    will be zero.'
  prefs: []
  type: TYPE_NORMAL
- en: The Power Rule
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The derivative of a power of *x* uses the *power rule*,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/168equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where *a* is a constant and *n* is an exponent that doesn’t need to be an integer.
    Let’s see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/168equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We often build algebraic expressions out of terms that are added and subtracted.
    Differentiation is a linear operator, so we can write
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/168equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This means we calculate derivatives term by term. For example, with the set
    of rules we have so far, we now know how to calculate the derivative of a polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/168equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In general, then,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/169equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we see that the derivative of a polynomial of degree *n* is another polynomial
    of degree *n* − 1 and that any constant term in the original polynomial drops
    to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The Product Rule
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Differentiation of functions multiplied together has its own rule, the *product
    rule*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/169equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The derivative of the product is the derivative of the first function times
    the second plus the derivative of the second times the first. Consider the following
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/169equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Quotient Rule
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The derivative of a function divided by another function follows the *quotient
    rule*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/169equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This leads to examples like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/170equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Chain Rule
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The next rule concerns the composition of functions. Two functions are composed
    when the output of one is used as the input to another. The *chain rule* applies
    to function compositions, and it’s of fundamental importance in the training of
    neural networks. The rule is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/170equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We multiply the derivative of the outer function, using *g*(*x*) as the variable,
    by the derivative of the inner function with respect to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first example, consider the function *f*(*x*) = (*x*² + 2*x* + 3)². Is
    this the composition of two functions? It is. Let’s define *f*(*g*) = *g*² and
    *g*(*x*) = *x*² + 2*x* + 3\. Then, we can find *f*(*x*) by replacing every instance
    of *g* in *f*(*g*) with the definition of *g* in terms of *x*: *g*(*x*) = *x*²
    + 2*x* + 3\. This gives'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) = *g*² = (*x*² + 2*x* + 3)²'
  prefs: []
  type: TYPE_NORMAL
- en: which is, naturally, what we started with. To find *f*′(*x*), we first find
    *f*′(*g*), the derivative of *f* with respect to *g*, and then multiply by *g*′(*x*),
    the derivative of *g* with respect to *x*. As a final step, we replace references
    to *g* with its definition in terms of *x*. So, we calculate *f*′(*x*) as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/170equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We typically don’t explicitly call out *f*(*g*) and *g*(*x*), but mentally we
    work through the same process. Let’s see some more examples. In this one, we want
    to find
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/171equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we use the chain rule, we see that we have *f*(*g*) = 2*g*² + 3 and *g*(*x*)
    = 4*x* − 5\. Therefore, we can write
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/171equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *f*′(*g*) = 2*g* and *g*′(*x*) = 4\. With some practice, we’d mentally
    picture the 4*x* − 5 of *f*(*x*) as its own variable (the substitution of *g*)
    and then remember to multiply the derivative of 4*x* − 5 when done. What if we
    didn’t see the composition? What if we expanded the entire function, *f*(*x*),
    and then took the derivative? We’d better get the answer we found above by using
    the chain rule. Let’s see . . .
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/171equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is what we found above, so our application of the chain rule is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at another example. If ![Image](Images/171equ04.jpg), how should
    we think of calculating the derivative? If we look at the function as ![Image](Images/171equ05.jpg),
    with *u*(*x*) = 1 and *v*(*x*) = 3*x*², we can use the quotient rule and get the
    following.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/172equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we used a shorthand notation with *u* and *v* that drops the formal function
    of *x* notation.
  prefs: []
  type: TYPE_NORMAL
- en: We can also picture *f*(*x*) as (3*x*²)^(−1). If we think of it this way, we
    can apply the chain rule and power rule to get
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/172equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: proving that sometimes there’s more than one way to calculate a derivative.
  prefs: []
  type: TYPE_NORMAL
- en: We presented the chain rule using Lagrange’s notation. Later in the chapter,
    we’ll see it again using Leibniz’s notation. Let’s move on now and present a set
    of rules for trigonometric functions.
  prefs: []
  type: TYPE_NORMAL
- en: Rules for Trigonometric Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The derivatives of the basic trigonometric functions are straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/172equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see the last rule is correct if we apply the basic differentiation rules
    to the definition of the tangent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/173equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Remember that sec *x* = 1/ cos *x* and sin² *x* + cos² *x* = 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some examples using the new trig rules. We’ll start with one
    composing a function with a trig function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/173equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that this is a composition of *f*(*g*) = sin(*g*) and *g*(*x*) =
    *x*³ − 3*x*, so we know we can apply the chain rule to get the derivative as *f*′(*g*)*g*′(*x*)
    with *f*′(*g*) = cos(*g*) and *g*′(*x*) = 3*x*² − 3\. The second line simplifies
    the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a more complicated composition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/173equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This time, we break up the composition as *f*(*g*) = *g*² and *g*(*x*) = sin(*x*³
    − 3*x*). However, *g*(*x*) is itself a composition of *g*(*u*) = sin(*u*) and
    *u*(*x*) = *x*³ − 3*x*, as we had in the previous example. So, the first step
    is to write the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/174equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first line is the definition of the derivative of a composition. The second
    line substitutes the derivative of *f*(*g*), which is *2g*, and the third line
    replaces *g*(*x*) with sin(*x*³ − 3*x*). Now, we just need to find *g*′(*x*),
    which we can do by using the chain rule a second time with *g*(*u*) = sin *u*
    and *u*(*x*) = *x*³ − 3*x*, as we did for the example above. Doing this gives
    us *g*′(*x*) = cos(*x*³ − 3*x*)(3*x*² − 3), so now we know that *f*′(*x*) is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/174equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let’s do one more example. This one will involve more than one trig function.
    We want to see how to calculate
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/174equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As is often the case when working with trig functions, identities come into
    play. Here, we see that *f*(*x*) can be rewritten:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/174equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now the derivative uses the trig rules, the definition of the secant, the chain
    rule, and the product rule, as shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/175equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let’s move on and look at derivatives of exponentials and logarithms.
  prefs: []
  type: TYPE_NORMAL
- en: Rules for Exponentials and Logarithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The derivative of *e**^x*, where *e* is the base of the natural logarithm (*e*
    ≈ 2.718 . . .), is particularly simple. It is itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/175equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When the argument is a function of *x*, this becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the derivative of *e^x* is *e^x*, what is the derivative of *a^x* when *a*
    is a real number other than *e*? To see the answer, we need to remember that *e^x*
    and ln *x*, the natural logarithm using base *e*, are inverse functions, so *e*^(ln*a*)
    = *a*. Then, we can write
  prefs: []
  type: TYPE_NORMAL
- en: '*a^x* = (*e*^(ln *a*))^(*x*) = *e^x*^(ln *a*)'
  prefs: []
  type: TYPE_NORMAL
- en: We know how to find the derivative of *e^x*^(ln*a*) from [Equation 7.3](ch07.xhtml#ch07equ03)
    above. It is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/175equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: but *e x* ln *a* = *a^x*, so we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/175equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and, in general,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice, if *a* = *e*, we have ln(*e*) = 1, and [Equation 7.4](ch07.xhtml#ch07equ04)
    becomes [Equation 7.3](ch07.xhtml#ch07equ03).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look now at the derivative of the natural log itself. It is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/176equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When the argument is a function of *x*, this becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You may be wondering: How do we find the derivative of a logarithm that uses
    a base other than *e*? For example, what is the derivative of log[10] *x*? To
    answer this question, we do something similar to what we did above for the derivative
    of *a**^x*. We write the logarithm of *x* for some base, *b*, in terms of the
    natural log, as'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/176equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ln *b* is a constant that does not depend on *x*. Also, we now know how
    to find the derivative of ln *x*, so we see that the derivative of log*[b] x*
    must be
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/176equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for any real number base *b* ≠ 1\. And, still more generally,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where, again, we notice that if *b* = *e*, we get ln *e* = 1, and [Equation
    7.6](ch07.xhtml#ch07equ06) becomes [Equation 7.5](ch07.xhtml#ch07equ05).
  prefs: []
  type: TYPE_NORMAL
- en: With [Equation 7.6](ch07.xhtml#ch07equ06), we’ve reached the end of our rules
    for derivatives. Let’s now put them together in a single table we can refer back
    to throughout the remainder of the book. The result is [Table 7-1](ch07.xhtml#ch07tab01).
  prefs: []
  type: TYPE_NORMAL
- en: We know how to find derivatives now. I would encourage you to look for practice
    sheets with worked-out answers to convince yourself that you understand the rules
    and how to apply them. Let’s move on and look at how we can use derivatives to
    find the minima and maxima of functions. Finding minima is critical to the training
    of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-1:** The Rules of Differentiation'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Rule** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Constants | ![Image](Images/177equ01.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Powers | ![Image](Images/177equ02.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Sums | ![Image](Images/177equ03.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Products | ![Image](Images/177equ04.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Quotients | ![Image](Images/177equ05.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Chain | ![Image](Images/177equ06.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Trigonometry | ![Image](Images/177equ07.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Exponents | ![Image](Images/177equ08.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Logarithms | ![Image](Images/177equ09.jpg) |'
  prefs: []
  type: TYPE_TB
- en: Minima and Maxima of Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, I defined stationary points as places where the first derivative of
    a function is zero, that is, places where the slope of the tangent line is zero.
    We can use this information to decide if a particular point, call it *x[m]*, is
    a minimum or maximum of the function, *f*(*x*). If *x[m]* is a minimum, it is
    a low point of the function, where *f*(*x[m]*) is smaller than any point to the
    immediate left or right of *f (x[m]*). Similarly, if *f*(*x[m]*) is higher than
    any point to the immediate left or right, *f*(*x[m]*) is a maximum. We collectively
    refer to minima and maxima as *extrema* of *f*(*x*) (singular, *extremum*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the derivative, a *minimum* is a place where the derivative of
    points immediately to the left of *x[m]* are negative, and derivatives of points
    directly to the right of *x[m]* are positive. *Maxima* are the reverse: derivatives
    to the left are positive, and derivatives to the right of *x[m]* are negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Look back at [Figure 7-1](ch07.xhtml#ch07fig01). There, we have a local maximum
    at about *x* = −0.8 and a local minimum at about *x* = 0.3\. Let’s say that the
    maximum is actually at *x[m]* = −0.8\. This is a maximum because if we look at
    any point *x[p]* in the vicinity of *x[m]*, *f*(*x[p]*) is less than *f*(*x[m]*).
    Likewise, if the minimum is at *x[m]* = 0.3, that’s because any point *x[p]* near
    it has *f*(*x[p]*) > *f*(*x[m]*). If we imagine the tangent line sliding along
    the graph, as it approaches *x* = −0.8, we see that the slope is positive but
    heading toward zero. If we move past *x* = −0.8, the slope is now negative. The
    reverse is true for the minimum at *x* = 0.3\. Tangent lines approaching from
    the left have negative slope, but once they’re past *x* = 0.3, the slope is positive.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll read and hear the terms *global* and *local* applied to minima and maxima.
    The global minimum of *f*(*x*) is the lowest of all the minima of *f*(*x*), and
    the global maximum is the highest of all the maxima. Other minima and maxima,
    then, are considered local; they are effective over a particular region, but there
    are other minima that are lower or maxima that are higher. We should note that
    not all functions have minima or maxima. For example, a line, *f*(*x*) = *mx*
    + *b*, has no minima or maxima, because there are no points on the line that satisfy
    the requirements for either.
  prefs: []
  type: TYPE_NORMAL
- en: So, if the first derivative, *f*′(*x*), is zero, we have a minimum or maximum,
    right? Not so fast. At other stationary points, the first derivative may be zero,
    but the remaining criteria for a minimum or maximum are not met. These points
    are often called *inflection points* or, if in multiple dimensions, *saddle points*.
    For example, consider *y* = *x*³. The first derivative is *y*′ = 3*x*², and the
    second derivative is *y*′′ = 6*x*. Both the first and second derivative are zero
    at *x* = 0\. However, as we can see in [Figure 7-2](ch07.xhtml#ch07fig02), the
    slope is positive to both the immediate left and immediate right of *x* = 0\.
    Therefore, the slope never switches from positive to negative or negative to positive,
    meaning *x* = 0 is not an extremum but is an inflection point.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-2: A graph of y = x³ showing an inflection point at x = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: Now assume that *x[s]* is a stationary point so that *f*′(*x[s]*) = 0\. If we
    pick two other points, *x[s−∊]* and *x[s+∊]*, one just to the left of *x[s]* and
    the other just to the right, for some very small ∊ (epsilon), we have four possibilities
    for the values of *f*′(*x[s]*[−∊]) and *f*′(*x[s]*[+∊]), shown in [Table 7-2](ch07.xhtml#ch07tab02).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 7-2:** Identifying Stationary Points'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sign of *f’*(*x[s]* – ∊), *f’*(*x[s]* + ∊)** | **Type of stationary point
    at *x[s]* – ∊ < *x[s]* < *x[s]* + ∊** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| +, – | Maximum |'
  prefs: []
  type: TYPE_TB
- en: '| –, + | Minimum |'
  prefs: []
  type: TYPE_TB
- en: '| +, + | Neither |'
  prefs: []
  type: TYPE_TB
- en: '| –, – | Neither |'
  prefs: []
  type: TYPE_TB
- en: Therefore, the value of the first derivative at a candidate stationary point
    isn’t enough to tell us whether the point represents a minimum or maximum. We
    can look at the region around the candidate point to help us decide. We can also
    look at the value of *f*′′(*x*), the second derivative of *f*(*x*). If *x[s]*
    is a stationary point where *f*′(*x[s]*) = 0, the sign of *f*′′(*x[s]*) can tell
    us about what type of stationary point *x[s]* might be. If *f*′′(*x[s]*) < 0,
    then *x[s]* is a *maximum* of *f*(*x*). If *f*′′(*x[s]*) > 0, *x[s]* is a minimum.
    If *f*′′(*x[s]*) = 0, the second derivative isn’t helpful; we will need to explicitly
    test nearby points with the first derivative.
  prefs: []
  type: TYPE_NORMAL
- en: How do we find candidate stationary points in the first place? For algebraic
    functions, we solve *f*′(*x*) = 0; we find the solution set of all the *x* values
    that make the first derivative of *f*(*x*) zero. We then use the derivative tests
    to decide which are minima, maxima, or inflection points.
  prefs: []
  type: TYPE_NORMAL
- en: 'For many functions, we can find the solutions to *f*′(*x*) = 0 directly. For
    example, if *f*(*x*) = *x*³ − 2*x* + 4, we have *f*′(*x*) = 3*x*² − 2\. If we
    set this equal to zero, *3x*[2] − 2 = 0, and solve using the quadratic formula,
    we find that there are two stationary points: ![Image](Images/179equ01.jpg) and
    ![Image](Images/179equ02.jpg). The second derivative of *f*(*x*) is *f*′′(*x*)
    = 6*x*. The sign of *f*′′(*x*[0]) is negative; therefore, *x*[0] represents a
    maximum. And because the sign of *f*′′(*x*[1]) is positive, *x*[1] is a minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the derivative tests are correct. The top of [Figure 7-3](ch07.xhtml#ch07fig03)
    shows us a plot of *f*(*x*) = *x*³ − 2*x* + 4, where *x*[0] is a maximum and *x*[1]
    is a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at one more example. This time, we have *f*(*x*) = *x*⁵−2*x*³+*x*+2,
    the bottom plot of [Figure 7-3](ch07.xhtml#ch07fig03). We find the first derivative
    and set it to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*′(*x*) = 5*x*⁴ − 6*x*² + 1 = 0'
  prefs: []
  type: TYPE_NORMAL
- en: If we substitute *u* = *x*², we can solve for the roots of *f*′(*x*) by finding
    the roots of 5*u*²−6*u*+1 and setting those equal to *x*². Doing this gives us
    ![Image](Images/179equ03.jpg) and ![Image](Images/179equ04.jpg), so we have four
    stationary points. To test them, we can use the second derivative test. The second
    derivative is *f*′′(*x*) = 20*x*³ − 12*x*.
  prefs: []
  type: TYPE_NORMAL
- en: Substituting the stationary points into *f*′′ gives
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/180equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: meaning *x*[0] is a maximum, *x*[1] is a minimum, *x*[2] is another maximum,
    and *x*[3] is a minimum. [Figure 7-3](ch07.xhtml#ch07fig03) again confirms our
    conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-3: Plots of f(x) = x³ – 2x + 4 (top) and x⁵ – 2x³ + x + 2 (bottom),
    with extrema marked*'
  prefs: []
  type: TYPE_NORMAL
- en: What if we can’t easily find the stationary points of a function? Perhaps we
    can’t solve the function algebraically, or maybe it can’t be expressed in closed
    form, meaning no finite set of operations represents it. A typical calculus course
    isn’t interested in these situations. Still, we need to be, because one way to
    think of a neural network is as a function approximator, one whose function can’t
    be expressed directly. Can we still profitably use our new knowledge of derivatives?
    The answer is yes, we can. We can use the derivative as a pointer to tell us how
    to move closer and closer to the extrema. This is what gradient descent does,
    and we’ll spend quite a bit of time discussing it later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: For now, however, let’s move on and examine functions of more than one variable
    and see what this does to the idea of a derivative.
  prefs: []
  type: TYPE_NORMAL
- en: Partial Derivatives
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve focused exclusively on functions of one variable, *x*. What happens
    to the notion of differentiation when we have functions of more than one variable,
    say, *f*(*x*, *y*), or *f*(*x*[0], *x*[1], *x*[2], . . . , *x[n]*)? To handle
    these cases, we’ll introduce the idea of a *partial derivative*. Note that, for
    clarity, we’ll use Leibniz’s notation in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 7.2](ch07.xhtml#ch07equ02) defined the derivative of *f*(*x*) with
    respect to *x*. If *x* is the only variable, why did we add the extra phrase “with
    respect to *x*”? Now we’ll find out why: the partial derivative with respect to
    one of the variables in the expression is found by holding all the other variables
    fixed. We treat them as if they were constants. Then we say we’re calculating
    the partial derivative with respect to the one not held fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. Let *f*(*x*, *y*) = *xy* + *x*/*y*. Then, we can
    calculate *two* partial derivatives, one with respect to *x* and the other with
    respect to *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/181equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The rules of differentiation we learned earlier in the chapter still apply.
    Notice the *d* has changed to ∂. This indicates that the function, *f*, is of
    more than one variable. Also, see that when calculating the respective derivatives,
    we held the other variable fixed as if it were a parameter. As far as calculating
    partial derivatives, that’s all there is to it. Let’s see a few more examples
    to help you make the idea more concrete.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *f*(*x*, *y*, *z*) = *x*² + *y*² + *z*² + 3*xyz*, we can find three partial
    derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/182equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The other two variables are considered constant. This is why, for example, in
    the partial derivative with respect to *x*, *y*² and *z*² become 0, and 3*xyz*
    becomes *3yz*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![Image](Images/182equ02.jpg), we have four partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/182equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As a more complex example, consider *f*(*x*, *y*) = *e^(xy)* cos *x* sin *y*.
    The partial derivatives are listed next where we use the product rule in each
    case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/183equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mixed Partial Derivatives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just as with the derivative of a function of a single variable, we can take
    partial derivatives of a partial derivative. These are known as *mixed partials*.
    Additionally, we have more flexibility because we can change which variable we
    take the next partial derivative with respect to. For example, above, we saw that
    the partial derivative of ![Image](Images/183equ02.jpg) with respect to *z* is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/183equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'which is still a function of *x*, *y*, *z*, and *t*. Therefore, we can calculate
    second partial derivatives like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/183equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I’ll explain the notation. We started with the partial derivative of *f* with
    respect to *z*, so we write ∂*f*/∂*z*. Then, from this starting point, we are
    taking other partial derivatives. So, if we want to denote the partial with respect
    to *x*, we think of it this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/184equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where we can think of the partial derivative operator “multiplying” the “numerator”
    and “denominator” like a fraction. To be clear, however, these are not fractions;
    the notation has just inherited the flavor of a fraction from its slope origins.
    Still, if the mnemonic is helpful, then it’s helpful. For a second partial derivative,
    the variable it’s taken with respect to is on the left. Also, if the variables
    are the same, an exponent (of sorts) is used, as in ∂²*f*/∂*z*².
  prefs: []
  type: TYPE_NORMAL
- en: The Chain Rule for Partial Derivatives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To apply the chain rule to partial derivatives, we need to track all the variables.
    So, if we have *f*(*x*, *y*), where both *x* and *y* are functions of other variables,
    *x*(*r*, *s*) and *y*(*r*, *s*), then we can find the partials of *f* with respect
    to *r* and *s* by applying the chain rule for each variable, *x* and *y*. Specifically,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/184equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As an example, let *f*(*x*, *y*) = *x*³+*y*³ with *x*(*r*, *s*) = 3*r*+2*s*
    and *y*(*r*, *s*) = *r*²−3*s*. Now find ∂*f*/∂*r* and ∂*f*/∂*s*. To find these
    partials, we’ll need to calculate six expressions,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/184equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: so that the desired partials are
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/185equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Just as with a function of a single variable, the chain rule for functions of
    more than one variable is recursive, such that if *r* and *s* were themselves
    functions of another variable, we could apply the chain rule one more time to
    find the partial of *f* with respect to that variable. For example, if we have
    *x*(*r*, *s*), *y*(*r*, *s*), with *r*(*w*), *s*(*w*), we find ∂*f*/∂*w* by using
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/185equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the end, we need to remember that ∂*f*/∂*w* tells us how *f* will change
    for a small change in *w*. We’ll use this fact during gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: This section concerned itself with the mechanical calculation of partial derivatives.
    Let’s move on to explore more of the meaning behind these quantities. This will
    lead us to the idea of a gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.xhtml#ch08), we will dive into the matrix calculus representation
    we use in deep learning. However, before we do that, we will conclude this chapter
    by introducing the idea of a *gradient*. The gradient builds on the derivatives
    we’ve been calculating. In short, the gradient tells us how a function of more
    than one variable is changing and the direction in which it is changing the most.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the Gradient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we have *f*(*x*, *y*, *z*), we saw above how to calculate partial derivatives
    with respect to each of the variables. If we interpret the variables as positions
    on coordinate axes, we see that *f* is a function that returns a scalar, a single
    number, for any position in 3D space, (*x*, *y*, *z*). We could even go so far
    as to write *f*(***x***) where ***x*** = (*x*, *y*, *z*) to acknowledge that *f*
    is a function of a vector input. As in previous chapters, we’ll use lowercase
    bold letters to represent vectors, ***x***. Note, some people use ![Image](Images/xbar.jpg)
    to represent vectors.
  prefs: []
  type: TYPE_NORMAL
- en: We can write vectors horizontally, as a row vector, like in the previous paragraph,
    or vertically,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/186equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: as a column vector, where we’ve also used square brackets instead of parentheses.
    Either notation is acceptable. Unless we’re intentionally sloppy, usually when
    we’re discussing a vector in code, we’ll assume that our vectors are column vectors.
    This means a vector is a matrix with *n* rows and one column, *n* × 1.
  prefs: []
  type: TYPE_NORMAL
- en: A function that accepts a vector input and returns a single number as output
    is known as a *scalar field*. The canonical example of a scalar field is temperature.
    We can measure the temperature at any point in a room. We represent the location
    as a 3D vector relative to some chosen origin point, and the temperature is the
    value at that point, the average kinetic energy of the molecules in that region.
    We can also talk about functions that accept vectors as input and return a vector
    as output. These are known as *vector fields*. In both cases, the field part refers
    to the fact that, over some suitable domain, the function has a value for all
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient is the derivative of a function that accepts a vector as input.
    Mathematically, we represent the gradient as a generalization of the idea of partial
    derivatives to *n* dimensions. For example, in 3D space, we can write
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/187equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the gradient operator, ▽, takes the partial derivative of *f* along each
    of its dimensions. The ▽ operator goes by multiple names, like *del*, *grad*,
    or *nabla*. We’ll use ▽ and call it *del* when we’re not simply saying “gradient
    operator.”
  prefs: []
  type: TYPE_NORMAL
- en: In general, we can write
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s parse [Equation 7.7](ch07.xhtml#ch07equ07). First, we have a function,
    *f*, that accepts a vector input, ***x***, and returns a scalar value. To this
    function, we apply the gradient operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/187equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This returns a *vector* (***y***). The gradient operator turns the scalar output
    of *f* into a vector. Let’s spend some time thinking about what this means, what
    it’s telling us about the value of the scalar field at a given position in space.
    (We’ll use *space* when working with vectors, even if there’s no meaningful way
    to visualize the space. An analogy to 3D space is helpful but only goes so far;
    mathematically, the idea of space is more general.)
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider a function in 2D space, *f*(***x***) = *f*(*x*, *y*)
    = *x*² + *xy* + *y*². The gradient of *f* is then
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/07equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since *f* is a scalar field, every point on the 2D plane has a function value.
    This is the output of *f*(***x***) = *f*(*x*, *y*). So, we can plot *f* in 3D
    to show us a surface changing with position. The gradient, however, gives us a
    set of equations. These equations collectively tell us the direction and magnitude
    of the change in the function value at a point, ***x*** = (*x*, *y*).
  prefs: []
  type: TYPE_NORMAL
- en: For a function of a single variable, there’s only one slope at each point. Look
    again at the tangent line of [Figure 7-1](ch07.xhtml#ch07fig01). At the point
    *x[t]*, there’s only one slope. The sign of the derivative at *x[t]* gives the
    direction of the slope, and the absolute value of the derivative gives the magnitude
    (steepness) of the slope.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, once we change to more than one dimension, we have a bit of a conundrum.
    Instead of only one slope tangent to the function, we now have an infinite number.
    We can imagine a line tangent to the function at some point and that the line
    points in any direction we so desire. The slope of the line tells us how the function
    value is changing in that particular direction. We can find the value of this
    change from the *directional derivative*, the dot product between the gradient
    at the point under consideration and a unit vector in the direction we’re interested
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: '*D**[u]**f*(***x***) ≡ ***u**• ▽f*(***x***) = ***u**^T*▽*f*(***x***) = ||***u***||||▽*f*(***x***)||
    cos *θ*'
  prefs: []
  type: TYPE_NORMAL
- en: where ***u*** is a unit vector in a particular direction, ▽*f*(***x***) is the
    gradient of the function at the point ***x***, and *θ* is the angle between them.
    The directional derivative is maximized when cos *θ* is maximized, and this happens
    at *θ* = 0\. Therefore, the direction of the maximum change in a function at any
    point is the gradient at that point.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s pick a point on the 2D plane, say ***x*** = (*x*, *y*)
    = (0.5, −0.4) with *f*(*x*, *y*) = *x*² + *xy* + *y*² from above. Then, the function
    value at ***x*** is *x*² + *xy* + *y*² = (0.5)² + (0.5)(−0.4) + (−0.4)² = 0.21,
    a scalar. However, the gradient at this point is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/188equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, we now know that at the point (0.5, −0.4), the direction of the largest
    change in *f* is in the direction (0.6, −0.3) and has a magnitude of ![Image](Images/188equ02.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Gradient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s make all of this less abstract. The top part of [Figure 7-4](ch07.xhtml#ch07fig04)
    shows a plot of *f*(*x*, *y*) = *x*² + *xy* + *y*² at selected points.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/07fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7-4: A plot of x² + xy + y² (top) and a 2D projection of the associated
    gradient field (bottom)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to generate this plot is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: import numpy as np
  prefs: []
  type: TYPE_NORMAL
- en: x = np.linspace(-1.0,1.0,50)
  prefs: []
  type: TYPE_NORMAL
- en: y = np.linspace(-1.0,1.0,50)
  prefs: []
  type: TYPE_NORMAL
- en: xx = []; yy = []; zz = []
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range (50):'
  prefs: []
  type: TYPE_NORMAL
- en: 'for j in range (50):'
  prefs: []
  type: TYPE_NORMAL
- en: xx.append(x[i])
  prefs: []
  type: TYPE_NORMAL
- en: yy.append(y[j])
  prefs: []
  type: TYPE_NORMAL
- en: zz.append(x[i]*x[i]+x[i]*y[j]+y[j]*y[j])
  prefs: []
  type: TYPE_NORMAL
- en: x = np.array(xx)
  prefs: []
  type: TYPE_NORMAL
- en: y = np.array(yy)
  prefs: []
  type: TYPE_NORMAL
- en: z = np.array(zz)
  prefs: []
  type: TYPE_NORMAL
- en: Here, we explicitly loop to generate the set of scatter plot points, x, y, and
    z, to clearly show what’s happening. First, we use NumPy to generate vectors of
    50 evenly spaced points [−1, 1] in x and y. Then we set up a double loop so that
    each x gets paired with each y to calculate the function value, z. Temporary lists
    xx, yy, and zz hold the triplets. Finally, we convert the lists to NumPy arrays
    for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: The code to generate the scatter plot is
  prefs: []
  type: TYPE_NORMAL
- en: from mpl_toolkits.mplot3d import Axes3D
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pylab as plt
  prefs: []
  type: TYPE_NORMAL
- en: fig = plt.figure()
  prefs: []
  type: TYPE_NORMAL
- en: ax = fig.add_subplot(111, projection='3d')
  prefs: []
  type: TYPE_NORMAL
- en: ax.scatter(x, y, z, marker='.', s=2, color='b')
  prefs: []
  type: TYPE_NORMAL
- en: ax.view_init(30,20)
  prefs: []
  type: TYPE_NORMAL
- en: plt.draw()
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: We first load the matplotlib extension for 3D plotting, and then we set the
    subplot for a 3D projection. The plot itself is made with ax.scatter, while ax.view_init
    and plt.draw rotate the plot to give us a better view of the shape of the function
    before showing it.
  prefs: []
  type: TYPE_NORMAL
- en: In the bottom part of [Figure 7-4](ch07.xhtml#ch07fig04), we see a vector plot
    of the gradient field for *x*² + *xy* + *y*². This plot shows the direction and
    relative magnitude of the gradient vector at a grid of points, (*x*, *y*). Recall,
    the gradient is a vector field, so each point on the *xy*-plane has an associated
    vector pointing in the direction of the greatest change in the function value.
    Mentally, we can see how the vector plot relates to the function plot in the top
    part of [Figure 7-4](ch07.xhtml#ch07fig04), where function values near (−1, −1)
    and (1, 1) are changing quickly, whereas for points near (0, 0) they’re changing
    slowly.
  prefs: []
  type: TYPE_NORMAL
- en: The code to generate the vector field plot is
  prefs: []
  type: TYPE_NORMAL
- en: fig = plt.figure()
  prefs: []
  type: TYPE_NORMAL
- en: ax = fig.add_subplot(111)
  prefs: []
  type: TYPE_NORMAL
- en: x = np.linspace(-1.0,1.0,20)
  prefs: []
  type: TYPE_NORMAL
- en: y = np.linspace(-1.0,1.0,20)
  prefs: []
  type: TYPE_NORMAL
- en: xv, yv = np.meshgrid(x, y, indexing='ij', sparse=False)
  prefs: []
  type: TYPE_NORMAL
- en: dx = 2*xv + yv
  prefs: []
  type: TYPE_NORMAL
- en: dy = 2*yv + xv
  prefs: []
  type: TYPE_NORMAL
- en: ax.quiver(xv, yv, dx, dy, color='b')
  prefs: []
  type: TYPE_NORMAL
- en: plt.axis('equal')
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: We first define the figure (fig) and subplot for 2D (no projection keyword).
    Then, we need a grid of points. Above, we looped to get this grid so we could
    understand what needed to be generated. Here, we use NumPy to generate the grid
    for us via np.meshgrid. Note, we pass np.meshgrid the same x and y vectors we
    had above to define the domain.
  prefs: []
  type: TYPE_NORMAL
- en: The next two lines are a direct implementation of the gradient of *f*, [Equation
    7.8](ch07.xhtml#ch07equ08). These are the vectors we want to plot, with dx and
    dy giving us the direction and magnitude, while xv and yv are the set of input
    points—400 total.
  prefs: []
  type: TYPE_NORMAL
- en: The plot uses ax.quiver (since it plots arrows). The arguments are the grid
    of points (xv, yv) and associated *x* and *y* values of the vectors at those points
    (dx, dy). Finally, we ensure the axes are equal (plt.axis) to avoid warping the
    vector display, then show the plot.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll conclude our introduction of gradients here. We’ll see them again throughout
    the remainder of the book, in the notation in [Chapter 8](ch08.xhtml#ch08) and
    the gradient descent discussions of [Chapter 11](ch11.xhtml#ch11).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter introduced the main concepts of differential calculus. We started
    with the notion of slope and learned the difference between secant and tangent
    lines for a function of a single variable. We then formally defined the derivative
    as the slope of a secant line as it approaches a single point. From there, we
    learned the basic rules of differentiation and saw how to apply them.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about the minima and maxima of a function and how to find these
    points using derivatives. We then introduced partial derivatives as a way to calculate
    derivatives for functions of more than one variable. Partial derivatives then
    led us to the gradient, which turns a scalar field into a vector field and tells
    us the direction in which the function is changing the most. We calculated an
    example gradient in 2D and saw how to generate plots showing the relationship
    between the function and the gradient. We learned the crucial fact that the gradient
    of a function points in the direction of the maximum change in the function value
    at a point.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue our exploration of the math behind deep learning and move into
    the world of matrix calculus.
  prefs: []
  type: TYPE_NORMAL
