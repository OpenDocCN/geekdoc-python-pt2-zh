- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Basics of Data
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Data* means different things to different people: a stock trader might think
    of data as real-time stock quotes, while a NASA engineer might associate data
    with signals coming from a Mars rover. When it comes to data processing and analysis,
    however, the same or similar approaches and techniques can be applied to a variety
    of datasets, regardless of their origin. All that matters is how the data is structured.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides a conceptual introduction to data processing and analysis.
    We’ll first look at the main categories of data you may have to deal with, then
    touch on common data sources. Next, we’ll consider the steps in a typical data
    processing pipeline (that is, the actual process of obtaining, preparing, and
    analyzing data). Finally, we’ll examine Python’s unique advantages as a data science
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: Categories of Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Programmers divide data into three main categories: unstructured, structured,
    and semistructured. In a data processing pipeline, the source data is typically
    unstructured; from this, you form structured or semistructured datasets for further
    processing. Some pipelines, however, use structured data from the start. For example,
    an application processing geographical locations might receive structured data
    directly from GPS sensors. The following sections explore the three main categories
    of data as well as time series data, a special type of data that can be structured
    or semistructured.'
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unstructured data is data with no predefined organizational system, or schema.
    This is the most widespread form of data, with common examples including images,
    videos, audio, and natural language text. To illustrate, consider the following
    financial statement from a pharmaceutical company:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This text is considered unstructured data because the information found in
    it isn’t organized with a predefined schema. Instead, the information is randomly
    scattered within the statement. You could rewrite this statement in any number
    of ways while still conveying the same information. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Despite its lack of structure, unstructured data may contain important information,
    which you can extract and convert to structured or semistructured data through
    appropriate transformation and analysis steps. For example, image recognition
    tools first convert the collection of pixels within an image into a dataset of
    a predefined format and then analyze this data to identify content in the image.
    Similarly, the following section will show a few ways in which the data extracted
    from our financial statement could be structured.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structured data has a predefined format that specifies how the data is organized.
    Such data is usually stored in a repository like a relational database or just
    a *.**csv* (comma-separated values) file. The data fed into such a repository
    is called a *record*, and the information in it is organized in *fields* that
    must arrive in a sequence matching the expected structure. Within a database,
    records of the same structure are logically grouped in a container called a *table*.
    A database may contain various tables, with each table having a set structure
    of fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two basic types of structured data: numerical and categorical. *Categorical
    data* is that which can be categorized on the basis of similar characteristics;
    cars, for example, might be categorized by make and model. *Numerical data*, on
    the other hand, expresses information in numerical form, allowing you to perform
    mathematical operations on it.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that categorical data can sometimes take on numerical values. For
    example, consider ZIP codes or phone numbers. Although they are expressed with
    numbers, it wouldn’t make any sense to perform math operations on them, such as
    finding the median ZIP code or average phone number.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we organize the text sample introduced in the previous section into
    structured data? We’re interested in specific information in this text, such as
    company names, dates, and stock prices. We want to present that information in
    fields in the following format, ready for insertion into a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using techniques of *natural language processing* *(**NLP**)*, a discipline
    that trains machines to understand human-readable text, we can extract information
    appropriate for these fields. For example, we look for a company name by recognizing
    a categorical data variable that can only be one of many preset values, such as
    Google, Apple, or GoodComp. Likewise, we can recognize a date by matching its
    explicit ordering to one of a set of explicit ordering formats, such as `yyyy-mm-dd`.
    In our example, we recognize, extract, and present our data in the predefined
    format like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To store this record in a database, it’s better to present it as a row-like
    sequence of fields. We therefore might reorganize the record as a rectangular
    data object, or a 2D matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The information you choose to extract from the same unstructured data source
    depends on your requirements. Our example statement not only contains the change
    in GoodComp’s stock value for a certain date but also indicates the reason for
    that change, in the phrase “the company announced positive early-stage trial results
    for its vaccine.” Taking the statement from this angle, you might create a record
    with these fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare this to the first record we extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that these two records contain different fields and therefore have different
    structures. As a result, they must be stored in two different database tables.
  prefs: []
  type: TYPE_NORMAL
- en: Semistructured Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In cases where the structural identity of the information doesn’t conform to
    stringent formatting requirements, we may need to process semistructured data
    formats, which let us have records of different structures within the same container
    (database table or document). Like unstructured data, semistructured data isn’t
    tied to a predefined organizational schema; unlike unstructured data, however,
    samples of semistructured data do exhibit some degree of structure, usually in
    the form of self-describing tags or other markers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common semistructured data formats include XML and JSON. This is what
    our financial statement might look like in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here you can recognize the key information that we previously extracted from
    the statement. Each piece of information is paired with a descriptive tag, such
    as `"Company"` or `"Date"`. Thanks to the tags, the information is organized similarly
    to how it appeared in the previous section, but now we have a fourth tag, `"Details"`,
    paired with an entire fragment of the original statement, which looks unstructured.
    This example shows how semistructured data formats can accommodate both structured
    and unstructured pieces of data within a single record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, you can put multiple records of unequal structure into the same container.
    Here, we store the two different records derived from our example financial statement
    in the same JSON document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Recall from the discussion in the previous section that a relational database,
    being a rigidly structured data repository, cannot accommodate records of varying
    structures in the same table.
  prefs: []
  type: TYPE_NORMAL
- en: Time Series Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A time series is a set of data points indexed or listed in time order. Many
    financial datasets are stored as a time series due to the fact that financial
    data typically consists of observations at a specific time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series data can be either structured or semistructured. Imagine you’re
    receiving location data in records from a taxi’s GPS tracking device at regular
    time intervals. The data might arrive in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A new data record arrives every minute that includes the latest location coordinates
    (latitude/longitude) from `cab_238`. Each record has the same sequence of fields,
    and each field has a consistent structure from one record to the next, allowing
    you to store this time series data in a relational database table as regular structured
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now suppose the data comes at unequal intervals, which is often the case in
    practice, and that you receive more than one set of coordinates in one minute.
    The incoming structure might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that the first `coord` field includes two sets of coordinates and is thus
    not consistent with the second `coord` field. This data is semistructured.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you know what the main categories of data are, what are the sources
    from which you might receive such data? Generally speaking, data may come from
    many different sources, including texts, videos, images, and device sensors, among
    others. From the standpoint of Python scripts that you’ll write, however, the
    most common data sources are:'
  prefs: []
  type: TYPE_NORMAL
- en: An application programming interface (API)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list isn’t intended to be comprehensive or restrictive; there are many
    other sources of data. In Chapter 9, for example, you’ll see how to use a smartphone
    as a GPS data provider for your data processing pipeline, specifically by using
    a bot application as a go-between connecting the smartphone and your Python script.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, all of the options listed here require you to use a corresponding
    Python library. For example, before you can obtain data from an API, you’ll need
    to install a Python wrapper for the API or use the Requests Python library to
    make HTTP requests to the API directly. Likewise, in order to access data from
    a database, you’ll need to install a connector from within your Python code that
    enables you to access databases of that particular type.
  prefs: []
  type: TYPE_NORMAL
- en: While many of these libraries must be downloaded and installed, some libraries
    used to load data are distributed with Python by default. For example, to load
    data from a JSON file, you can take advantage of Python’s built-in `json` package.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapters 4 and 5, we’ll take up the data sourcing discussion in greater detail.
    In particular, you’ll learn how to load specific data from different sources into
    data structures in your Python script for further processing. For now, we’ll take
    a brief look at each of the common source types mentioned in the preceding list.
  prefs: []
  type: TYPE_NORMAL
- en: APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perhaps the most common way of acquiring data today is via an API (a software
    intermediary that enables two applications to interact with each other). As mentioned,
    to take advantage of an API in Python, you may need to install a wrapper for that
    API in the form of a Python library. The most common way to do this nowadays is
    via the `pip` command.
  prefs: []
  type: TYPE_NORMAL
- en: Not all APIs have their own Python wrapper, but this doesn’t necessarily mean
    you can’t make calls to them from Python. If an API serves HTTP requests, you
    can interact with that API from Python using the Requests library. This opens
    you up to thousands of APIs that you can use in your Python code to request datasets
    for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'When choosing an API for a particular task, you should take the following into
    account:'
  prefs: []
  type: TYPE_NORMAL
- en: Functionality Many APIs provide similar functionalities, so you need to understand
    your precise requirements. For example, many APIs let you conduct a web search
    from within your Python script, but only some allow you to narrow down your search
    results by date of publication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cost Many APIs allow you to use a so-called *developer key*, which is usually
    provided for free but with certain limitations, such as a limited number of calls
    per day.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stability Thanks to the Python Package Index (PyPI) repository ([https://pypi.org](https://pypi.org)),
    anyone can pack an API into a `pip` package and make it publicly available. As
    a result, there’s an API (or several) for virtually any task you can imagine,
    but not all of these are completely reliable. Fortunately, the PyPI repository
    tracks the performance and usage of packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Documentation Popular APIs usually have a corresponding documentation website,
    allowing you to see all of the API commands with sample usages. As a good model,
    look at the documentation page for the Nasdaq Data Link (aka Quandl) API ([https://docs.data.nasdaq.com/docs/python-time-series](https://docs.data.nasdaq.com/docs/python-time-series)),
    where you’ll find examples of making different time series calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Many APIs return results in one of the following three formats: JSON, XML,
    or CSV. Data in any of these formats can easily be translated into data structures
    that are either built into or commonly used with Python. For example, the Yahoo
    Finance API retrieves and analyzes stock data, then returns the information already
    translated into a pandas DataFrame, a widely used structure we’ll discuss in Chapter
    3.'
  prefs: []
  type: TYPE_NORMAL
- en: Web Pages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Web pages can be static or generated on the fly in response to a user’s interaction,
    in which case they may contain information from many different sources. In either
    case, a program can read a web page and extract parts of it. Called *web scraping*,
    this is quite legal as long as the page is publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical scraping scenario in Python involves two libraries: Requests and
    BeautifulSoup. Requests fetches the source code of the page, and then BeautifulSoup
    creates a *parse tree* for the page, which is a hierarchical representation of
    the page’s content. You can search the parse tree and extract data from it using
    Pythonic idioms. For example, the following fragment of a parse tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'can be easily transformed into the following list of items within a `for` loop
    in your Python script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is an example of transforming semistructured data into structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another common source of data is a relational database, a structure that provides
    a mechanism to efficiently store, access, and manipulate your structured data.
    You fetch from or send a portion of data to tables in the database using a Structured
    Query Language (SQL) request. For instance, the following request issued to an
    `employees` table in the database retrieves the list of only those programmers
    who work in the IT department, making it unnecessary to fetch the entire table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Python has a built-in database engine, SQLite. Alternatively, you can employ
    any other available database. Before you can access a database, you’ll need to
    install the database client software in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the conventional rigidly structured databases, there’s been
    an ever-increasing need in recent years for the ability to store heterogeneous
    and unstructured data in database-like containers. This has led to the rise of
    so-called *NoSQL* (*non-SQL* or *not only SQL*) databases. NoSQL databases use
    flexible data models, allowing you to store large volumes of unstructured data
    using the *key-value* method, where each piece of data can be accessed using an
    associated key. Here’s what our earlier sample financial statement might look
    like if stored in a NoSQL database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The entire statement is paired with an identifying key, `26`. It might seem
    odd to store the entire statement in a database. Recall, however, that several
    possible records can be extracted from a single statement. Storing the whole statement
    gives us the flexibility to extract different pieces of information at a later
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Files may contain structured, semistructured, and unstructured data. Python’s
    built-in `open()` function allows you to open a file so you can use its data within
    your script. However, depending on the format of the data (for example, CSV, JSON,
    or XML), you may need to import a corresponding library to be able to perform
    read, write, and/or append operations on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plaintext files don’t require a library to be further processed and are simply
    considered as sequences of lines in Python. As an example, look at the following
    message that a Cisco router might send to a logfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You’ll be able to read this line by line, looking for the required information.
    Thus, if your task is to find messages that include information about CPU utilization
    and extract particular figures from it, your script should recognize the last
    line in the snippet as a message to be selected. In Chapter 2, you’ll see an example
    of how to extract specific information from text data using text processing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The Data Processing Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll take a conceptual look at the steps involved in data
    processing, also known as the data processing pipeline. The usual steps applied
    to the data are:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquisition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cleansing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you’ll see, these steps aren’t always clear-cut. In some applications you’ll
    be able to combine multiple steps into one or omit some steps altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Acquisition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you can do anything with data, you need to acquire it. That’s why data
    acquisition is the first step in any data processing pipeline. In the previous
    section, you learned about the most common types of data sources. Some of those
    sources allow you to load only the required portion of the data in accordance
    with your request.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a request to the Yahoo Finance API requires you to specify the
    ticker of a company and a period of time over which to retrieve stock prices for
    that company. Similarly, the News API, which allows you to retrieve news articles,
    can process a number of parameters to narrow down the list of articles being requested,
    including the source and date of publication. Despite these qualifying parameters,
    however, the retrieved list may still need to be filtered further. That is, the
    data may require cleansing.
  prefs: []
  type: TYPE_NORMAL
- en: Cleansing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data cleansing is the process of detecting and correcting corrupt or inaccurate
    data, or removing unnecessary data. In some cases, this step isn’t required, and
    the data being obtained is immediately ready for analysis. For example, the yfinance
    library (a Python wrapper for Yahoo Finance API) returns stock data as a readily
    usable pandas DataFrame object. This usually allows you to skip the cleansing
    and transformation steps and move straight to data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if your acquisition tool is a web scraper, the data certainly will
    need cleansing because fragments of HTML markup will probably be included along
    with the payload data, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After cleansing, this text fragment should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the HTML markup, the scraped text may include other unwanted text,
    as in the following example, where the phrase *A View full text* is simply hyperlink
    text. You might need to open this link to access the text within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You can also use a data cleansing step to filter out specific entities. After
    requesting a set of articles from the News API, for example, you may need to select
    only those articles in the specified period where the titles include a money or
    percent phrase. This filter can be considered a data cleansing operation because
    the goal is to remove unnecessary data and prepare for the data transformation
    and data analysis operations.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data transformation is the process of changing the format or structure of data
    in preparation for analysis. For example, to extract the information from our
    GoodComp unstructured text data as we did in “Structured Data,” you might shred
    it into individual words or *tokens* so that a named entity recognition (NER)
    tool can look for the desired information. In information extraction, a *named
    entity* typically represents a real-world object, such as a person, an organization,
    or a product, that can be identified by a proper noun. There are also named entities
    that represent dates, percentages, financial terms, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many NLP tools can handle this kind of transformation for you automatically.
    After such a transformation, the shredded GoodComp data would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Other forms of data transformation are deeper, with text data being converted
    into numerical data. For example, if we’ve gathered a collection of news articles,
    we might transform them by performing *sentiment analysis*, a text processing
    technique that generates a number representing the emotions expressed within a
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis can be implemented with tools like SentimentAnalyzer, which
    can be found in the `nltk.sentiment` package. A typical analysis output might
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Each entry in our dataset now includes a number, such as `0.9313`, representing
    the sentiment expressed within the corresponding article. With the sentiment of
    each article expressed numerically, we can calculate the average sentiment of
    the entire dataset, allowing us to determine the overall sentiment toward an object
    of interest, such as a certain company or product.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Analysis is the key step in the data processing pipeline. Here you interpret
    the raw data, enabling you to draw conclusions that aren’t immediately apparent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with our sentiment analysis example, you might want to study the
    sentiment toward a company over a specified period in relation to that company’s
    stock price. Or you might compare stock market index figures, such as those on
    the S&P 500, with the sentiment expressed in a broad sampling of news articles
    for this same period. The following fragment illustrates what the dataset might
    look like, with S&P 500 data shown alongside the overall sentiment of that day’s
    news:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Since both the sentiment figures and stock figures are expressed in numbers,
    you might plot two corresponding graphs on the same plot for visual analysis,
    as illustrated in [Figure 1-1](#figure1-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![A line graph uses two different lines to plot different data points on the
    y-axis across the same sequence of days on the x-axis.](image_fi/502208c01/f01001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-1: An example of visual data analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Visual analysis is one of the most commonly used and efficient methods for interpreting
    data. We’ll discuss visual analysis in greater detail in Chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most cases, you’ll need to store the results generated during the data analysis
    process to make them available for later use. Your storage options typically include
    files and databases. The latter is preferable if you anticipate frequent reuse
    of your data.
  prefs: []
  type: TYPE_NORMAL
- en: The Pythonic Way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When doing data science with Python, your code is expected to be written in
    a *Pythonic* way, meaning it should be concise and efficient. Pythonic code is
    often associated with the use of *list comprehensions*, which are ways to implement
    useful data processing functionality with a single line of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover list comprehensions in more detail in Chapter 2, but for now, the
    following quick example illustrates how the Pythonic concept works in practice.
    Say you need to process this multisentence fragment of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Specifically, you need to split the text by sentences, creating a list of individual
    words for each sentence, excluding punctuation symbols. Thanks to Python’s list
    comprehension feature, all of this can be implemented in a single line of code,
    a so-called *one-liner*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `for line in txt` loop ❷ splits the text into sentences and stores those
    sentences in a list. Then the `for w in line` loop ❶ splits each sentence into
    individuals words and stores the words in a list within the larger list. As a
    result, you get the following list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here you’ve managed to accomplish two steps of the data processing pipeline
    within a single line of code: cleansing and transformation. You’ve cleansed the
    data by removing punctuation symbols from the text, and you’ve transformed it
    by separating the words from each other to form a word list for each sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve come to Python from another programming language, try implementing
    this task with that language. How many lines of code does it take?
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After reading this chapter, you should have a cursory understanding of the main
    categories of data, where data comes from, and how a typical data processing pipeline
    is organized.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you’ve seen, there are three major categories of data: unstructured, structured,
    and semistructured. The raw input material in a data processing pipeline is typically
    unstructured data, which is passed through cleansing and transformation steps
    to turn it into structured or semistructured data that is ready for analysis.
    You also learned about data processing pipelines that use structured or semistructured
    data from the start, acquired from an API or a relational database.'
  prefs: []
  type: TYPE_NORMAL
