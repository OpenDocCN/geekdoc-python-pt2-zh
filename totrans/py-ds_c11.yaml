- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gaining Insights from Data
  prefs: []
  type: TYPE_NORMAL
- en: '![](image_fi/book_art/chapterart.png)'
  prefs: []
  type: TYPE_IMG
- en: Companies generate vast amounts of data every day in the form of raw facts,
    figures, and events, but what does all that data really tell you? To extract knowledge
    and gain insight from the data, you need to transform, analyze, and visualize
    it. In other words, you need to turn the raw data into meaningful information
    that you can use to make decisions, answer questions, and solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the case of a supermarket that collects large volumes of customer transaction
    data. Analysts at the supermarket may be interested in studying this data to gain
    insight into customers’ buying preferences. In particular, they may want to perform
    a *market basket analysis*, a data mining technique that analyzes transactions
    and identifies items that are commonly purchased together. Armed with this knowledge,
    the supermarket could make more informed business decisions—for instance, about
    the layout of items in the store, or about how to bundle items together into discounts.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore this example in detail, examining how to gain
    insights from transaction data by performing a market basket analysis with Python.
    You’ll learn how to use the mlxtend library and the Apriori algorithm to identify
    items that are commonly purchased together, and you’ll see how that knowledge
    can be leveraged to make smart business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Although identifying buyer preferences will be the focus of this chapter, this
    isn’t the only application for a market basket analysis. The technique is also
    used in domains such as telecommunications, web usage mining, banking, and health
    care. In web usage mining, for example, a market basket analysis can determine
    where the user of a web page will likely go next and generate associations of
    pages frequently visited together.
  prefs: []
  type: TYPE_NORMAL
- en: Association Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A market basket analysis is about measuring the strength of the relationships
    between objects based on their co-occurrence in the same transactions. The relationships
    between objects are represented as *association rules*, which are denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`X` and `Y`, referred to as the *antecedent* and *consequent* of the rule,
    respectively, represent distinct *itemsets*, or groups of one or more items from
    the transaction data being mined. For example, an association rule that describes
    a relationship between the items *curd* and *sour cream* would be denoted like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `curd` is the antecedent and `sour cream` is the consequent. The
    rule is asserting that people who buy curd are also likely to buy sour cream.
  prefs: []
  type: TYPE_NORMAL
- en: By itself, an association rule such as this doesn’t actually tell you very much.
    The key to a successful market basket analysis is to use the transaction data
    to evaluate the strength of association rules based on various metrics. To demonstrate,
    we’ll take a simple example. Suppose we have 100 customer transactions, 25 of
    which contain curd and 30 of which contain sour cream. Of those 30 transactions
    that contain sour cream, 20 transactions also contain curd. [Table 11-1](#table11-1)
    summarizes these figures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11-1: Transaction Figures for Curd and Sour Cream'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Curd** | **Sour cream** | **Curd and sour cream** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Transactions** | 25 | 30 | 20 | 100 |'
  prefs: []
  type: TYPE_TB
- en: Given this transaction data, we can evaluate the strength of the association
    rule `curd -> sour cream` using metrics such as support, confidence, and lift.
    These metrics will help us gauge whether or not there truly is an association
    between curd and sour cream.
  prefs: []
  type: TYPE_NORMAL
- en: Support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Support* is the ratio of transactions that include one or more items to the
    total number of transactions. For example, the support for curd in the sample
    transaction data can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the context of an association rule, support is the ratio of transactions
    that include both the antecedent and consequent to the total number of transactions.
    The support of the `curd -> sour cream` association rule is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The support metric falls in the range of 0 to 1, and it tells you what percent
    of the time an itemset appears in a transaction. In this case, we can see that
    20 percent of transactions included both curd and sour cream. Support is symmetric
    for any given association rule; that is, the support for `curd -> sour cream`
    is the same as the support for `sour cream -> curd`.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The *confidence* of an association rule is the ratio of transactions where
    both the antecedent and consequent are bought to transactions where the antecedent
    is bought. In other words, confidence measures what share of transactions containing
    the antecedent also contain the consequent. The confidence for the `curd -> sour
    cream` association rule can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can interpret this as meaning that if a customer purchased curd, there’s
    an 80 percent likelihood that they also purchased sour cream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like support, confidence falls within a range of 0 to 1, but unlike support,
    confidence isn’t symmetric. That means the confidence for the rule `curd -> sour
    cream` may be different from the confidence for the rule `sour cream -> curd`,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, you get a lower confidence value when the antecedent and consequent
    of the association rule are reversed. This tells you it’s less likely that someone
    buying sour cream will also buy curd than it is that someone buying curd will
    also buy sour cream.
  prefs: []
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Lift* assesses the strength of an association rule compared to the random
    co-occurrence of the items that appear in the rule. The lift of the association
    rule `curd -> sour cream` is the ratio of the observed support for `curd -> sour
    cream` to that expected if curd and sour cream were independent of each other.
    This can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Lift values are symmetric—if you swap the antecedent and consequent, the lift
    remains the same. The possible values for lift range from 0 to infinity, and the
    larger the lift ratio, the stronger the association. In particular, a lift ratio
    larger than 1 indicates that the relationship between the antecedent and consequent
    is stronger than would be expected if they were independent, meaning the two items
    are often bought together. A lift ratio equal to 1 indicates no correlation between
    the antecedent and consequent. A lift ratio less than 1 tells you that there’s
    a negative correlation between the antecedent and consequent, meaning they’re
    unlikely to be purchased together. In this case, you can interpret the lift ratio
    of 2.66 as meaning that when a customer buys curd, there’s a 166 percent increase
    in expectation that they will also purchase sour cream.
  prefs: []
  type: TYPE_NORMAL
- en: The Apriori Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You’ve learned what association rules are and seen some metrics for evaluating
    their strength, but how do you actually generate association rules for a market
    basket analysis? One way is to use the *Apriori algorithm*, an automated process
    for analyzing transaction data. In general terms, the algorithm consists of two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify all the *frequent itemsets*, or groups of one or more items that appear
    in many transactions, in the dataset. The algorithm does this by finding all the
    items or groups of items whose support value exceeds a certain threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate association rules for these frequent itemsets by considering all possible
    binary partitions of each itemset (that is, all divisions of the itemset into
    an antecedent group and a consequent group) and calculating a set of association
    metrics for each partition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the association rules have been generated, you can evaluate them based
    on the metrics discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several third-party Python libraries come with implementations of the Apriori
    algorithm. One is the mlxtend library. Short for *machine learning extensions*,
    mlxtend includes tools for carrying out a number of common data science tasks.
    In this section, we’ll walk through an example market basket analysis using mlxtend’s
    Apriori algorithm implementation. But first, install the mlxtend library with
    `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Creating a Transaction Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To conduct your market basket analysis, you need some sample transaction data.
    For simplicity, you can use just a few transactions, defined as a list of lists,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Each inner list contains the itemset for a single transaction, with the whole
    `transactions` list containing 20 transactions in total. To maintain the quantitative
    proportions defined in the original curd/sour cream example, the dataset contains
    five transactions with curd, six transactions with sour cream, and four that contain
    both curd and sour cream.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the transaction data through mlxtend’s Apriori algorithm, you need to
    transform it into a *one-hot encoded Boolean array*, a structure where each column
    represents an item that can be purchased, each row represents a transaction, and
    each value is either `True` or `False` (`True` if the transaction included that
    particular item or `False` if not). Here you perform the necessary transformation
    using mlxtend’s `TransactionEncoder` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You create a `TransactionEncoder` object ❶ and use it to transform the `transactions`
    list of lists into a one-hot encoded Boolean array called `encoded_array` ❷. Then
    you convert the array into a pandas DataFrame called `df_itemsets` ❸, a fragment
    of which is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame consists of 20 rows and 20 columns, where the rows represent
    the transactions and the columns represent the items. To confirm that the original
    list of lists included 20 transactions drawing on 20 possible items, use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, you should get `20`.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Frequent Itemsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the transaction data is in a usable format, you can use mlxtend’s
    `apriori()` function to identify all the frequent itemsets in the transaction
    data—that is, all the items or groups of items with a high enough support metric.
    Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You import the `apriori()` function from the `mlxtend.frequent_patterns` module.
    Then you call the function, passing the DataFrame containing the transaction data
    as the first parameter. You also set the `min_support` parameter to `0.1` to return
    the itemsets with at least 10 percent support. (Remember, the support metric indicates
    what percentage of the transactions an item or group of items occurs in.) You
    set `use_colnames` to `True` to identify the columns included in each itemset
    by name (such as `curd` or `sour cream`) rather than by index number. As a result,
    `apriori()` returns the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As noted earlier, an itemset can consist of one or more items, and indeed,
    `apriori()` has returned several single-item itemsets. Ultimately, mlxtend will
    omit these single-item itemsets when it formulates association rules, but it will
    nevertheless need data on *all* the frequent itemsets (including those with one
    item) to successfully generate the rules. Still, as a matter of interest, you
    may wish at this point to view only those itemsets with multiple items. To do
    so, first add a `length` column to the `frequent_itemsets` DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use pandas’s select syntax to filter the DataFrame to just those rows
    with a `length` field of `2` or more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see the following result, without any of the single-item itemsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To reiterate, however, mlxtend requires information on all the frequent itemsets
    when generating association rules. Therefore, make sure you don’t actually remove
    any rows from the original `frequent_itemsets` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Association Rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ve identified all the itemsets that meet the desired support threshold.
    The second step of the Apriori algorithm is to generate association rules for
    those itemsets. For this, you use the `association_rules()` function from mlxtend’s
    `frequent_patterns` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here you invoke the `association_rules()` function, passing in the `frequent_itemsets`
    DataFrame as the first parameter. You also choose a metric for evaluating the
    rules and set a threshold value for that metric. Specifically, you say that the
    function should only return those association rules with a confidence metric of
    0.5 or more. As noted in the previous section, the function will automatically
    skip generating rules for single-member itemsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `association_rules()` function returns the rules in the form of a DataFrame,
    where each row represents a single association rule. The DataFrame has columns
    for the antecedents, consequents, and various metrics, including support, confidence,
    and lift. Here, you print a selection of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Looking over these rules, some may seem redundant. For example, there’s both
    a `bread -> butter` rule and a `butter -> bread` rule. Likewise, there are several
    rules based on the `(bread, cheese, butter)` itemset. In part, this is because,
    as noted earlier in the chapter, confidence isn’t symmetric; if you swap the antecedent
    and consequent in a rule, the confidence value can change. Additionally, for a
    three-member itemset, the lift can change depending on which items are part of
    the antecedent and which are part of the consequent. Thus, `(bread, cheese) ->
    butter` has a different lift than `(bread, butter) -> cheese`.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Association Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you learned in Chapter 8, visualization is a simple yet powerful technique
    for analyzing data. In the context of a market basket analysis, visualization
    provides a convenient way to evaluate the strength of a set of association rules
    by viewing the metrics for different antecedent/consequent pairs. In this section,
    you’ll use Matplotlib to visualize the association rules you generated in the
    previous section as an annotated heatmap.
  prefs: []
  type: TYPE_NORMAL
- en: A *heatmap* is a grid-like plot where cells are color-coded to indicate their
    value. In this example, you’ll create a heatmap showing the lift metric of the
    various association rules. You’ll arrange all the antecedents along the y-axis
    and the consequents along the x-axis, and fill in the area where a rule’s antecedent
    and consequent intersect with a color to indicate that rule’s lift value. The
    darker the color, the higher the lift.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the visualization, you first make an empty DataFrame, into which
    you copy the `antecedents`, `consequents`, and `lift` columns of the `rules` DataFrame
    created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You use lambda functions to convert the values of the `antecedents` and `consequents`
    columns from the `rules` DataFrame into strings, which will make it easier to
    use them as labels in the visualization. Originally the values were frozensets,
    immutable versions of Python sets. You use another lambda function to round the
    lift values to two decimal places.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to transform the newly created `rules_plot` DataFrame into a
    matrix that will be used for creating the heatmap, with the consequents arranged
    horizontally and the antecedents arranged vertically. For that, you can reshape
    `rules_plot` so that the unique values in the `antecedents` column form the index
    and the unique values in the `consequents` column become the new columns, while
    the values of the `lift` column are used for populating the reshaped DataFrame’s
    values. You use the `rules_plot` DataFrame’s `pivot()` method for this, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You specify the `antecedents` and `consequents` columns to form axes of the
    resulting `pivot` DataFrame and draw on the `lift` column for the values. If you
    print `pivot`, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This DataFrame contains everything you need to build your heatmap: the values
    of the index (the antecedents) will become the y-axis labels, the names of the
    columns (the consequents) will become the x-axis labels, and the grid of numbers
    and `NaN`s will become the values for the plot. (In this context, a `NaN` indicates
    no association rule was generated for that antecedent/consequent pair.) Here,
    you extract these components into separate variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you have the y-axis labels in the `antecedents` list, the x-axis labels
    in the `consequents` list, and the values for the plot in the `pivot` NumPy array.
    You use all these components in the following script to build the heatmap with
    Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The key points of plotting with Matplotlib were covered in Chapter 8. Here,
    we’ll only consider the lines specific to this particular example. The `imshow()`
    method converts the data from the `pivot` array into a color-coded 2D image ❶.
    With the method’s `cmap` parameter, you specify how to map the numeric values
    from the array to colors. Matplotlib has a number of built-in color mappings you
    can choose from, including the `Reds` mapping used here.
  prefs: []
  type: TYPE_NORMAL
- en: After creating the axis labels, you use the `setp()` method to rotate the x-axis
    labels by 45 degrees ❷. This helps fit the labels within the horizontal space
    allotted. Then, you loop over the data in the `pivot` array ❸ and create text
    annotations for each square in the heatmap using the `text()` method ❺. The first
    two parameters, `j` and `i`, are the x- and y-coordinates for the label. The next
    parameter, `pivot[i, j]`, is the text of the label, and the remaining parameters
    set the label’s justification. Before calling the `text()` method, you use an
    `if` statement to filter out the antecedent/consequent pairs without any lift
    data ❹. Otherwise, a `NaN` label would appear in each empty square of the heatmap.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-1](#figure11-1) shows the resulting visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A chart with products arranged along the x- and y-axes. At the intersections
    of some products, a shaded square contains those products’ lift metric. The higher
    the number, the darker the shading.](image_fi/502208c11/f11001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11-1: A heatmap of the lift metric for the sample association rules'
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap helps you immediately see which association rules have the highest
    lift values based on the darkness of the shading. Looking at this visualization,
    you can state with a high degree of certainty that a customer who buys milk is
    also likely to buy eggs. Similarly, you can be pretty sure that a customer who
    buys pasta will also buy cheese. Other associations, such as butter with cheese,
    also exist, but as you can see, they aren’t backed as strongly by the lift metric.
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap also illustrates how the lift metric is symmetric. Look, for example,
    at the values of the `bread -> butter` and `butter -> bread` rules. They’re the
    same. You may notice, however, that some antecedent/consequent pairs in the plot
    don’t have a symmetric lift value. For example, the lift for the `cheese -> bread`
    rule is shown as `1.5`, but there isn’t a lift value for `bread ->` `cheese` on
    the plot. This is because when you originally generated the association rules
    with mlxtend’s `association_rules()` function, you set a 50 percent confidence
    threshold. This excluded many potential association rules, including `bread ->
    cheese`, which has a 37.5 percent confidence rating compared to the 60 percent
    confidence rating of `cheese -> bread`. Thus, no data for a `bread -> cheese`
    rule was available to plot.
  prefs: []
  type: TYPE_NORMAL
- en: Gaining Actionable Insights from Association Rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the Apriori algorithm, you identified the frequent itemsets in a sample
    batch of transaction data, and you generated association rules based on those
    itemsets. These rules tell you, in essence, how likely it is that a customer will
    buy one product if they’ve bought another, and by visualizing the rules’ lift
    metrics on a heatmap, you saw which rules were particularly convincing. The next
    logical question to consider is how a business could actually benefit from this
    information.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll look at two different ways a business could derive useful
    insights from a collection of association rules. We’ll examine how to generate
    product recommendations based on the items a customer has purchased and how to
    efficiently plan discounts based around frequent itemsets. Both of these applications
    have the potential to increase revenue for the business while also delivering
    a better experience for customers.
  prefs: []
  type: TYPE_NORMAL
- en: Generating Recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After an item has appeared in a customer’s basket, which item is likely to be
    added next? Of course, you can’t say for sure, but you can make a prediction based
    on the association rules mined from your transaction data. The results of this
    prediction can form the basis for a set of recommendations of items that are frequently
    bought together with the item currently in the basket. Retailers commonly use
    such recommendations to show customers other items they may want to purchase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most natural way to generate recommendations of this type is to
    look at all the association rules where the item currently in the basket acts
    as the antecedent. Then, you identify the strongest rules—perhaps the three rules
    with the highest confidence values—and extract their consequents. The following
    example illustrates how to do this for the item *butter*. You start by finding
    the rules where *butter* is the antecedent, using the pandas library’s filtering
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here you sort the rules by the `confidence` column so that the rules with the
    highest confidence rating appear at the beginning of the `butter_antecedent` DataFrame.
    Next you use a list comprehension to extract the top three consequents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In this list comprehension, you loop over the `consequents` column in the `butter_antecedent`
    DataFrame, picking up the first three values. Based on the `butter_consequents`
    list, you can generate a recommendation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The recommendation will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that customers who buy butter also often buy either bread or
    cheese, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Planning Discounts Based on Association Rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The association rules generated for frequent itemsets can also be used for choosing
    which products to discount. Ideally, you should have a discounted item in each
    significant group of products to satisfy as many customers as possible. In other
    words, you should choose a single item to be discounted in each frequent itemset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this, the first thing you need is a set of frequent itemsets
    to work on. Unfortunately, the `rules` DataFrame generated earlier by the `association_rules()`
    function has columns for antecedents and consequents but not for the rules’ complete
    itemsets. You therefore need to create an `itemsets` column by merging the `antecedents`
    and `consequents` columns, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You use the `reduce()` function from Python’s `functools` module to apply the
    `frozenset.union()` method to the values of the `antecedents` and `consequents`
    columns. This combines the separate frozensets from these columns into a single
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what you have as a result, you might print the newly created `itemsets`
    column along with the `antecedents` and `consequents` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that there are some duplicates in the new `itemsets` column. As discussed
    earlier, the same itemset may form more than one association rule, since the order
    of items influences some rule metrics. The order of items in an itemset doesn’t
    matter for the current task, however, so you can safely remove duplicate itemsets,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: You use the DataFrame’s `drop_duplicates()` method for this, specifying to look
    for duplicates in the `itemsets` column. You keep the first row within a set of
    duplicates, and by setting `inplace` to `True`, you delete the duplicate rows
    from the existing DataFrame rather than creating a new DataFrame with the duplicates
    removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now print out the `itemsets` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'you’ll only see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you choose one item from each itemset to be discounted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You first create the `discounted` list to accumulate the items being chosen
    for discount and the `others` list to receive the items in an itemset that aren’t
    chosen for discount. Then you iterate over each itemset ❶ and each item in it
    ❷. You look for an item not already included in the `others` list, since such
    an item would either not be in any of the preceding itemsets or have already been
    chosen as the discounted item for a preceding itemset, meaning it would be efficient
    to choose it as this itemset’s discounted item too ❸. You send the chosen item
    to the `discounted` list ❹, then send the remaining items of the itemset to the
    `others` list ❺. If you’ve iterated over all the items in an itemset and failed
    to find an item that isn’t included in the `others` list, you choose the last
    item in the itemset and send it to the `discounted` list ❻.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting `discounted` list will vary, since the Python frozensets representing
    the itemsets are unordered, but it will look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare the result with the `itemsets` column shown earlier, and you’ll see
    that each itemset has one discounted item in it. Moreover, you’ve distributed
    the discounts very efficiently, so the actual number of discounted items is significantly
    less than the number of itemsets. You can see this by removing the duplicates
    from the `discounted` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As the output shows, even though there are eight itemsets, you’ve only had
    to discount five items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Thus, you’ve managed to discount one item in each itemset (a significant benefit
    for many customers) without actually having to discount many items (a significant
    benefit for the business).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you’ve seen, performing a market basket analysis is a valuable way to extract
    useful information from large amounts of transaction data. In this chapter, you
    learned how to use the Apriori algorithm to mine transaction data for association
    rules, and you saw how to evaluate those rules along different metrics. In this
    way, you were able to gain insight about what items are commonly purchased together.
    You used that knowledge to make product recommendations to customers and to efficiently
    plan discounts.
  prefs: []
  type: TYPE_NORMAL
