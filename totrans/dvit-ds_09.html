<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_191" title="191"/>9</span><br/>
<span class="ChapterTitle">Recommendation Systems</span></h1>
</header>
<figure class="opener">
<img alt="" height="200" src="image_fi/book_art/chapterart.png" width="200"/>
</figure>
<p class="ChapterIntro">Every talented salesperson knows how to make intelligent, targeted recommendations to customers, and as online retailers have grown in size and sophistication, they have enthusiastically automated this sales tactic. But these recommendations are hard to make. For this reason, many businesses create automated <em>recommendation systems</em> that analyze data about products and customers to determine which customers would be most receptive to which products.</p>
<p>In this chapter, we’ll go over recommendation systems in detail. We’ll start with the simplest possible recommendation system: one that merely recommends the most popular items to every customer. We’ll go on to discuss an important technique called <em>collaborative filtering</em> that enables us to <span epub:type="pagebreak" id="Page_192" title="192"/>make unique, personalized recommendations for each customer and each product. We’ll go over two types of collaborative filtering: item based and user based. We’ll conclude with a case study and advanced ideas related to recommendation systems.</p>
<h2 id="h1-502888c09-0001">Popularity-Based Recommendations</h2>
<p class="BodyFirst">Before we write code for recommendation systems, we should consider how to make recommendations in general. Imagine that you’re a salesperson and you want to make recommendations to a customer who walks into your store. If you’re acquainted with the customer, you could make recommendations based on your knowledge of the customer’s tastes and situation. If a new customer walks into your store and you want to make recommendations without knowing anything about the person, you could observe what they’re browsing and make a recommendation based on that. But it’s possible that you’ll be asked to make a recommendation before they’ve browsed anything at all. The dilemma of needing to make intelligent recommendations without any specific knowledge about the customer is referred to as the <em>cold-start problem</em>.</p>
<p>One reasonable thing to do when faced with the cold-start problem is to recommend the most popular items. Doing this is simple and easy. It doesn’t have the sophistication of knowing everything about a customer and making a personalized recommendation, but if something is popular with the general public, it’s reasonable to think it could be appealing to your new customer.</p>
<p>Online retailers have an analogous challenge: new visitors visit their websites, and maybe those visitors don’t have browsing history or are unfamiliar to the online retailers. The retailers want to make personalized recommendations based on detailed knowledge about customers, but when they face the cold-start problem, they have to fall back on something else like general popularity. The cold-start problem is especially common for online retailers, since it’s easy for prospective customers to view a website anonymously without giving any personal information to the website or its sales team.</p>
<p>Let’s think about the code that we would use to make a popularity-based recommendation. For this, or any other recommendation system, having data related to transaction history is helpful. We can download, read, and look at some fabricated transaction history data as follows:</p>
<pre><code>import pandas as pd
import numpy as np
interaction=pd.read_csv('https://bradfordtuckfield.com/purchasehistory1.csv')
interaction.set_index("Unnamed: 0", inplace = True)
print(interaction)</code></pre>
<p>Here, we import the pandas package to do data wrangling. We read a <em>.csv</em> file from the internet into the <code>interaction</code> variable, and we store it as a pandas dataframe. We specify that the first column of the data should be the index (the row name) and print the dataframe. The output we see at the end is shown in <a href="#listing9-1" id="listinganchor9-1">Listing 9-1</a>.</p>
<pre><code><span epub:type="pagebreak" id="Page_193" title="193"/>  Unnamed: 0  user1  user2  user3  user4  user5
0      item1      1      1      0      1      1
1      item2      1      0      1      1      0
2      item3      1      1      0      1      1
3      item4      1      0      1      0      1
4      item5      1      1      0      0      1</code></pre>
<p class="CodeListingCaption"><a id="listing9-1">Listing 9-1</a>: An interaction matrix, showing the history of purchases for each item</p>
<p><a href="#listing9-1">Listing 9-1</a> shows a matrix representing the sales history of a retailer that has five customers and five items for sale. Note we’re calling the customers <em>users</em>, assuming that they’re users of the retailer’s website. But whatever we call them, the recommendation techniques we use will be the same.</p>
<p class="BodyFirst">The matrix contains a <code>0</code> if a user did not purchase a particular item and a <code>1</code> if a user did. For example, you can see that <code>user2</code> purchased <code>item3</code> but not <code>item2</code>, and <code>user3</code> purchased <code>item2</code> but not <code>item3</code>. This type of 0/1 matrix is a common format to encounter when we’re building recommendation systems. We can call this matrix the <em>interaction matrix</em>; it represents information about interactions between users and items. Since nearly every company has records related to its items and their purchase histories, building recommendation systems based on interaction matrices is an extremely common practice.</p>
<p>Suppose that a new customer, whom we’ll call <code>user6</code>, walks into your store (or visits your website). You face a cold start, since you know nothing about <code>user6</code>. If you want to make recommendations for items that <code>user6</code> can purchase, you could make a list of the most popular items, as follows:</p>
<pre><code>interaction_withcounts=interaction.copy()
interaction_withcounts.loc[:,'counts']=interaction_withcounts.sum(axis=1)
interaction_withcounts=interaction_withcounts.sort_values(by='counts',ascending=False)
print(list(interaction_withcounts.index))</code></pre>
<p>Here we create a copy of our interaction matrix called <code>interaction_withcounts</code>. We’ll use this copy to find the most popular items by counting the number of users who have ever purchased each item. Note that our matrix doesn’t record whether a user purchased an item multiple times or only once, so our analysis will look at only whether users have purchased items at all; we won’t analyze how many times each user purchased each item.</p>
<p>Since each row of our matrix records purchases of a unique item, we use the <code>sum()</code> method to take the sum of purchases in each row and store the result in a new column called <code>counts</code>. We then use the <code>sort_values()</code> method, which sorts the rows of our matrix from the highest to lowest purchase counts. By sorting from most to least purchased, it is ordering the items by popularity. Finally, we print out the index of our sorted matrix, which tells us the item names of all of our items, sorted from most to least popular:</p>
<pre><code>['item1', 'item3', 'item2', 'item4', 'item5']</code></pre>
<p><span epub:type="pagebreak" id="Page_194" title="194"/>We can interpret this to mean that <code>item1</code> is the most popular item (in fact, tied with <code>item3</code>), <code>item2</code> is the third-most popular, and so on.</p>
<p>Now that you have this list, you’re ready to make recommendations to unfamiliar customers. The way you present your recommendations will depend on your business strategy, your web development team’s capabilities, and your marketing team’s preferences. The data science portion of a recommendation system project is to create the list of prioritized recommendations and let marketers or web developers present these to users. This is one reason that recommendation system projects can be challenging: they require cooperation among several teams.</p>
<p>We can create a function that generates popularity-based recommendations for any interaction matrix by putting together all of our code so far:</p>
<pre><code>def popularity_based(interaction):
    interaction_withcounts=interaction.copy()
    interaction_withcounts.loc[:,'counts']=interaction_withcounts.sum(axis=1)
    sorted = interaction_withcounts.sort_values(by='counts',ascending=False)
    most_popular=list(sorted.index)
    return(most_popular)</code></pre>
<p>This function merely wraps up the capabilities we’ve written code for previously in the chapter. It takes an interaction matrix as its input. It sums up the counts of purchases for each item, sorts by number of purchases, and returns a list of item names that is sorted from most to least popular. This final sorted list can be used to make recommendations to customers, even if you’re unfamiliar with the customers. You can call this function on your interaction matrix by running <code>print(popularity_based(interaction))</code> in Python.</p>
<p>A popularity-based recommendation system is a simple, reasonable way to solve the cold-start problem and make some kind of recommendation to users. You can see popularity-based recommendations on many websites today, where <em>trending</em> content is highlighted. You can also see popularity-based recommendations in brick-and-mortar retailers, like bookstores that prominently display bestsellers.</p>
<p>But popularity-based recommendations are not as effective as personalized ones. Recommendation systems that can use detailed information about people and items can be more successful than generic popularity-based recommendation systems. Let’s take a look at one now.</p>
<h2 id="h1-502888c09-0002">Item-Based Collaborative Filtering</h2>
<p class="BodyFirst">Suppose that you don’t face a completely cold start. Instead, you have just a little information about a sixth customer: in particular, you know that they’re interested in <code>item1</code>. This information is all you need to make recommendations when using <em>collaborative filtering</em>.</p>
<p><span epub:type="pagebreak" id="Page_195" title="195"/>Let’s look at our interaction matrix again to get some ideas for how we should make recommendations to someone who’s interested in <code>item1</code>:</p>
<pre><code>  Unnamed: 0  user1  user2  user3  user4  user5
0      item1      1      1      0      1      1
1      item2      1      0      1      1      0
2      item3      1      1      0      1      1
3      item4      1      0      1      0      1
4      item5      1      1      0      0      1</code></pre>
<p>If we look at the first row of our interaction matrix, we can see the full history of customer interactions with <code>item1</code>. This item was purchased by <code>user1</code>, <code>user2</code>, <code>user4</code>, and <code>user5</code>, and it was not purchased by <code>user3</code>. If we look at <code>item3</code>, we can see that it has exactly the same purchase history as <code>item1</code>. They could be similar items, like two James Bond movies, or they could be complementary, like peanut butter and jelly. Regardless, if two items were purchased together in the past, they’re likely to be purchased together in the future.</p>
<p>By contrast, look at the purchase histories of <code>item1</code> and <code>item2</code>; they have less overlap in customers. These items do not have highly similar purchase histories. Since they haven’t been purchased together often in the past, they won’t likely be purchased together often in the future. One way to make intelligent recommendations is by using this idea: if a user is interested in an item, recommend to that customer other items whose purchase histories have the most in common with the item they’re interested in. This method is called <em>item-based collaborative filtering</em>.</p>
<p>To recommend items with the most similar purchase histories, we need a way to quantitatively measure exactly how similar two purchase histories are. We saw that <code>item1</code> and <code>item3</code> have very similar (identical) purchase histories, while <code>item1</code> and <code>item2</code> have more different purchase histories. If we compare <code>item1</code> and <code>item5</code>, we can see some similarity in their histories and some differences. But instead of making qualitative judgments that two purchase histories are <em>very similar</em> or <em>not very similar</em>, using numbers to precisely quantify that similarity will be useful. If we can find a metric that quantifies the similarity of two items, we can use that metric to recommend items.</p>
<h3 id="h2-502888c09-0001">Measuring Vector Similarity</h3>
<p class="BodyFirst">Let’s look more closely at one item’s purchase history to get ideas for ways to quantitatively measure similarities:</p>
<pre><code>print(list(interaction.loc['item1',:]))</code></pre>
<p>This line of code prints out the purchase history of <code>item1</code>. The output looks like this:</p>
<pre><code>[1,1,0,1,1]</code></pre>
<p><span epub:type="pagebreak" id="Page_196" title="196"/>We can think of this purchase history in several ways. It may seem like nothing more than a collection of numbers. Since the numbers are sandwiched between square brackets, Python will interpret this collection as a list. We could also think of it as a row of a matrix (our interaction matrix). Most importantly, we can think of this collection of numbers as a <em>vector</em>. You may remember from math class that a vector is a directed line segment. One way to write a vector is as a collection of coordinate numbers. For example, <a href="#figure9-1" id="figureanchor9-1">Figure 9-1</a> depicts two vectors, <em>A⃗</em> and <em>B⃗</em>.</p>
<figure>
<img alt="" class="" height="245" src="image_fi/502888c09/f09001.png" width="288"/>
<figcaption><p><a id="figure9-1">Figure 9-1</a>: Two vectors, represented by coordinate pairs</p></figcaption>
</figure>
<p class="BodyFirst">In this example, <em>A⃗</em> and <em>B⃗</em> are directed line segments, or vectors. They are both two-dimensional. Just like every vector, both can be fully described by their coordinates: after we know that both vectors start at the origin, the coordinate pair (3,7) fully describes vector <em>A⃗</em>, and the coordinate pair (8,4) fully describes vector <em>B⃗</em>. The purchase history we looked at previously, <code>[1,1,0,1,1]</code>, can be thought of as the vector representing the purchase history of <code>item1</code>. In fact, all the rows of our interaction matrix, or any interaction matrix, can be thought of as vectors.</p>
<p>Since we have vectors representing items, we may want to draw our vectors in a plot like <a href="#figure9-1">Figure 9-1</a>. However, in our interaction matrix, our item vectors have five coordinates each, so if we wanted to draw them, we would have to draw them in a five-dimensional plot, which is not possible to do in a way that humans can easily comprehend. Since we can’t draw our item vectors, let’s look at the <em>A⃗</em> and <em>B⃗</em> vectors in <a href="#figure9-1">Figure 9-1</a> to understand how to measure vector similarity, then apply what we learn to our item vectors later.</p>
<p>You can see that vectors <em>A⃗</em> and <em>B⃗</em> are somewhat similar: both are pointing generally upward and generally toward the right. We want to find a quantitative measurement that signifies exactly how similar the two vectors are. All we need to do is measure the angle between the two vectors, as in <a href="#figure9-2" id="figureanchor9-2">Figure 9-2</a>.</p>
<span epub:type="pagebreak" id="Page_197" title="197"/><figure>
<img alt="" class="" height="191" src="image_fi/502888c09/f09002.png" width="221"/>
<figcaption><p><a id="figure9-2">Figure 9-2</a>: An angle between two vectors, represented by the Greek letter theta</p></figcaption>
</figure>
<p>Every pair of vectors will have an angle between them that we can measure. In two dimensions, we can get out a protractor and physically measure the angle between two vectors. In <a href="#figure9-2">Figure 9-2</a>, the angle is labeled with the Greek letter theta. If the angle theta is small, we conclude that the two vectors are similar. If theta is large, we conclude that the two vectors are very different. The smallest possible angle between two vectors is 0; a 0-degree angle between two vectors means that they’re pointing in exactly the same direction (they overlap).</p>
<p>This isn’t a geometry book, but try to remember just one more thing from your math and geometry classes: the <em>cosine</em>. The cosine is a function that we can measure for every angle. The cosine of a 0-degree angle is 1; that’s the maximum value a cosine can be. As an angle gets larger than 0, its cosine decreases. For a 90-degree angle (also called a <em>perpendicular</em>, or <em>right</em>, angle), the cosine will be 0.</p>
<p class="BodyFirst">The cosine is important because we can use it to measure the similarity of two vectors. If two vectors are similar, the angle between them will be small, so the cosine of the angle between them will be large (1 or close to 1). If two vectors are perpendicular, they’re quite different, and the cosine of the angle between them will be 0. Vectors like <em>A⃗</em> and <em>B⃗</em> in <a href="#figure9-1">Figure 9-1</a> are not completely similar and not completely different, so the cosine of the angle between them will be between 0 and 1. When comparing vectors, we often refer to the <em>cosine similarity</em> of the two vectors (the cosine of the angle between the vectors). Similar vectors will have high cosine similarity, and different vectors will have low cosine similarity.</p>
<p>When vectors have many dimensions, like the five-dimensional vectors in our purchase histories, we don’t physically measure the angles. Instead, we can use a special formula that enables us to calculate the cosine of the angle between any pair of vectors without requiring the physical use of a protractor; see <a href="#figure9-3" id="figureanchor9-3">Figure 9-3</a>.</p>
<span epub:type="pagebreak" id="Page_198" title="198"/><figure>
<img alt="" class="" height="110" src="image_fi/502888c09/f09003.png" width="384"/>
<figcaption><p><a id="figure9-3">Figure 9-3</a>: A formula for calculating the cosine of the angle between two vectors</p></figcaption>
</figure>
<p>We’ll unpack this formula in the next section. </p>
<h3 id="h2-502888c09-0002">Calculating Cosine Similarity</h3>
<p class="BodyFirst">Let’s look more closely at the formula in <a href="#figure9-3">Figure 9-3</a>. The numerator is <em>A</em> · <em>B</em>. In this case the dot between the vectors <em>A⃗</em> and <em>B⃗</em> is indicating a <em>dot product</em>, a special way to multiply vectors together. The following function calculates the dot product of any two vectors that are the same length:</p>
<pre><code>def dot_product(vector1,vector2):
    thedotproduct=np.sum([vector1[k]*vector2[k] for k in range(0,len(vector1))])
    return(thedotproduct)</code></pre>
<p>The denominator of the formula in <a href="#figure9-3">Figure 9-3</a> shows pipe symbols (<code>||</code>) surrounding both <em>A</em> and <em>B</em>. These pipe symbols indicate the respective sizes of vectors <em>A⃗</em> and <em>B⃗</em>, also called their vector <em>norms</em>. The following function calculates the vector norm of any vector:</p>
<pre><code>def vector_norm(vector):
    thenorm=np.sqrt(dot_product(vector,vector))
    return(thenorm)</code></pre>
<p>The cosine of the angle between any two vectors (that is, the cosine similarity of the two vectors) is the dot product of the two vectors, divided by the product of the norms of the vectors. We can create a Python function that calculates the cosine similarity of any two vectors by using the two functions we just defined, combined as shown in the formula in <a href="#figure9-3">Figure 9-3</a>:</p>
<pre><code>def cosine_similarity(vector1,vector2):
    thedotproduct=dot_product(vector1,vector2)
    thecosine=thedotproduct/(vector_norm(vector1)*vector_norm(vector2))
    thecosine=np.round(thecosine,4)
    return(thecosine)</code></pre>
<p>The cosine similarity that this function calculates is a common similarity measurement that’s used in many data science applications, not just for recommendation systems.</p>
<p><span epub:type="pagebreak" id="Page_199" title="199"/>Let’s try to calculate some cosine similarity measurements for our item vectors:</p>
<pre><code>import numpy as np
item1=interaction.loc['item1',:]
item3=interaction.loc['item3',:]
print(cosine_similarity(item1,item3))</code></pre>
<p>This snippet yields a simple output:</p>
<pre><code>1.0</code></pre>
<p>We can see that <code>item1</code> and <code>item3</code> have a cosine similarity of <code>1.0</code>, meaning that the angle between these vectors is 0. Therefore, they’re identical vectors, and they’re as similar as it is possible to be. By contrast, you can check the cosine similarity of <code>item2</code> and <code>item5</code> by running the following snippet:</p>
<pre><code>item2=list(interaction.loc['item2',:])
item5=list(interaction.loc['item5',:])
print(cosine_similarity(item2,item5))</code></pre>
<p>These have a cosine similarity of <code>0.3333</code>, meaning that the angle between these vectors is relatively large—about 71 degrees, not far from a right angle. Therefore, these two items are very different from each other. We can see that when we look at their vectors: only one user out of five purchased both items. If we follow a similar process to check the cosine similarity of <code>item3</code> and <code>item5</code>, we find that it’s <code>0.866</code>, indicating that these vectors are similar but not completely identical.</p>
<p>Now that we can measure the similarity of any two items’ histories, we’re ready to use this calculation to create a recommendation system.</p>
<h3 id="h2-502888c09-0003">Implementing Item-Based Collaborative Filtering</h3>
<p class="BodyFirst">Let’s think back to our hypothetical salesperson and a hypothetical sales scenario. You have an interaction matrix that describes the purchase history of all five of your customers and all five of your items. You observe a new, unfamiliar customer entering your store (or visiting your website), and all you know about the new customer is that they are interested in <code>item1</code>. How should you make recommendations to them?</p>
<p>You can rank every item based on how similar its purchase history is to the purchase history of <code>item1</code>. Your recommendations will be an ordered list of items, ranked from the item whose purchase history is the most similar to <code>item1</code> to the item whose purchase history is the least similar to <code>item1</code>.</p>
<p>Let’s write Python code for this, using cosine similarity. We can start by defining the vectors we’ll need to do our calculations:</p>
<pre><code>ouritem='item1'
otherrows=[rowname for rowname in interaction.index if rowname!=ouritem]
otheritems=interaction.loc[otherrows,:]
theitem=interaction.loc[ouritem,:]</code></pre>
<p><span epub:type="pagebreak" id="Page_200" title="200"/>Next, we can calculate how similar each item is to our selected item and make recommendations by finding the other items that are most similar to our selected item:</p>
<pre><code>similarities=[]
for items in otheritems.index:
    similarities.append(cosine_similarity(theitem,otheritems.loc[items,:]))

otheritems['similarities']=similarities
recommendations = list(otheritems.sort_values(by='similarities',ascending=False).index)</code></pre>
<p>In this snippet, we create a <code>similarities</code> variable, which starts as an empty list. Then we create a loop that calculates the cosine similarity between our item and every other item. After that, we get our final list of recommendations: a list of all other items, sorted from most similar to least similar to our item.</p>
<p>You can check the recommendations by running <code>print(recommendations)</code>, which will show you the following list:</p>
<pre><code>['item3', 'item5', 'item2', 'item4']</code></pre>
<p>This list is the output of your recommendation system. This final output is similar to the output of our popularity-based recommendation system: just a list of items, sorted in order of most relevant to least relevant (the highest-priority to the lowest-priority recommendations). The difference is that instead of measuring relevance in terms of overall popularity, we are measuring relevance in terms of the similarity of purchase histories: the more similar purchase histories are rated as more relevant, and therefore a higher priority to recommend to users.</p>
<p>We can also create a function that combines all of these capabilities together:</p>
<pre><code>def get_item_recommendations(interaction,itemname):
    otherrows=[rowname for rowname in interaction.index if rowname!=itemname]
    otheritems=interaction.loc[otherrows,:]
    theitem=list(interaction.loc[itemname,:])
    similarities=[]
    for items in otheritems.index:
        similarities.append(cosine_similarity(theitem,list(otheritems.loc[items,:])))
    otheritems['similarities']=similarities
    return list(otheritems.sort_values(by='similarities',ascending=False).index)</code></pre>
<p>You can run <code>get_item_recommendations(interaction,'item1')</code> to see the items recommended for any user who’s interested in <code>item1</code>. You could also substitute any other item for <code>item1</code> to see recommendations for users interested in other items.</p>
<p>The recommendation system we have created here is <em>item-based collaborative filtering</em>. It’s <em>filtering</em> because instead of recommending every item to users, we filter and show only the most relevant. It’s <em>collaborative</em> because we’re using information related to all items and all users, so it’s as if the <span epub:type="pagebreak" id="Page_201" title="201"/>users and items are collaborating to help us determine relevance. It’s <em>item-based</em> because our recommendations are based on the similarity between items’ purchase histories, rather than similarity between users or anything else.</p>
<p>Item-based collaborative filtering is relatively simple to implement, and it can be used to make “warm” recommendations even if we know only one fact about a potential customer (a single item that they’re interested in). You can see that it can be implemented with just a few lines of code, and the only input data that’s needed is an interaction matrix.</p>
<p>Item-based collaborative filtering has a reputation for making <em>obvious</em> recommendations. Multiple James Bond films are likely to have high overlap in their purchase histories, so using item-based collaborative filtering to make recommendations related to one James Bond film will likely yield a recommendation to view a different James Bond film. But James Bond fans are already familiar with James Bond films and don’t need to get a recommendation to watch a film that they’re already familiar with. Recommendation systems can be more valuable when they recommend items that are less obvious. Next, let’s take a look at a method that has a reputation for generating some less obvious recommendations.</p>
<h2 id="h1-502888c09-0003">User-Based Collaborative Filtering</h2>
<p class="BodyFirst">Suppose you want to make recommendations for a customer you’re already familiar with. For example, suppose that our fifth customer, <code>user5</code>, walks into your store (or visits your website). Your interaction matrix already has detailed records related to <code>user5</code> and everything they’ve purchased before. We can use this detailed information to make intelligent, “warm” recommendations for <code>user5</code> with <em>user-based collaborative filtering</em>.</p>
<p>This approach is based on the idea that people who are similar may be interested in the same items. If we need to make recommendations for a particular customer, we find the customers who are most similar to that customer and recommend items that those similar customers purchased.</p>
<p>Let’s start by looking at our interaction matrix yet again:</p>
<pre><code>  Unnamed: 0  user1  user2  user3  user4  user5
0      item1      1      1      0      1      1
1      item2      1      0      1      1      0
2      item3      1      1      0      1      1
3      item4      1      0      1      0      1
4      item5      1      1      0      0      1</code></pre>
<p>This time, instead of thinking of the <em>rows</em> as vectors related to <em>items</em>, let’s think of the <em>columns</em> as vectors related to <em>customers</em>. The vector <code>[1,0,1,1,1]</code> (the last column of the matrix) represents the full purchase history of <code>user5</code>. If we look at other customer purchase history vectors, we can see that <code>user2</code> has a purchase history that’s similar to <code>user5</code>’s purchase history. We can see that <code>user3</code> has a purchase history that’s very different from the purchase history of <code>user5</code>—almost no overlap. Just as we did when we were implementing <span epub:type="pagebreak" id="Page_202" title="202"/>item-based collaborative filtering, we can calculate the similarity of customers based on their purchase histories:</p>
<pre><code>user2=interaction.loc[:,'user2']
user5=interaction.loc[:,'user5']
print(cosine_similarity(user2,user5))</code></pre>
<p>The output of this snippet is <code>0.866</code>, indicating that <code>user2</code> and <code>user5</code> have relatively high cosine similarity (remember that the closer to 1 this measurement is, the more similar two vectors are). We can change the users whose similarity we calculate by making small adjustments to this snippet:</p>
<pre><code>user3=interaction.loc[:,'user3']
user5=interaction.loc[:,'user5']
print(cosine_similarity(user3,user5))</code></pre>
<p>Here, we find that <code>user3</code> and <code>user5</code> have cosine similarity 0.3536, which is relatively low, just as expected.</p>
<p>We can also create a function that calculates the most similar customers to a given customer:</p>
<pre><code>def get_similar_users(interaction,username):
    othercolumns=[columnname for columnname in interaction.columns if columnname!=username]
    otherusers=interaction[othercolumns]
    theuser=list(interaction[username])
    similarities=[]
    for users in otherusers.columns:
        similarities.append(cosine_similarity(theuser,list(otherusers.loc[:,users])))
    otherusers.loc['similarities',:]=similarities
    return list(otherusers.sort_values(by='similarities',axis=1,ascending=False).columns)</code></pre>
<p>This function takes a customer name and an interaction matrix as its inputs. It calculates the similarity of the input customer to all other customers specified in the interaction matrix. The final output is a ranked list of customers, sorted from the most similar customer to the least similar customer.</p>
<p>We can use this function in several ways to get recommendations. Here’s one way to get recommendations for <code>user5</code>:</p>
<ol class="decimal">
<li value="1">Calculate how similar every user is to <code>user5</code>.</li>
<li value="2">Rank customers from most similar to least similar to <code>user5</code>.</li>
<li value="3">Find the most similar customer to <code>user5</code>.</li>
<li value="4">Recommend everything the most similar customer has purchased that <code>user5</code> has not purchased.</li>
</ol>
<p>We can write code that implements this algorithm as follows:</p>
<pre><code>def get_user_recommendations(interaction,username):
    similar_users=get_similar_users(interaction,username)
    purchase_history=interaction[similar_users[0]]
    purchased=list(purchase_history.loc[purchase_history==1].index)
<span epub:type="pagebreak" id="Page_203" title="203"/>    purchased2=list(interaction.loc[interaction[username]==1,:].index)
    recs=sorted(list(set(purchased) - set(purchased2)))
    return(recs)</code></pre>
<p>In this snippet, we have a function that takes an interaction matrix and a user name as its inputs. It finds the most similar user to the input user, and it stores the purchase history of that user in a variable called <code>purchase_history</code>. Next, it finds everything that the most similar user purchased (stored in the variable <code>purchased</code>) and everything that the input user purchased (stored in the variable <code>purchased2</code>). Then it finds everything that the most similar user purchased that was not purchased by the input user. It does this by using the <code>set()</code> function. The <code>set()</code> function creates a collection of the unique elements in a list. So when you run <code>set(purchased) - set(purchased2)</code>, you’ll get the unique elements of <code>purchased</code> that are not also elements of <code>purchased2</code>. Finally, it returns the list of these elements as the final recommendations.</p>
<p>You can run this function simply, by running <code>get_user_recommendations(interaction,'user2')</code>. You should see the following output:</p>
<pre><code>['item4']</code></pre>
<p>In this case, <code>item4</code> is our recommendation because it was purchased by <code>user5</code>, the user who’s most similar to <code>user2</code>, and it hasn’t yet been purchased by <code>user2</code>. We’ve created a function that performs user-based collaborative filtering!</p>
<p>You could make many adjustments to this function. For example, you might want more recommendations than you get by looking at one similar customer. If so, you could look at more similar customers than just one. You could also add item-based similarity calculations so that you recommend only items that were purchased by similar users and are also similar to the items that have been purchased by the focal user.</p>
<p>Ensuring that you understand the similarities and differences between user-based and item-based collaborative filtering is worthwhile. Both rely on cosine similarity calculations, and both rely on an interaction matrix as input. In item-based collaborative filtering, we calculate the cosine similarity between items and recommend items that are similar to an item of interest. In user-based collaborative filtering, we calculate the cosine similarity between users and recommend items from the purchase histories of similar users. Both can lead to good recommendations.</p>
<p>To determine which method is right for your business, you could try both and check which one leads to better results: either more revenue, more profit, more satisfied customers, more customer engagement, or more of whatever metric you want to maximize. The best way to do this type of experimental comparison is with an A/B test, which you’ve already learned in <span class="xref" itemid="xref_target_Chapter 4">Chapter 4</span>.</p>
<p>User-based collaborative filtering has a reputation for giving more surprising results than item-based collaborative filtering. However, it also tends to be more computationally difficult. Most retailers have more customers than items, so user-based collaborative filtering usually requires more calculations than item-based collaborative filtering.</p>
<p><span epub:type="pagebreak" id="Page_204" title="204"/>So far, we’ve been working with an unrealistically tiny and entirely fabricated dataset. Applying the ideas we’ve gone over so far to data that comes from a real business would be more beneficial, with real users and their real interaction histories. In the next section, we’ll do just that: we’ll go through a case study of generating recommendations for real users and the real items that we expect will interest them.</p>
<h2 id="h1-502888c09-0004">Case Study: Music Recommendations</h2>
<p class="BodyFirst">We’ll use data from Last.fm (<a class="LinkURL" href="https://last.fm">https://last.fm</a>). This website allows people to log in and listen to music. In this case, the “items” in our interaction matrix will be musical artists, and a 1 in the interaction matrix will indicate that a user has listened to an artist, rather than representing a purchase. Despite these minor differences, we can use all the methods we’ve discussed in this chapter to make recommendations about music that users should listen to next.</p>
<p>Let’s read some data related to Last.fm users and look at it:</p>
<pre><code>import pandas as pd
lastfm = pd.read_csv("https://bradfordtuckfield.com/lastfm-matrix-germany.csv")
print(lastfm.head())</code></pre>
<p>As usual, we import the pandas package, read our <em>.csv</em> file, and store the data in the <code>lastfm</code> variable. When we print out the top few rows of the data, we see the following output:</p>
<pre><code>   user  a perfect circle  abba  ...  underoath  volbeat  yann tiersen
0     1                 0     0  ...          0        0             0
1    33                 0     0  ...          0        0             0
2    42                 0     0  ...          0        0             0
3    51                 0     0  ...          0        0             0
4    62                 0     0  ...          0        0             0</code></pre>
<p>In this data, every row represents a unique (anonymous) user. Every column represents a musical artist. The entries in the matrix can be interpreted in the same way as the entries in our previous interaction matrix: every entry equal to <code>1</code> means that a particular user has listened to a particular artist, and every entry equal to <code>0</code> means that the user has not listened to that artist. In this case, we can talk about a user’s or an item’s listening history instead of purchase history. Regardless, the entries of this matrix show the history of interactions between users and items. We don’t need the first column (the user ID number), so we can drop it:</p>
<pre><code>lastfm.drop(['user'],axis=1,inplace=True)</code></pre>
<p>Before we proceed, notice the difference between this interaction matrix and the previous one. In our previous interaction matrix, the rows corresponded to items, and the columns corresponded to users. This interaction matrix is reversed: the rows correspond to users, and the columns <span epub:type="pagebreak" id="Page_205" title="205"/>correspond to items (songs). The functions we wrote are meant to work with interaction matrices that have the former shape (rows for items, columns for users). To make sure our interaction matrix can work with our functions, we should <em>transpose</em> it, or rewrite its rows as columns and its columns as rows:</p>
<pre><code>lastfmt=lastfm.T</code></pre>
<p>This snippet uses our matrix’s <code>T</code> attribute to transpose our interaction matrix, and it stores the result in the variable <code>lastfmt</code>. Let’s check the number of rows and columns in this data:</p>
<pre><code>print(lastfmt.shape)</code></pre>
<p>The output here is <code>(285,1257)</code>: the data has 285 rows and 1,257 columns. So, we’re looking at information about 1,257 real users and 285 real artists whose music these users listened to. This is much more substantial than our previous, fabricated data. Let’s get recommendations for these users. It’s as simple as calling a function we already created earlier in the chapter:</p>
<pre><code>get_item_recommendations(lastfmt,'abba')[0:10]</code></pre>
<p>You’ll see the following output:</p>
<pre><code>['madonna', 'robbie williams', 'elvis presley', 'michael jackson', 'queen',
'the beatles', 'kelly clarkson', 'groove coverage', 'duffy', 'mika']</code></pre>
<p>For people who are interested in music by ABBA, these artists are recommended via item-based collaborative filtering. They’re listed in order of most relevant to least relevant. Remember, these artists were selected based on similar purchase histories: out of all artists, Madonna’s listening history was the most similar to ABBA’s, and Robbie Williams’s listening history was the second most similar, and so on.</p>
<p>This is all it takes; we can call the recommendation function for any artist that interests us. Going from fabricated to real data is quite simple. We can also call our user recommendation function:</p>
<pre><code>print(get_user_recommendations(lastfmt,0)[0:3])</code></pre>
<p>The output shows us three recommendations for the first user (the user with index 0 in the dataset):</p>
<pre><code>['billy talent', 'bob marley', 'die toten hosen']</code></pre>
<p>These recommendations were obtained using user-based collaborative filtering. Remember what that means: our code found the user whose listening history is the most similar to the listening history of the first user. The final recommendations are artists that the most similar user listened to but the focal user has not yet listened to.</p>
<h2 id="h1-502888c09-0005"><span epub:type="pagebreak" id="Page_206" title="206"/>Generating Recommendations with Advanced Systems</h2>
<p class="BodyFirst">Collaborative filtering is the most common way to build recommendation systems, but it’s not the only one. Several other techniques allow generation of intelligent recommendations. One approach, called <em>singular value decomposition</em>, relies on matrix algebra to <em>decompose</em> the interaction matrix into several smaller matrices. These smaller matrices can be multiplied in various ways to predict which products will appeal to which customers. Singular-value decomposition is one of several methods that use linear algebra to predict customer preferences. Another such linear algebra method is called <em>alternating least squares</em>.</p>
<p>The clustering methods we discussed in <span class="xref" itemid="xref_target_Chapter 7">Chapter 7</span> can also be used to generate recommendation systems. These clustering-based recommendation systems use an approach like the following:</p>
<ol class="decimal">
<li value="1">Generate clusters of users.</li>
<li value="2">Find the most popular items in each cluster of users.</li>
<li value="3">Recommend those popular items, but only within each cluster.</li>
</ol>
<p>This method is the same as the popularity-based recommendation system we discussed at the beginning of the chapter, with one improvement: we look at popularity within clusters of similar customers, instead of global popularity.</p>
<p>Other recommendation systems rely on the analysis of content. For example, to make recommendations about songs on a music-streaming service, you might download a database of song lyrics. You could use some NLP tools to measure the similarity between distinct songs’ lyrics. If a user listened to Song X, you could recommend that they listen to the songs whose lyrics have the highest similarity to Song X’s lyrics. This is an item-based recommendation system, but instead of using purchase histories, it uses item attributes to find relevant recommendations. <em>Attribute-based systems</em> (also called <em>content-based recommender systems</em>) like this can work effectively in some situations. Many corporations that implement recommendation systems today collect a wide variety of data to use as inputs, and a wide variety of prediction methods, including neural networks, to predict what each user will like. The problem with a content-based approach is that it can be difficult to get attribute data that’s reliable and comparable across items.</p>
<p>Attribute data is not the only kind of data that can be added to recommendation systems. Using dates in your recommendation systems could also be valuable. In a popularity-based system, dates or timestamps can enable you to replace <em>all-time most popular</em> lists with <em>today’s most popular</em> lists, or lists that show trending content across the most recent hour, week, or any other time frame.</p>
<p class="BodyFirst">You may also need to build recommendation systems with interaction matrices that are not 0/1 matrices. For example, you could have an interaction matrix whose entries indicate the number of times a song has been played, rather than a 0/1 indicator of whether a song has been played. You might also find an interaction matrix that contains ratings rather than <span epub:type="pagebreak" id="Page_207" title="207"/>interactions. The same methods you implemented in this chapter can be applied to these alternative types of interaction matrices: you can still calculate cosine similarities and make recommendations based on the most similar items and users.</p>
<p>The world of recommendation systems is big. There’s room for creativity and new approaches, and you can open your mind while trying to discover new ways to improve the field.</p>
<h2 id="h1-502888c09-0006">Summary</h2>
<p class="BodyFirst">In this chapter, we discussed recommendation systems. We started with popularity-based systems to show how to recommend trending items and bestsellers. We continued with collaborative filtering, including how to measure the similarity of items and customers and how to use similarity to make item-based and user-based recommendations. We presented a case study in which we used our collaborative-filtering code to get recommendations related to a music-streaming service. We concluded with some advanced considerations, including other approaches that could be used and other data that can be leveraged.</p>
<p>Next, we’ll go over some advanced natural language processing methods for analysis of text.</p>
</section>
</div></body></html>