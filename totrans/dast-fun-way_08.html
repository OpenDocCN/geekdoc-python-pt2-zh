<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_113" title="113"/>8</span><br/>
<span class="ChapterTitle">Grids</span></h1>
</header>
<figure class="opener">
<img alt="" height="99" src="image_fi/book_art/chapterart.png" width="98"/>
</figure>
<p class="ChapterIntro">In this chapter, we look at what happens as we consider multidimensional values and targets. The data structures we’ve examined so far have all shared a common constraint—they organize data based on a single value. Many real-world problems involve multiple important dimensions, and we need to extend our data structures to handle searches over such data.</p>
<p>This chapter starts by introducing nearest-neighbor search, which will serve as our motivating use case for multidimensional data. As we will see, the generality of nearest-neighbor search makes it very flexible and applicable to a wide range of spatial and non-spatial problems. It can help us find the cup of coffee closest to our current location or the brand best suited to our tastes.</p>
<p>We then introduce the grid data structure and show how it facilitates nearest-neighbor search over two dimensions, using spatial relationships within the data to prune out infeasible regions of the search space. We briefly discuss how these approaches can be extended to more than two dimensions. We will also see how these data structures fall short, providing the motivation for further spatial data structures. </p>
<h2 id="h1-502604c08-0001"><span epub:type="pagebreak" id="Page_114" title="114"/>Introducing Nearest-Neighbor Search</h2>
<p class="BodyFirst">As its name implies, <em>nearest-neighbor search</em> consists of finding a particular data point closest to a given search target—for example, the coffee shop nearest our current location. Formally, we define nearest-neighbor search as follows: </p>
<blockquote class="review">
<p class="Blockquote">Given a set of <em>N</em> data points <em>X</em> = {<em>x</em><sub>1</sub>,<em> x</em><sub>2</sub>, … ,<em> x</em><sub>N</sub>}, a target value <em>x</em>’, and a distance function <em>dist</em>(<em>x</em>,<em>y</em>), find the point <em>x</em><sub><em>i</em></sub> <span class="NSSymbol">∈</span> <em>X</em> that minimizes <em>dist</em>(<em>x</em>’,<em>x</em><sub><em>i</em></sub>).</p></blockquote>
<p class="BodyContinued">Nearest-neighbor search is closely related to the target value search we used to motivate binary search in <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. Both algorithms search for a specific data point within a set of data. The key difference lies in the success criteria. Whereas binary search tests for an exact match within a data set, which may or may not be present, nearest-neighbor search is only concerned with finding the closest match.</p>
<p>This framing makes nearest-neighbor search useful for many types of multiple-dimensional data. We could be searching a map for nearby coffee shops, a list of historical temperatures for days similar to the current date, or a list of “close” misspellings of a given word. As long as we can define a distance between the search target and other values, we can find nearest neighbors.</p>
<p>In past chapters, we primarily considered targets that are individual numeric values, like the data stored in binary search trees and heaps. While we sometimes included auxiliary data, the targets themselves remained simple. In contrast, nearest-neighbor search is most interesting when dealing with multidimensional data, which may be stored in a variety of other data structures such as arrays, tuples, or composite data structures. Later in this chapter, we look at example two-dimensional search problems and their targets. For now, though, let’s introduce a basic algorithm for this search.</p>
<h3 id="h2-502604c08-0001">Nearest-Neighbor Search with Linear Scan</h3>
<p class="BodyFirst">As a baseline algorithm for nearest-neighbor search, we start with a modified version of the linear scan algorithm from <span class="xref" itemid="xref_target_Chapter 2">Chapter 2</span>. The linear scan algorithm isn’t particularly exciting; you can implement it with a simple loop in most programming languages. Yet, because of its simplicity, linear scan provides a good starting point from which to examine more complex and efficient algorithms.</p>
<p>Consider the problem of nearest-neighbor search with numbers using the absolute distance: <em>dist</em>(<em>x</em>,<em>y</em>) = |<em>x</em> – <em>y</em>|. Given a list of numbers and a search target, we want to find the closest number on the list.  Perhaps we wake up in a new city and need to find our first cup of coffee in the morning. The hotel’s concierge provides a list of coffee shops on the same street, along with a helpful map. Not recognizing any of the businesses, we resolve to prioritize expedience and visit the coffee shop closest to the hotel.</p>
<p>We can visualize this search with a number line shown in <a href="#figure8-1" id="figureanchor8-1">Figure 8-1</a>. The points represent different coffee shops and their location with respect to the start of the map, while the X represents our hotel with a location of 2.2 miles along the street.</p>
<span epub:type="pagebreak" id="Page_115" title="115"/><figure>
<img alt="A number line with seven candidate neighbors and one target point. The target point sits at 2.2 with the closest neighbor at 2.6." class="" height="40" src="image_fi/502604c08/f08001.png" width="244"/>
<figcaption><p><a id="figure8-1">Figure 8-1</a>: A one-dimensional nearest-neighbor search represented as a number line</p></figcaption>
</figure>
<p>In a program, the points in <a href="#figure8-1">Figure 8-1</a> might represent unsorted values within an array. However, visualizing these values along a real-valued number line has two advantages in the context of nearest-neighbor search. First, it clarifies the importance of distance: we can see the gaps between our target value and each data point. Second, it helps us generalize the techniques beyond a single dimension, as we’ll see in the next section.</p>
<p>For now, the linear scan proceeds through each data point, as shown in <a href="#figure8-2" id="figureanchor8-2">Figure 8-2</a>, computing the distance for the current data point and comparing it to the minimum distance found so far. Here we consider the points in sorted order since they are already along the number line, but linear scan does not require a particular ordering. It uses the data’s ordering within the list. </p>
<figure>
<img alt="The linear scan computes each data point’s distance from the target point. A series of number lines shows each pair of points and their corresponding distance." class="" height="587" src="image_fi/502604c08/f08002.png" width="244"/>
<figcaption><p><a id="figure8-2">Figure 8-2</a>: A linear scan through the data points in a one-dimensional nearest-neighbor search</p></figcaption>
</figure>
<p>In the first comparison in <a href="#figure8-2">Figure 8-2</a>, we find a point at distance 1.8. This becomes our best option so far, our <em>candidate nearest neighbor</em>. It might not be a <em>good</em> neighbor—1.8 miles is a bit far to walk for our morning cup of joe—but it’s the best we’ve seen. The next two steps discover better <span epub:type="pagebreak" id="Page_116" title="116"/>candidates at distances 1.2 and 0.4, respectively. Alas, the remaining four comparisons don’t produce a better candidate; the point at distance 0.4 remains the closest we’ve found. In the end, the algorithm returns that third point on our number line as the nearest neighbor. We head to the coffee shop, confident we’re heading to the closest one on the street.</p>
<p><a href="#listing8-1" id="listinganchor8-1">Listing 8-1</a> shows the code for a linear scan using an arbitrary distance function. We use floating-point values for the one-dimensional case but can extend the function to multiple dimensions by using composite data structures or other representations.</p>
<pre><code>LinearScanClosestNeighbor(Array: A, Float: target, Function: dist):
    Integer: N = length(A)
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> IF N == 0:
        return null

  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> Float: candidate = A[0]
    Float: closest_distance = dist(target, candidate)

    Integer: i = 1
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> WHILE i &lt; N:
        Float: current_distance = dist(target, A[i])
      <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> IF current_distance &lt; closest_distance:
            closest_distance = current_distance
            candidate = A[i]
        i = i + 1
  <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> return candidate</code></pre>
<p class="CodeListingCaption"><a id="listing8-1">Listing 8-1</a>: The code for a linear scan nearest-neighbor algorithm</p>
<p>The code starts by checking whether the array is empty and, if so, returning <code>null</code> <span aria-label="annotation1" class="CodeAnnotation">❶</span>, since there is no closest point. The code then picks the first item in the array as the initial candidate nearest neighbor and computes the distance from that point to the target <span aria-label="annotation2" class="CodeAnnotation">❷</span>. This information provides a starting point for our search: we compare all future points against the best candidate and distance so far. The remainder of the code uses a <code>WHILE</code> loop to iterate over the remaining elements in the array <span aria-label="annotation3" class="CodeAnnotation">❸</span>, computing the distance to the target and comparing that to the best distance found so far. The code updates the best candidate and best distance found whenever it finds a closer candidate <span aria-label="annotation4" class="CodeAnnotation">❹</span>, then returns the closest neighbor <span aria-label="annotation5" class="CodeAnnotation">❺</span>.</p>
<p>Beyond providing a simple implementation of nearest-neighbor search, the linear scan algorithm also trivially supports different distance functions or even higher-dimensional points. First, let’s look at some example problems in this two-dimensional space. </p>
<h3 id="h2-502604c08-0002">Searching Spatial Data</h3>
<p class="BodyFirst">Imagine that you are multiple hours into a cross-country road trip and desperately need a coffee refill. Panic floods your mind as you realize that you haven’t mapped out the optimal coffee shops along your route. You take a few deep breaths, pull out the map shown in <a href="#figure8-3" id="figureanchor8-3">Figure 8-3</a>, and locate numerous towns with known coffee establishments. Prioritizing expedience over quality, you vow to find the closest café.</p>
<span epub:type="pagebreak" id="Page_117" title="117"/><figure>
<img alt="A two‐dimensional map of towns. The target point lies in the middle left near the towns of Gridville, Cartesian, and Fort Fortran." class="" height="264" src="image_fi/502604c08/f08003.png" width="265"/>
<figcaption><p><a id="figure8-3">Figure 8-3</a>: A map as an example of two-dimensional data</p></figcaption>
</figure>
<p>The data consists of two-dimensional points—towns with <em>x</em>, <em>y</em> coordinates. These data points can be stored as an ordered tuple (<em>x</em>, <em>y</em>); a small, fixed-size array [<em>x</em>, <em>y</em>]; or even a composite data structure for two-dimensional spatial points:</p>
<pre><code>Point {
    Float: x
    Float: y
}</code></pre>
<p>When determining which town is closest, we’ll focus on just the straight-line distance to the coffee shop. In any real-world navigation task, we’d also need to consider obstacles standing between us and our coffee. For now, though, let’s just consider the Euclidean distance to the coffee shops. If our current point is (<em>x</em><sub>1</sub><em>, y</em><sub>1</sub>) and the coffee shop is at (<em>x</em><sub>2</sub><em>, y</em><sub>2</sub>), then the distance is:</p>
<figure class="graphic">
<img alt="G08001" height="30" src="image_fi/502604c08/G08001.png" width="213"/></figure>
<p>We could use the linear-scan algorithm in <a href="#listing8-1">Listing 8-1</a>. The algorithm computes the distance from our target to each candidate point, as shown in <a href="#figure8-4" id="figureanchor8-4">Figure 8-4</a>. </p>
<figure>
<img alt="An illustration of the distance computations from our target point to each of the 11 towns on the map. The distances are represented by dashed lines from the target point to the town." class="" height="264" src="image_fi/502604c08/f08004.png" width="265"/>
<figcaption><p><a id="figure8-4">Figure 8-4</a>: A linear scan nearest-neighbor search computes the distance from the target to each candidate point.</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_118" title="118"/>The point with the smallest distance to the target, shown in <a href="#figure8-5" id="figureanchor8-5">Figure 8-5</a>, is the target’s nearest neighbor. The dashed line represents the distance to the closest point, and the dotted circle shows the area of our map that is closer than (or equal to) the closest point. No other points lie closer than the nearest neighbor.</p>
<figure>
<img alt="The map with a dashed line from the target point to the nearest neighbor and a dotted circle indicating the space that falls within this radius." class="" height="264" src="image_fi/502604c08/f08005.png" width="265"/>
<figcaption><p><a id="figure8-5">Figure 8-5</a>: The point with the smallest distance to the target is that target’s nearest neighbor.</p></figcaption>
</figure>
<p>As we’ve seen multiple times, though, this type of linear scan search quickly becomes inefficient as the number of points increases. If the current edition of the <em>Coffee Lover’s Guide to Roadside Coffee </em>lists 100,000 coffee shops, it would be needlessly time-consuming to check each one.</p>
<p>We shouldn’t need to look at every single data point in two-dimensional space. Some points are too far away to matter. We would never consider Alaskan coffee shops when driving through Florida. This is not to disparage Alaskan cafés—I’m sure there are plenty that equal their Floridian peers in terms of taste and quality. It’s simply a matter of expedience. We can’t survive an hour without our coffee, let alone a multi-day drive. If we are driving through northern Florida, we need to focus on northern Floridian coffee establishments.</p>
<p>As we saw in binary search, we can often use structure within the data to help eliminate large numbers of candidates. We can even adapt binary search to find nearest neighbors in one-dimensional space. Unfortunately, a simple sort will not help in the two-dimensional case. If we sort and search either x or y dimensions, as shown in <a href="#figure8-6" id="figureanchor8-6">Figure 8-6</a>, we get the wrong answer—the closest neighbor in the one-dimensional space is not the same as the closest two-dimensional neighbor. </p>
<p>We need to use information from all relevant dimensions to make accurate pruning decisions. A point close to our target along a single dimension might be staggeringly far away in other dimensions. If we sort our list of coffee shops by latitude, our search for locations near our current latitude in northern Florida might return quite a few “close” results from Houston. Similarly, if we sort by longitude, we might be swamped with entries from Cleveland. We need to explore new approaches, adapted from our experience with one-dimensional data but also making use of the structure inherent in higher dimensions. </p>
<span epub:type="pagebreak" id="Page_119" title="119"/><figure>
<img alt="On the left, a projection of the map points from Figure 8‐3 onto the Y‐axis. On the right, a projection of the points onto the X‐axis. In both cases, the closest neighbor in one dimension is not the same as the closest neighbor in two dimensions." class="" height="268" src="image_fi/502604c08/f08006b.png" width="607"/>
<figcaption><p><a id="figure8-6">Figure 8-6</a>: Projecting the data to one dimension along either the y-axis (left) or x-axis (right) removes important spatial information about the other dimension.</p></figcaption>
</figure>
<h2 id="h1-502604c08-0002">Grids</h2>
<p class="BodyFirst"><em>Grids</em> are data structures for storing two-dimensional data. Like arrays, grids consist of a fixed set of <em>bins</em>, or <em>cells</em>. Since we are initially covering two-dimensional data, we use a two-dimensional arrangement of bins and index each bin with two numbers, <em>xbin</em> and <em>ybin</em>, representing the bin numbers along the x-axis and y-axis respectively. <a href="#figure8-7" id="figureanchor8-7">Figure 8-7</a> shows an example grid.</p>
<figure>
<img alt="A two‐by‐two grid over the map. Bins are labeled 0 and 1 along both the X and Y dimension, resulting in a total of four quadrants." class="" height="287" src="image_fi/502604c08/f08007.png" width="281"/>
<figcaption><p><a id="figure8-7">Figure 8-7</a>: A 2×2 grid of spatial data points</p></figcaption>
</figure>
<p>Unlike arrays, we can’t restrict each bin to hold a single value. Grid cells are defined by spatial bounds—a high and low edge along each dimension. No matter how finely we draw the grid, multiple points might fall within the same cell, so we need our bins to store multiple elements apiece. Each bin stores a list of <em>all</em> data points that fall within that bin’s range. </p>
<p>We can visualize the difference between grids and arrays as different forms of refrigerator storage. The egg carton is an array with one individual space for each egg. In contrast, the vegetable drawer is like a grid bin. It contains multiple items of the same type, all vegetables. We might stuff a single drawer with twenty-five onions. The egg carton, on the other hand, contains only a fixed number of eggs, each in its designated place. While <span epub:type="pagebreak" id="Page_120" title="120"/>vegetable drawers may generate intense debates about where to correctly store tomatoes or cucumbers, the bounds of a grid cell are defined with mathematical precision.</p>
<p>Grids use the points’ coordinates to determine their storage, allowing us to use the data’s spatial structure to limit our searches. To see how this is possible, we first need to consider the details of the grid’s structure. </p>
<h3 id="h2-502604c08-0003">Grid Structure</h3>
<p class="BodyFirst">The top-level data structure for our grid contains some extra bookkeeping information. As shown in <a href="#figure8-8" id="figureanchor8-8">Figure 8-8</a>, we need to include multiple pieces of information along each dimension. In addition to the number of bins along the x- and y-dimensions, we must track the spatial bounds along each dimension. We use <code>x_start</code> and <code>x_end</code> to indicate the minimum and maximum values of x included in our grid. Similarly, <code>y_start</code> and <code>y_end</code> capture the spatial bounds for y.</p>
<figure>
<img alt="The grid spans x_start to x_end along the x‐axis and y_start to y_end along the y‐axis. the grid has 6 bins along each dimension, and the resulting bin widths are shown. " class="" height="269" src="image_fi/502604c08/f08008.png" width="342"/>
<figcaption><p><a id="figure8-8">Figure 8-8</a>: A grid with specified starting and ending values along each dimension</p></figcaption>
</figure>
<p>We can derive some top-level information from the number of bins and the spatial bounds, but we often want to store that additional information about the grid for convenience. Precomputing the width of the bins along each dimension simplifies later code:</p>
<pre><code>x_bin_width = (x_end – x_start) / num_x_bins
y_bin_width = (y_end – y_start) / num_y_bins</code></pre>
<p>Other useful information might include the total number of points stored in the grid or the number of empty bins. We can track all this information in a composite data structure. For two-dimensional data, our typical data structure would look like this:</p>
<pre><code>Grid {
    Integer: num_x_bins
    Integer: num_y_bins
    Float: x_start
    Float: x_end
<span epub:type="pagebreak" id="Page_121" title="121"/>    Float: x_bin_width
    Float: y_start
    Float: y_end
    Float: y_bin_width
    Matrix of GridPoints: bins
}</code></pre>
<p>For a fixed-size grid, we can map from a point’s spatial coordinates to the grid’s bin using a simple mathematical computation:</p>
<pre><code>xbin = Floor((x – x_start) / x_bin_width)
ybin = Floor((y – y_start) / y_bin_width)</code></pre>
<p>The switch from “one bin, one value” to “spatial partitioning” has important consequences beyond index mapping. It means that we can no longer store the data as a set of fixed bins in the computer’s memory. Each square could contain an arbitrary number of data points. Each grid square needs its own internal data structure to store its points. One common and effective data structure for storing points within the bin is a linked list like the one in <a href="#figure8-9" id="figureanchor8-9">Figure 8-9</a>. </p>
<figure>
<img alt="A grid shown as a list of four bins (a flattened matrix), each pointing to the start of a linked list of points. The first three bins contain three points, and the final bin (xbin = 1, ybin = 1) includes two points." class="" height="314" src="image_fi/502604c08/f08009.png" width="544"/>
<figcaption><p><a id="figure8-9">Figure 8-9</a>: A representation of the data structure used to store points in a grid</p></figcaption>
</figure>
<p>Each bin stores the pointer to the head of a linked list, which contains all the points in that bin. We accomplish this with another, internal data structure to store individual points: </p>
<pre><code>GridPoint {
    Float: x
    Float: y
    GridPoint: next
}</code></pre>
<p class="BodyContinued">Alternatively, we could use the <code>LinkedListNode</code> data structure from <span class="xref" itemid="xref_target_Chapter 3">Chapter 3</span> and store a pair to represent the <em>x</em>, <em>y</em> coordinates.</p>
<h3 id="h2-502604c08-0004"><span epub:type="pagebreak" id="Page_122" title="122"/>Building Grids and Inserting Points</h3>
<p class="BodyFirst">We construct a grid for our data set by allocating an empty grid data structure and iteratively inserting points using a single <code>FOR</code> loop over the data points. The high-level structure of the grid itself (the spatial bounds and number of bins along each dimension) is fixed at time of creation and does not change with the data added.</p>
<p>As shown in <a href="#listing8-2" id="listinganchor8-2">Listing 8-2</a>, inserting a point consists of finding the correct bin and prepending the new point to the beginning of the linked list for that bin.</p>
<pre><code>GridInsert(Grid: g, Float: x, Float: y):
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> Integer: xbin = Floor((x - g.x_start) / g.x_bin_width)
    Integer: ybin = Floor((y - g.y_start) / g.y_bin_width)
 
    # Check that the point is within the grid.
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> IF xbin &lt; 0 OR xbin &gt;= g.num_x_bins:
        return False
    IF ybin &lt; 0 OR ybin &gt;= g.num_y_bins:
        return False
    
    # Add the point to the front of the list. 
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> GridPoint: next_point = g.bins[xbin][ybin]
    g.bins[xbin][ybin] = GridPoint(x, y)
    g.bins[xbin][ybin].next = next_point

  <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> return True</code></pre>
<p class="CodeListingCaption"><a id="listing8-2">Listing 8-2</a>: A function to insert a new point into a grid</p>
<p>The code first computes the x and y bins for the new point <span aria-label="annotation1" class="CodeAnnotation">❶</span> and confirms that the new point falls within a valid bin <span aria-label="annotation2" class="CodeAnnotation">❷</span>. While it’s always important to confirm that you are accessing an in-bounds array index whenever using an array, spatial data structures present additional concerns. We might not be able to predefine a fixed, finite grid that works for every possible future point. Therefore, it is important to consider what happens when previously unseen points fall outside the range covered by your spatial data structure. In this example, we return a Boolean to indicate whether or not the point could be inserted <span aria-label="annotation4" class="CodeAnnotation">❹</span>. However, you might prefer other mechanisms, such as throwing an exception, depending on the programming language.</p>
<p>Once we have determined that point does fit within our grid, the code finds the appropriate bin. The code prepends the new point to the front of the list, creating a new list if the bin was previously empty <span aria-label="annotation3" class="CodeAnnotation">❸</span>. The function concludes by returning <code>True</code> <span aria-label="annotation4" class="CodeAnnotation">❹</span>. </p>
<h3 id="h2-502604c08-0005">Deleting Points</h3>
<p class="BodyFirst">We can use a similar approach to insertion for deleting points from a grid. One additional difficulty is determining which point in the bin to delete. In many use cases, the user might insert arbitrarily close or even duplicate points into the grid. For example, if we are storing a list of ground coffees <span epub:type="pagebreak" id="Page_123" title="123"/>available to purchase, we might insert multiple points for a single coffee shop. Ideally, we use other identifying information, such as the name or ID number of the coffee, to determine which of the points to delete. In this section, we present the simple and general approach of deleting the first matching point in our linked list. </p>
<p>Due to the limited precision of floating-point variables, we also might not be able to use a direct equality test. In <a href="#listing8-3" id="listinganchor8-3">Listing 8-3</a>, we use a helper function to find a point that is close enough. The <code>approx_equal</code> function returns <code>True</code> if both points are within a threshold distance in both dimensions.</p>
<pre><code>approx_equal(Float: x1, Float: y1, Float: x2, Float: y2):
    IF abs(x1 – x2) &gt; threshold:
        return False
    IF abs(y1 – y2) &gt; threshold:
        return False
    return True</code></pre>
<p class="CodeListingCaption"><a id="listing8-3">Listing 8-3</a>: Code to check whether two data points, represented as a pair of floating-point numbers, are equal</p>
<p>The code checks each dimension independently and compares the distance to a threshold. The threshold will depend on the use case and the numerical precision of your programming language. Generally, we want these thresholds to be just large enough to account for the float’s numerical precision.</p>
<p>Deletion consists of finding the correct bin, traversing the linked list until we find a match, and removing the match by splicing it out of the list. Our delete function returns <code>True</code> if a point was found and deleted and <code>False</code> otherwise.</p>
<pre><code>GridDelete(Grid: g, Float: x, Float: y):
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> Integer: xbin = Floor((x - g.x_start) / g.x_bin_width)
    Integer: ybin = Floor((y - g.y_start) / g.y_bin_width)
 
    # Check that the point is within the grid.
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> IF xbin &lt; 0 OR xbin &gt;= g.num_x_bins:
        return False
    IF ybin &lt; 0 OR ybin &gt;= g.num_y_bins:
        return False
     
    # Check if the bin is empty.
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> IF g.bins[xbin][ybin] == null:
        return False

    # Find the first matching point and remove it.
  <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> GridPoint: current = g.bins[xbin][ybin]
    GridPoint: previous = null
    WHILE current != null:
      <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> IF approx_equal(x, y, current.x, current.y):
          <span aria-label="annotation6" class="CodeAnnotationCode">❻</span> IF previous == null:
                g.bins[xbin][ybin] = current.next
            ELSE:
                previous.next = current.next
<span epub:type="pagebreak" id="Page_124" title="124"/>            return True
      <span aria-label="annotation7" class="CodeAnnotationCode">❼</span> previous = current
        current = current.next
    return False</code></pre>
<p>The code first computes the x and y bins for the new point <span aria-label="annotation1" class="CodeAnnotation">❶</span> and confirms that the new point falls within a valid bin <span aria-label="annotation2" class="CodeAnnotation">❷</span>. Next it checks whether the target bin is empty <span aria-label="annotation3" class="CodeAnnotation">❸</span>, returning <code>False</code> if it is. </p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	While the code above checks a single bin for simplicity, we could theoretically see (extremely rare) edge cases where the deleted point lies right on the bin’s boundary. We could further account for the limited precision of floats in that case with additional checks.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<p>If there are points to check, the code iterates through the list <span aria-label="annotation4" class="CodeAnnotation">❹</span>. Unlike the code for insertion, we track both the current node and the previous one so that we can splice the target node out of the list. The code uses the <code>approx_equal</code> helper function from <a href="#listing8-3">Listing 8-3</a> to test each point <span aria-label="annotation5" class="CodeAnnotation">❺</span>. When it finds a matching point, it splices it out of the list, taking care to correctly handle the special case of the first node in the list <span aria-label="annotation6" class="CodeAnnotation">❻</span>, and returns <code>True</code>. Thus, only the <em>first</em> matching point in the list is removed. If the current point does not match, the search continues to the next node in the list <span aria-label="annotation7" class="CodeAnnotation">❼</span>. If the search finishes the entire list, the function returns <code>False</code> to indicate that no matching node was deleted. </p>
<h2 id="h1-502604c08-0003">Searches Over Grids</h2>
<p class="BodyFirst">Now that we’ve learned how to construct grid data structures, let’s use them to improve our nearest-neighbor searches. First, we examine the problem of pruning grid cells that are too far away, which will allow us to avoid unnecessary computations within grid cells. We then consider two basic searches: a linear scan over all the bins and an expanding search.</p>
<h3 id="h2-502604c08-0006">Pruning Bins</h3>
<p class="BodyFirst">The grid’s spatial structure allows us to limit how many points we need to check, excluding those outside the range we’re interested in (northern Florida). Once we have a candidate neighbor and its associated distance, we can use that distance to <em>prune bins</em><em/>. Before checking the points in a bin, we ask whether <em>any</em> point within the bin’s spatial bounds could be closer than the current best distance. If not, we can ignore the bin.</p>
<p>Determining whether <em>any</em> point within a bin lies within a given distance from our target point may sound like a daunting task. However, if we are using Euclidean distance <span class="GraphicInline"><img alt="i08001" height="30" src="image_fi/502604c08/i08001.png" width="213"/></span>, which we can encapsulate in this simple helper function:</p>
<pre><code>euclidean_dist(Float: x1, Float: y1, Float: x2, Float: y2):
    return sqrt((x1-x2)*(x1-x2) + (y1-y2)*(y1-y2))</code></pre>
<p class="BodyContinued"><span epub:type="pagebreak" id="Page_125" title="125"/>then<em> </em>the test boils down to simple mathematics. We start by finding the closest possible point in the grid cell and use that for our pruning test. Specifically, if the closest possible point in the grid cell is further than our current best candidate, there is no reason to check any of the actual points stored in the bin. They all must be further away. If a target point falls within the cell—that is, if its x and y values are within the cell’s x and y ranges, respectively—the distance to the cell (and thus the closest possible point) is zero. </p>
<p>If the point falls outside the cell, then the closest possible point in the cell must lie on the edge of the cell. <a href="#figure8-10" id="figureanchor8-10">Figure 8-10</a> shows a variety of points outside the grid cell and the corresponding closest points within the cell. For points outside the grid cell, we need to compute the distance to the closest edge point.</p>
<figure>
<img alt="Eight target points shown as gray circles and their corresponding closest points within the cell. The closest points to targets outside the cell are on the cell’s perimeter." class="" height="238" src="image_fi/502604c08/f08010.png" width="231"/>
<figcaption><p><a id="figure8-10">Figure 8-10</a>: Points outside a grid cell (gray circles) and the corresponding closest points within the cell (solid circles)</p></figcaption>
</figure>
<p>We can compute the Euclidean distance between a point and the nearest edge of a grid’s bin by considering each dimension independently. We find the minimum distance needed to shift the x value within the bin’s range and the minimum distance to shift the y value within the bin’s range. For the grid bin (<em>xbin</em>, <em>ybin</em>), the minimum and maximum x and y dimensions are:</p>
<pre><code>x_min = x_start + xbin * x_bin_width
x_max = x_start + (xbin + 1) * x_bin_width
y_min = y_start + ybin * y_bin_width
y_max = y_start + (ybin + 1) * y_bin_width</code></pre>
<p>We can compute the distance as follows (in the case of Euclidean distance):</p>
<figure class="graphic">
<img alt="g08002" height="30" src="image_fi/502604c08/g08002.png" width="169"/></figure>
<p class="BodyContinued">where</p>
<p class="Equation">IF <em>x</em> &lt; <em>x</em>_<em>min</em> THEN <em>x</em><sub>dist</sub><em> = x</em>_<em>min</em> − <em>x</em></p>
<p class="Equation">IF <em>x</em>_<em>min</em> <span class="NSSymbol">≤</span> <em>x</em> <span class="NSSymbol">≤</span> <em>x</em>_<em>max</em> THEN <em>x</em><sub>dist</sub> = 0</p>
<p class="Equation">IF <em>x </em>&gt; <em>x</em>_<em>max</em> THEN <em>x</em><sub>dist</sub> = <em>x</em> – <em>x</em>_<em>max</em></p>
<p class="BodyContinued"><span epub:type="pagebreak" id="Page_126" title="126"/>and</p>
<p class="Equation">IF <em>y</em> &lt; <em>y_min</em> THEN <em>y</em><sub>dist</sub> = <em>y_min</em> − <em>y</em></p>
<p class="Equation">IF <em>y_min</em> <span class="NSSymbol">≤</span> <em>y</em> <span class="NSSymbol">≤</span> <em>y_max</em> THEN <em>y</em><sub>dist</sub> = 0</p>
<p class="Equation">IF <em>y</em> &gt; <em>y_max</em> THEN <em>y</em><sub>dist</sub> = <em>y</em> − <em>y_max</em></p>
<p class="BodyContinued">If the minimum distance to any possible point in the bin is greater than that of our current closest point, then nothing in the bin could replace the current closest point. We can ignore the entire bin!</p>
<p>The code for computing the minimum distance from a point to a bin can be encapsulated into the following helper function. This function implements the mathematical logic above.</p>
<pre><code>MinDistToBin(Grid: g, Integer: xbin, Integer: ybin, Float: x, Float: y):
    # Check that the bin is valid.
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> IF xbin &lt; 0 OR xbin &gt;= g.num_x_bins:
        return Inf
    IF ybin &lt; 0 OR ybin &gt;= g.num_y_bins:
        return Inf

  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> Float: x_min = g.x_start + xbin * g.x_bin_width
    Float: x_max = g.x_start + (xbin + 1) * g.x_bin_width
    Float: x_dist = 0
    IF x &lt; x_min:
      x_dist = x_min - x
    IF x &gt; x_max:
      x_dist = x - x_max

  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> Float: y_min = g.y_start + ybin * g.y_bin_width
    Float: y_max = g.y_start + (ybin + 1) * g.y_bin_width
    Float: y_dist = 0
    IF y &lt; y_min:
      y_dist = y_min - y
    IF y &gt; y_max:
      y_dist = y - y_max
    return sqrt(x_dist*x_dist + y_dist*y_dist)</code></pre>
<p class="CodeListingCaption"><a id="listing8-4">Listing 8-4</a>: A helper function that computes the closest distance from a target point to a given bin</p>
<p>The code starts by checking that the bin indices are valid <span aria-label="annotation1" class="CodeAnnotation">❶</span>. In this example, we use an infinite distance to indicate that the function’s caller has referenced an invalid bin. This logic allows us to use this lookup function in pruning computations that might ask about invalid bins. However, this may lead to confusion: Why is the function returning any distance for an invalid bin? Depending on the usage, it might be preferable to throw an error indicating that the bin indices are invalid. Either way, the function’s behavior should be clearly documented for users.</p>
<p><span epub:type="pagebreak" id="Page_127" title="127"/>The remainder of the code proceeds through the distance logic above for the x and y dimensions (<span aria-label="annotation2" class="CodeAnnotation">❷</span> and <span aria-label="annotation3" class="CodeAnnotation">❸</span>, respectively). The code computes the minimum and maximum values for the bin, compares them with the point’s value along that dimension, and computes the distance.</p>
<p>To visualize this distance test, imagine that a raucous game of catch sends a ball over our fence into the yard of our friendly, but exceedingly lazy, neighbor. They will return the ball, of course, but without exerting any more effort than absolutely necessary. What is the shortest distance they need to throw the ball in order for it to (just barely) return to our yard? If their longitude already falls within the bounds of our yard, they will throw in a pure north or south direction so as not to add any unnecessary east/west distance. In the end, their throw always lands exactly on the fence such that it falls back into our property. Our neighbor may be lazy, but they have some impressive throwing skills. </p>
<h3 id="h2-502604c08-0007">Linear Scan Over Bins</h3>
<p class="BodyFirst">The simplest approach to searching a grid would iterate through all the grid’s bins using a linear scan and only check those that could contain a potential nearest neighbor. This is not a particularly good algorithm, but it provides an easy introduction to working with and pruning bins. </p>
<p>The linear search algorithm simply applies the aforementioned minimum distance test to each bin before checking its contents:</p>
<pre><code>GridLinearScanNN(Grid: g, Float: x, Float: y): 
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> Float: best_dist = Inf
    GridPoint: best_candidate = null

    Integer: xbin = 0
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> WHILE xbin &lt; g.num_x_bins:
        Integer: ybin = 0
        WHILE ybin &lt; g.num_y_bins:

            # Check if we need to process the bin.
          <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> IF MinDistToBin(g, xbin, ybin, x, y) &lt; best_dist:

                # Check every point in the bin's linked list.
                GridPoint: current = g.bins[xbin][ybin]
              <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> WHILE current != null:
                    Float: dist = euclidean_dist(x, y, current.x, current.y)
                  <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> IF dist &lt; best_dist:
                        best_dist = dist
                        best_candidate = current
                    current = current.next
            ybin = ybin + 1
        xbin = xbin + 1
  <span aria-label="annotation6" class="CodeAnnotationCode">❻</span> return best_candidate</code></pre>
<p class="CodeListingCaption"><a id="listing8-5">Listing 8-5</a>: A nearest-neighbor search that uses a linear scan over a grids bin with pruning tests on each bin.</p>
<p><span epub:type="pagebreak" id="Page_128" title="128"/>The code starts by setting the best distance to infinity to indicate that no best point has been found so far <span aria-label="annotation1" class="CodeAnnotation">❶</span>. Then the algorithm scans through each bin using a pair of nested <code>WHILE</code> loops that iterate over the x and y bins <span aria-label="annotation2" class="CodeAnnotation">❷</span>. Before checking the individual points in the bin, the code performs a minimum distance test to check whether <em>any</em> point in the bin could be a better neighbor <span aria-label="annotation3" class="CodeAnnotation">❸</span>. If the bin may contain better neighbors, the code uses a third <code>WHILE</code> loop to iterate through the linked list of points in the bin <span aria-label="annotation4" class="CodeAnnotation">❹</span>. It tests the distance to each point and compares it with the best distance found so far <span aria-label="annotation5" class="CodeAnnotation">❺</span>. The function completes by returning the best candidate found, which may be <code>null</code> if the grid is empty <span aria-label="annotation6" class="CodeAnnotation">❻</span>.</p>
<p>The algorithm in <a href="#listing8-5" id="listinganchor8-5">Listing 8-5</a> allows us to prune out entire bins, along with all the points they contain, whenever we determine that minimum distance to any point in the bin is greater than the distance to the best point seen so far. If we have a large number of points per bin, this can lead to significant savings. However, if the grid is sparsely populated, we might end up paying more to check each of the bins than we would have if we’d checked each point individually. </p>
<p>Unlike the <code>GridInsert</code> function in <a href="#listing8-2">Listing 8-2</a>, our linear scan works with target points inside or outside the grid’s spatial bounds. <code>GridLinearScanNN</code> does not need to map the target point to a bin and therefore does not care if the target is on the grid itself. It will still return the nearest neighbor from the grid (or <code>null</code> if the grid is empty). This provides an additional level of flexibility to our nearest-neighbor search that can be useful when encountering new, non-typical targets. </p>
<h3 id="h2-502604c08-0008">Ideal Expanding Search over Bins</h3>
<p class="BodyFirst">While the linear scan algorithm allows us to prune out entire bins based on their minimum distance to our target point, we’re still not using all the spatial information at our disposal. We waste a significant amount of computation by testing bins that are far from our target point. We can do better if we prioritize bins by their proximity to their target, first searching the bins closest to our target point and halting the search when the remaining bins are further than the nearest neighbor we have found so far. We call this an <em>expanding search</em>, because we effectively expand out from the bin containing the target point until we have found the nearest neighbor.</p>
<p>To visualize this improved scanning method, imagine a panicked search for our car keys in the morning. We start with the area (comparable to a grid cell) where the car keys would be if we had stored them correctly. We inspect every inch of the kitchen counter before admitting that we must have misplaced the keys. Spiraling out to other parts of the house (that is, other bins), we check nearby locations, such as the coffee table and the floor, before venturing further and further away. This search continues, exploring less and less likely locations, until we find the keys mysteriously sitting in the sock drawer. </p>
<p>For an example of an expanding scan in action, consider our map overlaid with a four-by-four grid, as shown in <a href="#figure8-11" id="figureanchor8-11">Figure 8-11</a>. We find the closest bin to our target point by asking, “Into which bin does our target point fall?” <span epub:type="pagebreak" id="Page_129" title="129"/>and using the grid index-mapping equations. Since it is possible that our target point falls outside the grid, we might also need to shift the computed bin indices into the valid range. In <a href="#figure8-11">Figure 8-11</a>, the target point is in the third bin up in the leftmost column (<em>xbin</em> = 0 and <em>ybin</em> = 2 in our notation).</p>
<figure>
<img alt="The map points from figure 8‐3 placed in a four‐by‐four grid." class="" height="264" src="image_fi/502604c08/f08011.png" width="265"/>
<figcaption><p><a id="figure8-11">Figure 8-11</a>: A 4×4 grid of two-dimensional points</p></figcaption>
</figure>
<p>We can start our search in the target point’s bin and test every point in that bin. As long as the bin isn’t empty, we are guaranteed to find our first <em>candidate</em> nearest neighbor, as shown in <a href="#figure8-12" id="figureanchor8-12">Figure 8-12</a>. Unfortunately, since we’re not organizing or sorting the points within each bin, we can’t do better in this case than a linear scan through that bin’s points. Of course, if the initial bin is empty, we must progress our search outward to neighboring bins, incrementally trying further and further bins until we find one containing a data point to be our candidate nearest neighbor.</p>
<figure>
<img alt="The four‐by‐four grid of map points from Figure 8‐11 with a dashed line to the candidate nearest neighbor. The first candidate falls within the same bin." class="" height="264" src="image_fi/502604c08/f08012.png" width="265"/>
<figcaption><p><a id="figure8-12">Figure 8-12</a>: An initial candidate nearest neighbor found in the same bin as the target point</p></figcaption>
</figure>
<p>Once we obtain this initial candidate for nearest neighbor, we are still not done. The candidate is just that—a candidate. It’s possible there could be a closer point in one of the adjacent bins. This is more likely if our target point is near the edge of a bin. In <a href="#figure8-13" id="figureanchor8-13">Figure 8-13</a>, the dashed circle represents the space of all points that are closer to or at the same distance from the current candidate. Any other point that falls within the circle could be the true nearest neighbor. The shaded grid cells are those that overlap this region.</p>
<span epub:type="pagebreak" id="Page_130" title="130"/><figure>
<img alt="The four‐by‐four grid of map points from Figure 8‐11 with a dotted circle showing the region of space with distance closer than or equal to the current candidate neighbor. Four grid cells overlapping this circle are shaded, indicating that they could contain a closer neighbor." class="" height="264" src="image_fi/502604c08/f08013.png" width="265"/>
<figcaption><p><a id="figure8-13">Figure 8-13</a>: A candidate nearest neighbor and the gridcells that could contain points closer to the target </p></figcaption>
</figure>
<p>To visualize why we need to continue to check other bins, imagine you want to determine the closest person to you at an outdoor party. You’re telling a particularly embarrassing story, involving the use of spoiled milk in your coffee, and want to make sure that only the intended audience hears you. Your best friend standing by the house might appear to be closest to you, but, if you’re near your fence, you also need to consider people on the other side. If your neighbor is planting flowers along their side of the fence, they might actually be closer and hear all of the humiliating details. You can’t discount them because there is a fence in the way. This is why we always check neighboring bins—and why you should always be careful about telling embarrassing stories in public.</p>
<p>We continue to expand out the search to include <em>all</em> nearby bins until we can guarantee that no possible point in the remaining bins could be closer than our candidate nearest neighbor. Once we have checked all the bins within the radius of our nearest-neighbor candidate, we can ignore any bins beyond that. We don’t even need to check their distance.</p>
<p>The tradeoff for this improved grid search is algorithmic complexity. Instead of scanning across every one of the bins—an algorithm we could implement with a nested pair of <code>FOR</code> loops—the optimized search spirals out from a single bin, exploring further and further away until we can prove that none of the unexplored bins could contain a better neighbor. This requires additional logic in the search order (outward spiral), bounds checking (avoiding testing bins off the edge of the grid), and termination criteria (knowing when to stop). The next section presents a simple example of an expanding search for illustrative purposes.</p>
<h3 id="h2-502604c08-0009">Simplified Expanding Search </h3>
<p class="BodyFirst">Let’s consider a simplified (and non-optimized) version of an expanding search that moves outward in a diamond-shaped pattern. Instead of executing a perfect spiral out from the initial bin, the search uses an increasing distance from an initial bin. For simplicity of implementation, we will use a Manhattan distance on the grid indices that counts the steps between grid cells:</p>
<p class="Equation"><em>d</em> = |<em>xbin</em><sub>1 </sub>− <em>xbin</em><sub>2</sub>| + |<em>ybin</em><sub>1 </sub>− <em>ybin</em><sub>2</sub>|</p>
<p class="BodyContinued"><span epub:type="pagebreak" id="Page_131" title="131"/>While this search pattern is unlikely to be efficient for grids with drastically different bin widths along each dimension, it provides an easy-to-follow illustration.</p>
<p><a href="#figure8-14" id="figureanchor8-14">Figure 8-14</a> shows the first four iterations of the search. During the first iteration in <a href="#figure8-14">Figure 8-14</a>(a), we search the bin containing the target point (zero steps away). During the next iteration in <a href="#figure8-14">Figure 8-14</a>(b), we search all bins a single step away. On each subsequent iteration, we search all the bins that are one step further out.</p>
<figure>
<img alt="Four iterations of an example expanding search. In the first iteration, a single bin containing the target point is shaded. In the second iteration, four bins, all one step away, are shaded. In the third iteration, eight bins, all two steps away, are shaded." class="" height="582" src="image_fi/502604c08/f08014.png" width="558"/>
<figcaption><p><a id="figure8-14">Figure 8-14</a>: The first four iterations of a simplified expanding search on a grid</p></figcaption>
</figure>
<p>We start with a helper function that checks whether any points within a specified bin are closer to our target point (<code>x</code>,  <code>y</code>) than a given <code>threshold</code>. This function encodes our linear scan through the bin’s points. If there is at least one point closer than <code>threshold</code>, the function returns the closest such point. The use of a threshold value will allow us to use the helper function to compare the bin’s points to the best candidates from other bins.</p>
<pre><code>GridCheckBin(Grid: g, Integer: xbin, Integer: ybin, 
             Float: x, Float: y, Float: threshold):
    # Check that it is a valid bin and within the pruning threshold.
  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> IF xbin &lt; 0 OR xbin &gt;= g.num_x_bins:
        return null
    IF ybin &lt; 0 OR ybin &gt;= g.num_y_bins:
        return null

<span epub:type="pagebreak" id="Page_132" title="132"/>    # Check each of the points in the bin one by one.
    GridPoint: best_candidate = null
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> Float: best_dist = threshold
    GridPoint: current = g.bins[xbin][ybin]
  <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> WHILE current != null:
      <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> Float: dist = euclidean_dist(x, y, current.x, current.y)
        IF dist &lt; best_dist:
            best_dist = dist
            best_candidate = current
        current = current.next
  <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> return best_candidate</code></pre>
<p class="CodeListingCaption"><a id="listing8-6">Listing 8-6</a>: A helper function that returns the closest point in a bin to the target point as long as it is below the given threshold</p>
<p>The code starts with a safety check that we are accessing a valid bin <span aria-label="annotation1" class="CodeAnnotation">❶</span>. If not, it returns <code>null</code> to indicate that there are no valid points. The code then uses a <code>WHILE</code> loop to iterate through each point in the bin <span aria-label="annotation3" class="CodeAnnotation">❸</span>, computing its distance from the target point, comparing it to the best distance seen so far, and saving it as the new best candidate if it is closer <span aria-label="annotation4" class="CodeAnnotation">❹</span>. The code finishes by returning the closest point <span aria-label="annotation5" class="CodeAnnotation">❺</span>. Since the code initially set <code>best_dist</code> to the <code>threshold</code> value before checking any points <span aria-label="annotation2" class="CodeAnnotation">❷</span>, it will only mark points with a distance less than <code>threshold</code> as new candidates. The function returns <code>null</code> if none of the bin’s points are closer than <code>threshold</code>. </p>
<p>The code for performing the expanding search works by iterating through a different number of steps and checking all bins that can be reached in that number of steps. As in previous searches, we track the best candidate seen so far. The search concludes after iteration <em>d</em> if there are no valid grid cells <em>d</em> steps away that could contain a closer neighbor.</p>
<pre><code>GridSearchExpanding(Grid: g, Float: x, Float: y):
    Float: best_d = Inf
    GridPoint: best_pt = null

  <span aria-label="annotation1" class="CodeAnnotationCode">❶</span> # Find the starting x and y bins for our search.
    Integer: xb = Floor((x - g.x_start) / g.x_bin_width)
    IF xb &lt; 0:
        xb = 0
    IF xb &gt;= g.num_x_bins:
        xb = g.num_x_bins - 1

    Integer: yb = Floor((y - g.y_start) / g.y_bin_width)
    IF yb &lt; 0:
        yb = 0
    IF yb &gt;= g.num_y_bins:
        yb = g.num_y_bins - 1

<span epub:type="pagebreak" id="Page_133" title="133"/>    Integer: steps = 0
    Boolean: explore = True
  <span aria-label="annotation2" class="CodeAnnotationCode">❷</span> WHILE explore:
        explore = False

      <span aria-label="annotation3" class="CodeAnnotationCode">❸</span> Integer: xoff = -steps
        WHILE xoff &lt;= steps:
          <span aria-label="annotation4" class="CodeAnnotationCode">❹</span> Integer: yoff = steps - abs(xoff)
          <span aria-label="annotation5" class="CodeAnnotationCode">❺</span> IF MinDistToBin(g, xb + xoff, yb - yoff, x, y) &lt; best_d:
              <span aria-label="annotation6" class="CodeAnnotationCode">❻</span> GridPoint: pt = GridCheckBin(g, xb + xoff, yb - yoff, 
                                             x, y, best_d)
                IF pt != null:
                    best_d = euclidean_dist(x, y, pt.x, pt.y)
                    best_pt = pt
              <span aria-label="annotation7" class="CodeAnnotationCode">❼</span> explore = True

          <span aria-label="annotation8" class="CodeAnnotationCode">❽</span> IF (MinDistToBin(g, xb + xoff, yb + yoff, x, y) &lt; best_d
                AND yoff != 0):
                GridPoint: pt = GridCheckBin(g, xb + xoff, yb + yoff, 
                                             x, y, best_d)
                IF pt != null:
                    best_d = euclidean_dist(x, y, pt.x, pt.y)
                    best_pt = pt
              <span aria-label="annotation9" class="CodeAnnotationCode">❾</span> explore = True

            xoff = xoff + 1
        steps = steps + 1
    return best_pt</code></pre>
<p>This code starts by finding the closest bin within the grid to our target point, taking care to map targets outside the grid to their closest bin in the grid <span aria-label="annotation1" class="CodeAnnotation">❶</span>. The resulting bin (<code>xb</code>, <code>yb</code>) will be the starting point for the search. By mapping bins outside the grid to a valid bin, the function can return the nearest neighbor for target points that lie outside the grid itself.</p>
<p>The code then uses a <code>WHILE</code> loop to explore outward from this initial bin by increasing amounts <span aria-label="annotation2" class="CodeAnnotation">❷</span>. The variable <code>steps</code> tracks the distance used for the current iteration. The <code>WHILE</code> loop is conditioned on the variable <code>explore</code>, which indicates that the next iteration may include a valid bin and we should thus continue exploring at the next value of <code>steps</code>. As we will see shortly, the <code>WHILE</code> loop terminates as soon as it completes a full iteration where <em>none</em> of the bins visited could have held a closer neighbor.</p>
<p>Within the main <code>WHILE</code> loop, the code iterates across the different x-index offsets from <code>-steps</code> to <code>steps</code> as though scanning horizontally across the grid <span aria-label="annotation3" class="CodeAnnotation">❸</span>. The total number of steps in the x-direction and y-direction are fixed by <code>steps</code>, so the code can programmatically compute the remaining number of steps to use in the (positive or negative) y-direction <span aria-label="annotation4" class="CodeAnnotation">❹</span>. Starting with the negative y-direction, the code uses <code>MinDistToBin</code> from <a href="#listing8-4" id="listinganchor8-4">Listing 8-4</a> <span epub:type="pagebreak" id="Page_134" title="134"/>to check whether the bin indices are valid and, if so, determine the distance to that bin <span aria-label="annotation5" class="CodeAnnotation">❺</span>. It can skip any bins that are invalid or too far away. If the bin could contain a closer point than our current candidate, the code uses <code>GridCheckBin</code> from <a href="#listing8-6" id="listinganchor8-6">Listing 8-6</a> to check for such a point <span aria-label="annotation6" class="CodeAnnotation">❻</span>. Whenever a closer point has been found, the code saves it as the new closest candidate and updates its estimate of the closest distance. The second <code>IF</code> block performs the same checks in the positive y-direction as long as the y-offset is not zero (in which case we have already checked the bin in the negative y-direction) <span aria-label="annotation8" class="CodeAnnotation">❽</span>. </p>
<p>During an iteration of the outer <code>WHILE</code> loop <span aria-label="annotation2" class="CodeAnnotation">❷</span>, the code resets <code>explore</code> to <code>False</code>. It later updates <code>explore</code> to <code>True</code> if any of the calls to <code>MinDistToBin</code> indicate that a bin could contain a closer neighbor (<span aria-label="annotation7" class="CodeAnnotation">❼</span> and <span aria-label="annotation9" class="CodeAnnotation">❾</span>). Thus, the outer loop continues until it reaches a number of steps where every bin is either further than <code>best_d</code> or lies off the grid (and is therefore invalid). While other termination criteria may provide more exact tests and terminate earlier, we use this rule in the code due to its simplicity. </p>
<h2 id="h1-502604c08-0004">The Importance of Grid Size</h2>
<p class="BodyFirst">The size of our grid’s bins has a massive impact on the efficiency of our search. The larger our bins, the more points we may need to check per bin. Remember that our grid searches still do a linear scan through the points within each visited bin. However, partitioning the grid into finer bins has tradeoffs both in terms of memory and the number of empty bins we may encounter. As we shrink the size of the grid’s bins, we often need to search more individual bins before we even find the first candidate nearest neighbor and the cost of checking bins increases.</p>
<p><a href="#figure8-15" id="figureanchor8-15">Figure 8-15</a> shows an extreme case where the grid is too fine.</p>
<figure>
<img alt="The map points from figure 8‐3 placed in a 17‐by‐17 grid. There are now 36 bins that are closer than the nearest neighbor." class="" height="257" src="image_fi/502604c08/f08015.png" width="257"/>
<figcaption><p><a id="figure8-15">Figure 8-15</a>: A fine grid in which most of the bins are empty</p></figcaption>
</figure>
<p>In <a href="#figure8-15">Figure 8-15</a>, we must search 36 bins in order to find the nearest neighbor. This is clearly more expensive than the example in <a href="#figure8-13">Figure 8-13</a>, where we only needed to check four bins and two individual points. Sadly, it might even be more expensive than the linear scan search, which checked every one of the 11 data points.</p>
<p><span epub:type="pagebreak" id="Page_135" title="135"/>Consider this in the context of our search for coffee shops. If we partition the space too finely, such as in 1 m by 1 m squares, we’ll be facing a grid that contains mostly empty bins. If we partition the space more coarsely, such as 5 km by 5 km squares, we might bucket entire cities and their multitudes of coffee shops in a single bin while still leaving (to our utmost horror) a large number of bins nearly or completely empty.</p>
<p>The optimal grid size often depends on multiple factors, including the number of points and their distribution. More complex techniques, such as non-uniform grids, can be used to dynamically adapt to the data. In the next chapter, we will consider several tree-based data structures that dynamically enable this type of adaptation. </p>
<h2 id="h1-502604c08-0005">Beyond Two Dimensions</h2>
<p class="BodyFirst">The grid-based techniques developed for two dimensions can be scaled to higher dimensional data as well. We might need to search a multi-floor office building for the closest available conference room. We can search for nearest neighbors in three-dimensional data by incorporating the <em>z</em> coordinate into our distance computation:</p>
<figure class="graphic">
<img alt="g08003" height="30" src="image_fi/502604c08/g08003.png" width="297"/></figure>
<p class="BodyContinued">Or, more generally, we can define Euclidean distance over <em>d</em>-dimensional data as:</p>
<figure class="graphic">
<img alt="g08004" height="31" src="image_fi/502604c08/g08004.png" width="239"/></figure>
<p class="BodyContinued">where <em>x</em><sub>i</sub>[<em>d</em>]<sub> </sub>is the <em>d</em>th dimension of the <em>i</em>th data point. </p>
<p>Higher-dimensional data comes with another challenge for the grid-based approach we’ve considered in this chapter: it requires us to partition the space along more dimensions. The space required to store such data structures explodes quickly as we consider higher dimensions. For data with <em>D</em> dimensions and <em>K</em> bins per dimension, we need <em>K</em><sup><em>D</em></sup> individual bins! This can require a huge amount of memory. <a href="#figure8-16" id="figureanchor8-16">Figure 8-16</a> shows a three-dimensional example, a 5×5×5 grid, which already includes a large number of individual bins. </p>
<p>Worse, as we increase the number of grid bins, we’re likely increasing the percentage of empty bins. Checking those empty bins is wasted work. For this reason, grids aren’t ideal for higher-dimensional problems. In the next chapter, we will introduce a better approach for scaling to higher-dimensional data—the k-d tree.</p>
<p>While it is difficult to think of an everyday spatial problem using more than three dimensions, we can use our nearest-neighbor formulation on data beyond spatial points. In the next section, we will see how nearest-neighbor search can be used to help us find similar coffee or days with similar weather conditions.</p>
<span epub:type="pagebreak" id="Page_136" title="136"/><figure>
<img alt="A five‐by‐five‐by‐five grid for three‐dimensional points." class="" height="393" src="image_fi/502604c08/f08016.png" width="393"/>
<figcaption><p><a id="figure8-16">Figure 8-16</a>: A grid of three-dimensional points</p></figcaption>
</figure>
<h2 id="h1-502604c08-0006">Beyond Spatial Data</h2>
<p class="BodyFirst">Spatial data, such as locations on a map, provides a simple visual example for both nearest-neighbor search and grids themselves. We’re accustomed to thinking about locations in terms of proximity, since we regularly ask ourselves questions like “What is the closest gas station?” or “Where is the closest hotel to the conference center?” Yet the nearest-neighbor problem extends beyond spatial data. </p>
<p>Let’s consider the critical problem of selecting the next-best brand of coffee to purchase when our favorite brand is out of stock. To find something similar to our favorite brew, we might consider what attributes we liked about that coffee, such as strength or acidity level, and then look for other coffees with similar attributes. We can extend nearest-neighbor search to find these “close” coffees. To do so, we first record every coffee we have ever sampled in a coffee log, noting such properties as strength and acidity, as shown in <a href="#figure8-17" id="figureanchor8-17">Figure 8-17</a>.</p>
<p>Over the years, we build a comprehensive mapping of the coffee landscape. Performing a nearest-neighbor search on this data allows us to find varieties of coffee similar to a target value. Looking for a strong, low-acidity brew to fuel the hurried work before a tight deadline? We can picture exactly the coffee we want, that sublime brew we had once in Hawai’i. Unfortunately, the upcoming deadline does not provide enough slack to justify a quick trip to Hawai’i. But have no fear! We use our thorough analysis of the coffee’s attributes, as captured in our coffee log, to define a search target and then search for a local brand that might be similar enough. </p>
<span epub:type="pagebreak" id="Page_137" title="137"/><figure>
<img alt="On the left is a two‐dimensional plot of data points with one axis labeled strength and the other acidity. On the right is the same points with a grid overlaid." class="" height="371" src="image_fi/502604c08/f08017b.png" width="734"/>
<figcaption><p><a id="figure8-17">Figure 8-17</a>: An example of coffee attributes as two-dimensional data (left) and those points in a grid (right)</p></figcaption>
</figure>
<p>To perform this search, we just need a way to compute distances for attributes like coffee strength or acidity. The nearest-neighbor algorithm relies on our ability to distinguish “near” versus “far” neighbors. While it is possible to define distance measures over other types, such as strings, we restrict our discussion in this chapter to real-valued attributes for consistency.</p>
<p>With spatial data points, we have simple standard measures of the distances between two points (<em>x</em><sub>1</sub><em>, y</em><sub>1</sub>) and (<em>x</em><sub>2</sub><em>, y</em><sub>2</sub>), such as the Euclidean distance used earlier<em>.</em> But the optimal distance measure for any problem will depend on the problem itself. When evaluating brands of coffee, we might want to weight the attributes differently in different situations. Before our impending deadline, caffeine content takes precedence over factors such as the acidity level.</p>
<p>One common distance measure for non-spatial data is weighted Euclidean distance:</p>
<figure class="graphic">
<img alt="g08005" height="31" src="image_fi/502604c08/g08005.png" width="268"/></figure>
<p class="BodyContinued">where <em>x</em><sub><em>i</em></sub>[<em>d</em>]<sub> </sub>is the <em>d</em>th dimension of the <em>i</em>th data point and <em>w</em><sub><em>d</em></sub><em> </em>is the weighting for the <em>d</em>th dimension. This formulation allows us to weight the impact of the different dimensions. In this case, we might set the weight of caffeine content to twice that of acidity, skewing the search toward coffees that are similarly caffeinated. We can even vary the weights per search.</p>
<p>Of course, our search makes no guarantees as to the suitability of other aspects of the coffee. We’re only measuring proximity along the specified dimensions. If we are searching for an everyday coffee by matching only strength and acidity, then we do not consider roast level, batch size, growing conditions, caffeine content, or even the concentration of nutrients in the soil. If the nearest neighbor turns out to be decaf, our search wouldn’t <span epub:type="pagebreak" id="Page_138" title="138"/>account for this travesty. We’d be left with substandard coffee and tears of disappointment. It is important to make sure your distance computation takes into consideration all the dimensions of interest.  </p>
<h2 id="h1-502604c08-0007">Why This Matters</h2>
<p class="BodyFirst">Nearest-neighbor search allows us to find points that are “close” to some target value, whether spatial or non-spatial. From an algorithmic point of view, nearest-neighbor search moves us from searching for an exact target to searching based on distance metrics. The details of search get more complex as we step away from one-dimensional data sets into the realm of multidimensional data. As we saw with the shift from arrays to grids, this extension opens a range of new questions in terms of how we organize and search the data. It’s no longer possible to consider a simple ordering, as we did with a binary search for one-dimensional data. We need to adapt our data structures to a new type of multidimensional structure. Grids provide a new way to structure data based on aggregating points within the same spatial regions into the same bin. </p>
<p>At the same time, grids illustrate a different structure than the one-bucket, one-value structure we have seen with arrays. Grids use linked-list or other internal data structures to store multiple values per bin, a technique we will reuse in future chapters. By using this structure, grids also introduce a new tradeoff to consider—the size of the bins. By increasing the size of the bins, we can shift cost from evaluating many small bins to scanning through a large number of points per bin. Choosing the right number of bins is an example of the common task of <em>tuning</em> our data structure for the specific problem at hand. </p>
<p>In the next chapter, we’ll take spatial partitioning further by combining the adaptive properties of trees with the spatial properties of grids. In doing so, we’ll address some of the major drawbacks of grids—and make the search for a good cup of coffee significantly more efficient. </p>
</section>
</div></body></html>