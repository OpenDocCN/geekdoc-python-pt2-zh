- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LINEAR ALGEBRA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Formally, linear algebra is the study of linear equations, in which the highest
    power of the variable is one. However, for our purposes, *linear algebra* refers
    to multidimensional mathematical objects—like vectors and matrices—and operations
    on them. This is how linear algebra is typically applied in deep learning, and
    how data is manipulated in programs that implement deep learning algorithms. By
    making this distinction, we are throwing away a massive amount of fascinating
    mathematics, but as our goal is to understand the mathematics used and applied
    in deep learning, we can hopefully be forgiven.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll introduce the objects used in deep learning, specifically
    scalars, vectors, matrices, and tensors. As we’ll see, all of these objects are
    actually tensors of various orders. We’ll discuss tensors from a mathematical,
    notational perspective and then experiment with them using NumPy. NumPy was explicitly
    designed to add multidimensional arrays to Python, and they are good, though incomplete,
    analogues for the mathematical objects we’ll work with in this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: We’ll spend the bulk of the chapter learning how to do arithmetic with tensors,
    which is of fundamental importance in deep learning. Most of the effort in implementing
    highly performant deep learning toolkits involves finding ways to do arithmetic
    with tensors as efficiently as possible.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Scalars, Vectors, Matrices, and Tensors
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s introduce our cast of characters. I’ll relate them to Python variables
    and NumPy arrays to show how we’ll implement these objects in code. Then I’ll
    present a handy conceptual mapping between tensors and geometry.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Scalars
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even if you’re not familiar with the word, you’ve known what a scalar is since
    the day you first learned to count. A *scalar* is just a number, like 7, 42, or
    π. In expressions, we’ll use *x* to mean a scalar, that is, the ordinary notation
    used for variables. To a computer, a scalar is a simple numeric variable:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s = 66'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '>>> s'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '66'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *vector* is a 1D array of numbers. Mathematically, a vector has an orientation,
    either horizontal or vertical. If horizontal, it’s a *row vector*. For example,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: is a row vector of three elements or components. Note, we’ll use *x*, a lowercase
    letter in bold, to mean a vector.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, vectors are usually assumed to be *column vectors*,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: where ***y*** has four components, making it a four-dimensional (4D) vector.
    Notice that in [Equation 5.1](ch05.xhtml#ch05equ01) we used square brackets, whereas
    in [Equation 5.2](ch05.xhtml#ch05equ02) we used parentheses. Either notation is
    acceptable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we usually implement vectors as 1D arrays:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '>>> import numpy as np'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '>>> x = np.array([1,2,3])'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(x)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[1 2 3]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(x.reshape((3,1)))'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[2]'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[3]]'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ve used reshape to turn the three-element row vector into a column
    vector of three rows and one column.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The components of a vector are often interpreted as lengths along a set of coordinate
    axes. For example, a three-component vector might be used to represent a point
    in 3D space. In this vector,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '***x*** = [*x*, *y*, *z*]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '*x* could be the length along the x-axis, *y* the length along the y-axis,
    and *z* the length along the z-axis. These are the Cartesian coordinates and serve
    to uniquely identify all points in 3D space.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: However, in deep learning, and machine learning in general, the components of
    a vector are often unrelated to each other in any strict geometric sense. Rather,
    they’re used to represent *features*, qualities of some sample that the model
    will use to attempt to arrive at a useful output, like a class label, or a regression
    value. That said, the vector representing the collection of features, called the
    *feature vector*, is sometimes thought about geometrically. For example, some
    machine learning models, like *k*-nearest neighbors, interpret the vector as representing
    some coordinate in geometric space.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: You’ll often hear deep learning people discuss the *feature space* of a problem.
    The feature space refers to the set of possible inputs. The training set for a
    model needs to accurately represent the feature space of the possible inputs the
    model will encounter when used. In this sense, the feature vector is a point,
    a location in this *n*-dimensional space where *n* is the number of features in
    the feature vector.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *matrix* is a 2D array of numbers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/105equ01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: The elements of ***A*** are subscripted by the row number and column number.
    The matrix ***A*** has three rows and four columns, so we say that it’s a 3 ×
    4 matrix, where 3 × 4 is the *order* of the matrix. Notice that ***A*** uses subscripts
    starting with 0\. Math texts often begin with 1, but increasingly, they’re using
    0 so that there isn’t an offset between the math notation and the computer representation
    of the matrix. Note, also, that we’ll use ***A***, an uppercase letter in bold,
    to mean a matrix.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, matrices are represented as 2D arrays:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '>>> A = np.array([[1,2,3],[4,5,6],[7,8,9]])'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(A)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[[1 2 3]'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[4 5 6]'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[7 8 9]]'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.arange(12).reshape((3,4)))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 0 1 2 3]'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[ 4 5 6 7]'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[ 8 9 10 11]]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: To get element *a*[12] of ***A*** in Python, we write A[1,2]. Notice that when
    we printed the arrays, there was an extra [ and ] around them. NumPy uses these
    brackets to indicate that the 2D array can be thought of as a row vector in which
    each element is itself a vector. In Python-speak, this means that a matrix can
    be thought of as a list of sublists in which each sublist is of the same length.
    Of course, this is exactly how we defined A to begin with.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of vectors as matrices with a single row or column. A column vector
    with three elements is a 3 × 1 matrix: it has three rows and one column. Similarly,
    a row vector of four elements acts like a 1 × 4 matrix: it has one row and four
    columns. We’ll make use of this observation later.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将向量看作是具有单行或单列的矩阵。例如，一个包含三个元素的列向量是一个3 × 1的矩阵：它有三行一列。类似地，一个包含四个元素的行向量相当于一个1
    × 4的矩阵：它有一行四列。我们稍后会用到这个观察结果。
- en: Tensors
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量
- en: 'A scalar has no dimensions, a vector has one, and a matrix has two. As you
    might suspect, we don’t need to stop there. A mathematical object with more than
    two dimensions is colloquially referred to as a *tensor*. When necessary, we’ll
    represent tensors like this: T, as a sans serif capital letter.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 标量没有维度，向量有一维，矩阵有二维。正如你可能猜到的，我们不止于此。一个具有超过两维的数学对象通常被称为*张量*。在必要时，我们会用T表示张量，T是一个无衬线的大写字母。
- en: The number of dimensions a tensor has defines its *order*, which is not to be
    confused with the order of a matrix. A 3D tensor has order 3\. A matrix is a tensor
    of order 2\. A vector is an order-1 tensor, and a scalar is an order-0 tensor.
    When we discuss the flow of data through a deep neural network in [Chapter 9](ch09.xhtml#ch09),
    we’ll see that many toolkits use tensors of order 4 (or more).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的维度数量定义了它的*阶*，这与矩阵的阶不同。一个三维张量的阶是3，一个矩阵是阶为2的张量，一个向量是阶为1的张量，一个标量是阶为0的张量。当我们在[第9章](ch09.xhtml#ch09)讨论深度神经网络中的数据流时，我们会看到许多工具包使用阶为4（或更高）的张量。
- en: 'In Python, NumPy arrays with three or more dimensions are used to implement
    tensors. For example, we can define an order-3 tensor in Python as shown below:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，具有三维或更多维的NumPy数组用于实现张量。例如，我们可以如下定义一个阶为3的张量：
- en: '>>> t = np.arange(36).reshape((3,3,4))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> t = np.arange(36).reshape((3,3,4))'
- en: '>>> print(t)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(t)'
- en: '[[[ 0 1 2 3]'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[[[ 0 1 2 3]'
- en: '[ 4 5 6 7]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 4 5 6 7]'
- en: '[ 8 9 10 11]]'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 8 9 10 11]]'
- en: '[[12 13 14 15]'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[[12 13 14 15]'
- en: '[16 17 18 19]'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[16 17 18 19]'
- en: '[20 21 22 23]]'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[20 21 22 23]]'
- en: '[[24 25 26 27]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24 25 26 27]'
- en: '[28 29 30 31]'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[28 29 30 31]'
- en: '[32 33 34 35]]]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[32 33 34 35]]]'
- en: 'Here, we use np.arange to define t to be a vector of 36 elements holding the
    numbers 0 . . . 35\. Then, we immediately reshape the vector into a tensor of
    3 × 3 × 4 elements (3 × 3 × 4 = 36). One way to think of a 3 × 3 × 4 tensor is
    that it contains a stack of three 3 × 4 images. If we keep this in mind, the following
    statements make sense:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用np.arange定义t为一个包含36个元素的向量，元素值为0到35。然后，我们立即将该向量重塑为一个包含3 × 3 × 4个元素的张量（3
    × 3 × 4 = 36）。一种理解3 × 3 × 4张量的方式是，它包含三张3 × 4的图像。如果我们记住这一点，接下来的陈述就能理解了：
- en: '>>> print(t[0])'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(t[0])'
- en: '[[ 0 1 2 3]'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 0 1 2 3]'
- en: '[ 4 5 6 7]'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 4 5 6 7]'
- en: '[ 8 9 10 11]]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 8 9 10 11]]'
- en: '>>> print(t[0,1])'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(t[0,1])'
- en: '[4 5 6 7]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[4 5 6 7]'
- en: '>>> print(t[0,1,2])'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(t[0,1,2])'
- en: '6'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '6'
- en: Asking for t[0] will return the first 3 × 4 *image* in the stack. Asking for
    t[0,1], then, should return the second row of the first image, which it does.
    Finally, we get to an individual element of t by asking for the image number (0),
    the row number (1), and the element of that row (2).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请求t[0]将返回堆栈中的第一张3 × 4*图像*。接着，t[0,1]应该返回第一张图像的第二行，正如它所做的那样。最后，我们通过请求图像编号（0），行编号（1）和该行的元素（2）来得到t的一个单独元素。
- en: 'Assigning the dimensions of a tensor to successively smaller collections of
    something is a handy way to keep the meaning of the dimensions in mind. For example,
    we can define an order-5 tensor like so:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将张量的维度分配给一个个越来越小的集合，是帮助记住维度含义的一种有效方式。例如，我们可以这样定义一个阶为5的张量：
- en: '>>> w = np.zeros((9,9,9,9,9))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> w = np.zeros((9,9,9,9,9))'
- en: '>>> w[4,1,2,0,1]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> w[4,1,2,0,1]'
- en: '0.0'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '0.0'
- en: But, what does asking for w[4,1,2,0,1] mean? The exact meaning depends on the
    application. For example, we might think of w as representing a bookcase. The
    first index selects the shelf, and the second selects the book on the shelf. Then,
    the third index selects the page within the book, and the fourth selects the line
    on the page. The final index selects the word on the line. Therefore, w[4,1,2,0,1]
    is asking for the second word of the first line of the third page of the second
    book on the fifth shelf of the bookcase, understood by reading the indices from
    right to left.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，w[4,1,2,0,1]意味着什么呢？其确切含义取决于应用场景。例如，我们可能将w看作表示一个书架。第一个索引选择书架的层，第二个索引选择书架上的书籍。然后，第三个索引选择书中的页面，第四个索引选择页面上的行。最后一个索引选择行上的单词。因此，w[4,1,2,0,1]是在询问书架第五层上的第二本书的第三页中的第一行的第二个单词，这个含义是从右到左读索引来理解的。
- en: The bookcase analogy does have its limitations. NumPy arrays have fixed dimensions,
    meaning that if w is a bookcase, there are nine shelves, and each shelf has *exactly*
    nine books. Likewise, each book has exactly nine pages, and each page has nine
    lines. Finally, each line has precisely nine words. NumPy arrays ordinarily use
    contiguous memory in the computer, so the size of each dimension is fixed when
    the array is defined. Doing so, and selecting the specific data type, like unsigned
    integer, makes locating an element of the array an indexing operation using a
    simple formula to compute an offset from a base memory address. This is what makes
    NumPy arrays so much faster than Python lists.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Any tensor of less than order *n* can be represented as an order-*n* tensor
    by supplying the missing dimensions of length one. We saw an example of this above
    when I said that an *m*-component vector could be thought of as a 1 × *m* or an
    *m* × 1 matrix. The order-1 tensor (the vector) is turned into an order-2 tensor
    (matrix) by adding a missing dimension of length one.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'As an extreme example, we can treat a scalar (order-0 tensor) as an order-5
    tensor, like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.array(42).reshape((1,1,1,1,1))'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(t)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[[[[[42]]]]]'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t.shape'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: (1, 1, 1, 1, 1)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t[0,0,0,0,0]'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '42'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we reshape the scalar 42 into an order-5 tensor (a five-dimensional [5D]
    array) with length one on each axis. Notice that NumPy tells us that the tensor
    t has five dimensions with the [[[[[ and ]]]]] around 42. Asking for the shape
    of t confirms that it is a 5D tensor. Finally, as a tensor, we can get the value
    of the single element it contains by specifying all the dimensions with t[0,0,0,0,0].
    We’ll often use this trick of adding new dimensions of length one. In fact, in
    NumPy, there is a way to do this directly, which you’ll see when using deep learning
    toolkits:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.array([[1,2,3],[4,5,6]])'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(t)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[[1 2 3]'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[4 5 6]]'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '>>> w = t[np.newaxis,:,:]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '>>> w.shape'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: (1, 2, 3)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(w)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[[[1 2 3]'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[4 5 6]]]'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ve turned t, an order-2 tensor (a matrix), into an order-3 tensor by
    using np.newaxis to create a new axis of length one. That’s why w.shape returns
    (1,2,3) and not (2,3), as it would for t.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'There are analogues between tensors up to order-3 and geometry that are helpful
    in visualizing the relationships between the different orders:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '| **Order (dimensions)** | **Tensor name** | **Geometric name** |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| 0 | Scalar | Point |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| 1 | Vector | Line |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: '| 2 | Matrix | Plane |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| 3 | Tensor | Volume |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: Notice, I used *tensor* in its common sense in the table. There seems to be
    no standardized name for an order-3 tensor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we defined the mathematical objects of deep learning in relation
    to multidimensional arrays, since that’s how they are implemented in code. We’ve
    thrown away a lot of mathematics by doing this, but we’ve preserved what we need
    to understand deep learning. Let’s move on now and see how to use tensors in expressions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic with Tensors
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of this section is to detail operations on tensors, with special
    emphasis on tensors of order-1 (vectors) and order-2 (matrices). We’ll assume
    operations with scalars are well in hand at this point.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with what I’m calling *array operations*, by which I mean the element-wise
    operations that toolkits like NumPy perform on arrays of all dimensions. Then
    we’ll move on to operations particular to vectors. This sets the stage for the
    critical topic of matrix multiplication. Finally, we’ll discuss block matrices.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Array Operations
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The way we’ve used the NumPy toolkit so far has shown us that all the normal
    scalar arithmetic operations translate directly into the world of multidimensional
    arrays. This includes standard operations like addition, subtraction, multiplication,
    division, and exponentiation, as well as the application of functions to an array.
    In all of these cases, the scalar operation is applied element-wise to each element
    of the array. The examples here will set the tone for the rest of this section
    and will also let us explore some NumPy broadcasting rules that we haven’t called
    out yet.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first define some arrays to work with:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([[1,2,3],[4,5,6]])'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = np.array([[7,8,9],[10,11,12]])'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '>>> c = np.array([10,100,1000])'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = np.array([10,11])'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[[1 2 3]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[4 5 6]]'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(b)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 7 8 9]'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[10 11 12]]'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(c)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[  10 100 1000]'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(d)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[10 11]'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'Element-wise arithmetic is straightforward for arrays with dimensions that
    match:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a+b)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 8 10 12]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[14 16 18]]'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a-b)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[[-6 -6 -6]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[-6 -6 -6]]'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a*b)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[[ 7 16 27]'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[40 55 72]]'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a/b)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[[0.14285714 0.25       0.33333333]'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[0.4        0.45454545 0.5       ]]'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(b**a)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[[       7       64      729]'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[   10000   161051  2985984]]'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: These results are all easy enough to interpret; NumPy applies the desired operation
    to the corresponding elements of each array. Element-wise multiplication on two
    matrices (a and b) is often known as the *Hadamard product*. (You’ll encounter
    this term from time to time in the deep learning literature.)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: The NumPy toolkit extends the idea of element-wise operations into what it calls
    *broadcasting*. When broadcasting, NumPy applies rules, which we’ll see via examples,
    where one array is passed over another to produce a meaningful output.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already encountered a form of broadcasting when operating on an array
    with a scalar. In that case, the scalar value was broadcast to every value of
    the array.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'For our first example, even though a is a 2 × 3 matrix, NumPy allows operations
    on it with c, a three-component vector, by applying broadcasting:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a+c)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[[  11  102 1003]'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[  14  105 1006]]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(c*a)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[[  10 200 3000]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[  40 500 6000]]'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a/c)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[[0.1  0.02  0.003]'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[0.4  0.05  0.006]]'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Here, the three-component vector, c, has been broadcast over the rows of the
    2 × 3 matrix, a. NumPy recognized that the last dimensions of a and c were both
    three, so the vector could be passed over the matrix to produce the given output.
    When looking at deep learning code, much of which is in Python, you’ll see situations
    like this. At times, some thought is necessary, along with some experimentation
    at the Python prompt, to understand what’s happening.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we broadcast d, a two-component vector, over a, a 2 × 3 matrix? If we try
    to do so the same way we broadcast c over a, we’ll fail:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a+d)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Traceback (most recent call last):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: File "<stdin>", line 1, in <module>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'ValueError: operands could not be broadcast together with shapes (2,3) (2,)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the broadcasting rules for NumPy accommodate dimensions of length
    one. The shape of d is 2; it’s a two-element vector. If we reshape d so that it’s
    a 2D array with shape 2 × 1, we’ll give NumPy what it needs:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d = d.reshape((2,1))'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '>>> d.shape'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: (2, 1)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(a+d)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[[11 12 13]'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[15 16 17]]'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: We now see that Numpy has added d across the columns of a.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the world of mathematics and look at operations on vectors.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Vector Operations
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vectors are represented in code as a collection of numbers that can be interpreted
    as values along a set of coordinate axes. Here, we’ll define several operations
    that are unique to vectors.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Geometrically, we can understand vectors as having a direction and a length.
    They’re often drawn as arrows, and we’ll see an example of a vector plot in [Chapter
    6](ch06.xhtml#ch06). People speak of the length of a vector as its *magnitude*.
    Therefore, the first vector operation we’ll consider is calculating its magnitude.
    For a vector, ***x***, with *n* components, the formula for its magnitude is
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ03.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: In [Equation 5.3](ch05.xhtml#ch05equ03), the double vertical bars around the
    vector represent its magnitude. You’ll often see people use single bars here as
    well. Single bars are also used for absolute value; we usually rely on context
    to tell the difference between the two.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Where did [Equation 5.3](ch05.xhtml#ch05equ03) come from? Consider a vector
    in 2D, ***x*** = (*x*, *y*). If *x* and *y* are lengths along the x-axis and y-axis,
    respectively, we see that *x* and *y* form the sides of a right triangle. The
    length of the hypotenuse of this right triangle is the length of the vector. Therefore,
    according to Pythagoras, and the Babylonians long before him, this length is ![Image](Images/112equ01.jpg),
    which, generalized to *n* dimensions, becomes [Equation 5.3](ch05.xhtml#ch05equ03).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Unit Vectors
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we can calculate the magnitude of a vector, we can introduce a useful
    form of a vector known as a *unit vector*. If we divide the components of a vector
    by its magnitude, we’re left with a vector that points in the same direction as
    the original vector but has a magnitude of one. This is the unit vector. For a
    vector, ***v***, the unit vector in the same direction is
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算向量的大小，接下来我们引入一个有用的向量形式，称为 *单位向量*。如果我们将一个向量的分量除以它的大小，我们就得到一个方向与原向量相同，但大小为一的单位向量。这就是单位向量。对于一个向量
    ***v***，与其方向相同的单位向量是
- en: '![Image](Images/112equ02.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/112equ02.jpg)'
- en: where the hat over the vector serves to identify it as a unit vector. Let’s
    see a concrete example. Our example vector is ***v*** = (2, –4,3). Therefore,
    the unit vector in the same direction as ***v*** is
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，向量上方的帽子标记表示它是一个单位向量。让我们看一个具体的例子。我们的示例向量是 ***v*** = (2, –4,3)。因此，方向与 ***v***
    相同的单位向量是
- en: '![Image](Images/112equ03.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/112equ03.jpg)'
- en: 'In code, we calculate the unit vector as the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们通过以下方式计算单位向量：
- en: '>>> v = np.array((2, -4, 3))'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> v = np.array((2, -4, 3))'
- en: '>>> u = v / np.sqrt((v*v).sum())'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> u = v / np.sqrt((v*v).sum())'
- en: '>>> print(u)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(u)'
- en: '[ 0.37139068 -0.74278135 0.55708601 ]'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.37139068 -0.74278135 0.55708601 ]'
- en: Here, we make use of the fact that to square each element of v, we multiply
    it by itself, element-wise, and then add the components together by calling sum
    to get the magnitude squared.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们利用了这样的事实：要平方 v 的每个元素，我们将其与自身逐元素相乘，然后通过调用 sum 将各个分量加在一起，得到平方的大小。
- en: Vector Transpose
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量转置
- en: We mentioned earlier that row vectors can be thought of as 1 × *n* matrices,
    while column vectors are *n* × 1 matrices. The act of changing a row vector into
    a column vector and vice versa is known as taking the *transpose*. We’ll see in
    [Chapter 6](ch06.xhtml#ch06) that the transpose also applies to matrices. Notationally,
    we denote the vector transpose of ***y*** as ***y***^⊤. Therefore, we have
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，行向量可以被看作是 1 × *n* 的矩阵，而列向量是 *n* × 1 的矩阵。将行向量转换为列向量，反之亦然，称为取 *转置*。我们将在[第六章](ch06.xhtml#ch06)中看到转置同样适用于矩阵。在符号表示上，我们用
    ***y***^⊤ 来表示向量 ***y*** 的转置。因此，我们有
- en: '![Image](Images/113equ01.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/113equ01.jpg)'
- en: Of course, we’re not limited to just three components.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不仅仅局限于三个分量。
- en: 'In code, we transpose vectors in several ways. As we saw above, we can use
    reshape to reshape the vector into a 1 × *n* or *n* × 1 matrix. We can also call
    the transpose method on the vector, with some care, or use the transpose shorthand.
    Let’s see examples of all of these approaches. First, let’s define a NumPy vector
    and see how reshape turns it into a 3 × 1 column vector and a 1 × 3 row vector,
    as opposed to a plain vector of three elements:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以通过几种方式转置向量。如上所示，我们可以使用 reshape 将向量重塑为 1 × *n* 或 *n* × 1 的矩阵。我们也可以谨慎地调用转置方法，或者使用转置的简写。让我们来看所有这些方法的示例。首先，我们定义一个
    NumPy 向量，看看 reshape 如何将它转换为 3 × 1 的列向量和 1 × 3 的行向量，而不是普通的三元素向量：
- en: '>>> v = np.array([1,2,3])'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> v = np.array([1,2,3])'
- en: '>>> print(v)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v)'
- en: '[1 2 3]'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 2 3]'
- en: '>>> print(v.reshape((3,1)))'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v.reshape((3,1)))'
- en: '[[1]'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]'
- en: '[2]'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]'
- en: '[3]]'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]]'
- en: '>>> print(v.reshape((1,3)))'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v.reshape((1,3)))'
- en: '[[1 2 3]]'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1 2 3]]'
- en: Notice the difference between the first print(v) and the last after calling
    reshape((1,3)). The output now has an extra set of brackets around it to indicate
    the leading dimension of one.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一次打印 v 和最后一次调用 reshape((1,3)) 后的区别。输出现在多了一组括号，表示它的前导维度为 1。
- en: 'Next, we apply the transpose operation on v:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对 v 应用转置操作：
- en: '>>> print(v.transpose())'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v.transpose())'
- en: '[1 2 3]'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 2 3]'
- en: '>>> print(v.T)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v.T)'
- en: '[1 2 3]'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 2 3]'
- en: 'Here, we see that calling transpose or T changes nothing about v. This is because
    the shape of v is simply 3, not (1,3) or (3,1). If we explicitly alter v to be
    a 1 × 3 matrix, we see that transpose and T have the desired effect:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们看到调用转置或 T 对 v 没有任何变化。这是因为 v 的形状仅为 3，而不是 (1,3) 或 (3,1)。如果我们显式地将 v 改为一个 1
    × 3 的矩阵，我们会看到转置和 T 达到了预期效果：
- en: '>>> v = v.reshape((1,3))'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> v = v.reshape((1,3))'
- en: '>>> print(v.transpose())'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v.transpose())'
- en: '[[1]'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]'
- en: '[2]'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]'
- en: '[3]]'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]]'
- en: '>>> print(v.T)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> print(v.T)'
- en: '[[1]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]'
- en: '[2]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]'
- en: '[3]]'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]]'
- en: Here, v goes from being a row vector to a column vector, as we expect. The lesson,
    then, is to be careful about the actual dimensionality of vectors in NumPy code.
    Most of the time, we can be sloppy, but sometimes we need to be explicit and care
    about the distinction between plain vectors, row vectors, and column vectors.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Inner Product
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Perhaps the most common vector operation is the *inner product*, or, as it is
    frequently called, the *dot product*. Notationally, the inner product between
    two vectors is written as
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ04.jpg)![Image](Images/05equ05.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'Here, *θ* is the angle between the two vectors if they’re interpreted geometrically.
    The result of the inner product is a scalar. The 〈***a***, ***b***〉 notation is
    seen frequently, though the ***a*** • ***b*** dot notation seems more common in
    the deep learning literature. The ***a***^⊤***b*** matrix multiplication notation
    explicitly calls out how to calculate the inner product, but we’ll wait until
    we discuss matrix multiplication to explain its meaning. For the present, the
    summation tells us what we need to know: the inner product of two vectors of length
    *n* is the sum of the products of the *n* components.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'The inner product of a vector with itself is the magnitude squared:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '***a*** • ***a*** = ||***a***||²'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The inner product is commutative,
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '***a*** • ***b*** = ***b*** • ***a***'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: and distributive,
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '***a*** • (***b*** + ***c***) = ***a*** • ***b*** + ***a*** • ***c***'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: but not associative, as the output of the first inner product is a scalar, not
    a vector, and multiplying a vector by a scalar is not an inner product.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Finally, notice that the inner product is zero when the angle between the vectors
    is 90 degrees; this is because cos *θ* is zero ([Equation 5.5](ch05.xhtml#ch05equ05)).
    This means the two vectors are perpendicular, or *orthogonal*, to each other.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some examples of the inner product. First, we’ll be literal and
    implement [Equation 5.4](ch05.xhtml#ch05equ04) explicitly:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([1,2,3,4])'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = np.array([5,6,7,8])'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '>>> def inner(a,b):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '...   s = 0.0'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '...   for i in range(len(a)):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '...     s += a[i]*b[i]'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '...   return s'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '>>> inner(a,b)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '70.0'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'However, since a and b are NumPy arrays, we know we can be more efficient:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '>>> (a*b).sum()'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '70'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, probably most efficient of all, we’ll let NumPy do it for us by using np.dot:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.dot(a,b)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '70'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see np.dot frequently in deep learning code. It can do more than calculate
    the inner product, as we’ll see below.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 5.5](ch05.xhtml#ch05equ05) tells us that the angle between two vectors
    is'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/115equ01.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: In the code, this could be calculated as
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '>>> A = np.sqrt(np.dot(a,a))'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '>>> B = np.sqrt(np.dot(b,b))'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.arccos(np.dot(a,b)/(A*B))'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t*(180/np.pi)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '14.335170291600924'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that the angle between ***a*** and ***b*** is approximately 14°
    after converting t from radians.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider vectors in 3D space, we see that the dot product between orthogonal
    vectors is zero, implying that the angle between them is 90°:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([1,0,0])'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = np.array([0,1,0])'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.dot(a,b)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '0'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t = np.arccos(0)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '>>> t*(180/np.pi)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '90.0'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: This is true because a is a unit vector along the x-axis, b is a unit vector
    along the y-axis, and we know there’s a right angle between them.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: With the inner product in our toolkit, let’s see how we can use it to project
    one vector onto another.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Projection
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The projection of one vector onto another calculates the amount of the first
    vector that’s in the direction of the second. The projection of ***a*** onto ***b***
    is
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/116equ01.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: '[Figure 5-1](ch05.xhtml#ch05fig01) shows graphically what projection means
    for 2D vectors.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig01.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: A graphical representation of the projection of **a** onto **b**
    in 2D*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Projection finds the component of ***a*** in the direction of ***b***. Note
    the projection of ***a*** onto ***b*** is not the same as the projection of ***b***
    onto ***a***.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Because we use an inner product in the numerator, we can see that the projection
    of a vector onto another vector that’s orthogonal to it is zero. No component
    of the first vector is in the direction of the second. Think again of the x-axis
    and y-axis. The entire reason we use Cartesian coordinates is because the two
    axes, or three in 3D space, are all mutually orthogonal; no part of one is in
    the direction of the others. This lets us specify any point, and the vector from
    the origin to that point, by specifying the components along these axes. We’ll
    see this breaking up of an object into mutually orthogonal components later when
    we discuss eigenvectors and PCA in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, calculating the projection is straightforward:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([1,1])'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = np.array([1,0])'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '>>> p = (np.dot(a,b)/np.dot(b,b))*b'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(p)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. 0.]'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '>>> c = np.array([-1,1])'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '>>> p = (np.dot(c,b)/np.dot(b,b))*b'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(p)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[-1\. -0.]'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, ***a*** points in the direction 45° up from the x-axis,
    while ***b*** points along the x-axis. We’d then expect the projection of ***a***
    to be along the x-axis, which it is (p). In the second example, ***c*** points
    in the direction 135° = 90° + 45° from the x-axis. Therefore, we’d expect the
    component of ***c*** along ***b*** to be along the x-axis but in the opposite
    direction from ***b***, which it is.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '*Projecting **c** along **b** returned a y-axis component of –0. The negative
    sign is a quirk of the IEEE 754 representation used for floating-point numbers.
    The significand (mantissa) of the internal representation is zero, but the sign
    can still be specified, leading to an output of negative zero from time to time.
    For a detailed explanation of computer number formats, including floating-point,
    please see my book,* Numbers and Computers *(Springer-Verlag, 2017).*'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now to consider the outer product of two vectors.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Outer Product
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The inner product of two vectors returned a scalar value. The *outer product*
    of two vectors instead returns a matrix. Note that unlike the inner product, the
    outer product does not require the two vectors to have the same number of components.
    Specifically, for vectors ***a*** of *m* components and ***b*** of *n* components,
    the outer product is the matrix formed by multiplying each element of ***a***
    by each element of ***b***, as shown next.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/118equ01.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: The ***ab***^⊤ notation is how to calculate the outer product via matrix multiplication.
    Notice that this notation is not the same as the inner product, ***a***^⊤***b***,
    and that it assumes ***a*** and ***b*** to be column vectors. No operator symbol
    is consistently used for the outer product, primarily because it’s so easily specified
    via matrix multiplication and because it’s less common than the dot product. However,
    ⊗ seems the most commonly used when the outer product is presented with a binary
    operator.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, NumPy has kindly provided an outer product function for us:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([1,2,3,4])'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = np.array([5,6,7,8])'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.dot(a,b)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '70'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '>>> np.outer(a,b)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: array([[ 5, 6, 7, 8],
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[10, 12, 14, 16],'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[15, 18, 21, 24],'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[20, 24, 28, 32]])'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: We used a and b above when discussing the inner product. As expected, np.dot
    gives us a scalar output for ***a***•***b***. However, the np.outer function returns
    a 4 × 4 matrix, where we see that each row is vector b multiplied successively
    by each element of vector a, first 1, then 2, then 3, and finally 4. Therefore,
    each element of a has multiplied each element of b. The resulting matrix is 4
    × 4 because both a and b have four elements.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: THE CARTESIAN PRODUCT
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: There is a direct analogue between the outer product of two vectors and the
    Cartesian product of two sets, *A* and *B*. The *Cartesian product* is a new set,
    each element of which is one of the possible pairings of elements from *A* and
    *B*. So, if *A*={1,2,3,4} and *B*={5,6,7,8}, the Cartesian product can be written
    as
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/118equ02.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Here, we see that if we replace each entry with the product of the pair, we
    get the corresponding vector product we saw above with NumPy np.outer. Also, note
    that × is typically used for the Cartesian product when working with sets.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The ability of the outer product to mix all combinations of its inputs has been
    used in deep learning for neural collaborative filtering and visual question answering
    applications. These functions are performed by advanced networks that make recommendations
    or answer text questions about an image. The outer product appears as a mixing
    of two different embedding vectors. *Embeddings* are the vectors generated by
    lower layers of a network, for example, the next to last fully connected layer
    before the softmax layer’s output of a traditional convolutional neural network
    (CNN). The embedding layer is usually viewed as having learned a new representation
    of the network input. It can be thought of as mapping complex inputs, like images,
    to a reduced space of several hundred to several thousands of dimensions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Cross Product
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our final vector-vector operator is the *cross product*. This operator is only
    defined for 3D space (ℝ³). The cross product of ***a*** and ***b*** is a new vector
    that is perpendicular to the plane containing ***a*** and ***b***. Note, this
    does not imply that ***a*** and ***b*** are themselves perpendicular. The cross
    product is defined as
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ06.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: where ![Image](Images/ncap.jpg) is a unit vector and *θ* is the angle between
    ***a*** and ***b***. The direction of ![Image](Images/ncap.jpg) is given by the
    *right-hand rule*. With your right hand, point your index finger in the direction
    of ***a*** and your middle finger in the direction of ***b***. Then, your thumb
    will be pointing in the direction of ![Image](Images/ncap.jpg). [Equation 5.6](ch05.xhtml#ch05equ06)
    gives the actual ℝ³ components of the cross product vector.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy implements the cross product via np.cross:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '>>> a = np.array([1,0,0])'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '>>> b = np.array([0,1,0])'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.cross(a,b))'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[0 0 1]'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '>>> c = np.array([1,1,0])'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '>>> print(np.cross(a,c))'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[0 0 1]'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example, a points along the x-axis and b along the y-axis. Therefore,
    we expect the cross product to be perpendicular to these axes, and it is: the
    cross product points along the z-axis. The second example shows that it doesn’t
    matter if a and b are perpendicular to each other. Here, c is at a 45° angle to
    the x-axis, but a and c are still in the xy-plane. Therefore, the cross product
    is still along the z-axis.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the cross product involves sin *θ*, while the inner product
    uses cos *θ*. The inner product is zero when the two vectors are orthogonal to
    each other. The cross product, on the other hand, is zero when the two vectors
    are in the same direction and is maximized when the vectors are perpendicular.
    The second NumPy example above works out because the magnitude of c is ![Image](Images/120equ01.jpg)
    and sin ![Image](Images/120equ02.jpg). As a result, the ![Image](Images/120equ01.jpg)
    factors cancel out to leave a magnitude of 1 for the cross product because a is
    a unit vector.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The cross product is widely used in physics and other sciences but is less often
    used in deep learning because of its restriction to 3D space. Nontheless, you
    should be familiar with it if you’re going to tackle the deep learning literature.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our look at vector-vector operations. Let’s leave the 1D world
    and move on to consider the most important operation for all deep learning: matrix
    multiplication.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how to multiply two vectors in various ways:
    Hadamard product, inner (dot) product, outer product, and cross product. In this
    section, we’ll investigate multiplication of matrices, recalling that row and
    column vectors are themselves matrices with one row or column.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Matrix Multiplication
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll define the matrix product operation shortly, but, before we do, let’s
    look at the properties of matrix multiplication. Let ***A***, ***B***, and ***C***
    be matrices. Then, following the algebra convention of multiplying symbols by
    placing them next to each other,
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '***(AB)C*** = ***A***(***BC***)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'meaning matrix multiplication is associative. Second, matrix multiplication
    is distributive:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ07.jpg)![Image](Images/05equ08.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: 'However, in general, matrix multiplication is *not* commutative:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '***AB*** ≠ ***BA***'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Equation 5.8](ch05.xhtml#ch05equ08), matrix multiplication
    over addition from the right produces a different result than matrix multiplication
    over addition from the left, as shown in [Equation 5.7](ch05.xhtml#ch05equ07).
    This explains why we showed both [Equation 5.7](ch05.xhtml#ch05equ07) and [Equation
    5.8](ch05.xhtml#ch05equ08); matrix multiplication can be performed from the left
    or the right, and the result will be different.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: How to Multiply Two Matrices
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To calculate ***AB***, knowing that ***A*** must be on the left of ***B***,
    we first need to verify that the matrices are compatible. It’s only possible to
    multiply two matrices if the number of columns in ***A*** is the same as the number
    of rows in ***B***. Therefore, if ***A*** is an *n × m* matrix and ***B*** is
    an *m × k* matrix, then the product, ***AB***, can be found and will be a new
    *n* × *k* matrix.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the product, we perform a series of inner product multiplications
    between the row vectors of ***A*** and the column vectors of ***B***. [Figure
    5-2](ch05.xhtml#ch05fig02) illustrates the process for a 3 × 3 matrix ***A***
    and a 3 × 2 matrix ***B***.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig02.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: Multiplying a 3* × *3 matrix by a 3* × *2 matrix*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-2](ch05.xhtml#ch05fig02), the first row of the output matrix is
    found by computing the inner product of the first row of ***A*** with each of
    the columns of ***B***. The first element of the output matrix is shown where
    the first row of ***A*** is multiplied by the first column of ***B***. The remaining
    first row of the output matrix is found by repeating the dot product of the first
    row of ***A*** by the remaining column of ***B***.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s present a worked example with actual numbers for the matrices in [Figure
    5-2](ch05.xhtml#ch05fig02):'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/121equ01.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: Notice, ***AB*** is defined, but ***BA*** is not, because we can’t multiply
    a 3 × 2 matrix by a 3 × 3 matrix. The number of columns in ***B*** needs to be
    the same as the number of rows in ***A***.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of matrix multiplication is by considering what goes into
    making up each of the output matrix elements. For example, if ***A*** is *n* ×
    *m* and ***B*** is *m* × *p*, we know that the matrix product exists as an *n*
    × *p* matrix, ***C***. We find the output elements by computing
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ09.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: for *i* = 0, . . . , *n* − 1 and *j* = 0, . . . , *p* − 1\. In the example above,
    we find *c*[21] by summing the products *a*[20]*b*[01] + *a*[21]*b*[11] + *a*[22]*b*[21],
    which fits [Equation 5.9](ch05.xhtml#ch05equ09) with *i* = 2, *j* = 1 and *k*
    = 0, 1, 2.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 5.9](ch05.xhtml#ch05equ09) tells us how to find a single output matrix
    element. If we loop over *i* and *j*, we can find the entire output matrix. This
    implies a straightforward implementation of matrix multiplication:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'def matrixmul(A,B):'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: I,K = A.shape
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: J = B.shape[1]
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: C = np.zeros((I,J), dtype=A.dtype)
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(I):'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'for j in range(J):'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'for k in range(K):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: C[i,j] += A[i,k]*B[k,j]
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: return C
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: We’ll assume the arguments, ***A*** and ***B***, are compatible matrices. We
    set the number of rows (I) and columns (J) of the output matrix, ***C***, and
    use them as the loop limits for the elements of ***C***. We create the output
    matrix, C, and give it the same data type as A. Then starts a triple loop. The
    loop over i covers all the rows of the output. The next loop, over j, covers the
    columns of the current row, and the innermost loop, over k, covers the combining
    of elements from A and B, as in [Equation 5.9](ch05.xhtml#ch05equ09). When all
    loops finish, we return the matrix product, C.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The function matrixmul works. It finds the matrix product. However, in terms
    of implementation, it’s quite naive. Advanced algorithms exist, as do many optimizations
    of the naive approach when using compiled code. As we’ll see below, NumPy supports
    matrix multiplication and internally uses highly optimized compiled code libraries
    that far outstrip the performance of the simple code above.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Notation for Inner and Outer Products
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We are now in a position to understand the matrix notation above for the inner
    product, ***a***^⊤***b***, and the outer product, ***ab***^⊤, of two vectors.
    In the first case, we have a 1 × *n* row vector, because of the transpose, and
    an *n* × 1 column vector. The algorithm says to form the inner product of the
    row vector and the column vector to arrive at an output matrix that is 1 × 1,
    that is, a single scalar number. Notice that there must be *n* components in both
    ***a*** and ***b***.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: For the outer product, we have an *n* × 1 column vector on the left and a 1
    × *m* row vector on the right. Therefore, we know the output matrix is *n* × *m*.
    If *m* = *n*, we’ll have an output matrix that’s *n* × *n*. A matrix with as many
    rows as it has columns is a *square matrix*. These have special properties, some
    of which we’ll see in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: To find the outer product of two vectors by matrix multiplication, we multiply
    each element of the rows of ***a*** by each of the columns of ***b** as a row
    vector*,
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/123equ01.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: where each column of ***b***^⊤, a single scalar number, is passed down the rows
    of ***a***, thereby forming each possible product between the elements of the
    two vectors.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen how to perform matrix multiplication manually. Let’s take a look
    now at how NumPy supports matrix multiplication.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication in NumPy
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NumPy provides two different functions that we can use for matrix multiplication.
    The first, we’ve seen already, np.dot, though we’ve only used it so far to compute
    inner products of vectors. The second is np.matmul, which is also called when
    using the @ binary operator available in Python 3.5 and later. Matrix multiplication
    with either function works as we expect. However, NumPy sometimes treats 1D arrays
    differently from row or column vectors.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use shape to decide if a NumPy array is a 1D array, a row vector, or
    a column vector, as shown in [Listing 5-1](ch05.xhtml#ch05ex01):'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '>>> av = np.array([1,2,3])'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '>>> ar = np.array([[1,2,3]])'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '>>> ac = np.array([[1],[2],[3]])'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '>>> av.shape'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: (3,)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '>>> ar.shape'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: (1, 3)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '>>> ac.shape'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: (3, 1)
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5-1: NumPy vectors*'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see that a 1D array with three elements, av, has a shape different
    from a row vector with three components, ar, or a column vector of three components,
    ac. However, each of these arrays contains the same three integers: 1, 2, and
    3.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run an experiment to help us understand how NumPy implements matrix multiplication.
    We’ll test np.dot, but the results are the same if we use np.matmul or the @ operator.
    We need a collection of vectors and matrices to work with. We’ll then apply combinations
    of them to np.dot and consider the output, which may very well be an error if
    the operation is undefined for that combination of arguments.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the arrays, vectors, and matrices we’ll need:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: a1 = np.array([1,2,3])
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: ar = np.array([[1,2,3]])
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: ac = np.array([[1],[2],[3]])
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: b1 = np.array([1,2,3])
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: br = np.array([[1,2,3]])
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: bc = np.array([[1],[2],[3]])
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: A = np.array([[1,2,3],[4,5,6],[7,8,9]])
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: B = np.array([[9,8,7],[6,5,4],[3,2,1]])
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: The shape of the objects should be discernible from the definition, if we keep
    the results of [Listing 5-1](ch05.xhtml#ch05ex01) in mind. We’ll also define two
    3 × 3 matrices, A and B.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define a helper function to wrap the call to NumPy so we can trap
    any errors:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'def dot(a,b):'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'try:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: return np.dot(a,b)
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'except:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: return "fails"
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: This function calls np.dot and returns the word fails if the call doesn’t succeed.
    [Table 5-1](ch05.xhtml#ch05tab01) shows the output of dot for the given combinations
    of the inputs defined above.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-1](ch05.xhtml#ch05tab01) illustrates how NumPy sometimes treats 1D
    arrays differently from row or column vectors. See the difference in [Table 5-1](ch05.xhtml#ch05tab01)
    for a1,A versus ar,A and A,ac. The output of A,ac is what we’d expect to see mathematically,
    with the column vector ***a[c]*** multiplied on the left by ***A***.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Is there any real difference between np.dot and np.matmul? Yes, some. For 1D
    and 2D arrays, there is no difference. However, there is a difference between
    how each function handles arrays greater than two dimensions, although we won’t
    work with those here. Also, np.dot allows one of its arguments to be a scalar
    and multiplies each element of the other argument by it. Multiplying by a scalar
    with np.matmul throws an error.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Results of Applying dot or matmul to Different Types of Arguments'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '| **Arguments** | **Result of np.dot or np.matmul** |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| a1,b1 | 14 (scalar) |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| a1,br | fails |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| a1,bc | [14] (1 vector) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| ar,b1 | [14] (1 vector) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| ar,br | fails |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| ar,bc | [14] (1 × 1 matrix) |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| ac,b1 | fails |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| ac,br | ![Image](Images/124equ01.jpg) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| ac,bc | fails |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| A,a1 | [14 32 50] (3 vector) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| A,ar | fails |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| A,ac | ![Image](Images/124equ02.jpg) |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| a1,A | [30 36 42] (3 vector) |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| ar,A | [30 36 42] (1 × 3 matrix) |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| ac,A | fails |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| A,B | ![Image](Images/124equ03.jpg) |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: Kronecker Product
  id: totrans-426
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final form of matrix multiplication we’ll discuss is the *Kronecker product*
    or *matrix direct product* of two matrices. When computing the matrix product,
    we mixed individual elements of the matrices, multiplying them together. For the
    Kronecker product, we multiply the elements of one matrix by an entire matrix
    to produce an output matrix that is larger than the input matrices. The Kronecker
    product is also a convenient place to introduce the idea of a *block matrix*,
    or a matrix constructed from smaller matrices (the blocks).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have three matrices
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ10.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: we can define a block matrix, ***M***, as the following.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ01.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
- en: where each element of ***M*** is a smaller matrix stacked on top of each other.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: We can most easily define the Kronecker product using a visual example involving
    a block matrix. The Kronecker product of ***A*** and ***B***, typically written
    as ***A*** ⊗ ***B***, is
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ02.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: for ***A***, an *m* × *n* matrix. This is a block matrix because of ***B***,
    so, when written out completely, the Kronecker product results in a matrix larger
    than either ***A*** or ***B***. Note, unlike matrix multiplication, the Kronecker
    product is defined for arbitrarily sized ***A*** and ***B*** matrices. For example,
    using ***A*** and ***B*** from [Equation 5.10](ch05.xhtml#ch05equ10), the Kronecker
    product is
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ03.jpg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
- en: Notice above that we used ⊗ for the Kronecker product. This is the convention,
    though the symbol ⊗ is sometimes abused and is used for other things too. We used
    it for the outer product of two vectors, for example. NumPy supports the Kronecker
    product via np.kron.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the mathematical objects used in deep learning:
    scalars, vector, matrices, and tensors. We then explored arithmetic with tensors,
    in particular with vectors and matrices. We saw how to perform operations on these
    objects, both mathematically and in code via NumPy.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration of linear algebra is not complete, however. In the next chapter,
    we’ll dive deeper into matrices and their properties to discuss just a handful
    of the important things that we can do with or know about them.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
