- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5'
- en: LINEAR ALGEBRA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Formally, linear algebra is the study of linear equations, in which the highest
    power of the variable is one. However, for our purposes, *linear algebra* refers
    to multidimensional mathematical objects—like vectors and matrices—and operations
    on them. This is how linear algebra is typically applied in deep learning, and
    how data is manipulated in programs that implement deep learning algorithms. By
    making this distinction, we are throwing away a massive amount of fascinating
    mathematics, but as our goal is to understand the mathematics used and applied
    in deep learning, we can hopefully be forgiven.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，线性代数是研究线性方程的学科，其中变量的最高次幂为一。然而，对于我们的目的来说，*线性代数*指的是多维数学对象——如向量和矩阵——以及对它们的运算。这就是线性代数在深度学习中的典型应用方式，也是实现深度学习算法的程序中数据操作的方式。通过做出这个区分，我们丢弃了大量有趣的数学内容，但由于我们的目标是理解深度学习中使用和应用的数学，因此我们希望能够得到宽恕。
- en: In this chapter, I’ll introduce the objects used in deep learning, specifically
    scalars, vectors, matrices, and tensors. As we’ll see, all of these objects are
    actually tensors of various orders. We’ll discuss tensors from a mathematical,
    notational perspective and then experiment with them using NumPy. NumPy was explicitly
    designed to add multidimensional arrays to Python, and they are good, though incomplete,
    analogues for the mathematical objects we’ll work with in this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将介绍在深度学习中使用的对象，特别是标量、向量、矩阵和张量。正如我们所看到的，这些对象实际上都是不同阶数的张量。我们将从数学和符号的角度讨论张量，然后使用
    NumPy 进行实验。NumPy 明确设计用于向 Python 添加多维数组，它们是我们在本章中处理的数学对象的良好，但并不完全相同的类比。
- en: We’ll spend the bulk of the chapter learning how to do arithmetic with tensors,
    which is of fundamental importance in deep learning. Most of the effort in implementing
    highly performant deep learning toolkits involves finding ways to do arithmetic
    with tensors as efficiently as possible.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的大部分时间里学习如何进行张量运算，这对于深度学习至关重要。实现高性能深度学习工具包的大部分努力，都涉及找到尽可能高效地进行张量运算的方法。
- en: Scalars, Vectors, Matrices, and Tensors
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标量、向量、矩阵和张量
- en: Let’s introduce our cast of characters. I’ll relate them to Python variables
    and NumPy arrays to show how we’ll implement these objects in code. Then I’ll
    present a handy conceptual mapping between tensors and geometry.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来介绍一下我们的角色阵容。我会将它们与 Python 变量和 NumPy 数组相关联，展示我们如何在代码中实现这些对象。然后，我会展示张量与几何之间的一个便捷概念映射。
- en: Scalars
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标量
- en: 'Even if you’re not familiar with the word, you’ve known what a scalar is since
    the day you first learned to count. A *scalar* is just a number, like 7, 42, or
    π. In expressions, we’ll use *x* to mean a scalar, that is, the ordinary notation
    used for variables. To a computer, a scalar is a simple numeric variable:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不熟悉这个词，从你第一次学习计数那天起，你就已经知道标量是什么了。*标量*就是一个数字，比如 7、42 或 π。在表达式中，我们将使用*x*来表示标量，也就是用于表示变量的普通符号。对于计算机来说，标量是一个简单的数值变量：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Vectors
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量
- en: A *vector* is a 1D array of numbers. Mathematically, a vector has an orientation,
    either horizontal or vertical. If horizontal, it’s a *row vector*. For example,
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量*是一个一维数字数组。从数学上讲，向量有方向性，可以是水平或垂直的。如果是水平的，它就是一个*行向量*。例如，'
- en: '![Image](Images/05equ01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/05equ01.jpg)'
- en: is a row vector of three elements or components. Note, we’ll use *x*, a lowercase
    letter in bold, to mean a vector.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个由三个元素或分量构成的行向量。请注意，我们将使用**x**，一个加粗的小写字母，来表示向量。
- en: Mathematically, vectors are usually assumed to be *column vectors*,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，向量通常被认为是*列向量*，
- en: '![Image](Images/05equ02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/05equ02.jpg)'
- en: where ***y*** has four components, making it a four-dimensional (4D) vector.
    Notice that in [Equation 5.1](ch05.xhtml#ch05equ01) we used square brackets, whereas
    in [Equation 5.2](ch05.xhtml#ch05equ02) we used parentheses. Either notation is
    acceptable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***y***有四个分量，因此它是一个四维（4D）向量。请注意，在[公式 5.1](ch05.xhtml#ch05equ01)中我们使用了方括号，而在[公式
    5.2](ch05.xhtml#ch05equ02)中我们使用了圆括号。两种符号表示方法都是可以接受的。
- en: 'In code, we usually implement vectors as 1D arrays:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们通常将向量实现为一维数组：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we’ve used `reshape` to turn the three-element row vector into a column
    vector of three rows and one column.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`reshape`将三元素行向量转变为一个三行一列的列向量。
- en: The components of a vector are often interpreted as lengths along a set of coordinate
    axes. For example, a three-component vector might be used to represent a point
    in 3D space. In this vector,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的分量通常被解释为沿一组坐标轴的长度。例如，三分量向量可能用于表示三维空间中的一个点。在这个向量中，
- en: '***x*** = [*x*, *y*, *z*]'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***x*** = [*x*, *y*, *z*]'
- en: '*x* could be the length along the x-axis, *y* the length along the y-axis,
    and *z* the length along the z-axis. These are the Cartesian coordinates and serve
    to uniquely identify all points in 3D space.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* 可能是沿x轴的长度，*y* 是沿y轴的长度，*z* 是沿z轴的长度。这些是笛卡尔坐标，旨在唯一地标识三维空间中的所有点。'
- en: However, in deep learning, and machine learning in general, the components of
    a vector are often unrelated to each other in any strict geometric sense. Rather,
    they’re used to represent *features*, qualities of some sample that the model
    will use to attempt to arrive at a useful output, like a class label, or a regression
    value. That said, the vector representing the collection of features, called the
    *feature vector*, is sometimes thought about geometrically. For example, some
    machine learning models, like *k*-nearest neighbors, interpret the vector as representing
    some coordinate in geometric space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深度学习和机器学习中，向量的分量通常在几何意义上彼此不相关。相反，它们被用来表示*特征*，即模型用来尝试得出有用输出（如类别标签或回归值）的一些样本的性质。也就是说，表示特征集合的向量，称为*特征向量*，有时会从几何角度进行思考。例如，一些机器学习模型，如*k*-最近邻算法，将向量解释为几何空间中的某个坐标。
- en: You’ll often hear deep learning people discuss the *feature space* of a problem.
    The feature space refers to the set of possible inputs. The training set for a
    model needs to accurately represent the feature space of the possible inputs the
    model will encounter when used. In this sense, the feature vector is a point,
    a location in this *n*-dimensional space where *n* is the number of features in
    the feature vector.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会听到深度学习领域的人讨论问题的*特征空间*。特征空间指的是可能输入的集合。模型的训练集需要准确地表示模型在使用时会遇到的可能输入的特征空间。从这个意义上讲，特征向量是一个点，一个在这个*n*维空间中的位置，其中*n*是特征向量中的特征数量。
- en: Matrices
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵
- en: 'A *matrix* is a 2D array of numbers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*矩阵*是一个二维数组：'
- en: '![Image](Images/105equ01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/105equ01.jpg)'
- en: The elements of ***A*** are subscripted by the row number and column number.
    The matrix ***A*** has three rows and four columns, so we say that it’s a 3 ×
    4 matrix, where 3 × 4 is the *order* of the matrix. Notice that ***A*** uses subscripts
    starting with 0\. Math texts often begin with 1, but increasingly, they’re using
    0 so that there isn’t an offset between the math notation and the computer representation
    of the matrix. Note, also, that we’ll use ***A***, an uppercase letter in bold,
    to mean a matrix.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***的元素通过行号和列号进行下标标记。矩阵***A***有三行四列，所以我们说它是一个3 × 4矩阵，其中3 × 4是矩阵的*阶*。请注意，***A***使用的是从0开始的下标。数学文本通常从1开始，但越来越多的情况下，使用0，这样就不会在数学表示和矩阵的计算机表示之间产生偏移。还要注意，我们将使用***A***，一个加粗的大写字母，表示一个矩阵。'
- en: 'In code, matrices are represented as 2D arrays:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，矩阵表示为二维数组：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To get element *a*[12] of ***A*** in Python, we write `A[1,2]`. Notice that
    when we printed the arrays, there was an extra `[` and `]` around them. NumPy
    uses these brackets to indicate that the 2D array can be thought of as a row vector
    in which each element is itself a vector. In Python-speak, this means that a matrix
    can be thought of as a list of sublists in which each sublist is of the same length.
    Of course, this is exactly how we defined `A` to begin with.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，要获取矩阵***A***的元素*a*[12]，我们写作`A[1,2]`。请注意，当我们打印数组时，它们周围有一个额外的`[`和`]`。NumPy使用这些括号来表示二维数组可以被视为行向量，其中每个元素本身就是一个向量。在Python术语中，这意味着矩阵可以被视为一个子列表的列表，每个子列表的长度相同。当然，这正是我们最初定义`A`的方式。
- en: 'We can think of vectors as matrices with a single row or column. A column vector
    with three elements is a 3 × 1 matrix: it has three rows and one column. Similarly,
    a row vector of four elements acts like a 1 × 4 matrix: it has one row and four
    columns. We’ll make use of this observation later.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将向量看作是只有一行或一列的矩阵。一个三元素的列向量是一个3 × 1矩阵：它有三行和一列。类似地，一个四元素的行向量就像一个1 × 4矩阵：它有一行和四列。我们稍后将利用这一观察结果。
- en: Tensors
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量
- en: 'A scalar has no dimensions, a vector has one, and a matrix has two. As you
    might suspect, we don’t need to stop there. A mathematical object with more than
    two dimensions is colloquially referred to as a *tensor*. When necessary, we’ll
    represent tensors like this: `T`, as a sans serif capital letter.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 标量没有维度，向量有一个维度，矩阵有两个维度。正如你可能猜到的，我们并不止步于此。一个具有超过两维的数学对象通常被称为*张量*。在必要时，我们会像这样表示张量：`T`，作为无衬线大写字母。
- en: The number of dimensions a tensor has defines its *order*, which is not to be
    confused with the order of a matrix. A 3D tensor has order 3\. A matrix is a tensor
    of order 2\. A vector is an order-1 tensor, and a scalar is an order-0 tensor.
    When we discuss the flow of data through a deep neural network in [Chapter 9](ch09.xhtml#ch09),
    we’ll see that many toolkits use tensors of order 4 (or more).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的维度数量定义了其*阶*，这与矩阵的阶不同。一个3D张量的阶是3。矩阵是一个阶为2的张量。向量是一个阶为1的张量，而标量是一个阶为0的张量。当我们在[第9章](ch09.xhtml#ch09)讨论数据流过深度神经网络时，我们会看到许多工具包使用阶为4（或更多）的张量。
- en: 'In Python, NumPy arrays with three or more dimensions are used to implement
    tensors. For example, we can define an order-3 tensor in Python as shown below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，具有三维或更多维度的NumPy数组用于实现张量。例如，我们可以如下定义一个阶为3的张量：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we use `np.arange` to define `t` to be a vector of 36 elements holding
    the numbers 0 . . . 35\. Then, we immediately `reshape` the vector into a tensor
    of 3 × 3 × 4 elements (3 × 3 × 4 = 36). One way to think of a 3 × 3 × 4 tensor
    is that it contains a stack of three 3 × 4 images. If we keep this in mind, the
    following statements make sense:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`np.arange`定义`t`为一个包含36个元素的向量，数值为0 . . . 35。然后，我们立即将这个向量`reshape`成一个3
    × 3 × 4的张量（3 × 3 × 4 = 36）。可以将3 × 3 × 4的张量理解为包含三张3 × 4图像的堆叠。如果我们记住这一点，以下语句就能理解：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Asking for `t[0]` will return the first 3 × 4 *image* in the stack. Asking for
    `t[0,1]`, then, should return the second row of the first image, which it does.
    Finally, we get to an individual element of `t` by asking for the image number
    (`0`), the row number (`1`), and the element of that row (`2`).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请求`t[0]`将返回堆叠中的第一张3 × 4*图像*。那么，接着请求`t[0,1]`将返回第一张图像的第二行，正如它所做的那样。最后，我们通过请求图像编号（`0`）、行编号（`1`）和该行的元素（`2`）来访问`t`的单个元素。
- en: 'Assigning the dimensions of a tensor to successively smaller collections of
    something is a handy way to keep the meaning of the dimensions in mind. For example,
    we can define an order-5 tensor like so:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将张量的维度分配给一个个更小的集合是帮助记住维度含义的便捷方式。例如，我们可以如下定义一个阶为5的张量：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: But, what does asking for `w[4,1,2,0,1]` mean? The exact meaning depends on
    the application. For example, we might think of `w` as representing a bookcase.
    The first index selects the shelf, and the second selects the book on the shelf.
    Then, the third index selects the page within the book, and the fourth selects
    the line on the page. The final index selects the word on the line. Therefore,
    `w[4,1,2,0,1]` is asking for the second word of the first line of the third page
    of the second book on the fifth shelf of the bookcase, understood by reading the
    indices from right to left.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，要求`w[4,1,2,0,1]`是什么意思呢？确切含义取决于应用。例如，我们可能认为`w`代表一个书架。第一个索引选择书架，第二个选择书架上的书。接着，第三个索引选择书中的页面，第四个选择页面上的行。最后一个索引选择行中的单词。因此，`w[4,1,2,0,1]`就是要求从书架的第五层架子上的第二本书的第三页的第一行中获取第二个单词，从右到左读取索引可以理解这一点。
- en: The bookcase analogy does have its limitations. NumPy arrays have fixed dimensions,
    meaning that if `w` is a bookcase, there are nine shelves, and each shelf has
    *exactly* nine books. Likewise, each book has exactly nine pages, and each page
    has nine lines. Finally, each line has precisely nine words. NumPy arrays ordinarily
    use contiguous memory in the computer, so the size of each dimension is fixed
    when the array is defined. Doing so, and selecting the specific data type, like
    unsigned integer, makes locating an element of the array an indexing operation
    using a simple formula to compute an offset from a base memory address. This is
    what makes NumPy arrays so much faster than Python lists.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 书架类比确实有其局限性。NumPy数组具有固定的维度，这意味着如果`w`是一个书架，它有九层架子，每一层架子上有*正好*九本书。同样，每本书有正好九页，每一页有九行。最后，每一行有正好九个单词。NumPy数组通常在计算机中使用连续内存，因此在定义数组时每个维度的大小是固定的。这样做，并选择特定的数据类型，如无符号整数，使得定位数组中的元素成为一个使用简单公式计算基址偏移量的索引操作。这也是NumPy数组比Python列表快得多的原因。
- en: Any tensor of less than order *n* can be represented as an order-*n* tensor
    by supplying the missing dimensions of length one. We saw an example of this above
    when I said that an *m*-component vector could be thought of as a 1 × *m* or an
    *m* × 1 matrix. The order-1 tensor (the vector) is turned into an order-2 tensor
    (matrix) by adding a missing dimension of length one.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 任何低于 *n* 阶的张量都可以通过提供缺失的长度为 1 的维度，表示为一个 *n* 阶的张量。我们在上面看到过一个例子，当我说一个 *m* 维向量可以看作是一个
    1 × *m* 或 *m* × 1 矩阵时。通过添加一个长度为 1 的缺失维度，1 阶张量（向量）变成了 2 阶张量（矩阵）。
- en: 'As an extreme example, we can treat a scalar (order-0 tensor) as an order-5
    tensor, like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个极端例子，我们可以将一个标量（0 阶张量）当作一个 5 阶张量来处理，如下所示：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we `reshape` the scalar `42` into an order-5 tensor (a five-dimensional
    [5D] array) with length one on each axis. Notice that NumPy tells us that the
    tensor `t` has five dimensions with the `[[[[[` and `]]]]]` around `42`. Asking
    for the shape of `t` confirms that it is a 5D tensor. Finally, as a tensor, we
    can get the value of the single element it contains by specifying all the dimensions
    with `t[0,0,0,0,0]`. We’ll often use this trick of adding new dimensions of length
    one. In fact, in NumPy, there is a way to do this directly, which you’ll see when
    using deep learning toolkits:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将标量 `42` 重新塑形为一个 5 阶张量（一个五维 [5D] 数组），每个轴的长度都是 1。注意，NumPy 告诉我们张量 `t` 有五个维度，`42`
    周围的 `[[[[[` 和 `]]]]]` 表明了这一点。请求 `t` 的形状验证了它是一个 5D 张量。最后，作为张量，我们可以通过指定所有维度 `t[0,0,0,0,0]`
    来获取它包含的唯一元素的值。我们经常会使用这个添加新维度的技巧。事实上，在 NumPy 中，有一种直接执行此操作的方法，您将在使用深度学习工具包时看到：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we’ve turned `t`, an order-2 tensor (a matrix), into an order-3 tensor
    by using `np.newaxis` to create a new axis of length one. That’s why `w.shape`
    returns `(1,2,3)` and not `(2,3)`, as it would for `t`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过使用 `np.newaxis` 创建一个长度为 1 的新轴，将 `t`（一个 2 阶张量，即矩阵）转变为 3 阶张量。这就是为什么 `w.shape`
    返回 `(1,2,3)` 而不是 `(2,3)`，就像 `t` 那样。
- en: 'There are analogues between tensors up to order-3 and geometry that are helpful
    in visualizing the relationships between the different orders:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 张量与三阶及以下几何体之间存在类比，有助于我们可视化不同阶数之间的关系：
- en: '| **Order (dimensions)** | **Tensor name** | **Geometric name** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **阶数（维度）** | **张量名称** | **几何名称** |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | Scalar | Point |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 标量 | 点 |'
- en: '| 1 | Vector | Line |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 向量 | 线 |'
- en: '| 2 | Matrix | Plane |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 矩阵 | 平面 |'
- en: '| 3 | Tensor | Volume |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 张量 | 体积 |'
- en: Notice, I used *tensor* in its common sense in the table. There seems to be
    no standardized name for an order-3 tensor.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我在表格中使用了张量（*tensor*）这个常见的定义。似乎没有标准化的名称来表示 3 阶张量。
- en: In this section, we defined the mathematical objects of deep learning in relation
    to multidimensional arrays, since that’s how they are implemented in code. We’ve
    thrown away a lot of mathematics by doing this, but we’ve preserved what we need
    to understand deep learning. Let’s move on now and see how to use tensors in expressions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将深度学习中的数学对象与多维数组相关联，因为它们在代码中是如何实现的。通过这样做，我们丢弃了很多数学内容，但保留了理解深度学习所需的部分。现在让我们继续，看看如何在表达式中使用张量。
- en: Arithmetic with Tensors
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量的算术运算
- en: The purpose of this section is to detail operations on tensors, with special
    emphasis on tensors of order-1 (vectors) and order-2 (matrices). We’ll assume
    operations with scalars are well in hand at this point.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是详细说明张量的操作，特别强调 1 阶张量（向量）和 2 阶张量（矩阵）的操作。我们将假设标量的操作已经非常熟练。
- en: We’ll start with what I’m calling *array operations*, by which I mean the element-wise
    operations that toolkits like NumPy perform on arrays of all dimensions. Then
    we’ll move on to operations particular to vectors. This sets the stage for the
    critical topic of matrix multiplication. Finally, we’ll discuss block matrices.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我所称之为*数组操作*开始，指的是像 NumPy 这样的工具包在各种维度的数组上执行的逐元素操作。然后我们将继续讲解特定于向量的操作。这为关键的矩阵乘法话题奠定了基础。最后，我们将讨论块矩阵。
- en: Array Operations
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数组操作
- en: The way we’ve used the NumPy toolkit so far has shown us that all the normal
    scalar arithmetic operations translate directly into the world of multidimensional
    arrays. This includes standard operations like addition, subtraction, multiplication,
    division, and exponentiation, as well as the application of functions to an array.
    In all of these cases, the scalar operation is applied element-wise to each element
    of the array. The examples here will set the tone for the rest of this section
    and will also let us explore some NumPy broadcasting rules that we haven’t called
    out yet.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用NumPy工具包的方式展示了所有常见的标量算术运算都可以直接转换到多维数组的世界中。这包括加法、减法、乘法、除法和指数运算等标准操作，以及对数组应用函数。在所有这些情况下，标量操作都逐元素地应用到数组的每个元素上。这里的示例将为本节的其余部分定下基调，并且也会让我们探索一些我们尚未提到的NumPy广播规则。
- en: 'Let’s first define some arrays to work with:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们定义一些数组来进行操作：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Element-wise arithmetic is straightforward for arrays with dimensions that
    match:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于维度匹配的数组，逐元素的算术运算是非常简单的：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These results are all easy enough to interpret; NumPy applies the desired operation
    to the corresponding elements of each array. Element-wise multiplication on two
    matrices (`a` and `b`) is often known as the *Hadamard product*. (You’ll encounter
    this term from time to time in the deep learning literature.)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果都很容易理解；NumPy将所需的操作应用到每个数组的对应元素上。两个矩阵（`a`和`b`）之间的逐元素乘法通常称为*Hadamard积*。（你会在深度学习文献中不时遇到这个术语。）
- en: The NumPy toolkit extends the idea of element-wise operations into what it calls
    *broadcasting*. When broadcasting, NumPy applies rules, which we’ll see via examples,
    where one array is passed over another to produce a meaningful output.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy工具包将逐元素操作的概念扩展到了它所称的*广播*。在广播时，NumPy应用一些规则，我们通过示例可以看到这些规则，其中一个数组会广播到另一个数组上，生成有意义的输出。
- en: We’ve already encountered a form of broadcasting when operating on an array
    with a scalar. In that case, the scalar value was broadcast to every value of
    the array.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经遇到过在标量和数组操作时的一种广播形式。在那个例子中，标量值被广播到数组的每个值上。
- en: 'For our first example, even though `a` is a 2 × 3 matrix, NumPy allows operations
    on it with `c`, a three-component vector, by applying broadcasting:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，尽管`a`是一个2 × 3矩阵，NumPy通过应用广播允许它与三分量向量`c`进行操作：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, the three-component vector, `c`, has been broadcast over the rows of the
    2 × 3 matrix, `a`. NumPy recognized that the last dimensions of `a` and `c` were
    both three, so the vector could be passed over the matrix to produce the given
    output. When looking at deep learning code, much of which is in Python, you’ll
    see situations like this. At times, some thought is necessary, along with some
    experimentation at the Python prompt, to understand what’s happening.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，三分量向量`c`已经广播到2 × 3矩阵`a`的行上。NumPy识别到`a`和`c`的最后一个维度都是三维的，因此可以将向量传递到矩阵上，生成所需的输出。在查看深度学习代码时，尤其是Python代码，你会看到像这样的情况。有时需要一些思考，并结合在Python提示符下进行一些实验，才能理解发生了什么。
- en: 'Can we broadcast `d`, a two-component vector, over `a`, a 2 × 3 matrix? If
    we try to do so the same way we broadcast `c` over `a`, we’ll fail:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否将`d`（一个包含两个元素的向量）广播到`a`（一个2 × 3矩阵）上？如果我们尝试用和将`c`广播到`a`相同的方式进行操作，我们将会失败：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'However, the broadcasting rules for NumPy accommodate dimensions of length
    one. The shape of `d` is 2; it’s a two-element vector. If we reshape `d` so that
    it’s a 2D array with shape 2 × 1, we’ll give NumPy what it needs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，NumPy的广播规则支持长度为1的维度。`d`的形状是2，它是一个包含两个元素的向量。如果我们将`d`重塑为一个形状为2 × 1的二维数组，那么我们就给NumPy提供了它所需要的信息：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We now see that Numpy has added `d` across the columns of `a`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在看到NumPy将`d`加到`a`的列上。
- en: Let’s return to the world of mathematics and look at operations on vectors.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到数学的世界，来看一下向量的运算。
- en: Vector Operations
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量运算
- en: Vectors are represented in code as a collection of numbers that can be interpreted
    as values along a set of coordinate axes. Here, we’ll define several operations
    that are unique to vectors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 向量在代码中通常表示为一组数值，这些数值可以解释为沿一组坐标轴的值。在这里，我们将定义几种独特的向量运算。
- en: Magnitude
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大小
- en: Geometrically, we can understand vectors as having a direction and a length.
    They’re often drawn as arrows, and we’ll see an example of a vector plot in [Chapter
    6](ch06.xhtml#ch06). People speak of the length of a vector as its *magnitude*.
    Therefore, the first vector operation we’ll consider is calculating its magnitude.
    For a vector, ***x***, with *n* components, the formula for its magnitude is
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何意义上讲，我们可以理解向量具有方向和长度。它们通常被画成箭头，我们将在[第6章](ch06.xhtml#ch06)中看到向量图的一个例子。人们称向量的长度为它的*大小*。因此，我们将考虑的第一个向量运算是计算其大小。对于一个具有*n*个分量的向量***x***，其大小的公式是
- en: '![Image](Images/05equ03.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/05equ03.jpg)'
- en: In [Equation 5.3](ch05.xhtml#ch05equ03), the double vertical bars around the
    vector represent its magnitude. You’ll often see people use single bars here as
    well. Single bars are also used for absolute value; we usually rely on context
    to tell the difference between the two.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在[方程 5.3](ch05.xhtml#ch05equ03)中，向量周围的双竖线表示它的大小。你也常常会看到有人使用单竖线。单竖线也用于表示绝对值；我们通常依赖上下文来区分这两者。
- en: Where did [Equation 5.3](ch05.xhtml#ch05equ03) come from? Consider a vector
    in 2D, ***x*** = (*x*, *y*). If *x* and *y* are lengths along the x-axis and y-axis,
    respectively, we see that *x* and *y* form the sides of a right triangle. The
    length of the hypotenuse of this right triangle is the length of the vector. Therefore,
    according to Pythagoras, and the Babylonians long before him, this length is ![Image](Images/112equ01.jpg),
    which, generalized to *n* dimensions, becomes [Equation 5.3](ch05.xhtml#ch05equ03).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 5.3](ch05.xhtml#ch05equ03)是从哪里来的？考虑一个二维向量***x*** = (*x*, *y*)。如果*x*和*y*分别是沿x轴和y轴的长度，我们就会看到*x*和*y*构成了一个直角三角形的两边。这个直角三角形的斜边长度就是向量的长度。因此，根据毕达哥拉斯定理，以及比他早得多的巴比伦人，这个长度是
    ![Image](Images/112equ01.jpg)，广义到*n*维度后，变为[方程 5.3](ch05.xhtml#ch05equ03)。'
- en: Unit Vectors
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单位向量
- en: Now that we can calculate the magnitude of a vector, we can introduce a useful
    form of a vector known as a *unit vector*. If we divide the components of a vector
    by its magnitude, we’re left with a vector that points in the same direction as
    the original vector but has a magnitude of one. This is the unit vector. For a
    vector, ***v***, the unit vector in the same direction is
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够计算向量的大小，我们可以引入一种有用的向量形式，叫做*单位向量*。如果我们将向量的各个分量除以它的大小，我们就得到了一个与原始向量方向相同但大小为1的向量，这就是单位向量。对于向量***v***，方向相同的单位向量是
- en: '![Image](Images/112equ02.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/112equ02.jpg)'
- en: where the hat over the vector serves to identify it as a unit vector. Let’s
    see a concrete example. Our example vector is ***v*** = (2, –4,3). Therefore,
    the unit vector in the same direction as ***v*** is
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 向量上方的帽子用来标识它是一个单位向量。让我们来看一个具体的例子。我们的示例向量是***v*** = (2, –4,3)。因此，方向与***v***相同的单位向量是
- en: '![Image](Images/112equ03.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/112equ03.jpg)'
- en: 'In code, we calculate the unit vector as the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们计算单位向量的方式如下：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we make use of the fact that to square each element of `v`, we multiply
    it by itself, element-wise, and then add the components together by calling `sum`
    to get the magnitude squared.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们利用了这样一个事实：要平方`v`的每个元素，我们通过将其与自身相乘（逐元素操作），然后通过调用`sum`将各个分量加在一起，得到平方后的大小。
- en: Vector Transpose
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量转置
- en: We mentioned earlier that row vectors can be thought of as 1 × *n* matrices,
    while column vectors are *n* × 1 matrices. The act of changing a row vector into
    a column vector and vice versa is known as taking the *transpose*. We’ll see in
    [Chapter 6](ch06.xhtml#ch06) that the transpose also applies to matrices. Notationally,
    we denote the vector transpose of ***y*** as ***y***^⊤. Therefore, we have
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，行向量可以被看作是1 × *n*矩阵，而列向量是*n* × 1矩阵。将行向量转变为列向量，反之亦然，被称为进行*转置*操作。在[第6章](ch06.xhtml#ch06)中，我们将看到转置也适用于矩阵。在符号上，我们将向量***y***的转置表示为***y***^⊤。因此，我们有
- en: '![Image](Images/113equ01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/113equ01.jpg)'
- en: Of course, we’re not limited to just three components.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不仅仅局限于三个分量。
- en: 'In code, we transpose vectors in several ways. As we saw above, we can use
    `reshape` to reshape the vector into a 1 × *n* or *n* × 1 matrix. We can also
    call the `transpose` method on the vector, with some care, or use the transpose
    shorthand. Let’s see examples of all of these approaches. First, let’s define
    a NumPy vector and see how `reshape` turns it into a 3 × 1 column vector and a
    1 × 3 row vector, as opposed to a plain vector of three elements:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们以多种方式转置向量。正如我们上面所看到的，我们可以使用`reshape`将向量重塑为 1 × *n* 或 *n* × 1 矩阵。我们还可以在向量上调用`transpose`方法，或者小心地使用转置简写。让我们看一下所有这些方法的示例。首先，定义一个
    NumPy 向量，看看`reshape`如何将其变成一个 3 × 1 列向量和一个 1 × 3 行向量，而不是一个包含三个元素的普通向量：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice the difference between the first `print(v)` and the last after calling
    `reshape((1,3))`. The output now has an extra set of brackets around it to indicate
    the leading dimension of one.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第一次调用`print(v)`和最后调用`reshape((1,3))`后的区别。输出现在多了一对括号，以表示前导维度为一。
- en: 'Next, we apply the transpose operation on `v`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对`v`进行转置操作：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we see that calling `transpose` or `T` changes nothing about `v`. This
    is because the shape of `v` is simply `3`, not `(1,3)` or `(3,1)`. If we explicitly
    alter `v` to be a 1 × 3 matrix, we see that `transpose` and `T` have the desired
    effect:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们看到调用`transpose`或`T`并没有改变`v`。这是因为`v`的形状只是`3`，而不是`(1,3)`或`(3,1)`。如果我们显式地将`v`改为
    1 × 3 矩阵，我们会看到`transpose`和`T`产生了期望的效果：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, `v` goes from being a row vector to a column vector, as we expect. The
    lesson, then, is to be careful about the actual dimensionality of vectors in NumPy
    code. Most of the time, we can be sloppy, but sometimes we need to be explicit
    and care about the distinction between plain vectors, row vectors, and column
    vectors.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`v`从行向量变成了列向量，正如我们所期望的那样。那么，教训是，在 NumPy 代码中，要小心向量的实际维度。大多数时候，我们可以不太注意，但有时我们需要明确区分普通向量、行向量和列向量。
- en: Inner Product
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内积
- en: Perhaps the most common vector operation is the *inner product*, or, as it is
    frequently called, the *dot product*. Notationally, the inner product between
    two vectors is written as
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最常见的向量运算是*内积*，或者常被称为*点积*。在符号上，两个向量的内积写作
- en: '![Image](Images/05equ04.jpg)![Image](Images/05equ05.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05equ04.jpg)![图片](Images/05equ05.jpg)'
- en: 'Here, *θ* is the angle between the two vectors if they’re interpreted geometrically.
    The result of the inner product is a scalar. The 〈***a***, ***b***〉 notation is
    seen frequently, though the ***a*** • ***b*** dot notation seems more common in
    the deep learning literature. The ***a***^⊤***b*** matrix multiplication notation
    explicitly calls out how to calculate the inner product, but we’ll wait until
    we discuss matrix multiplication to explain its meaning. For the present, the
    summation tells us what we need to know: the inner product of two vectors of length
    *n* is the sum of the products of the *n* components.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*θ*是两个向量之间的角度（如果从几何角度解释）。内积的结果是标量。〈***a***, ***b***〉符号经常出现，尽管在深度学习文献中，***a***
    • ***b*** 点积符号似乎更为常见。***a***^⊤***b*** 矩阵乘法符号明确指出了如何计算内积，但我们将在讨论矩阵乘法时解释其含义。目前，求和告诉我们所需的知识：两个长度为
    *n* 的向量的内积是 *n* 个分量的乘积之和。
- en: 'The inner product of a vector with itself is the magnitude squared:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 向量与自身的内积是其大小的平方：
- en: '***a*** • ***a*** = ||***a***||²'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '***a*** • ***a*** = ||***a***||²'
- en: The inner product is commutative,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 内积是可交换的，
- en: '***a*** • ***b*** = ***b*** • ***a***'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '***a*** • ***b*** = ***b*** • ***a***'
- en: and distributive,
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 和分配的，
- en: '***a*** • (***b*** + ***c***) = ***a*** • ***b*** + ***a*** • ***c***'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '***a*** • (***b*** + ***c***) = ***a*** • ***b*** + ***a*** • ***c***'
- en: but not associative, as the output of the first inner product is a scalar, not
    a vector, and multiplying a vector by a scalar is not an inner product.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 但不是结合的，因为第一个内积的结果是标量，而不是向量，乘以标量的向量不是内积。
- en: Finally, notice that the inner product is zero when the angle between the vectors
    is 90 degrees; this is because cos *θ* is zero ([Equation 5.5](ch05.xhtml#ch05equ05)).
    This means the two vectors are perpendicular, or *orthogonal*, to each other.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，当向量之间的角度为 90 度时，内积为零；这是因为 cos *θ* 为零（[方程式 5.5](ch05.xhtml#ch05equ05)）。这意味着这两个向量彼此垂直，或者说是*正交*的。
- en: 'Let’s look at some examples of the inner product. First, we’ll be literal and
    implement [Equation 5.4](ch05.xhtml#ch05equ04) explicitly:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几个内积的例子。首先，我们将直观地实现[方程式 5.4](ch05.xhtml#ch05equ04)：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'However, since `a` and `b` are NumPy arrays, we know we can be more efficient:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于 `a` 和 `b` 是 NumPy 数组，我们知道我们可以更高效地计算：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Or, probably most efficient of all, we’ll let NumPy do it for us by using `np.dot`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可能是最有效的做法，我们可以通过使用 `np.dot` 让 NumPy 为我们计算：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You’ll see `np.dot` frequently in deep learning code. It can do more than calculate
    the inner product, as we’ll see below.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习代码中，你会经常看到 `np.dot`。它的功能不仅仅是计算内积，正如我们下面将看到的。
- en: '[Equation 5.5](ch05.xhtml#ch05equ05) tells us that the angle between two vectors
    is'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 5.5](ch05.xhtml#ch05equ05) 告诉我们两向量之间的角度为：'
- en: '![Image](Images/115equ01.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/115equ01.jpg)'
- en: In the code, this could be calculated as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这可以计算为：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This tells us that the angle between ***a*** and ***b*** is approximately 14°
    after converting `t` from radians.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，在将 `t` 从弧度转换后，***a*** 和 ***b*** 之间的角度大约为 14°。
- en: 'If we consider vectors in 3D space, we see that the dot product between orthogonal
    vectors is zero, implying that the angle between them is 90°:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑三维空间中的向量，我们可以看到正交向量之间的点积为零，这意味着它们之间的角度是 90°：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is true because `a` is a unit vector along the x-axis, `b` is a unit vector
    along the y-axis, and we know there’s a right angle between them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以如此，是因为 `a` 是沿 x 轴的单位向量，`b` 是沿 y 轴的单位向量，我们知道它们之间有一个直角。
- en: With the inner product in our toolkit, let’s see how we can use it to project
    one vector onto another.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有了内积作为工具，让我们看看如何用它将一个向量投影到另一个向量上。
- en: Projection
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 投影
- en: The projection of one vector onto another calculates the amount of the first
    vector that’s in the direction of the second. The projection of ***a*** onto ***b***
    is
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个向量在另一个向量上的投影计算的是第一个向量在第二个向量方向上的分量。***a*** 在 ***b*** 上的投影为：
- en: '![Image](Images/116equ01.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/116equ01.jpg)'
- en: '[Figure 5-1](ch05.xhtml#ch05fig01) shows graphically what projection means
    for 2D vectors.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](ch05.xhtml#ch05fig01) 图示了二维向量投影的含义。'
- en: '![image](Images/05fig01.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig01.jpg)'
- en: '*Figure 5-1: A graphical representation of the projection of **a** onto **b**
    in 2D*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-1：**a** 在二维中投影到 **b** 的图形表示*'
- en: Projection finds the component of ***a*** in the direction of ***b***. Note
    the projection of ***a*** onto ***b*** is not the same as the projection of ***b***
    onto ***a***.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 投影找到的是 ***a*** 在 ***b*** 方向上的分量。请注意，***a*** 在 ***b*** 上的投影与 ***b*** 在 ***a***
    上的投影是不一样的。
- en: Because we use an inner product in the numerator, we can see that the projection
    of a vector onto another vector that’s orthogonal to it is zero. No component
    of the first vector is in the direction of the second. Think again of the x-axis
    and y-axis. The entire reason we use Cartesian coordinates is because the two
    axes, or three in 3D space, are all mutually orthogonal; no part of one is in
    the direction of the others. This lets us specify any point, and the vector from
    the origin to that point, by specifying the components along these axes. We’ll
    see this breaking up of an object into mutually orthogonal components later when
    we discuss eigenvectors and PCA in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在分子中使用了内积，所以可以看到，将一个向量投影到与其正交的另一个向量上，其结果为零。第一个向量在第二个向量的方向上没有任何分量。再想一想 x
    轴和 y 轴。我们使用笛卡尔坐标系的整个原因是这两条轴，或者在三维空间中的三条轴，都是互相正交的；一个轴的任何部分都不在其他轴的方向上。这使得我们能够通过指定沿这些轴的分量来确定任何点，以及从原点到该点的向量。稍后，当我们讨论特征向量和主成分分析时，我们将看到如何将一个物体分解成互相正交的分量，见
    [第 6 章](ch06.xhtml#ch06)。
- en: 'In code, calculating the projection is straightforward:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，计算投影是直接的：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the first example, ***a*** points in the direction 45° up from the x-axis,
    while ***b*** points along the x-axis. We’d then expect the projection of ***a***
    to be along the x-axis, which it is (`p`). In the second example, ***c*** points
    in the direction 135° = 90° + 45° from the x-axis. Therefore, we’d expect the
    component of ***c*** along ***b*** to be along the x-axis but in the opposite
    direction from ***b***, which it is.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，***a*** 指向从 x 轴向上 45° 的方向，而 ***b*** 指向 x 轴。我们会预期 ***a*** 的投影沿 x 轴方向，这也是它所做的（`p`）。在第二个例子中，***c***
    指向从 x 轴开始 135° = 90° + 45° 的方向。因此，我们会预期 ***c*** 沿 ***b*** 的分量应沿 x 轴方向，但与 ***b***
    相反方向，这正是它所做的。
- en: '**NOTE**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Projecting **c** along **b** returned a y-axis component of `–0`. The negative
    sign is a quirk of the IEEE 754 representation used for floating-point numbers.
    The significand (mantissa) of the internal representation is zero, but the sign
    can still be specified, leading to an output of negative zero from time to time.
    For a detailed explanation of computer number formats, including floating-point,
    please see my book,* Numbers and Computers *(Springer-Verlag, 2017).*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now to consider the outer product of two vectors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Outer Product
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The inner product of two vectors returned a scalar value. The *outer product*
    of two vectors instead returns a matrix. Note that unlike the inner product, the
    outer product does not require the two vectors to have the same number of components.
    Specifically, for vectors ***a*** of *m* components and ***b*** of *n* components,
    the outer product is the matrix formed by multiplying each element of ***a***
    by each element of ***b***, as shown next.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/118equ01.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: The ***ab***^⊤ notation is how to calculate the outer product via matrix multiplication.
    Notice that this notation is not the same as the inner product, ***a***^⊤***b***,
    and that it assumes ***a*** and ***b*** to be column vectors. No operator symbol
    is consistently used for the outer product, primarily because it’s so easily specified
    via matrix multiplication and because it’s less common than the dot product. However,
    ⊗ seems the most commonly used when the outer product is presented with a binary
    operator.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, NumPy has kindly provided an outer product function for us:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We used `a` and `b` above when discussing the inner product. As expected, `np.dot`
    gives us a scalar output for ***a***•***b***. However, the `np.outer` function
    returns a 4 × 4 matrix, where we see that each row is vector `b` multiplied successively
    by each element of vector `a`, first `1`, then `2`, then `3`, and finally `4`.
    Therefore, each element of `a` has multiplied each element of `b`. The resulting
    matrix is 4 × 4 because both `a` and `b` have four elements.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: THE CARTESIAN PRODUCT
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: There is a direct analogue between the outer product of two vectors and the
    Cartesian product of two sets, *A* and *B*. The *Cartesian product* is a new set,
    each element of which is one of the possible pairings of elements from *A* and
    *B*. So, if *A*={1,2,3,4} and *B*={5,6,7,8}, the Cartesian product can be written
    as
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/118equ02.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Here, we see that if we replace each entry with the product of the pair, we
    get the corresponding vector product we saw above with NumPy `np.outer`. Also,
    note that × is typically used for the Cartesian product when working with sets.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The ability of the outer product to mix all combinations of its inputs has been
    used in deep learning for neural collaborative filtering and visual question answering
    applications. These functions are performed by advanced networks that make recommendations
    or answer text questions about an image. The outer product appears as a mixing
    of two different embedding vectors. *Embeddings* are the vectors generated by
    lower layers of a network, for example, the next to last fully connected layer
    before the softmax layer’s output of a traditional convolutional neural network
    (CNN). The embedding layer is usually viewed as having learned a new representation
    of the network input. It can be thought of as mapping complex inputs, like images,
    to a reduced space of several hundred to several thousands of dimensions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Cross Product
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our final vector-vector operator is the *cross product*. This operator is only
    defined for 3D space (ℝ³). The cross product of ***a*** and ***b*** is a new vector
    that is perpendicular to the plane containing ***a*** and ***b***. Note, this
    does not imply that ***a*** and ***b*** are themselves perpendicular. The cross
    product is defined as
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ06.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: where ![Image](Images/ncap.jpg) is a unit vector and *θ* is the angle between
    ***a*** and ***b***. The direction of ![Image](Images/ncap.jpg) is given by the
    *right-hand rule*. With your right hand, point your index finger in the direction
    of ***a*** and your middle finger in the direction of ***b***. Then, your thumb
    will be pointing in the direction of ![Image](Images/ncap.jpg). [Equation 5.6](ch05.xhtml#ch05equ06)
    gives the actual ℝ³ components of the cross product vector.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy implements the cross product via `np.cross`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the first example, `a` points along the x-axis and `b` along the y-axis.
    Therefore, we expect the cross product to be perpendicular to these axes, and
    it is: the cross product points along the z-axis. The second example shows that
    it doesn’t matter if `a` and `b` are perpendicular to each other. Here, `c` is
    at a 45° angle to the x-axis, but `a` and `c` are still in the xy-plane. Therefore,
    the cross product is still along the z-axis.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the cross product involves sin *θ*, while the inner product
    uses cos *θ*. The inner product is zero when the two vectors are orthogonal to
    each other. The cross product, on the other hand, is zero when the two vectors
    are in the same direction and is maximized when the vectors are perpendicular.
    The second NumPy example above works out because the magnitude of `c` is ![Image](Images/120equ01.jpg)
    and sin ![Image](Images/120equ02.jpg). As a result, the ![Image](Images/120equ01.jpg)
    factors cancel out to leave a magnitude of 1 for the cross product because `a`
    is a unit vector.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The cross product is widely used in physics and other sciences but is less often
    used in deep learning because of its restriction to 3D space. Nontheless, you
    should be familiar with it if you’re going to tackle the deep learning literature.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our look at vector-vector operations. Let’s leave the 1D world
    and move on to consider the most important operation for all deep learning: matrix
    multiplication.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how to multiply two vectors in various ways:
    Hadamard product, inner (dot) product, outer product, and cross product. In this
    section, we’ll investigate multiplication of matrices, recalling that row and
    column vectors are themselves matrices with one row or column.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Matrix Multiplication
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll define the matrix product operation shortly, but, before we do, let’s
    look at the properties of matrix multiplication. Let ***A***, ***B***, and ***C***
    be matrices. Then, following the algebra convention of multiplying symbols by
    placing them next to each other,
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '***(AB)C*** = ***A***(***BC***)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'meaning matrix multiplication is associative. Second, matrix multiplication
    is distributive:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ07.jpg)![Image](Images/05equ08.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'However, in general, matrix multiplication is *not* commutative:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '***AB*** ≠ ***BA***'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Equation 5.8](ch05.xhtml#ch05equ08), matrix multiplication
    over addition from the right produces a different result than matrix multiplication
    over addition from the left, as shown in [Equation 5.7](ch05.xhtml#ch05equ07).
    This explains why we showed both [Equation 5.7](ch05.xhtml#ch05equ07) and [Equation
    5.8](ch05.xhtml#ch05equ08); matrix multiplication can be performed from the left
    or the right, and the result will be different.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: How to Multiply Two Matrices
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To calculate ***AB***, knowing that ***A*** must be on the left of ***B***,
    we first need to verify that the matrices are compatible. It’s only possible to
    multiply two matrices if the number of columns in ***A*** is the same as the number
    of rows in ***B***. Therefore, if ***A*** is an *n × m* matrix and ***B*** is
    an *m × k* matrix, then the product, ***AB***, can be found and will be a new
    *n* × *k* matrix.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the product, we perform a series of inner product multiplications
    between the row vectors of ***A*** and the column vectors of ***B***. [Figure
    5-2](ch05.xhtml#ch05fig02) illustrates the process for a 3 × 3 matrix ***A***
    and a 3 × 2 matrix ***B***.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig02.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: Multiplying a 3* × *3 matrix by a 3* × *2 matrix*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-2](ch05.xhtml#ch05fig02), the first row of the output matrix is
    found by computing the inner product of the first row of ***A*** with each of
    the columns of ***B***. The first element of the output matrix is shown where
    the first row of ***A*** is multiplied by the first column of ***B***. The remaining
    first row of the output matrix is found by repeating the dot product of the first
    row of ***A*** by the remaining column of ***B***.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s present a worked example with actual numbers for the matrices in [Figure
    5-2](ch05.xhtml#ch05fig02):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/121equ01.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Notice, ***AB*** is defined, but ***BA*** is not, because we can’t multiply
    a 3 × 2 matrix by a 3 × 3 matrix. The number of columns in ***B*** needs to be
    the same as the number of rows in ***A***.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of matrix multiplication is by considering what goes into
    making up each of the output matrix elements. For example, if ***A*** is *n* ×
    *m* and ***B*** is *m* × *p*, we know that the matrix product exists as an *n*
    × *p* matrix, ***C***. We find the output elements by computing
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ09.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: for *i* = 0, . . . , *n* − 1 and *j* = 0, . . . , *p* − 1\. In the example above,
    we find *c*[21] by summing the products *a*[20]*b*[01] + *a*[21]*b*[11] + *a*[22]*b*[21],
    which fits [Equation 5.9](ch05.xhtml#ch05equ09) with *i* = 2, *j* = 1 and *k*
    = 0, 1, 2.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 5.9](ch05.xhtml#ch05equ09) tells us how to find a single output matrix
    element. If we loop over *i* and *j*, we can find the entire output matrix. This
    implies a straightforward implementation of matrix multiplication:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’ll assume the arguments, ***A*** and ***B***, are compatible matrices. We
    set the number of rows (`I`) and columns (`J`) of the output matrix, ***C***,
    and use them as the loop limits for the elements of ***C***. We create the output
    matrix, `C`, and give it the same data type as `A`. Then starts a triple loop.
    The loop over `i` covers all the rows of the output. The next loop, over `j`,
    covers the columns of the current row, and the innermost loop, over `k`, covers
    the combining of elements from `A` and `B`, as in [Equation 5.9](ch05.xhtml#ch05equ09).
    When all loops finish, we return the matrix product, `C`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The function `matrixmul` works. It finds the matrix product. However, in terms
    of implementation, it’s quite naive. Advanced algorithms exist, as do many optimizations
    of the naive approach when using compiled code. As we’ll see below, NumPy supports
    matrix multiplication and internally uses highly optimized compiled code libraries
    that far outstrip the performance of the simple code above.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Notation for Inner and Outer Products
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We are now in a position to understand the matrix notation above for the inner
    product, ***a***^⊤***b***, and the outer product, ***ab***^⊤, of two vectors.
    In the first case, we have a 1 × *n* row vector, because of the transpose, and
    an *n* × 1 column vector. The algorithm says to form the inner product of the
    row vector and the column vector to arrive at an output matrix that is 1 × 1,
    that is, a single scalar number. Notice that there must be *n* components in both
    ***a*** and ***b***.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: For the outer product, we have an *n* × 1 column vector on the left and a 1
    × *m* row vector on the right. Therefore, we know the output matrix is *n* × *m*.
    If *m* = *n*, we’ll have an output matrix that’s *n* × *n*. A matrix with as many
    rows as it has columns is a *square matrix*. These have special properties, some
    of which we’ll see in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: To find the outer product of two vectors by matrix multiplication, we multiply
    each element of the rows of ***a*** by each of the columns of ***b** as a row
    vector*,
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/123equ01.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: where each column of ***b***^⊤, a single scalar number, is passed down the rows
    of ***a***, thereby forming each possible product between the elements of the
    two vectors.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen how to perform matrix multiplication manually. Let’s take a look
    now at how NumPy supports matrix multiplication.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication in NumPy
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NumPy provides two different functions that we can use for matrix multiplication.
    The first, we’ve seen already, `np.dot`, though we’ve only used it so far to compute
    inner products of vectors. The second is `np.matmul`, which is also called when
    using the `@` binary operator available in Python 3.5 and later. Matrix multiplication
    with either function works as we expect. However, NumPy sometimes treats 1D arrays
    differently from row or column vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `shape` to decide if a NumPy array is a 1D array, a row vector,
    or a column vector, as shown in [Listing 5-1](ch05.xhtml#ch05ex01):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Listing 5-1: NumPy vectors*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see that a 1D array with three elements, `av`, has a shape different
    from a row vector with three components, `ar`, or a column vector of three components,
    `ac`. However, each of these arrays contains the same three integers: 1, 2, and
    3.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run an experiment to help us understand how NumPy implements matrix multiplication.
    We’ll test `np.dot`, but the results are the same if we use `np.matmul` or the
    `@` operator. We need a collection of vectors and matrices to work with. We’ll
    then apply combinations of them to `np.dot` and consider the output, which may
    very well be an error if the operation is undefined for that combination of arguments.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the arrays, vectors, and matrices we’ll need:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The shape of the objects should be discernible from the definition, if we keep
    the results of [Listing 5-1](ch05.xhtml#ch05ex01) in mind. We’ll also define two
    3 × 3 matrices, `A` and `B`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define a helper function to wrap the call to NumPy so we can trap
    any errors:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This function calls `np.dot` and returns the word `fails` if the call doesn’t
    succeed. [Table 5-1](ch05.xhtml#ch05tab01) shows the output of `dot` for the given
    combinations of the inputs defined above.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-1](ch05.xhtml#ch05tab01) illustrates how NumPy sometimes treats 1D
    arrays differently from row or column vectors. See the difference in [Table 5-1](ch05.xhtml#ch05tab01)
    for `a1,A` versus `ar,A` and `A,ac`. The output of `A,ac` is what we’d expect
    to see mathematically, with the column vector ***a[c]*** multiplied on the left
    by ***A***.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Is there any real difference between `np.dot` and `np.matmul`? Yes, some. For
    1D and 2D arrays, there is no difference. However, there is a difference between
    how each function handles arrays greater than two dimensions, although we won’t
    work with those here. Also, `np.dot` allows one of its arguments to be a scalar
    and multiplies each element of the other argument by it. Multiplying by a scalar
    with `np.matmul` throws an error.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Results of Applying `dot` or `matmul` to Different Types of
    Arguments'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '| **Arguments** | **Result of `np.dot` or `np.matmul`** |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| `a1,b1` | 14 (scalar) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| `a1,br` | fails |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| `a1,bc` | [14] (1 vector) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| `ar,b1` | [14] (1 vector) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| `ar,br` | fails |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| `ar,bc` | [14] (1 × 1 matrix) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| `ac,b1` | fails |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| `ac,br` | ![Image](Images/124equ01.jpg) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| `ac,bc` | fails |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| `A,a1` | [14 32 50] (3 vector) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| `A,ar` | fails |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| `A,ac` | ![Image](Images/124equ02.jpg) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| `a1,A` | [30 36 42] (3 vector) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| `ar,A` | [30 36 42] (1 × 3 matrix) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| `ac,A` | fails |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| `A,B` | ![Image](Images/124equ03.jpg) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: Kronecker Product
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final form of matrix multiplication we’ll discuss is the *Kronecker product*
    or *matrix direct product* of two matrices. When computing the matrix product,
    we mixed individual elements of the matrices, multiplying them together. For the
    Kronecker product, we multiply the elements of one matrix by an entire matrix
    to produce an output matrix that is larger than the input matrices. The Kronecker
    product is also a convenient place to introduce the idea of a *block matrix*,
    or a matrix constructed from smaller matrices (the blocks).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have three matrices
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ10.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: we can define a block matrix, ***M***, as the following.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ01.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: where each element of ***M*** is a smaller matrix stacked on top of each other.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: We can most easily define the Kronecker product using a visual example involving
    a block matrix. The Kronecker product of ***A*** and ***B***, typically written
    as ***A*** ⊗ ***B***, is
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ02.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: for ***A***, an *m* × *n* matrix. This is a block matrix because of ***B***,
    so, when written out completely, the Kronecker product results in a matrix larger
    than either ***A*** or ***B***. Note, unlike matrix multiplication, the Kronecker
    product is defined for arbitrarily sized ***A*** and ***B*** matrices. For example,
    using ***A*** and ***B*** from [Equation 5.10](ch05.xhtml#ch05equ10), the Kronecker
    product is
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ03.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: Notice above that we used ⊗ for the Kronecker product. This is the convention,
    though the symbol ⊗ is sometimes abused and is used for other things too. We used
    it for the outer product of two vectors, for example. NumPy supports the Kronecker
    product via `np.kron`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the mathematical objects used in deep learning:
    scalars, vector, matrices, and tensors. We then explored arithmetic with tensors,
    in particular with vectors and matrices. We saw how to perform operations on these
    objects, both mathematically and in code via NumPy.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration of linear algebra is not complete, however. In the next chapter,
    we’ll dive deeper into matrices and their properties to discuss just a handful
    of the important things that we can do with or know about them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
