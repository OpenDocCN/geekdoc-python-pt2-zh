- en: '**5'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LINEAR ALGEBRA**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Formally, linear algebra is the study of linear equations, in which the highest
    power of the variable is one. However, for our purposes, *linear algebra* refers
    to multidimensional mathematical objects—like vectors and matrices—and operations
    on them. This is how linear algebra is typically applied in deep learning, and
    how data is manipulated in programs that implement deep learning algorithms. By
    making this distinction, we are throwing away a massive amount of fascinating
    mathematics, but as our goal is to understand the mathematics used and applied
    in deep learning, we can hopefully be forgiven.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll introduce the objects used in deep learning, specifically
    scalars, vectors, matrices, and tensors. As we’ll see, all of these objects are
    actually tensors of various orders. We’ll discuss tensors from a mathematical,
    notational perspective and then experiment with them using NumPy. NumPy was explicitly
    designed to add multidimensional arrays to Python, and they are good, though incomplete,
    analogues for the mathematical objects we’ll work with in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll spend the bulk of the chapter learning how to do arithmetic with tensors,
    which is of fundamental importance in deep learning. Most of the effort in implementing
    highly performant deep learning toolkits involves finding ways to do arithmetic
    with tensors as efficiently as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars, Vectors, Matrices, and Tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s introduce our cast of characters. I’ll relate them to Python variables
    and NumPy arrays to show how we’ll implement these objects in code. Then I’ll
    present a handy conceptual mapping between tensors and geometry.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Even if you’re not familiar with the word, you’ve known what a scalar is since
    the day you first learned to count. A *scalar* is just a number, like 7, 42, or
    π. In expressions, we’ll use *x* to mean a scalar, that is, the ordinary notation
    used for variables. To a computer, a scalar is a simple numeric variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *vector* is a 1D array of numbers. Mathematically, a vector has an orientation,
    either horizontal or vertical. If horizontal, it’s a *row vector*. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is a row vector of three elements or components. Note, we’ll use *x*, a lowercase
    letter in bold, to mean a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, vectors are usually assumed to be *column vectors*,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ***y*** has four components, making it a four-dimensional (4D) vector.
    Notice that in [Equation 5.1](ch05.xhtml#ch05equ01) we used square brackets, whereas
    in [Equation 5.2](ch05.xhtml#ch05equ02) we used parentheses. Either notation is
    acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we usually implement vectors as 1D arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’ve used `reshape` to turn the three-element row vector into a column
    vector of three rows and one column.
  prefs: []
  type: TYPE_NORMAL
- en: The components of a vector are often interpreted as lengths along a set of coordinate
    axes. For example, a three-component vector might be used to represent a point
    in 3D space. In this vector,
  prefs: []
  type: TYPE_NORMAL
- en: '***x*** = [*x*, *y*, *z*]'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* could be the length along the x-axis, *y* the length along the y-axis,
    and *z* the length along the z-axis. These are the Cartesian coordinates and serve
    to uniquely identify all points in 3D space.'
  prefs: []
  type: TYPE_NORMAL
- en: However, in deep learning, and machine learning in general, the components of
    a vector are often unrelated to each other in any strict geometric sense. Rather,
    they’re used to represent *features*, qualities of some sample that the model
    will use to attempt to arrive at a useful output, like a class label, or a regression
    value. That said, the vector representing the collection of features, called the
    *feature vector*, is sometimes thought about geometrically. For example, some
    machine learning models, like *k*-nearest neighbors, interpret the vector as representing
    some coordinate in geometric space.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll often hear deep learning people discuss the *feature space* of a problem.
    The feature space refers to the set of possible inputs. The training set for a
    model needs to accurately represent the feature space of the possible inputs the
    model will encounter when used. In this sense, the feature vector is a point,
    a location in this *n*-dimensional space where *n* is the number of features in
    the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A *matrix* is a 2D array of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/105equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The elements of ***A*** are subscripted by the row number and column number.
    The matrix ***A*** has three rows and four columns, so we say that it’s a 3 ×
    4 matrix, where 3 × 4 is the *order* of the matrix. Notice that ***A*** uses subscripts
    starting with 0\. Math texts often begin with 1, but increasingly, they’re using
    0 so that there isn’t an offset between the math notation and the computer representation
    of the matrix. Note, also, that we’ll use ***A***, an uppercase letter in bold,
    to mean a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, matrices are represented as 2D arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To get element *a*[12] of ***A*** in Python, we write `A[1,2]`. Notice that
    when we printed the arrays, there was an extra `[` and `]` around them. NumPy
    uses these brackets to indicate that the 2D array can be thought of as a row vector
    in which each element is itself a vector. In Python-speak, this means that a matrix
    can be thought of as a list of sublists in which each sublist is of the same length.
    Of course, this is exactly how we defined `A` to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of vectors as matrices with a single row or column. A column vector
    with three elements is a 3 × 1 matrix: it has three rows and one column. Similarly,
    a row vector of four elements acts like a 1 × 4 matrix: it has one row and four
    columns. We’ll make use of this observation later.'
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A scalar has no dimensions, a vector has one, and a matrix has two. As you
    might suspect, we don’t need to stop there. A mathematical object with more than
    two dimensions is colloquially referred to as a *tensor*. When necessary, we’ll
    represent tensors like this: `T`, as a sans serif capital letter.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of dimensions a tensor has defines its *order*, which is not to be
    confused with the order of a matrix. A 3D tensor has order 3\. A matrix is a tensor
    of order 2\. A vector is an order-1 tensor, and a scalar is an order-0 tensor.
    When we discuss the flow of data through a deep neural network in [Chapter 9](ch09.xhtml#ch09),
    we’ll see that many toolkits use tensors of order 4 (or more).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, NumPy arrays with three or more dimensions are used to implement
    tensors. For example, we can define an order-3 tensor in Python as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we use `np.arange` to define `t` to be a vector of 36 elements holding
    the numbers 0 . . . 35\. Then, we immediately `reshape` the vector into a tensor
    of 3 × 3 × 4 elements (3 × 3 × 4 = 36). One way to think of a 3 × 3 × 4 tensor
    is that it contains a stack of three 3 × 4 images. If we keep this in mind, the
    following statements make sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Asking for `t[0]` will return the first 3 × 4 *image* in the stack. Asking for
    `t[0,1]`, then, should return the second row of the first image, which it does.
    Finally, we get to an individual element of `t` by asking for the image number
    (`0`), the row number (`1`), and the element of that row (`2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Assigning the dimensions of a tensor to successively smaller collections of
    something is a handy way to keep the meaning of the dimensions in mind. For example,
    we can define an order-5 tensor like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: But, what does asking for `w[4,1,2,0,1]` mean? The exact meaning depends on
    the application. For example, we might think of `w` as representing a bookcase.
    The first index selects the shelf, and the second selects the book on the shelf.
    Then, the third index selects the page within the book, and the fourth selects
    the line on the page. The final index selects the word on the line. Therefore,
    `w[4,1,2,0,1]` is asking for the second word of the first line of the third page
    of the second book on the fifth shelf of the bookcase, understood by reading the
    indices from right to left.
  prefs: []
  type: TYPE_NORMAL
- en: The bookcase analogy does have its limitations. NumPy arrays have fixed dimensions,
    meaning that if `w` is a bookcase, there are nine shelves, and each shelf has
    *exactly* nine books. Likewise, each book has exactly nine pages, and each page
    has nine lines. Finally, each line has precisely nine words. NumPy arrays ordinarily
    use contiguous memory in the computer, so the size of each dimension is fixed
    when the array is defined. Doing so, and selecting the specific data type, like
    unsigned integer, makes locating an element of the array an indexing operation
    using a simple formula to compute an offset from a base memory address. This is
    what makes NumPy arrays so much faster than Python lists.
  prefs: []
  type: TYPE_NORMAL
- en: Any tensor of less than order *n* can be represented as an order-*n* tensor
    by supplying the missing dimensions of length one. We saw an example of this above
    when I said that an *m*-component vector could be thought of as a 1 × *m* or an
    *m* × 1 matrix. The order-1 tensor (the vector) is turned into an order-2 tensor
    (matrix) by adding a missing dimension of length one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an extreme example, we can treat a scalar (order-0 tensor) as an order-5
    tensor, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we `reshape` the scalar `42` into an order-5 tensor (a five-dimensional
    [5D] array) with length one on each axis. Notice that NumPy tells us that the
    tensor `t` has five dimensions with the `[[[[[` and `]]]]]` around `42`. Asking
    for the shape of `t` confirms that it is a 5D tensor. Finally, as a tensor, we
    can get the value of the single element it contains by specifying all the dimensions
    with `t[0,0,0,0,0]`. We’ll often use this trick of adding new dimensions of length
    one. In fact, in NumPy, there is a way to do this directly, which you’ll see when
    using deep learning toolkits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’ve turned `t`, an order-2 tensor (a matrix), into an order-3 tensor
    by using `np.newaxis` to create a new axis of length one. That’s why `w.shape`
    returns `(1,2,3)` and not `(2,3)`, as it would for `t`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are analogues between tensors up to order-3 and geometry that are helpful
    in visualizing the relationships between the different orders:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Order (dimensions)** | **Tensor name** | **Geometric name** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Scalar | Point |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Vector | Line |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Matrix | Plane |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Tensor | Volume |'
  prefs: []
  type: TYPE_TB
- en: Notice, I used *tensor* in its common sense in the table. There seems to be
    no standardized name for an order-3 tensor.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we defined the mathematical objects of deep learning in relation
    to multidimensional arrays, since that’s how they are implemented in code. We’ve
    thrown away a lot of mathematics by doing this, but we’ve preserved what we need
    to understand deep learning. Let’s move on now and see how to use tensors in expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic with Tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of this section is to detail operations on tensors, with special
    emphasis on tensors of order-1 (vectors) and order-2 (matrices). We’ll assume
    operations with scalars are well in hand at this point.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with what I’m calling *array operations*, by which I mean the element-wise
    operations that toolkits like NumPy perform on arrays of all dimensions. Then
    we’ll move on to operations particular to vectors. This sets the stage for the
    critical topic of matrix multiplication. Finally, we’ll discuss block matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Array Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The way we’ve used the NumPy toolkit so far has shown us that all the normal
    scalar arithmetic operations translate directly into the world of multidimensional
    arrays. This includes standard operations like addition, subtraction, multiplication,
    division, and exponentiation, as well as the application of functions to an array.
    In all of these cases, the scalar operation is applied element-wise to each element
    of the array. The examples here will set the tone for the rest of this section
    and will also let us explore some NumPy broadcasting rules that we haven’t called
    out yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first define some arrays to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Element-wise arithmetic is straightforward for arrays with dimensions that
    match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: These results are all easy enough to interpret; NumPy applies the desired operation
    to the corresponding elements of each array. Element-wise multiplication on two
    matrices (`a` and `b`) is often known as the *Hadamard product*. (You’ll encounter
    this term from time to time in the deep learning literature.)
  prefs: []
  type: TYPE_NORMAL
- en: The NumPy toolkit extends the idea of element-wise operations into what it calls
    *broadcasting*. When broadcasting, NumPy applies rules, which we’ll see via examples,
    where one array is passed over another to produce a meaningful output.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already encountered a form of broadcasting when operating on an array
    with a scalar. In that case, the scalar value was broadcast to every value of
    the array.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our first example, even though `a` is a 2 × 3 matrix, NumPy allows operations
    on it with `c`, a three-component vector, by applying broadcasting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, the three-component vector, `c`, has been broadcast over the rows of the
    2 × 3 matrix, `a`. NumPy recognized that the last dimensions of `a` and `c` were
    both three, so the vector could be passed over the matrix to produce the given
    output. When looking at deep learning code, much of which is in Python, you’ll
    see situations like this. At times, some thought is necessary, along with some
    experimentation at the Python prompt, to understand what’s happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we broadcast `d`, a two-component vector, over `a`, a 2 × 3 matrix? If
    we try to do so the same way we broadcast `c` over `a`, we’ll fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the broadcasting rules for NumPy accommodate dimensions of length
    one. The shape of `d` is 2; it’s a two-element vector. If we reshape `d` so that
    it’s a 2D array with shape 2 × 1, we’ll give NumPy what it needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We now see that Numpy has added `d` across the columns of `a`.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the world of mathematics and look at operations on vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vectors are represented in code as a collection of numbers that can be interpreted
    as values along a set of coordinate axes. Here, we’ll define several operations
    that are unique to vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Geometrically, we can understand vectors as having a direction and a length.
    They’re often drawn as arrows, and we’ll see an example of a vector plot in [Chapter
    6](ch06.xhtml#ch06). People speak of the length of a vector as its *magnitude*.
    Therefore, the first vector operation we’ll consider is calculating its magnitude.
    For a vector, ***x***, with *n* components, the formula for its magnitude is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In [Equation 5.3](ch05.xhtml#ch05equ03), the double vertical bars around the
    vector represent its magnitude. You’ll often see people use single bars here as
    well. Single bars are also used for absolute value; we usually rely on context
    to tell the difference between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Where did [Equation 5.3](ch05.xhtml#ch05equ03) come from? Consider a vector
    in 2D, ***x*** = (*x*, *y*). If *x* and *y* are lengths along the x-axis and y-axis,
    respectively, we see that *x* and *y* form the sides of a right triangle. The
    length of the hypotenuse of this right triangle is the length of the vector. Therefore,
    according to Pythagoras, and the Babylonians long before him, this length is ![Image](Images/112equ01.jpg),
    which, generalized to *n* dimensions, becomes [Equation 5.3](ch05.xhtml#ch05equ03).
  prefs: []
  type: TYPE_NORMAL
- en: Unit Vectors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we can calculate the magnitude of a vector, we can introduce a useful
    form of a vector known as a *unit vector*. If we divide the components of a vector
    by its magnitude, we’re left with a vector that points in the same direction as
    the original vector but has a magnitude of one. This is the unit vector. For a
    vector, ***v***, the unit vector in the same direction is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/112equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the hat over the vector serves to identify it as a unit vector. Let’s
    see a concrete example. Our example vector is ***v*** = (2, –4,3). Therefore,
    the unit vector in the same direction as ***v*** is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/112equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In code, we calculate the unit vector as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we make use of the fact that to square each element of `v`, we multiply
    it by itself, element-wise, and then add the components together by calling `sum`
    to get the magnitude squared.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Transpose
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We mentioned earlier that row vectors can be thought of as 1 × *n* matrices,
    while column vectors are *n* × 1 matrices. The act of changing a row vector into
    a column vector and vice versa is known as taking the *transpose*. We’ll see in
    [Chapter 6](ch06.xhtml#ch06) that the transpose also applies to matrices. Notationally,
    we denote the vector transpose of ***y*** as ***y***^⊤. Therefore, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/113equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Of course, we’re not limited to just three components.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we transpose vectors in several ways. As we saw above, we can use
    `reshape` to reshape the vector into a 1 × *n* or *n* × 1 matrix. We can also
    call the `transpose` method on the vector, with some care, or use the transpose
    shorthand. Let’s see examples of all of these approaches. First, let’s define
    a NumPy vector and see how `reshape` turns it into a 3 × 1 column vector and a
    1 × 3 row vector, as opposed to a plain vector of three elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice the difference between the first `print(v)` and the last after calling
    `reshape((1,3))`. The output now has an extra set of brackets around it to indicate
    the leading dimension of one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we apply the transpose operation on `v`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see that calling `transpose` or `T` changes nothing about `v`. This
    is because the shape of `v` is simply `3`, not `(1,3)` or `(3,1)`. If we explicitly
    alter `v` to be a 1 × 3 matrix, we see that `transpose` and `T` have the desired
    effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, `v` goes from being a row vector to a column vector, as we expect. The
    lesson, then, is to be careful about the actual dimensionality of vectors in NumPy
    code. Most of the time, we can be sloppy, but sometimes we need to be explicit
    and care about the distinction between plain vectors, row vectors, and column
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Inner Product
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Perhaps the most common vector operation is the *inner product*, or, as it is
    frequently called, the *dot product*. Notationally, the inner product between
    two vectors is written as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ04.jpg)![Image](Images/05equ05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *θ* is the angle between the two vectors if they’re interpreted geometrically.
    The result of the inner product is a scalar. The 〈***a***, ***b***〉 notation is
    seen frequently, though the ***a*** • ***b*** dot notation seems more common in
    the deep learning literature. The ***a***^⊤***b*** matrix multiplication notation
    explicitly calls out how to calculate the inner product, but we’ll wait until
    we discuss matrix multiplication to explain its meaning. For the present, the
    summation tells us what we need to know: the inner product of two vectors of length
    *n* is the sum of the products of the *n* components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The inner product of a vector with itself is the magnitude squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '***a*** • ***a*** = ||***a***||²'
  prefs: []
  type: TYPE_NORMAL
- en: The inner product is commutative,
  prefs: []
  type: TYPE_NORMAL
- en: '***a*** • ***b*** = ***b*** • ***a***'
  prefs: []
  type: TYPE_NORMAL
- en: and distributive,
  prefs: []
  type: TYPE_NORMAL
- en: '***a*** • (***b*** + ***c***) = ***a*** • ***b*** + ***a*** • ***c***'
  prefs: []
  type: TYPE_NORMAL
- en: but not associative, as the output of the first inner product is a scalar, not
    a vector, and multiplying a vector by a scalar is not an inner product.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, notice that the inner product is zero when the angle between the vectors
    is 90 degrees; this is because cos *θ* is zero ([Equation 5.5](ch05.xhtml#ch05equ05)).
    This means the two vectors are perpendicular, or *orthogonal*, to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some examples of the inner product. First, we’ll be literal and
    implement [Equation 5.4](ch05.xhtml#ch05equ04) explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'However, since `a` and `b` are NumPy arrays, we know we can be more efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, probably most efficient of all, we’ll let NumPy do it for us by using `np.dot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see `np.dot` frequently in deep learning code. It can do more than calculate
    the inner product, as we’ll see below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 5.5](ch05.xhtml#ch05equ05) tells us that the angle between two vectors
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/115equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the code, this could be calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that the angle between ***a*** and ***b*** is approximately 14°
    after converting `t` from radians.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider vectors in 3D space, we see that the dot product between orthogonal
    vectors is zero, implying that the angle between them is 90°:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This is true because `a` is a unit vector along the x-axis, `b` is a unit vector
    along the y-axis, and we know there’s a right angle between them.
  prefs: []
  type: TYPE_NORMAL
- en: With the inner product in our toolkit, let’s see how we can use it to project
    one vector onto another.
  prefs: []
  type: TYPE_NORMAL
- en: Projection
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The projection of one vector onto another calculates the amount of the first
    vector that’s in the direction of the second. The projection of ***a*** onto ***b***
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/116equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 5-1](ch05.xhtml#ch05fig01) shows graphically what projection means
    for 2D vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-1: A graphical representation of the projection of **a** onto **b**
    in 2D*'
  prefs: []
  type: TYPE_NORMAL
- en: Projection finds the component of ***a*** in the direction of ***b***. Note
    the projection of ***a*** onto ***b*** is not the same as the projection of ***b***
    onto ***a***.
  prefs: []
  type: TYPE_NORMAL
- en: Because we use an inner product in the numerator, we can see that the projection
    of a vector onto another vector that’s orthogonal to it is zero. No component
    of the first vector is in the direction of the second. Think again of the x-axis
    and y-axis. The entire reason we use Cartesian coordinates is because the two
    axes, or three in 3D space, are all mutually orthogonal; no part of one is in
    the direction of the others. This lets us specify any point, and the vector from
    the origin to that point, by specifying the components along these axes. We’ll
    see this breaking up of an object into mutually orthogonal components later when
    we discuss eigenvectors and PCA in [Chapter 6](ch06.xhtml#ch06).
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, calculating the projection is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the first example, ***a*** points in the direction 45° up from the x-axis,
    while ***b*** points along the x-axis. We’d then expect the projection of ***a***
    to be along the x-axis, which it is (`p`). In the second example, ***c*** points
    in the direction 135° = 90° + 45° from the x-axis. Therefore, we’d expect the
    component of ***c*** along ***b*** to be along the x-axis but in the opposite
    direction from ***b***, which it is.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Projecting **c** along **b** returned a y-axis component of `–0`. The negative
    sign is a quirk of the IEEE 754 representation used for floating-point numbers.
    The significand (mantissa) of the internal representation is zero, but the sign
    can still be specified, leading to an output of negative zero from time to time.
    For a detailed explanation of computer number formats, including floating-point,
    please see my book,* Numbers and Computers *(Springer-Verlag, 2017).*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on now to consider the outer product of two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Outer Product
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The inner product of two vectors returned a scalar value. The *outer product*
    of two vectors instead returns a matrix. Note that unlike the inner product, the
    outer product does not require the two vectors to have the same number of components.
    Specifically, for vectors ***a*** of *m* components and ***b*** of *n* components,
    the outer product is the matrix formed by multiplying each element of ***a***
    by each element of ***b***, as shown next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/118equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ***ab***^⊤ notation is how to calculate the outer product via matrix multiplication.
    Notice that this notation is not the same as the inner product, ***a***^⊤***b***,
    and that it assumes ***a*** and ***b*** to be column vectors. No operator symbol
    is consistently used for the outer product, primarily because it’s so easily specified
    via matrix multiplication and because it’s less common than the dot product. However,
    ⊗ seems the most commonly used when the outer product is presented with a binary
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, NumPy has kindly provided an outer product function for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We used `a` and `b` above when discussing the inner product. As expected, `np.dot`
    gives us a scalar output for ***a***•***b***. However, the `np.outer` function
    returns a 4 × 4 matrix, where we see that each row is vector `b` multiplied successively
    by each element of vector `a`, first `1`, then `2`, then `3`, and finally `4`.
    Therefore, each element of `a` has multiplied each element of `b`. The resulting
    matrix is 4 × 4 because both `a` and `b` have four elements.
  prefs: []
  type: TYPE_NORMAL
- en: THE CARTESIAN PRODUCT
  prefs: []
  type: TYPE_NORMAL
- en: There is a direct analogue between the outer product of two vectors and the
    Cartesian product of two sets, *A* and *B*. The *Cartesian product* is a new set,
    each element of which is one of the possible pairings of elements from *A* and
    *B*. So, if *A*={1,2,3,4} and *B*={5,6,7,8}, the Cartesian product can be written
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/118equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see that if we replace each entry with the product of the pair, we
    get the corresponding vector product we saw above with NumPy `np.outer`. Also,
    note that × is typically used for the Cartesian product when working with sets.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of the outer product to mix all combinations of its inputs has been
    used in deep learning for neural collaborative filtering and visual question answering
    applications. These functions are performed by advanced networks that make recommendations
    or answer text questions about an image. The outer product appears as a mixing
    of two different embedding vectors. *Embeddings* are the vectors generated by
    lower layers of a network, for example, the next to last fully connected layer
    before the softmax layer’s output of a traditional convolutional neural network
    (CNN). The embedding layer is usually viewed as having learned a new representation
    of the network input. It can be thought of as mapping complex inputs, like images,
    to a reduced space of several hundred to several thousands of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Cross Product
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our final vector-vector operator is the *cross product*. This operator is only
    defined for 3D space (ℝ³). The cross product of ***a*** and ***b*** is a new vector
    that is perpendicular to the plane containing ***a*** and ***b***. Note, this
    does not imply that ***a*** and ***b*** are themselves perpendicular. The cross
    product is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where ![Image](Images/ncap.jpg) is a unit vector and *θ* is the angle between
    ***a*** and ***b***. The direction of ![Image](Images/ncap.jpg) is given by the
    *right-hand rule*. With your right hand, point your index finger in the direction
    of ***a*** and your middle finger in the direction of ***b***. Then, your thumb
    will be pointing in the direction of ![Image](Images/ncap.jpg). [Equation 5.6](ch05.xhtml#ch05equ06)
    gives the actual ℝ³ components of the cross product vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy implements the cross product via `np.cross`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first example, `a` points along the x-axis and `b` along the y-axis.
    Therefore, we expect the cross product to be perpendicular to these axes, and
    it is: the cross product points along the z-axis. The second example shows that
    it doesn’t matter if `a` and `b` are perpendicular to each other. Here, `c` is
    at a 45° angle to the x-axis, but `a` and `c` are still in the xy-plane. Therefore,
    the cross product is still along the z-axis.'
  prefs: []
  type: TYPE_NORMAL
- en: The definition of the cross product involves sin *θ*, while the inner product
    uses cos *θ*. The inner product is zero when the two vectors are orthogonal to
    each other. The cross product, on the other hand, is zero when the two vectors
    are in the same direction and is maximized when the vectors are perpendicular.
    The second NumPy example above works out because the magnitude of `c` is ![Image](Images/120equ01.jpg)
    and sin ![Image](Images/120equ02.jpg). As a result, the ![Image](Images/120equ01.jpg)
    factors cancel out to leave a magnitude of 1 for the cross product because `a`
    is a unit vector.
  prefs: []
  type: TYPE_NORMAL
- en: The cross product is widely used in physics and other sciences but is less often
    used in deep learning because of its restriction to 3D space. Nontheless, you
    should be familiar with it if you’re going to tackle the deep learning literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concludes our look at vector-vector operations. Let’s leave the 1D world
    and move on to consider the most important operation for all deep learning: matrix
    multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how to multiply two vectors in various ways:
    Hadamard product, inner (dot) product, outer product, and cross product. In this
    section, we’ll investigate multiplication of matrices, recalling that row and
    column vectors are themselves matrices with one row or column.'
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Matrix Multiplication
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll define the matrix product operation shortly, but, before we do, let’s
    look at the properties of matrix multiplication. Let ***A***, ***B***, and ***C***
    be matrices. Then, following the algebra convention of multiplying symbols by
    placing them next to each other,
  prefs: []
  type: TYPE_NORMAL
- en: '***(AB)C*** = ***A***(***BC***)'
  prefs: []
  type: TYPE_NORMAL
- en: 'meaning matrix multiplication is associative. Second, matrix multiplication
    is distributive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ07.jpg)![Image](Images/05equ08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in general, matrix multiplication is *not* commutative:'
  prefs: []
  type: TYPE_NORMAL
- en: '***AB*** ≠ ***BA***'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Equation 5.8](ch05.xhtml#ch05equ08), matrix multiplication
    over addition from the right produces a different result than matrix multiplication
    over addition from the left, as shown in [Equation 5.7](ch05.xhtml#ch05equ07).
    This explains why we showed both [Equation 5.7](ch05.xhtml#ch05equ07) and [Equation
    5.8](ch05.xhtml#ch05equ08); matrix multiplication can be performed from the left
    or the right, and the result will be different.
  prefs: []
  type: TYPE_NORMAL
- en: How to Multiply Two Matrices
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To calculate ***AB***, knowing that ***A*** must be on the left of ***B***,
    we first need to verify that the matrices are compatible. It’s only possible to
    multiply two matrices if the number of columns in ***A*** is the same as the number
    of rows in ***B***. Therefore, if ***A*** is an *n × m* matrix and ***B*** is
    an *m × k* matrix, then the product, ***AB***, can be found and will be a new
    *n* × *k* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the product, we perform a series of inner product multiplications
    between the row vectors of ***A*** and the column vectors of ***B***. [Figure
    5-2](ch05.xhtml#ch05fig02) illustrates the process for a 3 × 3 matrix ***A***
    and a 3 × 2 matrix ***B***.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](Images/05fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5-2: Multiplying a 3* × *3 matrix by a 3* × *2 matrix*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 5-2](ch05.xhtml#ch05fig02), the first row of the output matrix is
    found by computing the inner product of the first row of ***A*** with each of
    the columns of ***B***. The first element of the output matrix is shown where
    the first row of ***A*** is multiplied by the first column of ***B***. The remaining
    first row of the output matrix is found by repeating the dot product of the first
    row of ***A*** by the remaining column of ***B***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s present a worked example with actual numbers for the matrices in [Figure
    5-2](ch05.xhtml#ch05fig02):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/121equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice, ***AB*** is defined, but ***BA*** is not, because we can’t multiply
    a 3 × 2 matrix by a 3 × 3 matrix. The number of columns in ***B*** needs to be
    the same as the number of rows in ***A***.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think of matrix multiplication is by considering what goes into
    making up each of the output matrix elements. For example, if ***A*** is *n* ×
    *m* and ***B*** is *m* × *p*, we know that the matrix product exists as an *n*
    × *p* matrix, ***C***. We find the output elements by computing
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for *i* = 0, . . . , *n* − 1 and *j* = 0, . . . , *p* − 1\. In the example above,
    we find *c*[21] by summing the products *a*[20]*b*[01] + *a*[21]*b*[11] + *a*[22]*b*[21],
    which fits [Equation 5.9](ch05.xhtml#ch05equ09) with *i* = 2, *j* = 1 and *k*
    = 0, 1, 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 5.9](ch05.xhtml#ch05equ09) tells us how to find a single output matrix
    element. If we loop over *i* and *j*, we can find the entire output matrix. This
    implies a straightforward implementation of matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We’ll assume the arguments, ***A*** and ***B***, are compatible matrices. We
    set the number of rows (`I`) and columns (`J`) of the output matrix, ***C***,
    and use them as the loop limits for the elements of ***C***. We create the output
    matrix, `C`, and give it the same data type as `A`. Then starts a triple loop.
    The loop over `i` covers all the rows of the output. The next loop, over `j`,
    covers the columns of the current row, and the innermost loop, over `k`, covers
    the combining of elements from `A` and `B`, as in [Equation 5.9](ch05.xhtml#ch05equ09).
    When all loops finish, we return the matrix product, `C`.
  prefs: []
  type: TYPE_NORMAL
- en: The function `matrixmul` works. It finds the matrix product. However, in terms
    of implementation, it’s quite naive. Advanced algorithms exist, as do many optimizations
    of the naive approach when using compiled code. As we’ll see below, NumPy supports
    matrix multiplication and internally uses highly optimized compiled code libraries
    that far outstrip the performance of the simple code above.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Notation for Inner and Outer Products
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We are now in a position to understand the matrix notation above for the inner
    product, ***a***^⊤***b***, and the outer product, ***ab***^⊤, of two vectors.
    In the first case, we have a 1 × *n* row vector, because of the transpose, and
    an *n* × 1 column vector. The algorithm says to form the inner product of the
    row vector and the column vector to arrive at an output matrix that is 1 × 1,
    that is, a single scalar number. Notice that there must be *n* components in both
    ***a*** and ***b***.
  prefs: []
  type: TYPE_NORMAL
- en: For the outer product, we have an *n* × 1 column vector on the left and a 1
    × *m* row vector on the right. Therefore, we know the output matrix is *n* × *m*.
    If *m* = *n*, we’ll have an output matrix that’s *n* × *n*. A matrix with as many
    rows as it has columns is a *square matrix*. These have special properties, some
    of which we’ll see in [Chapter 6](ch06.xhtml#ch06).
  prefs: []
  type: TYPE_NORMAL
- en: To find the outer product of two vectors by matrix multiplication, we multiply
    each element of the rows of ***a*** by each of the columns of ***b** as a row
    vector*,
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/123equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where each column of ***b***^⊤, a single scalar number, is passed down the rows
    of ***a***, thereby forming each possible product between the elements of the
    two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve seen how to perform matrix multiplication manually. Let’s take a look
    now at how NumPy supports matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Multiplication in NumPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NumPy provides two different functions that we can use for matrix multiplication.
    The first, we’ve seen already, `np.dot`, though we’ve only used it so far to compute
    inner products of vectors. The second is `np.matmul`, which is also called when
    using the `@` binary operator available in Python 3.5 and later. Matrix multiplication
    with either function works as we expect. However, NumPy sometimes treats 1D arrays
    differently from row or column vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `shape` to decide if a NumPy array is a 1D array, a row vector,
    or a column vector, as shown in [Listing 5-1](ch05.xhtml#ch05ex01):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 5-1: NumPy vectors*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see that a 1D array with three elements, `av`, has a shape different
    from a row vector with three components, `ar`, or a column vector of three components,
    `ac`. However, each of these arrays contains the same three integers: 1, 2, and
    3.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run an experiment to help us understand how NumPy implements matrix multiplication.
    We’ll test `np.dot`, but the results are the same if we use `np.matmul` or the
    `@` operator. We need a collection of vectors and matrices to work with. We’ll
    then apply combinations of them to `np.dot` and consider the output, which may
    very well be an error if the operation is undefined for that combination of arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the arrays, vectors, and matrices we’ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The shape of the objects should be discernible from the definition, if we keep
    the results of [Listing 5-1](ch05.xhtml#ch05ex01) in mind. We’ll also define two
    3 × 3 matrices, `A` and `B`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll define a helper function to wrap the call to NumPy so we can trap
    any errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This function calls `np.dot` and returns the word `fails` if the call doesn’t
    succeed. [Table 5-1](ch05.xhtml#ch05tab01) shows the output of `dot` for the given
    combinations of the inputs defined above.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5-1](ch05.xhtml#ch05tab01) illustrates how NumPy sometimes treats 1D
    arrays differently from row or column vectors. See the difference in [Table 5-1](ch05.xhtml#ch05tab01)
    for `a1,A` versus `ar,A` and `A,ac`. The output of `A,ac` is what we’d expect
    to see mathematically, with the column vector ***a[c]*** multiplied on the left
    by ***A***.'
  prefs: []
  type: TYPE_NORMAL
- en: Is there any real difference between `np.dot` and `np.matmul`? Yes, some. For
    1D and 2D arrays, there is no difference. However, there is a difference between
    how each function handles arrays greater than two dimensions, although we won’t
    work with those here. Also, `np.dot` allows one of its arguments to be a scalar
    and multiplies each element of the other argument by it. Multiplying by a scalar
    with `np.matmul` throws an error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 5-1:** Results of Applying `dot` or `matmul` to Different Types of
    Arguments'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Arguments** | **Result of `np.dot` or `np.matmul`** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `a1,b1` | 14 (scalar) |'
  prefs: []
  type: TYPE_TB
- en: '| `a1,br` | fails |'
  prefs: []
  type: TYPE_TB
- en: '| `a1,bc` | [14] (1 vector) |'
  prefs: []
  type: TYPE_TB
- en: '| `ar,b1` | [14] (1 vector) |'
  prefs: []
  type: TYPE_TB
- en: '| `ar,br` | fails |'
  prefs: []
  type: TYPE_TB
- en: '| `ar,bc` | [14] (1 × 1 matrix) |'
  prefs: []
  type: TYPE_TB
- en: '| `ac,b1` | fails |'
  prefs: []
  type: TYPE_TB
- en: '| `ac,br` | ![Image](Images/124equ01.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| `ac,bc` | fails |'
  prefs: []
  type: TYPE_TB
- en: '| `A,a1` | [14 32 50] (3 vector) |'
  prefs: []
  type: TYPE_TB
- en: '| `A,ar` | fails |'
  prefs: []
  type: TYPE_TB
- en: '| `A,ac` | ![Image](Images/124equ02.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| `a1,A` | [30 36 42] (3 vector) |'
  prefs: []
  type: TYPE_TB
- en: '| `ar,A` | [30 36 42] (1 × 3 matrix) |'
  prefs: []
  type: TYPE_TB
- en: '| `ac,A` | fails |'
  prefs: []
  type: TYPE_TB
- en: '| `A,B` | ![Image](Images/124equ03.jpg) |'
  prefs: []
  type: TYPE_TB
- en: Kronecker Product
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final form of matrix multiplication we’ll discuss is the *Kronecker product*
    or *matrix direct product* of two matrices. When computing the matrix product,
    we mixed individual elements of the matrices, multiplying them together. For the
    Kronecker product, we multiply the elements of one matrix by an entire matrix
    to produce an output matrix that is larger than the input matrices. The Kronecker
    product is also a convenient place to introduce the idea of a *block matrix*,
    or a matrix constructed from smaller matrices (the blocks).
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have three matrices
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/05equ10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: we can define a block matrix, ***M***, as the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where each element of ***M*** is a smaller matrix stacked on top of each other.
  prefs: []
  type: TYPE_NORMAL
- en: We can most easily define the Kronecker product using a visual example involving
    a block matrix. The Kronecker product of ***A*** and ***B***, typically written
    as ***A*** ⊗ ***B***, is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: for ***A***, an *m* × *n* matrix. This is a block matrix because of ***B***,
    so, when written out completely, the Kronecker product results in a matrix larger
    than either ***A*** or ***B***. Note, unlike matrix multiplication, the Kronecker
    product is defined for arbitrarily sized ***A*** and ***B*** matrices. For example,
    using ***A*** and ***B*** from [Equation 5.10](ch05.xhtml#ch05equ10), the Kronecker
    product is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](Images/126equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice above that we used ⊗ for the Kronecker product. This is the convention,
    though the symbol ⊗ is sometimes abused and is used for other things too. We used
    it for the outer product of two vectors, for example. NumPy supports the Kronecker
    product via `np.kron`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the mathematical objects used in deep learning:
    scalars, vector, matrices, and tensors. We then explored arithmetic with tensors,
    in particular with vectors and matrices. We saw how to perform operations on these
    objects, both mathematically and in code via NumPy.'
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration of linear algebra is not complete, however. In the next chapter,
    we’ll dive deeper into matrices and their properties to discuss just a handful
    of the important things that we can do with or know about them.
  prefs: []
  type: TYPE_NORMAL
