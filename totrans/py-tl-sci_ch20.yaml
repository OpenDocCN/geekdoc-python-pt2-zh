- en: '**20'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PANDAS, SEABORN, AND SCIKIT-LEARN**
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A common scientific practice is evaluating data and using it to generate predictive
    models. In this chapter, we’ll use three of Python’s most popular open source
    libraries to solve a zoological classification problem. Using this hands-on, project-based
    approach will showcase the functionality and synergy among the libraries and demonstrate
    what’s involved in doing science with Python.
  prefs: []
  type: TYPE_NORMAL
- en: For data loading, analysis, and manipulation, we’ll use the pandas package (*[https://pandas.pydata.org/](https://pandas.pydata.org/)).*
    Built on NumPy and Matplotlib, pandas uses array-based computing under the hood
    but has simpler syntax, making coding and plotting faster, easier, and more error
    free. Unlike native Python, pandas can intelligently read tabular text-file data,
    recognizing columns, rows, headers, and so on. And unlike NumPy, on which it’s
    built, pandas can handle heterogeneous data types such as mixtures of text and
    numbers.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also use the seaborn library (*[https://seaborn.pydata.org/](https://seaborn.pydata.org/)*),
    which wraps Matplotlib to produce more attractive and easier visualizations. It
    represents a nice plotting compromise between the highly customizable but verbose
    syntax of Matplotlib and the bare-bones simplicity of pandas. Even better, seaborn
    is tightly integrated with pandas for seamless plotting and effective data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the scikit-learn library (*[https://scikit-learn.org/](https://scikit-learn.org/)*)
    is Python’s primary general-purpose machine learning toolkit. It provides algorithms
    for classification, regression, clustering, dimensionality reduction, preprocessing,
    and model selection.
  prefs: []
  type: TYPE_NORMAL
- en: In the sections that follow, you’ll apply these libraries to a real-world problem
    and observe how they work together. But, due to their enormous size and scope,
    we won’t be able to study them in-depth. Whole books have been dedicated to each,
    and the admittedly non-exhaustive pandas overview in Wes McKinney’s *Python for
    Data Analysis*, 2nd edition requires no less than 270 pages!
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like a complete picture, I list some additional resources in the “Summary”
    section at the end of this chapter. You can also find useful tutorials and examples
    in the official websites, cited previously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Introducing the pandas Series and DataFrame**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pandas library contains data structures designed for working with common
    data sources such as Excel spreadsheets and SQL relational databases. Its two
    primary data structures are series and DataFrames. Other libraries, like seaborn,
    are designed to integrate well with these data structures and supply additional
    functionality, making pandas a great foundation to any data science project.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Series Data Structure***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *series* is a one-dimensional labeled array that can hold any type of data
    such as integers, floats, strings, and so on. Because pandas is based on NumPy,
    a series object is basically two associated arrays. One array contains the data
    point values, which can have any NumPy data type. The other array contains labels
    for each data point, called *indexes* ([Table 20-1](ch20.xhtml#ch020tab1)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-1:** A Series Object'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Index** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 42 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 549 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | '' Steve '' |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | –66.6 |'
  prefs: []
  type: TYPE_TB
- en: Unlike the indexes of Python list items, the indexes in a series don’t need
    to be an integer. In [Table 20-2](ch20.xhtml#ch020tab2), the indexes are the names
    of elements, and the values are their atomic numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-2:** A Series Object with Meaningful Indexes'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Index** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Silicon | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| Sodium | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| Argon | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| Cobalt | 27 |'
  prefs: []
  type: TYPE_TB
- en: A series acts much like a Python dictionary in so much as indexes represent
    keys. It can thus serve as a replacement for a dictionary in many contexts. Another
    useful feature is that different series will align by index label when doing arithmetic
    operations between them even if the labels don’t occur in the same order.
  prefs: []
  type: TYPE_NORMAL
- en: As with a list or NumPy array, you can slice a series or select individual elements
    by specifying an index. You can manipulate the series many ways, such as filtering
    it, performing mathematical operations on it, and merging it with other series.
    To see the many attributes and methods available for a series object, visit *[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html).*
  prefs: []
  type: TYPE_NORMAL
- en: '***The DataFrame Data Structure***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A *DataFrame* is a more complex structure made up of two dimensions. It’s a
    collection of objects organized using a tabular structure, like a spreadsheet,
    with columns, rows, and data ([Table 20-3](ch20.xhtml#ch020tab3)). You can think
    of it as an ordered collection of columns with two indexing arrays. Each column
    represents a pandas series.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-3:** A DataFrame Object'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Columns** |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Index** | **Country** | **State** | **County** | **Population** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | USA | Alabama | Autauga | 54,571 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | USA | Alabama | Baldwin | 182,265 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | USA | Alabama | Barbour | 27,457 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | USA | Alabama | Bibb | 22,915 |'
  prefs: []
  type: TYPE_TB
- en: The first index, for the rows, works much like the index array in a series.
    The second keeps track of the series of labels, with each label representing a
    column header. DataFrames also resemble dictionaries; the column names form the
    keys, and the series of data in each column forms the values. Like series, DataFrames
    come with many attributes and methods. For more information on these, see *[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating index objects and labels into their structure, you can easily
    manipulate DataFrames. We’ll look at some of this functionality as we work through
    the classification problem. You can also get up to speed on the basics by visiting
    the “10 Minutes to pandas” tutorial at *[https://pandas.pydata.org/docs/user_guide/10min.html](https://pandas.pydata.org/docs/user_guide/10min.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Palmer Penguins Project**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *Palmer Penguins dataset* consists of 342 observations of Antarctic penguins
    from three islands in the Palmer Archipelago ([Figure 20-1](ch20.xhtml#ch020fig1)).
    It was made available through the *Palmer Station Antarctica LTER* (*[https://pallter.marine.rutgers.edu/](https://pallter.marine.rutgers.edu/)*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-1: The location of Dream, Torgersen, and Biscoe Islands, Palmer
    Archipelago, Antarctica*'
  prefs: []
  type: TYPE_NORMAL
- en: Three different penguin species were sampled in the study. In order of decreasing
    body size, these are the Gentoo, Chinstrap, and Adélie ([Figure 20-2](ch20.xhtml#ch020fig2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-2: The three penguin species in the Palmer Penguins dataset, as
    drawn by Charles Joseph Hullmandel (courtesy of Wikimedia Commons)*'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this project will be to generate a model to predict the species
    of penguin from a combination of morphological features such as flipper length
    and body mass. In machine learning, this is considered a *classification* problem.
    We’ll use pandas to load, explore, validate, and clean the data, seaborn to plot
    the data, and scikit-learn to produce the predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Project Outline***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data science projects like this one follow a series of logical steps, as listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Frame the problem (the single most important step).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect raw data and set up the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the data (through cleaning, merging, infilling, reducing, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform in-depth analysis and develop models and algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the models and present the project results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jupyter Notebook is ideal for this process, as it can handle all the steps in
    order and is basically self-documenting. It can also be turned into a slideshow
    for presentations (as discussed in [Chapter 5](ch05.xhtml)).
  prefs: []
  type: TYPE_NORMAL
- en: '***Setting Up the Project***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For this project, we’ll use Jupyter Notebook in a dedicated project folder.
    We’ll install Notebook and the scientific and plotting libraries using the naive
    approach, in other words, directly in a conda environment within the project folder
    (see [Chapter 5](ch05.xhtml)). In general, you use the naive approach when you
    want to work with *specific* and *persistent* versions of a library or application.
    We’re using it here for practice, as we’ve previously been focusing on the modular
    approach, in which Notebook is installed in the *base* environment.
  prefs: []
  type: TYPE_NORMAL
- en: Start by making a folder named *penguins* under your user directory. Although
    you can do this through Anaconda Navigator, the command line is more succinct,
    so we’ll use that going forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the directories for the project, open Anaconda Prompt (in Windows)
    or a terminal (in macOS or Linux) and enter the following (using your own directory
    path):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This makes a *penguins* directory with a *notebooks* subdirectory. Next, create
    a conda environment named *penguins_env* under the project directory, activate
    it, and install the libraries we’ll use (substituting your own path where needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You now have a conda environment for the project that contains the notebook,
    pandas, python, scikit-learn, and seaborn packages. Remember from [Chapter 2](ch02.xhtml)
    that this environment is isolated and can’t “see” other packages on your system,
    such as those in the *base* environment.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your *penguins_env* should be active, and your project directory
    structure should look like [Figure 20-3](ch20.xhtml#ch020fig3). We’ll be loading
    the dataset straight from seaborn, so there’s no need for a *data* folder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-3: Directory structure for the penguins project*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a notebook for the project, first navigate to the *notebooks* folder
    using Anaconda Prompt or the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To launch Notebook, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You should now see the Jupyter dashboard in your browser. Click the **New**
    button and choose **Python[conda env:penguins_env]** to create a new notebook.
    A new notebook should appear in your browser. Click **Untitled**, name it *penguins_project*,
    and then click the **Save** button. You’re ready to go!
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to open the notebook in the future using Anaconda Navigator, launch
    Navigator, use the Environments tab to activate penguins_env, and then click the
    Launch button on the Jupyter Notebook tile. This will open the dashboard, where
    you can navigate to the notebook folder and launch penguins_project.ipynb. If
    you want to use Notebook in JupyterLab, see the instructions in [Chapter 6](ch06.xhtml)
    for installing JupyterLab and launching Notebook.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Importing Packages and Setting Up the Display***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the first notebook cell, import Matplotlib, seaborn, and pandas. Enter the
    following code and execute it using SHIFT-ENTER, which automatically moves you
    to a new blank cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Normally, we would perform all the imports here, but for the sake of the narrative,
    we’ll import the scikit-learn components later, so we can discuss them just before
    applying them.*'
  prefs: []
  type: TYPE_NORMAL
- en: By default, Notebook displays only one output per cell. The `%config` magic
    command overrides this, allowing us to see multiple outputs, such as a data table
    plus a bar chart, in a single output cell.
  prefs: []
  type: TYPE_NORMAL
- en: The default seaborn color palette is undeniably beautiful (see *[http://seaborn.pydata.org/tutorial/function_overview.html](http://seaborn.pydata.org/tutorial/function_overview.html)*),
    but it loses its charm somewhat in a black-and-white book. As a compromise, we’ll
    use the `whitegrid` stylesheet and reset the palette to black, red, and gray,
    one for each of the three penguin species in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '***Loading the Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Seaborn comes with a few practice datasets that are automatically downloaded
    during installation. These are all comma-separated values (*.csv*) files stored
    in a repository at *[https://github.com/mwaskom/seaborn-data/](https://github.com/mwaskom/seaborn-data/)*.
    If you ever need to get the dataset names, you can retrieve them by running `sns.get_dataset_names()`
    in a notebook or console (after importing seaborn, of course).
  prefs: []
  type: TYPE_NORMAL
- en: As a data analysis tool, pandas can read and write data stored in many types
    of media, such as files and databases ([Table 20-4](ch20.xhtml#ch020tab4)). Example
    syntax is `df = pd.read_excel('`filename.xlsx`')` and `df.to_excel('`filename.xlsx`'),`
    where `df` stands for *DataFrame*. For more options, visit *[https://pandas.pydata.org/docs/reference/io.html](https://pandas.pydata.org/docs/reference/io.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the methods in [Table 20-4](ch20.xhtml#ch020tab4), the `read_table()`
    method reads tabular data, such as text *(.txt*) files, in which the values are
    separated by spaces or tabs. Python can generally detect the separator in use,
    but you can also pass it as an argument, for example, `sep='\t'` for a tab.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-4:** Useful pandas I/O Methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input (Reader)** | **Output (Writer)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `read_csv()` | `to_csv()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_excel()` | `to_excel()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_hdf()` | `to_hdf()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_sql()` | `to_sql()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_json()` | `to_json()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_html()` | `to_html()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_stata()` | `to_stata()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_clipboard()` | `to_clipboard()` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_pickle()` | `to_pickle()` |'
  prefs: []
  type: TYPE_TB
- en: Besides loading external sources, you can create a DataFrame from many different
    types of input. These include 2D `ndarray`s, lists of lists or tuples, list of
    dictionaries or series, an existing DataFrame, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In spite of all these choices, we’ll use seaborn’s `load_dataset()` method
    to load the penguins dataset. This specialized method reads a CSV-format dataset
    from the seaborn repository and returns a pandas DataFrame object. Enter the following
    in the new cell and press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this notebook, I’m using simple comments, such as # Load penguins dataset,
    as cell headers. To make proper headers, you can add a Markdown cell before each
    code cell, as described in [Chapter 5](ch05.xhtml).*'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous code, we assigned the DataFrame to a variable named `df`. This
    is handy, as the name reflects the datatype. There’s no reason why you couldn’t
    use another name, however, such as `penguins_df`.
  prefs: []
  type: TYPE_NORMAL
- en: '***Displaying the DataFrame and Renaming Columns***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The first thing you’ll want to do after loading the data is look at it. For
    larger datasets such as penguins, pandas will show you part of the top and part
    of the bottom of a DataFrame by default. To see an example, enter the following,
    and then press CTRL-ENTER to execute the cell without leaving it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the entire DataFrame in a scrollable output cell, place this command
    at the top of the cell and rerun it: `pd.set_option(''display.max_rows'', None)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the DataFrame displays all the columns along with the first five rows
    and last five rows ([Figure 20-4](ch20.xhtml#ch020fig4)). When possible, column
    names should be descriptive and short. However, this isn’t always an option, so
    let’s practice changing a column header.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-4: DataFrame head and tail display*'
  prefs: []
  type: TYPE_NORMAL
- en: In the same cell, add the following code to rename the `sex` header to `gender`.
    The `inplace` argument tells pandas to alter the current DataFrame rather than
    return a copy. Press CTRL-SHIFT to execute the code and move to a new cell.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `head()` method displays the first five rows in a DataFrame, as shown in
    [Figure 20-5](ch20.xhtml#ch020fig5). To see more, just pass it the number of rows
    you want to see as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-5: The head of the DataFrame after changing the sex column header
    to gender*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 20-4](ch20.xhtml#ch020fig4) the number of rows and columns is included
    at the bottom of the output. You might immediately notice an issue: there are
    344 rows, but earlier, I stated that the dataset has 342 observations. The discrepancy
    could be due to one of two common dataset problems: duplicate or missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Checking for Duplicates***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s not uncommon for data rows to become duplicated by accident. This can happen
    during the initial creation of a dataset, in later edits, or during data transfers
    and transformations. You should remove this redundant data before you begin an
    analysis because it takes up memory, slows processing speeds, and distorts statistics
    due to the overweighting of the duplicate values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, pandas comes with the `duplicated()` method for finding duplicate
    rows. In the new cell, enter the following and then press CTRL-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following output, as there are no duplicate rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Had there been any duplicates in the dataset, we could have removed them using
    the `drop_duplicates()` method, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also look across specific columns for duplicate values. At the bottom
    of the current cell, enter the following and execute it by pressing SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that the inner square brackets define a Python list with column names,
    whereas the outer brackets represent “selection brackets” used to select data
    from a pandas DataFrame. We specified four of the seven columns, producing the
    output in [Figure 20-6](ch20.xhtml#ch020fig6).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-6: Row with duplicate values across the four columns with the float
    data type*'
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see in a moment, row 339 is a duplicate of row 3 (for the four columns
    specified). But even though there are duplicate values here, they’re not the kind
    that we need to treat as duplicates. Instead, they represent *missing values*,
    which we’ll cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '***Handling Missing Values***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The duplicate values in [Figure 20-6](ch20.xhtml#ch020fig6) are represented
    by the *Not a Number* (`NaN`) value. This is a special floating-point value recognized
    by all systems that use the standard IEEE floating-point representation. For computational
    speed and convenience, it serves as the default missing value marker for both
    NumPy and pandas. `NaN` and Python’s built-in `None` value are essentially interchangeable.
    By default, these null values are not included in computations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding Missing Values**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Missing data values reduce statistical power and can cause bias when estimating
    parameters and making predictions. To find the missing values in the penguins
    DataFrame, enter the following in a new cell and then press SHIFT-RETURN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The first method sums the missing values and displays the results as a table
    ([Figure 20-7](ch20.xhtml#ch020fig7)). The penguins dataset is missing 11 gender
    calls and a total of 8 morphological measurements.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-7: The output of df.isnull().sum()*'
  prefs: []
  type: TYPE_NORMAL
- en: The second call indexes the DataFrame where a value in *any* column is missing
    (as opposed to *all*). Remember, pandas is built on NumPy, so axis 1 refers to
    columns and axis 0 refers to rows. You should get the result depicted in [Figure
    20-8](ch20.xhtml#ch020fig8).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-8: All the DataFrame rows containing missing data*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filling and Removing Missing Values**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Missing values must be addressed before you try to conduct analyses or build
    models from a dataset. Although ignoring the issue is a possibility, it’s much
    better to either fill in the missing values or remove (drop) them completely.
    Methods for doing this are listed in [Table 20-5](ch20.xhtml#ch020tab5).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-5:** Useful Methods for Handling Missing Data'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `dropna` | Depending on arguments, remove row or column that contains missing
    data based on whether any or all values are null. |'
  prefs: []
  type: TYPE_TB
- en: '| `fillna` | Fill in missing value with a constant or an interpolation method.
    Arguments include the `ffill` and `bfill` methods. |'
  prefs: []
  type: TYPE_TB
- en: '| `ffill` | “Forward fill” by propagating the last valid observation forward.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `bfill` | “Back fill” by replacing missing values with values from the next
    row or column, as specified. |'
  prefs: []
  type: TYPE_TB
- en: '| `isnull` | Return Boolean indicating missing/NA values. |'
  prefs: []
  type: TYPE_TB
- en: '| `notnull` | Negate `isnull`. |'
  prefs: []
  type: TYPE_TB
- en: 'Options for filling in the missing values with `fillna()` include replacing
    them with the mean, median, or most frequent values in the dataset so that the
    overall statistics aren’t skewed. For example, to use the mean value of a column,
    you would use this syntax (don’t add this to your project code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*The pandas library tries to mimic the R programming language, and the na in
    the fillna() method stands for the NA (not available) marker used for missing
    data in R.*'
  prefs: []
  type: TYPE_NORMAL
- en: Filling in the missing data is important when a dataset is small and you need
    to take into account every observation. And if only a single column among many
    is missing a value, you might not want to “throw away” all the other useful data
    in the row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have a robust dataset and can’t easily impute and replace missing
    gender data, we’ll *drop* the rows with missing data. In the new cell, enter the
    following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using an assignment statement when calling `dropna()` causes the current DataFrame
    (`df`) to be overwritten. This allows the DataFrame to evolve over time, but be
    aware that to erase changes and restore the DataFrame to its previous state, you’ll
    need to run all the cells above the current cell. Passing a `how` argument of
    `any` to `dropna()` means that any row with at least one missing value will be
    deleted.
  prefs: []
  type: TYPE_NORMAL
- en: To check the results, rerun the `isnull()` method. You should get the output
    in [Figure 20-9](ch20.xhtml#ch020fig9).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-9: A summary of null values after dropping nulls*'
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame no longer includes missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reindexing**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Reindexing* refers to the process of making data conform to a given set of
    labels along a particular axis. Missing value markers will be automatically inserted
    in label locations where no data for the label exists.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we dropped the rows with null values in the previous section, we also
    deleted their corresponding indexes. To see the result, run the following code
    in the new cell and press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in [Figure 20-10](ch20.xhtml#ch020fig10), there is a gap in the
    DataFrame index (leftmost column) where row 3, which contained a null value, was
    dropped.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-10: Dropping rows results in missing DataFrame indexes.*'
  prefs: []
  type: TYPE_NORMAL
- en: To restore the indexes, run the following and then execute the cell using SHIFT-ENTER.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `reset_index()` method, `drop=True` signals that the old index isn’t
    preserved as a new column in the DataFrame, because there’s no need to keep that
    information. The `inplace=True` argument means that the method adjusts the current
    DataFrame rather than returning a copy. As an alternative, you could simply reassign
    the DataFrame, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Calling the `head()` method shows that the indexes are now consecutively ordered
    ([Figure 20-11](ch20.xhtml#ch020fig11)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-11: The DataFrame head after reindexing*'
  prefs: []
  type: TYPE_NORMAL
- en: Pandas includes several other reindexing functions, such as `reindex()` and
    `reindex_like()`. You can find these, and other DataFrame functions, at *[https://pandas.pydata.org/pandas-docs/stable/reference/frame.html](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html)*.
    For more on missing values, see *[https://pandas.pydata.org/docs/user_guide/missing_data.html](https://pandas.pydata.org/docs/user_guide/missing_data.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Exploring the Dataset***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At this point, you’ve cleaned the data by checking for duplicates, removing
    missing values, and reindexing the DataFrame. Of course, there might still be
    problems, such as incorrect values (a penguin body mass of one million grams,
    for example). Catching and correcting these requires an exploration of the dataset,
    and pandas and seaborn provide several methods to aid you in this process. These
    same methods will help you to understand the dataset so that you can formulate
    a plan for addressing the project goal.
  prefs: []
  type: TYPE_NORMAL
- en: '**Describing the DataFrame**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s explore the DataFrame using a combination of tables and graphs. To begin,
    we’ll look at the data types in play and overall statistics. In a new cell, enter
    the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This produces the output shown in [Figure 20-12](ch20.xhtml#ch020fig12).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-12: Output of the dtypes() and describe() methods*'
  prefs: []
  type: TYPE_NORMAL
- en: The `describe()` method returns a quick-look statistical overview of the DataFrame.
    Passing it `all` produces a statistical summary of *all* the columns. If you omit
    the `include` argument, you’ll see only a summary of the *numeric* columns.
  prefs: []
  type: TYPE_NORMAL
- en: The `NaN` values present in the table represent *not applicable* values rather
    than missing values. For example, you can’t take the mean of a categorical feature
    like `species`, so the result is presented as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: The stats table doesn’t tell you if every value in the dataset is valid, but
    it does help bracket how good or bad things can be. If the minimum, maximum, and
    mean values appear to be reasonable, the dataset is probably reliable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Counting Observations Using countplot**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tables of data, although useful, can be dense and difficult to interpret. For
    example, is the data skewed toward male or female penguins? The information is
    there, but you must work to back it out.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, it’s beneficial to create a visualization of the data. The seaborn
    library provides many statistical plot types for data exploration ([Table 20-6](ch20.xhtml#ch020tab6)).
    You can see examples of these in the seaborn gallery (*[https://seaborn.pydata.org/examples/index.html](https://seaborn.pydata.org/examples/index.html)*),
    and you can find a plotting tutorial at *[https://seaborn.pydata.org/tutorial.html](https://seaborn.pydata.org/tutorial.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 20-6:** Useful seaborn Plotting Methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `barplot()` | Categorical data presented with bars whose heights or lengths
    are proportional to the values that they represent. |'
  prefs: []
  type: TYPE_TB
- en: '| `boxplot()` | Graphical representation of the locality, spread, and skewness
    groups of numerical data through their quartiles. |'
  prefs: []
  type: TYPE_TB
- en: '| `countplot()` | A visualization of the counts of observations in each categorical
    bin using bars. |'
  prefs: []
  type: TYPE_TB
- en: '| `histplot()` | Series of bars used to bin and display continuous data in
    a categorical form. |'
  prefs: []
  type: TYPE_TB
- en: '| `jointgrid()` | Grid for drawing a bivariate plot with marginal univariate
    plots. |'
  prefs: []
  type: TYPE_TB
- en: '| `jointplot()` | `jointgrid()` wrapper for drawing `jointgrid()` of two variables
    using canned bivariate and univariate graphs. |'
  prefs: []
  type: TYPE_TB
- en: '| `lineplot()` | Graphical display of data along a number line where markers
    recorded above the responses indicate the number of occurrences. |'
  prefs: []
  type: TYPE_TB
- en: '| `pairgrid()` | Subplot grid for plotting pairwise relationships in a dataset.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `pairplot()` | Easier-to-use wrapper for `pairgrid()`. |'
  prefs: []
  type: TYPE_TB
- en: '| `relplot()` | Function for visualizing statistical relationships using scatter
    plots and line plots |'
  prefs: []
  type: TYPE_TB
- en: '| `scatterplot()` | Graph that uses Cartesian coordinates to display values
    for two variables. Additional variables can be incorporated through marker coding
    (color/size/shape). |'
  prefs: []
  type: TYPE_TB
- en: '| `stripplot()` | A scatterplot for which one variable is categorical. |'
  prefs: []
  type: TYPE_TB
- en: '| `swarmplot()` | A stripplot with non-overlapping points. |'
  prefs: []
  type: TYPE_TB
- en: '| `violinplot()` | A combination of boxplot and kernel density estimate showing
    the distribution of quantitative data across several levels of one (or more) categorical
    variables. |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s look at an example in which we plot the number of penguins and their
    gender. In a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This produces the output in [Figure 20-13](ch20.xhtml#ch020fig13).
  prefs: []
  type: TYPE_NORMAL
- en: By visualizing the data, we can instantly see that the Chinstrap species is
    a bit underrepresented, and the division between genders is close to equal.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-13: Bar chart of penguin species and gender counts*'
  prefs: []
  type: TYPE_NORMAL
- en: 'What about the distribution of penguins *per island*? Are they one big happy
    family, or do some prefer one island over another? To check, in a new cell, enter
    the following code and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This code counts the penguins per island, presents the results in a bar chart,
    and colors the bars based on species ([Figure 20-14](ch20.xhtml#ch020fig14)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-14: Bar chart of the number of penguins sampled per island, colored
    by species*'
  prefs: []
  type: TYPE_NORMAL
- en: Based on [Figure 20-14](ch20.xhtml#ch020fig14), we can see that the Adélie penguins
    live on all the islands, but the Chinstraps are found only on Dream Island, and
    the Gentoos only on Biscoe Island (see [Figure 20-1](ch20.xhtml#ch020fig1) for
    island locations). So, if you have measurements from Torgersen Island, you know
    you’re dealing with an Adélie. And the dimensional space is reduced for the other
    two islands, as you need to choose between only two species on both islands.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*An assumption here is that each island was thoroughly sampled. We’re saying
    that if a penguin species is not present in the dataset for a particular island,
    that species doesn’t live on that island. You’d want to verify this assumption
    in a real study, as absence of evidence is not evidence of absence.*'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to count each species per island is to use the pandas `get_dummies()`
    method in combination with the `groupby()` method. The first method converts categorical
    variables to *dummy variables*, which are numeric variables used to represent
    categorical data. The second method is used to group large amounts of data and
    compute operations on these groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we want to *sum* the penguin species per island, so we chain
    the methods and pass them the `species` column grouped by the `island` column,
    followed by the `sum()` method. In a new cell, enter the following code and then
    press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The call to `print()` lets you see the names of the new “dummy” columns (highlighted
    in bold):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The final line of code displays the new columns in the `count_df` DataFrame
    ([Figure 20-15](ch20.xhtml#ch020fig15)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-15: The count_df DataFrame that sums columns per island per penguin
    species*'
  prefs: []
  type: TYPE_NORMAL
- en: An advantage of checking tabular data is that low values are just as apparent
    as high values. With a bar chart, very low values may be mistaken for 0, due to
    the shortness of the bars.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the `get_dummies()` and `groupby()` methods at *[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)*
    and *[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)*,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting the Big Picture with pairplot**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because visualizations are so effective for understanding data, seaborn provides
    the `pairplot()` method for plotting pairwise relationships in a dataset. This
    method creates a grid of axes where each variable shares the y-axis across a single
    row and the x-axis across a single column. This lets you quickly spot patterns
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a pairplot, in a new cell, enter the following code and then press
    SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The arguments here are the name of the DataFrame, the column used to color the
    plots, and the marker types. You can find a list of marker types at *[https://matplotlib.org/stable/api/markers_api.html](https://matplotlib.org/stable/api/markers_api.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: Because the dataset contains only four numeric columns, the pairplot ([Figure
    20-16](ch20.xhtml#ch020fig16)) is very accessible and easy to consume.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-16: The pairplot for the penguins dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: The pairplot makes it easy to see data distributions and relationships. For
    example, the scatterplots where the points cluster into separate groups are important
    because they indicate that classification strategies such as principal component
    analysis (PCA) and *k*-nearest neighbors should be able to distinguish one species
    from another. Scatterplots with linear relationships, like flipper length versus
    body mass, suggest that regression techniques could predict one of these features
    when the other is known.
  prefs: []
  type: TYPE_NORMAL
- en: '**Digging into Details with scatterplot**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Despite being packed with information, even a pairplot can’t tell the whole
    story. For example, what role does gender play in determining the body mass and
    bill length of each species? To explore this, you’ll need more detailed plots.
    In a new cell, enter the following and press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the call to `scatterplot()`, the `hue`, `style`, and `size` arguments control
    marker color, shape, and size, respectively. The first two are based on `species`,
    and the latter on `gender`; thus, data points representing female penguins are
    sized differently than males of the same species. Calling `legend()` with the
    `bbox_to_anchor` argument prevents the legend from posting over the plot and obscuring
    some of the data. You should get the results in [Figure 20-17](ch20.xhtml#ch020fig17).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-17: A scatterplot of bill length versus body mass, colored by species
    and sized by gender*'
  prefs: []
  type: TYPE_NORMAL
- en: This plot shows that the female of each species tends to be smaller, with shorter
    bills and lower body mass than the males. Bill length also appears to be more
    strongly correlated with body mass for the Adélie and Gentoo species, regardless
    of gender.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about the scatterplot at *[https://seaborn.pydata.org/generated/seaborn.scatterplot.html](https://seaborn.pydata.org/generated/seaborn.scatterplot.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Investigating Categorical Scatter Using boxplot and stripplot**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can explore the gender relationships further using different types of plots
    such as *box plots* and *strip plots*. To make a box plot, in a new cell, enter
    the following code and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This produces the plot in [Figure 20-18](ch20.xhtml#ch020fig18).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-18: A box-and-whisker plot of penguin body mass by species by gender*'
  prefs: []
  type: TYPE_NORMAL
- en: Box plots provide insight on the symmetry, grouping, and skewness of data. Each
    box encompasses the first through third quartiles of the data distribution, with
    the vertical line within the box marking the median value. The “whiskers” extend
    to show the rest of the distribution, except for points that are considered “outliers,”
    which are represented by diamonds.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the box plot in [Figure 20-18](ch20.xhtml#ch020fig18), Adélie and Chinstrap
    penguins are similar in size and smaller than Gentoo penguins, and the females
    tend to be smaller for all species. There is overlap between the genders, however,
    meaning body mass alone cannot positively differentiate males from females.
  prefs: []
  type: TYPE_NORMAL
- en: 'The seaborn strip plot posts the actual data points rather than summarizing
    them, as in the box plot. Let’s examine bill length measurements in both species
    and genders. In a new cell, enter the following code and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `dodge` argument shifts points for each species to reduce overlap, making
    the plot easier to read ([Figure 20-19](ch20.xhtml#ch020fig19)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-19: A strip plot of penguin bill length by species by gender*'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the plot, we can see that the Adélie penguins have markedly shorter
    bills than the other two species. Gender differences are less distinct, though
    female penguins of all species tend to have shorter bills, on average.
  prefs: []
  type: TYPE_NORMAL
- en: '**Combining Views Using jointplot**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Another potentially characteristic feature of the penguins is the vertical thickness
    of the bill, referred to as its *depth*. You can see in [Figure 20-2](ch20.xhtml#ch020fig2)
    that Gentoos have narrow, pointed bills, whereas the other two species have more
    bulbous bills. Although there are numerous ways to compare these graphically,
    let’s try out a joint plot using a *kernel density estimation (KDE)*.
  prefs: []
  type: TYPE_NORMAL
- en: A KDE plot is a method for visualizing the distribution of observations in a
    dataset, much like a histogram. But whereas a histogram approximates the underlying
    probability density of the data by counting observations in discrete bins, a KDE
    plot smooths the observations using a Gaussian kernel, producing a continuous
    density estimate. This results in a less cluttered and more interpretable plot
    when drawing multiple distributions. The `joinplot()` method lets you plot two
    variables using bivariate and univariate KDE graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This produces the chart in [Figure 20-20](ch20.xhtml#ch020fig20).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-20: A joint plot of bill depth versus bill length by species*'
  prefs: []
  type: TYPE_NORMAL
- en: From the Gaussian curves along the edges of the joint plot, it’s clear that
    the Adélie is distinguished by its shorter bill length, and the Gentoo by its
    shallower bill depth.
  prefs: []
  type: TYPE_NORMAL
- en: You can customize joint plots in many ways. To see some examples, check out
    the documentation at *[http://seaborn.pydata.org/generated/seaborn.jointplot.html](http://seaborn.pydata.org/generated/seaborn.jointplot.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualizing Multiple Dimensions Using radviz**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The pandas library comes with its own plotting capability built on Matplotlib.
    This includes the `radviz()` (radial visualization) method for plotting multidimensional
    datasets in a 2D format ([Figure 20-21](ch20.xhtml#ch020fig21)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-21: An example radviz plot for an automotive dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: In a radial visualization, the dimensions in a DataFrame, such as a penguin’s
    body mass or bill length, are evenly spaced around the circumference of a circle.
    Data in these numerical columns are normalized to values between 0 and 1 so that
    all dimensions have equal weights. These are then projected into the circular
    2D space as if imaginary springs anchor them to the column labels along the circumference.
    A point is plotted where the sum of the “spring” forces acting on it equals zero.
  prefs: []
  type: TYPE_NORMAL
- en: Radial visualizations are intuitive by nature. Points with similar dimension
    values will plot near the center, as will points with similar values whose dimensions
    are opposite each other on the circle. Points with dimension values greater than
    others will be “pulled” toward that side of the circle. The penguins dataset has
    only four dimensions, but the `radviz()` method can handle many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a radial visualization, in a new cell, enter the following and then
    press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: For a better-looking radviz plot, reset the default seaborn plotting parameters
    using the `set_theme()` method and set the context to `'talk'` ➊. The `context`
    parameter controls the scaling of plot elements like label size and line thickness.
    The base context is `notebook`, and the other contexts are `paper`, `talk`, and
    `poster`, which are just versions of the `notebook` parameter scaled by different
    values. Using the `talk` argument ensures that the plot labels are easy to read.
    To better increase readability, manually set the figure size to 7" × 7".
  prefs: []
  type: TYPE_NORMAL
- en: Next, call pandas’ `plotting.radviz()` method ➋. This method accepts only one
    categorical column, called the `class_column`, which in this case will be `species`.
    The rest of the DataFrame columns are assumed to be numerical, so we must remove
    the `island` and `gender` columns, which don’t contain numerical data. You could
    do this by creating a copy of the DataFrame, but because we need only this revised
    DataFrame for plotting, we’ll temporarily delete the columns, using the `drop()`
    method, while passing the DataFrame to the `radviz()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `drop()` method takes two arguments: the column names as a list, and the
    axis number, where 0 = row and 1 = column. Unless you pass it an `inplace=True`
    argument, the DataFrame will be changed for only the current operation.'
  prefs: []
  type: TYPE_NORMAL
- en: Because we’re not plotting with seaborn, we need to remind pandas of the color
    scheme we’re using and then change the marker style and transparency to make over-posted
    points easier to see. Moving the legend to the side also helps. Notice how we’re
    able to mix in seaborn (`sns`) and Matplotlib’s `pyplot` (`plt`) with pandas `plotting`.
    This is because both seaborn and pandas are built on top of Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: You should see the plot depicted in [Figure 20-22](ch20.xhtml#ch020fig22).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-22: Radviz plot for the penguins dataset*'
  prefs: []
  type: TYPE_NORMAL
- en: In the plot, the Gentoo data points form a distinct cluster skewed toward body
    mass and flipper length. The similarly sized Chinstrap and Adélie penguins are
    distinguished mainly by bill length, which “pulls” the Chinstrap points to the
    right of center.
  prefs: []
  type: TYPE_NORMAL
- en: The radviz plot is another way of exploring the data, and it becomes more useful
    with more dimensions. To read more about the pandas implementation, visit *[https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html](https://pandas.pydata.org/docs/reference/api/pandas.plotting.radviz.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s worth noting here that we changed the plotting style. I’ll be continuing
    with this new look, but if you want to return to the previous `''whitegrid''`
    style, you’ll need to enter the following code in a new cell before making more
    plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Quantifying Correlations Using corr()**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The pandas `DataFrame` class comes with a `corr()` method that quantifies data
    correlations by computing a pairwise correlation of columns, excluding NA/null
    values. This is useful when you plan to use regression techniques to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The first line calls the `corr()` method and assigns the results to the correlations
    variable. The next line plots the results as a seaborn heatmap ([Figure 20-23](ch20.xhtml#ch020fig23)).
    The `center` argument is optional and tells the method the value at which to center
    the colormap when plotting divergent data. With a value of `1`, the best correlations
    will plot in black. The `annot` argument turns on the plot annotations within
    each colored square.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-23: The correlation heatmap*'
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap confirms and quantifies correlations that we noted in the pairplot
    ([Figure 20-16](ch20.xhtml#ch020fig16)). Flipper length and body mass are the
    most closely correlated, followed by flipper length and bill length.
  prefs: []
  type: TYPE_NORMAL
- en: For more on the `corr()` method and seaborn heatmap, visit *[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)*
    and *[https://seaborn.pydata.org/generated/seaborn.heatmap.html](https://seaborn.pydata.org/generated/seaborn.heatmap.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**TEST YOUR KNOWLEDGE**'
  prefs: []
  type: TYPE_NORMAL
- en: 1.  Which of the following are advantages of pandas series or DataFrames over
    NumPy arrays?
  prefs: []
  type: TYPE_NORMAL
- en: a.  Ability to use heterogeneous data types
  prefs: []
  type: TYPE_NORMAL
- en: b.  Ability to use either numbers *or* labels as indexes
  prefs: []
  type: TYPE_NORMAL
- en: c.  Ability to load Python dictionaries
  prefs: []
  type: TYPE_NORMAL
- en: d.  Ease of use with tabular data
  prefs: []
  type: TYPE_NORMAL
- en: '2.  True or False: Reindexing is required after renaming columns in a DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.  Convert the following dictionary into a DataFrame and rename the last column
    to “whales”:'
  prefs: []
  type: TYPE_NORMAL
- en: 'animals = {''canines'': [''husky'', ''poodle'', ''bulldog''],'
  prefs: []
  type: TYPE_NORMAL
- en: '''felines'': [''Siamese'', ''Persian'', ''Maine Coon''],'
  prefs: []
  type: TYPE_NORMAL
- en: '''cetaceans'': [''humpback'', ''sperm'', ''right'']}'
  prefs: []
  type: TYPE_NORMAL
- en: 4.  Display the first row of the `animals` DataFrame from the previous question.
  prefs: []
  type: TYPE_NORMAL
- en: '5.  Flip the rows and columns in the `animals` DataFrame (hint: look up the
    pandas `transpose()` method).'
  prefs: []
  type: TYPE_NORMAL
- en: '***Predicting Penguin Species Using k-Nearest Neighbors***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of this project is to develop a model that classifies penguins based
    on the Palmer Archipelago dataset. Our data exploration has revealed that four
    morphological features (bill length and depth, flipper length, and body mass)
    form separate but overlapping clusters in numerous plots. This implies that a
    machine learning classification algorithm should be able to handle the problem.
  prefs: []
  type: TYPE_NORMAL
- en: It’s always best to start simple, and if you do a little research, you’ll find
    that *k-Nearest Neighbors* (*k*-NN) is one of the most basic, beginner-friendly,
    yet important classification algorithms in machine learning. It uses distance
    measures to intuitively find the *k* nearest neighbors to a new, unknown data
    point and then uses those neighbors to make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 20-24](ch20.xhtml#ch020fig24), numerical data points for two categorical
    classes (A and B) are plotted in a scattergram. A new and unlabeled data point
    (⋆) falls between the two clusters. To classify this new point, the algorithm’s
    *k* parameter has been set to 7\. As most of the closest points belong to Class
    B, the new data point will be assigned to B. Because this is a “voting” algorithm,
    *k* should always be set to an odd number to avoid a tie.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-24: Example of the k-NN algorithm choosing the seven nearest neighbors
    to a new data point*'
  prefs: []
  type: TYPE_NORMAL
- en: Besides being intuitive and easy to explain, *k*-NN runs quickly, works well
    with small datasets, is robust to noise, and can be tuned for greater accuracy.
    It’s also versatile, given that it can be applied to both classification and regression
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm needs a dense dataset, however, so that points aren’t too far
    apart. The more data *dimensions* you have, like flipper length and body mass,
    the more data you need for *k*-NN to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, *k*-NN, like other machine learning algorithms, requires its own
    data preparation routines. Because the algorithm works only with numerical data,
    you’ll commonly need to convert categorical values to integers and normalize numerical
    values between 0 and 1\. Normalization is needed so that dimensions with larger
    values don’t skew the distance calculations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Converting Categorical Data to Numerical Data**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As stated previously, the *k*-NN algorithm uses *numerical* data. To take advantage
    of important non-numerical data, such as the island of origin and gender, you
    need to convert these values into numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this first for the `island` column, using the pandas `get_dummies()`
    method that we used previously when counting penguins per island. Next, we’ll
    repeat the exercise manually for gender so that you can practice DataFrame *indexing*.
    In a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To use `get_dummies()`, pass it the DataFrame and the column label that you
    want to convert. Assign the result to a new DataFrame named `knn_df`. The method
    will create three new columns—one for each island—with values of either 0 or 1,
    depending on the value in the `island` column (either `Biscoe`, `Dream`, or `Torgersen`).
  prefs: []
  type: TYPE_NORMAL
- en: Next, for demonstration purposes, we’ll use a different approach to convert
    the `gender` column to new `male` and `female` columns. We’ll create a new column
    for each class and fill it with zeros. Then, using conditional statements, we’ll
    find the rows where the targeted class exists and change the column values to
    ones. For example, the column for male penguins will contain `1` for rows in the
    `gender` column containing a male designation; for all other rows, the column
    will contain `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Start by creating a new column named `male`. Assign to that column a value of
    `0` ➊. Next, use the pandas `loc` indexing operator to select a *subset* of the
    `knn_df` DataFrame. Because pandas can use both label-based and integer-based
    indexing for rows and columns, it comes with two indexing operators. The `loc`
    operator is for strictly label-based indexing, and the `iloc` operator handles
    cases in which the labels are strictly integers. In our case, the columns use
    labels (such as “species”) and the rows use integers.
  prefs: []
  type: TYPE_NORMAL
- en: The current operation will convert `Male` values in the `gender` column to `1`
    values in the `male` column. So, select the gender column (`knn_df['gender']`)
    and then use a conditional to overwrite the `0` values that we set in the previous
    line. What you’re saying here is, “get the `gender` column and, if its value is
    `Male`, put a `1` in the `male` column.”
  prefs: []
  type: TYPE_NORMAL
- en: Repeat this code for the female column and then check the results by using the
    `iloc` operator to sample rows throughout the DataFrame ➋. This works like indexing
    a list, for which you start at the beginning, go up to index 300, and use a step
    of 30 to select every 30th row.
  prefs: []
  type: TYPE_NORMAL
- en: You should get the output in [Figure 20-24](ch20.xhtml#ch020fig24). Note the
    five new columns on the right side of the DataFrame. The categorical island and
    gender columns can now be used by the *k*-NN algorithm, so you can make use of
    all the data at your disposal.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-25: A sample of the new knn_df DataFrame with new numerical columns
    for islands and gender*'
  prefs: []
  type: TYPE_NORMAL
- en: By converting the gender and island data to numbers, we’ve supplemented our
    morphological data with two more dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: If your goal was to predict the species of penguins sampled at sea, you’d want
    to drop the island-related columns because you couldn’t be sure of a penguin’s
    point of origin.
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting Up the Training and Testing Data**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The *k*-NN classifier is a *supervised* learning algorithm. This means that
    you show it what the answer *should* look like by providing a set of examples
    known as the “training” dataset. You can’t use all the available data for the
    training set, however, as you’ll have no objective way to test the results. Consequently,
    you need to randomly split out a smaller subset of the data that you can use to
    test the model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: As a lazy learning algorithm, *k*-NN doesn’t have an actual training phase in
    which it “learns” a discriminative function to apply to new data. Instead, it
    loads, or memorizes, the data and performs calculations with it during the prediction
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, scikit-learn provides the `train_test_split()` method as part
    of the `sklearn.model_selection` module. In a new cell, enter the following and
    then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: After importing the module, call the pandas `select_dtypes()` method on the
    new `knn_df` DataFrame ➊. This method returns a subset of a DataFrame including
    or excluding columns based on their data type. We want the numerical columns for
    use with the *k*-NN algorithm, so set `include` equal to `'number'` and assign
    the result to a variable named `X`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, assign the `species` column to a variable named `y`. This represents the
    categorical class you’re trying to predict. Note that the uppercase “X,” lowercase
    “y” format follows the convention in the scikit-learn documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Split out the training and testing data using the `train_test_split()` method
    ➋. You’ll need to unpack four variables for both `X` and `y` training and testing.
    Because we passed the method DataFrames, it will return DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: A key argument here is `test_size`, expressed as a proportion of the complete
    dataset. By default, this is 0.25, or 25 percent. So, for our penguins dataset,
    this represents 83 samples (332 × 0.25).
  prefs: []
  type: TYPE_NORMAL
- en: To avoid biasing, the `train_test_split()` method randomly shuffles the data
    before splitting it. For reproducible output across multiple function calls, you
    can pass an integer to the `random_state` argument. As written, this code lets
    you produce one set of repeatable training and testing data. To generate a new
    random set, you’ll need to either change the `random_state` value or not use it
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: Although we don’t need it here to get a good result, the `train_test_split()`
    method comes with a `stratify` parameter that ensures the split preserves the
    proportions of samples of each target class as observed in the original dataset.
    So, if the original dataset sampled 25 percent of Class A and 75 percent of Class
    B, the training and testing sets would reflect this proportion. This helps you
    avoid sampling bias, wherein a sample is not representative of the true population.
  prefs: []
  type: TYPE_NORMAL
- en: To read more about the `train_test_split()` method, visit *[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalizing the Data**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each numerical data column in the training and testing sets should be normalized
    to values between 0 and 1\. This prevents columns with large numerical values
    from biasing the *k*-NN distance measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because column transformations, such as normalization, are a common operation
    in machine learning, scikit-learn comes with two modules, `compose` and `preprocessing`,
    to simplify the task. In a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Start by importing the `make_column_transformer()` method and the `MinMaxScaler()`
    method. The first method lets us transform columnar data; the second method specifies
    how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: To normalize the data, scale it so that the minimum and maximum values fall
    between 0 and 1\. Pass the `make_column_transformer()` method the `MinMaxScaler()`
    method ➊. Next, pass it the columns that you want to transform; in this case,
    the numerical columns that aren’t already scaled to 0 and 1\. By default, the
    transformer *drops* columns that you didn’t specify in the previous argument.
    To prevent this, set the `remainder` argument to `passthrough`.
  prefs: []
  type: TYPE_NORMAL
- en: Now you need to apply the transformer by calling its `fit_transform()` method
    and passing it the `X_train` variable you made in the previous section. This method
    transforms the data and concatenates the results, returning an array. To convert
    this array back into a DataFrame, call pandas’ `DataFrame` class and pass it the
    `X_train` array ➋. The column transformer renames the columns as well as transforming
    them, so for the `columns` argument, call the `get_feature_names_out()` method
    on the `column_transformer` object.
  prefs: []
  type: TYPE_NORMAL
- en: Call `X_train.head()` to see the results, and then repeat this code for the
    testing set. The two DataFrame heads are shown in [Figure 20-26](ch20.xhtml#ch020fig26).
    In both, the columns should have new names, all the columns should contain numerical
    data, and values should fall between 0.0 and 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-26: The head of the normalized X_train and X_test DataFrames (shown
    horizontally truncated)*'
  prefs: []
  type: TYPE_NORMAL
- en: To read more about the scikit-learn column transformer, visit *[https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: You now have numbers-only DataFrames that can be used for training and testing.
    The `x_test` and `y_test` DataFrames let you relate these numerical DataFrames
    back to a species call. With only seven dimensions, this is a *low-dimensional*
    dataset. A *high-dimensional* dataset, common in machine learning, could have
    100,000-plus features!
  prefs: []
  type: TYPE_NORMAL
- en: '**Running k-NN and Checking the Accuracy of the Prediction**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At this point, the data is ready to be used with the *k*-NN classifier. It’s
    time to name that penguin!
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell, enter the following code and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: To run *k*-NN, you need to import the `KNeighborsClassifier` class from the
    scikit-learn `neighbors` module (*[https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)*).
    To check the results, import the `accuracy_score()` method from the `metrics`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: Call the `KNeighborsClassifier` and pass it a `k` value of `5`, and a `p` value
    of `2`. The `p` value tells the classifier to use Euclidian, or straight-line,
    distance measurements. This is usually appropriate for low-dimensional datasets
    with few outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, call the classifier’s `fit()` method to train it and then run the `predict()`
    method on the `X_test` dataset. This will take the measurement data you *withheld*
    from training and predict the species.
  prefs: []
  type: TYPE_NORMAL
- en: Finish by calling the `accuracy_score()` method and passing it the `y_test`
    and `predictions` variables. This method will compare the two datasets and store
    the accuracy measure in the `accuracy` variable, which you then print ([Figure
    20-27](ch20.xhtml#ch020fig27)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-27: The accuracy of the k-NN model*'
  prefs: []
  type: TYPE_NORMAL
- en: Right out of the gate, the model correctly matched about 99 percent of the samples
    in the `X_test` dataset. But don’t get too excited. It’s only matched samples
    in a *randomly chosen* subset of the `penguins` dataset. If you generate a new
    test set by changing the `train_test_split()` method’s `random_state` argument
    from `300` to `500` and rerun the cells, you’ll get an accuracy of `0.9642857142857143`.
    Although this is about as good as it gets for a real-world dataset, let’s use
    this discrepancy to see how you might handle a larger mismatch in a different
    project.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing the k Value Using Cross-Validation**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The goal of supervised machine learning is to generalize *beyond* what we see
    in training samples so that we can reliably classify new data. The *k*-NN classifier
    uses numerous *hyperparameters* such as `k`, `p`, and `weights` to control the
    learning process. In addition, the `test_size` and other parameters in the `train_test_split()`
    method can have a big impact on model results. You can think of these parameters
    as knobs that you can turn to “tune” or “dial in” the model fit.
  prefs: []
  type: TYPE_NORMAL
- en: You must be careful however, not to tune things *too* finely. *Overfitting*
    is a common problem that can lurk behind an apparently accurate model ([Figure
    20-28](ch20.xhtml#ch020fig28)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-28: An example of model fits with original training set (left) and
    superimposed with new, unmatched training set (right)*'
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 20-28](ch20.xhtml#ch020fig28), the chart on the left shows three
    fits (over, under, and best) to a randomized training dataset. Any points that
    fall to the right of these lines will be classified as belonging to Class B.
  prefs: []
  type: TYPE_NORMAL
- en: If the left-hand chart represents all the data we would ever have, the overfitted
    model would be the most accurate. But look what happens when we choose and post
    a new training set (right side of [Figure 20-28](ch20.xhtml#ch020fig28)). Some
    points stay the same and others change, and the overfitted model no longer matches
    the data as well. On the other hand, the underfitted model can neither model the
    training data nor generalize sufficiently to match new data.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the smaller the *k* value, the “tighter” the fit of the model to
    the data and the more likely that it is overfit; the larger the *k* value, the
    more likely that the model is underfit.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 20-28](ch20.xhtml#ch020fig28), the more generalized “Best fit” model
    does a good job of matching the two datasets. To achieve this generalized model,
    we’ll need to find the best values for the important hyperparameters. But this
    isn’t something that can be intuited when working in multidimensional space. It
    requires iterative investigation, where parameters are changed multiple times
    and the results are tabulated and scored.
  prefs: []
  type: TYPE_NORMAL
- en: In the code that follows, we’ll investigate a range of *k* values using *cross-validation*
    (*cv* for short). This is a model validation technique for assessing how the results
    of a statistical analysis will generalize to a new, unknown dataset. It also helps
    flag issues like overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation resamples different parts of the training set to create testing
    sets. To ensure that the entire training set is evaluated, it repeats this sampling
    multiple times. As it iterates, it changes the value of a hyperparameter, like
    *k*, and scores the result based on model accuracy. When the optimal parameters
    are identified, you input them in the *k*-NN classifier and perform a final evaluation
    against the test dataset, which has been kept separate from the cv process ([Figure
    20-29](ch20.xhtml#ch020fig29)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-29: Building a predictive model using cross-validation (modified
    from [scikit-learn.org](http://scikit-learn.org))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use cross-validation on our penguins dataset model, in a new cell, enter
    the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use NumPy to make a range of *k* values to evaluate, and average, the
    results of each cv iteration. The `cross_validate()` method is found in the scikit-learn
    `model_selection` module.
  prefs: []
  type: TYPE_NORMAL
- en: Python dictionaries are great for storing data like test results, and they can
    easily be turned into pandas DataFrames. After the imports, create a dictionary
    named `cv_metrics` with keys for the average training set and cross-validation
    score per iteration. The initial values for these dictionary keys are empty lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, make a `num_neighbors` dictionary with one key-value pair: *k* and a
    1D `ndarray` from 1 to 25\. These represent the range of *k* values you’ll be
    testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Loop through the `num_neighbors` dictionary and pass the current *k* value to
    the `KNeighborsClassifier` ➊. Then, call the `cross_validate()` method, pass it
    the `knn` model and the training data, and set the `return_train_score` argument
    to `True`. Finish each loop by appending the score results to the appropriate
    key in the `cv_metrics` dictionary. Use the NumPy `mean()` method to average the
    scores for each data point during the process.
  prefs: []
  type: TYPE_NORMAL
- en: Outside the loop, turn the `cv_metrics` dictionary into a DataFrame ➋ and then
    add the `num_neighbors` dictionary as a new column on the leftmost side of the
    DataFrame. Do this by calling the `insert()` method on the DataFrame and passing
    it the first column position (`loc=0`), the column name, and the value, obtained
    by passing the `num_neighbors` dictionary the `k` key. Finish by calling `head(10)`
    to display the first 10 rows.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than scroll through the DataFrame looking for the *k* value with the
    best score, let pandas find it using the `idxmax()` method, which returns the
    index of the first occurrence of a maximum value over a requested axis. This is
    axis 0 (row) by default. When you print the result, you should see the output
    in [Figure 20-30](ch20.xhtml#ch020fig30).
  prefs: []
  type: TYPE_NORMAL
- en: Comparing cross-validation and training scores can be insightful with respect
    to model overfitting and underfitting. The process is computationally expensive
    for high-dimensionality datasets, however, and the training scores are not required
    to select the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-30: The first 10 rows in the cv_metrics DataFrame and the cv choice
    for best k value*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot the cv results, in a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The first line resets the seaborn color palette that we’ve been using for a
    consistent look. The next line prepares the DataFrame for plotting. In seaborn,
    plotting multiple columns against the same y-axis requires a call to the pandas
    `melt()` method. This method returns a new DataFrame reshaped into a long table
    with one row for each column. To learn about wide-form and long-form data, see
    *[https://seaborn.pydata.org/tutorial/data_structure.html](https://seaborn.pydata.org/tutorial/data_structure.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: With the new `df_melt` DataFrame, you can call seaborn’s `lineplot()` method
    to get the plot in [Figure 20-31](ch20.xhtml#ch020fig31). The top curve represents
    the average training score.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-31: A comparison of the average training scores and cross-validation
    scores with the k value*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you expect to work extensively with pandas, consider learning its plotting
    syntax. For example, you can re-create [Figure 20-31](ch20.xhtml#ch020fig31) with
    a single line: `cv_metrics_df.plot.line(x=''k'')`. The plots are less customizable
    than with seaborn or Matplotlib, but they’re more than suitable for data exploration.
    To learn more, visit *[https://pandas.pydata.org/docs/getting_started/intro_tutorials/04_plotting.html](https://pandas.pydata.org/docs/getting_started/intro_tutorials/04_plotting.html)*.'
  prefs: []
  type: TYPE_NORMAL
- en: A model that is underfit will have both low training and low testing accuracy,
    whereas an overfit model will have a high training accuracy but a low testing
    accuracy. On the left side of [Figure 20-31](ch20.xhtml#ch020fig31), where *k*
    = 1, the training set is perfectly accurate because the training data point is
    compared only to itself. The cv results, however, are less accurate. This indicates
    slight overfitting, which we would expect with a low value of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: On the right side of the figure, where *k* is greater than 20, accuracy falls
    off for both curves. The model is trying to accommodate too many data points and
    is becoming underfit.
  prefs: []
  type: TYPE_NORMAL
- en: When *k* = 4, the cv score reaches its highest average accuracy, and the two
    curves begin to meet and run parallel to each other. In this case, values of *k*
    between 5 and 10 will only add computational burden without increasing the model
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: If you change the `random_state` or the `test_size` parameters in the `train_test_split()`method
    and rerun the code, you’ll see variations in the choice of best *k* value. This
    is because the model starts out with such high accuracy that subtle stochastic
    effects can have a large relative impact with essentially no absolute impact.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing Multiple Hyperparameters Using GridSearchCV**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Running cross-validations can take time, so scikit-learn comes with a convenience
    class called `GridSearchCV`. It takes a dictionary of parameter names and values,
    cross-validates them, and reports the outcome. You can find the documentation
    at *[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell, enter the following and then press SHIFT-ENTER:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: A dictionary named `params` holds the hyperparameter ranges. In this example,
    the *k* (`n_neighbors`) range is a NumPy array, and the `weights` and `p` parameters
    use lists.
  prefs: []
  type: TYPE_NORMAL
- en: The `GridSearchCV` class needs to know the DataFrame you’re using (`knn`), the
    name of the parameters dictionary (`params`), what to score on (`accuracy`), and
    how much detail you want it to report. By passing it `verbose=1`, we suppressed
    most of the extraneous output.
  prefs: []
  type: TYPE_NORMAL
- en: After fitting the model, you can print the `best_params_` attribute to see the
    results ([Figure 20-32](ch20.xhtml#ch020fig32)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-32: Results of running GridSearchCV*'
  prefs: []
  type: TYPE_NORMAL
- en: Next, in a new cell, pass the best parameters identified by the grid search
    to the `KNeighborsClassifier`, fit the model, predict against the testing dataset,
    and evaluate the accuracy. This corresponds to the “retrained model” and “final
    evaluation” steps shown in [Figure 20-29](ch20.xhtml#ch020fig29).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You should get the output shown in [Figure 20-33](ch20.xhtml#ch020fig33).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/20fig33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 20-33: Model accuracy after applying the optimized hyperparameters*'
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that this score is worse than the one we got initially using
    `k=5` ([Figure 20-27](ch20.xhtml#ch020fig27)). That’s because this initial run
    on a *single* train-test dataset was equivalent to a lucky roll of dice. The current
    model built with `k=4` has been tested over *multiple* datasets and should, when
    used repeatedly, yield the same average accuracy as `k=5` (see [Figure 20-31](ch20.xhtml#ch020fig31))
    but with better computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Along these lines, we’ve used only 75 percent of the penguins dataset to train
    the model. How do we know that 80 percent wouldn’t produce better results? To
    find out, you could use a loop to run multiple combinations of the `test_size`
    and `random_state` parameters of the `train_test_split()` method and model each.
  prefs: []
  type: TYPE_NORMAL
- en: The need to test many parameter and dataset combinations, along with heavy memory
    use, renders the *k*-NN algorithm inappropriate for very large datasets. Otherwise,
    it has numerous benefits, including being simple to use, easy to understand, fast
    to train, versatile, and agnostic to assumptions about the underlying data distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '**TEST YOUR KNOWLEDGE**'
  prefs: []
  type: TYPE_NORMAL
- en: '6.  For a big picture overview of a dataset, call the:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  seaborn `relplot()` method
  prefs: []
  type: TYPE_NORMAL
- en: b.  pandas `radviz()` method
  prefs: []
  type: TYPE_NORMAL
- en: c.  seaborn `pairplot()` method
  prefs: []
  type: TYPE_NORMAL
- en: d.  seaborn `jointplot()` method
  prefs: []
  type: TYPE_NORMAL
- en: '7.  The *k*-NN algorithm is appropriate for:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  Classification in high-dimensional datasets
  prefs: []
  type: TYPE_NORMAL
- en: b.  Projects for which computer memory is at a premium
  prefs: []
  type: TYPE_NORMAL
- en: c.  Classification in a noisy, low-dimensional dataset
  prefs: []
  type: TYPE_NORMAL
- en: d.  All of the above
  prefs: []
  type: TYPE_NORMAL
- en: '8.  Using a very low *k* value with the *k*-NN algorithm can result in:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  Excessive run times
  prefs: []
  type: TYPE_NORMAL
- en: b.  An underfit model
  prefs: []
  type: TYPE_NORMAL
- en: c.  An overfit model
  prefs: []
  type: TYPE_NORMAL
- en: d.  A generalized model
  prefs: []
  type: TYPE_NORMAL
- en: '9.  In machine learning, a hyperparameter is:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  A parameter chosen automatically by the algorithm
  prefs: []
  type: TYPE_NORMAL
- en: b.  A parameter set at the top level of an algorithm
  prefs: []
  type: TYPE_NORMAL
- en: c.  An adjustable parameter used to control the learning process
  prefs: []
  type: TYPE_NORMAL
- en: d.  An overly excitable parameter
  prefs: []
  type: TYPE_NORMAL
- en: '10.  Cross-validation is used to:'
  prefs: []
  type: TYPE_NORMAL
- en: a.  Check the accuracy of a model against an independent test set
  prefs: []
  type: TYPE_NORMAL
- en: b.  Find the best hyperparameters from an input range of hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: c.  Check a dataset for duplicate samples
  prefs: []
  type: TYPE_NORMAL
- en: d.  Gain insight on model underfitting and overfitting
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Palmer penguins project provided a good overview of how pandas, seaborn,
    and scikit-learn work, how they work together, and what you can accomplish using
    them. At this point, though, you’ve barely glimpsed the enormity of these packages.
    To expand your knowledge, I recommend the official library documentation cited
    in the introduction to this chapter as well as the following books:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython*,
    2nd edition, by Wes McKinney (O’Reilly Media, 2018), is an indispensable guide
    by the creator of the pandas library.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python Data Science Handbook: Essential Tools for Working with Data*, by Jake
    VanderPlas (O’Reilly Media, 2016), is a thorough reference for important Python
    data science tools, including pandas.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hands-on Machine Learning with Scikit-Learn, Keras, & TensorFlow: Concepts,
    Tools, and Techniques to Build Intelligent Systems*, 2nd edition, by Aurélien
    Géron (O’Reilly Media, 2019), provides practical instruction for machine learning
    novices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the penguins project covered a lot of ground, it didn’t address one
    of the most important forms of structured data used by scientists: time series
    data. In the next and final chapter, we’ll look at methods for incorporating dates
    and times in your programs and plots.'
  prefs: []
  type: TYPE_NORMAL
