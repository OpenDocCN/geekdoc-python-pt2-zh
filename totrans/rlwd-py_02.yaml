- en: '2'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2'
- en: ATTRIBUTING AUTHORSHIP WITH STYLOMETRY
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文体学归属作者身份
- en: '![Image](../images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/common.jpg)'
- en: '*Stylometry* is the quantitative study of literary style through computational
    text analysis. It’s based on the idea that we all have a unique, consistent, and
    recognizable style to our writing. This includes our vocabulary, our use of punctuation,
    the average length of our sentences and words, and so on.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*文体学*是通过计算机文本分析对文学风格进行定量研究的学科。它基于这样一个观点：我们每个人都有独特、一致且可识别的写作风格。这包括我们的词汇、标点使用、句子和单词的平均长度等等。'
- en: A common application of stylometry is authorship attribution. Do you ever wonder
    if Shakespeare really wrote all his plays? Or if John Lennon or Paul McCartney
    wrote the song “In My Life”? Could Robert Galbraith, author of *A Cuckoo’s Calling*,
    really be J. K. Rowling in disguise? Stylometry can find the answer!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 文体学的一个常见应用是作者身份归属分析。你是否曾经怀疑莎士比亚真的写了他所有的戏剧？或者约翰·列侬或保罗·麦卡特尼真的是《我的一生》这首歌的作者？*《布谷鸟的呼唤》*的作者罗伯特·加尔布雷思真的就是伪装成作者的J.K.
    罗琳吗？文体学能给出答案！
- en: Stylometry has been used to overturn murder convictions and even helped identify
    and convict the Unabomber in 1996\. Other uses include detecting plagiarism and
    determining the emotional tone behind words, such as in social media posts. Stylometry
    can even be used to detect signs of mental depression and suicidal tendencies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 文体学曾被用于推翻谋杀定罪，甚至帮助识别并定罪“独行炸弹人”（Unabomber）。其他应用还包括检测抄袭以及确定文字背后的情感色调，如社交媒体帖子中的情感。文体学甚至可以用于检测抑郁症迹象和自杀倾向。
- en: In this chapter, you’ll use multiple stylometric techniques to determine whether
    Sir Arthur Conan Doyle or H. G. Wells wrote the novel *The Lost World*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用多种文体学技术来确定是亚瑟·柯南·道尔还是赫伯特·乔治·威尔斯写了小说*《失落的世界》*。
- en: '**Project #2: The Hound, The War, and The Lost World**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #2：猎犬、战争与失落的世界**'
- en: Sir Arthur Conan Doyle (1859–1930) is best known for the Sherlock Holmes stories,
    considered milestones in the field of crime fiction. H. G. Wells (1866–1946) is
    famous for several groundbreaking science fiction novels including *The War of
    The Worlds*, *The Time Machine*, *The Invisible Man*, and *The Island of Dr. Moreau*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·柯南·道尔爵士（1859–1930）最著名的作品是《福尔摩斯探案集》，被认为是犯罪小说领域的里程碑。赫伯特·乔治·威尔斯（1866–1946）以多部开创性的科幻小说而闻名，其中包括*《世界大战》*、*《时间机器》*、*《隐形人》*和*《莫罗博士岛》*。
- en: In 1912, the *Strand Magazine* published *The Lost World*, a serialized version
    of a science fiction novel. It told the story of an Amazon basin expedition, led
    by zoology professor George Edward Challenger, that encountered living dinosaurs
    and a vicious tribe of ape-like creatures.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1912年，*《海滨杂志》*刊登了*《失落的世界》*，这是一本科幻小说的连载版本。故事讲述了一次由动物学教授乔治·爱德华·挑战者领导的亚马逊盆地探险队，探险队在此过程中遇到了活恐龙和一群凶猛的类人猿部落。
- en: Although the author of the novel is known, for this project, let’s pretend it’s
    in dispute and it’s your job to solve the mystery. Experts have narrowed the field
    down to two authors, Doyle and Wells. Wells is slightly favored because *The Lost
    World* is a work of science fiction, which is his purview. It also includes brutish
    troglodytes redolent of the morlocks in his 1895 work *The Time Machine*. Doyle,
    on the other hand, is known for detective stories and historical fiction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然小说的作者已经知道，但为了这个项目，我们假设其身份存在争议，而你将负责解开这个谜团。专家们已将候选作者缩小为两位——道尔和威尔斯。威尔斯略微占优，因为*《失落的世界》*是一本科幻小说，这是他的专长。书中还出现了粗暴的穴居人，类似于他在1895年作品*《时间机器》*中的莫洛克人。而道尔则以侦探小说和历史小说闻名。
- en: THE OBJECTIVE
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Write a Python program that uses stylometry to determine whether Sir Arthur
    Conan Doyle or H. G. Wells wrote the novel *The Lost World*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个Python程序，利用文体学技术判断是亚瑟·柯南·道尔还是赫伯特·乔治·威尔斯写了小说*《失落的世界》*。
- en: '***The Strategy***'
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***策略***'
- en: The science of *natural language processing (NLP)* deals with the interactions
    between the precise and structured language of computers and the nuanced, frequently
    ambiguous “natural” language used by humans. Example uses for NLP include machine
    translations, spam detection, comprehension of search engine questions, and predictive
    text recognition for cell phone users.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理（NLP）*的科学处理计算机精确、结构化的语言与人类使用的含有细微差别且常常模糊不清的“自然”语言之间的相互作用。NLP的典型应用包括机器翻译、垃圾邮件检测、搜索引擎问题的理解，以及手机用户的预测文本识别。'
- en: 'The most common NLP tests for authorship analyze the following features of
    a text:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的自然语言处理（NLP）作者分析测试会分析以下文本特征：
- en: '**Word length** A frequency distribution plot of the length of words in a document'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**单词长度** 文档中单词长度的频率分布图'
- en: '**Stop words** A frequency distribution plot of stop words (short, noncontextual
    function words like *the*, *but*, and *if*)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**停用词** 停用词的频率分布图（如*the*、*but*和*if*等短小、无上下文的功能性词汇）'
- en: '**Parts of speech** A frequency distribution plot of words based on their syntactic
    functions (such as nouns, pronouns, verbs, adverbs, adjectives, and so on)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性** 根据词法功能（如名词、代词、动词、副词、形容词等）绘制的词频分布图'
- en: '**Most common words** A comparison of the most commonly used words in a text'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**最常用的词** 对文本中最常用词汇的比较'
- en: '**Jaccard similarity** A statistic used for gauging the similarity and diversity
    of a sample set'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jaccard相似度** 一种用于衡量样本集相似性和多样性的统计量'
- en: If Doyle and Wells have distinctive writing styles, these five tests should
    be enough to distinguish between them. We’ll talk about each test in more detail
    in the coding section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果道尔和威尔斯有明显不同的写作风格，这五个测试应该足以区分它们。我们将在编码部分更详细地讨论每个测试。
- en: To capture and analyze each author’s style, you’ll need a representative *corpus*,
    or a body of text. For Doyle, use the famous Sherlock Holmes novel *The Hound
    of the Baskervilles*, published in 1902\. For Wells, use *The War of the Worlds*,
    published in 1898\. Both these novels contain more than 50,000 words, more than
    enough for a sound statistical sampling. You’ll then compare each author’s sample
    to *The Lost World* to determine how closely the writing styles match.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉并分析每个作者的风格，你需要一个代表性的*语料库*，即一篇文本。对于道尔，可以使用著名的《福尔摩斯探案集》中的小说《巴斯克维尔的猎犬》，该书于1902年出版。对于威尔斯，可以使用《世界大战》，该书于1898年出版。这两部小说都包含超过50,000个单词，足够用于进行可靠的统计抽样。接下来，你将把每位作者的样本与《失落的世界》进行比较，以确定他们的写作风格有多相似。
- en: To perform stylometry, you’ll use the *Natural Language Toolkit (NLTK)*, a popular
    suite of programs and libraries for working with human language data in Python.
    It’s free and works on Windows, macOS, and Linux. Created in 2001 as part of a
    computational linguistics course at the University of Pennsylvania, NLTK has continued
    to develop and expand with the help of dozens of contributors. To learn more,
    check out the official NLTK website at *[http://www.nltk.org/](http://www.nltk.org/)*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行文体计量分析，你将使用*自然语言工具包（NLTK）*，这是一个用于处理Python中的人类语言数据的流行程序和库套件。它是免费的，支持Windows、macOS和Linux系统。NLTK最初于2001年作为宾夕法尼亚大学计算语言学课程的一部分创建，之后在众多贡献者的帮助下持续发展和扩展。要了解更多信息，可以访问官方的NLTK网站*
    [http://www.nltk.org/](http://www.nltk.org/)*。
- en: '***Installing NLTK***'
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***安装NLTK***'
- en: You can find installation instructions for NLTK at *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*.
    To install NLTK on Windows, open PowerShell and install it with Preferred Installer
    Program (pip).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在* [http://www.nltk.org/install.html](http://www.nltk.org/install.html)*找到NLTK的安装说明。在Windows上安装NLTK，打开PowerShell并使用首选安装程序（pip）进行安装。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you have multiple versions of Python installed, you’ll need to specify the
    version. Here’s the command for Python 3.7:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装了多个版本的Python，需要指定版本。以下是针对Python 3.7的命令：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To check that the installation was successful, open the Python interactive
    shell and enter the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查安装是否成功，打开Python交互式命令行，并输入以下内容：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you don’t get an error, you’re good to go. Otherwise, follow the installation
    instructions at *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有出现错误，那就说明一切正常。否则，请按照* [http://www.nltk.org/install.html](http://www.nltk.org/install.html)*上的安装说明进行操作。
- en: '**Downloading the Tokenizer**'
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**下载分词器**'
- en: 'To run the stylometric tests, you’ll need to break the multiple texts—or *corpora*—into
    individual words, referred to as *tokens*. At the time of this writing, the word_tokenize()
    method in NLTK implicitly calls sent_tokenize(), used to break a corpus into individual
    sentences. For handling sent_tokenize(), you’ll need the *Punkt Tokenizer Models*.
    Although this is part of NLTK, you’ll have to download it separately with the
    handy NLTK Downloader. To launch it, enter the following into the Python shell:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行文体测试，你需要将多个文本或*语料库*拆分成单独的单词，称为*标记*。在写作时，NLTK中的word_tokenize()方法会隐式调用sent_tokenize()，该方法用于将语料库拆分为独立的句子。要处理sent_tokenize()，你需要*Punkt
    Tokenizer模型*。虽然这是NLTK的一部分，但你需要通过方便的NLTK下载器单独下载。要启动它，请在Python命令行中输入以下内容：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The NLTK Downloader window should now be open ([Figure 2-1](ch02.xhtml#ch02fig1)).
    Click either the **Models** or **All Packages** tab near the top; then click **punkt**
    in the Identifier column. Scroll to the bottom of the window and set the Download
    Directory for your platform (see *[https://www.nltk.org/data.html](https://www.nltk.org/data.html)*).
    Finally, click the **Download** button to download the Punkt Tokenizer Models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该已经打开 NLTK Downloader 窗口 ([图 2-1](ch02.xhtml#ch02fig1))。点击顶部的 **Models**
    或 **All Packages** 标签，然后点击 Identifier 列中的 **punkt**。滚动到窗口底部并为你的平台设置下载目录（请参阅 *[https://www.nltk.org/data.html](https://www.nltk.org/data.html)*）。最后，点击
    **Download** 按钮以下载 Punkt Tokenizer 模型。
- en: '![Image](../images/fig02_01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig02_01.jpg)'
- en: 'Figure 2-1: Downloading the Punkt Tokenizer Models'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-1：下载 Punkt Tokenizer 模型
- en: 'Note that you can also download NLTK packages directly in the shell. Here’s
    an example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你也可以直接在 shell 中下载 NLTK 包。以下是一个示例：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You’ll also need access to the Stopwords Corpus, which can be downloaded in
    a similar manner.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要访问 Stopwords 语料库，可以通过类似的方式下载。
- en: '**Downloading the Stopwords Corpus**'
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**下载 Stopwords 语料库**'
- en: Click the **Corpora** tab in the NLTK Downloader window and download the Stopwords
    Corpus, as shown in [Figure 2-2](ch02.xhtml#ch02fig2).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 NLTK Downloader 窗口中的 **Corpora** 标签并下载 Stopwords 语料库，如 [图 2-2](ch02.xhtml#ch02fig2)
    所示。
- en: '![Image](../images/fig02_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig02_02.jpg)'
- en: 'Figure 2-2: Downloading the Stopwords Corpus'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-2：下载 Stopwords 语料库
- en: Alternatively, you can use the shell.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用 shell。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s download one more package to help you analyze parts of speech, like nouns
    and verbs. Click the **All Packages** tab in the NLTK Downloader window and download
    the Averaged Perceptron Tagger.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再下载一个包，帮助你分析词性，如名词和动词。在 NLTK Downloader 窗口中点击 **All Packages** 标签并下载 Averaged
    Perceptron Tagger。
- en: 'To use the shell, enter the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 shell，请输入以下内容：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When NLTK has finished downloading, exit the NLTK Downloader window and enter
    the following into the Python interactive shell:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当 NLTK 下载完成后，退出 NLTK Downloader 窗口，并在 Python 交互式命令行中输入以下内容：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then enter the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后输入以下内容：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you don’t encounter an error, the models and corpus successfully downloaded.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有遇到错误，模型和语料库已经成功下载。
- en: Finally, you’ll need matplotlib to make plots. If you haven’t installed it already,
    see the instructions for installing scientific packages on [page 6](ch01.xhtml#page_6).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要安装 matplotlib 来绘制图表。如果还没有安装，请参阅 [第 6 页](ch01.xhtml#page_6) 上关于安装科学包的说明。
- en: '***The Corpora***'
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***语料库***'
- en: You can download the text files for *The Hound of the Baskervilles* (*hound.txt*),
    *The War of the Worlds* (*war.txt*), and *The Lost World* (*lost.txt*), along
    with the book’s code, from *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/).*
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*
    下载《巴斯克维尔的猎犬》(*hound.txt*), 《世界大战》(*war.txt*), 和《失落的世界》(*lost.txt*) 的文本文件，以及书中的代码。
- en: These came from Project Gutenberg (*[http://www.gutenberg.org/](http://www.gutenberg.org/)*),
    a great source for public domain literature. So that you can use these texts right
    away, I’ve stripped them of extraneous material such as table of contents, chapter
    titles, copyright information, and so on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文本来自 Project Gutenberg (*[http://www.gutenberg.org/](http://www.gutenberg.org/)*)，这是一个提供公有领域文学作品的极好资源。为了让你能立即使用这些文本，我已经去除了多余的内容，如目录、章节标题、版权信息等。
- en: '***The Stylometry Code***'
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***风格计量代码***'
- en: The *stylometry.py* program you’ll write next loads the text files as strings,
    tokenizes them into words, and then runs the five stylometric analyses listed
    on [pages 28](ch02.xhtml#page_28)–[29](ch02.xhtml#page_29). The program will output
    a combination of plots and shell messages that will help you determine who wrote
    *The Lost World*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你接下来编写的 *stylometry.py* 程序会将文本文件作为字符串加载，进行分词处理，并运行列出在 [第 28 页](ch02.xhtml#page_28)–[29
    页](ch02.xhtml#page_29) 的五种风格计量分析。程序会输出包含图表和终端消息的组合，帮助你确定《失落的世界》的作者。
- en: Keep the program in the same folder as the three text files. If you don’t want
    to enter the code yourself, just follow along with the downloadable code available
    at *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 保持程序和三个文本文件在同一个文件夹中。如果你不想自己输入代码，只需跟随可下载的代码，网址为 *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*。
- en: '**Importing Modules and Defining the main() Function**'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入模块并定义 main() 函数**'
- en: '[Listing 2-1](ch02.xhtml#ch02list1) imports NLTK and matplotlib, assigns a
    constant, and defines the main() function to run the program. The functions used
    in main() will be described in detail later in the chapter.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 2-1](ch02.xhtml#ch02list1) 导入 NLTK 和 matplotlib，分配常量，并定义 main() 函数以运行程序。main()
    中使用的函数将在本章后面详细描述。'
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Listing 2-1: Importing modules and defining the main() function'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2-1：导入模块并定义 main() 函数
- en: Start by importing NLTK and the Stopwords Corpus. Then import matplotlib.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入 NLTK 和 Stopwords 语料库。然后导入 matplotlib。
- en: Create a variable called LINES and use the all-caps convention to indicate it
    should be treated as a constant. By default, matplotlib plots in color, but you’ll
    still want to designate a list of symbols for color-blind people and this black-and-white
    book!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 LINES 的变量，并使用全大写字母命名约定来表示它应该被视为常量。默认情况下，matplotlib 会以颜色绘制图形，但你仍然需要为色盲人士和这本黑白书籍指定一组符号！
- en: 'Define main() at the start of the program. The steps in this function are almost
    as readable as pseudocode and provide a good overview of what the program will
    do. The first step will be to initialize a dictionary to hold the text for each
    author ➊. The text_to_string() function will load each corpus into this dictionary
    as a string. The name of each author will be the dictionary key (using unknown
    for *The Lost World*), and the string of text from their novel will be the value.
    For example, here’s the key, Doyle, with the value text string greatly truncated:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在程序开始时定义 main()。这个函数中的步骤几乎像伪代码一样可读，并且提供了一个关于程序将要做什么的良好概述。第一步将是初始化一个字典，用于存储每个作者的文本
    ➊。text_to_string() 函数将把每个语料库加载到这个字典中作为字符串。每个作者的名字将作为字典的键（*失落的世界*用 unknown 表示），而他们小说中的文本字符串将作为值。例如，以下是键
    Doyle，其对应的值文本字符串已大幅截断：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Immediately after populating the dictionary, print the first 300 items for
    the doyle key to ensure things went as planned. This should produce the following
    printout:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在填充字典后，立即打印出 doyle 键的前 300 个条目，以确保一切按计划进行。此时应产生如下输出：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With the corpora loaded correctly, the next step is to tokenize the strings
    into words. Currently, Python doesn’t recognize words but instead works on *characters*,
    such as letters, numbers, and punctuation marks. To remedy this, you’ll use the
    make_word_dict() function to take the strings_by_author dictionary as an argument,
    split out the words in the strings, and return a dictionary called words_by_author
    with the authors as keys and a list of words as values ➋.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在语料库正确加载后，下一步是将字符串标记化为单词。目前，Python 不识别单词，而是处理 *字符*，如字母、数字和标点符号。为了解决这个问题，你将使用
    make_word_dict() 函数，将 strings_by_author 字典作为参数，分离出字符串中的单词，并返回一个名为 words_by_author
    的字典，其中作者为键，单词列表为值 ➋。
- en: Stylometry relies on word counts, so it works best when each corpus is the same
    length. There are multiple ways to ensure apples-to-apples comparisons. With *chunking*,
    you divide the text into blocks of, say, 5,000 words, and compare the blocks.
    You can also normalize by using relative frequencies, rather than direct counts,
    or by truncating to the shortest corpus.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 风格计量学依赖于单词计数，因此它在每个语料库长度相同的情况下效果最佳。有多种方法可以确保进行公平的比较。通过 *分块处理*，你可以将文本划分为若干块，例如每块
    5,000 个单词，并对这些块进行比较。你还可以使用相对频率而非直接计数，或通过截断到最短的语料库来进行归一化。
- en: Let’s explore the truncation option. Pass the words dictionary to another function,
    find_shortest_corpus(), which calculates the number of words in each author’s
    list and returns the length of the shortest corpus. [Table 2-1](ch02.xhtml#ch02table1)
    shows the length of each corpus.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下截断选项。将单词字典传递给另一个函数 find_shortest_corpus()，该函数计算每个作者列表中的单词数并返回最短语料库的长度。[表
    2-1](ch02.xhtml#ch02table1) 显示了每个语料库的长度。
- en: '**Table 2-1:** Length (Word Count) of Each Corpus'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2-1：** 每个语料库的长度（单词数）'
- en: '| Corpus | Length |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 语料库 | 长度 |'
- en: '| --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Hound (Doyle) | 58,387 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 猎犬 (道尔) | 58,387 |'
- en: '| War (Wells) | 59,469 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 战争 (威尔斯) | 59,469 |'
- en: '| World (Unknown) | 74,961 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 世界 (未知) | 74,961 |'
- en: Since the shortest corpus here represents a robust dataset of almost 60,000
    words, you’ll use the len_shortest_corpus variable to truncate the other two corpora
    to this length, prior to doing any analysis. The assumption, of course, is that
    the backend content of the truncated texts is not significantly different from
    that in the front.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此处最短的语料库代表了一个强大的数据集，包含近 60,000 个单词，你将使用 len_shortest_corpus 变量将其他两个语料库截断为这个长度，然后再进行任何分析。当然，假设是截断后的文本在后端内容上与前端内容没有显著差异。
- en: The next five lines call functions that perform the stylometric analysis, as
    listed in “The Strategy” on [page 28](ch02.xhtml#page_28) ➌. All the functions
    take the words_by_author dictionary as an argument, and most take len_shortest_corpus,
    as well. We’ll look at these functions as soon as we finish preparing the texts
    for analysis.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的五行调用了执行样式计量分析的函数，如《策略》一章中 [第28页](ch02.xhtml#page_28) ➌ 所列。所有函数都将 words_by_author
    字典作为参数，大多数函数还接受 len_shortest_corpus 作为参数。我们将在准备好文本进行分析后立即查看这些函数。
- en: '**Loading Text and Building a Word Dictionary**'
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**加载文本并构建词典**'
- en: '[Listing 2-2](ch02.xhtml#ch02list2) defines two functions. The first reads
    in a text file as a string. The second builds a dictionary with each author’s
    name as the key and his novel, now tokenized into individual words rather than
    a continuous string, as the value.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 2-2](ch02.xhtml#ch02list2) 定义了两个函数。第一个将文本文件作为字符串读取。第二个构建一个字典，以每个作者的名字作为键，小说（现在已被标记为单独的单词，而不是一个连续的字符串）作为值。'
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Listing 2-2: Defining the text_to_string() and make_word_dict() functions'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 2-2：定义 text_to_string() 和 make_word_dict() 函数
- en: First, define the text_to_string() function to load a text file. The built-in
    read() function reads the whole file as an individual string, allowing relatively
    easy file-wide manipulations. Use with to open the file so that it will be closed
    automatically regardless of how the block terminates. Just like putting away your
    toys, closing files is good practice. It prevents bad things from happening, like
    running out of file descriptors, locking files from further access, corrupting
    files, or losing data if writing to files.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义 text_to_string() 函数来加载文本文件。内置的 read() 函数将整个文件读取为一个单独的字符串，从而便于对文件进行广泛的操作。使用
    with 打开文件，这样文件无论如何结束都会自动关闭。就像收拾玩具一样，关闭文件是一种良好的习惯。它可以防止发生一些不好的事情，比如文件描述符耗尽、文件被锁定无法访问、文件损坏，或者写入文件时丢失数据。
- en: 'Some users may encounter a UnicodeDecodeError like the following one when loading
    the text:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一些用户在加载文本时可能会遇到类似下面的 UnicodeDecodeError 错误：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Encoding* and *decoding* refer to the process of converting from characters
    stored as bytes to human-readable strings. The problem is that the default encoding
    for the built-in function open() is platform dependent and depends on the value
    of locale.getpreferredencoding(). For example, you’ll get the following encoding
    if you run this on Windows 10:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*编码* 和 *解码* 指的是将存储为字节的字符转换为人类可读的字符串的过程。问题是，内置函数 open() 的默认编码是与平台相关的，并且依赖于 locale.getpreferredencoding()
    的值。例如，如果你在 Windows 10 上运行该代码，你将得到以下编码：'
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: CP-1252 is a legacy Windows character encoding. If you run the same code on
    a Mac, it may return something different, like 'US-ASCII' or 'UTF-8'.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: CP-1252 是一种传统的 Windows 字符编码。如果你在 Mac 上运行相同的代码，可能会返回不同的编码，比如 'US-ASCII' 或 'UTF-8'。
- en: UTF stands for *Unicode Transformational Format*, which is a text character
    format designed for backward compatibility with ASCII. Although UTF-8 can handle
    all character sets—and is the dominant form of encoding used on the World Wide
    Web—it’s not the default option for many text editors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: UTF 代表 *Unicode 转换格式*，它是一种文本字符格式，旨在与 ASCII 向后兼容。尽管 UTF-8 能处理所有字符集，并且是全球网络上使用的主要编码形式，但它并不是许多文本编辑器的默认选项。
- en: Additionally, Python 2 assumed all text files were encoded with latin-1, used
    for the Latin alphabet. Python 3 is more sophisticated and tries to detect encoding
    problems as early as possible. It may throw an error, however, if the encoding
    isn’t specified.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Python 2 假设所有文本文件都使用 latin-1 编码，它适用于拉丁字母表。Python 3 更加智能，尽量尽早检测编码问题。然而，如果没有指定编码，它可能会抛出错误。
- en: So, the first troubleshooting step should be to pass open() the encoding argument
    and specify UTF-8.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，第一步故障排除应该是传递编码参数给 open() 函数，并指定 UTF-8。
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you still have problems loading the corpora files, try adding an errors
    argument as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然遇到加载语料库文件的问题，可以尝试添加一个错误参数，如下所示：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can ignore errors because these text files were downloaded as UTF-8 and
    have already been tested using this approach. For more on UTF-8, see *[https://docs.python.org/3/howto/unicode.html](https://docs.python.org/3/howto/unicode.html)*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以忽略错误，因为这些文本文件是作为 UTF-8 下载的，并且已经通过这种方法进行了测试。关于 UTF-8 的更多内容，请参见 *[https://docs.python.org/3/howto/unicode.html](https://docs.python.org/3/howto/unicode.html)*。
- en: Next, define the make_word_dict() function that will take the dictionary of
    strings by author and return a dictionary of words by author ➊. First, initialize
    an empty dictionary named words_by_author. Then, loop through the keys in the
    strings_by_author dictionary. Use NLTK’s word_tokenize() method and pass it the
    string dictionary’s key. The result will be a list of tokens that will serve as
    the dictionary value for each author. Tokens are just chopped up pieces of a corpus,
    typically sentences or words.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义 `make_word_dict()` 函数，该函数将接受按作者分类的字符串字典，并返回按作者分类的单词字典➊。首先，初始化一个名为 `words_by_author`
    的空字典。然后，遍历 `strings_by_author` 字典中的键。使用NLTK的 `word_tokenize()` 方法，并传递字典的键。返回的结果将是一个标记的列表，这些标记将作为每个作者的字典值。标记就是语料库中被切割的小块，通常是句子或单词。
- en: 'The following snippet demonstrates how the process turns a continuous string
    into a list of tokens (words and punctuation):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了该过程如何将一个连续的字符串转换为标记的列表（包括单词和标点）：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is similar to using Python’s built-in split() function, but split() doesn’t
    achieve tokens from a linguistic standpoint (note that the period is not tokenized).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这与使用Python内置的 `split()` 函数类似，但 `split()` 从语言学的角度来说并不生成标记（请注意，句点没有被标记化）。
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once you have the tokens, populate the words_by_author dictionary using list
    comprehension ➋. *List comprehension* is a shorthand way to execute loops in Python.
    You need to surround the code with square brackets to indicate a list. Convert
    the tokens to lowercase and use the built-in isalpha() method, which returns True
    if all the characters in a token are part of the alphabet and False otherwise.
    This will filter out numbers and punctuation. It will also filter out hyphenated
    words or names. Finish by returning the words_by_author dictionary.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了标记，使用列表推导式➋填充 `words_by_author` 字典。*列表推导式*是Python中执行循环的一种简写方式。你需要用方括号将代码括起来，以表示这是一个列表。将标记转换为小写，并使用内置的
    `isalpha()` 方法，如果标记中的所有字符都是字母则返回True，否则返回False。这样可以过滤掉数字和标点符号，也会过滤掉带连字符的单词或名字。最后，返回
    `words_by_author` 字典。
- en: '**Finding the Shortest Corpus**'
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**查找最短的语料库**'
- en: In computational linguistics, *frequency* refers to the number of occurrences
    in a corpus. Thus, frequency means the *count*, and methods you’ll use later return
    a dictionary of words and their counts. To compare counts in a meaningful way,
    the corpora should all have the same number of words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算语言学中，*频率*指的是语料库中某个词出现的次数。因此，频率意味着*计数*，你稍后将使用的方法会返回一个包含单词及其计数的字典。为了以有意义的方式比较计数，所有语料库应该具有相同的单词数量。
- en: Because the three corpora used here are large (see [Table 2-1](ch02.xhtml#ch02table1)),
    you can safely normalize the corpora by truncating them all to the length of the
    shortest. [Listing 2-3](ch02.xhtml#ch02list3) defines a function that finds the
    shortest corpus in the words_by_author dictionary and returns its length.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里使用的三个语料库都很大（见 [表 2-1](ch02.xhtml#ch02table1)），你可以安全地通过将它们都截断为最短的长度来对语料库进行规范化。[示例
    2-3](ch02.xhtml#ch02list3) 定义了一个函数，用于在 `words_by_author` 字典中查找最短的语料库并返回其长度。
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Listing 2-3: Defining the find_shortest_corpus() function'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2-3：定义 `find_shortest_corpus()` 函数
- en: Define the function that takes the words_by_author dictionary as an argument.
    Immediately start an empty list to hold a word count.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个函数，接受 `words_by_author` 字典作为参数。立刻开始一个空列表，用于保存单词计数。
- en: Loop through the authors (keys) in the dictionary. Get the length of the value
    for each key, which is a list object, and append the length to the word_count
    list. The length here represents the number of words in the corpus. For each pass
    through the loop, print the author’s name and the length of his tokenized corpus.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历字典中的作者（键）。获取每个键对应值的长度，该值是一个列表对象，并将长度追加到 `word_count` 列表中。这里的长度表示语料库中的单词数。每次循环时，打印作者的名字以及其标记化语料库的长度。
- en: When the loop ends, use the built-in min() function to get the lowest count
    and assign it to the len_shortest_corpus variable. Print the answer and then return
    the variable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当循环结束时，使用内置的 `min()` 函数获取最小的计数，并将其赋值给 `len_shortest_corpus` 变量。打印答案，然后返回该变量。
- en: '**Comparing Word Lengths**'
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**比较单词长度**'
- en: Part of an author’s distinctive style is the words they use. Faulkner observed
    that Hemingway never sent a reader running to the dictionary; Hemingway accused
    Faulkner of using “10-dollar words.” Authorial style is expressed in the length
    of words and in vocabulary, which we’ll look at later in the chapter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作者独特风格的一部分就是他们使用的单词。福克纳观察到，海明威从不让读者跑去查字典；海明威指责福克纳使用“10美元的单词”。作者的风格通过单词的长度和词汇表达出来，这一点我们将在本章后面讨论。
- en: '[Listing 2-4](ch02.xhtml#ch02list4) defines a function to compare the length
    of words per corpus and plot the results as a frequency distribution. In a frequency
    distribution, the lengths of words are plotted against the number of counts for
    each length. For words that are six letters long, for example, one author may
    have a count of 4,000, and another may have a count of 5,500\. A frequency distribution
    allows comparison across a range of word lengths, rather than just at the average
    word length.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 2-4](ch02.xhtml#ch02list4) 定义了一个函数，用于比较每个语料库中单词的长度，并将结果绘制为频率分布图。在频率分布中，单词的长度与每个长度的计数数目进行对比。例如，对于长度为六个字母的单词，一个作者可能有
    4,000 次出现，而另一个作者可能有 5,500 次出现。频率分布允许比较不同单词长度的范围，而不仅仅是平均单词长度。'
- en: The function in [Listing 2-4](ch02.xhtml#ch02list4) uses list slicing to truncate
    the word lists to the length of the shortest corpus so the results aren’t skewed
    by the size of the novel.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 2-4](ch02.xhtml#ch02list4) 中的函数使用列表切片将单词列表截断到最短语料库的长度，以避免小说大小对结果的影响。'
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Listing 2-4: Defining the word_length_test() function'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2-4：定义 `word_length_test()` 函数
- en: All the stylometric functions will use the dictionary of tokens; almost all
    will use the length of the shortest corpus parameter to ensure consistent sample
    sizes. Use these variable names as the function parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的风格计量函数将使用标记的字典；几乎所有函数都将使用最短语料库的长度参数，以确保样本大小一致。使用这些变量名作为函数的参数。
- en: Start an empty dictionary to hold the frequency distribution of word lengths
    by author and then start making plots. Since you are going to make multiple plots,
    start by instantiating a figure object named 1. So that all the plots stay up
    after creation, turn on the interactive plot mode with plt.ion().
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个空字典来保存按作者划分的单词长度的频率分布，然后开始制作图表。由于你将制作多个图表，首先实例化一个名为 `1` 的图形对象。为了确保所有图表在创建后都能保持显示，开启交互式绘图模式
    `plt.ion()`。
- en: Next, start looping through the authors in the tokenized dictionary ➊. Use the
    enumerate() function to generate an index for each author that you’ll use to choose
    a line style for the plot. For each author, use list comprehension to get the
    length of each word in the value list, with the range truncated to the length
    of the shortest corpus. The result will be a list where each word has been replaced
    by an integer representing its length.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，开始遍历分词后的字典中的作者 ➊。使用 `enumerate()` 函数为每个作者生成一个索引，供你选择绘图的线条样式。对于每个作者，使用列表推导获取每个单词在值列表中的长度，范围被截断到最短语料库的长度。结果将是一个列表，其中每个单词都被一个表示其长度的整数所替代。
- en: Now, start populating your new by-author dictionary to hold frequency distributions.
    Use nltk.FreqDist(), which takes the list of word lengths and creates a data object
    of word frequency information that can be plotted.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，开始填充你的新按作者划分的词典，以保存频率分布。使用 `nltk.FreqDist()`，它接受一个单词长度的列表并创建一个包含单词频率信息的数据对象，该信息可以进行绘制。
- en: You can plot the dictionary directly using the class method plot(), without
    the need to reference pyplot through plt ➋. This will plot the most frequently
    occurring sample first, followed by the number of samples you specify, in this
    case, 15. This means you will see the frequency distribution of words from 1 to
    15 letters long. Use i to select from the LINES list and finish by providing a
    label and a title. The label will be used in the legend, called using plt.legend().
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接使用类方法 `plot()` 绘制词典，而无需通过 `plt` 引用 `pyplot` ➋。这将首先绘制出现频率最高的样本，接着是你指定的样本数量，在本例中为
    15。这意味着你将看到长度从 1 到 15 个字母的单词频率分布。使用 `i` 从 `LINES` 列表中选择，并通过提供标签和标题来完成。标签将用于图例，使用
    `plt.legend()` 调用。
- en: Note that you can change how the frequency data plots using the cumulative parameter.
    If you specify cumulative=True, you will see a cumulative distribution ([Figure
    2-3](ch02.xhtml#ch02fig3), left). Otherwise, plot() will default to cumulative=False,
    and you will see the actual counts, arranged from highest to lowest ([Figure 2-3](ch02.xhtml#ch02fig3),
    right). Continue to use the default option for this project.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以通过 cumulative 参数更改频率数据图的显示方式。如果指定 cumulative=True，你将看到累计分布（[图 2-3](ch02.xhtml#ch02fig3)，左）。否则，plot()
    默认为 cumulative=False，你将看到实际计数，从高到低排列（[图 2-3](ch02.xhtml#ch02fig3)，右）。在此项目中继续使用默认选项。
- en: '![Image](../images/fig02_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig02_03.jpg)'
- en: 'Figure 2-3: The NLTK cumulative plot (left) versus the default frequency plot
    (right)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-3：NLTK 累计图（左）与默认频率图（右）
- en: 'Finish by calling the plt.show() method to display the plot, but leave it commented
    out. If you want to see the plot immediately after coding this function, you can
    uncomment it. Also note that if you launch this program via Windows PowerShell,
    the plots may close immediately unless you use the block flag: plt.show(block=True).
    This will keep the plot up but halt execution of the program until the plot is
    closed.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，调用 plt.show() 方法来显示图表，但要将其注释掉。如果你希望在编写完此函数后立即看到图表，可以取消注释。还要注意，如果你通过 Windows
    PowerShell 启动该程序，图表可能会立即关闭，除非使用 block 标志：plt.show(block=True)。这会保持图表打开，但会暂停程序的执行，直到图表关闭为止。
- en: Based solely on the word length frequency plot in [Figure 2-3](ch02.xhtml#ch02fig3),
    Doyle’s style matches the unknown author’s more closely, though there are segments
    where Wells compares the same or better. Now let’s run some other tests to see
    whether we can confirm that finding.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 单凭 [图 2-3](ch02.xhtml#ch02fig3) 中的词长频率图，Doyle 的风格与未知作者的风格更为相似，尽管也有一些段落是 Wells
    相比之下相同或更优的。现在，让我们进行其他测试，看看是否能确认这一发现。
- en: '**Comparing Stop Words**'
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**比较停用词**'
- en: A *stop word* is a small word used often, like *the*, *by*, and *but*. These
    words are filtered out for tasks like online searches, because they provide no
    contextual information, and they were once thought to be of little value in identifying
    authorship.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*停用词* 是一些常用的小词，如 *the*、*by* 和 *but*。这些词在在线搜索等任务中被过滤掉，因为它们不提供上下文信息，曾被认为在识别作者方面价值不大。'
- en: But stop words, used frequently and without much thought, are perhaps the best
    signature for an author’s style. And since the texts you’re comparing are usually
    about different subjects, these stop words become important, as they are agnostic
    to content and common across all texts.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但停用词由于频繁且随意使用，或许是最能体现作者风格的标志。而且，由于你比较的文本通常涉及不同的主题，这些停用词变得非常重要，因为它们与内容无关，并且在所有文本中都有出现。
- en: '[Listing 2-5](ch02.xhtml#ch02list5) defines a function to compare the use of
    stop words in the three corpora.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 2-5](ch02.xhtml#ch02list5) 定义了一个函数，用于比较三种语料库中停用词的使用情况。'
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Listing 2-5: Defining the stopwords_test() function'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 2-5：定义 stopwords_test() 函数
- en: Define a function that takes the words dictionary and the length of the shortest
    corpus variables as arguments. Then initialize a dictionary to hold the frequency
    distribution of stop words for each author. You don’t want to cram all the plots
    in the same figure, so start a new figure named 2.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个函数，接受单词字典和最短语料库长度作为参数。然后初始化一个字典，用于保存每个作者的停用词频率分布。你不想将所有图表都放在同一个图形中，因此从新图形
    2 开始。
- en: Assign a local variable, stop_words, to the NLTK stop words corpus for English.
    Sets are quicker to search than lists, so make the corpus a set for faster lookups
    later. The next two lines, currently commented out, print the number of stop words
    (179) and the stop words themselves.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个本地变量 stop_words 赋值为 NLTK 的英语停用词语料库。集合比列表查找速度更快，因此将语料库设置为集合，以便以后更快地查找。接下来的两行代码目前被注释掉，它们会打印出停用词的数量（179）及停用词本身。
- en: Now, start looping through the authors in the words_by_author dictionary. Use
    list comprehension to pull out all the stop words in each author’s corpus and
    use these as the value in a new dictionary named stopwords_by_author. In the next
    line, you’ll pass this dictionary to NLTK’s FreqDist() method and use the output
    to populate the stopwords_by_author_freq_dist dictionary. This dictionary will
    contain the data needed to make the frequency distribution plots for each author.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the code you used to plot the word lengths in [Listing 2-4](ch02.xhtml#ch02list4),
    but set the number of samples to 50 and give it a different title. This will plot
    the top 50 stop words in use ([Figure 2-4](ch02.xhtml#ch02fig4)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_04.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-4: Frequency plot of top 50 stop words by author'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Both Doyle and the unknown author use stop words in a similar manner. At this
    point, two analyses have favored Doyle as the most likely author of the unknown
    text, but there’s still more to do.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Parts of Speech**'
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now let’s compare the parts of speech used in the three corpora. NLTK uses a
    part-of-speech (POS) tagger, called PerceptronTagger, to identify parts of speech.
    POS taggers process a sequence of tokenized words and attach a POS tag to each
    word (see [Table 2-2](ch02.xhtml#ch02table2)).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 2-2:** Parts of Speech with Tag Values'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '| Part of Speech | Tag | Part of Speech | Tag |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Coordinating conjunction | CC | Possessive pronoun | PRP$ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Cardinal number | CD | Adverb | RB |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| Determiner | DT | Adverb, comparative | RBR |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| Existential there | EX | Adverb, superlative | RBS |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| Foreign word | FW | Particle | RP |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Preposition or subordinating conjunction | IN | Symbol | SYM |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Adjective | JJ | To | TO |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| Adjective, comparative | JJR | Interjection | UH |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Adjective, superlative | JJS | Verb, base form | VB |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| List item marker | LS | Verb, past tense | VBD |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Modal | MD | Verb, gerund or present participle | VBG |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Noun, singular or mass | NN | Verb, past participle | VBN |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| Noun, plural | NNS | Verb, non-third-person singular present | VBP |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Noun, proper noun, singular | NNP | Verb, third-person singular present |
    VBZ |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Noun, proper noun, plural | NNPS | Wh-determiner, which | WDT |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Predeterminer | PDT | Wh-pronoun, who, what | WP |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Possessive ending | POS | Possessive wh-pronoun, whose | WP$ |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Personal pronoun | PRP | Wh-adverb, where, when | WRB |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: The taggers are typically trained on large datasets like the *Penn Treebank*
    or *Brown Corpus*, making them highly accurate though not perfect. You can also
    find training data and taggers for languages other than English. You don’t need
    to worry about all these various terms and their abbreviations. As with the previous
    tests, you’ll just need to compare lines in a chart.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 2-6](ch02.xhtml#ch02list6) defines a function to plot the frequency
    distribution of POS in the three corpora.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Listing 2-6: Defining the parts_of_speech_test() function'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that takes as arguments—you guessed it—the words dictionary
    and the length of the shortest corpus. Then initialize a dictionary to hold the
    frequency distribution for the POS for each author, followed by a function call
    for a third figure.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Start looping through the authors in the words_by_author dictionary and use
    list comprehension and the NLTK pos_tag() method to build a list called pos_by_author.
    For each author, this creates a list with each word in the author’s corpus replaced
    by its corresponding POS tag, as shown here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Next, make a frequency distribution of the POS list and with each loop plot
    the curve, using the top 35 samples. Note that there are only 36 POS tags and
    several, such as *list item markers*, rarely appear in novels.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: This is the final plot you’ll make, so call plt.show() to draw all the plots
    to the screen. As pointed out in the discussion of [Listing 2-4](ch02.xhtml#ch02list4),
    if you’re using Windows PowerShell to launch the program, you may need to use
    plt.show(block=True) to keep the plots from closing automatically.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The previous plots, along with the current one ([Figure 2-5](ch02.xhtml#ch02fig5)),
    should appear after about 10 seconds.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_05.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-5: Frequency plot of top 35 parts of speech by author'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the match between the Doyle and unknown curves is clearly better
    than the match of unknown to Wells. This suggests that Doyle is the author of
    the unknown corpus.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Author Vocabularies**'
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To compare the vocabularies among the three corpora, you’ll use the *chi-squared
    random variable* (X²), also known as the *test statistic*, to measure the “distance”
    between the vocabularies employed in the unknown corpus and each of the known
    corpora. The closest vocabularies will be the most similar. The formula is
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/equ_page_43_01.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: where *O* is the observed word count and *E* is the expected word count assuming
    the corpora being compared are both by the same author.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: If Doyle wrote both novels, they should both have the same—or a similar—proportion
    of the most common words. The test statistic lets you quantify how similar they
    are by measuring how much the counts for each word differ. The lower the chi-squared
    test statistic, the greater the similarity between two distributions.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 2-7](ch02.xhtml#ch02list7) defines a function to compare vocabularies
    among the three corpora.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Listing 2-7: Defining the vocab_test() function'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: The vocab_test() function needs the word dictionary but not the length of the
    shortest corpus. Like the previous functions, however, it starts by creating a
    new dictionary to hold the chi-squared value per author and then loops through
    the word dictionary.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: To calculate chi-squared, you’ll need to join each author’s corpus with the
    unknown corpus. You don’t want to combine unknown with itself, so use a conditional
    to avoid this ➊. For the current loop, combine the author’s corpus with the unknown
    one and then get the current author’s proportion by dividing the length of his
    corpus by the length of the combined corpus. Then get the frequency distribution
    of the combined corpus by calling nltk.FreqDist().
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算卡方分数，你需要将每个作者的语料库与未知语料库连接。你不想将未知与其自身合并，因此使用条件语句避免这种情况 ➊。对于当前的循环，将作者的语料库与未知语料库结合起来，然后通过将作者语料库的长度除以合并后语料库的长度来获取当前作者的比例。然后，通过调用`nltk.FreqDist()`获取合并语料库的频率分布。
- en: Now, make a list of the 1,000 most common words in the combined text by using
    the most_common() method and passing it 1000. There is no hard-and-fast rule for
    how many words you should consider in a stylometric analysis. Suggestions in the
    literature call for the most common 100 to 1,000 words. Since you are working
    with large texts, err on the side of the larger value.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过使用`most_common()`方法并传递1000，来生成合并文本中1000个最常见的单词列表。在风格计量分析中，没有硬性规定需要考虑多少个单词。文献中的建议是考虑最常见的100到1000个单词。由于你正在处理大量文本，最好偏向较大的数值。
- en: Initialize the chisquared variable with 0; then start a nested for loop that
    works through the most_common_words list ➋. The most_common() method returns a
    list of tuples, with each tuple containing the word and its count.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 用0初始化`chisquared`变量；然后启动一个嵌套的for循环，遍历`most_common_words`列表 ➋。`most_common()`方法返回一个元组列表，每个元组包含一个单词及其计数。
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, you get the observed count per author from the word dictionary. For Doyle,
    this would be the count of the most common words in the corpus of *The Hound of
    the Baskervilles*. Then, you get the expected count, which for Doyle would be
    the count you would expect if he wrote both *The Hound of the Baskervilles* and
    the unknown corpus. To do this, multiply the number of counts in the combined
    corpus by the previously calculated author’s proportion. Then apply the formula
    for chi-squared and add the result to the dictionary that tracks each author’s
    chi-squared score ➌. Display the result for each author.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从单词字典中获取每个作者的观察到的计数。对于道尔来说，这将是《巴斯克维尔的猎犬》语料库中最常见单词的计数。然后，获取预期计数，对于道尔来说，这将是如果他写了《巴斯克维尔的猎犬》和未知语料库，应该得到的计数。为此，将合并语料库中的计数数目乘以之前计算的作者比例。然后，应用卡方公式，并将结果添加到跟踪每个作者卡方分数的字典中
    ➌。显示每个作者的结果。
- en: 'To find the author with the lowest chi-squared score, call the built-in min()
    function and pass it the dictionary and dictionary key, which you obtain with
    the get() method. This will yield the *key* corresponding to the minimum *value*.
    This is important. If you omit this last argument, min() will return the minimum
    *key* based on the alphabetical order of the names, *not* their chi-squared score!
    You can see this mistake in the following snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到具有最低卡方分数的作者，调用内置的`min()`函数，并传递字典和字典键，你可以通过`get()`方法获取该键。这将返回对应最小*值*的*键*。这很重要。如果你省略了最后一个参数，`min()`将基于名称的字母顺序返回最小的*键*，*而不是*它们的卡方分数！你可以在以下代码片段中看到这个错误：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It’s easy to assume that the min() function returns the minimum numerical *value*,
    but as you saw, it looks at dictionary *keys* by default.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易假设`min()`函数返回最小的数值*值*，但正如你所看到的，默认情况下它是查看字典的*键*。
- en: Complete the function by printing the most likely author based on the chi-squared
    score.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 完成函数，通过打印基于卡方分数最可能的作者。
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Yet another test suggests that Doyle is the author!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个测试表明道尔是作者！
- en: '**Calculating Jaccard Similarity**'
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**计算Jaccard相似度**'
- en: To determine the degree of similarity among sets created from the corpora, you’ll
    use the *Jaccard similarity coefficient*. Also called the *intersection over union*,
    this is simply the area of overlap between two sets divided by the area of union
    of the two sets ([Figure 2-6](ch02.xhtml#ch02fig6)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定由语料库创建的集合之间的相似度，你将使用*Jaccard相似系数*。也叫做*交集与并集比*，它是两个集合的重叠区域面积除以两个集合的并集区域面积（[图2-6](ch02.xhtml#ch02fig6)）。
- en: '![Image](../images/fig02_06.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/fig02_06.jpg)'
- en: 'Figure 2-6: Intersection-over-union for a set is the area of overlap divided
    by the area of union.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-6：集合的交集与并集比是重叠区域的面积除以并集区域的面积。
- en: The more overlap there is between sets created from two texts, the more likely
    they were written by the same author. [Listing 2-8](ch02.xhtml#ch02list8) defines
    a function for gauging the similarity of sample sets.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 两个文本创建的集合之间重叠越多，它们更有可能是由同一位作者写的。[示例 2-8](ch02.xhtml#ch02list8)定义了一个函数，用于衡量样本集合的相似度。
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Listing 2-8: Defining the jaccard_test() function'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2-8：定义 `jaccard_test()` 函数
- en: Like most of the previous tests, the jaccard_test() function takes the word
    dictionary and length of the shortest corpus as arguments. You’ll also need a
    dictionary to hold the Jaccard coefficient for each author.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的大多数测试一样，`jaccard_test()` 函数将词典和最短语料库的长度作为参数。你还需要一个字典来存储每个作者的 Jaccard 系数。
- en: Jaccard similarity works with unique words, so you’ll need to turn the corpora
    into sets to remove duplicates. First, you’ll build a set from the unknown corpus.
    Then you’ll loop through the known corpora, turning them into sets and comparing
    them to the unknown set. Be sure to truncate all the corpora to the length of
    the shortest corpus when making the sets.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard 相似度基于唯一单词，因此你需要将语料库转换为集合，以便去除重复项。首先，你会从未知语料库中构建一个集合。然后，你会遍历已知语料库，将它们转换为集合，并与未知集合进行比较。在制作集合时，务必将所有语料库截断为最短语料库的长度。
- en: Prior to running the loop, use a generator expression to get the names of the
    authors, other than unknown, from the words_by_author dictionary ➊. A *generator
    expression* is a function that returns an object that you can iterate over one
    value at a time. It looks a lot like list comprehension, but instead of square
    brackets, it’s surrounded by parentheses. And instead of constructing a potentially
    memory-intensive list of items, the generator yields them in real time. Generators
    are useful when you have a large set of values that you need to use only once.
    I use one here as an opportunity to demonstrate the process.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行循环之前，使用生成器表达式从 `words_by_author` 字典中获取除未知作者外的作者名称 ➊。*生成器表达式*是一个返回可以逐个值进行迭代的对象的函数。它看起来很像列表推导式，但它是用圆括号包围的，而不是方括号。而且，它不会构建一个可能占用大量内存的项列表，而是实时生成这些项。当你需要使用大量的值，但只用一次时，生成器非常有用。我在这里使用它，作为展示这一过程的机会。
- en: 'When you assign a generator expression to a variable, all you get is a type
    of iterator called a *generator object*. Compare this to making a list, as shown
    here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将生成器表达式赋值给变量时，你得到的只是一个称为*生成器对象*的迭代器类型。将此与创建列表进行对比，如下所示：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The generator expression in the previous snippet is the same as this generator
    function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段中的生成器表达式与这个生成器函数是一样的：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Whereas the return statement ends a function, the yield statement *suspends*
    the function’s execution and sends a value back to the caller. Later, the function
    can resume where it left off. When a generator reaches its end, it’s “empty” and
    can’t be called again.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与返回语句结束函数不同，`yield` 语句*暂停*函数的执行，并将一个值返回给调用者。稍后，函数可以从暂停的地方继续执行。当生成器到达末尾时，它“空”了，无法再次调用。
- en: Back to the code, start a for loop using the authors generator. Find the unique
    words for each known author, just as you did for unknown. Then use the built-in
    intersection() function to find all the words shared between the current author’s
    set of words and the set for unknown. The *intersection* of two given sets is
    the largest set that contains all the elements that are common to both. With this
    information, you can calculate the Jaccard similarity coefficient ➋.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 回到代码，使用作者生成器开始一个 `for` 循环。找到每个已知作者的唯一单词，就像你对未知作者所做的那样。然后使用内置的 `intersection()`
    函数，找到当前作者单词集和未知单词集之间所有共享的单词。两个给定集合的*交集*是包含两个集合中所有共同元素的最大集合。通过这些信息，你可以计算 Jaccard
    相似系数 ➋。
- en: Update the jaccard_by_author dictionary and print each outcome in the interpreter
    window. Then find the author with the maximum Jaccard value ➌ and print the results.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 `jaccard_by_author` 字典，并在解释器窗口中打印每个结果。然后，找到具有最大 Jaccard 值 ➌ 的作者并打印结果。
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The outcome should favor Doyle.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该偏向道尔（Doyle）。
- en: Finish *stylometry.py* with the code to run the program as an imported module
    or in stand-alone mode.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 完成*stylometry.py*，编写代码以便在作为导入模块或独立模式下运行程序。
- en: '**Summary**'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: The true author of *The Lost World* is Doyle, so we’ll stop here and declare
    victory. If you want to explore further, a next step might be to add more known
    texts to doyle and wells so that their combined length is closer to that for *The
    Lost World* and you don’t have to truncate it. You could also test for sentence
    length and punctuation style or employ more sophisticated techniques like neural
    nets and genetic algorithms.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: You can also refine existing functions, like vocab_test() and jaccard_test(),
    with *stemming* and *lemmatization* techniques that reduce words to their root
    forms for better comparisons. As the program is currently written, *talk*, *talking*,
    and *talked* are all considered completely different words even though they share
    the same root.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, stylometry can’t prove with absolute certainty that Sir
    Arthur Conan Doyle wrote *The Lost World*. It can only suggest, through weight
    of evidence, that he is the more likely author than Wells. Framing the question
    very specifically is important, since you can’t evaluate all possible authors.
    For this reason, successful authorship attribution begins with good old-fashioned
    detective work that trims the list of candidates to a manageable length.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Natural Language Processing with Python: Analyzing Text with the Natural Language
    Toolkit* (O’Reilly, 2009), by Steven Bird, Ewan Klein, and Edward Loper, is an
    accessible introduction to NLP using Python, with lots of exercises and useful
    integration with the NLTK website. A new version of the book, updated for Python
    3 and NLTK 3, is available online at *[http://www.nltk.org/book/](http://www.nltk.org/book/)*.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1995, novelist Kurt Vonnegut proposed the idea that “stories have shapes
    that can be drawn on graph paper” and suggested “feeding them into computers.”
    In 2018, researchers followed up on this idea using more than 1,700 English novels.
    They applied an NLP technique called *sentiment analysis* that finds the emotional
    tone behind words. An interesting summary of their results, “Every Story in the
    World Has One of These Six Basic Plots,” can be found on the [BBC.com](http://BBC.com)
    website: *[http://www.bbc.com/culture/story/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots/](http://www.bbc.com/culture/story/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots/)*.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**Practice Project: Hunting the Hound with Dispersion**'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NLTK comes with a fun little feature, called a *dispersion plot*, that lets
    you post the location of a word in a text. More specifically, it plots the occurrences
    of a word versus how many words from the beginning of the corpus that it appears.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-7](ch02.xhtml#ch02fig7) is a dispersion plot for major characters
    in *The Hound of the Baskervilles*.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_07.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-7: Dispersion plot for major characters in The Hound of the Baskervilles'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: If you’re familiar with the story—and I won’t spoil it if you’re not—then you’ll
    appreciate the sparse occurrence of Holmes in the middle, the almost
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: bimodal distribution of Mortimer, and the late story overlap of Barrymore, Selden,
    and the hound.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Dispersion plots can have more practical applications. For example, as the author
    of technical books, I need to define a new term when it first appears. This sounds
    easy, but sometimes the editing process can shuffle whole chapters, and issues
    like this can fall through the cracks. A dispersion plot, built with a long list
    of technical terms, can make finding these first occurrences a lot easier.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: For another use case, imagine you’re a data scientist working with paralegals
    on a criminal case involving insider trading. To find out whether the accused
    talked to a certain board member just prior to making the illegal trades, you
    can load the subpoenaed emails of the accused as a continuous string and generate
    a dispersion plot. If the board member’s name appears as expected, case closed!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: For this practice project, write a Python program that reproduces the dispersion
    plot shown in [Figure 2-7](ch02.xhtml#ch02fig7). If you have problems loading
    the *hound.txt* corpus, revisit the discussion of Unicode on [page 35](ch02.xhtml#page_35).
    You can find a solution, *practice_hound_dispersion.py*, in the appendix and online.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**Practice Project: Punctuation Heatmap**'
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *heatmap* is a diagram that uses colors to represent data values. Heatmaps
    have been used to visualize the punctuation habits of famous authors (*[https://www.fastcompany.com/3057101/the-surprising-punctuation-habits-of-famous-authors-visualized/](https://www.fastcompany.com/3057101/the-surprising-punctuation-habits-of-famous-authors-visualized/)*)
    and may prove helpful in attributing authorship for *The Lost World*.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that tokenizes the three novels used in this chapter
    based solely on punctuation. Then focus on the use of semicolons. For each author,
    plot a heatmap that displays semicolons as blue and all other marks as yellow
    or red. [Figure 2-8](ch02.xhtml#ch02fig8) shows example heatmaps for Wells’ *The
    War of the Worlds* and Doyle’s *The Hound of the Baskervilles*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_08.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-8: Heatmap of semicolon use (dark squares) for Wells (left) and Doyle
    (right)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Compare the three heatmaps. Do the results favor Doyle or Wells as the author
    for *The Lost World*?
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: You can find a solution, *practice_heatmap_semicolon.py*, in the appendix and
    online.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Project: Fixing Frequency**'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted previously, frequency in NLP refers to counts, but it can also be expressed
    as the number of occurrences per unit time. Alternatively, it can be expressed
    as a ratio or percent.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Define a new version of the nltk.FreqDist() method that uses percentages, rather
    than counts, and use it to make the charts in the *stylometry.py* program. For
    help, see the Clearly Erroneous blog (*[https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/](https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/)*).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个新的版本的nltk.FreqDist()方法，使用百分比而不是计数，并用它来生成*stylometry.py*程序中的图表。如需帮助，请参见Clearly
    Erroneous博客（*[https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/](https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/)*）。
