- en: '2'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ATTRIBUTING AUTHORSHIP WITH STYLOMETRY
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/common.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Stylometry* is the quantitative study of literary style through computational
    text analysis. It’s based on the idea that we all have a unique, consistent, and
    recognizable style to our writing. This includes our vocabulary, our use of punctuation,
    the average length of our sentences and words, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: A common application of stylometry is authorship attribution. Do you ever wonder
    if Shakespeare really wrote all his plays? Or if John Lennon or Paul McCartney
    wrote the song “In My Life”? Could Robert Galbraith, author of *A Cuckoo’s Calling*,
    really be J. K. Rowling in disguise? Stylometry can find the answer!
  prefs: []
  type: TYPE_NORMAL
- en: Stylometry has been used to overturn murder convictions and even helped identify
    and convict the Unabomber in 1996\. Other uses include detecting plagiarism and
    determining the emotional tone behind words, such as in social media posts. Stylometry
    can even be used to detect signs of mental depression and suicidal tendencies.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll use multiple stylometric techniques to determine whether
    Sir Arthur Conan Doyle or H. G. Wells wrote the novel *The Lost World*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Project #2: The Hound, The War, and The Lost World**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sir Arthur Conan Doyle (1859–1930) is best known for the Sherlock Holmes stories,
    considered milestones in the field of crime fiction. H. G. Wells (1866–1946) is
    famous for several groundbreaking science fiction novels including *The War of
    The Worlds*, *The Time Machine*, *The Invisible Man*, and *The Island of Dr. Moreau*.
  prefs: []
  type: TYPE_NORMAL
- en: In 1912, the *Strand Magazine* published *The Lost World*, a serialized version
    of a science fiction novel. It told the story of an Amazon basin expedition, led
    by zoology professor George Edward Challenger, that encountered living dinosaurs
    and a vicious tribe of ape-like creatures.
  prefs: []
  type: TYPE_NORMAL
- en: Although the author of the novel is known, for this project, let’s pretend it’s
    in dispute and it’s your job to solve the mystery. Experts have narrowed the field
    down to two authors, Doyle and Wells. Wells is slightly favored because *The Lost
    World* is a work of science fiction, which is his purview. It also includes brutish
    troglodytes redolent of the morlocks in his 1895 work *The Time Machine*. Doyle,
    on the other hand, is known for detective stories and historical fiction.
  prefs: []
  type: TYPE_NORMAL
- en: THE OBJECTIVE
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that uses stylometry to determine whether Sir Arthur
    Conan Doyle or H. G. Wells wrote the novel *The Lost World*.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Strategy***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The science of *natural language processing (NLP)* deals with the interactions
    between the precise and structured language of computers and the nuanced, frequently
    ambiguous “natural” language used by humans. Example uses for NLP include machine
    translations, spam detection, comprehension of search engine questions, and predictive
    text recognition for cell phone users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common NLP tests for authorship analyze the following features of
    a text:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word length** A frequency distribution plot of the length of words in a document'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop words** A frequency distribution plot of stop words (short, noncontextual
    function words like *the*, *but*, and *if*)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parts of speech** A frequency distribution plot of words based on their syntactic
    functions (such as nouns, pronouns, verbs, adverbs, adjectives, and so on)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Most common words** A comparison of the most commonly used words in a text'
  prefs: []
  type: TYPE_NORMAL
- en: '**Jaccard similarity** A statistic used for gauging the similarity and diversity
    of a sample set'
  prefs: []
  type: TYPE_NORMAL
- en: If Doyle and Wells have distinctive writing styles, these five tests should
    be enough to distinguish between them. We’ll talk about each test in more detail
    in the coding section.
  prefs: []
  type: TYPE_NORMAL
- en: To capture and analyze each author’s style, you’ll need a representative *corpus*,
    or a body of text. For Doyle, use the famous Sherlock Holmes novel *The Hound
    of the Baskervilles*, published in 1902\. For Wells, use *The War of the Worlds*,
    published in 1898\. Both these novels contain more than 50,000 words, more than
    enough for a sound statistical sampling. You’ll then compare each author’s sample
    to *The Lost World* to determine how closely the writing styles match.
  prefs: []
  type: TYPE_NORMAL
- en: To perform stylometry, you’ll use the *Natural Language Toolkit (NLTK)*, a popular
    suite of programs and libraries for working with human language data in Python.
    It’s free and works on Windows, macOS, and Linux. Created in 2001 as part of a
    computational linguistics course at the University of Pennsylvania, NLTK has continued
    to develop and expand with the help of dozens of contributors. To learn more,
    check out the official NLTK website at *[http://www.nltk.org/](http://www.nltk.org/)*.
  prefs: []
  type: TYPE_NORMAL
- en: '***Installing NLTK***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find installation instructions for NLTK at *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*.
    To install NLTK on Windows, open PowerShell and install it with Preferred Installer
    Program (pip).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have multiple versions of Python installed, you’ll need to specify the
    version. Here’s the command for Python 3.7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that the installation was successful, open the Python interactive
    shell and enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t get an error, you’re good to go. Otherwise, follow the installation
    instructions at *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Downloading the Tokenizer**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To run the stylometric tests, you’ll need to break the multiple texts—or *corpora*—into
    individual words, referred to as *tokens*. At the time of this writing, the word_tokenize()
    method in NLTK implicitly calls sent_tokenize(), used to break a corpus into individual
    sentences. For handling sent_tokenize(), you’ll need the *Punkt Tokenizer Models*.
    Although this is part of NLTK, you’ll have to download it separately with the
    handy NLTK Downloader. To launch it, enter the following into the Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The NLTK Downloader window should now be open ([Figure 2-1](ch02.xhtml#ch02fig1)).
    Click either the **Models** or **All Packages** tab near the top; then click **punkt**
    in the Identifier column. Scroll to the bottom of the window and set the Download
    Directory for your platform (see *[https://www.nltk.org/data.html](https://www.nltk.org/data.html)*).
    Finally, click the **Download** button to download the Punkt Tokenizer Models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-1: Downloading the Punkt Tokenizer Models'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you can also download NLTK packages directly in the shell. Here’s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You’ll also need access to the Stopwords Corpus, which can be downloaded in
    a similar manner.
  prefs: []
  type: TYPE_NORMAL
- en: '**Downloading the Stopwords Corpus**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Click the **Corpora** tab in the NLTK Downloader window and download the Stopwords
    Corpus, as shown in [Figure 2-2](ch02.xhtml#ch02fig2).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-2: Downloading the Stopwords Corpus'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can use the shell.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let’s download one more package to help you analyze parts of speech, like nouns
    and verbs. Click the **All Packages** tab in the NLTK Downloader window and download
    the Averaged Perceptron Tagger.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the shell, enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When NLTK has finished downloading, exit the NLTK Downloader window and enter
    the following into the Python interactive shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t encounter an error, the models and corpus successfully downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you’ll need matplotlib to make plots. If you haven’t installed it already,
    see the instructions for installing scientific packages on [page 6](ch01.xhtml#page_6).
  prefs: []
  type: TYPE_NORMAL
- en: '***The Corpora***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the text files for *The Hound of the Baskervilles* (*hound.txt*),
    *The War of the Worlds* (*war.txt*), and *The Lost World* (*lost.txt*), along
    with the book’s code, from *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/).*
  prefs: []
  type: TYPE_NORMAL
- en: These came from Project Gutenberg (*[http://www.gutenberg.org/](http://www.gutenberg.org/)*),
    a great source for public domain literature. So that you can use these texts right
    away, I’ve stripped them of extraneous material such as table of contents, chapter
    titles, copyright information, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '***The Stylometry Code***'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The *stylometry.py* program you’ll write next loads the text files as strings,
    tokenizes them into words, and then runs the five stylometric analyses listed
    on [pages 28](ch02.xhtml#page_28)–[29](ch02.xhtml#page_29). The program will output
    a combination of plots and shell messages that will help you determine who wrote
    *The Lost World*.
  prefs: []
  type: TYPE_NORMAL
- en: Keep the program in the same folder as the three text files. If you don’t want
    to enter the code yourself, just follow along with the downloadable code available
    at *[https://nostarch.com/real-world-python/](https://nostarch.com/real-world-python/)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing Modules and Defining the main() Function**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 2-1](ch02.xhtml#ch02list1) imports NLTK and matplotlib, assigns a
    constant, and defines the main() function to run the program. The functions used
    in main() will be described in detail later in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-1: Importing modules and defining the main() function'
  prefs: []
  type: TYPE_NORMAL
- en: Start by importing NLTK and the Stopwords Corpus. Then import matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: Create a variable called LINES and use the all-caps convention to indicate it
    should be treated as a constant. By default, matplotlib plots in color, but you’ll
    still want to designate a list of symbols for color-blind people and this black-and-white
    book!
  prefs: []
  type: TYPE_NORMAL
- en: 'Define main() at the start of the program. The steps in this function are almost
    as readable as pseudocode and provide a good overview of what the program will
    do. The first step will be to initialize a dictionary to hold the text for each
    author ➊. The text_to_string() function will load each corpus into this dictionary
    as a string. The name of each author will be the dictionary key (using unknown
    for *The Lost World*), and the string of text from their novel will be the value.
    For example, here’s the key, Doyle, with the value text string greatly truncated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Immediately after populating the dictionary, print the first 300 items for
    the doyle key to ensure things went as planned. This should produce the following
    printout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: With the corpora loaded correctly, the next step is to tokenize the strings
    into words. Currently, Python doesn’t recognize words but instead works on *characters*,
    such as letters, numbers, and punctuation marks. To remedy this, you’ll use the
    make_word_dict() function to take the strings_by_author dictionary as an argument,
    split out the words in the strings, and return a dictionary called words_by_author
    with the authors as keys and a list of words as values ➋.
  prefs: []
  type: TYPE_NORMAL
- en: Stylometry relies on word counts, so it works best when each corpus is the same
    length. There are multiple ways to ensure apples-to-apples comparisons. With *chunking*,
    you divide the text into blocks of, say, 5,000 words, and compare the blocks.
    You can also normalize by using relative frequencies, rather than direct counts,
    or by truncating to the shortest corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore the truncation option. Pass the words dictionary to another function,
    find_shortest_corpus(), which calculates the number of words in each author’s
    list and returns the length of the shortest corpus. [Table 2-1](ch02.xhtml#ch02table1)
    shows the length of each corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 2-1:** Length (Word Count) of Each Corpus'
  prefs: []
  type: TYPE_NORMAL
- en: '| Corpus | Length |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hound (Doyle) | 58,387 |'
  prefs: []
  type: TYPE_TB
- en: '| War (Wells) | 59,469 |'
  prefs: []
  type: TYPE_TB
- en: '| World (Unknown) | 74,961 |'
  prefs: []
  type: TYPE_TB
- en: Since the shortest corpus here represents a robust dataset of almost 60,000
    words, you’ll use the len_shortest_corpus variable to truncate the other two corpora
    to this length, prior to doing any analysis. The assumption, of course, is that
    the backend content of the truncated texts is not significantly different from
    that in the front.
  prefs: []
  type: TYPE_NORMAL
- en: The next five lines call functions that perform the stylometric analysis, as
    listed in “The Strategy” on [page 28](ch02.xhtml#page_28) ➌. All the functions
    take the words_by_author dictionary as an argument, and most take len_shortest_corpus,
    as well. We’ll look at these functions as soon as we finish preparing the texts
    for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading Text and Building a Word Dictionary**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Listing 2-2](ch02.xhtml#ch02list2) defines two functions. The first reads
    in a text file as a string. The second builds a dictionary with each author’s
    name as the key and his novel, now tokenized into individual words rather than
    a continuous string, as the value.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-2: Defining the text_to_string() and make_word_dict() functions'
  prefs: []
  type: TYPE_NORMAL
- en: First, define the text_to_string() function to load a text file. The built-in
    read() function reads the whole file as an individual string, allowing relatively
    easy file-wide manipulations. Use with to open the file so that it will be closed
    automatically regardless of how the block terminates. Just like putting away your
    toys, closing files is good practice. It prevents bad things from happening, like
    running out of file descriptors, locking files from further access, corrupting
    files, or losing data if writing to files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some users may encounter a UnicodeDecodeError like the following one when loading
    the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Encoding* and *decoding* refer to the process of converting from characters
    stored as bytes to human-readable strings. The problem is that the default encoding
    for the built-in function open() is platform dependent and depends on the value
    of locale.getpreferredencoding(). For example, you’ll get the following encoding
    if you run this on Windows 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: CP-1252 is a legacy Windows character encoding. If you run the same code on
    a Mac, it may return something different, like 'US-ASCII' or 'UTF-8'.
  prefs: []
  type: TYPE_NORMAL
- en: UTF stands for *Unicode Transformational Format*, which is a text character
    format designed for backward compatibility with ASCII. Although UTF-8 can handle
    all character sets—and is the dominant form of encoding used on the World Wide
    Web—it’s not the default option for many text editors.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Python 2 assumed all text files were encoded with latin-1, used
    for the Latin alphabet. Python 3 is more sophisticated and tries to detect encoding
    problems as early as possible. It may throw an error, however, if the encoding
    isn’t specified.
  prefs: []
  type: TYPE_NORMAL
- en: So, the first troubleshooting step should be to pass open() the encoding argument
    and specify UTF-8.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you still have problems loading the corpora files, try adding an errors
    argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can ignore errors because these text files were downloaded as UTF-8 and
    have already been tested using this approach. For more on UTF-8, see *[https://docs.python.org/3/howto/unicode.html](https://docs.python.org/3/howto/unicode.html)*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, define the make_word_dict() function that will take the dictionary of
    strings by author and return a dictionary of words by author ➊. First, initialize
    an empty dictionary named words_by_author. Then, loop through the keys in the
    strings_by_author dictionary. Use NLTK’s word_tokenize() method and pass it the
    string dictionary’s key. The result will be a list of tokens that will serve as
    the dictionary value for each author. Tokens are just chopped up pieces of a corpus,
    typically sentences or words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet demonstrates how the process turns a continuous string
    into a list of tokens (words and punctuation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to using Python’s built-in split() function, but split() doesn’t
    achieve tokens from a linguistic standpoint (note that the period is not tokenized).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the tokens, populate the words_by_author dictionary using list
    comprehension ➋. *List comprehension* is a shorthand way to execute loops in Python.
    You need to surround the code with square brackets to indicate a list. Convert
    the tokens to lowercase and use the built-in isalpha() method, which returns True
    if all the characters in a token are part of the alphabet and False otherwise.
    This will filter out numbers and punctuation. It will also filter out hyphenated
    words or names. Finish by returning the words_by_author dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finding the Shortest Corpus**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In computational linguistics, *frequency* refers to the number of occurrences
    in a corpus. Thus, frequency means the *count*, and methods you’ll use later return
    a dictionary of words and their counts. To compare counts in a meaningful way,
    the corpora should all have the same number of words.
  prefs: []
  type: TYPE_NORMAL
- en: Because the three corpora used here are large (see [Table 2-1](ch02.xhtml#ch02table1)),
    you can safely normalize the corpora by truncating them all to the length of the
    shortest. [Listing 2-3](ch02.xhtml#ch02list3) defines a function that finds the
    shortest corpus in the words_by_author dictionary and returns its length.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-3: Defining the find_shortest_corpus() function'
  prefs: []
  type: TYPE_NORMAL
- en: Define the function that takes the words_by_author dictionary as an argument.
    Immediately start an empty list to hold a word count.
  prefs: []
  type: TYPE_NORMAL
- en: Loop through the authors (keys) in the dictionary. Get the length of the value
    for each key, which is a list object, and append the length to the word_count
    list. The length here represents the number of words in the corpus. For each pass
    through the loop, print the author’s name and the length of his tokenized corpus.
  prefs: []
  type: TYPE_NORMAL
- en: When the loop ends, use the built-in min() function to get the lowest count
    and assign it to the len_shortest_corpus variable. Print the answer and then return
    the variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Word Lengths**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Part of an author’s distinctive style is the words they use. Faulkner observed
    that Hemingway never sent a reader running to the dictionary; Hemingway accused
    Faulkner of using “10-dollar words.” Authorial style is expressed in the length
    of words and in vocabulary, which we’ll look at later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 2-4](ch02.xhtml#ch02list4) defines a function to compare the length
    of words per corpus and plot the results as a frequency distribution. In a frequency
    distribution, the lengths of words are plotted against the number of counts for
    each length. For words that are six letters long, for example, one author may
    have a count of 4,000, and another may have a count of 5,500\. A frequency distribution
    allows comparison across a range of word lengths, rather than just at the average
    word length.'
  prefs: []
  type: TYPE_NORMAL
- en: The function in [Listing 2-4](ch02.xhtml#ch02list4) uses list slicing to truncate
    the word lists to the length of the shortest corpus so the results aren’t skewed
    by the size of the novel.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-4: Defining the word_length_test() function'
  prefs: []
  type: TYPE_NORMAL
- en: All the stylometric functions will use the dictionary of tokens; almost all
    will use the length of the shortest corpus parameter to ensure consistent sample
    sizes. Use these variable names as the function parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Start an empty dictionary to hold the frequency distribution of word lengths
    by author and then start making plots. Since you are going to make multiple plots,
    start by instantiating a figure object named 1. So that all the plots stay up
    after creation, turn on the interactive plot mode with plt.ion().
  prefs: []
  type: TYPE_NORMAL
- en: Next, start looping through the authors in the tokenized dictionary ➊. Use the
    enumerate() function to generate an index for each author that you’ll use to choose
    a line style for the plot. For each author, use list comprehension to get the
    length of each word in the value list, with the range truncated to the length
    of the shortest corpus. The result will be a list where each word has been replaced
    by an integer representing its length.
  prefs: []
  type: TYPE_NORMAL
- en: Now, start populating your new by-author dictionary to hold frequency distributions.
    Use nltk.FreqDist(), which takes the list of word lengths and creates a data object
    of word frequency information that can be plotted.
  prefs: []
  type: TYPE_NORMAL
- en: You can plot the dictionary directly using the class method plot(), without
    the need to reference pyplot through plt ➋. This will plot the most frequently
    occurring sample first, followed by the number of samples you specify, in this
    case, 15. This means you will see the frequency distribution of words from 1 to
    15 letters long. Use i to select from the LINES list and finish by providing a
    label and a title. The label will be used in the legend, called using plt.legend().
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can change how the frequency data plots using the cumulative parameter.
    If you specify cumulative=True, you will see a cumulative distribution ([Figure
    2-3](ch02.xhtml#ch02fig3), left). Otherwise, plot() will default to cumulative=False,
    and you will see the actual counts, arranged from highest to lowest ([Figure 2-3](ch02.xhtml#ch02fig3),
    right). Continue to use the default option for this project.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-3: The NLTK cumulative plot (left) versus the default frequency plot
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finish by calling the plt.show() method to display the plot, but leave it commented
    out. If you want to see the plot immediately after coding this function, you can
    uncomment it. Also note that if you launch this program via Windows PowerShell,
    the plots may close immediately unless you use the block flag: plt.show(block=True).
    This will keep the plot up but halt execution of the program until the plot is
    closed.'
  prefs: []
  type: TYPE_NORMAL
- en: Based solely on the word length frequency plot in [Figure 2-3](ch02.xhtml#ch02fig3),
    Doyle’s style matches the unknown author’s more closely, though there are segments
    where Wells compares the same or better. Now let’s run some other tests to see
    whether we can confirm that finding.
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Stop Words**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A *stop word* is a small word used often, like *the*, *by*, and *but*. These
    words are filtered out for tasks like online searches, because they provide no
    contextual information, and they were once thought to be of little value in identifying
    authorship.
  prefs: []
  type: TYPE_NORMAL
- en: But stop words, used frequently and without much thought, are perhaps the best
    signature for an author’s style. And since the texts you’re comparing are usually
    about different subjects, these stop words become important, as they are agnostic
    to content and common across all texts.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 2-5](ch02.xhtml#ch02list5) defines a function to compare the use of
    stop words in the three corpora.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-5: Defining the stopwords_test() function'
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that takes the words dictionary and the length of the shortest
    corpus variables as arguments. Then initialize a dictionary to hold the frequency
    distribution of stop words for each author. You don’t want to cram all the plots
    in the same figure, so start a new figure named 2.
  prefs: []
  type: TYPE_NORMAL
- en: Assign a local variable, stop_words, to the NLTK stop words corpus for English.
    Sets are quicker to search than lists, so make the corpus a set for faster lookups
    later. The next two lines, currently commented out, print the number of stop words
    (179) and the stop words themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Now, start looping through the authors in the words_by_author dictionary. Use
    list comprehension to pull out all the stop words in each author’s corpus and
    use these as the value in a new dictionary named stopwords_by_author. In the next
    line, you’ll pass this dictionary to NLTK’s FreqDist() method and use the output
    to populate the stopwords_by_author_freq_dist dictionary. This dictionary will
    contain the data needed to make the frequency distribution plots for each author.
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the code you used to plot the word lengths in [Listing 2-4](ch02.xhtml#ch02list4),
    but set the number of samples to 50 and give it a different title. This will plot
    the top 50 stop words in use ([Figure 2-4](ch02.xhtml#ch02fig4)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-4: Frequency plot of top 50 stop words by author'
  prefs: []
  type: TYPE_NORMAL
- en: Both Doyle and the unknown author use stop words in a similar manner. At this
    point, two analyses have favored Doyle as the most likely author of the unknown
    text, but there’s still more to do.
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Parts of Speech**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now let’s compare the parts of speech used in the three corpora. NLTK uses a
    part-of-speech (POS) tagger, called PerceptronTagger, to identify parts of speech.
    POS taggers process a sequence of tokenized words and attach a POS tag to each
    word (see [Table 2-2](ch02.xhtml#ch02table2)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 2-2:** Parts of Speech with Tag Values'
  prefs: []
  type: TYPE_NORMAL
- en: '| Part of Speech | Tag | Part of Speech | Tag |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Coordinating conjunction | CC | Possessive pronoun | PRP$ |'
  prefs: []
  type: TYPE_TB
- en: '| Cardinal number | CD | Adverb | RB |'
  prefs: []
  type: TYPE_TB
- en: '| Determiner | DT | Adverb, comparative | RBR |'
  prefs: []
  type: TYPE_TB
- en: '| Existential there | EX | Adverb, superlative | RBS |'
  prefs: []
  type: TYPE_TB
- en: '| Foreign word | FW | Particle | RP |'
  prefs: []
  type: TYPE_TB
- en: '| Preposition or subordinating conjunction | IN | Symbol | SYM |'
  prefs: []
  type: TYPE_TB
- en: '| Adjective | JJ | To | TO |'
  prefs: []
  type: TYPE_TB
- en: '| Adjective, comparative | JJR | Interjection | UH |'
  prefs: []
  type: TYPE_TB
- en: '| Adjective, superlative | JJS | Verb, base form | VB |'
  prefs: []
  type: TYPE_TB
- en: '| List item marker | LS | Verb, past tense | VBD |'
  prefs: []
  type: TYPE_TB
- en: '| Modal | MD | Verb, gerund or present participle | VBG |'
  prefs: []
  type: TYPE_TB
- en: '| Noun, singular or mass | NN | Verb, past participle | VBN |'
  prefs: []
  type: TYPE_TB
- en: '| Noun, plural | NNS | Verb, non-third-person singular present | VBP |'
  prefs: []
  type: TYPE_TB
- en: '| Noun, proper noun, singular | NNP | Verb, third-person singular present |
    VBZ |'
  prefs: []
  type: TYPE_TB
- en: '| Noun, proper noun, plural | NNPS | Wh-determiner, which | WDT |'
  prefs: []
  type: TYPE_TB
- en: '| Predeterminer | PDT | Wh-pronoun, who, what | WP |'
  prefs: []
  type: TYPE_TB
- en: '| Possessive ending | POS | Possessive wh-pronoun, whose | WP$ |'
  prefs: []
  type: TYPE_TB
- en: '| Personal pronoun | PRP | Wh-adverb, where, when | WRB |'
  prefs: []
  type: TYPE_TB
- en: The taggers are typically trained on large datasets like the *Penn Treebank*
    or *Brown Corpus*, making them highly accurate though not perfect. You can also
    find training data and taggers for languages other than English. You don’t need
    to worry about all these various terms and their abbreviations. As with the previous
    tests, you’ll just need to compare lines in a chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 2-6](ch02.xhtml#ch02list6) defines a function to plot the frequency
    distribution of POS in the three corpora.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-6: Defining the parts_of_speech_test() function'
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that takes as arguments—you guessed it—the words dictionary
    and the length of the shortest corpus. Then initialize a dictionary to hold the
    frequency distribution for the POS for each author, followed by a function call
    for a third figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start looping through the authors in the words_by_author dictionary and use
    list comprehension and the NLTK pos_tag() method to build a list called pos_by_author.
    For each author, this creates a list with each word in the author’s corpus replaced
    by its corresponding POS tag, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Next, make a frequency distribution of the POS list and with each loop plot
    the curve, using the top 35 samples. Note that there are only 36 POS tags and
    several, such as *list item markers*, rarely appear in novels.
  prefs: []
  type: TYPE_NORMAL
- en: This is the final plot you’ll make, so call plt.show() to draw all the plots
    to the screen. As pointed out in the discussion of [Listing 2-4](ch02.xhtml#ch02list4),
    if you’re using Windows PowerShell to launch the program, you may need to use
    plt.show(block=True) to keep the plots from closing automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The previous plots, along with the current one ([Figure 2-5](ch02.xhtml#ch02fig5)),
    should appear after about 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-5: Frequency plot of top 35 parts of speech by author'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the match between the Doyle and unknown curves is clearly better
    than the match of unknown to Wells. This suggests that Doyle is the author of
    the unknown corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '**Comparing Author Vocabularies**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To compare the vocabularies among the three corpora, you’ll use the *chi-squared
    random variable* (X²), also known as the *test statistic*, to measure the “distance”
    between the vocabularies employed in the unknown corpus and each of the known
    corpora. The closest vocabularies will be the most similar. The formula is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/equ_page_43_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *O* is the observed word count and *E* is the expected word count assuming
    the corpora being compared are both by the same author.
  prefs: []
  type: TYPE_NORMAL
- en: If Doyle wrote both novels, they should both have the same—or a similar—proportion
    of the most common words. The test statistic lets you quantify how similar they
    are by measuring how much the counts for each word differ. The lower the chi-squared
    test statistic, the greater the similarity between two distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 2-7](ch02.xhtml#ch02list7) defines a function to compare vocabularies
    among the three corpora.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-7: Defining the vocab_test() function'
  prefs: []
  type: TYPE_NORMAL
- en: The vocab_test() function needs the word dictionary but not the length of the
    shortest corpus. Like the previous functions, however, it starts by creating a
    new dictionary to hold the chi-squared value per author and then loops through
    the word dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate chi-squared, you’ll need to join each author’s corpus with the
    unknown corpus. You don’t want to combine unknown with itself, so use a conditional
    to avoid this ➊. For the current loop, combine the author’s corpus with the unknown
    one and then get the current author’s proportion by dividing the length of his
    corpus by the length of the combined corpus. Then get the frequency distribution
    of the combined corpus by calling nltk.FreqDist().
  prefs: []
  type: TYPE_NORMAL
- en: Now, make a list of the 1,000 most common words in the combined text by using
    the most_common() method and passing it 1000. There is no hard-and-fast rule for
    how many words you should consider in a stylometric analysis. Suggestions in the
    literature call for the most common 100 to 1,000 words. Since you are working
    with large texts, err on the side of the larger value.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the chisquared variable with 0; then start a nested for loop that
    works through the most_common_words list ➋. The most_common() method returns a
    list of tuples, with each tuple containing the word and its count.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Next, you get the observed count per author from the word dictionary. For Doyle,
    this would be the count of the most common words in the corpus of *The Hound of
    the Baskervilles*. Then, you get the expected count, which for Doyle would be
    the count you would expect if he wrote both *The Hound of the Baskervilles* and
    the unknown corpus. To do this, multiply the number of counts in the combined
    corpus by the previously calculated author’s proportion. Then apply the formula
    for chi-squared and add the result to the dictionary that tracks each author’s
    chi-squared score ➌. Display the result for each author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the author with the lowest chi-squared score, call the built-in min()
    function and pass it the dictionary and dictionary key, which you obtain with
    the get() method. This will yield the *key* corresponding to the minimum *value*.
    This is important. If you omit this last argument, min() will return the minimum
    *key* based on the alphabetical order of the names, *not* their chi-squared score!
    You can see this mistake in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: It’s easy to assume that the min() function returns the minimum numerical *value*,
    but as you saw, it looks at dictionary *keys* by default.
  prefs: []
  type: TYPE_NORMAL
- en: Complete the function by printing the most likely author based on the chi-squared
    score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Yet another test suggests that Doyle is the author!
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating Jaccard Similarity**'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To determine the degree of similarity among sets created from the corpora, you’ll
    use the *Jaccard similarity coefficient*. Also called the *intersection over union*,
    this is simply the area of overlap between two sets divided by the area of union
    of the two sets ([Figure 2-6](ch02.xhtml#ch02fig6)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-6: Intersection-over-union for a set is the area of overlap divided
    by the area of union.'
  prefs: []
  type: TYPE_NORMAL
- en: The more overlap there is between sets created from two texts, the more likely
    they were written by the same author. [Listing 2-8](ch02.xhtml#ch02list8) defines
    a function for gauging the similarity of sample sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 2-8: Defining the jaccard_test() function'
  prefs: []
  type: TYPE_NORMAL
- en: Like most of the previous tests, the jaccard_test() function takes the word
    dictionary and length of the shortest corpus as arguments. You’ll also need a
    dictionary to hold the Jaccard coefficient for each author.
  prefs: []
  type: TYPE_NORMAL
- en: Jaccard similarity works with unique words, so you’ll need to turn the corpora
    into sets to remove duplicates. First, you’ll build a set from the unknown corpus.
    Then you’ll loop through the known corpora, turning them into sets and comparing
    them to the unknown set. Be sure to truncate all the corpora to the length of
    the shortest corpus when making the sets.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to running the loop, use a generator expression to get the names of the
    authors, other than unknown, from the words_by_author dictionary ➊. A *generator
    expression* is a function that returns an object that you can iterate over one
    value at a time. It looks a lot like list comprehension, but instead of square
    brackets, it’s surrounded by parentheses. And instead of constructing a potentially
    memory-intensive list of items, the generator yields them in real time. Generators
    are useful when you have a large set of values that you need to use only once.
    I use one here as an opportunity to demonstrate the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you assign a generator expression to a variable, all you get is a type
    of iterator called a *generator object*. Compare this to making a list, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The generator expression in the previous snippet is the same as this generator
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Whereas the return statement ends a function, the yield statement *suspends*
    the function’s execution and sends a value back to the caller. Later, the function
    can resume where it left off. When a generator reaches its end, it’s “empty” and
    can’t be called again.
  prefs: []
  type: TYPE_NORMAL
- en: Back to the code, start a for loop using the authors generator. Find the unique
    words for each known author, just as you did for unknown. Then use the built-in
    intersection() function to find all the words shared between the current author’s
    set of words and the set for unknown. The *intersection* of two given sets is
    the largest set that contains all the elements that are common to both. With this
    information, you can calculate the Jaccard similarity coefficient ➋.
  prefs: []
  type: TYPE_NORMAL
- en: Update the jaccard_by_author dictionary and print each outcome in the interpreter
    window. Then find the author with the maximum Jaccard value ➌ and print the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The outcome should favor Doyle.
  prefs: []
  type: TYPE_NORMAL
- en: Finish *stylometry.py* with the code to run the program as an imported module
    or in stand-alone mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The true author of *The Lost World* is Doyle, so we’ll stop here and declare
    victory. If you want to explore further, a next step might be to add more known
    texts to doyle and wells so that their combined length is closer to that for *The
    Lost World* and you don’t have to truncate it. You could also test for sentence
    length and punctuation style or employ more sophisticated techniques like neural
    nets and genetic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: You can also refine existing functions, like vocab_test() and jaccard_test(),
    with *stemming* and *lemmatization* techniques that reduce words to their root
    forms for better comparisons. As the program is currently written, *talk*, *talking*,
    and *talked* are all considered completely different words even though they share
    the same root.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, stylometry can’t prove with absolute certainty that Sir
    Arthur Conan Doyle wrote *The Lost World*. It can only suggest, through weight
    of evidence, that he is the more likely author than Wells. Framing the question
    very specifically is important, since you can’t evaluate all possible authors.
    For this reason, successful authorship attribution begins with good old-fashioned
    detective work that trims the list of candidates to a manageable length.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Natural Language Processing with Python: Analyzing Text with the Natural Language
    Toolkit* (O’Reilly, 2009), by Steven Bird, Ewan Klein, and Edward Loper, is an
    accessible introduction to NLP using Python, with lots of exercises and useful
    integration with the NLTK website. A new version of the book, updated for Python
    3 and NLTK 3, is available online at *[http://www.nltk.org/book/](http://www.nltk.org/book/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1995, novelist Kurt Vonnegut proposed the idea that “stories have shapes
    that can be drawn on graph paper” and suggested “feeding them into computers.”
    In 2018, researchers followed up on this idea using more than 1,700 English novels.
    They applied an NLP technique called *sentiment analysis* that finds the emotional
    tone behind words. An interesting summary of their results, “Every Story in the
    World Has One of These Six Basic Plots,” can be found on the [BBC.com](http://BBC.com)
    website: *[http://www.bbc.com/culture/story/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots/](http://www.bbc.com/culture/story/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots/)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Practice Project: Hunting the Hound with Dispersion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NLTK comes with a fun little feature, called a *dispersion plot*, that lets
    you post the location of a word in a text. More specifically, it plots the occurrences
    of a word versus how many words from the beginning of the corpus that it appears.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-7](ch02.xhtml#ch02fig7) is a dispersion plot for major characters
    in *The Hound of the Baskervilles*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-7: Dispersion plot for major characters in The Hound of the Baskervilles'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re familiar with the story—and I won’t spoil it if you’re not—then you’ll
    appreciate the sparse occurrence of Holmes in the middle, the almost
  prefs: []
  type: TYPE_NORMAL
- en: bimodal distribution of Mortimer, and the late story overlap of Barrymore, Selden,
    and the hound.
  prefs: []
  type: TYPE_NORMAL
- en: Dispersion plots can have more practical applications. For example, as the author
    of technical books, I need to define a new term when it first appears. This sounds
    easy, but sometimes the editing process can shuffle whole chapters, and issues
    like this can fall through the cracks. A dispersion plot, built with a long list
    of technical terms, can make finding these first occurrences a lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: For another use case, imagine you’re a data scientist working with paralegals
    on a criminal case involving insider trading. To find out whether the accused
    talked to a certain board member just prior to making the illegal trades, you
    can load the subpoenaed emails of the accused as a continuous string and generate
    a dispersion plot. If the board member’s name appears as expected, case closed!
  prefs: []
  type: TYPE_NORMAL
- en: For this practice project, write a Python program that reproduces the dispersion
    plot shown in [Figure 2-7](ch02.xhtml#ch02fig7). If you have problems loading
    the *hound.txt* corpus, revisit the discussion of Unicode on [page 35](ch02.xhtml#page_35).
    You can find a solution, *practice_hound_dispersion.py*, in the appendix and online.
  prefs: []
  type: TYPE_NORMAL
- en: '**Practice Project: Punctuation Heatmap**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A *heatmap* is a diagram that uses colors to represent data values. Heatmaps
    have been used to visualize the punctuation habits of famous authors (*[https://www.fastcompany.com/3057101/the-surprising-punctuation-habits-of-famous-authors-visualized/](https://www.fastcompany.com/3057101/the-surprising-punctuation-habits-of-famous-authors-visualized/)*)
    and may prove helpful in attributing authorship for *The Lost World*.
  prefs: []
  type: TYPE_NORMAL
- en: Write a Python program that tokenizes the three novels used in this chapter
    based solely on punctuation. Then focus on the use of semicolons. For each author,
    plot a heatmap that displays semicolons as blue and all other marks as yellow
    or red. [Figure 2-8](ch02.xhtml#ch02fig8) shows example heatmaps for Wells’ *The
    War of the Worlds* and Doyle’s *The Hound of the Baskervilles*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/fig02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-8: Heatmap of semicolon use (dark squares) for Wells (left) and Doyle
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the three heatmaps. Do the results favor Doyle or Wells as the author
    for *The Lost World*?
  prefs: []
  type: TYPE_NORMAL
- en: You can find a solution, *practice_heatmap_semicolon.py*, in the appendix and
    online.
  prefs: []
  type: TYPE_NORMAL
- en: '**Challenge Project: Fixing Frequency**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted previously, frequency in NLP refers to counts, but it can also be expressed
    as the number of occurrences per unit time. Alternatively, it can be expressed
    as a ratio or percent.
  prefs: []
  type: TYPE_NORMAL
- en: Define a new version of the nltk.FreqDist() method that uses percentages, rather
    than counts, and use it to make the charts in the *stylometry.py* program. For
    help, see the Clearly Erroneous blog (*[https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/](https://martinapugliese.github.io/plotting-the-actual-frequencies-in-a-FreqDist-in-nltk/)*).
  prefs: []
  type: TYPE_NORMAL
