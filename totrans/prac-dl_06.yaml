- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: CLASSICAL MACHINE LEARNING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: It’s satisfying to be able to write “Classical Machine Learning” as it implies
    that there is something newer that makes older techniques “classical.” Of course,
    we know by now that there is—deep learning—and we’ll get to it in the chapters
    that follow. But first, we need to build our intuition by examining older techniques
    that will help cement concepts for us and, frankly, because the older techniques
    are still useful when the situation warrants.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 能够写出“经典机器学习”这一说法令人满意，因为它意味着有更新的技术，使得旧的技术变得“经典”。当然，我们现在知道是有的——深度学习——我们将在后续章节中讨论它。但首先，我们需要通过检查较早的技术来建立我们的直觉，这些技术有助于我们巩固概念，坦率地说，因为当情况合适时，旧技术仍然是有用的。
- en: It’s tempting to include some sort of history here. To keep to the practical
    nature of this book, we won’t, but a full history of machine learning is needed,
    and as of this writing, I have not found one. Historians reading this, please
    take note. I will say that machine learning is not new; the techniques of this
    chapter go back decades and have had considerable success on their own.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想在这里加入一些历史内容。为了保持本书的实用性，我们不会这样做，但机器学习的完整历史是需要的，截至目前，我还没有找到相关的历史资料。阅读此书的历史学家，请注意。我想说的是，机器学习并不新鲜；本章的技术可以追溯数十年，并且已经取得了相当的成功。
- en: However, the successes were always limited in a way that deep learning has now
    largely overcome. Still, owning a hammer doesn’t make everything a nail. You will
    encounter problems that are well suited to these older techniques. This might
    be because there’s too little data available to train a deep model, because the
    problem is simple and easily solved by a classical technique, or because the operating
    environment is not conducive to a large, deep model (think microcontroller). Besides,
    many of these techniques are easier to understand, conceptually, than a deep model
    is, and all the comments of earlier chapters about building datasets, as well
    as the comments in [Chapter 11](ch11.xhtml#ch11) about evaluating models, still
    apply.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些成功总是有限的，而深度学习如今在很大程度上已经克服了这些局限。尽管如此，拥有一把锤子并不意味着一切问题都能用锤子解决。你会遇到适合这些旧技术的问题。这可能是因为可用的数据太少，无法训练深度模型，或者因为问题简单且容易通过经典技术解决，或者因为操作环境不适合大型深度模型（比如微控制器）。此外，许多这些技术在概念上比深度模型更容易理解，而早期章节关于构建数据集的所有评论，以及[第11章](ch11.xhtml#ch11)中关于评估模型的评论，仍然适用。
- en: The following sections will introduce several popular classical models, not
    in great detail, but in essence. All of these models are supported by sklearn.
    In [Chapter 7](ch07.xhtml#ch07), we’ll apply the models to some of the datasets
    we developed in [Chapter 5](ch05.xhtml#ch05). This will give us an idea of the
    relative performance of the models when compared to each other as well as giving
    us a baseline for comparing the performance of deep models in subsequent chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几个部分将介绍几种流行的经典模型，不会详细展开，但会讲解其本质。所有这些模型都受支持于sklearn。在[第7章](ch07.xhtml#ch07)中，我们将把这些模型应用到我们在[第5章](ch05.xhtml#ch05)中开发的一些数据集上。这将帮助我们了解这些模型相互之间的相对性能，同时为后续章节中深度模型的性能比较提供基准。
- en: We’ll examine six classical models. The order in which we discuss them roughly
    tracks with the complexity of the type of model. The first three, Nearest Centroid,
    *k*-Nearest Neighbors, and Naïve Bayes, are quite simple to understand and implement.
    The last three, Decision Trees, Random Forests, and Support Vector Machines, are
    harder, but we’ll do our best to explain what’s going on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论六种经典模型。我们讨论这些模型的顺序大致与模型类型的复杂度相关。前三个，最近质心、*k*-近邻算法和朴素贝叶斯，都非常容易理解和实现。最后三个，决策树、随机森林和支持向量机，难度较大，但我们会尽力解释其原理。
- en: Nearest Centroid
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最近质心
- en: Assume we want to build a classifier and that we have a properly designed dataset
    of *n* classes (see [Chapter 4](ch04.xhtml#ch04)). For simplicity, we’ll assume
    that we have *m* samples of each of the *n* classes. This isn’t necessary but
    saves us from adding many subscripts to things. Since our dataset is properly
    designed, we have training samples and test samples. We don’t need validation
    samples in this case, so we can throw them into the training set. Our goal is
    to have a model that uses the training set to learn so we can apply the model
    to the test set to see how it will do with new, unknown samples. Here the sample
    is a feature vector of floating-point values.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要构建一个分类器，并且我们有一个经过合理设计的包含 *n* 个类别的数据集（见[第4章](ch04.xhtml#ch04)）。为了简化起见，我们假设每个
    *n* 个类别中都有 *m* 个样本。虽然这不是必须的，但它避免了我们在公式中添加许多下标。由于我们的数据集设计得当，我们有训练样本和测试样本。在这种情况下，我们不需要验证样本，因此可以将它们放入训练集。我们的目标是有一个模型，利用训练集进行学习，然后将该模型应用到测试集上，看看它在面对新的、未知的样本时表现如何。在这里，样本是一个由浮点值组成的特征向量。
- en: 'The goal of selecting components for a feature vector is to end up with a feature
    vector that makes the different classes distinct in the feature space. Let’s say
    that the feature vector has *w* features. This means we can think of the feature
    vector as the coordinates of a point in a *w*-dimensional space. If *w* = 2 or
    *w* = 3, we can graph the feature vectors. However, mathematically, there’s no
    reason for us to restrict *w* to 2 or 3; all of what we describe here works in
    100, 500, or 1000 dimensions. Note it won’t work equally well: the dreaded curse
    of dimensionality will creep in and eventually require an exponentially large
    training dataset, but we’ll ignore this elephant in the room for now.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特征向量组件的目标是得到一个能使不同类别在特征空间中具有区分性的特征向量。假设特征向量有 *w* 个特征。这意味着我们可以将特征向量看作是 *w*
    维空间中一个点的坐标。如果 *w* = 2 或 *w* = 3，我们可以绘制特征向量。然而，从数学上讲，没有理由限制 *w* 为2或3；我们所描述的一切同样适用于100维、500维或1000维。请注意，这样做并不会总是效果一样：我们将面临维度灾难，最终可能需要一个指数级增长的训练数据集，但我们暂时忽略这个问题。
- en: If the features are well chosen, we might expect a plot of the points in the
    *w*-dimensional space to group the classes so that all of the samples from class
    0 are near each other, and all of the samples from class 1 are near each other
    but distinct from class 0, and so forth. If this is our expectation, then how
    might we use this knowledge to assign a new, unknown sample to a particular class?
    Of course, this is the goal of classification, but in this case, given our assumption
    that the classes are well separated in the feature space, what is something simple
    we could do?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征选择得当，我们可能会期望在 *w* 维空间中绘制出的点能将各类别分组，使得类别0的所有样本彼此接近，类别1的所有样本彼此接近且与类别0区分开来，依此类推。如果这是我们的预期，那么我们如何利用这些知识将一个新的、未知的样本分配到特定类别呢？当然，这是分类的目标，但在这种情况下，考虑到我们假设类别在特征空间中已经很好地区分，那么我们可以做什么简单的操作呢？
- en: '[Figure 6-1](ch06.xhtml#ch6fig1) shows a hypothetical 2D feature space with
    four distinct classes. The different classes are clearly separated in this toy
    example. A new, unknown feature vector will fall into this space as a point. The
    goal is to assign a class label to the new point, either square, star, circle,
    or triangle.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-1](ch06.xhtml#ch6fig1)展示了一个假设的二维特征空间，其中包含四个不同的类别。在这个玩具示例中，不同的类别被清晰地分隔开。一个新的、未知的特征向量将作为一个点落入这个空间。目标是为新点分配一个类别标签，可能是方形、星形、圆形或三角形。'
- en: '![image](Images/06fig01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig01.jpg)'
- en: '*Figure 6-1: A hypothetical 2D feature space with four distinct classes*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-1：一个假设的二维特征空间，包含四个不同的类别*'
- en: Since the points of [Figure 6-1](ch06.xhtml#ch6fig1) are so well grouped, we
    might think that we could represent each group by an average position in the feature
    space. Instead of the 10 square points, we’d use a single point to represent the
    squares. This seems an entirely reasonable thing to do.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[图6-1](ch06.xhtml#ch6fig1)中的点已经很好地分组，我们可能会认为可以通过特征空间中的平均位置来表示每个组。我们不再使用10个小方块来表示这些点，而是用一个点来代表这些方块。这似乎是一个完全合理的做法。
- en: 'It turns out, the average point of a group of points has a name: the *centroid*,
    the center point. We know how to compute the average of a set of numbers: add
    them up and divide by how many we added. To find the centroid of a set of points
    in 2D space, we first find the average of all the x-axis coordinates and then
    the average of all the y-axis coordinates. If we have three dimensions, we’ll
    do this for the x-, y-, and z-axes. If we have *w* dimensions, we’ll do it for
    each of the dimensions. In the end, we’ll have a single point that we can use
    to represent the entire group. If we do this for our toy example, we get [Figure
    6-2](ch06.xhtml#ch6fig2), where the centroid is shown as the large marker.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，一组点的平均点有一个名字：*重心*，即中心点。我们知道如何计算一组数字的平均值：将它们加起来并除以加起来的数量。要找到二维空间中一组点的重心，我们首先找到所有
    x 轴坐标的平均值，然后找到所有 y 轴坐标的平均值。如果我们有三维空间，我们将对 x、y 和 z 轴分别计算。如果我们有 *w* 维空间，我们将对每个维度进行此操作。最后，我们将得到一个可以代表整个组的单一点。如果我们对我们的示例进行这样的计算，得到的结果是
    [图 6-2](ch06.xhtml#ch6fig2)，其中重心显示为大标记。
- en: '![image](Images/06fig02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig02.jpg)'
- en: '*Figure 6-2: A hypothetical 2D feature space with four distinct classes and
    their centroids*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：一个假设的二维特征空间，包含四个不同的类别及其重心*'
- en: 'How is the centroid helpful to us? Well, if a new, unknown sample is given
    to us, it will be a point in the feature space as mentioned previously. We can
    then measure the distance between this point and each of the centroids and assign
    the class label of the closest centroid. The idea of *distance* is somewhat ambiguous;
    there are many different ways to define distance. One obvious way is to draw straight
    line between the two points; this distance is known as the *Euclidean distance*,
    and it’s easy enough to compute. If we have two points, (*x*[0],*y*[0]) and (*x*[1],*y*[1])
    then the Euclidean distance between them is simply the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 重心对我们有何帮助？好吧，如果给我们一个新的、未知的样本，它将是前面提到的特征空间中的一个点。我们可以测量这个点与每个重心之间的距离，并分配离它最近的重心的类别标签。*距离*的概念有些模糊；定义距离的方式有很多种。一种显而易见的方法是画一条直线连接这两个点；这个距离被称为*欧几里得距离*，计算起来相当简单。如果我们有两个点，(*x*[0],
    *y*[0]) 和 (*x*[1], *y*[1])，那么它们之间的欧几里得距离就是以下公式：
- en: '![image](Images/110equ01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/110equ01.jpg)'
- en: If we have three dimensions, the distance between two points becomes
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有三维空间，两个点之间的距离变为：
- en: '![image](Images/110equ02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/110equ02.jpg)'
- en: which can be generalized to *w* dimensions for two points, *x*[0] and *x*[1],
    as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以推广到 *w* 维空间，对于两个点 *x*[0] 和 *x*[1]，公式如下：
- en: '![image](Images/110equ03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/110equ03.jpg)'
- en: where ![Image](Images/110equ04.jpg) is the *i*-th component of the point *x*[0].
    This means, component by component, find the difference between the two points,
    square it, and add it to the squared difference of all the other components. Then,
    take the square root.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/110equ04.jpg) 是点 *x*[0] 的第 *i* 个分量。这意味着，逐个分量地找出两个点之间的差值，平方它，然后将所有其他分量的平方差加起来，最后取平方根。
- en: '[Figure 6-3](ch06.xhtml#ch6fig3) shows a sample point in the feature space
    as well as the distances to the centroids. The shortest distance is to the circle
    class, so we’d assign the new sample to that class.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-3](ch06.xhtml#ch6fig3)展示了特征空间中的一个样本点以及它与重心之间的距离。最短的距离是到圆形类别的，因此我们将该新样本分配给该类别。'
- en: '![image](Images/06fig03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig03.jpg)'
- en: '*Figure 6-3: A hypothetical 2D feature space with four distinct classes, their
    centroids, and a new, unknown sample*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-3：一个假设的二维特征空间，包含四个不同的类别、它们的重心和一个新的、未知的样本*'
- en: The process we just implemented is known as a *Nearest Centroid* classifier.
    It’s also sometimes called *template matching*. The centroids of the classes learned
    from the training data are used as a proxy for the class as a whole. Then, new
    samples use those centroids to decide on a label.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才实现的过程被称为 *最近重心* 分类器。有时也称为 *模板匹配*。从训练数据中学习到的类别的重心被用作代表整个类别的代理。然后，新的样本利用这些重心来决定标签。
- en: This seems so simple and perhaps even somewhat obvious, so why isn’t this classifier
    used more? Well, there are several reasons. One has already been mentioned, the
    curse of dimensionality. As the number of features increases, the space gets larger
    and larger, and we need exponentially more training data to get a good idea of
    where the centroids should be. So, a large feature space implies that this might
    not be the right approach.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常简单，甚至可能有些显而易见，那么为什么这个分类器没有被广泛使用呢？嗯，有几个原因。一个已经提到过了，那就是维度灾难。随着特征数量的增加，空间变得越来越大，我们需要指数级的更多训练数据来更好地确定质心的位置。所以，大的特征空间意味着这可能不是最合适的方法。
- en: 'There’s a more severe problem, however. Our toy example had very tight groups.
    What if the groups are more diffuse, even overlapping? Then the selection of the
    Nearest Centroid becomes problematic: how would we know whether the closest centroid
    represents class A or class B?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一个更严重的问题。我们的玩具例子中，各个组非常紧密。如果这些组更为分散，甚至是重叠的呢？那么选择最近质心就变得有问题了：我们怎么知道最近的质心代表的是类别A还是类别B呢？
- en: Still more severe is that a particular class might fall into *two* or more distinct
    groups. If we calculate the centroid of only the class as a whole, the centroid
    will be between the groups for the class and not represent either cluster well.
    We’d need to know that the class is split between groups and use multiple centroids
    for the class. If the feature space is small, we can plot it and see that the
    class is divided between groups. However, if the feature space is larger, there’s
    no easy way for us to decide that the class is divided between multiple groups
    and that multiple centroids are required. Still, for elementary problems, this
    approach might be ideal. Not every application deals with difficult data. We might
    be building an automated system that needs to make simple, easy decisions on new
    inputs. In that case, this simple classifier might be a perfect fit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更为严重的是，某个特定类别可能会分布在*两个*或更多的不同组中。如果我们只计算整个类别的质心，这个质心将位于类别的各个组之间，并不能很好地代表任何一个集群。我们需要知道这个类别被分割成多个组，并且需要为该类别使用多个质心。如果特征空间较小，我们可以将其绘制出来，看到该类别在不同的组之间被划分。然而，如果特征空间较大，我们就没有简单的方法来判断类别是否分布在多个组中，并且是否需要多个质心。不过，对于初级问题来说，这种方法可能是理想的。并非每个应用都涉及复杂的数据。我们可能正在构建一个自动化系统，需要在新输入上做出简单、轻松的决策。在这种情况下，这个简单的分类器可能是完美的选择。
- en: k-Nearest Neighbors
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-最近邻
- en: As we saw earlier, one problem with a centroid approach is that the classes
    might be divided among multiple groups in the feature space. As the number of
    groups increases, so would the number of centroids necessary to specify the class.
    This implies another approach. Instead of computing per class centroids, what
    if we used the training data as is and selected the class label for a new input
    sample by finding the closest member of the training set and using its label?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，质心方法的一个问题是类别可能在特征空间中分布在多个组中。随着组数的增加，指定类别所需的质心数量也会增加。这意味着需要另一种方法。我们可以不计算每个类别的质心，而是直接使用训练数据本身，找到训练集中与新输入样本最接近的成员，并使用其标签来为新样本分配类别。
- en: This type of classifier is called a *Nearest Neighbor* classifier. If we look
    at only the closest sample in the training set, we are using one neighbor, so
    we call the classifier a *1-Nearest Neighbor* or *1-NN classifier*. But we don’t
    need to look at only the nearest training point. We might want to look at several
    and then vote to assign a new sample the most common class label. In the event
    of a tie, we can select one of the class labels at random. If we use three nearest
    neighbors, we have a 3-NN classifier, and if we use *k* neighbors, we have a *k*-NN
    classifier.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分类器被称为*最近邻*分类器。如果我们只看训练集中最近的样本，我们就使用了一个邻居，所以我们把这个分类器称为*1-最近邻*或*1-NN分类器*。但是，我们不必只看最近的训练点。我们可能想看多个训练点，然后投票决定为新样本分配最常见的类别标签。如果出现平局，我们可以随机选择一个类别标签。如果我们使用三个最近邻，就得到一个3-NN分类器，如果使用*k*个邻居，就得到一个*k*-NN分类器。
- en: Let’s revisit the hypothetical dataset of [Figure 6-1](ch06.xhtml#ch6fig1) but
    generate a new version where the tight clusters are more spread out. We still
    have two features and four classes with 10 examples each. Let’s set *k* = 3, a
    typical value. To assign a label to a new sample, we plot the sample in the feature
    space and then find the three closest training data points to it. [Figure 6-4](ch06.xhtml#ch6fig4)
    shows the three nearest neighbors for three unknown samples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视[图 6-1](ch06.xhtml#ch6fig1)中的假设数据集，但生成一个新版本，其中紧密的聚类更加分散。我们仍然有两个特征和四个类别，每个类别有
    10 个示例。我们设定 *k* = 3，这是一个典型的值。为了为新样本分配标签，我们将在特征空间中绘制该样本，然后找到与它最接近的三个训练数据点。[图 6-4](ch06.xhtml#ch6fig4)
    展示了三个未知样本的三个最近邻。
- en: 'The three training data points closest to Sample A are square, square, and
    star. Therefore, by majority vote, we assign Sample A to the class square. Similarly,
    the three closest training data points for Sample B are circle, triangle, and
    triangle. Therefore, we declare Sample B to be of class triangle. Things are more
    interesting with Sample C. In this case, the three closest training samples are
    each from a different class: circle, star, and triangle. So, voting is a tie.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 离样本 A 最近的三个训练数据点分别是方形、方形和星形。因此，通过多数投票，我们将样本 A 分配到方形类别。同样，样本 B 的三个最近训练数据点分别是圆形、三角形和三角形。因此，我们将样本
    B 标记为三角形类别。样本 C 更有意思。在这种情况下，三个最近的训练样本分别来自不同的类别：圆形、星形和三角形。因此，投票结果是平局。
- en: When this happens, the *k*-NN implementation has to make a choice. The simplest
    thing to do is select the class label at random since one might argue that any
    of the three are equally as likely. Alternatively, one might believe a little
    more strongly in the value of the distance between the unknown sample and the
    training data and select the one with the shortest distance. In this case, we’d
    label Sample C with class star, since that’s the training sample closest to it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当出现这种情况时，*k*-NN 实现需要做出选择。最简单的方法是随机选择类别标签，因为有人可能会认为三者的可能性是一样的。或者，也可以更加强烈地相信未知样本与训练数据之间的距离值，并选择距离最短的那个。在这种情况下，我们会将样本
    C 标记为星形类别，因为这是与其最近的训练样本。
- en: '![image](Images/06fig04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig04.jpg)'
- en: '*Figure 6-4: Applying *k*-NN for *k* =3 to three unknown samples A, B, and
    C*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-4：将 *k*-NN 应用于 *k* = 3，处理三个未知样本 A、B 和 C*'
- en: The beauty of a *k*-NN classifier is that the training data *is* the model—no
    training step is necessary. Of course, the training data must be carried around
    with the model and, depending upon the size of the training set, finding the *k*
    nearest neighbors for a new input sample might be computationally very expensive.
    People have worked for decades to try to speed up the neighbor search or store
    the training data more efficiently, but in the end, the curse of dimensionality
    is still there and still an issue.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN 分类器的魅力在于训练数据本身就是模型——无需训练步骤。当然，训练数据必须和模型一起存储，并且根据训练集的大小，寻找新输入样本的 *k*
    个最近邻可能会非常耗费计算资源。人们已经花费了几十年的时间来加速邻居搜索或更高效地存储训练数据，但最终，维度灾难仍然存在，依然是一个问题。'
- en: 'However, some *k*-NN classifiers have performed very well: if the dimensionality
    of the feature space is small enough, *k*-NN might be attractive. There needs
    to be a balance between training data size, which leads to better performance
    but more storage and more laborious searching for neighbors, and the dimensionality
    of the feature space. The same sort of scenario that might make Nearest Centroid
    a good fit will also make *k*-NN a good fit. However, *k*-NN is perhaps more robust
    to diffuse and somewhat overlapping class groups than Nearest Centroid is. If
    the samples for a class are split between several groups, *k*-NN will be superior
    to Nearest Centroid.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些 *k*-NN 分类器表现得非常出色：如果特征空间的维度足够小，*k*-NN 可能会非常有吸引力。训练数据的大小需要平衡：虽然较大的训练数据集能带来更好的性能，但也意味着更多的存储空间以及更繁琐的邻居搜索过程，同时还要考虑特征空间的维度。能够让
    Nearest Centroid 成为合适选择的情景，也可能使 *k*-NN 成为一个好的选择。然而，*k*-NN 可能比 Nearest Centroid
    更加健壮，尤其是在类别组之间有所重叠或分散时。如果某一类的样本被分配到多个组，*k*-NN 将优于 Nearest Centroid。
- en: Naïve Bayes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Widely used in natural language processing research, the *Naïve Bayes* classifier
    is simple to implement and straightforward to understand, though we’ll have to
    include some math to do it. However, I promise, the description of what’s happening
    will make the math understandable even if the notation isn’t so familiar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理研究中广泛使用的*朴素贝叶斯*分类器实现简单，理解直观，尽管我们需要一些数学来完成它。然而，我保证，即使符号不太熟悉，所描述的内容也会让数学变得容易理解。
- en: The technique uses Bayes’ theorem (see Thomas Bayes’ “An Essay Towards Solving
    a Problem in the Doctrine of Chances” published in 1763). The theorem relates
    probabilities, and its modern formulation is
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术使用贝叶斯定理（参见托马斯·贝叶斯于1763年发表的《解决概率学问题的论文》）。该定理涉及概率关系，其现代表述是：
- en: '![image](Images/114equ01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/114equ01.jpg)'
- en: which uses some mathematical notation from probability theory that we need to
    describe to understand how we’ll use this theorem to implement a classifier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用了一些来自概率论的数学符号，我们需要对其进行描述，以便理解如何使用这个定理来实现分类器。
- en: The expression *P*(*A|B*) represents the probability that event A has occurred,
    given event B has already occurred. In this context, it’s called the *posterior
    probability*. Similarly, *P*(*B|A*) represents the probability that event B has
    occurred, given event A has occurred. We call *P*(*B|A*) the *likelihood* of B,
    given A. Finally, *P*(*A*) and *P*(*B*) represent, respectively, the probability
    that event A has occurred, regardless of event B, and the probability that event
    B has occurred, regardless of event A. We call *P*(*A*) the *prior probability*
    of A. *P*(*B*) is the probability of *B* happening regardless of *A*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式 *P*(*A|B*) 表示在已知事件B已经发生的情况下，事件A发生的概率。在此上下文中，它被称为*后验概率*。类似地，*P*(*B|A*) 表示在已知事件A发生的情况下，事件B发生的概率。我们将
    *P*(*B|A*) 称为给定A时B的*似然*。最后，*P*(*A*) 和 *P*(*B*) 分别表示事件A发生的概率（不考虑事件B），以及事件B发生的概率（不考虑事件A）。我们将
    *P*(*A*) 称为A的*先验概率*，而 *P*(*B*) 则表示事件B发生的概率，不考虑事件A。
- en: 'Bayes’ theorem gives us the probability of something happening (event A) given
    that we already know something else has happened (event B). So how does this help
    us classify? We want to know whether a feature vector belongs to a given class.
    We know the feature vector, but we don’t know the class. So if we have a dataset
    of *m* feature vectors, where each feature vector has *n* features, *x* = *{x*[1],*x*[2],*x*[3],…,*x*[*n*]*}*,
    then we can replace the *B* in Bayes’ theorem with each of the features in the
    feature vector. We can also replace *A* with *y*, the class label we want to assign
    to a new, unknown feature vector *x*. The theorem now looks like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理给出了在已知某些事情已经发生（事件B）的情况下，某个事件A发生的概率。那么这如何帮助我们分类呢？我们想知道一个特征向量是否属于某个给定类别。我们知道特征向量，但不知道类别。所以，如果我们有一个包含*m*个特征向量的数据集，其中每个特征向量有*n*个特征，*x*
    = *{x*[1],*x*[2],*x*[3],…,*x*[*n*]*}*，那么我们可以用特征向量中的每个特征来替代贝叶斯定理中的*B*。我们还可以将*A*替换为*y*，即我们想为新的未知特征向量*x*分配的类别标签。定理现在看起来是这样的：
- en: '![image](Images/114equ02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/114equ02.jpg)'
- en: Let’s explain things a bit. Bayes’ theorem states that if we know the likelihood
    of having *x* be our feature vector given that *y* is the class, and we know how
    often class *y* shows up (this is *P*(*y*), the prior probability of *y*), then
    we can calculate the probability that the class of the feature vector *x* is *y*.
    If we are able to do this for all the possible classes, all the different *y*
    values, we can select the highest probability and label the input feature vector
    *x* as belonging to that class, *y*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微解释一下。贝叶斯定理表明，如果我们知道在*y*是类别的情况下，*x*是我们的特征向量的可能性，并且我们知道类别*y*出现的频率（即*y*的先验概率，记作
    *P*(*y*)），那么我们就可以计算特征向量*x*属于类别*y*的概率。如果我们能够对所有可能的类别，所有不同的*y*值都做出这个计算，我们就可以选择概率最大的类别，并将输入的特征向量*x*标记为属于该类别*y*。
- en: Recall that a training dataset is a set of pairs, (*x*^(*i*),*y*^(*i*)), for
    a known feature vector, *x*^(*i*), and a known class it belongs to, *y*^(*i*).
    Here the *i* superscript is counting the feature vector and label pairs in the
    training dataset. Now, given a dataset like this, we can calculate *P*(*y*) by
    making a histogram of how often each class label shows up in the training set.
    We believe that the training set fairly represents the parent distribution of
    possible feature vectors so that we can use the training data to calculate the
    values we need to make use of Bayes’ theorem. (See [Chapter 4](ch04.xhtml#ch04)
    for techniques to ensure that the dataset is a good one.)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，训练数据集是由一组对（*x*^(*i*),*y*^(*i*））组成的，其中 *x*^(*i*) 是已知的特征向量，*y*^(*i*) 是已知的类别标签。这里的
    *i* 上标用来标记训练数据集中的特征向量和标签对。现在，给定一个这样的数据集，我们可以通过制作每个类别标签在训练集中出现的频率的直方图来计算 *P*(*y*)。我们相信训练集能够公平地代表可能的特征向量的母体分布，因此我们可以使用训练数据来计算我们需要的值，以便应用贝叶斯定理。（关于如何确保数据集是有效的，参见[第4章](ch04.xhtml#ch04)的内容。）
- en: 'Once we have *P*(*y*), we need to know the likelihood, *P*(*x*[1],*x*[2],*x*[3],…,*x*[*n*]*|y*).
    Unfortunately, we can’t calculate this directly. But all is not lost: we’ll make
    an assumption that will let us move ahead. We’ll assume that each of the features
    in *x* is *statistically independent*. This means that the fact that we measure
    a particular *x*[1] has nothing whatsoever to do with the values of any of the
    other *n –* 1 features. This isn’t true always, or even most of the time, but
    in practice, it turns out that this assumption is often close enough to true that
    we can get by. This is why it’s called *Naïve Bayes*, as it’s naïve to assume
    the features are independent of each other. That assumption is most definitely
    not true, for example, when our input is an image. The pixels of an image are
    highly dependent upon each other. Pick one at random, and the pixels next to it
    are almost certainly within a few values of it.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 *P*(*y*)，我们需要知道似然 *P*(*x*[1],*x*[2],*x*[3],…,*x*[*n*]*|y*)。不幸的是，我们无法直接计算这一点。但并非一切都失去希望：我们将做一个假设，允许我们继续前进。我们假设
    *x* 中的每个特征都是*统计独立*的。这意味着我们测量一个特定的 *x*[1] 与其他 *n –* 1 个特征的值之间没有任何关系。这并不总是正确的，甚至大多数情况下也不成立，但实际上，这个假设通常足够接近真理，以至于我们可以继续使用它。这就是所谓的*朴素贝叶斯*，因为假设特征彼此独立是天真的。这个假设显然不对，例如，当我们的输入是图像时。图像的像素之间是高度依赖的。随机选一个像素，它旁边的像素几乎肯定与它相差不多。
- en: 'When two events are independent, their *joint probability*, the probability
    that they both happen, is simply the product of their individual probabilities.
    The independence assumption lets us change the likelihood portion of Bayes’ theorem
    like so:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个事件是独立时，它们的*联合概率*，即两者同时发生的概率，仅仅是各自概率的乘积。独立性假设允许我们像这样改变贝叶斯定理中的似然部分：
- en: '![image](Images/115equ01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/115equ01.jpg)'
- en: The ∏ symbol means *multiplied together*, much like the ∑ symbol means *added
    together*. The right side of the equation is saying that if we know the probability
    of measuring a particular value of a feature, say feature *x*[*i*], given that
    the class label is *y*, we can get the likelihood of the entire feature vector
    *x*, given class label *y*, by multiplying each of the per feature probabilities
    together.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ∏ 符号表示*相乘*，就像 ∑ 符号表示*相加*一样。方程式的右侧表示，如果我们知道测量特定特征值的概率，比如特征 *x*[*i*]，给定类别标签 *y*，我们可以通过将每个特征的概率相乘，得到整个特征向量
    *x*，在给定类别标签 *y* 的情况下的似然。
- en: If our dataset consists of categorical values, or discrete values like integers
    (for example, age), then we can use the dataset to calculate the *P*(*x*[*i*]*|y*)
    values by building a histogram for each feature for each class. For example, if
    feature *x*[2] for class 1 has the following values
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据集由类别值或离散值（如整数，例如年龄）组成，那么我们可以通过为每个类别的每个特征构建直方图，使用数据集来计算 *P*(*x*[*i*]*|y*)
    的值。例如，如果类别1的特征 *x*[2] 有以下值
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: then each value occurs with the following probability
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 那么每个值的发生概率为
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: which comes from the number of times each value occurs divided by 100, the total
    number of values in the dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该概率来自于每个值发生的次数除以100，即数据集中值的总数。
- en: This histogram is exactly what we need to find *P*(*x*[2]*|y* = 1), the probability
    for feature 2 when the class label is 1\. For example, we can expect a new feature
    vector of class 1 to have *x*[2] = 4 about 24 percent of the time and to have
    *x*[2] = 1 about 8 percent of the time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个直方图正是我们需要的，用来找到*P*(*x*[2]*|y* = 1)，即当类别标签为1时，特征2的概率。例如，我们可以预期一个类别为1的新特征向量，其*x*[2]
    = 4的概率约为24%，而*x*[2] = 1的概率约为8%。
- en: By building tables like this for each feature and each class label, we can complete
    our classifier for the categorical and discrete cases. For a new feature vector,
    we use the tables to find the probability that each feature would have that value.
    We multiply each of those probabilities together and then multiply by the prior
    probability of that class. This, repeated for each of the *m* classes in the dataset,
    will give us a set of *m* posterior probabilities. To classify the new feature
    vector, select the largest of these *m* values, and assign the corresponding class
    label.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为每个特征和每个类别标签建立这样的表格，我们可以完成分类器，用于分类和离散情况。对于一个新的特征向量，我们使用这些表格来找到每个特征具有该值的概率。我们将这些概率相乘，然后再乘以该类别的先验概率。对于数据集中每个*m*类别，重复此过程，将得到一组*m*后验概率。要对新的特征向量进行分类，选择这些*m*值中的最大值，并分配相应的类别标签。
- en: How do we calculate *P*(*x*[*i*]*|y*) if the feature values are continuous?
    One way would be to bin the continuous values and then make tables as in the discrete
    case. Another is to make one more assumption. We need to make an assumption about
    the distribution of possible *x*[*i*] feature values that we could measure. Most
    natural phenomena seem to follow a normal distribution. We discussed the normal
    distribution in [Chapter 1](ch01.xhtml#ch01). Let’s assume, then, that the features
    all follow normal distributions. A normal distribution is defined by its mean
    value (*μ*, mu) and a standard deviation (*σ*, sigma). The mean value is just
    the average value we’d expect if we drew samples from the distribution repeatedly.
    The standard deviation is a measure of how wide the distribution is—how spread
    out it is around the mean value.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征值是连续的，我们如何计算*P*(*x*[*i*]*|y*)？一种方法是将连续值分箱，然后像离散情况一样建立表格。另一种方法是做出更多假设。我们需要对可以测量的*x*[*i*]特征值的分布做出假设。大多数自然现象似乎遵循正态分布。我们在[第1章](ch01.xhtml#ch01)中讨论了正态分布。那么，我们假设这些特征都遵循正态分布。正态分布由其均值(*μ*,
    mu)和标准差(*σ*, sigma)定义。均值就是我们如果从分布中反复抽取样本时，期望的平均值。标准差是分布宽度的度量——即它围绕均值的分散程度。
- en: Mathematically, what we want to do is replace each *P*(*x*[*i*]*|y*) like so
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们想要做的是将每个*P*(*x*[*i*]*|y*)替换为如下形式：
- en: '*P*(*x*[*i*]|*y*) ≈ *N*(*μ*[*i*], *σ*[*i*])'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x*[*i*]|*y*) ≈ *N*(*μ*[*i*], *σ*[*i*])'
- en: for each feature in our feature vector. Here N(*μ*[*i*],*σ*[*i*]) is notation
    meaning a normal distribution centered around some mean value (*μ*) and defined
    by a spread (*σ*).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征向量中的每个特征。这里的N(*μ*[*i*],*σ*[*i*])表示以某个均值(*μ*)为中心，并由分布范围(*σ*)定义的正态分布。
- en: 'We don’t really know the exact *μ* and *σ* values, but we can approximate them
    from the training data. For example, assume the training data consists of 25 samples,
    where the class label is 0\. Further, assume that the following are the values
    of feature 3, that is, *x*[3], in those cases:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不完全知道精确的*μ*和*σ*值，但我们可以从训练数据中对它们进行近似。例如，假设训练数据包含25个样本，其中类别标签为0\。进一步假设以下是特征3，也就是*x*[3]，在这些情况下的值：
- en: 0.21457111,  4.3311102,   5.50481251,  0.80293956,  2.5051598,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 0.21457111, 4.3311102, 5.50481251, 0.80293956, 2.5051598,
- en: 2.37655204,  2.4296739,   2.84224169, -0.11890662,  3.18819152,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2.37655204, 2.4296739, 2.84224169, -0.11890662, 3.18819152,
- en: 1.6843311,   4.05982237,  4.14488722,  4.29148855,  3.22658406,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 1.6843311, 4.05982237, 4.14488722, 4.29148855, 3.22658406,
- en: 6.45507675,  0.40046778,  1.81796124,  0.2732696,   2.91498336,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 6.45507675, 0.40046778, 1.81796124, 0.2732696, 2.91498336,
- en: 1.42561983,  2.73483704,  1.68382843,  3.80387653,  1.53431146
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 1.42561983, 2.73483704, 1.68382843, 3.80387653, 1.53431146
- en: Then we’d use *μ*[3] = 2.58 and *σ*[3] = 1.64 when setting up the normal distribution
    for feature 3 for class 0 since the average of these values is 2.58, and the standard
    deviation, the spread around the mean value, is 1.64.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当为类别0设置特征3的正态分布时，我们将使用*μ*[3] = 2.58和*σ*[3] = 1.64，因为这些值的平均值为2.58，标准差，即围绕均值的分布范围，为1.64。
- en: When a new unknown sample is given to the classifier, we would compute the probability
    of the given *x*[3] happening if the actual class was class 0 by using the following
    equation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当给定一个新的未知样本时，我们将使用以下方程计算在实际类别为类别0的情况下给定的*x*[3]发生的概率。
- en: '![image](Images/117equ01.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/117equ01.jpg)'
- en: This equation comes from the definition of a normal distribution with mean *μ*
    and standard deviation *σ*. It says that the likelihood of a particular feature
    value, given the class is *y*, is distributed around the mean value we measured
    from the training data according to the normal distribution. This is an assumption
    we are making on top of the independence assumption between features.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程来自于正态分布的定义，其中均值为*μ*，标准差为*σ*。它表明，给定类别为*y*的情况下，某一特征值的可能性是围绕我们从训练数据中测量到的均值按照正态分布分布的。这是我们在特征之间独立性假设基础上做出的假设。
- en: 'We use this equation for each of the features in the unknown feature vector.
    We’d then multiply the resulting probabilities together, and multiply that value
    by the prior probability of class 0 happening. We’d repeat this process for each
    of the classes. In the end, we’ll have *m* numbers, the probabilities of the feature
    vector belonging to each of the *m* classes. To make a final decision, we’d do
    what we did before: select the largest of these probabilities and label the input
    as being of the corresponding class.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会对未知特征向量中的每个特征使用这个方程。然后将得到的概率相乘，并将这个值乘以类别0的先验概率。我们会对每个类别重复这一过程。最终，我们会得到*m*个数字，即特征向量属于每个*m*类别的概率。为了做出最终决定，我们将像之前一样：选择这些概率中的最大值，并将输入标记为对应的类别。
- en: Some readers may complain that we ignored the denominator of Bayes’ theorem.
    We did that because it’s a constant across all the calculations, and since we
    always select the largest posterior probability, we really don’t care whether
    we divide each value by a constant. We’ll select the same class label, regardless.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有些读者可能会抱怨我们忽略了贝叶斯定理中的分母。我们之所以这么做，是因为它在所有计算中是一个常数，而且由于我们总是选择最大的后验概率，所以其实我们并不在意是否将每个值除以常数。我们无论如何都会选择相同的类别标签。
- en: Also, for the discrete case, it’s possible that our training set does not have
    any instances of a value that rarely shows up. We ignored that, too, but it is
    a problem since if the value never shows up, the *P*(*x*[*i*]*|y*) we use would
    be 0, making the entire posterior probability 0\. This often happens in natural
    language processing, where a particular word is rarely used. A technique called
    *Laplace smoothing* gets around this, but for our purposes, we claim that a “good”
    training set will represent *all* possible values for the features and simply
    press ahead. The sklearn `MultinomialNB` Naïve Bayes classifier for discrete data
    uses Laplace smoothing by default.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于离散情况，可能我们的训练集没有包含那些极少出现的值的实例。我们忽略了这一点，但这是一个问题，因为如果某个值从未出现，那么我们使用的*P*(*x*[*i*]*|y*)将为0，从而使整个后验概率为0。这个问题经常出现在自然语言处理中，某些词汇使用非常少。一个叫做*拉普拉斯平滑*的技术可以解决这个问题，但就我们而言，我们主张一个“好的”训练集将能够表示特征的*所有*可能值，并继续前进。sklearn的`MultinomialNB`朴素贝叶斯分类器对离散数据默认使用拉普拉斯平滑。
- en: Decision Trees and Random Forests
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树与随机森林
- en: 'The left side of [Figure 6-5](ch06.xhtml#ch6fig5) shows an x-ray image of a
    puppy with a malformed right hip socket. Since the puppy is on its back in the
    x-ray, the right hip socket is on the left side of the image. The right side of
    [Figure 6-5](ch06.xhtml#ch6fig5) shows the corresponding histogram of the pixel
    intensities (8-bit values, [0,255]). There are two modes to this histogram, corresponding
    to the dark background and the lighter-intensity x-ray data. If we want to classify
    each pixel of the image into either background or x-ray, we can do so with the
    following rule: “If the pixel intensity is less than 11, call the pixel background.”'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-5](ch06.xhtml#ch6fig5)的左侧展示了一只小狗的X光图像，右髋关节窝发育不良。由于小狗在X光片中是仰躺的，右髋关节窝位于图像的左侧。图像右侧展示了对应的像素强度直方图（8位值，[0,255]）。该直方图有两个峰值，分别对应于黑色背景和较亮的X光数据。如果我们想将图像中的每个像素分类为背景或X光，我们可以使用以下规则：“如果像素强度小于11，将该像素归为背景。”'
- en: '![image](Images/06fig05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig05.jpg)'
- en: '*Figure 6-5: An x-ray image of a puppy (left). The corresponding histogram
    of 8-bit pixel values [0,255] (right).*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-5：一只小狗的X光图像（左）。对应的8位像素值直方图[0,255]（右）。*'
- en: This rule implements a decision made about the data based on one of the features,
    in this case, the pixel intensity value. Simple decisions like this are at the
    heart of *Decision Trees*, the classification algorithm we’ll explore in this
    section. For completeness, if we apply the decision rule to each pixel in the
    image and output 0 or 255 (maximum pixel value) for background versus x-ray data,
    we get a mask showing us which pixels are part of the image. See [Figure 6-6](ch06.xhtml#ch6fig6).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则实现了基于数据某个特征（在此为像素强度值）做出的决策。像这样的简单决策正是*决策树*——我们将在本节中探讨的分类算法——的核心。如果我们将这个决策规则应用于图像中的每个像素，并输出
    0 或 255（背景与 X 光数据的最大像素值），就能得到一个掩码，显示哪些像素是图像的一部分。见[图 6-6](ch06.xhtml#ch6fig6)。
- en: '![image](Images/06fig06.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig06.jpg)'
- en: '*Figure 6-6: An x-ray image of a puppy (left). The corresponding pixel mask
    generated by the decision rule. White pixels are part of the x-ray image (right).*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-6：一张小狗的 X 光图像（左）。由决策规则生成的相应像素掩码。白色像素为 X 光图像的一部分（右）。*'
- en: A Decision Tree is a set of nodes. The nodes either define a condition and branch
    based on the truth or falsehood of the condition, or select a particular class.
    Nodes that do not branch are called *leaf nodes*. Decision Trees are called *trees*
    because, especially for the binary case we’ll consider here, they branch like
    trees. [Figure 6-7](ch06.xhtml#ch6fig7) shows a Decision Tree learned by the sklearn
    `DecisionTreeClassifier` class for the full iris dataset using the first three
    features. See [Chapter 5](ch05.xhtml#ch05).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一组节点。这些节点要么根据条件的真假定义一个条件并分支，要么选择一个特定的类别。没有分支的节点称为*叶节点*。决策树之所以叫做*树*，是因为，特别是在我们这里讨论的二分类情况下，它们像树一样分支。[图
    6-7](ch06.xhtml#ch6fig7)展示了一个通过 sklearn `DecisionTreeClassifier` 类针对完整的鸢尾花数据集，使用前几个特征所学习到的决策树。见[第
    5 章](ch05.xhtml#ch05)。
- en: '![image](Images/06fig07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig07.jpg)'
- en: '*Figure 6-7: A Decision Tree classifier for the iris dataset*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-7：鸢尾花数据集的决策树分类器*'
- en: By convention, the first node in the tree, the *root*, is drawn at the top.
    For this tree, the root node asks the question, “Is the petal length ≤ 2.45?”
    If it is, the left branch is taken, and the tree immediately reaches a leaf node
    and assigns a label of “virginica” (class 0). We’ll discuss the other information
    in the nodes shortly. If the petal length is not ≤ 2.45, the right branch is taken,
    leading to a new node that asks, “Is the petal length ≤ 4.75?” If so, we move
    to a node that asks a question about the sepal length. If not, we move to the
    right node and consider the petal length again. This process continues until a
    leaf is reached, which determines the class label.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，树中的第一个节点，即*根节点*，绘制在最上方。对于这棵树，根节点提出了问题：“花瓣长度是否 ≤ 2.45？”如果是，则选择左侧分支，树立即到达叶节点并赋予标签“virginica”（类别
    0）。稍后我们将讨论节点中的其他信息。如果花瓣长度不 ≤ 2.45，则选择右侧分支，进入一个新节点，该节点询问：“花瓣长度是否 ≤ 4.75？”如果是，我们进入一个询问萼片长度的问题的节点。如果不是，则进入右侧节点，再次考虑花瓣长度。这个过程持续进行，直到到达叶节点，进而确定类别标签。
- en: The process just described is exactly how a Decision Tree is used after it’s
    created. For any new feature vector, the series of questions is asked starting
    with the root node, and the tree is traversed until a leaf node is reached to
    decide the class label. This is a human-friendly way to move through the classification
    process, which is why Decision Trees are handy in situations where the “why” of
    the class assignment is as important to know as the class assignment itself. A
    Decision Tree can explain itself.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才描述的过程正是决策树在创建后使用的方式。对于任何新的特征向量，从根节点开始一系列问题被提问，树会被遍历，直到到达叶节点以决定类别标签。这是一种对人类友好的分类过程，因此决策树在需要了解类别分配“为什么”与类别分配本身同样重要的场合非常有用。决策树能够自我解释。
- en: Using a Decision Tree is simple enough, but how is the tree created in the first
    place? Unlike the simple algorithms in the previous sections, the tree-building
    process is more involved, but not so involved that we can’t follow through the
    main steps to build some intuition as to what goes into defining the tree.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树足够简单，但树是如何一开始就创建出来的呢？与前几节中的简单算法不同，构建树的过程更为复杂，但也并非如此复杂，以至于我们无法跟随主要步骤，从中获得一些关于定义树的直觉。
- en: Recursion Primer
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 递归基础
- en: Before we talk about the Decision Tree algorithm, however, we need to discuss
    the concept of *recursion*. If you’re familiar with computer science, you probably
    already know that tree-like data structures and recursion go hand in hand. If
    not, don’t worry; recursion is a straightforward but powerful concept. The essence
    of a recursive algorithm is that the algorithm repeats itself at different levels.
    When implemented as a function in a programming language, this generally means
    that the function calls itself on a smaller version of the problem. Naturally,
    if the function calls itself indefinitely, we’ll have an infinite loop, so the
    recursion needs a stopping condition—something that says we no longer need to
    recurse.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在讨论决策树算法之前，我们需要先讨论*递归*的概念。如果你熟悉计算机科学，你可能已经知道树状数据结构和递归是密不可分的。如果你不太了解，别担心；递归是一个简单但强大的概念。递归算法的本质是算法在不同的层次上重复自己。当作为编程语言中的函数实现时，通常意味着该函数在问题的较小版本上递归调用自己。当然，如果函数无限递归调用自己，我们就会陷入无限循环，因此递归需要一个停止条件——即某种方式来说明我们不再需要继续递归。
- en: Let’s introduce the idea of recursion mathematically. The factorial of an integer,
    *n*, denoted *n*!, is defined to be
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数学上引入递归的概念。一个整数 *n* 的阶乘，记作 *n*!，定义为
- en: '*n*! = *n*(*n* – 1)(*n* – 2)(*n* – 3) . . . (*n* – *n* + 1)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*! = *n*(*n* – 1)(*n* – 2)(*n* – 3) . . . (*n* – *n* + 1)'
- en: which just means multiply together all the integers from 1 to *n*. By definition,
    0! = 1\. Therefore, the factorial of 5 is 120 because
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着将从 1 到 *n* 的所有整数相乘。根据定义，0! = 1。所以，5 的阶乘是 120，因为
- en: 5! = 5 × 4 × 3 × 2 × 1 = 120
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 5! = 5 × 4 × 3 × 2 × 1 = 120
- en: If we look at 5! we see that it is nothing more than 5 × 4! or, in general,
    that, *n*! = *n* × (*n –* 1)!. Now, let’s write a Python function to calculate
    factorials recursively using this insight. The code is simple, also a hallmark
    of many recursive functions, as [Listing 6-1](ch06.xhtml#ch6lis1) shows.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看 5!，我们会发现它不过是 5 × 4!，或者更一般地说，*n*! = *n* × (*n –* 1)!. 现在，让我们用这个洞察力编写一个递归计算阶乘的
    Python 函数。代码很简单，也是许多递归函数的典型特征，正如 [清单 6-1](ch06.xhtml#ch6lis1) 所示。
- en: 'def fact(n):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'def fact(n):'
- en: 'if (n <= 1):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (n <= 1):'
- en: return 1
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: return 1
- en: 'else:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: return n*fact(n-1)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: return n*fact(n-1)
- en: '*Listing 6-1: Calculating the factorial*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-1：计算阶乘*'
- en: The code is a direct implementation of the rule that the factorial of *n* is
    *n* times the factorial of *n –* 1\. To find the factorial of `n`, we first ask
    if `n` is 1\. If it is, we know the factorial is 1 so we return 1—this is our
    stopping condition. If `n` is not 1, we know that the factorial of `n` is simply
    `n` times the factorial of `n-1`, which we find by calling `fact` with `n-1` as
    the argument.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是阶乘公式的直接实现：*n* 的阶乘等于 *n* 乘以 *n - 1* 的阶乘。要计算 `n` 的阶乘，我们首先判断 `n` 是否为 1。如果是，我们知道阶乘是
    1，于是返回 1——这就是我们的停止条件。如果 `n` 不是 1，我们知道 *n* 的阶乘是 *n* 乘以 *n-1* 的阶乘，我们通过将 `n-1` 作为参数递归调用
    `fact` 来得到 *n-1* 的阶乘。
- en: Building Decision Trees
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建决策树
- en: The algorithm to build a Decision Tree is also recursive. Let’s walk through
    what happens at a high level. The algorithm starts with the root node, determines
    the proper rule for that node, and then calls itself on the left and right branches.
    The call to the left branch will start again as if the left branch is the root
    node. This will continue until a stopping condition is met.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的算法也是递归的。让我们从一个高层次上了解发生了什么。算法从根节点开始，确定该节点的适当规则，然后对左右分支递归调用。对左分支的调用将重新开始，就好像左分支是根节点一样。这一过程会一直持续，直到满足停止条件为止。
- en: For a Decision Tree, the stopping condition is a leaf node (we’ll discuss how
    a Decision Tree knows whether to create a leaf node next). Once a leaf node is
    created, the recursion terminates, and the algorithm returns to that leaf’s parent
    node and calls itself on the right branch. The algorithm then starts again as
    if the right branch were the root node. Once both recursive calls terminate, and
    a node’s left and right subtrees are created, the algorithm returns to that node’s
    parent, and so on and so forth until the entire tree is constructed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树来说，停止条件是叶节点（我们将接下来讨论决策树如何知道何时创建叶节点）。一旦创建了叶节点，递归终止，算法返回到该叶节点的父节点，并对右分支递归调用。然后算法重新开始，就好像右分支是根节点一样。一旦两个递归调用终止，并且节点的左右子树创建完毕，算法会返回到该节点的父节点，依此类推，直到整个树构建完成。
- en: 'Now to get a little more specific. How is the training data used to build the
    tree? When the root node is defined, all the training data is present—say, all
    *n* samples. This is the set of samples used to pick the rule the root node implements.
    Once that rule has been selected and applied to the training samples, we have
    two new sets of samples: one for the left side (the true side) and one for the
    right side (the false side).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来稍微具体一些。训练数据是如何被用来构建树的呢？当根节点被定义时，所有的训练数据都会被使用——假设是所有的*n*个样本。这些样本集将用于选择根节点实现的规则。一旦规则被选择并应用到训练样本中，我们就得到了两个新的样本集：一个是左侧的（即为真侧），另一个是右侧的（即为假侧）。
- en: The recursion then works with these nodes, using their respective set of training
    samples, to define the rule for the left and right branches. Every time a branch
    node is created, the training set for that branch node gets split into samples
    that meet the rule and samples that don’t meet the rule. A leaf node is declared
    when the set of training samples is either too small, of a sufficiently high proportion
    of one class, or the maximum tree depth has been reached.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，递归将在这些节点上工作，利用各自的训练样本集来定义左右分支的规则。每当一个分支节点被创建时，该分支节点的训练集会被分成满足规则的样本和不满足规则的样本。当训练样本集过小、某一类别的比例足够高，或达到最大树深度时，叶节点就会被声明。
- en: By now you’re probably wondering, “How do we select the rule for a branch node?”
    The rule relates a single input feature, like the petal length, to a particular
    value. The Decision Tree is a *greedy* algorithm; this means that at every node
    it selects the best rule for the current set of information available to it. In
    this case, this is the current set of training samples that are available to the
    node. The best rule is the one that best separates the classes into two groups.
    This implies that we need a way to select possible candidate rules and that we
    have a way to determine that a candidate rule is “best.” The Decision Tree algorithm
    uses brute force to locate candidate rules. It runs through all possible combinations
    of features and values, making continuous values discrete by binning, and evaluates
    the purity of the left and right training sets after the rule is applied. The
    best-performing rule is the one kept at that node.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能在想，“我们如何为分支节点选择规则？”这个规则将单一的输入特征（比如花瓣长度）与一个特定的值关联。决策树是一个*贪心*算法；这意味着在每个节点，它会选择一个最适合当前可用信息的规则。在这种情况下，当前可用的信息集是该节点所拥有的训练样本集。最好的规则是能够最好地将类别分为两组的规则。这意味着我们需要一种方法来选择可能的候选规则，并且有一种方法来确定候选规则是否是“最佳的”。决策树算法使用暴力搜索来找到候选规则。它遍历所有可能的特征和值的组合，通过分箱将连续值离散化，并在应用规则后评估左右训练集的纯度。表现最好的规则会被保留在该节点。
- en: “Best performing" is determined by the *purity* of the split into left and right
    training sample subsets. One way to measure purity is to use the *Gini index*.
    This is the metric sklearn uses. The Gini index of each node in the iris example
    of [Figure 6-7](ch06.xhtml#ch6fig7) is shown. It’s calculated as
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: “表现最好”是通过划分后的左右训练样本子集的*纯度*来决定的。衡量纯度的一种方法是使用*基尼指数*。这是sklearn使用的度量标准。每个节点在[图6-7](ch06.xhtml#ch6fig7)中的基尼指数都会被计算出来。其计算公式如下：
- en: '![image](Images/122equ01.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/122equ01.jpg)'
- en: where *P*(*y*[*i*]) is the fraction of training examples in the subset for the
    current node that are of class *i*. A perfect split between classes, all of one
    class and none of the other, will result in a Gini index of 0\. A 50-50 split
    has a Gini index of 0.5\. The algorithm seeks to minimize the Gini index at each
    node by selecting the candidate rule that results in the smallest Gini index.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*P*(*y*[*i*])是当前节点子集内属于类别*i*的训练样本的比例。类别之间的完美划分，即一类全为一种类别，另一类全为另一类别，将导致基尼指数为0。50-50的划分则基尼指数为0.5。该算法通过选择能够使基尼指数最小化的候选规则，在每个节点上尽量减少基尼指数。
- en: For example, in [Figure 6-7](ch06.xhtml#ch6fig7) the right-hand node below the
    root has a Gini index of 0.5\. This means that the rule for *the node above*,
    the root, will result in a subset of the training data that has petal length >
    2.45, and that subset will be evenly divided between classes 1 and 2\. This is
    the meaning of the “value” line in the node text. It shows the number of training
    samples in the subset that defined the node. The “class” line is the class that
    would be assigned if the tree were stopped at that node. It’s simply the class
    label of the class that has the largest number of training samples in the node’s
    subset. When the tree is used on new, unknown samples, it’s run from root to a
    leaf, always.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图6-7](ch06.xhtml#ch6fig7)中，根节点下方的右侧节点的基尼指数为0.5。这意味着*上面的节点*，即根节点，所定义的规则会导致一个训练数据的子集，其中花瓣长度>2.45，而这个子集将会在类别1和类别2之间均匀分配。这就是节点文本中“value”行的含义。它显示了定义该节点的子集中的训练样本数量。“class”行则是如果树在该节点停止时所分配的类别。它只是该节点子集中训练样本最多的类别标签。当决策树应用于新的未知样本时，它始终是从根节点一直运行到叶子节点。
- en: Random Forests
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: Decision Trees are useful when the data is discrete or categorical or has missing
    values. Continuous data needs to be binned first (sklearn does this for you).
    Decision Trees have a bad habit, however, of *overfitting* the training data.
    This means that they are likely to learn meaningless statistical nuances of the
    training data that you happened to use, instead of learning meaningful general
    features of the data that are useful when applied to unknown data samples. Decision
    Trees also grow very large, as the number of features grows, unless managed by
    depth parameters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在数据是离散的、分类的或有缺失值时非常有用。连续数据需要先进行分箱（sklearn会为你处理这一过程）。然而，决策树有一个坏习惯，就是*过拟合*训练数据。这意味着它们可能会学习到训练数据中的一些无意义的统计特征，而不是学习到数据中有用的、在处理未知数据样本时能够应用的有意义的通用特征。决策树也会变得非常庞大，特别是在特征数量增加的情况下，除非通过深度参数来加以管理。
- en: 'Decision Tree overfitting can be mitigated by using *Random Forests*. In fact,
    unless your problem is simple, you probably want to look at using a Random Forest
    from the start. The following three concepts lead from a Decision Tree to a Random
    Forest:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用*随机森林*，可以缓解决策树的过拟合问题。事实上，除非你的问题很简单，否则你可能希望从一开始就使用随机森林。以下三个概念将决策树引导到随机森林：
- en: Ensembles of classifiers and voting between them
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器集成及其投票
- en: Resampling of the training set by selecting samples *with replacement*
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择带有*放回*的样本对训练集进行重采样
- en: Selection of random feature subsets
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机特征子集的选择
- en: If we have a set of classifiers, each trained on different data or of a different
    type, like a *k*-NN and a Naïve Bayes, we can use their outputs to vote on the
    actual category to assign to any particular unknown sample. This is called an
    *ensemble* and, with diminishing returns as the number of classifiers increases,
    it will, in general, improve the performance over that of any individual classifier.
    We can employ a similar idea and imagine an ensemble, or *forest*, of Decision
    Trees, but unless we do something more with the training set, we’ll have a forest
    of *exactly* the same tree because a particular set of training examples will
    always lead to the exact same Decision Tree. The algorithm to create a Decision
    Tree is deterministic—it always returns the same results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一组分类器，每个分类器都基于不同的数据或不同类型的模型进行训练，比如一个*k*-NN和一个朴素贝叶斯，我们可以利用它们的输出对任何特定的未知样本进行投票，从而决定实际的分类。这就是所谓的*集成*，尽管随着分类器数量的增加，收益会递减，但通常，它会提高整体性能，超过任何单个分类器的表现。我们也可以应用类似的思想，想象一个由决策树组成的*森林*，但除非我们对训练集进行更多处理，否则我们将得到一片*完全相同的*树，因为特定的训练样本集总是会产生完全相同的决策树。创建决策树的算法是确定性的——它总是返回相同的结果。
- en: A way to deal with the particular statistical nuances of the training set you
    have to work with is to select a new training set from the original training set
    but allow the same training set sample to be selected more than once. This is
    selection with replacement. Think of it as choosing colored marbles from a bag,
    but before you select the next marble, put the one you just selected back in the
    bag so you might pick it again. A new dataset selected in this way is known as
    a *bootstrap sample*. Building a collection of new datasets in this way is known
    as *bagging*, and it is models built from this collection of resampled datasets
    that build the Random Forest.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 处理训练集中特定统计细节的一种方法是从原始训练集中选择一个新的训练集，但允许同一训练集样本被选择多次。这就是**带替换的选择**。可以把它想象成从袋子里挑选彩色弹珠，在选择下一个弹珠之前，把刚才选中的弹珠放回袋子中，这样你可能会再次选中它。通过这种方式选出的新数据集被称为*自助采样*。以这种方式构建多个新数据集的过程称为*集成法（bagging）*，由这些重新采样的数据集构建的模型就形成了随机森林（Random
    Forest）。
- en: If we train multiple trees, each with a resampled training set with replacement,
    we’ll get a forest of trees, each one slightly different from the others. This
    alone, along with ensemble voting, will probably improve things. However, there’s
    one issue. If some of the features are highly predictive, they will dominate,
    and the resulting forest of trees will be very similar to one another and therefore
    suffer from very similar weaknesses. This is where the *random* of *Random Forest*
    comes into play.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练多个树，每棵树都使用带替换的重新采样训练集，我们将得到一片树林，每棵树略有不同。仅凭这一点，再加上集成投票，可能就能改善性能。然而，这里有一个问题。如果某些特征高度预测，那么它们将主导结果，导致生成的树林中的树木非常相似，从而面临相似的弱点。这就是*随机*（Random）森林中“随机”的作用。
- en: Instead of just bagging, which changes the distribution of samples in the per
    tree training set but not the set of features examined, what if we also randomly
    selected, for each tree in the forest, a subset of the *features* themselves and
    trained on only those features? Doing this would break the correlation between
    the trees and increase the overall robustness of the forest. In practice, if there
    are *n* features per feature vector, each tree will randomly select ![Image](Images/nsqt.jpg)
    of them over which to build the tree. Random Forests are supported in sklearn
    as well.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅进行集成法不同，集成法改变了每棵树的训练集中样本的分布，但没有改变检查的特征集。如果我们为森林中的每棵树随机选择一个特征子集，并仅在这些特征上进行训练，结果会怎样？这样做可以打破树与树之间的相关性，从而提高森林的整体鲁棒性。实际上，如果每个特征向量有*n*个特征，那么每棵树会随机选择![Image](Images/nsqt.jpg)个特征来构建树。Random
    Forest在sklearn中也有支持。
- en: Support Vector Machines
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Our final classical machine learning model is the one that held neural networks
    at bay for most of the 1990s, the *Support Vector Machine (SVM)*. If the neural
    network is a highly data-driven, empirical approach to generating a model, the
    SVM is a highly elegant, mathematically founded, approach. We’ll discuss the performance
    of an SVM at a conceptual level as the mathematics involved is beyond what we
    want to introduce here. If you’re so inclined, the classic reference is “Support-Vector
    Networks” by Cortes and Vapnik (1995).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终经典机器学习模型是那个在1990年代大部分时间里与神经网络抗衡的模型——*支持向量机（SVM）*。如果神经网络是一种高度数据驱动的经验方法来生成模型，那么SVM是一种高度优雅、基于数学的方式。我们将从概念层面讨论SVM的性能，因为涉及的数学超出了我们在这里想要介绍的范围。如果你有兴趣，经典的参考文献是Cortes和Vapnik（1995年）的《Support-Vector
    Networks》。
- en: We can summarize what a Support Vector Machine is doing by gaining intuition
    about the concepts of margins, support vectors, optimization, and kernels. Let’s
    look at each concept in turn.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过直观理解边距、支持向量、优化和核函数的概念来总结支持向量机的工作原理。接下来我们依次来看每个概念。
- en: Margins
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 边距
- en: '[Figure 6-8](ch06.xhtml#ch6fig8) shows a two-class dataset with two features.
    We’ve plotted each sample in the dataset with feature 1 along the x-axis and feature
    2 along the y-axis. Class 0 is shown as circles, class 1 as diamonds. This is
    obviously a contrived dataset, one that’s easily separated by plotting a line
    between the circles and the diamonds.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](ch06.xhtml#ch6fig8)展示了一个具有两个特征的二分类数据集。我们已将数据集中的每个样本绘制出来，特征1沿x轴，特征2沿y轴。类别0用圆圈表示，类别1用菱形表示。这显然是一个人为构造的数据集，通过在圆圈和菱形之间绘制一条线就能轻松分开。'
- en: '![image](Images/06fig08.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig08.jpg)'
- en: '*Figure 6-8: A toy dataset with two classes, circles and diamonds, and two
    features, x-axis and y-axis*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-8：一个包含两类、圆圈和菱形的玩具数据集，以及两个特征，x轴和y轴*'
- en: A classifier can be thought of as locating one or more *planes* that split the
    space of the training data into homogeneous groups. In the case of [Figure 6-8](ch06.xhtml#ch6fig8)
    the separating “plane” is a line. If we had three features, the separating plane
    would be a 2D plane. With four features, the separating plane would be three-dimensional,
    and for *n* dimensions the separating plane is *n –* 1 dimensional. Since the
    plane is multidimensional, we refer to it as a *hyperplane* and say that the goal
    of the classifier is to separate the training feature space into groups using
    hyperplanes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器可以被看作是定位一个或多个 *平面*，将训练数据空间分成同类群组。在 [图 6-8](ch06.xhtml#ch6fig8) 的情况下，分隔“平面”是一条线。如果我们有三个特征，分隔平面将是一个二维平面。有四个特征，分隔平面将是三维的，对于
    *n* 维度，分隔平面是 *n –* 1 维度的。由于平面是多维的，我们称之为 *超平面*，并且说分类器的目标是使用超平面将训练特征空间分成群组。
- en: If we look again at [Figure 6-8](ch06.xhtml#ch6fig8), we can imagine an infinite
    set of lines that separate the training data into two groups, with all of class
    0 on one side and all of class 1 on the other. Which one do we want to use? Well,
    let’s think a bit about what the position of a line separating the two classes
    implies. If we draw a line more to the right side, just before any of the diamonds,
    we’ll have separated the training data, but only barely so. Recall, we’re using
    the training data as a surrogate for the true distribution of samples of each
    class. The more training data we have, the more faithfully we’ll know that true
    distribution. However, we don’t really know it.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次看一下 [图 6-8](ch06.xhtml#ch6fig8)，我们可以想象无限多条线将训练数据分为两组，所有类别 0 的在一边，所有类别
    1 的在另一边。我们应该选择哪一条？好吧，让我们稍微思考一下，一条分隔两类的线的位置意味着什么。如果我们将一条线画得更靠右一些，就在任何菱形的右边，我们将分隔训练数据，但仅仅是勉强分隔开来。记住，我们使用训练数据作为每个类别样本的真实分布的替代品。我们拥有的训练数据越多，我们就越能忠实地了解真实分布。然而，我们实际上并不了解它。
- en: A new, unknown sample, which must be of class 0 or class 1, will fall somewhere
    on the graph. It’s reasonable to believe that there are class 1 (diamond) samples
    in the wild that will fall even closer to the circles than any of the samples
    in the training set. If the separating line is too close to the diamonds, we run
    the risk of calling valid class 1 samples *class 0* because the separating line
    is too far to the right. We can make a similar claim if we place the separating
    line very close to the class 0 points (circles). Then we run the risk of mislabeling
    class 0 samples as *class 1* (diamonds).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的未知样本，必须是类别 0 或类别 1，将会落在图上的某个位置。可以合理地认为，在野外可能存在类别 1（菱形）的样本，它们将比训练集中的任何样本更接近圆圈。如果分隔线太靠近菱形，我们面临将有效的类别
    1 样本误判为 *类别 0* 的风险，因为分隔线太靠右。如果我们将分隔线放置得非常接近类别 0 点（圆圈），那么我们面临将类别 0 样本误判为 *类别 1*（菱形）的风险。
- en: Therefore, in the absence of more training data, it seems most reasonable to
    choose the separating line that is as far from both classes as possible. This
    is the line that is farthest from the rightmost circles while still being as far
    to the left of the diamonds as possible. This line is the maximal margin location,
    where the *margin* is defined as the distance from the closest sample points.
    [Figure 6-9](ch06.xhtml#ch6fig9) shows the maximal margin location as the heavy
    line with the maximum margin indicated by the two dotted lines. The goal of an
    SVM is to locate the maximum margin position, as this is the location where we
    can be most certain to not misclassify new samples, given the knowledge gained
    from the training set.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在没有更多训练数据的情况下，选择尽可能远离两类的分隔线似乎是最合理的选择。这条线是离最右侧的圆圈最远，并且仍然尽可能靠近菱形的线。这条线是最大间隔位置，其中
    *间隔* 被定义为最接近样本点的距离。 [图 6-9](ch06.xhtml#ch6fig9) 显示了最大间隔位置，以粗线表示，并由两条虚线表示最大间隔。支持向量机的目标是找到最大间隔位置，因为这是我们可以最确保不会错误分类新样本的位置，根据从训练集中获取的知识。
- en: '![image](Images/06fig09.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/06fig09.jpg)'
- en: '*Figure 6-9: The toy dataset of [Figure 6-8](ch06.xhtml#ch6fig8) with the maximal
    margin separating line (heavy) and the maximum margins (dotted)*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-9：[图 6-8](ch06.xhtml#ch6fig8) 的玩具数据集，带有最大间隔分隔线（粗线）和最大间隔（虚线）*'
- en: Support Vectors
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 支持向量
- en: Look again at [Figure 6-9](ch06.xhtml#ch6fig9). Notice the four training data
    points on the margin? These are the training samples that define the margin, or,
    in other words, support the margin; hence they are *support vectors*. This is
    the origin of the name *Support Vector Machine*. The support vectors define the
    margin, but how can we use them to locate the margin position? Here is where we’ll
    simplify things a bit to avoid a large amount of complex vector mathematics that
    will only muddy the waters for us. For a more mathematical treatment, see “A Tutorial
    on Support Vector Machines for Pattern Recognition” by Christopher Burges (1998).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看 [图 6-9](ch06.xhtml#ch6fig9)。注意到边界上的四个训练数据点了吗？这些训练样本定义了边界，换句话说，支持了边界；因此它们被称为
    *支持向量*。这就是 *支持向量机* 这一名称的由来。支持向量定义了边界，但我们如何利用它们来确定边界的位置呢？在这里，我们将简化一些内容，以避免涉及大量复杂的向量数学，这只会让我们更加困惑。如果需要更数学化的处理，请参考
    Christopher Burges 的《用于模式识别的支持向量机教程》（1998）。
- en: Optimization
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化
- en: Mathematically, we can find the maximum margin hyperplane by solving an optimization
    problem. Recall that in an optimization problem, we have a quantity that depends
    on certain parameters, and we want to find the set of parameter values that makes
    the quantity as small or as large as possible.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，我们可以通过求解优化问题来找到最大间隔超平面。回想一下，在优化问题中，我们有一个依赖于某些参数的量，我们想要找到一组参数值，使得该量尽可能小或尽可能大。
- en: In the SVM case, the orientation of the hyperplane can be specified by a vector,
    ![Image](Images/wbar.jpg). There is also an offset, *b*, which we must find. Finally,
    before we can do the optimization, we need to change the way we specify the class
    labels. Instead of using 0 or 1 for *y*[*i*], the label of the *i*-th training
    sample, *x*[*i*], we’ll use –1 or +1\. This will let us define the condition of
    the optimization problem more simply.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SVM 的情况下，超平面的方向可以通过一个向量来指定，![Image](Images/wbar.jpg)。还有一个偏移量 *b*，我们需要找到它。最后，在进行优化之前，我们需要改变指定类别标签的方式。我们不再使用
    0 或 1 来表示 *y*[*i*]，即第 *i* 个训练样本 *x*[*i*] 的标签，而是使用 -1 或 +1。这将使我们能够更简单地定义优化问题的条件。
- en: 'So, mathematically, what we want is to find ![Image](Images/wbar.jpg) and *b*
    so that the quantity ![Image](Images/126equ01.jpg) is as small as possible, given
    that ![Image](Images/126equ02.jpg) for all *y*[*i*] labels and *x*[*i*] training
    vectors in the dataset. This sort of optimization problem is readily solved via
    a technique called *quadratic programming*. (We’re ignoring another important
    mathematical step here: the actual optimization problem solved uses a Lagrangian
    to solve the dual form, but again, we’ll try to avoid muddying the waters too
    much.)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，我们想要的是找到 ![Image](Images/wbar.jpg) 和 *b*，使得给定条件下，![Image](Images/126equ01.jpg)
    的数量尽可能小，前提是对于数据集中的所有 *y*[*i*] 标签和 *x*[*i*] 训练向量都有![Image](Images/126equ02.jpg)。这种优化问题可以通过一种称为
    *二次规划* 的技术轻松解决。（我们在这里忽略了另一个重要的数学步骤：实际的优化问题是通过拉格朗日方法求解其对偶形式，但我们将尽量避免使事情复杂化。）
- en: The preceding formulation is for a case where the dataset, assumed to have only
    two classes, can be separated by a hyperplane. This is the linearly separable
    case. In reality, as we well appreciate by now, not every dataset can be separated
    this way. So, the full form of the optimization problem includes a fudge factor,
    *C*, which affects the size of the margin found. This factor shows up in the sklearn
    SVM class and needs to be specified to some level. From a practical point of view,
    *C* is a *hyperparameter* of the SVM, a value that we need to set to get the SVM
    to train properly. The right value of *C* is problem dependent. In general, any
    parameter of a model that is not learned by the model but must be set to use the
    model, like *C* for an SVM or *k* for *k*-NN, is a hyperparameter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式适用于数据集假设只有两类且可以通过超平面分离的情况。这是线性可分的情况。实际上，正如我们现在已经深刻理解的那样，并非所有数据集都能通过这种方式分离。因此，优化问题的完整形式包括一个调整因子
    *C*，它影响找到的间隔的大小。这个因子出现在 sklearn 的 SVM 类中，需要设置一个合适的值。从实践角度来看，*C* 是 SVM 的一个 *超参数*，是我们需要设置的值，以便让
    SVM 正常训练。*C* 的最佳值依赖于具体问题。通常来说，模型的任何一个不是由模型学习的、而必须手动设置以使用模型的参数（比如 SVM 的 *C* 或 *k*-NN
    的 *k*）都是超参数。
- en: Kernels
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核函数
- en: There’s one more mathematical concept we need to introduce, with suitable hand
    waving. The preceding description is for a linear SVM and uses the training data
    directly (the ![Image](Images/127equ01.jpg)). The nonlinear case maps the training
    data to another space by passing it through a function, typically called ![Image](Images/127equ02.jpg),
    that produces a new version of the training data vector, ![Image](Images/xbar1.jpg).
    The SVM algorithm uses inner products, ![Image](Images/127equ03.jpg), which means
    that the mapped version will use ![Image](Images/127equ04.jpg). In this notation,
    vectors are thought of as a column of numbers so that *T*, the transpose, produces
    a row vector. Then normal matrix multiplication of a 1 × *n* row vector and an
    *n* × 1 column vector will result in a 1 × 1 output, which is a scalar. The inner
    product is typically written as
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个数学概念需要介绍，带有适当的手势。前述描述是针对线性SVM，并直接使用训练数据（![Image](Images/127equ01.jpg)）。非线性情况通过将训练数据传递给一个函数来将其映射到另一个空间，通常这个函数叫做![Image](Images/127equ02.jpg)，它会生成训练数据向量的新版本，![Image](Images/xbar1.jpg)。SVM算法使用内积，![Image](Images/127equ03.jpg)，这意味着映射后的版本将使用![Image](Images/127equ04.jpg)。在这种表示法中，向量被视为一列数字，因此*T*，即转置，会生成一个行向量。然后，1
    × *n*的行向量与*n* × 1的列向量进行常规矩阵乘法，将得到一个1 × 1的输出，这是一个标量。内积通常写作
- en: '![image](Images/127equ05.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/127equ05.jpg)'
- en: and the function ![Image](Images/127equ06.jpg) is called a *kernel*. The linear
    kernel is simply ![Image](Images/127equ07.jpg), but other kernels are possible.
    The *Gaussian kernel* is a popular one, also known as a *radial basis function
    (RBF)* kernel. In practical use, this kernel introduces a new parameter, apart
    from *C*, which is *γ*. This parameter relates to how spread out the Gaussian
    kernel is around a particular training point, with smaller values extending the
    range of influence of the training sample. Typically, one uses a grid search over
    *C* and, if using the RBF kernel, *γ*, to locate the best performing model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个函数 ![Image](Images/127equ06.jpg) 被称为*核*。线性核函数就是简单的![Image](Images/127equ07.jpg)，但也可以使用其他核函数。*高斯核*是一个常用的核函数，也叫做*径向基函数（RBF）*核。在实际使用中，这个核函数引入了一个新的参数，除了*C*之外，就是*γ*。这个参数决定了高斯核在特定训练点周围的扩展范围，较小的值会扩展训练样本的影响范围。通常，使用网格搜索来调整*C*，如果使用RBF核，也会调整*γ*，以找到性能最佳的模型。
- en: To summarize, then, a Support Vector Machine uses the training data, mapped
    through a kernel function, to optimize the orientation and location of a hyperplane
    that produces the maximum margin between the hyperplane and the support vectors
    of the training data. The user needs to select the kernel function and associated
    parameters like *C* and *γ* so that the model best fits the training data.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，支持向量机通过将训练数据通过核函数映射，来优化超平面的方向和位置，从而在超平面和训练数据的支持向量之间产生最大边界。用户需要选择核函数以及相关参数，如*C*和*γ*，以使模型最适应训练数据。
- en: Support Vector Machines dominated machine learning in the 1990s and early 2000s,
    before the advent of deep learning. This is because they’re trained efficiently
    and don’t need extensive computational resources to be successful. Since the arrival
    of deep learning, however, SVMs have fallen somewhat by the wayside because powerful
    computers have enabled neural networks to do what previously was not possible
    with more limited computing resources. Still, SVMs have a place at the table.
    One popular approach uses a large neural network trained on a particular dataset
    as a preprocessor for a different dataset with an SVM trained on the output of
    the neural network (minus the top layers).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机在1990年代和2000年代初期主导了机器学习领域，直到深度学习的兴起。这是因为SVM训练效率高，且不需要大量的计算资源即可成功。 然而，随着深度学习的到来，SVM逐渐被边缘化，因为强大的计算机使得神经网络能够完成以前在计算资源有限的情况下无法实现的任务。然而，SVM仍然有其独特的地位。一种流行的方法是使用在特定数据集上训练的大型神经网络作为另一个数据集的预处理器，然后在神经网络输出（去掉顶层）上训练SVM。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we introduced six of the most common classic machine learning
    models: Nearest Centroid, *k*-NN, Näive Bayes, Decision Tree, Random Forest, and
    SVMs. These models are classic because they have been used for decades. They are
    also still relevant if the conditions they support best are present. At times,
    the classic model is still the correct choice. An experienced machine learning
    practitioner will know when to fall back to the classics.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了六种最常见的经典机器学习模型：最近质心、*k*-NN、朴素贝叶斯、决策树、随机森林和支持向量机（SVM）。这些模型之所以被称为经典，是因为它们已经使用了几十年。如果条件适合它们的最佳支持，它们依然是有用的。有时候，经典模型仍然是正确的选择。一位经验丰富的机器学习实践者会知道何时回归经典模型。
- en: In the next chapter, we’ll use each of these models, via sklearn, to perform
    a number of experiments that will build our intuition of how the models work and
    when to use them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过sklearn使用每个模型，进行一系列实验，帮助我们理解这些模型是如何工作的，以及何时使用它们。
