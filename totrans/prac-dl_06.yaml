- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: CLASSICAL MACHINE LEARNING**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: It’s satisfying to be able to write “Classical Machine Learning” as it implies
    that there is something newer that makes older techniques “classical.” Of course,
    we know by now that there is—deep learning—and we’ll get to it in the chapters
    that follow. But first, we need to build our intuition by examining older techniques
    that will help cement concepts for us and, frankly, because the older techniques
    are still useful when the situation warrants.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 能够写下“经典机器学习”这一术语让人感到满意，因为它暗示着有一些更新的技术，使得旧的技术成为了“经典”。当然，我们现在知道，确实有更先进的——深度学习——我们将在后续的章节中讨论它。但首先，我们需要通过研究一些旧的技术来建立直觉，这些技术将帮助我们巩固概念，坦白说，因为这些旧技术在特定情况下仍然非常有用。
- en: It’s tempting to include some sort of history here. To keep to the practical
    nature of this book, we won’t, but a full history of machine learning is needed,
    and as of this writing, I have not found one. Historians reading this, please
    take note. I will say that machine learning is not new; the techniques of this
    chapter go back decades and have had considerable success on their own.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想在这里加入一些历史背景。为了保持本书的实用性，我们不会这样做，但机器学习的完整历史是必要的，截至目前，我还没有找到这样的资料。历史学家们请注意，我可以说，机器学习并不新鲜，本章所涉及的技术已经有几十年的历史，并且取得了相当大的成功。
- en: However, the successes were always limited in a way that deep learning has now
    largely overcome. Still, owning a hammer doesn’t make everything a nail. You will
    encounter problems that are well suited to these older techniques. This might
    be because there’s too little data available to train a deep model, because the
    problem is simple and easily solved by a classical technique, or because the operating
    environment is not conducive to a large, deep model (think microcontroller). Besides,
    many of these techniques are easier to understand, conceptually, than a deep model
    is, and all the comments of earlier chapters about building datasets, as well
    as the comments in [Chapter 11](ch11.xhtml#ch11) about evaluating models, still
    apply.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些成功总是受到某种限制，而这些限制深度学习现在基本上已经克服了。尽管如此，拥有一把锤子并不意味着一切都是钉子。你将会遇到一些非常适合使用这些老技术的问题。这可能是因为可用的数据太少，无法训练深度模型，可能是因为问题本身简单，通过经典技术就能轻松解决，或者是因为操作环境不适合使用大型深度模型（比如微控制器）。此外，许多这些技术在概念上比深度模型更容易理解，之前章节中关于构建数据集的所有评论，以及[第
    11 章](ch11.xhtml#ch11)中关于评估模型的评论，依然适用。
- en: The following sections will introduce several popular classical models, not
    in great detail, but in essence. All of these models are supported by sklearn.
    In [Chapter 7](ch07.xhtml#ch07), we’ll apply the models to some of the datasets
    we developed in [Chapter 5](ch05.xhtml#ch05). This will give us an idea of the
    relative performance of the models when compared to each other as well as giving
    us a baseline for comparing the performance of deep models in subsequent chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将介绍几种流行的经典模型，虽然不会详细展开，但会讲解其核心内容。这些模型都得到了 sklearn 的支持。在[第 7 章](ch07.xhtml#ch07)中，我们将把这些模型应用到我们在[第
    5 章](ch05.xhtml#ch05)中开发的一些数据集上。这将帮助我们了解这些模型相互比较时的相对表现，并为后续章节中比较深度模型的表现提供基准。
- en: We’ll examine six classical models. The order in which we discuss them roughly
    tracks with the complexity of the type of model. The first three, Nearest Centroid,
    *k*-Nearest Neighbors, and Naïve Bayes, are quite simple to understand and implement.
    The last three, Decision Trees, Random Forests, and Support Vector Machines, are
    harder, but we’ll do our best to explain what’s going on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论六种经典模型。我们讨论的顺序大致是根据模型类型的复杂度来排列的。前三种，最近质心、*k*-最近邻和朴素贝叶斯，理解和实现起来都非常简单。后面三种，决策树、随机森林和支持向量机，较为复杂，但我们会尽力解释它们的原理。
- en: Nearest Centroid
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最近质心
- en: Assume we want to build a classifier and that we have a properly designed dataset
    of *n* classes (see [Chapter 4](ch04.xhtml#ch04)). For simplicity, we’ll assume
    that we have *m* samples of each of the *n* classes. This isn’t necessary but
    saves us from adding many subscripts to things. Since our dataset is properly
    designed, we have training samples and test samples. We don’t need validation
    samples in this case, so we can throw them into the training set. Our goal is
    to have a model that uses the training set to learn so we can apply the model
    to the test set to see how it will do with new, unknown samples. Here the sample
    is a feature vector of floating-point values.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想构建一个分类器，并且我们有一个正确设计的*n*类数据集（参见[第4章](ch04.xhtml#ch04)）。为简便起见，我们假设每个*n*类都有*m*个样本。这并不是必要的，但可以避免在公式中添加许多下标。由于我们的数据集设计得当，我们有训练样本和测试样本。在这种情况下，我们不需要验证样本，因此可以将它们放入训练集。我们的目标是拥有一个模型，利用训练集进行学习，然后将模型应用于测试集，以观察它如何处理新的、未知的样本。在这里，样本是由浮动点值组成的特征向量。
- en: 'The goal of selecting components for a feature vector is to end up with a feature
    vector that makes the different classes distinct in the feature space. Let’s say
    that the feature vector has *w* features. This means we can think of the feature
    vector as the coordinates of a point in a *w*-dimensional space. If *w* = 2 or
    *w* = 3, we can graph the feature vectors. However, mathematically, there’s no
    reason for us to restrict *w* to 2 or 3; all of what we describe here works in
    100, 500, or 1000 dimensions. Note it won’t work equally well: the dreaded curse
    of dimensionality will creep in and eventually require an exponentially large
    training dataset, but we’ll ignore this elephant in the room for now.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 选择特征向量组件的目标是使得特征向量在特征空间中能够区分不同的类。假设特征向量有*w*个特征。这意味着我们可以将特征向量视为*w*维空间中一个点的坐标。如果*w*
    = 2或*w* = 3，我们可以绘制特征向量的图形。然而，从数学角度来看，我们没有必要将*w*限制为2或3；我们在这里描述的所有内容同样适用于100、500或1000维。值得注意的是，这种方法并不会在所有情况下都有效：可怕的维度灾难将悄然出现，并最终要求一个指数级大的训练数据集，但我们暂时不讨论这个问题。
- en: If the features are well chosen, we might expect a plot of the points in the
    *w*-dimensional space to group the classes so that all of the samples from class
    0 are near each other, and all of the samples from class 1 are near each other
    but distinct from class 0, and so forth. If this is our expectation, then how
    might we use this knowledge to assign a new, unknown sample to a particular class?
    Of course, this is the goal of classification, but in this case, given our assumption
    that the classes are well separated in the feature space, what is something simple
    we could do?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征选择得当，我们可能期望在*w*维空间中绘制的点图能够将各个类别分组，使得类别0的所有样本彼此接近，类别1的所有样本彼此接近并与类别0不同，依此类推。如果这是我们的期望，那么我们该如何利用这些信息将一个新的、未知的样本分配到特定的类中呢？当然，这是分类的目标，但在这种情况下，考虑到我们假设类别在特征空间中是分开的，我们能做的简单方法是什么？
- en: '[Figure 6-1](ch06.xhtml#ch6fig1) shows a hypothetical 2D feature space with
    four distinct classes. The different classes are clearly separated in this toy
    example. A new, unknown feature vector will fall into this space as a point. The
    goal is to assign a class label to the new point, either square, star, circle,
    or triangle.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-1](ch06.xhtml#ch6fig1)展示了一个假设的二维特征空间，其中有四个不同的类。在这个简单的例子中，不同的类在空间中被清晰地分开了。一个新的、未知的特征向量将作为一个点落入这个空间。目标是为这个新点分配一个类别标签，可以是方形、星形、圆形或三角形。'
- en: '![image](Images/06fig01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig01.jpg)'
- en: '*Figure 6-1: A hypothetical 2D feature space with four distinct classes*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-1：一个假设的二维特征空间，包含四个不同的类*'
- en: Since the points of [Figure 6-1](ch06.xhtml#ch6fig1) are so well grouped, we
    might think that we could represent each group by an average position in the feature
    space. Instead of the 10 square points, we’d use a single point to represent the
    squares. This seems an entirely reasonable thing to do.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[图6-1](ch06.xhtml#ch6fig1)中的点分布得非常好，我们可能会认为，可以通过在特征空间中找到每个组的平均位置来表示每个组。我们不再使用10个方形的点，而是用一个点来代表所有的方形。这似乎是一个完全合理的做法。
- en: 'It turns out, the average point of a group of points has a name: the *centroid*,
    the center point. We know how to compute the average of a set of numbers: add
    them up and divide by how many we added. To find the centroid of a set of points
    in 2D space, we first find the average of all the x-axis coordinates and then
    the average of all the y-axis coordinates. If we have three dimensions, we’ll
    do this for the x-, y-, and z-axes. If we have *w* dimensions, we’ll do it for
    each of the dimensions. In the end, we’ll have a single point that we can use
    to represent the entire group. If we do this for our toy example, we get [Figure
    6-2](ch06.xhtml#ch6fig2), where the centroid is shown as the large marker.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，一组点的平均点有一个名字：*质心*，即中心点。我们知道如何计算一组数字的平均值：将它们加起来，然后除以我们加的个数。为了找到二维空间中一组点的质心，我们首先计算所有
    x 轴坐标的平均值，然后计算所有 y 轴坐标的平均值。如果我们有三维空间，我们将对 x、y 和 z 轴分别进行计算。如果我们有 *w* 维空间，我们将对每个维度进行计算。最后，我们会得到一个点，用来表示整个组。如果我们为我们的玩具示例进行此操作，我们得到
    [图 6-2](ch06.xhtml#ch6fig2)，其中质心显示为大标记。
- en: '![image](Images/06fig02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig02.jpg)'
- en: '*Figure 6-2: A hypothetical 2D feature space with four distinct classes and
    their centroids*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-2：一个假设的二维特征空间，包含四个不同的类别及其质心*'
- en: 'How is the centroid helpful to us? Well, if a new, unknown sample is given
    to us, it will be a point in the feature space as mentioned previously. We can
    then measure the distance between this point and each of the centroids and assign
    the class label of the closest centroid. The idea of *distance* is somewhat ambiguous;
    there are many different ways to define distance. One obvious way is to draw straight
    line between the two points; this distance is known as the *Euclidean distance*,
    and it’s easy enough to compute. If we have two points, (*x*[0],*y*[0]) and (*x*[1],*y*[1])
    then the Euclidean distance between them is simply the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 质心对我们有什么帮助呢？嗯，如果我们给定一个新的、未知的样本，它将是一个特征空间中的点，如前所述。然后，我们可以测量这个点与每个质心之间的距离，并分配最近质心的类别标签。*距离*的概念有些模糊；定义距离的方式有很多种。一种明显的方式是画一条连接两个点的直线；这种距离被称为
    *欧几里得距离*，并且计算起来非常简单。如果我们有两个点，(*x*[0],*y*[0]) 和 (*x*[1],*y*[1])，那么它们之间的欧几里得距离就是：
- en: '![image](Images/110equ01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/110equ01.jpg)'
- en: If we have three dimensions, the distance between two points becomes
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有三维空间，两个点之间的距离变为
- en: '![image](Images/110equ02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/110equ02.jpg)'
- en: which can be generalized to *w* dimensions for two points, *x*[0] and *x*[1],
    as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以推广到 *w* 维空间，适用于两个点，*x*[0] 和 *x*[1]，表示为
- en: '![image](Images/110equ03.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/110equ03.jpg)'
- en: where ![Image](Images/110equ04.jpg) is the *i*-th component of the point *x*[0].
    This means, component by component, find the difference between the two points,
    square it, and add it to the squared difference of all the other components. Then,
    take the square root.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/110equ04.jpg) 是点 *x*[0] 的 *i* 维分量。这意味着，逐个分量地，找到两个点之间的差值，将其平方，并将所有其他分量的平方差加在一起。然后，取平方根。
- en: '[Figure 6-3](ch06.xhtml#ch6fig3) shows a sample point in the feature space
    as well as the distances to the centroids. The shortest distance is to the circle
    class, so we’d assign the new sample to that class.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-3](ch06.xhtml#ch6fig3) 显示了特征空间中的一个样本点以及到质心的距离。最短的距离是到圆形类别的，因此我们会将新样本分配到该类别。'
- en: '![image](Images/06fig03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig03.jpg)'
- en: '*Figure 6-3: A hypothetical 2D feature space with four distinct classes, their
    centroids, and a new, unknown sample*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-3：一个假设的二维特征空间，包含四个不同的类别及其质心，以及一个新的、未知的样本*'
- en: The process we just implemented is known as a *Nearest Centroid* classifier.
    It’s also sometimes called *template matching*. The centroids of the classes learned
    from the training data are used as a proxy for the class as a whole. Then, new
    samples use those centroids to decide on a label.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚实现的过程被称为 *最近质心* 分类器。有时它也叫做 *模板匹配*。通过训练数据学到的类别的质心被用作整个类别的代表。然后，新的样本使用这些质心来决定标签。
- en: This seems so simple and perhaps even somewhat obvious, so why isn’t this classifier
    used more? Well, there are several reasons. One has already been mentioned, the
    curse of dimensionality. As the number of features increases, the space gets larger
    and larger, and we need exponentially more training data to get a good idea of
    where the centroids should be. So, a large feature space implies that this might
    not be the right approach.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来如此简单，甚至有点显而易见，那么为什么这个分类器没有得到更广泛的使用呢？其实有几个原因。其中一个已经提到过，那就是维度灾难。随着特征数量的增加，空间变得越来越大，我们需要指数级地增加训练数据，才能准确了解质心应该在哪里。因此，特征空间很大意味着这可能不是合适的方法。
- en: 'There’s a more severe problem, however. Our toy example had very tight groups.
    What if the groups are more diffuse, even overlapping? Then the selection of the
    Nearest Centroid becomes problematic: how would we know whether the closest centroid
    represents class A or class B?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一个更严重的问题。我们的玩具示例中，类别非常紧凑。如果组更加分散，甚至相互重叠呢？那么，选择最近质心就变得有问题了：我们如何知道最近的质心代表的是类别A还是类别B呢？
- en: Still more severe is that a particular class might fall into *two* or more distinct
    groups. If we calculate the centroid of only the class as a whole, the centroid
    will be between the groups for the class and not represent either cluster well.
    We’d need to know that the class is split between groups and use multiple centroids
    for the class. If the feature space is small, we can plot it and see that the
    class is divided between groups. However, if the feature space is larger, there’s
    no easy way for us to decide that the class is divided between multiple groups
    and that multiple centroids are required. Still, for elementary problems, this
    approach might be ideal. Not every application deals with difficult data. We might
    be building an automated system that needs to make simple, easy decisions on new
    inputs. In that case, this simple classifier might be a perfect fit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更严重的是，某个特定的类别可能会落入*两个*或更多的不同组。如果我们仅计算类别的整体质心，那么质心将位于该类别的各个组之间，并不能很好地代表任何一个簇。我们需要知道该类别在多个组之间分布，并为该类别使用多个质心。如果特征空间较小，我们可以绘制它并看到该类别在不同组之间分布。然而，如果特征空间较大，我们就没有简单的方法来判断该类别是否分布在多个组之间，进而需要多个质心。尽管如此，对于简单的问题，这种方法可能是理想的选择。并非所有的应用都处理复杂数据。我们可能正在构建一个自动化系统，需要对新输入做出简单、易于决策的判断。在这种情况下，这个简单的分类器可能是完美的选择。
- en: k-Nearest Neighbors
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-最近邻
- en: As we saw earlier, one problem with a centroid approach is that the classes
    might be divided among multiple groups in the feature space. As the number of
    groups increases, so would the number of centroids necessary to specify the class.
    This implies another approach. Instead of computing per class centroids, what
    if we used the training data as is and selected the class label for a new input
    sample by finding the closest member of the training set and using its label?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，质心方法的一个问题是，类别可能在特征空间中被划分为多个组。随着组数的增加，指定类别所需的质心数量也会增加。这意味着另一种方法。我们可以不计算每个类别的质心，而是直接使用训练数据，并通过找到训练集中的最接近样本来为新输入样本选择类别标签，并使用该样本的标签。
- en: This type of classifier is called a *Nearest Neighbor* classifier. If we look
    at only the closest sample in the training set, we are using one neighbor, so
    we call the classifier a *1-Nearest Neighbor* or *1-NN classifier*. But we don’t
    need to look at only the nearest training point. We might want to look at several
    and then vote to assign a new sample the most common class label. In the event
    of a tie, we can select one of the class labels at random. If we use three nearest
    neighbors, we have a 3-NN classifier, and if we use *k* neighbors, we have a *k*-NN
    classifier.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分类器称为*最近邻*分类器。如果我们只查看训练集中的最近样本，那么我们使用的是一个邻居，因此我们称该分类器为*1-最近邻*或*1-NN分类器*。但我们不需要只看最近的训练点。我们可能希望查看多个邻居，然后通过投票来为新样本分配最常见的类别标签。如果出现平局，我们可以随机选择一个类别标签。如果我们使用三个最近邻，我们就有了一个3-NN分类器，如果我们使用*k*个邻居，我们就有了一个*k*-NN分类器。
- en: Let’s revisit the hypothetical dataset of [Figure 6-1](ch06.xhtml#ch6fig1) but
    generate a new version where the tight clusters are more spread out. We still
    have two features and four classes with 10 examples each. Let’s set *k* = 3, a
    typical value. To assign a label to a new sample, we plot the sample in the feature
    space and then find the three closest training data points to it. [Figure 6-4](ch06.xhtml#ch6fig4)
    shows the three nearest neighbors for three unknown samples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视[图 6-1](ch06.xhtml#ch6fig1)中的假设数据集，但生成一个新的版本，其中紧密的簇更加分散。我们仍然有两个特征和四个类别，每个类别有
    10 个示例。设定*k* = 3，这是一个典型的值。为了给一个新样本分配标签，我们将该样本绘制在特征空间中，然后找到与其最接近的三个训练数据点。[图 6-4](ch06.xhtml#ch6fig4)展示了三个未知样本的三个最近邻。
- en: 'The three training data points closest to Sample A are square, square, and
    star. Therefore, by majority vote, we assign Sample A to the class square. Similarly,
    the three closest training data points for Sample B are circle, triangle, and
    triangle. Therefore, we declare Sample B to be of class triangle. Things are more
    interesting with Sample C. In this case, the three closest training samples are
    each from a different class: circle, star, and triangle. So, voting is a tie.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 A 最接近的三个训练数据点是方形、方形和星形。因此，通过多数投票，我们将样本 A 分配给方形类别。同样，样本 B 最接近的三个训练数据点是圆形、三角形和三角形。因此，我们将样本
    B 宣布为三角形类别。样本 C 更有趣。在这种情况下，三个最接近的训练样本分别来自不同的类别：圆形、星形和三角形。所以，投票结果平局。
- en: When this happens, the *k*-NN implementation has to make a choice. The simplest
    thing to do is select the class label at random since one might argue that any
    of the three are equally as likely. Alternatively, one might believe a little
    more strongly in the value of the distance between the unknown sample and the
    training data and select the one with the shortest distance. In this case, we’d
    label Sample C with class star, since that’s the training sample closest to it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种情况发生时，*k*-NN 实现必须做出选择。最简单的方法是随机选择类别标签，因为有人可能会认为这三者的可能性是相等的。或者，也可以稍微更多地依赖于未知样本与训练数据之间的距离，并选择距离最短的那个。在这种情况下，我们将样本
    C 标记为星形类别，因为它是离该样本最近的训练样本。
- en: '![image](Images/06fig04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig04.jpg)'
- en: '*Figure 6-4: Applying *k*-NN for *k* =3 to three unknown samples A, B, and
    C*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-4：将 *k*-NN 应用于 *k* = 3 的三个未知样本 A、B 和 C*'
- en: The beauty of a *k*-NN classifier is that the training data *is* the model—no
    training step is necessary. Of course, the training data must be carried around
    with the model and, depending upon the size of the training set, finding the *k*
    nearest neighbors for a new input sample might be computationally very expensive.
    People have worked for decades to try to speed up the neighbor search or store
    the training data more efficiently, but in the end, the curse of dimensionality
    is still there and still an issue.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN 分类器的优点在于训练数据就是模型——无需训练步骤。当然，训练数据必须与模型一起携带，并且根据训练集的大小，为一个新输入样本找到*k*个最近邻可能在计算上非常昂贵。人们已经花费了几十年的时间尝试加速邻居搜索或更有效地存储训练数据，但最终，维度灾难依然存在，并且仍然是一个问题。'
- en: 'However, some *k*-NN classifiers have performed very well: if the dimensionality
    of the feature space is small enough, *k*-NN might be attractive. There needs
    to be a balance between training data size, which leads to better performance
    but more storage and more laborious searching for neighbors, and the dimensionality
    of the feature space. The same sort of scenario that might make Nearest Centroid
    a good fit will also make *k*-NN a good fit. However, *k*-NN is perhaps more robust
    to diffuse and somewhat overlapping class groups than Nearest Centroid is. If
    the samples for a class are split between several groups, *k*-NN will be superior
    to Nearest Centroid.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些*k*-NN 分类器表现得非常好：如果特征空间的维度足够小，*k*-NN 可能是一个很有吸引力的选择。训练数据的大小需要平衡，这可以提高性能，但也需要更多的存储和更加繁琐的邻居搜索过程，以及特征空间的维度。正是这种情况可能使得最近质心（Nearest
    Centroid）适用，也会让*k*-NN成为一种合适的选择。然而，*k*-NN 可能比最近质心更加适应于分散且略有重叠的类别组。如果某个类别的样本分布在多个组之间，*k*-NN
    将优于最近质心。
- en: Naïve Bayes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Widely used in natural language processing research, the *Naïve Bayes* classifier
    is simple to implement and straightforward to understand, though we’ll have to
    include some math to do it. However, I promise, the description of what’s happening
    will make the math understandable even if the notation isn’t so familiar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*朴素贝叶斯*分类器在自然语言处理研究中被广泛使用，它实现简单，易于理解，尽管我们在实现过程中需要包含一些数学内容。不过，我保证，尽管符号可能不太熟悉，描述所发生的事情会让这些数学内容变得易于理解。'
- en: The technique uses Bayes’ theorem (see Thomas Bayes’ “An Essay Towards Solving
    a Problem in the Doctrine of Chances” published in 1763). The theorem relates
    probabilities, and its modern formulation is
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术使用贝叶斯定理（参见托马斯·贝叶斯的《解决机会论中的一个问题的论文》，发表于1763年）。该定理涉及概率关系，其现代形式为：
- en: '![image](Images/114equ01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/114equ01.jpg)'
- en: which uses some mathematical notation from probability theory that we need to
    describe to understand how we’ll use this theorem to implement a classifier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用了一些来自概率论的数学符号，我们需要描述这些符号，以理解如何利用这个定理来实现分类器。
- en: The expression *P*(*A|B*) represents the probability that event A has occurred,
    given event B has already occurred. In this context, it’s called the *posterior
    probability*. Similarly, *P*(*B|A*) represents the probability that event B has
    occurred, given event A has occurred. We call *P*(*B|A*) the *likelihood* of B,
    given A. Finally, *P*(*A*) and *P*(*B*) represent, respectively, the probability
    that event A has occurred, regardless of event B, and the probability that event
    B has occurred, regardless of event A. We call *P*(*A*) the *prior probability*
    of A. *P*(*B*) is the probability of *B* happening regardless of *A*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式*P*(*A|B*)表示在已知事件B发生的情况下，事件A发生的概率。在这个上下文中，它被称为*后验概率*。类似地，*P*(*B|A*)表示在事件A发生的情况下，事件B发生的概率。我们称*P*(*B|A*)为在给定A的情况下，B的*似然*。最后，*P*(*A*)和*P*(*B*)分别表示事件A发生的概率（不考虑事件B）和事件B发生的概率（不考虑事件A）。我们称*P*(*A*)为A的*先验概率*，*P*(*B*)是事件B发生的概率，不考虑事件A。
- en: 'Bayes’ theorem gives us the probability of something happening (event A) given
    that we already know something else has happened (event B). So how does this help
    us classify? We want to know whether a feature vector belongs to a given class.
    We know the feature vector, but we don’t know the class. So if we have a dataset
    of *m* feature vectors, where each feature vector has *n* features, *x* = *{x*[1],*x*[2],*x*[3],…,*x*[*n*]*}*,
    then we can replace the *B* in Bayes’ theorem with each of the features in the
    feature vector. We can also replace *A* with *y*, the class label we want to assign
    to a new, unknown feature vector *x*. The theorem now looks like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理给出了在已知事件B发生的情况下，事件A发生的概率。那么这如何帮助我们分类呢？我们想知道一个特征向量是否属于某个给定的类别。我们知道特征向量，但不知道类别。所以，如果我们有一个包含*m*个特征向量的数据集，其中每个特征向量有*n*个特征，*x*
    = *{x*[1],*x*[2],*x*[3],…,*x*[*n*]*}*，那么我们可以将贝叶斯定理中的*B*替换为特征向量中的每个特征。我们也可以将*A*替换为*y*，即我们希望赋予一个新的、未知的特征向量*x*的类别标签。这个定理现在看起来是这样的：
- en: '![image](Images/114equ02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/114equ02.jpg)'
- en: Let’s explain things a bit. Bayes’ theorem states that if we know the likelihood
    of having *x* be our feature vector given that *y* is the class, and we know how
    often class *y* shows up (this is *P*(*y*), the prior probability of *y*), then
    we can calculate the probability that the class of the feature vector *x* is *y*.
    If we are able to do this for all the possible classes, all the different *y*
    values, we can select the highest probability and label the input feature vector
    *x* as belonging to that class, *y*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微解释一下。贝叶斯定理指出，如果我们知道在给定类别*y*的情况下，*x*是我们的特征向量的似然性，并且我们知道类别*y*的出现频率（这是*P*(*y*)，即*y*的先验概率），那么我们就可以计算特征向量*x*属于类别*y*的概率。如果我们能够对所有可能的类别、所有不同的*y*值进行这种计算，我们可以选择最高的概率，并将输入特征向量*x*标记为属于该类别*y*。
- en: Recall that a training dataset is a set of pairs, (*x*^(*i*),*y*^(*i*)), for
    a known feature vector, *x*^(*i*), and a known class it belongs to, *y*^(*i*).
    Here the *i* superscript is counting the feature vector and label pairs in the
    training dataset. Now, given a dataset like this, we can calculate *P*(*y*) by
    making a histogram of how often each class label shows up in the training set.
    We believe that the training set fairly represents the parent distribution of
    possible feature vectors so that we can use the training data to calculate the
    values we need to make use of Bayes’ theorem. (See [Chapter 4](ch04.xhtml#ch04)
    for techniques to ensure that the dataset is a good one.)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，训练数据集是由一组对（*x*^(*i*), *y*^(*i*)）组成的，其中* x*^(*i*)是已知的特征向量，*y*^(*i*)是它所属的已知类别。这里的*
    i*上标表示训练数据集中每个特征向量和标签对的计数。现在，给定这样的数据集，我们可以通过制作每个类标签在训练集中出现的频率直方图来计算*P*(*y*)。我们认为训练集可以公平地代表可能特征向量的父分布，因此我们可以使用训练数据来计算贝叶斯定理所需的值。（请参阅[第4章](ch04.xhtml#ch04)，了解如何确保数据集的质量。）
- en: 'Once we have *P*(*y*), we need to know the likelihood, *P*(*x*[1],*x*[2],*x*[3],…,*x*[*n*]*|y*).
    Unfortunately, we can’t calculate this directly. But all is not lost: we’ll make
    an assumption that will let us move ahead. We’ll assume that each of the features
    in *x* is *statistically independent*. This means that the fact that we measure
    a particular *x*[1] has nothing whatsoever to do with the values of any of the
    other *n –* 1 features. This isn’t true always, or even most of the time, but
    in practice, it turns out that this assumption is often close enough to true that
    we can get by. This is why it’s called *Naïve Bayes*, as it’s naïve to assume
    the features are independent of each other. That assumption is most definitely
    not true, for example, when our input is an image. The pixels of an image are
    highly dependent upon each other. Pick one at random, and the pixels next to it
    are almost certainly within a few values of it.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了*P*(*y*)，我们还需要知道似然，*P*(*x*[1], *x*[2], *x*[3], …, *x*[*n*]*|y*)。不幸的是，我们不能直接计算这个值。但并非一无所获：我们将做出一个假设，帮助我们继续前进。我们假设*x*中的每个特征是*统计独立的*。这意味着我们测量特定的*x*[1]与其他*n-1*个特征的值毫无关系。这个假设并非总是成立，甚至大多数时候并不成立，但在实际操作中，这个假设通常足够接近真实情况，我们可以继续使用它。这就是为什么称其为*朴素贝叶斯*，因为假设特征相互独立是一个简单而幼稚的假设。这个假设在某些情况下确实不成立，例如当我们的输入是图像时。图像的像素高度依赖于彼此。随机挑选一个像素，旁边的像素几乎可以肯定与它的值相差不大。
- en: 'When two events are independent, their *joint probability*, the probability
    that they both happen, is simply the product of their individual probabilities.
    The independence assumption lets us change the likelihood portion of Bayes’ theorem
    like so:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个事件是独立时，它们的*联合概率*，即它们同时发生的概率，就是它们各自概率的乘积。独立性假设允许我们像这样改变贝叶斯定理中的似然部分：
- en: '![image](Images/115equ01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/115equ01.jpg)'
- en: The ∏ symbol means *multiplied together*, much like the ∑ symbol means *added
    together*. The right side of the equation is saying that if we know the probability
    of measuring a particular value of a feature, say feature *x*[*i*], given that
    the class label is *y*, we can get the likelihood of the entire feature vector
    *x*, given class label *y*, by multiplying each of the per feature probabilities
    together.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ∏符号表示*相乘*，就像∑符号表示*相加*一样。方程右侧的意思是，如果我们知道测量特征的特定值的概率，例如特征 *x*[*i*]，假设类别标签为 *y*，那么我们可以通过将每个特征的概率相乘来得到整个特征向量
    *x* 给定类别标签 *y* 的似然性。
- en: If our dataset consists of categorical values, or discrete values like integers
    (for example, age), then we can use the dataset to calculate the *P*(*x*[*i*]*|y*)
    values by building a histogram for each feature for each class. For example, if
    feature *x*[2] for class 1 has the following values
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据集由类别值或离散值（如整数，例如年龄）组成，那么我们可以通过为每个类的每个特征构建直方图来计算*P*(*x*[*i*]*|y*)值。例如，如果类1的特征
    *x*[2]有以下值：
- en: 7, 4, 3, 1, 6, 5, 2, 8, 5, 4, 4, 2, 7, 1, 3, 1, 1, 3, 3, 3, 0, 3,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 7, 4, 3, 1, 6, 5, 2, 8, 5, 4, 4, 2, 7, 1, 3, 1, 1, 3, 3, 3, 0, 3,
- en: 4, 4, 2, 3, 4, 5, 2, 4, 2, 3, 2, 4, 4, 1, 3, 3, 3, 2, 2, 4, 6, 5,
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 4, 4, 2, 3, 4, 5, 2, 4, 2, 3, 2, 4, 4, 1, 3, 3, 3, 2, 2, 4, 6, 5,
- en: 2, 6, 5, 2, 6, 6, 3, 5, 2, 4, 2, 4, 5, 4, 5, 5, 2, 5, 3, 4, 3, 1,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2, 6, 5, 2, 6, 6, 3, 5, 2, 4, 2, 4, 5, 4, 5, 5, 2, 5, 3, 4, 3, 1,
- en: 6, 6, 5, 3, 4, 3, 3, 4, 1, 1, 3, 5, 4, 4, 7, 0, 6, 2, 4, 7, 4, 3,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 6, 6, 5, 3, 4, 3, 3, 4, 1, 1, 3, 5, 4, 4, 7, 0, 6, 2, 4, 7, 4, 3,
- en: 4, 3, 5, 4, 6, 2, 5, 4, 4, 5, 6, 5
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 4, 3, 5, 4, 6, 2, 5, 4, 4, 5, 6, 5
- en: then each value occurs with the following probability
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 那么每个值的出现概率如下：
- en: '0: 0.02'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '0: 0.02'
- en: '1: 0.08'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 0.08'
- en: '2: 0.15'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 0.15'
- en: '3: 0.20'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 0.20'
- en: '4: 0.24'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '4: 0.24'
- en: '5: 0.16'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '5: 0.16'
- en: '6: 0.10'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '6: 0.10'
- en: '7: 0.04'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '7: 0.04'
- en: '8: 0.01'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '8: 0.01'
- en: which comes from the number of times each value occurs divided by 100, the total
    number of values in the dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这来自于每个值出现的次数除以 100，即数据集中值的总数。
- en: This histogram is exactly what we need to find *P*(*x*[2]*|y* = 1), the probability
    for feature 2 when the class label is 1\. For example, we can expect a new feature
    vector of class 1 to have *x*[2] = 4 about 24 percent of the time and to have
    *x*[2] = 1 about 8 percent of the time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个直方图正是我们用来找出 *P*(*x*[2]*|y* = 1) 所需要的，它表示特征 2 在类别标签为 1 时的概率。例如，我们可以期望类别 1 的新特征向量中，*x*[2]
    = 4 的概率约为 24%，*x*[2] = 1 的概率约为 8%。
- en: By building tables like this for each feature and each class label, we can complete
    our classifier for the categorical and discrete cases. For a new feature vector,
    we use the tables to find the probability that each feature would have that value.
    We multiply each of those probabilities together and then multiply by the prior
    probability of that class. This, repeated for each of the *m* classes in the dataset,
    will give us a set of *m* posterior probabilities. To classify the new feature
    vector, select the largest of these *m* values, and assign the corresponding class
    label.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为每个特征和每个类别标签构建这样的表格，我们可以完成分类器，适用于分类变量和离散数据的情况。对于一个新的特征向量，我们使用这些表格来找出每个特征取该值的概率。我们将每个概率相乘，然后再乘以该类别的先验概率。对于数据集中的每个
    *m* 类别，这个过程重复进行，将给我们一组 *m* 个后验概率。为了对新的特征向量进行分类，选择这 *m* 个值中最大的一个，并分配相应的类别标签。
- en: How do we calculate *P*(*x*[*i*]*|y*) if the feature values are continuous?
    One way would be to bin the continuous values and then make tables as in the discrete
    case. Another is to make one more assumption. We need to make an assumption about
    the distribution of possible *x*[*i*] feature values that we could measure. Most
    natural phenomena seem to follow a normal distribution. We discussed the normal
    distribution in [Chapter 1](ch01.xhtml#ch01). Let’s assume, then, that the features
    all follow normal distributions. A normal distribution is defined by its mean
    value (*μ*, mu) and a standard deviation (*σ*, sigma). The mean value is just
    the average value we’d expect if we drew samples from the distribution repeatedly.
    The standard deviation is a measure of how wide the distribution is—how spread
    out it is around the mean value.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征值是连续的，我们该如何计算 *P*(*x*[*i*]*|y*) 呢？一种方法是将连续值进行分箱，然后像离散情况一样制作表格。另一种方法是做出更多的假设。我们需要假设可以测量到的
    *x*[*i*] 特征值的分布。大多数自然现象似乎遵循正态分布。我们在[第一章](ch01.xhtml#ch01)中讨论了正态分布。那么假设所有特征都遵循正态分布。正态分布由其均值
    (*μ*, mu) 和标准差 (*σ*, sigma) 定义。均值是我们如果多次从分布中抽取样本时，期望得到的平均值。标准差是衡量分布宽度的指标——即围绕均值的分布范围有多宽。
- en: Mathematically, what we want to do is replace each *P*(*x*[*i*]*|y*) like so
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，我们想做的是将每个 *P*(*x*[*i*]*|y*) 替换成如下形式
- en: '*P*(*x*[*i*]|*y*) ≈ *N*(*μ*[*i*], *σ*[*i*])'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x*[*i*]|*y*) ≈ *N*(*μ*[*i*], *σ*[*i*])'
- en: for each feature in our feature vector. Here N(*μ*[*i*],*σ*[*i*]) is notation
    meaning a normal distribution centered around some mean value (*μ*) and defined
    by a spread (*σ*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们特征向量中的每个特征，N(*μ*[*i*],*σ*[*i*]) 是一种符号表示，意味着一个围绕某个均值 (*μ*) 进行分布的正态分布，并且由一个分布范围
    (*σ*) 定义。
- en: 'We don’t really know the exact *μ* and *σ* values, but we can approximate them
    from the training data. For example, assume the training data consists of 25 samples,
    where the class label is 0\. Further, assume that the following are the values
    of feature 3, that is, *x*[3], in those cases:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不知道确切的 *μ* 和 *σ* 值，但我们可以通过训练数据进行近似。例如，假设训练数据由 25 个样本组成，其中类别标签为 0。进一步假设在这些情况下，特征
    3（即 *x*[3]）的值如下：
- en: 0.21457111,  4.3311102,   5.50481251,  0.80293956,  2.5051598,
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 0.21457111,  4.3311102,   5.50481251,  0.80293956,  2.5051598,
- en: 2.37655204,  2.4296739,   2.84224169, -0.11890662,  3.18819152,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 2.37655204,  2.4296739,   2.84224169, -0.11890662,  3.18819152,
- en: 1.6843311,   4.05982237,  4.14488722,  4.29148855,  3.22658406,
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 1.6843311,   4.05982237,  4.14488722,  4.29148855,  3.22658406,
- en: 6.45507675,  0.40046778,  1.81796124,  0.2732696,   2.91498336,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 6.45507675,  0.40046778,  1.81796124,  0.2732696,   2.91498336,
- en: 1.42561983,  2.73483704,  1.68382843,  3.80387653,  1.53431146
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1.42561983,  2.73483704,  1.68382843,  3.80387653,  1.53431146
- en: Then we’d use *μ*[3] = 2.58 and *σ*[3] = 1.64 when setting up the normal distribution
    for feature 3 for class 0 since the average of these values is 2.58, and the standard
    deviation, the spread around the mean value, is 1.64.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在为类别 0 设置特征 3 的正态分布时，使用 *μ*[3] = 2.58 和 *σ*[3] = 1.64，因为这些值的平均值为 2.58，标准差，即均值的分布范围，约为
    1.64。
- en: When a new unknown sample is given to the classifier, we would compute the probability
    of the given *x*[3] happening if the actual class was class 0 by using the following
    equation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个新的未知样本被提供给分类器时，我们将通过以下方程计算在实际类别为类别0的情况下，给定的*x*[3]发生的概率。
- en: '![image](Images/117equ01.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/117equ01.jpg)'
- en: This equation comes from the definition of a normal distribution with mean *μ*
    and standard deviation *σ*. It says that the likelihood of a particular feature
    value, given the class is *y*, is distributed around the mean value we measured
    from the training data according to the normal distribution. This is an assumption
    we are making on top of the independence assumption between features.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程来自正态分布的定义，其中均值为*μ*，标准差为*σ*。它表示在给定类别*y*的情况下，某个特定特征值的可能性是围绕我们从训练数据中测得的均值，根据正态分布分布的。这是我们在特征之间的独立性假设之上做出的一个假设。
- en: 'We use this equation for each of the features in the unknown feature vector.
    We’d then multiply the resulting probabilities together, and multiply that value
    by the prior probability of class 0 happening. We’d repeat this process for each
    of the classes. In the end, we’ll have *m* numbers, the probabilities of the feature
    vector belonging to each of the *m* classes. To make a final decision, we’d do
    what we did before: select the largest of these probabilities and label the input
    as being of the corresponding class.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对未知特征向量中的每个特征使用这个方程。然后我们将得到的概率相乘，并将该值乘以类别0发生的先验概率。我们会对每个类别重复这个过程。最终，我们会得到*m*个数字，即特征向量属于每个*m*类别的概率。为了做出最终决策，我们会像之前那样：选择这些概率中最大的一个，并将输入标记为相应类别。
- en: Some readers may complain that we ignored the denominator of Bayes’ theorem.
    We did that because it’s a constant across all the calculations, and since we
    always select the largest posterior probability, we really don’t care whether
    we divide each value by a constant. We’ll select the same class label, regardless.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一些读者可能会抱怨我们忽略了贝叶斯定理的分母。我们这么做是因为它在所有计算中是一个常数，并且由于我们总是选择最大的后验概率，我们真的不在乎是否将每个值除以常数。无论如何，我们会选择相同的类别标签。
- en: Also, for the discrete case, it’s possible that our training set does not have
    any instances of a value that rarely shows up. We ignored that, too, but it is
    a problem since if the value never shows up, the *P*(*x*[*i*]*|y*) we use would
    be 0, making the entire posterior probability 0\. This often happens in natural
    language processing, where a particular word is rarely used. A technique called
    *Laplace smoothing* gets around this, but for our purposes, we claim that a “good”
    training set will represent *all* possible values for the features and simply
    press ahead. The sklearn MultinomialNB Naïve Bayes classifier for discrete data
    uses Laplace smoothing by default.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于离散情况，我们的训练集可能没有出现某些很少出现的值。我们也忽略了这一点，但这是一个问题，因为如果某个值从未出现过，我们使用的*P*(*x*[*i*]*|y*)将是0，这使得整个后验概率为0。这在自然语言处理中经常发生，因为某些特定的单词很少被使用。一种叫做*拉普拉斯平滑*的技术可以解决这个问题，但就我们的目的而言，我们认为一个“好的”训练集会表示特征的*所有*可能值，并且直接继续前进。sklearn的多项式朴素贝叶斯分类器对于离散数据默认使用拉普拉斯平滑。
- en: Decision Trees and Random Forests
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树和随机森林
- en: 'The left side of [Figure 6-5](ch06.xhtml#ch6fig5) shows an x-ray image of a
    puppy with a malformed right hip socket. Since the puppy is on its back in the
    x-ray, the right hip socket is on the left side of the image. The right side of
    [Figure 6-5](ch06.xhtml#ch6fig5) shows the corresponding histogram of the pixel
    intensities (8-bit values, [0,255]). There are two modes to this histogram, corresponding
    to the dark background and the lighter-intensity x-ray data. If we want to classify
    each pixel of the image into either background or x-ray, we can do so with the
    following rule: “If the pixel intensity is less than 11, call the pixel background.”'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-5](ch06.xhtml#ch6fig5)的左侧显示了一个小狗的X光图像，其右侧髋关节窝畸形。由于小狗在X光片中是仰卧的，右侧髋关节窝位于图像的左侧。[图
    6-5](ch06.xhtml#ch6fig5)的右侧显示了相应的像素强度直方图（8位值，[0,255]）。这个直方图有两个峰值，分别对应黑色背景和较亮的X光数据。如果我们想将图像中的每个像素分类为背景或X光数据，可以使用以下规则：“如果像素强度小于11，则将该像素标记为背景。”'
- en: '![image](Images/06fig05.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig05.jpg)'
- en: '*Figure 6-5: An x-ray image of a puppy (left). The corresponding histogram
    of 8-bit pixel values [0,255] (right).*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6-5：一只小狗的X光图像（左）。对应的8位像素值[0,255]的直方图（右）。*'
- en: This rule implements a decision made about the data based on one of the features,
    in this case, the pixel intensity value. Simple decisions like this are at the
    heart of *Decision Trees*, the classification algorithm we’ll explore in this
    section. For completeness, if we apply the decision rule to each pixel in the
    image and output 0 or 255 (maximum pixel value) for background versus x-ray data,
    we get a mask showing us which pixels are part of the image. See [Figure 6-6](ch06.xhtml#ch6fig6).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则实现了基于数据的一个特征做出的决策，在本例中是像素强度值。像这样的简单决策是*决策树*的核心，决策树是我们在本节中将要探索的分类算法。为了完整性，如果我们将决策规则应用到图像中的每个像素，并输出
    0 或 255（最大像素值），用以区分背景和 X 光数据，我们得到一个掩膜，显示哪些像素属于图像。见[图6-6](ch06.xhtml#ch6fig6)。
- en: '![image](Images/06fig06.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig06.jpg)'
- en: '*Figure 6-6: An x-ray image of a puppy (left). The corresponding pixel mask
    generated by the decision rule. White pixels are part of the x-ray image (right).*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-6：一只小狗的 X 光图像（左）。决策规则生成的相应像素掩膜。白色像素是 X 光图像的一部分（右）。*'
- en: A Decision Tree is a set of nodes. The nodes either define a condition and branch
    based on the truth or falsehood of the condition, or select a particular class.
    Nodes that do not branch are called *leaf nodes*. Decision Trees are called *trees*
    because, especially for the binary case we’ll consider here, they branch like
    trees. [Figure 6-7](ch06.xhtml#ch6fig7) shows a Decision Tree learned by the sklearn
    DecisionTreeClassifier class for the full iris dataset using the first three features.
    See [Chapter 5](ch05.xhtml#ch05).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一组节点。节点要么定义一个条件并根据条件的真假来分支，要么选择一个特定的类别。没有分支的节点叫做*叶节点*。决策树之所以叫做*树*，是因为，尤其在我们这里考虑的二分类情况下，它们像树一样分支。[图6-7](ch06.xhtml#ch6fig7)展示了使用前3个特征，通过
    sklearn 的 DecisionTreeClassifier 类学习到的完整鸢尾花数据集的决策树。见[第5章](ch05.xhtml#ch05)。
- en: '![image](Images/06fig07.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig07.jpg)'
- en: '*Figure 6-7: A Decision Tree classifier for the iris dataset*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-7：用于鸢尾花数据集的决策树分类器*'
- en: By convention, the first node in the tree, the *root*, is drawn at the top.
    For this tree, the root node asks the question, “Is the petal length ≤ 2.45?”
    If it is, the left branch is taken, and the tree immediately reaches a leaf node
    and assigns a label of “virginica” (class 0). We’ll discuss the other information
    in the nodes shortly. If the petal length is not ≤ 2.45, the right branch is taken,
    leading to a new node that asks, “Is the petal length ≤ 4.75?” If so, we move
    to a node that asks a question about the sepal length. If not, we move to the
    right node and consider the petal length again. This process continues until a
    leaf is reached, which determines the class label.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 按惯例，树的第一个节点，即*根节点*，画在顶部。对于这棵树，根节点提出问题：“花瓣长度 ≤ 2.45 吗？”如果是，选择左侧分支，树会立即到达一个叶节点，并分配“virginica”（类别
    0）标签。我们稍后会讨论节点中的其他信息。如果花瓣长度不 ≤ 2.45，则选择右侧分支，进入一个新节点，提出问题：“花瓣长度 ≤ 4.75 吗？”如果是，则进入一个询问萼片长度的问题的节点。如果不是，则进入右侧节点，再次考虑花瓣长度。这个过程持续下去，直到到达叶节点，从而确定类别标签。
- en: The process just described is exactly how a Decision Tree is used after it’s
    created. For any new feature vector, the series of questions is asked starting
    with the root node, and the tree is traversed until a leaf node is reached to
    decide the class label. This is a human-friendly way to move through the classification
    process, which is why Decision Trees are handy in situations where the “why” of
    the class assignment is as important to know as the class assignment itself. A
    Decision Tree can explain itself.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才描述的过程正是决策树在创建后如何使用的方式。对于任何新的特征向量，从根节点开始，依次提出问题，遍历树直到到达叶节点，以决定类别标签。这是一个对人类友好的分类过程，因此在类别分配的“原因”与类别本身一样重要的情况下，决策树非常有用。决策树可以自我解释。
- en: Using a Decision Tree is simple enough, but how is the tree created in the first
    place? Unlike the simple algorithms in the previous sections, the tree-building
    process is more involved, but not so involved that we can’t follow through the
    main steps to build some intuition as to what goes into defining the tree.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树足够简单，但树是如何创建的呢？与前面章节中的简单算法不同，建树过程更为复杂，但也并非复杂到我们无法跟随主要步骤来建立对定义树的直觉。
- en: Recursion Primer
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 递归基础
- en: Before we talk about the Decision Tree algorithm, however, we need to discuss
    the concept of *recursion*. If you’re familiar with computer science, you probably
    already know that tree-like data structures and recursion go hand in hand. If
    not, don’t worry; recursion is a straightforward but powerful concept. The essence
    of a recursive algorithm is that the algorithm repeats itself at different levels.
    When implemented as a function in a programming language, this generally means
    that the function calls itself on a smaller version of the problem. Naturally,
    if the function calls itself indefinitely, we’ll have an infinite loop, so the
    recursion needs a stopping condition—something that says we no longer need to
    recurse.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们讨论决策树算法之前，我们需要讨论一下*递归*的概念。如果你对计算机科学有所了解，你可能已经知道树状数据结构和递归是相辅相成的。如果不了解，也不用担心；递归是一个简单但强大的概念。递归算法的本质在于算法在不同的层次上重复执行。当递归作为编程语言中的函数实现时，通常意味着该函数在问题的一个较小版本上调用自身。自然地，如果函数无限制地调用自身，就会出现无限循环，因此递归需要一个停止条件——即一种表明我们不再需要递归的机制。
- en: Let’s introduce the idea of recursion mathematically. The factorial of an integer,
    *n*, denoted *n*!, is defined to be
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数学角度引入递归的概念。整数*n*的阶乘，记作*n*!，定义为
- en: '*n*! = *n*(*n* – 1)(*n* – 2)(*n* – 3) . . . (*n* – *n* + 1)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*! = *n*(*n* – 1)(*n* – 2)(*n* – 3) . . . (*n* – *n* + 1)'
- en: which just means multiply together all the integers from 1 to *n*. By definition,
    0! = 1\. Therefore, the factorial of 5 is 120 because
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是意味着将从1到*n*的所有整数相乘。根据定义，0! = 1。因此，5的阶乘是120，因为
- en: 5! = 5 × 4 × 3 × 2 × 1 = 120
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 5! = 5 × 4 × 3 × 2 × 1 = 120
- en: If we look at 5! we see that it is nothing more than 5 × 4! or, in general,
    that, *n*! = *n* × (*n –* 1)!. Now, let’s write a Python function to calculate
    factorials recursively using this insight. The code is simple, also a hallmark
    of many recursive functions, as [Listing 6-1](ch06.xhtml#ch6lis1) shows.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看5!，就会发现它无非是5 × 4!，或者一般地，*n*! = *n* × (*n –* 1)!。现在，让我们用这个思路写一个递归计算阶乘的Python函数。代码很简单，这也是许多递归函数的特点，参见[清单
    6-1](ch06.xhtml#ch6lis1)。
- en: 'def fact(n):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'def fact(n):'
- en: 'if (n <= 1):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (n <= 1):'
- en: return 1
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: return 1
- en: 'else:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: return n*fact(n-1)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: return n*fact(n-1)
- en: '*Listing 6-1: Calculating the factorial*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6-1：计算阶乘*'
- en: The code is a direct implementation of the rule that the factorial of *n* is
    *n* times the factorial of *n –* 1\. To find the factorial of n, we first ask
    if n is 1\. If it is, we know the factorial is 1 so we return 1—this is our stopping
    condition. If n is not 1, we know that the factorial of n is simply n times the
    factorial of n-1, which we find by calling fact with n-1 as the argument.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是对阶乘规则的直接实现，即*n*的阶乘等于*n*乘以*n – 1*的阶乘。为了计算n的阶乘，我们首先检查n是否为1。如果是，我们知道阶乘是1，因此返回1——这是我们的停止条件。如果n不是1，我们知道n的阶乘就是n乘以n-1的阶乘，我们通过递归调用fact函数并传入n-1来找到n-1的阶乘。
- en: Building Decision Trees
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建决策树
- en: The algorithm to build a Decision Tree is also recursive. Let’s walk through
    what happens at a high level. The algorithm starts with the root node, determines
    the proper rule for that node, and then calls itself on the left and right branches.
    The call to the left branch will start again as if the left branch is the root
    node. This will continue until a stopping condition is met.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的算法也是递归的。我们来简要了解一下大致流程。该算法从根节点开始，确定该节点的适当规则，然后递归调用左右子树。左子树的递归调用会像从根节点一样重新开始。这种过程会持续，直到满足停止条件为止。
- en: For a Decision Tree, the stopping condition is a leaf node (we’ll discuss how
    a Decision Tree knows whether to create a leaf node next). Once a leaf node is
    created, the recursion terminates, and the algorithm returns to that leaf’s parent
    node and calls itself on the right branch. The algorithm then starts again as
    if the right branch were the root node. Once both recursive calls terminate, and
    a node’s left and right subtrees are created, the algorithm returns to that node’s
    parent, and so on and so forth until the entire tree is constructed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树，停止条件是一个叶子节点（接下来我们会讨论决策树如何判断是否创建叶子节点）。一旦叶子节点创建完成，递归终止，算法会返回到该叶子节点的父节点，并在右子树上再次调用自己。算法随后会重新开始，仿佛右子树就是根节点。一旦两个递归调用都终止，且一个节点的左右子树都创建完成，算法会返回到该节点的父节点，如此反复，直到整个树构建完成。
- en: 'Now to get a little more specific. How is the training data used to build the
    tree? When the root node is defined, all the training data is present—say, all
    *n* samples. This is the set of samples used to pick the rule the root node implements.
    Once that rule has been selected and applied to the training samples, we have
    two new sets of samples: one for the left side (the true side) and one for the
    right side (the false side).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在稍微具体一点。训练数据是如何用于构建决策树的呢？当根节点被定义时，所有的训练数据都在——假设是所有的*n*样本。这些样本是用于选择根节点所实现的规则的样本集。一旦规则被选定并应用到训练样本中，我们就得到了两个新的样本集：一个是左侧（真实侧）的样本，另一个是右侧（假侧）的样本。
- en: The recursion then works with these nodes, using their respective set of training
    samples, to define the rule for the left and right branches. Every time a branch
    node is created, the training set for that branch node gets split into samples
    that meet the rule and samples that don’t meet the rule. A leaf node is declared
    when the set of training samples is either too small, of a sufficiently high proportion
    of one class, or the maximum tree depth has been reached.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后递归算法会在这些节点上继续工作，使用它们各自的训练样本集，来定义左侧和右侧分支的规则。每当一个分支节点被创建时，该分支节点的训练集会根据是否满足规则进行分割，得到满足规则的样本和不满足规则的样本。当训练样本集太小，或者某一类别的比例足够高，或者达到了最大树深时，就会宣布该节点为叶节点。
- en: By now you’re probably wondering, “How do we select the rule for a branch node?”
    The rule relates a single input feature, like the petal length, to a particular
    value. The Decision Tree is a *greedy* algorithm; this means that at every node
    it selects the best rule for the current set of information available to it. In
    this case, this is the current set of training samples that are available to the
    node. The best rule is the one that best separates the classes into two groups.
    This implies that we need a way to select possible candidate rules and that we
    have a way to determine that a candidate rule is “best.” The Decision Tree algorithm
    uses brute force to locate candidate rules. It runs through all possible combinations
    of features and values, making continuous values discrete by binning, and evaluates
    the purity of the left and right training sets after the rule is applied. The
    best-performing rule is the one kept at that node.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里你可能会想，“我们如何选择分支节点的规则？”这个规则将一个输入特征（比如花瓣长度）与某个特定值相关联。决策树是一种*贪婪*算法；这意味着在每个节点，它都会为当前可用的信息选择最佳规则。在这种情况下，就是当前可用于节点的训练样本集。最佳规则是将类别最有效地分割成两个组的规则。这意味着我们需要一种方法来选择可能的候选规则，并且我们需要一种方法来确定候选规则是否是“最佳的”。决策树算法使用暴力搜索来定位候选规则。它遍历所有特征和值的可能组合，通过分箱将连续值离散化，并在应用规则后评估左右训练集的纯度。表现最好的规则会被保留在该节点。
- en: “Best performing" is determined by the *purity* of the split into left and right
    training sample subsets. One way to measure purity is to use the *Gini index*.
    This is the metric sklearn uses. The Gini index of each node in the iris example
    of [Figure 6-7](ch06.xhtml#ch6fig7) is shown. It’s calculated as
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳表现”是由左侧和右侧训练样本子集的*纯度*决定的。衡量纯度的一种方法是使用*基尼指数*。这是sklearn使用的度量标准。在[图6-7](ch06.xhtml#ch6fig7)中的鸢尾花示例中，显示了每个节点的基尼指数。其计算公式为：
- en: '![image](Images/122equ01.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/122equ01.jpg)'
- en: where *P*(*y*[*i*]) is the fraction of training examples in the subset for the
    current node that are of class *i*. A perfect split between classes, all of one
    class and none of the other, will result in a Gini index of 0\. A 50-50 split
    has a Gini index of 0.5\. The algorithm seeks to minimize the Gini index at each
    node by selecting the candidate rule that results in the smallest Gini index.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*P*(*y*[*i*])是当前节点子集中属于类别*i*的训练样本所占的比例。类之间的完美分割——即全为一类，另一类完全没有——将导致基尼指数为0。50-50的分割则基尼指数为0.5。该算法通过选择使基尼指数最小的候选规则来在每个节点最小化基尼指数。
- en: For example, in [Figure 6-7](ch06.xhtml#ch6fig7) the right-hand node below the
    root has a Gini index of 0.5\. This means that the rule for *the node above*,
    the root, will result in a subset of the training data that has petal length >
    2.45, and that subset will be evenly divided between classes 1 and 2\. This is
    the meaning of the “value” line in the node text. It shows the number of training
    samples in the subset that defined the node. The “class” line is the class that
    would be assigned if the tree were stopped at that node. It’s simply the class
    label of the class that has the largest number of training samples in the node’s
    subset. When the tree is used on new, unknown samples, it’s run from root to a
    leaf, always.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图 6-7](ch06.xhtml#ch6fig7)中，根节点下方的右侧节点的基尼指数为0.5。这意味着*上方节点*（即根节点）的规则将导致训练数据的子集满足花瓣长度
    > 2.45，并且该子集将在类别1和类别2之间均匀划分。这就是节点文本中“值”行的含义。它显示了定义节点的子集中的训练样本数量。“类别”行是如果树在该节点停止时所分配的类别。它仅仅是子集中具有最大训练样本数的类别标签。当树被应用到新的、未知的样本时，它会从根节点一直运行到叶节点。
- en: Random Forests
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: Decision Trees are useful when the data is discrete or categorical or has missing
    values. Continuous data needs to be binned first (sklearn does this for you).
    Decision Trees have a bad habit, however, of *overfitting* the training data.
    This means that they are likely to learn meaningless statistical nuances of the
    training data that you happened to use, instead of learning meaningful general
    features of the data that are useful when applied to unknown data samples. Decision
    Trees also grow very large, as the number of features grows, unless managed by
    depth parameters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据是离散的、分类的或有缺失值时，决策树非常有用。连续数据需要先进行分箱（sklearn会为你处理这一点）。然而，决策树有一个坏习惯，就是*过拟合*训练数据。这意味着它们可能学习到训练数据中的无意义的统计细节，而不是学习到数据中有意义的通用特征，这些特征在应用到未知数据样本时非常有用。决策树也会长得非常大，特别是随着特征数量的增加，除非通过深度参数来管理。
- en: 'Decision Tree overfitting can be mitigated by using *Random Forests*. In fact,
    unless your problem is simple, you probably want to look at using a Random Forest
    from the start. The following three concepts lead from a Decision Tree to a Random
    Forest:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用*随机森林*来缓解决策树的过拟合问题。事实上，除非你的问题非常简单，否则你可能从一开始就想使用随机森林。以下三个概念是从决策树到随机森林的过渡：
- en: Ensembles of classifiers and voting between them
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器集成与投票
- en: Resampling of the training set by selecting samples *with replacement*
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择带有*放回*的样本对训练集进行重采样
- en: Selection of random feature subsets
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机特征子集选择
- en: If we have a set of classifiers, each trained on different data or of a different
    type, like a *k*-NN and a Naïve Bayes, we can use their outputs to vote on the
    actual category to assign to any particular unknown sample. This is called an
    *ensemble* and, with diminishing returns as the number of classifiers increases,
    it will, in general, improve the performance over that of any individual classifier.
    We can employ a similar idea and imagine an ensemble, or *forest*, of Decision
    Trees, but unless we do something more with the training set, we’ll have a forest
    of *exactly* the same tree because a particular set of training examples will
    always lead to the exact same Decision Tree. The algorithm to create a Decision
    Tree is deterministic—it always returns the same results.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一组分类器，每个分类器都在不同的数据或类型上进行训练，例如*k*-最近邻和朴素贝叶斯，我们可以使用它们的输出对任何特定未知样本分配实际类别进行投票。这被称为*集成*，并且随着分类器数量的增加，虽然收益递减，但通常会提高比任何单个分类器更好的性能。我们可以采用类似的思路，想象一个决策树的*森林*，但除非我们对训练集做些更多的处理，否则我们将得到一片*完全相同的*树，因为一个特定的训练样本集将始终导致完全相同的决策树。创建决策树的算法是确定性的——它总是返回相同的结果。
- en: A way to deal with the particular statistical nuances of the training set you
    have to work with is to select a new training set from the original training set
    but allow the same training set sample to be selected more than once. This is
    selection with replacement. Think of it as choosing colored marbles from a bag,
    but before you select the next marble, put the one you just selected back in the
    bag so you might pick it again. A new dataset selected in this way is known as
    a *bootstrap sample*. Building a collection of new datasets in this way is known
    as *bagging*, and it is models built from this collection of resampled datasets
    that build the Random Forest.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 处理你所使用的训练集中特定统计细节的一种方法是从原始训练集中选择一个新的训练集，但允许相同的训练集样本被选中多次。这就是带替换的抽样。可以将其视为从袋子里挑选彩色弹珠，在选择下一个弹珠之前，将刚选的那个放回袋子中，这样你可能会再次选中它。以这种方式选择的新数据集被称为*自助采样*（bootstrap
    sample）。通过这种方式构建的新数据集集合被称为*袋装法*（bagging），由这些重新抽样的数据集构建的模型就是随机森林。
- en: If we train multiple trees, each with a resampled training set with replacement,
    we’ll get a forest of trees, each one slightly different from the others. This
    alone, along with ensemble voting, will probably improve things. However, there’s
    one issue. If some of the features are highly predictive, they will dominate,
    and the resulting forest of trees will be very similar to one another and therefore
    suffer from very similar weaknesses. This is where the *random* of *Random Forest*
    comes into play.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们训练多个决策树，每棵树都使用带有替换的重新抽样训练集，那么我们将得到一片树森林，其中每棵树都与其他树稍微不同。仅此一项，再加上集成投票，可能就能改进模型。然而，有一个问题。如果某些特征具有很高的预测能力，它们会占主导地位，导致生成的树森林之间非常相似，因此会遭遇类似的弱点。这就是*随机森林*中“随机”部分发挥作用的地方。
- en: Instead of just bagging, which changes the distribution of samples in the per
    tree training set but not the set of features examined, what if we also randomly
    selected, for each tree in the forest, a subset of the *features* themselves and
    trained on only those features? Doing this would break the correlation between
    the trees and increase the overall robustness of the forest. In practice, if there
    are *n* features per feature vector, each tree will randomly select ![Image](Images/nsqt.jpg)
    of them over which to build the tree. Random Forests are supported in sklearn
    as well.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不仅仅是使用袋装法（bagging），即改变每棵树训练集中的样本分布，但不改变检查的特征集，那如果我们每棵树在森林中随机选择一部分*特征*来训练，仅仅在这些特征上训练呢？这样做会打破树之间的相关性，增加森林的整体鲁棒性。实际上，如果每个特征向量有*n*个特征，那么每棵树将随机选择![image](Images/nsqt.jpg)个特征来构建树。Random
    Forest在sklearn中也得到了支持。
- en: Support Vector Machines
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Our final classical machine learning model is the one that held neural networks
    at bay for most of the 1990s, the *Support Vector Machine (SVM)*. If the neural
    network is a highly data-driven, empirical approach to generating a model, the
    SVM is a highly elegant, mathematically founded, approach. We’ll discuss the performance
    of an SVM at a conceptual level as the mathematics involved is beyond what we
    want to introduce here. If you’re so inclined, the classic reference is “Support-Vector
    Networks” by Cortes and Vapnik (1995).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终经典机器学习模型是那种在1990年代大部分时间里压制神经网络的模型——*支持向量机（SVM）*。如果神经网络是一种高度依赖数据、经验性地生成模型的方法，那么支持向量机则是一种高度优雅、数学基础扎实的方法。我们将在概念层面讨论支持向量机的表现，因为其中涉及的数学超出了我们这里所要介绍的范围。如果你感兴趣，经典参考文献是Cortes和Vapnik（1995年）撰写的《支持向量网络》。
- en: We can summarize what a Support Vector Machine is doing by gaining intuition
    about the concepts of margins, support vectors, optimization, and kernels. Let’s
    look at each concept in turn.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过了解边距、支持向量、优化和核的概念来总结支持向量机的工作原理。让我们逐一讨论这些概念。
- en: Margins
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 边距
- en: '[Figure 6-8](ch06.xhtml#ch6fig8) shows a two-class dataset with two features.
    We’ve plotted each sample in the dataset with feature 1 along the x-axis and feature
    2 along the y-axis. Class 0 is shown as circles, class 1 as diamonds. This is
    obviously a contrived dataset, one that’s easily separated by plotting a line
    between the circles and the diamonds.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](ch06.xhtml#ch6fig8)展示了一个具有两个特征的二类数据集。我们已经在数据集上绘制了每个样本，其中特征1沿x轴，特征2沿y轴。类别0用圆圈表示，类别1用菱形表示。显然这是一个人为构造的数据集，通过在圆圈和菱形之间绘制一条直线就可以轻松分离。'
- en: '![image](Images/06fig08.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig08.jpg)'
- en: '*Figure 6-8: A toy dataset with two classes, circles and diamonds, and two
    features, x-axis and y-axis*'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-8：一个包含两类（圆圈和菱形）和两个特征（x轴和y轴）的玩具数据集*'
- en: A classifier can be thought of as locating one or more *planes* that split the
    space of the training data into homogeneous groups. In the case of [Figure 6-8](ch06.xhtml#ch6fig8)
    the separating “plane” is a line. If we had three features, the separating plane
    would be a 2D plane. With four features, the separating plane would be three-dimensional,
    and for *n* dimensions the separating plane is *n –* 1 dimensional. Since the
    plane is multidimensional, we refer to it as a *hyperplane* and say that the goal
    of the classifier is to separate the training feature space into groups using
    hyperplanes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类器可以被视为找到一个或多个*平面*，将训练数据的空间分成同质的组。在[图6-8](ch06.xhtml#ch6fig8)中，分隔的“平面”是直线。如果我们有三个特征，分隔的平面就是一个二维平面。对于四个特征，分隔平面是三维的，对于*n*个维度，分隔平面是*n
    - 1*维的。由于平面是多维的，我们称其为*超平面*，并且说分类器的目标是使用超平面将训练特征空间划分成不同的组。
- en: If we look again at [Figure 6-8](ch06.xhtml#ch6fig8), we can imagine an infinite
    set of lines that separate the training data into two groups, with all of class
    0 on one side and all of class 1 on the other. Which one do we want to use? Well,
    let’s think a bit about what the position of a line separating the two classes
    implies. If we draw a line more to the right side, just before any of the diamonds,
    we’ll have separated the training data, but only barely so. Recall, we’re using
    the training data as a surrogate for the true distribution of samples of each
    class. The more training data we have, the more faithfully we’ll know that true
    distribution. However, we don’t really know it.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次查看[图6-8](ch06.xhtml#ch6fig8)，我们可以想象有一组无限的直线将训练数据分成两组，一组是类0，另一组是类1。我们想使用哪一条呢？好吧，先让我们思考一下分隔两类的直线的位置意味着什么。如果我们把直线画得更靠右一些，恰好在任何一个菱形之前，我们就能分隔训练数据，但这只是勉强做到的。回想一下，我们使用训练数据来代替每个类的真实样本分布。我们拥有的训练数据越多，我们就越能忠实地了解这个真实分布。然而，我们并不知道它的真实情况。
- en: A new, unknown sample, which must be of class 0 or class 1, will fall somewhere
    on the graph. It’s reasonable to believe that there are class 1 (diamond) samples
    in the wild that will fall even closer to the circles than any of the samples
    in the training set. If the separating line is too close to the diamonds, we run
    the risk of calling valid class 1 samples *class 0* because the separating line
    is too far to the right. We can make a similar claim if we place the separating
    line very close to the class 0 points (circles). Then we run the risk of mislabeling
    class 0 samples as *class 1* (diamonds).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的、未知的样本，必须属于类0或类1，将落在图上的某个位置。可以合理地认为，在实际情况中，可能存在类1（菱形）样本，它们会比训练集中任何样本更接近圆形。如果分隔线靠得太近菱形，我们就有可能将有效的类1样本误判为*类0*，因为分隔线太靠右了。如果我们将分隔线画得非常靠近类0点（圆形），我们也面临着类似的风险：将类0样本误标为*类1*（菱形）。
- en: Therefore, in the absence of more training data, it seems most reasonable to
    choose the separating line that is as far from both classes as possible. This
    is the line that is farthest from the rightmost circles while still being as far
    to the left of the diamonds as possible. This line is the maximal margin location,
    where the *margin* is defined as the distance from the closest sample points.
    [Figure 6-9](ch06.xhtml#ch6fig9) shows the maximal margin location as the heavy
    line with the maximum margin indicated by the two dotted lines. The goal of an
    SVM is to locate the maximum margin position, as this is the location where we
    can be most certain to not misclassify new samples, given the knowledge gained
    from the training set.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在没有更多训练数据的情况下，选择一条尽可能远离两类的分隔线似乎是最合理的选择。这条线是距离最右边的圆形最远，同时尽可能地靠左于菱形。这条线就是最大间隔位置，其中*间隔*定义为从最近的样本点到分隔线的距离。[图6-9](ch06.xhtml#ch6fig9)展示了最大间隔位置，重线表示最大间隔，最大间隔由两条虚线标出。SVM的目标是找到最大间隔位置，因为这是我们最有可能不误分类新样本的位置，前提是我们从训练集获得的知识。
- en: '![image](Images/06fig09.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig09.jpg)'
- en: '*Figure 6-9: The toy dataset of [Figure 6-8](ch06.xhtml#ch6fig8) with the maximal
    margin separating line (heavy) and the maximum margins (dotted)*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-9：[图6-8](ch06.xhtml#ch6fig8)的玩具数据集，展示了最大间隔分隔线（粗线）和最大间隔（虚线）*'
- en: Support Vectors
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 支持向量
- en: Look again at [Figure 6-9](ch06.xhtml#ch6fig9). Notice the four training data
    points on the margin? These are the training samples that define the margin, or,
    in other words, support the margin; hence they are *support vectors*. This is
    the origin of the name *Support Vector Machine*. The support vectors define the
    margin, but how can we use them to locate the margin position? Here is where we’ll
    simplify things a bit to avoid a large amount of complex vector mathematics that
    will only muddy the waters for us. For a more mathematical treatment, see “A Tutorial
    on Support Vector Machines for Pattern Recognition” by Christopher Burges (1998).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 再看一下 [图 6-9](ch06.xhtml#ch6fig9)。注意边界上的四个训练数据点吗？这些就是定义边界的训练样本，换句话说，支持边界的样本；因此，它们是
    *支持向量*。这就是 *支持向量机* 这个名字的由来。支持向量定义了边界，但我们如何使用它们来定位边界的位置呢？这里我们会简化一下，以避免大量复杂的向量数学，免得弄得更加混乱。如果需要更详细的数学处理，可以参见
    Christopher Burges 的 “A Tutorial on Support Vector Machines for Pattern Recognition”（1998）。
- en: Optimization
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化
- en: Mathematically, we can find the maximum margin hyperplane by solving an optimization
    problem. Recall that in an optimization problem, we have a quantity that depends
    on certain parameters, and we want to find the set of parameter values that makes
    the quantity as small or as large as possible.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以通过解决一个优化问题来找到最大边界超平面。回想一下，在优化问题中，我们有一个依赖于某些参数的量，我们希望找到使得该量尽可能小或尽可能大的参数值集合。
- en: In the SVM case, the orientation of the hyperplane can be specified by a vector,
    ![Image](Images/wbar.jpg). There is also an offset, *b*, which we must find. Finally,
    before we can do the optimization, we need to change the way we specify the class
    labels. Instead of using 0 or 1 for *y*[*i*], the label of the *i*-th training
    sample, *x*[*i*], we’ll use –1 or +1\. This will let us define the condition of
    the optimization problem more simply.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SVM 的情况下，超平面的方向可以通过一个向量来指定，![Image](Images/wbar.jpg)。还有一个偏移量 *b*，我们需要找到它。最后，在进行优化之前，我们需要改变指定类标签的方式。我们不再使用
    0 或 1 作为 *y*[*i*]（第 *i* 个训练样本 *x*[*i*] 的标签），而是使用 -1 或 +1。这样可以更简单地定义优化问题的条件。
- en: 'So, mathematically, what we want is to find ![Image](Images/wbar.jpg) and *b*
    so that the quantity ![Image](Images/126equ01.jpg) is as small as possible, given
    that ![Image](Images/126equ02.jpg) for all *y*[*i*] labels and *x*[*i*] training
    vectors in the dataset. This sort of optimization problem is readily solved via
    a technique called *quadratic programming*. (We’re ignoring another important
    mathematical step here: the actual optimization problem solved uses a Lagrangian
    to solve the dual form, but again, we’ll try to avoid muddying the waters too
    much.)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，从数学上讲，我们要做的是找到 ![Image](Images/wbar.jpg) 和 *b*，使得给定所有 *y*[*i*] 标签和 *x*[*i*]
    训练向量的条件下，数量 ![Image](Images/126equ01.jpg) 尽可能小。这个优化问题可以通过一种叫做 *二次规划* 的技术轻松解决。（这里我们忽略了另一个重要的数学步骤：实际的优化问题是通过拉格朗日法则解决对偶形式的，但我们会尽量避免过于复杂化问题。）
- en: The preceding formulation is for a case where the dataset, assumed to have only
    two classes, can be separated by a hyperplane. This is the linearly separable
    case. In reality, as we well appreciate by now, not every dataset can be separated
    this way. So, the full form of the optimization problem includes a fudge factor,
    *C*, which affects the size of the margin found. This factor shows up in the sklearn
    SVM class and needs to be specified to some level. From a practical point of view,
    *C* is a *hyperparameter* of the SVM, a value that we need to set to get the SVM
    to train properly. The right value of *C* is problem dependent. In general, any
    parameter of a model that is not learned by the model but must be set to use the
    model, like *C* for an SVM or *k* for *k*-NN, is a hyperparameter.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式是针对数据集只有两个类并且可以被超平面分割的情况。这是线性可分的情况。实际上，正如我们现在所明白的，并不是每个数据集都能这样分割。所以，优化问题的完整形式包括一个修正因子
    *C*，它影响找到的边界的大小。这个因子出现在 sklearn 的 SVM 类中，并且需要设置到某个水平。从实际的角度来看，*C* 是 SVM 的 *超参数*，是我们需要设置的值，以便
    SVM 正常训练。*C* 的正确值依赖于具体问题。一般来说，模型的任何一个参数如果不是由模型学习到的，而必须在使用模型时设置，比如 SVM 的 *C* 或
    *k*-NN 的 *k*，都称为超参数。
- en: Kernels
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核函数
- en: There’s one more mathematical concept we need to introduce, with suitable hand
    waving. The preceding description is for a linear SVM and uses the training data
    directly (the ![Image](Images/127equ01.jpg)). The nonlinear case maps the training
    data to another space by passing it through a function, typically called ![Image](Images/127equ02.jpg),
    that produces a new version of the training data vector, ![Image](Images/xbar1.jpg).
    The SVM algorithm uses inner products, ![Image](Images/127equ03.jpg), which means
    that the mapped version will use ![Image](Images/127equ04.jpg). In this notation,
    vectors are thought of as a column of numbers so that *T*, the transpose, produces
    a row vector. Then normal matrix multiplication of a 1 × *n* row vector and an
    *n* × 1 column vector will result in a 1 × 1 output, which is a scalar. The inner
    product is typically written as
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个数学概念需要介绍，配合适当的手势说明。前面的描述适用于线性支持向量机，直接使用训练数据（即 ![Image](Images/127equ01.jpg)）。非线性情况将训练数据通过一个函数映射到另一个空间，通常这个函数被称为
    ![Image](Images/127equ02.jpg)，它生成训练数据向量的新版本，! [Image](Images/xbar1.jpg)。支持向量机算法使用内积，!
    [Image](Images/127equ03.jpg)，这意味着映射版本将使用 ![Image](Images/127equ04.jpg)。在这种符号表示中，向量被看作是一个列向量，因此
    *T*（转置操作）会产生一个行向量。然后，1 × *n* 的行向量与 *n* × 1 的列向量进行常规矩阵乘法，结果是一个 1 × 1 的输出，即标量。内积通常写作
- en: '![image](Images/127equ05.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/127equ05.jpg)'
- en: and the function ![Image](Images/127equ06.jpg) is called a *kernel*. The linear
    kernel is simply ![Image](Images/127equ07.jpg), but other kernels are possible.
    The *Gaussian kernel* is a popular one, also known as a *radial basis function
    (RBF)* kernel. In practical use, this kernel introduces a new parameter, apart
    from *C*, which is *γ*. This parameter relates to how spread out the Gaussian
    kernel is around a particular training point, with smaller values extending the
    range of influence of the training sample. Typically, one uses a grid search over
    *C* and, if using the RBF kernel, *γ*, to locate the best performing model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数 ![Image](Images/127equ06.jpg) 被称为 *核函数*。线性核函数简单地表示为 ![Image](Images/127equ07.jpg)，但也可以使用其他核函数。*高斯核函数*
    是一种常见的选择，也被称为 *径向基函数（RBF）* 核函数。在实际使用中，除了 *C* 参数外，该核函数还引入了一个新参数，即 *γ*。这个参数与高斯核函数在特定训练点周围的扩展程度相关，较小的值会扩展训练样本的影响范围。通常，使用
    *C* 的网格搜索，并在使用 RBF 核函数时，搜索 *γ* 参数，以定位表现最佳的模型。
- en: To summarize, then, a Support Vector Machine uses the training data, mapped
    through a kernel function, to optimize the orientation and location of a hyperplane
    that produces the maximum margin between the hyperplane and the support vectors
    of the training data. The user needs to select the kernel function and associated
    parameters like *C* and *γ* so that the model best fits the training data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，支持向量机（Support Vector Machine）使用训练数据，通过核函数映射，来优化超平面的方向和位置，从而在超平面与训练数据的支持向量之间产生最大间隔。用户需要选择核函数和相关参数，如
    *C* 和 *γ*，以使模型最适合训练数据。
- en: Support Vector Machines dominated machine learning in the 1990s and early 2000s,
    before the advent of deep learning. This is because they’re trained efficiently
    and don’t need extensive computational resources to be successful. Since the arrival
    of deep learning, however, SVMs have fallen somewhat by the wayside because powerful
    computers have enabled neural networks to do what previously was not possible
    with more limited computing resources. Still, SVMs have a place at the table.
    One popular approach uses a large neural network trained on a particular dataset
    as a preprocessor for a different dataset with an SVM trained on the output of
    the neural network (minus the top layers).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机在1990年代和2000年代初期主导了机器学习，直到深度学习的出现。之所以如此，是因为支持向量机训练高效，并且不需要大量的计算资源就能取得成功。然而，自从深度学习的到来后，支持向量机略微被边缘化了，因为强大的计算机使得神经网络能够做以前在计算资源有限的情况下无法做到的事情。尽管如此，支持向量机仍然有一席之地。一种流行的方法是使用在特定数据集上训练的大型神经网络，作为对不同数据集的预处理器，并将支持向量机应用于神经网络输出（去除顶层层次后的部分）。
- en: Summary
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we introduced six of the most common classic machine learning
    models: Nearest Centroid, *k*-NN, Näive Bayes, Decision Tree, Random Forest, and
    SVMs. These models are classic because they have been used for decades. They are
    also still relevant if the conditions they support best are present. At times,
    the classic model is still the correct choice. An experienced machine learning
    practitioner will know when to fall back to the classics.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了六种最常见的经典机器学习模型：最近质心法、*k*-近邻法、朴素贝叶斯、决策树、随机森林和支持向量机（SVM）。这些模型之所以被称为经典，是因为它们已经使用了几十年。如果满足它们最佳支持的条件，这些模型依然具有相关性。有时，经典模型仍然是正确的选择。一位经验丰富的机器学习从业者会知道何时回归经典方法。
- en: In the next chapter, we’ll use each of these models, via sklearn, to perform
    a number of experiments that will build our intuition of how the models work and
    when to use them.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过使用`sklearn`库，利用这些模型进行一系列实验，从而加深我们对模型工作原理的直觉理解，并帮助我们判断何时使用它们。
