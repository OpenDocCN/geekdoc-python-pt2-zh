- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Caches
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: This chapter introduces *caches*, data structures that seek to alleviate high
    data access costs by storing some data closer to the computation. As we have seen
    throughout the previous chapters, the cost of data access is a critical factor
    in the efficiency of our algorithms. This is important not just to how we organize
    the data in storage but also to the type of storage we use. We say data is *local*
    when it is stored closer to the processor and thus quickly available for processing.
    If we copy some of the data from more expensive, distant locations to local storage,
    we can read the data significantly faster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了*缓存*，一种数据结构，通过将一些数据存储在离计算更近的位置来缓解高数据访问成本。正如我们在前几章中所看到的，数据访问的成本是我们算法效率的一个关键因素。这不仅与我们如何组织存储中的数据有关，还与我们使用的存储类型有关。当数据存储得离处理器更近时，我们称之为*本地*数据，这样可以更快地进行处理。如果我们将一些数据从更昂贵、距离较远的位置复制到本地存储中，我们就可以显著加快数据的读取速度。
- en: For example, we might use caches to accelerate access to web pages. Loading
    a web page consists of querying a server for the information contained on the
    page, transferring that data to your local computer, and rendering this information
    visually. If the web page contains large elements, such as images or videos, we
    may need to transfer a lot of data. To reduce this cost, browsers cache commonly
    accessed data. Instead of redownloading the logo of our favorite site every time
    we access the page, the browser keeps this image in a local cache on the computer’s
    hard drive so that it can quickly read it from disk.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能会使用缓存来加速网页访问。加载一个网页的过程包括查询服务器获取页面上的信息，将这些数据传输到本地计算机，并将其以视觉形式呈现。如果网页包含大的元素，例如图片或视频，我们可能需要传输大量数据。为了减少这一成本，浏览器会缓存常用的数据。这样，我们每次访问网页时，不必重新下载我们最喜欢网站的logo，浏览器会将这个图片保存在本地缓存中，以便从硬盘快速读取。
- en: How should we choose which data to load into our cache? Obviously, we can’t
    store everything. If we could, we wouldn’t need the cache in the first place—we’d
    just copy the entire data set into the closest memory. There are a range of caching
    strategies whose effectiveness depends, as always, on the use case. In this chapter,
    we combine data structures we’ve previously explored to build one such strategy,
    the least recently used (LRU) cache, which can greatly facilitate the operations
    of our favorite coffee shop. We also briefly discuss a few other caching strategies
    for comparison.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何选择将哪些数据加载到缓存中呢？显然，我们不可能将所有东西都存储在缓存里。如果能这样做，我们根本不需要缓存——我们只需将整个数据集复制到最近的内存中。本章中，我们将结合之前探索过的数据结构，构建一种缓存策略——最少最近使用（LRU）缓存，它能大大提高我们最爱的咖啡店的运营效率。同时，我们也将简要讨论几种其他的缓存策略进行对比。
- en: Introducing Caches
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存介绍
- en: Up to this point, we’ve treated all of the computer’s storage equally, a single
    bookshelf from which we can grab any item we need with approximately the same
    amount of effort. However, data storage is not this straightforward. We can think
    of the storage as being like a large, multistory library. We have shelves of popular
    books arranged near the entrance to satisfy the majority of patrons, while we
    relegate the musty stacks of old computer science journals to the basement. We
    might even have a warehouse offsite where rarely used books are stored until a
    patron specifically requests them. Want that resource on the PILOT programming
    language? Odds are, it won’t be in the popular picks section.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们把计算机的所有存储都视作相同的，像一个单一的书架，我们可以从中取出任何需要的物品，所需的努力大致相同。然而，数据存储并非如此简单。我们可以将存储想象成一个大型的多层图书馆。热门书籍的书架位于入口附近，以满足大多数顾客的需求，而那些发霉的老旧计算机科学期刊则被放置在地下室。我们甚至可能有一个远离现场的仓库，专门存储那些很少使用的书籍，直到顾客特意要求它们。想要找到关于PILOT编程语言的资料？很可能它不会出现在热门推荐区。
- en: In addition to paying attention to how we organize data within our program,
    we need to consider *where* the data is stored. Not all data storage is equal.
    In fact, for different mediums, there is often a tradeoff among the size of storage
    available, its speed, and its cost. Memory on the CPU itself (registers or local
    caches) is incredibly fast but can only hold a very limited amount of data. A
    computer’s random-access memory (RAM) provides a larger space at slower speeds.
    Hard drives are significantly larger, but also slower than RAM. Calling out to
    a network allows access to a huge range of storage, such as the whole internet,
    but incurs corresponding overhead. When dealing with very large data sets, it
    might not be possible to load the entirety of the data into memory, which can
    have dramatic impact on the algorithms’ performance. It’s important to understand
    how these tradeoffs impact the performance of algorithms.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了注意我们如何在程序中组织数据外，我们还需要考虑*数据存储的位置*。并非所有的数据存储方式都是相同的。实际上，对于不同的介质，存储容量、速度和成本之间通常存在权衡。CPU
    内存（寄存器或本地缓存）非常快速，但只能存储非常有限的数量。计算机的随机存取存储器（RAM）提供了更大的存储空间，但速度较慢。硬盘的存储量比 RAM 大得多，但比
    RAM 慢得多。调用网络可以访问海量存储，例如整个互联网，但也会带来相应的开销。处理非常大的数据集时，可能无法将整个数据集加载到内存中，这会对算法的性能产生重大影响。理解这些权衡如何影响算法性能非常重要。
- en: Compare this to our own morning coffee-making routine. While it would be ideal
    to have thousands of varieties at our fingertips, our apartment can store only
    a limited amount of coffee. Down the street is a coffee distributor stocked with
    hundreds of varieties, but do we really want to walk there every time we want
    to make a new cup of coffee? Instead, we store small quantities of our preferred
    coffees in our apartment. This local storage speeds up the production of our morning
    brew and lets us enjoy our first cup before venturing out into the world.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们自己的早晨咖啡制作过程进行对比。虽然理想情况下我们可以随时得到成千上万种咖啡，但我们的公寓只能存储有限量的咖啡。街角有一家咖啡分销商，提供数百种咖啡品种，但我们真的想每次做咖啡时都走到那里去吗？相反，我们将少量我们喜欢的咖啡存放在公寓里。这种本地存储加快了我们早晨咖啡的制作速度，让我们在走出家门之前就能享受到第一杯咖啡。
- en: Caches are a step before accessing expensive data, as shown in [Figure 11-1](#figure11-1).
    Before calling out to a remote server or even accessing our local hard drive,
    we check whether we already have the data stored locally in the cache. If we do,
    we access the data directly, and cheaply, from the cache. When we find the data
    in the cache, that’s a *cache hit*. We can stop the lookup and avoid calling out
    to the more expensive storage. If the data isn’t in the cache, we have a *cache
    miss*. We give a defeated sigh and proceed with the more expensive access.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是访问昂贵数据之前的一个步骤，如[图 11-1](#figure11-1)所示。在调用远程服务器或访问本地硬盘之前，我们会检查数据是否已被存储在本地缓存中。如果有，我们直接从缓存中快速、便宜地访问数据。当我们在缓存中找到数据时，这就是一次*缓存命中*。我们可以停止查找，避免访问更昂贵的存储。如果数据不在缓存中，那就是*缓存未命中*。我们叹息一声，然后继续执行更昂贵的访问。
- en: '![The cache sits between the algorithm and expensive data store. Arrows labeled
    “query” point from the algorithm to the cache and from the cache to the data store.
    Arrows labeled “reply” point the other direction.](image_fi/502604c11/f11001.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![缓存位于算法和昂贵数据存储之间。标有“查询”的箭头指向缓存和数据存储，标有“回复”的箭头指向相反方向。](image_fi/502604c11/f11001.png)'
- en: 'Figure 11-1: A cache sits between the algorithm and a slow, expensive data
    store.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-1：缓存位于算法和缓慢、昂贵的数据存储之间。
- en: Picture this in the context of a busy coffee counter. The daily menu contains
    10 coffees ranging in popularity from the mega-popular house brew to the rarely
    purchased mint-cinnamon-pumpkin explosion. At the far end of the counter sits
    a coffee station with 10 pots of coffee, each on its own heater. Upon receiving
    an order, the barista walks to the coffee station and fills a cup with the appropriate
    coffee. After miles of walking each day, the barista asks the owner to install
    another heater right beside the register so they can give their tired feet a rest.
    The owner agrees but asks, “Which coffee will you keep near the register?” This
    question weighs on the barista’s mind and they try a few strategies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 试想一下在一个繁忙的咖啡柜台上的场景。每日菜单包含10种咖啡，从极受欢迎的常规咖啡到很少有人购买的薄荷肉桂南瓜爆炸。柜台的另一端是一个咖啡站，站上有10个咖啡壶，每个壶上都有加热器。当收到订单时，咖啡师走到咖啡站，拿起适当的咖啡装入杯中。每天走了几英里后，咖啡师请求老板在收银台旁安装另一个加热器，这样他们可以让疲惫的双脚得到休息。老板同意了，但问道：“你打算将哪种咖啡放在收银台旁？”这个问题让咖啡师陷入沉思，他们尝试了一些策略。
- en: The barista quickly realizes that it isn’t just a question of which coffee,
    but also when they should change it. They can store different coffees locally
    at different points of the day. They try keeping the house brew in the cache all
    day, moving the decaf closer at night, and swapping out the current selection
    for the house brew when there hasn’t been a customer in the last 10 minutes. Some
    strategies work and others, especially anything involving caching mint-cinnamon-pumpkin
    explosion, fail miserably. If the barista chooses correctly, most orders result
    in a cache hit, and they can avoid the long walk. If they choose incorrectly,
    the local coffee is useless for most orders.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 咖啡师很快意识到，这不仅仅是选择哪种咖啡的问题，还要考虑何时该更换咖啡。他们可以根据一天中的不同时间在本地存储不同的咖啡。他们尝试将常规咖啡保持在缓存中整天，晚上将低咖啡因咖啡放得更近，当过去10分钟内没有顾客时，则将当前选择更换为常规咖啡。一些策略有效，而其他的，尤其是涉及缓存薄荷肉桂南瓜爆炸的策略，失败得很惨。如果咖啡师做出正确的选择，大多数订单会导致缓存命中，他们可以避免长时间走路。如果选择错误，本地咖啡对大多数订单都没用。
- en: The barista could further improve the situation by installing a second heater
    by the register. Instead of storing a single flavor nearby, they can now store
    two. The cost, however, is that this second heater uses up precious counter real
    estate. Both computers and coffee shops have a limited amount of local space.
    We cannot store every coffee next to the register, and we cannot store the entire
    internet in our computer’s RAM. If we make the cache too large, we will need to
    use slower storage for it. This is the fundamental tradeoff with a cache—memory
    used versus the speedup provided.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 咖啡师可以进一步改善情况，在收银台旁安装第二个加热器。这样他们就可以不只存储一种咖啡口味，而是存储两种。然而，代价是这个第二个加热器占用了宝贵的柜台空间。无论是计算机还是咖啡店，都有有限的本地空间。我们不能把所有的咖啡都存放在收银台旁，也不能将整个互联网存储在电脑的RAM中。如果我们把缓存做得太大，就需要使用更慢的存储来存储它。这就是缓存的基本权衡——使用的内存与提供的加速之间的权衡。
- en: When the cache fills up, we determine which data to keep and which data to replace.
    We say the replaced data is *evicted* from the cache. Perhaps the triple-caffeinated
    blend replaces the decaf during a fall morning rush. By evicting the decaf from
    the nearby heater, the barista saves hours of walking to retrieve the trendier
    blend. Different caching approaches use different eviction strategies to determine
    which data to replace, from counting how often the data has been accessed thus
    far to making predictions about whether the data will be used again in the near
    future.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当缓存满了，我们需要确定保留哪些数据，替换掉哪些数据。我们说被替换的数据是*从缓存中驱逐*出去的。也许在秋季早高峰时，三倍咖啡因的混合咖啡替代了低咖啡因咖啡。通过将低咖啡因咖啡从附近的加热器中驱逐，咖啡师节省了几个小时的路程去取更时髦的混合咖啡。不同的缓存方法使用不同的驱逐策略来确定替换哪些数据，从统计数据被访问的次数到预测这些数据是否会在不久的将来再次被使用。
- en: LRU Eviction and Caches
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LRU 驱逐和缓存
- en: The *least recently used (LRU) cache* stores recently used information. LRU
    caches are a simple and common approach that provide a good illustration of the
    types of tradeoffs we face in using caches. The intuition behind this caching
    strategy is that we are likely to re-access information that we recently needed,
    which fits well with our web-browsing example above. If we commonly access the
    same sets of pages over and over, it pays to store some of the (unchanging) elements
    of these pages locally. We don’t need to re-request the site’s logo with each
    visit. LRU caches consist of a set amount of storage filled with the most recently
    accessed items. The cache starts evicting older items—those least recently used—when
    the cache is full.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近最少使用（LRU）缓存*存储最近使用的信息。LRU缓存是一种简单而常见的做法，它很好地展示了我们在使用缓存时面临的各种权衡。这个缓存策略的直觉是：我们很可能会重新访问最近需要的信息，这与我们上面提到的浏览网页的例子非常契合。如果我们反复访问相同的一组网页，保存这些网页的一些（不变的）元素在本地是有好处的。我们不需要每次访问时重新请求网站的logo。LRU缓存由一组存储空间组成，存放最近访问的项目。当缓存满时，它会开始逐出较旧的项目——那些最近最少使用的项目。'
- en: If our barista decides to treat the heater closest to the register as an LRU
    cache, they keep the last ordered coffee on that heater. When they receive an
    order for something different, they take the current cached coffee back to the
    coffee station, place it on the heater there, and retrieve the new coffee. They
    bring this new coffee back to the register, use it to fulfill the order, and leave
    it there.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的咖啡师决定将靠近收银台的加热器当作LRU缓存，他们会将最近点的咖啡放在那个加热器上。当他们收到不同的订单时，他们会将当前缓存的咖啡拿回咖啡站，放在那里的加热器上，并取回新咖啡。然后，他们将这杯新咖啡带回收银台，用来完成订单，并将其留在那里。
- en: 'If every customer orders something different, this strategy just adds overhead.
    Instead of walking to the end of the counter with a cup, the barista is now making
    the same journey with a pot of coffee—probably complaining under their breadth
    about the length of the counter or resentfully judging customers’ tastes: “Who
    orders mint-cinnamon-pumpkin explosion?” However, if most customers order in clumps
    of similar coffees, this strategy can be hugely effective. Three customers ordering
    regular coffee in a row means two round trips saved. The advantage can compound
    with the effect of customer interaction. After seeing the person in front of them
    order the pumpkin concoction, the next customer makes the (questionable) decision
    to copy their predecessor, saying, “I’ll have the same.”'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个顾客点的都是不同的东西，那么这个策略就只会增加开销。咖啡师不再是拿着一杯咖啡走到柜台的尽头，而是拿着一壶咖啡走同样的路——可能会在心里抱怨柜台太长，或者在心里对顾客的口味进行评判：“谁会点薄荷肉桂南瓜爆炸味？”然而，如果大多数顾客点的咖啡相似，那么这个策略就会非常有效。三位顾客连续点普通咖啡就意味着节省了两趟来回的时间。随着顾客之间的互动效果，这个优势还会得到进一步的放大。在看到前面的人点了南瓜口味的混合咖啡后，下一位顾客做出了（值得怀疑的）决定，决定复制前者的选择，说：“我也要一样的。”
- en: This is exactly the scenario we can run into when browsing our favorite website.
    We move from one page of the site to another page of the same site. Certain elements,
    such as logos, will reappear on subsequent pages, leading to a significant savings
    from keeping those items in the cache.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在浏览自己最喜欢的网站时可能遇到的情况。我们从网站的一个页面跳转到同一网站的另一个页面。一些元素，如logo，会在后续页面中重复出现，因此将这些元素保存在缓存中可以带来显著的节省。
- en: Building an LRU Cache
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建LRU缓存
- en: 'Caching with the LRU eviction policy requires us to support two operations:
    looking up an arbitrary element and finding the least recently used element (for
    eviction). We must be able to perform both operations quickly. After all, the
    purpose of a cache is to accelerate lookups. If we have to scan through an entire
    unstructured data set to check for a cache hit, we are more likely to add overhead
    than to save time.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LRU逐出策略的缓存要求我们支持两个操作：查找任意元素和找到最近最少使用的元素（用于逐出）。我们必须能够快速执行这两个操作。毕竟，缓存的目的是加速查找。如果我们必须扫描整个未结构化的数据集来检查缓存是否命中，那么我们更可能增加开销而不是节省时间。
- en: 'We can construct an LRU cache using two previously explored components: a hash
    table and a queue. The hash table allows us to perform fast lookups, efficiently
    retrieving any items that are in the cache. The queue is a FIFO data structure
    that allows us to track which item hasn’t been used recently. Instead of scanning
    through each item and checking the timestamp, we can dequeue the *earliest* item
    in the queue, as shown in the following code. This provides an efficient way to
    determine which data to remove.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两个之前探讨过的组件来构建一个LRU缓存：哈希表和队列。哈希表使我们能够快速查找，高效地检索缓存中的任何项。队列是一个先进先出（FIFO）的数据结构，它允许我们追踪哪些项最近没有被使用。我们可以通过将队列中*最早*的项出队，而不是扫描每一项并检查时间戳，来高效地确定要移除的数据，如以下代码所示。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each entry in the hash table stores a composite data structure containing at
    least three entries: the key, the corresponding value (or data for that entry),
    and a pointer to the corresponding node in the cache’s queue. This last piece
    of information is essential, because we need a way to look up and modify entries
    in the queue. As in [Listing 11-1](#listing11-1), we store those pieces of information
    directly in the value entry of the hash table’s nodes.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希表中的每个条目都存储一个复合数据结构，其中包含至少三个条目：键、对应的值（或该条目的数据），以及指向缓存队列中相应节点的指针。最后一条信息至关重要，因为我们需要一种方法来查找和修改队列中的条目。正如在[列表
    11-1](#listing11-1)中所示，我们将这些信息直接存储在哈希表节点的值条目中。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Listing 11-1: The data structure for a cache entry containing the data’s key,
    value, and a link to its node in the queue'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11-1：包含数据键、值和链接到队列节点的缓存条目的数据结构
- en: '[Figure 11-2](#figure11-2) shows the resulting diagram of how these pieces
    fit together. The diagram looks a bit complex but becomes easier to understand
    when we break it down into two parts. On the left is the hash table. As in Chapter
    10, each hash value corresponds to a linked list of entries. The value for these
    entries is the `CacheEntry` data structure in [Listing 11-1](#listing11-1). On
    the right-hand side is the queue data structure, which stores the entry’s keys.
    A single pointer links across these data structures, pointing from the `CacheEntry`
    data structure to the queue node with the corresponding key.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-2](#figure11-2)展示了这些组件如何组合在一起的结果图。图看起来有点复杂，但当我们将其拆分成两部分时会变得更容易理解。左侧是哈希表。如同第10章所述，每个哈希值对应一个链表。链表中条目的值是[列表
    11-1](#listing11-1)中的`CacheEntry`数据结构。右侧是队列数据结构，用来存储条目的键。一个单独的指针连接这些数据结构，将`CacheEntry`数据结构链接到队列中具有相应键的节点。'
- en: Of course, the real-world layout of the components in [Figure 11-2](#figure11-2)
    throughout the computer’s memory is even messier than it appears in the diagram,
    because the nodes do not actually reside next to each other.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算机内存中[图 11-2](#figure11-2)中组件的实际布局比图示看起来更加混乱，因为节点并不真正彼此相邻。
- en: '![A hash table on the left includes an entry for each cache entry with pointers
    to an additional CacheEntry node. Each of the CacheEntry nodes has a single pointer
    that links to an entry in the queue on the right.](image_fi/502604c11/f11002.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![左侧的哈希表包含每个缓存项的条目，并指向一个额外的CacheEntry节点。每个CacheEntry节点有一个指针，指向右侧队列中的一个条目。](image_fi/502604c11/f11002.png)'
- en: 'Figure 11-2: An LRU cache implemented as a combination of a hash table and
    queue'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-2：作为哈希表和队列结合体实现的LRU缓存
- en: In [Listing 11-2](#listing11-2), we define a single lookup function, `CacheLookup`,
    that returns the value for a given lookup key. If there is a cache hit, the lookup
    function returns the value directly and updates how recently that data was accessed.
    If there is a cache miss, the function fetches the data via the expensive lookup,
    inserts it into the cache, and, if necessary, removes the oldest piece of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 11-2](#listing11-2)中，我们定义了一个名为`CacheLookup`的查找函数，该函数返回给定查找键的值。如果是缓存命中，查找函数会直接返回该值，并更新数据最近的访问时间。如果是缓存未命中，函数将通过昂贵的查找获取数据，将其插入缓存中，并在必要时删除最旧的数据。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 11-2: Code for looking up an item by its key'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11-2：根据键查找项的代码
- en: This code starts by checking whether `key` occurs in our cache table ❶. If so,
    we have a cache hit. Otherwise, we have a cache miss.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先检查`key`是否出现在我们的缓存表中 ❶。如果是，我们就得到了缓存命中。否则，我们就是缓存未命中。
- en: We deal with the case of a cache miss first. If the hash table returns `null`,
    we need to fetch the data from the more expensive data store. We store this newly
    retrieved value in the cache, evicting the (oldest) item from the front of the
    queue. We do this in three steps. First, if the cache is full ❷, we dequeue the
    key of the oldest item from the queue and use it to remove the corresponding entry
    in the hash table. This completes the eviction of the oldest item. Second, we
    retrieve the new data ❸. Third, we insert the (`key`, `data`) pair into the cache.
    We enqueue `key` into the back of the queue ❹. Then we create a new hash table
    entry with the new `key`, `data`, and pointer to the key’s corresponding location
    in the queue (using the queue’s `back` pointer). We store this entry in the hash
    table ❺.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先处理缓存未命中的情况。如果哈希表返回`null`，我们需要从更昂贵的数据存储中获取数据。我们将新检索到的值存储在缓存中，并从队列的前端驱逐（最旧的）项目。我们通过三步操作来完成这项工作。首先，如果缓存已满❷，我们从队列中出队最旧项的键，并使用它从哈希表中删除相应的条目。这样就完成了最旧项的驱逐。第二步，我们检索新的数据❸。第三步，我们将（`key`，`data`）对插入到缓存中。我们将`key`入队到队尾❹。然后，我们创建一个新的哈希表条目，包含新的`key`、`data`和指向队列中该键相应位置的指针（使用队列的`back`指针）。我们将此条目存储在哈希表中❺。
- en: The final block of code handles the case of a cache hit. When we see a cache
    hit, we want to move the element for this key from its current position to the
    back of the queue. After all, we’ve just seen it. It should now be the last element
    we want to discard. We move this element in two steps. First, we remove it from
    the queue with the `RemoveNode` function, using the pointer to the node ❻. Second,
    we re-enqueue the key at the back of the queue ❼ and update the pointer to that
    queue node ❽.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一段代码处理缓存命中的情况。当我们看到缓存命中时，我们希望将该键的元素从当前位置移动到队列的尾部。毕竟，我们刚刚看过它。现在它应该是我们最后一个想丢弃的元素。我们通过两步来移动该元素。首先，我们使用`RemoveNode`函数将其从队列中删除，使用指向该节点的指针❻。其次，我们将键重新入队到队尾❼，并更新指向该队列节点的指针❽。
- en: We can picture this update operation in the context of a line of customers in
    a coffee shop. If a customer leaves the line, they lose their position. When they
    rejoin the line in the future, they do so at the back.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此更新操作想象成咖啡店排队的场景。如果有顾客离开排队，他们就失去了原本的位置。当他们将来重新加入队伍时，他们会排到队尾。
- en: Updating an Element’s Recency
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新元素的最近性
- en: 'In order to support the `RemoveNode` operation in [Listing 11-2](#listing11-2),
    we need to change our queue to support *updating* an element’s position. We need
    to move recently accessed items from their current position in the queue to the
    back in order to indicate that they were the most recently accessed. First, we
    modify our queue implementation from Chapter 4 to use a doubly linked list, which
    will allow us to efficiently remove items from the middle of the queue:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持在[清单 11-2](#listing11-2)中执行`RemoveNode`操作，我们需要修改队列以支持*更新*元素的位置。我们需要将最近访问的项目从当前队列位置移到队尾，以表明它们是最近访问的。首先，我们将第4章中的队列实现修改为使用双向链表，这将使我们能够高效地从队列中间删除项目：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As illustrated in [Figure 11-3](#figure11-3), the `next` field refers to the
    node directly behind the current node (the next node after this one to be dequeued),
    where the `prev` field refers to the node preceding this one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 11-3](#figure11-3)所示，`next`字段指向当前节点直接后面的节点（下一个出队的节点），而`prev`字段指向当前节点之前的节点。
- en: '![A queue with elements 31, 41, 5, 92, and 65\. Each node contains a single
    number and links to both the next and previous number in the queue.](image_fi/502604c11/f11003.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![包含元素31、41、5、92和65的队列。每个节点包含一个数字，并且链接到队列中下一个和前一个数字。](image_fi/502604c11/f11003.png)'
- en: 'Figure 11-3: A queue implemented as a doubly linked list'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11-3：一个实现为双向链表的队列
- en: 'Second, we modify the enqueue and dequeue operations accordingly. For both
    enqueue and dequeue, we add extra logic to update the previous pointers. The primary
    change from Chapter 4 is in how we set the `prev` pointer:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们相应地修改入队和出队操作。对于入队和出队，我们添加了额外的逻辑来更新前驱指针。与第4章的主要不同在于我们如何设置`prev`指针：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the enqueue operation, the code needs to set the new node’s `prev` pointer
    to the previous last element `q.back` ❶.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在入队操作中，代码需要将新节点的`prev`指针设置为前一个最后元素`q.back`❶。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The dequeue operation sets the `prev` pointer of the new front node to `null`
    to indicate that there is no node in front of it ❶.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 出队操作将新的队头节点的`prev`指针设置为`null`，表示它前面没有节点❶。
- en: Finally, we add the `RemoveNode` operation, which provides the ability to remove
    a node from the middle of our queue. This is where the use of a doubly linked
    list helps. By keeping pointers in both directions, we do not need to scan the
    entire queue to find the entry before the current one. The code for `RemoveNode`
    adjusts the pointers into the node from the adjacent nodes in the linked list
    (`node.prev` and `node.next`), the front of the queue (`q.front`), and the back
    of the queue (`q.back`).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加了`RemoveNode`操作，它提供了从队列中间移除节点的功能。这时双向链表的使用就派上了用场。通过在两个方向上保持指针，我们无需扫描整个队列来找到当前节点前面的节点。`RemoveNode`的代码通过调整来自相邻节点的指针（`node.prev`
    和 `node.next`）、队列前端的指针（`q.front`）和队列后端的指针（`q.back`），来完成这一操作。
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code contains multiple `IF` statements to handle the special cases of adding
    or removing from either end of the queue.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码包含多个`IF`语句，用来处理从队列两端添加或移除元素的特殊情况。
- en: 'One question remains: How can we efficiently find the element we want to remove
    and reinsert? We can’t afford to scan through the entire queue while searching
    for the node to update. Supporting a cache hit requires speedy lookups. In the
    case of our cache above, we maintain a pointer directly from the cache’s entry
    to the element in the queue. This allows us to access, remove, and reinsert the
    element in constant time by following the pointers.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个问题需要解决：我们如何高效地找到我们想要移除和重新插入的元素？我们不能在搜索更新节点时扫描整个队列。支持缓存命中的前提是快速查找。在上面的缓存示例中，我们直接从缓存条目指向队列中的元素。这使得我们能够通过跟随指针，在常数时间内访问、移除和重新插入元素。
- en: Other Eviction Strategies
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他驱逐策略
- en: Let’s consider how three of the many alternate eviction strategies—most recently
    used, least frequently used, and predictive eviction—compare to LRU eviction.
    The goal isn’t to provide an in-depth analysis of these methods but rather to
    help develop your intuition about the types of tradeoffs that arise when picking
    a caching strategy. The optimal caching strategy for a particular scenario will
    depend on the specifics of that scenario. Understanding the tradeoffs will help
    you pick the best strategy for your use case.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑三种替代的驱逐策略——最近使用、最少使用和预测驱逐——与LRU驱逐的比较。目标并不是深入分析这些方法，而是帮助你培养对选择缓存策略时所涉及的各种权衡的直觉。对于特定情境，最优的缓存策略将取决于该情境的具体情况。了解这些权衡将帮助你为你的使用场景选择最佳策略。
- en: LRU is a good eviction strategy when we see bursts of common usage among certain
    cache items but expect the distribution of cached items to change over time. The
    local pot of coffee near the register described earlier is an excellent example.
    When someone orders a new type of coffee, the barista walks to the end of the
    counter, returns the old blend, retrieves the new blend, and leaves that carafe
    by the register.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LRU 是一种有效的驱逐策略，适用于当我们看到某些缓存项的使用频繁出现突增，但预计缓存项的分布会随时间变化的情况。前面提到的靠近收银台的本地咖啡壶就是一个很好的例子。当有人点了一种新类型的咖啡时，咖啡师会走到柜台的另一端，拿走旧的咖啡混合物，取回新的混合物，并把咖啡壶放在收银台旁边。
- en: Let’s contrast that with evicting the *most recently used (MRU)* element. You
    can picture this in the context of an ebook reader that prefetches and caches
    books that you might enjoy. Since it is unlikely that we will read a book twice
    in a row, it might make sense to discard the recently completed book from our
    cache in order to free up space for a new one. While MRU can be a good approach
    for items with infrequent repetition, the same eviction would create constant
    tragedy in our coffee pantry. Imagine if we were forced to evict our favorite,
    go-to blend anytime we brewed a cup.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与逐出*最近使用（MRU）*元素进行对比。在电子书阅读器的情境下，可以想象它预取并缓存你可能喜欢的书籍。由于我们不太可能连续两次读同一本书，因此可能会合理地从缓存中丢弃最近阅读完的书籍，以腾出空间放入新的书籍。虽然
    MRU 对于重复出现频率不高的项目来说是一种有效的方法，但这种驱逐策略会在我们的咖啡库中带来持续的麻烦。想象一下，如果每次我们煮咖啡时都被迫驱逐掉我们最喜欢的咖啡混合物，那该有多糟。
- en: Instead of considering *when* the item was last accessed, we could also track
    how many times it has been accessed in total. The *least frequently used (LFU)*
    eviction strategy discards the element with the smallest count. This approach
    is advantageous when our cached population remains stable, such as with the tried-and-true
    coffee varieties we keep at home. We don’t evict one of our three go-to varieties
    simply because we recently tried the seasonal blend at our local coffeehouse.
    The items in the cache have a proven track record and are there to stay—at least,
    until we find a new favorite. Unfortunately, when preferences change, it can take
    a while for newly popular items to accumulate enough counts to make it into our
    cache. If we encounter a new brew that is better than anything in our home collection,
    we’d need to sample individual cups of the coffee many times, making numerous
    trips to the coffee shop, before finally buying beans for our own pantry.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不考虑*最后一次访问*的时间，而是跟踪该项总共被访问了多少次。*最少频繁使用（LFU）*驱逐策略会丢弃访问次数最少的元素。当我们的缓存数据保持稳定时，这种方法非常有效，比如我们家里常备的经典咖啡品种。我们不会因为最近在当地咖啡店尝试了季节性咖啡豆就把三种常用的咖啡豆中的一种淘汰掉。缓存中的物品有着良好的历史记录，且通常会留在缓存中——至少在我们找到新的最爱之前。然而，当偏好发生变化时，新出现的热门物品可能需要一段时间才能积累足够的访问次数进入缓存。如果我们遇到一种比家里所有咖啡豆都更好的新咖啡，我们可能需要反复品尝许多杯咖啡，进行多次到咖啡店的拜访，才最终会为家里购买该咖啡豆。
- en: The *predictive evictio**n* strategy provides a forward-looking approach that
    tries to predict what elements will be needed in the future. Instead of relying
    on simple counts or timestamps, we can build a model to predict what cache entries
    are most likely to be accessed in the future. The effectiveness of this cache
    depends on the accuracy of the model. If we have a highly accurate model, such
    as predicting that we will switch from our go-to java blends to a seasonal fall
    blend only in October and November, we greatly improve the hit rate. However,
    if the model is inaccurate, perhaps associating each month with the first coffee
    we consumed that month last year, we can find ourselves reliving that onetime
    mistake of drinking mint-cinnamon-pumpkin lattes in August. Another downside of
    predictive eviction is that it adds complexity to the cache itself. It is no longer
    sufficient to track simple counts or timestamps; instead, the cache must learn
    a model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*预测驱逐*策略提供了一种前瞻性的方式，尝试预测未来需要哪些元素。我们可以构建一个模型，预测哪些缓存条目最有可能在未来被访问，而不是依赖简单的计数或时间戳。如果我们有一个高度准确的模型，比如预测我们只会在十月和十一月之间从常用的咖啡豆切换到季节性的秋季咖啡豆，我们就可以大大提高命中率。然而，如果模型不准确，可能会错误地将每个月与去年那个时候我们喝的第一杯咖啡关联起来，这样我们可能会发现自己在八月再次犯下那个错误，喝着薄荷肉桂南瓜拿铁。预测驱逐的另一个缺点是，它增加了缓存本身的复杂性。现在，仅仅跟踪简单的计数或时间戳已经不够了；相反，缓存必须学习一个模型。'
- en: Why This Matters
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么这很重要
- en: Caches can alleviate some of the access cost when dealing with expensive storage
    mediums. Instead of calling out to the expensive storage, caches store a portion
    of this data in a closer and faster location. If we choose the correct caching
    strategy, we can save significant amounts of time by pulling data from the cache
    instead of the slower locations. This makes caches a common and powerful tool
    in our computational toolbox. They are used throughout the real world, from web
    browsers to library shelves to coffee shops.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存可以在处理昂贵的存储介质时减轻一些访问成本。缓存将部分数据存储在更接近且更快速的地方，而不是每次都调用昂贵的存储。如果我们选择正确的缓存策略，就可以通过从缓存中获取数据而非从较慢的位置提取数据，节省大量时间。这使得缓存成为我们计算工具箱中常见且强大的工具。它们在现实世界中得到了广泛应用，从网页浏览器到图书馆书架，再到咖啡店。
- en: Caches also illustrate several key concepts. First, they highlight the potential
    tradeoffs to consider when accessing data from different mediums. If we could
    store everything in the fastest memory, we wouldn’t need caches. Unfortunately,
    this usually isn’t possible; our algorithms will need to access larger, slower
    data stores, and it’s worth understanding the potential cost this adds. In the
    next chapter, we introduce the B-tree, a tree-based data structure that reduces
    the number of data accesses needed. This optimization helps reduce the overall
    cost when we cannot fit our data in nearby, fast memory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存还展示了几个关键概念。首先，它们突出了在从不同媒介访问数据时需要考虑的潜在权衡。如果我们能将所有数据存储在最快的内存中，就不需要缓存了。不幸的是，这通常不可行；我们的算法将需要访问更大、更慢的数据存储，因此理解这一过程可能带来的成本是很有意义的。在下一章中，我们将介绍B树，一种基于树的数据结构，它减少了所需的数据访问次数。这种优化有助于降低在数据无法存放在附近的快速内存中时的整体成本。
- en: Second, caches revisit the data structure–tuning question that we saw in previous
    chapters. Both the size of the cache and the eviction strategy are parameters
    that can have massive impact on your cache’s performance. Consider the problem
    of selecting the size of the cache. If the cache is too small, it might not store
    enough to provide a benefit. The single nearby pot of coffee in our café only
    helps so much. On the other hand, too large a cache will take important memory
    resources that might be needed for other parts of our algorithm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，缓存重新审视了我们在前几章中看到的数据结构调整问题。缓存的大小和驱逐策略都是可能对缓存性能产生巨大影响的参数。考虑选择缓存大小的问题。如果缓存太小，它可能无法存储足够的数据以提供好处。我们咖啡馆里那只单独的咖啡壶帮忙也有限。另一方面，缓存过大将占用重要的内存资源，这些资源可能会被算法的其他部分所需要。
- en: Finally, and most importantly for this book’s purposes, caches illustrate how
    we can combine basic data structures, such as the hash table and the queue, to
    provide more complex and impactful behaviors. In this case, by using pointers
    to tie hash table entries to nodes in a queue, we can efficiently track which
    entry in our cache should be removed next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也是对本书最重要的一点，缓存展示了我们如何将基本数据结构，如哈希表和队列，结合起来以提供更复杂和更具影响力的行为。在这种情况下，通过使用指针将哈希表条目与队列中的节点连接起来，我们可以高效地跟踪缓存中下一个应被移除的条目。
