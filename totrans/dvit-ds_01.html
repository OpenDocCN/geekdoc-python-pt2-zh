<html><head></head><body>
<div id="sbo-rt-content"><section>
<header>
<h1 class="chapter">
<span class="ChapterNumber"><span epub:type="pagebreak" id="Page_1" title="1"/>1</span><br/>
<span class="ChapterTitle">Exploratory Data Analysis</span></h1>
</header>
<figure class="opener">
<img alt="" height="200" src="image_fi/book_art/chapterart.png" width="200"/>
</figure>
<p class="ChapterIntro">This is a data science book, so let’s start by diving into some data. This is something you should get used to: the first step of every data science problem is exploring your data. Looking closely at its minutest details will help you understand it better and give you clearer ideas for next steps and more sophisticated analyses. It will also help you catch any errors or problems with your data as early as possible. These first steps in the data science process are referred to as <em>exploratory data analysis</em>.</p>
<p>We’ll start this first chapter by introducing a business scenario and describing how data might be used to better run a business. We’ll talk about reading data in Python and checking basic summary statistics. We’ll <span epub:type="pagebreak" id="Page_2" title="2"/>then introduce some Python tools to create plots of data. We’ll go over simple exploratory analyses we can perform and talk about the questions they can answer for us. Finally, we’ll close with a discussion of how the analyses can help us improve business practices. The simple analyses that we’ll do in this chapter are the same types of analyses that you can do as a first step of working on any data science problem you ever encounter. Let’s begin!</p>
<h2 id="h1-502888c01-0001">Your First Day as CEO</h2>
<p class="BodyFirst">Imagine you receive a job offer to be the CEO of a company in Washington, DC, that provides bicycles that people can rent for short periods to ride around the city. Even though you don’t have any experience running bike-sharing companies, you accept the offer.</p>
<p>You show up to work on your first day and think about your business goals as a CEO. Some of the goals you might think about could be related to issues like customer satisfaction, employee morale, brand recognition, market share maximization, cost reduction, or revenue growth. How can you decide which of these goals you should pursue first, and how should you pursue it? For example, think about increasing customer satisfaction. Before focusing on that goal, you’d need to find out whether your customers are satisfied and, if not, discover what’s making satisfaction suffer and how it can be improved. Or suppose you’re more interested in working to increase revenue. You would need to know what your revenue is right now before figuring out how it could be increased. In other words, you won’t be able to choose your initial focus until you better understand your company.</p>
<p>If you want to understand your company, you need data. You might try to look at charts and reports that summarize your company’s data, but no prepared report can tell you as much as you’ll learn by diving into the data yourself.</p>
<h3 id="h2-502888c01-0001">Finding Patterns in Datasets</h3>
<p class="BodyFirst">Let’s look at some data from a real bike-sharing service and imagine that this is data from your company. You can download this data from <a class="LinkURL" href="https://bradfordtuckfield.com/hour.csv">https://bradfordtuckfield.com/hour.csv</a>. (This file is in a special format called <em>.csv</em>, which we’ll talk more about soon.) You can open this file in a spreadsheet editor like Microsoft Excel or LibreOffice Calc; you should see something that looks like <a href="#figure1-1" id="figureanchor1-1">Figure 1-1</a>.</p>
<aside epub:type="sidebar">
<div class="top hr"><hr/></div>
<section class="note">
<h2><span class="NoteHead">NOTE</span></h2>
<p>	The original source of the bike-sharing data is Capital Bikeshare (<a class="LinkURL" href="https://ride.capitalbikeshare.com/system-data">https://ride.capitalbikeshare.com/system-data</a>). The data was compiled and augmented by Hadi Fanaee-T and Joao Gama and posted online by Mark Kaghazgarian.</p>
<div class="bottom hr"><hr/></div>
</section>
</aside>
<span epub:type="pagebreak" id="Page_3" title="3"/><figure>
<img alt="" class="keyline" height="390" src="image_fi/502888c01/f01001.png" width="694"/>
<figcaption><p><a id="figure1-1">Figure 1-1</a>: Bike-sharing data, viewed in a spreadsheet</p></figcaption>
</figure>
<p>This dataset is no different from many other datasets that you’ve probably seen before: a rectangular array of rows and columns. In this dataset, each row represents information about a particular hour between midnight on January 1, 2011, and 11:59 <span class="KeyCaps">PM</span> on December 31, 2012—more than 17,000 hours total. The rows are arranged in order, so the first few rows give us information about the first few hours of 2011, and the last few rows relate to the last few hours of 2012.</p>
<p>Each column contains a particular metric that has been measured for each of these hours. For example, the <code>windspeed</code> column gives us hourly measurements of wind speed at a particular weather-recording station in Washington, DC. Notice that this measurement isn’t in familiar units like miles per hour. Instead, the measurements have been transformed so that they’re always between 0 and 1; all we need to know is that 1 represents a fast wind speed and 0 represents no wind.</p>
<p>If you look at the first few rows, you’ll see that the <code>windspeed</code> value is <code>0</code> for each of these rows, meaning there was no measured wind for the first few hours of the bike-sharing service’s existence. On the seventh row (counting the heading as the first row), you can see that there was finally some wind, and its measured speed was 0.0896. If you look at the <code>hr</code> column, you can see that this wind was recorded when <code>hr</code> = <code>5</code>, or at 5 <span class="KeyCaps">AM</span>. We know that this row gives us information about January 1 because the <code>dteday</code> column on the seventh row has the value <code>2011-01-01</code>.</p>
<p>Just by looking at a few values in the data, we can already start to tell a story, albeit an unexciting one: a still New Year’s night that turned into a slightly less still New Year’s morning. If we want to know some stories about the bike-sharing company and its performance instead of just the weather, we’ll have to look at other, more relevant columns.</p>
<p><span epub:type="pagebreak" id="Page_4" title="4"/>The columns with the most important information are the last three: <code>casual</code>, <code>registered</code>, and <code>count</code>. These columns indicate the number of people who used your company’s bikes each hour. People who register with your service to get discounts and benefits are registered users, and their bike use is recorded in the <code>registered</code> column. But people can also use your bikes without registering, and their bike use is recorded in the <code>casual</code> column. The sum of the <code>casual</code> and <code>registered</code> columns is the total count of users during each hour, and it’s recorded in the <code>count</code> column.</p>
<p>Now that you’re familiar with some of the more relevant columns in this dataset, you can learn a great deal just by glancing at their numbers. Looking at the first 20 or so hours shown in <a href="#figure1-1">Figure 1-1</a>, for example, you can see that in most hours, you have more registered users than casual users (higher values in the <code>registered</code> column than the <code>casual</code> column). This is just a simple numeric fact, but as the CEO, you should think through its implications for your business. Having more registered than casual users might mean that you’re doing well at convincing people to register, but it also might mean that using your service casually without registering isn’t as easy as it should be. You’ll have to think about which segment of customers is more important for you to target: the regular, registered users, like daily commuters, or the casual, infrequent users, like sightseeing tourists.</p>
<p>We can look more closely at the daily patterns of casual and registered users to see if we can learn more about them. Let’s look at the hours shown in <a href="#figure1-1">Figure 1-1</a> again. We see that casual users are sparse until the afternoon of the first day and peak around 1 <span class="KeyCaps">PM</span>. Registered users are relatively numerous even at 1 <span class="KeyCaps">AM</span> of the first day and peak at 2 <span class="KeyCaps">PM</span>. The differences between the behavior of registered and casual users are small but could be meaningful. For example, they could indicate demographic differences between these groups. This, in turn, could require using different marketing strategies targeted to each group.</p>
<p>Consider what we’ve done already: just by looking at a few columns of the first 24 rows of our data, we’ve already learned several important things about the company and started to get some business ideas. Data science has a reputation for requiring arcane knowledge about sophisticated math and computer science, but simply glancing at a dataset, thinking a little, and applying common sense can go a long way toward improving any business scenario.</p>
<h3 id="h2-502888c01-0002">Using .csv Files to Review and Store Data</h3>
<p class="BodyFirst">Let’s look even more closely at our data. If you open the data file (<em>hour.csv</em>) in a spreadsheet editor, it will look like <a href="#figure1-1">Figure 1-1</a>. However, you can also open this file in a text editor like Notepad (if you’re using Windows) or TextEdit (if you’re using macOS) or GNU Emacs or gedit (if you’re using Linux). When you open this file in a text editor, it will look like <a href="#figure1-2" id="figureanchor1-2">Figure 1-2</a>.</p>
<span epub:type="pagebreak" id="Page_5" title="5"/><figure>
<img alt="" class="keyline" height="390" src="image_fi/502888c01/f01002.png" width="694"/>
<figcaption><p><a id="figure1-2">Figure 1-2</a>: Bike-sharing data viewed as raw text</p></figcaption>
</figure>
<p>This raw data (that is, every single text character) constitutes our <em>hour.csv</em> file, without the alignment of straight columns you’d see in a spreadsheet. Notice the many commas. This file’s extension, .<em>csv</em>, is short for <em>comma-separated values</em> because the numeric values in each row are separated from each other by commas.</p>
<p>When you use a spreadsheet editor to open a .<em>csv</em> file, the editor will attempt to interpret every comma as a boundary between spreadsheet cells so that it can display the data in straight, aligned rows and columns. But the data itself is not stored that way: it’s just raw text with rows of values, with each value separated from other values by commas.</p>
<p>The simplicity of <em>.csv</em> files means that they can be easily created, easily opened by many types of programs, and easily changed. That’s why data scientists commonly store their data in .<em>csv</em> format.</p>
<h2 id="h1-502888c01-0002">Displaying Data with Python</h2>
<p class="BodyFirst">Using Python will allow us to do more sophisticated analyses than are possible in text editors and spreadsheet programs. It will also allow us to automate our processes and run analyses more quickly. We can easily open <em>.csv </em>files in Python. The following three lines of Python code will read the <em>hour.csv</em> file into your Python session and display its first five rows:</p>
<pre><code>import pandas as pd
hour=pd.read_csv('hour.csv')
print(hour.head())</code></pre>
<p><span epub:type="pagebreak" id="Page_6" title="6"/>We’ll look more closely at the output of this snippet later. For now, let’s look at the code itself. Its purpose is to read and display our data. The second line reads the data by using the <code>read_csv()</code> method. A <em>method</em> is a unit of code that performs a single, well-defined function. As its name suggests, <code>read_csv()</code> is specifically designed to read data that’s stored in <em>.csv</em> files. After you run this line, the <code>hour</code> variable will contain all the data in the <em>hour.csv</em> file; then you can access this data in Python.</p>
<p>On the third line, we use the <code>print()</code> function to display (print) our data onscreen. We could change the third line to <code>print(hour)</code> to see the entire dataset printed out. But datasets can be very large and hard to read all at once. Therefore, we add the <code>head()</code> method because it returns only the dataset’s first five rows.</p>
<p>Both <code>read_csv()</code> and <code>head()</code> can be very useful to us. But they’re not part of the <em>Python standard library</em>—the standard Python capabilities installed by default. They are instead part of a package, a third-party body of code that’s optional to install and use in Python scripts.</p>
<p>These two methods are part of a popular package called <em>pandas</em>, which contains code for working with data. That’s why the first line of the previous snippet is <code>import pandas as pd</code>: this <em>imports</em>, or brings in, the pandas package so we can access it in our Python session. When we write <code>as pd</code>, this gives the package an <em>alias</em>, so every time we want to access a pandas capability, we can write <code>pd</code> instead of the full name pandas. So when we write <code>pd.read_csv()</code>, we’re accessing the <code>read_csv()</code> method that’s part of the pandas package.</p>
<p>If you get an error when you run <code>import pandas as pd</code>, it’s possible that pandas is not installed on your computer. (Packages need to be installed before you can import them.) To install pandas, or any other Python package, you should use the standard Python package installer, called pip. You can find instructions for how to install pip and use it to install Python packages like pandas in this book’s introduction. Throughout this book, every time you import a package, you should make sure that you’ve first installed it on your computer by using pip.</p>
<p>You might get another error when you run this snippet. One of the most common errors will occur when Python isn’t able to find the <em>hour.csv</em> file. If that happens, Python will print out an error report. The last line of the error report might say this:</p>
<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'hour.csv'</code></pre>
<p>Even if you’re not a Python expert, you can surmise what this means: Python tried to read the <em>hour.csv</em> file but wasn’t able to find it. This can be a frustrating error, but one that’s possible to resolve. First, make sure that you’ve downloaded the <em>hour.csv</em> file and that it has the name <em>hour.csv</em> on your computer. The filename on your computer needs to exactly match the filename in your Python code.</p>
<p>If the name <em>hour.csv</em> is spelled correctly in your Python code (entirely with lowercase letters), the problem is probably with the file’s location. Remember that every file on your computer has a unique filepath that <span epub:type="pagebreak" id="Page_7" title="7"/>specifies exactly where you need to navigate to get to it. A filepath might look like this:</p>
<pre><code>C:\Users\DonQuixote\Documents\hour.csv</code></pre>
<p>This filepath is in the format used in the Windows operating system. If you’re using Windows, try to make sure that your directories and filenames don’t use any special characters (like characters from non-English alphabets) because filepaths with special characters can lead to errors. The following is another example of a filepath, one that’s in the format used by Unix-style operating systems (including macOS and Linux):</p>
<pre><code>/home/DonQuixote/Documents/hour.csv</code></pre>
<p>You’ll notice that Windows filepaths look different from macOS and Linux filepaths. In macOS and Linux, we use forward slashes exclusively, and we start with a slash (<code>/</code>) instead of a drive name like <code>C:\</code>. When you read a file into Python, the most straightforward way to avoid an error is to specify the full filepath, as follows:</p>
<pre><code>import pandas as pd
hour=pd.read_csv('/home/DonQuixote/Documents/hour.csv')
print(hour.head())</code></pre>
<p>When you run this snippet, you can replace the filepath in the <code>read_csv()</code> method with the filepath on your own computer. When you run the previous snippet, with the filepath correctly specified to match the location of <em>hour.csv</em> on your computer, you should get the following output:</p>
<pre><code>   instant      dteday  season  yr  ...  windspeed  casual  registered count
0        1  2011-01-01       1   0  ...        0.0       3          13    16
1        2  2011-01-01       1   0  ...        0.0       8          32    40
2        3  2011-01-01       1   0  ...        0.0       5          27    32
3        4  2011-01-01       1   0  ...        0.0       3          10    13
4        5  2011-01-01       1   0  ...        0.0       0           1     1

[5 rows x 17 columns]</code></pre>
<p>This output shows the first five rows of our data. You can see that the data is arranged by column, in a way that looks similar to our spreadsheet output. Just as in <a href="#figure1-1">Figure 1-1</a>, each row contains numeric values related to a particular hour of the bike-sharing company’s history.</p>
<p>Here, we see ellipses in place of some columns so that it’s easier to read onscreen and not too hard to read or copy and paste into text documents. (You might see all the columns instead of ellipses—your display will depend on the details of how Python and pandas are configured on your computer.) Just as we did when we opened the file in a spreadsheet editor, we can start looking at some of these numbers to discover stories about the company’s history and get ideas for running the business.</p>
<h2 id="h1-502888c01-0003"><span epub:type="pagebreak" id="Page_8" title="8"/>Calculating Summary Statistics</h2>
<p class="BodyFirst">Besides just looking at our data, quantifying its important attributes will be helpful. We can start by calculating the mean of one of the columns, as follows:</p>
<pre><code>print(hour['count'].mean())</code></pre>
<p>Here, we access the <code>count</code> column of our <code>hour</code> dataset by using square brackets (<code>[]</code>) and the column name (<code>count</code>). If you run <code>print(hour['count'])</code> alone, you’ll see the entire column printed to your screen. But we want just the <em>mean</em> of the column, not the column itself, so we add the <code>mean()</code> method—yet another capability provided by pandas. We see that the mean is about 189.46. This is interesting to know from a business perspective; it’s a rough measurement of the size of the business over the two years covered by the data.</p>
<p>In addition to calculating the mean, we could calculate other important metrics as follows:</p>
<pre><code>print(hour['count'].median())
print(hour['count'].std())
print(hour['registered'].min())
print(hour['registered'].max())</code></pre>
<p>Here, we calculate the median of the <code>count</code> column by using the <code>median()</code> method. We also use the <code>std()</code> method to calculate a standard deviation of our <code>count</code> variable. (You may already know that a <em>standard deviation</em> is a measurement of how far spread out a set of numbers is. It’s useful to help us understand the amount of variation that exists in ridership counts between hours in our data.) We also calculate the minimum and maximum of the <code>registered</code> variable, using the <code>min()</code> and <code>max()</code> methods, respectively. The number of registered users ranges from 0 to 886, and this tells us the hourly record you’ve set, and the record you’ll need to break if you want your business to do better than it ever has before.</p>
<p>These simple calculations are called <em>summary statistics</em>, and they’re useful to check for every dataset you ever work with. Checking the summary statistics of a dataset can help you better understand your data and, in this case, help you better understand your business.</p>
<p>As simple as these summary statistics might seem, many CEOs, if put on the spot, couldn’t even tell you their company’s exact number of customers. Knowing simple things like the mean number of customers on any hour of any day can help you understand how big your company is and how much room you have to grow.</p>
<p>These summary statistics can also be combined with other information to tell us even more. For example, if you look up how much your company charges for one hour of bike usage, you can multiply that by the mean of the <code>count</code> column to get your total revenue over the two years covered by the data.</p>
<p><span epub:type="pagebreak" id="Page_9" title="9"/>You can check summary statistics manually by using pandas methods like <code>mean()</code> and <code>median()</code> as we did previously. But another method makes summary statistics easy to check:</p>
<pre><code>print(hour.describe())</code></pre>
<p>Here, we use the <code>describe()</code> method to check the summary statistics of all variables in the dataset. The output looks like this:</p>
<pre><code>          instant        season  ...    registered         count
count  17379.0000  17379.000000  ...  17379.000000  17379.000000
mean    8690.0000      2.501640  ...    153.786869    189.463088
std     5017.0295      1.106918  ...    151.357286    181.387599
min        1.0000      1.000000  ...      0.000000      1.000000
25%     4345.5000      2.000000  ...     34.000000     40.000000
50%     8690.0000      3.000000  ...    115.000000    142.000000
75%    13034.5000      3.000000  ...    220.000000    281.000000
max    17379.0000      4.000000  ...    886.000000    977.000000

[8 rows x 16 columns]</code></pre>
<p>You can see that <code>describe()</code> provides an entire table to us, and this table contains several useful metrics, including the mean, minimum, and maximum of each of our variables. The output of <code>describe()</code> also contains percentiles. The <code>25%</code> row, for example, contains the 25th percentile of each variable in the <code>hour</code> data. We can see that the 25th percentile of the <code>count</code> variable is 40, meaning that 25 percent of the hours in our dataset had 40 users or fewer, while 75 percent had more than 40 users.</p>
<p>The table that we get from the <code>describe()</code> method is also useful to help us check for problems with the data. It’s common for datasets to contain major errors that can be spotted in the output of <code>describe()</code>. For example, if you run the <code>describe()</code> method on a dataset of people and see that their average age is 200, your data has errors. This may sound obvious, but that exact error (average ages greater than 200) was recently found in a well-known research paper published in a top academic journal—if only those researchers had used <code>describe()</code>! You should look at the output of <code>describe()</code> for every dataset you work with to make sure that all the values are at least plausible. If you find average ages over 200, or other data that doesn’t look credible, you’ll have to locate the problems in the data and fix them.</p>
<p>At this stage, we can already start to use what we’ve learned from the data to get ideas for improving the business. For example, we’ve seen that in the first 24 hours of our data, rider numbers at night are much lower than rider numbers during the day. We’ve also seen a wide variation in the hourly count of users: 25 percent of hours have fewer than 40 riders, but one hour had 886 riders. As the CEO, you may want more hours that have closer to 886 riders and fewer hours that have fewer than 40 riders.</p>
<p>You could pursue this goal in many ways. For example, you might lower prices during the night to get more customers at that time and therefore have fewer hours with low ridership. Just through simple exploration, you can continue to learn from the data and get ideas for improving the business.</p>
<h2 id="h1-502888c01-0004"><span epub:type="pagebreak" id="Page_10" title="10"/>Analyzing Subsets of Data</h2>
<p class="BodyFirst">We’ve checked summary statistics related to the full dataset, and then considered offering lower prices at night to increase nighttime ridership. If we really want to pursue this idea, we should check summary statistics related to just the nighttime.</p>
<h3 id="h2-502888c01-0003">Nighttime Data</h3>
<p class="BodyFirst">We can start by using the <code>loc()</code> method:</p>
<pre><code>print(hour.loc[3,'count'])</code></pre>
<p>This <code>loc()</code> method allows us to specify a subset of our full data. When we use <code>loc()</code>, we specify the subset we want to select by using square brackets with this pattern: <code>[&lt;</code><var>row</var><code>&gt;,&lt;</code><var>column</var><code>&gt;]</code>. Here, we specify <code>[3,'count']</code>, indicating that we want to select row 3 of our data and the <code>count</code> column. The output we get from this is 13, and if you look at the data in <a href="#figure1-1">Figure 1-1</a> or <a href="#figure1-2">Figure 1-2</a>, you can see that this is correct.</p>
<p>One important thing to point out here is that the standard practice in Python, as well as in pandas, is to use <em>zero-based indexing</em>. We count from zero, so if our dataset has four rows, we label them row 0, row 1, row 2, and row 3. The fourth row of your data is called row 3, or we say its index is 3. Similarly, the third row of your data has index 2, the second row has index 1, and the first row has index 0. That’s why when we run <code>print(hour.loc[3,'count'])</code>, we get 13, which is the fourth value stored in the data (the value from the row with index 3), instead of 32, which is the third value stored in the data (the value from the row with index 2). Zero-based indexing doesn’t feel natural to many people, but with experience, you can get used to it and feel comfortable with it.</p>
<p>In the previous snippet, we looked at a subset that consists of a single number (the <code>count</code> from a single row and a single column). But you may want to know about a subset that consists of multiple rows or multiple columns. By using a colon (<code>:</code>), we can specify a range of rows we want to look at:</p>
<pre><code>print(hour.loc[2:4,'registered'])</code></pre>
<p>In this snippet, we specify that we want values of the <code>registered</code> variable. By specifying <code>2:4</code> in the square brackets, we indicate that we want all the rows between row 2 and row 4, so we get three numbers as output: 27, 10, and 1. If you look at these rows, you can see that these observations are related to the hours 2 <span class="KeyCaps">AM</span>, 3 <span class="KeyCaps">AM</span>, and 4 <span class="KeyCaps">AM</span>. Instead of printing out all the data, we’re printing out just three rows. Since we are printing out only a subset, we can call this process <em>subsetting</em>—selecting subsets of data. This can be useful when exploring and analyzing data.</p>
<p>Instead of looking at a few adjacent rows at a time, let’s look at all the nighttime observations in our data. We can use logical conditions with the <code>loc()</code> method:</p>
<pre><code>print(hour.loc[hour['hr']&lt;5,'registered'].mean())</code></pre>
<p><span epub:type="pagebreak" id="Page_11" title="11"/>This snippet uses <code>loc()</code> to access a subset of the data, just as we’ve done before. However, instead of specifying particular row numbers, it specifies a logical condition: <code>hour['hr']&lt;5</code>, meaning that it will select every row in our data for which the value of the <code>hr</code> variable is less than 5. This will give us a subset of the data corresponding to the earliest hours of the morning (midnight to 4 <span class="KeyCaps">AM</span>). We can specify multiple conditions for more complex logic. For example, we can check specifically for ridership counts on colder early mornings or warmer early mornings:</p>
<pre><code>print(hour.loc[(hour['hr']&lt;5) &amp; (hour['temp']&lt;.50),'count'].mean())
print(hour.loc[(hour['hr']&lt;5) &amp; (hour['temp']&gt;.50),'count'].mean())</code></pre>
<p>Here, we specify multiple logical conditions, separated by an <code>&amp;</code> character to mean <em>and</em>, which indicates that two things must be true simultaneously. The first line selects rows that have an <code>hr</code> value less than 5 <em>and</em> a <code>temp</code> value less than 0.50. In this dataset, the <code>temp</code> variable records temperatures, but not on a Fahrenheit or Celsius scale that we’re familiar with. Instead, it uses a special scale that puts all temperatures between 0 and 1, where 0 represents a very cold temperature, and 1 represents a very warm temperature. Whenever you’re working with data, it’s important to make sure you know exactly which units are used for each variable. We specify <code>hour['temp']&lt;.50</code> to select hours with colder temperatures and <code>hour['temp']&gt;.50</code> to select hours with warmer temperatures. Together, these lines allow us to compare average ridership on cold early mornings with average ridership on warm early mornings.</p>
<p>We can also use the <code>|</code> symbol to signify <em>or</em>. This could be useful in an example like this:</p>
<pre><code>print(hour.loc[(hour['temp']&gt;0.5) | (hour['hum']&gt;0.5),'count'].mean())</code></pre>
<p>This line selects the mean readership count for rows with either high temperatures <em>or</em> high humidity—both aren’t required. Being able to select these complex conditions could help you choose ways to improve ridership during hours with uncomfortable weather.</p>
<h3 id="h2-502888c01-0004">Seasonal Data</h3>
<p class="BodyFirst">A nighttime discount is not the only possible strategy for improving ridership and revenue. You could also consider specials during certain seasons or at certain times of the year. In our data, the <code>season</code> variable records 1 for winter, 2 for spring, 3 for summer, and 4 for fall. We can use the <code>groupby()</code> method to find the mean number of users during each of these seasons:</p>
<pre><code>print(hour.groupby(['season'])['count'].mean())</code></pre>
<p>Much of this snippet should look familiar. We’re using <code>print()</code> to look at metrics related to the <code>hour</code> data. We use the <code>mean()</code> method, indicating that we’re looking at averages. And we use <code>['count']</code> to access the <code>count</code> column of the data. So it’s already clear that we’re going to be looking at average ridership counts in our <code>hour</code> data.</p>
<p><span epub:type="pagebreak" id="Page_12" title="12"/>The only new part is <code>groupby(['season'])</code>. This is a method that splits the data into groups—in this case, one group for each unique value that appears in the <code>season</code> column. The output shows us the mean ridership counts for each individual season:</p>
<pre><code>season
1    111.114569
2    208.344069
3    236.016237
4    198.868856
Name: count, dtype: float64</code></pre>
<p>Interpreting this output is straightforward: in the first season (winter), average ridership per hour is about 111.115; in the second season (spring), average ridership per hour is about 208.344; and so on. A definite seasonal pattern exists: higher ridership in the spring and summer seasons, and lower ridership in the fall and winter. The <code>groupby()</code> method can also group on multiple columns, as follows:</p>
<pre><code>print(hour.groupby(['season','holiday'])['count'].mean())</code></pre>
<p>The result is the following:</p>
<pre><code>season  holiday
1       0          112.685875
        1           72.042683
2       0          208.428472
        1          204.552083
3       0          235.976818
        1          237.822917
4       0          199.965998
        1          167.722222
Name: count, dtype: float64</code></pre>
<p>Here, we specify two columns to group on: <code>season</code> and <code>holiday</code>. This splits our hourly data into the four individual seasons, and then splits each season into holidays (denoted by 1s) and non-holidays (denoted by 0s). It shows us average ridership counts on holidays and non-holidays separately for each season. The result is that we can see the differences between holidays and non-holidays seasonally. It seems like holidays in the colder seasons have ridership that’s lower than that on non-holidays, and holidays in the warmer seasons have ridership that’s roughly equal to that on non-holidays. Understanding these differences can help you make decisions about how to run the business and might give you ideas about strategies you can pursue during different seasons or different holidays.</p>
<p>This dataset is big, and there’s no end to the different ways it can be examined. We’ve begun to look at a few subsets and started to get a few ideas. You should do much more: examine subsets related to all the columns and explore many perspectives on the data. Even without doing advanced statistics and machine learning, you can learn a great deal and get many useful ideas.</p>
<h2 id="h1-502888c01-0005"><span epub:type="pagebreak" id="Page_13" title="13"/>Visualizing Data with Matplotlib</h2>
<p class="BodyFirst">Summary statistics are valuable and useful for exploration. However, there’s an extremely important part of exploratory data analysis that we haven’t done yet: <em>plotting</em>, or visualizing the data in organized charts.</p>
<h3 id="h2-502888c01-0005">Drawing and Displaying a Simple Plot</h3>
<p class="BodyFirst">You should plot your data early and often every time you’re doing data analysis. We’ll use a popular plotting package called <em>Matplotlib</em>. We can draw a simple plot of our data as follows:</p>
<pre><code>import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(x = hour['instant'], y = hour['count'])
plt.show()</code></pre>
<p>Here, we import the Matplotlib package, giving it the alias <code>plt</code>. Next, we create a figure, called <code>fig</code>, and an axis, called <code>ax</code>. The figure, <code>fig</code>, will contain all the information about whatever plot or group of plots we draw. The axis, <code>ax</code>, will give us access to useful methods for actually drawing plots. The <code>subplots()</code> method creates both of these for us, and inside that method, we can specify a figure size (<code>figsize</code>). In this case, we specify a figure size of <code>(10,6)</code>, meaning that our figure will have a width of 10 inches and a height of 6 inches.</p>
<p>Next, we draw our plot by using the <code>scatter()</code> method. In <code>scatter()</code>, we specify <code>x=hour['instant']</code> so the x-axis will show the <code>instant</code> variable in our <code>hour</code> data. We specify <code>y=hour['count']</code> so the y-axis will show the <code>count</code> variable. Finally, we use <code>plt.show()</code> to display this plot onscreen. This snippet creates a plot that should look like <a href="#figure1-3" id="figureanchor1-3">Figure 1-3</a>.</p>
<figure>
<img alt="" class="" height="411" src="image_fi/502888c01/f01003.png" width="694"/>
<figcaption><p><a id="figure1-3">Figure 1-3</a>: Ridership counts every hour for two years</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_14" title="14"/>In this plot, you can see that every single point is an hour whose information is recorded in the dataset. The first hour (the beginning of 2011) is the one that appears at the farthest left of the plot. The last hour (the end of 2012) is the one that appears at the farthest right, and all other hours proceed in order in between.</p>
<p>This plot, known as a <em>scatterplot</em>, is a good first plot to draw because it shows every observation in the data; it also makes relationships easy to visually identify. In this case, we can see a full representation of the seasonal variation that our <code>groupby()</code> statement previously gave us a hint about. We can also see the general growth of ridership over time.</p>
<h3 id="h2-502888c01-0006">Clarifying Plots with Titles and Labels</h3>
<p class="BodyFirst">The plot in <a href="#figure1-3">Figure 1-3</a> shows the data, but it’s not as clearly presented as it should be. We can add titles and labels to our plot as follows:</p>
<pre><code>fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(x = hour['instant'], y = hour['count'])
plt.xlabel("Hour")
plt.ylabel("Count")
plt.title("Ridership Count by Hour")
plt.show()</code></pre>
<p>This snippet uses <code>xlabel()</code> to add a label to our x-axis, <code>ylabel()</code> to add a label to our y-axis, and <code>title()</code> to add a title to the plot. You can specify any text in these methods to get any labels you like. The output should look like <a href="#figure1-4" id="figureanchor1-4">Figure 1-4</a>.</p>
<figure>
<img alt="" class="" height="435" src="image_fi/502888c01/f01004.png" width="694"/>
<figcaption><p><a id="figure1-4">Figure 1-4</a>: Ridership counts by hour, with axis labels and a title</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_15" title="15"/>Our dataset is very large, and looking at all the data at once is hard. Let’s look at how to plot smaller subsets of our data.</p>
<h3 id="h2-502888c01-0007">Plotting Subsets of Data</h3>
<p class="BodyFirst">We can use the subsetting we did previously to plot only a subset of the data:</p>
<pre><code>hour_first48=hour.loc[0:48,:]
fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(x = hour_first48['instant'], y = hour_first48['count'])
plt.xlabel("Hour")
plt.ylabel("Count")
plt.title("Count by Hour - First Two Days")
plt.show()</code></pre>
<p>Here, we define a new variable called <code>hour_first48</code>. This variable contains data related to row 0 through row 48 of the original data, corresponding roughly to the first two full days in the data.</p>
<p>Notice that we select this subset by writing <code>hour.loc[0:48,:]</code>. This is the same <code>loc()</code> method that we’ve used before. We use <code>0:48</code> to specify that we want the rows with indexes up to 48, but we don’t specify any columns—we just write a colon (<code>:</code>) where we would normally specify column names to select. This is a useful shortcut: a colon alone placed there tells pandas that we want to select every column of the dataset, so we don’t need to write out each column name individually. The plot of this subset looks like <a href="#figure1-5" id="figureanchor1-5">Figure 1-5</a>.</p>
<figure>
<img alt="" class="" height="438" src="image_fi/502888c01/f01005.png" width="694"/>
<figcaption><p><a id="figure1-5">Figure 1-5</a>: Ridership counts by hour for the first two days</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_16" title="16"/>By plotting only two days instead of two years of data, we avoid the problem of points overlapping and hiding one another. We can see every observation much more clearly. When you have a big dataset, it’s a good idea to do both: plot the entire dataset at once (to understand the general, overall patterns) as well as plot smaller subsets of the data (to understand individual observations and smaller-scale patterns). In this case, we can see patterns within each day of the data in addition to the longer-term seasonal patterns within its years.</p>
<h3 id="h2-502888c01-0008">Testing Different Plot Types</h3>
<p class="BodyFirst">We have many ways to change the appearance of a plot. Our <code>scatter()</code> function contains parameters that we can adjust to get different looks:</p>
<pre><code>fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(x = hour_first48['instant'], y = hour_first48['count'],c='red',marker='+')
plt.xlabel("Hour")
plt.ylabel("Count")
plt.title("Count by Hour - First Two Days")
plt.show()</code></pre>
<p>Here, we use the <code>c</code> argument to specify a color for our plot points (red). We also specify a <code>marker</code> argument to change the <em>marker style</em>, or the shape of the points that are drawn. By specifying <code>+</code> for our marker argument, we get plot points that look like little pluses instead of little dots. <a href="#figure1-6" id="figureanchor1-6">Figure 1-6</a> shows the output.</p>
<figure>
<img alt="" class="" height="421" src="image_fi/502888c01/f01006.png" width="694"/>
<figcaption><p><a id="figure1-6">Figure 1-6</a>: Ridership counts, with different style choices</p></figcaption>
</figure>
<p>This book isn’t printed in color, so you will not see the specified red color displayed on this page. But you should see red points if you run this code at home.</p>
<p><span epub:type="pagebreak" id="Page_17" title="17"/>Scatterplots are not the only type of plot we can draw. Let’s try a line plot:</p>
<pre><code>fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(hour_first48['instant'], hour_first48['casual'],c='red',label='casual',linestyle='-')
ax.plot(hour_first48['instant'],\
hour_first48['registered'],c='blue',label='registered',linestyle='--')
ax.legend()
plt.show()</code></pre>
<p>In this case, we use <code>ax.plot()</code> instead of <code>ax.scatter()</code> to draw the plot. The <code>ax.plot()</code> method allows us to draw a line plot. Here, we call <code>ax.plot()</code> twice to draw two lines on a single plot. This enables us to compare casual and registered users (<a href="#figure1-7" id="figureanchor1-7">Figure 1-7</a>).</p>
<figure>
<img alt="" class="" height="450" src="image_fi/502888c01/f01007.png" width="694"/>
<figcaption><p><a id="figure1-7">Figure 1-7</a>: A line plot showing casual and registered riders over the first two days</p></figcaption>
</figure>
<p>This plot shows that the number of casual riders is almost always lower than the number of registered riders. The plot’s legend indicates different colors for casual and registered users as well as the different line styles (solid for casual riders, dashed for registered riders). Run this code at home to see the colors and their contrast more clearly.</p>
<p>We can also try a different kind of plot:</p>
<pre><code>import seaborn as sns
fig, ax = plt.subplots(figsize=(10, 6))
sns.boxplot(x='hr', y='registered', data=hour)
plt.xlabel("Hour")
plt.ylabel("Count")
plt.title("Counts by Hour")
plt.show()</code></pre>
<p><span epub:type="pagebreak" id="Page_18" title="18"/>This time, we import a package called <em>seaborn</em>. This package is based on Matplotlib, so it includes all the capabilities of Matplotlib, plus more features that help create beautiful, informative plots quickly. We use seaborn’s <code>boxplot()</code> method to create a new kind of plot: a <em>box plot</em>. <a href="#figure1-8" id="figureanchor1-8">Figure 1-8</a> shows the box plots that this snippet creates.</p>
<figure>
<img alt="" class="" height="439" src="image_fi/502888c01/f01008.png" width="694"/>
<figcaption><p><a id="figure1-8">Figure 1-8</a>: Box plots showing ridership counts grouped by the hour of the day</p></figcaption>
</figure>
<p>You can see 24 vertical box plots, drawn parallel to one another—each one representing information about a particular hour of the day. A box plot is a simple kind of plot, but one that gives a great deal of information. In a box plot, the upper and lower horizontal boundaries of each rectangle represent the 75th and 25th percentiles of the plotted data, respectively. The horizontal line inside the rectangle represents the median (or 50th percentile). The vertical lines extending from the top and bottom of each rectangle represent the full range of all observations that are not considered outliers. The individually drawn points beyond the ranges of the vertical lines are regarded as outliers.</p>
<p>Seeing the box plots together in <a href="#figure1-8">Figure 1-8</a> enables you to compare ridership at different times of day. For example, the median ridership during hour 5 (around 5 <span class="KeyCaps">AM</span>) is quite low, but the median ridership at hour 6 (around 6 <span class="KeyCaps">AM</span>) is much higher. At hour 7 (around 7 <span class="KeyCaps">AM</span>), the median ridership is higher still. High ridership occurs again around 5 <span class="KeyCaps">PM</span> and 6 <span class="KeyCaps">PM</span>; maybe these peaks indicate that many of your customers use your bikes to commute to and from work.</p>
<p>As you might expect, we can draw many more types of plots. Another useful one is a <em>histogram</em>, which you can create as follows:</p>
<pre><code>fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(hour['count'],bins=80)
plt.xlabel("Ridership")
<span epub:type="pagebreak" id="Page_19" title="19"/>plt.ylabel("Frequency")
plt.title("Ridership Histogram")
plt.show()</code></pre>
<p>This snippet uses the <code>hist()</code> command to draw a histogram. <a href="#figure1-9" id="figureanchor1-9">Figure 1-9</a> shows the output.</p>
<figure>
<img alt="" class="" height="438" src="image_fi/502888c01/f01009.png" width="694"/>
<figcaption><p><a id="figure1-9">Figure 1-9</a>: A histogram showing the frequency of each ridership count</p></figcaption>
</figure>
<p>In a histogram, the height of every bar represents frequency. In this case, our histogram shows the frequencies of every ridership count. For example, if you look at the x-axis around 800, you’ll see bars that have a height close to 0. This means that very few hours in our dataset had around 800 riders. By contrast, at about 200 on the x-axis, you see higher bars, with height closer to 500. This indicates that for close to 500 individual hours in our data, ridership was close to 200. The pattern we see in this histogram is a common one for businesses: many hours have few customers, and few hours have many customers.</p>
<p>You could use this kind of histogram to think about the capacity of your company. For example, maybe your company has 1,000 bicycles available to rent today. You think that it might be good to save money by selling 200 of your bicycles—that way, you’ll earn some extra cash and won’t have to worry about maintenance and storage of superfluous bikes. This would leave you with 800 bicycles available to rent. By looking at the histogram, you can see exactly how much you would expect that change to impact your company: since only a small fraction of hours have demand higher than 800, this should have a relatively small impact on your capacity. You could look at the histogram to decide exactly how many of your bicycles you feel comfortable selling.</p>
<p><span epub:type="pagebreak" id="Page_20" title="20"/>Another type of plot, a <em>pair plot</em>, draws every possible scatterplot for every possible pair of variables in your data:</p>
<pre><code>thevariables=['hr','temp','windspeed']
hour_first100=hour.loc[0:100,thevariables]
sns.pairplot(hour_first100, corner=True)
plt.show()</code></pre>
<p>Here, we create a <code>thevariables</code> variable, which is a list of three variables we’ll plot. (We’re plotting only three instead of all variables because of the limited space in the book.) We also create <code>hour_first100</code>, which is a subset of our full data containing only the rows with index 100 or less in the <code>hour</code> dataset. Again, the seaborn package helps us by providing the <code>pairplot()</code> method that we can call to create our plot. The result, <a href="#figure1-10" id="figureanchor1-10">Figure 1-10</a>, is a collection of plots, including both scatterplots and histograms.</p>
<figure>
<img alt="" class="" height="562" src="image_fi/502888c01/f01010.png" width="694"/>
<figcaption><p><a id="figure1-10">Figure 1-10</a>: Pair plots showing relationships among selected variables</p></figcaption>
</figure>
<p>The pair plot shows scatterplots for every possible combination of variables in the subset of the data we selected, as well as histograms for the individual variables we selected. A lot of data is plotted here, but the scatterplots don’t show much apparent relationship among the variables; these relationships appear to be essentially random.</p>
<p>Sometimes when we draw pair plots, we see more than just randomness. Instead, we can see clear relationships among variables. For example, if we had a measurement of snowfall in our data, we would see that as <span epub:type="pagebreak" id="Page_21" title="21"/>temperature goes up, snowfall levels go down, and vice versa. This type of clear relationship between variables is called <em>correlation</em>, and we’ll explore it in the next section.</p>
<h2 id="h1-502888c01-0006">Exploring Correlations</h2>
<p class="BodyFirst">Two variables are <em>correlated</em> if a change in one variable tends to occur together with a change in the other variable. We say that two variables are positively correlated if they change <em>together</em>: one variable tends to go up when the other goes up, and one variable tends to go down when the other goes down. We can find innumerable examples of positive correlations in the world. The number of domestic house cats in a city is positively correlated with the amount of cat food purchased in that city. If one of these variables is high, the other one also tends to be high, and if one of these variables is low, the other one also tends to be low.</p>
<p>We can also talk about negative correlations: two variables are negatively correlated if one tends to go up when the other goes down, or if one tends to go down when the other goes up. Negative correlations are also common in the world. For example, the average temperature of a city is negatively correlated with the average amount of money a typical resident spends on thick winter coats every year. In cities where one of these numbers is high, the other tends to be low, and in cities where one of these numbers is low, the other tends to be high.</p>
<p>In the world of data science, it’s extremely important to find and understand correlations, both positive and negative ones. Your performance as CEO will improve if you can find and understand these correlations. For example, you might find that the count of riders is positively correlated with the temperature. If so, this means that ridership tends to be low when temperatures are low. You could even consider selling some of your bikes during seasons with low ridership to generate cash flow instead of letting many of your bikes sit idle. Exactly what you choose to do will depend on many other details of your situation, but understanding the data on a deep level will help you make the best possible business decisions.</p>
<h3 id="h2-502888c01-0009">Calculating Correlations</h3>
<p class="BodyFirst">We can calculate correlations in Python:</p>
<pre><code>print(hour['casual'].corr(hour['registered']))
print(hour['temp'].corr(hour['hum']))</code></pre>
<p>Here, we use the <code>corr()</code> method, yet another capability provided by pandas. The <code>corr()</code> method calculates a number called the <em>correlation coefficient</em>. We can calculate many types of correlation coefficients, but by default, <code>corr()</code> calculates the Pearson correlation coefficient. This is the most commonly used correlation coefficient, so whenever we refer to a correlation coefficient in this book, we’ll be referring to the Pearson correlation coefficient.</p>
<p><span epub:type="pagebreak" id="Page_22" title="22"/>The Pearson correlation coefficient is a number that’s always between –1 and 1, and it’s often named with the variable <em>r</em>. It’s meant to describe the relationship between two variables; its sign describes the type of correlation, and its size describes the strength of the correlation. If the correlation coefficient <em>r</em> is a positive number, our two variables are positively correlated, and if <em>r</em> is a negative number, they’re negatively correlated. If the correlation coefficient is 0, or very close to 0, we say that the variables are <em>uncorrelated</em>.</p>
<p>In this case, the first line of this snippet calculates the correlation coefficient describing the relationship between the <code>casual</code> and <code>registered</code> variables in our data. For these variables, <em>r </em>is about 0.51, a positive number that indicates a positive correlation.</p>
<h3 id="h2-502888c01-0010">Understanding Strong vs. Weak Correlations</h3>
<p class="BodyFirst">In addition to noticing whether correlation coefficients are positive, negative, or 0, we pay attention to their exact <em>magnitude</em>, or size. If a correlation coefficient is large (far from 0 and close to either 1 or –1), we often say that the correlation is <em>strong</em>. Take a look at <a href="#figure1-11" id="figureanchor1-11">Figure 1-11</a> to see examples of correlations.</p>
<figure>
<img alt="" class="" height="711" src="image_fi/502888c01/f01011.png" width="683"/>
<figcaption><p><a id="figure1-11">Figure 1-11</a>: Positively correlated variables</p></figcaption>
</figure>
<p><span epub:type="pagebreak" id="Page_23" title="23"/>Here, you can see two plots. The first plot shows the relationship between Fahrenheit and Celsius temperatures. You can see that Fahrenheit and Celsius are positively correlated: when one goes up, the other goes up, and vice versa. The second plot shows the relationship between casual and registered ridership in your company. Again, we see a positive correlation: when casual ridership goes up, registered ridership tends to go up as well, and vice versa.</p>
<p>The two correlations in <a href="#figure1-11">Figure 1-11</a> are both positive, but we can see a qualitative difference between them. The relationship between Fahrenheit and Celsius is deterministic: knowing the Fahrenheit temperature allows us to know the Celsius temperature exactly, with no uncertainty or guesswork. This kind of deterministic positive correlation that appears as a straight line on a plot is also called a <em>perfect</em> correlation, and when we measure a correlation coefficient for a perfect positive correlation, we’ll find that <em>r </em>= 1.</p>
<p>By contrast, the relationship between casual and registered ridership is<em> not</em> deterministic. Often a higher number of casual riders corresponds to a higher number of registered riders. But sometimes it doesn’t; we can’t perfectly predict one variable by using the other one. When two variables are correlated but don’t have a deterministic relationship, we say that the relationship between the two variables has “noise,” or randomness.</p>
<p><em>Randomness</em> is hard to define precisely, but you can think of it as unpredictability. When you know a Fahrenheit temperature, you can predict the Celsius temperature with perfect accuracy. By contrast, when you know the casual ridership, you can predict the registered ridership, but your prediction may not be perfectly accurate. When unpredictability like this exists, the two variables will have a correlation coefficient that’s less than 1. In this case, we can calculate the correlation of casual and registered ridership and find that <em>r </em>= 0.51.</p>
<p>You can think of the size of the correlation coefficient as a measure of the amount of randomness in the relationship between two variables. A larger correlation coefficient corresponds to less randomness (closer to a deterministic relationship, like the relationship between Fahrenheit and Celsius). A smaller correlation coefficient corresponds to more randomness and less predictability. You can think of a 0 correlation coefficient, indicating no relationship at all between variables, as an indication of pure randomness, or pure noise.</p>
<p>You can look at <a href="#figure1-12" id="figureanchor1-12">Figure 1-12</a> to see examples of negative correlations of different magnitudes.</p>
<span epub:type="pagebreak" id="Page_24" title="24"/><figure class="graphic">
<img alt="" class="" height="250" src="image_fi/502888c01/f01012a.png" width="429"/></figure>
<figure>
<img alt="" class="" height="444" src="image_fi/502888c01/f01012.png" width="694"/>
<figcaption><p><a id="figure1-12">Figure 1-12</a>: Negatively correlated variables</p></figcaption>
</figure>
<p>Here, we see the same ideas as in <a href="#figure1-11">Figure 1-11</a>. The first plot shows a <em>perfect</em> negative correlation: this time, the deterministic relationship between pressure and volume. The correlation here is exactly <em>r </em>= –1, indicating that no randomness occurs in the relationship between the variables; each variable is perfectly predictable by using the other one.</p>
<p>The second plot shows the relationship between temperature and humidity in our data. These two variables also have a negative correlation, but with a much smaller coefficient: <em>r </em>is about –0.07. Just as we did with the positive correlations in <a href="#figure1-11">Figure 1-11</a>, we can interpret these correlation coefficients as measurements of randomness: a correlation coefficient with a larger magnitude (meaning that it’s closer to 1 or –1) is a correlation that’s highly predictable and not very random, while a coefficient with a smaller magnitude (closer to 0) is a correlation that has more randomness. When we see <em>r</em> = –0.07 here, we interpret that to mean that temperature and humidity are negatively correlated, but their correlation is very weak—it’s not far from pure randomness.</p>
<p><span epub:type="pagebreak" id="Page_25" title="25"/>One important thing to remember when you look at correlations is a famous saying: “Correlation does not imply causation.” When we observe strong correlations, all we can be certain of is that two variables tend to change together; we can’t be certain that one causes the other.</p>
<p>For example, suppose we study Silicon Valley startups and find that their monthly revenues are correlated with the number of Ping-Pong tables they purchase. We may hastily conclude from this correlation that Ping-Pong tables are causing revenue to increase; maybe the relaxation and camaraderie that they facilitate leads to higher productivity, or maybe the fun atmosphere they create leads to better retention and hiring success.</p>
<p>On the other hand, these ideas may be completely mistaken, and maybe the causation flows in the opposite direction; companies that have success (totally independent of their Ping-Pong tables) have higher revenues, and since their budget has suddenly increased, they use some of their new extra money for a fun purchase like a Ping-Pong table. In that case, revenue would be causing Ping-Pong table purchases, not the other way around.</p>
<p>Finally, the correlation could be mere coincidence. Maybe Ping-Pong tables don’t lead to higher revenues, and revenues don’t lead to more Ping-Pong tables, but instead we’ve observed a <em>spurious correlation:</em> a correlation that occurs only by coincidence and does not indicate any causation or special relationship. A correlation could also be due to an <em>omitted variable</em>, something we haven’t observed but is independently causing revenue increases and Ping-Pong table purchases simultaneously.</p>
<p>In any case, the important thing is to always be cautious when you find and interpret correlations. Correlations mean that two variables tend to change together, and they can help us make predictions, but they don’t necessarily imply that one variable causes the other, or even that they have any real relationship.</p>
<p>Discovering and understanding correlation coefficients can help you in your CEO duties, especially when you find surprising correlations. For example, you may find a strong, positive correlation between the size of groups that rent bicycles together and their level of customer satisfaction after the rental. Maybe this can give you some ideas about encouraging people to rent bikes with their friends as a chance to get more satisfied customers. Finding correlations, and understanding the magnitude of correlations and what that tells you about predictability, can be valuable in business.</p>
<h3 id="h2-502888c01-0011">Finding Correlations Between Variables</h3>
<p class="BodyFirst">We can do more than calculate individual correlations between pairs of variables. We can go further by creating a <em>correlation matrix</em>, which is a matrix (or rectangular array) of numbers, each of whose elements is the correlation coefficient measuring the relationship between two particular variables. A correlation matrix will show the relationships among all of our variables:</p>
<pre><code>thenames=['hr','temp','windspeed']
cor_matrix = hour[thenames].corr()
print(cor_matrix)</code></pre>
<p><span epub:type="pagebreak" id="Page_26" title="26"/>Here, we use the same <code>corr()</code> method that we’ve used before. When we use <code>corr()</code> without any arguments inside the parentheses, it creates a correlation matrix for all the variables in a dataset. In this case, we create a smaller correlation matrix that shows correlations among just three selected variables. The correlation matrix that we calculate here looks like this:</p>
<pre><code>                 hr      temp  windspeed
hr         1.000000  0.137603   0.137252
temp       0.137603  1.000000  -0.023125
windspeed  0.137252 -0.023125   1.000000</code></pre>
<p>Here, we have a 3×3 matrix. Every entry in this matrix is a correlation coefficient. For example, in the second row, third column, you can see that the correlation between <code>windspeed</code> and <code>temp</code> is about <em>r </em>= –0.023. Technically, this is a negative correlation, though it’s so close to 0 that we would typically describe the two variables as uncorrelated.</p>
<p>You also can see that three correlations in the matrix are equal to 1.0. That’s expected: these perfect correlations are measuring the correlation of each variable with itself (the correlation of <code>hr</code> with <code>hr</code>, <code>temp</code> with <code>temp</code>, and <code>windspeed</code> with <code>windspeed</code>). Every variable will always have a perfect correlation with itself. Creating a correlation matrix can be a quick, simple way to find correlations among all the variables in your data and find any surprising positive or negative correlations.</p>
<h2 id="h1-502888c01-0007">Creating Heat Maps</h2>
<p class="BodyFirst">After creating a correlation matrix, we can create a plot of all these correlations to make the matrix more easily readable:</p>
<pre><code>plt.figure(figsize=(14,10))
corr = hour[thenames].corr()
sns.heatmap(corr, annot=True,cmap='binary',
        fmt=".3f",
        xticklabels=thenames,
        yticklabels=thenames)
plt.show()</code></pre>
<p>Here, we create a heat map. In this type of plot, the color or darkness of a cell indicates the value of the number in that cell. The heat map in <a href="#figure1-13" id="figureanchor1-13">Figure 1-13</a> shows measurements of correlations between variables.</p>
<span epub:type="pagebreak" id="Page_27" title="27"/><figure>
<img alt="" class="" height="546" src="image_fi/502888c01/f01013.png" width="694"/>
<figcaption><p><a id="figure1-13">Figure 1-13</a>: Correlations shown in a heat map</p></figcaption>
</figure>
<p>This heat map shows a collection of nine rectangles. As the legend on the right indicates, a darker fill in a rectangle indicates that a particular correlation is higher, and a lighter fill indicates that a particular correlation is lower. A heat map of a correlation matrix can provide an even quicker way to check for patterns and relationships among variables, since strong relationships will quickly catch the eye.</p>
<p>If you prefer a color plot instead of a grayscale one, you can change the <code>cmap='binary'</code> parameter in the <code>sns.heatmap()</code> method. This <code>cmap</code> parameter refers to the <em>color map</em> of the heat map, and by choosing a different <code>cmap</code> value, you can get different color schemes. For example, if you use <code>cmap='coolwarm'</code>, you’ll see a heat map in which higher values are represented by reddish colors and lower values are represented by bluish colors.</p>
<p>Heat maps can be drawn for variables other than correlation matrices. For example, we can draw a heat map showing the number of riders at each hour throughout a week:</p>
<pre><code># Create a pivot table
df_hm =hour.pivot_table(index = 'hr',columns ='weekday',values ='count')
# Draw a heatmap
plt.figure(figsize = (20,10)) # To resize the plot
sns.heatmap(df_hm,  fmt="d", cmap='binary',linewidths=.5, vmin = 0)
plt.show()</code></pre>
<p><span epub:type="pagebreak" id="Page_28" title="28"/>To create this plot, we have to create a pivot table, a table of grouped values. If you’ve spent a lot of time working with Excel or other spreadsheet programs, you’ve likely encountered pivot tables before. Here, our pivot table has grouped values from our full dataset based on their day of the week and hour of the day. We have the average ridership for each hour (0 through 23) of each day (Sunday through Saturday). After creating a pivot table with data grouped in this way, we can use the same <code>heatmap()</code> method to create the heat map shown in <a href="#figure1-14" id="figureanchor1-14">Figure 1-14</a>.</p>
<figure>
<img alt="" class="" height="508" src="image_fi/502888c01/f01014.png" width="694"/>
<figcaption><p><a id="figure1-14">Figure 1-14</a>: Ridership counts for each hour of every day</p></figcaption>
</figure>
<p>This heat map contains darker rectangles for hours that had more riders and lighter rectangles for hours that had fewer riders. We can see commuters who spike in activity around 8 <span class="KeyCaps">AM</span> and 5 <span class="KeyCaps">PM</span>. We can also see weekend outings on Saturday and Sunday afternoons.</p>
<p>From a business perspective, this heat map could give us any number of business ideas. For instance, seeing the spike in ridership around 8 <span class="KeyCaps">AM</span> on weekdays could give you an idea for increasing your revenue. Just as we imagined providing discounts during times of low activity, we might also consider the mirror image strategy: surge pricing (temporarily higher prices) during especially active times. Other transportation companies like Uber, Lyft, and Grab use this surge pricing strategy, not only to increase revenue but also to ensure high availability of their products.</p>
<h2 id="h1-502888c01-0008"><span epub:type="pagebreak" id="Page_29" title="29"/>Exploring Further</h2>
<p class="BodyFirst">So far, we’ve looked at only one dataset, and we’ve done only a few of the infinite explorations that are possible with it. As you continue from your first morning as CEO to your first afternoon, then your second day, and further on, you will need to make many decisions about your business and the way it operates. The exploration that we’ve done in this chapter can be applied to any other business question you ever encounter. For example, you might consider bundling bike rentals with refreshing drinks and earning extra revenue that way (not to mention keeping your riders healthier and safer). Analyzing data related to your customers, their riding patterns, and how thirsty they get during bike rides could help you figure out whether this strategy is a good idea.</p>
<p>Other analyses could be related to the repairs your bikes need. How often are your bikes being repaired, and how much do repairs cost? You could check for the times of repairs and make sure they’re not being done during peak hours. You could check the costs of repairs of various types of bikes. You could check a histogram of the prices of repairs and check whether any outliers are increasing your costs too much. These explorations would help you better understand your business and help you get ideas for running it better.</p>
<p>So far, our analyses have not been extremely sophisticated; mostly we’ve calculated only summary statistics and drawn plots. But these simple calculations and plots, when combined with common sense, can be valuable as a first step in making business decisions. Some CEOs don’t look at data enough, and others want to look at data but depend on staff to provide reports to them, and those reports may be slow or imperfect. A CEO who can confidently check data related to their company is a CEO who can be effective. CEOs can become good at data, and this can combine with their business knowledge to make them even better at their jobs. Similarly, data scientists can become good at business, and when their data skills are combined with business acumen, they can really become a force to be reckoned with.</p>
<h2 id="h1-502888c01-0009">Summary</h2>
<p class="BodyFirst">In this chapter, we started with a simple business scenario: becoming a CEO and making decisions related to running a business better. We went over some ideas for what a CEO needs to do and how exploratory data analysis can be helpful. We covered how to read data into Python, calculate summary statistics, draw plots, and interpret results in a business context. In the next chapter, we’ll go over linear regression, a more sophisticated method that can be used not only for exploration but also for forecasting. Let’s continue!</p>
</section>
</div></body></html>