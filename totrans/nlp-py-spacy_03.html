<html><head></head><body>
<div id="sbo-rt-content"><h2 class="h2" id="ch03"><span epub:type="pagebreak" id="page_31"/><strong><span class="big">3</span><br/>WORKING WITH CONTAINER OBJECTS AND CUSTOMIZING SPACY</strong></h2>&#13;
<div class="image1"><img src="../Images/comm1.jpg" alt="Image" width="191" height="191"/></div>&#13;
<p class="noindents">You can divide the main objects composing the spaCy API into two categories: containers (such as Tokens and Doc objects) and processing pipeline components (such as the part-of-speech tagger and named entity recognizer). This chapter explores container objects further. Using container objects and their methods, you can access the linguistic annotations that spaCy assigns to each token in a text.</p>&#13;
<p class="indent">You’ll also learn how to customize the pipeline components to suit your needs and use Cython code to speed up time-consuming NLP tasks.</p>&#13;
<h3 class="h3" id="lev26"><strong>spaCy’s Container Objects</strong></h3>&#13;
<p class="noindent">A <em>container object</em> groups multiple elements into a single unit. It can be a collection of objects, like tokens or sentences, or a set of annotations related to <span epub:type="pagebreak" id="page_32"/>a single object. For example, spaCy’s Token object is a container for a set of annotations related to a single token in a text, such as that token’s part of speech. Container objects in spaCy mimic the structure of natural language texts: a text is composed of sentences, and each sentence contains tokens.</p>&#13;
<p class="indent">Token, Span, and Doc, the most widely used container objects in spaCy from a user’s standpoint, represent a token, a phrase or sentence, and a text, respectively. A container can contain other containers—for example, a Doc contains Tokens. In this section, we’ll explore working with these container objects.</p>&#13;
<h4 class="h4" id="lev27"><strong><em>Getting the Index of a Token in a Doc Object</em></strong></h4>&#13;
<p class="noindent">A Doc object contains a collection of the Token objects generated as a result of the tokenization performed on a submitted text. These tokens have indices, allowing you to access them based on their positions in the text, as shown in <a href="../Text/ch03.xhtml#ch03fig01">Figure 3-1</a>.</p>&#13;
<div class="image"><a id="ch03fig01"/><img src="../Images/fig3-1.jpg" alt="image" width="546" height="225"/></div>&#13;
<p class="figcap"><em>Figure 3-1: The tokens in a Doc object</em></p>&#13;
<p class="indent">The tokens are indexed starting with 0, which makes the length of the document minus 1 the index of the end position. To shred the Doc instance into tokens, you derive the tokens into a Python list by iterating over the Doc from the start token to the end token:</p>&#13;
<pre>&gt;&gt;&gt; [doc[i] for i in range(len(doc))]<br/>&#13;
[A, severe, storm, hit, the, beach, .]</pre>&#13;
<p class="indent">It’s worth noting that we can create a Doc object using its constructor explicitly, as illustrated in the following example:</p>&#13;
<pre>&gt;&gt;&gt; from spacy.tokens.doc import Doc<br/>&#13;
&gt;&gt;&gt; from spacy.vocab import Vocab<br/>&#13;
&gt;&gt;&gt; doc = Doc(<span class="ent">➊</span>Vocab(), <span class="ent">➋</span>words=[u'Hi', u'there'])<br/>&#13;
doc<br/>&#13;
Hi there</pre>&#13;
<p class="indent">We invoke the Doc’s constructor, passing it the following two parameters: a <em>vocab object</em> <span class="ent">➊</span>—which is a storage container that provides vocabulary data, such as lexical types (adjective, verb, noun, and so on)—and a list of tokens to add to the Doc object being created <span class="ent">➋</span>.</p>&#13;
<h4 class="h4" id="lev28"><span epub:type="pagebreak" id="page_33"/><strong><em>Iterating over a Token’s Syntactic Children</em></strong></h4>&#13;
<p class="noindent">Suppose we need to find the leftward children of a token in the syntactic dependency parse of a sentence. For example, we can apply this operation to a noun to obtain its adjectives, if any. We might need to do this if we want to know what adjectives are able to modify a given noun. As an example, consider the following sentence:</p>&#13;
<pre>I want a green apple.</pre>&#13;
<p class="indent">The diagram in <a href="../Text/ch03.xhtml#ch03fig02">Figure 3-2</a> highlights the syntactic dependencies of interest.</p>&#13;
<div class="image"><a id="ch03fig02"/><img src="../Images/fig3-2.jpg" alt="image" width="411" height="219"/></div>&#13;
<p class="figcap"><em>Figure 3-2: An example of leftward syntactic dependencies</em></p>&#13;
<p class="indent">To obtain the leftward syntactic children of the word “apple” in this sample sentence programmatically, we might use the following code:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'I want a green apple.')<br/>&#13;
&gt;&gt;&gt; [w for w in doc[4].lefts]<br/>&#13;
[a, green]</pre>&#13;
<p class="indent">In this script, we simply iterate through the apple’s children, outputting them in a list.</p>&#13;
<p class="indent">It’s interesting to note that in this example, the leftward syntactic children of the word “apple” represent the entire sequence of the token’s syntactic children. In practice, this means that we might replace <code>Token.lefts</code> with <code>Token.children</code>, which finds all of a token’s syntactic children:</p>&#13;
<pre>&gt;&gt;&gt; [w for w in doc[4].children]</pre>&#13;
<p class="indent">The result list will remain the same.</p>&#13;
<p class="indent">We could also use <code>Token.rights</code> to get a token’s rightward syntactic children: in this example, the word “apple” is a rightward child of the word “want,” as shown in <a href="../Text/ch03.xhtml#ch03fig01">Figure 3-1</a>.</p>&#13;
<h4 class="h4" id="lev29"><strong><em>The doc.sents Container</em></strong></h4>&#13;
<p class="noindent">Typically, the linguistic annotations assigned to a token make sense only in the context of the sentence in which the token occurs. For example, <span epub:type="pagebreak" id="page_34"/>information about whether the word is a noun or a verb might apply only to the sentence in which this word is located (like the word “count,” discussed in previous chapters). In such cases, it would be useful to have the ability to access the tokens in the document with sentence-level indices.</p>&#13;
<p class="indent">The Doc object’s <code>doc.sents</code> property lets us separate a text into its individual sentences, as illustrated in the following example:</p>&#13;
<pre>   &gt;&gt;&gt; doc = nlp(u'A severe storm hit the beach. It started to rain.')<br/>&#13;
<span class="ent">➊</span> &gt;&gt;&gt; for sent in doc.sents:<br/>&#13;
<span class="ent">➋</span> ...   [sent[i] for i in range(len(sent))]<br/>&#13;
   ...<br/>&#13;
   [A, severe, storm, hit, the, beach, .]<br/>&#13;
   [It, started, to, rain, .]<br/>&#13;
   &gt;&gt;&gt;</pre>&#13;
<p class="indent">We iterate over the sentences in the <code>doc</code> <span class="ent">➊</span>, creating a separate list of tokens for each sentence <span class="ent">➋</span>.</p>&#13;
<p class="indent">At the same time, we can still refer to the tokens in a multi-sentence text using the global, or document-level, indices, as shown here:</p>&#13;
<pre>&gt;&gt;&gt; [doc[i] for i in range(len(doc))]<br/>&#13;
[A, severe, storm, hit, the, beach, ., It, started, to, rain, .]</pre>&#13;
<p class="indent">The ability to refer to the Token objects in a document by their sentence-level indices can be useful if, for example, we need to check whether the first word in the second sentence of the text being processed is a pronoun (say we want to figure out the connection between two sentences: the first of which contains a noun and the second of which contains a pronoun that refers to the noun):</p>&#13;
<pre>&gt;&gt;&gt; for i,sent in enumerate(doc.sents):<br/>&#13;
...   if i==1 and sent[0].pos_== 'PRON':<br/>&#13;
...     print('The second sentence begins with a pronoun.')<br/>&#13;
The second sentence begins with a pronoun.</pre>&#13;
<p class="indent">In this example, we use an enumerator in the <code>for</code> loop to distinguish the sentences by index. This allows us to filter out sentences that we’re not interested in and check only the second sentence.</p>&#13;
<p class="indent">Identifying the first word in a sentence is a breeze, because its index is always 0. But what about the last one? For example, what if we need to find out how many sentences in the text end with a verb—(not counting any periods, of course)?</p>&#13;
<pre>&gt;&gt;&gt; counter = 0<br/>&#13;
&gt;&gt;&gt; for sent in doc.sents:<br/>&#13;
...   if sent[len(sent)-2].pos_ == 'VERB':<br/>&#13;
...     counter+=1<br/>&#13;
&gt;&gt;&gt; print(counter)<br/>&#13;
1</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_35"/>Although the lengths of sentences vary, we can easily determine the length of a given sentence using the <code>len()</code> function. We reduce the value of <code>len(sent)</code> by 2 for the following reasons: first, the indices always start at 0 and end at size-1. Second, the last token in both sentences in the sample is a period, which we need to ignore.</p>&#13;
<h4 class="h4" id="lev30"><strong><em>The doc.noun_chunks Container</em></strong></h4>&#13;
<p class="noindent">A Doc object’s <code>doc.noun_chunks</code> property allows us to iterate over the noun chunks in the document. A <em>noun chunk</em> is a phrase that has a noun as its head. For example, the previous sentence contains the following noun chunks:</p>&#13;
<pre>A noun chunk<br/>&#13;
a phrase<br/>&#13;
a noun<br/>&#13;
its head</pre>&#13;
<p class="indent">With <code>doc.noun_chunks</code>, we can extract them as follows:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'A noun chunk is a phrase that has a noun as its head.')<br/>&#13;
&gt;&gt;&gt; for chunk in doc.noun_chunks:<br/>&#13;
...   print(chunk)</pre>&#13;
<p class="indent">Alternatively, we might extract noun chunks by iterating over the nouns in the sentence and finding the syntactic children for each noun to form a chunk. Earlier in “<a href="../Text/ch03.xhtml#lev28">Iterating over a Token’s Syntactic Children</a>” on <a href="../Text/ch03.xhtml#page_33">page 33</a>, you saw an example of how to extract a phrase based on the syntactic dependency parse. Now let’s apply this technique to the sample sentence in this example to compose noun chunks manually:</p>&#13;
<pre> for token in doc:<br/>&#13;
<span class="ent">➊</span> if token.pos_=='NOUN':<br/>&#13;
     chunk = ''<br/>&#13;
  <span class="ent">➋</span> for w in token.children:<br/>&#13;
     <span class="ent">➌</span> if w.pos_ == 'DET' or w.pos_ == 'ADJ':<br/>&#13;
         chunk = chunk + w.text + ' '<br/>&#13;
<span class="ent">➍</span> chunk = chunk + token.text<br/>&#13;
   print(chunk)</pre>&#13;
<p class="indent">Iterating over the tokens, we pick up only nouns <span class="ent">➊</span>. Next, in the inner loop, we iterate over a noun’s children <span class="ent">➋</span>, picking up only the tokens that are either determiners or adjectives for the noun chunk (noun chunks can also include some other parts of speech, say, adverbs) <span class="ent">➌</span>. Then we append the noun to the chunk <span class="ent">➍</span>. As a result, the output of the script should be the same as in the previous example.</p>&#13;
<h4 class="h4" id="lev31"><strong><em>Try This</em></strong></h4>&#13;
<p class="noindent">Notice that the words used to modify a noun (determiners and adjectives) are always the leftward syntactic children of the noun. This makes it possible <span epub:type="pagebreak" id="page_36"/>to replace <code>Token.children</code> with <code>Token.lefts</code> in the previous code and then remove the check for the children to be either a determiner or an adjective, as necessary.</p>&#13;
<p class="indent">Rewrite the previous snippet, incorporating the changes suggested here. The resulting set of noun chunks should remain the same in your script.</p>&#13;
<h4 class="h4" id="lev32"><strong><em>The Span Object</em></strong></h4>&#13;
<p class="noindent">The Span object is a slice from a Doc object. In the previous sections, you saw how to use it as a container for a sentence and a noun chunk, derived from <code>doc.sents</code> and <code>doc.noun_chunks</code>, respectively.</p>&#13;
<p class="indent">The Span object’s usage isn’t limited to being a container for sentences or noun chunks only. We can use it to contain an arbitrary set of neighboring tokens in the document by specifying a range of indices, as in the following example:</p>&#13;
<pre>&gt;&gt;&gt; doc=nlp('I want a green apple.')<br/>&#13;
&gt;&gt;&gt; doc[2:5]<br/>&#13;
a green apple</pre>&#13;
<p class="indent">The Span object contains several methods, one of the most interesting of which is <code>span.merge()</code>, which allows us to merge the span into a single token, retokenizing the document. This can be useful when the text contains names consisting of several words.</p>&#13;
<p class="indent">The sample sentence in the following example contains two place names consisting of several words (“Golden Gate Bridge” and “San Francisco”) that we might want to group together. The default tokenization won’t recognize these multi-word place names as single tokens. Look at what happens when we list the text’s tokens:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'The Golden Gate Bridge is an iconic landmark in San Francisco.')<br/>&#13;
&gt;&gt;&gt; [doc[i] for i in range(len(doc))]<br/>&#13;
[The, Golden, Gate, Bridge, is, an, iconic, landmark, in, San, Francisco, .]</pre>&#13;
<p class="indent">Each word and punctuation mark is its own token.</p>&#13;
<p class="indent">With the <code>span.merge()</code> method, we can change this default behavior:</p>&#13;
<pre>&gt;&gt;&gt; span = doc[1:4]<br/>&#13;
&gt;&gt;&gt; lem_id = doc.vocab.strings[span.text]<br/>&#13;
&gt;&gt;&gt; span.merge(lemma = lem_id)<br/>&#13;
Golden Gate Bridge</pre>&#13;
<p class="indent">In this example, we create a lemma for the “Golden Gate Bridge” span, and then pass the lemma to <code>span.merge()</code> as a parameter. (To be precise, we pass on the lemma’s id obtained through the <code>doc.vocab.string</code> attribute.)</p>&#13;
<p class="noindent">Note that the <code>span.merge()</code> method doesn’t merge the corresponding lemmas by default. When called without parameters, it sets the lemma of the merged token to the lemma of the first token of the span being merged. To specify the lemma we want to assign to the merged token, we pass it to <code>span.merge()</code> as the lemma parameter, as illustrated here.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_37"/>Let’s check whether the lemmatizer, part-of-speech tagger, and dependency parser can handle the newly created lemma correctly:</p>&#13;
<pre>&gt;&gt;&gt; for token in doc:<br/>&#13;
      print(token.text, token.lemma_, token.pos_, token.dep_)</pre>&#13;
<p class="indent">This should produce the following output:</p>&#13;
<pre>&#13;
The                the                DET   det<br/>&#13;
Golden Gate Bridge Golden Gate Bridge PROPN nsubj<br/>&#13;
is                 be                 VERB  ROOT<br/>&#13;
an                 an                 DET   det<br/>&#13;
iconic             iconic             ADJ   amod<br/>&#13;
landmark           landmark           NOUN  attr<br/>&#13;
in                 in                 ADP   prep<br/>&#13;
San                san                PROPN compound<br/>&#13;
Francisco          francisco          PROPN pobj<br/>&#13;
.                  .                  PUNCT punct</pre>&#13;
<p class="indent">All the attributes shown in the listing have been assigned to the “Golden Gate Bridge” token correctly.</p>&#13;
<h4 class="h4" id="lev33"><strong><em>Try This</em></strong></h4>&#13;
<p class="noindent">The sentence in the preceding example also contains San Francisco, another multi-word place name that you might want to merge into a single token. To achieve this, perform the same operations as listed in the previous code snippets for the “Golden Gate Bridge” span.</p>&#13;
<p class="indent">When determining the start and end positions for the “San Francisco” span in the document, don’t forget that the indices of the tokens located to the right of the newly created “Golden Gate Bridge” token have been shifted downward respectively.</p>&#13;
<h3 class="h3" id="lev34"><strong>Customizing the Text-Processing Pipeline</strong></h3>&#13;
<p class="noindent">In the previous sections, you learned how spaCy’s container objects represent linguistic units, such as a text and an individual token, allowing you to extract linguistic features associated with them. Let’s now look at the objects in the spaCy API that create those containers and fill them with relevant data.</p>&#13;
<p class="indent">These objects are referred to as processing pipeline components. As you’ve already learned, a pipeline set includes—by default—a part-of-speech tagger, a dependency parser, and an entity recognizer. You can check what pipeline components are available for your nlp object like this:</p>&#13;
<pre>&gt;&gt;&gt; nlp.pipe_names<br/>&#13;
['tagger', 'parser', 'ner']</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_38"/>As discussed in the following sections, spaCy allows you to customize the components in your pipeline to best suit your needs.</p>&#13;
<h4 class="h4" id="lev35"><strong><em>Disabling Pipeline Components</em></strong></h4>&#13;
<p class="noindent">spaCy allows you to load a selected set of pipeline components, disabling those that aren’t necessary. You can do this when creating an nlp object by setting the <code>disable</code> parameter:</p>&#13;
<pre>nlp = spacy.load('en', disable=['parser'])</pre>&#13;
<p class="indent">In this example, we create a processing pipeline without a dependency parser. If we call this nlp instance on a text, the tokens won’t receive dependency labels. The following example illustrates this point clearly:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'I want a green apple.')<br/>&#13;
&gt;&gt;&gt; for token in doc:<br/>&#13;
...   print(<span class="ent">➊</span>token.text, <span class="ent">➋</span>token.pos_, <span class="ent">➌</span>token.dep_)<br/>&#13;
I     PRON<br/>&#13;
want  VERB<br/>&#13;
a     DET<br/>&#13;
green ADJ<br/>&#13;
apple NOUN<br/>&#13;
.     PUNCT</pre>&#13;
<p class="indent">We try to print out the following information for each token from the sample sentence: the text content <span class="ent">➊</span>, a part-of-speech tag <span class="ent">➋</span>, and a dependency label <span class="ent">➌</span>. But the dependency labels don’t appear.</p>&#13;
<h4 class="h4" id="lev36"><strong><em>Loading a Model Step by Step</em></strong></h4>&#13;
<p class="noindent">You can perform several operations in one step with <code>spacy.load()</code>, which loads a model. For example, when you make this call:</p>&#13;
<pre>nlp = spacy.load('en')</pre>&#13;
<p class="noindent">spaCy performs the following steps behind the scenes:</p>&#13;
<ol>&#13;
<li class="noindent">Looking at the name of the model to be loaded, spaCy identifies what Language class it should initialize. In this example, spaCy creates an English class instance with shared vocabulary and other language data.</li>&#13;
<li class="noindent">spaCy iterates over the processing pipeline names, creates corresponding components, and adds them to the processing pipeline.</li>&#13;
<li class="noindent">spaCy loads the model data from disk and makes it available to the Language class instance.</li>&#13;
</ol>&#13;
<p class="indent">These implementation details are hidden by<code> spacy.load()</code>, which in most cases saves you effort and time. But sometimes, you might need to implement these steps explicitly to have fine-grained control over the process. For example, you might need to load a custom component to the processing pipeline. <span epub:type="pagebreak" id="page_39"/>The component could print some information about the Doc object in the pipeline, such as the number of tokens or the presence or absence of certain parts of speech.</p>&#13;
<p class="indent">As usual, more fine-grained control requires you to provide more information. First, rather than specifying a shortcut, you’ll need to obtain the actual model name so you can get the path to the model package.</p>&#13;
<p class="indent">You can identify the full name of the model as follows:</p>&#13;
<pre>&gt;&gt;&gt; print(nlp.meta['lang'] + '_' + nlp.meta['name'])<br/>&#13;
en_core_web_sm</pre>&#13;
<p class="indent">The <code>nlp.meta</code> attribute used in this code is a dictionary that contains the metadata of the loaded model. What you need in this example is the model’s language and the model’s name.</p>&#13;
<p class="indent">Now that you know the model’s name, you can find its location in your system by using the <code>get_package_path</code> utility function:</p>&#13;
<pre>&gt;&gt;&gt; from spacy import util<br/>&#13;
&gt;&gt;&gt; util.get_package_path('en_core_web_sm')<br/>&#13;
PosixPath('/usr/local/lib/python3.5/site-packages/en_core_web_sm')</pre>&#13;
<p class="indent">The path specified in this code might be different on your machine, depending on your Python installation directory. Regardless, this is not the full path. You’ll need to append one more folder to it. The name of this folder is composed of the model name and the model version appended to it. (This is where the model package is located.) You can determine its name as follows:</p>&#13;
<pre>&gt;&gt;&gt; print(nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp.<br/>&#13;
meta['version'])<br/>&#13;
en_core_web_sm-2.0.0</pre>&#13;
<p class="indent">You might also want to look at the list of pipeline components used with the model. (It’s important to know what components are supported in the context of the model and therefore can be loaded to the pipeline.) You can obtain this information via the <code>nlp.meta</code> attribute’s<code> 'pipeline'</code> field, as shown here (or via the <code>nlp.pipe_names</code> attribute introduced in the beginning of “<a href="../Text/ch03.xhtml#lev34">Customizing the Text-Processing Pipeline</a>” on <a href="../Text/ch03.xhtml#page_37">page 37</a>):</p>&#13;
<pre>&gt;&gt;&gt; nlp.meta['pipeline']<br/>&#13;
['tagger', 'parser', 'ner']</pre>&#13;
<p class="indent">With this information, we can create a script that implements the steps provided at the beginning of this section:</p>&#13;
<pre>   &gt;&gt;&gt; lang = 'en'<br/>&#13;
   &gt;&gt;&gt; pipeline = ['tagger', 'parser', 'ner']<br/>&#13;
   &gt;&gt;&gt; model_data_path = '/usr/local/lib/python3.5/site-packages/en_core_web_sm/<br/>&#13;
   en_core_web_sm-2.0.0'<br/>&#13;
<span class="ent">➊</span> &gt;&gt;&gt; lang_cls = spacy.util.get_lang_class(lang) <br/>&#13;
<span epub:type="pagebreak" id="page_40"/>   &gt;&gt;&gt; nlp = lang_cls() <br/>&#13;
<span class="ent">➋</span> &gt;&gt;&gt; for name in pipeline:<br/>&#13;
<span class="ent">➌</span> ...   component = nlp.create_pipe(name) <br/>&#13;
<span class="ent">➍</span> ...   nlp.add_pipe(component)<br/>&#13;
<span class="ent">➎</span> &gt;&gt;&gt; nlp.from_disk(model_data_path)</pre>&#13;
<p class="indent">In this script, we use <code>spacy.util.get_lang_class()</code> <span class="ent">➊</span> to load a Language class. Which class we load depends on the two-letter language code specified as the parameter. In this example, we load English. Next, in a loop <span class="ent">➋</span>, we create <span class="ent">➌</span> and add <span class="ent">➍</span> the pipeline components to the processing pipeline. Then we load a model from disk, specifying the path to it used on your machine <span class="ent">➎</span>.</p>&#13;
<p class="indent">Looking at the code in this script, it might seem that the pipeline components become functional once we’ve added them to the processing pipeline. Actually, we can’t use them until we load the model data, so if we omit the last line of code in the script, we won’t even be able to create a Doc object using this nlp instance.</p>&#13;
<h4 class="h4" id="lev37"><strong><em>Customizing the Pipeline Components</em></strong></h4>&#13;
<p class="noindent">By customizing pipeline components, you can best meet the needs of your application. For example, suppose you want your model’s named entity recognizer system to recognize the word Festy as a city district. By default, it recognizes it as an organization, as illustrated in the following example:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'I need a taxi to Festy.')<br/>&#13;
&gt;&gt;&gt; for ent in doc.ents:<br/>&#13;
...  print(ent.text, ent.label_)<br/>&#13;
<br/>&#13;
Festy ORG</pre>&#13;
<p class="indent">The label <code>ORG</code> stands for companies, agencies, and other institutions. But you want to make the entity recognizer classify it as an entity of type <code>DISTRICT</code> instead.</p>&#13;
<p class="indent">The entity recognizer component is implemented in the spaCy API as the <code>EntityRecognizer</code> class. Using this class’s methods, you can initialize an instance of <code>ner</code> and then apply it to a text. In most cases, you don’t need to perform these operations explicitly; spaCy does it for you under the hood when you create an nlp object and then create a Doc object, respectively.</p>&#13;
<p class="indent">But when you want to update the named entity recognition system of an existing model with your own examples, you’ll need to work with some of the <code>ner</code> object’s methods explicitly.</p>&#13;
<p class="indent">In the following example, you’ll first have to add a new label called <code>DISTRICT</code> to the list of supported entity types. Then you need to create a training example, which is what you’ll show the entity recognizer so it will learn what to apply the <code>DISTRICT</code> label to. The simplest implementation of the preparation steps might look as follows:</p>&#13;
<pre>LABEL = 'DISTRICT'<br/>&#13;
TRAIN_DATA = [<br/>&#13;
<span epub:type="pagebreak" id="page_41"/><span class="ent">➊</span> ('We need to deliver it to Festy.', {<br/>&#13;
    <span class="ent">➋</span> 'entities': [(25, 30, 'DISTRICT')]<br/>&#13;
  }),<br/>&#13;
<span class="ent">➌</span> ('I like red oranges', {<br/>&#13;
'entities': []<br/>&#13;
  })<br/>&#13;
]</pre>&#13;
<p class="indent">For simplicity, this training set contains just two training samples (typically, you need to provide many more). Each training sample includes a sentence that might or might not contain an entity (or entities) to which the new entity label should be assigned <span class="ent">➊</span>. If there is an entity in the sample, you specify its start and end position <span class="ent">➋</span>. The second sentence in the training set doesn’t contain the word Festy at all <span class="ent">➌</span>. This is due to the way the training process is organized. <a href="../Text/ch10.xhtml#ch10">Chapter 10</a> covers the details of this process in more depth.</p>&#13;
<p class="indent">Your next step is to add a new entity label <code>DISTRICT</code> to the entity recognizer: but before you can do this, you must get the instance of the <code>ner</code> pipeline component. You can do this as follows:</p>&#13;
<pre>ner = nlp.get_pipe('ner')</pre>&#13;
<p class="indent">Once you have a <code>ner</code> object, you can add a new label to it using the <code>ner.add_label()</code> method, as shown here:</p>&#13;
<pre>ner.add_label(LABEL)</pre>&#13;
<p class="indent">Another action you need to take before you can start training the entity recognizer is to disable the other pipes to make sure that only the entity recognizer will be updated during the training process:</p>&#13;
<pre>nlp.disable_pipes('tagger')<br/>&#13;
nlp.disable_pipes('parser')</pre>&#13;
<p class="indent">Then you can start training the entity recognizer using the training samples in the <code>TRAIN_DATA</code> list created earlier in this section:</p>&#13;
<pre>optimizer = nlp.entity.create_optimizer()<br/>&#13;
import random<br/>&#13;
<br/>&#13;
for i in range(25):<br/>&#13;
    random.shuffle(TRAIN_DATA)<br/>&#13;
    for text, annotations in TRAIN_DATA:<br/>&#13;
        nlp.update([text], [annotations], sgd=optimizer)</pre>&#13;
<p class="indent">During training, the sample examples are shown to the model in a loop, in random order, to efficiently update the underlying model’s data and avoid any generalizations based on the order of training examples. The execution will take a while.</p>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_42"/>Once the preceding code has successfully completed, you can test how the updated optimizer recognizes the token Festy:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'I need a taxi to Festy.')<br/>&#13;
&gt;&gt;&gt; for ent in doc.ents:<br/>&#13;
... print(ent.text, ent.label_) <br/>&#13;
...<br/>&#13;
Festy DISTRICT</pre>&#13;
<p class="indent">According to the output, it works correctly.</p>&#13;
<p class="indent">Keep in mind that the updates you just made will be lost when you close this Python interpreter session. To address this problem, the <code>Pipe</code> class—the parent of the <code>EntityRecognizer</code> class and other pipeline components classes—has the <code>to_disk()</code> method, which allows you to serialize the pipe to disk:</p>&#13;
<pre>&gt;&gt;&gt; ner.to_disk('/usr/to/ner')</pre>&#13;
<p class="indent">Now you can load the updated component to a new session with the <code>from_disk()</code> method. To make sure it works, close your current interpreter session, start a new one, and then run the following code:</p>&#13;
<pre>   &gt;&gt;&gt; import spacy<br/>&#13;
   &gt;&gt;&gt; from spacy.pipeline import EntityRecognizer<br/>&#13;
<span class="ent">➊</span> &gt;&gt;&gt; nlp = spacy.load('en', disable=['ner'])<br/>&#13;
<span class="ent">➋</span> &gt;&gt;&gt; ner = EntityRecognizer(nlp.vocab)<br/>&#13;
<span class="ent">➌</span> &gt;&gt;&gt; ner.from_disk('/usr/to/ner')<br/>&#13;
<span class="ent">➍</span> &gt;&gt;&gt; nlp.add_pipe(ner)</pre>&#13;
<p class="indent">You load the model, disabling its default <code>ner</code> component <span class="ent">➊</span>. Next, you create a new <code>ner</code> instance <span class="ent">➋</span> and then load it with the data from disk <span class="ent">➌</span>. Then you add the <code>ner</code> component to the processing pipeline <span class="ent">➍</span>.</p>&#13;
<p class="indent">Now you can test it, like this:</p>&#13;
<pre>&gt;&gt;&gt; doc = nlp(u'We need to deliver it to Festy.')<br/>&#13;
&gt;&gt;&gt; for ent in doc.ents:<br/>&#13;
... print(ent.text, ent.label_)<br/>&#13;
<br/>&#13;
Festy DISTRICT</pre>&#13;
<p class="indent">As you can see, the entity recognizer labels the name Festy correctly.</p>&#13;
<p class="indent">Although I’ve shown you how to customize the named entity recognizer only, you can also customize the other pipeline components in a similar way.</p>&#13;
<h3 class="h3" id="lev38"><strong>Using spaCy’s C-Level Data Structures</strong></h3>&#13;
<p class="noindent">Even with spaCy, NLP operations that involve processing large amounts of text can be very time-consuming. For example, you might need to compose a list of the adjectives most appropriate for a certain noun, and to do this, you’ll have to examine a large amount of text. If processing speed is critical to your application, spaCy allows you to take advantage of Cython’s C-level <span epub:type="pagebreak" id="page_43"/>data structures and interfaces. Cython is one of the languages in which spaCy is written (the other one is Python). Because it’s a superset of Python, Cython considers almost all Python code valid Cython code. In addition to Python’s functionality, Cython allows you to natively call C functions and declare fast C types, enabling the compiler to generate very efficient code. You might want to use Cython to speed up time-consuming text processing operations.</p>&#13;
<p class="indent">spaCy’s core data structures are implemented as Cython objects, and spaCy’s public API allows you to access those structures. For details, refer to the Cython Architecture page in the documentation at <em><a href="https://spacy.io/api/cython/">https://spacy.io/api/cython/</a></em>.</p>&#13;
<h4 class="h4" id="lev39"><strong><em>How It Works</em></strong></h4>&#13;
<p class="noindent">To use Cython code with spaCy, you must turn it into a Python extension module that you can then import into your program, as illustrated in <a href="../Text/ch03.xhtml#ch03fig03">Figure 3-3</a>.</p>&#13;
<div class="image"><a id="ch03fig03"/><img src="../Images/fig3-3.jpg" alt="image" width="627" height="98"/></div>&#13;
<p class="figcap"><em>Figure 3-3: Building a Python extension module from a Cython script</em></p>&#13;
<p class="indent">You can do this by saving Cython code in a <em>.pyx</em> file and then running a <em>setup.py</em> Python script that first converts Cython code into corresponding C or C++ code and then invokes a C or C++ compiler. The script generates the Python extension module.</p>&#13;
<h4 class="h4" id="lev40"><strong><em>Preparing Your Working Environment and Getting Text Files</em></strong></h4>&#13;
<p class="noindent">Before you can start building Cython code, you need to install Cython on your machine and obtain a large text file to work with.</p>&#13;
<p class="indent">Install Cython on your machine using <code>pip</code>:</p>&#13;
<pre>pip install Cython</pre>&#13;
<p class="indent">Next, to simulate a time-consuming task and measure performance, you’ll need a large text file. For this, you can use a <em>Wikipedia dump file</em>, which contains a set of pages wrapped in XML. Wikipedia dump files are available for download at <em><a href="https://dumps.wikimedia.org/enwiki/latest/">https://dumps.wikimedia.org/enwiki/latest/</a></em>. Scroll down to the <em>enwiki-latest-pages-articles*.xml-*.bz2</em> files and choose one that is large enough for your tests. But don’t choose one that is too large unless you want to spend hours waiting for your machine to complete your test code. A dump file of 10–100MB should be appropriate.</p>&#13;
<p class="indent">Once you’ve downloaded the file, extract raw text from it with a tool like <em>gensim.corpora.wikicorpus</em> (<em><a href="https://radimrehurek.com/gensim/corpora/wikicorpus.html">https://radimrehurek.com/gensim/corpora/wikicorpus.html</a></em>), which is designed specifically for constructing a text corpus from a Wikipedia database dump.</p>&#13;
<h4 class="h4" id="lev41"><span epub:type="pagebreak" id="page_44"/><strong><em>Your Cython Script</em></strong></h4>&#13;
<p class="noindent">Now let’s write a Cython script that analyzes the text file. For simplicity, suppose all you want to do is count the number of personal pronouns in the submitted text. That means you need to count the number of tokens with the <code>PRP</code> part-of-speech tag assigned to them.</p>&#13;
<p class="notet"><strong><span class="notes">WARNING</span></strong></p>&#13;
<p class="notep"><em>As stated in the documentation, C-level methods intended for use from Cython are designed for speed over safety. Mistakes in the code might cause the execution to crash abruptly.</em></p>&#13;
<p class="indent">In a directory in your local filesystem, create a file called <em>spacytext.pyx</em> and insert the following code into it:</p>&#13;
<pre>   from cymem.cymem cimport Pool<br/>&#13;
   from spacy.tokens.doc cimport Doc<br/>&#13;
   from spacy.structs cimport TokenC<br/>&#13;
   from spacy.typedefs cimport hash_t<br/>&#13;
<br/>&#13;
<span class="ent">➊</span> cdef struct DocStruct:<br/>&#13;
       TokenC* c<br/>&#13;
       int length<br/>&#13;
<br/>&#13;
<span class="ent">➋</span> cdef int counter(DocStruct* doc, hash_t tag):<br/>&#13;
       cdef int cnt = 0<br/>&#13;
       for c in doc.c[:doc.length]:<br/>&#13;
          if c.tag == tag:<br/>&#13;
             cnt += 1<br/>&#13;
       return cnt<br/>&#13;
<br/>&#13;
<span class="ent">➌</span> cpdef main(Doc mydoc):<br/>&#13;
       cdef int cnt<br/>&#13;
       cdef Pool mem = Pool()<br/>&#13;
       cdef DocStruct* doc_ptr = &lt;DocStruct*&gt;mem.alloc(1, sizeof(DocStruct))<br/>&#13;
       doc_ptr.c = mydoc.c<br/>&#13;
       doc_ptr.length = mydoc.length<br/>&#13;
       tag = mydoc.vocab.strings.add('PRP')<br/>&#13;
       cnt = counter(doc_ptr, tag)<br/>&#13;
       print(doc_ptr.length)<br/>&#13;
       print(cnt)</pre>&#13;
<p class="indent">We start with a set of <code>cimport</code> statements to import necessary Cython modules, mostly from the spaCy library.</p>&#13;
<p class="indent">Then we define the Cython struct <code>DocStruct</code> as the container for the text being processed and the <code>TokenC*</code> variable <span class="ent">➊</span>, a pointer to a <code>TokenC</code> struct used in spaCy as the data container for the Token object.</p>&#13;
<p class="indent">Next, we define a Cython function <code>counter</code> <span class="ent">➋</span> that counts the number of personal pronouns in the text.</p>&#13;
<div class="note">&#13;
<p class="notet"><span epub:type="pagebreak" id="page_45"/><strong><span class="notes">NOTE</span></strong></p>&#13;
<p class="notep"><em>The</em> <code>cdef</code> <em>functions won’t be available in the Python code that imports the module. If you want to create a function that will be visible to Python and to take advantage of C-level data structures and interfaces at the same time, you need to declare that function as</em> <code>cpdef</code>.</p>&#13;
</div>&#13;
<p class="indent">Finally, we define a <code>cpdef</code> Cython/Python main function <span class="ent">➌</span> that we can use in Python.</p>&#13;
<h4 class="h4" id="lev42"><strong><em>Building a Cython Module</em></strong></h4>&#13;
<p class="noindent">Unlike Python, you must compile Cython code. You can do this in several ways, the best of which is to write a distutils/setuptools <em>setup.py</em> Python script. Create a <em>setup.py</em> file in the same directory as your Cython script. Your <em>setup.py</em> file should include the following code:</p>&#13;
<pre>   from distutils.core import setup<br/>&#13;
   from Cython.Build import cythonize<br/>&#13;
<span class="ent">➊</span> import numpy<br/>&#13;
<br/>&#13;
   setup(name='spacy text app',<br/>&#13;
     <span class="ent">➋</span> ext_modules=cythonize("spacytext.pyx", language="c++"),<br/>&#13;
      <span class="ent">➌</span> include_dirs=[numpy.get_include()]<br/>&#13;
          )</pre>&#13;
<p class="indent">This is a regular distutils/setuptools <em>setup.py</em> script except for two additions related to the example we’re working with. First, we import <code>numpy</code> <span class="ent">➊</span> and then explicitly specify where to find the <em>.h</em> files of the library <span class="ent">➌</span>. We do this to avoid the <em>numpy/arrayobject.h</em> compilation error that occurs in some systems. We use the other setup option, <code>language = "c++"</code> <span class="ent">➋</span> to instruct the setup process to employ a C++ compiler rather than performing C compilation, which is the default.</p>&#13;
<p class="indent">Now that we have the setup script, you can build your Cython code. You can do this from within a system terminal, as follows:</p>&#13;
<pre>python setup.py build_ext --inplace</pre>&#13;
<p class="indent">A bunch of messages will display during the compilation process. Some of them might be warnings, but they’re rarely critical. For example, you might see this message, which is not critical for the process:</p>&#13;
<pre>#warning "Using deprecated NumPy API ...</pre>&#13;
<h4 class="h4" id="lev43"><strong><em>Testing the Module</em></strong></h4>&#13;
<p class="noindent">After the compilation completes successfully, the <code>spacytext</code> module will be added to your Python environment. To test the newly created module, open a Python session and run the following command:</p>&#13;
<pre>&gt;&gt;&gt; from spacytext import main</pre>&#13;
<p class="indent"><span epub:type="pagebreak" id="page_46"/>If it displays no errors, you can enter the following commands (this assumes your text data is in a <em>test.txt</em> file):</p>&#13;
<pre>   &gt;&gt;&gt; import spacy<br/>&#13;
   &gt;&gt;&gt; nlp = spacy.load('en')<br/>&#13;
<span class="ent">➊</span> &gt;&gt;&gt; f= open("test.txt","rb")<br/>&#13;
   &gt;&gt;&gt; contents =f.read()<br/>&#13;
<span class="ent">➋</span> &gt;&gt;&gt; doc = nlp(contents[:100000].decode('utf8'))<br/>&#13;
<span class="ent">➌</span> &gt;&gt;&gt; main(doc)<br/>&#13;
   21498<br/>&#13;
   216</pre>&#13;
<p class="indent">You open the file in which you have text data for this example in binary mode to obtain a bytes object <span class="ent">➊</span>. If the file is too big, you can pick up only part of its content when creating a Doc object <span class="ent">➋</span>. Once you’ve created the Doc object, you can test the <code>spacytext</code> module you just created with Cython, invoking its <code>main()</code> function <span class="ent">➌</span>.</p>&#13;
<p class="indent">The first figure in the output generated by the <code>spacytext.main()</code> function shows the total number of tokens found in the submitted text. The second figure is the number of personal pronouns found in this same text.</p>&#13;
<h3 class="h3" id="lev44"><strong>Summary</strong></h3>&#13;
<p class="noindent">In this chapter, you looked at the most important of spaCy’s container objects. You also learned how to customize your text-processing pipeline and use spaCy’s C-level data structures and interfaces from Cython.</p>&#13;
</div>&#13;
</body></html>