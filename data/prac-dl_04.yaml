- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: WORKING WITH DATA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据处理**'
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Developing a proper dataset is the single most important part of building a
    successful machine learning model. Machine learning models live and die by the
    phrase “garbage in, garbage out.” As you saw in [Chapter 1](ch01.xhtml#ch01),
    the model uses the training data to configure itself to the problem. If the training
    data is not a good representation of the data the model will receive when it is
    used, we can’t expect our model to perform well. In this chapter, we’ll learn
    how to create a good dataset that represents the data the model will encounter
    in the wild.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一个合适的数据集是构建成功机器学习模型最重要的部分。机器学习模型的成败取决于“垃圾进，垃圾出”这一原则。正如你在[第1章](ch01.xhtml#ch01)中看到的，模型使用训练数据来调整自己，以解决问题。如果训练数据不能很好地代表模型实际应用时将接收到的数据，我们就不能指望模型表现得很好。在本章中，我们将学习如何创建一个能代表模型在实际应用中遇到的数据的好数据集。
- en: Classes and Labels
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别与标签
- en: 'In this book, we’re exploring *classification*: we’re building models that
    put things into discrete categories, or *classes*, like dog breed, flower type,
    digit, and so on. To represent classes, we give each input in our training set
    an identifier called a *label*. A label could be the string “Border Collie" or,
    better still, a number like 0 or 1.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们探讨的是*分类*：我们正在构建将事物划分为离散类别或*类*的模型，比如狗的品种、花的种类、数字等。为了表示类别，我们为训练集中的每个输入赋予一个标识符，称为*标签*。标签可以是“边境牧羊犬”这个字符串，或者更好的是像0或1这样的数字。
- en: Models don’t know what their inputs represent. They don’t care whether the input
    is a picture of a border collie or the value of Google stock. To the model, it’s
    all numbers. The same is true of labels. Because the label for the input has no
    intrinsic meaning to the model, we can represent classes however we choose. In
    practice, class labels are usually integers starting with 0\. So, if there are
    10 classes, the class labels are 0, 1, 2, …, 9\. In [Chapter 5](ch05.xhtml#ch05),
    we’ll work with a dataset that has 10 classes representing images of different
    real-world things. We’ll simply map them to the integers as in [Table 4-1](ch04.xhtml#ch4tab1).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不知道它们的输入代表什么。它们不在乎输入是边境牧羊犬的照片，还是谷歌股票的价值。对模型来说，这一切都是数字。标签也是如此。因为输入的标签对模型没有内在意义，所以我们可以根据需要以任何方式表示类别。在实践中，类别标签通常是从0开始的整数。所以，如果有10个类别，类别标签就是0、1、2、……、9。在[第5章](ch05.xhtml#ch05)中，我们将使用一个包含10个类别的数据集，代表不同现实世界物体的图像。我们只需将它们映射到整数，如[表
    4-1](ch04.xhtml#ch4tab1)所示。
- en: '**Table 4-1:** Label Classes with Integers: 0, 1, 2, . . .'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-1：** 用整数标记类别：0、1、2、……'
- en: '| **Label** | **Actual class** |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| **标签** | **实际类别** |'
- en: '| --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | airplanes |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 飞机 |'
- en: '| 1 | cars |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 汽车 |'
- en: '| 2 | birds |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 鸟 |'
- en: '| 3 | cats |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 猫 |'
- en: '| 4 | deer |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 鹿 |'
- en: '| 5 | dogs |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 狗 |'
- en: '| 6 | frogs |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 青蛙 |'
- en: '| 7 | horses |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 马 |'
- en: '| 8 | ships |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 船 |'
- en: '| 9 | trucks |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 卡车 |'
- en: With that labeling, every training input that is a dog is labeled 5, while every
    input that is a truck is labeled 9\. But what exactly is it that we’re labeling?
    In the next section, we’ll cover features and feature vectors, the very lifeblood
    of machine learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种标记，每个狗的训练输入被标记为5，而每个卡车的输入被标记为9。但我们到底在标记什么呢？在下一节中，我们将介绍特征和特征向量，它们是机器学习的核心。
- en: Features and Feature Vectors
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征与特征向量
- en: Machine learning models take *features* as inputs and deliver, in the case of
    a classifier, a label as output. So what are these features and where do they
    come from?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型将*特征*作为输入，在分类器的情况下输出一个标签。那么，这些特征是什么，来自哪里？
- en: For most models, features are numbers. What the numbers represent depends upon
    the task at hand. If we’re interested in identifying flowers based on measurements
    of their physical properties, our features are those measurements. If we’re interested
    in using the dimensions of cells in a medical sample to predict whether a tumor
    is breast cancer or not, the features are those dimensions. With modern techniques,
    the features might be the pixels of an image (numbers), or a sound’s frequency
    (numbers) or even how many foxes were counted by a camera trap over a two-week
    period (numbers).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数模型来说，特征是数字。这些数字代表什么，取决于当前任务。如果我们希望根据花卉的物理属性测量来识别花卉，那么我们的特征就是这些测量值。如果我们希望使用医学样本中细胞的尺寸来预测肿瘤是否是乳腺癌，那么特征就是这些尺寸。使用现代技术时，特征可能是图像的像素（数字），或声音的频率（数字），甚至是相机陷阱在两周内记录到的狐狸数量（数字）。
- en: Features, then, are whatever numbers we want to use as inputs. The goal of training
    the model is to get it to learn a relationship between the input features and
    the output label. We assume that a relationship exists between the input features
    and output label before training the model. If the model fails to train, it might
    be that there is no relationship to learn.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，特征就是我们想要作为输入使用的任何数字。训练模型的目标是让它学习输入特征和输出标签之间的关系。我们假设在训练模型之前，输入特征和输出标签之间就存在某种关系。如果模型无法训练，那么可能是因为没有可以学习的关系。
- en: After training, feature vectors with unknown class labels are given to the model,
    and the model’s output predicts the class label based on the relationships it
    discovered during training. If the model is repeatedly making poor predictions,
    one possibility is that the selected features are not sufficiently capturing that
    relationship. Before we go into what makes a good feature, let’s take a closer
    look at the features themselves.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后，将具有未知类别标签的特征向量输入模型，模型的输出将基于训练过程中发现的关系预测类别标签。如果模型反复做出错误预测，一个可能的原因是所选特征无法充分捕捉到这种关系。在我们深入讨论什么是好的特征之前，让我们先仔细看一下这些特征本身。
- en: Types of Features
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征类型
- en: To recap, features are numbers representing something that is measured or known,
    and *feature vectors* are sets of these numbers used as inputs to the model. There
    are different kinds of numbers you could use as features, and as you’ll see, they’re
    not all created equal. Sometimes you’ll have to manipulate them before you can
    input them into your model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，特征是表示某些已测量或已知事物的数字，*特征向量*是这些数字的集合，作为模型的输入。有不同类型的数字可以作为特征，正如你将看到的，它们并非都一样。你有时需要在将它们输入模型之前进行一些操作。
- en: Floating-Point Numbers
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 浮动点数
- en: In [Chapter 5](ch05.xhtml#ch05), we’ll be building a historic flower dataset.
    The features of that dataset are actual measurements of things like a flower’s
    sepal width and height (in centimeters). A typical measurement might be 2.33 cm.
    This is a *floating-point* number—a number with a decimal point, or, if you remember
    your high school math courses, a *real* number. Most models want to work with
    floating-point numbers, so you can just use the measurements as they are. Floating-point
    numbers are *continuous*, meaning there are an infinite number of values between
    one integer and the next, so we have a smooth transition between them. As we’ll
    see later on, some models expect continuous values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](ch05.xhtml#ch05)中，我们将构建一个历史花卉数据集。该数据集的特征是花卉的实际测量值，如花萼的宽度和高度（以厘米为单位）。一个典型的测量值可能是
    2.33 厘米。这是一个*浮动点*数——一个带有小数点的数字，或者如果你记得高中数学课程的话，它是一个*实数*。大多数模型希望处理浮动点数，因此你可以直接使用这些测量值。浮动点数是*连续*的，这意味着在一个整数和下一个整数之间存在无限多个值，因此它们之间有平滑的过渡。正如我们稍后将看到的，某些模型期望连续值。
- en: Interval Values
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 间隔值
- en: 'Floating-point numbers don’t work for everything, however. Clearly, flowers
    cannot have 10.14 petals, though they might have 9, 10, or 11\. These numbers
    are *integers*: whole numbers without a fractional part or a decimal point. Unlike
    floating-point numbers, they are *discrete*, which means they pick out only certain
    values, leaving gaps in between. Fortunately for us, integers are just special
    real numbers, so models can use them as they are.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，浮动点数并不适用于所有情况。显然，花朵不可能有 10.14 片花瓣，尽管它们可能有 9、10 或 11 片花瓣。这些数字是*整数*：没有小数部分或小数点的整数。与浮动点数不同，它们是*离散*的，这意味着它们只选取某些特定的值，并且中间会留有空隙。幸运的是，整数只是特殊的实数，因此模型可以直接使用它们。
- en: 'In our petal example, the difference between 9, 10, and 11 is meaningful in
    that 11 is bigger than 10, and 10 is bigger than 9\. Not only that, but 11 is
    bigger than 10 in exactly the same way that 10 is bigger than 9\. The difference,
    or interval, between the values is the same: 1\. This value is called an *interval*
    value.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的花瓣示例中，9、10 和 11 之间的差异是有意义的，因为 11 大于 10，10 又大于 9。不仅如此，11 比 10 大的方式正好与 10
    比 9 大的方式相同。这些值之间的差异或间隔是相同的：1。这个值称为*间隔*值。
- en: The pixels in an image are interval values, because they represent the (assumed
    linear) response of some measurement device, like a camera or an MRI machine,
    to some physical process like intensity and color of visible light or the number
    of hydrogen protons in free water in tissue. The key point is that if value *x*
    is the next number in the sequence after value *y*, and value *z* is the number
    before value *y*, then the difference between *x* and *y* is the same difference
    as between *y* and *z*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的像素是区间值，因为它们表示某些测量设备（如相机或 MRI 机器）对某些物理过程（如可见光的强度和颜色，或组织中自由水分子中的氢原子数）的（假定为线性）响应。关键点是，如果值
    *x* 是值 *y* 序列中的下一个数字，且值 *z* 是值 *y* 之前的数字，则 *x* 和 *y* 之间的差异与 *y* 和 *z* 之间的差异是相同的。
- en: Ordinal Values
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有序值
- en: Sometimes the interval between the values is not the same. For example, some
    models include someone’s educational level to predict whether or not they will
    default on a loan. If we encode someone’s educational level by counting their
    years of schooling, we could use that safely since the difference between 10 years
    of schooling and 8 is the same as the difference between 8 years of schooling
    and 6\. However, if we simply assign 1 for “completed high school,” 2 for “has
    an undergraduate degree,” and 3 for “has a doctorate or other professional degree,”
    we’d probably be in trouble; while 3 > 2 > 1 is true, whether or not meaningful
    for our model, the difference between the values represented by 3 and 2 and 2
    and 1 is not the same. Features like these are called *ordinal* because they express
    an ordering, but the differences between the values are not necessarily always
    the same.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，值之间的间隔并不相同。例如，一些模型使用一个人的教育水平来预测他们是否会违约。如果我们通过计算一个人的受教育年限来编码他们的教育水平，我们可以安全地使用它，因为
    10 年受教育和 8 年受教育之间的差距与 8 年受教育和 6 年受教育之间的差距是相同的。然而，如果我们仅仅将 1 分配给“完成高中”，将 2 分配给“有本科学位”，将
    3 分配给“有博士学位或其他专业学位”，我们可能会遇到问题；虽然 3 > 2 > 1 是对的，但对于我们的模型来说，3 和 2 以及 2 和 1 之间的差异并不相同。像这样的特征被称为*有序*，因为它们表示一种排序，但值之间的差异不一定总是相同的。
- en: Categorical Values
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分类值
- en: Sometimes we use numbers as codes. We might encode sex as 0 for male and 1 for
    female, for example. In this case, 1 is not understood to be greater than 0 or
    less than 0, so these are not interval or ordinal values. Instead, these are *categorical*
    values. They express a category but say nothing about any relationship between
    the categories.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们使用数字作为代码。例如，我们可能将性别编码为 0 表示男性，1 表示女性。在这种情况下，1 不被理解为大于 0 或小于 0，因此这些不是区间值或有序值。相反，这些是*分类*值。它们表示一个类别，但没有说明类别之间的任何关系。
- en: Another common example, perhaps relevant to classifying flowers, is color. We
    might use 0 for red, 1 for green, and 2 for blue. Again, no relationship exists
    between 0, 1, or 2 in this case. This doesn’t mean we can’t use categorical features
    with our models, but it does mean that we usually can’t use them as they are since
    most types of machine learning models expect at least ordinal, if not interval
    numbers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的例子，或许与分类花卉相关，是颜色。我们可以用 0 表示红色，1 表示绿色，2 表示蓝色。同样地，在这种情况下，0、1 或 2 之间没有任何关系。这并不意味着我们不能在模型中使用分类特征，但这确实意味着我们通常不能直接使用它们，因为大多数机器学习模型至少需要有序的（如果不是区间型）数字。
- en: We can make categorical values at least ordinal by using the following trick.
    If we wanted to use a person’s sex as an input, instead of saying 0 for male and
    1 for female, we would create a two-element vector, one element for each possibility.
    The first digit in the vector will indicate whether the input is male by signaling
    either 0 (meaning they’re not male) or 1 (meaning they are). The second digit
    will indicate whether or not they are female. We map the categorical values to
    a binary vector, as shown in [Table 4-2](ch04.xhtml#ch4tab2).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下技巧将分类值至少转化为有序值。如果我们想将一个人的性别作为输入，而不是将 0 赋给男性、1 赋给女性，我们可以创建一个由两个元素组成的向量，每个元素对应一个可能性。向量中的第一个数字将指示输入是否为男性，通过表示
    0（意味着不是男性）或 1（意味着是男性）。第二个数字将指示是否为女性。我们将分类值映射到一个二进制向量，如[表 4-2](ch04.xhtml#ch4tab2)所示。
- en: '**Table 4-2:** Representing Categories as Vectors'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-2：** 将类别表示为向量'
- en: '| **Categorical value** |  | **Vector representation** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **分类值** |  | **向量表示** |'
- en: '| --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | → | 1 0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 0 | → | 1 0 |'
- en: '| 1 | → | 0 1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 1 | → | 0 1 |'
- en: Here a 0 in the “is male” feature is meaningfully less than a 1 in that feature,
    which fits the definition of an ordinal value. The price we pay is to expand the
    number of features in our feature vector, as we need one feature for each of the
    possible categorical values. With five colors, for example, we would need a five-element
    vector; with five thousand, a five-thousand-element vector.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在“是否为男性”这一特征中，0 的值比 1 的值更具有实际意义，这符合有序值（ordinal value）的定义。我们为此付出的代价是增加特征向量的维度，因为我们需要为每一个可能的类别值分配一个特征。例如，如果有五种颜色，我们需要一个五元素的向量；如果有五千种颜色，则需要一个五千元素的向量。
- en: To use this scheme, the categories must be mutually exclusive, meaning there
    will be only one 1 in each row. Because there’s always only one nonzero value
    per row, this approach is sometimes called a *one-hot encoding*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种方案，类别必须是互斥的，意味着每一行中只有一个 1。由于每行中始终只有一个非零值，这种方法有时被称为*独热编码（one-hot encoding）*。
- en: Feature Selection and the Curse of Dimensionality
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征选择与维度灾难
- en: 'This section is about *feature selection*, the process of selecting which features
    to use in your feature vectors, and why you shouldn’t include features you don’t
    need. Here’s a good rule of thumb: the feature vector should contain only features
    that capture aspects of the data that allow the model to generalize to new data.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的是*特征选择*，即选择在特征向量中使用哪些特征，以及为什么不应包含不需要的特征。这里有一个好的经验法则：特征向量应仅包含能够捕捉数据中有助于模型推广到新数据的方面的特征。
- en: In other words, features should capture aspects of the data that help the model
    separate the classes. It’s impossible to be more explicit, since the set of best
    features are always dataset specific, unknowable in advance. But that doesn’t
    mean we can’t say things that might be helpful in guiding us toward a useful set
    of features for whatever dataset we’re working with.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，特征应该捕捉到有助于模型区分不同类别的数据方面。不可能更加明确地说明这一点，因为最佳特征的集合总是特定于数据集的，事先无法知晓。但这并不意味着我们不能说一些有助于引导我们朝着合适的特征选择方向前进的话，无论我们在处理哪个数据集。
- en: Like many things in machine learning, selecting features comes with trade-offs.
    We need enough features to capture all the relevant parts of the data so that
    the model has something to learn from, but if we have too many features, we fall
    victim to the *curse of dimensionality*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多机器学习中的问题一样，选择特征也需要权衡。我们需要足够的特征来捕捉数据中的所有相关部分，以便模型能够从中学习，但如果特征过多，我们就会陷入*维度灾难（curse
    of dimensionality）*。
- en: To explain what this means, let’s look at an example. Suppose our features are
    all restricted to the range [0,1). That’s not a typo; we’re using interval notation,
    where a square bracket means the bound is included in the range, and a parenthesis
    means the bound is excluded. So here 0 is allowed but 1 isn’t. We’ll also assume
    our feature vectors are either two-dimensional or three-dimensional. That way,
    we can plot each feature vector as a point in a 2D or 3D space. Finally, we’ll
    simulate datasets by selecting feature vectors, 2D or 3D, uniformly at random
    so that each element of the vector is in [0,1).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这意味着什么，让我们看一个例子。假设我们的特征值都限制在 [0,1) 范围内。这不是打字错误；我们使用的是区间表示法，其中方括号表示边界值包含在范围内，而圆括号表示边界值不包含在范围内。所以这里
    0 是允许的，但 1 不允许。我们还假设我们的特征向量是二维或三维的。这样，我们可以将每个特征向量作为一个点绘制在二维或三维空间中。最后，我们将通过从 [0,1)
    范围内随机选择特征向量（二维或三维）来模拟数据集。
- en: Let’s fix the number of samples at 100\. If we have two features, or a 2D space,
    we can represent 100 randomly selected 2D vectors as in the top of [Figure 4-1](ch04.xhtml#ch4fig1).
    Now, if we have three features, or a 3D space, those same 100 features look like
    the bottom of [Figure 4-1](ch04.xhtml#ch4fig1).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 假设样本数固定为 100。如果我们有两个特征，或者是二维空间，我们可以将 100 个随机选择的二维向量表示为 [图 4-1](ch04.xhtml#ch4fig1)
    上方的形式。现在，如果我们有三个特征，或者是三维空间，那么这 100 个特征看起来就像 [图 4-1](ch04.xhtml#ch4fig1) 下方的形式。
- en: '![image](Images/04fig01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig01.jpg)'
- en: '*Figure 4-1: One hundred random samples in 2D space (top) and in 3D space (bottom)*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-1：二维空间中 100 个随机样本（上）和三维空间中 100 个随机样本（下）*'
- en: Since we’re assuming our feature vectors can come from anywhere in the 2D or
    3D space, we want our dataset to sample as much of that space as possible so that
    it represents the space well. We can get a measure of how well the 100 points
    are filling the space by splitting each axis into 10 equal sections. Let’s call
    these sections *bins*. We’ll end up with 100 bins in the 2D space, because it
    has two axes (10 × 10), and 1,000 in the 3D space, because it has three axes (10
    × 10 × 10). Now, if we count the number of bins occupied by at least one point
    and divide that number by the total number of bins, we’ll get the fraction of
    bins that are occupied.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设特征向量可以来自 2D 或 3D 空间的任何位置，因此我们希望我们的数据集能够尽可能多地从这个空间中采样，以便它能很好地代表这个空间。我们可以通过将每个轴分成
    10 个等分来衡量 100 个点填充空间的效果。我们称这些分段为 *bins*。在 2D 空间中，我们将得到 100 个 bins，因为它有两个轴（10 ×
    10）；而在 3D 空间中，我们将得到 1,000 个 bins，因为它有三个轴（10 × 10 × 10）。现在，如果我们统计至少有一个点占据的 bins
    数量，并将这个数量除以总的 bins 数量，我们就能得到占据的 bins 的比例。
- en: 'Doing this gives us 0.410 (out of a maximum of 1.0) for the 2D space and 0.048
    for the 3D space. This means that 100 samples were able to sample about half of
    the 2D feature space. Not bad! But 100 samples in the 3D feature space sampled
    only about 5 percent of the space. To fill the 3D space to the same fraction as
    the 2D space, we’d need about 1,000—or 10 times as many as we have. This general
    rule applies as the dimensionality increases: a 4D feature space would need about
    10,000 samples, while a 10D feature space would need about 10,000,000,000! As
    the number of features increases, the amount of training data we need to get a
    representative sampling of the possible feature space increases dramatically,
    approximately as 10^(*d*), where *d* is the number of dimensions. This is the
    *curse of dimensionality*, and it was the bane of machine learning for decades.
    Fortunately for us, modern deep learning has overcome this curse, but it’s still
    relevant when working with traditional models like the ones we will explore in
    [Chapter 6](ch06.xhtml#ch06).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这样计算得到 2D 空间的占据率为 0.410（最大为 1.0），3D 空间为 0.048。这意味着 100 个样本能够大约填充一半的 2D 特征空间。还不错！但是
    100 个样本在 3D 特征空间中只填充了大约 5% 的空间。为了使 3D 空间的填充率与 2D 空间相同，我们需要大约 1,000 个样本——也就是我们现有样本的
    10 倍。这个普遍规律随着维度的增加而适用：一个 4D 特征空间需要大约 10,000 个样本，而一个 10D 特征空间需要大约 10,000,000,000
    个样本！随着特征数量的增加，为了得到一个代表性的特征空间采样，我们需要的训练数据量急剧增加，约为 10^(*d*)，其中 *d* 是维度的数量。这就是 *维度灾难*，它曾经是机器学习的噩梦。幸运的是，现代深度学习已经克服了这一困境，但在处理传统模型时，这一问题依然存在，就像我们将在[第6章](ch06.xhtml#ch06)中探讨的那样。
- en: For example, a typical color image on your computer might have 1,024 pixels
    on a side where each pixel requires 3 bytes to specify the color as a mix of red,
    green, and blue. If we wanted to use this image as input to a model, we’d need
    a feature vector with *d* = 1024 × 1024 × 3 = 3,145,728 elements. This means we’d
    need some 10^(3,145,728) samples to populate our feature space. Clearly, this
    is not possible. We’ll see in [Chapter 12](ch12.xhtml#ch12) how to overcome this
    curse by using a convolutional neural network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，计算机上典型的彩色图像可能有 1,024 个像素，每个像素需要 3 个字节来指定红色、绿色和蓝色的混合。如果我们想将这张图像作为输入传递给模型，我们需要一个特征向量，*d*
    = 1024 × 1024 × 3 = 3,145,728 个元素。这意味着我们需要大约 10^(3,145,728) 个样本来填充我们的特征空间。显然，这是不可能的。我们将在[第12章](ch12.xhtml#ch12)中看到如何通过使用卷积神经网络来克服这一困境。
- en: Now that we know about classes, features, and feature vectors, let’s describe
    what it means to have a good dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了类别、特征和特征向量，让我们来描述一下什么是一个好的数据集。
- en: Features of a Good Dataset
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 好数据集的特征
- en: 'The dataset is everything. This is no exaggeration, since we build the model
    from the dataset. The model has parameters—be they the weights and biases of a
    neural network, the probabilities of each feature occurring in a Naïve Bayes model,
    or the training data itself in the case of Nearest Neighbors. The parameters are
    what we use the training data to find out: they encode the knowledge of the model
    and are learned by the training algorithm.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集就是一切。这一点并不夸张，因为我们是从数据集中构建模型的。模型有参数——无论是神经网络的权重和偏置，还是在朴素贝叶斯模型中每个特征出现的概率，或者是最近邻模型中的训练数据本身。参数是我们用训练数据来求解的内容：它们编码了模型的知识，并通过训练算法进行学习。
- en: Let’s back up a little bit and define the term *dataset* as we we’ll use it
    in this book. Intuitively, we understand what a dataset is, but let’s be more
    scientific and define it as a collection of pairs of values, {*X,Y*}, where *X*
    is an *input* to the model and *Y* is a label. Here *X* is some set of values
    that we’ve measured and grouped together, like length and width of flower parts,
    and *Y* is the thing we want to teach the model to tell us, such as which flower
    or which animal the data best represents.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微退后一点，定义一下在本书中将要使用的术语*数据集*。直观上，我们理解什么是数据集，但我们需要更科学地定义它：数据集是由一对对值组成的集合，{*X,Y*}，其中
    *X* 是模型的*输入*，*Y* 是标签。这里的 *X* 是我们测量并组合在一起的一些值，比如花朵部位的长度和宽度，而 *Y* 是我们希望教会模型告诉我们的东西，比如这些数据最好代表哪种花或哪种动物。
- en: For *supervised* learning, we act as the teacher, and the model acts as the
    student. We are teaching the student by presenting example after example, saying
    things like “this is a cat” and “this is a dog,” much as we would teach a small
    child with a picture book. In this case, the *dataset* is a collection of examples,
    and *training* consists of showing the examples to the model repeatedly, until
    the model “gets it”—that is, until the parameters of the model are conditioned
    and adjusted to minimize the error made by the model for this particular dataset.
    This is the learning part of machine learning.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*监督学习*，我们充当老师，模型则充当学生。我们通过一个接一个地呈现示例来教学生，就像我们教小孩子看图画书一样，比如“这是猫”和“这是狗”。在这种情况下，*数据集*是一个示例集合，*训练*是将这些示例反复展示给模型，直到模型“明白”为止——也就是，直到模型的参数被调节到最小化在特定数据集上的误差。这就是机器学习中的学习部分。
- en: Interpolation and Extrapolation
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 插值与外推
- en: '*Interpolation* is the process of estimating within a certain known range.
    And *extrapolation* occurs when we use the data we have to estimate outside the
    known range. Generally speaking, our models are more accurate when they in some
    sense interpolate, which means we need a dataset that is a comprehensive representation
    of the range of values that could be used as inputs to the model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*插值*是指在已知范围内进行估算的过程。而*外推*发生在我们使用已有的数据来估算已知范围之外的值。一般来说，当我们的模型在某种程度上进行插值时，它的准确性更高，这意味着我们需要一个数据集，它能够全面地代表所有可能作为模型输入的值的范围。'
- en: As an example, let’s look at world population, in billions, from 1910 to 1960
    ([Table 4-3](ch04.xhtml#ch4tab3)). We have data for every 10 years in our known
    range, 1910 to 1960.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，看看1910年到1960年间的世界人口（单位：十亿）数据（[表4-3](ch04.xhtml#ch4tab3)）。我们在已知的范围内（1910年到1960年）每10年有一组数据。
- en: '**Table 4-3:** The World Population by Decade'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**表4-3：** 按十年划分的世界人口'
- en: '| **Year** | **Population (billions)** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **年份** | **人口（十亿）** |'
- en: '| --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1910 | 1.750 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1910 | 1.750 |'
- en: '| 1920 | 1.860 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 1920 | 1.860 |'
- en: '| 1930 | 2.070 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 1930 | 2.070 |'
- en: '| 1940 | 2.300 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1940 | 2.300 |'
- en: '| 1950 | 2.557 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1950 | 2.557 |'
- en: '| 1960 | 3.042 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1960 | 3.042 |'
- en: 'If we find the “best fitting” line to plot through this data, we can use it
    as a model to predict values. This is called *linear regression*, and it allows
    us to estimate the population for any year we choose. We’ll skip the actual fitting
    process, which you can do simply with online tools, and jump to the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到一条“最佳拟合”线来绘制这些数据，我们可以利用这条线作为模型来预测值。这就是*线性回归*，它允许我们估算任何我们选择年份的人口。我们将跳过实际拟合的过程（你可以通过在线工具轻松完成），直接进入模型：
- en: '*p* = 0.02509*y* – 46.28'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 0.02509*y* – 46.28'
- en: 'For any year, *y*, we can get an estimate of the population, *p*. What was
    the world population in 1952? We don’t have actual data for 1952 in our table,
    but using the model, we can estimate it like so:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何一年，*y*，我们都可以估算出人口*P*。1952年的世界人口是多少？我们的表格中没有1952年的实际数据，但通过模型我们可以这样估算：
- en: '*p* = 0.02509(1952) – 46.28 = 2.696 billion'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 0.02509(1952) – 46.28 = 2.696 十亿'
- en: By checking the actual world population data for 1952, we know that it was 2.637
    billion, so our estimate of 2.696 billion was only some 60 million off. The model
    seems to be pretty good!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看1952年实际的世界人口数据，我们知道它是26.37亿，所以我们估算的26.96亿只有大约6000万的误差。看来这个模型还不错！
- en: In using the model to estimate the world population in 1952, we performed interpolation.
    We made an estimate for a value that was between data points we had, and the model
    gave us a good result. Extrapolation, on the other hand, is measuring beyond what
    is known, outside the range of our data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用该模型估算1952年世界人口时，我们进行了插值。我们为一个在已知数据点之间的值进行了估算，而模型给出了一个不错的结果。另一方面，外推是指超出已知数据范围进行估算，即在数据之外进行测量。
- en: 'Let’s use our model to estimate world population in 2000, 40 years after the
    data we used to build our model ends:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用我们的模型来估算2000年的世界人口，距离我们用来构建模型的数据结束40年：
- en: '*p* = 0.02509(2000) – 46.28 = 3.900 billion'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* = 0.02509(2000) – 46.28 = 39.00亿'
- en: According to the model, it should be close to 3.9 billion, but we know from
    actual data that the world population in 2000 was 6.089 billion. Our model is
    off by over 2 billion people. What happened here is that we applied the model
    to input it wasn’t suited for. If we remain in the range of inputs that the model
    is “trained” to know about, namely, dates from 1910 through 1960, then the model
    performs well enough. Once we went beyond the model’s training, however, it fell
    apart because it assumed knowledge we didn’t possess.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型，它应该接近39亿，但我们从实际数据中知道，2000年世界人口是60.89亿。我们的模型差了超过20亿人。发生这种情况是因为我们将模型应用于它不适合处理的输入。如果我们仍然在模型“训练”过的输入范围内，也就是1910到1960年的数据，那么模型的表现足够好。然而，一旦超出模型的训练范围，它就会崩溃，因为它假设了我们并不掌握的知识。
- en: When we interpolate, the model will see examples that are similar to the set
    of examples it saw during training. Perhaps unsurprisingly, it will do better
    on these examples than when we extrapolate and ask the model to go beyond its
    training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行插值时，模型会看到与它在训练中看到的例子相似的样本。也许并不奇怪的是，在这些例子上，它的表现会比我们进行外推并要求模型超越其训练范围时要好。
- en: 'When it comes to classification, it’s essential we have comprehensive training
    data. Let’s assume we’re training a model to identify dog breeds. In our dataset,
    we have hundreds of images of classic black-and-white border collies like the
    one on the left in [Figure 4-2](ch04.xhtml#ch4fig2). If we then give the model
    a new image of a classic border collie, we will, hopefully, get back a correct
    label: “Border Collie.” This is akin to asking the model to interpolate: it’s
    working with something is has already seen before because the “Border Collie”
    label in the training data included many examples of classic border collies.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类时，拥有全面的训练数据是至关重要的。假设我们正在训练一个模型来识别犬种。在我们的数据集中，我们有数百张经典黑白相间的边境牧羊犬的图片，就像[图4-2](ch04.xhtml#ch4fig2)左侧的那只。如果我们接着给模型一张新的经典边境牧羊犬图片，我们希望能够得到正确的标签：“边境牧羊犬”。这就类似于让模型进行插值：它正在处理一些它以前见过的东西，因为训练数据中的“边境牧羊犬”标签包含了许多经典边境牧羊犬的例子。
- en: '![image](Images/04fig02.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig02.jpg)'
- en: '*Figure 4-2: A border collie with classic markings (left), a border collie
    with liver-colored markings (middle), an Australian shepherd (right)*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-2：一只具有经典标记的边境牧羊犬（左），一只具有肝色标记的边境牧羊犬（中），一只澳大利亚牧羊犬（右）*'
- en: However, not every border collie has the classic border collie markings. Some
    are marked like the collie in the middle of [Figure 4-2](ch04.xhtml#ch4fig2).
    Since we didn’t include images like this in the training set, the model must now
    try to go beyond what it was trained to do and give a correct output label for
    an instance of a class it was trained on but of a type it was not trained with.
    It will likely fail, giving a false output like “Australian Shepherd,” a breed
    similar to a border collie, as seen on the right of [Figure 4-2](ch04.xhtml#ch4fig2).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不是每只边境牧羊犬都有典型的边境牧羊犬标记。有些狗的标记像[图4-2](ch04.xhtml#ch4fig2)中间的牧羊犬。由于我们在训练集中没有包括这样的图片，模型现在必须尝试超越它的训练内容，并为它所接受训练的类别实例提供正确的输出标签，尽管它没有接受过这种类型的训练。它可能会失败，给出类似“澳大利亚牧羊犬”的错误输出，正如[图4-2](ch04.xhtml#ch4fig2)右侧所示，澳大利亚牧羊犬与边境牧羊犬相似。
- en: The key concept to remember, however, is that the dataset must cover the full
    range of variation *within* the classes the model will see when the model is predicting
    labels for unknown inputs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要记住的关键概念是，数据集必须涵盖模型在预测未知输入标签时所看到的类别*内部*的所有变化范围。
- en: The Parent Distribution
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 父分布
- en: The dataset must be representative of the classes it’s modeling. Buried in this
    idea is the assumption that our data has a *parent distribution*, an unknown data
    generator that created the particular dataset we’re using.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集必须代表它所建模的类别。这个观点中隐含了一个假设，即我们的数据有一个*父分布*，即一个未知的数据生成器，它创造了我们所使用的特定数据集。
- en: Consider this parallel from philosophy. The ancient Greek philosopher Plato
    uses the concept of ideals. In his view, there was an ideal chair somewhere “out
    there,” and all existing chairs were more or less perfect copies of that ideal
    chair. This is what we mean by the relationship between the dataset we are using,
    the copy, and the parent distribution, the ideal generator. We want the dataset
    to be a representation of the ideal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参考一下哲学中的这一类比。古希腊哲学家柏拉图使用了理想的概念。在他看来，某个地方“外面”有一个理想的椅子，所有现存的椅子都是该理想椅子的或多或少完美的复制品。这就是我们所说的数据集、复制品与父分布（理想生成器）之间的关系。我们希望数据集能够作为理想的一个表现。
- en: We can think of a dataset as a sample from some unknown process that produces
    data according to the parent distribution. The type of data it produces—the values
    and ranges of the features—will follow some unknown, statistical rule. For example,
    when you roll a die, each of the six values is equally likely in the long run.
    We call this a *uniform parent distribution*. If you make a bar graph of the number
    of times each value appears as you roll the die many times, then you will get
    a (more or less) horizontal line since each value is equally likely to happen.
    We see a different distribution when we measure the height of adults. The distribution
    of heights will have a form with two humps, one around mean male height and the
    other around mean female height.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把数据集看作是来自某个未知过程的样本，这个过程根据父分布生成数据。它生成的数据类型——特征的值和范围——将遵循某种未知的统计规则。例如，当你掷骰子时，六个值在长时间内是等概率出现的。我们称这种分布为*均匀父分布*。如果你绘制一张柱状图，显示每个值出现的次数，随着你多次掷骰子，你会得到一条（或多或少）水平的线，因为每个值出现的概率是相等的。当我们测量成年人身高时，我们看到的是不同的分布。身高的分布呈现两峰形态，一峰集中在男性平均身高附近，另一峰则集中在女性平均身高附近。
- en: The parent distribution is what generates this overall shape. The training data,
    the test data, and the data you give the model to make decisions must all come
    from the same parent distribution. This is a fundamental assumption models make,
    and one that shouldn’t seem too surprising to us. Still, sometimes it’s easy to
    mix things up and train with data from one parent distribution while testing or
    using the model with data from a different parent distribution. (How to train
    with one parent distribution and use that model with data from a different distribution
    is a very active research area at the moment. Search for “domain adaptation.”)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 父分布是生成这种整体形状的来源。训练数据、测试数据以及你给模型做决策的数据都必须来自同一个父分布。这是模型的一个基本假设，也不应该让我们感到太惊讶。不过，有时很容易混淆，训练时使用的是一个父分布的数据，而测试或使用模型时则使用来自不同父分布的数据。（如何在一个父分布上训练模型，并在不同分布的数据上使用这个模型，正是目前一个非常活跃的研究领域。你可以搜索“领域适应”。）
- en: Prior Class Probabilities
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 先验类别概率
- en: The *prior class probability* is the probability with which each class in the
    dataset appears in the wild.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*先验类别概率*是数据集中每个类别在自然界中出现的概率。'
- en: In general, we want our dataset to match the prior probabilities of the classes.
    If class A appears 85 percent of the time and class B only 15 percent of the time,
    then we want class A to appear 85 percent of the time and class B to appear 15
    percent of the time in our training set.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们希望数据集能够匹配类别的先验概率。如果类别A出现的概率是85%，类别B只有15%，那么我们希望在训练集里类别A出现85%的时间，类别B出现15%的时间。
- en: There are exceptions, however. Say one of the classes we want the model to learn
    is rare, showing up only once for every 10,000 inputs. If we make the dataset
    strictly follow the actual prior probabilities, the model might not see enough
    examples of the rare class to learn anything helpful about it. And, worse yet,
    what if the rare class is the class we are most interested in?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在一些例外。例如，假设我们希望模型学习的某个类别是稀有的，在每10,000个输入中只出现一次。如果我们让数据集严格遵循实际的先验概率，模型可能无法看到足够的稀有类别示例，从而无法学到关于它的有用信息。更糟糕的是，如果这个稀有类别正是我们最感兴趣的类别呢？
- en: For example, let’s pretend we’re building a robot that locates four-leaf clovers.
    We’ll assume that we already know that the input to the model is a clover; we
    just want to know whether it has three or four leaves. We know that an estimated
    1 in every 5,000 clovers is a four-leaf clover. Building a dataset with 5,000
    three-leaf clovers for every instance of a four-leaf clover seems reasonable until
    we realize that a model that simply says every input is a three-leaf clover will
    be right, on average, 4,999 times out of 5,000! It will be an extremely accurate
    but completely useless model because it never finds the class we’re interested
    in.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在构建一个能够定位四叶草的机器人。我们假设我们已经知道输入给模型的是一株草，只是想知道它是三叶草还是四叶草。我们知道，大约每5000株草中就有一株是四叶草。构建一个包含每一株四叶草对应5000株三叶草的数据集似乎是合理的，直到我们意识到，一个简单地认为每个输入都是三叶草的模型，平均来说，5000次中有4999次是正确的！它将是一个极其准确但完全无用的模型，因为它从未找出我们感兴趣的类别。
- en: Instead, we might use a 10:1 ratio of three-leaf to four-leaf clovers. Or, when
    training the model, we might start with an even number of three- and four-leaf
    clovers, and then, after training for a time, change to a mix that is increasingly
    closer to the actual prior probability. This trick doesn’t work for all model
    types, but it does work for neural networks. Why this trick works is poorly understood
    but, intuitively, we can imagine the network learning first about the visual difference
    between a three-leaf and four-leaf clover and then learning something about the
    actual likelihood of encountering a four-leaf clover as the mix changes to be
    closer to the actual prior probabilities.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可能会使用三叶草和四叶草的10:1比例。或者，在训练模型时，我们可能先使用相同数量的三叶草和四叶草，然后在训练一段时间后，调整为越来越接近实际先验概率的混合比例。这个技巧并不是所有模型类型都适用，但它对于神经网络是有效的。为什么这个技巧有效尚不完全明了，但直觉上，我们可以想象，网络首先学习的是三叶草和四叶草之间的视觉差异，然后随着比例逐渐接近实际的先验概率，网络开始学习到遇到四叶草的实际可能性。
- en: In reality, the trick is used because it often results in better-performing
    models. For much of machine learning, especially deep learning, empirical tricks
    and techniques are well in advance of any theory to back them up. “It just works
    better; that’s why” is still a valid, though ultimately unsatisfying, answer to
    many questions about why a particular approach works well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用这个技巧是因为它通常能产生表现更好的模型。对于大部分机器学习，尤其是深度学习，经验技巧和技术通常远远领先于任何理论的支持。“它就是效果更好；这就是为什么”仍然是一个有效的答案，尽管最终令人不满足，但它回答了关于为什么某种方法能很好地工作的许多问题。
- en: How to work with imbalanced data is something the research community is still
    actively investigating. Some choose to start with a more balanced ratio of classes;
    others use data augmentation (see [Chapter 5](ch05.xhtml#ch05)) to boost the number
    of samples from the underrepresented class.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理不平衡数据是研究界仍在积极探讨的问题。有些人选择从更加平衡的类别比例开始；另一些人使用数据增强（见[第5章](ch05.xhtml#ch05)）来增加来自少数类别的样本数量。
- en: Confusers
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混淆者
- en: We said that we need to include examples in our dataset that reflect all the
    natural variation in the classes we want to learn. This is definitely true, but
    at times it is particularly important to include training samples that are similar
    to one or more of our classes but really are not examples of that class.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说过，数据集中需要包含反映我们想要学习的类别中所有自然变化的示例。这确实是对的，但有时特别重要的是，包括那些与我们某一类或多类相似，但实际上并不属于该类别的训练样本。
- en: Consider two models. The first learns the difference between images of dogs
    and images of cats. The second learns the difference between images of dogs and
    images that are not dogs. The first model has it easy. The input is either a dog
    or a cat, and the model is trained using images of dogs and images of cats. The
    second model, however, has it rougher. It’s obvious that we need images of dogs
    for training. But, what should the “not dog” images be? Given the preceding discussion,
    we should be starting to intuit that we’ll need images that cover the space of
    images the model will see in the wild.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个模型。第一个模型学习的是狗和猫的图像之间的区别。第二个模型学习的是狗和非狗图像之间的区别。第一个模型很容易。输入要么是狗，要么是猫，模型通过狗和猫的图像进行训练。然而，第二个模型就比较困难了。显然，我们需要狗的图像进行训练。那么，"非狗"的图像应该是什么呢？结合前面的讨论，我们应该开始直觉地意识到，我们需要的是覆盖模型在实际中将看到的图像空间的图像。
- en: We can take this one step further. If we want to tell the difference between
    dogs and not dogs, we should be sure to include wolves in the “not a dog” class
    when training. If we don’t, the model might not learn enough to tell the difference
    when it encounters a wolf and will return a “dog” classification. If we build
    the dataset by using hundreds of “not dog” images that are all pictures of penguins
    and parrots, should we be surprised if the model decides to call a wolf a dog?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更进一步。如果我们想区分狗和非狗，我们应该确保在训练时将狼纳入“非狗”类别。如果我们不这么做，模型在遇到狼时可能无法学会足够的区分特征，可能会返回“狗”的分类。如果我们通过使用数百张“非狗”图片（全是企鹅和鹦鹉的照片）来构建数据集，那么如果模型最终把狼当成狗来分类，我们还会感到惊讶吗？
- en: In general, we need to make sure the dataset includes *confusers*, or *hard
    negatives*—examples that are similar enough to other classes to be mistaken for
    them, but don’t belong in the class. Confusers give the model a chance to learn
    the more precise features of a class. Hard negatives are particularly useful when
    distinguishing between something and everything else, as in “dog” versus “not
    dog.”
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们需要确保数据集包含*困惑者*或*硬负样本*——这些示例与其他类别足够相似，容易被误分类，但并不属于该类别。困惑者为模型提供了学习类别更精确特征的机会。硬负样本在区分某个类别和其他所有类别时尤为有用，正如“狗”与“非狗”之间的区分。
- en: Dataset Size
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集大小
- en: So far we’ve talked about what kind of data to include in a dataset, but how
    much of it do we need? “All of it" is a temptingly cheeky answer. For our model
    to be as precise as possible, we should use as many examples as possible. But
    it’s rarely possible to get all of the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们谈论了在数据集中应该包含什么样的数据，但我们需要多少数据呢？“所有数据”是一个诱人的调皮答案。为了让我们的模型尽可能精确，我们应该使用尽可能多的示例。但实际上，获取所有数据几乎是不可能的。
- en: Choosing the size of your dataset means considering a trade-off between accuracy
    and the time and energy it takes to acquire the data. Acquiring data can be expensive
    or slow, or, as we saw with our clover example, sometimes the key class of the
    dataset is rare and seldom encountered. Because labeled data is generally expensive
    and slow to acquire, we should have some idea of how much we need before we get
    started.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 选择数据集的大小意味着需要在准确性和获取数据所花费的时间和精力之间做权衡。获取数据可能既昂贵又缓慢，或者，正如我们在三叶草的例子中看到的那样，有时候数据集中的关键类别稀少且不常遇到。由于标注数据通常昂贵且获取缓慢，我们应该在开始之前就对需要多少数据有一定的了解。
- en: Unfortunately, the truth is that there is no formula that answers the question
    of how much data is enough data. After a certain point, there are diminishing
    returns on the benefit of additional data. Moving from 100 examples to 1,000 examples
    might boost the accuracy of the model dramatically, but moving from 1,000 to 10,000
    examples might offer only a small increase in accuracy. The increased accuracy
    needs to be balanced against the effort and expense of acquiring an additional
    9,000 training examples.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，事实是没有一个公式能回答“多少数据足够”这个问题。数据的边际效益会随着数据量的增加而递减。从100个样本到1000个样本，可能会显著提高模型的准确性，但从1000个样本增加到10000个样本，可能只会略微提高准确性。提高的准确性需要与获取额外9000个训练样本的努力和费用进行平衡。
- en: Another factor to consider is the model itself. Models have a *capacity*, which
    determines the complexity they can support relative to the amount of training
    data available. The capacity of a model is directly related to its number of parameters.
    A larger model with more parameters will require a lot of training data to be
    able to find the proper parameter settings. And though it’s often a good idea
    to have more training examples than model parameters, deep learning can work well
    when there is less training data than parameters. For example, if the classes
    are very different from each other—think buildings versus oranges—and it’s easy
    for us to tell the difference, the model likely will also learn the difference
    quickly, so we can get away with fewer training examples. On the other hand, if
    we’re trying to separate wolves from huskies, we might need a lot more data. We
    will discuss what to do when you don’t have a lot of training data in [Chapter
    5](ch05.xhtml#ch05), but none of those tricks are a good substitute for simply
    getting more data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是模型本身。模型有一个*容量*，决定了它能够支持的复杂度，与可用的训练数据量相关。模型的容量与其参数数量直接相关。一个拥有更多参数的大模型需要大量的训练数据，才能找到适当的参数设置。尽管通常来说，训练样本的数量应该大于模型参数的数量，但在数据量少于参数数量的情况下，深度学习仍然能很好地工作。例如，如果各个类别之间差异非常大——比如建筑与橘子——而且我们容易分辨差异，模型可能也会很快学习到这些差异，因此我们可以使用较少的训练样本。另一方面，如果我们要区分狼和哈士奇，我们可能需要更多的数据。在[第5章](ch05.xhtml#ch05)中，我们会讨论当训练数据不多时该怎么办，但这些技巧并不能替代增加更多数据的做法。
- en: 'The only correct answer to the question of how much data is needed is “all
    of it.” Get as much as is *practical*, given the constraints of the problem: expense,
    time, rarity, and so forth.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“需要多少数据”这个问题，唯一正确的答案是“所有数据”。在问题的约束条件下，尽可能多地获取数据：成本、时间、稀缺性等等。
- en: Data Preparation
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Before we move on to building actual datasets, we’re going to cover two situations
    you’ll likely encounter before you can feed your dataset to a model: how to scale
    features, and what to do if a feature value is missing.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续构建实际数据集之前，我们需要讨论你可能在将数据集输入模型之前遇到的两种情况：如何缩放特征，以及如果某个特征值缺失应该怎么办。
- en: Scaling Features
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征缩放
- en: A feature vector built from a set of different features might have a variety
    of ranges. One feature might take on a wide range of values, say, –1000 to 1000,
    while another might be restricted to a range of 0 to 1\. Some models will not
    work well when this happens, as one feature dominates the others because of its
    range. Also, some model types are happiest when features have a mean value that
    is close to 0.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由一组不同特征构成的特征向量可能具有多种范围。有些特征可能会有较大的值范围，比如从-1000到1000，而另一些则可能限制在0到1之间。当发生这种情况时，一些模型可能会表现不佳，因为某个特征由于其值范围过大而主导了其他特征。此外，一些模型类型在特征的平均值接近0时效果最好。
- en: The solution to these issues is scaling. We’ll assume for the time being that
    every feature in the feature vector is continuous. We’ll work with a fake dataset
    consisting of five features and 15 samples. This means that our dataset has 15
    samples—feature vectors and their labels—and each of the feature vectors has five
    elements. We’ll assume there are three classes. The dataset looks like [Table
    4-4](ch04.xhtml#ch4tab4).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的方法是缩放。我们暂时假设特征向量中的每个特征都是连续的。我们将使用一个假数据集，包含五个特征和15个样本。这意味着我们的数据集有15个样本——特征向量及其标签——每个特征向量有五个元素。我们假设有三个类别。数据集的样子可以参考[表格
    4-4](ch04.xhtml#ch4tab4)。
- en: '**Table 4-4:** A Hypothetical Dataset'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格 4-4：** 假设数据集'
- en: '| **Sample** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]**
    | **Label** |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **样本** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |
    **标签** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
- en: '| *1* | 6580 | 0.4908 | 3.0150 | 0.00004484 | 38462706 | 1 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| *1* | 6580 | 0.4908 | 3.0150 | 0.00004484 | 38462706 | 1 |'
- en: '| *2* | 7563 | 0.9349 | 4.3465 | 0.00001003 | 6700340 | 2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| *2* | 7563 | 0.9349 | 4.3465 | 0.00001003 | 6700340 | 2 |'
- en: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
- en: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
- en: '| *5* | 9498 | 0.0244 | 2.7887 | 0.00008880 | 78543394 | 2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| *5* | 9498 | 0.0244 | 2.7887 | 0.00008880 | 78543394 | 2 |'
- en: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 | 19101443 | 2 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 | 19101443 | 2 |'
- en: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
- en: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
- en: '| *9* | 843 | 0.7892 | 0.9804 | 0.00005798 | 10179751 | 1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| *9* | 843 | 0.7892 | 0.9804 | 0.00005798 | 10179751 | 1 |'
- en: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
- en: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
- en: '| *12* | 923 | 0.4159 | 1.7821 | 0.00002467 | 52404615 | 1 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| *12* | 923 | 0.4159 | 1.7821 | 0.00002467 | 52404615 | 1 |'
- en: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
- en: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
- en: As this is the first dataset covered in the book, let’s go over it thoroughly
    to introduce some notation and see what is what. The first column in [Table 4-4](ch04.xhtml#ch4tab4)
    is the sample number. The sample is an input, in this case a collection of five
    features representing a feature vector. Notice that the numbering starts at 0\.
    As we’ll be using Python arrays (NumPy arrays) for data, we’ll start counting
    at 0 in all cases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是本书中讲解的第一个数据集，我们将详细讲解它，介绍一些符号并弄清楚每一部分的含义。[表 4-4](ch04.xhtml#ch4tab4)中的第一列是样本编号。样本是一个输入，在本例中是由五个特征组成的特征向量。注意编号从0开始。由于我们将使用Python数组（NumPy数组）处理数据，我们将在所有情况下从0开始计数。
- en: The next five columns are the features in each sample, labeled *x*[0] to *x*[4],
    again starting indices at 0\. The final column is the class label. Since there
    are three classes, the labels run from 0 through 2\. There are five samples from
    class 0, five from class 1, and five from class 2\. Therefore, this is a small
    but balanced dataset; the prior probability of each class is 33 percent, which
    should, ideally, be close to the actual prior probability of the classes appearing
    in the wild.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下一五列是每个样本的特征，标记为 *x*[0] 到 *x*[4]，索引从0开始。最后一列是类别标签。由于有三类，标签从0到2。类别0有五个样本，类别1有五个样本，类别2有五个样本。因此，这是一个小而平衡的数据集；每个类别的先验概率为33%，理想情况下应该接近现实中各类出现的先验概率。
- en: If we had a model, then each row would be its own input. Writing {*x*[0], *x*[1],
    *x*[2], *x*[3], *x*[4]} to refer to these is tedious, so instead, when we are
    referring to a full feature vector, we’ll use an uppercase letter. For example,
    we’d refer to Sample 2 as *X*[2] for dataset *X*. We’ll also sometimes use matrices—2D
    arrays of numbers—that are also labeled with uppercase letters, for clarity. When
    we want to refer to a single feature, we’ll use a lowercase letter with subscript,
    for example, *x*[3].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个模型，那么每一行都将是一个输入。用 {*x*[0], *x*[1], *x*[2], *x*[3], *x*[4]} 来表示这些特征很繁琐，因此，当我们要引用完整的特征向量时，我们将使用大写字母。例如，我们会将数据集
    *X* 中的样本2称为 *X*[2]。我们有时还会使用矩阵——二维数字数组——这些矩阵也用大写字母标记，以提高清晰度。当我们想引用单一特征时，我们会使用带下标的小写字母，例如
    *x*[3]。
- en: Let’s look at the ranges of the features. The minimum, maximum, and range (the
    difference between the maximum and minimum) of each feature are shown in [Table
    4-5](ch04.xhtml#ch4tab5).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看特征的范围。每个特征的最小值、最大值和范围（最大值与最小值的差）在 [表 4-5](ch04.xhtml#ch4tab5) 中显示。
- en: '**Table 4-5:** The Minimum, Maximum, and Range of the Features in [Table 4-4](ch04.xhtml#ch4tab4)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-5：** [表 4-4](ch04.xhtml#ch4tab4)中各特征的最小值、最大值和范围'
- en: '| **Feature** | **Minimum** | **Maximum** | **Range** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **最小值** | **最大值** | **范围** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *x*[0] | 843.0 | 9498.0 | 8655.0 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| *x*[0] | 843.0 | 9498.0 | 8655.0 |'
- en: '| *x*[1] | 0.0002 | 0.9564 | 0.9562 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| *x*[1] | 0.0002 | 0.9564 | 0.9562 |'
- en: '| *x*[2] | 0.0705 | 4.8231 | 4.7526 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| *x*[2] | 0.0705 | 4.8231 | 4.7526 |'
- en: '| *x*[3] | 4.03e-06 | 9.564e-05 | 9.161e-05 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| *x*[3] | 4.03e-06 | 9.564e-05 | 9.161e-05 |'
- en: '| *x*[4] | 6700340.0 | 96703562.0 | 90003222.0 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| *x*[4] | 6700340.0 | 96703562.0 | 90003222.0 |'
- en: 'Note the use of computer notation like 9.161e-05\. This how computers represent
    scientific notation: 9.161 × 10^(*–*5) = 0.00009161\. Notice, also, that each
    feature covers a very different range. Because of this, we’ll want to scale the
    features so their ranges are more similar. Scaling is a valid thing to do prior
    to training a model as long as you scale all new inputs the same way.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用计算机表示法，如9.161e-05。这是计算机表示科学记数法的方式：9.161 × 10^(*–*5) = 0.00009161。还要注意，每个特征的范围差异很大。由于这个原因，我们需要对特征进行缩放，使它们的范围更加相似。缩放是训练模型前的一个有效步骤，只要对所有新的输入都采用相同的缩放方式。
- en: Mean Centering
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 均值中心化
- en: 'The simplest form of scaling is *mean centering*. This is easy to do: from
    each feature, simply subtract the mean (average) value of the feature over the
    entire dataset. The mean over a set of values, *x*[*i*] *i* = 0, 1, 2, … is simply
    the sum of each value divided by the number of values:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的缩放方法是 *均值中心化*。这很容易做到：从每个特征中，简单地减去该特征在整个数据集中的均值。值的均值 *x*[*i*] *i* = 0, 1,
    2, … 就是每个值的总和除以值的数量：
- en: '![image](Images/064equ01.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/064equ01.jpg)'
- en: 'The mean value for feature *x*[0] is 5022, so to center *x*[0], we replace
    each value like so:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 *x*[0] 的均值是 5022，因此为了对 *x*[0] 进行中心化，我们将每个值替换为：
- en: '*x[i]* ← *x[i]* – 5022, *i* = 0, 1, 2, . . .'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*x[i]* ← *x[i]* – 5022, *i* = 0, 1, 2, . . .'
- en: where in this case the *i* index is across the samples, not the other elements
    of the feature vector.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*i* 索引是跨样本的，而不是特征向量的其他元素。
- en: Repeating the preceding steps for the mean value of all the other features will
    center the entire dataset. The result is that the mean value of each feature,
    over the dataset, is now 0, meaning the feature values themselves are all above
    and below 0\. For deep learning, mean centering is often done by subtracting a
    mean image from each input image.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对其他特征的均值执行上述步骤，将使整个数据集居中。结果是，每个特征在数据集中的均值现在是 0，也就是说，特征值本身都在 0 的上下范围内。对于深度学习，均值中心化通常是通过从每个输入图像中减去均值图像来完成的。
- en: Changing the Standard Deviation to 1
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将标准差改为 1
- en: 'Mean centering helps, but the distribution of values around 0 remains the same
    as before the mean was subtracted. All we did was shift the data down toward 0\.
    The spread of values around the mean has a formal name: it’s called the *standard
    deviation*, and it’s computed as the average difference of the data values and
    the mean:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化有帮助，但值围绕 0 的分布与均值被减去之前保持一致。我们所做的只是将数据向 0 方向平移。围绕均值的值的分布有一个正式的名称：它叫做 *标准差*，它的计算方式是数据值与均值之间的平均差值：
- en: '![image](Images/065equ01.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/065equ01.jpg)'
- en: The letter *σ* (sigma) is the usual name for the standard deviation in mathematics.
    You don’t need to memorize this formula. It’s there to show us how to calculate
    a measure of the spread, or range, of the data relative to the mean value of the
    data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 字母 *σ*（sigma）是数学中常用来表示标准差的符号。你不需要记住这个公式，它只是用来告诉我们如何计算数据相对于均值的分布或范围的度量。
- en: Mean centering changes ![Image](Images/xbar.jpg) to 0, but it does not change
    *σ*. Sometimes we want to go further and, along with mean centering, change the
    spread of the data so that the ranges are the same, meaning the standard deviation
    for each feature is 1\. Fortunately, doing this is straightforward. We replace
    each feature value, *x*, with
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 均值中心化将 ![Image](Images/xbar.jpg) 改为 0，但不会改变 *σ*。有时我们希望更进一步，除了进行均值中心化外，还希望改变数据的分布，使得所有特征的范围相同，也就是说每个特征的标准差为
    1。幸运的是，这个操作是很直接的。我们将每个特征值 *x* 替换为
- en: '![image](Images/065equ02.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/065equ02.jpg)'
- en: where ![Image](Images/xbar.jpg) and *σ* are the mean and standard deviation
    of each feature across the dataset. For example, the preceding toy dataset can
    be stored as a 2D NumPy array
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/xbar.jpg) 和 *σ* 是数据集中每个特征的均值和标准差。例如，前面的玩具数据集可以存储为 2D NumPy
    数组
- en: x = [
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: x = [
- en: '[6998, 0.1361, 0.3408, 0.00007350, 78596048],'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[6998, 0.1361, 0.3408, 0.00007350, 78596048],'
- en: '[6580, 0.4908, 3.0150, 0.00004484, 38462706],'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[6580, 0.4908, 3.0150, 0.00004484, 38462706],'
- en: '[7563, 0.9349, 4.3465, 0.00001003,  6700340],'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[7563, 0.9349, 4.3465, 0.00001003, 6700340],'
- en: '[8355, 0.6529, 2.1271, 0.00002966, 51430391],'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[8355, 0.6529, 2.1271, 0.00002966, 51430391],'
- en: '[2393, 0.4605, 2.7561, 0.00003395, 27284192],'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[2393, 0.4605, 2.7561, 0.00003395, 27284192],'
- en: '[9498, 0.0244, 2.7887, 0.00008880, 78543394],'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[9498, 0.0244, 2.7887, 0.00008880, 78543394],'
- en: '[4030, 0.6467, 4.8231, 0.00000403, 19101443],'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[4030, 0.6467, 4.8231, 0.00000403, 19101443],'
- en: '[5275, 0.3560, 0.0705, 0.00000899, 96029352],'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[5275, 0.3560, 0.0705, 0.00000899, 96029352],'
- en: '[8094, 0.7979, 3.9897, 0.00006691,  7307156],'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[8094, 0.7979, 3.9897, 0.00006691, 7307156],'
- en: '[ 843, 0.7892, 0.9804, 0.00005798, 10179751],'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[843, 0.7892, 0.9804, 0.00005798, 10179751],'
- en: '[1221, 0.9564, 2.3944, 0.00007815, 14241835],'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[1221, 0.9564, 2.3944, 0.00007815, 14241835],'
- en: '[5879, 0.0329, 2.0085, 0.00009564, 34243070],'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[5879, 0.0329, 2.0085, 0.00009564, 34243070],'
- en: '[ 923, 0.4159, 1.7821, 0.00002467, 52404615],'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[923, 0.4159, 1.7821, 0.00002467, 52404615],'
- en: '[5882, 0.0002, 1.5362, 0.00005066, 18728752],'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[5882, 0.0002, 1.5362, 0.00005066, 18728752],'
- en: '[1796, 0.7247, 2.3190, 0.00001332, 96703562],'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[1796, 0.7247, 2.3190, 0.00001332, 96703562],'
- en: ']'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: 'so that the entire dataset can be processed in one line of code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这样整个数据集就可以用一行代码处理：
- en: x = (x - x.mean(axis=0)) / x.std(axis=0)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: x = (x - x.mean(axis=0)) / x.std(axis=0)
- en: This approach is called *standardization* or *normalizing*, and you should do
    it to most datasets, especially when using one of the traditional models discussed
    in [Chapter 6](ch06.xhtml#ch06). Whenever possible, standardize your dataset so
    that the features have 0 mean and a standard deviation of 1.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法叫做*标准化*或*归一化*，你应该对大多数数据集进行此操作，尤其是当使用[第六章](ch06.xhtml#ch06)中讨论的传统模型时。尽可能地，将数据集标准化，使得特征的均值为0，标准差为1。
- en: If we standardize the preceding dataset, what will it look like? Subtracting,
    per feature, the mean value of that feature and dividing by the standard deviation
    gives us a new dataset ([Table 4-6](ch04.xhtml#ch4tab6)). Here, we’ve shortened
    the numbers to four decimal digits for display and have dropped the label.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对前面的数据集进行标准化，它会是什么样子呢？对每个特征，减去该特征的均值，再除以标准差，得到的是一个新的数据集（[表 4-6](ch04.xhtml#ch4tab6)）。在这里，我们将数字缩短到四位小数进行显示，并且省略了标签。
- en: '**Table 4-6:** The Data in [Table 4-4](ch04.xhtml#ch4tab4) Standardized'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-6：** [表 4-4](ch04.xhtml#ch4tab4)中的数据标准化'
- en: '| **Sample** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]**
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **样本** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| *0* | 0.6930 | –1.1259 | –1.5318 | 0.9525 | 1.1824 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| *0* | 0.6930 | –1.1259 | –1.5318 | 0.9525 | 1.1824 |'
- en: '| *1* | 0.5464 | –0.0120 | 0.5051 | –0.0192 | –0.1141 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| *1* | 0.5464 | –0.0120 | 0.5051 | –0.0192 | –0.1141 |'
- en: '| *2* | 0.8912 | 1.3826 | 1.5193 | –1.1996 | –1.1403 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| *2* | 0.8912 | 1.3826 | 1.5193 | –1.1996 | –1.1403 |'
- en: '| *3* | 1.1690 | 0.4970 | –0.1712 | –0.5340 | 0.3047 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| *3* | 1.1690 | 0.4970 | –0.1712 | –0.5340 | 0.3047 |'
- en: '| *4* | –0.9221 | –0.1071 | 0.3079 | –0.3885 | –0.4753 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| *4* | –0.9221 | –0.1071 | 0.3079 | –0.3885 | –0.4753 |'
- en: '| *5* | 1.5699 | –1.4767 | 0.3327 | 1.4714 | 1.1807 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| *5* | 1.5699 | –1.4767 | 0.3327 | 1.4714 | 1.1807 |'
- en: '| *6* | –0.3479 | 0.4775 | 1.8823 | –1.4031 | –0.7396 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| *6* | –0.3479 | 0.4775 | 1.8823 | –1.4031 | –0.7396 |'
- en: '| *7* | 0.0887 | –0.4353 | –1.7377 | –1.2349 | 1.7456 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| *7* | 0.0887 | –0.4353 | –1.7377 | –1.2349 | 1.7456 |'
- en: '| *8* | 1.0775 | 0.9524 | 1.2475 | 0.7291 | –1.1207 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| *8* | 1.0775 | 0.9524 | 1.2475 | 0.7291 | –1.1207 |'
- en: '| *9* | –1.4657 | 0.9250 | –1.0446 | 0.4262 | –1.0279 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| *9* | –1.4657 | 0.9250 | –1.0446 | 0.4262 | –1.0279 |'
- en: '| *10* | –1.3332 | 1.4501 | 0.0323 | 1.1102 | –0.8966 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| *10* | –1.3332 | 1.4501 | 0.0323 | 1.1102 | –0.8966 |'
- en: '| *11* | 0.3005 | –1.4500 | –0.2615 | 1.7033 | –0.2505 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| *11* | 0.3005 | –1.4500 | –0.2615 | 1.7033 | –0.2505 |'
- en: '| *12* | –1.4377 | –0.2472 | –0.4340 | –0.7032 | 0.3362 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| *12* | –1.4377 | –0.2472 | –0.4340 | –0.7032 | 0.3362 |'
- en: '| *13* | 0.3016 | –1.5527 | –0.6213 | 0.1780 | –0.7517 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| *13* | 0.3016 | –1.5527 | –0.6213 | 0.1780 | –0.7517 |'
- en: '| *14* | –1.1315 | 0.7225 | –0.0250 | –1.0881 | 1.7674 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| *14* | –1.1315 | 0.7225 | –0.0250 | –1.0881 | 1.7674 |'
- en: If you compare the two tables, you’ll see that after our manipulations, the
    features are more similar than they were in the original set. If we look at *x*[3],
    we’ll see that the mean of the values is *–* 1.33*e –* 16 = *–*1.33 × 10^(*–*16)
    = *–*0.000000000000000133, which is virtually 0\. Good! This is what we want.
    If you do the calculations, you’d see that the means of the other features are
    similarly close to 0\. What about the standard deviation? For *x*[3] it’s 0.99999999,
    which is virtually 1—again, this is what we’d like. We’ll use this new, transformed,
    dataset to train the model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对比这两张表，你会看到在我们进行操作后，特征变得比原始数据集中的更为相似。如果我们看一下 *x*[3]，会发现其值的均值为 *–* 1.33*e
    –* 16 = *–*1.33 × 10^(*–*16) = *–*0.000000000000000133，几乎等于0。好极了！这正是我们想要的。如果你做一下计算，你会发现其他特征的均值也同样接近于0。那标准差呢？对于
    *x*[3]，它的标准差是0.99999999，几乎等于1——同样，这是我们希望的。我们将使用这个新的、经过变换的数据集来训练模型。
- en: 'Therefore, we must apply the per feature means and standard deviations, as
    measured on the training set, to any new inputs we’re giving to the model:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须应用每个特征的均值和标准差（通过训练集测量得到），到我们输入到模型中的任何新数据：
- en: '![image](Images/067equ01.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/067equ01.jpg)'
- en: Here, *x*[new] is the new feature vector we want to apply to the model, and
    ![Image](Images/067equ02.jpg) and *σ*[train] are the mean and standard deviation,
    per feature, from the training set.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[new] 是我们想要输入到模型中的新特征向量，而![Image](Images/067equ02.jpg)和 *σ*[train] 是来自训练集的每个特征的均值和标准差。
- en: Missing Features
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失特征
- en: Sometimes we don’t have all the features we need for a sample. We might have
    forgotten to make a measurement, for example. These are *missing features*, and
    we need to find a way to correct them, since most models don’t have the ability
    to accept missing data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们没有某个样本所需的所有特征。例如，可能是忘记做某项测量了。这些是*缺失特征*，我们需要找到一种方法来修正它们，因为大多数模型不能接受缺失的数据。
- en: One solution is to fill in the missing values with values that are outside of
    the feature’s range, in the hopes that the model will learn to ignore those values
    or make more use of other features. Indeed, some more advanced deep learning models
    intentionally zero some of the input as a form of regularization (we’ll see what
    that means in later chapters).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是用超出特征范围的值填充缺失值，希望模型能学会忽略这些值，或更多地利用其他特征。事实上，一些更先进的深度学习模型会故意将某些输入置零，作为一种正则化形式（我们将在后面的章节中看到这意味着什么）。
- en: 'For now, we’ll learn the second most obvious solution: replacing missing features
    with the mean value of features over the dataset. Let’s look again at our practice
    dataset from earlier. This time, we’ll have some missing data to deal with ([Table
    4-7](ch04.xhtml#ch4tab7)).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习第二种最明显的解决方案：用数据集中每个特征的均值来替代缺失的特征值。让我们再次看看之前的实践数据集。这次，我们将处理一些缺失数据（[表
    4-7](ch04.xhtml#ch4tab7)）。
- en: '**Table 4-7:** Our Sample Dataset ([Table 4-4](ch04.xhtml#ch4tab4)) with Some
    Holes'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-7：** 我们的示例数据集（[表 4-4](ch04.xhtml#ch4tab4)）含有一些缺失值'
- en: '| **Sample** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]**
    | **Label** |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| **样本** | ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |
    **标签** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| *0* | 6998 | 0.1361 | 0.3408 | 0.00007350 | 78596048 | 0 |'
- en: '| *1* |  | 0.4908 |  | 0.00004484 | 38462706 | 1 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| *1* |  | 0.4908 |  | 0.00004484 | 38462706 | 1 |'
- en: '| *2* | 7563 | 0.9349 | 4.3465 |  | 6700340 | 2 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| *2* | 7563 | 0.9349 | 4.3465 |  | 6700340 | 2 |'
- en: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| *3* | 8355 | 0.6529 | 2.1271 | 0.00002966 | 51430391 | 0 |'
- en: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| *4* | 2393 | 0.4605 | 2.7561 | 0.00003395 | 27284192 | 0 |'
- en: '| *5* | 9498 |  | 2.7887 | 0.00008880 | 78543394 | 2 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| *5* | 9498 |  | 2.7887 | 0.00008880 | 78543394 | 2 |'
- en: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 |  | 2 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| *6* | 4030 | 0.6467 | 4.8231 | 0.00000403 |  | 2 |'
- en: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| *7* | 5275 | 0.3560 | 0.0705 | 0.00000899 | 96029352 | 0 |'
- en: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| *8* | 8094 | 0.7979 | 3.9897 | 0.00006691 | 7307156 | 1 |'
- en: '| *9* |  |  | 0.9804 |  | 10179751 | 1 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| *9* |  |  | 0.9804 |  | 10179751 | 1 |'
- en: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| *10* | 1221 | 0.9564 | 2.3944 | 0.00007815 | 14241835 | 0 |'
- en: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| *11* | 5879 | 0.0329 | 2.0085 | 0.00009564 | 34243070 | 2 |'
- en: '| *12* | 923 |  |  | 0.00002467 |  | 1 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| *12* | 923 |  |  | 0.00002467 |  | 1 |'
- en: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| *13* | 5882 | 0.0002 | 1.5362 | 0.00005066 | 18728752 | 2 |'
- en: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| *14* | 1796 | 0.7247 | 2.3190 | 0.00001332 | 96703562 | 1 |'
- en: The blank spaces indicate missing values. The means of each feature, ignoring
    missing values, are shown in [Table 4-8](ch04.xhtml#ch4tab8).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 空白处表示缺失值。每个特征的均值（忽略缺失值）显示在[表 4-8](ch04.xhtml#ch4tab8)中。
- en: '**Table 4-8:** The Means for Features in [Table 4-7](ch04.xhtml#ch4tab7)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-8：** [表 4-7](ch04.xhtml#ch4tab7)中各特征的均值'
- en: '| ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| ***x*[0]** | ***x*[1]** | ***x*[2]** | ***x*[3]** | ***x*[4]** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 5223.6 | 0.5158 | 2.345 | 4.71e-05 | 42957735.0 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 5223.6 | 0.5158 | 2.345 | 4.71e-05 | 42957735.0 |'
- en: If we replace each missing value with the mean, we’ll get a dataset we can standardize
    and use to train a model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用均值替换每个缺失值，我们将得到一个可以标准化并用于训练模型的数据集。
- en: Of course, real data is better, but the mean is the simplest substitute we can
    reasonably use. If the dataset is large enough, we might instead generate a histogram
    of the values of each feature and select the mode—the most common value—but using
    the mean should work out just fine, especially if your dataset has a lot of samples
    and the number of missing features is fairly small.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，真实的数据更好，但均值是我们可以合理使用的最简单的替代方法。如果数据集足够大，我们可能会生成每个特征值的直方图并选择众数——即最常见的值——但使用均值应该也能很好地工作，尤其是当你的数据集有足够多的样本，并且缺失特征的数量相对较少时。
- en: Training, Validation, and Test Data
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据、验证数据和测试数据
- en: Now that we have a dataset—a collection of feature vectors—we’re ready to start
    training a model, right? Well, actually, no. That’s because we don’t want to use
    the entire dataset for training. We’ll need to use some of the data for other
    purposes, and so we need to split it into at least two subsets, although ideally
    we’d have three. We call these subsets the training data, validation data, and
    test data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个数据集——一个特征向量的集合——准备开始训练模型了，对吗？其实，不完全是。原因在于我们不想用整个数据集来训练。我们需要将一些数据用于其他目的，因此需要将数据划分为至少两个子集，理想情况下我们应该有三个子集。我们称这些子集为训练数据、验证数据和测试数据。
- en: The Three Subsets
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 三个子集
- en: The *training data* is the subset we use to train the model. The important thing
    here is selecting feature vectors that well represent the parent distribution
    of the data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练数据*是我们用来训练模型的子集。这里的关键是选择能够很好代表数据父分布的特征向量。'
- en: The *test data* is the subset used to evaluate how well the trained model is
    doing. We *never* use the test data when training the model; that would be cheating,
    because we’d be testing the model on data it has seen before. Put the test dataset
    aside, resist the temptation to touch it until the model is complete, and then
    use it to evaluate the model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*测试数据*是用来评估已训练模型表现的子集。我们*绝不*在训练模型时使用测试数据；那样是作弊，因为我们会在模型已经见过的数据上进行测试。将测试数据集放到一边，直到模型完成后再使用它来评估模型。'
- en: The third dataset is the *validation data*. Not every model needs a validation
    dataset, but for deep learning models, having one is helpful. We use the validation
    dataset during training as though it’s test data to get an idea of how well the
    training is working. It can help us decide things like when to stop training and
    whether we’re using the proper model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个数据集是*验证数据*。并不是每个模型都需要验证数据集，但对于深度学习模型，拥有一个验证集是非常有帮助的。在训练过程中，我们使用验证数据集，就像它是测试数据一样，以了解训练效果如何。这有助于我们决定什么时候停止训练，以及我们是否使用了合适的模型。
- en: For example, a neural network has some number of layers, each with some number
    of nodes. We call this the *architecture* of the model. During training, we can
    test the performance of the neural network with the validation data to figure
    out whether we should continue training or stop and try a different architecture.
    We don’t train the model with the validation set, and we don’t use the validation
    set to modify model parameters. We also can’t use validation data when reporting
    actual model performance, since we used results based on the validation data to
    select the model in the first place. Again, this would make it seem like the model
    is doing better than it is.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个神经网络有若干层，每一层包含若干节点。我们称这个结构为模型的*架构*。在训练过程中，我们可以使用验证数据来测试神经网络的性能，以判断是否应该继续训练，还是停止并尝试不同的架构。我们不使用验证集来训练模型，也不使用验证集来修改模型参数。我们同样不能在报告实际模型性能时使用验证数据，因为我们最初就是根据验证数据的结果来选择模型的。这样做会让模型看起来比实际表现更好。
- en: '[Figure 4-3](ch04.xhtml#ch4fig3) illustrates the three subsets and their relationships
    to one another. On the left is the whole dataset. This is the entire collection
    of feature vectors and associated labels. On the right are the three subsets.
    The training data and the validation data work together to train and develop the
    model, while the test data is held back until the model is ready for it. The size
    of the cylinders reflects the relative amount of data that should fall into each
    subset, though in practice the validation and test subsets might be even smaller.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-3](ch04.xhtml#ch4fig3)展示了三个子集及它们之间的关系。左侧是整个数据集。这是所有特征向量及其相关标签的集合。右侧是三个子集。训练数据和验证数据共同作用于训练和开发模型，而测试数据则在模型准备好之后才会使用。圆柱体的大小反映了每个子集中应该包含的数据量，尽管在实际情况中，验证集和测试集的大小可能会更小。'
- en: '![image](Images/04fig03.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig03.jpg)'
- en: '*Figure 4-3: Relationships among training, validation, and test subsets*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-3：训练集、验证集和测试集之间的关系*'
- en: 'To recap: use the training and validation sets to build the model and the test
    set to evaluate it.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：使用训练集和验证集来构建模型，使用测试集来评估模型。
- en: Partitioning the Dataset
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的划分
- en: How much data should go into each dataset?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集应该包含多少数据？
- en: A typical split is 90 percent for training, 5 percent for validation, and 5
    percent for testing. For deep learning models, this is fairly standard. If you’re
    working with a very large dataset, you could go as low as 1 percent each for validation
    and testing. For classic models, which might not learn as well, we might want
    to make the test dataset larger to ensure we are able to generalize to a wide
    variety of possible inputs. In those cases, you might try something like 80 percent
    for training and 10 percent each for validation and test. If you’re not using
    validation data, the full 20 percent might go to testing. These larger test sets
    might be appropriate for multiclass models that have classes with low prior probabilities.
    Or, since the test set is not used to define the model, you might increase the
    number of rare classes in the test set. This might be of particular value should
    missing the rare class be a costly event (think missing a tumor in a medical image).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的划分方式是 90% 用于训练，5% 用于验证，5% 用于测试。对于深度学习模型，这种划分方式是相当标准的。如果你在处理一个非常大的数据集，你可以将验证和测试的比例降到
    1%。对于经典模型，可能学习效果不如深度学习模型，这时我们可能希望增加测试集的大小，以确保模型能够泛化到各种可能的输入。在这种情况下，你可以尝试将训练集设为
    80%，验证集和测试集各占 10%。如果不使用验证数据，那么全部 20% 可能都用于测试。这种较大的测试集可能适用于多类模型，尤其是那些具有低先验概率的类别。或者，由于测试集不用于定义模型，你可以增加测试集中稀有类别的数量。如果漏掉稀有类别是一个代价高昂的事件（比如在医学影像中漏掉肿瘤），这种做法可能特别有价值。
- en: 'Now that we’ve determined how much data to put into each set, let’s use sklearn
    to generate a dummy dataset that we can partition:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了每个集合中应该放入多少数据，接下来让我们使用 sklearn 来生成一个可以划分的虚拟数据集：
- en: '>>> import numpy as np'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import numpy as np'
- en: '>>> from sklearn.datasets import make_classification'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from sklearn.datasets import make_classification'
- en: '>>> x,y = make_classification(n_samples=10000, weights=(0.9,0.1))'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> x,y = make_classification(n_samples=10000, weights=(0.9,0.1))'
- en: '>>> x.shape'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> x.shape'
- en: (10000, 20)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: (10000, 20)
- en: '>>> len(np.where(y == 0)[0])'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> len(np.where(y == 0)[0])'
- en: '8969'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '8969'
- en: '>>> len(np.where(y == 1)[0])'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> len(np.where(y == 1)[0])'
- en: '1031'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '1031'
- en: Here, we’ve used two classes and 20 features to generate 10,000 samples. The
    dataset is imbalanced, with 90 percent of the samples in class 0 and 10 percent
    in class 1\. The output is a 2D array of samples (x) and associated 0 or 1 labels
    (y). The dataset is generated from multidimensional Gaussians that are the analogs
    of the normal bell curve in more than one dimension, but that doesn’t matter to
    us right now. The useful part for us is that we have a collection of feature vectors
    and labels, so that we can look at ways in which the dataset might be split into
    subsets.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两个类别和 20 个特征生成了 10,000 个样本。数据集是不平衡的，90%的样本属于类别 0，10%的样本属于类别 1。输出的是一个二维数组，其中包含样本（x）和相应的
    0 或 1 标签（y）。数据集是由多维高斯分布生成的，类似于一维正态分布的钟形曲线，但这对我们来说并不重要。对我们有用的部分是，我们有一组特征向量和标签，可以查看如何将数据集划分为子集。
- en: The key to the preceding code is the call to make_classification, which accepts
    the number of samples requested and the fraction for each class. The np.where
    calls simply find all the class 0 and class 1 instances so that len can count
    them.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的关键是调用 `make_classification`，它接受请求的样本数量和每个类别的比例。`np.where` 的调用只是为了找到所有类别
    0 和类别 1 的实例，以便 `len` 可以对它们进行计数。
- en: 'Earlier, we talked about the importance of preserving—or at least approaching—the
    actual prior probabilities of the different classes in our dataset. If one class
    makes up 10 percent of real world cases, it would ideally make up 10 percent of
    our dataset. Now we need to find a way to preserve this prior class probability
    in the subsets we make for training, validation, and test. There are two main
    ways to do this: partitioning by class and random sampling.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们讨论了保留——或至少接近——数据集中不同类别的实际先验概率的重要性。如果一个类别在现实世界中占据了 10%的比例，那么理想情况下它在我们的数据集中也应该占
    10%。现在我们需要找到一种方法，在我们为训练、验证和测试划分子集时，能够保留这个先验类别概率。这里有两种主要的做法：按类别划分和随机采样。
- en: Partitioning by Class
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 按类别划分
- en: The exact approach, which is suitable when the dataset is small or perhaps when
    one class is rare, is to determine the number of samples representing each class,
    and then set aside selected percentages of each, by class, before merging them
    together. So, if there are 9,000 samples from class 0, and 1,000 samples from
    class 1, and we want to put 90 percent of the data into training and 5 percent
    each into validation and test, we would select 8,100 samples, *at random*, from
    the class 0 collection and 900 samples, *at random*, from the class 1 collection
    to make up the training set. Similarly, we would randomly select 450 of the remaining
    900 unused class 0 samples for the validation set along with 50 of the remaining
    unused class 1 data. The remaining class 0 and class 1 samples become the test
    set.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 具体的方法适用于数据集较小，或者某个类别较少的情况，首先确定每个类别所代表的样本数量，然后按类别将选定的百分比样本分配出来，再合并在一起。所以，如果类别0有9,000个样本，类别1有1,000个样本，并且我们希望将90%的数据用于训练，5%用于验证，5%用于测试，我们会从类别0的集合中随机选择8,100个样本，从类别1的集合中随机选择900个样本，组成训练集。同样，我们会从剩余的900个类别0样本中随机选择450个样本作为验证集，并从剩余的类别1样本中选择50个。剩余的类别0和类别1样本将成为测试集。
- en: '[Listing 4-1](ch04.xhtml#ch4lis1) shows the code to construct the subsets using
    a 90/5/5 split of the original data.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4-1](ch04.xhtml#ch4lis1) 显示了使用90/5/5拆分原始数据来构建子集的代码。'
- en: import numpy as np
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: from sklearn.datasets import make_classification
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.datasets import make_classification
- en: ❶ a,b = make_classification(n_samples=10000, weights=(0.9,0.1))
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ a,b = make_classification(n_samples=10000, weights=(0.9,0.1))
- en: idx = np.where(b == 0)[0]
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.where(b == 0)[0]
- en: x0 = a[idx,:]
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: x0 = a[idx,:]
- en: y0 = b[idx]
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = b[idx]
- en: idx = np.where(b == 1)[0]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.where(b == 1)[0]
- en: x1 = a[idx,:]
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = a[idx,:]
- en: y1 = b[idx]
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = b[idx]
- en: ❷ idx = np.argsort(np.random.random(y0.shape))
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ idx = np.argsort(np.random.random(y0.shape))
- en: y0 = y0[idx]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = y0[idx]
- en: x0 = x0[idx]
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: x0 = x0[idx]
- en: idx = np.argsort(np.random.random(y1.shape))
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y1.shape))
- en: y1 = y1[idx]
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = y1[idx]
- en: x1 = x1[idx]
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = x1[idx]
- en: ❸ ntrn0 = int(0.9*x0.shape[0])
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ ntrn0 = int(0.9*x0.shape[0])
- en: ntrn1 = int(0.9*x1.shape[0])
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ntrn1 = int(0.9*x1.shape[0])
- en: xtrn = np.zeros((int(ntrn0+ntrn1),20))
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn = np.zeros((int(ntrn0+ntrn1),20))
- en: ytrn = np.zeros(int(ntrn0+ntrn1))
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = np.zeros(int(ntrn0+ntrn1))
- en: xtrn[:ntrn0] = x0[:ntrn0]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn[:ntrn0] = x0[:ntrn0]
- en: xtrn[ntrn0:] = x1[:ntrn1]
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: xtrn[ntrn0:] = x1[:ntrn1]
- en: ytrn[:ntrn0] = y0[:ntrn0]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn[:ntrn0] = y0[:ntrn0]
- en: ytrn[ntrn0:] = y1[:ntrn1]
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn[ntrn0:] = y1[:ntrn1]
- en: ❹ n0 = int(x0.shape[0]-ntrn0)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ n0 = int(x0.shape[0]-ntrn0)
- en: n1 = int(x1.shape[0]-ntrn1)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: n1 = int(x1.shape[0]-ntrn1)
- en: xval = np.zeros((int(n0/2+n1/2),20))
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: xval = np.zeros((int(n0/2+n1/2),20))
- en: yval = np.zeros(int(n0/2+n1/2))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: yval = np.zeros(int(n0/2+n1/2))
- en: xval[:(n0//2)] = x0[ntrn0:(ntrn0+n0//2)]
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: xval[:(n0//2)] = x0[ntrn0:(ntrn0+n0//2)]
- en: xval[(n0//2):] = x1[ntrn1:(ntrn1+n1//2)]
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: xval[(n0//2):] = x1[ntrn1:(ntrn1+n1//2)]
- en: yval[:(n0//2)] = y0[ntrn0:(ntrn0+n0//2)]
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: yval[:(n0//2)] = y0[ntrn0:(ntrn0+n0//2)]
- en: yval[(n0//2):] = y1[ntrn1:(ntrn1+n1//2)]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: yval[(n0//2):] = y1[ntrn1:(ntrn1+n1//2)]
- en: ❺ xtst = np.concatenate((x0[(ntrn0+n0//2):],x1[(ntrn1+n1//2):]))
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ xtst = np.concatenate((x0[(ntrn0+n0//2):],x1[(ntrn1+n1//2):]))
- en: ytst = np.concatenate((y0[(ntrn0+n0//2):],y1[(ntrn1+n1//2):]))
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = np.concatenate((y0[(ntrn0+n0//2):],y1[(ntrn1+n1//2):]))
- en: '*Listing 4-1: Exact construction of training, validation, and test datasets*'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-1：训练、验证和测试数据集的精确构建*'
- en: There’s a lot of bookkeeping in this code. First, we create the dummy dataset
    ❶ and split it into class 0 and class 1 collections, stored in x0,y0 and x1,y1,
    respectively. We then randomize the ordering ❷. This will let us pull off the
    first *n* samples for the subsets without worrying that we might be introducing
    a bias because of ordering in the data. Because of how sklearn generates the dummy
    dataset, this step isn’t required, but it’s always a good idea to ensure randomness
    in the ordering of samples.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码涉及很多的记录工作。首先，我们创建了虚拟数据集❶，并将其分为类别0和类别1的集合，分别存储在x0,y0和x1,y1中。接着，我们随机化数据的顺序❷。这样，我们可以从子集中抽取前*n*个样本，而不必担心因为数据顺序问题可能引入偏差。由于sklearn生成虚拟数据集的方式，这一步并不是必须的，但确保样本顺序的随机性始终是个好主意。
- en: We use a trick that’s helpful when reordering samples. Because we store the
    feature vectors in one array and the labels in another, the NumPy shuffle methods
    will not work. Instead, we generate a random vector of the same length as our
    number of samples and then use argsort to return the indices of the vector that
    would put it in sorted order. Since the values in the vector are random, the ordering
    of the indices used to sort it will also be random. These indices then reorder
    the samples and labels so that the each label is still associated with the correct
    feature vector.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个在重新排序样本时很有帮助的技巧。由于我们将特征向量存储在一个数组中，而标签存储在另一个数组中，NumPy的shuffle方法无法直接工作。相反，我们生成一个与样本数量相同长度的随机向量，然后使用argsort返回将该向量排序所需的索引。由于向量中的值是随机的，排序所使用的索引也会是随机的。然后，这些索引重新排序样本和标签，使得每个标签仍然与正确的特征向量相关联。
- en: Next, we extract the first 90 percent of samples for the two classes and build
    the training subset with samples in xtrn and labels in ytrn ❸. We do the same
    for the 5 percent validation set ❹ and the remaining 5 percent for the test set
    ❺.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从两个类别的前90%的样本中提取出数据，使用xtrn中的样本和ytrn中的标签来构建训练子集❸。我们对5%的验证集❹和剩下的5%的测试集❺也做相同的操作。
- en: Partitioning by class is tedious, to say the least. We do know, however, that
    the class 0 to class 1 ratio in each of the subsets is exactly the same.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别划分是相当繁琐的，至少可以这么说。不过，我们确实知道，每个子集中的类别0与类别1的比例是完全相同的。
- en: Random Sampling
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机抽样
- en: Must we be so precise? In general, no. The second common method for partitioning
    the full dataset is via random sampling. If we have enough data—and 10,000 samples
    is enough data—we can build our subsets by randomizing the full dataset and then
    extracting the first 90 percent of samples as the training set, the next 5 percent
    as the validation set, and the last 5 percent as the test set. This is what we
    show in [Listing 4-2](ch04.xhtml#ch4lis2).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非得这么精确吗？一般来说，不需要。另一种常见的数据集划分方法是通过随机抽样。如果我们有足够的数据——例如10,000个样本就是足够的——我们可以通过随机化整个数据集来构建子集，然后提取前90%的样本作为训练集，接下来的5%作为验证集，最后5%作为测试集。这就是[列表
    4-2](ch04.xhtml#ch4lis2)中展示的方法。
- en: ❶ x,y = make_classification(n_samples=10000, weights=(0.9,0.1))
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x, y = make_classification(n_samples=10000, weights=(0.9, 0.1))
- en: idx = np.argsort(np.random.random(y.shape[0]))
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: idx = np.argsort(np.random.random(y.shape[0]))
- en: x = x[idx]
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]
- en: y = y[idx]
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[idx]
- en: ❷ ntrn = int(0.9*y.shape[0])
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ntrn = int(0.9*y.shape[0])
- en: nval = int(0.05*y.shape[0])
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: nval = int(0.05*y.shape[0])
- en: ❸ xtrn = x[:ntrn]
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xtrn = x[:ntrn]
- en: ytrn = y[:ntrn]
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ytrn = y[:ntrn]
- en: xval = x[ntrn:(ntrn+nval)]
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: xval = x[ntrn:(ntrn+nval)]
- en: yval = y[ntrn:(ntrn+nval)]
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: yval = y[ntrn:(ntrn+nval)]
- en: xtst = x[(ntrn+nval):]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: xtst = x[(ntrn+nval):]
- en: ytst = y[(ntrn+nval):]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ytst = y[(ntrn+nval):]
- en: '*Listing 4-2: Random construction of training, validation, and test datasets*'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 4-2：随机构建训练集、验证集和测试集*'
- en: We randomize the dummy dataset stored in x and y ❶. We need to know how many
    samples to include in each of the subsets. First, the number of samples for the
    training set is 90 percent of the total in the dataset ❷, while the number in
    the validation set is 5 percent of the total. The remainder, also 5 percent, is
    the test set ❸.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对存储在x和y中的虚拟数据集进行随机化❶。我们需要知道每个子集中应该包含多少样本。首先，训练集的样本数是数据集总数的90%❷，验证集的样本数是总数的5%。剩下的5%则作为测试集❸。
- en: This method is so much simpler than the one shown in [Listing 4-1](ch04.xhtml#ch4lis1).
    What’s the downside of using it? The possible downside is that the mix of classes
    in each of these subsets might not quite be the fractions we want. For example,
    imagine we want a training set of 9,000 samples, or 90 percent of the original
    10,000 samples, with 8,100 of them from class 0, and 900 of them from class 1\.
    Running the [Listing 4-2](ch04.xhtml#ch4lis2) code 10 times gives the splits between
    class 0 and class 1 in the training set that are shown in [Table 4-9](ch04.xhtml#ch4tab9).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法比[列表 4-1](ch04.xhtml#ch4lis1)中展示的方法简单得多。那么，使用它有什么缺点呢？可能的缺点是，这些子集中的每个类别的混合比例可能与我们想要的比例不完全一致。例如，假设我们想要一个包含9,000个样本的训练集，或者说是原始10,000个样本的90%，其中8,100个来自类别0，900个来自类别1。运行[列表
    4-2](ch04.xhtml#ch4lis2)代码10次，得到的训练集中类别0和类别1的分割情况如[表 4-9](ch04.xhtml#ch4tab9)所示。
- en: '**Table 4-9:** Ten Training Splits Generated by Random Sampling'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-9：** 随机抽样生成的十个训练集分割'
- en: '| **Run** | **Class 0** | **Class 1** |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **运行** | **类别0** | **类别1** |'
- en: '| --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 8058 (89.5) | 942 (10.5) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8058 (89.5) | 942 (10.5) |'
- en: '| 2 | 8093 (89.9) | 907 (10.1) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8093 (89.9) | 907 (10.1) |'
- en: '| 3 | 8065 (89.6) | 935 (10.4) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 8065 (89.6) | 935 (10.4) |'
- en: '| 4 | 8081 (89.8) | 919 (10.2) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 8081 (89.8) | 919 (10.2) |'
- en: '| 5 | 8045 (89.4) | 955 (10.6) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 8045 (89.4) | 955 (10.6) |'
- en: '| 6 | 8045 (89.4) | 955 (10.6) |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 8045 (89.4) | 955 (10.6) |'
- en: '| 7 | 8066 (89.6) | 934 (10.4) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 8066 (89.6) | 934 (10.4) |'
- en: '| 8 | 8064 (89.6) | 936 (10.4) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8064 (89.6) | 936 (10.4) |'
- en: '| 9 | 8071 (89.7) | 929 (10.3) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 8071 (89.7) | 929 (10.3) |'
- en: '| 10 | 8063 (89.6) | 937 (10.4) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 8063 (89.6) | 937 (10.4) |'
- en: The number of samples in class 1 ranges from as few as 907 samples to as many
    as 955 samples. As the number of samples of a particular class in the full dataset
    decreases, the number in the subsets will start to vary more. This is especially
    true of smaller subsets, like the validation and test sets. Let’s do a separate
    run, this time looking at the number of samples from each class in the *test*
    set ([Table 4-10](ch04.xhtml#ch4tab10)).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 1 的样本数量从最少的 907 个样本到最多的 955 个样本不等。随着特定类别在完整数据集中的样本数量减少，子集中的数量会开始更多地变化。这一点在较小的子集，如验证集和测试集中尤为明显。我们进行一次单独的运行，这次查看
    *测试* 集合中每个类别的样本数量（[表 4-10](ch04.xhtml#ch4tab10)）。
- en: '**Table 4-10:** Ten Test Splits Generated by Random Sampling'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-10：** 通过随机抽样生成的十个测试集划分'
- en: '| **Run** | **Class 0** | **Class 1** |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| **运行** | **类别 0** | **类别 1** |'
- en: '| --- | --- | --- |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 446 (89.2) | 54 (10.8) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 446 (89.2) | 54 (10.8) |'
- en: '| 2 | 450 (90.0) | 50 (10.0) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 450 (90.0) | 50 (10.0) |'
- en: '| 3 | 444 (88.8) | 56 (11.2) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 444 (88.8) | 56 (11.2) |'
- en: '| 4 | 450 (90.0) | 50 (10.0) |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 450 (90.0) | 50 (10.0) |'
- en: '| 5 | 451 (90.2) | 49 (9.8) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 451 (90.2) | 49 (9.8) |'
- en: '| 6 | 462 (92.4) | 38 (7.6) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 462 (92.4) | 38 (7.6) |'
- en: '| 7 | 441 (88.2) | 59 (11.8) |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 441 (88.2) | 59 (11.8) |'
- en: '| 8 | 449 (89.8) | 51 (10.2) |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 449 (89.8) | 51 (10.2) |'
- en: '| 9 | 449 (89.8) | 51 (10.2) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 449 (89.8) | 51 (10.2) |'
- en: '| 10 | 438 (87.6) | 62 (12.4) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 438 (87.6) | 62 (12.4) |'
- en: In the test set, the number of samples from class 1 ranges from 38 to 62.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集中，类别 1 的样本数量从 38 到 62 不等。
- en: Will these differences influence how the model learns? Probably not, but they
    might make the test results look better than they are, as most models struggle
    to identify the classes that are least common in the training set. The possibility
    exists of a pathological split that results in having no examples from a particular
    class, but in practice, it’s not really that likely unless your pseudorandom number
    generator is particularly poor. Still, it’s worth keeping the possibility in mind.
    If concerned, use the exact split approach in [Listing 4-1](ch04.xhtml#ch4lis1).
    In truth, the better solution is, as always, to get more data.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异会影响模型学习的方式吗？可能不会，但它们可能使测试结果看起来比实际情况更好，因为大多数模型在识别训练集中最不常见的类别时会遇到困难。也有可能出现病态拆分，导致某个类别没有样本，但实际上，除非伪随机数生成器非常差，否则这种情况不太可能发生。不过，还是值得考虑这种可能性。如果担心，使用[清单
    4-1](ch04.xhtml#ch4lis1)中的精确拆分方法。事实上，最好的解决方案，像往常一样，是获取更多数据。
- en: 'Algorithmically, the steps to produce the training, validation, and test splits
    are as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 算法上，生成训练集、验证集和测试集的步骤如下：
- en: Randomize the order of the full dataset so that classes are evenly mixed.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化整个数据集的顺序，以确保类别均匀混合。
- en: Calculate the number of samples in the training (ntrn) and validation (nval)
    sets by multiplying the number of samples in the full dataset by the desired fraction.
    The remaining samples will fall into the test set.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将完整数据集中样本的数量乘以所需的比例，计算训练集（ntrn）和验证集（nval）中的样本数量。剩余的样本将分配到测试集中。
- en: Assign the first ntrn samples to the training set.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前 ntrn 样本分配到训练集。
- en: Assign the next nval samples to the validation set.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将下一个 nval 样本分配到验证集。
- en: Finally, assign the remaining samples to the test set.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将剩余的样本分配到测试集。
- en: At all times, ensure that the order of the samples is truly random, and that
    when reordering the feature vectors, you’re sure to reorder the labels in the
    exact same sequence. If this is done, this simple splitting process will give
    a good split unless the dataset is very small or some classes are very rare.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 始终确保样本的顺序是真正随机的，并且在重新排列特征向量时，要确保标签按照完全相同的顺序重新排列。如果做到了这一点，除非数据集非常小或者某些类别非常稀有，否则这个简单的拆分过程会产生一个好的拆分。
- en: We neglected to discuss one consequence of this approach. If the full dataset
    is small to begin with, partitioning it will make the training set even smaller.
    In [Chapter 7](ch07.xhtml#ch07), we’ll see a powerful approach to dealing with
    a small dataset, one that’s used heavily in deep learning. But first, let’s look
    at a principled way to work with a small dataset to get an idea of how well it
    will perform on new data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们忽略了讨论这种方法的一个后果。如果原始数据集本身很小，将其划分后训练集会变得更小。在[第7章](ch07.xhtml#ch07)中，我们将看到一种处理小数据集的强大方法，这种方法在深度学习中得到广泛应用。但首先，让我们看看一种有原则的方法，用来处理小数据集，以了解它在新数据上表现如何。
- en: k-Fold Cross Validation
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: k折交叉验证
- en: Modern deep learning models typically need very large datasets, and therefore,
    you’re able to use a single training/validation/test split as described previously.
    More traditional machine learning models, like those in [Chapter 6](ch06.xhtml#ch06),
    however, often work with datasets that are too small (in general) for deep learning
    models. If we use a single training/validation/test split on those datasets, we
    might be holding too much data back for testing, or else have too few samples
    in the test set to get a meaningful measurement of how well the model is working.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习模型通常需要非常大的数据集，因此，你可以像前面描述的那样，使用单一的训练/验证/测试划分。然而，像[第6章](ch06.xhtml#ch06)中的那些传统机器学习模型，通常使用的数据集对于深度学习模型来说过小（一般而言）。如果我们在这些数据集上使用单一的训练/验证/测试划分，可能会把太多数据用于测试，或者测试集中的样本太少，无法对模型的效果做出有意义的评估。
- en: One way to address this issue is to use *k-fold cross validation*, a technique
    that ensures each sample in the dataset is used at some point for training and
    testing. Use this technique for small datasets intended for traditional machine
    learning models. It can also be helpful as a way to decide between different models.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用*k*-折交叉验证，这是一种确保数据集中每个样本在某个时间点都用于训练和测试的技术。对于小数据集，可以使用这种技术，特别是传统机器学习模型。它也可以作为在不同模型之间做出选择的一个有用工具。
- en: To do *k*-fold cross validation, first partition the full, randomized dataset
    into *k* non-overlapping groups, *x*[0],*x*[1],*x*[2],…,*x*[*k–*1]. Your *k* value
    is arbitrary, though it typically ranges from 5 to 10\. [Figure 4-4](ch04.xhtml#ch4fig4)a
    shows this split, imagining the entire dataset laid out horizontally.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行*k*-折交叉验证，首先将完整的随机化数据集划分为*k*个互不重叠的组，*x*[0]、*x*[1]、*x*[2]、……、*x*[*k–*1]。你的*k*值是任意的，通常在5到10之间。[图4-4](ch04.xhtml#ch4fig4)a展示了这种划分，假设整个数据集是水平展开的。
- en: We can train a model by holding *x*[0] back as test data and using the other
    groups, *x*[1],*x*[2],…,*x*[*k–*1] as training data. We’ll ignore validation data
    for the time being; after building the current training data, we can always hold
    some of it back as validation data if we want. Call this trained model *m*[0].
    You can then start over from scratch, this time holding back *x*[1] as test data
    and training with all the other groups, including *x*[0]. We’ll get a new trained
    model. Call it *m*[1]. By design, *m*[0] and *m*[1] are the same *type* of model.
    What we are interested in here is multiple instances of the same type of model
    trained with different subsets of the full dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将*x*[0]作为测试数据，其它组*x*[1]、*x*[2]、……、*x*[*k–*1]作为训练数据来训练一个模型。我们暂时忽略验证数据；在构建当前训练数据后，如果需要，我们可以随时将一部分数据保留作为验证数据。将这个训练后的模型称为*m*[0]。然后，你可以从头开始，这次将*x*[1]作为测试数据，并用其余的组（包括*x*[0]）来训练。我们将得到一个新的训练模型，称为*m*[1]。根据设计，*m*[0]和*m*[1]是相同*类型*的模型。我们关心的是同类型的模型在不同的完整数据集子集上训练得到的多个实例。
- en: Repeat this process for each of the groups, as in [Figure 4-4](ch04.xhtml#ch4fig4)b,
    and we’ll have *k* models trained with (*k –* 1)/*k* of the data each, holding
    1/*k* of the data back for testing. What *k* should be depends upon how much data
    is in the full dataset. Larger *k* means more training data but less test data.
    If the per model training time is low, tend toward a larger *k* as this increases
    the per model training set size.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个组重复这个过程，如[图4-4](ch04.xhtml#ch4fig4)b所示，我们将得到用(*k –* 1)/*k*的数据训练的*k*个模型，每个模型保留1/*k*的数据用于测试。*k*的值应该根据完整数据集中的数据量来确定。较大的*k*意味着更多的训练数据，但测试数据较少。如果每个模型的训练时间较短，倾向于选择较大的*k*，因为这样可以增加每个模型的训练数据集大小。
- en: '![image](Images/04fig04.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig04.jpg)'
- en: '*Figure 4-4:* k-*fold cross validation. Partitioning the dataset into non-overlapping
    regions*, k=*7(a). The first three train/test splits using first* x[0] *for test,
    then* x[1] *for test, and so on (b)*.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4-4：*k-*折交叉验证。将数据集划分为不重叠的区域*，k=*7（a）。前三个训练/测试拆分，首先使用* x[0] *作为测试集，然后是* x[1]
    *作为测试集，依此类推（b）。'
- en: Once the *k* models are trained, you can evaluate them individually and average
    their metrics to get an idea of how a model trained on the full dataset would
    behave. See [Chapter 11](ch11.xhtml#ch11) to learn about ways to evaluate a model.
    If using *k*-fold cross validation to select among two or more models (say, between
    using *k*-NN or a Support Vector Machine^([1](ch04.xhtml#ch04fn1))), repeat the
    full training and evaluation process for each type of model and compare their
    results.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练了*k*个模型，你可以单独评估它们，并通过平均它们的度量标准来了解在整个数据集上训练的模型的表现。请参阅[第11章](ch11.xhtml#ch11)了解如何评估模型。如果使用*k*-折交叉验证来选择多个模型之间的最佳模型（例如，在使用*k*-NN或支持向量机之间选择([1](ch04.xhtml#ch04fn1))），则需要对每种模型重复完整的训练和评估过程，并比较它们的结果。
- en: 'Once we have an idea of how well the model is performing on the averaged evaluation
    metrics, we can start over again and train the selected model type using *all*
    of the dataset for training. This is the advantage of *k*-fold cross validation:
    it lets you have your cake and eat it, too.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了模型在平均评估度量上的表现的概念，我们可以重新开始，使用*所有*数据集进行训练，训练所选择的模型类型。这就是*k*-折交叉验证的优势：它让你既能享受成功，又能兼顾其他需求。
- en: Look at Your Data
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看你的数据
- en: 'It’s quite easy to assemble features and feature vectors, and then go ahead
    and put the training, validation, and test sets together without pausing to *look*
    at the data to see if it makes sense. This is especially true with deep learning
    models using huge collections of images or other multidimensional data. Here are
    a few problems you’ll want to look out for:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征和特征向量组装起来，然后继续将训练集、验证集和测试集合并，而不暂停*查看*数据是否合理，这非常容易做到。对于使用大量图像或其他多维数据的深度学习模型，这尤其如此。以下是一些你需要留意的问题：
- en: '**Mislabeled data** Assume we’re building a large dataset—one with hundreds
    of thousands of labeled samples. Further, assume that we’re going to use the dataset
    to build a model that will be able to tell the difference between dogs and cats.
    Naturally, we need to feed the model many, many dog images and many, many cat
    images. No problem, you say; we’ll just collect a lot of images using something
    like Google Images. Okay, that’ll work. But if you simply set up a script to download
    image search results matching “dog” and “cat,” you’ll also get a lot of other
    images that are not of dogs or cats, or images that contain dogs and cats along
    with other things. The labels won’t be perfect. While it is true that deep learning
    models can be resistant to such label noise, you want to avoid it whenever possible.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签错误的数据** 假设我们正在构建一个大型数据集——一个包含数十万条标注样本的数据集。进一步假设，我们将使用该数据集构建一个能够区分狗和猫的模型。当然，我们需要向模型提供大量的狗图像和猫图像。你可能会说，这没问题；我们只需要使用像Google
    Images这样的工具收集大量图像。好吧，这可以工作。但如果你简单地设置一个脚本，下载与“狗”和“猫”匹配的图像搜索结果，你还会得到许多其他不是狗或猫的图像，或者包含狗和猫以及其他事物的图像。标签将不完美。尽管深度学习模型对这种标签噪声有一定的抗性，但你仍然希望尽可能避免它。'
- en: '**Missing or outlier data** Imagine you have a collection of feature vectors,
    and you have no idea how common it is that features are missing. If a large percentage
    of a particular feature is missing, that feature will become a hindrance to the
    model and you should eliminate it. Or, if there are extreme outliers in the data,
    you might want to remove those samples, especially if you’re going to standardize,
    since outliers will strongly affect the mean subtracted from the feature values.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺失或异常数据** 想象一下，你有一组特征向量，但你完全不知道特征缺失的情况有多普遍。如果某个特征的大部分数据缺失，那么这个特征将成为模型的障碍，你应该将其剔除。或者，如果数据中存在极端的异常值，你可能会希望删除这些样本，特别是如果你要进行标准化处理，因为异常值会强烈影响从特征值中减去的均值。'
- en: Searching for Problems in the Data
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在数据中搜索问题
- en: How can we look for these problems in the data? Well, for feature vectors, we
    can often load the dataset into a spreadsheet, if it isn’t too large. Or we could
    write a Python script to summarize the data, feature by feature, or bring the
    data into a statistics program and examine it that way.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在数据中查找这些问题呢？对于特征向量，如果数据集不太大，我们通常可以将其加载到电子表格中。或者我们可以编写一个Python脚本，逐个特征地汇总数据，或者将数据导入统计程序进行检查。
- en: Typically, when summarizing values statistically, we look at the mean and standard
    deviation, both defined previously, as well as the largest value and the smallest
    value. We could also look at the median, which is the value we get when we sort
    the values from smallest to largest and pick the one in the middle. (If the number
    of values is even, we’d average the two middle values.) Let’s look at one of the
    features from our earlier example. After sorting the values from smallest to largest,
    we can summarize the data in the following way.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在统计总结数据时，我们会查看均值和标准差（如前所述），以及最大值和最小值。我们还可以查看中位数，即将数据从小到大排序后，取中间的那个值。（如果数据点的数量为偶数，我们则取中间两个值的平均值。）让我们来看一下之前例子中的某个特征。将值从小到大排序后，我们可以按以下方式总结数据。
- en: '| ***x*[2]** |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| ***x*[2]** |'
- en: '| --- |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| 0.0705 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 0.0705 |'
- en: '| 0.3408 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 0.3408 |'
- en: '| 0.9804 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 0.9804 |'
- en: '| 1.5362 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 1.5362 |'
- en: '| 1.7821 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 1.7821 |'
- en: '| 2.0085 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 2.0085 |'
- en: '| 2.1271 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 2.1271 |'
- en: '| **2.3190** |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| **2.3190** |'
- en: '| 2.3944 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 2.3944 |'
- en: '| 2.7561 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 2.7561 |'
- en: '| 2.7887 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 2.7887 |'
- en: '| 3.0150 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 3.0150 |'
- en: '| 3.9897 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 3.9897 |'
- en: '| 4.3465 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 4.3465 |'
- en: '| 4.8231 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 4.8231 |'
- en: '| Mean (![Image](Images/xbar.jpg)) | = | 2.3519 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 (![Image](Images/xbar.jpg)) | = | 2.3519 |'
- en: '| Standard deviation (*σ*) | = | 1.3128 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 (*σ*) | = | 1.3128 |'
- en: '| Standard error (SE) | = | 0.3390 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 标准误差 (SE) | = | 0.3390 |'
- en: '| Median | = | 2.3190 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | = | 2.3190 |'
- en: '| Minimum | = | 0.0705 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 最小值 | = | 0.0705 |'
- en: '| Maximum | = | 4.8231 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | = | 4.8231 |'
- en: We’ve already explored the concepts of mean, minimum, maximum, and standard
    deviation. The median is there, as well; I’ve highlighted it in the list of features
    on the left. Notice that after sorting, the median appears in the exact middle
    of the list. It’s often known as the *50th percentile*, because the same amount
    of data is above it as below.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了均值、最小值、最大值和标准差的概念。中位数也在其中；我已在左侧的特征列表中标出了它。注意，排序后，中位数正好处于列表的中间。它通常被称为*第50百分位数*，因为其上方和下方的数据量相等。
- en: 'There is also a new value listed, the *standard error*, also called the *standard
    error of the mean*. This is the standard deviation divided by the square root
    of the number of values in the dataset:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个列出的新值是*标准误差*，也叫做*均值的标准误差*。它是标准差除以数据集中值的数量的平方根：
- en: '![image](Images/077equ01.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/077equ01.jpg)'
- en: 'The standard error is a measure of the difference between our mean value, ![Image](Images/xbar.jpg),
    and the mean value of the parent distribution. The basic idea is this: if we have
    more measurements, we’ll have a better idea of the parent distribution that is
    generating the data, and so the mean value of the measurements will be closer
    to the mean value of the parent distribution.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 标准误差是衡量我们计算的均值（![Image](Images/xbar.jpg)）与母体分布均值之间差异的一个指标。基本思想是：如果我们有更多的测量数据，就能更好地了解产生这些数据的母体分布，因此测量的均值将更接近母体分布的均值。
- en: Notice also that the mean and the median are relatively close to each other.
    The phrase *relatively close* has no rigorous mathematical meaning, of course,
    but we can use it as an ad hoc indicator that the data might be normally distributed,
    meaning we could reasonably replace the missing values by the mean (or median),
    as we saw previously.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意的是，均值和中位数相对接近。术语*相对接近*当然没有严格的数学定义，但我们可以将其作为一个临时的指示器，表明数据可能呈正态分布，这意味着我们可以合理地用均值（或中位数）来替代缺失值，正如我们之前所看到的那样。
- en: The preceding values were computed easily using NumPy, as seen in [Listing 4-3](ch04.xhtml#ch4lis3).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数值是通过NumPy轻松计算得到的，如[Listing 4-3](ch04.xhtml#ch4lis3)所示。
- en: import numpy as np
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: ❶ f = [0.3408,3.0150,4.3465,2.1271,2.7561,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f = [0.3408,3.0150,4.3465,2.1271,2.7561,
- en: 2.7887,4.8231,0.0705,3.9897,0.9804,
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 2.7887,4.8231,0.0705,3.9897,0.9804,
- en: 2.3944,2.0085,1.7821,1.5362,2.3190]
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3944,2.0085,1.7821,1.5362,2.3190]
- en: f = np.array(f)
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: f = np.array(f)
- en: print
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: print
- en: print("mean  = %0.4f" % f.mean())
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: print("平均值 = %0.4f" % f.mean())
- en: print("std   = %0.4f" % f.std())
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: print("标准差 = %0.4f" % f.std())
- en: ❷ print("SE    = %0.4f" % (f.std()/np.sqrt(f.shape[0])))
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ print("标准误差 = %0.4f" % (f.std()/np.sqrt(f.shape[0])))
- en: print("median= %0.4f" % np.median(f))
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: print("中位数 = %0.4f" % np.median(f))
- en: print("min   = %0.4f" % f.min())
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: print("最小值 = %0.4f" % f.min())
- en: print("max   = %0.4f" % f.max())
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: print("最大值 = %0.4f" % f.max())
- en: '*Listing 4-3: Calculating basic statistics. See* feature_stats.py.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 4-3: 计算基本统计数据。见* feature_stats.py。'
- en: After loading NumPy, we manually define the *x*[2] features (f) and turn them
    into a NumPy array ❶. Once the data is a NumPy array, calculating the desired
    values is straightforward, as all of them, except the standard error, are simple
    method or function calls. The standard error is calculated via the preceding formula
    ❷ where the first element of the tuple NumPy returns for the shape is the number
    of elements in a vector.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载 NumPy 后，我们手动定义 *x*[2] 特征（f），并将其转化为 NumPy 数组 ❶。一旦数据变为 NumPy 数组，计算所需的值就变得很简单，除了标准误差外，其他都是直接通过方法或函数调用得到的。标准误差通过以下公式计算
    ❷，其中 NumPy 返回的元组的第一个元素表示向量中的元素数量。
- en: Numbers are nice, but pictures are often better. You can visualize the data
    with a *box plot* in Python. Let’s generate one to view the standardized values
    of our dataset. Then we’ll discuss what the plot is showing us. The code to create
    the plot is in [Listing 4-4](ch04.xhtml#ch4lis4).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 数字很重要，但图像往往更能直观表现数据。你可以通过 Python 生成 *箱线图* 来可视化数据。让我们生成一个箱线图，查看我们数据集的标准化值。然后我们将讨论这个图表向我们展示了什么。生成图表的代码在
    [清单 4-4](ch04.xhtml#ch4lis4) 中。
- en: import numpy as np
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pyplot as plt
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: ❶ d = [[ 0.6930, -1.1259, -1.5318,  0.9525,  1.1824],
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ d = [[ 0.6930, -1.1259, -1.5318,  0.9525,  1.1824],
- en: '[ 0.5464, -0.0120,  0.5051, -0.0192, -0.1141],'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.5464, -0.0120,  0.5051, -0.0192, -0.1141],'
- en: '[ 0.8912,  1.3826,  1.5193, -1.1996, -1.1403],'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.8912,  1.3826,  1.5193, -1.1996, -1.1403],'
- en: '[ 1.1690,  0.4970, -0.1712, -0.5340,  0.3047],'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.1690,  0.4970, -0.1712, -0.5340,  0.3047],'
- en: '[-0.9221, -0.1071,  0.3079, -0.3885, -0.4753],'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[-0.9221, -0.1071,  0.3079, -0.3885, -0.4753],'
- en: '[ 1.5699, -1.4767,  0.3327,  1.4714,  1.1807],'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.5699, -1.4767,  0.3327,  1.4714,  1.1807],'
- en: '[-0.3479,  0.4775,  1.8823, -1.4031, -0.7396],'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[-0.3479,  0.4775,  1.8823, -1.4031, -0.7396],'
- en: '[ 0.0887, -0.4353, -1.7377, -1.2349,  1.7456],'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.0887, -0.4353, -1.7377, -1.2349,  1.7456],'
- en: '[ 1.0775,  0.9524,  1.2475,  0.7291, -1.1207],'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 1.0775,  0.9524,  1.2475,  0.7291, -1.1207],'
- en: '[-1.4657,  0.9250, -1.0446,  0.4262, -1.0279],'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.4657,  0.9250, -1.0446,  0.4262, -1.0279],'
- en: '[-1.3332,  1.4501,  0.0323,  1.1102, -0.8966],'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.3332,  1.4501,  0.0323,  1.1102, -0.8966],'
- en: '[ 0.3005, -1.4500, -0.2615,  1.7033, -0.2505],'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.3005, -1.4500, -0.2615,  1.7033, -0.2505],'
- en: '[-1.4377, -0.2472, -0.4340, -0.7032,  0.3362],'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.4377, -0.2472, -0.4340, -0.7032,  0.3362],'
- en: '[ 0.3016, -1.5527, -0.6213,  0.1780, -0.7517],'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0.3016, -1.5527, -0.6213,  0.1780, -0.7517],'
- en: '[-1.1315,  0.7225, -0.0250, -1.0881,  1.7674]]'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[-1.1315,  0.7225, -0.0250, -1.0881,  1.7674]]'
- en: ❷ d = np.array(d)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ d = np.array(d)
- en: plt.boxplot(d)
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: plt.boxplot(d)
- en: plt.show()
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: '*Listing 4-4: A box plot of the standardized toy dataset*. See box_plot.py.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 4-4：标准化玩具数据集的箱线图*。参见 box_plot.py。'
- en: The values themselves are in [Table 4-6](ch04.xhtml#ch4tab6). We can store the
    data as a 2D array and make the box plot using [Listing 4-4](ch04.xhtml#ch4lis4).
    We manually define the array ❶ and then plot it ❷. The plot is interactive, so
    experiment with the environment provided until you feel comfortable with it. The
    old-school floppy disk icon will store the plot to your disk.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值本身可以在 [表 4-6](ch04.xhtml#ch4tab6) 中找到。我们可以将数据存储为一个二维数组，并使用 [清单 4-4](ch04.xhtml#ch4lis4)
    生成箱线图。我们手动定义数组 ❶，然后绘制图形 ❷。该图是交互式的，因此可以在提供的环境中进行实验，直到你熟悉为止。旧式的软盘图标将把图形存储到你的磁盘中。
- en: The box plot generated by the program is shown in [Figure 4-5](ch04.xhtml#ch4fig5).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 程序生成的箱线图如 [图 4-5](ch04.xhtml#ch4fig5) 所示。
- en: '![image](Images/04fig05.jpg)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig05.jpg)'
- en: '*Figure 4-5: The box plot produced by [Listing 4-4](ch04.xhtml#ch4lis4)*'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-5：由 [清单 4-4](ch04.xhtml#ch4lis4) 生成的箱线图*'
- en: How do we interpret the box plot? I’ll show you by examining the box representing
    the standardized feature *x*[2], shown in [Figure 4-6](ch04.xhtml#ch4fig6).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读箱线图呢？我将通过检查代表标准化特征 *x*[2] 的箱体来展示，见 [图 4-6](ch04.xhtml#ch4fig6)。
- en: The lower box line, Q1, marks the end of the first quartile. This means that
    25 percent of the data values for a feature are less than this value. The median,
    Q2, is the 50 percent mark, and therefore is the end of the second quartile. Half
    the data values are less than this value. The upper box line, Q3, is the 75 percent
    mark. The remaining 25 percent of the data values are above Q3.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 下箱线 Q1 标志着第一四分位数的结束。这意味着该特征的 25% 数据值小于这个值。中位数 Q2 是 50% 的分界点，因此是第二四分位数的结束。数据中有一半的值小于这个值。上箱线
    Q3 是 75% 的分界点。剩余的 25% 数据值大于 Q3。
- en: '![image](Images/04fig06.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/04fig06.jpg)'
- en: '*Figure 4-6: The standardized feature x2 from our dataset*'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-6：来自我们数据集的标准化特征 x2*'
- en: Two lines above and below the box are also shown. These are the *whiskers*.
    (Matplotlib calls them *fliers*, but this is an unconventional term.) The whiskers
    are the values at Q1 *–* 1.5 × IQR and Q3 + 1.5 × IQR. By convention, values outside
    this range are considered *outliers*.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 盒子上下的两条线也被展示出来。这些是*须状线*。（Matplotlib称之为*飞点*，但这是一个不常用的术语。）须状线的值为Q1 *–* 1.5 × IQR和Q3
    + 1.5 × IQR。根据惯例，超出这个范围的值被认为是*异常值*。
- en: Looking at outliers can be helpful, because you might realize they’re mistakes
    in data entry and drop them from the dataset. Whatever you do with the outliers,
    however, be prepared to justify it should you ever plan on publishing or otherwise
    presenting results based on the dataset. Similarly, you might be able to drop
    samples with missing values, but make sure there’s no systematic error causing
    the missing data, and check that you’re not introducing bias into the data by
    dropping those samples. In the end, common sense should override slavish adherence
    to convention.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 查看异常值可能会有所帮助，因为你可能会意识到它们是数据输入错误，并将它们从数据集中删除。无论你对异常值做什么，然而，如果你计划发布或以其他方式展示基于该数据集的结果，都应该准备好为此辩护。类似地，你可能会删除缺失值的样本，但要确保没有系统性错误导致数据缺失，并检查删除这些样本是否会给数据带来偏差。最终，常识应当优于对惯例的盲目遵守。
- en: Cautionary Tales
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 警示故事
- en: So, at the risk of being repetitive, *look at your data*. The more you work
    with it, the more you will understand it, and the more effectively you will be
    able to make reasonable decisions about what goes in and what comes out, and *why*.
    Recall that the goal of the dataset is to faithfully and completely capture the
    parent distribution, or what the data will look like in the wild when the model
    is used.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，为了避免重复，*请查看你的数据*。你与数据打交道的时间越长，你对它的理解就越深入，你就越能有效地做出合理的决定，了解哪些数据应该被包括，哪些应该被排除，以及*为什么*。请记住，数据集的目标是忠实而完整地捕捉母体分布，或者说，当模型投入使用时，数据在实际环境中将呈现的样貌。
- en: Two quick anecdotes come to mind. They both illustrate ways models may well
    learn things we did not intend or even consider.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 两个简单的轶事浮现在我的脑海。它们都说明了模型可能学到了我们没有预料到甚至没有考虑到的东西。
- en: The first was told to me as an undergraduate student in the 1980s. In this story,
    an early form of neural network was tasked with detecting tank and non-tank images.
    The neural network seemed to work well in testing, but when used in the field,
    the detection rate dropped rapidly. The researchers realized that the tank images
    were taken on a cloudy day, and the non-tank were taken on a sunny day. The recognition
    system had not learned the difference between tanks and non-tanks at all; instead,
    it had learned the difference between cloudy and sunny days. The moral of this
    story is that the training set needs to include *all* of the conditions the model
    will see in the wild.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个故事是在我1980年代作为本科生时听到的。在这个故事中，早期形式的神经网络被赋予了识别坦克和非坦克图像的任务。神经网络在测试中似乎表现得很好，但在实际应用中，检测率迅速下降。研究人员意识到，坦克的图像是在多云的日子里拍摄的，而非坦克的图像是在晴天拍摄的。这个识别系统根本没有学到坦克和非坦克之间的区别；相反，它学到了多云和晴天之间的区别。这个故事的教训是，训练集需要包括模型在实际环境中会遇到的*所有*条件。
- en: The second anecdote is more recent. I heard it in a talk at the Neural Information
    Processing Systems (NIPS) 2016 conference in Barcelona, Spain, and later found
    it repeated in the researchers’ paper.^([2](ch04.xhtml#ch04fn2)) In this case,
    the authors, who were demonstrating their technique for getting a model to explain
    its decisions, trained a model that claimed to tell the difference between images
    of huskies and images of wolves. The model appeared to work rather well, and during
    the talk, the authors polled the audience composed of machine learning researchers
    about how believable the model was. Most thought it was a good model. Then, using
    their technique, the speaker revealed that the network had not learned much, if
    anything, about the difference between huskies and wolves. Instead, it had learned
    that the wolf pictures had snow in the background and the husky pictures did not.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个轶事较为近期。我在2016年西班牙巴塞罗那的神经信息处理系统（NIPS）会议上听到过，后来在研究人员的论文中发现了这一故事的重复^[2](ch04.xhtml#ch04fn2)。在这个案例中，作者们展示了他们让模型解释决策的技术，他们训练了一个模型，声称能够区分哈士奇和狼的图像。该模型似乎工作得相当好，在演讲过程中，作者们向由机器学习研究人员组成的观众群体询问该模型的可信度。大多数人认为这是一个很好的模型。然后，使用他们的技术，演讲者揭示了网络实际上并没有学到哈士奇和狼之间的任何区别。相反，它学到的是，狼的图片背景有雪，而哈士奇的图片没有雪。
- en: Think about your data and be on the lookout for unintended consequences. Models
    are not human. We bring a lot of preconceived notions and unintended biases to
    the dataset.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 思考你的数据，并留意可能出现的意外后果。模型不是人类，我们带入数据集的往往是许多先入为主的观念和无意的偏见。
- en: Summary
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we described the components of a dataset (classes, labels,
    features, feature vectors) and then characterized a good dataset, emphasizing
    the importance of ensuring that the dataset well represents the parent distribution.
    We then described basic data preparation techniques including how to scale data
    and one approach for dealing with missing features. After that, we learned how
    to separate the full dataset into training, validation, and test subsets and how
    to apply *k*-fold cross validation, which is especially useful with small datasets.
    We ended the chapter with tips on how to simply examine the data to make sure
    it makes sense.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们描述了数据集的组成部分（类别、标签、特征、特征向量），并定义了一个良好的数据集，强调了确保数据集能很好地代表父分布的重要性。接着，我们介绍了基本的数据准备技术，包括如何对数据进行标准化以及处理缺失特征的一种方法。之后，我们学习了如何将完整的数据集划分为训练集、验证集和测试集，并且介绍了如何应用
    *k*-折交叉验证，这对于小型数据集特别有用。最后，我们以如何简单检查数据确保其合理性作为本章的结尾。
- en: In the next chapter, we’ll take what we have learned in this chapter and apply
    it directly to construct the datasets we will use throughout the remainder of
    this book.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将直接应用本章所学的内容，构建本书其余部分将使用的数据集。
- en: '[1.](ch04.xhtml#Rch04fn1) These are examples of classical machine learning
    models. We’ll learn more about them later in the book.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '[1.](ch04.xhtml#Rch04fn1) 这些是经典的机器学习模型示例。我们将在本书的后续章节中进一步学习它们。'
- en: '[2.](ch04.xhtml#Rch04fn2) Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin.
    “Why Should I Trust You?: Explaining the Predictions of Any Classifier.” *In Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, pp. 1135–1144\. ACM, 2016.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[2.](ch04.xhtml#Rch04fn2) Ribeiro, Marco Tulio, Sameer Singh, 和 Carlos Guestrin.
    “Why Should I Trust You?: 解释任何分类器的预测。” *《第22届ACM SIGKDD国际知识发现与数据挖掘大会论文集》*，第1135–1144页，ACM，2016年。'
