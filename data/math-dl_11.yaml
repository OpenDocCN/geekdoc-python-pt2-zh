- en: '**11'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**11'
- en: GRADIENT DESCENT**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/common.jpg)'
- en: 'In this final chapter, we’ll slow down a bit and consider gradient descent
    afresh. We’ll begin by reviewing the idea of gradient descent using illustrations,
    discussing what it is and how it works. Next, we’ll explore the meaning of *stochastic*
    in *stochastic gradient descent*. Gradient descent is a simple algorithm that
    invites tweaking, so after we explore stochastic gradient descent, we’ll consider
    a useful and commonly used tweak: momentum. We’ll conclude the chapter by discussing
    more advanced, adaptive gradient descent algorithms, specifically RMSprop, Adagrad,
    and Adam.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章的最后，我们将稍微放慢速度，重新审视梯度下降。我们将从通过插图回顾梯度下降的概念开始，讨论它是什么以及如何工作。接下来，我们将探讨*随机*在*随机梯度下降*中的含义。梯度下降是一个简单的算法，允许我们进行调整，因此在探讨随机梯度下降后，我们将考虑一个有用且常用的调整：动量。最后，我们将通过讨论更先进的自适应梯度下降算法来结束本章，具体包括RMSprop、Adagrad和Adam。
- en: This is a math book, but gradient descent is very much applied math, so we’ll
    learn by experimentation. The equations are straightforward, and the math we saw
    in previous chapters is relevant as background. Therefore, consider this chapter
    an opportunity to apply what we’ve learned so far.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一本数学书，但梯度下降非常应用数学，因此我们将通过实验来学习。方程式很简单，之前章节看到的数学内容作为背景是相关的。因此，可以将本章视为一个将我们迄今为止所学应用的机会。
- en: The Basic Idea
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本概念
- en: 'We’ve encountered gradient descent several times already. We know the form
    of the basic gradient descent update equations from [Equation 10.14](ch10.xhtml#ch10equ14):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到过几次梯度下降。我们知道基本的梯度下降更新方程的形式，来自[方程式 10.14](ch10.xhtml#ch10equ14)：
- en: '![Image](Images/11equ01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11equ01.jpg)'
- en: Here, **Δ*W*** and **Δ*b*** are errors based on the partial derivatives of the
    weights and biases, respectively; η (eta) is a step size or learning rate, a value
    we use to adjust how we move.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**Δ*W*** 和 **Δ*b*** 是基于权重和偏差的偏导数的误差；η（希腊字母eta）是步长或学习率，一个用来调整我们如何移动的值。
- en: '[Equation 11.1](ch11.xhtml#ch11equ01) isn’t specific to machine learning. We
    can use the same form to implement gradient descent on arbitrary functions. Let’s
    discuss gradient descent using 1D and 2D examples to lay a foundation for how
    it operates. We’ll use an unmodified form of gradient descent known as *vanilla
    gradient descent*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 11.1](ch11.xhtml#ch11equ01)并不限于机器学习。我们可以使用相同的形式来对任意函数实现梯度下降。让我们通过一维和二维的例子来讨论梯度下降，奠定它如何运作的基础。我们将使用一种未经修改的梯度下降形式，称为*原始梯度下降*。'
- en: Gradient Descent in One Dimension
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一维梯度下降
- en: 'Let’s begin with a scalar function of *x*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个标量函数*x*开始：
- en: '![Image](Images/11equ02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11equ02.jpg)'
- en: '[Equation 11.2](ch11.xhtml#ch11equ02) is a parabola facing upward. Therefore,
    it has a minimum. Let’s find the minimum analytically by setting the derivative
    to zero and solving for *x*:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 11.2](ch11.xhtml#ch11equ02)是一个向上的抛物线。因此，它有一个最小值。让我们通过将导数设为零并解出*x*来解析地找到最小值：'
- en: '![Image](Images/272equ01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/272equ01.jpg)'
- en: The minimum of the parabola is at *x* = 1\. Now, let’s instead use gradient
    descent to find the minimum of [Equation 11.2](ch11.xhtml#ch11equ02). How should
    we begin?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 抛物线的最小值在*x* = 1。现在，让我们改用梯度下降来找到[方程式 11.2](ch11.xhtml#ch11equ02)的最小值。我们应该如何开始？
- en: First, we need to write the proper update equation, the form of [Equation 11.1](ch11.xhtml#ch11equ01)
    that applies in this case. We need the gradient, which for a 1D function is simply
    the derivative, *f*′(*x*) = 12*x* − 12\. With the derivative, gradient descent
    becomes
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要写出适当的更新方程式，适用于此情况的[方程式 11.1](ch11.xhtml#ch11equ01)的形式。我们需要梯度，对于一维函数，梯度就是导数，*f*′(*x*)
    = 12*x* − 12。通过导数，梯度下降变为
- en: '![Image](Images/11equ03.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11equ03.jpg)'
- en: Notice that we subtract η (12*x* − 12). This is why the algorithm is called
    gradient *descent*. Recall that the gradient points in the direction of maximum
    change in the function’s value. We’re interested in minimizing the function, not
    maximizing it, so we move in the direction opposite to the gradient toward smaller
    function values; therefore, we subtract.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们减去了η (12*x* − 12)。这就是为什么这个算法被称为梯度*下降*。回想一下，梯度指向函数值变化最大的方向。我们关心的是最小化函数，而不是最大化它，因此我们朝着与梯度相反的方向移动，趋向较小的函数值；因此，我们要减去。
- en: '[Equation 11.3](ch11.xhtml#ch11equ03) is one gradient descent step. It moves
    from an initial position, *x*, to a new position based on the value of the slope
    at the current position. Again η, the learning rate, governs how far we move.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.3](ch11.xhtml#ch11equ03)是一个梯度下降步骤。它根据当前位置的斜率值，将位置从初始位置 *x* 移动到新位置。同样，η（学习率）决定了我们移动的距离。'
- en: Now that we have the equation, let’s implement gradient descent. We’ll plot
    [Equation 11.2](ch11.xhtml#ch11equ02), pick a starting position, say *x* = −0.9,
    and iterate [Equation 11.3](ch11.xhtml#ch11equ03), plotting the function value
    at each new position of *x*. If we do this, we should see a series of points on
    the function that move ever closer to the minimum position at *x* = 1\. Let’s
    write some code.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了方程，让我们实现梯度下降。我们将绘制 [方程 11.2](ch11.xhtml#ch11equ02)，选择一个起始位置，比如 *x* = −0.9，并迭代
    [方程 11.3](ch11.xhtml#ch11equ03)，在每个新位置 *x* 处绘制函数值。如果我们这样做，我们应该会看到一系列的点，这些点在函数上逐步靠近
    *x* = 1 的最小值位置。让我们写些代码吧。
- en: 'First, we implement [Equation 11.2](ch11.xhtml#ch11equ02) and its derivative:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们实现 [方程 11.2](ch11.xhtml#ch11equ02) 及其导数：
- en: 'def f(x):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f(x):'
- en: return 6*x**2 - 12*x + 3
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: return 6*x**2 - 12*x + 3
- en: 'def d(x):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'def d(x):'
- en: return 12*x - 12
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: return 12*x - 12
- en: 'Next, we plot the function, and then we iterate [Equation 11.3](ch11.xhtml#ch11equ03),
    plotting the new pair, (*x*, *f*(*x*)), each time:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制函数图像，然后迭代 [方程 11.3](ch11.xhtml#ch11equ03)，每次绘制新的点对（*x*，*f*（*x*））：
- en: import numpy as np
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pylab as plt
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pylab as plt
- en: ❶ x = np.linspace(-1,3,1000)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x = np.linspace(-1,3,1000)
- en: plt.plot(x,f(x))
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(x,f(x))
- en: ❷ x = -0.9
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = -0.9
- en: eta = 0.03
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: eta = 0.03
- en: '❸ for i in range(15):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ for i in range(15):'
- en: plt.plot(x, f(x), marker='o', color='r')
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(x, f(x), marker='o', color='r')
- en: ❹ x = x - eta * d(x)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ x = x - eta * d(x)
- en: Let’s walk through the code. After importing NumPy and Matplotlib, we plot [Equation
    11.2](ch11.xhtml#ch11equ02) ❶. Next, we set our initial *x* position ❷ and take
    15 gradient descent steps ❸. We plot before stepping, so we see the initial *x*
    but do not plot the last step, which is fine in this case.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过代码。在导入 NumPy 和 Matplotlib 后，我们绘制 [方程 11.2](ch11.xhtml#ch11equ02) ❶。接下来，我们设置初始
    *x* 位置 ❷ 并进行 15 步梯度下降 ❸。我们在步进之前绘制，所以我们可以看到初始的 *x*，但不绘制最后一步，这在这种情况下是可以的。
- en: The final line ❹ is key. It implements [Equation 11.3](ch11.xhtml#ch11equ03).
    We update the current *x* position by multiplying the derivative’s value at *x*
    by η = 0.03 as the step size. The code above is in the file *gd_1d.py*. If we
    run it, we get [Figure 11-1](ch11.xhtml#ch11fig01).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行 ❹ 很关键。它实现了 [方程 11.3](ch11.xhtml#ch11equ03)。我们通过将当前位置 *x* 处的导数值乘以 η = 0.03
    作为步长来更新当前的 *x* 位置。上面的代码位于文件 *gd_1d.py* 中。如果我们运行它，将得到 [图 11-1](ch11.xhtml#ch11fig01)。
- en: '![image](Images/11fig01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig01.jpg)'
- en: '*Figure 11-1: Gradient descent in one dimension with small steps (η = 0.03)*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-1：带有小步长的单维梯度下降（η = 0.03）*'
- en: Our initial position, which we can think of as an initial guess at the location
    of the minimum, is *x* = −0.9\. Clearly, this isn’t the minimum. As we take gradient
    descent steps, we move successively closer to the minimum, as the sequence of
    circles moving toward it shows.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始位置可以看作是对最小值位置的初步猜测，*x* = −0.9。显然，这不是最小值。当我们进行梯度下降步伐时，我们会逐步接近最小值，正如一系列向最小值移动的圆圈所示。
- en: 'Notice two things here. First, we do get closer and closer to the minimum.
    After 14 steps, we are, for all intents and purposes, at the minimum: *x* = 0.997648\.
    Second, each gradient descent step leads to smaller and smaller changes in *x*.
    The learning rate is constant at *η* = 0.03, so the source of the smaller updates
    to *x* must be smaller and smaller values of the derivative at each *x* position.
    This makes sense if we think about it. As we approach the minimum position, the
    derivative gets smaller and smaller, until it reaches zero at the minimum, so
    the update using the derivative gets successively smaller as well.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里注意两点。首先，我们确实越来越接近最小值。经过 14 步后，我们基本上已经到达最小值：*x* = 0.997648。其次，每一步梯度下降都会导致
    *x* 的变化越来越小。学习率保持为 *η* = 0.03，因此 *x* 的更小更新源于每个 *x* 位置处的导数值逐渐减小。如果我们仔细想想，这是有道理的。当我们接近最小位置时，导数会变得越来越小，直到在最小值处为零，因此使用导数的更新也会逐步变小。
- en: We selected the step size for [Figure 11-1](ch11.xhtml#ch11fig01) to move smoothly
    toward the minimum of the parabola. What if we change the step size? Further along
    in *gd_1d.py*, the code repeats the steps above, starting at *x* = 0.75 and setting
    *η* = 0.15 to take steps that are five times larger than those plotted in [Figure
    11-1](ch11.xhtml#ch11fig01). The result is [Figure 11-2](ch11.xhtml#ch11fig02).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为[图 11-1](ch11.xhtml#ch11fig01)选择的步长使得梯度下降平滑地朝着抛物线的最小值移动。如果我们改变步长会怎样呢？在*gd_1d.py*中，代码重复了上述步骤，从*x*
    = 0.75开始，并设置*η* = 0.15，步长是[图 11-1](ch11.xhtml#ch11fig01)中绘制步长的五倍。结果是[图 11-2](ch11.xhtml#ch11fig02)。
- en: '![image](Images/11fig02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig02.jpg)'
- en: '*Figure 11-2: Gradient descent in one dimension with large steps (η = 0.15)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-2：带有大步长（η = 0.15）的单维梯度下降*'
- en: In this case, the steps overshoot the minimum. The new *x* positions oscillate,
    bouncing back and forth over the true minimum position. The dashed lines connect
    successive *x* positions. The overall search still approaches the minimum but
    takes longer to reach it, as the large step size makes each update to *x* tend
    to move past the minimum.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，步长超过了最小值。新的*x*位置振荡，在最小值位置前后来回跳动。虚线连接了连续的*x*位置。总体搜索依然接近最小值，但因为步长较大，每次更新*x*时都倾向于越过最小值，因此需要更多的时间才能达到最小值。
- en: Small gradient descent steps move short distances along the function, whereas
    large steps move large distances. If the learning rate is too small, many gradient
    descent steps are necessary. If the learning rate is too large, the search overshoots
    and oscillates around the minimum position. The proper learning rate is not immediately
    obvious, so intuition and experience come into play when selecting it. Additionally,
    these examples fixed *η*. There’s no reason why *η* has to be a constant. In many
    deep learning applications, the learning rate is not constant but evolves as training
    progresses, effectively making *η* a function of the number of gradient descent
    steps taken.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 小的梯度下降步长沿着函数移动的距离较短，而大的步长则移动较大的距离。如果学习率太小，需要很多梯度下降步骤。如果学习率过大，搜索就会超越最小值并在最小位置附近振荡。合适的学习率并不立即显现，因此在选择时需要凭直觉和经验。此外，这些示例中固定了*η*。*η*不必是一个常数，许多深度学习应用中，学习率并不是常数，而是随着训练的进行而逐步变化，实际上使*η*成为梯度下降步骤数的函数。
- en: Gradient Descent in Two Dimensions
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二维梯度下降
- en: Gradient descent in one dimension is straightforward enough. Let’s move to two
    dimensions to increase our intuition about the algorithm. The code referenced
    below is in the file *gd_2d.py*. We’ll first consider the case where the function
    has a single minimum, then look at cases with multiple minima.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一维梯度下降足够简单。现在让我们进入二维梯度下降，以便更好地理解这个算法。下面引用的代码位于文件*gd_2d.py*中。我们将首先考虑函数有一个最小值的情况，然后再看有多个最小值的情况。
- en: Gradient Descent with a Single Minimum
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有单一最小值的梯度下降法
- en: To work in two dimensions, we need a scalar function of a vector, *f*(***x***)
    = *f*(*x*, *y*), where, to make it easier to follow, we separate the vector into
    its components, ***x*** = (*x*, *y*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要在二维中工作，我们需要一个向量的标量函数，*f*(***x***) = *f*(*x*, *y*)，为了便于理解，我们将向量分解为其分量，***x***
    = (*x*, *y*)。
- en: The first function we’ll work with is
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的第一个函数是
- en: '*f*(*x*, *y*) = 6*x*² + 9*y*² − 12*x* − 14*y* + 3'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*, *y*) = 6*x*² + 9*y*² − 12*x* − 14*y* + 3'
- en: 'To implement gradient descent, we need the partial derivatives as well:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现梯度下降，我们还需要偏导数：
- en: '![Image](Images/276equ01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/276equ01.jpg)'
- en: Our update equations become
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的更新方程变为
- en: '![Image](Images/276equ02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/276equ02.jpg)'
- en: 'In code, we define the function and partial derivatives:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们定义了函数和偏导数：
- en: 'def f(x,y):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f(x,y):'
- en: return 6*x**2 + 9*y**2 - 12*x - 14*y + 3
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: return 6*x**2 + 9*y**2 - 12*x - 14*y + 3
- en: 'def dx(x):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'def dx(x):'
- en: return 12*x - 12
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: return 12*x - 12
- en: 'def dy(y):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'def dy(y):'
- en: return 18*y - 14
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: return 18*y - 14
- en: Since the partial derivatives are independent of the other variable, we get
    away with passing only *x* or *y*. We’ll see an example later in this section
    where that’s not the case.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于偏导数与另一个变量无关，我们可以只传递*x*或*y*。稍后在本节中，我们会看到一个不同的例子，其中并非如此。
- en: 'Gradient descent follows the same pattern as before: select an initial position,
    this time a vector, iterate for some number of steps, and plot the path. The function
    is 2D, so we first plot it using contours, as shown next.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降遵循与之前相同的模式：选择一个初始位置，这次是一个向量，进行若干步的迭代并绘制路径。由于函数是二维的，我们首先使用等高线图来绘制它，如下所示。
- en: N = 100
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: N = 100
- en: x,y = np.meshgrid(np.linspace(-1,3,N), np.linspace(-1,3,N))
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: x,y = np.meshgrid(np.linspace(-1,3,N), np.linspace(-1,3,N))
- en: z = f(x,y)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: z = f(x,y)
- en: plt.contourf(x,y,z,10, cmap="Greys")
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: plt.contourf(x,y,z,10, cmap="Greys")
- en: plt.contour(x,y,z,10, colors='k', linewidths=1)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: plt.contour(x,y,z,10, colors='k', linewidths=1)
- en: plt.plot([0,0],[-1,3],color='k',linewidth=1)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot([0,0],[-1,3],color='k',linewidth=1)
- en: plt.plot([-1,3],[0,0],color='k',linewidth=1)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot([-1,3],[0,0],color='k',linewidth=1)
- en: plt.plot(1,0.7777778,color='k',marker='+')
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(1,0.7777778,color='k',marker='+')
- en: This code requires some explanation. To plot contours, we need a representation
    of the function over a grid of (*x*, *y*) pairs. To generate the grid, we use
    NumPy, specifically np.meshgrid. The arguments to np.meshgrid are the *x* and
    *y* points, here provided by np.linspace, which itself generates a vector from
    −1 to 3 of *N* = 100 evenly spaced values. The np.meshgrid function returns two
    100 × 100 matrices. The first contains the *x* values over the given range, and
    the second contains the *y* values. All possible (*x*, *y*) pairs are represented
    in the return value to form a grid of points covering the region of −1 . . . 3
    in both *x* and *y*. Passing these points to the function then returns z, a 100
    × 100 matrix of the function value at each (*x*, *y*) pair.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要一些解释。为了绘制等高线，我们需要在 (*x*, *y*) 点对的网格上表示该函数。为了生成这个网格，我们使用 NumPy，特别是 np.meshgrid。np.meshgrid
    的参数是 *x* 和 *y* 点，这里由 np.linspace 提供，后者本身生成从 −1 到 3 的 *N* = 100 个均匀间隔的数值。np.meshgrid
    函数返回两个 100 × 100 的矩阵。第一个矩阵包含给定范围内的 *x* 值，第二个矩阵包含 *y* 值。所有可能的 (*x*, *y*) 点对都在返回值中表示，从而形成一个覆盖
    −1 到 3 范围的网格。将这些点传递给函数后，会返回 z，这是一个 100 × 100 的矩阵，包含每个 (*x*, *y*) 点对的函数值。
- en: We could plot the function in 3D, but that’s difficult to see and unnecessary
    in this case. Instead, we’ll use the function values in *x*, *y*, and *z* to generate
    contour plots. Contour plots show 3D information as a series of lines of equal
    *z* value. Think of lines around a hill on a topographic map, where each line
    is at the same altitude. As the hill gets higher, the lines enclose successively
    smaller regions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 3D 中绘制该函数，但这样不容易观察，且在这种情况下并不必要。相反，我们将使用 *x*、*y* 和 *z* 的函数值来生成等高线图。等高线图通过一系列具有相同
    *z* 值的线条展示 3D 信息。可以将其想象为地形图上环绕山丘的等高线，其中每条线代表相同的海拔高度。随着山丘的升高，线条会围绕越来越小的区域。
- en: Contour plots come in two varieties, as either lines of equal function value
    or shading over ranges of the function. We’ll plot both varieties using a grayscale
    map. That’s the net result of calling Matplotlib’s plt.contourf and plt.contour
    functions. The remaining plt.plot calls show the axes and mark the function minimum
    with a plus sign. The contour plots are such that lighter shades imply lower function
    values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 等高线图有两种形式，一种是相等函数值的线条，另一种是函数值范围的阴影区域。我们将使用灰度图来绘制这两种形式。这是调用 Matplotlib 的 plt.contourf
    和 plt.contour 函数的最终结果。其余的 plt.plot 调用展示了坐标轴，并用加号标记了函数的最小值。等高线图的阴影较浅意味着函数值较低。
- en: We’re now ready to plot the sequence of gradient descent steps. We’ll plot each
    position in the sequence and connect them with a dashed line to make the path
    clear (see [Listing 11-1](ch11.xhtml#ch11ex01)). In code, that’s
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备绘制梯度下降步骤的序列。我们将绘制序列中的每个位置，并用虚线将它们连接起来，以便清晰地显示路径（见 [Listing 11-1](ch11.xhtml#ch11ex01)）。代码如下：
- en: x = xold = -0.5
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: x = xold = -0.5
- en: y = yold = 2.9
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: y = yold = 2.9
- en: 'for i in range(12):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(12):'
- en: plt.plot([xold,x],[yold,y], marker='o', linestyle='dotted', color='k')
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot([xold,x],[yold,y], marker='o', linestyle='dotted', color='k')
- en: xold = x
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: xold = x
- en: yold = y
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: yold = y
- en: x = x - 0.02 * dx(x)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: x = x - 0.02 * dx(x)
- en: y = y - 0.02 * dy(y)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: y = y - 0.02 * dy(y)
- en: '*Listing 11-1: Gradient descent in two dimensions*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 11-1: 二维梯度下降*'
- en: We begin at (*x*, *y*) = (−0.5, 2.9) and take 12 gradient descent steps. To
    connect the last position to the new position using a dashed line, we track both
    the current position in *x* and *y* and the previous position, (*x*[old], *y*[old]).
    The gradient descent step updates both *x* and *y* using *η* = 0.02 and calling
    the respective partial derivative functions, dx and dy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 (*x*, *y*) = (−0.5, 2.9) 开始，并进行 12 步梯度下降。为了用虚线连接最后的位置和新位置，我们追踪当前的 *x* 和
    *y* 位置以及之前的位置 (*x*[old], *y*[old])。梯度下降步骤使用 *η* = 0.02 更新 *x* 和 *y*，并调用相应的偏导数函数
    dx 和 dy。
- en: '[Figure 11-3](ch11.xhtml#ch11fig03) shows the gradient descent path that [Listing
    11-1](ch11.xhtml#ch11ex01) follows (circles) along with two other paths starting
    at (1.5, −0.8) (squares) and (2.7, 2.3) (triangles).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 11-3](ch11.xhtml#ch11fig03) 展示了梯度下降路径，该路径沿 [Listing 11-1](ch11.xhtml#ch11ex01)（圆圈）与从
    (1.5, −0.8)（方块）和 (2.7, 2.3)（三角形）出发的另外两条路径。'
- en: '![image](Images/11fig03.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig03.jpg)'
- en: '*Figure 11-3: Gradient descent in two dimensions for small steps*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-3：对于小步长的二维梯度下降法*'
- en: All three gradient descent paths converge toward the minimum of the function.
    This isn’t surprising, as the function has only one minimum. If the function has
    a single minimum, then gradient descent will eventually find it. If the step size
    is too small, many steps might be necessary, but they will ultimately converge
    on the minimum. If the step size is too large, gradient descent may oscillate
    around the minimum but continually step over it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个梯度下降路径都收敛到该函数的最小值。 这并不令人惊讶，因为该函数只有一个最小值。如果函数只有一个最小值，那么梯度下降最终会找到它。如果步长太小，可能需要很多步，但它们最终会收敛到最小值。如果步长太大，梯度下降可能会围绕最小值振荡，但会不断越过最小值。
- en: 'Let’s change our function a bit to stretch it in the *x* direction relative
    to the *y* direction:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微改变一下函数，将其在 *x* 方向上相对于 *y* 方向拉伸：
- en: '*f*(*x*, *y*) = 6*x*² + 40*y*² − 12*x* − 30*y* + 3'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*, *y*) = 6*x*² + 40*y*² − 12*x* − 30*y* + 3'
- en: This function has partials ∂*f*/∂*x* = 12*x* − 12 and ∂*f*/∂*y* = 80*y* − 30.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的偏导数为 ∂*f*/∂*x* = 12*x* − 12 和 ∂*f*/∂*y* = 80*y* − 30。
- en: Additionally, let’s pick two starting locations, (−0.5, 2.3) and (2.3, 2.3),
    and generate a sequence of gradient descent steps with *η* = 0.02 and *η* = 0.01,
    respectively. [Figure 11-4](ch11.xhtml#ch11fig04) shows the resulting paths.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们选择两个起始位置（−0.5，2.3）和（2.3，2.3），并分别生成步长为 *η* = 0.02 和 *η* = 0.01 的梯度下降步骤序列。[图
    11-4](ch11.xhtml#ch11fig04) 显示了结果路径。
- en: '![image](Images/11fig04.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig04.jpg)'
- en: '*Figure 11-4: Gradient descent in 2D with larger steps and a slightly different
    function*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-4：具有较大步长和略有不同的函数的二维梯度下降法*'
- en: Consider the *η* = 0.02 (circle) path first. The new function is like a canyon,
    narrow in *y* but long in *x*. The larger step size oscillates up and down in
    *y* as it moves toward the minimum in *x*. Bouncing off the canyon walls aside,
    we still find the minimum.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑 *η* = 0.02（圆形）路径。新的函数像一个峡谷，*y* 方向较窄，而 *x* 方向较长。较大的步长在 *y* 方向上上下振荡，同时向 *x*
    方向的最小值移动。跳跃过峡谷壁 aside，我们仍然能够找到最小值。
- en: Now, take a look at the *η* = 0.01 (square) path. It quickly falls into the
    canyon and then moves slowly over the flat region along the canyon floor toward
    the minimum position. The component of the vector gradient (the *x* and *y* partial
    derivative values) along the *x* direction is small in the canyon, so motion along
    *x* is proportionately slow. There is no motion in the *y* direction—the canyon
    is steep, and the relatively small learning rate has already located the canyon
    floor, where the gradient is primarily along *x*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看 *η* = 0.01（方形）路径。它快速进入峡谷，然后沿着峡谷底部的平坦区域缓慢地向最小位置移动。沿 *x* 方向的梯度向量分量（*x* 和
    *y* 的偏导数值）在峡谷中很小，因此在 *x* 方向的运动相对较慢。沿 *y* 方向没有运动——峡谷很陡峭，而且相对较小的学习率已经定位到峡谷底部，在那里梯度主要沿
    *x* 方向。
- en: 'What’s the lesson here? Again, the step size matters. However, the shape of
    the function matters even more. The minimum of the function lies at the bottom
    of a long, narrow canyon. The gradient along the canyon is tiny; the canyon floor
    is flat in the *x* direction, so motion is slow because it depends on the gradient
    value. We frequently encounter this effect in deep learning: if the gradient is
    small, learning is slow. This is why the rectified linear unit has come to dominate
    deep learning; the gradient is a constant one for positive inputs. For a sigmoid
    or hyperbolic tangent, the gradient approaches zero when inputs are far from zero.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的教训是什么？再次强调，步长很重要。然而，函数的形状更为重要。函数的最小值位于一个长而窄的峡谷底部。峡谷中的梯度很小；峡谷底部在 *x* 方向是平坦的，所以运动很慢，因为它依赖于梯度值。我们在深度学习中经常遇到这种现象：如果梯度很小，学习速度会很慢。这就是为什么修正线性单元（ReLU）在深度学习中占据主导地位的原因；对于正输入，梯度是常数。而对于
    sigmoid 或双曲正切函数，当输入远离零时，梯度接近于零。
- en: Gradient Descent with Multiple Minima
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 具有多个最小值的梯度下降法
- en: 'The functions we’ve examined so far have a single minimum value. What if that
    isn’t the case? Let’s see what happens to gradient descent when the function has
    more than one minimum. Consider this function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考察的函数都有一个最小值。如果情况不是这样呢？让我们看看当函数有多个最小值时梯度下降会发生什么。考虑这个函数：
- en: '![Image](Images/11equ04.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ04.jpg)'
- en: '[Equation 11.4](ch11.xhtml#ch11equ04) is the sum of two inverted Gaussians,
    one with a minimum value of −2 at (−1, 1) and the other with a minimum of −1 at
    (1, −1). If gradient descent is to find the global minimum, it should find it
    at (−1, 1). The code for this example is in *gd_multiple.py*.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式11.4](ch11.xhtml#ch11equ04)是两个倒转高斯函数的和，一个在（-1, 1）处有最小值-2，另一个在（1, -1）处有最小值-1。如果梯度下降要找到全局最小值，它应该在（-1,
    1）找到它。此示例的代码位于 *gd_multiple.py* 中。'
- en: The partial derivatives are
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数为
- en: '![Image](Images/280equ01.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/280equ01.jpg)'
- en: 'which translates into the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这转换成以下代码：
- en: 'def f(x,y):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f(x,y):'
- en: return -2*np.exp(-0.5*((x+1)**2+(y-1)**2)) + \
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: return -2*np.exp(-0.5*((x+1)**2+(y-1)**2)) + \
- en: -np.exp(-0.5*((x-1)**2+(y+1)**2))
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: -np.exp(-0.5*((x-1)**2+(y+1)**2))
- en: 'def dx(x,y):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'def dx(x,y):'
- en: return 2*(x+1)*np.exp(-0.5*((x+1)**2+(y-1)**2)) + \
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: return 2*(x+1)*np.exp(-0.5*((x+1)**2+(y-1)**2)) + \
- en: (x-1)*np.exp(-0.5*((x-1)**2+(y+1)**2))
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (x-1)*np.exp(-0.5*((x-1)**2+(y+1)**2))
- en: 'def dy(x,y):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'def dy(x,y):'
- en: return (y+1)*np.exp(-0.5*((x-1)**2+(y+1)**2)) + \
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: return (y+1)*np.exp(-0.5*((x-1)**2+(y+1)**2)) + \
- en: 2*(y-1)*np.exp(-0.5*((x+1)**2+(y-1)**2))
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 2*(y-1)*np.exp(-0.5*((x+1)**2+(y-1)**2))
- en: Notice, in this case, the partial derivatives do depend on both *x* and *y*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，偏导数确实依赖于 *x* 和 *y*。
- en: The code for the gradient descent portion of *gd_multiple.py* is as before.
    Let’s run the cases in [Table 11-1](ch11.xhtml#ch11tab01).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降部分的代码与之前的 *gd_multiple.py* 相同。让我们运行[表11-1](ch11.xhtml#ch11tab01)中的案例。
- en: '**Table 11-1:** Different Starting Positions and Number of Gradient Descent
    Steps Taken'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**表11-1：** 不同起始位置和梯度下降步骤数量'
- en: '| **Starting point** | **Steps** | **Symbol** |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **起始点** | **步骤数** | **符号** |'
- en: '| --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| (–1.5,1.2) | 9 | circle |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (-1.5,1.2) | 9 | 圆形 |'
- en: '| (1.5,–1.8) | 9 | square |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (1.5,–1.8) | 9 | 方块 |'
- en: '| (0,0) | 20 | plus |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (0,0) | 20 | 加号 |'
- en: '| (0.7,–0.2) | 20 | triangle |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (0.7,–0.2) | 20 | 三角形 |'
- en: '| (1.5,1.5) | 30 | asterisk |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| (1.5,1.5) | 30 | 星号 |'
- en: The Symbol column refers to the plot symbol used in [Figure 11-5](ch11.xhtml#ch11fig05).
    For all cases, *η* = 0.4.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 符号列指的是[图11-5](ch11.xhtml#ch11fig05)中使用的绘图符号。对于所有情况，*η* = 0.4。
- en: '![image](Images/11fig05.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig05.jpg)'
- en: '*Figure 11-5: Gradient descent for a function with two minima*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11-5：具有两个最小值的函数的梯度下降*'
- en: The gradient descent paths indicated in [Figure 11-5](ch11.xhtml#ch11fig05)
    make sense. In three of the five cases, the path does move into the well that
    the deeper of the two minima defines—a successful search. However, for the triangle
    and the square, gradient descent fell into the wrong minimum. Clearly, how successful
    gradient descent is, in this case, depends on where we start the process. Once
    the path moves downhill to a deeper position, gradient descent has no way to escape
    upward to find a potentially better minimum.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图11-5](ch11.xhtml#ch11fig05)中指示的梯度下降路径是合理的。在五种情况中，有三种路径确实进入了由较深的最小值定义的深坑——这是一次成功的搜索。然而，对于三角形和方块，梯度下降进入了错误的最小值。显然，在这种情况下，梯度下降的成功与我们开始过程的位置有关。一旦路径向下滑动到一个更深的位置，梯度下降就无法向上逃逸去寻找一个潜在的更好的最小值。
- en: Current thinking is that the loss landscape for a deep learning model contains
    many minima. It’s also currently believed that in most cases, the minima are pretty
    similar, which partially explains the success of deep learning models—to train
    them, you don’t need to find the one, magic, global minimum of the loss, only
    one of the (probably) many that are (probably) about as good as any of the others.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的观点是，深度学习模型的损失景观包含许多最小值。现在也认为，在大多数情况下，这些最小值非常相似，这部分解释了深度学习模型的成功——训练它们时，你不需要找到那个唯一的魔法全局最小值，只需要找到（可能）许多最小值中的一个，它们（可能）和其他任何一个一样好。
- en: 'I selected the initial positions used for the examples in this section intentionally
    based on knowledge of the function’s form. For a deep learning model, picking
    the starting point means random initialization of the weights and biases. In general,
    we don’t know the form of the loss function, so initialization is a shot in the
    dark. Most of the time, or at least much of the time, gradient descent produces
    a well-performing model. Sometimes, however, it doesn’t; it fails miserably. In
    those cases, it’s possible the initial position was like the square in [Figure
    11-5](ch11.xhtml#ch11fig05): it fell into an inferior local minimum because it
    started in a bad place.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我有意选择了本节示例中使用的初始位置，基于对函数形式的理解。对于深度学习模型，选择起始点意味着对权重和偏差进行随机初始化。通常，我们不知道损失函数的具体形式，所以初始化就是一次盲目的尝试。大多数情况下，或者至少大部分情况下，梯度下降会产生一个表现良好的模型。然而，有时它会失败，表现得非常糟糕。在这些情况下，可能是因为初始位置就像[图11-5](ch11.xhtml#ch11fig05)中的方形位置一样：它陷入了一个劣质的局部最小值，因为一开始就处于一个不好的位置。
- en: Now that we have a handle on gradient descent, what it is, and how it works,
    let’s investigate how we can apply it in deep learning.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了梯度下降的基本概念、原理和工作方式，接下来让我们探讨如何在深度学习中应用它。
- en: Stochastic Gradient Descent
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Training a neural network is primarily the act of minimizing the loss function
    while preserving generalizability via various forms of regularization. In [Chapter
    10](ch10.xhtml#ch10), we wrote the loss as *L*(**θ**; ***x***, *y*) for a vector
    of the weights and biases, **θ** (theta), and training instances (***x***, *y*),
    where ***x*** is the input vectors and y is the known labels. Note how here, ***x***
    is a stand-in for *all* training data, not just a single sample.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的主要任务是最小化损失函数，同时通过各种正则化形式保持泛化能力。在[第10章](ch10.xhtml#ch10)中，我们将损失写为 *L*(**θ**;
    ***x***, *y*)，其中 **θ**（theta）是权重和偏差的向量，***x*** 和 *y* 是训练实例，***x*** 是输入向量，y 是已知标签。注意，这里
    ***x*** 是代表*所有*训练数据的符号，而不仅仅是单个样本。
- en: 'Gradient descent needs ∂*L*/∂**θ**, which we get via backpropagation. The expression
    ∂*L*/∂**θ** is a concise way of referring to all the individual weight and bias
    error terms backpropagation gives us. We get ∂*L*/∂**θ** by averaging the error
    over the training data. This begs the question: Do we average over all of the
    training data or only some of the training data?'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降需要 ∂*L*/∂**θ**，我们通过反向传播获得这个结果。表达式 ∂*L*/∂**θ** 是指代反向传播给我们提供的所有单独的权重和偏差误差项的简洁方式。我们通过对训练数据进行误差平均化来得到
    ∂*L*/∂**θ**。这就引出了一个问题：我们是对所有训练数据进行平均，还是只对部分训练数据进行平均？
- en: Passing all the training data through the model before taking a gradient descent
    step is called batch training. At first blush, batch training seems sensible.
    After all, if our training set is a good sample from the parent distribution that
    generates the sort of data our model intends to work with, then why not use all
    of that sample to do gradient descent?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次梯度下降步骤之前将所有训练数据通过模型的过程称为批量训练。乍一看，批量训练似乎很合理。毕竟，如果我们的训练集是从生成模型数据的母体分布中抽取的一个良好样本，那么为什么不利用这个完整的样本来进行梯度下降呢？
- en: When datasets were small, batch training was the natural thing to do. However,
    models got bigger, as did datasets, and suddenly the computational burden of passing
    *all* the training data through the model for each gradient descent step became
    too much. This chapter’s examples already hint that many gradient descent steps
    might be necessary to find a good minimum position, especially for tiny learning
    rates.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集较小的时候，批量训练是自然而然的选择。然而，随着模型和数据集的增大，逐渐地将*所有*训练数据通过模型进行每一步梯度下降的计算负担变得过重。本章的示例已经暗示了，要找到一个良好的最小值位置，可能需要多次梯度下降步骤，尤其是在学习率很小的情况下。
- en: Therefore, practitioners began to use subsets of the training data for each
    gradient descent step—the *minibatch*. Minibatch training was probably initially
    viewed as a compromise, as the gradient calculated over the minibatch was “wrong”
    because it wasn’t based on the performance of the full training set.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实践者开始在每次梯度下降步骤中使用训练数据的子集——*小批量*。小批量训练最初可能被视为一种妥协，因为它计算的梯度是“错误的”，因为它不是基于完整训练集的表现。
- en: Of course, the difference between *batch* and *minibatch* is just an agreed-upon
    fiction. In truth, it’s a continuum from a minibatch of one to a minibatch of
    all available samples. With that in mind, all the gradients computed during network
    training are “wrong,” or at least incomplete, as they are based on incomplete
    knowledge of the data generator and the full set of data it could generate.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，*批量*（batch）和*mini-batch* 之间的区别只是一个约定的虚构。实际上，它是从一个 mini-batch 到包含所有可用样本的 mini-batch
    之间的一个连续体。考虑到这一点，在网络训练过程中计算的所有梯度都是“错误的”，或者至少是不完整的，因为它们是基于对数据生成器和它能够生成的完整数据集的部分了解而得出的。
- en: Rather than a concession, then, minibatch training is reasonable. The gradient
    over a small minibatch is noisy compared to that computed over a larger minibatch,
    in the sense that the small minibatch gradient is a coarser estimate of the “real”
    gradient. When things are noisy or random, the word *stochastic* tends to show
    up, as it does here. Gradient descent with minibatches is *stochastic gradient
    descent (SGD)*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，mini-batch 训练是合理的，而非一种让步。与较大 mini-batch 计算的梯度相比，小 mini-batch 上的梯度更加嘈杂，因为小
    mini-batch 的梯度是“真实”梯度的粗略估计。当事物嘈杂或随机时，*随机*一词通常会出现，就像这里一样。使用 mini-batch 进行梯度下降就是*随机梯度下降（SGD）*。
- en: In practice, gradient descent using smaller minibatches often leads to models
    that perform better than those trained with larger minibatches. The rationale
    generally given is that the noisy gradient of the smaller minibatch helps gradient
    descent avoid falling into poor local minima of the loss landscape. We saw this
    effect in [Figure 11-5](ch11.xhtml#ch11fig05), where the triangle and the square
    both fell into the wrong minimum.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用较小 mini-batch 的梯度下降通常会得到比使用较大 mini-batch 训练的模型更好的效果。通常给出的理由是，小 mini-batch
    的嘈杂梯度帮助梯度下降避免陷入损失函数的较差局部最小值。我们在[图 11-5](ch11.xhtml#ch11fig05)中看到了这个效果，那里三角形和正方形都陷入了错误的最小值。
- en: Again, we find ourselves strangely fortunate. Before, we were fortunate because
    first-order gradient descent succeeded in training models that shouldn’t train
    due to nonlinear loss landscapes, and now we get a boost by intentionally using
    small amounts of data to estimate gradients, thereby skipping a computational
    burden likely to make the entire enterprise of deep learning too cumbersome to
    implement in many cases.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们发现自己在某种程度上是幸运的。之前我们之所以幸运，是因为一阶梯度下降成功地训练了那些由于非线性损失函数的形状而无法训练的模型，而现在，通过故意使用少量数据来估计梯度，我们得到了一个额外的助力，从而避免了可能会使深度学习在许多情况下变得过于繁琐的计算负担。
- en: How large should our minibatch be? Minibatch size is a *hyperparameter*, something
    we need to select to train the model, but is not part of the model itself. The
    proper minibatch size is dataset-dependent. For example, in the extreme, we could
    take a gradient descent step for each sample, which sometimes works well. This
    case is often referred to as *online learning*. However, especially if we use
    layers like batch normalization, we need a minibatch large enough to make the
    calculated means and standard deviations reasonable estimates. Again, as with
    most everything else in deep learning at present, it’s empirical, and you need
    to both have intuition and try many variations to optimize the training of the
    model. This is why people work on *AutoML* systems, systems that seek to do all
    the hyperparameter tuning for you.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 mini-batch 应该有多大？mini-batch 大小是一个*超参数*，是我们在训练模型时需要选择的内容，但它不属于模型本身。合适的 mini-batch
    大小取决于数据集。例如，在极端情况下，我们可以对每个样本进行一次梯度下降，这有时效果很好。这个情况通常被称为*在线学习*。然而，特别是当我们使用诸如批量归一化（batch
    normalization）这样的层时，我们需要一个足够大的 mini-batch 来使计算得到的均值和标准差成为合理的估计。再次强调，像当前深度学习中的大多数问题一样，这是经验性的，你需要既有直觉，也要尝试多种变化来优化模型的训练。这也是人们研究*AutoML*系统的原因，AutoML
    系统旨在为你完成所有超参数调整的工作。
- en: 'Another good question: What should be in the minibatch? That is, what small
    subset of the full dataset should we use? Typically, the order of the samples
    in the training set is randomized, and minibatches are pulled from the set as
    successive chunks of samples until all samples have been used. Using all the samples
    in the dataset defines one epoch, so the number of samples in the training set
    divided by the minibatch size determines the number of minibatches per epoch.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得思考的问题是：mini-batch 中应该包含什么？也就是说，我们应该使用数据集中的哪个小子集？通常，训练集中的样本顺序是随机的，mini-batches
    是从训练集中依次抽取样本，直到所有样本都被使用。使用数据集中的所有样本定义一个训练周期（epoch），因此训练集中的样本数除以 mini-batch 大小决定了每个训练周期中的
    mini-batch 数量。
- en: Alternatively, as we did for *NN.py*, a minibatch might genuinely be a random
    sampling from the available data. It’s possible that a particular training sample
    is never used while another is used many times, but on the whole, the majority
    of the dataset is used during training.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，正如我们对*NN.py*所做的那样，一个小批次可能真的是从可用数据中随机采样的。可能某个特定的训练样本从未被使用，而另一个被多次使用，但总体来说，训练过程中大部分数据集都会被使用。
- en: Some toolkits train for a specified number of minibatches. Both *NN.py* and
    Caffe operate this way. Other toolkits, like Keras and sklearn, use epochs. Gradient
    descent steps happen after a minibatch is processed. Larger minibatches result
    in fewer gradient descent steps per epoch. To compensate, practitioners using
    toolkits that use epochs need to ensure that the number of gradient descent steps
    increases as minibatch size increases—larger minibatches require more epochs to
    train well.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工具包会训练指定数量的小批次。*NN.py*和Caffe都以这种方式工作。其他工具包，如Keras和sklearn，使用训练周期（epochs）。梯度下降步骤在每个小批次处理后进行。较大的小批次会导致每个训练周期中较少的梯度下降步骤。为了补偿，使用训练周期的工具包的从业者需要确保随着小批次大小的增加，梯度下降步骤的数量也增加——较大的小批次需要更多的训练周期才能很好地训练。
- en: 'To recap, deep learning does not use full batch training for at least the following
    reasons:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，深度学习不使用全批次训练，至少有以下几个原因：
- en: The computational burden is too great to pass the entire training set through
    the model for each gradient descent step.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将整个训练集传递通过模型进行每一次梯度下降步骤的计算负担过于沉重。
- en: The gradient computed from the average loss over a minibatch is a noisy but
    reasonable estimate of the true, and ultimately unknowable, gradient.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个小批次的平均损失计算出的梯度是一个嘈杂但合理的估计值，代表了真实的梯度，而真实的梯度是最终不可知的。
- en: The noisy gradient points in a slightly wrong direction in the loss landscape,
    thereby possibly avoiding bad minima.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嘈杂的梯度在损失地形中指向一个稍微错误的方向，从而可能避开不良的极小值。
- en: Minibatch training simply works better in practice for many datasets.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小批次训练在实际中对于许多数据集效果更好。
- en: 'Reason #4 should not be underestimated: many practices in deep learning are
    employed initially because they simply work better. Only later are they justified
    by theory, if at all.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 不应低估理由#4：深度学习中的许多实践最初被采用，是因为它们更有效。只有后来，理论才会对这些实践提供解释，甚至有时没有解释。
- en: As we already implemented SGD in [Chapter 10](ch10.xhtml#ch10) (see *NN.py*),
    we won’t reimplement it here, but in the next section, we’ll add momentum to see
    how that affects neural network training.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第10章](ch10.xhtml#ch10)中实现了SGD（参见*NN.py*），因此这里不再重新实现，但在下一节中，我们将加入动量，看看它如何影响神经网络的训练。
- en: Momentum
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: Vanilla gradient descent relies solely on the value of the partial derivative
    multiplied by the learning rate. If the loss landscape has many local minima,
    especially if they’re steep, vanilla gradient descent might fall into one of the
    minima and be unable to recover. To compensate, we can modify vanilla gradient
    descent to include a *momentum* term, a term that uses a fraction of the previous
    step’s update. Including this momentum in gradient descent adds inertia to the
    algorithm’s motion through the loss landscape, thereby potentially allowing gradient
    descent to move past bad local minima.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 普通梯度下降仅依赖于偏导数的值乘以学习率。如果损失地形有很多局部极小值，尤其是当它们很陡峭时，普通梯度下降可能会陷入某个极小值并无法恢复。为了弥补这一点，我们可以修改普通梯度下降，加入一个*动量*项，这个项使用前一步更新的部分值。将动量加入到梯度下降中，为算法在损失地形中的运动增加了惯性，从而可能使梯度下降越过不良的局部极小值。
- en: Let’s define and then experiment with momentum using 1D and 2D examples, as
    we did earlier. After that, we’ll update our *NN.py* toolkit to use momentum to
    see how that affects models trained on more complex datasets.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义动量，然后通过1D和2D的示例进行实验，正如我们之前所做的。之后，我们将更新我们的*NN.py*工具包，使用动量来观察它如何影响在更复杂数据集上训练的模型。
- en: What Is Momentum?
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是动量？
- en: In physics, the momentum of a moving object is defined as the mass times the
    velocity, ***p*** = *m**v***. However, velocity itself is the first derivative
    of the position, ***v*** = *d**x***/*dt*, so momentum is mass times how fast the
    position of the object is changing in time.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理学中，运动物体的动量定义为质量乘以速度，***p*** = *m**v***。然而，速度本身是位置的第一导数，***v*** = *d**x***/*dt*，因此动量就是质量与物体位置随时间变化速度的乘积。
- en: For gradient descent, *position* is the function value, and *time* is the argument
    to the function. The *velocity*, then, is how fast the function value changes
    with a change in the argument, ∂*f*/∂***x***. Therefore, we can think of *momentum*
    as a scaled velocity term. In physics, the scale factor is the mass. For gradient
    descent, the scale factor is *μ* (mu), a number between zero and one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于梯度下降，*位置*是函数值，*时间*是函数的自变量。因此，*速度*是函数值如何随着自变量变化而变化，即∂*f*/∂***x***。因此，我们可以将*动量*视为一个缩放的速度项。在物理学中，缩放因子是质量。对于梯度下降，缩放因子是*μ*（mu），它是一个介于
    0 和 1 之间的数字。
- en: If we call the gradient including the momentum term ***v***, then the gradient
    descent update equation that was
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将包含动量项的梯度称为***v***，那么梯度下降的更新方程是
- en: '![Image](Images/285equ01.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/285equ01.jpg)'
- en: becomes
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 变为
- en: '![Image](Images/11equ05.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ05.jpg)'
- en: for some initial velocity, ***v*** = 0, and the “mass,” *μ*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些初始速度，***v*** = 0，以及“质量”，*μ*。
- en: Let’s walk through [Equation 11.5](ch11.xhtml#ch11equ05) to understand what
    it means. The two-step update, first ***v*** and then ***x***, makes it easy to
    iterate, as we know we must do for gradient descent. If we substitute ***v***
    into the update equation for ***x***, we get
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过[方程 11.5](ch11.xhtml#ch11equ05)来理解它的含义。这个两步更新，先更新***v***，再更新***x***，使得迭代变得简单，因为我们知道这正是梯度下降的要求。如果我们将***v***代入***x***的更新方程，我们得到
- en: '![Image](Images/285equ02.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/285equ02.jpg)'
- en: This makes it clear that the update includes the gradient step we had previously
    but adds back in a fraction of the previous step size. It’s a fraction because
    we restrict *μ* to [0, 1]. If *μ* = 0, we’re back to vanilla gradient descent.
    It might be helpful to think of *μ* as a scale factor, the fraction of the previous
    velocity to keep along with the current gradient value.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明，更新包含了我们之前的梯度步骤，同时还加入了前一步大小的一部分。它是一个分数，因为我们将*μ*限制在[0, 1]之间。如果*μ* = 0，我们就回到了普通的梯度下降。可以将*μ*视为一个缩放因子，它表示保留前一步速度与当前梯度值之间的比例。
- en: The momentum term tends to keep motion through the loss landscape heading in
    its previous direction. The value of *μ* determines the strength of that tendency.
    Deep learning practitioners typically use *μ* = 0.9, so most of the previous update
    direction is maintained in the next step, with the current gradient providing
    a small adjustment. Again, like many things in deep learning, this number was
    chosen empirically.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 动量项倾向于保持在损失函数空间中沿着先前的方向移动。*μ*的值决定了这一趋势的强度。深度学习实践者通常使用*μ* = 0.9，因此大部分先前更新的方向会在下一步中得以保留，而当前的梯度则提供了一个小的调整。同样，像许多深度学习中的参数一样，这个值是通过经验选择的。
- en: Newton’s first law of motion states that an object in motion remains in motion
    unless acted upon by an outside force. Resistance to an external force is related
    to the object’s mass and is called *inertia*. So, we might also view the *μ**v***
    term as inertia, which might have been a better name for it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿的第一运动定律指出，物体一旦运动，除非受到外力作用，否则将保持运动状态。对外力的抵抗与物体的质量有关，称为*惯性*。因此，我们也可以将*μ**v***项视为惯性，它本可以是一个更合适的名称。
- en: Regardless of the name, now that we have it, let’s see what it does to the 1D
    and 2D examples we worked through earlier using vanilla gradient descent.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 无论名称如何，既然我们有了它，接下来让我们看看它对之前使用普通梯度下降处理过的一维和二维示例的影响。
- en: Momentum in 1D
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一维动量
- en: Let’s modify the 1D and 2D examples above to use a momentum term. We’ll start
    with the 1D case. The updated code is in the file *gd_1d_momentum.py* and appears
    here as [Listing 11-2](ch11.xhtml#ch11ex02).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改上面的 1D 和 2D 示例，加入动量项。我们将从一维情况开始。更新后的代码在文件*gd_1d_momentum.py*中，下面是[清单 11-2](ch11.xhtml#ch11ex02)。
- en: import matplotlib.pylab as plt
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pylab as plt
- en: 'def f(x):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f(x):'
- en: return 6*x**2 - 12*x + 3
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: return 6*x**2 - 12*x + 3
- en: 'def d(x):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'def d(x):'
- en: return 12*x - 12
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: return 12*x - 12
- en: ❶ m = ['o','s','>','<','*','+','p','h','P','D']
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ m = ['o','s','>','<','*','+','p','h','P','D']
- en: x = np.linspace(0.75,1.25,1000)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.linspace(0.75,1.25,1000)
- en: plt.plot(x,f(x))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(x,f(x))
- en: ❷ x = xold = 0.75
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = xold = 0.75
- en: eta = 0.09
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: eta = 0.09
- en: mu = 0.8
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: mu = 0.8
- en: v = 0.0
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: v = 0.0
- en: 'for i in range(10):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: ❸ plt.plot([xold,x], [f(xold),f(x)], marker=m[i], linestyle='dotted',
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ plt.plot([xold,x], [f(xold),f(x)], marker=m[i], linestyle='dotted',
- en: color='r')
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: color='r')
- en: xold = x
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: xold = x
- en: v = mu*v - eta * d(x)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: v = mu*v - eta * d(x)
- en: x = x + v
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: x = x + v
- en: 'for i in range(40):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(40):'
- en: v = mu*v - eta * d(x)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: v = mu*v - eta * d(x)
- en: x = x + v
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: x = x + v
- en: ❹ plt.plot(x,f(x),marker='X', color='k')
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ plt.plot(x,f(x),marker='X', color='k')
- en: '*Listing 11-2: Gradient descent in one dimension with momentum*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-2：带动量的一维梯度下降*'
- en: '[Listing 11-2](ch11.xhtml#ch11ex02) is a bit dense, so let’s parse it out.
    First, we are plotting, so we include Matplotlib. Next, we define the function,
    f(x), and its derivative, d(x), as we did before. To configure plotting, we define
    a collection of markers ❶ and then plot the function itself. As before, we begin
    at *x* = 0.75 ❷ and set the step size (eta), momentum (mu), and initial velocity
    (v).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11-2](ch11.xhtml#ch11ex02)有些复杂，所以我们来逐步解析。首先，我们要绘图，因此需要导入 Matplotlib。接下来，我们定义函数
    f(x) 和它的导数 d(x)，就像之前那样。为了配置绘图，我们定义了一组标记 ❶，然后绘制函数本身。像之前一样，我们从 *x* = 0.75 ❷ 开始，设置步长
    (eta)，动量 (mu)，以及初始速度 (v)。'
- en: We’re now ready to iterate. We’ll use two gradient descent loops. The first
    plots each step ❸ and the second continues gradient descent to demonstrate that
    we do eventually locate the minimum, which we mark with an 'X' ❹. For each step,
    we calculate the new velocity by mimicking [Equation 11.5](ch11.xhtml#ch11equ05),
    and then we add the velocity to the current position to get the next position.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好进行迭代了。我们将使用两个梯度下降循环。第一个循环绘制每一步 ❸，第二个循环继续梯度下降，演示我们最终确实能够找到最小值，并用“X” ❹
    标记它。对于每一步，我们通过模仿[方程 11.5](ch11.xhtml#ch11equ05)计算新的速度，然后将速度加到当前的位置，得到下一个位置。
- en: '[Figure 11-6](ch11.xhtml#ch11fig06) shows the output of *gd_1d_momentum.py*.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-6](ch11.xhtml#ch11fig06)展示了 *gd_1d_momentum.py* 的输出。'
- en: '![image](Images/11fig06.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig06.jpg)'
- en: '*Figure 11-6: Gradient descent in one dimension with momentum*'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-6：带动量的一维梯度下降*'
- en: Note that we intentionally used a large step size (*η*), so we overshoot the
    minimum. The momentum term tends to overshoot minima as well. If you follow the
    dashed line and the sequence of plot markers, you can walk through the first 10
    gradient descent steps. There is oscillation, but the oscillation is damped and
    eventually settles at the minimum, as marked. Adding momentum enhanced the overshoot
    due to the large step size. However, even with the momentum term, which isn’t
    advantageous here, because there’s only one minimum, with enough gradient descent
    steps, we find the minimum in the end.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们故意使用了较大的步长 (*η*)，因此会超过最小值。动量项也会倾向于超越最小值。如果你沿着虚线和绘图标记的顺序走，你可以看到前 10 步梯度下降的过程。虽然有振荡，但振荡逐渐减弱，最终会稳定在最小值处，正如标记所示。由于较大的步长，动量增强了这种超调现象。然而，即使有动量项，在这里并没有特别的优势，因为这里只有一个最小值，但经过足够的梯度下降步骤后，我们最终还是找到了最小值。
- en: Momentum in 2D
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2D 动量
- en: 'Now, let’s update our 2D example. We’re working with the code in *gd _momentum.py*.
    Recall that, for the 2D example, the function is the sum of two inverted Gaussians.
    Including momentum updates the code slightly, as shown in [Listing 11-3](ch11.xhtml#ch11ex03):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新我们的二维示例。我们正在使用 *gd_momentum.py* 中的代码。回想一下，对于二维示例，函数是两个反向高斯函数的和。包括动量使得代码稍有更新，如[清单
    11-3](ch11.xhtml#ch11ex03)所示：
- en: 'def gd(x,y, eta,mu, steps, marker):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 'def gd(x,y, eta,mu, steps, marker):'
- en: xold = x
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: xold = x
- en: yold = y
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: yold = y
- en: ❶ vx = vy = 0.0
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ vx = vy = 0.0
- en: 'for i in range(steps):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(steps):'
- en: plt.plot([xold,x],[yold,y], marker=marker,
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot([xold,x],[yold,y], marker=marker,
- en: linestyle='dotted', color='k')
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: linestyle='dotted', color='k')
- en: xold = x
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: xold = x
- en: yold = y
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: yold = y
- en: ❷ vx = mu*vx - eta * dx(x,y)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ vx = mu*vx - eta * dx(x,y)
- en: vy = mu*vy - eta * dy(x,y)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: vy = mu*vy - eta * dy(x,y)
- en: ❸ x = x + vx
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x = x + vx
- en: y = y + vy
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: y = y + vy
- en: ❹ gd( 0.7,-0.2, 0.1, 0.9, 25, '>')
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ gd( 0.7,-0.2, 0.1, 0.9, 25, '>')
- en: gd( 1.5, 1.5, 0.02, 0.9, 90, '*')
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: gd( 1.5, 1.5, 0.02, 0.9, 90, '*')
- en: '*Listing 11-3: Gradient descent in two dimensions with momentum*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-3：带动量的二维梯度下降*'
- en: Here, we have the new function, gd, which performs gradient descent with momentum
    beginning at (x,y), using the given *μ* and *η*, and runs for steps iterations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是新的函数 gd，它执行从 (x,y) 开始的带动量的梯度下降，使用给定的 *μ* 和 *η*，并运行指定的步骤次数。
- en: The initial velocity is set ❶, and the loop begins. The velocity update of [Equation
    11.5](ch11.xhtml#ch11equ05) becomes vx = mu*vx - eta * dx(x,y) ❷, and the position
    update becomes x = x + vx ❸. As before, a line is plotted between the last position
    and the current one to track motion through the function landscape.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 初始速度设为❶，然后开始循环。[方程 11.5](ch11.xhtml#ch11equ05)的速度更新为 vx = mu*vx - eta * dx(x,y)
    ❷，位置更新为 x = x + vx ❸。像之前一样，绘制一条线连接上一个位置和当前的位置，以跟踪函数景观中的运动。
- en: The code in *gd_momentum.py* traces the motion starting at two of the points
    we used before, (0.7, −0.2) and (1.5, 1.5) ❹. Note the number of steps and learning
    rate vary by point to keep the plot from becoming too cluttered. The output of
    *gd_momentum.py* is [Figure 11-7](ch11.xhtml#ch11fig07).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*gd_momentum.py* 中的代码追踪了从我们之前使用的两个点（0.7, −0.2）和（1.5, 1.5）❹ 开始的运动。请注意，每个点的步骤数和学习率不同，以避免图表过于拥挤。*gd_momentum.py*
    的输出是 [图 11-7](ch11.xhtml#ch11fig07)。'
- en: '![image](Images/11fig07.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig07.jpg)'
- en: '*Figure 11-7: Gradient descent in two dimensions with momentum*'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-7：带动量的二维梯度下降*'
- en: Compare the paths in [Figure 11-7](ch11.xhtml#ch11fig07) with those in [Figure
    11-5](ch11.xhtml#ch11fig05). Adding momentum has pushed the paths, so they tend
    to keep moving in the same direction. Notice how the path beginning at (1.5, 1.5)
    spirals toward the minimum, while the other path curves toward the shallower minimum,
    passes it, and backtracks toward it again.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [图 11-7](ch11.xhtml#ch11fig07) 中的路径与 [图 11-5](ch11.xhtml#ch11fig05) 中的路径进行对比。添加动量后，路径发生了偏移，因此它们倾向于保持在同一方向上移动。注意从（1.5,
    1.5）开始的路径如何螺旋向最小值，而另一条路径则弯向更浅的最小值，超过它后又返回。
- en: The momentum term alters the dynamics of motion through the function space.
    However, it’s not immediately evident that momentum adds anything helpful. After
    all, the (1.5, 1.5) starting position using vanilla gradient descent moved directly
    to the minimum position without spiraling.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 动量项改变了通过函数空间的运动动态。然而，动量是否有助于并不马上显现出来。毕竟，使用普通梯度下降法，从（1.5, 1.5）起始位置直接移动到最小值位置，而没有螺旋式下降。
- en: Let’s add momentum to our *NN.py* toolkit and see if it buys us anything when
    training real neural networks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将动量加入到 *NN.py* 工具包中，看看在训练实际神经网络时它是否有任何效果。
- en: Training Models with Momentum
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用动量训练模型
- en: 'To support momentum in *NN.py*, we need to tweak the FullyConnectedLayer method
    in two places. First, as shown in [Listing 11-4](ch11.xhtml#ch11ex04), we modify
    the constructor to allow a momentum keyword:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 *NN.py* 中支持动量，我们需要在两个地方调整 FullyConnectedLayer 方法。首先，如 [清单 11-4](ch11.xhtml#ch11ex04)
    所示，我们修改构造函数，允许使用动量关键词：
- en: 'def __init__(self, input_size, output_size, momentum=0.0):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'def __init__(self, input_size, output_size, momentum=0.0):'
- en: self.delta_w = np.zeros((input_size, output_size))
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: self.delta_w = np.zeros((input_size, output_size))
- en: self.delta_b = np.zeros((1,output_size))
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: self.delta_b = np.zeros((1, output_size))
- en: self.passes = 0
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: self.passes = 0
- en: self.weights = np.random.rand(input_size, output_size) - 0.5
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: self.weights = np.random.rand(input_size, output_size) - 0.5
- en: self.bias = np.random.rand(1, output_size) - 0.5
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: self.bias = np.random.rand(1, output_size) - 0.5
- en: ❶ self.vw = np.zeros((input_size, output_size))
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ self.vw = np.zeros((input_size, output_size))
- en: self.vb = np.zeros((1, output_size))
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: self.vb = np.zeros((1, output_size))
- en: self.momentum = momentum
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: self.momentum = momentum
- en: '*Listing 11-4: Adding the momentum keyword*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-4：添加动量关键词*'
- en: Here, we add a momentum keyword, with a default of zero, into the argument list.
    Then, we define initial velocities for the weights (vw) and biases (vb) ❶. These
    are matrices of the proper shape initialized to zero. We also keep the momentum
    argument for later use.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将动量关键词添加到参数列表中，默认值为零。然后，我们为权重（vw）和偏置（vb）定义初始速度 ❶。这些是初始化为零的适当形状的矩阵。我们还保留动量参数，以供后续使用。
- en: 'The second modification is to the step method, as [Listing 11-5](ch11.xhtml#ch11ex05)
    shows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个修改是针对 step 方法的，如 [清单 11-5](ch11.xhtml#ch11ex05) 所示：
- en: 'def step(self, eta):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'def step(self, eta):'
- en: ❶ self.vw = self.momentum * self.vw - eta * self.delta_w / self.passes
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ self.vw = self.momentum * self.vw - eta * self.delta_w / self.passes
- en: self.vb = self.momentum * self.vb - eta * self.delta_b / self.passes
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: self.vb = self.momentum * self.vb - eta * self.delta_b / self.passes
- en: ❷ self.weights = self.weights + self.vw
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ self.weights = self.weights + self.vw
- en: self.bias = self.bias + self.vb
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: self.bias = self.bias + self.vb
- en: self.delta_w = np.zeros(self.weights.shape)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: self.delta_w = np.zeros(self.weights.shape)
- en: self.delta_b = np.zeros(self.bias.shape)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: self.delta_b = np.zeros(self.bias.shape)
- en: self.passes = 0
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: self.passes = 0
- en: '*Listing 11-5: Updating the step to include momentum*'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11-5：更新 step 方法以包含动量*'
- en: 'We implement [Equation 11.5](ch11.xhtml#ch11equ05), first for the weights ❶,
    then for the biases in the line after. We multiply the momentum (*μ*) by the previous
    velocity, then subtract the average error over the minibatch, multiplied by the
    learning rate. We then move the weights and biases by adding the velocity ❷. That’s
    all we need to do to incorporate momentum. Then, to use it, we add the momentum
    keyword to each fully connected layer when building the network, as shown in [Listing
    11-6](ch11.xhtml#ch11ex06):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了[公式 11.5](ch11.xhtml#ch11equ05)，首先处理权重 ❶，然后是下一行的偏置。我们将动量（*μ*）与之前的速度相乘，然后减去平均误差乘以学习率。接着，我们通过加上速度
    ❷ 来更新权重和偏置。这就是我们融入动量所需的全部步骤。然后，要使用它，我们在构建网络时，在每个全连接层中添加动量关键字，如[示例 11-6](ch11.xhtml#ch11ex06)所示：
- en: net = Network()
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: net = Network()
- en: net.add(FullyConnectedLayer(14*14, 100, momentum=0.9))
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: net.add(FullyConnectedLayer(14*14, 100, momentum=0.9))
- en: net.add(ActivationLayer())
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: net.add(ActivationLayer())
- en: net.add(FullyConnectedLayer(100, 50, momentum=0.9))
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: net.add(FullyConnectedLayer(100, 50, momentum=0.9))
- en: net.add(ActivationLayer())
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: net.add(ActivationLayer())
- en: net.add(FullyConnectedLayer(50, 10, momentum=0.9))
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: net.add(FullyConnectedLayer(50, 10, momentum=0.9))
- en: net.add(ActivationLayer())
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: net.add(ActivationLayer())
- en: '*Listing 11-6: Specifying momentum when building the network*'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11-6：构建网络时指定动量*'
- en: Adding momentum per layer opens up the possibility of using layer-specific momentum
    values. While I’m unaware of any research doing so, it seems a fairly obvious
    thing to try, so by now, someone has likely experimented with it. For our purposes,
    we’ll set the momentum of all layers to 0.9 and move on.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 每层添加动量使得可以为每一层设置特定的动量值。虽然我不知道有任何研究这么做，但这似乎是一个很明显的尝试方向，因此现在可能已经有人对此进行了实验。对于我们的目的，我们将所有层的动量设为
    0.9，继续进行下去。
- en: 'How should we test our new momentum? We could use the MNIST dataset we used
    above, but it’s not a good candidate, because it’s too easy. Even a simple fully
    connected network achieves better than 97 percent accuracy. Therefore, we’ll replace
    the MNIST digits dataset with another, similar dataset that’s known to be more
    of a challenge: the Fashion-MNIST dataset. (See “Fashion-MNIST: A Novel Image
    Dataset for Benchmarking Machine Learning Algorithms” by Han Xiao et al., arXiv:1708.07747
    [2017].)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '我们如何测试新的动量呢？我们可以使用上面提到的 MNIST 数据集，但它并不是一个好的选择，因为它太简单了。即使是一个简单的全连接网络，也能达到超过
    97% 的准确率。因此，我们将用另一个已知更具挑战性的类似数据集来替代 MNIST：Fashion-MNIST 数据集。（参见 Han Xiao 等人的《Fashion-MNIST:
    A Novel Image Dataset for Benchmarking Machine Learning Algorithms》，arXiv:1708.07747
    [2017]。）'
- en: 'The *Fashion-MNIST dataset (FMNIST)* is a drop-in replacement for the existing
    MNIST dataset. It contains images from 10 classes of clothing, all 28×28-pixel
    grayscale. For our purposes, we’ll do as we did for MNIST and reduce the 28×28-pixel
    images to 14 ×14 pixels. The images are in the dataset directory as NumPy arrays.
    Let’s train a model using them. The code for the model is similar to that of [Listing
    10-7](ch10.xhtml#ch10ex07), except in [Listing 11-7](ch11.xhtml#ch11ex07) we replace
    the MNIST dataset with FMNIST:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*Fashion-MNIST 数据集（FMNIST）* 是现有 MNIST 数据集的替代品。它包含来自 10 类服装的图像，所有图像为 28×28 像素的灰度图。为了我们的目的，我们将像
    MNIST 一样，将 28×28 像素的图像缩小到 14×14 像素。图像存储在数据集目录中，格式为 NumPy 数组。让我们使用它们训练一个模型。模型的代码与[示例
    10-7](ch10.xhtml#ch10ex07)相似，不同之处在于，在[示例 11-7](ch11.xhtml#ch11ex07)中，我们将 MNIST
    数据集替换为 FMNIST：'
- en: x_train = np.load("fmnist_train_images_small.npy")/255
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("fmnist_train_images_small.npy")/255
- en: x_test = np.load("fmnist_test_images_small.npy")/255
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("fmnist_test_images_small.npy")/255
- en: y_train = np.load("fmnist_train_labels_vector.npy")
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("fmnist_train_labels_vector.npy")
- en: y_test = np.load("fmnist_test_labels.npy")
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("fmnist_test_labels.npy")
- en: '*Listing 11-7: Loading the Fashion-MNIST dataset*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11-7：加载 Fashion-MNIST 数据集*'
- en: We also include code to calculate the Matthews correlation coefficient (MCC)
    on the test data. We first encountered the MCC in [Chapter 4](ch04.xhtml#ch04),
    where we learned that it’s a better measure of a model’s performance than the
    accuracy is. The code to run is in *fmnist.py*. Taking around 18 minutes on an
    older Intel i5 box, one run of it produced
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还包括了计算测试数据集上的 Matthews 相关系数（MCC）的代码。在[第 4 章](ch04.xhtml#ch04)中，我们第一次遇到了 MCC，我们了解到它比准确率更能衡量模型的表现。运行代码位于
    *fmnist.py* 中。在一台较旧的 Intel i5 计算机上运行，约 18 分钟后，得到的结果是：
- en: '[[866   1  14  28   8   1  68   0  14   0]'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[[866   1  14  28   8   1  68   0  14   0]'
- en: '[  5 958   2  25   5   0   3   0   2   0]'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 5 958   2  25   5   0   3   0   2   0]'
- en: '[ 20   1 790  14 126   0  44   1   3   1]'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 20  1 790  14 126   0  44   1   3   1]'
- en: '[ 29  21  15 863  46   1  20   0   5   0]'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 29  21  15 863  46   1  20   0   5   0]'
- en: '[  0   0  91  22 849   1  32   0   5   0]'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0   0  91  22 849   1  32   0   5   0]'
- en: '[  0   0   0   1   0 960   0  22   2  15]'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0   0   1   0 960   0  22   2  15]'
- en: '[161   2 111  38 115   0 556   0  17   0]'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[161   2 111  38 115   0 556   0  17   0]'
- en: '[  0   0   0   0   0  29   0 942   0  29]'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0   0   0   0  29   0 942   0  29]'
- en: '[  1   0   7   5   6   2   2   4 973   0]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[  1   0   7   5   6   2   2   4 973   0]'
- en: '[  0   0   0   0   0   6   0  29   1 964]]'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0   0   0   0   6   0  29   1 964]]'
- en: accuracy = 0.8721000
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 = 0.8721000
- en: MCC = 0.8584048
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: MCC = 0.8584048
- en: The confusion matrix, still 10 × 10 because of the 10 classes in FMNIST, is
    quite noisy compared to the very clean confusion matrix we saw with MNIST proper.
    This is a challenging dataset for fully connected models. Recall that the MCC
    is a measure where the closer it is to one, the better the model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵，仍然是 10 × 10，因为 FMNIST 有 10 个类别，相较于我们在 MNIST 数据集中看到的非常干净的混淆矩阵，它相当嘈杂。这是一个对于全连接模型来说具有挑战性的数据集。回想一下，MCC
    是一个衡量标准，值越接近 1 表示模型越好。
- en: The confusion matrix above is for a model trained without momentum. The learning
    rate was 1.0, and it was trained for 40,000 minibatches of 64 samples. What happens
    if we add momentum of 0.9 to each fully connected layer and reduce the learning
    rate to 0.2? When we add momentum, it makes sense to reduce the learning rate
    so we aren’t taking large steps compounded by the momentum already moving in a
    particular direction. Do explore what happens if you run *fmnist.py* with a learning
    rate of 0.2 and no momentum.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的混淆矩阵是为一个没有动量训练的模型生成的。学习率是 1.0，并且训练了 40,000 个包含 64 个样本的小批量。如果我们为每个全连接层添加 0.9
    的动量并将学习率降低到 0.2，会发生什么呢？当我们添加动量时，降低学习率是有道理的，这样我们就不会因为动量已经在某个方向上移动而采取过大的步伐。请尝试运行
    *fmnist.py*，学习率为 0.2 且没有动量，看看会发生什么。
- en: The version of the code with momentum is in *fmnist_momentum.py*. After about
    20 minutes, one run of this code produced
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 带动量的代码版本在 *fmnist_momentum.py* 中。大约 20 分钟后，这段代码运行一次产生了
- en: '[[766   5  14  61   2   1 143   0   8   0]'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[[766   5  14  61   2   1 143   0   8   0]'
- en: '[  1 958   2  30   3   0   6   0   0   0]'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[  1 958   2  30   3   0   6   0   0   0]'
- en: '[ 12   0 794  16  98   0  80   0   0   0]'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 12   0 794  16  98   0  80   0   0   0]'
- en: '[  8  11  13 917  21   0  27   0   3   0]'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[  8  11  13 917  21   0  27   0   3   0]'
- en: '[  0   0  84  44 798   0  71   0   3   0]'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0  84  44 798   0  71   0   3   0]'
- en: '[  0   0   0   1   0 938   0  31   1   29]'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0   0   1   0 938   0  31   1  29]'
- en: '[ 76   2  87  56  60   0 714   0   5   0]'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[  76   2  87  56  60   0 714   0   5   0]'
- en: '[  0   0   0   0   0  11   0 963   0  26]'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0   0   0   0  11   0 963   0  26]'
- en: '[  1   1   6   8   5   1  10   4 964   0]'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[  1   1   6   8   5   1  10   4 964   0]'
- en: '[  0   0   0   0   0   6   0  33   0 961]]'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[  0   0   0   0   0   6   0  33   0 961]]'
- en: accuracy = 0.8773000
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 = 0.8773000
- en: MCC = 0.8638721
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: MCC = 0.8638721
- en: giving us a slightly higher MCC. Does that mean momentum helped? Maybe. As we
    well understand by now, training neural networks is a stochastic process. So,
    we can’t rely on results from a single training of the models. We need to train
    the models many times and perform statistical tests on the results. Excellent!
    This gives us a chance to put the hypothesis testing knowledge we gained in [Chapter
    4](ch04.xhtml#ch04) to good use.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 给我们带来稍微更高的 MCC。这意味着动量有帮助吗？也许有。正如我们现在所理解的，训练神经网络是一个随机过程。因此，我们不能依赖于单次训练结果。我们需要多次训练模型并对结果进行统计检验。太好了！这给了我们一个机会，充分利用我们在[第
    4 章](ch04.xhtml#ch04)中学到的假设检验知识。
- en: Instead of running *fmnist.py* and *fmnist_momentum.py* one time each, let’s
    run them 22 times each. This takes the better part of a day on my old Intel i5
    system, but patience is a virtue. The net result is 22 MCC values for the model
    with momentum and 22 for the model without momentum. There’s nothing magical about
    22 samples, but we intend to use the Mann-Whitney U test, and the rule of thumb
    for that test is to have at least 20 samples in each dataset.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 不要一次性运行 *fmnist.py* 和 *fmnist_momentum.py*，而是分别运行它们 22 次。这在我旧的英特尔 i5 系统上花费了一整天的时间，但耐心是美德。最终结果是有
    22 个带动量的模型的 MCC 值和 22 个不带动量的模型的 MCC 值。22 个样本并没有什么神奇之处，但我们打算使用曼-惠特尼 U 检验，而该检验的经验法则是每个数据集至少要有
    20 个样本。
- en: '[Figure 11-8](ch11.xhtml#ch11fig08) displays histograms of the results.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-8](ch11.xhtml#ch11fig08) 显示了结果的直方图。'
- en: '![image](Images/11fig08.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig08.jpg)'
- en: '*Figure 11-8: Histograms showing the distribution of MCC for models trained
    with momentum (light gray) and without (dark gray)*'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-8：显示带动量（浅灰色）和不带动量（深灰色）训练的模型的 MCC 分布的直方图*'
- en: The darker gray bars are the no-momentum MCC values, and the lighter bars are
    those with momentum. Visually, the two are largely distinct from each other. The
    code producing [Figure 11-8](ch11.xhtml#ch11fig08) is in the file *fmnist_analyze.py*.
    Do take a look at the code. It uses SciPy’s ttest_ind and mannwhitneyu along with
    the implementation we gave in [Chapter 4](ch04.xhtml#ch04) of Cohen’s *d* to calculate
    the effect size. The MCC values themselves are in the NumPy files listed in the
    code.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 深灰色的条形表示没有动量的MCC值，浅灰色的条形表示有动量的MCC值。从视觉上看，两者有很大的不同。生成[图11-8](ch11.xhtml#ch11fig08)的代码位于*fmnist_analyze.py*文件中。请务必查看该代码。它使用了SciPy的ttest_ind和mannwhitneyu，以及我们在[Cohen’s
    *d*](ch04.xhtml#ch04)中给出的实现来计算效应大小。MCC值本身位于代码中列出的NumPy文件中。
- en: 'Along with the graph, *fmnist_analyze.py* produces the following output:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图表，*fmnist_analyze.py*还生成了以下输出：
- en: 'no momentum: 0.85778 +/- 0.00056'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 无动量：0.85778 +/- 0.00056
- en: 'momentum   : 0.86413 +/- 0.00075'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '动量         : 0.86413 +/- 0.00075'
- en: 't-test momentum vs no (t,p): (6.77398299, 0.00000003)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: t检验动量与无动量 (t,p)：(6.77398299, 0.00000003)
- en: 'Mann-Whitney U             : (41.00000000, 0.00000126)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mann-Whitney U         : (41.00000000, 0.00000126)'
- en: 'Cohen''s d                  : 2.04243'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cohen’s d          : 2.04243'
- en: where the top two lines are the mean and the standard error of the mean. The
    t-test results are (*t*, *p*), the *t*-test statistic and associated *p*-value.
    Similarly, the Mann-Whitney U test results are (*U*, *p*), the *U* statistic and
    its *p*-value. Recall how the Mann-Whitney U test is a nonparametric test assuming
    nothing about the shape of the distribution of MCC values. The t-test assumes
    they are normally distributed. As we have only 22 samples each, we really can’t
    make any definitive statement about whether the results are normally distributed;
    the histograms don’t look much like Gaussian curves. That’s why we included the
    Mann-Whitney U test results.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 其中前两行是均值和均值标准误差。t检验结果为(*t*, *p*)，即*t*统计量和相关的*p*值。同样，Mann-Whitney U检验的结果为(*U*,
    *p*)，即*U*统计量及其*p*值。回顾一下，Mann-Whitney U检验是一种非参数检验，它并不假设MCC值的分布形态，而t检验则假设数据服从正态分布。由于我们每组只有22个样本，因此我们无法就结果是否符合正态分布做出明确的结论；直方图看起来也不像高斯曲线。这就是为什么我们还包括了Mann-Whitney
    U检验结果。
- en: A glance at the respective *p*-values tells us that the difference in means
    between the MCC values with and without momentum is highly statistically significant
    in favor of the with-momentum results. The *t*-value is positive, and the with-momentum
    result was the first argument. What of Cohen’s *d*-value? It’s a bit above 2.0,
    indicating a (very) large effect size.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下各自的*p*值，我们可以发现，带动量和不带动量的MCC值均值差异在统计学上非常显著，支持带动量的结果。*t*值为正，且带动量的结果为第一个参数。那么Cohen的*d*值呢？它略高于2.0，表示（非常）大的效应大小。
- en: Can we *now* say that momentum helps in this case? Probably. It produced better
    performing models given the hyperparameters we used. The stochastic nature of
    training neural networks makes it possible that we could tweak the hyperparameters
    of both models to eliminate the difference we see in the data we have. The architecture
    between the two is fixed, but nothing says the learning rate and minibatch size
    are optimized for either model.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们*现在*可以说动量在这种情况下有效吗？可能可以。它在我们使用的超参数下产生了更好的模型性能。训练神经网络的随机性使得我们可能通过调整两种模型的超参数来消除我们在现有数据中看到的差异。两者的架构是固定的，但没有什么表明学习率和小批量大小是为任何一个模型优化的。
- en: A punctilious researcher would feel compelled to run an optimization process
    over the hyperparameters and, once satisfied that they’d found the very best model
    for both approaches, make a more definite statement after repeating the experiment.
    We, thankfully, are not punctilious researchers. Instead, we’ll use the evidence
    we have, along with the several decades of wisdom acquired by the world’s machine
    learning researchers regarding the utility of momentum in gradient descent, to
    state that, yes, momentum helps models learn, and you should use it in most cases.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一位细致的研究者可能会觉得有必要对超参数进行优化过程，并且在确认找到最佳模型后，通过重复实验来做出更明确的结论。幸运的是，我们不是那种细致的研究者。相反，我们将利用现有的证据，结合世界各地机器学习研究者对梯度下降中动量的有效性所积累的数十年智慧，来说明：是的，动量有助于模型学习，在大多数情况下你应该使用它。
- en: However, the normality question is begging for further investigation. We are,
    after all, seeking to improve our mathematical *and* practical intuition regarding
    deep learning. Therefore, let’s train the with-momentum model for FMNIST, not
    22 times but 100 times. As a concession, we’ll reduce the number of minibatches
    from 40,000 to 10,000\. Still, expect to spend the better part of a day waiting
    for the program to finish. The code, which we won’t walk through here, is in *fmnist_repeat.py*.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正态性问题需要进一步研究。毕竟，我们是在寻求提高我们关于深度学习的数学*和*实践直觉。因此，我们将使用动量模型对FMNIST进行训练，不是训练22次，而是训练100次。作为让步，我们将小批量的数量从40,000减少到10,000。尽管如此，仍然预计你将花费大部分时间等待程序完成。代码（我们在这里不逐步讲解）位于*fmnist_repeat.py*中。
- en: '[Figure 11-9](ch11.xhtml#ch11fig09) presents a histogram of the results.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-9](ch11.xhtml#ch11fig09)展示了结果的直方图。'
- en: Clearly, this distribution does not look at all like a normal curve. The output
    of *fmnist_repeat.py* includes the result of SciPy’s normaltest function. This
    function performs a statistical test on a set of data under the null hypothesis
    that the data *is* normally distributed. Therefore, a *p*-value below, say, 0.05
    or 0.01, indicates data that is not normally distributed. Our *p*-value is virtually
    zero.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这个分布看起来一点也不像正态曲线。*fmnist_repeat.py*的输出包含了SciPy的normaltest函数的结果。这个函数对一组数据进行统计检验，假设数据是正态分布的。因此，如果*p*值低于比如说0.05或0.01，就表示数据不是正态分布的。我们的*p*值几乎为零。
- en: 'What to make of [Figure 11-9](ch11.xhtml#ch11fig09)? First, as the results
    are certainly not normal, we aren’t justified in using a t-test. However, we also
    used the nonparametric Mann-Whitney U test and found highly statistically significant
    results, so our claims above are still valid. Second, the long tail of the distribution
    in [Figure 11-9](ch11.xhtml#ch11fig09) is to the left. We might even make an argument
    that the result is possibly bimodal: that there are two peaks, one near 0.83 and
    the other, smaller one near an MCC of 0.75.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解读[图 11-9](ch11.xhtml#ch11fig09)？首先，由于结果显然不是正态分布的，因此我们没有理由使用t检验。然而，我们也使用了非参数的Mann-Whitney
    U检验，并且得到了高度统计显著的结果，所以我们上述的结论依然有效。其次，[图 11-9](ch11.xhtml#ch11fig09)中分布的长尾在左边。我们甚至可以认为结果可能是双峰的：一个峰值接近0.83，另一个较小的峰值接近MCC为0.75。
- en: '![image](Images/11fig09.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig09.jpg)'
- en: '*Figure 11-9: Distribution of MCC values for 100 trainings of the FMNIST model*'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-9：FMNIST模型100次训练的MCC值分布*'
- en: Most models trained to a relatively consistent level of performance, with an
    MCC near 0.83\. However, the long tail indicates that when the model wasn’t reasonably
    good, it was just plain horrid.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型的训练性能相对一致，MCC接近0.83。然而，长尾表明，当模型表现不佳时，它通常是非常糟糕的。
- en: Intuitively, [Figure 11-9](ch11.xhtml#ch11fig09) seems reasonable to me. We
    know stochastic gradient descent is susceptible to improper initialization, and
    our little toolkit is using old-school small random value initialization. It seems
    likely that we have an increased chance of starting at a poor location in the
    loss landscape and are doomed after that to poor performance.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 从直觉上看，[图 11-9](ch11.xhtml#ch11fig09)对我来说是合理的。我们知道随机梯度下降容易受到不当初始化的影响，而我们的小工具包使用的是老式的小随机值初始化。似乎更可能的情况是，我们有较大的机会从一个不好的位置开始，并且之后的表现注定会很差。
- en: What if the tail were on the right? What might that indicate? A long tail on
    the right would mean most model performance is mediocre to poor, but, on occasion,
    an especially “bright” model comes along. Such a scenario would mean that better
    models are out there, but that our training and/or initialization strategy isn’t
    particularly good at finding them. I think the tail on the left is preferable—most
    models find reasonably good local minima, so most trainings, unless horrid, end
    up in pretty much the same place in terms of performance.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尾巴在右边呢？那可能意味着什么？右边的长尾表示大多数模型的表现平庸或差劲，但偶尔会有一个特别“亮眼”的模型出现。这样的情况意味着更好的模型是存在的，但我们的训练和/或初始化策略并不擅长找到它们。我认为左边的长尾更可取——大多数模型能找到合理的局部最小值，所以大多数训练，除非非常糟糕，否则最终会在性能上差不多。
- en: Now, let’s examine a common variant of momentum, one that you’ll no doubt run
    across during your sojourn through deep learning.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看动量的一个常见变体，这个变体你在深度学习的旅程中肯定会遇到。
- en: Nesterov Momentum
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Nesterov 动量
- en: Many deep learning toolkits include the option to use *Nesterov momentum* during
    gradient descent. Nesterov momentum is a modification of gradient descent widely
    used in the optimization community. The version typically implemented in deep
    learning updates standard momentum from
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习工具包包括在梯度下降过程中使用 *Nesterov 动量* 的选项。Nesterov 动量是梯度下降的一个修改版本，在优化社区中广泛使用。深度学习中通常实现的版本将标准动量更新为
- en: '![Image](Images/295equ01.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/295equ01.jpg)'
- en: to
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 变为
- en: '![Image](Images/11equ06.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11equ06.jpg)'
- en: where we’re using gradient notation instead of partials of a loss function to
    indicate that the technique is general and applies to any function, *f*(***x***).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用梯度符号而不是损失函数的偏导数，表示该技术是通用的，适用于任何函数，*f*(***x***).
- en: The difference between standard momentum and deep learning Nesterov momentum
    is subtle, just a term that’s added to the argument of the gradient. The idea
    is to use the existing momentum to calculate the gradient, not at the current
    position, ***x***, but the position gradient descent would be at if it continued
    further using the current momentum, ***x*** + *μ **v***. We then use the gradient’s
    value at that position to update the current position, as before.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 标准动量与深度学习中的 Nesterov 动量之间的区别是微妙的，只是添加了一个项到梯度的参数中。其思想是使用现有的动量来计算梯度，而不是在当前位置 ***x***
    计算，而是在使用当前动量继续前进时，梯度下降将会到达的位置，即 ***x*** + *μ **v***。然后我们使用该位置的梯度值来更新当前的位置，如同之前一样。
- en: The claim, well demonstrated for optimization in general, is that this tweak
    leads to faster convergence, meaning gradient descent will find the minimum in
    fewer steps. However, even though toolkits implement it, there is reason to believe
    the noise that stochastic gradient descent with minibatches introduces offsets
    the adjustment to the point where it’s unlikely Nesterov momentum is any more
    useful for training deep learning models than regular momentum. (For more on this,
    see the comment on page 292 of *Deep Learning* by Ian Goodfellow et al.)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这个声明在优化中得到了很好的验证，即这一调整有助于更快的收敛，意味着梯度下降会在更少的步骤内找到最小值。然而，尽管工具包已经实现了它，但有理由相信，随机梯度下降与小批量引入的噪声会抵消这种调整，使得
    Nesterov 动量在训练深度学习模型时，不太可能比常规动量更有用。（有关更多内容，请参阅 *深度学习*（Ian Goodfellow 等人）第292页的评论。）
- en: However, the 2D example in this chapter uses the actual function to calculate
    gradients, so we might expect Nesterov momentum to be effective in that case.
    Let’s update the 2D example, minimizing the sum of two inverted Gaussians, and
    see if Nesterov momentum improves convergence, as claimed. The code we’ll run
    is in *gd_nesterov.py* and is virtually identical to the code in *gd_momentum.py*.
    Additionally, I tweaked both files a tiny bit to return the final position after
    gradient descent is complete. That way, we can see how close we are to the known
    minima.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本章的二维示例使用实际函数来计算梯度，因此我们可以期待在这种情况下 Nesterov 动量是有效的。让我们更新二维示例，最小化两个反转高斯的和，看看
    Nesterov 动量是否能如声明的那样提高收敛性。我们将运行的代码在 *gd_nesterov.py* 中，与 *gd_momentum.py* 中的代码几乎相同。此外，我稍微修改了这两个文件，以便在梯度下降完成后返回最终位置。这样，我们就能看到我们与已知最小值的接近程度。
- en: Implementing [Equation 11.6](ch11.xhtml#ch11equ06) is straightforward and affects
    only the velocity update, causing
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 [方程 11.6](ch11.xhtml#ch11equ06) 是直接的，仅影响速度更新，导致
- en: vx = mu*vx - eta * dx(x,y)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: vx = mu*vx - eta * dx(x,y)
- en: vy = mu*vy - eta * dy(x,y)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: vy = mu*vy - eta * dy(x,y)
- en: to become
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 变为
- en: vx = mu * vx - eta * dx(x + mu * vx,y)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: vx = mu * vx - eta * dx(x + mu * vx,y)
- en: vy = mu * vy - eta * dy(x,y + mu * vy)
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: vy = mu * vy - eta * dy(x,y + mu * vy)
- en: to add the momentum for each component, *x* and *y*. Everything else remains
    the same.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个分量 *x* 和 *y* 添加动量。其他部分保持不变。
- en: '[Figure 11-10](ch11.xhtml#ch11fig010) compares standard momentum (top, from
    [Figure 11-7](ch11.xhtml#ch11fig07)) and Nesterov momentum (bottom).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-10](ch11.xhtml#ch11fig010) 比较了标准动量（顶部，来自 [图 11-7](ch11.xhtml#ch11fig07)）和
    Nesterov 动量（底部）。'
- en: '![image](Images/11fig10.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/11fig10.jpg)'
- en: '*Figure 11-10: Standard momentum (top) and Nesterov momentum (bottom)*'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-10：标准动量（顶部）和 Nesterov 动量（底部）*'
- en: Visually, Nesterov momentum shows less of an overshoot, especially for the spiral
    marking the path beginning at (1.5, 1.5). What about the final location that each
    approach returns? We get [Table 11-2](ch11.xhtml#ch11tab02).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，Nesterov 动量显示出较少的过冲，特别是对于从 (1.5, 1.5) 开始的螺旋路径。那么，每种方法返回的最终位置如何呢？我们可以查看
    [表 11-2](ch11.xhtml#ch11tab02)。
- en: '**Table 11-2**: Final Location for Gradient Descent With and Without Nesterov
    Momentum'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 11-2**：使用和不使用 Nesterov 动量的梯度下降最终位置'
- en: '| **Initial point** | **Standard** | **Nesterov** | **Minimum** |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| **初始点** | **标准** | **Nesterov** | **最小值** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| (1.5,1.5) | (–0.9496, 0.9809) | (–0.9718, 0.9813) | (–1,1) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| (1.5,1.5) | (–0.9496, 0.9809) | (–0.9718, 0.9813) | (–1,1) |'
- en: '| (0.7,–0.2) | (0.8807, –0.9063) | (0.9128, –0.9181) | (1,–1) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| (0.7,–0.2) | (0.8807, –0.9063) | (0.9128, –0.9181) | (1,–1) |'
- en: The Nesterov momentum results are closer to the known minima than the standard
    momentum results after the same number of gradient descent steps.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Nesterov 动量的结果比标准动量在相同数量的梯度下降步骤后更接近已知的最小值。
- en: Adaptive Gradient Descent
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应梯度下降
- en: 'The gradient descent algorithm is almost trivial, which invites adaptation.
    In this section, we’ll walk through the math behind three variants of gradient
    descent popular with the deep learning community: RMSprop, Adagrad, and Adam.
    Of the three, Adam is the most popular by far, but the others are well worth understanding,
    as they build in succession leading up to Adam. All three of these algorithms
    adapt the learning rate on the fly in some manner.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法几乎是简单的，这使得它适合进行适应性调整。在本节中，我们将介绍三种在深度学习社区中非常流行的梯度下降变体：RMSprop、Adagrad 和
    Adam。在这三者中，Adam 无疑是最受欢迎的，但其他两个也非常值得理解，因为它们是逐步构建，最终到达 Adam。这三种算法都以某种方式动态地调整学习率。
- en: RMSprop
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RMSprop
- en: Geoffrey Hinton introduced *RMSprop*, which stands for *root mean square propagation*,
    in his 2012 Coursera lecture series. Much like momentum (with which it can be
    combined), RMSprop is gradient descent that tracks the value of the gradient as
    it changes and uses that value to modify the step taken.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey Hinton 在他 2012 年的 Coursera 讲座中介绍了 *RMSprop*，即 *均方根传播*。与动量类似（它们可以结合使用），RMSprop
    是一种梯度下降方法，它跟踪梯度值的变化，并使用该值来修改步长。
- en: RMSprop uses a *decay term*, γ (gamma), to calculate a running average of the
    gradients as the algorithm progresses. In his lecture, Hinton uses γ = 0.9.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 使用一个*衰减项*，γ（gamma），来计算梯度的移动平均值，随着算法的进展而变化。在他的讲座中，Hinton 使用 γ = 0.9。
- en: The gradient descent update becomes
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新变为
- en: '![Image](Images/11equ07.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ07.jpg)'
- en: First, we update *m*, the running average of the squares of the gradients, weighted
    by γ, the decay term. Next comes the velocity term, which is almost the same as
    in vanilla gradient descent, but we divide the learning rate by the running average’s
    square root, hence the RMS part of RMSprop. We then subtract the scaled velocity
    from the current position to take the step. We’re writing the step as an addition,
    similar to the momentum equations above ([Equations 11.5](ch11.xhtml#ch11equ05)
    and [11.6](ch11.xhtml#ch11equ06)); note the minus sign before the velocity update.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们更新 *m*，即梯度平方的移动平均值，并通过 γ 来加权，γ 是衰减项。接下来是速度项，几乎与普通的梯度下降相同，但我们将学习率除以移动平均值的平方根，这就是
    RMSprop 中的 RMS 部分。然后，我们将缩放后的速度从当前位置中减去，以便迈出一步。我们将这一步写作加法，类似于上面的动量方程（[方程 11.5](ch11.xhtml#ch11equ05)
    和 [11.6](ch11.xhtml#ch11equ06)）；注意速度更新前的负号。
- en: 'RMSprop works with momentum as well. For example, extending RMSprop with Nesterov
    momentum is straightforward:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 也可以与动量一起使用。例如，扩展 RMSprop 与 Nesterov 动量结合是很直接的：
- en: '![Image](Images/11equ08.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ08.jpg)'
- en: with *μ* the momentum factor, as before.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *μ* 是动量因子，与之前一样。
- en: It’s claimed that RMSprop is a robust classifier. We’ll see below how it fared
    on one test. We’re considering it an adaptive technique because the learning rate
    (*η*) is scaled by the square root of the running gradient mean; therefore, the
    effective learning rate is adjusted based on the history of the descent—it isn’t
    fixed once and for all.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 据称，RMSprop 是一种稳健的分类器。我们将在下面看到它在一个测试中的表现。我们将其视为一种自适应技术，因为学习率（*η*）是通过梯度的移动均值的平方根进行缩放的；因此，实际的学习率是基于下降历史进行调整的——它不是一成不变的。
- en: RMSprop is often used in reinforcement learning, the branch of machine learning
    that attempts to learn how to act. For example, playing Atari video games uses
    reinforcement learning. RMSprop is believed to be robust when the optimization
    process is *nonstationary*, meaning the statistics change in time. Conversely,
    a *stationary* process is one where the statistics do not change in time. Training
    classifiers using supervised learning is stationary, as the training set is, typically,
    fixed and not changing, as should be the data fed to the classifier over time,
    though that is harder to enforce. In reinforcement learning, time is a factor,
    and the statistics of the dataset might change over time; therefore, reinforcement
    learning might involve nonstationary optimization.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: RMSprop 常用于强化学习，这是机器学习的一个分支，旨在学习如何行动。例如，玩 Atari 视频游戏就使用强化学习。RMSprop 被认为在优化过程中是*非平稳*的，即统计量随时间变化时具有鲁棒性。相反，*平稳*过程是指统计量在时间上不发生变化。使用监督学习训练分类器是平稳的，因为训练集通常是固定的，不会变化，尽管输入给分类器的数据可能会随时间改变，这一点更难强制执行。在强化学习中，时间是一个因素，数据集的统计量可能会随时间变化；因此，强化学习可能涉及非平稳优化。
- en: Adagrad and Adadelta
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Adagrad 和 Adadelta
- en: '*Adagrad* appeared in 2011 (see “Adaptive Subgradient Methods for Online Learning
    and Stochastic Optimization” by John Duchi et al., *Journal of Machine Learning
    Research* 12[7], [2011]). At first glance, it looks quite similar to RMSprop,
    though there are important differences.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adagrad* 出现于 2011 年（参见 John Duchi 等人所著的《在线学习与随机优化的自适应子梯度方法》，*机器学习研究杂志* 12[7]，[2011]）。乍一看，它与
    RMSprop 很相似，但也有一些重要的区别。'
- en: We can write the basic update rule for Adagrad as
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 Adagrad 的基本更新规则写为
- en: '![Image](Images/11equ09.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ09.jpg)'
- en: This requires some explanation.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一些解释。
- en: First, notice the *i* subscript on the velocity update, both on the velocity,
    ***v***, and the gradient, *▽f **(x***). Here, *i* refers to a component of the
    velocity, meaning the update must be applied per component. The top of [Equation
    11.9](ch11.xhtml#ch11equ09) repeats for all the components of the system. For
    a deep neural network, this means all the weights and biases.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，注意速度更新中的 *i* 下标，无论是在速度 ***v*** 还是梯度 *▽f **(x*** )上。这里，*i* 指的是速度的一个分量，意味着更新必须针对每个分量进行。
    [方程 11.9](ch11.xhtml#ch11equ09) 的顶部对系统的所有分量都重复一次。对于深度神经网络，这意味着所有的权重和偏置。
- en: Next, look at the sum in the denominator of the per-component velocity update.
    Here, τ (tau) is a counter over *all* the gradient steps taken during the optimization
    process, meaning for each component of the system, Adagrad tracks the sum of the
    square of the gradient calculated at each step. If we’re using [Equation 11.9](ch11.xhtml#ch11equ09)
    for the 11th gradient descent step, then the sum in the denominator will have
    11 terms, and so on. As before, *η* is a learning rate, which here is global to
    all components.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看看每个分量速度更新的分母中的和。这里，τ（tau）是一个计数器，记录在优化过程中所采取的*所有*梯度步骤，这意味着对于系统的每个分量，Adagrad
    跟踪每一步计算的梯度平方和。如果我们正在使用[方程 11.9](ch11.xhtml#ch11equ09)来进行第 11 步梯度下降，那么分母中的和将有 11
    项，依此类推。如前所述，*η* 是学习率，这里是全局性的，适用于所有分量。
- en: 'A variant of Adagrad is also in widespread use: *Adadelta*. (See “Adadelta:
    An Adaptive Learning Rate Method” by Matthew Zeiler, [2012].) Adadelta replaces
    the square root of the sum over all steps in the velocity update with a running
    average of the last few steps, much like the running average of RMSprop. Adadelta
    also replaces the manually selected global learning rate, *η*, with a running
    average of the previous few velocity updates. This eliminates the selection of
    an appropriate *η* but introduces a new parameter, γ, to set the window’s size,
    as was done for RMSprop. It’s likely that γ is less sensitive to the properties
    of the dataset than *η* is. Note how in the original Adadelta paper, γ is written
    as *ρ* (rho).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 的一个变种也得到了广泛使用：*Adadelta*。（参见 Matthew Zeiler 的《Adadelta：一种自适应学习率方法》，[2012]。）Adadelta
    用最近几步的滑动平均值替代了速度更新中对所有步骤求和后的平方根，类似于 RMSprop 的滑动平均值。Adadelta 还用前几次速度更新的滑动平均值替代了手动选择的全局学习率
    *η*。这消除了选择合适的 *η* 的需要，但引入了一个新参数 γ，用来设置窗口的大小，正如在 RMSprop 中所做的那样。γ 可能对数据集的性质比 *η*
    更不敏感。请注意，在原始的 Adadelta 论文中，γ 被写作 *ρ*（rho）。
- en: Adam
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Adam
- en: 'Kingma and Ba published *Adam*, from “adaptive moment estimation,” in 2015,
    and it has been cited over 66,000 times as of this writing. Adam uses the square
    of the gradient, as RMSprop and Adagrad do, but also tracks a momentum-like term.
    Let’s present the update equations and then walk through them:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Kingma 和 Ba 于2015年发布了 *Adam*，来自“自适应矩估计”一词，截至目前已被引用超过66,000次。Adam 和 RMSprop、Adagrad
    一样使用梯度的平方，但也跟踪类似动量的项。我们将呈现更新方程并逐步讲解：
- en: '![Image](Images/11equ10.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/11equ10.jpg)'
- en: The first two lines of [Equation 11.10](ch11.xhtml#ch11equ10) define ***m***
    and ***v*** as running averages of the first and second moments. The first moment
    is the mean; the second moment is akin to the variance, which is the second moment
    of the difference between a data point and the mean. Note the squaring of the
    gradient value in the definition of ***v***. The running moments are weighted
    by two scalar parameters, *β*[1] and *β*[2].
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.10](ch11.xhtml#ch11equ10)的前两行将 ***m*** 和 ***v*** 定义为一阶和二阶矩的移动平均值。一阶矩是均值；二阶矩类似于方差，即数据点与均值之间差值的二阶矩。请注意，在定义
    ***v*** 时对梯度值进行了平方操作。移动矩被两个标量参数 *β*[1] 和 *β*[2] 加权。'
- en: The next two lines define ![Image](Images/300equ01.jpg) and ![Image](Images/300equ02.jpg).
    These are bias correction terms to make ***m*** and ***v*** better estimates of
    the first and second moments. Here, *t*, an integer starting at zero, is the timestep.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两行定义了 ![Image](Images/300equ01.jpg) 和 ![Image](Images/300equ02.jpg)。这些是偏差修正项，用来使
    ***m*** 和 ***v*** 更好地估计一阶和二阶矩。这里，*t* 是从零开始的整数，表示时间步。
- en: The actual step updates ***x*** by subtracting the bias-corrected first moment,
    ![Image](Images/300equ01.jpg), scaled by the ratio of the global learning rate,
    *η*, and the square root of the bias-corrected second moment, ![Image](Images/300equ02.jpg).
    The ∊ term is a constant to avoid division by zero.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的步更新通过从偏差修正后的第一矩 ![Image](Images/300equ01.jpg) 中减去，乘以全局学习率 *η* 和偏差修正后第二矩 ![Image](Images/300equ02.jpg)
    的平方根的比率。∊ 项是一个常数，用于避免除零错误。
- en: '[Equation 11.10](ch11.xhtml#ch11equ10) has four parameters, which seems excessive,
    but three of them are straightforward to set and are seldom changed. The original
    paper suggests *β*[1] = 0.9, *β*[2] = 0.999, and ∊ = 10^(−8). Therefore, as with
    vanilla gradient descent, the user is left to select *η*. For example, Keras defaults
    to *η* = 0.001, which works well in many cases.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 11.10](ch11.xhtml#ch11equ10)有四个参数，看起来有些过多，但其中三个设置起来比较直接，并且很少更改。原文建议 *β*[1]
    = 0.9，*β*[2] = 0.999，和 ∊ = 10^(−8)。因此，像传统的梯度下降一样，用户仍需选择 *η*。例如，Keras 默认 *η* =
    0.001，这在很多情况下效果不错。'
- en: The Kingma and Ba paper shows via experiment that Adam generally outperforms
    SGD with Nesterov momentum, RMSprop, Adagrad, and Adadelta. This is likely why
    Adam is currently the go-to optimizer for many deep learning tasks.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: Kingma 和 Ba 的论文通过实验表明，Adam 通常优于带有 Nesterov 动量的 SGD、RMSprop、Adagrad 和 Adadelta。这也可能是为什么
    Adam 当前是许多深度学习任务首选优化器的原因。
- en: Some Thoughts About Optimizers
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于优化器的一些思考
- en: Which optimization algorithm to use and when depends on the dataset. As mentioned,
    Adam is currently favored for many tasks, though properly tuned SGD can be quite
    effective as well, and some swear by it. While it’s not possible to make a blanket
    statement about which is the best algorithm, for there is no such thing, we can
    conduct a little experiment and discuss the results.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪个优化算法以及何时使用取决于数据集。如前所述，Adam 当前被许多任务青睐，尽管适当调整的 SGD 也能非常有效，有些人对此深信不疑。虽然无法对哪个算法是最好的作出普遍性陈述，因为没有绝对的最佳算法，但我们可以做一个小实验，并讨论结果。
- en: 'This experiment, for which I’ll present only the results, trained a small convolutional
    neural network on MNIST using 16,384 random samples for the training set, a minibatch
    of 128, and 12 epochs. The results show the mean and standard error of the mean
    for five runs of each optimizer: SGD, RMSprop, Adagrad, and Adam. Of interest
    is the accuracy of the test set and the training clock time. I trained all models
    on the same machine, so relative timing is what we should look at. No GPU was
    used.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验，我只呈现结果，训练了一个小型卷积神经网络，使用16,384个随机样本作为训练集，批量大小为128，训练12个周期。结果展示了每个优化器（SGD、RMSprop、Adagrad
    和 Adam）五次运行的均值和标准误差。关注点是测试集的准确率和训练的时钟时间。我在同一台机器上训练了所有模型，所以我们应该关注相对时间。未使用 GPU。
- en: '[Figure 11-11](ch11.xhtml#ch11fig011) shows the overall test set accuracy (top)
    and the training time (bottom) by optimizer.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-11](ch11.xhtml#ch11fig011)展示了按优化器划分的总体测试集准确率（上）和训练时间（下）。'
- en: On average, SGD and RMSprop were about 0.5 percent less accurate than the other
    optimizers, with RMSprop varying widely but never matching Adagrad or Adam. Arguably,
    Adam performed the best in terms of accuracy. For training time, SGD was the fastest
    and Adam the slowest, as we might expect, given the multiple per-step calculations
    Adam performs relative to the simplicity of SGD. Overall, the results support
    the community’s intuition that Adam is a good optimizer.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 平均来说，SGD和RMSprop的准确度比其他优化器低约0.5%，其中RMSprop的表现差异较大，但始终没有达到Adagrad或Adam的效果。可以说，Adam在准确度方面表现最好。在训练时间上，SGD最快，而Adam最慢，正如我们所预期的那样，因为相较于SGD的简单性，Adam需要进行多次每步计算。总体而言，结果支持了社区对Adam作为优化器的直觉判断。
- en: '![image](Images/11fig11.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/11fig11.jpg)'
- en: '*Figure 11-11: MNIST model accuracy (top) and training time (bottom) by optimizer*'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11-11：MNIST模型的准确度（顶部）和训练时间（底部）按优化器分类*'
- en: Summary
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented gradient descent, working through the basic form, vanilla
    gradient descent, with 1D and 2D examples. We followed by introducing stochastic
    gradient descent and justified its use in deep learning.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了梯度下降，讲解了其基本形式——原始梯度下降，并通过1D和2D示例加以说明。接着，我们介绍了随机梯度下降，并解释了其在深度学习中的应用理由。
- en: We discussed momentum next, both standard and Nesterov. With standard momentum,
    we demonstrated that it does help in training deep models (well, relatively “deep”).
    We showed the effect of Nesterov momentum visually using a 2D example and discussed
    why Nesterov momentum and stochastic gradient descent might counteract each other.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了动量，包括标准动量和Nesterov动量。通过标准动量，我们证明它在训练深度模型（好吧，相对“深”的模型）时确实有所帮助。我们使用2D示例直观地展示了Nesterov动量的效果，并讨论了为什么Nesterov动量和随机梯度下降可能相互抵消。
- en: The chapter concluded with a look at the gradient descent update equations for
    advanced algorithms, thereby illustrating how vanilla gradient descent invites
    modification. A simple experiment gave us insight into how the algorithms perform
    and appeared to justify the deep learning community’s belief in Adam’s general
    suitability over SGD.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过查看高级算法的梯度下降更新方程作总结，从而说明了原始梯度下降如何受到修改。一个简单的实验让我们对算法的表现有所了解，并似乎证实了深度学习社区对Adam优化器优于SGD的普遍认同。
- en: And, with this chapter, our exploration of the mathematics of deep learning
    draws to a close. All that remains is a final appendix that points you to places
    where you can go to learn more.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的结束，我们对深度学习数学的探索也接近尾声。剩下的只是最后的附录，指引你去了解更多学习资源。
- en: Epilogue
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后记
- en: As the great computer scientist Edsger W. Dijkstra said, “There should be no
    such thing as boring mathematics.” I sincerely hope you didn’t find this book
    boring. I’d hate to offend Dijkstra’s ghost. If you’re still reading at this point,
    I suspect you did find something of merit. Good! Thanks for sticking with it.
    Math should never be boring.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 正如伟大的计算机科学家Edsger W. Dijkstra所说：“不应存在无聊的数学。”我真诚地希望你不会觉得这本书无聊。我可不想冒犯Dijkstra的幽灵。如果你现在还在读下去，我猜测你确实找到了有价值的内容。好样的！感谢你一直坚持阅读。数学绝不应是无聊的。
- en: 'We’ve covered the basics of what you need to understand and work with deep
    learning. Don’t stop here, however: use the references in the Appendix and continue
    your mathematical explorations. You should never be satisfied with your knowledge
    base—always seek to broaden it.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了你需要理解和使用深度学习的基本内容。然而，千万不要停步于此：请利用附录中的参考资料，继续你的数学探索。你永远不应满足于现有的知识体系——永远追求扩展它。
- en: If you have questions or comments, please do reach out to me at *[mathfordeeplearning@gmail.com](mailto:mathfordeeplearning@gmail.com)*.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或评论，请随时通过* [mathfordeeplearning@gmail.com](mailto:mathfordeeplearning@gmail.com)*与我联系。
