- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: The Basics of Data
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据基础
- en: '![](image_fi/book_art/chapterart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](image_fi/book_art/chapterart.png)'
- en: '*Data* means different things to different people: a stock trader might think
    of data as real-time stock quotes, while a NASA engineer might associate data
    with signals coming from a Mars rover. When it comes to data processing and analysis,
    however, the same or similar approaches and techniques can be applied to a variety
    of datasets, regardless of their origin. All that matters is how the data is structured.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据*对不同的人意味着不同的东西：股票交易员可能认为数据是实时股票报价，而NASA工程师可能会将数据与来自火星探测器的信号联系在一起。然而，在数据处理和分析中，无论数据的来源如何，相同或类似的方法和技术可以应用于各种数据集。重要的是数据是如何被结构化的。'
- en: This chapter provides a conceptual introduction to data processing and analysis.
    We’ll first look at the main categories of data you may have to deal with, then
    touch on common data sources. Next, we’ll consider the steps in a typical data
    processing pipeline (that is, the actual process of obtaining, preparing, and
    analyzing data). Finally, we’ll examine Python’s unique advantages as a data science
    tool.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了数据处理和分析的概念性介绍。我们首先将介绍你可能需要处理的主要数据类别，然后简单讨论常见的数据来源。接下来，我们将考虑典型数据处理管道中的步骤（即获取、准备和分析数据的实际过程）。最后，我们将探讨Python作为数据科学工具的独特优势。
- en: Categories of Data
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据类别
- en: 'Programmers divide data into three main categories: unstructured, structured,
    and semistructured. In a data processing pipeline, the source data is typically
    unstructured; from this, you form structured or semistructured datasets for further
    processing. Some pipelines, however, use structured data from the start. For example,
    an application processing geographical locations might receive structured data
    directly from GPS sensors. The following sections explore the three main categories
    of data as well as time series data, a special type of data that can be structured
    or semistructured.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员将数据分为三大类：非结构化数据、结构化数据和半结构化数据。在数据处理管道中，源数据通常是非结构化的；从这些数据中，你可以形成结构化或半结构化的数据集以便进一步处理。然而，一些管道从一开始就使用结构化数据。例如，处理地理位置的应用程序可能会直接从GPS传感器接收结构化数据。以下章节将探讨三大数据类别以及时间序列数据，这是一种可以是结构化或半结构化的数据类型。
- en: Unstructured Data
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非结构化数据
- en: 'Unstructured data is data with no predefined organizational system, or schema.
    This is the most widespread form of data, with common examples including images,
    videos, audio, and natural language text. To illustrate, consider the following
    financial statement from a pharmaceutical company:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据是没有预定义组织系统或模式的数据。这是最广泛的数据显示形式，常见的例子包括图像、视频、音频和自然语言文本。举个例子，考虑一下来自制药公司的财务报表：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This text is considered unstructured data because the information found in
    it isn’t organized with a predefined schema. Instead, the information is randomly
    scattered within the statement. You could rewrite this statement in any number
    of ways while still conveying the same information. For example:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这段文本被认为是非结构化数据，因为其中的信息并没有按照预定义的模式进行组织。相反，信息在报表中是随机分散的。你可以用许多不同的方式重写这份报表，同时传达相同的信息。例如：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Despite its lack of structure, unstructured data may contain important information,
    which you can extract and convert to structured or semistructured data through
    appropriate transformation and analysis steps. For example, image recognition
    tools first convert the collection of pixels within an image into a dataset of
    a predefined format and then analyze this data to identify content in the image.
    Similarly, the following section will show a few ways in which the data extracted
    from our financial statement could be structured.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管缺乏结构，非结构化数据可能包含重要信息，你可以通过适当的转换和分析步骤将其提取并转化为结构化或半结构化数据。例如，图像识别工具首先将图像中的像素集合转换为预定义格式的数据集，然后分析这些数据以识别图像中的内容。类似地，接下来的部分将展示一些方法，通过这些方法，我们可以将从财务报表中提取的数据进行结构化。
- en: Structured Data
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化数据
- en: Structured data has a predefined format that specifies how the data is organized.
    Such data is usually stored in a repository like a relational database or just
    a *.**csv* (comma-separated values) file. The data fed into such a repository
    is called a *record*, and the information in it is organized in *fields* that
    must arrive in a sequence matching the expected structure. Within a database,
    records of the same structure are logically grouped in a container called a *table*.
    A database may contain various tables, with each table having a set structure
    of fields.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据具有预定义的格式，指定了数据的组织方式。这类数据通常存储在像关系数据库这样的存储库中，或者只是存储在一个*.**csv*（逗号分隔值）文件中。输入到这样的存储库中的数据称为*记录*，其中的信息按必须与预期结构匹配的顺序组织在*字段*中。在数据库中，具有相同结构的记录被逻辑地分组在一个名为*表*的容器中。一个数据库可以包含多个表，每个表都有一组结构化字段。
- en: 'There are two basic types of structured data: numerical and categorical. *Categorical
    data* is that which can be categorized on the basis of similar characteristics;
    cars, for example, might be categorized by make and model. *Numerical data*, on
    the other hand, expresses information in numerical form, allowing you to perform
    mathematical operations on it.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据有两种基本类型：数值型和分类型。*分类数据*是指可以根据相似特征进行分类的数据；例如，汽车可能根据品牌和型号进行分类。*数值数据*则以数字形式表示信息，允许对其进行数学运算。
- en: Keep in mind that categorical data can sometimes take on numerical values. For
    example, consider ZIP codes or phone numbers. Although they are expressed with
    numbers, it wouldn’t make any sense to perform math operations on them, such as
    finding the median ZIP code or average phone number.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，分类数据有时可以采用数值形式。例如，考虑邮政编码或电话号码。虽然它们用数字表示，但对它们进行数学运算没有意义，比如找出中位数邮政编码或平均电话号码。
- en: 'How can we organize the text sample introduced in the previous section into
    structured data? We’re interested in specific information in this text, such as
    company names, dates, and stock prices. We want to present that information in
    fields in the following format, ready for insertion into a database:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将上一节中介绍的文本样本组织为结构化数据？我们关注的是文本中的特定信息，例如公司名称、日期和股票价格。我们希望将这些信息以以下格式的字段呈现，准备插入数据库：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using techniques of *natural language processing* *(**NLP**)*, a discipline
    that trains machines to understand human-readable text, we can extract information
    appropriate for these fields. For example, we look for a company name by recognizing
    a categorical data variable that can only be one of many preset values, such as
    Google, Apple, or GoodComp. Likewise, we can recognize a date by matching its
    explicit ordering to one of a set of explicit ordering formats, such as `yyyy-mm-dd`.
    In our example, we recognize, extract, and present our data in the predefined
    format like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*自然语言处理*（*NLP*）技术，这一学科训练机器理解人类可读的文本，我们可以提取适合这些字段的信息。例如，我们通过识别一个只能是预设值之一的分类数据变量来查找公司名称，比如Google、Apple或GoodComp。同样，我们可以通过将日期的显式顺序与一组显式的排序格式（如`yyyy-mm-dd`）进行匹配来识别日期。在我们的示例中，我们识别、提取并以预定义的格式呈现数据，如下所示：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To store this record in a database, it’s better to present it as a row-like
    sequence of fields. We therefore might reorganize the record as a rectangular
    data object, or a 2D matrix:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将此记录存储在数据库中，最好将其呈现为类似行的字段序列。因此，我们可能会将记录重新组织为矩形数据对象或二维矩阵：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The information you choose to extract from the same unstructured data source
    depends on your requirements. Our example statement not only contains the change
    in GoodComp’s stock value for a certain date but also indicates the reason for
    that change, in the phrase “the company announced positive early-stage trial results
    for its vaccine.” Taking the statement from this angle, you might create a record
    with these fields:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择从同一非结构化数据源中提取的信息取决于你的需求。我们的示例语句不仅包含了GoodComp公司某一日期的股票变化，还指出了变化的原因，即“公司宣布其疫苗的早期阶段试验结果积极”。从这个角度来看，你可能会创建一个包含以下字段的记录：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Compare this to the first record we extracted:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们提取的第一个记录进行比较：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that these two records contain different fields and therefore have different
    structures. As a result, they must be stored in two different database tables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这两个记录包含不同的字段，因此具有不同的结构。结果，它们必须存储在两个不同的数据库表中。
- en: Semistructured Data
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半结构化数据
- en: In cases where the structural identity of the information doesn’t conform to
    stringent formatting requirements, we may need to process semistructured data
    formats, which let us have records of different structures within the same container
    (database table or document). Like unstructured data, semistructured data isn’t
    tied to a predefined organizational schema; unlike unstructured data, however,
    samples of semistructured data do exhibit some degree of structure, usually in
    the form of self-describing tags or other markers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息的结构标识与严格的格式要求不符的情况下，我们可能需要处理半结构化数据格式，它允许我们在同一个容器（数据库表或文档）中存储不同结构的记录。与非结构化数据一样，半结构化数据不依赖于预定义的组织模式；然而，与非结构化数据不同，半结构化数据的样本通常具有一定的结构，通常表现为自描述标签或其他标记。
- en: 'The most common semistructured data formats include XML and JSON. This is what
    our financial statement might look like in JSON format:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的半结构化数据格式包括XML和JSON。这是我们的财务报表可能在JSON格式中的样子：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here you can recognize the key information that we previously extracted from
    the statement. Each piece of information is paired with a descriptive tag, such
    as `"Company"` or `"Date"`. Thanks to the tags, the information is organized similarly
    to how it appeared in the previous section, but now we have a fourth tag, `"Details"`,
    paired with an entire fragment of the original statement, which looks unstructured.
    This example shows how semistructured data formats can accommodate both structured
    and unstructured pieces of data within a single record.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以识别我们之前从声明中提取的关键信息。每一条信息都配有一个描述性标签，如“`公司`”或“`日期`”。感谢这些标签，信息被组织得与前一部分中出现的方式相似，但现在我们有了一个第四个标签，“`详情`”，它与原始声明中的整个片段配对，该片段看起来没有结构。这个例子展示了半结构化数据格式如何在单个记录中容纳结构化和非结构化数据。
- en: 'Moreover, you can put multiple records of unequal structure into the same container.
    Here, we store the two different records derived from our example financial statement
    in the same JSON document:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以将多个不同结构的记录放入同一个容器中。在这里，我们将从示例财务报表中衍生的两条不同记录存储在同一个JSON文档中：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Recall from the discussion in the previous section that a relational database,
    being a rigidly structured data repository, cannot accommodate records of varying
    structures in the same table.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回想前一部分的讨论，关系型数据库作为一种严格结构化的数据存储库，不能在同一表中容纳具有不同结构的记录。
- en: Time Series Data
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间序列数据
- en: A time series is a set of data points indexed or listed in time order. Many
    financial datasets are stored as a time series due to the fact that financial
    data typically consists of observations at a specific time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列是一组按时间顺序排列或列出的数据点。许多金融数据集作为时间序列存储，因为金融数据通常包含特定时间的观察结果。
- en: 'Time series data can be either structured or semistructured. Imagine you’re
    receiving location data in records from a taxi’s GPS tracking device at regular
    time intervals. The data might arrive in the following format:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据可以是结构化的或半结构化的。想象一下，你从出租车的GPS跟踪设备中按定时间隔接收位置数据。数据可能以以下格式到达：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A new data record arrives every minute that includes the latest location coordinates
    (latitude/longitude) from `cab_238`. Each record has the same sequence of fields,
    and each field has a consistent structure from one record to the next, allowing
    you to store this time series data in a relational database table as regular structured
    data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每分钟会有一条新的数据记录，其中包含来自`cab_238`的最新位置坐标（纬度/经度）。每条记录的字段顺序相同，且每个字段在一条记录到下一条记录之间保持一致的结构，从而可以将这些时间序列数据存储在关系型数据库表中，作为常规结构化数据。
- en: 'Now suppose the data comes at unequal intervals, which is often the case in
    practice, and that you receive more than one set of coordinates in one minute.
    The incoming structure might look like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据以不等时间间隔到达，这在实际中经常发生，并且你每分钟收到不止一组坐标。接收到的数据结构可能如下所示：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that the first `coord` field includes two sets of coordinates and is thus
    not consistent with the second `coord` field. This data is semistructured.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个`coord`字段包含两组坐标，因此与第二个`coord`字段不一致。这些数据是半结构化的。
- en: Sources of Data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据来源
- en: 'Now that you know what the main categories of data are, what are the sources
    from which you might receive such data? Generally speaking, data may come from
    many different sources, including texts, videos, images, and device sensors, among
    others. From the standpoint of Python scripts that you’ll write, however, the
    most common data sources are:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了数据的主要类别，那么你可能从哪些来源获取这些数据呢？一般来说，数据可以来自很多不同的来源，包括文本、视频、图像和设备传感器等。从你将编写的
    Python 脚本的角度来看，最常见的数据来源有：
- en: An application programming interface (API)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序编程接口（API）
- en: A web page
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页
- en: A database
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库
- en: A file
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件
- en: This list isn’t intended to be comprehensive or restrictive; there are many
    other sources of data. In Chapter 9, for example, you’ll see how to use a smartphone
    as a GPS data provider for your data processing pipeline, specifically by using
    a bot application as a go-between connecting the smartphone and your Python script.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不是为了全面或限制性地列出所有选项；还有很多其他的数据来源。例如，在第九章，你将看到如何使用智能手机作为数据处理管道的 GPS 数据提供者，具体来说是通过使用一个机器人应用程序作为中介，连接智能手机和你的
    Python 脚本。
- en: Technically, all of the options listed here require you to use a corresponding
    Python library. For example, before you can obtain data from an API, you’ll need
    to install a Python wrapper for the API or use the Requests Python library to
    make HTTP requests to the API directly. Likewise, in order to access data from
    a database, you’ll need to install a connector from within your Python code that
    enables you to access databases of that particular type.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，这里列出的所有选项都需要你使用相应的 Python 库。例如，在你能够从 API 获取数据之前，你需要安装该 API 的 Python 封装器，或者直接使用
    Requests Python 库向 API 发起 HTTP 请求。同样，为了从数据库中访问数据，你需要在你的 Python 代码中安装一个连接器，以便能够访问特定类型的数据库。
- en: While many of these libraries must be downloaded and installed, some libraries
    used to load data are distributed with Python by default. For example, to load
    data from a JSON file, you can take advantage of Python’s built-in `json` package.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多这些库需要下载和安装，但有些用于加载数据的库默认情况下与 Python 一起分发。例如，要从 JSON 文件中加载数据，你可以利用 Python
    内置的 `json` 包。
- en: In Chapters 4 and 5, we’ll take up the data sourcing discussion in greater detail.
    In particular, you’ll learn how to load specific data from different sources into
    data structures in your Python script for further processing. For now, we’ll take
    a brief look at each of the common source types mentioned in the preceding list.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章和第 5 章中，我们将更详细地讨论数据源问题。特别是，你将学习如何将来自不同来源的特定数据加载到 Python 脚本中的数据结构中，以便进一步处理。现在，我们简要看看前面提到的每种常见数据源类型。
- en: APIs
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API
- en: Perhaps the most common way of acquiring data today is via an API (a software
    intermediary that enables two applications to interact with each other). As mentioned,
    to take advantage of an API in Python, you may need to install a wrapper for that
    API in the form of a Python library. The most common way to do this nowadays is
    via the `pip` command.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 也许今天获取数据最常见的方式是通过 API（一个软件中介，它使两个应用程序能够相互交互）。如前所述，要在 Python 中利用 API，你可能需要为该
    API 安装一个 Python 库封装器。如今最常见的做法是通过 `pip` 命令。
- en: Not all APIs have their own Python wrapper, but this doesn’t necessarily mean
    you can’t make calls to them from Python. If an API serves HTTP requests, you
    can interact with that API from Python using the Requests library. This opens
    you up to thousands of APIs that you can use in your Python code to request datasets
    for further processing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有 API 都有自己的 Python 封装器，但这并不意味着你不能通过 Python 向它们发起请求。如果一个 API 提供 HTTP 请求，你可以通过
    Python 的 Requests 库与该 API 进行交互。这使你能够访问成千上万的 API，能够在你的 Python 代码中请求数据集以供进一步处理。
- en: 'When choosing an API for a particular task, you should take the following into
    account:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在为特定任务选择 API 时，你应该考虑以下几点：
- en: Functionality Many APIs provide similar functionalities, so you need to understand
    your precise requirements. For example, many APIs let you conduct a web search
    from within your Python script, but only some allow you to narrow down your search
    results by date of publication.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 功能性 许多 API 提供类似的功能，因此你需要了解你的具体需求。例如，许多 API 允许你在 Python 脚本中进行网页搜索，但只有一些 API 允许你按发布时间来缩小搜索结果范围。
- en: Cost Many APIs allow you to use a so-called *developer key*, which is usually
    provided for free but with certain limitations, such as a limited number of calls
    per day.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成本 许多 API 允许你使用所谓的 *开发者密钥*，通常是免费的，但会有一些限制，例如每天调用次数的限制。
- en: Stability Thanks to the Python Package Index (PyPI) repository ([https://pypi.org](https://pypi.org)),
    anyone can pack an API into a `pip` package and make it publicly available. As
    a result, there’s an API (or several) for virtually any task you can imagine,
    but not all of these are completely reliable. Fortunately, the PyPI repository
    tracks the performance and usage of packages.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稳定性得益于Python包索引（PyPI）仓库（[https://pypi.org](https://pypi.org)），任何人都可以将API打包成`pip`包并公开发布。因此，几乎所有你能想到的任务都有对应的API（或多个API），但并非所有API都完全可靠。幸运的是，PyPI仓库会跟踪包的性能和使用情况。
- en: Documentation Popular APIs usually have a corresponding documentation website,
    allowing you to see all of the API commands with sample usages. As a good model,
    look at the documentation page for the Nasdaq Data Link (aka Quandl) API ([https://docs.data.nasdaq.com/docs/python-time-series](https://docs.data.nasdaq.com/docs/python-time-series)),
    where you’ll find examples of making different time series calls.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档：流行的API通常有相应的文档网站，允许你查看所有API命令及其示例用法。作为一个好的示范，查看Nasdaq Data Link（又名Quandl）API的文档页面（[https://docs.data.nasdaq.com/docs/python-time-series](https://docs.data.nasdaq.com/docs/python-time-series)），在这里你可以找到进行不同时间序列调用的示例。
- en: 'Many APIs return results in one of the following three formats: JSON, XML,
    or CSV. Data in any of these formats can easily be translated into data structures
    that are either built into or commonly used with Python. For example, the Yahoo
    Finance API retrieves and analyzes stock data, then returns the information already
    translated into a pandas DataFrame, a widely used structure we’ll discuss in Chapter
    3.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 许多API会以以下三种格式之一返回结果：JSON、XML或CSV。这些格式中的数据可以轻松转换为Python内置或常用的数据结构。例如，Yahoo Finance
    API检索并分析股票数据，然后将信息转换为pandas DataFrame，这是一种我们将在第3章讨论的广泛使用的数据结构。
- en: Web Pages
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网页
- en: Web pages can be static or generated on the fly in response to a user’s interaction,
    in which case they may contain information from many different sources. In either
    case, a program can read a web page and extract parts of it. Called *web scraping*,
    this is quite legal as long as the page is publicly available.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网页可以是静态的，也可以是根据用户的互动动态生成的，在这种情况下，它们可能包含来自多个不同来源的信息。无论是哪种情况，程序都可以读取网页并提取其中的部分内容。这种操作被称为*网页抓取*，只要页面是公开可用的，这种行为是合法的。
- en: 'A typical scraping scenario in Python involves two libraries: Requests and
    BeautifulSoup. Requests fetches the source code of the page, and then BeautifulSoup
    creates a *parse tree* for the page, which is a hierarchical representation of
    the page’s content. You can search the parse tree and extract data from it using
    Pythonic idioms. For example, the following fragment of a parse tree:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中的典型抓取场景涉及两个库：Requests和BeautifulSoup。Requests用于获取页面的源代码，然后BeautifulSoup为页面创建一个*解析树*，它是页面内容的层次化表示。你可以搜索解析树并使用Pythonic的习惯用法从中提取数据。例如，以下是解析树的一个片段：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'can be easily transformed into the following list of items within a `for` loop
    in your Python script:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在Python脚本中的`for`循环中轻松转换为以下项目列表：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is an example of transforming semistructured data into structured data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将半结构化数据转换为结构化数据的一个示例。
- en: Databases
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库
- en: 'Another common source of data is a relational database, a structure that provides
    a mechanism to efficiently store, access, and manipulate your structured data.
    You fetch from or send a portion of data to tables in the database using a Structured
    Query Language (SQL) request. For instance, the following request issued to an
    `employees` table in the database retrieves the list of only those programmers
    who work in the IT department, making it unnecessary to fetch the entire table:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的数据来源是关系型数据库，这是一种提供高效存储、访问和操作结构化数据机制的结构。你可以通过结构化查询语言（SQL）请求，从数据库中的表格中提取数据或将数据发送到表格中。例如，以下请求发往数据库中的`employees`表格，只会检索在IT部门工作的程序员列表，这样就无需提取整个表格：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Python has a built-in database engine, SQLite. Alternatively, you can employ
    any other available database. Before you can access a database, you’ll need to
    install the database client software in your environment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Python有一个内置的数据库引擎SQLite。你也可以使用其他可用的数据库。在访问数据库之前，你需要在环境中安装数据库客户端软件。
- en: 'In addition to the conventional rigidly structured databases, there’s been
    an ever-increasing need in recent years for the ability to store heterogeneous
    and unstructured data in database-like containers. This has led to the rise of
    so-called *NoSQL* (*non-SQL* or *not only SQL*) databases. NoSQL databases use
    flexible data models, allowing you to store large volumes of unstructured data
    using the *key-value* method, where each piece of data can be accessed using an
    associated key. Here’s what our earlier sample financial statement might look
    like if stored in a NoSQL database:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的严格结构化数据库外，近年来对在类数据库容器中存储异构和非结构化数据的需求日益增长。这促使了所谓的 *NoSQL*（*非 SQL* 或 *不仅仅是
    SQL*）数据库的兴起。NoSQL 数据库使用灵活的数据模型，允许你使用 *键值* 方法存储大量非结构化数据，其中每个数据项都可以通过关联的键进行访问。以下是我们早先示例的财务报表，如果存储在
    NoSQL 数据库中，它可能看起来是这样的：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The entire statement is paired with an identifying key, `26`. It might seem
    odd to store the entire statement in a database. Recall, however, that several
    possible records can be extracted from a single statement. Storing the whole statement
    gives us the flexibility to extract different pieces of information at a later
    time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 整个语句与一个标识键 `26` 配对存储。将整个语句存储在数据库中似乎有些奇怪。然而，回想一下，从单个语句中可以提取多个记录。存储整个语句为我们提供了灵活性，可以在稍后提取不同的数据信息。
- en: Files
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件
- en: Files may contain structured, semistructured, and unstructured data. Python’s
    built-in `open()` function allows you to open a file so you can use its data within
    your script. However, depending on the format of the data (for example, CSV, JSON,
    or XML), you may need to import a corresponding library to be able to perform
    read, write, and/or append operations on it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可能包含结构化、半结构化和非结构化的数据。Python 内置的 `open()` 函数允许你打开文件并在脚本中使用其数据。然而，根据数据的格式（例如
    CSV、JSON 或 XML），你可能需要导入相应的库才能执行读取、写入和/或追加操作。
- en: 'Plaintext files don’t require a library to be further processed and are simply
    considered as sequences of lines in Python. As an example, look at the following
    message that a Cisco router might send to a logfile:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 普通文本文件不需要额外的库进行处理，只需在 Python 中将其视为一系列行即可。例如，查看下面一条可能由思科路由器发送到日志文件的消息：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You’ll be able to read this line by line, looking for the required information.
    Thus, if your task is to find messages that include information about CPU utilization
    and extract particular figures from it, your script should recognize the last
    line in the snippet as a message to be selected. In Chapter 2, you’ll see an example
    of how to extract specific information from text data using text processing techniques.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以逐行读取文件，查找所需的信息。因此，如果你的任务是找到包含 CPU 利用率信息的消息并提取其中的特定数字，你的脚本应该能够将该片段的最后一行识别为要选中的消息。在第二章中，你将看到如何使用文本处理技术从文本数据中提取特定信息的示例。
- en: The Data Processing Pipeline
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理管道
- en: 'In this section, we’ll take a conceptual look at the steps involved in data
    processing, also known as the data processing pipeline. The usual steps applied
    to the data are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从概念上了解数据处理的步骤，这也被称为数据处理管道。应用于数据的常见步骤包括：
- en: Acquisition
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取
- en: Cleansing
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗
- en: Transformation
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换
- en: Analysis
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析
- en: Storage
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储
- en: As you’ll see, these steps aren’t always clear-cut. In some applications you’ll
    be able to combine multiple steps into one or omit some steps altogether.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这些步骤并不总是非常明确。在某些应用中，你可以将多个步骤合并为一个，或者完全省略某些步骤。
- en: Acquisition
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取
- en: Before you can do anything with data, you need to acquire it. That’s why data
    acquisition is the first step in any data processing pipeline. In the previous
    section, you learned about the most common types of data sources. Some of those
    sources allow you to load only the required portion of the data in accordance
    with your request.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在你对数据进行任何操作之前，你需要先获取数据。这就是为什么数据采集是任何数据处理管道中的第一步。在上一节中，你已经了解了最常见的数据源类型。部分数据源允许你根据需求仅加载所需的数据部分。
- en: For example, a request to the Yahoo Finance API requires you to specify the
    ticker of a company and a period of time over which to retrieve stock prices for
    that company. Similarly, the News API, which allows you to retrieve news articles,
    can process a number of parameters to narrow down the list of articles being requested,
    including the source and date of publication. Despite these qualifying parameters,
    however, the retrieved list may still need to be filtered further. That is, the
    data may require cleansing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，向 Yahoo Finance API 发出的请求要求你指定公司的股票代码以及要检索该公司股票价格的时间段。类似地，新闻 API 允许你检索新闻文章，并可以处理多个参数以缩小请求的文章列表，包括来源和发布日期。然而，尽管有这些限定参数，检索到的文章列表仍可能需要进一步过滤。也就是说，数据可能需要清理。
- en: Cleansing
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清理
- en: Data cleansing is the process of detecting and correcting corrupt or inaccurate
    data, or removing unnecessary data. In some cases, this step isn’t required, and
    the data being obtained is immediately ready for analysis. For example, the yfinance
    library (a Python wrapper for Yahoo Finance API) returns stock data as a readily
    usable pandas DataFrame object. This usually allows you to skip the cleansing
    and transformation steps and move straight to data analysis.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理是检测和修正损坏或不准确数据的过程，或者移除不必要的数据。在某些情况下，这一步骤是无需执行的，获取的数据可以直接用于分析。例如，yfinance
    库（一个 Python 封装的 Yahoo Finance API）返回的股票数据是一个现成可用的 pandas DataFrame 对象。通常，这让你可以跳过清理和转换步骤，直接进入数据分析。
- en: 'However, if your acquisition tool is a web scraper, the data certainly will
    need cleansing because fragments of HTML markup will probably be included along
    with the payload data, as shown here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你的采集工具是一个网页爬虫，那么数据肯定需要清理，因为 HTML 标记的片段可能会与有效载荷数据一起被包含在内，如下所示：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After cleansing, this text fragment should look like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后，这段文本应该是这样的：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Besides the HTML markup, the scraped text may include other unwanted text,
    as in the following example, where the phrase *A View full text* is simply hyperlink
    text. You might need to open this link to access the text within it:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 HTML 标记外，抓取的文本可能还包括其他不需要的文本，如以下示例中，*A View full text* 只是一个超链接文本。你可能需要打开这个链接才能访问其中的文本：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can also use a data cleansing step to filter out specific entities. After
    requesting a set of articles from the News API, for example, you may need to select
    only those articles in the specified period where the titles include a money or
    percent phrase. This filter can be considered a data cleansing operation because
    the goal is to remove unnecessary data and prepare for the data transformation
    and data analysis operations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用数据清理步骤来过滤特定的实体。例如，在从新闻 API 请求一组文章后，你可能需要选择仅包含标题中带有金钱或百分比短语的指定时期内的文章。这个过滤步骤可以视为数据清理操作，因为它的目的是移除不必要的数据，并为数据转换和数据分析操作做好准备。
- en: Transformation
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换
- en: Data transformation is the process of changing the format or structure of data
    in preparation for analysis. For example, to extract the information from our
    GoodComp unstructured text data as we did in “Structured Data,” you might shred
    it into individual words or *tokens* so that a named entity recognition (NER)
    tool can look for the desired information. In information extraction, a *named
    entity* typically represents a real-world object, such as a person, an organization,
    or a product, that can be identified by a proper noun. There are also named entities
    that represent dates, percentages, financial terms, and more.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换是将数据的格式或结构改变，以便为分析做好准备。例如，为了像在“结构化数据”中那样提取我们从 GoodComp 非结构化文本数据中获得的信息，你可能会将其分解为单独的单词或
    *标记*，以便命名实体识别（NER）工具可以寻找所需的信息。在信息提取中，*命名实体* 通常代表一个现实世界的对象，如一个人、一个组织或一个产品，这些对象可以通过专有名词来识别。还有一些命名实体代表日期、百分比、金融术语等。
- en: 'Many NLP tools can handle this kind of transformation for you automatically.
    After such a transformation, the shredded GoodComp data would look like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 NLP 工具可以自动处理这种类型的转换。在经过这样的转换后，处理过的 GoodComp 数据会是这样的：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Other forms of data transformation are deeper, with text data being converted
    into numerical data. For example, if we’ve gathered a collection of news articles,
    we might transform them by performing *sentiment analysis*, a text processing
    technique that generates a number representing the emotions expressed within a
    text.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其他形式的数据转换更为深入，例如将文本数据转化为数字数据。例如，如果我们收集了一些新闻文章，我们可以通过执行*情感分析*来转换这些文章，情感分析是一种文本处理技术，可以生成一个表示文本中表达的情感的数字。
- en: 'Sentiment analysis can be implemented with tools like SentimentAnalyzer, which
    can be found in the `nltk.sentiment` package. A typical analysis output might
    look like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析可以使用像 SentimentAnalyzer 这样的工具来实现，它可以在`nltk.sentiment`包中找到。典型的分析输出可能如下所示：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Each entry in our dataset now includes a number, such as `0.9313`, representing
    the sentiment expressed within the corresponding article. With the sentiment of
    each article expressed numerically, we can calculate the average sentiment of
    the entire dataset, allowing us to determine the overall sentiment toward an object
    of interest, such as a certain company or product.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中每个条目现在都包含一个数字，例如`0.9313`，表示对应文章中表达的情感。通过将每篇文章的情感转化为数字，我们可以计算整个数据集的平均情感，从而确定对某个感兴趣对象（如某公司或产品）的总体情感。
- en: Analysis
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析
- en: Analysis is the key step in the data processing pipeline. Here you interpret
    the raw data, enabling you to draw conclusions that aren’t immediately apparent.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 分析是数据处理流程中的关键步骤。在此步骤中，您对原始数据进行解读，从而得出那些不立即显现的结论。
- en: 'Continuing with our sentiment analysis example, you might want to study the
    sentiment toward a company over a specified period in relation to that company’s
    stock price. Or you might compare stock market index figures, such as those on
    the S&P 500, with the sentiment expressed in a broad sampling of news articles
    for this same period. The following fragment illustrates what the dataset might
    look like, with S&P 500 data shown alongside the overall sentiment of that day’s
    news:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的情感分析示例，您可能想要研究在某一特定期间内，某公司股票价格与该公司情感的关系。或者，您可能会将股市指数数据（例如标准普尔500指数）与同一时期内广泛采样的新闻文章中表达的情感进行比较。以下片段展示了数据集的可能样子，其中
    S&P 500 数据与当天新闻的整体情感一同呈现：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since both the sentiment figures and stock figures are expressed in numbers,
    you might plot two corresponding graphs on the same plot for visual analysis,
    as illustrated in [Figure 1-1](#figure1-1).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于情感数据和股票数据均以数字形式表示，您可以在同一图表上绘制两个相应的图形进行视觉分析，如[图 1-1](#figure1-1)所示。
- en: '![A line graph uses two different lines to plot different data points on the
    y-axis across the same sequence of days on the x-axis.](image_fi/502208c01/f01001.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![折线图使用两条不同的线在 x 轴上的相同时间序列中绘制不同的数据点。](image_fi/502208c01/f01001.png)'
- en: 'Figure 1-1: An example of visual data analysis'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-1：数据可视化分析示例
- en: Visual analysis is one of the most commonly used and efficient methods for interpreting
    data. We’ll discuss visual analysis in greater detail in Chapter 8.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化分析是最常用和高效的解读数据方法之一。我们将在第八章中更详细地讨论可视化分析。
- en: Storage
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储
- en: In most cases, you’ll need to store the results generated during the data analysis
    process to make them available for later use. Your storage options typically include
    files and databases. The latter is preferable if you anticipate frequent reuse
    of your data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您需要存储在数据分析过程中生成的结果，以便以后使用。您的存储选项通常包括文件和数据库。如果您预计数据将频繁重用，数据库更为理想。
- en: The Pythonic Way
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pythonic 方式
- en: When doing data science with Python, your code is expected to be written in
    a *Pythonic* way, meaning it should be concise and efficient. Pythonic code is
    often associated with the use of *list comprehensions*, which are ways to implement
    useful data processing functionality with a single line of code.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Python 进行数据科学时，您的代码应当以*Pythonic*的方式编写，这意味着代码应该简洁高效。Pythonic 代码通常与使用*列表推导式*相关，列表推导式是一种通过单行代码实现有用数据处理功能的方法。
- en: 'We’ll cover list comprehensions in more detail in Chapter 2, but for now, the
    following quick example illustrates how the Pythonic concept works in practice.
    Say you need to process this multisentence fragment of text:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第二章中更详细地介绍列表推导式，但现在，以下快速示例演示了 Pythonic 概念在实践中的运作方式。假设您需要处理以下多句子文本片段：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Specifically, you need to split the text by sentences, creating a list of individual
    words for each sentence, excluding punctuation symbols. Thanks to Python’s list
    comprehension feature, all of this can be implemented in a single line of code,
    a so-called *one-liner*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你需要按句子拆分文本，为每个句子创建一个单独的单词列表，且不包含标点符号。由于Python的列表推导特性，所有这些都可以在一行代码中实现，这就是所谓的*一行代码*：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `for line in txt` loop ❷ splits the text into sentences and stores those
    sentences in a list. Then the `for w in line` loop ❶ splits each sentence into
    individuals words and stores the words in a list within the larger list. As a
    result, you get the following list of lists:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`for line in txt` 循环 ❷ 将文本拆分为句子，并将这些句子存储在一个列表中。接着，`for w in line` 循环 ❶ 将每个句子拆分成单独的单词，并将这些单词存储在更大列表中的子列表里。最终，你会得到以下的列表嵌套列表：'
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here you’ve managed to accomplish two steps of the data processing pipeline
    within a single line of code: cleansing and transformation. You’ve cleansed the
    data by removing punctuation symbols from the text, and you’ve transformed it
    by separating the words from each other to form a word list for each sentence.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行代码中，你已经成功地完成了数据处理管道的两个步骤：数据清洗和转换。你通过去除文本中的标点符号清洗了数据，并通过将单词彼此分隔，将每个句子转化为一个单词列表。
- en: If you’ve come to Python from another programming language, try implementing
    this task with that language. How many lines of code does it take?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是从其他编程语言转到Python的，可以尝试用那个语言来实现这个任务。需要多少行代码呢？
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: After reading this chapter, you should have a cursory understanding of the main
    categories of data, where data comes from, and how a typical data processing pipeline
    is organized.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你应该对数据的主要类别、数据来源以及典型的数据处理管道的组织方式有一个大致的了解。
- en: 'As you’ve seen, there are three major categories of data: unstructured, structured,
    and semistructured. The raw input material in a data processing pipeline is typically
    unstructured data, which is passed through cleansing and transformation steps
    to turn it into structured or semistructured data that is ready for analysis.
    You also learned about data processing pipelines that use structured or semistructured
    data from the start, acquired from an API or a relational database.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，数据主要分为三大类：非结构化数据、结构化数据和半结构化数据。在数据处理管道中，原始输入材料通常是非结构化数据，经过清洗和转换步骤后，它会变成结构化或半结构化数据，准备好进行分析。你还了解了那些一开始就使用结构化或半结构化数据的数据处理管道，这些数据通常来自API或关系型数据库。
