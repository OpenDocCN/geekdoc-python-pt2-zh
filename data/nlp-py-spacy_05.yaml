- en: '**5'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5'
- en: WORKING WITH WORD VECTORS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用词向量**'
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: Word vectors are the series of real numbers that represent the meanings of natural
    language words. As you learned in [Chapter 1](../Text/ch01.xhtml#ch01), they allow
    machines to understand human language. In this chapter, you’ll use word vectors
    to calculate the semantic similarity of different texts, which will allow you
    to, for example, classify those texts based on the topics they cover.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量是表示自然语言单词意义的一系列实数。正如你在[第1章](../Text/ch01.xhtml#ch01)中学到的，它们使机器能够理解人类语言。在这一章中，你将使用词向量计算不同文本的语义相似度，这将帮助你，例如，根据文本所涉及的主题对这些文本进行分类。
- en: You’ll start by taking a conceptual look at word vectors so you can get an idea
    of how to mathematically calculate the semantic similarity between the words represented
    in the form of vectors. Then you’ll learn how machine learning algorithms generate
    the word vectors implemented in spaCy models. You’ll use spaCy’s *similarity method*,
    which compares the word vectors of container objects to determine the closeness
    of their meanings. You’ll also learn how to use word vectors in practice and perform
    preprocessing steps, such as choosing keywords, to make your operations more efficient.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先从概念上了解词向量，以便你能大致了解如何通过数学计算来评估以向量形式表示的单词之间的语义相似度。接着，你将学习机器学习算法如何生成在spaCy模型中实现的词向量。你将使用spaCy的*相似度方法*，该方法通过比较容器对象的词向量来确定它们意义的相近程度。你还将学习如何在实际操作中使用词向量，并执行一些预处理步骤，如选择关键词，以提高操作效率。
- en: '**Understanding Word Vectors**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**理解词向量**'
- en: When building statistical models, we map words to vectors of real numbers that
    reflect the words’ semantic similarity. You can imagine a word vector space as
    a cloud in which the vectors of words with similar meanings are located nearby.
    For instance, the vector representing the word “potato” should be closer to the
    vector of the word “carrot” than to that of the word “crying.” To generate these
    vectors, we must be able to encode the meaning of these words. There are a few
    approaches to encoding meaning, which we’ll outline in this section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建统计模型时，我们将单词映射到反映其语义相似度的实数向量。你可以把词向量空间想象成一个云，词义相似的单词的向量靠得比较近。例如，表示“土豆”的词向量应该比表示“哭泣”的词向量更靠近表示“胡萝卜”的词向量。为了生成这些向量，我们必须能够编码这些单词的意义。这里有几种编码意义的方法，我们将在本节中介绍。
- en: '***Defining Meaning with Coordinates***'
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***通过坐标定义意义***'
- en: 'One way to generate meaningful word vectors is by assigning an object or category
    from the real world to each coordinate of a word vector. For example, suppose
    you’re generating word vectors for the following words: Rome, Italy, Athens, and
    Greece. The word vectors should mathematically reflect the fact that Rome is the
    capital of Italy and is related to Italy in a way that Athens is not. At the same
    time, they should reflect the fact that Athens and Rome are capital cities, and
    that Greece and Italy are countries. [Table 5-1](../Text/ch05.xhtml#ch05tab01)
    illustrates what this vector space might look like in the form of a matrix.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 生成有意义的词向量的一种方式是为每个词向量的坐标分配来自现实世界的对象或类别。例如，假设你正在为以下单词生成词向量：罗马、意大利、雅典和希腊。这些词向量应当在数学上反映出罗马是意大利的首都，并且与意大利的关系不同于雅典与意大利的关系。同时，它们应当反映出雅典和罗马是首都城市，而希腊和意大利是国家。[表5-1](../Text/ch05.xhtml#ch05tab01)展示了这个向量空间的样子，以矩阵的形式呈现。
- en: '**Table 5-1:** A Simplified Word Vector Space'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**表5-1：** 简化的词向量空间'
- en: '|  | **Country** | **Capital** | **Greek** | **Italian** |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '|  | **国家** | **首都** | **希腊语** | **意大利语** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Italy | 1 | 0 | 0 | 1 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 意大利 | 1 | 0 | 0 | 1 |'
- en: '| Rome | 0 | 1 | 0 | 1 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 罗马 | 0 | 1 | 0 | 1 |'
- en: '| Greece | 1 | 0 | 1 | 0 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 希腊 | 1 | 0 | 1 | 0 |'
- en: '| Athens | 0 | 1 | 1 | 0 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 雅典 | 0 | 1 | 1 | 0 |'
- en: We’ve distributed the meaning of each word between its coordinates in a four-dimensional
    space, representing the categories “Country,” “Capital,” “Greek,” and “Italian.”
    In this simplified example, a coordinate value can be either 1 or 0, indicating
    whether or not a corresponding word belongs to the category.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将每个词的意义分配到四维空间中的坐标上，表示“国家”、“首都”、“希腊语”和“意大利语”这几个类别。在这个简化的例子中，坐标值可以是1或0，表示一个对应的单词是否属于某个类别。
- en: 'Once you have a vector space in which vectors of numbers capture the meaning
    of corresponding words, you can use vector arithmetic on this vector space to
    gain insight into a word’s meaning. To find out which country Athens is the capital
    of, you could use the following equation, where each token stands for its corresponding
    vector and X is an unknown vector:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有一个向量空间，其中数字向量捕捉了相应单词的含义，你就可以在这个向量空间上进行向量运算，以深入了解单词的含义。为了找出雅典是哪个国家的首都，你可以使用以下方程，其中每个符号代表其对应的向量，X是一个未知向量：
- en: Italy - Rome = X - Athens
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 意大利 - 罗马 = X - 雅典
- en: 'This equation expresses an analogy in which X represents the word vector that
    has the same relationship to Athens as Italy has to Rome. To solve for X, we can
    rewrite the equation like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表示一种类比，其中X代表与雅典的关系与意大利与罗马的关系相同的词向量。为了解出X，我们可以将方程重写为：
- en: X = Italy - Rome + Athens
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: X = 意大利 - 罗马 + 雅典
- en: We first subtract the vector Rome from the vector Italy by subtracting the corresponding
    vector elements. Then we add the sum of the resulting vector and the vector Athens.
    [Table 5-2](../Text/ch05.xhtml#ch05tab02) summarizes this calculation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过逐个减去相应的词向量元素，将罗马的向量从意大利的向量中减去。然后我们将得到的向量与雅典的向量相加。[表 5-2](../Text/ch05.xhtml#ch05tab02)总结了这一计算过程。
- en: '**Table 5-2:** Performing a Vector Math Operation on a Word Vector Space'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5-2：** 对词向量空间进行向量运算'
- en: '|  |  | **Country** | **Capital** | **Greek** | **Italian** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **国家** | **首都** | **希腊语** | **意大利语** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ― | Italy | 1 | 0 | 0 | 1 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ― | 意大利 | 1 | 0 | 0 | 1 |'
- en: '| + | Rome | 0 | 1 | 0 | 1 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| + | 罗马 | 0 | 1 | 0 | 1 |'
- en: '|  | Athens | 0 | 1 | 1 | 0 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | 雅典 | 0 | 1 | 1 | 0 |'
- en: '|  | Greece | 1 | 0 | 1 | 0 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | 希腊 | 1 | 0 | 1 | 0 |'
- en: By subtracting the word vector for Rome from the word vector for Italy and then
    adding the word vector for Athens, we get a vector that is equal to the vector
    Greece.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从意大利的词向量中减去罗马的词向量，再加上雅典的词向量，我们得到一个与希腊词向量相等的向量。
- en: '***Using Dimensions to Represent Meaning***'
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用维度表示含义***'
- en: Although the vector space we just created had only four categories, a real-world
    vector space might require tens of thousands. A vector space of this size would
    be impractical for most applications, because it would require a huge word-embedding
    matrix. For example, if you had 10,000 categories and 1,000,000 entities to encode,
    you’d need a 10,000 × 1,000,000 embedding matrix, making operations on it too
    time-consuming. The obvious approach to reducing the size of the embedding matrix
    is to reduce the number of categories in the vector space.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们刚刚创建的向量空间只有四个类别，但现实世界中的向量空间可能需要数万个类别。如此大小的向量空间对于大多数应用来说是不可行的，因为它需要一个庞大的词嵌入矩阵。例如，如果你有10,000个类别和1,000,000个实体需要编码，那么你将需要一个10,000
    × 1,000,000的嵌入矩阵，这使得对它的操作非常耗时。减少嵌入矩阵大小的显而易见的方法是减少向量空间中的类别数量。
- en: Instead of using coordinates to represent all categories, a real-world implementation
    of a word vector space uses the distance between vectors to quantify and categorize
    semantic similarities. The individual dimensions typically don’t have inherent
    meanings. Instead, they represent locations in the vector space, and the distance
    between vectors indicates the similarity of the corresponding words’ meanings.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中实现的词向量空间并不是通过坐标来表示所有类别，而是通过向量之间的距离来量化和分类语义相似性。各个维度通常没有固有的含义。相反，它们表示向量空间中的位置，向量之间的距离表示相应单词含义的相似度。
- en: 'The following is a fragment of the 300-dimensional word vector space extracted
    from the *fastText,* a word vector library, which you can download at *[https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从*fastText*词向量库中提取的300维词向量空间的一部分，你可以在* [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)
    *下载该库：*
- en: compete   -0.0535 -0.0207 0.0574 0.0562 ... -0.0389 -0.0389
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: compete   -0.0535 -0.0207 0.0574 0.0562 ... -0.0389 -0.0389
- en: equations -0.0337 0.2013 -0.1587 0.1499 ...  0.1504 0.1151
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: equations -0.0337 0.2013 -0.1587 0.1499 ...  0.1504 0.1151
- en: Upper     -0.1132 -0.0927 0.1991 -0.0302 ... -0.1209 0.2132
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Upper     -0.1132 -0.0927 0.1991 -0.0302 ... -0.1209 0.2132
- en: mentor     0.0397 0.1639 0.1005 -0.1420 ... -0.2076 -0.0238
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: mentor     0.0397 0.1639 0.1005 -0.1420 ... -0.2076 -0.0238
- en: reviewer  -0.0424 -0.0304 -0.0031 0.0874 ... 0.1403 -0.0258
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: reviewer  -0.0424 -0.0304 -0.0031 0.0874 ... 0.1403 -0.0258
- en: Each line contains a word represented as a vector of real numbers in multidimensional
    space. Graphically, we can represent a 300-dimensional vector space like this
    one with either a 2D or 3D projection. To prepare such a projection, we can use
    first two or three principal coordinates of a vector, respectively. [Figure 5-1](../Text/ch05.xhtml#ch05fig01)
    shows vectors from a 300-dimensional vector space in a 2D projection.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行包含一个作为实数向量表示的词，位于多维空间中。从图形上看，我们可以用二维或三维投影来表示一个 300 维的向量空间。为了准备这种投影，我们可以分别使用向量的前两或前三个主坐标。[图
    5-1](../Text/ch05.xhtml#ch05fig01) 显示了来自 300 维向量空间的向量在二维投影中的表现。
- en: '![image](../Images/fig5-1.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig5-1.jpg)'
- en: '*Figure 5-1: A fragment of a 2D projection of a multidimensional vector space*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5-1：多维向量空间二维投影的一个片段*'
- en: One interesting detail you might notice here is that the lines connecting Greece
    with Athens and Italy with Rome, respectively, are almost parallel. Their lengths
    also look comparable. In practice, this means that if you have three out of the
    above four vectors, you can calculate an approximate location of the missing one,
    since you know where to shift the vector and how far.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到的一个有趣细节是，连接希腊与雅典和意大利与罗马的线条几乎是平行的。它们的长度看起来也很相似。在实际应用中，这意味着如果你有上述四个向量中的三个，你可以计算出缺失的那个的近似位置，因为你知道应该如何移动向量以及移动的距离。
- en: The vectors in the diagram illustrate a country-capital relation, but they could
    easily have another type of relation, such as male-female, verb tense, and so
    on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的向量表示了一个国家与首都之间的关系，但它们也可以表示其他类型的关系，比如男女关系、动词时态等。
- en: '***The Similarity Method***'
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***相似度方法***'
- en: In spaCy, every type of container object has a *similarity method* that allows
    you to calculate a semantic similarity estimate between two container objects
    of any type by comparing their word vectors. To calculate the similarity of spans
    and documents, which don’t have their own word vectors, spaCy averages the word
    vectors of the tokens they contain.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spaCy 中，每种容器对象都有一个*相似度方法*，通过比较它们的词向量，允许你计算两个任意类型容器对象之间的语义相似度估算值。对于没有自己词向量的跨度和文档，spaCy
    会对其包含的标记词向量取平均值以计算相似度。
- en: '**NOTE**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*spaCy’s small models (those whose model size indicator is %sm) don’t include
    word vectors. You can still use the similarity method with these models to compare
    tokens, spans, and documents, but the results won’t be as accurate.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*spaCy 的小型模型（模型大小指示器为 %sm 的那些）不包括词向量。你仍然可以使用相似度方法来比较标记、跨度和文档，但结果的准确性较差。*'
- en: You can calculate the semantic similarity of two container objects even if the
    two objects are different. For example, you can compare a Token object with a
    Span object, a Span object with a Doc object, and so on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 即使两个容器对象不同，你也可以计算它们的语义相似度。例如，你可以将一个 Token 对象与一个 Span 对象进行比较，将一个 Span 对象与一个 Doc
    对象进行比较，等等。
- en: 'The following example computes how similar a Span object is to a Doc object:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例计算了一个 Span 对象与一个 Doc 对象的相似度：
- en: '>>> doc=nlp(''I want a green apple.'')'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc=nlp(''I want a green apple.'')'
- en: '>>> doc.similarity(doc[2:5])'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc.similarity(doc[2:5])'
- en: '0.7305813588233471'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '0.7305813588233471'
- en: This code calculates a semantic similarity estimate between the sentence “I
    want a green apple.” and the phrase “a green apple” derived from this same sentence.
    As you can see, the computed degree of similarity is high enough to consider the
    content of two objects similar (the degree of similarity ranges from 0 to 1).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码计算了句子“I want a green apple.”与从同一句子中提取的短语“a green apple”之间的语义相似度估算值。如你所见，计算出的相似度足够高，可以认为这两个对象的内容相似（相似度的范围从
    0 到 1）。
- en: 'Not surprisingly, the similarity() method returns 1 when you compare an object
    with itself:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，当你将一个对象与它自身进行比较时，similarity() 方法返回 1：
- en: '>>> doc.similarity(doc)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc.similarity(doc)'
- en: '1.0'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: '>>> doc[2:5].similarity(doc[2:5])'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc[2:5].similarity(doc[2:5])'
- en: '1.0'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: 'You can also compare a Doc object with a slice from another Doc object:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将一个 Doc 对象与另一个 Doc 对象的切片进行比较：
- en: '>>> doc2=nlp(''I like red oranges.'')'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc2=nlp(''I like red oranges.'')'
- en: '>>> doc2.similarity(doc[2:5])'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc2.similarity(doc[2:5])'
- en: '0.28546574467463354'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '0.28546574467463354'
- en: Here, we compare the sentence “I like red oranges.” stored in doc2 with the
    span “a green apple” extracted from doc. In this case, the degree of similarity
    is not so high this time. Yes, oranges and apples are both fruits (the similarity
    method recognizes this fact), but the verbs “want” and “like” express different
    states of being.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将存储在 doc2 中的句子“I like red oranges.”与从 doc 中提取的短语“a green apple”进行比较。在这种情况下，相似度并不高。是的，橙子和苹果都是水果（相似度方法识别了这一事实），但是动词“want”和“like”表达了不同的状态。
- en: You can also compare two tokens. In the following example, we compare the Token
    object “oranges” to a Span object containing a single token “apple.”
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以比较两个词项。在以下示例中，我们将 Token 对象“oranges”与包含单一词项“apple”的 Span 对象进行比较。
- en: '>>> token = doc2[3:4][0]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> token = doc2[3:4][0]'
- en: '>>> token'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> token'
- en: oranges
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: oranges
- en: '>>> token.similarity(doc[4:5])'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> token.similarity(doc[4:5])'
- en: '0.3707084280155993'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '0.3707084280155993'
- en: First, we explicitly convert the Span object containing a single token “oranges”
    to a Token object by referring to the first element in the span. Then we calculate
    how similar it is to the span “apple.”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过引用该跨度中的第一个元素，将包含单一词项“oranges”的 Span 对象显式转换为 Token 对象。然后，我们计算它与短语“apple”之间的相似度。
- en: The similarity() method can recognize words that belong to the same or similar
    categories and that often appear in related contexts, showing a high level of
    similarity for such words.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: similarity() 方法可以识别属于相同或相似类别且经常出现在相关语境中的单词，对这些单词显示出较高的相似度。
- en: '***Choosing Keywords for Semantic Similarity Calculations***'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***选择语义相似度计算的关键词***'
- en: 'The similarity method will calculate semantic similarity for you, but for the results
    of that calculation to be useful, you need to choose the right keywords to compare.
    To understand why, consider the following text snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度方法将为你计算语义相似度，但为了使计算结果有用，你需要选择合适的关键词进行比较。为了理解为什么，考虑以下文本片段：
- en: Redwoods are the tallest trees in the world. They are most common in the coastal
    forests of California.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 红杉是世界上最高的树木。它们在加利福尼亚的沿海森林中最为常见。
- en: We might classify this text in a variety of ways depending on the set of categories
    we want to use. If, for example, we’re searching for texts about highest plants
    on the planet, the phrases “tallest trees” and “in the world” will be the key
    ones. Comparing these phrases with the search phrases “highest plants” and “on
    the planet” should show a high level of the semantic similarity. We can do this
    by extracting noun chunks using a Doc object’s doc.noun_chunk property and then
    checking the similarity of those noun chunks and the search phrases using the
    similarity method.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能根据想要使用的类别集以多种方式对这段文本进行分类。例如，如果我们在寻找有关地球上最高植物的文本，“tallest trees”和“in the
    world”将是关键短语。将这些短语与搜索短语“highest plants”和“on the planet”进行比较，应该会显示出较高的语义相似度。我们可以通过使用
    Doc 对象的 doc.noun_chunk 属性提取名词短语，然后通过相似度方法检查这些名词短语与搜索短语的相似度来做到这一点。
- en: 'But if we’re looking for texts about places in the world, “California” will
    be the keyword. Of course, we don’t know in advance which geopolitical name might
    occur in a text: it could be California or, say, Amazonia. But whatever it is,
    it should be semantically similar to a word like “geography,” which we can compare
    with the text’s other nouns (or, even better, with its named entities only). If
    we’re able to determine that there’s a high level of similarity, we can assume
    that the named entity in question represents a geopolitical name. (We might also
    extract the token.ent_type attribute of a Token object to do this, as described
    in [Chapter 2](../Text/ch02.xhtml#ch02). But we wouldn’t be able to use named
    entity recognition to check the similarity of words that aren’t named entities,
    say, fruits.)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们在寻找关于世界各地的文本，那么“California”将是关键词。当然，我们无法预先知道文本中可能出现哪个地名：它可能是加利福尼亚，或者比如亚马逊。无论是什么，它应该在语义上与像“地理”这样的词相似，我们可以将其与文本中的其他名词（或者更好地，只与其命名实体）进行比较。如果我们能够确定存在较高的相似度，那么我们可以假设该命名实体代表的是一个地名。（我们还可以提取
    Token 对象的 token.ent_type 属性来做到这一点，正如在[第 2 章](../Text/ch02.xhtml#ch02)中所描述的。但是，我们无法使用命名实体识别来检查那些不是命名实体的单词的相似度，比如水果。）
- en: '**Installing Word Vectors**'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**安装词向量**'
- en: If a spaCy model is installed in your Python environment, you can start using
    word vectors right away. You can also install a third-party word vector package.
    Various statistical models use different word vectors, so the results of your
    operations will differ slightly based on the model you’re using. You can try several
    models to determine which one works better in your particular application.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在你的Python环境中安装了spaCy模型，你可以立即开始使用词向量。你也可以安装第三方词向量包。不同的统计模型使用不同的词向量，因此你执行操作时的结果会根据所使用的模型略有不同。你可以尝试多个模型，以确定哪一个在你的特定应用中效果更好。
- en: '***Taking Advantage of Word Vectors That Come with spaCy Models***'
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***利用spaCy模型中提供的词向量***'
- en: Word vectors come as part of many spaCy models. For example, en_vectors_web_lg
    includes more than one million unique word vectors defined on a 300-dimensional
    vector space. Check out *[https://github.com/explosion/spacy-models/releases/](https://github.com/explosion/spacy-models/releases/)*
    for details on a particular model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量是许多spaCy模型的一部分。例如，en_vectors_web_lg包含超过一百万个唯一的词向量，定义在300维向量空间中。有关特定模型的详细信息，请查看*[https://github.com/explosion/spacy-models/releases/](https://github.com/explosion/spacy-models/releases/)*。
- en: Typically, small models (those whose names end with sm) don’t contain word vectors.
    Instead, they come with context-sensitive *tensors*, which still allow you to
    work with the similarity methods to compare tokens, spans, and documents—although
    at the expense of accuracy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，小型模型（名称以sm结尾）不包含词向量。相反，它们提供上下文敏感的*tensors*，这些仍然允许你使用相似性方法比较标记、跨度和文档——尽管牺牲了准确性。
- en: To follow along with the examples given in this chapter, you can use any spaCy
    model, even small ones. But you’ll get more accurate results if you install a
    larger model. For details on how to install a spaCy model, refer to “[Installing
    Statistical Models for spaCy](../Text/ch02.xhtml#lev14)” on [page 16](../Text/ch02.xhtml#page_16).
    Note that you might have more than one model installed in your environment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本章中的示例，你可以使用任何spaCy模型，甚至是小型模型。但是，如果你安装更大的模型，你将获得更准确的结果。有关如何安装spaCy模型的详细信息，请参阅“[安装spaCy统计模型](../Text/ch02.xhtml#lev14)”的[第16页](../Text/ch02.xhtml#page_16)。请注意，你的环境中可能安装了多个模型。
- en: '***Using Third-Party Word Vectors***'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用第三方词向量***'
- en: You can also use third-party packages of word vectors with spaCy. You can check
    whether a third-party will work better for your application than native word vectors
    available in a spaCy model. For example, you can use a fastText pretrained model
    with English word vectors, which you can download at *[https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)*.
    The name of a package will identify the size of the package and word vectors,
    and the kind of data used to train the word vectors. For example, *wiki-news-300d-1M.vec.zip*
    indicates that it contains one million 300-dimensional word vectors trained on
    Wikipedia and the *statmt.org* news dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用第三方的词向量包与spaCy一起使用。你可以检查第三方是否比spaCy模型中提供的本地词向量更适合你的应用。例如，你可以使用包含英语词向量的fastText预训练模型，这些词向量可以从*[https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)*下载。包的名称将指示包的大小、词向量的维度以及用于训练词向量的数据类型。例如，*wiki-news-300d-1M.vec.zip*表示它包含一百万个300维的词向量，这些词向量是基于维基百科和*statmt.org*新闻数据集进行训练的。
- en: 'After downloading a package, unzip it, and then create a new model from the
    vectors in the package that you can use with spaCy. To do this, navigate to the
    folder where you saved the package, and then use the init-model command line utility,
    like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下载包后，解压缩它，然后从包中的向量创建一个新的模型，可以与spaCy一起使用。为此，请导航到保存包的文件夹，然后使用init-model命令行工具，如下所示：
- en: $ python -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc wiki-news-300d-1M.vec
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: $ python -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc wiki-news-300d-1M.vec
- en: 'The command converts the vectors taken from the *wiki-news-300d-1M.vec* file
    into spaCy’s format and creates the new model directory */tmp/en_vectors_wiki_lg*
    for them. If everything goes well, you’ll see the following messages:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将从*wiki-news-300d-1M.vec*文件中提取的向量转换为spaCy的格式，并为它们创建新的模型目录*/tmp/en_vectors_wiki_lg*。如果一切顺利，你将看到以下信息：
- en: Reading vectors from wiki-news-300d-1M.vec
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从wiki-news-300d-1M.vec读取向量
- en: Open loc
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 打开位置
- en: 999994it [02:05, 7968.84it/s]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 999994it [02:05, 7968.84it/s]
- en: Creating model...
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正在创建模型...
- en: 0it [00:00, ?it/s]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 0it [00:00, ?it/s]
- en: Successfully compiled vocab
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 成功编译词汇
- en: 999731 entries, 999994 vectors
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 999731条条目，999994个向量
- en: 'Once you’ve created the model, you can load it like a regular spaCy model:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了模型，你就可以像常规的spaCy模型一样加载它：
- en: nlp = spacy.load('/tmp/en_vectors_wiki_lg')
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('/tmp/en_vectors_wiki_lg')
- en: 'Then you can create a Doc object as you normally would:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以像平常一样创建一个Doc对象：
- en: doc = nlp(u'Hi there!')
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'Hi there!')
- en: 'Unlike a regular spaCy model, a third-party model converted for use in spaCy
    might not support some of spaCy’s operations against text contained in a doc object.
    For example, if you try to shred a doc into sentences using doc.sents, you’ll
    get the following error: ValueError: [E030] Sentence boundaries unset...'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '与常规的spaCy模型不同，转换为spaCy使用的第三方模型可能不支持对doc对象中包含的文本进行某些spaCy操作。例如，如果你尝试使用doc.sents将一个doc分解成句子，你会遇到以下错误：ValueError:
    [E030] 句子边界未设置...'
- en: '**Comparing spaCy Objects**'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**比较spaCy对象**'
- en: Let’s use word vectors to calculate the similarity of container objects, the
    most common task for which we use word vectors. In the rest of this chapter, we’ll
    explore some scenarios in which you’d want to determine the semantic similarity
    of linguistic units.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用词向量来计算容器对象的相似性，这是我们使用词向量最常见的任务。在本章的其余部分，我们将探索一些你可能想要确定语言单元语义相似性的场景。
- en: '***Using Semantic Similarity for Categorization Tasks***'
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用语义相似性进行分类任务***'
- en: 'Determining two objects’ syntactic similarity can help you sort texts into
    categories or pick out only the relevant texts. For example, suppose you’re sorting
    through user comments posted to a website to find all the comments related to
    the word “fruits.” Let’s say you have the following utterances to evaluate:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 确定两个对象的句法相似性可以帮助你将文本分类，或者挑选出相关的文本。例如，假设你正在整理网站上发布的用户评论，找出所有与“水果”相关的评论。假设你有以下几个需要评估的句子：
- en: I want to buy this beautiful book at the end of the week.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在本周末买这本漂亮的书。
- en: Sales of citrus have increased over the last year.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 柑橘类的销售在过去一年有所增加。
- en: How much do you know about this type of tree?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你对这种树了解多少？
- en: You can easily recognize that only the second sentence is directly related to
    fruits because it contains the word “citrus.” But to pick out this sentence programmatically,
    you’ll have to compare the word vector for the word “fruits” with word vectors
    in the sample sentences.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以很容易地识别出只有第二个句子与水果直接相关，因为它包含了单词“柑橘”。但要通过编程挑选出这个句子，你必须将“水果”这个词的词向量与样本文本中的词向量进行比较。
- en: 'Let’s start with the simplest but least successful way of doing this task:
    comparing “fruits” to each of the sentences. As stated earlier, spaCy determines
    the similarity of two container objects by comparing their corresponding word
    vectors. To compare a single token with an entire sentence, spaCy averages the
    sentence’s word vectors to generate an entirely new vector. The following script
    compares each of the preceding sentence samples with the word “fruits”:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单但最不成功的方式开始做这个任务：将“水果”与每个句子进行比较。如前所述，spaCy通过比较两个容器对象的对应词向量来确定它们的相似性。为了将单个词语与整个句子进行比较，spaCy会对句子的词向量求平均，从而生成一个全新的向量。以下脚本将每个前面的句子样本与词“水果”进行比较：
- en: import spacy
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: ➊ token = nlp(u'fruits')[0]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ token = nlp(u'fruits')[0]
- en: ➋ doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales
    of
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ doc = nlp(u'我想在本周末买这本漂亮的书。柑橘类的销售在过去一年有所增加。')
- en: citrus have increased over the last year. How much do you know about this type
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 柑橘类的销售在过去一年有所增加。你对这种类型的树了解多少
- en: of tree?')
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 的树？')
- en: '➌ for sent in doc.sents:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ for sent in doc.sents:'
- en: print(sent.text)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: print(sent.text)
- en: ➍ print('similarity to', token.text, 'is', token.similarity(sent),'\n')
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ print('与', token.text, '的相似度是', token.similarity(sent),'\n')
- en: We first create a Token object for the word “fruits” ➊. Then we apply the pipeline
    to the sentences we’re categorizing, creating a single Doc object to hold all
    of them ➋. We shred the doc into sentences ➌, and then print each of the sentences
    and their semantic similarity to the token “fruits,” which we acquire using the
    token object’s similarity method ➍.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为单词“水果”创建一个Token对象 ➊。然后，我们将管道应用于我们正在分类的句子，创建一个单一的Doc对象来保存它们 ➋。我们将文档分解成句子
    ➌，然后打印每个句子及其与词“水果”的语义相似度，我们通过使用Token对象的相似性方法来获取这一相似度 ➍。
- en: 'The output should look something like this (although the actual figures will
    depend on the model you use):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是这样的（尽管实际的数字将取决于你使用的模型）：
- en: I want to buy this beautiful book at the end of the week.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在本周末买这本漂亮的书。
- en: similarity to fruits is 0.06307832979619851
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与水果的相似度是0.06307832979619851
- en: Sales of citrus have increased over the last year.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 柑橘的销量在过去一年中有所增加。
- en: similarity to fruits is 0.2712141843864381
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与水果的相似度是0.2712141843864381
- en: How much do you know about this type of tree?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你对这种类型的树了解多少？
- en: similarity to fruits is 0.24646341651210604
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与水果的相似度是0.24646341651210604
- en: The degree of similarity between the word “fruits” and the first sentence is
    very small, indicating that the sentence has nothing to do with fruits. The second
    sentence—the one that includes the word “citrus”—is the most closely related to
    “fruits,” meaning the script correctly identified the relevant sentence.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: “水果”这个词与第一句话的相似度非常小，表明这句话与水果没有关系。第二句话——包含“柑橘”这个词的句子——与“水果”最为相关，这意味着脚本正确地识别出了相关句子。
- en: But notice that the script also identified the third sentence as being somehow
    related to fruits, probably because it includes the word “tree,” and fruits grow
    on trees. It would be naive to think that the similarity measuring algorithm “knows”
    that orange and citrus are fruits. All it knows is that these words (“orange”
    and “citrus”) often share the same context with word “fruit” and therefore they’ve
    been put close to it in the vector space. But the word “tree” can also often be
    found in the same context as the word “fruit.” For example, the phrase “fruit
    tree” is not uncommon. For that reason the level of similarity calculated between
    “fruits” (or “fruit” as its lemma) and “tree” is close to the result we got for
    “citrus” and “fruits.”
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 但是请注意，脚本也将第三句话识别为与水果相关，可能是因为它包含了“树”这个词，而水果通常长在树上。如果认为相似度测量算法“知道”橙子和柑橘是水果，那就太天真了。它所知道的仅仅是这些词（“橙子”和“柑橘”）通常与“水果”共享相同的上下文，因此它们在向量空间中被放得很接近。但“树”这个词也经常出现在与“水果”相关的上下文中。例如，“果树”这个词组并不少见。基于这个原因，计算得出的“水果”（或其词元“果”）与“树”的相似度值与“柑橘”和“水果”的相似度结果接近。
- en: There’s another problem with this approach to categorizing texts. In practice,
    of course, you might sometimes have to deal with texts that are much larger than
    the sample texts used in this section. If the text you’re averaging is very large,
    the most important words might have little to no effect on the syntactic similarity
    value.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类文本的方法还有另一个问题。实际上，当然，你有时可能需要处理比本节中使用的示例文本大得多的文本。如果你正在计算的文本非常大，那么最重要的词汇可能对句法相似度值几乎没有影响。
- en: To get more accurate results from the similarity method, we’ll need to perform
    some preparations on a text. Let’s look at how we can improve the script.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从相似度方法中获得更准确的结果，我们需要对文本进行一些准备工作。让我们看看如何改进脚本。
- en: '***Extracting Nouns as a Preprocessing Step***'
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***提取名词作为预处理步骤***'
- en: A better technique for performing categorization would be to extract the most
    important words and compare only those. Preparing a text for processing in this
    way is called *preprocessing*, and it can help make your NLP operations more successful.
    For example, instead of comparing the word vectors for the entire objects, you
    could try comparing the word vectors for certain parts of speech. In most cases,
    you’ll focus on nouns—whether they act as subjects, direct objects, or indirect
    objects—to recognize the meaning conveyed in the text in which they occur. For
    example, in the sentence “Nearly all wild lions live in Africa,” you’ll probably
    focus on lions, Africa, or lions in Africa. Similarly, in the sentence about fruits,
    we focused on picking out the noun “citrus.” In other cases, you’ll need other
    words, like verbs, to decide what a text is about. Suppose you run an agricultural
    produce business and must classify offers from those who produce, process, and
    sell farm products. You often see sentences like, “We grow vegetables,” or “We
    take tomatoes for processing.” In this example, the verbs are just as important
    as nouns in the utterances in the previous examples.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 执行分类的更好方法是提取最重要的词汇，只比较这些词。以这种方式准备文本进行处理叫做*预处理*，它可以帮助提高你的自然语言处理操作的成功率。例如，你可以尝试不比较整个对象的词向量，而是比较某些词性（如名词）的词向量。在大多数情况下，你会集中注意力在名词上——无论它们作为主语、直接宾语还是间接宾语——来识别它们所传达的意义。例如，在句子“几乎所有野生狮子生活在非洲”中，你可能会专注于“狮子”、“非洲”或“狮子在非洲”这些词。类似地，在关于水果的句子中，我们集中精力挑选出名词“柑橘”。在其他情况下，你可能需要其他词汇，如动词，来判断一篇文章的主题是什么。假设你经营着一个农产品业务，必须对来自种植、加工和销售农产品的供应商的报价进行分类。你经常会看到像“我们种植蔬菜”或“我们将西红柿进行加工”这样的句子。在这个例子中，动词与前面例子中的名词一样重要。
- en: 'Let’s modify the script on [page 70](../Text/ch05.xhtml#page_70). Instead of
    comparing “fruits” to entire sentences, we’ll compare it to the sentences’ nouns
    only:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来修改[第70页](../Text/ch05.xhtml#page_70)的脚本。我们不再将“fruits”与整个句子进行比较，而是仅与句子中的名词进行比较：
- en: import spacy
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: ➊ token = nlp(u'fruits')[0]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ token = nlp(u'fruits')[0]
- en: doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales
    of
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales
    of
- en: citrus have increased over the last year. How much do you know about this type
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 柑橘类水果的销量在过去一年有所增加。你对这种类型了解多少？
- en: of tree?')
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 的树？')
- en: similarity = {}
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: similarity = {}
- en: '➋ for i, sent in enumerate(doc.sents):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ for i, sent in enumerate(doc.sents):'
- en: ➌ noun_span_list = [sent[j].text for j in range(len(sent)) if sent[j].pos_
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ noun_span_list = [sent[j].text for j in range(len(sent)) if sent[j].pos_
- en: == 'NOUN']
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: == 'NOUN']
- en: ➍ noun_span_str = ' '.join(noun_span_list)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ noun_span_str = ' '.join(noun_span_list)
- en: ➎ noun_span_doc = nlp(noun_span_str)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ noun_span_doc = nlp(noun_span_str)
- en: ➏ similarity.update({i:token.similarity(noun_span_doc)})
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ similarity.update({i:token.similarity(noun_span_doc)})
- en: print(similarity)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: print(similarity)
- en: We start by defining the token “fruits,” which is then used for a series of
    comparisons ➊. Iterating over the tokens in each sentence ➋, we extract the nouns
    and store them in a Python list ➌. Next, we join the nouns in the list into a
    plain string ➍, and then convert that string into a Doc object ➎. We then compare
    this Doc with the token “fruits” to determine their degree of semantic similarity.
    We store each token’s syntactic similarity value in a Python dictionary ➏, which
    we finally print out.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了“fruits”这个标记，然后用它进行一系列比较 ➊。通过遍历每个句子中的标记 ➋，我们提取名词并将它们存储在一个 Python 列表中
    ➌。接着，我们将列表中的名词合并成一个普通字符串 ➍，然后将该字符串转换为一个 Doc 对象 ➎。然后我们将这个 Doc 与标记“fruits”进行比较，以确定它们的语义相似度。我们将每个标记的句法相似度值存储在一个
    Python 字典中 ➏，最后打印出来。
- en: 'The script’s output should look something like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的输出应类似如下：
- en: '{0: 0.17012682516221458, 1: 0.5063824302533686, 2: 0.6277196645922878}'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '{0: 0.17012682516221458, 1: 0.5063824302533686, 2: 0.6277196645922878}'
- en: 'If you compare these figures with the results of the previous script, you’ll
    notice that this time the level of the similarity with the word “fruits” is higher
    for each sentence. But the overall results look similar: the similarity of the
    first sentence is the lowest, whereas the similarity of the other two are much
    higher.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将这些数字与之前脚本的结果进行比较，您会注意到，这次每个句子与“fruits”一词的相似度都更高了。但总体结果相似：第一句的相似度最低，而另外两句的相似度则明显更高。
- en: '***Try This***'
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: In the previous example, comparing “fruits” to nouns only, you improved the
    results of the similarity calculations by taking into account only the words that
    matter most (nouns, in this case). You compared the word “fruits” with all the
    nouns extracted from each sentence, combined. Taking it one step further, you
    could look at how each of these nouns is semantically related to the word “fruits”
    to find out which one shows the highest level of similarity. This can be useful
    in evaluating the overall similarity of the document to the word “fruits.” To
    accomplish this, you need to modify the previous script so it determines the similarity
    between the token “fruits” and each of the nouns in a sentence, finding the noun
    that shows the highest level of similarity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，您仅将“fruits”与名词进行比较，通过只考虑最重要的单词（在这种情况下是名词），您提高了相似度计算的结果。您将“fruits”与每个句子提取出来的所有名词进行比较。进一步来说，您可以查看这些名词与“fruits”之间的语义关系，找出哪个名词与“fruits”最为相似。这对于评估文档与“fruits”一词的整体相似度是非常有用的。为此，您需要修改之前的脚本，以便它能确定“fruits”与每个句子中的名词之间的相似度，并找到与之最相似的名词。
- en: '***Extracting and Comparing Named Entities***'
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***提取和比较命名实体***'
- en: 'In some cases, instead of extracting every noun from the texts you’re comparing,
    you might want to extract a certain kind of noun only, such as named entities.
    Let’s say you’re comparing the following texts:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能不想从比较的文本中提取每一个名词，而是只提取某种特定类型的名词，比如命名实体。假设您正在比较以下文本：
- en: “Google Search, often referred to as simply Google, is the most used search
    engine nowadays. It handles a huge number of searches each day.”
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: “谷歌搜索，通常简称为谷歌，是如今最常用的搜索引擎。它每天处理大量的搜索。”
- en: “Microsoft Windows is a family of proprietary operating systems developed and
    sold by Microsoft. The company also produces a wide range of other software for
    desktops and servers.”
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: “Microsoft Windows是一系列由Microsoft开发和销售的专有操作系统。该公司还生产其他广泛的桌面和服务器软件。”
- en: “Titicaca is a large, deep, mountain lake in the Andes. It is known as the highest
    navigable lake in the world.”
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: “Titicaca是安第斯山脉中的一个大而深的湖泊。它被认为是世界上最高的可通航湖泊。”
- en: 'Ideally, your script should recognize that the first two texts are about large
    technology companies, but the third text isn’t. But comparing all the nouns in
    this text wouldn’t be very helpful, because many of them, such as “number” in
    the first sentence, aren’t relevant to the context. The differences between the
    sentences involve the following words: “Google,” “Search,” “Microsoft,” “Windows,”
    “Titicaca,” and “Andes.” spaCy recognizes all of these as named entities, which
    makes it a breeze to find and extract them from a text, as illustrated in the
    following script:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，您的脚本应识别出前两篇文本讨论的是大型科技公司，而第三篇文本则不是。但比较该文本中的所有名词并没有什么帮助，因为许多词汇，如第一句中的“number”，与上下文无关。句子之间的差异体现在以下词汇：“Google”，“Search”，“Microsoft”，“Windows”，“Titicaca”和“Andes”。spaCy将这些词都识别为命名实体，这使得从文本中提取它们变得轻松，如以下脚本所示：
- en: import spacy
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 导入spacy
- en: nlp = spacy.load('en')
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: '#first sample text'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#第一个示例文本'
- en: doc1 = nlp(u'Google Search, often referred to as simply Google, is the most
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: doc1 = nlp(u'Google Search, 通常简称为Google，是最常用的搜索引擎。')
- en: used search engine nowadays. It handles a huge number of searches each day.')
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当前最常用的搜索引擎。每天处理大量的搜索请求。')
- en: '#second sample text'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#第二个示例文本'
- en: doc2 = nlp(u'Microsoft Windows is a family of proprietary operating systems
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: doc2 = nlp(u'Microsoft Windows是一系列由Microsoft开发和销售的专有操作系统')
- en: developed and sold by Microsoft. The company also produces a wide range of
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由Microsoft开发和销售。该公司还生产一系列广泛的
- en: other software for desktops and servers.')
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其他桌面和服务器软件。')
- en: '#third sample text'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#第三个示例文本'
- en: doc3 = nlp(u"Titicaca is a large, deep, mountain lake in the Andes. It is
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: doc3 = nlp(u"Titicaca是安第斯山脉中的一个大而深的湖泊。它被认为是世界上最高的可通航湖泊。")
- en: known as the highest navigable lake in the world.")
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 被认为是世界上最高的可通航湖泊。”
- en: ➊ docs = [doc1,doc2,doc3]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ docs = [doc1, doc2, doc3]
- en: ➋ spans = {}
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ spans = {}
- en: '➌ for j,doc in enumerate(docs):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ for j, doc in enumerate(docs):'
- en: ➍ named_entity_span = [doc[i].text for i in range(len(doc)) if
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ named_entity_span = [doc[i].text for i in range(len(doc)) if
- en: doc[i].ent_type != 0]
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: doc[i].ent_type != 0]
- en: ➎ print(named_entity_span)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ print(named_entity_span)
- en: ➏ named_entity_span = ' '.join(named_entity_span)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ named_entity_span = ' '.join(named_entity_span)
- en: ➐ named_entity_span = nlp(named_entity_span)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ named_entity_span = nlp(named_entity_span)
- en: ➑ spans.update({j:named_entity_span})
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ spans.update({j:named_entity_span})
- en: We group the Docs with the sample texts into a list to make it possible to iterate
    over them in a loop ➊. We define a Python dictionary to store the keywords for
    each text ➋. In a loop iterating over the Docs ➌, we extract these keywords in
    a separate list for each text, selecting only the words marked as named entities
    ➍. Then we print out the list to see what it contains ➎. Next, we convert this
    list into a plain string ➏ to which we then apply the pipeline, converting it
    to a Doc object ➐. We then append the Doc to the spans dictionary defined earlier
    ➑.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Docs与示例文本分组到一个列表中，以便可以在循环中迭代 ➊。我们定义一个Python字典来存储每个文本的关键字 ➋。在一个遍历Docs的循环中
    ➌，我们为每个文本提取这些关键字，单独列出仅标记为命名实体的单词 ➍。然后，我们打印出该列表，以查看它包含什么内容 ➎。接下来，我们将此列表转换为普通字符串
    ➏，然后应用管道，将其转换为Doc对象 ➐。然后，我们将Doc追加到之前定义的spans字典 ➑。
- en: 'The script should produce the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本应生成以下输出：
- en: '[''Google'', ''Search'', ''Google'']'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[''Google'', ''Search'', ''Google'']'
- en: '[''Microsoft'', ''Windows'', ''Microsoft'']'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[''Microsoft'', ''Windows'', ''Microsoft'']'
- en: '[''Titicaca'', ''Andes'']'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[''Titicaca'', ''Andes'']'
- en: Now we can see the words in each text whose vectors we’ll compare.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到每个文本中我们将比较其向量的单词。
- en: 'Next, we call similarity() on these spans and print the results:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调用这些spans的similarity()方法并打印结果：
- en: print('doc1 is similar to doc2:',spans[0].similarity(spans[1]))
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: print('doc1与doc2相似度：', spans[0].similarity(spans[1]))
- en: print('doc1 is similar to doc3:',spans[0].similarity(spans[2]))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: print('doc1与doc3相似度：', spans[0].similarity(spans[2]))
- en: print('doc2 is similar to doc3:',spans[1].similarity(spans[2]))
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: print('doc2与doc3相似度：', spans[1].similarity(spans[2]))
- en: 'This time the output should look as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这次的输出应如下所示：
- en: 'doc1 is similar to doc2: 0.7864886939527678'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: doc1与doc2相似度：0.7864886939527678
- en: 'doc1 is similar to doc3: 0.6797676349647936'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: doc1与doc3相似度：0.6797676349647936
- en: 'doc2 is similar to doc3: 0.6621659567003596'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: doc2与doc3相似度：0.6621659567003596
- en: These figures indicate that the highest level of similarity exists between the
    first and second texts, which are both about American IT companies. How can word
    vectors “know” about this fact? They probably know because the words “Google”
    and “Microsoft” have been found more often in the same texts of the training text
    corpus rather than in the company of the words “Titicaca” and “Andes.”
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据表明，第一篇和第二篇文本之间的相似性最高，它们都涉及美国IT公司。词向量是如何“知道”这一事实的呢？它们可能知道，因为“Google”和“Microsoft”这两个词在训练文本语料库中的相同文本中出现得更频繁，而不是和“湖泊”（Titicaca）和“安第斯山脉”（Andes）这些词一起出现。
- en: '**Summary**'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you worked with word vectors, which are vectors of real numbers
    that represent the meanings of words. These representations let you use math to
    determine the semantic similarity of linguistic units, a useful task for categorizing
    texts.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你使用了词向量，它们是表示单词意义的实数向量。这些表示方式让你能够利用数学来确定语言单位的语义相似性，这是一个有用的任务，用于文本分类。
- en: But the math approach might not work as well when you’re trying to determine
    the similarity of two texts without applying any preliminary steps to those texts.
    By applying preprocessing, you can reduce the text to the words that are most
    important in figuring out what the text is about. In particularly large texts,
    you might pick out the named entities found in it, because they most likely best
    describe the text’s category.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当你试图在没有对文本进行任何预处理的情况下确定两篇文本的相似性时，数学方法可能效果不佳。通过应用预处理，你可以将文本简化为最重要的单词，这些单词有助于了解文本的主题。在特别大的文本中，你可能会挑选出其中的命名实体，因为它们最可能最好地描述文本的类别。
