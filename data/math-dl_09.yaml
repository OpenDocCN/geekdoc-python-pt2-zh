- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9'
- en: DATA FLOW IN NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的数据流**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In this chapter, I’ll present how data flows through a trained neural network.
    In other words, we’ll look at how to go from an input vector or tensor to the
    output, and the form the data takes along the way. If you’re already familiar
    with how neural networks function, great, but if not, walking through how data
    flows from layer to layer will help you build an understanding of the processes
    involved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我将展示数据是如何在训练好的神经网络中流动的。换句话说，我们将查看如何从输入向量或张量转换到输出，以及数据在过程中所呈现的形式。如果你已经熟悉神经网络的运作原理，那就太好了；如果没有，跟随数据从一层流向另一层的过程，将帮助你建立对这些过程的理解。
- en: First, we’ll look at how we represent data in two different kinds of networks.
    Then, we’ll work through a traditional feedforward network to give ourselves a
    solid foundation. We’ll see just how compact inference with a neural network can
    be in terms of code. Finally, we’ll follow data through a convolutional neural
    network by introducing convolutional and pooling layers. The goal of this chapter
    isn’t to present how popular toolkits pass data around. The toolkits are highly
    optimized pieces of software, and such low-level knowledge isn’t helpful to us
    at this stage. Instead, the goal is to help you see how the data flows from input
    to output.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解如何在两种不同类型的网络中表示数据。接着，我们将通过一个传统的前馈网络来为自己打下坚实的基础。我们将看到在神经网络中进行推理时，代码是如何简洁的。最后，我们将通过引入卷积层和池化层，追踪数据在卷积神经网络中的流动。本章的目标不是展示流行工具包如何传递数据。这些工具包是高度优化的软件，其低层次的知识在此阶段对我们帮助不大。相反，目标是帮助你理解数据是如何从输入流向输出的。
- en: Representing Data
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据表示
- en: 'In the end, everything in deep learning is about data. We have data that we’re
    using to create a model, which we test with more data, ultimately letting us make
    predictions about even more data. We’ll start by looking at how we represent data
    in two types of neural networks: traditional neural networks and deep convolutional
    networks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，深度学习的一切都与数据有关。我们使用数据来创建模型，然后用更多的数据进行测试，最终让我们能够对更多的数据进行预测。我们将从了解如何在两种类型的神经网络中表示数据开始：传统神经网络和深度卷积网络。
- en: Traditional Neural Networks
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 传统神经网络
- en: For a *traditional neural network* or other classical machine learning models,
    the input is a vector of numbers, the feature vector. The training data is a collection
    of these vectors, each with an associated label. (We’ll restrict ourselves to
    basic supervised learning in this chapter.) A collection of feature vectors is
    conveniently implemented as a matrix, where each row is a feature vector and the
    number of rows matches the number of samples in the dataset. As we now know, a
    computer conveniently represents a matrix using a 2D array. Therefore, when working
    with traditional neural networks or other classical models (support vector machines,
    random forests, and so on), we’ll represent datasets as 2D arrays.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*传统神经网络*或其他经典机器学习模型，输入是一个数字向量，即特征向量。训练数据是一组这些特征向量，每个特征向量都带有一个关联的标签。（本章我们将限制在基本的监督学习上。）特征向量集合方便地实现为一个矩阵，每一行是一个特征向量，行数与数据集中的样本数相匹配。正如我们现在所知道的，计算机方便地使用二维数组表示矩阵。因此，在处理传统神经网络或其他经典模型（如支持向量机、随机森林等）时，我们将把数据集表示为二维数组。
- en: 'For example, the iris dataset, which we first encountered in [Chapter 6](ch06.xhtml#ch06),
    has four features in each feature vector. We represented it as a matrix:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第六章](ch06.xhtml#ch06)中我们首次接触的鸢尾花数据集，每个特征向量包含四个特征。我们将它表示为一个矩阵：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we’ve shown the first five samples as we did in [Chapter 6](ch06.xhtml#ch06).
    The samples above are all for class 0 (*I. setosa*). To pass this knowledge to
    the model, we need a matching vector of class labels; `X[i]` returns the feature
    vector for sample `i`, and `Y[i]` returns the class label. The class label is
    usually an integer and counts up from zero for each class in the dataset. Some
    toolkits prefer one-hot-encoded class labels, but we can easily create them from
    the more standard integer labels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们展示了前五个样本，就像在[第六章](ch06.xhtml#ch06)中做的那样。上面的样本全部属于类别 0（*I. setosa*）。为了将这些知识传递给模型，我们需要一个与之匹配的类别标签向量；`X[i]`返回样本`i`的特征向量，`Y[i]`返回类别标签。类别标签通常是一个整数，并且从零开始为数据集中每个类别编号。一些工具包更喜欢使用独热编码（one-hot
    encoding）的类别标签，但我们可以轻松地从更标准的整数标签中生成它们。
- en: Therefore, a traditional dataset uses matrices between layers to hold weights,
    with the input and output of each layer a vector. This is straightforward enough.
    What about a more modern, deep network?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，传统的数据集在层与层之间使用矩阵来保存权重，每一层的输入和输出是一个向量。这相对直接。那么，更现代的深度网络呢？
- en: Deep Convolutional Networks
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度卷积网络
- en: Deep networks might use feature vectors, especially if the model implements
    1D convolutions, but more often than not, the entire point of using a deep network
    is to allow *convolutional layers* to take advantage of spatial relationships
    in the data. Usually, this means the inputs are images, which we represent using
    2D arrays. But the input doesn’t always need to be an image. The model is blissfully
    unaware of *what* the input represents; only the model designer knows, and they
    decide the architecture based on that knowledge. For simplicity, we’ll assume
    the inputs are images, since we’re already aware of how computers work with images,
    at least at a high level.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络可能会使用特征向量，尤其是在模型实现一维卷积时，但更多情况下，使用深度网络的核心目的是让*卷积层*利用数据中的空间关系。通常，这意味着输入是图像，我们使用二维数组表示图像。但是，输入不一定非得是图像。模型并不关心输入代表的*是什么*；只有模型设计者知道，并根据这些知识决定架构。为了简单起见，我们假设输入是图像，因为我们已经了解计算机如何处理图像，至少从高层次来看是这样的。
- en: 'A black-and-white image, or one with shades of gray, known as a grayscale image,
    uses a single number to represent each pixel’s intensity. Therefore, a grayscale
    image consists of a single matrix represented in the computer as a 2D array. However,
    most of the images we see on our computers are color images, not grayscale. Most
    software represents a pixel’s color by three numbers: the amount of red, the amount
    of green, and the amount of blue. This is the origin of the *RGB* label given
    to color images on a computer. There are many other ways of representing colors,
    but RGB is by far the most common. The blending of these primary colors allows
    computers to display millions of colors. If each pixel needs three numbers, then
    a color image isn’t a single 2D array, but three 2D arrays, one for each color.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 黑白图像，或者带有灰度的图像，称为灰度图像，使用单个数字表示每个像素的强度。因此，灰度图像由一个矩阵组成，在计算机中表示为二维数组。然而，我们在计算机上看到的大多数图像都是彩色图像，而非灰度图像。大多数软件通过三个数字表示一个像素的颜色：红色的量、绿色的量和蓝色的量。这就是计算机上彩色图像被标记为*RGB*的原因。还有许多其他表示颜色的方法，但RGB是最常见的。通过这些基础色的混合，计算机能够显示数百万种颜色。如果每个像素需要三个数字，那么彩色图像就不再是一个二维数组，而是三个二维数组，每个数组代表一种颜色。
- en: 'For example, in [Chapter 4](ch04.xhtml#ch04), we loaded a color image from
    `sklearn`. Let’s look at it again to see how it’s arranged in memory:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第4章](ch04.xhtml#ch04)中，我们从`sklearn`加载了一张彩色图像。我们再来看一遍，看看它是如何在内存中排列的：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The image is returned as a NumPy array. Asking for the shape of the array returns
    a tuple: (427, 640, 3). The array has three dimensions. The first is the height
    of the image, 427 pixels. The second is the width of the image, 640 pixels. The
    third is the number of *bands* or *channels*, here three because it’s an RGB image.
    The first channel is the red component of the color of each pixel, the second
    the green, and the last the blue. We can look at each channel as a grayscale image
    if we want:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图像以NumPy数组的形式返回。请求数组的形状会返回一个元组：(427, 640, 3)。这个数组有三个维度。第一个是图像的高度，427个像素。第二个是图像的宽度，640个像素。第三个是*通道*的数量，这里是三，因为它是RGB图像。第一个通道是每个像素的红色分量，第二个是绿色，最后一个是蓝色。如果需要的话，我们可以将每个通道当作一张灰度图像来看：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'PIL refers to Pillow, Python’s library for working with images. If you don’t
    already have it installed, this will install it for you:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PIL指的是Pillow，这是Python用于处理图像的库。如果你还没有安装它，运行以下命令可以为你安装：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Each image looks similar, but if you place them side by side, you’ll notice
    differences. See [Figure 9-1](ch09.xhtml#ch09fig01). The net effect of each per-channel
    image creates the actual color displayed. Replace `china[:,:,0]` with just `china`
    to see the full color image.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像看起来相似，但如果将它们并排放置，你会注意到一些差异。见[图9-1](ch09.xhtml#ch09fig01)。每个通道图像的合成效果形成了显示的实际颜色。将`china[:,:,0]`替换为`china`，即可查看完整的彩色图像。
- en: '![image](Images/09fig01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig01.jpg)'
- en: '*Figure 9-1: The red (left), green (middle), and blue (right)* `*china*` *image
    channels*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-1：红色（左）、绿色（中）、蓝色（右）*`*china*`*图像通道*'
- en: 'Inputs to deep networks are often multidimensional. If the input’s a color
    image, we need to use a 3D tensor to contain the image. We’re not quite done,
    however. Each input sample to the model is a 3D tensor, but we seldom work with
    a single sample at a time. When training a deep network, we use *minibatches*,
    sets of samples processed together to get an average loss. This implies yet another
    dimension to the input tensor, one that lets us specify *which* member of the
    minibatch we want. Therefore, the input is a 4D tensor: *N* × *H* × *W* × *C*,
    with *N* being the number of samples in the minibatch, *H* the height of each
    image in the minibatch, *W* the width of each image, and *C* the number of channels.
    We’ll sometimes write this in tuple form as (*N*, *H*, *W*, *C*).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络的输入通常是多维的。如果输入是彩色图像，我们需要使用一个3D张量来包含图像。然而，这还没有完毕。每个输入样本是一个3D张量，但我们通常不会一次只处理一个样本。在训练深度网络时，我们使用*小批量*，即一组样本一起处理以得到平均损失。这意味着输入张量还需要多出一个维度，用来指定我们想要的小批量中的*哪个*成员。因此，输入是一个4D张量：*N*
    × *H* × *W* × *C*，其中*N*是小批量中的样本数，*H*是每个图像的高度，*W*是每个图像的宽度，*C*是通道数。我们有时会将其写成元组形式：(*N*,
    *H*, *W*, *C*)。
- en: 'Let’s take a look at some actual data meant for a deep network. The data is
    the CIFAR-10 dataset. It’s a widely used benchmark dataset and is available here:
    *[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)*.
    You don’t need to download the raw dataset, however. We’ve included NumPy versions
    with the code for this book. As mentioned above, we need two arrays: one for the
    images and the other for the associated labels. You’ll find them in the *cifar10_test_images.npy*
    and *cifar10_test_labels.npy* files, respectively. Let’s take a look:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下用于深度网络的一些实际数据。数据集是CIFAR-10数据集。这是一个广泛使用的基准数据集，可以在这里找到：*[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)*。不过，你并不需要下载原始数据集。我们已经在本书的代码中包含了NumPy版本。正如前面提到的，我们需要两个数组：一个用于图像，另一个用于对应的标签。你可以在*cifar10_test_images.npy*和*cifar10_test_labels.npy*文件中找到它们。让我们来看看：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notice that the `images` array has four dimensions. The first is the number
    of images in the array (*N* = 10,000). The second and third tell us that the images
    are 32×32 pixels. The last tells us that there are three channels, implying the
    dataset consists of color images. Note that, in general, the number of channels
    might refer to any collection of data grouped that way—it need not be an actual
    image. The `labels` vector has 10,000 elements as well. These are the class labels,
    of which there are 10 classes, a mix of animals and vehicles. For example,
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`images`数组具有四个维度。第一个是数组中图像的数量（*N* = 10,000）。第二个和第三个表示图像的大小为32×32像素。最后一个维度表示有三个通道，这意味着数据集包含的是彩色图像。需要注意的是，通常情况下，通道的数量可以指代任何按这种方式分组的数据集合——它不一定是实际的图像。`labels`向量也有10,000个元素。这些是类标签，共有10个类别，包含动物和车辆。例如，
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This indicates that image 123 is of class 2 (bird) and that the label is correct;
    the image displayed should be that of a bird. Recall that, in NumPy, asking for
    a single index returns the entire subarray, so `images[123]` is equivalent to
    `images[123,:,:,:]`. The `fromarray` method of the `Image` class converts a NumPy
    array to an image so `show` can display it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示图像123属于第2类（鸟类），并且标签是正确的；显示的图像应该是鸟类的图像。回想一下，在NumPy中，要求单个索引时会返回整个子数组，所以`images[123]`等价于`images[123,:,:,:]`。`Image`类的`fromarray`方法将NumPy数组转换为图像，以便`show`可以显示它。
- en: 'Working with minibatches means we pass a subset of the entire dataset through
    the model. If our model uses minibatches of 24, then the input to the deep network,
    if using CIFAR-10, is a (24, 32, 32, 3) array: 24 images, each of which has 32
    rows, 32 columns, and 3 channels. We’ll see below that the idea of channels is
    not restricted to the input to a deep network; it also applies to the shape of
    the data passed between layers.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小批量时，我们将整个数据集的一个子集传递给模型。如果我们的模型使用24个样本的小批量，那么深度网络的输入是一个（24，32，32，3）数组：24张图像，每张图像有32行、32列和3个通道。稍后我们会看到，通道的概念不仅仅局限于深度网络的输入，它同样适用于在各层之间传递的数据形状。
- en: We’ll return to data for deep networks shortly. But for now, let’s switch gears
    to the more straightforward topic of dataflow in a traditional, feedforward neural
    network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会回到深度网络的数据。现在，让我们先转向更直接的主题：传统前馈神经网络中的数据流。
- en: Data Flow in Traditional Neural Networks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统神经网络中的数据流
- en: As we indicated above, in a traditional neural network, the weights between
    layers are stored as matrices. If layer *i* has *n* nodes and layer *i* − 1 has
    *m* outputs, then the weight matrix between the two layers, ***W[i]***, is an
    *n* × *m* matrix. When this matrix is multiplied on the right by the *m* × 1 column
    vector of outputs from layer *i* − 1, the result is an *n* × 1 output representing
    the input to the *n* nodes for layer *i*. Specifically, we calculate
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，在传统的神经网络中，层与层之间的权重以矩阵的形式存储。如果第*i*层有*n*个节点，第*i*−1层有*m*个输出，那么这两层之间的权重矩阵***W[i]***就是一个*n*
    × *m*的矩阵。当这个矩阵与第*i*−1层的*m* × 1列向量相乘时，结果是一个*n* × 1的输出，表示输入到第*i*层的*n*个节点的值。具体来说，我们计算
- en: '***a[i]*** = σ(***W[i]a[i−1]*** + ***b[i]***)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '***a[i]*** = σ(***W[i]a[i−1]*** + ***b[i]***)'
- en: where ***a[i]***[−1], the *m* × 1 vector of outputs from layer *i* − 1, multiplies
    ***W[i]*** to produce an *n* × 1 column vector. We add ***b[i]***, the bias values
    for layer *i*, to this vector and apply the activation function, σ, to every element
    of the resulting vector, ***W[i]a[i]***[−1] + ***b[i]***, to produce ***a[i]***,
    the activations for layer *i*. We feed the activations to layer *i* + 1 as the
    output of layer *i*. By using matrices and vectors, the rules of matrix multiplication
    automatically calculate all the necessary products without explicit loops in the
    code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，***a[i]***[−1]是来自第*i*−1层的*m* × 1输出向量，它与***W[i]***相乘，产生一个*n* × 1列向量。我们将第*i*层的偏置值***b[i]***加到该向量中，并对结果向量***W[i]a[i]***[−1]
    + ***b[i]***的每个元素应用激活函数σ，从而得到***a[i]***，即第*i*层的激活值。我们将激活值作为第*i*层的输出传递给第*i*+1层。通过使用矩阵和向量，矩阵乘法规则自动计算所有必要的乘积，而无需在代码中显式使用循环。
- en: Let’s see an example with a simple neural network. We’ll generate a random dataset
    with two features and then split this dataset into train and test groups. We’ll
    use `sklearn` to train a simple feedforward neural network on the training set.
    The network has a single hidden layer with five nodes and uses a rectified linear
    activation function (ReLU). We’ll then test the trained network to see how well
    it learned and, most importantly, look at the actual weight matrices and bias
    vectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单神经网络的例子。我们将生成一个包含两个特征的随机数据集，然后将该数据集分成训练组和测试组。我们将使用`sklearn`在训练集上训练一个简单的前馈神经网络。该网络有一个隐藏层，包含五个节点，并使用修正线性激活函数（ReLU）。然后我们将测试训练好的网络，看看它学得如何，最重要的是，查看实际的权重矩阵和偏置向量。
- en: 'To build the dataset, we’ll select a set of points in 2D space that are clustered
    but slightly overlapping. We want the network to have to learn something that
    isn’t completely trivial. Here is the code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建数据集，我们将选择一组在二维空间中聚集但略有重叠的点。我们希望网络学习一些不完全简单的内容。以下是代码：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We need the `MLPClassifier` class from `sklearn`, so we load it first. We then
    define a 2D dataset, `x`, consisting of two clouds of 50 points each. The points
    are randomly distributed (`x0`, `y0` and `x1`, `y1`) but centered at (0.2, 0.8)
    and (0.8, 0.2), respectively ❶. Note, we set the NumPy random number seed to a
    fixed value, so each run produces the same set of numbers we’ll see below. Feel
    free to remove this line and experiment with how well the network trains for various
    generations of the dataset.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从`sklearn`导入`MLPClassifier`类，因此首先加载它。然后我们定义一个二维数据集`x`，由两组各50个点组成。点是随机分布的（`x0`，`y0`和`x1`，`y1`），但分别集中在(0.2,
    0.8)和(0.8, 0.2)位置 ❶。请注意，我们将NumPy的随机数种子设置为固定值，因此每次运行都会生成相同的一组数字。如有需要，可以删除这一行并尝试在不同数据集生成的情况下，网络的训练效果。
- en: We know the first 50 points in `x` are from what we’ll call class 0, and the
    next 50 points are class 1, so we define a label vector, `y` ❷. Finally, we randomize
    the order of the points in `x` ❸, being careful to alter the labels in the same
    way, and we split them into a training set (`x_train`) and labels (`y_train`)
    and a test set (`x_test`) and labels (`y_test`). We keep 75 percent of the data
    for training and leave the remaining 25 percent for testing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道`x`中的前50个点来自我们所称之为类别0，接下来的50个点是类别1，因此我们定义一个标签向量`y` ❷。最后，我们随机化`x` ❸中的点的顺序，并小心地以相同的方式调整标签，然后将它们分成训练集（`x_train`）和标签（`y_train`），以及测试集（`x_test`）和标签（`y_test`）。我们保留75%的数据用于训练，剩下的25%用于测试。
- en: '[Figure 9-2](ch09.xhtml#ch09fig02) shows a plot of the full dataset, with each
    feature on one of the axes. The circles correspond to class 0 instances and the
    squares to class 1 instances. There is clear overlap between the two classes.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2](ch09.xhtml#ch09fig02)显示了完整数据集的图形，其中每个特征位于一个坐标轴上。圆圈表示类0实例，方块表示类1实例。两个类别之间有明显的重叠。'
- en: '![image](Images/09fig02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig02.jpg)'
- en: '*Figure 9-2: The dataset used to train the neural network, with the class 0
    instances shown as circles and the class 1 instances as squares*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-2：用于训练神经网络的数据集，类0实例以圆圈表示，类1实例以方块表示*'
- en: 'We’re now ready to train the model. The `sklearn` toolkit makes it easy for
    us, if we use the defaults:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练模型了。如果使用默认设置，`sklearn`工具包使这变得非常简单：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Training involves creating an instance of the model class ❶. Notice that by
    using the defaults, which include using a ReLU activation function, we only need
    to specify the number of nodes in the hidden layers. We want one hidden layer
    with five nodes, so we pass in the tuple `(5,)`. Training is a single call to
    the `fit` function passing in the training data, `x_train`, and the associated
    labels, `y_train`. When complete, we test the model by computing the accuracy
    (`score`) on the test set (`x_test`, `y_test`) and display the result.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括创建模型类的一个实例。注意，使用默认设置时（包括使用ReLU激活函数），我们只需指定隐藏层中节点的数量。我们希望有一个包含五个节点的隐藏层，因此传入元组`(5,)`。训练只需要调用一次`fit`函数，传入训练数据`x_train`和相应的标签`y_train`。完成后，我们通过计算测试集`(x_test,
    y_test)`上的准确率（`score`）来测试模型，并显示结果。
- en: Neural networks are initialized randomly, but because we fixed the NumPy random
    number seed when we generated the dataset, and because `sklearn` uses the NumPy
    random number generator as well, the outcome of training the network should be
    the same for each run of the code. The model has an accuracy of 92 percent on
    the test data ❷. This is convenient for us but concerning as well—so many toolkits
    use NumPy under the hood that interactions due to fixing the random number seed
    are probable, usually undesirable, and perhaps challenging to detect.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是随机初始化的，但由于我们在生成数据集时固定了NumPy随机数种子，并且由于`sklearn`也使用NumPy的随机数生成器，因此每次运行代码时，训练网络的结果应该是相同的。模型在测试数据上的准确率为92%。这对我们很方便，但也令人担忧——如此多的工具包在底层使用NumPy，因而固定随机数种子所导致的交互是很可能发生的，通常是不希望出现的，并且可能很难检测。
- en: We’re now finally ready to get the weight matrices and bias vectors from the
    trained network ❸. Because `sklearn` uses `np.dot` for matrix multiplication,
    we take the transpose of the weight matrices, `W0` and `W1`, to get them in a
    form that’s easier to follow mathematically. We’ll see precisely why this is necessary
    below. Likewise, `b0`, the bias vector for the hidden layer, is a 1D NumPy array,
    so we change it to a column vector. The output layer bias, `b1`, is a scalar,
    as there is only one output for this network, the value we pass to the sigmoid
    function to get the probability of class 1 membership.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在终于准备好从训练好的网络中获取权重矩阵和偏置向量。由于`sklearn`使用`np.dot`进行矩阵乘法，我们取权重矩阵`W0`和`W1`的转置，以便将它们转换为数学上更易于理解的形式。稍后我们将详细说明为什么这样做是必要的。同样，`b0`，隐藏层的偏置向量，是一个1D的NumPy数组，因此我们将其转换为列向量。输出层的偏置`b1`是一个标量，因为该网络只有一个输出，即我们传递给sigmoid函数的值，用于获得属于类1的概率。
- en: Let’s walk through the network for the first test sample. To save space, we’ll
    only show the first three digits of the numeric values, but our calculations will
    use full precision. The input to the network is
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跟随网络计算第一个测试样本。为了节省空间，我们只展示数值的前三位，但我们的计算将使用完整精度。网络的输入是
- en: '![Image](Images/228equ01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/228equ01.jpg)'
- en: We want the network to give us an output leading to the likelihood of this input
    belonging to class 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望网络给出一个输出，表示该输入属于类1的可能性。
- en: 'To get the output of the hidden layer, we multiply ***x*** by the weight matrix,
    ***W***[0], add the bias vector, ***b***[0], and pass that result through the
    ReLU:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得隐藏层的输出，我们将***x***与权重矩阵***W***[0]相乘，加入偏置向量***b***[0]，然后将结果通过ReLU：
- en: '![Image](Images/228equ02.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/228equ02.jpg)'
- en: 'The hidden layer to output transition uses the same form, with ***a***[0] in
    place of ***x***, but here, there is no ReLU applied:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层到输出层的过渡使用相同的形式，用***a***[0]代替***x***，但这里没有应用ReLU：
- en: '![Image](Images/229equ01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/229equ01.jpg)'
- en: 'To get the final output probability, we use ***a*****[1]**, a scalar value,
    as the argument to the *sigmoid function*, also called the *logistic function*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最终的输出概率，我们使用***a*****[1]**，一个标量值，作为*sigmoid 函数*（也称为*logistic 函数*）的参数：
- en: '![Image](Images/229equ02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/229equ02.jpg)'
- en: 'This means the network has assigned a 35.5 percent likelihood of the input
    value being a member of class 1\. The usual threshold for class assignment for
    a binary model is 50 percent, so the network would assign ***x*** to class 0\.
    A peek at `y_test[0]` tells us the network is correct in this case: ***x*** is
    from class 0.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着网络已将输入值属于类别 1 的可能性设定为 35.5%。对于二分类模型，通常的类别分配阈值为 50%，因此网络会将***x***分配给类别 0。查看`y_test[0]`可以告诉我们，网络在此情况下是正确的：***x***来自类别
    0。
- en: Data Flow in Convolutional Neural Networks
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络中的数据流
- en: We saw above how data flow through a traditional neural network was straightforward
    matrix-vector math. To track data flow through a *convolutional neural network
    (CNN)*, we need to learn first what the convolution operation is and how it works.
    Specifically, we’ll learn how to pass data through convolutional and pooling layers
    to a fully connected layer at the top of the model. This sequence accounts for
    many CNN architectures, at least at a conceptual level.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面看到，数据在传统神经网络中的流动是直接的矩阵-向量运算。为了跟踪数据在*卷积神经网络（CNN）*中的流动，我们首先需要了解卷积操作是什么，以及它是如何工作的。具体来说，我们将学习如何通过卷积层和池化层将数据传递到模型顶部的全连接层。这个过程涵盖了许多
    CNN 架构，至少在概念层面上是如此。
- en: Convolution
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积
- en: Convolution involves two functions and the *sliding* of one over the other.
    If the functions are *f*(*x*) and *g*(*x*), convolution is defined as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积涉及两个函数，并且是一个在另一个上滑动的过程。如果函数是*f*(*x*)和*g*(*x*)，则卷积定义为
- en: '![Image](Images/09equ01.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/09equ01.jpg)'
- en: Fortunately for us, we’re working in a discrete domain and more often than not
    with 2D inputs, so the integral is not actually used, though * is still a useful
    notation for the operation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在离散域中工作，而且通常是2D输入，因此积分实际上并没有被使用，尽管*仍然是该操作的有用符号。
- en: The net effect of [Equation 9.1](ch09.xhtml#ch09equ01) is to slide *g*(*x*)
    over *f*(*x*) for different shifts. Let’s clarify using a 1D, discrete example.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 9.1](ch09.xhtml#ch09equ01)的净效果是将*g*(*x*)滑动到*f*(*x*)上，进行不同的位移。让我们用一个一维离散示例来澄清。'
- en: Convolution in One Dimension
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一维卷积
- en: '[Figure 9-3](ch09.xhtml#ch09fig03) shows a plot on the bottom and two sets
    of numbers labeled *f* and *g* on the top.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)显示了底部的图和顶部标有*f*和*g*的两组数字。'
- en: '![image](Images/09fig03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig03.jpg)'
- en: '*Figure 9-3: A 1D, discrete convolution*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-3：一维离散卷积*'
- en: Let’s start with the numbers shown at the top of [Figure 9-3](ch09.xhtml#ch09fig03).
    The first row lists the discrete values of *f*. Below that is *g*, a three-element
    linear ramp. Convolution aligns *g* with the left edge of *f* as shown. We multiply
    corresponding elements between the two arrays,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[图 9-3](ch09.xhtml#ch09fig03)顶部显示的数字开始。第一行列出了*f*的离散值。下面是*g*，一个三元素的线性斜坡。卷积将*g*与*f*的左边对齐，如图所示。我们将两个数组之间的相应元素相乘，
- en: '[2, 6, 15] × [−1, 0, 1] = [−2, 0, 15]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[2, 6, 15] × [−1, 0, 1] = [−2, 0, 15]'
- en: and then sum the resulting values,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将得到的值相加，
- en: −2 + 0 + 15 = 13
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: −2 + 0 + 15 = 13
- en: to produce the value that goes in the indicated element of the output, *f* *
    *g*. To complete the convolution, *g* slides one element to the right, and the
    process repeats. Note that in [Figure 9-3](ch09.xhtml#ch09fig03), we’re showing
    every other alignment of *f* and *g* for clarity, so it’ll appear as though *g*
    is sliding two elements to the right. In general, we refer to *g* as a *kernel*,
    the set of values that slide over the input, *f*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到输出中指定元素的值，*f* * *g*。为了完成卷积，*g*向右滑动一个元素，过程重复进行。请注意，在[图 9-3](ch09.xhtml#ch09fig03)中，为了清晰起见，我们展示了*f*和*g*的每个其他对齐方式，所以看起来好像*g*向右滑动了两个元素。通常，我们将*g*称为*核*，它是滑动到输入*f*上的值集合。
- en: The plot on the bottom of [Figure 9-3](ch09.xhtml#ch09fig03) is *f*(*x*) = ⌊255
    exp(−0.5*x*²)⌋ for *x* in [−3, 3] at the points marked with circles. The floor
    operation makes the output an integer to simplify the discussion below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)底部的图是*f*(*x*) = ⌊255 exp(−0.5*x*²)⌋，其中*x*在[−3,
    3]之间，圆点标记了对应的点。向下取整操作使输出为整数，以便简化下面的讨论。'
- en: The square points in [Figure 9-3](ch09.xhtml#ch09fig03) are the output of the
    convolution of *f*(*x*) with *g*(*x*) = [−1, 0, 1].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)中的方形点是*f*(*x*)与*g*(*x*) = [−1, 0, 1]卷积的输出。'
- en: The *f* and *f* * *g* points in [Figure 9-3](ch09.xhtml#ch09fig03) are generated
    via
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](ch09.xhtml#ch09fig03)中的*f*和*f* * *g*点是通过以下方式生成的：'
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code requires some explanation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要一些解释。
- en: First, we have `x`, a vector spanning [−3, 3] in 20 steps; this vector generates
    `f` (*f*(*x*) above). We want `f` to be of integer type, which is what `astype`
    does for us. Next, we define `g`, the small linear ramp. As we’ll see, the convolution
    operation slides `g` over the elements of `f` to produce the output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有`x`，一个在[−3, 3]范围内按20步生成的向量；这个向量生成了`f`（上面的*f*（*x*））。我们希望`f`是整数类型，这就是`astype`为我们做的事情。接下来，我们定义了`g`，这是一个小的线性斜坡。正如我们所看到的，卷积操作将`g`滑动到*f*的各个元素上以生成输出。
- en: The convolution operation comes next. As convolution is commonly used, NumPy
    supplies a 1D convolution function, `np.convolve`. The first argument is *f*,
    and the second is *g*. I’ll explain shortly why we added `[::-1]` to `g` to reverse
    it. I’ll also explain the meaning of `mode='same'`. The output of the convolution
    is stored in `fp`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是卷积操作。由于卷积是常用的操作，NumPy提供了一个一维卷积函数`np.convolve`。第一个参数是*f*，第二个是*g*。稍后我会解释为什么我们要在`g`上添加`[::-1]`来反转它。我还会解释`mode='same'`的含义。卷积的输出将存储在`fp`中。
- en: The first position shown in the top part of [Figure 9-3](ch09.xhtml#ch09fig03)
    fills in the 13 in the output. Where does the 6 to the left of the 13 come from?
    Convolution has issues at the edges of *f*, where the kernel does not entirely
    cover the input. For a three-element kernel, there will be one edge element on
    each end of *f*. Kernels typically have an odd number of values, so there is a
    clear middle element. If *g* had five elements, there would be two elements on
    each end of *f* that *g* wouldn’t cover.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](ch09.xhtml#ch09fig03)顶部显示的第一个位置填入了输出中的13。那么，13左边的6是从哪里来的呢？卷积在*f*的边缘存在问题，因为卷积核并没有完全覆盖输入数据。对于一个包含三个元素的卷积核，*f*的每一端都会有一个边缘元素。卷积核通常有奇数个值，因此会有一个明确的中间元素。如果*g*有五个元素，那么在*f*的两端会有两个元素是*g*无法覆盖的。'
- en: Convolution functions need to make a choice about these edge cases. One option
    would be to return only the valid portion of the convolution, to ignore the edge
    cases. If we had used this approach, called *valid convolution*, the output, `yp`,
    would start with element 13 and be two less in length than the input, `y`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积函数需要在这些边缘情况做出选择。一个选择是仅返回卷积的有效部分，忽略边缘情况。如果我们采用这种方法，称为*有效卷积*，那么输出`yp`将从元素13开始，长度比输入`y`少两个。
- en: Another approach is to fill in missing values in *f* with zero. This is known
    as *zero padding*, and we typically use it to make the output of a convolution
    operation the same size as the input.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用零填充*f*中的缺失值。这被称为*零填充*，我们通常使用它使卷积操作的输出与输入大小相同。
- en: 'Using `mode=''same''` with `np.convolve` selects zero padding. This explains
    the 6 to the left of the 13\. It’s what we get when adding a zero before the 2
    in *f* and applying the kernel:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mode='same'`与`np.convolve`一起时，选择了零填充。这解释了13左边的6。它是我们在*f*的2前面加上0并应用卷积核时得到的结果：
- en: '[0, 2, 6] × [−1, 0, 1] = [0, 0, 6], 0 + 0 + 6 = 6'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[0, 2, 6] × [−1, 0, 1] = [0, 0, 6]，0 + 0 + 6 = 6'
- en: If we wanted only the valid output values, we would have used `mode='valid'`
    instead.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只想要有效的输出值，我们会使用`mode='valid'`。
- en: The call to `np.convolve` above didn’t use `g`. We passed `g[::-1]` instead,
    the reverse of `g`. We did this to make `np.convolve` act like the convolutions
    used in deep neural networks. From a mathematical and signal processing perspective,
    convolution uses the reverse of the kernel. The `np.convolve` function, therefore,
    reverses the kernel, meaning we need to reverse it beforehand to get the effect
    we want. To be technical, if we perform the operation we’ve called *convolution*
    without flipping the kernel, we’re actually performing *cross-correlation*. This
    issue seldom comes up in deep learning because we *learn* the kernel elements
    during training—we don’t assign them ahead of time. With that in mind, any flipping
    of the kernel by the toolkit process implementing the convolution operation won’t
    affect the outcome, because the learned kernel values were learned with that flip
    in place. We’ll assume going forward that there is no flip and, when necessary,
    we’ll flip the kernels we give to NumPy and SciPy functions. Additionally, we’ll
    continue to use the term *convolution* in this no-flip-of-the-kernel deep learning
    sense.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 上述对`np.convolve`的调用并没有使用`g`，我们传入的是`g[::-1]`，即`g`的反向。我们这样做是为了让`np.convolve`的行为像深度神经网络中使用的卷积。从数学和信号处理的角度来看，卷积操作使用的是核的反向。因此，`np.convolve`函数会反转核，这意味着我们需要提前反转核，才能得到我们想要的效果。更技术一点地说，如果我们执行的操作被称为*卷积*，但没有翻转核，那么我们实际上在做*交叉相关*。在深度学习中，这个问题很少出现，因为我们在训练过程中*学习*核的元素，而不是提前指定它们。因此，工具包实现卷积操作时对核进行的任何翻转都不会影响结果，因为学习到的核值就是在翻转后的状态下学习得到的。我们假设接下来没有翻转，并且在必要时会翻转我们传递给NumPy和SciPy函数的核。另外，我们将继续使用*卷积*这一术语，指的是在深度学习中没有翻转核的情况。
- en: In general, convolution with discrete inputs involves placing the kernel over
    the input starting on the left, multiplying matching elements, summing, and putting
    the result in the output at the point where the center of the kernel matches.
    The kernel then slides one element to the right, and the process repeats. We can
    extend the discrete convolution operation to two dimensions. Most modern deep
    CNNs use 2D kernels, though it’s possible to use 1D and 3D kernels as well.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，离散卷积操作涉及将核放置在输入数据上，从左侧开始，进行元素匹配相乘、求和，并将结果放入输出中，位置是核的中心与输入位置重合的地方。然后，核向右滑动一个元素，过程重复进行。我们可以将离散卷积操作扩展到二维。大多数现代深度卷积神经网络（CNN）使用二维核，尽管也可以使用一维和三维核。
- en: Convolution in Two Dimensions
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二维卷积
- en: 'Convolution with a 2D kernel requires a 2D array. Images are 2D arrays of values,
    and convolution is a common image processing operation. For example, let’s load
    an image, the face of the raccoon we saw in [Chapter 3](ch03.xhtml#ch03), and
    alter it with a 2D convolution. Consider the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二维核进行卷积需要一个二维数组。图像是值的二维数组，卷积是常见的图像处理操作。例如，我们加载一张图像，之前在[第3章](ch03.xhtml#ch03)中看到的浣熊面部图像，并使用二维卷积对其进行处理。考虑以下内容：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we’re using the SciPy `convolve2d` function from the `signal` module.
    First, we load the raccoon image and subset it to a 512×512-pixel image of the
    raccoon’s face (`img`). Next, we define a 3 × 3 kernel, `k`. Lastly, we convolve
    the kernel, as it is, with the face image, storing the result in `c`. The `mode='same'`
    keyword zero pads the image to handle the edge cases.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是来自`signal`模块的SciPy `convolve2d`函数。首先，我们加载浣熊图像，并将其裁剪为一个512×512像素的浣熊面部图像（`img`）。接着，我们定义一个3
    × 3的核，`k`。最后，我们将这个核与面部图像进行卷积，并将结果存储在`c`中。`mode='same'`关键字对图像进行零填充，以处理边缘情况。
- en: The code above leads to
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会导致
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we’re showing the upper 8 × 8 corner of the image and the valid portion
    of the convolution. Recall, the valid portion is the part where the kernel completely
    covers the input array.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们展示的是图像的上8×8角以及卷积的有效部分。回顾一下，有效部分是指核完全覆盖输入数组的部分。
- en: For the kernel and the image, the first valid convolution output is −209\. Mathematically,
    the first step is element-wise multiplication with the kernel,
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于核和图像，第一个有效的卷积输出是−209。数学上，第一步是与核进行逐元素相乘，
- en: '![Image](Images/233equ01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/233equ01.jpg)'
- en: followed by a summation,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后进行求和，
- en: 264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209
- en: Notice how the kernel used wasn’t `k` as we defined it. Instead, `convolve2d`
    flipped the kernel top to bottom and then left to right before it was applied.
    The remainder of `c` flows from moving the kernel one position to the right and
    repeating the multiplication and addition. At the end of a row, the kernel moves
    down one position and back to the left, until the entire image has been processed.
    Deep learning toolkits refer to this motion as the *stride*, and it need not be
    one position or equal in the horizontal and vertical directions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用的核并不是我们定义的`k`。相反，`convolve2d`首先将核上下翻转，然后左右翻转，之后再进行应用。剩下的`c`通过将核向右移动一个位置并重复乘法和加法运算来传递。在一行的末尾，核会向下移动一个位置并返回到左侧，直到整个图像处理完毕。深度学习工具包将这种移动称为*步幅*，并且步幅不一定是一个位置，也不一定在水平方向和垂直方向上相等。
- en: '[Figure 9-4](ch09.xhtml#ch09fig04) shows the effect of the convolution.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-4](ch09.xhtml#ch09fig04) 显示了卷积的效果。'
- en: '![image](Images/09fig04.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig04.jpg)'
- en: '*Figure 9-4: The original raccoon face image (left) and the convolution result
    (right)*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-4：原始浣熊面部图像（左）和卷积结果（右）*'
- en: To make the image, `c` was shifted up, so the minimum value was zero, and then
    divided by the maximum to map to [0, 1]. Finally, the output was multiplied by
    255 and displayed as a grayscale image. The original face image is on the left.
    The convolved image is on the right. Convolution of the image with the kernel
    has altered the image, emphasizing some features while suppressing others.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成图像，`c` 被向上移动，使最小值为零，然后除以最大值映射到[0, 1]。最后，输出乘以255并显示为灰度图像。原始的人脸图像在左侧，卷积后的图像在右侧。图像与核的卷积改变了图像，突出了某些特征，同时抑制了其他特征。
- en: 'Convolving kernels with images isn’t merely an exercise to help us understand
    the convolution operation. It’s of profound importance in the training of CNNs.
    Conceptually, a CNN consists of two main parts: a set of convolution and other
    layers taught to learn a new representation of the input, and a top-level classifier
    taught to use the new representation to classify the inputs. It’s the joint learning
    of the new representation and the classifier that makes CNNs so powerful. The
    key to learning a new representation of the input is the set of learned convolution
    kernels. How the kernels alter the input as data flows through the CNN creates
    the new representation. Training with gradient descent and backpropagation teaches
    the network which kernels to create.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 用卷积核对图像进行卷积不仅仅是为了帮助我们理解卷积操作。它在训练CNN时具有深远的意义。从概念上讲，CNN由两个主要部分组成：一组卷积层和其他层，用于学习输入的新表示，以及一个顶层分类器，用于利用新表示对输入进行分类。正是新表示和分类器的联合学习使得CNN如此强大。学习输入的新表示的关键是学习到的卷积核。卷积核如何改变输入，随着数据流经CNN创建新的表示。使用梯度下降和反向传播训练网络，教会它创建哪些卷积核。
- en: We’re now in a position to follow data through a CNN’s convolutional layers.
    Let’s take a look.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始跟踪数据通过CNN的卷积层。让我们看一看。
- en: Convolutional Layers
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积层
- en: Above, we discussed how deep networks pass tensors from layer to layer and how
    the tensor usually has four dimensions, *N* × *H* × *W* × *C*. To follow data
    through a convolutional layer, we’ll ignore *N*, knowing that what we discuss
    is applied to each sample in the tensor. This leaves us with inputs to the convolutional
    layer that are *H* × *W* × *C*, a 3D tensor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上面我们讨论了深度网络如何将张量从一层传递到另一层，以及张量通常具有四个维度，*N* × *H* × *W* × *C*。为了跟踪卷积层中的数据，我们将忽略*N*，知道我们讨论的内容适用于张量中的每个样本。这将留下卷积层的输入为*H*
    × *W* × *C*，即一个三维张量。
- en: The output of a convolutional layer is another 3D tensor. The height and width
    of the output depend on the convolution kernels’ particulars and how we decide
    to handle the edges. We’ll use valid convolution for the examples here, meaning
    we’ll discard parts of the input that the kernel doesn’t wholly cover. If the
    kernel is 3 × 3, the output will be two less in height and width, one less for
    each edge. A 5 × 5 kernel loses four in height and width, two less for each edge.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出是另一个三维张量。输出的高度和宽度取决于卷积核的具体情况以及我们如何处理边缘。在这里的示例中，我们将使用有效卷积，这意味着我们将丢弃核没有完全覆盖的输入部分。如果卷积核是3
    × 3，则输出的高度和宽度会少两个，每个边缘少一个。如果卷积核是5 × 5，则高度和宽度会少四个，每个边缘少两个。
- en: The convolutional layer uses sets of *filters* to accomplish its goal. A filter
    is a stack of kernels. We need one filter for each of the desired output channels.
    The number of kernels in the stack of each filter matches the number of channels
    in the input. So, if the input has *M* channels, and we want *N* output channels
    using *K* × *K* kernels, we need *N* filters, each of which is a stack of *M K*
    × *K* kernels.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层使用一组*滤波器*来完成其目标。一个滤波器是多个核的堆叠。我们需要为每个期望的输出通道配置一个滤波器。每个滤波器中的核的数量与输入中的通道数量相匹配。因此，如果输入有*M*个通道，并且我们想要使用*K*
    × *K* 核获得*N*个输出通道，我们需要*N*个滤波器，每个滤波器是一个堆叠了*M K* × *K* 核的集合。
- en: Additionally, we have a bias value for each of the *N* filters. We’ll see below
    how the bias is used, but we now know how many parameters we need to learn to
    implement a convolutional layer with *M* input channels, *K* × *K* kernels, and
    *N* outputs. It’s *K* × *K* × *M* × *N* for *N* filters with *K* × *K* × *M* parameters
    each, plus *N* bias terms—one per filter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个*N*个滤波器都有一个偏置值。我们将在下面看到偏置是如何使用的，但我们现在已经知道了实现一个具有*M*个输入通道、*K* × *K* 核和*N*个输出的卷积层需要学习多少个参数。需要的参数数量是
    *K* × *K* × *M* × *N*，其中每个滤波器有 *K* × *K* × *M* 个参数，再加上*N*个偏置项——每个滤波器一个。
- en: Let’s make all of this concrete. We have a convolutional layer. The input to
    the layer is an (*H*,*W*,*C*) = (5,5,2) tensor, meaning a height and width of
    five and two channels. We’ll use a 3 × 3 kernel with valid convolution, so the
    output in height and width is 3 × 3 from the 5 × 5 input. We get to select the
    number of output channels. Let’s use three. Therefore, we need to use convolution
    and kernels to map a (5,5,2) input to a (3,3,3) output. From what we discussed
    above, we know we need three filters, and each filter has 3 × 3 × 2 parameters,
    plus a bias term.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这一切具象化。我们有一个卷积层。该层的输入是一个 (*H*,*W*,*C*) = (5,5,2) 的张量，意味着高度和宽度都是五，并且有两个通道。我们将使用一个
    3 × 3 的卷积核，采用有效卷积，因此输出的高度和宽度为 3 × 3，来自 5 × 5 的输入。我们可以选择输出通道的数量。让我们选择三个。因此，我们需要使用卷积和卷积核将
    (5,5,2) 的输入映射到 (3,3,3) 的输出。从上面讨论的内容可以得出，我们需要三个滤波器，每个滤波器有 3 × 3 × 2 个参数，再加上一个偏置项。
- en: Our input stack is
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入堆叠是
- en: '![Image](Images/235equ01.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/235equ01.jpg)'
- en: We’ve split the third dimension to show the two input channels, each 5 × 5.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将第三维度分离开来，显示两个输入通道，每个为 5 × 5。
- en: The three filters are
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个滤波器是
- en: '![Image](Images/236equ01.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ01.jpg)'
- en: Again, we’ve separated the third dimension. Notice how each filter has two 3
    × 3 kernels, one for each channel of the 5 × 5 × 2 input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将第三维度分离开来。请注意，每个滤波器都有两个 3 × 3 的卷积核，每个卷积核对应于 5 × 5 × 2 输入的一个通道。
- en: 'Let’s work through applying the first filter, *f*[0]. We need to convolve the
    first channel of the input with the first kernel of *f*[0]:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过应用第一个滤波器*f*[0]来进行计算。我们需要将输入的第一个通道与*f*[0]的第一个卷积核进行卷积：
- en: '![Image](Images/236equ02.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ02.jpg)'
- en: 'Then, we need to convolve the second input channel with the second kernel of
    *f*[0]:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将第二个输入通道与*f*[0]的第二个核进行卷积：
- en: '![Image](Images/236equ03.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ03.jpg)'
- en: 'Finally, we add the two convolution outputs along with the single bias scalar:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个卷积输出与单个偏置标量相加：
- en: '![Image](Images/236equ04.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ04.jpg)'
- en: We now have the first 3 × 3 output.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在得到了第一个 3 × 3 的输出。
- en: Repeating the process above for *f*[1] and *f*[2] gives
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对*f*[1]和*f*[2]重复上述过程得到
- en: '![Image](Images/236equ05.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ05.jpg)'
- en: We’ve completed the convolutional layer and generated the 3 × 3 × 3 output.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了卷积层并生成了 3 × 3 × 3 的输出。
- en: Many toolkits make it easy to add operations in the call that sets up the convolutional
    layer, but, conceptually, these are layers of their own that accept the 3 × 3
    × 3 output as an input. For example, if requested, Keras will apply a ReLU to
    the output. Applying a ReLU, a nonlinearity, to the output of the convolution
    would give us
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具包使得在设置卷积层的调用中添加操作变得容易，但从概念上讲，这些操作本身就是独立的层，它们将 3 × 3 × 3 的输出作为输入。例如，如果需要，Keras
    会对输出应用 ReLU。对卷积的输出应用 ReLU（一种非线性操作）将得到
- en: '![Image](Images/237equ01.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/237equ01.jpg)'
- en: 'Note that all elements less than zero are now zero. We use a nonlinearity between
    convolutional layers for the same reason we use a nonlinear activation function
    in a traditional neural network: to keep the convolutional layers from collapsing
    into a single layer. Notice how the operation to generate the filter outputs is
    purely linear; each output element is a linear combination of input values. Adding
    the ReLU breaks this linearity.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在所有小于零的元素都变为零。我们在卷积层之间使用非线性激活函数，原因与在传统神经网络中使用非线性激活函数相同：防止卷积层坍塌为一个单一的线性层。请注意，生成滤波器输出的操作是纯线性的；每个输出元素是输入值的线性组合。添加
    ReLU 激活函数可以打破这种线性关系。
- en: One reason for the creation of convolutional layers was to reduce the number
    of learned parameters. For the example above, the input was 5 × 5 × 2 = 50 elements.
    The desired output was 3 × 3 × 3 = 27 elements. A fully connected layer between
    these would need to learn 50 × 27 = 1,350 weights, plus another 27 bias values.
    However, the convolutional layer learned three filters, each with 3 × 3 × 2 weights,
    as well as three bias values, for a total of 3(3 × 3 × 2) + 3 = 57 parameters.
    Adding the convolutional layer saved learning some 1,300 additional weights.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 创建卷积层的一个原因是为了减少需要学习的参数数量。在上面的例子中，输入是 5 × 5 × 2 = 50 个元素。期望的输出是 3 × 3 × 3 = 27
    个元素。如果在这些元素之间使用全连接层，则需要学习 50 × 27 = 1,350 个权重，再加上 27 个偏置值。然而，卷积层只学习了三个过滤器，每个过滤器有
    3 × 3 × 2 个权重，以及三个偏置值，总共需要 3(3 × 3 × 2) + 3 = 57 个参数。添加卷积层可以节省大约 1,300 个额外的权重学习。
- en: The output of a convolutional layer is often the input to a pooling layer. Let’s
    consider that type of layer next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出通常是池化层的输入。接下来我们将考虑这种类型的层。
- en: Pooling Layers
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 池化层
- en: Convolutional networks often use *pooling layers* after convolutional layers.
    Their use is a bit controversial, as they discard information, and the loss of
    information might make it harder for the network to learn spatial relationships.
    Pooling is generally performed in the spatial domain along the input tensor’s
    height and width while preserving the number of channels.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络通常在卷积层后使用*池化层*。它们的使用有些争议，因为池化会丢失信息，而信息的丢失可能使得网络更难学习空间关系。池化通常在空间域内进行，沿着输入张量的高度和宽度，同时保留通道数。
- en: 'The pooling operation is straightforward: you move a window over the image,
    usually 2 × 2 with a stride of two, to group values. The specific pooling operation
    performed on each group is either max or average. The max-pooling operation preserves
    the maximum value in the window and discards the rest. Average pooling takes the
    mean of the values in the window.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作很简单：你将一个窗口在图像上滑动，通常是 2 × 2 的窗口，步长为二，以便将值分组。对每个分组执行的具体池化操作是最大池化或平均池化。最大池化操作保留窗口中的最大值，其余的值会被丢弃。平均池化则取窗口中所有值的均值。
- en: A 2 × 2 window with a stride of two results in a reduction of a factor of two
    in each spatial direction. Therefore, a (24,24,32) input tensor leads to a (12,12,32)
    output tensor. [Figure 9-5](ch09.xhtml#ch09fig05) illustrates the process for
    maximum pooling.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 2 × 2 的窗口，步长为二，会导致每个空间方向上的尺寸减半。因此，一个 (24,24,32) 的输入张量会变成一个 (12,12,32) 的输出张量。[图
    9-5](ch09.xhtml#ch09fig05)展示了最大池化的过程。
- en: '![image](Images/09fig05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig05.jpg)'
- en: '*Figure 9-5: Max pooling with a 2* × *2 window and a stride of two*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-5：使用 2* × *2 窗口和步长为二的最大池化*'
- en: One channel of the input, with a height and width of eight, is on the left.
    The 2 × 2 window slides over the input, jumping by two, so there is no overlap
    of windows. The output for each 2 × 2 region of the input is the maximum value.
    Average pooling would instead output the mean of the four numbers. As with normal
    convolution, at the end of the row, the window slides down two positions, and
    the process repeats to change the 8 × 8 input channel to a 4 × 4 output channel.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的一个通道，具有 8 的高度和宽度，位于左侧。2 × 2 的窗口滑动在输入上，每次跳跃两个位置，因此窗口之间没有重叠。每个 2 × 2 区域的输出是最大值。平均池化则会输出四个数字的均值。与正常的卷积一样，在每行的末尾，窗口会向下滑动两个位置，过程重复进行，将
    8 × 8 的输入通道转换为 4 × 4 的输出通道。
- en: As mentioned above, pooling without overlap in the windows loses spatial information.
    This has caused some in the deep learning community, most notably Geoffrey Hinton,
    to lament its use, as dropping spatial information distorts the relationship between
    objects or parts of objects in the input. For example, applying a 2 × 2 max pooling
    window with a stride of one instead of two to the input matrix of [Figure 9-5](ch09.xhtml#ch09fig05)
    produces
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，没有重叠的池化窗口会丢失空间信息。这使得深度学习界的一些人，尤其是Geoffrey Hinton，感到遗憾，因为丢失空间信息会扭曲输入中物体或物体部分之间的关系。例如，将一个2
    × 2的最大池化窗口，步幅为一而不是二，应用于[图9-5](ch09.xhtml#ch09fig05)中的输入矩阵会产生
- en: '![Image](Images/238equ01.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/238equ01.jpg)'
- en: This is a 7 × 7 output, which only loses one row and column of the original
    8 × 8 input. In this case, the input matrix was randomly generated, so we should
    expect a max-pooling operation biased toward eights and nines—there is no structure
    to capture. This is not usually the case in an actual CNN, of course, as it’s
    the spatial structure inherent in the inputs we wish to utilize.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个7 × 7的输出，只丢失了原始8 × 8输入的一个行和列。在这种情况下，输入矩阵是随机生成的，因此我们应该期待一个偏向于8和9的最大池化操作——没有可以捕捉的结构。当然，这在实际的CNN中通常不是这样，因为我们希望利用输入中固有的空间结构。
- en: Pooling is commonly used in deep learning, especially for CNNs, so it’s essential
    to understand what a pooling operation is doing and be aware of its potential
    pitfalls. Let’s move on now to the output end of a CNN, typically the fully connected
    layers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 池化在深度学习中被广泛使用，尤其是在卷积神经网络（CNN）中，因此理解池化操作的作用并意识到其潜在陷阱是至关重要的。接下来我们将进入CNN的输出端，通常是全连接层。
- en: Fully Connected Layers
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全连接层
- en: A fully connected layer in a deep network is, in terms of weights and data,
    identical to a regular layer in a traditional neural network. Many deep networks
    concerned with classification pass the output of a set of convolution and pooling
    layers to the first fully connected layer via a layer that flattens the tensor,
    essentially unraveling it into a vector. Once the output is a vector, the fully
    connected layer uses a weight matrix in the same way a traditional neural network
    does to map a vector input to a vector output.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度网络中，全连接层在权重和数据方面与传统神经网络中的常规层是相同的。许多与分类相关的深度网络通过一个将张量展平的层将一组卷积层和池化层的输出传递给第一个全连接层，基本上是将张量展开为一个向量。一旦输出变为向量，全连接层就像传统神经网络一样，使用一个权重矩阵将向量输入映射到向量输出。
- en: Data Flow Through a Convolutional Neural Network
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据在卷积神经网络中的流动
- en: Let’s put all the pieces together to see how data flows through a CNN from input
    to output. We’ll use a simple CNN trained on the MNIST dataset, a collection of
    28×28-pixel grayscale images of handwritten digits. The architecture is shown
    next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有的部分放在一起，看看数据是如何从输入到输出通过CNN流动的。我们将使用一个简单的CNN，训练于MNIST数据集，这是一个由28×28像素灰度手写数字图像组成的集合。架构如下所示。
- en: Input → Conv(32) → Conv(64) → Pool → Flatten → Dense(128) → Dense(10)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 → 卷积(32) → 卷积(64) → 池化 → 展平 → 全连接(128) → 全连接(10)
- en: The input is a 28×28-pixel grayscale image (one channel). The convolutional
    layers (conv) use 3 × 3 kernels and valid convolution, so their output’s height
    and width are two less than their input. The first convolutional layer learns
    32 filters while the second learns 64\. We’re ignoring layers that do not affect
    the amount of data in the network, like the ReLU layers after the convolutional
    layers. The max-pooling layer is assumed to use a 2 × 2 window with a stride of
    two. The first fully connected layer (dense) has 128 nodes, followed by an output
    layer of 10 nodes, one for each digit, 0 to 9.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个28×28像素的灰度图像（一个通道）。卷积层（conv）使用3 × 3的卷积核和有效卷积，因此其输出的高度和宽度比输入小两位。第一层卷积学习32个过滤器，第二层学习64个过滤器。我们忽略了不会影响网络中数据量的层，比如卷积层后的ReLU层。最大池化层假设使用2
    × 2的窗口，步幅为二。第一个全连接层（dense）有128个节点，之后是一个具有10个节点的输出层，每个数字对应一个节点，范围从0到9。
- en: The tensors passed through this network for a single input sample are
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 单个输入样本通过该网络传递的张量是
- en: (28,28,1) →(26,26,32) →(24,24,64) →(12,12,64) → 9216 → 128 → 10
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (28,28,1) →(26,26,32) →(24,24,64) →(12,12,64) → 9216 → 128 → 10
- en: Input        Conv        Conv         Pool      Flatten  Dense   Dense
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输入        卷积        卷积         池化      展平  全连接   全连接
- en: The flatten layer unravels the (12,12,64) tensor to form a vector of 9,216 elements
    (12 × 12 × 64 = 9,216). We pass the 9,216 elements that the flatten layer outputs
    through the first dense layer to generate 128 output values, and the last step
    takes the 128-element vector and maps it to 10 output values.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 展平层将 (12,12,64) 张量展开形成一个包含 9,216 个元素的向量（12 × 12 × 64 = 9,216）。我们将展平层输出的 9,216
    个元素传递通过第一层全连接层，生成 128 个输出值，最后一步将这个 128 元素的向量映射到 10 个输出值。
- en: Note, the values above refer to the *data* passed through the network for each
    input sample, one of the *N* samples in the minibatch. This is not the same as
    the number parameters (weights and biases) the network needed to learn during
    training.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述值是指传入网络的每个输入样本的数据，属于小批量中的 *N* 个样本之一。这与网络在训练过程中需要学习的参数（权重和偏置）不同。
- en: The network shown above was trained on the MNIST digits using Keras. [Figure
    9-6](ch09.xhtml#ch09fig06) illustrates the action of the network for two inputs
    by showing, visually, the output of each layer. Specifically, it shows each layer’s
    output for two input images, depicting a 4 and a 6.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述网络是使用 Keras 在 MNIST 数字数据集上训练的。[图 9-6](ch09.xhtml#ch09fig06)通过视觉展示了网络在处理两个输入时的每一层的输出，具体来说，它展示了输入图像
    4 和 6 的每一层输出。
- en: '![image](Images/09fig06.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig06.jpg)'
- en: '*Figure 9-6: A visual representation of the output of a CNN for two sample
    inputs*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-6：CNN 对两个样本输入的输出的可视化表示*'
- en: Starting at the top, we see the two inputs. For the figure, intensities have
    been reversed, so darker represents higher numeric values. The input is a (28,28,1)
    tensor, the 1 indicating a single-channel grayscale image. Valid convolution with
    a 3 × 3 kernel returns a 26 × 26 output. The first convolutional layer learned
    32 filters, so the output is a (26,26,32) tensor. In the figure, we show the output
    of each filter as an image. Zero is scaled to midlevel gray (intensity 128), more
    positive values are darker, and more negative values are lighter. We see differences
    in how the inputs have been affected by the learned filters. The single input
    channel means each filter in this layer is a single 3 × 3 kernel. Transitions
    between light and dark indicate edges in particular orientations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部开始，我们看到两个输入。图中，强度已被反转，因此较暗的部分表示更高的数值。输入是一个 (28,28,1) 张量，1 表示单通道灰度图像。有效的卷积操作使用
    3 × 3 的卷积核返回一个 26 × 26 的输出。第一层卷积学习了 32 个滤波器，因此输出是一个 (26,26,32) 张量。在图中，我们将每个滤波器的输出显示为图像。零被缩放为中灰色（强度为
    128），更正的值会变得更暗，而更负的值则变得更亮。我们可以看到输入如何被学习到的滤波器所影响。单一输入通道意味着该层的每个滤波器都是一个单一的 3 × 3
    卷积核。亮暗之间的过渡表示特定方向的边缘。
- en: We pass the (26,26,32) tensor through a ReLU (not shown here) and then through
    the second convolutional layer. The output of this layer is a (24,24,64) tensor
    shown as an 8 × 8 grid of images in the figure. We can see many parts of the input
    digits highlighted.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 (26,26,32) 张量传递通过 ReLU（此处未显示），然后通过第二层卷积层。该层的输出是一个 (24,24,64) 张量，图中将其显示为一个
    8 × 8 网格的图像。我们可以看到许多输入数字的部分被高亮显示。
- en: The pooling layer preserves the number of channels but reduces the spatial dimension
    by two. In image form, the 8 × 8 grid of 24×24-pixel images is now an 8 × 8 grid
    of 12×12-pixel images. The flatten operation maps the (12,12,64) tensor to a 9,216-element
    vector.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层保留了通道数，但将空间维度缩小了两倍。在图像中，24×24 像素的 8 × 8 网格图像现在变成了 12×12 像素的 8 × 8 网格图像。展平操作将
    (12,12,64) 张量映射到一个 9,216 元素的向量。
- en: The output of the first dense layer is a vector of 128 numbers. For [Figure
    9-6](ch09.xhtml#ch09fig06), we show this as a 128-element bar code. The values
    run from left to right. The height of each bar is unimportant and was selected
    only to make the bar code easy to see. The bar code generated from the input image
    is the final representation that the top layer of 10 nodes uses to create the
    output passed through the softmax function. The highest softmax output is used
    to select the class label, “4” or “6.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层全连接层的输出是一个 128 个数字的向量。在[图 9-6](ch09.xhtml#ch09fig06)中，我们将其显示为一个包含 128 元素的条形码。值从左到右排列。每个条形的高度不重要，只是为了让条形码更加易于查看。输入图像生成的条形码是最后一层包含
    10 个节点的输出表示，经过 softmax 函数后用于生成最终输出。softmax 输出最高的值被用来选择类别标签，“4”或“6”。
- en: Therefore, we can think of all the CNN layers through the first dense layer
    as mapping inputs to a new representation, one that makes it easy for a simple
    classifier to handle. Indeed, if we pass 10 examples of “4” and “6” digits through
    this network and display the resulting 128-node feature vectors, we get [Figure
    9-7](ch09.xhtml#ch09fig07), where we can easily see the difference between the
    digit patterns.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将所有 CNN 层看作是通过第一个全连接层将输入映射到新的表示上，这种表示使得简单分类器容易处理。实际上，如果我们将“4”和“6”这两种数字的
    10 个示例通过这个网络，并显示出结果中的 128 节点特征向量，我们可以得到[图 9-7](ch09.xhtml#ch09fig07)，在图中我们可以轻松地看到这两种数字模式之间的区别。
- en: '![image](Images/09fig07.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig07.jpg)'
- en: '*Figure 9-7: The first fully connected layer outputs for multiple “4” and “6”
    inputs*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-7：多种“4”和“6”输入的第一层全连接层输出*'
- en: Of course, the entire point of writing digits as we do is to make it easy for
    humans to see the differences between them. While we could teach ourselves to
    differentiate digits using the 128-element vector images, we naturally prefer
    to use the written digits because of habitual use and the fact we already employ
    highly sophisticated hierarchical feature detectors via our brain’s visual system.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们书写数字的目的就是为了让人类更容易看到它们之间的差异。虽然我们可以通过 128 元素的向量图像来区分数字，但我们自然更喜欢使用书写的数字，因为习惯的使用以及我们的大脑视觉系统已经具备了高度复杂的分层特征检测器。
- en: The example of a CNN learning a new input representation that’s more conducive
    to interpretation by a machine is worth bearing in mind, since what a human might
    use in an image as a clue to its classification is not necessarily what a network
    learns to use. This might explain, in part, why certain preprocessing steps, like
    the changes made to training samples during data augmentation, are so effective
    in helping the network learn to generalize, when many of those alterations seem
    strange to us.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 学习新输入表示的例子值得记住，因为人类在图像中用作分类线索的内容不一定是网络学习使用的内容。这或许能部分解释为什么某些预处理步骤（例如数据增强过程中对训练样本的修改）在帮助网络学习泛化时非常有效，而这些修改对我们来说似乎很奇怪。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The goal of this chapter was to demonstrate how neural networks manipulate
    data from input to output. Naturally, we couldn’t cover all network types, but,
    in general, the principles are the same: for traditional neural networks, data
    is passed from layer to layer as a vector, and for deep networks, it’s passed
    as a tensor, typically of four dimensions.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是演示神经网络如何处理从输入到输出的数据。自然，我们无法覆盖所有网络类型，但总体来说，原理是相同的：对于传统神经网络，数据作为向量从一层传递到另一层，而对于深度网络，它作为张量传递，通常是四维张量。
- en: We learned how to present data to a network, either as a feature vector or a
    multidimensional input. We followed this by looking at how to pass data through
    a traditional neural network. We saw how the vectors used as input to, and output
    from, a layer made the implementation of a traditional neural network a straightforward
    exercise in matrix-vector multiplication and addition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何将数据以特征向量或多维输入的形式呈现给网络。接着，我们查看了如何将数据传递通过传统神经网络。我们看到，作为输入和输出的向量使得传统神经网络的实现变得简单，实际上就是矩阵-向量乘法和加法的过程。
- en: Next, we saw how a deep convolutional network passes data from layer to layer.
    We learned first about the convolution operation and then about the specifics
    of how convolutional and pooling layers manipulate data as tensors—a 3D tensor
    for each sample in the input minibatch. At the top of a CNN meant for classification
    are fully connected layers, which we saw act precisely as they do in a traditional
    neural network.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看到深度卷积网络如何将数据从一层传递到另一层。我们首先了解了卷积操作，然后了解了卷积层和池化层如何作为张量操作数据——对于输入的小批量样本，每个样本都是一个三维张量。在用于分类的
    CNN 顶层是全连接层，我们看到它们的作用与传统神经网络中的作用完全一致。
- en: We ended the chapter by showing, visually, how input images moved through a
    CNN to produce an output representation, allowing the network to label the inputs
    correctly. We briefly discussed what this process might mean in terms of what
    a network picks up on during training and how that might differ from what a human
    naturally sees in an image.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后，我们通过可视化的方式展示了输入图像是如何通过 CNN 产生输出表示的，从而使网络能够正确地标注输入。我们简要讨论了这个过程可能意味着网络在训练过程中所捕捉到的内容，以及它与人类在图像中自然看到的内容之间的差异。
- en: We are now in a position to discuss backpropagation, the first of the two critical
    algorithms that, together with gradient descent, make training deep neural networks
    possible.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以讨论反向传播，它是两个关键算法中的第一个，和梯度下降一起使得深度神经网络的训练成为可能。
