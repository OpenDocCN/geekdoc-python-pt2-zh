- en: '**9'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**9'
- en: DATA FLOW IN NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的数据流**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In this chapter, I’ll present how data flows through a trained neural network.
    In other words, we’ll look at how to go from an input vector or tensor to the
    output, and the form the data takes along the way. If you’re already familiar
    with how neural networks function, great, but if not, walking through how data
    flows from layer to layer will help you build an understanding of the processes
    involved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将展示数据如何在一个训练好的神经网络中流动。换句话说，我们将看到如何从输入向量或张量到输出，并且数据在这个过程中所采取的形式。如果你已经熟悉神经网络的工作原理，那太好了；如果不熟悉，了解数据如何从一层流动到另一层将有助于你建立对相关过程的理解。
- en: First, we’ll look at how we represent data in two different kinds of networks.
    Then, we’ll work through a traditional feedforward network to give ourselves a
    solid foundation. We’ll see just how compact inference with a neural network can
    be in terms of code. Finally, we’ll follow data through a convolutional neural
    network by introducing convolutional and pooling layers. The goal of this chapter
    isn’t to present how popular toolkits pass data around. The toolkits are highly
    optimized pieces of software, and such low-level knowledge isn’t helpful to us
    at this stage. Instead, the goal is to help you see how the data flows from input
    to output.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看如何在两种不同的网络中表示数据。然后，我们将通过一个传统的前馈网络来打下坚实的基础。我们将看到，使用神经网络进行推理在代码方面是多么简洁。最后，我们将通过引入卷积层和池化层，跟踪数据在卷积神经网络中的流动。本章的目标不是展示流行工具包如何传递数据。这些工具包是高度优化的软件，了解这些低级细节在目前阶段对我们并无太大帮助。相反，目标是帮助你理解数据是如何从输入流向输出的。
- en: Representing Data
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据表示
- en: 'In the end, everything in deep learning is about data. We have data that we’re
    using to create a model, which we test with more data, ultimately letting us make
    predictions about even more data. We’ll start by looking at how we represent data
    in two types of neural networks: traditional neural networks and deep convolutional
    networks.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，深度学习中的一切都与数据有关。我们有数据用于创建模型，并用更多数据来测试模型，最终让我们能够对更多数据做出预测。我们将从如何在两种类型的神经网络中表示数据开始：传统神经网络和深度卷积神经网络。
- en: Traditional Neural Networks
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 传统神经网络
- en: For a *traditional neural network* or other classical machine learning models,
    the input is a vector of numbers, the feature vector. The training data is a collection
    of these vectors, each with an associated label. (We’ll restrict ourselves to
    basic supervised learning in this chapter.) A collection of feature vectors is
    conveniently implemented as a matrix, where each row is a feature vector and the
    number of rows matches the number of samples in the dataset. As we now know, a
    computer conveniently represents a matrix using a 2D array. Therefore, when working
    with traditional neural networks or other classical models (support vector machines,
    random forests, and so on), we’ll represent datasets as 2D arrays.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*传统神经网络*或其他经典机器学习模型，输入是一个数字向量，即特征向量。训练数据是这些向量的集合，每个向量都有一个相关的标签。（本章中我们将限制为基本的监督学习。）特征向量的集合通常实现为一个矩阵，其中每一行是一个特征向量，矩阵的行数与数据集中的样本数量相匹配。正如我们现在所知，计算机方便地使用二维数组来表示矩阵。因此，当我们使用传统神经网络或其他经典模型（如支持向量机、随机森林等）时，我们将数据集表示为二维数组。
- en: 'For example, the iris dataset, which we first encountered in [Chapter 6](ch06.xhtml#ch06),
    has four features in each feature vector. We represented it as a matrix:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们在[第6章](ch06.xhtml#ch06)首次遇到的鸢尾花数据集，每个特征向量包含四个特征。我们将其表示为一个矩阵：
- en: '>>> import numpy as np'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import numpy as np'
- en: '>>> from sklearn import datasets'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from sklearn import datasets'
- en: '>>> iris = datasets.load_iris()'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> iris = datasets.load_iris()'
- en: '>>> X = iris.data[:5]'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X = iris.data[:5]'
- en: '>>> X'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X'
- en: array([[5.1, 3.5, 1.4, 0.2],
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: array([[5.1, 3.5, 1.4, 0.2],
- en: '[4.9, 3\. , 1.4, 0.2],'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.9, 3\. , 1.4, 0.2],'
- en: '[4.7, 3.2, 1.3, 0.2],'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.7, 3.2, 1.3, 0.2],'
- en: '[4.6, 3.1, 1.5, 0.2],'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.6, 3.1, 1.5, 0.2],'
- en: '[5\. , 3.6, 1.4, 0.2]])'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[5\. , 3.6, 1.4, 0.2]])'
- en: '>>> Y = iris.target[:5]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> Y = iris.target[:5]'
- en: Here, we’ve shown the first five samples as we did in [Chapter 6](ch06.xhtml#ch06).
    The samples above are all for class 0 (*I. setosa*). To pass this knowledge to
    the model, we need a matching vector of class labels; X[i] returns the feature
    vector for sample i, and Y[i] returns the class label. The class label is usually
    an integer and counts up from zero for each class in the dataset. Some toolkits
    prefer one-hot-encoded class labels, but we can easily create them from the more
    standard integer labels.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了前五个样本，就像我们在[第6章](ch06.xhtml#ch06)中做的一样。上面的样本都是类 0（*I. setosa*）的。为了将这些知识传递给模型，我们需要一个匹配的类别标签向量；X[i]
    返回样本 i 的特征向量，Y[i] 返回类别标签。类别标签通常是一个整数，从零开始对数据集中的每个类别进行计数。一些工具包更喜欢使用独热编码的类别标签，但我们可以很容易地从更标准的整数标签中创建它们。
- en: Therefore, a traditional dataset uses matrices between layers to hold weights,
    with the input and output of each layer a vector. This is straightforward enough.
    What about a more modern, deep network?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，传统数据集使用层与层之间的矩阵来存储权重，每一层的输入和输出都是一个向量。这相当简单。那么更现代的深度网络呢？
- en: Deep Convolutional Networks
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度卷积网络
- en: Deep networks might use feature vectors, especially if the model implements
    1D convolutions, but more often than not, the entire point of using a deep network
    is to allow *convolutional layers* to take advantage of spatial relationships
    in the data. Usually, this means the inputs are images, which we represent using
    2D arrays. But the input doesn’t always need to be an image. The model is blissfully
    unaware of *what* the input represents; only the model designer knows, and they
    decide the architecture based on that knowledge. For simplicity, we’ll assume
    the inputs are images, since we’re already aware of how computers work with images,
    at least at a high level.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络可能会使用特征向量，特别是如果模型实现了 1D 卷积，但通常，使用深度网络的全部意义在于让*卷积层*利用数据中的空间关系。通常，这意味着输入是图像，我们使用二维数组来表示图像。但输入并不总是需要是图像。模型对输入表示的*是什么*一无所知；只有模型设计者知道，并且根据这些知识决定架构。为了简化起见，我们假设输入是图像，因为我们已经知道计算机如何处理图像，至少在高层次上是这样。
- en: 'A black-and-white image, or one with shades of gray, known as a grayscale image,
    uses a single number to represent each pixel’s intensity. Therefore, a grayscale
    image consists of a single matrix represented in the computer as a 2D array. However,
    most of the images we see on our computers are color images, not grayscale. Most
    software represents a pixel’s color by three numbers: the amount of red, the amount
    of green, and the amount of blue. This is the origin of the *RGB* label given
    to color images on a computer. There are many other ways of representing colors,
    but RGB is by far the most common. The blending of these primary colors allows
    computers to display millions of colors. If each pixel needs three numbers, then
    a color image isn’t a single 2D array, but three 2D arrays, one for each color.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 黑白图像，或者说带有灰度的图像，被称为灰度图像，使用一个数字来表示每个像素的强度。因此，灰度图像由一个矩阵组成，在计算机中表示为二维数组。然而，我们在计算机上看到的大多数图像都是彩色图像，而不是灰度图像。大多数软件通过三个数字来表示像素的颜色：红色的量，绿色的量和蓝色的量。这就是计算机上彩色图像被称为*RGB*的原因。还有许多其他表示颜色的方法，但
    RGB 绝对是最常见的。通过混合这些基本颜色，计算机可以显示数百万种颜色。如果每个像素需要三个数字，那么彩色图像就不是一个单一的二维数组，而是三个二维数组，每个颜色一个。
- en: 'For example, in [Chapter 4](ch04.xhtml#ch04), we loaded a color image from
    sklearn. Let’s look at it again to see how it’s arranged in memory:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第4章](ch04.xhtml#ch04)中，我们从 sklearn 加载了一张彩色图像。让我们再次看看它，了解它在内存中的排列方式：
- en: '>>> from sklearn.datasets import load_sample_image'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from sklearn.datasets import load_sample_image'
- en: '>>> china = load_sample_image(''china.jpg'')'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> china = load_sample_image(''china.jpg'')'
- en: '>>> china.shape'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> china.shape'
- en: (427, 640, 3)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (427, 640, 3)
- en: 'The image is returned as a NumPy array. Asking for the shape of the array returns
    a tuple: (427, 640, 3). The array has three dimensions. The first is the height
    of the image, 427 pixels. The second is the width of the image, 640 pixels. The
    third is the number of *bands* or *channels*, here three because it’s an RGB image.
    The first channel is the red component of the color of each pixel, the second
    the green, and the last the blue. We can look at each channel as a grayscale image
    if we want:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像作为 NumPy 数组返回。请求数组的形状将返回一个元组：(427, 640, 3)。该数组有三个维度。第一个是图像的高度，427 像素。第二个是图像的宽度，640
    像素。第三个是*波段*或*通道*的数量，这里是三个，因为它是一个 RGB 图像。第一个通道是每个像素的红色分量，第二个是绿色，最后一个是蓝色。如果我们愿意，可以将每个通道看作灰度图像：
- en: '>>> from PIL import Image'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> from PIL import Image'
- en: '>>> Image.fromarray(china).show()'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> Image.fromarray(china).show()'
- en: '>>> Image.fromarray(china[:,:,0]).show()'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> Image.fromarray(china[:,:,0]).show()'
- en: '>>> Image.fromarray(china[:,:,1]).show()'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> Image.fromarray(china[:,:,1]).show()'
- en: '>>> Image.fromarray(china[:,:,2]).show()'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> Image.fromarray(china[:,:,2]).show()'
- en: 'PIL refers to Pillow, Python’s library for working with images. If you don’t
    already have it installed, this will install it for you:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PIL指的是Pillow，Python的图像处理库。如果你还没有安装它，可以使用以下命令安装：
- en: pip3 install pillow
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: pip3 install pillow
- en: Each image looks similar, but if you place them side by side, you’ll notice
    differences. See [Figure 9-1](ch09.xhtml#ch09fig01). The net effect of each per-channel
    image creates the actual color displayed. Replace china[:,:,0] with just china
    to see the full color image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像看起来相似，但如果你把它们并排放置，你会注意到一些差异。见[图9-1](ch09.xhtml#ch09fig01)。每个通道图像的净效应共同创建了显示的实际颜色。将china[:,:,0]替换为china，可以查看完整的彩色图像。
- en: '![image](Images/09fig01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig01.jpg)'
- en: '*Figure 9-1: The red (left), green (middle), and blue (right)* *china* *image
    channels*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-1：红色（左）、绿色（中）和蓝色（右）* *china* *图像通道*'
- en: 'Inputs to deep networks are often multidimensional. If the input’s a color
    image, we need to use a 3D tensor to contain the image. We’re not quite done,
    however. Each input sample to the model is a 3D tensor, but we seldom work with
    a single sample at a time. When training a deep network, we use *minibatches*,
    sets of samples processed together to get an average loss. This implies yet another
    dimension to the input tensor, one that lets us specify *which* member of the
    minibatch we want. Therefore, the input is a 4D tensor: *N* × *H* × *W* × *C*,
    with *N* being the number of samples in the minibatch, *H* the height of each
    image in the minibatch, *W* the width of each image, and *C* the number of channels.
    We’ll sometimes write this in tuple form as (*N*, *H*, *W*, *C*).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络的输入通常是多维的。如果输入是彩色图像，我们需要使用3D张量来存储图像。然而，我们还没有完全完成。每个输入样本是一个3D张量，但我们很少一次只处理一个样本。在训练深度网络时，我们使用*minibatches*，即一组一同处理的样本，以获得平均损失。这意味着输入张量需要再多一个维度，来指定我们要选择的*minibatch*中的*哪一个*样本。因此，输入是一个4D张量：*N*
    × *H* × *W* × *C*，其中*N*是minibatch中的样本数，*H*是每张图像的高度，*W*是每张图像的宽度，*C*是通道数。我们有时会将其写成元组形式（*N*，*H*，*W*，*C*）。
- en: 'Let’s take a look at some actual data meant for a deep network. The data is
    the CIFAR-10 dataset. It’s a widely used benchmark dataset and is available here:
    *[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)*.
    You don’t need to download the raw dataset, however. We’ve included NumPy versions
    with the code for this book. As mentioned above, we need two arrays: one for the
    images and the other for the associated labels. You’ll find them in the *cifar10_test_images.npy*
    and *cifar10_test_labels.npy* files, respectively. Let’s take a look:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些实际的数据，这些数据是用于深度网络的。数据集是CIFAR-10数据集。这是一个广泛使用的基准数据集，可以在这里找到：[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)。不过你不需要下载原始数据集。我们已经为本书的代码提供了NumPy版本。如上所述，我们需要两个数组：一个用于图像，另一个用于对应的标签。你可以在*cifar10_test_images.npy*和*cifar10_test_labels.npy*文件中找到它们。让我们来看一下：
- en: '>>> images = np.load("cifar10_test_images.npy")'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> images = np.load("cifar10_test_images.npy")'
- en: '>>> labels = np.load("cifar10_test_labels.npy")'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> labels = np.load("cifar10_test_labels.npy")'
- en: '>>> images.shape'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> images.shape'
- en: (10000, 32, 32, 3)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (10000, 32, 32, 3)
- en: '>>> labels.shape'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> labels.shape'
- en: (10000,)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (10000,)
- en: Notice that the images array has four dimensions. The first is the number of
    images in the array (*N* = 10,000). The second and third tell us that the images
    are 32×32 pixels. The last tells us that there are three channels, implying the
    dataset consists of color images. Note that, in general, the number of channels
    might refer to any collection of data grouped that way—it need not be an actual
    image. The labels vector has 10,000 elements as well. These are the class labels,
    of which there are 10 classes, a mix of animals and vehicles. For example,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，图像数组有四个维度。第一个维度是数组中图像的数量（*N* = 10,000）。第二和第三个维度告诉我们图像是32×32像素。最后一个维度告诉我们有三个通道，意味着数据集包含彩色图像。请注意，通常情况下，通道数可能指任何按这种方式分组的数据集合——它不一定是实际的图像。标签向量也有10,000个元素。这些是类别标签，共有10个类别，混合了动物和交通工具。例如，
- en: '>>> labels[123]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> labels[123]'
- en: '2'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '2'
- en: '>>> Image.fromarray(images[123]).show()'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> Image.fromarray(images[123]).show()'
- en: This indicates that image 123 is of class 2 (bird) and that the label is correct;
    the image displayed should be that of a bird. Recall that, in NumPy, asking for
    a single index returns the entire subarray, so images[123] is equivalent to images[123,:,:,:].
    The fromarray method of the Image class converts a NumPy array to an image so
    show can display it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示图像123属于第2类（鸟类），标签是正确的；显示的图像应为鸟类图像。回想一下，在NumPy中，询问一个索引会返回整个子数组，所以images[123]等同于images[123,:,:,:]。Image类的fromarray方法将NumPy数组转换为图像，以便show能够显示它。
- en: 'Working with minibatches means we pass a subset of the entire dataset through
    the model. If our model uses minibatches of 24, then the input to the deep network,
    if using CIFAR-10, is a (24, 32, 32, 3) array: 24 images, each of which has 32
    rows, 32 columns, and 3 channels. We’ll see below that the idea of channels is
    not restricted to the input to a deep network; it also applies to the shape of
    the data passed between layers.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小批量处理意味着我们将整个数据集的一部分通过模型进行处理。如果我们的模型使用24个样本的小批量，那么深度网络的输入（如果使用CIFAR-10）就是一个(24,
    32, 32, 3)的数组：24张图像，每张图像有32行、32列和3个通道。我们将在下面看到，通道的概念不仅仅局限于深度网络的输入；它也适用于层间传递的数据形状。
- en: We’ll return to data for deep networks shortly. But for now, let’s switch gears
    to the more straightforward topic of dataflow in a traditional, feedforward neural
    network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会回到深度网络的数据处理部分。但现在，让我们转到传统前馈神经网络中数据流的更直接话题。
- en: Data Flow in Traditional Neural Networks
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统神经网络中的数据流
- en: As we indicated above, in a traditional neural network, the weights between
    layers are stored as matrices. If layer *i* has *n* nodes and layer *i* − 1 has
    *m* outputs, then the weight matrix between the two layers, ***W[i]***, is an
    *n* × *m* matrix. When this matrix is multiplied on the right by the *m* × 1 column
    vector of outputs from layer *i* − 1, the result is an *n* × 1 output representing
    the input to the *n* nodes for layer *i*. Specifically, we calculate
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在传统的神经网络中，层与层之间的权重存储为矩阵。如果第*i*层有*n*个节点，第*i* − 1层有*m*个输出，则这两层之间的权重矩阵***W[i]***是一个*n*
    × *m*的矩阵。当该矩阵与第*i* − 1层的*m* × 1列向量相乘时，结果是一个*n* × 1的输出，表示第*i*层的*n*个节点的输入。具体来说，我们计算
- en: '***a[i]*** = σ(***W[i]a[i−1]*** + ***b[i]***)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '***a[i]*** = σ(***W[i]a[i−1]*** + ***b[i]***)'
- en: where ***a[i]***[−1], the *m* × 1 vector of outputs from layer *i* − 1, multiplies
    ***W[i]*** to produce an *n* × 1 column vector. We add ***b[i]***, the bias values
    for layer *i*, to this vector and apply the activation function, σ, to every element
    of the resulting vector, ***W[i]a[i]***[−1] + ***b[i]***, to produce ***a[i]***,
    the activations for layer *i*. We feed the activations to layer *i* + 1 as the
    output of layer *i*. By using matrices and vectors, the rules of matrix multiplication
    automatically calculate all the necessary products without explicit loops in the
    code.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，***a[i]***[−1]是来自第*i* − 1层的*m* × 1输出向量，乘以***W[i]***产生一个*n* × 1的列向量。我们将***b[i]***（第*i*层的偏置值）加到这个向量上，并对结果向量***W[i]a[i]***[−1]
    + ***b[i]***中的每个元素应用激活函数σ，生成***a[i]***，即第*i*层的激活值。我们将激活值传递给第*i* + 1层作为第*i*层的输出。通过使用矩阵和向量，矩阵乘法的规则自动计算出所有必要的乘积，而不需要在代码中显式地使用循环。
- en: Let’s see an example with a simple neural network. We’ll generate a random dataset
    with two features and then split this dataset into train and test groups. We’ll
    use sklearn to train a simple feedforward neural network on the training set.
    The network has a single hidden layer with five nodes and uses a rectified linear
    activation function (ReLU). We’ll then test the trained network to see how well
    it learned and, most importantly, look at the actual weight matrices and bias
    vectors.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单神经网络的例子。我们将生成一个具有两个特征的随机数据集，然后将该数据集划分为训练集和测试集。我们将使用sklearn在训练集上训练一个简单的前馈神经网络。该网络有一个包含五个节点的隐藏层，并使用线性整流激活函数（ReLU）。然后，我们将测试训练后的网络，看它学习得如何，并且最重要的是，查看实际的权重矩阵和偏置向量。
- en: 'To build the dataset, we’ll select a set of points in 2D space that are clustered
    but slightly overlapping. We want the network to have to learn something that
    isn’t completely trivial. Here is the code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建数据集，我们将选择一组二维空间中的点，这些点有些簇状分布，但略微重叠。我们希望网络必须学习一些并非完全简单的内容。以下是代码：
- en: from sklearn.neural_network import MLPClassifier
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neural_network import MLPClassifier
- en: np.random.seed(8675309)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: np.random.seed(8675309)
- en: ❶ x0 = np.random.random(50)-0.3
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x0 = np.random.random(50)-0.3
- en: y0 = np.random.random(50)+0.3
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: y0 = np.random.random(50)+0.3
- en: x1 = np.random.random(50)+0.3
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: x1 = np.random.random(50)+0.3
- en: y1 = np.random.random(50)-0.3
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: y1 = np.random.random(50)-0.3
- en: x = np.zeros((100,2))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.zeros((100,2))
- en: x[:50,0] = x0; x[:50,1] = y0
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: x[:50,0] = x0; x[:50,1] = y0
- en: x[50:,0] = x1; x[50:,1] = y1
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: x[50:,0] = x1; x[50:,1] = y1
- en: ❷ y = np.array([0]*50+[1]*50)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ y = np.array([0]*50+[1]*50)
- en: ❸ idx = np.argsort(np.random.random(100))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ idx = np.argsort(np.random.random(100))
- en: x = x[idx]; y = y[idx]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[idx]; y = y[idx]
- en: x_train = x[:75]; x_test = x[75:]
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x[:75]; x_test = x[75:]
- en: y_train = y[:75]; y_test = y[75:]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y[:75]; y_test = y[75:]
- en: We need the MLPClassifier class from sklearn, so we load it first. We then define
    a 2D dataset, x, consisting of two clouds of 50 points each. The points are randomly
    distributed (x0, y0 and x1, y1) but centered at (0.2, 0.8) and (0.8, 0.2), respectively
    ❶. Note, we set the NumPy random number seed to a fixed value, so each run produces
    the same set of numbers we’ll see below. Feel free to remove this line and experiment
    with how well the network trains for various generations of the dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从 sklearn 导入 MLPClassifier 类，因此首先加载它。然后我们定义一个 2D 数据集 x，由两个各包含 50 个点的云组成。这些点是随机分布的（x0,
    y0 和 x1, y1），但分别集中在 (0.2, 0.8) 和 (0.8, 0.2) 处 ❶。注意，我们将 NumPy 随机数种子设置为固定值，因此每次运行都会产生相同的一组数字。你可以删除这行代码，尝试不同的种子值，看看神经网络在各种数据集生成上训练的效果如何。
- en: We know the first 50 points in x are from what we’ll call class 0, and the next
    50 points are class 1, so we define a label vector, y ❷. Finally, we randomize
    the order of the points in x ❸, being careful to alter the labels in the same
    way, and we split them into a training set (x_train) and labels (y_train) and
    a test set (x_test) and labels (y_test). We keep 75 percent of the data for training
    and leave the remaining 25 percent for testing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 x 中的前 50 个点来自我们所称的类 0，接下来的 50 个点来自类 1，因此我们定义了一个标签向量 y ❷。最后，我们随机化 x 中点的顺序
    ❸，同时确保以相同方式改变标签，并将其拆分为训练集 (x_train) 和标签 (y_train)，以及测试集 (x_test) 和标签 (y_test)。我们保留
    75% 的数据用于训练，剩余 25% 用于测试。
- en: '[Figure 9-2](ch09.xhtml#ch09fig02) shows a plot of the full dataset, with each
    feature on one of the axes. The circles correspond to class 0 instances and the
    squares to class 1 instances. There is clear overlap between the two classes.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-2](ch09.xhtml#ch09fig02) 显示了完整数据集的图示，每个特征在一个轴上。圆形对应类 0 的实例，方形对应类 1 的实例。两类之间有明显的重叠。'
- en: '![image](Images/09fig02.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig02.jpg)'
- en: '*Figure 9-2: The dataset used to train the neural network, with the class 0
    instances shown as circles and the class 1 instances as squares*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-2：用于训练神经网络的数据集，其中类 0 的实例以圆形表示，类 1 的实例以方形表示*'
- en: 'We’re now ready to train the model. The sklearn toolkit makes it easy for us,
    if we use the defaults:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练模型了。如果使用默认设置，sklearn 工具包会让我们轻松实现：
- en: ❶ clf = MLPClassifier(hidden_layer_sizes=(5,))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ clf = MLPClassifier(hidden_layer_sizes=(5,))
- en: clf.fit(x_train, y_train)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: ❷ score = clf.score(x_test, y_test)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ score = clf.score(x_test, y_test)
- en: 'print("Model accuracy on test set: %0.4f" % score)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: print("模型在测试集上的准确率：%0.4f" % score)
- en: ❸ W0 = clf.coefs_[0].T
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ W0 = clf.coefs_[0].T
- en: b0 = clf.intercepts_[0].reshape((5,1))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: b0 = clf.intercepts_[0].reshape((5,1))
- en: W1 = clf.coefs_[1].T
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: W1 = clf.coefs_[1].T
- en: b1 = clf.intercepts_[1]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: b1 = clf.intercepts_[1]
- en: Training involves creating an instance of the model class ❶. Notice that by
    using the defaults, which include using a ReLU activation function, we only need
    to specify the number of nodes in the hidden layers. We want one hidden layer
    with five nodes, so we pass in the tuple (5,). Training is a single call to the
    fit function passing in the training data, x_train, and the associated labels,
    y_train. When complete, we test the model by computing the accuracy (score) on
    the test set (x_test, y_test) and display the result.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括创建模型类的一个实例 ❶。注意，通过使用默认设置（包括使用 ReLU 激活函数），我们只需指定隐藏层中节点的数量。我们希望有一个包含五个节点的隐藏层，因此传入元组
    (5,)。训练只是调用 fit 函数，并传入训练数据 x_train 和相应的标签 y_train。训练完成后，我们通过计算测试集（x_test, y_test）上的准确率（得分）来测试模型，并显示结果。
- en: Neural networks are initialized randomly, but because we fixed the NumPy random
    number seed when we generated the dataset, and because sklearn uses the NumPy
    random number generator as well, the outcome of training the network should be
    the same for each run of the code. The model has an accuracy of 92 percent on
    the test data ❷. This is convenient for us but concerning as well—so many toolkits
    use NumPy under the hood that interactions due to fixing the random number seed
    are probable, usually undesirable, and perhaps challenging to detect.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是随机初始化的，但由于我们在生成数据集时固定了NumPy随机数种子，并且sklearn也使用NumPy随机数生成器，因此每次运行代码时，训练网络的结果应该是相同的。该模型在测试数据上的准确率为92%
    ❷。这对我们来说很方便，但也令人担忧——如此多的工具包在后台使用NumPy，因此由于固定随机数种子而产生的交互是很可能的，通常是不可取的，并且可能很难检测。
- en: We’re now finally ready to get the weight matrices and bias vectors from the
    trained network ❸. Because sklearn uses np.dot for matrix multiplication, we take
    the transpose of the weight matrices, W0 and W1, to get them in a form that’s
    easier to follow mathematically. We’ll see precisely why this is necessary below.
    Likewise, b0, the bias vector for the hidden layer, is a 1D NumPy array, so we
    change it to a column vector. The output layer bias, b1, is a scalar, as there
    is only one output for this network, the value we pass to the sigmoid function
    to get the probability of class 1 membership.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在终于准备好从训练好的网络中获取权重矩阵和偏置向量 ❸。因为sklearn使用np.dot进行矩阵乘法，我们取权重矩阵W0和W1的转置，以便使其在数学上更容易理解。我们将在下面看到为什么这是必要的。同样，b0，隐藏层的偏置向量，是一个1D的NumPy数组，因此我们将其转换为列向量。输出层偏置b1是一个标量，因为这个网络只有一个输出，即我们传递给sigmoid函数以得到类别1的概率。
- en: Let’s walk through the network for the first test sample. To save space, we’ll
    only show the first three digits of the numeric values, but our calculations will
    use full precision. The input to the network is
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析一下第一个测试样本的网络。为了节省空间，我们只显示数值的前三位数字，但我们的计算将使用全精度。网络的输入是
- en: '![Image](Images/228equ01.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/228equ01.jpg)'
- en: We want the network to give us an output leading to the likelihood of this input
    belonging to class 1.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望网络给出一个输出，表示此输入属于类别1的概率。
- en: 'To get the output of the hidden layer, we multiply ***x*** by the weight matrix,
    ***W***[0], add the bias vector, ***b***[0], and pass that result through the
    ReLU:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到隐藏层的输出，我们将***x***与权重矩阵***W***[0]相乘，加上偏置向量***b***[0]，然后通过ReLU函数传递这个结果：
- en: '![Image](Images/228equ02.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/228equ02.jpg)'
- en: 'The hidden layer to output transition uses the same form, with ***a***[0] in
    place of ***x***, but here, there is no ReLU applied:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层到输出层的过渡使用相同的形式，只是***a***[0]取代了***x***，但是这里没有应用ReLU：
- en: '![Image](Images/229equ01.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/229equ01.jpg)'
- en: 'To get the final output probability, we use ***a*****[1]**, a scalar value,
    as the argument to the *sigmoid function*, also called the *logistic function*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到最终的输出概率，我们使用***a*****[1]**，一个标量值，作为*sigmoid函数*（也称为*逻辑函数*）的参数：
- en: '![Image](Images/229equ02.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/229equ02.jpg)'
- en: 'This means the network has assigned a 35.5 percent likelihood of the input
    value being a member of class 1\. The usual threshold for class assignment for
    a binary model is 50 percent, so the network would assign ***x*** to class 0\.
    A peek at y_test[0] tells us the network is correct in this case: ***x*** is from
    class 0.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着网络给输入值属于类别1的概率为35.5%。对于二分类模型，通常的分类阈值是50%，因此网络会将***x***归为类别0。查看y_test[0]告诉我们，在这种情况下，网络是正确的：***x***来自类别0。
- en: Data Flow in Convolutional Neural Networks
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络中的数据流
- en: We saw above how data flow through a traditional neural network was straightforward
    matrix-vector math. To track data flow through a *convolutional neural network
    (CNN)*, we need to learn first what the convolution operation is and how it works.
    Specifically, we’ll learn how to pass data through convolutional and pooling layers
    to a fully connected layer at the top of the model. This sequence accounts for
    many CNN architectures, at least at a conceptual level.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上面看到，传统神经网络中的数据流是直接的矩阵-向量运算。要追踪数据在*卷积神经网络（CNN）*中的流动，我们首先需要了解卷积操作是什么以及它是如何工作的。具体来说，我们将学习如何将数据通过卷积层和池化层传递到模型顶部的全连接层。这一过程至少在概念层面上解释了许多CNN架构。
- en: Convolution
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积
- en: Convolution involves two functions and the *sliding* of one over the other.
    If the functions are *f*(*x*) and *g*(*x*), convolution is defined as
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积涉及两个函数，并将其中一个滑动到另一个上。如果函数是*f*(*x*)和*g*(*x*)，则卷积定义为
- en: '![Image](Images/09equ01.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/09equ01.jpg)'
- en: Fortunately for us, we’re working in a discrete domain and more often than not
    with 2D inputs, so the integral is not actually used, though * is still a useful
    notation for the operation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们工作在离散域中，并且大多数情况下使用2D输入，所以实际上并没有使用积分，尽管*仍然是这个操作的有用符号。
- en: The net effect of [Equation 9.1](ch09.xhtml#ch09equ01) is to slide *g*(*x*)
    over *f*(*x*) for different shifts. Let’s clarify using a 1D, discrete example.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 9.1](ch09.xhtml#ch09equ01)的净效应是将*g*(*x*)滑动过*f*(*x*)，进行不同的平移。让我们通过一个一维离散例子来澄清。'
- en: Convolution in One Dimension
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一维卷积
- en: '[Figure 9-3](ch09.xhtml#ch09fig03) shows a plot on the bottom and two sets
    of numbers labeled *f* and *g* on the top.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)显示了底部的图表，以及顶部标有*f*和*g*的两组数字。'
- en: '![image](Images/09fig03.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig03.jpg)'
- en: '*Figure 9-3: A 1D, discrete convolution*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-3：一维离散卷积*'
- en: Let’s start with the numbers shown at the top of [Figure 9-3](ch09.xhtml#ch09fig03).
    The first row lists the discrete values of *f*. Below that is *g*, a three-element
    linear ramp. Convolution aligns *g* with the left edge of *f* as shown. We multiply
    corresponding elements between the two arrays,
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[图 9-3](ch09.xhtml#ch09fig03)顶部显示的数字开始。第一行列出了*f*的离散值。下面是*g*，一个由三个元素组成的线性斜坡。卷积将*g*与*f*的左边缘对齐，如图所示。我们将两个数组之间对应的元素相乘，
- en: '[2, 6, 15] × [−1, 0, 1] = [−2, 0, 15]'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[2, 6, 15] × [−1, 0, 1] = [−2, 0, 15]'
- en: and then sum the resulting values,
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对结果值求和，
- en: −2 + 0 + 15 = 13
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: −2 + 0 + 15 = 13
- en: to produce the value that goes in the indicated element of the output, *f* *
    *g*. To complete the convolution, *g* slides one element to the right, and the
    process repeats. Note that in [Figure 9-3](ch09.xhtml#ch09fig03), we’re showing
    every other alignment of *f* and *g* for clarity, so it’ll appear as though *g*
    is sliding two elements to the right. In general, we refer to *g* as a *kernel*,
    the set of values that slide over the input, *f*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以产生输出中指定元素的值，*f* * *g*。为了完成卷积，*g*向右滑动一个元素，过程重复进行。请注意，在[图 9-3](ch09.xhtml#ch09fig03)中，为了清晰起见，我们显示了*f*和*g*的每隔一对齐，所以看起来像是*g*向右滑动了两个元素。通常，我们将*g*称为*kernel*，即滑过输入*f*的值集。
- en: The plot on the bottom of [Figure 9-3](ch09.xhtml#ch09fig03) is *f*(*x*) = ⌊255
    exp(−0.5*x*²)⌋ for *x* in [−3, 3] at the points marked with circles. The floor
    operation makes the output an integer to simplify the discussion below.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)底部的图表是*f*(*x*) = ⌊255 exp(−0.5*x*²)⌋，其中*x*在[−3,
    3]区间内，且标有圆圈的点为参考。取整操作使得输出为整数，以简化以下讨论。'
- en: The square points in [Figure 9-3](ch09.xhtml#ch09fig03) are the output of the
    convolution of *f*(*x*) with *g*(*x*) = [−1, 0, 1].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)中的方块点是*f*(*x*)与*g*(*x*) = [−1, 0, 1]的卷积结果。'
- en: The *f* and *f* * *g* points in [Figure 9-3](ch09.xhtml#ch09fig03) are generated
    via
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](ch09.xhtml#ch09fig03)中的*f*和*f* * *g*点是通过以下方式生成的'
- en: x = np.linspace(-3,3,20)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.linspace(-3,3,20)
- en: f = (255*np.exp(-0.5*x**2)).astype("int32")
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: f = (255*np.exp(-0.5*x**2)).astype("int32")
- en: g = np.array([-1,0,1])
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: g = np.array([-1,0,1])
- en: fp= np.convolve(f,g[::-1], mode='same')
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: fp= np.convolve(f,g[::-1], mode='same')
- en: This code requires some explanation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码需要一些解释。
- en: First, we have x, a vector spanning [−3, 3] in 20 steps; this vector generates
    f (*f*(*x*) above). We want f to be of integer type, which is what astype does
    for us. Next, we define g, the small linear ramp. As we’ll see, the convolution
    operation slides g over the elements of f to produce the output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有x，这是一个在[−3, 3]区间内取20步的向量；这个向量生成*f*（上面的*f*(*x*)）。我们希望*f*是整数类型，这就是astype为我们所做的。接下来，我们定义*g*，即小的线性斜坡。正如我们将看到的，卷积操作将*g*滑动过*f*的元素，以生成输出。
- en: The convolution operation comes next. As convolution is commonly used, NumPy
    supplies a 1D convolution function, np.convolve. The first argument is *f*, and
    the second is *g*. I’ll explain shortly why we added [::-1] to g to reverse it.
    I’ll also explain the meaning of mode='same'. The output of the convolution is
    stored in fp.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是卷积操作。由于卷积被广泛使用，NumPy提供了一个一维卷积函数np.convolve。第一个参数是*f*，第二个参数是*g*。我将很快解释为什么我们向*g*添加了[::-1]来反转它。我也会解释mode='same'的含义。卷积的输出存储在fp中。
- en: The first position shown in the top part of [Figure 9-3](ch09.xhtml#ch09fig03)
    fills in the 13 in the output. Where does the 6 to the left of the 13 come from?
    Convolution has issues at the edges of *f*, where the kernel does not entirely
    cover the input. For a three-element kernel, there will be one edge element on
    each end of *f*. Kernels typically have an odd number of values, so there is a
    clear middle element. If *g* had five elements, there would be two elements on
    each end of *f* that *g* wouldn’t cover.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](ch09.xhtml#ch09fig03)顶部显示的第一个位置填充了输出中的13。那么，13左侧的6是从哪里来的呢？卷积在*f*的边缘有问题，在那里卷积核不能完全覆盖输入。对于一个包含三个元素的卷积核，*f*的每一端都会有一个边缘元素。卷积核通常有奇数个值，所以有一个明确的中间元素。如果*g*有五个元素，那么*f*的两端就会有两个元素*g*无法覆盖。'
- en: Convolution functions need to make a choice about these edge cases. One option
    would be to return only the valid portion of the convolution, to ignore the edge
    cases. If we had used this approach, called *valid convolution*, the output, yp,
    would start with element 13 and be two less in length than the input, y.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积函数需要对这些边缘情况做出选择。一种选择是仅返回卷积的有效部分，忽略边缘情况。如果我们使用这种方法，叫做*有效卷积*，那么输出yp将从元素13开始，长度比输入y短2个元素。
- en: Another approach is to fill in missing values in *f* with zero. This is known
    as *zero padding*, and we typically use it to make the output of a convolution
    operation the same size as the input.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用零填充*f*中的缺失值。这被称为*零填充*，我们通常使用它来使卷积操作的输出与输入的大小相同。
- en: 'Using mode=''same'' with np.convolve selects zero padding. This explains the
    6 to the left of the 13\. It’s what we get when adding a zero before the 2 in
    *f* and applying the kernel:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用mode='same'与np.convolve选择零填充。这解释了13左侧的6。它是我们在*f*前添加零并应用卷积核时得到的结果：
- en: '[0, 2, 6] × [−1, 0, 1] = [0, 0, 6], 0 + 0 + 6 = 6'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[0, 2, 6] × [−1, 0, 1] = [0, 0, 6]，0 + 0 + 6 = 6'
- en: If we wanted only the valid output values, we would have used mode='valid' instead.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只需要有效输出值，我们会改用mode='valid'。
- en: The call to np.convolve above didn’t use g. We passed g[::-1] instead, the reverse
    of g. We did this to make np.convolve act like the convolutions used in deep neural
    networks. From a mathematical and signal processing perspective, convolution uses
    the reverse of the kernel. The np.convolve function, therefore, reverses the kernel,
    meaning we need to reverse it beforehand to get the effect we want. To be technical,
    if we perform the operation we’ve called *convolution* without flipping the kernel,
    we’re actually performing *cross-correlation*. This issue seldom comes up in deep
    learning because we *learn* the kernel elements during training—we don’t assign
    them ahead of time. With that in mind, any flipping of the kernel by the toolkit
    process implementing the convolution operation won’t affect the outcome, because
    the learned kernel values were learned with that flip in place. We’ll assume going
    forward that there is no flip and, when necessary, we’ll flip the kernels we give
    to NumPy and SciPy functions. Additionally, we’ll continue to use the term *convolution*
    in this no-flip-of-the-kernel deep learning sense.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上面对np.convolve的调用没有使用g，而是传递了g[::-1]，即g的反向。我们这么做是为了让np.convolve的行为像深度神经网络中的卷积操作。从数学和信号处理的角度看，卷积是使用卷积核的反向。因此，np.convolve函数会反转卷积核，这意味着我们需要事先反转它，以达到我们想要的效果。严格来说，如果我们执行的操作叫做*卷积*而不翻转卷积核，我们实际上是在执行*交叉相关*。在深度学习中，这个问题很少出现，因为我们在训练过程中*学习*卷积核的元素——而不是提前指定它们。考虑到这一点，工具包在执行卷积操作时翻转卷积核不会影响结果，因为学习到的卷积核值已经是在翻转状态下学习得到的。我们假设在接下来的过程中不会翻转卷积核，并且在必要时，我们会翻转传给NumPy和SciPy函数的卷积核。此外，我们将继续在没有翻转卷积核的深度学习语境下使用*卷积*这个术语。
- en: In general, convolution with discrete inputs involves placing the kernel over
    the input starting on the left, multiplying matching elements, summing, and putting
    the result in the output at the point where the center of the kernel matches.
    The kernel then slides one element to the right, and the process repeats. We can
    extend the discrete convolution operation to two dimensions. Most modern deep
    CNNs use 2D kernels, though it’s possible to use 1D and 3D kernels as well.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，离散输入的卷积涉及将卷积核放在输入的左侧，进行匹配元素的乘法、求和，并将结果放入输出中，当卷积核的中心与当前位置对齐时。然后，卷积核向右滑动一个元素，重复该过程。我们可以将离散卷积操作扩展到二维。大多数现代深度卷积神经网络（CNN）使用二维卷积核，尽管也可以使用一维或三维卷积核。
- en: Convolution in Two Dimensions
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 二维卷积
- en: 'Convolution with a 2D kernel requires a 2D array. Images are 2D arrays of values,
    and convolution is a common image processing operation. For example, let’s load
    an image, the face of the raccoon we saw in [Chapter 3](ch03.xhtml#ch03), and
    alter it with a 2D convolution. Consider the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与 2D 核的卷积需要一个 2D 数组。图像是值的 2D 数组，而卷积是常见的图像处理操作。例如，假设我们加载图像，即我们在[第 3 章](ch03.xhtml#ch03)中看到的浣熊的面部，并使用
    2D 卷积对其进行处理。考虑以下内容：
- en: from scipy.signal import convolve2d
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.signal import convolve2d
- en: from scipy.misc import face
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.misc import face
- en: img = face(True)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: img = face(True)
- en: img = img[:512,(img.shape[1]-612):(img.shape[1]-100)]
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: img = img[:512,(img.shape[1]-612):(img.shape[1]-100)]
- en: k = np.array([[1,0,0],[0,-8,0],[0,0,3]])
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: k = np.array([[1,0,0],[0,-8,0],[0,0,3]])
- en: c = convolve2d(img, k, mode='same')
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: c = convolve2d(img, k, mode='same')
- en: Here, we’re using the SciPy convolve2d function from the signal module. First,
    we load the raccoon image and subset it to a 512×512-pixel image of the raccoon’s
    face (img). Next, we define a 3 × 3 kernel, k. Lastly, we convolve the kernel,
    as it is, with the face image, storing the result in c. The mode='same' keyword
    zero pads the image to handle the edge cases.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用来自 signal 模块的 SciPy convolve2d 函数。首先，我们加载浣熊图像并将其子集化为一个 512×512 像素的浣熊面部图像
    (img)。接下来，我们定义一个 3 × 3 的卷积核 k。最后，我们将卷积核直接与面部图像进行卷积，并将结果存储在 c 中。mode='same' 关键字通过零填充图像来处理边缘情况。
- en: The code above leads to
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码得到的结果是：
- en: 'img[:8,:8]:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'img[:8,:8]:'
- en: '[[ 88 97 112 127 116  97  84  84]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 88 97 112 127 116  97  84  84]'
- en: '[ 62 70 100 131 126  88  52  51]'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 62 70 100 131 126  88  52  51]'
- en: '[ 41 46  87 127 146 116  78  56]'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 41 46  87 127 146 116  78  56]'
- en: '[ 42 45  76 107 145 137 112  76]'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 42 45  76 107 145 137 112  76]'
- en: '[ 58 59  69  79 111 106  90  68]'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 58 59  69  79 111 106  90  68]'
- en: '[ 74 73  68  60  72  74  72  67]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 74 73  68  60  72  74  72  67]'
- en: '[ 92 87  75  63  57  74  91  93]'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 92 87  75  63  57  74  91  93]'
- en: '[105 97  85  74  60  79 102 110]]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[105 97  85  74  60  79 102 110]]'
- en: 'k:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'k:'
- en: '[[ 1  0 0]'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ 1  0 0]'
- en: '[ 0 -8 0]'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0 -8 0]'
- en: '[ 0  0 3]]'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0  0 3]]'
- en: 'c[1:8,1:8]:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 'c[1:8,1:8]:'
- en: '[[-209 -382 -566 -511 -278  -69 -101]'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[[-209 -382 -566 -511 -278  -69 -101]'
- en: '[-106 -379 -571 -638 -438 -284 -241]'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[-106 -379 -571 -638 -438 -284 -241]'
- en: '[-168 -391 -484 -673 -568 -480 -318]'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[-168 -391 -484 -673 -568 -480 -318]'
- en: '[-278 -357 -332 -493 -341 -242 -143]'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[-278 -357 -332 -493 -341 -242 -143]'
- en: '[-335 -304 -216 -265 -168 -165 -184]'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[-335 -304 -216 -265 -168 -165 -184]'
- en: '[-389 -307 -240 -197 -274 -396 -427]'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[-389 -307 -240 -197 -274 -396 -427]'
- en: '[-404 -331 -289 -215 -368 -476 -488]]'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[-404 -331 -289 -215 -368 -476 -488]]'
- en: Here, we’re showing the upper 8 × 8 corner of the image and the valid portion
    of the convolution. Recall, the valid portion is the part where the kernel completely
    covers the input array.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们显示了图像的上方 8 × 8 区域以及卷积的有效部分。回想一下，有效部分是卷积核完全覆盖输入数组的部分。
- en: For the kernel and the image, the first valid convolution output is −209\. Mathematically,
    the first step is element-wise multiplication with the kernel,
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积核和图像，第一次有效的卷积输出是 −209。数学上，第一步是与卷积核进行逐元素相乘，
- en: '![Image](Images/233equ01.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/233equ01.jpg)'
- en: followed by a summation,
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 随后进行求和，
- en: 264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 264 + 0 + 0 + 0 + (−560) + 0 + 0 + 0 + 87 = −209
- en: Notice how the kernel used wasn’t k as we defined it. Instead, convolve2d flipped
    the kernel top to bottom and then left to right before it was applied. The remainder
    of c flows from moving the kernel one position to the right and repeating the
    multiplication and addition. At the end of a row, the kernel moves down one position
    and back to the left, until the entire image has been processed. Deep learning
    toolkits refer to this motion as the *stride*, and it need not be one position
    or equal in the horizontal and vertical directions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用的卷积核不是我们定义的 k。相反，convolve2d 在应用之前先将卷积核上下翻转，再左右翻转。c 的其余部分来自将卷积核向右移动一格，并重复乘法和加法操作。行末时，卷积核向下移动一格并回到左侧，直到整个图像处理完毕。深度学习工具包将这种运动称为
    *步幅*，它不一定是每次移动一格，也不必在水平和垂直方向上相等。
- en: '[Figure 9-4](ch09.xhtml#ch09fig04) shows the effect of the convolution.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-4](ch09.xhtml#ch09fig04) 显示了卷积的效果。'
- en: '![image](Images/09fig04.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig04.jpg)'
- en: '*Figure 9-4: The original raccoon face image (left) and the convolution result
    (right)*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-4：原始浣熊面部图像（左）和卷积结果（右）*'
- en: To make the image, c was shifted up, so the minimum value was zero, and then
    divided by the maximum to map to [0, 1]. Finally, the output was multiplied by
    255 and displayed as a grayscale image. The original face image is on the left.
    The convolved image is on the right. Convolution of the image with the kernel
    has altered the image, emphasizing some features while suppressing others.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成图像，c被向上移动，所以最小值变为零，然后除以最大值以映射到[0, 1]。最后，输出乘以255并显示为灰度图像。左边是原始人脸图像，右边是卷积后的图像。与内核的图像卷积改变了图像，强调了某些特征，同时抑制了其他特征。
- en: 'Convolving kernels with images isn’t merely an exercise to help us understand
    the convolution operation. It’s of profound importance in the training of CNNs.
    Conceptually, a CNN consists of two main parts: a set of convolution and other
    layers taught to learn a new representation of the input, and a top-level classifier
    taught to use the new representation to classify the inputs. It’s the joint learning
    of the new representation and the classifier that makes CNNs so powerful. The
    key to learning a new representation of the input is the set of learned convolution
    kernels. How the kernels alter the input as data flows through the CNN creates
    the new representation. Training with gradient descent and backpropagation teaches
    the network which kernels to create.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将内核与图像进行卷积不仅仅是帮助我们理解卷积操作的练习。它在卷积神经网络（CNN）的训练中具有深远的重要性。从概念上讲，CNN由两个主要部分组成：一组卷积层和其他层，用于学习输入的新表示；以及一个顶层分类器，用于利用新表示对输入进行分类。正是新表示和分类器的联合学习使得CNN如此强大。学习输入的新表示的关键是学习到的卷积内核。内核如何在数据通过CNN时改变输入，创造出新的表示。通过梯度下降和反向传播进行训练，教会网络创建哪些内核。
- en: We’re now in a position to follow data through a CNN’s convolutional layers.
    Let’s take a look.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以跟踪数据通过CNN的卷积层。让我们来看一看。
- en: Convolutional Layers
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积层
- en: Above, we discussed how deep networks pass tensors from layer to layer and how
    the tensor usually has four dimensions, *N* × *H* × *W* × *C*. To follow data
    through a convolutional layer, we’ll ignore *N*, knowing that what we discuss
    is applied to each sample in the tensor. This leaves us with inputs to the convolutional
    layer that are *H* × *W* × *C*, a 3D tensor.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们讨论了深度网络如何将张量从一层传递到另一层，以及张量通常有四个维度，*N* × *H* × *W* × *C*。为了跟踪数据通过卷积层的流动，我们将忽略*N*，知道我们讨论的内容是应用于张量中的每个样本。这就剩下输入到卷积层的是*H*
    × *W* × *C*，一个三维张量。
- en: The output of a convolutional layer is another 3D tensor. The height and width
    of the output depend on the convolution kernels’ particulars and how we decide
    to handle the edges. We’ll use valid convolution for the examples here, meaning
    we’ll discard parts of the input that the kernel doesn’t wholly cover. If the
    kernel is 3 × 3, the output will be two less in height and width, one less for
    each edge. A 5 × 5 kernel loses four in height and width, two less for each edge.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出是另一个三维张量。输出的高度和宽度取决于卷积核的具体情况以及我们决定如何处理边缘。在这里的例子中，我们将使用有效卷积，这意味着我们会丢弃卷积核未完全覆盖的输入部分。如果卷积核是3
    × 3，输出的高度和宽度会减少2，每个边缘减少1。一个5 × 5的卷积核会使高度和宽度减少4，每个边缘减少2。
- en: The convolutional layer uses sets of *filters* to accomplish its goal. A filter
    is a stack of kernels. We need one filter for each of the desired output channels.
    The number of kernels in the stack of each filter matches the number of channels
    in the input. So, if the input has *M* channels, and we want *N* output channels
    using *K* × *K* kernels, we need *N* filters, each of which is a stack of *M K*
    × *K* kernels.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层使用一组*滤波器*来实现其目标。滤波器是一个内核堆栈。每个期望的输出通道需要一个滤波器。每个滤波器中内核堆栈的数量与输入通道的数量相匹配。所以，如果输入有*M*个通道，我们想要使用*K*
    × *K*的内核得到*N*个输出通道，我们需要*N*个滤波器，每个滤波器是一个*M K* × *K*内核的堆栈。
- en: Additionally, we have a bias value for each of the *N* filters. We’ll see below
    how the bias is used, but we now know how many parameters we need to learn to
    implement a convolutional layer with *M* input channels, *K* × *K* kernels, and
    *N* outputs. It’s *K* × *K* × *M* × *N* for *N* filters with *K* × *K* × *M* parameters
    each, plus *N* bias terms—one per filter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们为每个*N*个滤波器都有一个偏置值。我们将在下面看到偏置是如何使用的，但现在我们已经知道了要实现一个具有*M*输入通道、*K* × *K*内核和*N*输出的卷积层，我们需要学习多少个参数。它是*K*
    × *K* × *M* × *N*，每个滤波器有*K* × *K* × *M*个参数，再加上*N*个偏置项——每个滤波器一个。
- en: Let’s make all of this concrete. We have a convolutional layer. The input to
    the layer is an (*H*,*W*,*C*) = (5,5,2) tensor, meaning a height and width of
    five and two channels. We’ll use a 3 × 3 kernel with valid convolution, so the
    output in height and width is 3 × 3 from the 5 × 5 input. We get to select the
    number of output channels. Let’s use three. Therefore, we need to use convolution
    and kernels to map a (5,5,2) input to a (3,3,3) output. From what we discussed
    above, we know we need three filters, and each filter has 3 × 3 × 2 parameters,
    plus a bias term.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们具体化一下。我们有一个卷积层。该层的输入是一个（*H*, *W*, *C*）= (5, 5, 2) 的张量，意味着高度和宽度都是五，且有两个通道。我们将使用一个
    3 × 3 的卷积核并进行有效卷积，因此从 5 × 5 的输入得到的输出高度和宽度为 3 × 3。我们可以选择输出通道的数量。我们选择三个。因此，我们需要使用卷积和卷积核将（5,
    5, 2）的输入映射为（3, 3, 3）的输出。从上面的讨论中，我们知道需要三个滤波器，每个滤波器有 3 × 3 × 2 个参数，并加上一个偏置项。
- en: Our input stack is
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入堆栈是
- en: '![Image](Images/235equ01.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/235equ01.jpg)'
- en: We’ve split the third dimension to show the two input channels, each 5 × 5.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将第三维度分割，以显示两个输入通道，每个通道为 5 × 5。
- en: The three filters are
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个滤波器是
- en: '![Image](Images/236equ01.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ01.jpg)'
- en: Again, we’ve separated the third dimension. Notice how each filter has two 3
    × 3 kernels, one for each channel of the 5 × 5 × 2 input.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们已经分离了第三维度。注意，每个滤波器有两个 3 × 3 的卷积核，每个卷积核对应 5 × 5 × 2 输入的一个通道。
- en: 'Let’s work through applying the first filter, *f*[0]. We need to convolve the
    first channel of the input with the first kernel of *f*[0]:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来应用第一个滤波器 *f*[0]。我们需要将输入的第一个通道与 *f*[0] 的第一个卷积核进行卷积：
- en: '![Image](Images/236equ02.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ02.jpg)'
- en: 'Then, we need to convolve the second input channel with the second kernel of
    *f*[0]:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将第二个输入通道与 *f*[0] 的第二个卷积核进行卷积：
- en: '![Image](Images/236equ03.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ03.jpg)'
- en: 'Finally, we add the two convolution outputs along with the single bias scalar:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个卷积输出与一个偏置标量相加：
- en: '![Image](Images/236equ04.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ04.jpg)'
- en: We now have the first 3 × 3 output.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了第一个 3 × 3 的输出。
- en: Repeating the process above for *f*[1] and *f*[2] gives
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *f*[1] 和 *f*[2] 重复上述过程得到
- en: '![Image](Images/236equ05.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/236equ05.jpg)'
- en: We’ve completed the convolutional layer and generated the 3 × 3 × 3 output.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已完成卷积层并生成了 3 × 3 × 3 的输出。
- en: Many toolkits make it easy to add operations in the call that sets up the convolutional
    layer, but, conceptually, these are layers of their own that accept the 3 × 3
    × 3 output as an input. For example, if requested, Keras will apply a ReLU to
    the output. Applying a ReLU, a nonlinearity, to the output of the convolution
    would give us
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具包使得在设置卷积层时添加操作变得容易，但从概念上讲，这些操作本身就是接受 3 × 3 × 3 输出作为输入的层。例如，如果需要，Keras 会对输出应用
    ReLU 激活函数。对卷积输出应用 ReLU（一个非线性函数）会给我们带来
- en: '![Image](Images/237equ01.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/237equ01.jpg)'
- en: 'Note that all elements less than zero are now zero. We use a nonlinearity between
    convolutional layers for the same reason we use a nonlinear activation function
    in a traditional neural network: to keep the convolutional layers from collapsing
    into a single layer. Notice how the operation to generate the filter outputs is
    purely linear; each output element is a linear combination of input values. Adding
    the ReLU breaks this linearity.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有小于零的元素现在都变成了零。我们在卷积层之间使用非线性操作，原因与我们在传统神经网络中使用非线性激活函数相同：防止卷积层塌缩为单一层。注意，生成滤波器输出的操作是纯线性的；每个输出元素都是输入值的线性组合。加入
    ReLU 之后，打破了这种线性关系。
- en: One reason for the creation of convolutional layers was to reduce the number
    of learned parameters. For the example above, the input was 5 × 5 × 2 = 50 elements.
    The desired output was 3 × 3 × 3 = 27 elements. A fully connected layer between
    these would need to learn 50 × 27 = 1,350 weights, plus another 27 bias values.
    However, the convolutional layer learned three filters, each with 3 × 3 × 2 weights,
    as well as three bias values, for a total of 3(3 × 3 × 2) + 3 = 57 parameters.
    Adding the convolutional layer saved learning some 1,300 additional weights.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 创建卷积层的一个原因是减少学习参数的数量。对于上面的例子，输入是 5 × 5 × 2 = 50 个元素。期望的输出是 3 × 3 × 3 = 27 个元素。如果是一个全连接层，它需要学习
    50 × 27 = 1,350 个权重，还需要 27 个偏置值。然而，卷积层只学到了三个滤波器，每个滤波器有 3 × 3 × 2 个权重，以及三个偏置值，总共是
    3(3 × 3 × 2) + 3 = 57 个参数。加入卷积层使得学习节省了大约 1,300 个额外的权重。
- en: The output of a convolutional layer is often the input to a pooling layer. Let’s
    consider that type of layer next.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出通常是池化层的输入。接下来我们考虑这种类型的层。
- en: Pooling Layers
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 池化层
- en: Convolutional networks often use *pooling layers* after convolutional layers.
    Their use is a bit controversial, as they discard information, and the loss of
    information might make it harder for the network to learn spatial relationships.
    Pooling is generally performed in the spatial domain along the input tensor’s
    height and width while preserving the number of channels.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络通常在卷积层之后使用 *池化层*。它们的使用有些争议，因为它们会丢弃信息，而信息的丢失可能会使网络更难学习空间关系。池化通常在空间域中执行，沿着输入张量的高度和宽度进行，同时保持通道数不变。
- en: 'The pooling operation is straightforward: you move a window over the image,
    usually 2 × 2 with a stride of two, to group values. The specific pooling operation
    performed on each group is either max or average. The max-pooling operation preserves
    the maximum value in the window and discards the rest. Average pooling takes the
    mean of the values in the window.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作非常简单：你将一个窗口在图像上滑动，通常是 2 × 2，步幅为二，以便对值进行分组。对每组进行的具体池化操作是最大池化或平均池化。最大池化操作保留窗口中的最大值，并丢弃其余值。平均池化则取窗口中所有值的平均值。
- en: A 2 × 2 window with a stride of two results in a reduction of a factor of two
    in each spatial direction. Therefore, a (24,24,32) input tensor leads to a (12,12,32)
    output tensor. [Figure 9-5](ch09.xhtml#ch09fig05) illustrates the process for
    maximum pooling.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步幅为二的 2 × 2 窗口会导致每个空间方向的大小减少一倍。因此，一个 (24,24,32) 的输入张量会变成一个 (12,12,32) 的输出张量。[图
    9-5](ch09.xhtml#ch09fig05) 说明了最大池化过程。
- en: '![image](Images/09fig05.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig05.jpg)'
- en: '*Figure 9-5: Max pooling with a 2* × *2 window and a stride of two*'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9-5：使用 2* × *2 窗口和步幅为二的最大池化*'
- en: One channel of the input, with a height and width of eight, is on the left.
    The 2 × 2 window slides over the input, jumping by two, so there is no overlap
    of windows. The output for each 2 × 2 region of the input is the maximum value.
    Average pooling would instead output the mean of the four numbers. As with normal
    convolution, at the end of the row, the window slides down two positions, and
    the process repeats to change the 8 × 8 input channel to a 4 × 4 output channel.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的一个通道，宽高均为八，位于左侧。2 × 2 窗口在输入上滑动，每次跳跃两个位置，因此窗口之间没有重叠。每个 2 × 2 区域的输出是最大值。平均池化则会输出四个数字的平均值。与正常的卷积一样，行末时，窗口会下移两个位置，然后重复该过程，将
    8 × 8 的输入通道转换为 4 × 4 的输出通道。
- en: As mentioned above, pooling without overlap in the windows loses spatial information.
    This has caused some in the deep learning community, most notably Geoffrey Hinton,
    to lament its use, as dropping spatial information distorts the relationship between
    objects or parts of objects in the input. For example, applying a 2 × 2 max pooling
    window with a stride of one instead of two to the input matrix of [Figure 9-5](ch09.xhtml#ch09fig05)
    produces
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，窗口内没有重叠的池化会丢失空间信息。这让深度学习社区中的一些人，特别是 Geoffrey Hinton，感到遗憾，因为丢失空间信息会扭曲输入中物体或物体部分之间的关系。例如，使用步幅为一的
    2 × 2 最大池化窗口，而不是步幅为二，作用于 [图 9-5](ch09.xhtml#ch09fig05) 的输入矩阵会产生
- en: '![Image](Images/238equ01.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/238equ01.jpg)'
- en: This is a 7 × 7 output, which only loses one row and column of the original
    8 × 8 input. In this case, the input matrix was randomly generated, so we should
    expect a max-pooling operation biased toward eights and nines—there is no structure
    to capture. This is not usually the case in an actual CNN, of course, as it’s
    the spatial structure inherent in the inputs we wish to utilize.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 7 × 7 的输出，只丢失了原始 8 × 8 输入中的一行和一列。在这种情况下，输入矩阵是随机生成的，所以我们可以预期最大池化操作偏向于八和九——没有结构可捕捉。当然，这在实际的卷积神经网络中通常不是这样，因为我们希望利用输入中固有的空间结构。
- en: Pooling is commonly used in deep learning, especially for CNNs, so it’s essential
    to understand what a pooling operation is doing and be aware of its potential
    pitfalls. Let’s move on now to the output end of a CNN, typically the fully connected
    layers.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 池化在深度学习中非常常见，尤其是在卷积神经网络中，因此了解池化操作的作用并意识到其潜在的弊端非常重要。接下来，我们将讨论卷积神经网络的输出端，通常是全连接层。
- en: Fully Connected Layers
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全连接层
- en: A fully connected layer in a deep network is, in terms of weights and data,
    identical to a regular layer in a traditional neural network. Many deep networks
    concerned with classification pass the output of a set of convolution and pooling
    layers to the first fully connected layer via a layer that flattens the tensor,
    essentially unraveling it into a vector. Once the output is a vector, the fully
    connected layer uses a weight matrix in the same way a traditional neural network
    does to map a vector input to a vector output.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络中的全连接层，在权重和数据的意义上，与传统神经网络中的常规层相同。许多关注分类的深度网络将卷积层和池化层的输出通过一个展平张量的层传递给第一个全连接层，这个层本质上是将张量展开为一个向量。一旦输出变成向量，全连接层便使用权重矩阵，像传统神经网络一样，将向量输入映射为向量输出。
- en: Data Flow Through a Convolutional Neural Network
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积神经网络中的数据流
- en: Let’s put all the pieces together to see how data flows through a CNN from input
    to output. We’ll use a simple CNN trained on the MNIST dataset, a collection of
    28×28-pixel grayscale images of handwritten digits. The architecture is shown
    next.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有部分整合在一起，看看数据如何从输入流经CNN直到输出。我们将使用一个在MNIST数据集上训练的简单CNN，MNIST数据集包含28×28像素的手写数字灰度图像。其架构如下所示。
- en: Input → Conv(32) → Conv(64) → Pool → Flatten → Dense(128) → Dense(10)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 → 卷积（32） → 卷积（64） → 池化 → 展平 → 全连接（128） → 全连接（10）
- en: The input is a 28×28-pixel grayscale image (one channel). The convolutional
    layers (conv) use 3 × 3 kernels and valid convolution, so their output’s height
    and width are two less than their input. The first convolutional layer learns
    32 filters while the second learns 64\. We’re ignoring layers that do not affect
    the amount of data in the network, like the ReLU layers after the convolutional
    layers. The max-pooling layer is assumed to use a 2 × 2 window with a stride of
    two. The first fully connected layer (dense) has 128 nodes, followed by an output
    layer of 10 nodes, one for each digit, 0 to 9.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个28×28像素的灰度图像（一个通道）。卷积层（conv）使用3 × 3的卷积核和有效卷积，因此其输出的高度和宽度比输入少两个像素。第一个卷积层学习32个滤波器，而第二个卷积层学习64个滤波器。我们忽略了那些不影响网络数据量的层，例如卷积层后的ReLU层。假设最大池化层使用2
    × 2窗口，并且步幅为2。第一个全连接层（dense）有128个节点，接着是一个输出层，包含10个节点，每个节点对应一个数字，0到9。
- en: The tensors passed through this network for a single input sample are
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个输入样本，传递通过该网络的张量是：
- en: (28,28,1) →(26,26,32) →(24,24,64) →(12,12,64) → 9216 → 128 → 10
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: (28,28,1) →(26,26,32) →(24,24,64) →(12,12,64) → 9216 → 128 → 10
- en: Input        Conv        Conv         Pool      Flatten  Dense   Dense
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输入        卷积        卷积         池化      展平    全连接   全连接
- en: The flatten layer unravels the (12,12,64) tensor to form a vector of 9,216 elements
    (12 × 12 × 64 = 9,216). We pass the 9,216 elements that the flatten layer outputs
    through the first dense layer to generate 128 output values, and the last step
    takes the 128-element vector and maps it to 10 output values.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 展平层将（12,12,64）的张量展开，形成一个包含9,216个元素的向量（12 × 12 × 64 = 9,216）。我们将展平层输出的9,216个元素传递到第一个全连接层，以生成128个输出值，最后一步是将这个128维的向量映射到10个输出值。
- en: Note, the values above refer to the *data* passed through the network for each
    input sample, one of the *N* samples in the minibatch. This is not the same as
    the number parameters (weights and biases) the network needed to learn during
    training.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述数值指的是传递通过网络的*数据*，对应每个输入样本，是*小批量*中的*N*个样本之一。这与网络在训练期间需要学习的参数（权重和偏置）数量不同。
- en: The network shown above was trained on the MNIST digits using Keras. [Figure
    9-6](ch09.xhtml#ch09fig06) illustrates the action of the network for two inputs
    by showing, visually, the output of each layer. Specifically, it shows each layer’s
    output for two input images, depicting a 4 and a 6.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示的网络是在MNIST数字数据集上使用Keras训练的。[图9-6](ch09.xhtml#ch09fig06)通过可视化每一层的输出，展示了该网络对两个输入的作用，具体而言，展示了每一层对于输入图像4和6的输出。
- en: '![image](Images/09fig06.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig06.jpg)'
- en: '*Figure 9-6: A visual representation of the output of a CNN for two sample
    inputs*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-6：CNN对于两个样本输入的输出的可视化表示*'
- en: Starting at the top, we see the two inputs. For the figure, intensities have
    been reversed, so darker represents higher numeric values. The input is a (28,28,1)
    tensor, the 1 indicating a single-channel grayscale image. Valid convolution with
    a 3 × 3 kernel returns a 26 × 26 output. The first convolutional layer learned
    32 filters, so the output is a (26,26,32) tensor. In the figure, we show the output
    of each filter as an image. Zero is scaled to midlevel gray (intensity 128), more
    positive values are darker, and more negative values are lighter. We see differences
    in how the inputs have been affected by the learned filters. The single input
    channel means each filter in this layer is a single 3 × 3 kernel. Transitions
    between light and dark indicate edges in particular orientations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部开始，我们看到两个输入。对于图示，强度已经反转，因此较暗表示较高的数值。输入是一个(28,28,1)张量，其中1表示单通道灰度图像。与3 × 3的核进行有效卷积会返回一个26
    × 26的输出。第一个卷积层学习了32个滤波器，因此输出是一个(26,26,32)张量。在图中，我们将每个滤波器的输出显示为图像。零被缩放为中等灰色（强度128），更高的值较暗，较低的值较亮。我们可以看到输入如何受到学习滤波器的影响。单一的输入通道意味着这一层的每个滤波器都是一个3
    × 3的核。明暗交替的过渡表示特定方向的边缘。
- en: We pass the (26,26,32) tensor through a ReLU (not shown here) and then through
    the second convolutional layer. The output of this layer is a (24,24,64) tensor
    shown as an 8 × 8 grid of images in the figure. We can see many parts of the input
    digits highlighted.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将(26,26,32)张量通过ReLU（此处未显示）传递，然后通过第二个卷积层。该层的输出是一个(24,24,64)张量，在图中以8 × 8的图像网格显示。我们可以看到输入数字的许多部分被突出显示。
- en: The pooling layer preserves the number of channels but reduces the spatial dimension
    by two. In image form, the 8 × 8 grid of 24×24-pixel images is now an 8 × 8 grid
    of 12×12-pixel images. The flatten operation maps the (12,12,64) tensor to a 9,216-element
    vector.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层保留了通道数，但将空间维度减少了两倍。以图像形式显示，24×24像素的8 × 8网格图像现在变成了12×12像素的8 × 8网格图像。展平操作将(12,12,64)张量映射为一个9,216元素的向量。
- en: The output of the first dense layer is a vector of 128 numbers. For [Figure
    9-6](ch09.xhtml#ch09fig06), we show this as a 128-element bar code. The values
    run from left to right. The height of each bar is unimportant and was selected
    only to make the bar code easy to see. The bar code generated from the input image
    is the final representation that the top layer of 10 nodes uses to create the
    output passed through the softmax function. The highest softmax output is used
    to select the class label, “4” or “6.”
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个密集层的输出是一个包含128个数字的向量。对于[图9-6](ch09.xhtml#ch09fig06)，我们将其显示为一个128元素的条形码。值从左到右排列。每个条形的高度并不重要，仅仅是为了让条形码更容易看清。由输入图像生成的条形码是顶部10个节点层用来创建输出并通过softmax函数传递的最终表示。最高的softmax输出用于选择类别标签，“4”或“6”。
- en: Therefore, we can think of all the CNN layers through the first dense layer
    as mapping inputs to a new representation, one that makes it easy for a simple
    classifier to handle. Indeed, if we pass 10 examples of “4” and “6” digits through
    this network and display the resulting 128-node feature vectors, we get [Figure
    9-7](ch09.xhtml#ch09fig07), where we can easily see the difference between the
    digit patterns.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将通过第一个密集层的所有CNN层视为将输入映射到一个新的表示形式，这种表示形式使得简单的分类器容易处理。事实上，如果我们通过这个网络传递10个“4”和“6”的示例，并显示结果的128节点特征向量，我们会得到[图9-7](ch09.xhtml#ch09fig07)，在那里我们可以轻松地看到数字模式之间的差异。
- en: '![image](Images/09fig07.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/09fig07.jpg)'
- en: '*Figure 9-7: The first fully connected layer outputs for multiple “4” and “6”
    inputs*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9-7：第一个全连接层输出多个“4”和“6”输入*'
- en: Of course, the entire point of writing digits as we do is to make it easy for
    humans to see the differences between them. While we could teach ourselves to
    differentiate digits using the 128-element vector images, we naturally prefer
    to use the written digits because of habitual use and the fact we already employ
    highly sophisticated hierarchical feature detectors via our brain’s visual system.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们写数字的整个目的就是让人类更容易看到它们之间的差异。虽然我们可以通过128元素的向量图像来区分数字，但我们自然更倾向于使用书写的数字，因为习惯的原因以及我们大脑视觉系统已经通过高度复杂的分层特征检测器来识别这些数字。
- en: The example of a CNN learning a new input representation that’s more conducive
    to interpretation by a machine is worth bearing in mind, since what a human might
    use in an image as a clue to its classification is not necessarily what a network
    learns to use. This might explain, in part, why certain preprocessing steps, like
    the changes made to training samples during data augmentation, are so effective
    in helping the network learn to generalize, when many of those alterations seem
    strange to us.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 记住CNN学习新的输入表示形式的例子是值得的，因为人类在图像中可能用作分类线索的东西，并不一定是网络学会使用的内容。这或许能部分解释为什么某些预处理步骤，比如在数据增强过程中对训练样本所做的改变，会在帮助网络学会泛化方面如此有效，尽管这些改变对我们来说似乎很奇怪。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'The goal of this chapter was to demonstrate how neural networks manipulate
    data from input to output. Naturally, we couldn’t cover all network types, but,
    in general, the principles are the same: for traditional neural networks, data
    is passed from layer to layer as a vector, and for deep networks, it’s passed
    as a tensor, typically of four dimensions.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是展示神经网络如何从输入到输出操作数据。自然地，我们无法涵盖所有类型的网络，但总体来说，原理是相同的：对于传统的神经网络，数据作为向量从一层传递到另一层；而对于深度网络，它作为张量传递，通常是四维的。
- en: We learned how to present data to a network, either as a feature vector or a
    multidimensional input. We followed this by looking at how to pass data through
    a traditional neural network. We saw how the vectors used as input to, and output
    from, a layer made the implementation of a traditional neural network a straightforward
    exercise in matrix-vector multiplication and addition.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何将数据呈现给网络，无论是作为特征向量还是多维输入。接着我们观察了如何通过传统神经网络传递数据。我们看到了作为输入和输出的向量如何使传统神经网络的实现变成了一个简单的矩阵-向量乘法和加法的练习。
- en: Next, we saw how a deep convolutional network passes data from layer to layer.
    We learned first about the convolution operation and then about the specifics
    of how convolutional and pooling layers manipulate data as tensors—a 3D tensor
    for each sample in the input minibatch. At the top of a CNN meant for classification
    are fully connected layers, which we saw act precisely as they do in a traditional
    neural network.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看到了深度卷积网络是如何将数据从一层传递到另一层的。我们首先学习了卷积操作，然后了解了卷积层和池化层如何将数据作为张量进行处理——每个输入小批量中的样本都是一个三维张量。在用于分类的卷积神经网络（CNN）顶部是全连接层，我们看到它们的作用与传统神经网络中的全连接层完全一致。
- en: We ended the chapter by showing, visually, how input images moved through a
    CNN to produce an output representation, allowing the network to label the inputs
    correctly. We briefly discussed what this process might mean in terms of what
    a network picks up on during training and how that might differ from what a human
    naturally sees in an image.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后我们通过可视化展示了输入图像如何通过CNN处理生成输出表示，从而使得网络能够正确地标注输入。我们简要讨论了这个过程在训练中网络所学习的内容，以及这可能与人类在图像中自然看到的内容有所不同。
- en: We are now in a position to discuss backpropagation, the first of the two critical
    algorithms that, together with gradient descent, make training deep neural networks
    possible.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以讨论反向传播，这是两个关键算法中的第一个，与梯度下降一起，使得训练深度神经网络成为可能。
