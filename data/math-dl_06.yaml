- en: '**6'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**6'
- en: MORE LINEAR ALGEBRA**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的线性代数**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: In this chapter, we’ll continue our exploration of linear algebra concepts.
    Some of these concepts are only tangentially related to deep learning, but they’re
    the sort of math you’ll eventually encounter. Think of this chapter as assumed
    background knowledge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探索线性代数的概念。虽然其中一些概念与深度学习的关系仅是间接的，但它们是你最终会遇到的数学内容。可以将本章看作是假定的背景知识。
- en: Specifically, we’ll learn more about the properties of and operations on square
    matrices, introducing terms you’ll encounter in the deep learning literature.
    After that, I’ll introduce the ideas behind the eigenvalues and eigenvectors of
    a square matrix and how to find them. Next, we’ll explore vector norms and other
    ways of measuring distance that are often encountered in deep learning. At that
    point, I’ll introduce the important concept of a covariance matrix.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将更深入地了解方阵的性质以及对方阵进行的操作，并介绍一些你在深度学习文献中常遇到的术语。之后，我将介绍方阵的特征值和特征向量的概念，并讲解如何找到它们。接下来，我们将探讨向量范数以及在深度学习中经常遇到的其他距离度量方式。到那个时候，我将介绍协方差矩阵这一重要概念。
- en: We’ll conclude the chapter by demonstrating principal component analysis (PCA)
    and singular value decomposition (SVD). These frequently used approaches depend
    heavily on the concepts and operators introduced throughout the chapter. We will
    see what PCA is, how to do it, and what it can buy us from a machine learning
    perspective. Similarly, we will work with SVD and see how we can use it to implement
    PCA as well as compute the pseudoinverse of a rectangular matrix.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章最后通过演示主成分分析（PCA）和奇异值分解（SVD）来总结这一章节。这些常用的方法非常依赖于本章中介绍的概念和操作符。我们将了解PCA是什么，如何执行PCA，以及从机器学习的角度它能给我们带来什么。类似地，我们将使用SVD，看看如何用它来实现PCA，以及如何计算矩形矩阵的伪逆。
- en: Square Matrices
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方阵
- en: Square matrices occupy a special place in the world of linear algebra. Let’s
    explore them in more detail. The terms used here will show up often in deep learning
    and other areas.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 方阵在线性代数的世界中占有特殊的地位。让我们更详细地探讨它们。这里使用的术语将在深度学习及其他领域中经常出现。
- en: Why Square Matrices?
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么是方阵？
- en: 'If we multiply a matrix by a column vector, we’ll get another column vector
    as output:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一个矩阵与一个列向量相乘，我们将得到另一个列向量作为输出：
- en: '![Image](Images/128equ01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/128equ01.jpg)'
- en: Interpreted geometrically, the 2 × 4 matrix has mapped the 4 × 1 column vector,
    a point in ℝ⁴, to a new point in ℝ². The mapping is linear because the point values
    are only being multiplied by the elements of the 2 × 4 matrix; there are no nonlinear
    operations, such as raising the components of the vector to a power, for example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何角度解释，2 × 4 的矩阵将 4 × 1 的列向量（ℝ⁴ 中的一个点）映射到了 ℝ² 中的一个新点。这个映射是线性的，因为点的值仅仅是与 2 ×
    4 矩阵的元素相乘；没有非线性操作，比如将向量的分量升幂等操作。
- en: Viewed this way, we can use a matrix to transform points between spaces. If
    the matrix is square, say, *n* × *n*, the mapping is from ℝ^(*n*) back to ℝ^(*n*).
    For example, consider
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度看，我们可以使用矩阵在不同空间之间进行点的变换。如果矩阵是方阵，比如 *n* × *n*，则映射是从 ℝ^(*n*) 到 ℝ^(*n*)。例如，考虑
- en: '![Image](Images/128equ02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/128equ02.jpg)'
- en: where the point (11, 12, 13) is mapped to the point (74, 182, 209), both in
    ℝ³.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，点 (11, 12, 13) 被映射到点 (74, 182, 209)，这两个点都位于 ℝ³ 中。
- en: Using a matrix to map points from one space to another makes it possible to
    rotate a set of points about an axis by using a *rotation matrix*. For simple
    rotations, we can define matrices in 2D,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵将点从一个空间映射到另一个空间，使得可以使用*旋转矩阵*将一组点绕某一轴旋转。对于简单的旋转，我们可以在二维中定义矩阵，
- en: '![Image](Images/06equ01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ01.jpg)'
- en: and in 3D,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在三维空间中，
- en: '![Image](Images/128equ03.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/128equ03.jpg)'
- en: Rotations are by an angle, *θ*, and for 3D, about the x-, y-, or z-axis, as
    indicated by the subscript.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转通过一个角度 *θ* 来进行，对于三维空间，旋转是围绕 x 轴、y 轴或 z 轴进行的，如下标所示。
- en: Using a matrix, we can create an *affine transformation*. An affine transformation
    maps a set of points into another set of points so that points on a line in the
    original space are still on a line in the mapped space. The transformation is
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵，我们可以创建一个*仿射变换*。仿射变换将一组点映射到另一组点，使得原空间中的直线上的点在映射后的空间中仍然位于直线上。该变换是
- en: '***y*** = ***Ax*** + ***b***'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***y*** = ***Ax*** + ***b***'
- en: The affine transform combines a matrix transform, ***A***, with a translation,
    ***b***, to map a vector, ***x***, to a new vector, ***y***. We can combine this
    operation into a single matrix multiplication by putting ***A*** in the upper-left
    corner of the matrix and adding ***b*** as a new column on the right. A row of
    all zeros at the bottom with a single 1 in the rightmost column completes the
    augmented transformation matrix. For an affine transformation matrix
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 仿射变换将矩阵变换 ***A*** 与平移 ***b*** 结合，以将一个向量 ***x*** 映射到一个新的向量 ***y***。我们可以通过将 ***A***
    放在矩阵的左上角，并将 ***b*** 添加为右侧的新列，将此操作合并为一个单一的矩阵乘法。底部的一行全是零，右侧列中有一个1，构成了增广的变换矩阵。对于仿射变换矩阵
- en: '![Image](Images/129equ01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/129equ01.jpg)'
- en: and translation vector
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 和平移向量
- en: '![Image](Images/129equ02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/129equ02.jpg)'
- en: we get
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到
- en: '![Image](Images/129equ03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/129equ03.jpg)'
- en: This form maps a point, *(x*, *y*), to a new point, (*x*′, *y*′).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式将一个点，*(x*, *y*)，映射到一个新的点，(*x*′, *y*′)。
- en: This maneuver is identical to the *bias trick* sometimes used when implementing
    a neural network to bury the bias in an augmented weight matrix by including an
    extra feature vector input set to 1\. In fact, we can view a feedforward neural
    network as a series of affine transformations, where the transformation matrix
    is the weight matrix between the layers, and the bias vector provides the translation.
    The activation function at each layer alters the otherwise linear relationship
    between the layers. It is this nonlinearity that lets the network learn a new
    way to map inputs so that the final output reflects the functional relationship
    the network is designed to learn.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这一操作与在实现神经网络时有时使用的 *偏置技巧* 完全相同，即通过在增广权重矩阵中包含一个额外的特征向量并将其设为1来“埋藏”偏置。事实上，我们可以将前馈神经网络视为一系列仿射变换，其中变换矩阵是层与层之间的权重矩阵，偏置向量提供了平移。每一层的激活函数改变了层与层之间的线性关系。正是这种非线性使得网络能够学习映射输入的全新方式，从而最终输出反映网络设计要学习的功能关系。
- en: We use square matrices, then, to map points from one space back into the same
    space, for example to rotate them about an axis. Let’s look now at some special
    properties of square matrices.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用方阵来将点从一个空间映射回同一空间，例如围绕某个轴旋转它们。现在让我们来看看方阵的一些特殊性质。
- en: Transpose, Trace, and Powers
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转置、迹和幂
- en: '[Chapter 5](ch05.xhtml#ch05) showed us the vector transpose to move between
    column and row vectors. The transpose operation is not restricted to vectors.
    It works for any matrix by flipping the rows and columns along the main diagonal.
    For example,'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[第五章](ch05.xhtml#ch05)向我们展示了通过向量转置在列向量和行向量之间移动。转置操作不仅限于向量，它对任何矩阵都有效，通过沿主对角线翻转行和列。例如，'
- en: '![Image](Images/129equ04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/129equ04.jpg)'
- en: 'The transpose is formed by flipping the indices of the matrix elements:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 转置是通过翻转矩阵元素的索引形成的：
- en: '*a[ij]* ← *a[ij]*, *i* = 0, 1, . . . , *n* − 1, *j* = 0, 1, . . . , *m* – 1'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*a[ij]* ← *a[ij]*， *i* = 0, 1, …… , *n* − 1, *j* = 0, 1, …… , *m* − 1'
- en: This changes an *n* × *m* matrix into an *m* × *n* matrix. Notice that the order
    of a square matrix remains the same under the transpose operation, and the values
    on the main diagonal don’t change.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这将一个 *n* × *m* 矩阵转换为一个 *m* × *n* 矩阵。注意，方阵在转置操作下其阶数保持不变，且主对角线上的值不变。
- en: In NumPy, you can call the `transpose` method on an array, but the transpose
    is so common that a shorthand notation (`.T`) also exists. For example,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，你可以对数组调用 `transpose` 方法，但由于转置操作非常常见，也有一种简写方式（`.T`）。例如，
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The *trace* is another common operation applied to square matrices:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*迹* 是另一个常见的操作，应用于方阵：'
- en: '![Image](Images/130equ01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/130equ01.jpg)'
- en: 'As an operator, the trace has certain properties. For example, it’s linear:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个算子，迹有某些特性。例如，它是线性的：
- en: tr(***A*** + ***B***) = tr***A*** + tr***B***
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: tr(***A*** + ***B***) = tr***A*** + tr***B***
- en: It’s also true that tr(***A***) = tr(***A****^T*) and tr(***AB***) = tr(***BA***).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样成立的是 tr(***A***) = tr(***A****^T*) 和 tr(***AB***) = tr(***BA***)。
- en: NumPy uses `np.trace` to quickly calculate the trace of a matrix and `np .diag`
    to return the diagonal elements of a matrix as a 1D array,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 使用 `np.trace` 快速计算矩阵的迹，并使用 `np.diag` 返回矩阵的对角线元素作为一维数组。
- en: (*a*[00], *a*[11], . . . , *a[n−1,n−1]*)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (*a*[00]， *a*[11]，……， *a[n−1,n−1]*)
- en: for an *n* × *n* or *n* × *m* matrix.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 *n* × *n* 或 *n* × *m* 矩阵。
- en: 'A matrix doesn’t need to be square for NumPy to return the elements along its
    diagonal. And although mathematically the trace generally only applies to square
    matrices, NumPy will calculate the trace of any matrix, returning the sum of the
    diagonal elements:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NumPy来说，矩阵不需要是方阵，也可以返回其对角线上的元素。尽管从数学上讲，迹通常只适用于方阵，但NumPy会计算任何矩阵的迹，返回对角线元素的和：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Lastly, you can multiply a square matrix by itself, implying that you can raise
    a square matrix to an integer power, *n*, by multiplying itself *n* times. Note
    that this is not the same as raising the elements of the matrix to a power. For
    example,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以将一个方阵与其自身相乘，这意味着你可以通过将方阵自乘*n*次来将其提升到整数幂*n*。请注意，这与将矩阵的元素提升到幂不同。例如，
- en: '![Image](Images/131equ01.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/131equ01.jpg)'
- en: 'The matrix power follows the same rules as raising any number to a power:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的幂遵循与将任何数字提升到幂相同的规则：
- en: '***A**^n**A**^m* = ***A**^(n+m)*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***A**^n**A**^m* = ***A**^(n+m)*'
- en: (***A**^n*)^(*m*) = ***A**^(nm)*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (***A**^n*)^(*m*) = ***A**^(nm)*
- en: for ![Image](Images/131equ01a.jpg) (positive integers) and where ***A*** is
    a square matrix.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于![Image](Images/131equ01a.jpg)（正整数），且***A***是方阵。
- en: 'NumPy provides a function to compute the power of a square matrix more efficiently
    than repeated calls to `np.dot`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy提供了一种方法，比反复调用`np.dot`更高效地计算方阵的幂：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let’s consider some special square matrices that you’ll encounter from time
    to time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一些你可能会遇到的特殊方阵。
- en: Special Square Matrices
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特殊方阵
- en: 'Many square (and nonsquare) matrices have received special names. Some are
    rather obvious, like matrices that are all zero or one, which are called *zeros
    matrices* and *ones matrices*, respectively. NumPy uses these extensively:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方阵（以及非方阵）已经得到了特殊的命名。有些名称非常直观，比如全是零或全是一次的矩阵，分别称为*零矩阵*和*单位矩阵*。NumPy广泛使用这些：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that you can find a matrix of any constant value, *c*, by multiplying the
    ones matrix by *c*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可以通过将单位矩阵乘以*c*来找到任何常数值的矩阵。
- en: Notice above that NumPy defaults to matrices of 64-bit floating-point numbers
    corresponding to a C-language type of `double`. See [Table 1-1](ch01.xhtml#ch01tab01)
    on page 6 for a list of possible numeric data types. You can specify the desired
    data type with the `dtype` keyword. In pure mathematics, we don’t care much about
    data types, but to work in deep learning, you need to pay attention to avoid defining
    arrays that are far more memory-hungry than needed. Many deep learning models
    are happy with arrays of 32-bit floats, which use half the memory per element
    than the NumPy default. Also, many toolkits make use of new or previously seldom-used
    data types, like 16-bit floats, to allow for even better use of memory. NumPy
    does support 16-bit floats by specifying `float16` as the `dtype`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上面提到的NumPy默认使用64位浮点数矩阵，对应于C语言中的`double`类型。请参见[表1-1](ch01.xhtml#ch01tab01)，了解可能的数值数据类型。在纯数学中，我们不太关心数据类型，但在深度学习中，你需要注意，避免定义比实际需要更多内存的数组。许多深度学习模型对32位浮点数组非常满意，它们每个元素使用的内存是NumPy默认值的一半。另外，许多工具包也开始使用新的或以前不常用的数据类型，比如16位浮点数，以便更好地利用内存。NumPy支持16位浮点数，可以通过将`dtype`设置为`float16`来实现。
- en: The Identity Matrix
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单位矩阵
- en: 'By far, the most important special matrix is the *identity matrix*. This is
    a square matrix with all ones on the diagonal:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，最重要的特殊矩阵是*单位矩阵*。这是一个方阵，所有对角线上的元素都为1：
- en: '![Image](Images/06equ02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ02.jpg)'
- en: The identity matrix acts like the number 1 when multiplying a matrix. Therefore,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 单位矩阵在乘法中起着类似于数字1的作用。因此，
- en: '***AI*** = ***IA*** = ***A***'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '***AI*** = ***IA*** = ***A***'
- en: for an *n* × *n* square matrix ***A*** and an *n* × *n* identity matrix ***I***.
    When necessary, we’ll add a subscript to indicate the order of the identity matrix,
    for example, ***I**[n]*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个*n* × *n*的方阵***A***和一个*n* × *n*的单位矩阵***I***，在必要时，我们会添加下标以表示单位矩阵的阶数，例如，***I**[n]*。
- en: 'NumPy uses `np.identity` or `np.eye` to generate identity matrices of a given
    size:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy使用`np.identity`或`np.eye`生成给定大小的单位矩阵：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Look carefully at the example above. Mathematically, we said that multiplication
    of a square matrix by the identity matrix of the same order returns the matrix.
    NumPy, however, did something we might not want. Matrix `a` was defined with integer
    elements, so it has a data type of `int64`, the NumPy default for integers. However,
    since we didn’t explicitly provide `np.identity` with a data type, NumPy defaulted
    to a 64-bit float. Therefore, matrix multiplication (`@`) between `a` and `i`
    returned a floating-point version of `a`. This subtle change of data type might
    be important for later calculations, so, again, we need to pay attention to data
    types when using NumPy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察上面的例子。从数学角度看，我们说将方阵与相同阶数的单位矩阵相乘会返回原矩阵。然而，NumPy 做了一件我们可能不希望发生的事情。矩阵 `a` 被定义为整数元素，因此它的数据类型是
    `int64`，这是 NumPy 对整数的默认数据类型。然而，由于我们没有显式地为 `np.identity` 指定数据类型，NumPy 默认使用了 64
    位浮点数。因此，矩阵乘法（`@`）操作 `a` 和 `i` 返回了 `a` 的浮点版本。这种微妙的数据类型变化可能对后续计算很重要，因此，再次提醒，在使用
    NumPy 时我们需要关注数据类型。
- en: It doesn’t matter if you use `np.identity` or `np.eye`. In fact, internally,
    `np.identity` is just a wrapper for `np.eye`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用 `np.identity` 还是 `np.eye` 都没关系。事实上，`np.identity` 实际上只是 `np.eye` 的一个封装。
- en: Triangular Matrices
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 三角矩阵
- en: 'Occasionally, you’ll hear about *triangular* matrices. There are two kinds:
    upper and lower. As you may intuit from the name, an upper triangular matrix is
    one with nonzero elements in the part on or above the main diagonal, whereas a
    lower triangular matrix only has elements on or below the main diagonal. For example,'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你会听到关于*三角*矩阵的说法。三角矩阵有两种：上三角和下三角。正如你从名字中可以直观推测的那样，上三角矩阵是指在主对角线或其上方有非零元素的矩阵，而下三角矩阵则只有在主对角线或其下方才有元素。例如，
- en: '![Image](Images/133equ01.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/133equ01.jpg)'
- en: is an upper triangular matrix, whereas
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个上三角矩阵，而
- en: '![Image](Images/133equ02.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/133equ02.jpg)'
- en: is a lower triangular matrix. A matrix that has elements only on the main diagonal
    is, not surprisingly, a *diagonal matrix*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个下三角矩阵。一个只在主对角线上有元素的矩阵，毫不奇怪，是一个*对角矩阵*。
- en: NumPy has two functions, `np.triu` and `np.tril`, to return the upper or lower
    triangular part of the given matrix, respectively. So,
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 有两个函数，`np.triu` 和 `np.tril`，分别返回给定矩阵的上三角部分或下三角部分。所以，
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We don’t frequently use triangular matrices in deep learning, but we do use
    them in linear algebra, in part to compute determinants, to which we now turn.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在深度学习中不常使用三角矩阵，但我们在线性代数中确实使用它们，部分原因是计算行列式，接下来我们将讨论这一点。
- en: Determinants
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 行列式
- en: We can think of the *determinant* of a square matrix, *n* × *n*, as a function
    mapping square matrices to a scalar. The primary use of the determinant in deep
    learning is to compute the eigenvalues of a matrix. We’ll see what that means
    later in this chapter, but for now think of eigenvalues as special scalar values
    associated with a matrix. The determinant also tells us something about whether
    or not a matrix has an inverse, as we’ll also see below. Notationally, we write
    the determinant of a matrix with vertical bars. For example, if ***A*** is a 3
    × 3 matrix, we write the determinant as
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将方阵 *n* × *n* 的*行列式*看作是一个将方阵映射到标量的函数。在深度学习中，行列式的主要用途是计算矩阵的特征值。我们将在本章后面看到这是什么意思，但现在可以将特征值看作是与矩阵相关的特殊标量值。行列式还可以告诉我们矩阵是否有逆矩阵，下面我们也会看到这一点。在符号上，我们用竖线表示矩阵的行列式。例如，如果
    ***A*** 是一个 3 × 3 的矩阵，我们写行列式为
- en: '![Image](Images/134equ01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/134equ01.jpg)'
- en: 'where we state explicitly that the value of the determinant is a scalar (element
    of ℝ). All square matrices have a determinant. For now, let’s consider some of
    the properties of the determinant:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们明确声明行列式的值是一个标量（ℝ 的元素）。所有方阵都有一个行列式。现在，我们来考虑一下行列式的一些性质：
- en: If any row or column of ***A*** is zero, then det(***A***) = 0.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 的任一行或列为零，则 det(***A***) = 0。
- en: If any two rows of ***A*** are identical, then det(***A***) = 0.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 的任意两行相同，则 det(***A***) = 0。
- en: If ***A*** is an upper or lower triangular, then det ![Image](Images/134equ02.jpg).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 是上三角或下三角矩阵，那么 det ![Image](Images/134equ02.jpg)。
- en: If ***A*** is a diagonal matrix, then det ![Image](Images/134equ03.jpg).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ***A*** 是对角矩阵，那么 det ![Image](Images/134equ03.jpg)。
- en: The determinant of the identity matrix, regardless of size, is 1.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单位矩阵的行列式，无论大小如何，都是 1。
- en: The determinant of a product of matrices is the product of the determinants,
    det(***AB***) = det(***A***)det(***B***).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵乘积的行列式等于行列式的乘积，det(***AB***) = det(***A***)det(***B***)。
- en: det(***A***) = det(***A***^⊤).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: det(***A***) = det(***A***^⊤)。
- en: det(***A**^n*) = det(***A***)^(*n*).
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: det(***A**^n*) = det(***A***)^(*n*)。
- en: Property 7 indicates that the transpose operation does not change the value
    of a determinant. Property 8 is a consequence of Property 6.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 属性7指出，转置操作不会改变行列式的值。属性8是属性6的一个结果。
- en: We have multiple ways we can calculate the determinant of a square matrix. We’ll
    examine only one way here, which involves using a recursive formula. All recursive
    formulas apply themselves, just as recursive functions in code call themselves.
    The general idea is that each recursion works on a simpler version of the problem,
    which can be combined to return the solution to the larger problem.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方法可以计算方阵的行列式。这里我们只研究一种方法，涉及使用递归公式。所有递归公式都适用于自身，就像代码中的递归函数调用自己一样。一般的思路是，每次递归都处理一个简化版的问题，最终将这些结果结合，得到大问题的解答。
- en: For example, we can calculate the factorial of an integer,
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以计算一个整数的阶乘，
- en: '*n*! = *n*(*n* − 1)(*n* − 2)(*n* − 3) . . . 1'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*! = *n*(*n* − 1)(*n* − 2)(*n* − 3) . . . 1'
- en: 'recursively if we notice the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们注意到以下几点，它是递归的：
- en: '![Image](Images/135equ01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/135equ01.jpg)'
- en: 'The first statement says that the factorial of *n* is *n* times the factorial
    of (*n* − 1). The second statement says that the factorial of zero is one. The
    recursion is the first statement, but this recursion will never end without some
    condition that returns a value. That’s the point of the second statement, the
    *base case*: it says the recursion ends when we get to zero.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个语句说，*n*的阶乘是*n*乘以(*n* − 1)的阶乘。第二个语句说零的阶乘是1。递归就是第一个语句，但没有某个条件返回值，这个递归将永远不会结束。这就是第二个语句的意义，即*基准情况*：它说当我们达到零时，递归结束。
- en: 'This might be clearer in code. We can define the factorial like so:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这在代码中可能会更清晰。我们可以这样定义阶乘：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that `factorial` calls itself on the argument minus one, unless the argument
    is zero, in which case it immediately returns one. The code works because of the
    Python call stack. The call stack keeps track of all the computations of `n*factorial(n-1)`.
    When we encounter the base case, all the pending multiplications are done, and
    we return the final value.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`factorial`在参数减去1时会调用自身，除非参数为零，在这种情况下，它立即返回1。这个代码能正常工作是因为Python的调用栈。调用栈跟踪所有`n*factorial(n-1)`的计算。当我们遇到基准情况时，所有待完成的乘法都已经完成，我们返回最终值。
- en: To calculate determinants recursively, then, we need a recursion statement,
    something that defines the determinant of a matrix in terms of simpler determinants.
    We also need a base case that gives us a definitive value. For determinants, the
    base case is when we get to a 1 × 1 matrix. For any 1 × 1 matrix, ***A***, we
    have
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了递归地计算行列式，我们需要一个递归语句，定义行列式为更简化的行列式的组合。我们还需要一个基准情况，它给出一个确定的值。对于行列式，基准情况是当我们得到一个1
    × 1的矩阵时。对于任何1 × 1矩阵***A***，我们有
- en: det(***A***) = *a*[00]
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A***) = *a*[00]
- en: meaning the determinant of a 1 × 1 matrix is the single value it contains.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 意味着1 × 1矩阵的行列式就是它包含的单一数值。
- en: Our plan is to calculate the determinant by breaking the calculation into successively
    simpler determinants until we reach the base case above. To do this, we need a
    statement involving recursion. However, we need to define a few things before
    we can make the statement. First, we need to define the *minor* of a matrix. The
    (*i*, *j*)-minor of a matrix, ***A***, is the matrix left after removing the *i*th
    row and *j*th column of ***A***. We’ll denote a minor matrix by ***A****[ij]*.
    For example, given
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计划是通过将计算分解为一个个逐渐简化的行列式来计算行列式，直到我们达到上述基准情况。为了做到这一点，我们需要一个涉及递归的语句。然而，在我们能够作出这个语句之前，我们需要定义一些内容。首先，我们需要定义矩阵的*次式*。矩阵***A***的(*i*,
    *j*)-次式是从***A***中删除第*i*行和第*j*列后剩下的矩阵。我们用***A****[ij]*表示次式矩阵。例如，给定
- en: '![Image](Images/135equ02.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/135equ02.jpg)'
- en: then
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![Image](Images/136equ01.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/136equ01.jpg)'
- en: where the minor, ***A***[11], is found by deleting row 1 and column 1 to leave
    only the underlined values.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，次式***A***[11]是通过删除第1行和第1列，只留下下划线的数值来找到的。
- en: Second, we need to define the *cofactor*, *C**[ij]*, of the minor, ***A****[ij]*.
    This is where our recursive statement appears. The definition is
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们需要定义次式，即*C**[ij]*，即***A****[ij]*的余因子。这就是我们递归语句出现的地方。定义是
- en: '*C**[ij]* = (−1)^(*i*+*j*+2)det(***A****[ij]*)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*C**[ij]* = (−1)^(*i*+*j*+2)det(***A****[ij]*)'
- en: 'The cofactor depends on the determinant of the minor. Notice the exponent on
    −1, written as *i* + *j* + 2\. If you look at most math books, you’ll see the
    exponent as *i* + *j*. We’ve made a conscious choice to define matrices with zero-based
    indices so the math and implementation in code match without being off by one.
    Here’s one place where that choice forces us to be less elegant than the math
    texts. Because our indices are “off” by one, we need to add that one back into
    the exponent of the cofactor so the pattern of positive and negative values that
    the cofactor uses is correct. This means adding one to each of the variables in
    the exponent: *i* → *i* + 1 and *j* → *j* + 1\. This makes the exponent *i* +
    *j* → (*i* + 1) + (*j* + 1) = *i* + *j* + 2.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 余因子取决于代数余子式的行列式。注意-1的指数，它写作*i* + *j* + 2。如果你查看大多数数学书籍，你会看到指数是*i* + *j*。我们做出了一个有意识的选择，将矩阵定义为从零开始的索引，这样数学和代码实现就可以匹配，而不至于因为偏移而出错。这里是我们选择的一个地方，这使得我们在表达上不如数学书籍中的优雅。由于我们的索引“错”了1，我们需要将这一点加回到余因子的指数中，这样余因子使用的正负值模式才是正确的。这意味着我们需要在指数中的每个变量上加1：*i*
    → *i* + 1 和 *j* → *j* + 1。这样，指数*i* + *j* → (*i* + 1) + (*j* + 1) = *i* + *j* +
    2。
- en: We’re now ready for our full recursive definition of the determinant of ***A***
    by using *cofactor expansion*. It turns out that summing the product of the matrix
    values and associated cofactors for any row or column of a square matrix will
    give us the determinant. So, we’ll use the first row of the matrix and calculate
    the determinant as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好使用*余因子展开*来定义***A***的行列式的完整递归公式。事实证明，求解一个方阵任意一行或一列的矩阵值和相应余因子的乘积并求和，将得到行列式。因此，我们将使用矩阵的第一行来计算行列式，公式如下：
- en: '![Image](Images/06equ03.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ03.jpg)'
- en: 'You may be wondering: Where’s the recursion in [Equation 6.3](ch06.xhtml#ch06equ03)?
    It shows up on the determinant of the minor. If ***A*** is an *n* × *n* matrix,
    the minor, ***A****[ij]*, is an *(n* − 1) × (*n* − 1) matrix. Therefore, to calculate
    the cofactors to find the determinant of an *n* × *n* matrix, we need to know
    how to find the determinant of an *(n* − 1) × (*n* − 1) matrix. However, we can
    use cofactor expansion to find the *(n* − 1) × (*n* − 1) determinant, which involves
    finding the determinant of an *(n* − 2) × (*n* − 2) matrix. This process continues
    until we get to a 1 × 1 matrix. We already know the determinant of a 1 × 1 matrix
    is the single value it contains.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：在[方程 6.3](ch06.xhtml#ch06equ03)中，递归在哪里？它出现在代数余子式的行列式中。如果***A***是一个*n*
    × *n*的矩阵，代数余子式***A****[ij]*是一个*(n* − 1) × (*n* − 1)的矩阵。因此，为了计算余因子以找到一个*n* × *n*矩阵的行列式，我们需要知道如何计算一个*(n*
    − 1) × (*n* − 1)矩阵的行列式。然而，我们可以使用余因子展开来计算*(n* − 1) × (*n* − 1)的行列式，这涉及到计算一个*(n*
    − 2) × (*n* − 2)矩阵的行列式。这个过程会持续，直到我们得到一个1 × 1的矩阵。我们已经知道，1 × 1矩阵的行列式就是它包含的唯一值。
- en: 'Let’s work through this process for a 2 × 2 matrix:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个2 × 2矩阵来演示这个过程：
- en: '![Image](Images/136equ02.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/136equ02.jpg)'
- en: Using cofactor expansion, we get
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用余因子展开，我们得到
- en: '![Image](Images/137equ01.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/137equ01.jpg)'
- en: which is the formula for the determinant of a 2 × 2 matrix. The minors of a
    2 × 2 matrix are 1 × 1 matrices, each returning either *d* or *c* in this case.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是2 × 2矩阵行列式的公式。2 × 2矩阵的代数余子式是1 × 1矩阵，在这种情况下，每个余子式的值分别为*d*或*c*。
- en: In NumPy, we calculate determinants with `np.linalg.det`. For example,
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中，我们使用`np.linalg.det`来计算行列式。例如，
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last line of code uses the formula for a 2 × 2 matrix we derived above
    for comparison purposes. Internally, NumPy does not use recursive cofactor expansion
    to calculate the determinant. Instead, it factors the matrix into the product
    of three matrices: (1) a *permutation matrix*, which looks like a scrambled identity
    matrix with only a single one in each row and column, (2) a lower triangular matrix,
    and (3) an upper triangular matrix. The determinant of the permutation matrix
    is either +1 or −1\. The determinant of a triangular matrix is the product of
    the diagonal elements, while the determinant of a product of matrices is the product
    of the per-matrix determinants.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后一行使用了我们之前推导出的2 × 2矩阵的公式作为对比。实际上，NumPy内部并不使用递归的余因子展开来计算行列式。相反，它将矩阵分解为三个矩阵的乘积：（1）一个*置换矩阵*，看起来像是一个打乱的单位矩阵，每行每列只有一个1，（2）一个下三角矩阵，以及（3）一个上三角矩阵。置换矩阵的行列式是+1或-1，三角矩阵的行列式是对角线元素的乘积，而矩阵乘积的行列式是各个矩阵行列式的乘积。
- en: We can use determinants to determine whether a matrix has an inverse. Let’s
    turn there now.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用行列式来判断一个矩阵是否有逆矩阵。现在让我们转到这个话题。
- en: Inverses
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逆矩阵
- en: '[Equation 6.2](ch06.xhtml#ch06equ02) defines the identity matrix. We said that
    this matrix acts like the number 1, so when it multiplies a square matrix, the
    same square matrix is returned. When multiplying scalars, we know that for any
    number, *x* ≠ 0, there exists another number, call it *y*, such that *xy* = 1\.
    This number is the multiplicative inverse of *x*. Furthermore, we know exactly
    what *y* is; it’s 1/*x* = *x*^(−1).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 6.2](ch06.xhtml#ch06equ02)定义了单位矩阵。我们说这个矩阵像数字1一样作用，因此当它与一个方阵相乘时，返回的仍然是相同的方阵。对于标量乘法，我们知道对于任何数字，*x*
    ≠ 0，存在另一个数字，记作*y*，使得*xy* = 1。这个数字是*x*的乘法逆元。此外，我们知道*y*的确切值，它是1/*x* = *x*^(−1)。'
- en: By analogy, then, we might wonder if, since we have an identity matrix that
    acts like the number 1, there is another square matrix, call it ***A***^(−1),
    for a given square matrix, ***A***, such that
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可能会想，既然我们有一个像数字1一样作用的单位矩阵，是否存在另一个方阵，记作***A***^(−1)，对于给定的方阵***A***，使得
- en: '***AA***^(−1) = ***A***^(−1) ***A*** = ***I***'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '***AA***^(−1) = ***A***^(−1) ***A*** = ***I***'
- en: 'If ***A***^(−1) exists, it’s known as the *inverse matrix* of ***A***, and
    ***A*** is said to be *invertable*. For real numbers, all numbers except zero
    have an inverse. For matrices, it isn’t so straightforward. Many square matrices
    don’t have inverses. To check if ***A*** has an inverse, we use the determinant:
    det(***A***) = 0 tells us that ***A*** has no inverse. Furthermore, if ***A***^(−1)
    exists, then'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果***A***^(−1)存在，那么它被称为***A***的*逆矩阵*，并且***A***被称为*可逆的*。对于实数，除零以外的所有数都有逆。对于矩阵来说，情况并不像实数那样简单。许多方阵没有逆。为了检查***A***是否有逆，我们使用行列式：det(***A***)
    = 0表示***A***没有逆。进一步地，如果***A***^(−1)存在，那么
- en: '![Image](Images/138equ01.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/138equ01.jpg)'
- en: Note also that (***A***^(−1))^(−1) = ***A***, as is the case for real numbers.
    Another useful property of inverses is
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，(***A***^(−1))^(−1) = ***A***，这与实数的情况相同。逆矩阵的另一个有用属性是
- en: (***AB***)^(−1) = ***B***^(−1) ***A***^(−1)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (***AB***)^(−1) = ***B***^(−1) ***A***^(−1)
- en: 'where the order of the product on the right-hand side is important. Finally,
    note that the inverse of a diagonal matrix is simply the reciprocal of the diagonal
    elements:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 右边乘积的顺序非常重要。最后，请注意，对角矩阵的逆仅仅是对角元素的倒数：
- en: '![Image](Images/138equ02.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/138equ02.jpg)'
- en: It’s possible to calculate the inverse by hand using row operations, which we’ve
    conveniently ignored here because they are seldom used in deep learning. Cofactor
    expansion techniques can also calculate the inverse, but to save time, we won’t
    elaborate on the process here. What’s important for us is to know that square
    matrices often have an inverse, and that we can calculate inverses with NumPy
    via `np.linalg.inv`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过行变换手动计算逆矩阵，我们在这里方便地忽略了这一点，因为在深度学习中很少使用。余因子展开法也可以计算逆矩阵，但为了节省时间，我们不会在这里详细说明过程。对我们来说，重要的是知道方阵通常有逆矩阵，并且我们可以通过NumPy的`np.linalg.inv`来计算逆矩阵。
- en: If a matrix is *not* invertible, the matrix is said to be *singular*. Therefore,
    the determinant of a singular matrix is zero. If a matrix has an inverse, it is
    a *nonsingular* or *nondegenerate* matrix.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个矩阵*不可逆*，则称该矩阵为*奇异矩阵*。因此，奇异矩阵的行列式为零。如果一个矩阵有逆矩阵，则它是*非奇异的*或*非退化的*矩阵。
- en: In NumPy, we use `np.linalg.inv` to calculate the inverse of a square matrix.
    For example,
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中，我们使用`np.linalg.inv`来计算方阵的逆。例如，
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice the inverse (`b`) working as we expect and giving the identity matrix
    when multiplying `a` from the left or right.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意逆矩阵（`b`）按预期工作，并在从左或右相乘时返回单位矩阵。
- en: Symmetric, Orthogonal, and Unitary Matrices
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对称矩阵、正交矩阵和酉矩阵
- en: If for a square matrix, ***A***, we have
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于一个方阵***A***，我们有
- en: '***A***^⊤ = ***A***'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***^⊤ = ***A***'
- en: then ***A*** is said to be a *symmetric matrix*. For example,
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 那么***A***称为*对称矩阵*。例如，
- en: '![Image](Images/139equ01.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/139equ01.jpg)'
- en: is a symmetric matrix, since ***A***^⊤ = ***A***.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个对称矩阵，因为***A***^⊤ = ***A***。
- en: 'Notice that diagonal matrices are symmetric, and the product of two symmetric
    matrices is commutative: ***AB*** = ***BA***. The inverse of a symmetric matrix,
    if it exists, is also a symmetric matrix.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意对角矩阵是对称的，并且两个对称矩阵的乘积是可交换的：***AB*** = ***BA***。如果存在的话，对称矩阵的逆也是对称矩阵。
- en: If the following is true,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立，
- en: '***AA***^⊤ = ***A***^⊤ ***A*** = ***I***'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '***AA***^⊤ = ***A***^⊤ ***A*** = ***I***'
- en: then ***A*** is an orthogonal matrix. If ***A*** is an orthogonal matrix, then
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***是一个正交矩阵。如果***A***是正交矩阵，那么'
- en: '***A***^(−1) = ***A***^⊤'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***^(−1) = ***A***^⊤'
- en: and, as a result,
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，
- en: det(***A***) = ±1
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A***) = ±1
- en: If the values in the matrix are allowed to be complex, which does not happen
    often in deep learning, and
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果矩阵中的值允许是复数（这种情况在深度学习中不常见），并且
- en: '***U***^****U*** = ***UU***^* = ***I***'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '***U***^****U*** = ***UU***^* = ***I***'
- en: then ***U*** is a *unitary matrix* with ***U***^* being the *conjugate transpose*
    of ***U***. The conjugate transpose is the ordinary matrix transpose followed
    by the complex conjugate operation to change ![Image](Images/140equ01.jpg) to
    −*i*. So, we might have
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 那么***U***是一个*单位矩阵*，其中***U***^*是***U***的*共轭转置*。共轭转置是普通的矩阵转置，随后执行复共轭操作，将![Image](Images/140equ01.jpg)变为−*i*。因此，我们可能会有
- en: '![Image](Images/140equ02.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ02.jpg)'
- en: Sometimes, especially in physics, the conjugate transpose is called the *Hermitian
    adjoint* and is denoted as ***A***^†. If a matrix is equal to its conjugate transpose,
    it is called a *Hermitian matrix*. Notice that real symmetric matrices are also
    Hermitian matrices because the conjugate transpose is the same as the ordinary
    transpose when the values are real numbers. Therefore, you might encounter the
    term *Hermitian* in place of *symmetric* when referring to matrices with real
    elements.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，特别是在物理学中，共轭转置被称为*埃尔米特伴随*，表示为***A***^†。如果一个矩阵等于它的共轭转置，那么它被称为*埃尔米特矩阵*。注意，实对称矩阵也是埃尔米特矩阵，因为当数值是实数时，共轭转置与普通转置相同。因此，在提到具有实数元素的矩阵时，你可能会遇到*埃尔米特*这个术语，代替*对称*。
- en: Definiteness of a Symmetric Matrix
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对称矩阵的定性
- en: We saw at the beginning of this section that an *n* × *n* square matrix maps
    a vector in ℝ^(*n*) to another vector in ℝ^(*n*). Let’s consider now a symmetric
    *n* × *n* matrix, ***B***, with real-valued elements. We can characterize this
    matrix by how it maps vectors using the inner product between the mapped vector
    and the original vector. Specifically, if ***x*** is a column vector (*n* × 1),
    then ***Bx*** is also an *n* × 1 column vector. Therefore, the inner product of
    this vector and the original vector, ***x***, is ***x***^⊤***Bx***, a scalar.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开头，我们看到一个*n* × *n*的方阵将一个ℝ^(*n*)中的向量映射到另一个ℝ^(*n*)中的向量。现在让我们考虑一个对称的*n* × *n*矩阵***B***，它的元素是实数值。我们可以通过它如何映射向量来表征这个矩阵，使用的是映射向量与原始向量之间的内积。具体来说，如果***x***是一个列向量（*n*
    × 1），那么***Bx***也是一个*n* × 1的列向量。因此，这个向量与原始向量***x***的内积是***x***^⊤***Bx***，这是一个标量。
- en: 'If the following is true:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立：
- en: '![Image](Images/140equ03.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ03.jpg)'
- en: then ***B*** is said to be *positive definite*. Here, the bolded **0** is the
    *n* × 1 column vector of all zeros, and ∀ is math notation meaning “for all.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 那么***B***就被称为*正定*的。这里，粗体字**0**是一个*n* × 1的全零列向量，∀是数学符号，表示“对于所有”。
- en: Similarly, if
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果
- en: '![Image](Images/140equ04.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ04.jpg)'
- en: then ***B*** is *negative definite*. Relaxing the inner product relationship
    and the nonzero requirement on ***x*** gives two additional cases. If
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 那么***B***是*负定*的。放松内积关系以及对***x***的非零要求会产生两个额外的情况。如果
- en: '![Image](Images/140equ05.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ05.jpg)'
- en: then ***B*** is said to be *positive semidefinite*, and
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 那么***B***被称为*正半定*，并且
- en: '![Image](Images/140equ06.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/140equ06.jpg)'
- en: makes ***B*** a *negative semidefinite* matrix. Finally, a real square symmetric
    matrix that is neither positive nor negative semidefinite is called an *indefinite
    matrix*. The definiteness of a matrix tells us something about the eigenvalues,
    which we’ll learn more about in the next section. If a symmetric matrix is positive
    definite, then all of its eigenvalues are positive. Similarly, a symmetric negative
    definite matrix has all negative eigenvalues. Positive and negative semidefinite
    symmetric matrices have eigenvalues that are all positive or zero or all negative
    or zero, respectively.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使得***B***成为*负半定*矩阵。最后，一个既不是正定也不是负定的实对称矩阵被称为*不定矩阵*。矩阵的定性告诉我们一些关于特征值的信息，接下来我们将在下一节中进一步学习。如果一个对称矩阵是正定的，那么它的所有特征值都是正数。类似地，一个对称的负定矩阵具有所有负的特征值。正半定和负半定对称矩阵的特征值分别全为正数或零，或全为负数或零。
- en: Let’s shift gears now from talking about types of matrices to discovering the
    importance of eigenvectors and eigenvalues, key properties of a matrix that we
    use frequently in deep learning.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从讨论矩阵类型转到探索特征向量和特征值的重要性，这是我们在深度学习中经常使用的矩阵的关键特性。
- en: Eigenvectors and Eigenvalues
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征向量与特征值
- en: We learned above that a square matrix maps a vector into another vector in the
    same dimensional space, ***v***′ = ***Av***, where both ***v***′ and ***v*** are
    *n*-dimensional vectors, if ***A*** is an *n* × *n* matrix.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面学习过，一个方阵将一个向量映射到同一维空间中的另一个向量，***v***′ = ***Av***，其中***v***′和***v***都是*n*维向量，如果***A***是一个*n*
    × *n*矩阵。
- en: Consider this equation,
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个方程，
- en: '![Image](Images/06equ04.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06equ04.jpg)'
- en: for some square matrix, ***A***, where λ is a scalar value and ***v*** is a
    nonzero column vector.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个方阵***A***，其中λ是标量值，***v***是一个非零列向量。
- en: '[Equation 6.4](ch06.xhtml#ch06equ04) says that the vector, ***v***, is mapped
    by ***A*** back into a scalar multiple of itself. We call ***v*** an *eigenvector*
    of ***A*** with *eigenvalue* λ. The prefix *eigen* comes from German and is often
    translated as “self,” “characteristic,” or even “proper.” Thinking geometrically,
    [Equation 6.4](ch06.xhtml#ch06equ04) says that the action of ***A*** on its eigenvectors
    in ℝ^(*n*) is to shrink or expand the vector without changing its direction. Note,
    while ***v*** is nonzero, it’s possible for λ to be zero.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 6.4](ch06.xhtml#ch06equ04)表示向量***v***被矩阵***A***映射回其自身的一个标量倍。我们称***v***为***A***的*特征向量*，特征值为λ。前缀*eigen*来自德语，通常翻译为“自我”、“特性”或“固有”。从几何角度来看，[方程
    6.4](ch06.xhtml#ch06equ04)表示矩阵***A***对其特征向量在ℝ^(*n*)中的作用是缩小或扩展向量，而不改变其方向。请注意，虽然***v***是非零的，但λ也可能是零。'
- en: How does [Equation 6.4](ch06.xhtml#ch06equ04) relate to the identity matrix,
    ***I*** ? By definition, the identity matrix maps a vector back into itself without
    scaling it. Therefore, the identity matrix has an infinite number of eigenvectors,
    and all of them have an eigenvalue of 1, since, for any ***x***, ***Ix*** = ***x***.
    Therefore, the same eigenvalue may apply to more than one eigenvector.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 6.4](ch06.xhtml#ch06equ04)如何与单位矩阵***I***相关？根据定义，单位矩阵将一个向量映射回自身，而不对其进行缩放。因此，单位矩阵具有无数个特征向量，并且它们的特征值都是1，因为对于任何***x***，有***Ix***
    = ***x***。因此，同一个特征值可能适用于多个特征向量。'
- en: Recall that [Equation 6.1](ch06.xhtml#ch06equ01) defines a rotation matrix in
    2D space for some given angle, *θ*. This matrix has no eigenvectors, because,
    for any nonzero vector, it rotates the vector by *θ*, so it can never map a vector
    back into its original direction. Therefore, not every matrix has eigenvectors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，[方程 6.1](ch06.xhtml#ch06equ01)定义了一个2D空间中的旋转矩阵，对于某个给定角度*θ*。这个矩阵没有特征向量，因为对于任何非零向量，它会将向量旋转*θ*，因此它永远无法将一个向量映射回其原始方向。因此，并非每个矩阵都有特征向量。
- en: Finding Eigenvalues and Eigenvectors
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 寻找特征值和特征向量
- en: 'To find the eigenvalues of a matrix, if there are any, we go back to [Equation
    6.4](ch06.xhtml#ch06equ04) and rewrite it:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到矩阵的特征值（如果有的话），我们回到[方程 6.4](ch06.xhtml#ch06equ04)并重写它：
- en: '![Image](Images/06equ05.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06equ05.jpg)'
- en: We can insert the identity matrix, ***I***, between λ and ***v*** because ***Iv***
    = ***v***. Therefore, to find the eigenvalues of ***A***, we need to find values
    of λ that cause the matrix ***A*** − λ***I*** to map a nonzero vector, ***v***,
    to the zero vector. [Equation 6.5](ch06.xhtml#ch06equ05) only has solutions other
    than the zero vector if the determinant of ***A*** − λ***I*** is zero.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在λ和***v***之间插入单位矩阵***I***，因为***Iv*** = ***v***。因此，要找到矩阵***A***的特征值，我们需要找到那些使矩阵***A***
    − λ***I***将一个非零向量***v***映射为零向量的λ值。[方程 6.5](ch06.xhtml#ch06equ05)只有在***A*** − λ***I***的行列式为零时，才有非零向量以外的解。
- en: 'The above gives us a way to find the eigenvalues. For example, consider what
    ***A*** − λ***I*** looks like for a 2 × 2 matrix:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容为我们提供了一种找到特征值的方法。例如，考虑对于一个2 × 2矩阵，***A*** − λ***I***的形式：
- en: '![Image](Images/142equ01.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/142equ01.jpg)'
- en: We learned above that the determinant of a 2 × 2 matrix has a simple form; therefore,
    the determinant of the matrix above is
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面学习过，2 × 2矩阵的行列式有一个简单的形式；因此，上述矩阵的行列式为
- en: det(***A*** − λ***I***) = (*a* − λ)(*d* − λ) − *bc*
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A*** − λ***I***) = (*a* − λ)(*d* − λ) − *bc*
- en: This equation is a second-degree polynomial in λ. Since we need the determinant
    to be zero, we set this polynomial to zero and find the roots. The roots are the
    eigenvalues of ***A***. The polynomial that this process finds is called the *characteristic
    polynomial*, and [Equation 6.5](ch06.xhtml#ch06equ05) is the *characteristic equation*.
    Notice above that the characteristic polynomial is a second-degree polynomial.
    In general, the characteristic polynomial of an *n* × *n* matrix is of degree
    *n*, so a matrix has at most *n* distinct eigenvalues, since an *n*th degree polynomial
    has at most *n* roots.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程是 λ 的二次多项式。由于我们需要行列式为零，我们将这个多项式设为零并找到它的根。根就是 ***A*** 的特征值。这个过程找到的多项式叫做*特征多项式*，[方程
    6.5](ch06.xhtml#ch06equ05)是*特征方程*。注意上面提到的特征多项式是二次多项式。通常，*n* × *n* 矩阵的特征多项式是 *n*
    次多项式，因此一个矩阵最多有 *n* 个不同的特征值，因为一个 *n* 次多项式最多有 *n* 个根。
- en: Once we know the roots of the characteristic polynomial, we can go back to [Equation
    6.5](ch06.xhtml#ch06equ05), substitute each root for λ, and solve to find the
    associated eigenvectors, the ***v***’s of [Equation 6.5](ch06.xhtml#ch06equ05).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了特征多项式的根，我们就可以返回到[方程 6.5](ch06.xhtml#ch06equ05)，将每个根代入 λ，并解出相关的特征向量，即[方程
    6.5](ch06.xhtml#ch06equ05)中的 ***v***。
- en: The eigenvalues of a triangular matrix, which includes diagonal matrices, are
    straightforward to calculate because the determinant of such a matrix is simply
    the product of the main diagonal. For example, for a 4 × 4 triangular matrix,
    the determinant of the characteristic equation is
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 三角矩阵的特征值（包括对角矩阵）容易计算，因为这种矩阵的行列式仅仅是主对角线元素的乘积。例如，对于一个 4 × 4 的三角矩阵，特征方程的行列式为
- en: det(***A*** − λ***I***) = (*a*[00] − λ)(*a*[11] − λ)(*a*[22] − λ)(*a*[33] −
    λ)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: det(***A*** − λ***I***) = (*a*[00] − λ)(*a*[11] − λ)(*a*[22] − λ)(*a*[33] −
    λ)
- en: 'which has four roots: the values of the diagonal. For triangular and diagonal
    matrices, the entries on the main diagonal *are* the eigenvalues.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 它有四个根：主对角线上的值。对于三角矩阵和对角矩阵，主对角线上的元素*就是*特征值。
- en: 'Let’s see a worked eigenvalue example for the following matrix:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下矩阵的特征值示例：
- en: '![Image](Images/142equ02.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/142equ02.jpg)'
- en: I selected this matrix to make the math nicer, but the process works for any
    matrix. The characteristic equation means we need the λ values that make the determinant
    zero, as shown next.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择这个矩阵是为了使数学计算更简便，但这个过程适用于任何矩阵。特征方程意味着我们需要找出使行列式为零的 λ 值，如下所示。
- en: '![Image](Images/143equ01.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/143equ01.jpg)'
- en: The characteristic polynomial is easily factored to give λ = −1, −2.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 特征多项式可以轻松因式分解得到 λ = −1, −2。
- en: 'In code, to find the eigenvalues and eigenvectors of a matrix, we use `np.linalg.eig`.
    Let’s check our calculation above to see if NumPy agrees:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，为了找到矩阵的特征值和特征向量，我们使用 `np.linalg.eig`。让我们检查一下上面的计算，看看 NumPy 是否同意：
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `np.linalg.eig` function returns a list. The first element is a vector of
    the eigenvalues of the matrix. The second element, which we are ignoring for the
    moment, is a matrix, the *columns* of which are the eigenvectors associated with
    each of the eigenvalues. Note that we also could have used `np.linalg.eigvals`
    to return just the eigenvalues. Regardless, we see that our calculation of the
    eigenvalues of ***A*** is correct.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.linalg.eig` 函数返回一个列表。第一个元素是矩阵的特征值向量。第二个元素（我们暂时忽略它）是一个矩阵，其*列*是与每个特征值相关的特征向量。注意，我们也可以使用
    `np.linalg.eigvals` 来仅返回特征值。无论如何，我们看到我们计算出的 ***A*** 的特征值是正确的。'
- en: To find the associated eigenvectors, we put each of the eigenvalues back into
    [Equation 6.5](ch06.xhtml#ch06equ05) and solve for ***v***. For example, for λ
    = −1, we get
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到相关的特征向量，我们将每个特征值代入[方程 6.5](ch06.xhtml#ch06equ05)并解出 ***v***。例如，当 λ = −1
    时，我们得到
- en: '![Image](Images/143equ02.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/143equ02.jpg)'
- en: 'which leads to the system of equations:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下方程组：
- en: '*v*[0] + *v*[1] = 0'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*v*[0] + *v*[1] = 0'
- en: −2*v*[0] − 2*v*[1] = 0
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: −2*v*[0] − 2*v*[1] = 0
- en: This system has many solutions, as long as *v*[0] = −*v*[1]. That means we can
    pick *v*[0] and *v*[1], as long as the relationship between them is preserved.
    Therefore, we have our eigenvector for
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统有许多解，只要 *v*[0] = −*v*[1]。这意味着我们可以选择 *v*[0] 和 *v*[1]，只要它们之间的关系被保留。由此，我们得到了我们的特征向量。
- en: '![Image](Images/143equ03.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/143equ03.jpg)'
- en: If we repeat this process for λ = −2, we get the relationship between the components
    of ***v*****[2]** to be 2*v*[0] = −*v*[1]. Therefore, we select ![Image](Images/143equ04.jpg)
    as the second eigenvector.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对λ = −2重复这个过程，我们得到***v*****[2]**的分量之间的关系是2*v*[0] = −*v*[1]。因此，我们选择![Image](Images/143equ04.jpg)作为第二个特征向量。
- en: 'Let’s see if NumPy agrees with us. This time, we’ll display the second list
    element returned by `np.linalg.eig`. This is a matrix where the columns of the
    matrix are the eigenvectors:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看NumPy是否与我们达成一致。这次，我们将显示`np.linalg.eig`返回的第二个列表元素。这是一个矩阵，矩阵的列是特征向量：
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Hmm . . . the columns of this matrix do not appear to match our selected eigenvectors.
    But don’t worry—we didn’t make a mistake. Recall that the eigenvectors were not
    uniquely determined, only the relationship between the components was determined.
    If we’d wanted to, we could have selected other values, as long as for one eigenvector
    they were of equal magnitude and opposite sign, and for the other they were in
    the ratio of 2:1 with opposite signs. What NumPy returns is a set of eigenvectors
    that are of unit length. So, to see that our hand calculation is correct, we need
    to make our eigenvectors unit vectors by dividing each component by the square
    root of the sum of the squares of the components. In code, it’s succinct, if a
    bit messy:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯……这个矩阵的列似乎与我们选择的特征向量不匹配。但别担心——我们没有犯错。回想一下，特征向量不是唯一确定的，只有分量之间的关系是确定的。如果我们愿意，我们可以选择其他值，只要对于一个特征向量，它们的大小相等且符号相反，对于另一个特征向量，它们的比值是2:1且符号相反。NumPy返回的是一组单位长度的特征向量。因此，为了验证我们的手工计算是正确的，我们需要通过将每个分量除以分量平方和的平方根，将我们的特征向量变成单位向量。在代码中，这样写很简洁，尽管有点乱：
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we see that we’re correct. The unit vector versions of the eigenvectors
    do match the columns of the matrix NumPy returned.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到我们是正确的。特征向量的单位向量版本确实与NumPy返回的矩阵的列匹配。
- en: We’ll use the eigenvectors and eigenvalues of a matrix often when we’re doing
    deep learning. For example, we’ll see them again later in the chapter when we
    investigate principal component analysis. But before we can learn about PCA, we
    need to change focus once again and learn about vector norms and distance metrics
    commonly used in deep learning, especially about the covariance matrix.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在做深度学习时经常使用矩阵的特征向量和特征值。例如，当我们研究主成分分析（PCA）时，我们会再次看到它们。但是在我们学习PCA之前，我们需要再次转换焦点，学习在深度学习中常用的向量范数和距离度量，特别是关于协方差矩阵的内容。
- en: Vector Norms and Distance Metrics
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量范数和距离度量
- en: In common deep learning parlance, people are somewhat sloppy and use the terms
    *norm* and *distance* interchangeably. We can forgive them for doing so; the difference
    between the terms is small in practice, as we’ll see below.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在常见的深度学习术语中，人们有些随便地将*范数*和*距离*互换使用。我们可以原谅他们这样做；正如我们下面将看到的，实际上这两个术语的区别很小。
- en: A *vector norm* is a function that maps a vector, real or complex, to some value,
    *x* ∈ ℝ, *x* ≥ 0\. A norm must satisfy some specific properties in a mathematical
    sense, but in practice, not everything that’s called a norm is, in fact, a norm.
    In deep learning, we usually use norms as distances between pairs of vectors.
    In practice, an important property for a distance measure is that the order of
    the inputs doesn’t matter. If *f*(*x*, *y*) is a distance, then *f*(*x*, *y*)
    = *f*(*y*, *x*). Again, this is not rigorously followed; for example, you’ll often
    see the Kullback-Leibler divergence (KL-divergence) used as a distance even though
    this property doesn’t hold.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量范数*是一个将向量（无论是实数还是复数）映射到某个值的函数，*x* ∈ ℝ，*x* ≥ 0。范数必须满足某些特定的数学性质，但在实践中，并不是所有称为范数的东西，实际上都是范数。在深度学习中，我们通常使用范数作为向量对之间的距离。实际上，距离度量的一个重要特性是输入的顺序不重要。如果*f*(*x*,
    *y*)是一个距离，那么*f*(*x*, *y*) = *f*(*y*, *x*)。再一次，这个性质并非严格遵循；例如，通常会看到使用Kullback-Leibler散度（KL散度）作为距离，尽管这个性质并不成立。'
- en: 'Let’s start with vector norms and see how we can easily use them as a distance
    measure between vectors. Then we’ll introduce the important concept of a covariance
    matrix, heavily used on its own in deep learning, and see how we can create a
    distance measure from it: the Mahalanobis distance. We’ll end the section by introducing
    the KL-divergence, which we can view as a measure between two discrete probability
    distributions.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向量范数开始，看看如何将它们作为向量之间的距离度量来使用。接着我们将介绍一个重要的概念——协方差矩阵，它在深度学习中有广泛应用，并且我们将看到如何从它创建一个距离度量：马氏距离。最后，我们将介绍
    KL 散度，它可以作为两种离散概率分布之间的度量。
- en: L-Norms and Distance Metrics
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: L-范数和距离度量
- en: For an *n*-dimensional vector, ***x***, we define the *p*-norm of the vector
    to be
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 *n*-维向量 ***x***，我们定义该向量的 *p*-范数为
- en: '![Image](Images/06equ06.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ06.jpg)'
- en: where *p* is a real number. Although we use *p* in the definition, people generally
    refer to these as L*[p]* norms. We saw one of these norms in [Chapter 5](ch05.xhtml#ch05)
    when we defined the magnitude of a vector. In that case, we were calculating the
    *L2-norm*,
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 是一个实数。尽管我们在定义中使用了 *p*，人们通常称这些为 L*[p]* 范数。我们在[第五章](ch05.xhtml#ch05)中看到过其中一种范数，当时我们定义了向量的大小。在那种情况下，我们计算的是
    *L2-范数*，
- en: '![Image](Images/145equ01.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/145equ01.jpg)'
- en: which is the square root of the inner product of ***x*** with itself.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 它是 ***x*** 与自身的内积的平方根。
- en: The norms we use most often in deep learning are the L2-norm and the *L1-norm*,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们最常用的范数是 L2-范数和 *L1-范数*，
- en: '![Image](Images/145equ02.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/145equ02.jpg)'
- en: which is nothing more than the sum of the absolute values of the components
    of ***x***. Another norm you’ll encounter is the *L**[∞]-norm*,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 它不过是 ***x*** 各个分量的绝对值之和。你还会遇到另一种范数，叫做 *L**[∞]-范数*，
- en: L[∞] = max |*x**[i]*|
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: L[∞] = max |*x**[i]*|
- en: the maximum absolute value of the components of ***x***.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '***x*** 各个分量的最大绝对值。'
- en: If we replace ***x*** with the difference of two vectors, ***x*** − ***y***,
    we can treat the norms as distance measures between the two vectors. Alternatively,
    we can picture the process as computing the vector norm on the vector that is
    the difference between ***x*** and ***y***.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 ***x*** 替换为两个向量的差 ***x*** − ***y***，我们可以将范数视为这两个向量之间的距离度量。或者，我们可以将这个过程看作是在向量
    ***x*** 和 ***y*** 之间的差异上计算范数。
- en: 'Switching from norm to distance makes a trivial change in [Equation 6.6](ch06.xhtml#ch06equ06):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 从范数到距离的转换在[方程 6.6](ch06.xhtml#ch06equ06)中只做了一个简单的变换：
- en: '![Image](Images/06equ07.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ07.jpg)'
- en: The L2-distance becomes
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: L2-距离变为
- en: '![Image](Images/145equ03.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/145equ03.jpg)'
- en: 'This is the *Euclidean distance* between two vectors. The L1-distance is often
    called the *Manhattan distance* (also called *city block distance*, *boxcar distance*,
    or *taxicab distance*):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个向量之间的 *欧几里得距离*。L1-距离通常称为 *曼哈顿距离*（也叫 *城市街区距离*、*箱车距离* 或 *出租车距离*）：
- en: '![Image](Images/146equ01.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/146equ01.jpg)'
- en: It’s so named because it corresponds to the length a taxicab would travel on
    the grid of streets in Manhattan. The L[∞]-distance is sometimes known as the
    *Chebyshev distance*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以这么命名，是因为它对应的是出租车在曼哈顿街区网格上行驶的距离。L[∞]-距离有时也被称为 *切比雪夫距离*。
- en: Norm equations have other uses in deep learning. For example, weight decay,
    used in deep learning as a regularizer, uses the L2-norm of the weights of the
    model to keep the weights from getting too large. The L1-norm of the weights is
    also sometimes used as a regularizer.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 范数方程在深度学习中有其他用途。例如，权重衰减作为深度学习中的正则化方法，使用模型权重的 L2-范数来防止权重过大。权重的 L1-范数有时也作为正则化器使用。
- en: Let’s move now to consider the important concept of a covariance matrix. It
    isn’t a distance metric itself but is used by one, and it will show up again later
    in the chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来讨论一个重要的概念：协方差矩阵。它本身不是一个距离度量，但在某些度量中会用到它，并且它将在本章稍后再次出现。
- en: Covariance Matrices
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: 'If we have a collection of measurements on multiple variables, like a training
    set with feature vectors, we can calculate the variance of the features with respect
    to each other. For example, here’s a matrix of observations of four variables,
    one per row:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个变量的测量集合，例如一个包含特征向量的训练集，我们可以计算特征之间的方差。例如，以下是一个包含四个变量的观测矩阵，每行代表一个变量：
- en: '![Image](Images/146equ02.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/146equ02.jpg)'
- en: 'In reality, ***X*** is the first five samples from the famous iris dataset.
    For the iris dataset, the features are measurements of the parts of iris flowers
    from three different species. You can load this dataset into NumPy using `sklearn`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，***X***是著名的鸢尾花数据集的前五个样本。对于鸢尾花数据集，特征是来自三种不同物种的鸢尾花各部分的测量值。你可以通过`sklearn`将此数据集加载到NumPy中：
- en: '[PRE12]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We could calculate the standard deviation of each of the features, the columns
    of ***A***, but that would only tell us about the variance of the values of that
    feature around its mean. Since we have multiple features, it would be nice to
    know something about how the features in, say, column zero and column one vary
    together. To determine this, we need to calculate the *covariance matrix*. This
    matrix captures the variance of the individual features along the main diagonal.
    Meanwhile, the off-diagonal values represent how one feature varies as another
    varies—these are the covariances. Since there are four features, the covariance
    matrix, which is always square, is, in this case, a 4 × 4 matrix. We find the
    elements of the covariance matrix, Σ, by calculating
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算每个特征的标准差，即***A***的列，但这仅能告诉我们该特征值围绕均值的方差。由于我们有多个特征，因此了解特征之间的变化关系会很有帮助，比如第零列和第一列之间的变化关系。为了确定这一点，我们需要计算*协方差矩阵*。该矩阵捕捉了各个特征沿主对角线的方差。与此同时，非对角线的值表示一个特征随着另一个特征的变化而变化——这些就是协方差。由于有四个特征，协方差矩阵总是方阵，在这种情况下是一个4
    × 4的矩阵。我们通过计算来找出协方差矩阵Σ的元素：
- en: '![Image](Images/06equ08.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ08.jpg)'
- en: 'assuming the rows of the matrix, ***X***, are the observations, and the columns
    of ***X*** represent the different features. The means of the features across
    all rows are ![Image](Images/147equ01.jpg) and ![Image](Images/147equ02.jpg) for
    features *i* and *j*. Here, *n* is the number of observations, the number of rows
    in ***X***. We can see that when *i* = *j*, the covariance value is the normal
    variance for that feature. When *i* ≠ *j*, the value is how *i* and *j* vary together.
    We often denote the covariance matrix as Σ, and it is always symmetric: ∑[*ij*]
    = ∑*[ji]*.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 假设矩阵***X***的行是观测值，列代表不同的特征。所有行中每个特征的均值分别为 ![Image](Images/147equ01.jpg) 和 ![Image](Images/147equ02.jpg)，分别对应特征*i*和*j*。这里，*n*是观测值的数量，即***X***中的行数。我们可以看到，当*i*
    = *j*时，协方差值是该特征的常规方差。当*i* ≠ *j*时，值表示*i*和*j*的变化关系。我们通常将协方差矩阵记作Σ，并且它总是对称的：∑[*ij*]
    = ∑*[ji]*。
- en: 'Let’s calculate some elements of the covariance matrix for ***X*** above. The
    per-feature means are ![Image](Images/147equ03.jpg). Let’s find the first row
    of Σ. This will tell us the variance of the first feature (column of **X**) and
    how that feature varies with the second, third, and fourth features. Therefore,
    we need to calculate ∑[00], ∑[01], ∑[02], and ∑[03]:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算上面***X***的协方差矩阵的一些元素。每个特征的均值为 ![Image](Images/147equ03.jpg)。我们来找出Σ的第一行。这将告诉我们第一个特征（**X**的列）的方差，以及该特征与第二、第三和第四个特征的变化关系。因此，我们需要计算∑[00]、∑[01]、∑[02]和∑[03]：
- en: '![Image](Images/147equ04.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/147equ04.jpg)'
- en: 'We can repeat this calculation for all the rows of Σ to give the complete covariance
    matrix:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对Σ的所有行重复这个计算，得到完整的协方差矩阵：
- en: '![Image](Images/147equ05.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/147equ05.jpg)'
- en: The elements along the diagonal represent the variance of the features of ***X***.
    Notice that for the fourth feature of ***X***, all the variances and covariances
    are zero. This makes sense because all the values for this feature in ***X***
    are the same; there is no variance.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线上的元素表示***X***特征的方差。请注意，***X***的第四个特征的所有方差和协方差都为零。这是有道理的，因为这个特征在***X***中的所有值都是相同的；没有方差。
- en: 'We can calculate the covariance matrix for a set of observations in code by
    using `np.cov`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`np.cov`在代码中计算一组观测值的协方差矩阵：
- en: '[PRE13]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that the call to `np.cov` includes `rowvar=False`. By default, `np.cov`
    expects each row of its argument to be a variable and the columns to be the observations
    of that variable. This is the opposite of the usual way a set of observations
    is typically stored in a matrix for deep learning. Therefore, we use the `rowvar`
    keyword to tell NumPy that the rows, not the columns, are the observations.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，调用`np.cov`时包含了`rowvar=False`。默认情况下，`np.cov`期望其参数的每一行是一个变量，而列是该变量的观测值。这与深度学习中通常存储观测值的矩阵方式相反。因此，我们使用`rowvar`关键字告诉NumPy，观测值是行，而不是列。
- en: I claimed above that the diagonal of the covariance matrix returns the variances
    of the features in ***X***. NumPy has a function, `np.std`, to calculate the standard
    deviation, and squaring the output of this function should give us the variances
    of the features by themselves. For ***X***, we get
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我上面提到过，协方差矩阵的对角线返回的是***X***中各特征的方差。NumPy 有一个函数 `np.std` 用于计算标准差，对该函数的输出进行平方应该能得到各特征的方差。对于***X***，我们得到
- en: '[PRE14]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These variances don’t look like the diagonal of the covariance matrix. The difference
    is due to the *n* − 1 in the denominator of the covariance equation, [Equation
    6.8](ch06.xhtml#ch06equ08). By default, `np.std` calculates what is known as a
    biased estimate of the sample variance. This means that instead of dividing by
    *n* − 1, it divides by *n*. To get `np.std` to calculate the unbiased estimator
    of the variance, we need to add the `ddof=1` keyword,
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方差看起来不像协方差矩阵的对角线。其差异源于协方差方程中分母的 *n* − 1，[公式 6.8](ch06.xhtml#ch06equ08)。默认情况下，`np.std`
    计算的是样本方差的偏倚估计。这意味着它不是除以 *n* − 1，而是除以 *n*。为了让 `np.std` 计算无偏方差估计，我们需要添加 `ddof=1`
    关键字，
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: then we’ll get the same values as along the diagonal of Σ.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将得到与 Σ 对角线相同的值。
- en: Now that we know how to calculate the covariance matrix, let’s use it in a distance
    metric.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何计算协方差矩阵，让我们在距离度量中使用它。
- en: Mahalanobis Distance
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 马哈拉诺比斯距离
- en: Above, we represented a dataset by a matrix where the rows of the dataset are
    observations and the columns are the values of variables that make up each observation.
    In machine learning terms, the rows are the feature vectors. As we saw above,
    we can calculate the mean of each feature across all the observations, and we
    can calculate the covariance matrix. With these values, we can define a distance
    metric called the *Mahalanobis distance*,
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们通过一个矩阵表示数据集，其中数据集的行是观测值，列是构成每个观测值的变量的值。在机器学习中，行是特征向量。正如我们上面看到的，我们可以计算每个特征在所有观测值中的均值，并可以计算协方差矩阵。通过这些值，我们可以定义一个距离度量，称为*马哈拉诺比斯距离*，
- en: '![Image](Images/06equ09.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06equ09.jpg)'
- en: where ***x*** is a vector, **μ** is the vector formed by the mean values of
    each feature, and Σ is the covariance matrix. Notice that this metric uses the
    *inverse* of the covariance matrix, not the covariance matrix itself.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，***x*** 是一个向量，**μ** 是由每个特征的均值构成的向量，Σ 是协方差矩阵。请注意，这个度量使用的是协方差矩阵的*逆*，而不是协方差矩阵本身。
- en: '[Equation 6.9](ch06.xhtml#ch06equ09) is, in some sense, measuring the distance
    between a vector and a distribution with the mean vector **μ**. The dispersion
    of the distribution is captured in Σ. If there is no covariance between the features
    in the dataset and each feature has the same standard deviation, then Σ becomes
    the identity matrix, which is its own inverse. In that case, Σ^(−1) effectively
    drops out of [Equation 6.9](ch06.xhtml#ch06equ09), and the Mahalanobis distance
    becomes the L2-distance (Euclidean distance).'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 6.9](ch06.xhtml#ch06equ09) 在某种意义上是在测量一个向量与具有均值向量 **μ** 的分布之间的距离。分布的离散程度由
    Σ 捕捉。如果数据集中的特征之间没有协方差，且每个特征具有相同的标准差，那么 Σ 就变成了单位矩阵，它是其自身的逆。在这种情况下，Σ^(−1) 在[公式 6.9](ch06.xhtml#ch06equ09)中实际上会被省略，马哈拉诺比斯距离就变成了
    L2 距离（欧几里得距离）。'
- en: Another way to think of the Mahalanobis distance is to replace **μ** with another
    vector, call it ***y***, that comes from the same dataset as ***x***. Then *D*[M]
    is the distance between the two vectors, taking the variance of the dataset into
    account.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解马哈拉诺比斯距离的方法是，将**μ**替换为另一个向量，称之为***y***，它来自与***x***相同的数据集。那么 *D*[M] 就是两个向量之间的距离，考虑到数据集的方差。
- en: We can use the Mahalanobis distance to build a simple classifier. If, given
    a dataset, we calculate the mean feature vector of each class in the dataset (this
    vector is also called the *centroid*), we can use the Mahalanobis distance to
    assign a label to an unknown feature vector, ***x***. We can do so by calculating
    all the Mahalanobis distances to the class centroids and assigning ***x*** to
    the class returning the smallest value. This type of classifier is sometimes called
    a *nearest centroid* classifier, and you’ll often see it implemented using the
    L2-distance in place of the Mahalanobis distance. Arguably, you can expect the
    Mahalanobis distance to be the better metric because it takes the variance of
    the dataset into account.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用马哈拉诺比斯距离来构建一个简单的分类器。如果给定一个数据集，我们计算数据集中每个类别的均值特征向量（这个向量也叫做*质心*），我们可以使用马哈拉诺比斯距离为一个未知特征向量***x***分配一个标签。我们通过计算所有马哈拉诺比斯距离到各个类别质心的距离，并将***x***分配给返回最小值的类别。此类分类器有时被称为*最近质心*分类器，你经常会看到它用L2距离代替马哈拉诺比斯距离来实现。可以说，马哈拉诺比斯距离更为优越，因为它考虑了数据集的方差。
- en: 'Let’s use the breast cancer dataset included with `sklearn` to build the nearest
    centroid classifier using the Mahalanobis distance. The breast cancer dataset
    has two classes: benign (0) and malignant (1). The dataset contains 569 observations,
    each of which has 30 features derived from histology slides. We’ll build two versions
    of the nearest centroid classifier: one using the Mahalanobis distance and the
    other using the Euclidean distance. Our expectation is that the classifier using
    the Mahalanobis distance will perform better.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`sklearn`附带的乳腺癌数据集，使用马哈拉诺比斯距离构建最近质心分类器。乳腺癌数据集有两个类别：良性（0）和恶性（1）。该数据集包含569个观测值，每个观测值有30个特征，来源于组织学切片。我们将构建两个版本的最近质心分类器：一个使用马哈拉诺比斯距离，另一个使用欧几里得距离。我们预计，使用马哈拉诺比斯距离的分类器会表现得更好。
- en: 'The code we need is straightforward:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码非常简单：
- en: '[PRE16]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We start by importing the modules we need, including `mahalanobis` from SciPy
    ❶. This function accepts two vectors and the inverse of a covariance matrix and
    returns *D*[M]. We get the dataset next in `d` with labels in `l`. We randomize
    the order ❷ and pull out the first 400 observations as training data (`xtrn`)
    with labels (`ytrn`). We hold back the remaining observations for testing (`xtst`,
    `ytst`).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的模块，包括SciPy中的`mahalanobis`❶。该函数接受两个向量和协方差矩阵的逆矩阵，并返回*D*[M]。接着我们在`d`中获取数据集，并在`l`中获取标签。我们随机化顺序❷并提取前400个观测值作为训练数据（`xtrn`）和标签（`ytrn`）。其余观测值则留作测试数据（`xtst`，`ytst`）。
- en: We *train* the model next. Training consists of pulling out all the observations
    belonging to each class ❸ and calculating `m0` and `m1`. These are the mean values
    of each of the 30 features for all class 0 and class 1 observations. We then calculate
    the covariance matrix of the entire training set (`S`) and its inverse (`SI`).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们*训练*模型。训练过程包括提取属于每个类别的所有观测值❸，并计算`m0`和`m1`。这两个值是类别0和类别1的所有观测值在30个特征上的均值。然后我们计算整个训练集的协方差矩阵（`S`）及其逆矩阵（`SI`）。
- en: The `score` function takes the test observations, a list of the class mean vectors,
    and the inverse of the covariance matrix. It runs through each test observation
    and calculates the Mahalanobis distances (`d`). It then uses the smallest distance
    to assign the class label (`c`). If the assigned label matches the actual test
    label, we count it (`nc`). At the end of the function, we return the overall accuracy.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`score`函数接收测试观测值、类别均值向量的列表以及协方差矩阵的逆矩阵。它遍历每个测试观测值并计算马哈拉诺比斯距离（`d`）。然后它使用最小的距离来分配类别标签（`c`）。如果分配的标签与实际测试标签匹配，我们就计数（`nc`）。在函数结束时，我们返回整体准确率。'
- en: We call the `score` function twice. The first call uses the inverse covariance
    matrix (`SI`), while the second call uses an identity matrix, thereby making `score`
    calculate the Euclidean distance instead. Finally, we print both results.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`score`函数两次。第一次调用使用逆协方差矩阵（`SI`），第二次调用使用单位矩阵，从而让`score`计算欧几里得距离。最后，我们打印出两个结果。
- en: The randomization of the dataset ❷ means that each time the code is run, it
    will output slightly different scores. Running the code 100 times gives the following
    mean scores (± the standard deviation).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的随机化❷意味着每次运行代码时，输出的得分会略有不同。运行代码100次得到以下平均得分（±标准差）。
- en: '| **Distance** | **Mean score** |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| **距离** | **平均得分** |'
- en: '| --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mahalanobis | 0.9595 ± 0.0142 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 马哈拉诺比斯 | 0.9595 ± 0.0142 |'
- en: '| Euclidean | 0.8914 ± 0.0185 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 欧几里得 | 0.8914 ± 0.0185 |'
- en: This clearly shows that using the Mahalanobis distance leads to better model
    performance, with about a 7 percent improvement in accuracy.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明，使用马氏距离可以提升模型的表现，准确率大约提高了7个百分点。
- en: One recent use of the Mahalanobis distance in deep learning is to take the top-level
    embedding layer values, a vector, and use the Mahalanobis distance to detect out-of-domain
    or adversarial inputs. An *out-of-domain input* is one that is significantly different
    from the type of data the model was trained to use. An *adversarial input* is
    one where an adversary is deliberately attempting to fool the model by supplying
    an input that isn’t of class X but that the model will label as class X.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，马氏距离在深度学习中的一个应用是提取顶层嵌入层的值（一个向量），并使用马氏距离来检测域外输入或对抗性输入。*域外输入*是指与模型训练时使用的数据类型有显著不同的输入。*对抗性输入*是指对手故意试图通过提供一个不是类X的数据来欺骗模型，尽管模型会将其标记为类X。
- en: Kullback-Leibler Divergence
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kullback-Leibler散度
- en: 'The *Kullback-Leibler divergence (KL-divergence)*, or *relative entropy*, is
    a measure of the similarity between two probability distributions: the lower the
    value, the more similar the distributions.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kullback-Leibler散度（KL散度）*，或称*相对熵*，是衡量两个概率分布相似度的一种度量：值越小，分布越相似。'
- en: If *P* and *Q* are discrete probability distributions, the KL-divergence is
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*P*和*Q*是离散概率分布，则KL散度为
- en: '![Image](Images/151equ01.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/151equ01.jpg)'
- en: where log[2] is the logarithm base-2\. This is an information-theoretic measure;
    the output is in bits of information. Sometimes the natural log, ln, is used,
    in which case the measure is said to be in *nats*. The SciPy function that implements
    the KL-divergence is in `scipy.special` as `rel_entr`. Note that `rel_entr` uses
    the natural log, not log base-2\. Note also that the KL-divergence isn’t a distance
    metric in the mathematical sense because it violates the symmetry property, *D*[KL](*P**||Q*)
    ≠ *D*[KL](*Q*||*P*), but that doesn’t stop people from using it as one from time
    to time.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 其中log[2]是以2为底的对数。这是一个信息论度量，输出的是比特信息。有时也使用自然对数ln，这种情况下度量单位称为*nats*。实现KL散度的SciPy函数在`scipy.special`中，命名为`rel_entr`。请注意，`rel_entr`使用的是自然对数，而不是以2为底的对数。还要注意，KL散度在数学意义上不是一种距离度量，因为它违反了对称性属性，*D*[KL](*P**||Q*)
    ≠ *D*[KL](*Q*||*P*)，但这并没有妨碍人们偶尔把它当作一种距离度量来使用。
- en: Let’s see an example of how we might use the KL-divergence to measure between
    different discrete probability distributions. We’ll measure the divergence between
    two different binomial distributions and a uniform distribution. Then, we’ll plot
    the distributions to see if, visually, we believe the numbers.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，了解如何使用KL散度来度量不同离散概率分布之间的差异。我们将测量两个不同的二项分布与一个均匀分布之间的散度。然后，我们将绘制这些分布，看看从视觉上是否相信这些数字。
- en: To generate the distributions, we’ll take many draws from a uniform distribution
    with 12 possible outputs. We can do this quickly in code by using `np.random.randint`.
    Then, we’ll take draws from two different binomial distributions, *B*(12, 0.4)
    and *B*(12, 0.9), meaning 12 trials with probabilities of 0.4 and 0.9 per trial.
    We’ll generate histograms of the resulting draws, divide by the sum of the counts,
    and use the rescaled histograms as our probability distributions. We can then
    measure the divergences between them.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成分布，我们将从一个有12个可能输出的均匀分布中抽取很多次。我们可以通过使用`np.random.randint`在代码中快速实现这一点。接着，我们将从两个不同的二项分布中抽取数据，*B*(12,
    0.4) 和 *B*(12, 0.9)，这意味着每次试验有12次，概率分别为0.4和0.9。我们将生成抽取结果的直方图，除以计数的总和，并将重新缩放的直方图作为我们的概率分布。然后，我们可以测量它们之间的散度。
- en: The code we need is
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的代码是
- en: '[PRE17]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We load `rel_entr` from SciPy and set the number of draws for each distribution
    to 1,000,000 (`N`). The code to generate the respective probability distributions
    follows the same method for each distribution. We draw `N` samples from the distribution,
    starting with the uniform ❶. We use `randint` because it returns integers in the
    range [0, 12] so we can match the discrete [0, 12] values that `binomial` returns
    for 12 trials. We get the histogram from the vector of draws by using `np.bincount`.
    This function counts the frequency of unique values in a vector ❷. Finally, we
    change the counts into fractions by dividing the histogram by the sum ❸. This
    gives us a 12-element vector in `p` representing the probability that `randint`
    will return the values 0 through 12\. Assuming `randint` uses a good pseudorandom
    number generator, we expect the probabilities to be roughly equal for each value
    in `p`. (NumPy uses the Mersenne Twister pseudorandom number generator, one of
    the better ones out there, so we’re confident that we’ll get good results.)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从SciPy加载`rel_entr`，并将每个分布的抽样次数设为1,000,000（`N`）。生成各自概率分布的代码对于每个分布的方法是相同的。我们从分布中抽取`N`个样本，首先从均匀分布开始❶。我们使用`randint`，因为它返回的整数范围是[0,
    12]，这样我们就可以匹配`binomial`在12次试验中返回的离散[0, 12]值。通过使用`np.bincount`，我们从抽样结果中获得直方图。这个函数会统计向量中唯一值的频率❷。最后，我们通过将直方图除以总和❸，将计数值转换为分数。这给了我们一个包含12个元素的`p`向量，表示`randint`返回0到12之间的值的概率。假设`randint`使用的是良好的伪随机数生成器，我们预计`p`中的每个值的概率大致相等。（NumPy使用的是Mersenne
    Twister伪随机数生成器，这是当前最好的之一，因此我们有信心能得到良好的结果。）
- en: We repeat this process, substituting `binomial` for `randint`, sampling from
    binomial distributions using probabilities of 0.9 and 0.4\. Again, histogramming
    the draws and converting the counts to fractions gives us the remaining probability
    distributions, `q` and `w`, based on 0.9 and 0.4, respectively.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程，将`binomial`替换为`randint`，使用概率分别为0.9和0.4的二项分布进行抽样。同样，通过对抽样结果进行直方图统计，并将计数转换为分数，我们得到了基于0.9和0.4的剩余概率分布，分别为`q`和`w`。
- en: We are finally ready to measure the divergence. The `rel_entr` function is a
    bit different from other functions in that it does not return *D*[KL] directly.
    Instead, it returns a vector of the same length as its arguments, where each element
    of the vector is part of the overall sum leading to *D*[KL]. Therefore, to get
    the actual divergence number, we need to add the elements of this vector. So,
    we print the sum of the output of `rel_entr`, comparing the two binomial distributions
    to the uniform distribution.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好测量偏差了。`rel_entr`函数与其他函数有些不同，因为它不会直接返回*D*[KL]。相反，它返回一个与其参数长度相同的向量，其中每个元素都是导致*D*[KL]的总和的一部分。因此，要得到实际的偏差值，我们需要将这个向量的元素相加。因此，我们打印`rel_entr`的输出的和，将两个二项分布与均匀分布进行比较。
- en: The random nature of the draws means we’ll get slightly different numbers each
    time we run the code. One run gave
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样的随机性质意味着每次运行代码时得到的数字略有不同。一次运行结果为：
- en: '| **Distributions** | **Divergence** |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **分布** | **偏差** |'
- en: '| --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *D*[KL](*Q**&#124;&#124;P*) | 1.1826 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| *D*[KL](*Q**&#124;&#124;P*) | 1.1826 |'
- en: '| *D*[KL](*W&#124;&#124;P*) | 0.6218 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| *D*[KL](*W&#124;&#124;P*) | 0.6218 |'
- en: This shows that the binomial distribution with probability 0.9 diverges more
    from a uniform distribution than the binomial distribution with probability 0.4\.
    Recall, the smaller the divergence, the closer the two probability distributions
    are to each other.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，概率为0.9的二项分布比概率为0.4的二项分布偏离均匀分布的程度更大。回想一下，偏离越小，两个概率分布就越相似。
- en: Do we believe this result? One way to check is visually, by plotting the three
    distributions and seeing if *B*(12, 0.4) looks more like a uniform distribution
    than *B*(12, 0.9) does. This leads to [Figure 6-1](ch06.xhtml#ch06fig01).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这个结果吗？一种检查方法是通过可视化，绘制三个分布并查看*B*(12, 0.4)是否比*B*(12, 0.9)更像一个均匀分布。这将导致[图6-1](ch06.xhtml#ch06fig01)。
- en: '![image](Images/06fig01.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig01.jpg)'
- en: '*Figure 6-1: Three different, discrete probability distributions: uniform (forward
    hash),* B(*12,0.4) (backward hash), and* B(*12,0.9) (horizontal hash)*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-1：三种不同的离散概率分布：均匀分布（前向哈希）、*B*(12, 0.4)（后向哈希）和*B*(12, 0.9)（水平哈希）*'
- en: Although it is clear that neither binomial distribution is particularly uniform,
    the *B*(12, 0.4) distribution is relatively centered in the range and spread across
    more values than the *B*(12, 0.9) distribution is. It seems reasonable to think
    of *B*(12, 0.4) as more like the uniform distribution, which is precisely what
    the KL-divergence told us by returning a smaller value.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然显然没有一个二项分布特别均匀，*B*(12, 0.4)分布相对集中在范围内，并且比*B*(12, 0.9)分布更广泛地分布在多个值上。将*B*(12,
    0.4)看作更像均匀分布似乎是合理的，这正是KL散度通过返回较小值告诉我们的。
- en: We now have everything we need to implement principal component analysis.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了实现主成分分析所需的一切。
- en: Principal Component Analysis
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Assume we have a matrix, ***X***, representing a dataset. We understand that
    the variance of each of the features need not be the same. If we think of each
    observation as a point in an *n*-dimensional space, where *n* is the number of
    features in each observation, we can imagine a cloud of points with a different
    amount of scatter in different directions.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个矩阵，***X***，表示一个数据集。我们理解每个特征的方差不一定相同。如果我们把每个观察值看作是一个在*n*维空间中的点，其中*n*是每个观察值的特征数量，我们可以想象出一群点，在不同的方向上有不同的散布程度。
- en: '*Principal component analysis (PCA)* is a technique to learn the directions
    of the scatter in the dataset, starting with the direction aligned along the greatest
    scatter. This direction is called the *principal component*. You then find the
    remaining components in order of decreasing scatter, with each new component orthogonal
    to all the others. The top part of [Figure 6-2](ch06.xhtml#ch06fig02) shows a
    2D dataset and two arrows. Without knowing anything about the dataset, we can
    see that the largest arrow points along the direction of the greatest scatter.
    This is what we mean by the principal component.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*主成分分析（PCA）*是一种用于学习数据集散布方向的技术，首先从最大散布方向开始。这个方向被称为*主成分*。然后你可以按散布递减的顺序找到剩余的成分，每个新的成分都与其他成分正交。[图6-2](ch06.xhtml#ch06fig02)的顶部显示了一个二维数据集和两条箭头。即使我们对数据集一无所知，我们也能看到最大的箭头指向散布最大的方向。这就是我们所说的主成分。'
- en: '![image](Images/06fig02.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/06fig02.jpg)'
- en: '*Figure 6-2: The first two features of the iris dataset and principal component
    directions (top), and the iris dataset after transformation by PCA (bottom)*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6-2：鸢尾花数据集的前两个特征及主成分方向（上图），以及PCA变换后的鸢尾花数据集（下图）*'
- en: We often use PCA to reduce the dimensionality of a dataset. If there are 100
    variables per observation, but the first two principal components explain 95 percent
    of the scatter in the data, then mapping the dataset along those two components
    and discarding the remaining 98 components might adequately characterize the dataset
    with only two variables. We can use PCA to augment a dataset as well, assuming
    continuous features.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用PCA来降低数据集的维度。如果每个观察值有100个变量，但前两个主成分解释了数据中95%的散布，那么将数据集映射到这两个成分上，并丢弃剩余的98个成分，可能足以用两个变量充分表征数据集。我们也可以使用PCA来增强数据集，前提是特征是连续的。
- en: 'So, how does PCA work? All this talk about the scatter of the data implies
    that PCA might be able to make use of the covariance matrix, and, indeed, it does.
    We can break the PCA algorithm down into a few steps:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，PCA是如何工作的呢？关于数据散布的讨论意味着PCA可能能够利用协方差矩阵，事实上，它的确是这样做的。我们可以将PCA算法分解为几个步骤：
- en: Mean center the data.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行均值中心化。
- en: Calculate the covariance matrix, Σ, of the mean-centered data.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算均值中心化数据的协方差矩阵，Σ。
- en: Calculate the eigenvalues and eigenvectors of Σ.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算Σ的特征值和特征向量。
- en: Sort the eigenvalues by decreasing absolute value.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照绝对值递减的顺序对特征值进行排序。
- en: Discard the weakest eigenvalues/eigenvectors (optional).
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃最弱的特征值/特征向量（可选）。
- en: Form a transformation matrix, ***W***, using the remaining eigenvectors.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用剩余的特征向量构造一个变换矩阵，***W***。
- en: Generate new transformed values from the existing dataset, ***x***′ = ***Wx***.
    These are sometimes referred to as *derived variables*.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从现有数据集中生成新的变换值，***x***′ = ***Wx***。这些值有时被称为*派生变量*。
- en: 'Let’s work through an example of this process using the iris dataset ([Listing
    6-1](ch06.xhtml#ch06ex01)). We’ll reduce the dimensionality of the data from four
    features to two. First the code, then the explanation:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用鸢尾花数据集（[Listing 6-1](ch06.xhtml#ch06ex01)）来演示这个过程。我们将把数据的维度从四个特征降至两个。先是代码，再是解释：
- en: '[PRE18]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Listing 6-1: Principal component analysis (PCA)*'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6-1：主成分分析（PCA）*'
- en: We start by loading the iris dataset, courtesy of `sklearn`. This gives us `iris`
    as a 150 × 4 matrix, since there are 150 observations, each with four features.
    We calculate the mean value of each feature ❶ and subtract it from the dataset,
    relying on NumPy’s broadcasting rules to subtract `m` from each row of `iris`.
    We’ll work with the mean-centered matrix `ir` going forward.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载鸢尾花数据集，感谢`sklearn`提供。这给我们提供了一个150 × 4的矩阵`iris`，因为有150个观察值，每个观察值有四个特征。我们计算每个特征的均值
    ❶，并从数据集中减去均值，利用NumPy的广播规则从`iris`的每一行中减去`m`。接下来我们将使用均值中心化后的矩阵`ir`。
- en: The next step is to compute the covariance matrix ❷. The output, `cv`, is a
    4 × 4 matrix, since we have four features per observation. We follow this by calculating
    the eigenvalues and eigenvectors of `cv` ❸ and then take the absolute value of
    the eigenvalues to get the magnitude. We want the eigenvalues in decreasing order
    of magnitude, so we get the indices that sort them that way ❹ using the Python
    idiom of `[::-1]` to reverse the order of a list or array.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算协方差矩阵 ❷。输出`cv`是一个4 × 4的矩阵，因为每个观察值有四个特征。接着，我们计算`cv`的特征值和特征向量 ❸，然后取特征值的绝对值以获得其大小。我们希望特征值按大小降序排列，因此我们使用Python的`[::-1]`惯用法反转列表或数组的顺序，从而获取排序后的索引
    ❹。
- en: The magnitude of the eigenvalues is proportional to the fraction of the variance
    in the dataset along each principal component; therefore, if we scale the eigenvalues
    by their overall sum, we get the proportion explained by each principal component
    (`ex`). The fraction of variance explained is
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值的大小与数据集中每个主成分沿着的方差比例成正比；因此，如果我们将特征值按它们的总和进行缩放，就可以得到每个主成分解释的比例（`ex`）。解释的方差比例为
- en: '[PRE19]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: indicating that two principal components explain nearly 98 percent of the variance
    in the iris dataset. Therefore, we’ll only keep the first two principal components
    going forward.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明两个主成分解释了鸢尾花数据集中近98%的方差。因此，接下来我们只保留前两个主成分。
- en: We create the transformation matrix, `w`, from the eigenvectors that go with
    the two largest eigenvalues ❺. Recall, `eig` returns the eigenvectors as the columns
    of the matrix `vec`. The transformation matrix, `w`, is a 2 × 4 matrix because
    it maps a four-component feature vector to a new two-component vector.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从与两个最大特征值对应的特征向量中创建变换矩阵`w` ❺。回顾一下，`eig`返回特征向量作为矩阵`vec`的列。变换矩阵`w`是一个2 × 4的矩阵，因为它将四维特征向量映射到新的二维向量。
- en: All that’s left is to create a place to hold the transformed observations and
    fill them in ❻. The new, reduced-dimension dataset is in `d`. We can now plot
    the entire transformed dataset, labeling each point by the class to which it belongs.
    The result is the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是创建一个地方来存放变换后的观察值并将其填充 ❻。新的、降维后的数据集存储在`d`中。现在我们可以绘制整个变换后的数据集，并根据每个点所属的类别进行标记。结果是[图6-2](ch06.xhtml#ch06fig02)的底部部分。
- en: In the top part of [Figure 6-2](ch06.xhtml#ch06fig02) is a plot of the original
    dataset using only the first two features. The arrows indicate the first two principal
    components, and the size of the arrows shows how much of the variance in the data
    these components explain. The first component explains most of the variance, which
    makes sense visually.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图6-2](ch06.xhtml#ch06fig02)的顶部是仅使用前两个特征绘制的原始数据集图。箭头表示前两个主成分，箭头的大小显示了这些主成分解释了数据中多少方差。第一个主成分解释了大部分方差，这在视觉上是合理的。
- en: In this example, the derived variables in the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02)
    have made the dataset easier to work with, as the classes are better separated
    than on the top using only two of the original features. Sometimes, PCA makes
    it easier for a model to learn because of the reduced feature vector size. However,
    this is not always the case. During PCA, you may lose a critical feature allowing
    class separation. As with most things in machine learning, experimentation is
    vital.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，[图6-2](ch06.xhtml#ch06fig02)底部的衍生变量使数据集变得更容易处理，因为各类之间的分离比仅使用原始的两个特征时更加明显。有时候，PCA使得模型更容易学习，因为特征向量的维度降低。然而，这并非总是如此。在PCA过程中，可能会丢失一个对于类分离至关重要的特征。正如机器学习中的大多数事情，实验非常重要。
- en: 'PCA is commonly used and is therefore well supported in multiple tool-kits.
    Instead of the dozen or so lines of code we used above, we can accomplish the
    same thing by using the `PCA` class from the `sklearn.decomposition` module:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是常用的，因此在多个工具包中得到了很好的支持。我们可以通过使用`sklearn.decomposition`模块中的`PCA`类来完成与上面相同的操作，而无需写上面的几十行代码：
- en: '[PRE20]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The new, reduced-dimension dataset is in `d`. Like other `sklearn` classes,
    after we tell `PCA` how many components we want it to learn, it uses `fit` to
    set up the transformation matrix (`w` in [Listing 6-1](ch06.xhtml#ch06ex01)).
    We then apply the transform by calling `fit_transform`.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 新的、降维后的数据集在`d`中。像其他`sklearn`类一样，在我们告诉`PCA`要学习多少个主成分后，它使用`fit`来设置变换矩阵（在[列表 6-1](ch06.xhtml#ch06ex01)中是`w`）。然后，我们通过调用`fit_transform`来应用变换。
- en: Singular Value Decomposition and Pseudoinverse
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奇异值分解与伪逆
- en: 'We’ll end this chapter with an introduction to *singular value decomposition
    (SVD)*. This is a powerful technique to factor any matrix into the product of
    three matrices, each with special properties. The derivation of SVD is beyond
    the scope of this book. I trust motivated readers to dig into the vast literature
    on linear algebra to locate a satisfactory presentation of where SVD comes from
    and how it is best understood. Our goal is more modest: to become familiar with
    the mathematics found in deep learning. Therefore, we’ll content ourselves with
    the definition of SVD, some idea of what it gives us, some of its uses, and how
    to work with it in Python. For deep learning, you’ll most likely encounter SVD
    when calculating the pseudoinverse of a nonsquare matrix. We’ll also see how that
    works in this section.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后，我们将介绍*奇异值分解（SVD）*。这是一种强大的技术，可以将任意矩阵分解为三个具有特殊性质的矩阵的乘积。SVD的推导超出了本书的范围。我相信有兴趣的读者可以深入研究线性代数的广泛文献，找到关于SVD的来源及其最佳理解方式的满意阐述。我们的目标更为
    modest：熟悉深度学习中使用的数学。因此，我们将满足于了解SVD的定义、它带给我们的理解、它的一些应用以及如何在Python中使用它。对于深度学习，你最有可能在计算非方阵的伪逆时遇到SVD。我们将在本节中看到如何操作。
- en: The output of SVD for an input matrix, ***A***, with real elements and shape
    *m* × *n*, where *m* does not necessarily equal *n* (though it could) is
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入矩阵***A***，其元素为实数且形状为*m* × *n*，其中*m*不一定等于*n*（尽管它们可能相等），SVD的输出是：
- en: '![Image](Images/06equ10.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06equ10.jpg)'
- en: '***A*** has been decomposed into three matrices: ***U***, Σ, and ***V***. Note
    that you might sometimes see ***V***^⊤ written as ***V***^*, the conjugate transpose
    of ***V***. This is the more general form that works with complex-valued matrices.
    We’ll restrict ourselves to real-valued matrices, so we only need the ordinary
    matrix transpose.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***已经被分解为三个矩阵：***U***，Σ和***V***。请注意，你有时可能会看到***V***^⊤被写作***V***^*，即***V***的共轭转置。这是适用于复数矩阵的更一般形式。我们将限制在实值矩阵的范围内，因此只需要普通的矩阵转置。'
- en: 'The SVD of an *m* × *n* matrix, ***A***, returns the following: ***U***, which
    is *m* × *m* and orthogonal; Σ, which is *m* × *n* and diagonal; and ***V***,
    which is *n* × *n* and orthogonal. Recall that the transpose of an orthogonal
    matrix is its inverse, so ***UU***^⊤ = ***I****[m]* and ***VV***^⊤ = ***I****[n]*,
    where the subscript on the identity matrix gives the order of the matrix, *m*
    × *m* or *n* × *n*.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*m* × *n*矩阵***A***的SVD返回以下结果：***U***，它是*m* × *m*且是正交的；Σ，它是*m* × *n*且是对角的；以及***V***，它是*n*
    × *n*且是正交的。回忆一下，正交矩阵的转置就是它的逆，因此***UU***^⊤ = ***I****[m]*，***VV***^⊤ = ***I****[n]*，其中单位矩阵的下标表示矩阵的阶数，*m*
    × *m*或*n* × *n*。
- en: At this point in the chapter, you may have raised an eyebrow at the statement
    “Σ, which is *m* × *n* and diagonal,” since we’ve only considered square matrices
    to be diagonal. Here, when we say *diagonal*, we mean a *rectangular diagonal
    matrix*. This is the natural extension to a diagonal matrix, where the elements
    of what would be the diagonal are nonzero and all others are zero. For example,
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一部分，你可能会对“Σ，尺寸为*m* × *n*且为对角矩阵”这一说法挑起眉头，因为我们通常认为只有方阵才是对角矩阵。这里，当我们说“对角矩阵”时，我们指的是*矩形对角矩阵*。这是对对角矩阵的自然扩展，其中原本属于对角线的元素是非零的，其他位置的元素为零。例如，
- en: '![Image](Images/157equ01.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/157equ01.jpg)'
- en: is a 3 × 5 rectangular diagonal matrix because only the main diagonal is nonzero.
    The “singular” in “singular value decomposition” comes from the fact that the
    elements of the diagonal matrix, Σ, are the singular values, the square roots
    of the positive eigenvalues of the matrix ***A**^T**A***.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个3 × 5的矩形对角矩阵，因为只有主对角线上的元素是非零的。术语“奇异”出现在“奇异值分解”中，来源于Σ对角矩阵中的元素，它们是矩阵***A**^T**A***的正特征值的平方根，即奇异值。
- en: SVD in Action
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SVD的实际应用
- en: Let’s be explicit and use SVD to decompose a matrix. Our test matrix is
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们明确一点，使用SVD来分解矩阵。我们的测试矩阵是：
- en: '![Image](Images/158equ01.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/158equ01.jpg)'
- en: We’ll show SVD in action as a series of steps. To get the SVD, we use `svd`
    from `scipy.linalg`,
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示SVD的实际应用步骤。为了得到SVD，我们使用`scipy.linalg`中的`svd`函数。
- en: '[PRE21]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'where `u` is ***U***, `vt` is ***V***^⊤, and `s` contains the singular values:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`u`是***U***，`vt`是***V***^⊤，`s`包含奇异值：
- en: '[PRE22]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s check that the singular values are indeed the square roots of the positive
    eigenvalues of ***A**^T**A***:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证奇异值确实是矩阵***A**^T**A***的正特征值的平方根：
- en: '[PRE23]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This shows us that, yes, 5 and 3 are the square roots of 25 and 9\. Recall
    that `eig` returns a list, the first element of which is a vector of the eigenvalues.
    Also note that there is a third eigenvalue: zero. You might ask: “How small a
    numeric value should we interpret as zero?” That’s a good question with no hard
    and fast answer. Typically, I interpret values below 10^(−9) to be zero.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，确实，5和3分别是25和9的平方根。回想一下，`eig`返回的是一个列表，其中的第一个元素是特征值的向量。还要注意，还有第三个特征值：零。你可能会问：“我们应该如何界定一个数值为零？”这是一个很好的问题，但没有固定的答案。通常，我会将小于10^(-9)的数值视为零。
- en: 'The claim of SVD is that ***U*** and ***V*** are unitary matrices. If so, their
    products with their transposes should be the identity matrix:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: SVD的声明是，***U***和***V***是单位矩阵。如果是这样，它们与自身转置的乘积应该是单位矩阵：
- en: '[PRE24]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Given the comment above about numeric values that we should interpret as zero,
    this is indeed the identity matrix. Notice that `svd` returned ***V***^⊤, not
    ***V***. However, since (***V***^⊤)^⊤ = ***V***, we’re still multiplying ***V***^⊤***V***.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上面提到的关于应当视为零的数值，这确实是单位矩阵。注意，`svd`返回的是***V***^⊤，而不是***V***。然而，由于(***V***^⊤)^⊤
    = ***V***，我们仍然在乘***V***^⊤***V***。
- en: 'The `svd` function returns not Σ but the diagonal values of Σ. Therefore, let’s
    reconstruct Σ and use it to see that SVD works, meaning we can use ***U***, Σ,
    and ***V***^⊤ to recover ***A***:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`svd`函数返回的不是Σ，而是Σ的对角线上的值。因此，我们需要重建Σ并使用它来验证SVD的效果，这意味着我们可以使用***U***、Σ和***V***^⊤来恢复***A***：'
- en: '[PRE25]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is the ***A*** we started with—almost: the recovered ***A*** is no longer
    of integer type, a subtle change worth remembering when writing code.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们开始时的***A***——几乎：恢复的***A***不再是整数类型，这是一个细微的变化，值得在编写代码时记住。
- en: Two Applications
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 两个应用
- en: 'SVD is a cute trick, but what can we do with it? The short answer is “a lot.”
    Let’s see two applications. The first is using SVD for PCA. The `sklearn PCA`
    class we used in the previous section uses SVD under the hood. The second example
    shows up in deep learning: using SVD to calculate the Moore-Penrose pseudoinverse,
    a generalization of the inverse of a square matrix to *m* × *n* matrices.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: SVD是一个巧妙的技巧，但我们可以用它做什么呢？简短的答案是“很多”。我们来看两个应用。第一个是使用SVD进行PCA。我们在上一节中使用的`sklearn
    PCA`类在内部就是使用SVD的。第二个例子出现在深度学习中：使用SVD计算Moore-Penrose伪逆，它是方阵逆矩阵的推广，适用于*m* × *n*矩阵。
- en: SVD for PCA
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PCA的SVD
- en: To see how to use SVD for PCA, let’s use the iris data from the previous section
    so we can compare with those results. The key is to truncate the Σ and ***V***^⊤
    matrices to keep only the desired number of largest singular values. The decomposition
    code will put the singular values in decreasing order along the diagonal of Σ
    for us, we need only retain the first *k* columns of Σ. In code, then,
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解如何使用SVD进行PCA，我们使用上一节中的鸢尾花数据，这样可以与之前的结果进行比较。关键在于对Σ和***V***^⊤矩阵进行截断，仅保留所需数量的最大奇异值。分解代码会将奇异值按降序排列在Σ的对角线上，我们只需要保留Σ的前*k*列。代码如下：
- en: '[PRE26]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we’re using `ir` from [Listing 6-1](ch06.xhtml#ch06ex01). This is the
    mean-centered version of the iris dataset matrix, with 150 rows of four features
    each. A call to `svd` gives us the decomposition of `ir`. The next three lines
    ❶ create the full Σ matrix in `S`. Because the iris dataset has four features,
    the `s` vector that `svd` returns will have four singular values.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用来自[列表6-1](ch06.xhtml#ch06ex01)的`ir`。这是一个均值中心化版本的鸢尾花数据集矩阵，包含150行，每行有四个特征。调用`svd`将为我们提供`ir`的分解。接下来的三行❶创建了矩阵Σ的完整矩阵`S`。由于鸢尾花数据集有四个特征，`svd`返回的`s`向量将包含四个奇异值。
- en: The truncation comes by keeping the first two columns of `S` ❷. Doing this changes
    Σ from a 150 × 4 matrix to a 150 × 2 matrix. Multiplying ***U*** by the new Σ
    gives us the transformed iris dataset. Since ***U*** is 150 × 150 and Σ is 150
    × 2, we get a 150 × 2 dataset in `T`. If we plot this as `T[:,0]` versus `T[:,1]`,
    we get the exact same plot as the bottom part of [Figure 6-2](ch06.xhtml#ch06fig02).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 截断通过保留`S`的前两列来实现❷。这样，Σ矩阵从150 × 4变为150 × 2。将***U***与新的Σ相乘得到转换后的鸢尾花数据集。由于***U***是150
    × 150，Σ是150 × 2，我们得到一个150 × 2的数据集`T`。如果我们将其绘制为`T[:,0]`与`T[:,1]`的关系，我们将得到与[图6-2](ch06.xhtml#ch06fig02)底部部分完全相同的图。
- en: The Moore-Penrose Pseudoinverse
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Moore-Penrose伪逆
- en: As promised, our second application is to compute ***A***^+, the Moore-Penrose
    pseudoinverse of an *m* × *n* matrix ***A***. The matrix ***A***^+ is called a
    pseudo-inverse because, in conjunction with ***A***, it acts like an inverse in
    that
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的，我们的第二个应用是计算***A***^+，即一个*m* × *n*矩阵***A***的Moore-Penrose伪逆。矩阵***A***^+被称为伪逆，因为它与***A***配合时，表现得像一个逆矩阵，其关系为
- en: '![Image](Images/06equ11.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/06equ11.jpg)'
- en: where ***AA***^+ is somewhat like the identity matrix, making ***A***^+ somewhat
    like the inverse of ***A***.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***AA***^+有点像单位矩阵，使得***A***^+有点像***A***的逆矩阵。
- en: Knowing that the pseudoinverse of a rectangular diagonal matrix is simply the
    reciprocal of the diagonal values, leaving zeros as zero, followed by a transpose,
    we can calculate the pseudoinverse of any general matrix as
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩形对角矩阵的伪逆仅仅是对角线值的倒数，其余位置为零，接着取转置，我们可以将任何一般矩阵的伪逆计算为
- en: '***A***^+ = ***V***Σ^+ ***U****'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '***A***^+ = ***V***Σ^+ ***U****'
- en: for ***A*** = ***U***Σ***V****, the SVD of ***A***. Notice, we’re using the
    conjugate transpose, ***V***^*, instead of the ordinary transpose, ***V***^⊤.
    If ***A*** is real, then the ordinary transpose is the same as the conjugate transpose.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 对于***A*** = ***U***Σ***V****，是矩阵***A***的SVD。注意，我们使用的是共轭转置***V***^*，而不是普通转置***V***^⊤。如果***A***是实数矩阵，那么普通转置和共轭转置是相同的。
- en: Let’s see if the claim regarding ***A***^+ is true. We’ll start with the ***A***
    matrix we used in the section above, compute the SVD, and use the parts to find
    the pseudoinverse. Finally, we’ll validate [Equation 6.11](ch06.xhtml#ch06equ11).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证一下关于***A***^+的声明是否正确。我们将从上面章节中使用的***A***矩阵开始，计算SVD，并使用这些部分来找到伪逆。最后，我们将验证[方程6.11](ch06.xhtml#ch06equ11)。
- en: 'We’ll start with ***A***, the same array we used above for the SVD example:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从***A***开始，使用我们在上面SVD示例中使用的相同数组：
- en: '[PRE27]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Applying SVD will give us ***U*** and ***V***^⊤ along with the diagonal of
    Σ. We’ll use the diagonal elements to construct Σ^+ by hand. Recall, Σ^+ is the
    transpose of Σ, where the diagonal elements that are not zero are changed to their
    reciprocals:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 应用SVD将为我们提供***U***和***V***^⊤以及Σ的对角线。我们将使用对角元素手动构造Σ^+。回想一下，Σ^+是Σ的转置，其中非零的对角线元素会被替换为它们的倒数：
- en: '[PRE28]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we can calculate ***A***^+ and verify that ***AA***^+***A*** = ***A***:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算***A***^+并验证***AA***^+***A*** = ***A***：
- en: '[PRE29]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And, in this case, ***AA***^+ is the identity matrix:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，***AA***^+是单位矩阵：
- en: '[PRE30]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This concludes our whirlwind look at SVD and our discussion of linear algebra.
    We barely scratched the surface, but we’ve covered what we need to know.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对SVD的快速回顾和线性代数的讨论。我们只是触及了表面，但我们已经涵盖了需要知道的内容。
- en: Summary
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: This heavy chapter and [Chapter 5](ch05.xhtml#ch05) before it plowed through
    a lot of linear algebra. As a mathematical topic, linear algebra is vastly richer
    than our presentation here.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章的内容以及前面的[第五章](ch05.xhtml#ch05)讲解了大量的线性代数。作为一个数学主题，线性代数远比我们在这里展示的要丰富得多。
- en: We focused the chapter on square matrices, as they have a special place in linear
    algebra. Specifically, we discussed general properties of square matrices, with
    examples. We learned about eigenvalues and eigenvectors, how to find them, and
    why they are useful. Next, we looked at vector norms and other ways to measure
    distance, as they show up often in deep learning. Finally, we ended the chapter
    by learning what PCA is and how it works, followed by a look at singular value
    decomposition, with two applications relevant to deep learning.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章重点放在了方阵上，因为它们在线性代数中占有特殊的地位。具体来说，我们讨论了方阵的一般性质，并提供了示例。我们学习了特征值和特征向量，如何求解它们，以及它们为何有用。接下来，我们研究了向量范数和其他测量距离的方法，因为这些在深度学习中经常出现。最后，我们通过学习主成分分析（PCA）及其原理结束了本章，并深入探讨了奇异值分解及其在深度学习中的两个相关应用。
- en: The next chapter shifts gears and covers differential calculus. This is, fortunately,
    the “easy” part of calculus, and is, in general, all that we need to understand
    the algorithms specific to deep learning. So, fasten your seat belts, make sure
    your arms and legs are fully within the vehicle, and prepare for departure to
    the world of differential calculus.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将转到微积分，重点讲解微分学。幸运的是，这部分是微积分中的“简单”部分，一般来说，它是我们理解深度学习中特定算法所需的全部内容。所以，系好安全带，确保你的手脚完全在车内，准备好启程进入微分学的世界吧。
