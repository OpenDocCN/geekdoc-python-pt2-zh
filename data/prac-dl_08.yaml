- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8'
- en: INTRODUCTION TO NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络简介**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: Neural networks are the heart of deep learning. In [Chapter 9](ch09.xhtml#ch09),
    we’ll take a deep dive into what we’ll call *traditional neural networks*. However,
    before we do that, we’ll introduce the anatomy of a neural network, followed by
    a quick example.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是深度学习的核心。在[第9章](ch09.xhtml#ch09)中，我们将深入探讨我们所称的*传统神经网络*。然而，在此之前，我们将介绍神经网络的结构，接着展示一个简单的示例。
- en: Specifically, we’ll present the components of a *fully connected feed- forward
    neural network*. Visually, you can imagine the network as shown in [Figure 8-1](ch08.xhtml#ch8fig1).
    We’ll refer to this figure often in this chapter and the next. Your mission, should
    you choose to accept it, is to commit this figure to memory to save wear and tear
    on the book by flipping back to it repeatedly.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将介绍一个*全连接前馈神经网络*的组成部分。从视觉上看，你可以将该网络想象为[图8-1](ch08.xhtml#ch8fig1)中所示的样子。在本章及下章中，我们将经常提到这一图形。你的任务，如果你选择接受的话，是将这一图形记住，以减少翻阅书籍时的磨损。
- en: '![image](Images/08fig01.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig01.jpg)'
- en: '*Figure 8-1: A sample neural network*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-1：一个示例神经网络*'
- en: After discussing the structure and parts of a neural network, we’ll explore
    training our example network to classify irises. From this initial experiment,
    [Chapter 9](ch09.xhtml#ch09) will lead us to gradient descent and the backpropagation
    algorithm—the standard way that neural networks, including advanced deep neural
    networks, are trained. This chapter is intended as a warm-up. The heavy lifting
    starts in [Chapter 9](ch09.xhtml#ch09).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了神经网络的结构和组成部分之后，我们将探索如何训练我们的示例网络来分类鸢尾花。通过这个初步实验，[第9章](ch09.xhtml#ch09)将引导我们进入梯度下降和反向传播算法——这是神经网络，包括先进的深度神经网络，训练的标准方法。本章旨在作为热身，真正的重头戏将在[第9章](ch09.xhtml#ch09)开始。
- en: Anatomy of a Neural Network
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的结构
- en: 'A *neural network* is a graph. In computer science, a *graph* is a series of
    *nodes*, universally drawn as circles, connected by *edges* (short line segments).
    This abstraction is useful for representing many different kinds of relationships:
    roads between cities, who knows whom on social media, the structure of the internet,
    or a series of basic computational units that can be used to approximate any mathematical
    function.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*神经网络*是一个图。在计算机科学中，*图*是由*节点*（通常绘制为圆形）通过*边*（短线段）连接而成的。这个抽象结构对于表示多种不同类型的关系非常有用：城市之间的道路、社交媒体上谁认识谁、互联网的结构，或是用来逼近任何数学函数的基本计算单元系列。'
- en: The last example is, of course, deliberate. Neural networks are universal function
    approximators. They use a graph structure to represent a series of computational
    steps mapping an input feature vector to an output value, typically interpreted
    as a probability. Neural networks are built in layers. Conceptually, they act
    from left to right, mapping an input feature vector to the output(s) by passing
    values along the edges to the nodes. Note, the nodes of a neural network are often
    referred to as *neurons*. We’ll see why shortly. The nodes calculate new values
    based on their inputs. The new values are then passed to the next layer of nodes
    and so on until the output nodes are reached. In [Figure 8-1](ch08.xhtml#ch8fig1),
    there’s an input layer on the left, a hidden layer to its right, another hidden
    layer right of that, and a single node in the output layer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个示例，当然是故意为之。神经网络是通用的函数逼近器。它们使用图结构来表示一系列计算步骤，将输入特征向量映射到输出值，通常被解释为概率。神经网络是分层构建的。从概念上讲，它们从左到右作用，通过将值沿着边传递到节点来将输入特征向量映射到输出。需要注意的是，神经网络的节点通常被称为*神经元*。我们稍后将解释原因。节点根据它们的输入计算新的值，这些新值随后传递到下一层节点，依此类推，直到到达输出节点。在[图8-1](ch08.xhtml#ch8fig1)中，左侧有一个输入层，右侧有一个隐藏层，再右边还有一个隐藏层，输出层中有一个单独的节点。
- en: The previous section included the phrase *fully connected feedforward neural
    network* without much explanation. Let’s break it down. The *fully connected*
    part means every node of a layer has its output sent to every node of the next
    layer. The *feedforward* part means that information passes from left to right
    through the network without being sent back to a previous layer; there is no *feedback*,
    no looping, in the network structure. This leaves only the *neural network* part.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 前一部分提到了*全连接前馈神经网络*这一术语，却没有做太多解释。让我们来解析一下。*全连接*部分意味着一层中的每个节点的输出都会传送到下一层的每个节点。*前馈*部分意味着信息从左到右通过网络传递，不会被送回到前一层；网络结构中没有*反馈*，也没有循环。这就剩下了*神经网络*部分。
- en: The Neuron
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经元
- en: Personally, I have a love/hate relationship with the phrase *neural network*.
    The phrase itself comes from the fact that in a very crude approximation, the
    basic unit of the network resembles a neuron in a brain. Consider [Figure 8-2](ch08.xhtml#ch8fig2),
    which we’ll describe in detail shortly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我对*神经网络*这个词有一种爱恨交织的情感。这个词本身源于这样一个事实：在非常粗略的近似中，网络的基本单元类似于大脑中的神经元。看看[图 8-2](ch08.xhtml#ch8fig2)，我们很快会详细描述它。
- en: '![image](Images/08fig02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig02.jpg)'
- en: '*Figure 8-2: A single neural network node*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-2：一个神经网络节点*'
- en: Recalling that our visualization of a network always moves from left to right,
    we see that the node (the circle) accepts input from the left, and has a single
    output on the right. Here there are two inputs, but it might be hundreds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们总是从左到右地可视化一个网络，我们看到节点（圆圈）从左侧接收输入，并且右侧有一个单一的输出。这里有两个输入，但它也可能是成百上千个。
- en: 'Many inputs mapped to a single output echo how a neuron in the brain works:
    structures called dendrites accept input from many other neurons, and the single
    axon is the output. I love this analogy because it leads to a cool way of talking
    and thinking about the networks. But I hate the analogy because these artificial
    neurons are, operationally, quite different from real ones, and the analogy quickly
    falls apart. There is an anatomical similarity to actual neurons, but they’re
    not the same, and it leads to confusion on the part of those who are not familiar
    with machine learning, causing some to believe that computer scientists are truly
    building artificial brains or that the networks think. The meaning of the word
    *think* is hard to pin down, but to me it doesn’t apply to what a neural network
    does.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多个输入映射到单一输出的方式与大脑中神经元的工作方式相似：被称为树突的结构接受来自其他许多神经元的输入，单一的轴突是输出。我喜欢这个类比，因为它引出了一个很酷的方式来谈论和思考网络。但我又讨厌这个类比，因为这些人工神经元在操作上与真实神经元差异很大，这个类比很快就会崩塌。它与实际神经元在解剖结构上有相似之处，但它们并不完全相同，这会导致那些不熟悉机器学习的人产生困惑，有些人甚至认为计算机科学家真的在构建人工大脑，或者认为网络会“思考”。“思考”这个词的意义难以捉摸，但在我看来，它并不适用于神经网络的行为。
- en: Returning now to [Figure 8-2](ch08.xhtml#ch8fig2), we see two squares on the
    left, a bunch of lines, a circle, a line on the right, and a bunch of labels with
    subscripts. Let’s sort this out. If we understand [Figure 8-2](ch08.xhtml#ch8fig2),
    we’ll be well on our way to understanding neural networks. Later, we’ll see an
    implementation of our visual model in code and be surprised to learn how simple
    it can be.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到[图 8-2](ch08.xhtml#ch8fig2)，我们看到左侧有两个方框，一些线条，一个圆圈，右侧有一条线，还有一些带下标的标签。让我们理清楚这些。如果我们理解了[图
    8-2](ch08.xhtml#ch8fig2)，我们就能很好地理解神经网络。稍后，我们会看到我们视觉模型的代码实现，并惊讶于它竟然如此简单。
- en: Everything in [Figure 8-2](ch08.xhtml#ch8fig2) focuses on the circle. This is
    the actual node. In reality, it implements a mathematical function called the
    *activation function*, which calculates the output of the node, a single number.
    The two squares are the inputs to the node. This node accepts features from an
    input feature vector; we use squares to differentiate from circles, but the input
    might just as well have come from another group of circular nodes in a previous
    network layer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 8-2](ch08.xhtml#ch8fig2)中，一切都集中在圆圈上。这就是实际的节点。实际上，它实现了一个称为*激活函数*的数学函数，用于计算节点的输出，即一个单一的数字。两个方框是节点的输入。这个节点接受来自输入特征向量的特征；我们用方框来与圆圈区分开，但输入也可以来自前一网络层中另一组圆形节点。
- en: Each input is a number, a single scalar value, which we’re calling *x*[0] and
    *x*[1]. These inputs move to the node along the two line segments labeled *w*[0]
    and *w*[1]. These line segments represent *weights*, the strength of the connection.
    Computationally, the inputs (*x*[0], *x*[1]) are multiplied by the weights (*w*[0],
    *w*[1]), summed, and given to the activation function of the node. Here we’re
    calling the activation function *h*, a fairly common thing to call it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入都是一个数字，一个标量值，我们称其为*x*[0]和*x*[1]。这些输入沿着标记为*w*[0]和*w*[1]的两条线段传递到节点。这些线段表示*权重*，即连接的强度。在计算上，输入（*x*[0]、*x*[1]）与权重（*w*[0]、*w*[1]）相乘、求和，然后传递给节点的激活函数。在这里，我们将激活函数称为*h*，这是一个相当常见的命名方式。
- en: The value of the activation function is the output of the node. Here we’re calling
    this output *a*. The inputs, multiplied by the weights, are added together and
    given to the activation function to produce an output value. We have yet to mention
    the *b*[0] value, which is also added in and passed to the activation function.
    This is the *bias* term. It’s an offset used to adjust the input range to make
    it suitable for the activation function. In [Figure 8-2](ch08.xhtml#ch8fig2),
    we added a zero subscript. There is a bias value for each node in each layer,
    so the subscript here implies that this node is the first node in the layer. (Remember
    computer people always count from zero, not one.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的值就是节点的输出值。在这里，我们将这个输出值称为*a*。输入值与权重相乘后相加，再传递给激活函数，产生输出值。我们还没有提到*b*[0]值，它也被加进去并传递给激活函数。这就是*偏置*项。它是一个偏移量，用于调整输入范围，使其适合激活函数。在[图8-2](ch08.xhtml#ch8fig2)中，我们添加了一个零下标。每个层中的每个节点都有一个偏置值，所以这里的下标意味着这是该层中的第一个节点。（记住，计算机领域的人总是从零开始计数，而不是从一开始。）
- en: 'This is all that a neural network node does: a neural network node accepts
    multiple inputs, *x*[0],*x*[1],…, multiplies each by a weight value, *w*[0],*w*[1],…,
    sums these products along with the bias term, *b*, and passes this sum to the
    activation function, *h*, to produce a single scalar output value, *a*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是神经网络节点的全部工作：神经网络节点接受多个输入，*x*[0]、*x*[1]、…，将每个输入与一个权重值* w*[0]、*w*[1]、…相乘，和偏置项*b*一起求和，然后将这个和传递给激活函数*h*，产生一个标量输出值*a*：
- en: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + … + *b*)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*a* = *h*(*w*[0]*x*[0] + *w*[1]*x*[1] + … + *b*)'
- en: 'That’s it. Get a bunch of nodes together, link them appropriately, figure out
    how to train them to set the weights and biases, and you have a useful neural
    network. As you’ll see in the next chapter, training a neural network is no easy
    feat. But once trained, they’re simple to use: feed it a feature vector, and out
    comes a classification.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。把一堆节点组合在一起，正确地连接它们，弄清楚如何训练它们来设定权重和偏置，你就得到了一个有用的神经网络。正如你在下一章看到的那样，训练一个神经网络并不容易。但一旦训练完成，它们的使用非常简单：给它输入一个特征向量，它就会输出一个分类。
- en: As an aside, we’ve been calling these graphs *neural networks*, and will continue
    to do so, sometimes using the abbreviation *NN*. If you read other books or papers,
    you might see them called *artificial neural networks (ANNs)* or even *multi-layer
    perceptrons (MLPs)*, as in the sklearn MLPClassifier class name. I recommend sticking
    with *neural network*, but that’s just me.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，我们一直在称这些图为*神经网络*，并将继续使用这个名称，有时也会使用缩写*NN*。如果你读过其他书籍或论文，可能会看到它们被称为*人工神经网络（ANNs）*，甚至是*多层感知器（MLPs）*，比如在sklearn的MLPClassifier类名中。我建议坚持使用*神经网络*，不过那只是我的看法。
- en: Activation Functions
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活函数
- en: Let’s talk about activation functions. The activation function for a node takes
    a single scalar input, the sum of the inputs times the weights plus the bias,
    and does something to it. In particular, we need the activation function to be
    nonlinear so that the model can learn complex functions. Mathematically, it’s
    easiest to see what a nonlinear function is by stating what a linear function
    is and then saying that any mapping that is not linear is . . . nonlinear.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈激活函数。节点的激活函数接收一个标量输入，即输入的加权和再加上偏置，然后对其进行处理。特别地，我们需要激活函数是非线性的，这样模型才能学习复杂的函数。从数学上讲，最容易理解非线性函数是什么的方法是先了解什么是线性函数，然后得出任何非线性映射都是...非线性的结论。
- en: A *linear function*, *g*, has output that is directly proportional to the input,
    *g*(*x*) ∝ *x*, where ∝ means *proportional to*. Alternatively, the graph of a
    linear function is a straight line. Therefore, any function whose graph is not
    a straight line is a nonlinear function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性函数*，*g*，的输出与输入成正比，*g*(*x*) ∝ *x*，其中 ∝ 表示 *与...成正比*。或者说，线性函数的图形是一条直线。因此，任何图形不是直线的函数都是非线性函数。'
- en: For example, the function
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，函数
- en: '*g*(*x*) = 3*x* + 2'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*g*(*x*) = 3*x* + 2'
- en: is a linear function because its graph is a straight line. A constant function
    like *g*(*x*) = 1 is also linear. However, the function
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个线性函数，因为它的图形是一条直线。像 *g*(*x*) = 1 这样的常数函数也是线性的。然而，函数
- en: '*g*(*x*) = *x*² + 2'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*g*(*x*) = *x*² + 2'
- en: is a nonlinear function because the exponent of *x* is 2\. Transcendental functions
    are also nonlinear. *Transcendental functions* are functions like *g*(*x*) = log*x*,
    or *g*(*x*) = *e*^(*x*), where *e* = 2.718*...* is the base of the natural logarithm.
    *Trigonometric functions* like sine and cosine, their inverses, and functions
    like tangent that are built from sine and cosine are also transcendental functions.
    These functions are transcendental because you cannot form them as finite combinations
    of elementary algebra operations. They are nonlinear because their graphs are
    not straight lines.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 是非线性函数，因为 *x* 的指数是 2。超越函数也是非线性的。*超越函数*是像 *g*(*x*) = log*x* 或 *g*(*x*) = *e*^(*x*)
    这样的函数，其中 *e* = 2.718*...* 是自然对数的底数。像正弦和余弦这样的三角函数、它们的反函数，以及从正弦和余弦构建的正切等函数也是超越函数。这些函数被称为超越函数，是因为你无法通过有限的代数运算将它们表示出来。它们是非线性的，因为它们的图形不是直线。
- en: The network needs nonlinear activation functions; otherwise, it will be able
    to learn only linear mappings, and linear mappings are not sufficient to make
    the networks generally useful. Consider a trivial network of two nodes, each with
    one input. This means there’s one weight and one bias value per node, and the
    output of the first node is the input of the second. If we set *h*(*x*) = 5*x
    –* 3, a linear function, then for input *x* the network computes output *a*[1]
    to be
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 网络需要非线性激活函数；否则，它只能学习线性映射，而线性映射不足以使网络具有广泛的实用性。考虑一个简单的网络，包含两个节点，每个节点有一个输入。这意味着每个节点都有一个权重和一个偏置值，第一个节点的输出就是第二个节点的输入。如果我们设置
    *h*(*x*) = 5*x –* 3，这是一个线性函数，那么对于输入 *x*，网络计算出的输出 *a*[1] 为
- en: '| *a*[1] | = *h*(*w*[1]*a*[0] + *b*[1]) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| *a*[1] | = *h*(*w*[1]*a*[0] + *b*[1]) |'
- en: '|  | = *h*(*w*[1]*h*(*w*[0]*x* + *b*[0]) + *b*[1]) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(*w*[1]*h*(*w*[0]*x* + *b*[0]) + *b*[1]) |'
- en: '|  | = *h*(*w*[1](5(*w*[0]*x* + *b*[0]) – 3) + *b*[1]) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(*w*[1](5(*w*[0]*x* + *b*[0]) – 3) + *b*[1]) |'
- en: '|  | = *h*(*w*[1](5*w*[0]*x* + 5*b*[0] – 3) + *b*[1]) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(*w*[1](5*w*[0]*x* + 5*b*[0] – 3) + *b*[1]) |'
- en: '|  | = *h*(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | = *h*(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) |'
- en: '|  | = 5(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) – 3 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | = 5(5*w*[1]*w*[0]*x* + 5*w*[1]*b*[0] – 3*w*[1] + *b*[1]) – 3 |'
- en: '|  | = (25*w*[1]*w*[0])*x* + (25*w*[1]*b*[0] – [1]5*w*[1] + 5*b*[1] – 3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | = (25*w*[1]*w*[0])*x* + (25*w*[1]*b*[0] – [1]5*w*[1] + 5*b*[1] – 3) |'
- en: '|  | = *W**x* + *B* |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | = *W**x* + *B* |'
- en: 'for *W* = 25*w*[1]*w*[0] and *B* = 25*w*[1]*b*[0] *–* 15*w*[1] + 5*b*[1] *–*
    3, which is also a linear function, another line with slope *W* and intercept
    *B* since neither *W* nor *B* depend on *x*. Therefore, a neural network with
    linear activation functions would learn only a linear model since the composition
    of linear functions is also linear. It’s precisely this limitation of linear activation
    functions that caused the first neural network “winter” in the 1970s: research
    into neural networks was effectively abandoned because they were thought to be
    too simple to learn complex functions.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *W* = 25*w*[1]*w*[0] 和 *B* = 25*w*[1]*b*[0] *–* 15*w*[1] + 5*b*[1] *–* 3，这也是一个线性函数，另一条斜率为
    *W* 和截距为 *B* 的直线，因为 *W* 和 *B* 都不依赖于 *x*。因此，使用线性激活函数的神经网络只能学习线性模型，因为线性函数的组合仍然是线性的。正是由于线性激活函数的这个局限性，导致了1970年代第一次神经网络“冬天”：对神经网络的研究几乎被放弃，因为人们认为它们过于简单，无法学习复杂的函数。
- en: Okay, so we want nonlinear activation functions. Which ones? There are an infinite
    number of possibilities. In practice, a few have risen to the top because of their
    proven usefulness or nice properties or both. Traditional neural networks used
    either sigmoid activation functions or hyperbolic tangents. A *sigmoid* is
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们需要非线性激活函数。哪些函数呢？实际上，可能的选择是无限的。经过实践验证，一些函数因其有效性、优良性质或两者兼具而脱颖而出。传统的神经网络使用的是
    sigmoid 激活函数或双曲正切函数。*Sigmoid* 是
- en: '![image](Images/174equ01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/174equ01.jpg)'
- en: and the *hyperbolic tangent* is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*双曲正切*是'
- en: '![image](Images/174equ02.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/174equ02.jpg)'
- en: Plots of both of these functions are in [Figure 8-3](ch08.xhtml#ch8fig3), with
    the sigmoid on the top and the hyperbolic tangent on the bottom.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数的图像见于[图8-3](ch08.xhtml#ch8fig3)，其中Sigmoid位于上方，双曲正切位于下方。
- en: The first thing to notice is that both of these functions have roughly the same
    “S” shape. The sigmoid runs from 0 as you go further left along the x-axis to
    1 as you go to the right. At 0, the function value is 0.5\. The hyperbolic tangent
    does the same but goes from –1 to +1 and is 0 at *x* = 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，这两个函数都有大致相同的“S”形状。Sigmoid函数在x轴向左延伸时从0开始，到向右延伸时达到1。在x=0时，函数值为0.5。双曲正切函数也有相同的形状，但从-1到+1变化，并且在*x*
    = 0时为0。
- en: More recently, the sigmoid and hyperbolic tangent have been replaced by the
    *rectified linear unit*, or *ReLU* for short. The ReLU is simple, and has convenient
    properties for neural networks. Even though the word *linear* is in the name,
    the ReLU is a nonlinear function—its graph is not a straight line. When we discuss
    backpropagation training of neural networks in [Chapter 9](ch09.xhtml#ch09), we’ll
    learn why this change has happened.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Sigmoid和双曲正切函数已被*修正线性单元*，简称*ReLU*所取代。ReLU简单且具有适用于神经网络的便捷特性。尽管名字中有*线性*，但ReLU是一个非线性函数——它的图像并不是一条直线。当我们在[第9章](ch09.xhtml#ch09)讨论神经网络的反向传播训练时，我们将了解为什么会发生这种变化。
- en: '![image](Images/08fig03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig03.jpg)'
- en: '*Figure 8-3: A sigmoid function (top) and a hyperbolic tangent function (bottom).
    Note that the y-axis scales are not the same*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-3：Sigmoid函数（上）和双曲正切函数（下）。请注意，y轴的尺度不同*。'
- en: The ReLU is as follows and is shown in [Figure 8-4](ch08.xhtml#ch8fig4).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU如下所示，并展示在[图8-4](ch08.xhtml#ch8fig4)中。
- en: '![image](Images/176equ01.jpg)![image](Images/08fig04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/176equ01.jpg)![image](Images/08fig04.jpg)'
- en: '*Figure 8-4: The rectified linear activation function, ReLU(x) = max(0,x)*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8-4：修正线性激活函数，ReLU(x) = max(0,x)*'
- en: ReLU is called *rectified* because it removes the negative values and replaces
    them with 0\. In truth, the machine learning community uses several different
    versions of this function, but all are essentially replacing negative values with
    a constant or some other value. The piecewise nature of the ReLU is what makes
    it nonlinear and, therefore, suitable for use as a neural network activation function.
    It’s also computationally simple, far faster to calculate than either the sigmoid
    or the hyperbolic tangent. This is because the latter functions use *e*^(*x*),
    which, in computer terms, means a call to the exp function. This function is typically
    implemented as a sum of terms of a series expansion, translating into dozens of
    floating-point operations in place of the single if statement necessary to implement
    a ReLU. Small savings like this add up in an extensive network with potentially
    thousands of nodes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU之所以被称为*修正*，是因为它去除了负值并将其替换为0。事实上，机器学习社区使用了几种不同版本的这个函数，但所有版本本质上都是将负值替换为常数或其他值。ReLU的分段性质使它成为一个非线性函数，因此适合用作神经网络的激活函数。它在计算上也非常简单，比Sigmoid或双曲正切要快得多。这是因为后两者函数使用*e*^(*x*)，在计算机术语中，这意味着调用exp函数。这个函数通常是通过一系列展开的和来实现的，这意味着它会在实际计算中产生数十次浮点运算，而实现ReLU只需一个简单的if语句。像这样的微小节省在一个可能有成千上万个节点的庞大网络中积累起来。
- en: Architecture of a Network
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络的架构
- en: We’ve discussed nodes and how they work, and hinted that nodes are connected
    to form networks. Let’s look more closely at how nodes are connected, the *architecture*
    of the network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了节点及其工作原理，并暗示了节点是如何连接形成网络的。让我们更仔细地看看节点是如何连接的，以及网络的*架构*。
- en: Standard neural networks like the ones we are working with in this chapter are
    built in layers, as you saw in [Figure 8-1](ch08.xhtml#ch8fig1). We don’t need
    to do this, but as we’ll see, this buys us some computational simplicity and greatly
    simplifies training. A feedforward network has an input layer, one or more hidden
    layers, and an output layer. The input layer is simply the feature vector, and
    the output layer is the prediction (probability). If the network is for a multiclass
    problem, the output layer might have more than one node, with each node representing
    the model’s prediction for each of the possible classes of inputs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 像本章中我们正在使用的标准神经网络一样，通常是分层构建的，正如你在[图8-1](ch08.xhtml#ch8fig1)中看到的。我们不一定要这样做，但正如我们将看到的，这样做能带来一些计算上的简便性，并大大简化训练过程。前馈网络包括一个输入层、一个或多个隐藏层和一个输出层。输入层仅仅是特征向量，而输出层则是预测（概率）。如果网络是用于多类问题，输出层可能有多个节点，每个节点表示模型对每个可能类别的预测。
- en: The hidden layers are made of nodes, and the nodes of layer *i* accept as input
    the output of the nodes of layer *i –* 1 and pass their outputs to the inputs
    of the nodes of layer *i* + 1\. The connections between the layers are typically
    fully connected, meaning every output of every node of layer *i –* 1 is used as
    an input to every node of layer *i*, hence *fully connected*. Again, we don’t
    need to do this, but it simplifies the implementation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层由节点构成，层 *i* 的节点接受来自层 *i –* 1 的节点输出作为输入，并将它们的输出传递给层 *i* + 1 的节点输入。层与层之间的连接通常是全连接的，这意味着层
    *i –* 1 的每个节点输出都作为输入用于层 *i* 中的每个节点，因此称为*全连接*。再次强调，我们不一定要这样做，但它简化了实现过程。
- en: The number of hidden layers and the number of nodes in each hidden layer define
    the architecture of the network. It has been proven that a single hidden layer
    with enough nodes can learn any function mapping. This is good because it means
    neural networks are applicable to machine learning problems since, in the end,
    the model acts as a complex function mapping inputs to output labels and probabilities.
    However, like many theoretical results, this does not mean that it’s practical
    for a single layer network to be used in all situations. As the number of nodes
    (and layers) in a network grows, so, too, does the number of parameters to learn
    (weights and biases), and therefore the amount of training data needed goes up
    as well. It’s the curse of dimensionality again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的数量和每个隐藏层中节点的数量定义了网络的架构。已经证明，单一隐藏层且节点足够多时，能够学习任何函数映射。这是件好事，因为这意味着神经网络可以应用于机器学习问题，因为最终，模型作为一个复杂的函数，将输入映射到输出标签和概率。然而，像许多理论结果一样，这并不意味着在所有情况下单层网络都是实用的。随着网络中节点（和层数）数量的增加，学习的参数数量（权重和偏置）也会增加，因此所需的训练数据量也会增加。这又是维度诅咒的体现。
- en: Issues like these stymied neural networks for a second time in the 1980s. Computers
    were too slow to train large networks, and, regardless, there was usually too
    little data available to train the network anyway. Practitioners knew that if
    both of these situations changed, then it would become possible to train large
    networks that would be far more capable than the small networks of the time. Fortunately
    for the world, the situation changed in the early 2000s.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题在1980年代再次困扰了神经网络。计算机太慢，无法训练大型网络，而且，无论如何，通常也没有足够的数据来训练网络。实践者们知道，如果这两种情况发生变化，那么训练大型网络就变得可能，这些网络将比当时的小型网络更强大。幸运的是，到了2000年代初，情况发生了变化。
- en: 'Selecting the proper neural network architecture has a huge impact on whether
    or not your model will learn anything. This is where experience and intuition
    come in. Selecting the right architecture is the dark art of using neural networks.
    Let’s try to be more helpful by giving some (crude) rules of thumb:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的神经网络架构对模型是否能够学习到有效的内容有着巨大影响。这就是经验和直觉发挥作用的地方。选择正确的架构是使用神经网络的“黑魔法”。为了更有帮助，我们可以给出一些（粗略的）经验法则：
- en: If your input has definite spatial relationships, like the parts of an image,
    you might want to use a convolutional neural network instead ([Chapter 12](ch12.xhtml#ch12)).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的输入有明确的空间关系，比如图像的各个部分，你可能想使用卷积神经网络（[第12章](ch12.xhtml#ch12)）。
- en: Use no more than three hidden layers. Recall, in theory, one sufficiently large
    hidden layer is all that is needed, so use as few hidden layers as necessary.
    If the model learns with one hidden layer, then add a second to see if that improves
    things.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最多使用三个隐藏层。回想一下，理论上，一个足够大的隐藏层就足够了，因此尽量只使用必要的隐藏层。如果模型在使用一个隐藏层时能够学习，那么可以尝试增加第二个隐藏层，看是否能够提高效果。
- en: The number of nodes in the first hidden layer should match or (ideally) exceed
    the number of input vector features.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个隐藏层的节点数应与输入特征向量的数量相匹配或（理想情况下）超过该数量。
- en: Except for the first hidden layer (see previous rule), the number of nodes per
    hidden layer should be the same as or some value between the number of nodes in
    the previous layer and the following layer. If layer *i –* 1 has *N* nodes and
    layer *i* + 1 has *M* nodes, then layer *i* might be good with *N* ≤ *x* ≤ *M*
    nodes.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了第一个隐藏层（参见前面的规则）外，每个隐藏层的节点数应与上一层和下一层的节点数相等或介于两者之间。如果第 *i –* 1 层有 *N* 个节点，第
    *i* + 1 层有 *M* 个节点，那么第 *i* 层可能适合有 *N* ≤ *x* ≤ *M* 个节点。
- en: The first rule says that a traditional neural network best applies to situations
    where your input does not have spatial relationships—that is, you have a feature
    vector, not an image. Also, when your input dimension is small, or when you do
    not have a lot of data, which makes it hard to train a larger convolutional network,
    you should give a traditional network a try. If you do think you are in a situation
    where a traditional neural network is called for, start small, and grow it as
    long as performance improves.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个规则表明，传统神经网络最适用于输入没有空间关系的情况——也就是说，输入是特征向量，而不是图像。此外，当输入维度较小，或者没有足够的数据，导致训练一个更大的卷积网络困难时，应该尝试使用传统网络。如果你认为自己处于需要传统神经网络的情况，建议从小开始，随着性能的提升逐步扩展。
- en: Output Layers
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出层
- en: The last layer of a neural network is the output layer. If the network is modeling
    a continuous value, known as *regression*, a use case we’re ignoring in this book,
    then the output layer is a node that doesn’t use an activation function; it simply
    reports the argument to *h* in [Figure 8-2](ch08.xhtml#ch8fig2). Note that this
    is the same as saying that the activation function is the identity function, *h*(*x*)
    = *x*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的最后一层是输出层。如果网络建模的是连续值，也就是 *回归*，这是本书忽略的用例，那么输出层是一个不使用激活函数的节点；它仅仅报告 *h* 的参数，如[图
    8-2](ch08.xhtml#ch8fig2)所示。请注意，这与说激活函数是身份函数 *h*(*x*) = *x* 是一样的。
- en: 'Our neural networks are for classification; we want them to output a decision
    value. If we have two classes labeled 0 and 1, we make the activation function
    of the final node a sigmoid. This will output a value between 0 and 1 that we
    can interpret as a likelihood or probability that the input belongs to class 1\.
    We make our classification decision based on the output value with a simple rule:
    if the activation value is less than 0.5, call the input class 0; otherwise, call
    it class 1\. We’ll see in [Chapter 11](ch11.xhtml#ch11) how changing this threshold
    of 0.5 can be used to tune a model’s performance for the task at hand.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络是用于分类的；我们希望它们输出一个决策值。如果我们有两个类别，分别标记为 0 和 1，我们将最终节点的激活函数设置为 sigmoid。这将输出一个介于
    0 和 1 之间的值，我们可以将其解释为输入属于类别 1 的可能性或概率。我们根据输出值使用简单规则做出分类决策：如果激活值小于 0.5，则将输入分类为 0
    类；否则，分类为 1 类。我们将在[第 11 章](ch11.xhtml#ch11)看到如何通过改变 0.5 的阈值来调整模型在特定任务中的表现。
- en: If we have more than two classes, we need to take a different approach. Instead
    of a single node in the output layer, we’ll have *N* output nodes, one for each
    class, each one using the identity function for *h*. Then, we apply a *softmax*
    operation to these *N* outputs and select the output with the largest softmax
    value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有多个类别，我们需要采取不同的方法。我们将不再使用输出层中的单个节点，而是拥有 *N* 个输出节点，每个类别一个，每个节点都使用身份函数作为 *h*。然后，我们对这
    *N* 个输出应用 *softmax* 操作，并选择具有最大 softmax 值的输出。
- en: Let’s illustrate what we mean by softmax. Suppose we have a dataset with four
    classes in it. What they represent doesn’t really matter; the network doesn’t
    know what they represent, either. The classes are labeled 0, 1, 2, and 3\. So,
    *N* = 4 means our network will have four output nodes, each one using the identity
    function for *h*. This looks like [Figure 8-5](ch08.xhtml#ch8fig5), where we have
    also shown the softmax operation and the resulting output vector.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来说明一下 softmax 的含义。假设我们有一个包含四个类别的数据集。它们表示什么并不重要；网络也不知道它们代表什么。这些类别被标记为 0、1、2
    和 3。因此，*N* = 4 表示我们的网络将有四个输出节点，每个节点都使用身份函数作为 *h*。这看起来像是 [图 8-5](ch08.xhtml#ch8fig5)，其中我们也展示了
    softmax 操作和由此产生的输出向量。
- en: '![image](Images/08fig05.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig05.jpg)'
- en: '*Figure 8-5: The last hidden layer *n*-1 and output layer (*n*, nodes numbered)
    for a neural network with four classes. The softmax operation is applied, producing
    a four-element output vector, *[p*0,*p*1,*p*2,*p*3*]*.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-5：具有四个类别的神经网络的最后一层隐藏层 *n*-1 和输出层 (*n*，节点编号)。应用 softmax 操作，生成一个四元素输出向量，*
    [p*0,*p*1,*p*2,*p*3*]*。'
- en: We select the index of the largest value in this output vector as the class
    label for the given input feature vector. The softmax operation ensures that the
    elements of this vector sum to 1, so we can again be a bit sloppy and call these
    values the probability of belonging to each of the four classes. That is why we
    take only the largest value to decide the output class label.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择该输出向量中最大值的索引作为给定输入特征向量的类别标签。softmax 操作确保该向量的元素总和为 1，因此我们可以稍微不那么严格地称这些值为属于四个类别中每个类别的概率。这就是为什么我们只取最大值来决定输出类别标签的原因。
- en: 'The softmax operation is straightforward: the probability for each of the outputs
    is simply'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 操作是直接的：每个输出的概率就是
- en: '![image](Images/179equ01.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/179equ01.jpg)'
- en: where *a*[*i*] is the *i*-th output, and the denominator is the sum over all
    the outputs. For the example, *i* = 0,1,2,3, and the index of the largest value
    will be the class label assigned to the input.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a*[*i*] 是第 *i* 个输出，分母是所有输出的总和。对于这个示例，*i* = 0,1,2,3，最大值的索引将是分配给输入的类别标签。
- en: As an example, assume the output of the four last layer nodes is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设四个最后一层节点的输出为
- en: '*a*[0] = 0.2'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[0] = 0.2'
- en: '*a*1 = 1.3'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*1 = 1.3'
- en: '*a*2 = 0.8'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*2 = 0.8'
- en: '*a*3 = 2.1'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*3 = 2.1'
- en: 'Then calculate the softmax as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下方式计算 softmax：
- en: '*p*[0] = *e*^(0.2)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.080'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[0] = *e*^(0.2)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.080'
- en: '*p*[1] = *e*^(1.3)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.240'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[1] = *e*^(1.3)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.240'
- en: '*p*[2] = *e*^(0.8)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.146'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[2] = *e*^(0.8)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.146'
- en: '*p*[3] = *e*^(2.1)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.534'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[3] = *e*^(2.1)/(*e*^(0.2) + *e*^(1.3) + *e*^(0.8) + *e*^(2.1)) = 0.534'
- en: Select class 3 because *p*[3] is the largest. Note that the sum of the *p*[*i*]
    values is 1.0, as we would expect.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 选择类 3，因为 *p*[3] 是最大的。注意，*p*[*i*] 值的总和是 1.0，这是我们预期的。
- en: 'Two points should be mentioned here. In the preceding equations, we used the
    sigmoid to calculate the output of the network. If we set the number of classes
    to 2 and calculate the softmax, we’ll get two output values: one will be some
    *p*, and the other will be 1 *– p*. This is identical to the sigmoid alone, selecting
    the probability of the input being of class 1.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两点需要提到。在前面的公式中，我们使用了 sigmoid 函数来计算网络的输出。如果我们将类别数设为 2 并计算 softmax，那么我们将得到两个输出值：一个是某个
    *p*，另一个是 1 *– p*。这与仅使用 sigmoid 是一致的，后者选择了输入属于类 1 的概率。
- en: The second point has to do with implementing the softmax. If the network outputs,
    the *a* values, are large, then *e*^(*a*) might be very large, which is something
    the computer will not like. Precision will be lost, at least, or the value might
    overflow and make the output meaningless. Numerically, if we subtract the largest
    *a* value from all the others before calculating the softmax, we’ll take the exponential
    over smaller values that are less likely to overflow. Doing this for the preceding
    example gives new *a* values
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点涉及到 softmax 的实现。如果网络输出的 *a* 值很大，那么 *e*^(*a*) 可能会非常大，这会是计算机不喜欢的情况。至少会丢失精度，或者值可能溢出使得输出没有意义。从数值上看，如果我们在计算
    softmax 之前从所有其他值中减去最大 *a* 值，那么我们就会对较小的值取指数，这些值不太可能发生溢出。对前面的例子进行这样的处理后，我们得到新的 *a*
    值
- en: '![image](Images/180equ02.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/180equ02.jpg)'
- en: where we subtract 2.1 because that is the largest *a* value. This leads to precisely
    the same *p* values we found before, but this time protected against overflow
    in the case that any of the *a* values are too large.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们减去 2.1，因为这是最大的 *a* 值。这导致了与之前找到的完全相同的 *p* 值，但这次对溢出的情况进行了保护，以防任何 *a* 值过大。
- en: Representing Weights and Biases
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表示权重和偏置
- en: Before we move on to an example neural network, let’s revisit the weights and
    biases and see that we can greatly simplify the implementation of a neural network
    by viewing it in terms of matrices and vectors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续进行神经网络示例之前，先回顾一下权重和偏置，并且可以看到通过将神经网络视为矩阵和向量的形式，我们可以大大简化神经网络的实现。
- en: Consider the mapping from an input feature vector of two elements to the first
    hidden layer with three nodes (*a*[1] in [Figure 8-1](ch08.xhtml#ch8fig1)). Let’s
    label the edges between the two layers (the weights) as *w*[*ij*] with *i* = 0,1
    for the inputs *x*[0] and *x*[1] and *j* = 0,1,2 for the three hidden layer nodes
    numbered from top to bottom of the figure. Additionally, we need three bias values
    that are not shown in the figure, one for each hidden node. We’ll call these *b*[0],
    *b*[1], and *b*[2], again, top to bottom.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将一个包含两个元素的输入特征向量映射到第一个具有三个节点的隐藏层（*a*[1]，见[图8-1](ch08.xhtml#ch8fig1)）。我们将两层之间的边（权重）标记为
    *w*[*ij*]，其中 *i* = 0,1 对应输入 *x*[0] 和 *x*[1]，*j* = 0,1,2 对应三个隐藏层节点，从图中自上而下编号。此外，我们还需要三个偏置值，这些值在图中未显示，每个隐藏节点一个。我们将它们称为
    *b*[0]、*b*[1] 和 *b*[2]，同样是自上而下排列。
- en: In order to calculate the outputs of the activation functions, *h*, for the
    three hidden nodes, we need to find the following.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算三个隐藏节点的激活函数 *h* 的输出，我们需要找出以下内容。
- en: '*a*[0] = *h*(*w*[00]*x*[0] + *w*[10]*x*[1] + *b*[0])'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[0] = *h*(*w*[00]*x*[0] + *w*[10]*x*[1] + *b*[0])'
- en: '*a*[1] = *h*(*w*[01]*x*[0] + *w*[11]*x*[1] + *b*[1])'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[1] = *h*(*w*[01]*x*[0] + *w*[11]*x*[1] + *b*[1])'
- en: '*a*[2] = *h*(*w*[02]*x*[0] + *w*[12]*x*[1] + *b*[2])'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[2] = *h*(*w*[02]*x*[0] + *w*[12]*x*[1] + *b*[2])'
- en: But, remembering how matrix multiplication and vector addition work, we see
    that this is exactly
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，记住矩阵乘法和向量加法的工作原理，我们可以看到这正是
- en: '![image](Images/181equ02.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/181equ02.jpg)'
- en: where ![Image](Images/181equ03.jpg), and *W* is a 3 × 2 matrix of weight values.
    In this case, the activation function, *h*, is given a vector of input values
    and produces a vector of output values. This is simply applying *h* to every element
    of ![Image](Images/181equ04.jpg). For example, applying *h* to a vector ![Image](Images/xbar1.jpg)
    with three elements is
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/181equ03.jpg)，*W* 是一个 3 × 2 的权重值矩阵。在这种情况下，激活函数 *h* 接收一个输入值的向量，并生成一个输出值的向量。这实际上是将
    *h* 应用到 ![Image](Images/181equ04.jpg) 的每个元素。例如，将 *h* 应用于一个包含三个元素的向量 ![Image](Images/xbar1.jpg)
- en: '![image](Images/181equ05.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/181equ05.jpg)'
- en: with *h* applied separately to each element of ![Image](Images/xbar1.jpg).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *h* 分别应用于 ![Image](Images/xbar1.jpg) 的每个元素。
- en: 'Since the NumPy Python module is designed to work with arrays, and matrices
    and vectors are arrays, we arrive at the pleasant conclusion that the weights
    and biases of a neural network can be stored in NumPy arrays and we need only
    simple matrix operations (calls to np.dot) and addition to work with a fully connected
    neural network. Note this is why we want to use fully connected networks: their
    implementation is straightforward.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 NumPy Python 模块设计用于处理数组，而矩阵和向量都是数组，我们得出一个令人愉快的结论，即神经网络的权重和偏置可以存储在 NumPy 数组中，只需简单的矩阵运算（调用
    np.dot）和加法即可操作完全连接的神经网络。请注意，这也是我们希望使用全连接网络的原因：它们的实现是直接的。
- en: 'To store the network of [Figure 8-1](ch08.xhtml#ch8fig1), we need a weight
    matrix and bias vector between each layer, giving us three matrices and three
    vectors: a matrix and vector each for the input to the first hidden layer, the
    first hidden layer to the second, and the second hidden layer to the output. The
    weight matrices are of dimensions 3 × 2, 2 × 3, and 1 × 2, respectively. The bias
    vectors are of length 3, 2, and 1.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储[图8-1](ch08.xhtml#ch8fig1)中的网络，我们需要在每一层之间设置权重矩阵和偏置向量，从而得到三个矩阵和三个向量：每一层的输入到第一个隐藏层，第一个隐藏层到第二个隐藏层，第二个隐藏层到输出层的矩阵和向量。权重矩阵的维度分别为
    3 × 2、2 × 3 和 1 × 2。偏置向量的长度分别为 3、2 和 1。
- en: Implementing a Simple Neural Network
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个简单的神经网络
- en: In this section, we’ll implement the sample neural network of [Figure 8-1](ch08.xhtml#ch8fig1)
    and train it on two features from the iris dataset. We’ll implement the network
    from scratch but use sklearn to train it. The goal of this section is to see how
    straightforward it is to implement a simple neural network. Hopefully, this will
    clear some of the fog that might be hanging around from the discussion of the
    previous sections.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现[图 8-1](ch08.xhtml#ch8fig1)中的示例神经网络，并使用鸢尾花数据集的两个特征进行训练。我们将从头实现网络，但使用
    sklearn 来训练它。本节的目标是看看实现一个简单神经网络有多直接。希望这能清除上一节讨论中可能存在的一些迷雾。
- en: The network of [Figure 8-1](ch08.xhtml#ch8fig1) accepts an input feature vector
    with two features. It has two hidden layers, one with three nodes and the other
    with two nodes. It has one sigmoid output. The activation functions of the hidden
    nodes are also sigmoids.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-1](ch08.xhtml#ch8fig1)中的网络接受一个具有两个特征的输入特征向量。它有两个隐藏层，一个包含三个节点，另一个包含两个节点。它有一个
    sigmoid 输出层。隐藏层节点的激活函数也是 sigmoid。'
- en: Building the Dataset
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建数据集
- en: Before we look at the neural network code, let’s build the dataset we’ll train
    against and see what it looks like. We know the iris dataset already, but for
    this example, we’ll use only two classes and only two of the four features. The
    code to build the train and test datasets is in [Listing 8-1](ch08.xhtml#ch8lis1).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看神经网络代码之前，先构建一下我们要训练的数据集，看看它是什么样的。我们已经知道鸢尾花数据集，但在这个例子中，我们只使用两个类别，并且只使用四个特征中的两个。构建训练和测试数据集的代码在[清单
    8-1](ch08.xhtml#ch8lis1)中。
- en: import numpy as np
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: ❶ d = np.load("iris_train_features_augmented.npy")
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ d = np.load("iris_train_features_augmented.npy")
- en: l = np.load("iris_train_labels_augmented.npy")
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: l = np.load("iris_train_labels_augmented.npy")
- en: d1 = d[np.where(l==1)]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: d1 = d[np.where(l==1)]
- en: d2 = d[np.where(l==2)]
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: d2 = d[np.where(l==2)]
- en: ❷ a=len(d1)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ a=len(d1)
- en: b=len(d2)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: b=len(d2)
- en: x = np.zeros((a+b,2))
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.zeros((a+b,2))
- en: x[:a,:] = d1[:,2:]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: x[:a,:] = d1[:,2:]
- en: x[a:,:] = d2[:,2:]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: x[a:,:] = d2[:,2:]
- en: ❸ y = np.array([0]*a+[1]*b)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ y = np.array([0]*a+[1]*b)
- en: i = np.argsort(np.random.random(a+b))
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.argsort(np.random.random(a+b))
- en: x = x[i]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: ❹ np.save("iris2_train.npy", x)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ np.save("iris2_train.npy", x)
- en: np.save("iris2_train_labels.npy", y)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris2_train_labels.npy", y)
- en: ❺ d = np.load("iris_test_features_augmented.npy")
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ d = np.load("iris_test_features_augmented.npy")
- en: l = np.load("iris_test_labels_augmented.npy")
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: l = np.load("iris_test_labels_augmented.npy")
- en: d1 = d[np.where(l==1)]
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: d1 = d[np.where(l==1)]
- en: d2 = d[np.where(l==2)]
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: d2 = d[np.where(l==2)]
- en: a=len(d1)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: a=len(d1)
- en: b=len(d2)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: b=len(d2)
- en: x = np.zeros((a+b,2))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: x = np.zeros((a+b,2))
- en: x[:a,:] = d1[:,2:]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: x[:a,:] = d1[:,2:]
- en: x[a:,:] = d2[:,2:]
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: x[a:,:] = d2[:,2:]
- en: y = np.array([0]*a+[1]*b)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: y = np.array([0]*a+[1]*b)
- en: i = np.argsort(np.random.random(a+b))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: i = np.argsort(np.random.random(a+b))
- en: x = x[i]
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: x = x[i]
- en: y = y[i]
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: y = y[i]
- en: np.save("iris2_test.npy", x)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris2_test.npy", x)
- en: np.save("iris2_test_labels.npy", y)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: np.save("iris2_test_labels.npy", y)
- en: '*Listing 8-1: Building the simple example dataset. See* nn_iris_dataset.py.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：构建简单的示例数据集。见* nn_iris_dataset.py。'
- en: This code is straightforward data munging. We start with the augmented dataset
    and load the samples and labels ❶. We want only class 1 and class 2, so we find
    the indices of those samples and pull them out. We’re keeping only features 2
    and 3 and put them in x ❷. Next, we build the labels (y) ❸. Note, we recode the
    class labels to 0 and 1\. Finally, we scramble the order of the samples and write
    the new dataset to disk ❹. Last of all, we repeat this process to build the test
    samples ❺.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是直接的数据处理。我们从增强数据集开始，加载样本和标签❶。我们只需要类别 1 和类别 2，所以我们找到这些样本的索引并将它们提取出来。我们只保留特征
    2 和 3，并将它们放入 x ❷。接下来，我们构建标签（y）❸。注意，我们将类别标签重新编码为 0 和 1。最后，我们打乱样本的顺序并将新的数据集写入磁盘❹。最后，我们重复这个过程来构建测试样本❺。
- en: '[Figure 8-6](ch08.xhtml#ch8fig6) shows the training set. We can plot it in
    this case because we have only two features.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-6](ch08.xhtml#ch8fig6)展示了训练集。因为我们只有两个特征，所以可以绘制它。'
- en: '![image](Images/08fig06.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig06.jpg)'
- en: '*Figure 8-6: The training data for the two-class, two-feature iris dataset*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-6：两类两特征鸢尾花数据集的训练数据*'
- en: We immediately see that this dataset is not trivially separable. There is no
    simple line we can draw that will correctly split the training set into two groups,
    one all class 0 and the other all class 1\. This makes things a little more interesting.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会发现这个数据集并不是简单地可分的。没有简单的线可以将训练集正确地分成两组，一组全是类别 0，另一组全是类别 1。这使得问题变得更有趣。
- en: Implementing the Neural Network
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现神经网络
- en: Let’s see how to implement the network of [Figure 8-1](ch08.xhtml#ch8fig1) in
    Python using NumPy. We’ll assume that it’s already trained, meaning we already
    know all the weights and biases. The code is in [Listing 8-2](ch08.xhtml#ch8lis2).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用 NumPy 在 Python 中实现 [图 8-1](ch08.xhtml#ch8fig1) 中的网络。我们假设网络已经训练好，也就是说我们已经知道所有的权重和偏置。代码见
    [清单 8-2](ch08.xhtml#ch8lis2)。
- en: import numpy as np
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pickle
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: import pickle
- en: import sys
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: 'def sigmoid(x):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 'def sigmoid(x):'
- en: return 1.0 / (1.0 + np.exp(-x))
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: return 1.0 / (1.0 + np.exp(-x))
- en: 'def evaluate(x, y, w):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'def evaluate(x, y, w):'
- en: ❶ w12,b1,w23,b2,w34,b3 = w
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ w12, b1, w23, b2, w34, b3 = w
- en: nc = nw = 0
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: nc = nw = 0
- en: prob = np.zeros(len(y))
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: prob = np.zeros(len(y))
- en: 'for i in range(len(y)):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(y)):'
- en: a1 = sigmoid(np.dot(x[i], w12) + b1)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: a1 = sigmoid(np.dot(x[i], w12) + b1)
- en: a2 = sigmoid(np.dot(a1, w23) + b2)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: a2 = sigmoid(np.dot(a1, w23) + b2)
- en: prob[i] = sigmoid(np.dot(a2, w34) + b3)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: prob[i] = sigmoid(np.dot(a2, w34) + b3)
- en: z  = 0 if prob[i] < 0.5 else 1
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: z = 0 if prob[i] < 0.5 else 1
- en: '❷ if (z == y[i]):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ if (z == y[i]):'
- en: nc += 1
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: nc += 1
- en: 'else:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: nw += 1
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: nw += 1
- en: return [float(nc) / float(nc + nw), prob]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: return [float(nc) / float(nc + nw), prob]
- en: ❸ xtest = np.load("iris2_test.npy")
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ xtest = np.load("iris2_test.npy")
- en: ytest = np.load("iris2_test_labels.npy")
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ytest = np.load("iris2_test_labels.npy")
- en: ❹ weights = pickle.load(open("iris2_weights.pkl","rb"))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ weights = pickle.load(open("iris2_weights.pkl", "rb"))
- en: score, prob = evaluate(xtest, ytest, weights)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: score, prob = evaluate(xtest, ytest, weights)
- en: print()
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'for i in range(len(prob)):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(prob)):'
- en: 'print("%3d:  actual: %d  predict: %d  prob: %0.7f" %'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%3d:  实际: %d  预测: %d  概率: %0.7f" %'
- en: (i, ytest[i], 0 if (prob[i] < 0.5) else 1, prob[i]))
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (i, ytest[i], 0 if (prob[i] < 0.5) else 1, prob[i]))
- en: print("Score = %0.4f" % score)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: print("得分 = %0.4f" % score)
- en: '*Listing 8-2: Using the trained weights and biases to classify held-out test
    samples. See* nn_iris_evaluate.py.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-2：使用训练好的权重和偏置对保留的测试样本进行分类。见* nn_iris_evaluate.py。'
- en: Perhaps the first thing we should notice is how short the code is. The evaluate
    function implements the network. We also need to define sigmoid as NumPy does
    not have it natively. The main code loads the test samples (xtest) and associated
    labels (ytest) ❸. These are the files generated by the preceding code, so we know
    that xtest is of shape 23 × 2 because we have 23 test samples, and each has two
    features. Similarly, ytest is a vector of 23 labels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们首先需要注意的是代码的简洁。evaluate 函数实现了网络。我们还需要定义 sigmoid，因为 NumPy 本身没有提供这个函数。主代码加载了测试样本（xtest）和相关标签（ytest）❸。这些文件是之前代码生成的，因此我们知道
    xtest 的形状是 23 × 2，因为我们有 23 个测试样本，每个样本有两个特征。同样，ytest 是一个包含 23 个标签的向量。
- en: When we train this network, we’ll store the weights and biases as a list of
    NumPy arrays. The Python way to store a list on disk is via the pickle module,
    so we use pickle to load the list from disk ❹. The list weights has six elements
    representing the three weight matrices and three bias vectors that define the
    network. These are the “magic” numbers that our training has conditioned to the
    dataset. Finally, we call evaluate to run each of the test samples through the
    network. This function returns the score (accuracy) and the output probabilities
    for each sample (prob). The remainder of the code displays the sample number,
    actual label, predicted label, and associated output probability of being class
    1\. Finally, the score (accuracy) is shown.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练这个网络时，我们会将权重和偏置存储为一个 NumPy 数组列表。Python 中存储列表到磁盘的方式是使用 pickle 模块，因此我们使用
    pickle 从磁盘加载列表 ❹。这个列表 weights 有六个元素，代表定义网络的三个权重矩阵和三个偏置向量。这些就是我们训练过程中调节到数据集的“魔法”数字。最后，我们调用
    evaluate 函数将每个测试样本通过网络运行。这个函数返回每个样本的分数（准确度）和输出的类别 1 的概率（prob）。其余的代码会显示样本编号、实际标签、预测标签以及与类别
    1 相关的输出概率。最后，显示分数（准确度）。
- en: 'The network is implemented in evaluate; let’s see how. First, pull the individual
    weight matrices and bias vectors from the supplied weight list ❶. These are NumPy
    arrays: w12 is a 2 × 3 matrix mapping the two-element input to the first hidden
    layer with three nodes, w23 is a 3 × 2 matrix mapping the first hidden layer to
    the second hidden layer, and w34 is a 2 × 1 matrix mapping the second hidden layer
    to the output. The bias vectors are b1, three elements; b2, two elements; and
    b3, a single element (a scalar).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在 evaluate 中实现；我们来看看它是如何实现的。首先，从提供的权重列表 ❶ 中提取单独的权重矩阵和偏置向量。这些是 NumPy 数组：w12
    是一个 2 × 3 的矩阵，将两个输入元素映射到具有三个节点的第一隐藏层，w23 是一个 3 × 2 的矩阵，将第一隐藏层映射到第二隐藏层，w34 是一个
    2 × 1 的矩阵，将第二隐藏层映射到输出。偏置向量分别是 b1（三个元素）、b2（两个元素）和 b3（单个元素，标量）。
- en: Notice the weight matrices are not of the same shape as we previously indicated
    they would be. They are transposes. This is because we’re multiplying vectors,
    which are treated as 1 × 2 matrices, by the weight matrices. Because scalar multiplication
    is commutative, meaning *ab* = *ba*, we see that we’re still calculating the same
    argument value for the activation function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，权重矩阵的形状与我们之前表示的并不相同。它们是转置的。这是因为我们将向量（被视为1 × 2的矩阵）与权重矩阵相乘。由于标量乘法是可交换的，即*ab*
    = *ba*，我们可以看到我们仍然在计算相同的激活函数参数值。
- en: Next, evaluate sets the number correct (nc) and number wrong (nw) counters to
    0\. These are for calculating the overall score across the entire test set. Similarly,
    we define prob, a vector to hold the output probability value for each of the
    test samples.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，evaluate将正确计数器(nc)和错误计数器(nw)初始化为0。它们用于计算整个测试集的总体得分。类似地，我们定义了prob，一个向量，用来存储每个测试样本的输出概率值。
- en: The loop applies the entire network to each test sample. First, we map the input
    vectors to the first hidden layer and calculate *a*[1], a vector of three numbers,
    the activation for each of the three hidden nodes. We then take these first hidden
    layer activations and calculate the second hidden layer activations, *a*[2]. This
    is a two-element vector as there are two nodes in the second hidden layer. Next,
    we calculate the output value for the current input vector and store it in the
    prob array. The class label, z, is assigned by checking if the output value of
    the network is < 0.5 or not. Finally, we increment the correct (nc) or incorrect
    (nw) counters based on the actual label for this sample (y[i]) ❷. When all samples
    have been passed through the network, the overall accuracy is returned as the
    number of correctly classified samples divided by the total number of samples.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 循环将整个网络应用于每个测试样本。首先，我们将输入向量映射到第一个隐藏层，并计算*a*[1]，这是一个包含三个数值的向量，表示每个隐藏节点的激活值。然后，我们将这些第一个隐藏层的激活值用于计算第二个隐藏层的激活值*a*[2]。这是一个包含两个元素的向量，因为第二个隐藏层有两个节点。接下来，我们计算当前输入向量的输出值，并将其存储在prob数组中。通过检查网络输出值是否小于0.5来为类标签z赋值。最后，根据该样本的实际标签(y[i])
    ❷，我们递增正确计数器(nc)或错误计数器(nw)。当所有样本通过网络处理后，返回总体准确率，它是正确分类样本数与总样本数的比值。
- en: This is all well and good; we can implement a network and pass input vectors
    through it to see how well it does. If the network had a third hidden layer, we
    would pass the output of the second hidden layer (a2) through it before calculating
    the final output value.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，我们可以实现一个网络，并将输入向量通过它来观察它的表现。如果网络有第三个隐藏层，我们将在计算最终输出值之前，将第二个隐藏层的输出(a2)传递给它。
- en: Training and Testing the Neural Network
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练和测试神经网络
- en: The code in [Listing 8-2](ch08.xhtml#ch8lis2) applies the trained model to the
    test data. To train the model in the first place, we’ll use sklearn. The code
    to train the model is in [Listing 8-3](ch08.xhtml#ch8lis3).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-2](ch08.xhtml#ch8lis2)中的代码将训练好的模型应用于测试数据。为了首先训练模型，我们将使用sklearn。训练模型的代码在[清单
    8-3](ch08.xhtml#ch8lis3)中。'
- en: import numpy as np
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pickle
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: import pickle
- en: from sklearn.neural_network import MLPClassifier
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neural_network import MLPClassifier
- en: xtrain= np.load("iris2_train.npy")
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: xtrain = np.load("iris2_train.npy")
- en: ytrain= np.load("iris2_train_labels.npy")
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ytrain = np.load("iris2_train_labels.npy")
- en: xtest = np.load("iris2_test.npy")
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: xtest = np.load("iris2_test.npy")
- en: ytest = np.load("iris2_test_labels.npy")
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ytest = np.load("iris2_test_labels.npy")
- en: ❶ clf = MLPClassifier(
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ clf = MLPClassifier(
- en: ❷ hidden_layer_sizes=(3,2),
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ hidden_layer_sizes=(3,2),
- en: ❸ activation="logistic",
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ activation="logistic",
- en: solver="adam", tol=1e-9,
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: solver="adam", tol=1e-9,
- en: max_iter=5000,
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: max_iter=5000,
- en: verbose=True)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=True)
- en: clf.fit(xtrain, ytrain)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(xtrain, ytrain)
- en: prob = clf.predict_proba(xtest)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: prob = clf.predict_proba(xtest)
- en: score = clf.score(xtest, ytest)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: score = clf.score(xtest, ytest)
- en: ❹ w12 = clf.coefs_[0]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ w12 = clf.coefs_[0]
- en: w23 = clf.coefs_[1]
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: w23 = clf.coefs_[1]
- en: w34 = clf.coefs_[2]
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: w34 = clf.coefs_[2]
- en: b1 = clf.intercepts_[0]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: b1 = clf.intercepts_[0]
- en: b2 = clf.intercepts_[1]
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: b2 = clf.intercepts_[1]
- en: b3 = clf.intercepts_[2]
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: b3 = clf.intercepts_[2]
- en: weights = [w12,b1,w23,b2,w34,b3]
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: weights = [w12,b1,w23,b2,w34,b3]
- en: pickle.dump(weights, open("iris2_weights.pkl","wb"))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: pickle.dump(weights, open("iris2_weights.pkl","wb"))
- en: print()
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: print("Test results:")
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: print("测试结果：")
- en: 'print("  Overall score: %0.7f" % score)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("  总体得分: %0.7f" % score)'
- en: print()
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: 'for i in range(len(ytest)):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(len(ytest)):'
- en: p = 0 if (prob[i,1] < 0.5) else 1
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: p = 0 if (prob[i,1] < 0.5) else 1
- en: 'print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%03d: %d - %d, %0.7f" % (i, ytest[i], p, prob[i,1]))'
- en: print()
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: '*Listing 8-3: Using sklearn to train the iris neural network. See* nn_iris_mlpclassifier.py.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-3：使用 sklearn 训练鸢尾花神经网络。请参见* nn_iris_mlpclassifier.py。'
- en: First, we load the training and testing data from disk. These are the same files
    we created previously. Then we set up the neural network object, an instance of
    MLPClassifier ❶. The network has two hidden layers, the first with three nodes
    and the second with two nodes ❷. This matches the architecture in [Figure 8-1](ch08.xhtml#ch8fig1).
    The network is also using *logistic* layers ❸. This is another name for a sigmoid
    layer. We train the model by calling fit just as we did for other sklearn model
    types. Since we set verbose to True, we’ll get output showing us the loss for
    each iteration.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从磁盘加载训练和测试数据。这些数据是我们之前创建的文件。接着，我们设置神经网络对象，这是 MLPClassifier 的一个实例❶。网络有两个隐藏层，第一个有三个节点，第二个有两个节点❷。这与[图
    8-1](ch08.xhtml#ch8fig1)中的架构相匹配。该网络还使用了*logistic*层❸。这是另一个名称，用于表示 Sigmoid 层。我们通过调用
    fit 方法来训练模型，正如我们对其他 sklearn 模型类型所做的那样。由于我们将 verbose 设置为 True，因此我们将看到每次迭代的损失输出。
- en: Calling predict_proba gives us the output probabilities on the test data. This
    method is also supported by most other sklearn models. This is the model’s certainty
    as to the assigned output label. We then call score to calculate the score over
    the test set as we have done before.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 predict_proba 方法可以得到测试数据的输出概率。大多数其他 sklearn 模型也支持此方法。这表示模型对分配的输出标签的确定性。接着，我们调用
    score 方法计算测试集上的分数，正如我们之前所做的那样。
- en: We want to store the learned weights and biases so we can use them with our
    test code. We can pull them directly from the trained model ❹. These are packed
    into a list (weights) and dumped to a Python pickle file.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要存储学习到的权重和偏置，以便在测试代码中使用它们。我们可以直接从训练好的模型中提取这些数据❹。这些数据被打包成一个列表（weights），然后被保存为
    Python pickle 文件。
- en: The remaining code prints the results of running the sklearn trained model against
    the held-out test data. For example, a particular run of this code gives
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的代码打印出运行 sklearn 训练模型并与保留的测试数据进行比较的结果。例如，某次运行此代码输出：
- en: 'Test results:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 测试结果：
- en: 'Overall score: 1.0000000'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 总体得分：1.0000000
- en: '000: 0 - 0, 0.0705069'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '000: 0 - 0, 0.0705069'
- en: '001: 1 - 1, 0.8066224'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '001: 1 - 1, 0.8066224'
- en: '002: 0 - 0, 0.0308244'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '002: 0 - 0, 0.0308244'
- en: '003: 0 - 0, 0.0205917'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '003: 0 - 0, 0.0205917'
- en: '004: 1 - 1, 0.9502825'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '004: 1 - 1, 0.9502825'
- en: '005: 0 - 0, 0.0527558'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '005: 0 - 0, 0.0527558'
- en: '006: 1 - 1, 0.9455174'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '006: 1 - 1, 0.9455174'
- en: '007: 0 - 0, 0.0365360'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '007: 0 - 0, 0.0365360'
- en: '008: 1 - 1, 0.9471218'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '008: 1 - 1, 0.9471218'
- en: '009: 0 - 0, 0.0304762'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '009: 0 - 0, 0.0304762'
- en: '010: 0 - 0, 0.0304762'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '010: 0 - 0, 0.0304762'
- en: '011: 0 - 0, 0.0165365'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '011: 0 - 0, 0.0165365'
- en: '012: 1 - 1, 0.9453844'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '012: 1 - 1, 0.9453844'
- en: '013: 0 - 0, 0.0527558'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '013: 0 - 0, 0.0527558'
- en: '014: 1 - 1, 0.9495079'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '014: 1 - 1, 0.9495079'
- en: '015: 1 - 1, 0.9129983'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '015: 1 - 1, 0.9129983'
- en: '016: 1 - 1, 0.8931552'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '016: 1 - 1, 0.8931552'
- en: '017: 0 - 0, 0.1197567'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '017: 0 - 0, 0.1197567'
- en: '018: 0 - 0, 0.0406094'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '018: 0 - 0, 0.0406094'
- en: '019: 0 - 0, 0.0282220'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '019: 0 - 0, 0.0282220'
- en: '020: 1 - 1, 0.9526721'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '020: 1 - 1, 0.9526721'
- en: '021: 0 - 0, 0.1436263'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '021: 0 - 0, 0.1436263'
- en: '022: 1 - 1, 0.9446458'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '022: 1 - 1, 0.9446458'
- en: indicating that the model was perfect against the small test dataset. The output
    shows the sample number, the actual class label, the assigned class label, and
    the output probability of being class 1\. If we run the pickle file holding the
    sklearn network’s weights and biases through our evaluation code, we see that
    the output probabilities are precisely the same as the preceding code, indicating
    that our hand-generated neural network implementation is working correctly.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表明模型在小型测试数据集上的表现完美。输出显示了样本编号、实际类标签、分配的类标签以及属于类 1 的输出概率。如果我们通过评估代码运行保存了 sklearn
    网络权重和偏置的 pickle 文件，我们会发现输出概率与前面的代码完全相同，表明我们手动生成的神经网络实现是正确的。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the anatomy of a neural network. We described
    the architecture, the arrangement of nodes, and the connections between them.
    We discussed the output layer nodes and the functions they compute. We then saw
    that all the weights and biases could be conveniently represented by matrices
    and vectors. Finally, we presented a simple network for classifying a subset of
    the iris data and showed how it could be trained and evaluated.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了神经网络的结构。我们描述了其架构、节点的排列以及它们之间的连接。我们讨论了输出层节点及其计算的函数。然后，我们看到所有的权重和偏置可以方便地通过矩阵和向量来表示。最后，我们展示了一个简单的网络，用于分类鸢尾花数据的子集，并展示了如何训练和评估它。
- en: Now that we have our feet wet, let’s move on and dive into the theory behind
    neural networks.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经初步了解了神经网络，接下来让我们深入探讨其背后的理论。
