- en: '**16'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**16'
- en: GOING FURTHER**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步探索**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: You now have what I feel is a good introduction to modern machine learning.
    We have covered building datasets, classical models, model evaluation, and introductory
    deep learning, from traditional neural networks to convolutional neural networks.
    This short chapter is intended to help you go further.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经拥有我认为是现代机器学习的良好入门知识。我们已经涵盖了数据集构建、经典模型、模型评估以及入门级深度学习，从传统神经网络到卷积神经网络。本章内容旨在帮助你更进一步。
- en: We’ll look at both short-term “what’s next” sorts of things as well as longer-term
    forks in the road you may wish to explore. We’ll also include online resources
    where you will find the latest and greatest (always cognizant that anything online
    is ephemeral). After that comes a necessarily subjective list of conferences you
    may wish to attend. We’ll close the chapter and book with a thank you and a goodbye.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨短期的“下一步”内容，也会探讨你可能希望探索的长期发展方向。我们还会提供一些在线资源，你可以在其中找到最新和最好的内容（始终记住，网络上的任何内容都是短暂的）。之后会列出一些你可能希望参加的会议。我们将在本章和全书的结尾，向你表示感谢并告别。
- en: Going Further with CNNs
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步探索CNN
- en: Even after four chapters’ worth of material, we’ve barely scratched the surface
    of what convolutional neural networks can do. In part, we limited ourselves so
    you could grasp the fundamentals. And, in part, we were limited because we made
    a conscious decision not to require a GPU. Training complex models with a GPU
    is, in general, 20 to 25 times faster than using a CPU. With a GPU in your system,
    preferably designed for deep learning applications, the possibilities increase
    dramatically.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 即使已经讲了四章内容，我们对卷积神经网络的能力也只是触及了表面。部分原因是我们有意识地限制了内容，以便你能掌握基础知识。另外部分原因是我们有意识地做出选择，不要求使用GPU。使用GPU训练复杂模型通常比使用CPU快20到25倍。如果你的系统中有GPU，最好是为深度学习应用设计的，那么可能性会大大增加。
- en: The models we developed were small, reminiscent of the original LeNet models
    LeCun developed in the 1990s. They get the point across, but they will not go
    too far in many cases. Modern CNNs come in a variety of flavors and now “standard”
    architectures. With a GPU, you can explore these larger architectures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发的模型比较小，类似于LeCun在1990年代开发的原始LeNet模型。它们能够传达要点，但在许多情况下无法取得更远的进展。现代CNN有多种不同的架构，现如今有了“标准”架构。有了GPU，你可以探索这些更大的架构。
- en: 'These architectures should be on your list of what to look at next:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下架构应在你接下来需要关注的列表中：
- en: ResNet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: U-Net
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net
- en: VGG
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG
- en: DenseNet
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: Inception
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception
- en: AlexNet
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlexNet
- en: YOLO
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO
- en: Fortunately, the Keras toolkit we introduced (but also barely explored) supports
    all of these architectures. The two that seem especially useful to me are ResNet
    and U-Net. The latter is for semantic segmentation of inputs and has been widely
    successful, especially in medical imaging. To successfully train any of these
    architectures before your computer’s power supply or hard drive has failed, to
    say nothing of your heart, you do need a GPU. Medium to higher-end gaming GPUs
    (from NVIDIA, for example) will support new enough versions of CUDA that you can
    get going with a card for under 500 USD. The real trick is ensuring that your
    computer will support the card. The power requirements are high, typically requiring
    a power supply of 600W or more, and a slot that supports a double-wide PCIe card.
    Go for RAM over performance; the more RAM the GPU has, the larger a model it will
    support.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们介绍的Keras工具包（尽管我们只略微探讨了它）支持所有这些架构。对我来说，特别有用的是ResNet和U-Net。后者用于输入的语义分割，在医学影像领域尤其取得了广泛的成功。要想在计算机的电源供应或硬盘坏掉之前成功训练这些架构，更不用说你的心脏了，你确实需要一个GPU。中高端的游戏GPU（例如NVIDIA）将支持足够新的CUDA版本，你可以以不到500美元的价格购买一张显卡。真正的挑战是确保你的计算机支持这张显卡。电源需求较高，通常需要600W或更高的电源，并且需要一个支持双宽PCIe显卡的插槽。选择内存而非性能；显卡的内存越大，它支持的模型也就越大。
- en: 'Even if you don’t upgrade your system with a GPU, it’s worth your time to study
    the aforementioned architectures to see what makes them special and to understand
    how additional layers work. Check out the Keras documentation for more details:
    [keras.io](http://keras.io).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不升级系统，安装GPU，花时间研究上述架构也是值得的。这样你可以理解它们的特别之处，并掌握额外层的工作原理。查看Keras文档了解更多细节：[keras.io](http://keras.io)。
- en: Reinforcement Learning and Unsupervised Learning
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习和无监督学习
- en: 'This book has dealt exclusively with supervised learning. Of the three main
    branches of machine learning, supervised learning is probably the most widely
    used. Recalling the Marx brothers, supervised learning is like Groucho, the one
    everyone remembers. That isn’t an insult to the memory of Harpo and Chico, nor
    is it an insult to the other two branches of machine learning: reinforcement learning
    and unsupervised learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专门讨论了监督学习。在机器学习的三个主要分支中，监督学习可能是最广泛应用的。回想一下马尔克斯兄弟，监督学习就像是格劳乔，那个大家都记得的。这个说法并不是对哈波和奇科的记忆的侮辱，也不是对机器学习的另外两个分支的侮辱：强化学习和无监督学习。
- en: '*Reinforcement learning* is goal-oriented; it encourages models to learn how
    to behave and act to maximize a reward. Instead of learning how to take an input
    and map it to a specific output class, as in supervised learning, reinforcement
    learning learns how to act in the current situation to maximize an overall goal,
    like winning a game. Many of the impressive news stories related to machine learning
    have involved reinforcement learning. These include the first Atari 2600 game-playing
    systems capable of beating the best humans, as well as the fall of the world Go
    champion to AlphaGo, and the even more impressive achievement of AlphaGo Zero,
    which mastered Go from scratch without learning from millions of games played
    by humans. Any self-driving car system is likely extremely complex, but it’s a
    sure bet that reinforcement learning is a key part of that system.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是以目标为导向的；它鼓励模型学习如何行动，以最大化奖励。与监督学习中学习如何将输入映射到特定输出类别不同，强化学习学习如何在当前情境中采取行动以最大化整体目标，比如赢得一场比赛。许多与机器学习相关的令人印象深刻的新闻故事都涉及强化学习。这些故事包括第一款能够击败人类顶级玩家的Atari
    2600游戏系统，以及世界围棋冠军被AlphaGo击败，甚至更令人印象深刻的是AlphaGo Zero的成就，它从零开始掌握围棋，而没有借助人类玩过的数百万局游戏。任何自动驾驶汽车系统可能都极其复杂，但可以确定的是，强化学习是该系统的关键组成部分。'
- en: '*Unsupervised learning* refers to systems that learn on their own from unlabeled
    input data. Historically, this meant clustering, algorithms like *k*-means that
    take unlabeled feature vectors and attempt to group them by some similarity metric.
    Currently, one might argue that unsupervised learning is viewed as somewhat unimportant,
    given the insane amount of work being done with supervised learning and reinforcement
    learning. This is only half true; a lot of supervised learning is attempting to
    use unlabeled data (search for *domain adaptation*). How much of our own learning
    is unsupervised? An autonomous system set loose on an alien world will likely
    be more successful if it can learn things its creators didn’t know it would need
    to know. This suggests the importance of unsupervised learning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*是指从未标记的输入数据中自我学习的系统。从历史上看，这意味着聚类算法，如*k*-均值算法，它通过某些相似性度量将未标记的特征向量进行分组。目前，有人可能认为无监督学习在监督学习和强化学习的巨大工作量面前显得有些不重要。这仅仅是部分正确；许多监督学习的研究正试图利用未标记的数据（可以搜索*领域适应*）。我们自己的学习有多少是无监督的呢？一个在外星世界上放任自流的自主系统，如果能学习到它的创造者不知道需要学习的内容，可能会更加成功。这表明无监督学习的重要性。'
- en: Generative Adversarial Networks
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '*Generative adversarial networks (GANs)* burst on the scene in 2014, the brainchild
    of deep learning researcher Ian Goodfellow. GANs were quickly heralded as the
    most significant advance in machine learning in 20 years (Yann LeCun, spoken at
    NIPS 2016, Barcelona).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成对抗网络（GANs）*在2014年横空出世，由深度学习研究员伊恩·古德费洛（Ian Goodfellow）提出。GANs很快被誉为20年来机器学习领域最重大的进展（扬·勒昆（Yann
    LeCun），在2016年NIPS大会巴塞罗那演讲中提到）。'
- en: Recent news about models that can generate an infinite number of photo-quality
    human faces use GANs. So do models that create simulated scenes and convert images
    of one style (say, a painting) to another (like a photograph). GANs wed a network
    that generates outputs, often based on some random setting of its input, to a
    discriminative network that tries to learn how to tell the difference between
    real inputs and inputs that came from the generative part. The two networks are
    trained together so that the generative network gets better and better at fooling
    the discriminative network. In contrast, the discriminative network gets better
    and better at learning how to tell the difference. The result is a generative
    network that is pretty good at outputting what you want it to output.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于能够生成无限数量逼真人脸的模型使用了GAN技术。创造模拟场景以及将一种风格（比如绘画）转换为另一种风格（比如照片）的模型也使用了GAN。GAN将一个生成输出的网络（通常基于输入的某种随机设置）与一个判别网络相结合，后者尝试学习如何区分真实输入和来自生成部分的输入。两个网络一起训练，以使生成网络越来越擅长欺骗判别网络。而判别网络则越来越擅长学习如何区分两者。最终，生成网络会非常擅长输出你希望它输出的内容。
- en: A proper study of GANs would require a book, but they are well worth a look
    and some of your time, at least to develop an intuitive sense of what is going
    on. A good place to start is with the particularly popular GAN architecture, CycleGAN,
    which has, in turn, spawned a small army of similar models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成对抗网络（GAN）的正确学习需要一本书，但它们非常值得一看，至少花一些时间去发展对其原理的直观理解。一个好的起点是特别流行的GAN架构——CycleGAN，CycleGAN又衍生出了许多相似的模型。
- en: Recurrent Neural Networks
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: A major topic entirely ignored by this book is *recurrent neural networks (RNNs)*.
    These are networks with feedback loops, and they work well for processing sequences
    like a time series of measurements—think sound samples or video frames. The most
    common form is the LSTM, the long short-term memory network. Recurrent networks
    are widely used in neural translation models like Google Translate that have made
    it possible to do real-time translation between dozens of languages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书完全忽略的一个重要话题是*循环神经网络（RNNs）*。这些是具有反馈回路的网络，适用于处理时间序列数据等顺序数据——比如声音样本或视频帧。最常见的形式是LSTM，即长短期记忆网络。循环神经网络广泛应用于神经翻译模型，如Google翻译，这使得实时翻译多种语言成为可能。
- en: Online Resources
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线资源
- en: 'The online resources for machine learning are legion and growing daily. Here
    are a few places that I find helpful and that are likely to stand the test of
    time. In no particular order:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的在线资源众多，且日益增长。以下是我觉得有用且可能经得起时间考验的几个地方，按顺序不分先后：
- en: '**Reddit Machine Learning (*[www.reddit.com/r/MachineLearning/](http://www.reddit.com/r/MachineLearning/)*)**
    Look here for up-to-the-minute news and discussions of the latest papers and research.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Reddit机器学习 (*[www.reddit.com/r/MachineLearning/](http://www.reddit.com/r/MachineLearning/)*)**
    在这里查找最新的新闻和关于最新论文与研究的讨论。'
- en: '**Arxiv (*[https://arxiv.org/](https://arxiv.org/)*)** Machine learning progresses
    too quickly for most papers to go through the lengthy peer-review process print
    journals require. Instead, almost without exception, researchers and many conferences
    place all their papers on this preprint server, providing free access to the very
    latest in machine learning research. It can be daunting to sift through. Personally,
    I use the Arxiv app for my phone and several times a week peruse the following
    categories: Computer Vision and Pattern Recognition, Artificial Intelligence,
    Neural and Evolutionary Computing, and Machine Learning. The number of papers
    appearing in just these categories per week is impressive and a good indication
    of how active this field really is. To address the insane quantity of papers,
    deep learning researcher Andrej Karpathy created the useful Arxiv Sanity site
    at *[http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Arxiv (*[https://arxiv.org/](https://arxiv.org/)*)** 机器学习进展迅速，大多数论文无法通过传统期刊要求的漫长同行评审过程。因此，几乎所有研究人员和许多会议都会将他们的论文发布在这个预印本服务器上，提供对最新机器学习研究的免费访问。尽管这样做可能让人感到有些压倒性，但我个人使用Arxiv手机应用，每周几次浏览以下类别：计算机视觉与模式识别、人工智能、神经网络与进化计算、机器学习。仅这些类别每周发布的论文数量令人印象深刻，也很好地反映了这一领域的活跃程度。为了应对大量的论文，深度学习研究员Andrej
    Karpathy创建了有用的Arxiv Sanity网站，地址是 *[http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)*。'
- en: '**GitHub (*[https://github.com/](https://github.com/)*)** This is a place where
    people can host software projects. Go to the site directly and search for machine
    learning projects or use a standard search engine and add the keyword *github*
    to the search. With the explosion of machine learning projects, a beautiful thing
    has happened. The vast majority of the projects are freely available, even for
    commercial use. This typically includes full source code and datasets. If you
    read about something in a paper on Arxiv, you’ll likely find an implementation
    of it on Github.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**GitHub (*[https://github.com/](https://github.com/)*)** 这是一个人们可以托管软件项目的地方。你可以直接访问该网站，搜索机器学习项目，或者使用标准的搜索引擎并在搜索中添加关键词
    *github*。随着机器学习项目的爆炸性增长，一件美妙的事情发生了。绝大多数项目都可以免费使用，甚至可以用于商业用途。这通常包括完整的源代码和数据集。如果你在
    Arxiv 上阅读到某个研究，通常可以在 GitHub 上找到该研究的实现。'
- en: '**Coursera (*[https://www.coursera.org/](https://www.coursera.org/)*)** Coursera
    is a premier site for online courses, the vast majority of which can be audited
    for free. There are other sites, but Coursera was co-founded by Andrew Ng, and
    his machine learning course is very popular.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Coursera (*[https://www.coursera.org/](https://www.coursera.org/)*)** Coursera
    是一个优质的在线课程平台，其中绝大多数课程可以免费旁听。虽然还有其他平台，但 Coursera 是由 Andrew Ng 共同创办的，他的机器学习课程非常受欢迎。'
- en: '**YouTube (*[https://www.youtube.com/](https://www.youtube.com/)*)** YouTube
    is a force of nature at this point, but it is chock-full of machine learning videos.
    Let the viewer beware, but with some digging and judicious selection, you’ll find
    a lot here, including demonstrations of the latest and greatest. Search for “Neural
    Networks for Machine Learning” taught by Geoffrey Hinton.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**YouTube (*[https://www.youtube.com/](https://www.youtube.com/)*)** YouTube
    目前已经成为一种自然力量，但它充满了机器学习视频。观众需要保持警惕，不过通过一些挖掘和明智的选择，你会发现这里有很多资源，包括最新和最棒的演示。搜索 “Neural
    Networks for Machine Learning” 这门由 Geoffrey Hinton 教授的课程。'
- en: '**Kaggle (*[https://www.kaggle.com/](https://www.kaggle.com/)*)** Kaggle hosts
    machine learning competitions and is a good resource for datasets. Winners detail
    their models and training processes, providing ample opportunity to learn the
    art.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kaggle (*[https://www.kaggle.com/](https://www.kaggle.com/)*)** Kaggle 主办机器学习竞赛，是获取数据集的好资源。获胜者会详细描述他们的模型和训练过程，提供了大量学习这一艺术的机会。'
- en: Conferences
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 会议
- en: One of the best ways to learn a new language is to immerse yourself in a culture
    that speaks the language. The same is true for machine learning. The way to immerse
    yourself in the culture of machine learning is to attend conferences. This can
    be expensive, but many schools and companies view it as important, so you might
    be able to get support for attending.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 学习新语言的最佳方式之一是沉浸在讲该语言的文化中。机器学习也不例外。沉浸在机器学习文化中的方式是参加会议。虽然这可能会很昂贵，但许多学校和公司认为这很重要，所以你可能能够获得支持参加会议。
- en: 'The massive explosion of interest in machine learning has caused a new phenomenon,
    one that I haven’t seen happen in other academic disciplines: conferences selling
    out. This is true of the biggest conferences, but it might be happening to other
    conferences as well. If you want to attend, be aware that timing matters. Again,
    in no particular order, and missing many good but smaller conferences, consider
    the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习兴趣的爆炸性增长导致了一种新现象，我在其他学科中没有见过：会议售罄。这不仅适用于最大的会议，可能其他一些较小的会议也会发生这种情况。如果你想参加，务必注意时机很重要。同样，以下内容没有特定顺序，并且漏掉了许多其他不错但较小的会议，考虑以下几项：
- en: '**NeurIPS (formerly NIPS)** Short for *Neural Information Processing Systems*,
    this is likely the biggest machine learning conference. At this academic conference,
    you can expect to see the latest research presented. NeurIPS has sold out quickly
    in recent years, in under 12 minutes in 2018 (!), and has now switched to a lottery
    system, so unless you are a presenter of some kind, getting the golden ticket
    email allowing you to register is not assured. It’s usually held in Canada.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**NeurIPS (前身为 NIPS)** 是 *Neural Information Processing Systems* 的缩写，这可能是最大的机器学习会议。在这个学术会议上，你可以看到最新的研究成果。近年来，NeurIPS
    的门票销售速度非常快，2018 年甚至在 12 分钟内就售罄，现已改为抽签系统，因此除非你是某种演讲者，否则很难收到允许注册的黄金票邮件。该会议通常在加拿大举行。'
- en: '**ICML** Short for *International Conference on Machine Learning*, this is
    perhaps the second largest annual conference. This academic conference has several
    tracks and workshops and is typically held in Europe or North America.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**ICML** 是 *International Conference on Machine Learning* 的缩写，这可能是第二大年度会议。这个学术会议有多个主题和工作坊，通常在欧洲或北美举行。'
- en: '**ICLR** The International Conference on Learning Representations is a deep
    learning–focused academic conference. If you want in-the-weeds technical presentations
    on deep learning, this is the place to be.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**ICLR** 国际学习表征会议是一个专注于深度学习的学术会议。如果你想要深入技术的深度学习讲解，这里就是你该去的地方。'
- en: '**CVPR** Computer Vision and Pattern Recognition is another large conference
    that’s perhaps slightly less academic than ICLR. CVPR is popular and not exclusively
    machine learning–oriented.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**CVPR** 计算机视觉与模式识别会议是另一个大型会议，可能相比 ICLR 稍微少一些学术性。CVPR 非常受欢迎，并且不仅仅专注于机器学习。'
- en: '**GTC** The GPU Technology Conference, sponsored by NVIDIA, is a technical
    conference as opposed to an academic conference. The annual presentation of new
    NVIDIA hardware happens here, along with a large expo, in San Jose, California.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**GTC** GPU 技术大会由 NVIDIA 主办，是一个技术型会议，而非学术型会议。每年在此展示新的 NVIDIA 硬件，并伴有大型博览会，地点在美国加利福尼亚州圣荷西市。'
- en: The Book
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这本书
- en: 'Saying there are a few machine learning books out there is like saying there
    are a few fish in the sea. However, as far as deep learning is concerned, one
    stands head-and-shoulders above the rest: *Deep Learning* by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press, 2016). See *[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 说市面上有几本机器学习书籍，就像说大海里有几条鱼一样。然而，就深度学习而言，有一本书脱颖而出：由 Ian Goodfellow、Yoshua Bengio
    和 Aaron Courville 合著的《*Deep Learning*》（MIT Press，2016）。参见 *[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)*
- en: '*Deep Learning* is the book you should go to if you want to get serious about
    being a machine learning researcher. Even if you don’t, it covers the key topics
    in depth and with mathematical rigor. The book is not for those looking to get
    better at using one toolkit or another, but for those who want to see the theory
    behind machine learning and the math that goes with it. In essence, it’s an advanced
    undergraduate—if not graduate-level text, but that shouldn’t put you off. At some
    point, you will want to take a look at this book, so keep it in the back of your
    mind—or on your bookshelf.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想认真成为一名机器学习研究者，《*Deep Learning*》是你应该阅读的书。即使你不打算深入研究，它也深入涵盖了关键主题，并且具有数学严谨性。这本书并不适合那些只想提高使用某个工具包的技能的人，而是为那些希望了解机器学习背后理论以及相关数学的人准备的。从本质上来说，这本书是一本高级本科—如果不是研究生水平的教材，但这不应该让你却步。总有一天，你会想要翻阅这本书，所以把它放在心里—或者放在书架上。
- en: So Long and Thanks for All the Fish
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 再见，感谢所有的鱼
- en: We’ve reached the end of the book. There’s no monster here, only ourselves,
    and the knowledge and intuition we’ve gained by working through the preceding
    chapters. Thank you for persevering. It’s been fun for me to write; I genuinely
    hope it’s been fun for you to read and contemplate. Don’t stop now—take what we’ve
    developed and run with it. If you’re like me, you’ll see applications for machine
    learning everywhere. Go forth and classify!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经读完了这本书。这里没有怪物，只有我们自己，以及通过前面章节的学习所获得的知识和直觉。感谢你坚持读完。写这本书对我来说很有趣；我真心希望你在阅读和思考时也能感到有趣。别停下，现在正是时候—拿着我们所学的内容去实践吧。如果你像我一样，你会发现机器学习在各个领域都有应用。去吧，开始分类吧！
