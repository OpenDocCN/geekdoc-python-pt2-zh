- en: '[15](nsp-venkitachalam503045-0008.xhtml#rch15)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[15](nsp-venkitachalam503045-0008.xhtml#rch15)'
- en: Audio ML on Pi
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树莓派上的音频机器学习（Audio ML）
- en: '![](images/nsp-venkitachalam503045-circle-image.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-circle-image.jpg)'
- en: In the past decade, *machine learning (ML)* has taken the world by storm. It’s
    everywhere from facial recognition to predictive text to self-driving cars, and
    we keep hearing about novel applications of ML seemingly every day. In this chapter,
    you’ll use Python and TensorFlow to develop an ML-based speech recognition system
    that will run on an inexpensive Raspberry Pi computer.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，*机器学习（ML）* 已经风靡全球。从面部识别到预测文本，再到自动驾驶汽车，机器学习无处不在，我们几乎每天都能听到关于机器学习的新应用。在这一章节中，你将使用Python和TensorFlow开发一个基于机器学习的语音识别系统，该系统将在一台廉价的树莓派计算机上运行。
- en: Speech recognition systems are already deployed in a huge number of devices
    and appliances in the form of voice assistants such as Alexa, Google, and Siri.
    These systems can perform tasks ranging from setting reminders to switching on
    your home lights from your office. But all of these platforms require your device
    to be connected to the internet and for you to sign up for their services. This
    brings up issues of privacy, security, and power consumption. Does your light
    bulb *really* need to be connected to the internet to respond to a voice command?
    The answer is *no*. With this project, you’ll get a sense of how to design a speech
    recognition system that works on a low-power device, without the device needing
    to be connected to the internet.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别系统已经在大量设备和家电中部署，形式多为语音助手，如Alexa、Google和Siri。这些系统可以执行从设置提醒到在办公室远程打开家中灯光等任务。但是所有这些平台都需要你的设备连接到互联网，并且你需要注册它们的服务。这引发了隐私、安全性和功耗等问题。你的灯泡*真的*需要连接到互联网才能响应语音命令吗？答案是*不*。通过这个项目，你将学到如何设计一个在低功耗设备上运行的语音识别系统，而且该设备无需连接互联网。
- en: 'Some of the concepts you’ll learn about through this project are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个项目，你将学习到一些概念，包括：
- en: • Using a machine learning workflow to solve a problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 使用机器学习工作流解决问题
- en: • Creating an ML model with TensorFlow and Google Colab
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 使用TensorFlow和Google Colab创建机器学习模型
- en: • Streamlining an ML model for use on a Raspberry Pi
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 精简机器学习模型以便在树莓派上使用
- en: • Processing audio and generating spectrograms with the short-time Fourier transform
    (STFT)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 使用短时傅里叶变换（STFT）处理音频并生成频谱图
- en: • Leveraging multiprocessing to run tasks in parallel
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 利用多进程并行执行任务
- en: '[A Machine Learning Overview](nsp-venkitachalam503045-0008.xhtml#rah1701)'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[机器学习概述](nsp-venkitachalam503045-0008.xhtml#rah1701)'
- en: It’s impossible to do justice to a topic as vast as machine learning in a single
    section of a single book chapter. Instead, our approach will be to treat ML as
    just another tool for solving a problem—in this case, how to distinguish between
    different spoken words. In truth, ML frameworks like TensorFlow have become so
    mature and easy to use these days that it’s possible to effectively apply ML to
    a problem without being an expert in the subject. So in this section, we’ll only
    briefly touch upon the ML terminology relevant to the project.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一本书的单独一章中，谈论如此广泛的机器学习主题几乎是不可能做到的。因此，我们的做法是将机器学习视为解决问题的另一种工具——在这个案例中，是如何区分不同的口语单词。实际上，像TensorFlow这样的机器学习框架如今已经成熟且易于使用，以至于即使你不是该领域的专家，也能有效地将机器学习应用于实际问题。因此，在这一部分，我们将简要介绍与项目相关的机器学习术语。
- en: ML is a small part of the larger computer science discipline of *artificial
    intelligence (AI)*, although when AI is mentioned in the popular press, ML is
    usually what they mean. ML itself is made of various subdisciplines that involve
    different approaches and algorithms. In this project, you’ll use a subset of ML
    called *deep learning*, which harnesses *deep neural networks (**DNNs)* to identify
    features and patterns in large sets of data. DNNs have their origin in *artificial
    neural networks (ANNs)*, which are loosely based on neurons in our brains. ANNs
    consists of a bunch of *nodes* with multiple inputs. Each node also has a *weight*
    associated with it. The output of an ANN is typically a nonlinear function of
    the inputs and weights. This output can be connected to the input of another ANN.
    When you have more than one layer of ANNs, the network becomes a deep neural network.
    Typically, the more layers the network has—that is, the deeper it goes—the more
    accurate the learning model becomes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ML 是 *人工智能（AI）* 这一计算机科学学科中的一小部分，尽管当大众媒体提到 AI 时，通常指的就是 ML。ML 本身由多个子学科组成，这些子学科涉及不同的方法和算法。在本项目中，你将使用
    ML 的一个子集，称为 *深度学习*，它利用 *深度神经网络（**DNNs**）* 来识别大量数据中的特征和模式。DNN 的起源可以追溯到 *人工神经网络（ANNs）*，它们大致模仿了我们大脑中的神经元。ANNs
    由一组具有多个输入的 *节点* 组成。每个节点都有一个与之关联的 *权重*。ANN 的输出通常是输入和权重的非线性函数。这个输出可以连接到另一个 ANN 的输入。当网络有多层
    ANNs 时，它就变成了深度神经网络。通常，网络的层数越多——也就是说，网络越深——学习模型的准确性也越高。
- en: For this project, you’ll be using a *supervised learning* process, which can
    be divided into two phases. First is the *training phase*, where you show the
    model several inputs and their expected outputs. For example, if you were trying
    to build a human presence detection system to recognize whether or not there’s
    a person in a video frame, you would use the training phase to show examples of
    both cases (human versus no human), with each example labeled correctly. Next
    is the *inference phase*, where you show new inputs and the model makes predictions
    about them based on what it learned during training. Continuing the example, you’d
    show your human presence detection system new video frames, and the model would
    predict whether or not there’s a human in each frame. (There are also *unsupervised
    learning* processes, in which the ML system attempts to find patterns by itself,
    based on unlabeled data.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本项目，你将使用 *监督学习* 过程，通常可以分为两个阶段。第一个阶段是 *训练阶段*，你会向模型展示几个输入及其期望的输出。例如，如果你正在构建一个人类存在检测系统来识别视频帧中是否有人，你会在训练阶段展示这两种情况的示例（有人的与没有人的），并为每个示例正确标注标签。接下来是
    *推理阶段*，你将展示新的输入，模型基于在训练过程中学到的知识，对这些输入做出预测。继续前面的例子，你会向人类存在检测系统展示新的视频帧，模型将预测每一帧中是否有人。（还有
    *无监督学习* 过程，在这种过程中，ML 系统会根据未标注的数据自行寻找模式。）
- en: An ML model has many numerical *parameters* that help it process data. During
    training, these parameters are adjusted automatically to minimize errors between
    the expected values and the values the model predicts. Usually a class of algorithms
    called *gradient descent* is used to minimize the error. In addition to the parameters
    of an ML model, which are adjusted during training, there are also *hyperparameters*,
    variables that are adjusted for the model as a whole, such as which neural network
    architecture to use or the size of your training batch. [Figure 15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1)
    shows the neural network architecture I’ve chosen for this project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 ML 模型有许多数值 *参数*，这些参数帮助它处理数据。在训练过程中，这些参数会自动调整，以最小化期望值与模型预测值之间的误差。通常，使用一种称为
    *梯度下降* 的算法来最小化误差。除了在训练过程中调整的 ML 模型参数外，还有 *超参数*，这些是针对整个模型调整的变量，例如选择使用哪种神经网络架构或训练批次的大小。[图
    15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1) 展示了我为本项目选择的神经网络架构。
- en: '![](images/nsp-venkitachalam503045-f15001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15001.jpg)'
- en: 'Figure 15-1: The neural network architecture for the speech recognition project'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-1：语音识别项目的神经网络架构
- en: Each layer in the network architecture represents some form of processing on
    the data that helps improve the model’s accuracy. The design of the network isn’t
    trivial, but just defining each layer won’t tell us much about how it works. A
    broader question to consider is *why* I’ve chosen this particular network. The
    answer is that one needs to determine the best network architecture for the project
    at hand via experimentation. It’s common to try different neural network architectures
    and see which one produces the most accurate results after training. There are
    also architectures published by ML researchers that are known to perform well,
    and that’s a good place to start for practical applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构中的每一层都代表了对数据的某种处理方式，旨在帮助提高模型的准确性。网络的设计并非简单，但仅仅定义每一层并不能告诉我们它是如何工作的。一个更广泛的问题是
    *为什么* 我选择了这个特定的网络。答案是：需要通过实验来确定适合当前项目的最佳网络架构。尝试不同的神经网络架构并观察哪个架构在训练后能产生最准确的结果是很常见的做法。此外，还有许多由机器学习研究人员发布的架构被证明效果很好，这是实际应用中的一个良好起点。
- en: 'NOTE For more information on machine learning, I highly recommend the book
    *Deep Learning: A Visual Approach* by Andrew Glassner (No Starch Press, 2021).
    The book will gives you a good intuition about the subject without getting too
    much into the math or code. For a comprehensive, hands-on approach, I also recommend
    the online ML courses on Coursera taught by Andrew Ng.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '注：如果你想了解更多关于机器学习的信息，我强烈推荐安德鲁·格拉斯纳（Andrew Glassner）编写的《深度学习：视觉方法》（Deep Learning:
    A Visual Approach，No Starch Press，2021年）一书。这本书能帮助你对该领域建立直观的理解，而无需深入数学或代码部分。若你希望有一个全面的动手实践方法，我还推荐由吴恩达（Andrew
    Ng）教授主讲的 Coursera 在线机器学习课程。'
- en: '[How It Works](nsp-venkitachalam503045-0008.xhtml#rah1702)'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[工作原理](nsp-venkitachalam503045-0008.xhtml#rah1702)'
- en: In this project, you’ll use Google’s TensorFlow machine learning framework to
    train a neural network using a collection of audio files containing speech commands.
    Then you’ll load an optimized version of the trained model onto a Raspberry Pi
    equipped with a microphone so the Pi can recognize the commands when you speak
    them. [Figure 15-2](nsp-venkitachalam503045-0030.xhtml#fig15-2) shows a block
    diagram for the project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用谷歌的 TensorFlow 机器学习框架，利用包含语音命令的音频文件集合来训练神经网络。然后，你将把训练好的模型的优化版本加载到配备麦克风的
    Raspberry Pi 上，这样当你说出命令时，Pi 就能够识别它们。[图 15-2](nsp-venkitachalam503045-0030.xhtml#fig15-2)
    展示了该项目的框图。
- en: '![](images/nsp-venkitachalam503045-f15002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15002.jpg)'
- en: 'Figure 15-2: A block diagram of the speech recognition project'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-2：语音识别项目的框图
- en: For the training portion of the project, you’ll work in Google Colab (short
    for Colaboratory), a free cloud-based service that lets you write and run Python
    programs in your web browser. There are two advantages to using Colab. First,
    you don’t need to install TensorFlow locally on your computer, nor deal with incompatibility
    issues related to various versions of TensorFlow. Second, Colab runs on machines
    that are likely much more powerful than yours, so the training process will go
    more quickly. For training data, you’ll use the Mini Speech Commands dataset from
    Google, a subset of a larger Speech Commands dataset published in 2018\. It consists
    of thousands of sample recordings of the words *yes*, *no*, *up*, *down*, *left*,
    *right*, *stop*, and *go*, all standardized as 16-bit WAV files with a 16,000
    Hz sampling rate. You’ll generate a *spectrogram* of each recording, an image
    that shows how the frequency content of the audio changes over time, and use those
    spectrograms to train a deep neural network (DNN) via TensorFlow.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于项目的训练部分，你将使用 Google Colab（即协作平台）进行工作，它是一个免费的基于云的服务，让你可以在网页浏览器中编写和运行 Python
    程序。使用 Colab 有两个优势。首先，你无需在本地计算机上安装 TensorFlow，也无需处理与不同版本 TensorFlow 相关的不兼容问题。其次，Colab
    运行在比你计算机更强大的机器上，因此训练过程会更快。对于训练数据，你将使用来自 Google 的 Mini Speech Commands 数据集，这是一个较大的
    Speech Commands 数据集的子集，后者于 2018 年发布。该数据集包含了成千上万的词汇录音样本，词汇包括 *yes*（是），*no*（不），*up*（上），*down*（下），*left*（左），*right*（右），*stop*（停止）和
    *go*（开始），所有录音都标准化为 16 位 WAV 文件，采样率为 16,000 Hz。你将生成每个录音的 *频谱图*，这是一种图像，展示了音频的频率内容随时间的变化，并使用这些频谱图通过
    TensorFlow 训练深度神经网络（DNN）。
- en: NOTE The training portion of this project takes inspiration from Google’s official
    TensorFlow example called “Simple Audio Recognition.” You’ll use the same neural
    network architecture as that example. However, the rest of the project deviates
    significantly from Google’s example, since our goal is to recognize live audio
    on a Raspberry Pi, whereas the latter runs inference on existing WAV files.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本项目的训练部分灵感来自谷歌官方的 TensorFlow 示例“简单音频识别”。你将使用与该示例相同的神经网络架构。然而，项目的其余部分与谷歌的示例有很大不同，因为我们的目标是在
    Raspberry Pi 上识别实时音频，而后者则是对现有的 WAV 文件进行推理。
- en: Once the training is complete, you’ll convert the trained model to a simplified
    format called TensorFlow Lite, which is designed to run on less capable hardware
    such as embedded systems, and load that streamlined model onto the Raspberry Pi.
    There you’ll run Python code to continuously monitor the audio input from a USB
    microphone, take spectrograms of the audio, and perform inference on that data
    to identify the spoken commands from the training set. You’ll print out the commands
    that the model identifies to the serial monitor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，你将把训练好的模型转换为一种简化格式，称为 TensorFlow Lite，它专为在嵌入式系统等性能较低的硬件上运行而设计，并将该精简版模型加载到
    Raspberry Pi 上。在那里，你将运行 Python 代码，持续监控来自 USB 麦克风的音频输入，获取音频的频谱图，并对这些数据进行推理，以识别训练集中的语音命令。你将把模型识别出的命令打印到串口监视器上。
- en: '[Spectrograms](nsp-venkitachalam503045-0008.xhtml#rbh1701)'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[频谱图](nsp-venkitachalam503045-0008.xhtml#rbh1701)'
- en: A key step in this project is generating spectrograms of the audio data—both
    the preexisting data used to train the model and the real-time data encountered
    during inference. In [Chapter 4](nsp-venkitachalam503045-0016.xhtml#ch04), you
    saw how a spectral plot reveals the frequencies present in an audio sample at
    a particular moment in time. Then, in [Chapter 13](nsp-venkitachalam503045-0028.xhtml#ch13),
    you learned how spectral plots are calculated with a mathematical tool called
    a *discrete Fourier transform (DFT)*. A spectrogram is essentially just a series
    of spectral plots, generated through a sequence of Fourier transformers, which
    together reveal how the frequency content of some audio data evolves over time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的关键步骤之一是生成音频数据的频谱图——包括用于训练模型的预先存在的数据和在推理过程中遇到的实时数据。在[第 4 章](nsp-venkitachalam503045-0016.xhtml#ch04)中，你已经了解了频谱图如何揭示音频样本在特定时刻的频率。然后，在[第
    13 章](nsp-venkitachalam503045-0028.xhtml#ch13)中，你学习了如何使用一种数学工具——*离散傅里叶变换（DFT）*——来计算频谱图。频谱图本质上就是一系列频谱图，它们通过一系列傅里叶变换器生成，展示了某些音频数据的频率内容如何随时间变化。
- en: You need a spectrogram, rather than a single spectral plot, of each audio sample
    because the sound of human speech is incredibly complex. Even in the case of a
    single word, the frequencies present in the sound change significantly—and in
    distinctive ways—as the word is spoken. For this project, you’ll be working with
    one-second-long audio clips, each consisting of 16,000 samples. If you computed
    a single DFT of the entire clip in one go, you wouldn’t get an accurate picture
    of how the frequencies change over the course of the clip, and thus you wouldn’t
    be able to reliably identify the word being spoken. Instead, you’ll divide the
    clip into a bunch of overlapping intervals and compute the DFT for each of these
    intervals, giving you the series of spectral plots needed for a spectrogram. [Figure
    15-3](nsp-venkitachalam503045-0030.xhtml#fig15-3) illustrates this type of computation,
    called a *short-time Fourier transform (**STFT)*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要每个音频样本的频谱图，而不是单一的频谱图，因为人类语音的声音非常复杂。即使是一个单词，随着这个单词的发音，音频中的频率变化也会非常显著，并且有其独特的变化方式。在这个项目中，你将处理每个一秒钟长的音频片段，每个片段包含
    16,000 个样本。如果你对整个片段一次性计算一个 DFT，你将无法准确看到频率如何在片段中变化，因此无法可靠地识别出所说的单词。相反，你将把片段分成多个重叠的间隔，并计算每个间隔的
    DFT，这样就能获得所需的频谱图序列。[图 15-3](nsp-venkitachalam503045-0030.xhtml#fig15-3)展示了这种计算方式，称为*短时傅里叶变换（**STFT）*。
- en: '![](images/nsp-venkitachalam503045-f15003.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15003.jpg)'
- en: 'Figure 15-3: Computing the spectrogram of a signal'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-3：计算信号的频谱图
- en: The STFT gives you *M* DFTs of the audio, taken at even time intervals. Time
    is shown along the x-axis of the spectrogram. Each DFT gives you *N* frequency
    bins and the intensity of the sound within each of those bins. The frequency bins
    are mapped to the y-axis of the spectrogram. Thus, the spectrogram takes the form
    of an *M*×*N* image. Each column of pixels in the image represents one of the
    DFTs, with color used to convey the intensity of the signal in a given frequency
    band.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: STFT 给你提供了音频的 *M* 个 DFT（离散傅里叶变换），它们是在均匀的时间间隔内采集的。时间在频谱图的 x 轴上显示。每个 DFT 给你 *N*
    个频率 bin 和每个 bin 中声音的强度。这些频率 bin 映射到频谱图的 y 轴上。因此，频谱图呈现为一个 *M*×*N* 的图像。图像中的每一列像素表示一个
    DFT，其中颜色用于表示在给定频率带中的信号强度。
- en: You might be wondering why we need to use Fourier transforms at all for this
    project. Why not use the waveforms of the audio clips directly, instead of extracting
    the frequency information from those waveforms? For an answer, consider [Figure
    15-4](nsp-venkitachalam503045-0030.xhtml#fig15-4).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，为什么我们在这个项目中需要使用傅里叶变换？为什么不直接使用音频片段的波形，而是从这些波形中提取频率信息呢？为了回答这个问题，看看[图 15-4](nsp-venkitachalam503045-0030.xhtml#fig15-4)。
- en: '![](images/nsp-venkitachalam503045-f15004_annotated.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15004_annotated.jpg)'
- en: 'Figure 15-4: The waveform and spectrogram of speech samples'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-4：语音样本的波形和频谱图
- en: The top half of the figure shows the waveform of a recording made by speaking
    the sequence “Left, right, left, right.” The bottom half of the figure shows a
    spectrogram of that recording. Looking just at the waveform, you can see that
    the two *left*s look vaguely similar, as do the two *right*s, but it’s hard to
    pick out strong identifying characteristics of each word’s waveform. By contrast,
    the spectrogram reveals more visual features associated with each word, like the
    bright C-shaped curve (shown by the arrows) in each instance of *right*. We can
    see these distinctive features more clearly with our own eyes, and a neural network
    can “see” them as well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图的上半部分显示的是通过说出“左，右，左，右”这一顺序录制的波形。图的下半部分显示的是该录音的频谱图。仅从波形来看，你可以看到两个*左*的波形有些相似，两个*右*的波形也类似，但很难从每个单词的波形中找出明显的识别特征。相比之下，频谱图揭示了每个单词的更多视觉特征，比如在每个*右*的实例中都能看到明亮的
    C 形曲线（由箭头标出）。我们可以用肉眼更清楚地看到这些独特的特征，神经网络也能“看到”它们。
- en: In the end, since a spectrogram is essentially an image, taking spectrograms
    of the data turns a speech recognition problem into an image classification problem,
    allowing us to leverage the rich set of ML techniques that already exist for classifying
    images. (Of course, a waveform can be treated as an image too, but as you’ve seen,
    a spectrogram is better at capturing the “signature” of the audio data and hence
    more suited for ML applications.)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，由于频谱图本质上是一张图像，将数据转化为频谱图将语音识别问题转化为图像分类问题，从而使我们能够利用现有的丰富机器学习技术来进行图像分类。（当然，波形也可以作为图像处理，但正如你所看到的，频谱图更擅长捕捉音频数据的“特征”，因此更适合机器学习应用。）
- en: '[Inference on the Raspberry Pi](nsp-venkitachalam503045-0008.xhtml#rbh1702)'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[树莓派上的推断](nsp-venkitachalam503045-0008.xhtml#rbh1702)'
- en: 'The code on the Raspberry Pi must accomplish several tasks: it needs to read
    the audio input from the attached microphone, compute the spectrogram of that
    audio, and do inference using the trained TensorFlow Lite model. Here’s one possible
    sequence of operations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 树莓派上的代码必须完成几个任务：它需要从连接的麦克风读取音频输入，计算该音频的频谱图，并使用训练好的 TensorFlow Lite 模型进行推断。以下是一个可能的操作顺序：
- en: 1\. Read microphone data for one second.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 读取麦克风数据一秒钟。
- en: 2\. Process data.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 处理数据。
- en: 3\. Do inference.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 执行推断。
- en: 4\. Repeat.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4\. 重复。
- en: 'There’s a big problem with this approach, however. While you’re busy with steps
    2 and 3, more speech data could be coming in, which you’ll end up missing. The
    solution is to use Python multiprocessing to perform different tasks concurrently.
    Your main process will just collect the audio data and put it in a queue. In a
    separate, simultaneous process, you’ll take this data out of the queue and run
    inference on it. Here’s what the new scheme looks like:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在一个大问题。当你忙于执行步骤 2 和 3 时，可能会有更多的语音数据进入，这样你就会错过它们。解决方案是使用 Python 多进程并发执行不同的任务。你的主进程将只收集音频数据并将其放入队列中。在一个单独的、同时进行的进程中，你将从队列中取出这些数据并对其进行推断处理。下面是新的方案：
- en: Main Process
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程
- en: 1\. Read microphone data for one second.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 读取麦克风数据一秒钟。
- en: 2\. Put data into the queue.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 将数据放入队列中。
- en: Inference Process
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 推断过程
- en: 1\. Check if there’s any data in the queue.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 检查队列中是否有数据。
- en: 2\. Run inference on the data.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 对数据进行推理处理。
- en: 'Now the main process won’t miss any audio input, since putting data into the
    queue is a very quick operation. But there’s another problem. You’re collecting
    one-second audio samples continuously from the microphone and processing them,
    but you can’t assume that all spoken commands will fit cleanly into those one-second
    intervals. A command could come at an edge and be broken up across two consecutive
    samples, in which case it probably won’t be identified during inference. A better
    approach is to create overlapping samples, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，主进程不会错过任何音频输入，因为将数据放入队列是一个非常快速的操作。但还有另一个问题。你正在从麦克风连续收集一秒钟的音频样本并进行处理，但你不能假设所有的语音命令都会完美地适应这些一秒钟的间隔。命令可能会出现在边缘，并被拆分到两个连续的样本中，在这种情况下，可能无法在推理时识别该命令。一个更好的方法是创建重叠样本，如下所示：
- en: Main Process
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程
- en: 1\. For the very first frame, collect a two-second sample.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 对于第一个帧，收集一个两秒钟的样本。
- en: 2\. Put the two-second sample into the queue.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 将两秒钟的样本放入队列。
- en: 3\. Collect another one-second sample.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 收集另一个一秒钟的样本。
- en: 4\. Create a two-second sample by moving the latter half of the sample from
    step 2 to the front and replacing the second half with the sample from step 3.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4\. 通过将第2步样本的后半部分移到前面，并用第3步的样本替换后半部分，创建一个两秒钟的样本。
- en: 5\. Return to step 2.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5\. 返回第2步。
- en: Inference Process
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程
- en: 1\. Check if there’s any data in the queue.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 检查队列中是否有数据。
- en: 2\. Do inference on a one-second portion of the two-second data based on peak
    amplitude.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 基于峰值幅度，对两秒钟数据的其中一秒部分进行推理。
- en: 3\. Return to step 1.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 返回第1步。
- en: In this new scheme, each sample placed into the queue is two seconds long, but
    there’s a one-second overlap between consecutive samples, as illustrated in [Figure
    15-5](nsp-venkitachalam503045-0030.xhtml#fig15-5). This way, even if a word is
    partially cut off in one sample, you’ll get the full word in the next sample.
    You’ll still run inference on only one-second clips, which you’ll center on the
    point in the two-second sample that has the highest amplitude value. This is the
    part of the sample most likely to contain a spoken word. You need the clips to
    be one second long for consistency with the training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新方案中，放入队列中的每个样本长度为两秒钟，但相邻样本之间有一秒钟的重叠，如[图15-5](nsp-venkitachalam503045-0030.xhtml#fig15-5)所示。这样，即使一个单词在一个样本中部分被截断，你也能在下一个样本中得到完整的单词。你仍然只对一秒钟的片段进行推理，且会聚焦于两秒样本中幅度值最高的部分。这是最可能包含已说单词的部分。你需要这些片段为一秒钟，以确保与训练数据的一致性。
- en: '![](images/nsp-venkitachalam503045-f15005.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15005.jpg)'
- en: 'Figure 15-5: The two-frame overlapping scheme'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图15-5：两帧重叠方案
- en: Through this combination of multiprocessing and overlapping samples, you’ll
    design a speech recognition system that minimizes missing inputs and improves
    inference results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种多进程和重叠样本的组合，你将设计一个语音识别系统，最大程度减少漏掉的输入，并提高推理结果。
- en: '[Requirements](nsp-venkitachalam503045-0008.xhtml#rah1703)'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[要求](nsp-venkitachalam503045-0008.xhtml#rah1703)'
- en: 'For this project, you’ll need to sign up with Google Colab to train your ML
    model. On the Raspberry Pi, you’ll need the following Python modules:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，你需要注册Google Colab来训练你的机器学习模型。在树莓派上，你将需要以下Python模块：
- en: • `tflite_runtime` for running the TensorFlow inference
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `tflite_runtime` 用于运行TensorFlow推理
- en: • `scipy` for computing the STFT of audio waveforms
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `scipy` 用于计算音频波形的STFT
- en: • `numpy` arrays for handling audio data
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `numpy` 数组用于处理音频数据
- en: • `pyaudio` for streaming audio data from the microphone input
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • `pyaudio` 用于从麦克风输入流式传输音频数据
- en: The installation for these modules is covered in [Appendix B](nsp-venkitachalam503045-0032.xhtml#appb).
    You’ll also use Python’s built-in `multiprocessing` module for running ML inference
    in a separate thread from the audio processing thread.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模块的安装方法可以参考[附录B](nsp-venkitachalam503045-0032.xhtml#appb)。你还将使用Python内置的`multiprocessing`模块，在与音频处理线程分开的线程中运行机器学习推理。
- en: 'In the hardware department, you’ll need the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件部分，你将需要以下设备：
- en: • One Raspberry Pi 3B+ or newer
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一台树莓派3B+或更新型号
- en: • One 5 V power supply for the Raspberry Pi
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一个5V电源用于树莓派
- en: • One 16GB SD card
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一张16GB的SD卡
- en: • One single-channel USB microphone compatible with the Pi
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: • 一个兼容树莓派的单声道USB麦克风
- en: Various types of USB microphones are compatible with the Raspberry Pi. [Figure
    15-6](nsp-venkitachalam503045-0030.xhtml#fig15-6) shows an example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 各种类型的 USB 麦克风与 Raspberry Pi 兼容。[图 15-6](nsp-venkitachalam503045-0030.xhtml#fig15-6)展示了一个示例。
- en: '![](images/nsp-venkitachalam503045-f15006.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15006.jpg)'
- en: 'Figure 15-6: A USB microphone for the Raspberry Pi'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-6：用于 Raspberry Pi 的 USB 麦克风
- en: 'To check if your Pi can recognize your USB microphone, SSH into your Pi and
    run the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查您的 Pi 是否能够识别您的 USB 麦克风，请通过 SSH 连接到您的 Pi，并运行以下命令：
- en: $ `dmesg -w`
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: $ `dmesg -w`
- en: 'Now plug your microphone into a USB port on the Pi. You should see something
    similar to the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将麦克风插入 Pi 的 USB 端口。您应该能看到类似于以下输出的内容：
- en: '[26965.023138] usb 1-1.3: New USB device found, idVendor=cafe, idProduct=4010,
    bcdDevice= 1.00'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023138] usb 1-1.3: New USB device found, idVendor=cafe, idProduct=4010,
    bcdDevice= 1.00'
- en: '[26965.023163] usb 1-1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023163] usb 1-1.3: New USB device strings: Mfr=1, Product=2, SerialNumber=3'
- en: '[26965.023179] usb 1-1.3: Product: Mico'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023179] usb 1-1.3: Product: Mico'
- en: '[26965.023194] usb 1-1.3: Manufacturer: Electronut Labs'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023194] usb 1-1.3: Manufacturer: Electronut Labs'
- en: '[26965.023209] usb 1-1.3: SerialNumber: 123456'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[26965.023209] usb 1-1.3: SerialNumber: 123456'
- en: The information should match the specs of your microphone, indicating it’s been
    correctly identified.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 信息应该与您的麦克风规格匹配，这表明它已经被正确识别。
- en: '[The Code](nsp-venkitachalam503045-0008.xhtml#rah1704)'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[代码](nsp-venkitachalam503045-0008.xhtml#rah1704)'
- en: 'The code for this project exists in two parts: the training portion, which
    you’ll run in Google Colab, and the inference portion, which you’ll run on your
    Raspberry Pi. We’ll examine these parts one at a time.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的代码分为两部分：训练部分，您将在 Google Colab 中运行，以及推理部分，您将在 Raspberry Pi 上运行。我们将逐一查看这些部分。
- en: '[Training the Model in Google Colab](nsp-venkitachalam503045-0008.xhtml#rbh1703)'
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[在 Google Colab 中训练模型](nsp-venkitachalam503045-0008.xhtml#rbh1703)'
- en: In this section, we’ll look at the Google Colab code needed to train the speech
    recognition model. I recommend working with Colab in the Chrome web browser. You’ll
    begin by getting set up and downloading the training dataset. Then you’ll run
    some code to get to know the data. You’ll clean up the data to prepare it for
    training and explore how to generate spectrograms from the data. Finally, you’ll
    put what you’ve learned to work by creating and training the model. The end result
    will be a *.tflite* file, a streamlined TensorFlow Lite version of the trained
    model that you can load onto your Raspberry Pi. You can also download this file
    from the book’s GitHub repository at [https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看在 Google Colab 中训练语音识别模型所需的代码。我建议您在 Chrome 浏览器中使用 Colab。您将首先进行设置并下载训练数据集。然后，您将运行一些代码以了解数据。接下来，您将清理数据并为训练做准备，探索如何从数据生成频谱图。最后，您将把学到的知识应用到创建和训练模型中。最终结果将是一个
    *.tflite* 文件，这是经过训练的模型的简化版 TensorFlow Lite 文件，您可以将其加载到 Raspberry Pi 上。您还可以从本书的
    GitHub 仓库下载此文件：[https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml)。
- en: A Google Colab notebook consists of a series of cells where you enter one or
    more lines of Python code. Once you’ve entered your desired code into a cell,
    you run it by clicking the Play icon in the top-left corner of the cell. Any output
    associated with that cell’s code will then appear beneath the cell. Throughout
    this section, each code listing will represent a complete Google Colab cell. The
    cell’s output, if any, will be shown in gray at the end of the listing, beneath
    a dashed line.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 笔记本由一系列单元格组成，您可以在其中输入一行或多行 Python 代码。输入所需的代码后，通过点击单元格左上角的播放图标来运行它。与该单元格代码相关的任何输出将显示在单元格下方。在本节中，每个代码清单将代表一个完整的
    Google Colab 单元格。该单元格的输出（如果有）将以灰色显示在清单的底部，位于虚线下方。
- en: Setting Up
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'You begin your Colab notebook with some initial setup. First you import the
    required Python modules:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您从 Colab 笔记本开始时需要进行一些初始设置。首先，您需要导入所需的 Python 模块：
- en: import os
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: import os
- en: import pathlib
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: import pathlib
- en: import matplotlib.pyplot as plt
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import numpy as np
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import scipy
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: import scipy
- en: import scipy.signal
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: import scipy.signal
- en: from scipy.io import wavfile
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.io import wavfile
- en: import glob
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: import glob
- en: import tensorflow as tf
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: import tensorflow as tf
- en: from tensorflow.keras.layers.experimental import preprocessing
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: from tensorflow.keras.layers.experimental import preprocessing
- en: from tensorflow.keras import layers
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: from tensorflow.keras import layers
- en: from tensorflow.keras import models
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: from tensorflow.keras import models
- en: from tensorflow.keras import applications
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: from tensorflow.keras import applications
- en: 'In the next cell, you do some initialization:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个单元格中，你进行了一些初始化：
- en: set seed for random functions
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为随机函数设置种子
- en: seed = 42
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: seed = 42
- en: tf.random.set_seed(seed)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: tf.random.set_seed(seed)
- en: np.random.seed(seed)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: np.random.seed(seed)
- en: Here you initialize the random functions you’ll be using to shuffle the input
    filenames.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你初始化了将用来打乱输入文件名的随机函数。
- en: 'Next, download the training data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下载训练数据：
- en: data_dir = 'data/mini_speech_commands'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: data_dir = 'data/mini_speech_commands'
- en: data_path = pathlib.Path(data_dir)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: data_path = pathlib.Path(data_dir)
- en: filename = 'mini_speech_commands.zip'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: filename = 'mini_speech_commands.zip'
- en: url = "http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip"
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: url = "http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip"
- en: 'if not data_path.exists():'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not data_path.exists():'
- en: tf.keras.utils.get_file(filename, origin=url, extract=True, cache_dir='.',
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: tf.keras.utils.get_file(filename, origin=url, extract=True, cache_dir='.',
- en: cache_subdir='data')
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: cache_subdir='data')
- en: This cell downloads the Mini Speech Commands dataset from Google and extracts
    the data into a directory called *data*. Since you’re using Colab, the data will
    be downloaded to the filesystem on the cloud, not to your local machine, and when
    your session ends, these files will be deleted. While the session is still active,
    however, you don’t want to have to keep downloading the data every time you run
    the cell. The `tf.keras.utils.``get_file()` function caches the data so you won’t
    need to keep downloading it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单元格从Google下载Mini Speech Commands数据集，并将数据解压到一个名为*data*的目录中。由于你在使用Colab，数据将被下载到云端的文件系统，而不是本地计算机，当你的会话结束时，这些文件会被删除。然而，在会话仍然活跃时，你不希望每次运行单元格时都重新下载数据。`tf.keras.utils.get_file()`函数会缓存数据，这样你就不需要每次都重新下载。
- en: Getting to Know the Data
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 了解数据
- en: 'Before you start training your model, it would be useful to take a look at
    what you just downloaded to get to know your data. You can use Python’s `glob`
    module, which helps you find files and directories through pattern matching:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，查看一下你刚下载的内容，以便更好地了解数据将会很有帮助。你可以使用Python的`glob`模块，它帮助你通过模式匹配查找文件和目录：
- en: glob.glob(data_dir + '/*')
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: glob.glob(data_dir + '/*')
- en: '[''data/mini_speech_commands/up'','
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[''data/mini_speech_commands/up'','
- en: '''data/mini_speech_commands/no'','
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/no'','
- en: '''data/mini_speech_commands/README.md'','
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/README.md'','
- en: '''data/mini_speech_commands/stop'','
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/stop'','
- en: '''data/mini_speech_commands/left'','
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/left'','
- en: '''data/mini_speech_commands/right'','
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/right'','
- en: '''data/mini_speech_commands/go'','
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/go'','
- en: '''data/mini_speech_commands/down'','
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/down'','
- en: '''data/mini_speech_commands/yes'']'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '''data/mini_speech_commands/yes'']'
- en: 'You pass `glob` the `''/*''` pattern to list all the first-level directories
    within the *data* directory (`*` is a wildcard character). The output shows you
    that the dataset comes with a *README.md* text file and eight subdirectories for
    the eight speech commands you’ll be training the model to identify. For convenience,
    you create a list of the commands:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你将`glob`传入‘/*’模式，以列出*data*目录下所有一级子目录（`*`是一个通配符字符）。输出结果显示，数据集包含一个*README.md*文本文件，以及八个子目录，分别对应你将训练模型识别的八个语音命令。为了方便，你创建了一个命令列表：
- en: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
- en: In your machine learning model, you’ll be matching audio samples to a `label_id`
    integer denoting one of the commands. These integers will correspond to the indices
    from the `commands` list. For example, a `label_id` of `0` indicates `'up'`, and
    a `label_id` of `6` indicates `'down'`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的机器学习模型中，你将音频样本与一个`label_id`整数进行匹配，`label_id`表示命令之一。这个整数将与`commands`列表中的索引对应。例如，`label_id`为`0`表示‘up’，而`label_id`为`6`表示‘down’。
- en: 'Now take a look at what’s in those subdirectories:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看一下这些子目录中的内容：
- en: ❶ wav_file_names = glob.glob(data_dir + '/*/*')
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ wav_file_names = glob.glob(data_dir + '/*/*')
- en: ❷ np.random.shuffle(wav_file_names)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ np.random.shuffle(wav_file_names)
- en: print(len(wav_file_names))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: print(len(wav_file_names))
- en: 'for file_name in wav_file_names[:5]:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'for file_name in wav_file_names[:5]:'
- en: print(file_name)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: print(file_name)
- en: '8000'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '8000'
- en: data/mini_speech_commands/down/27c30960_nohash_0.wav
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/down/27c30960_nohash_0.wav
- en: data/mini_speech_commands/go/19785c4e_nohash_0.wav
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/go/19785c4e_nohash_0.wav
- en: data/mini_speech_commands/yes/d9b50b8b_nohash_0.wav
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/yes/d9b50b8b_nohash_0.wav
- en: data/mini_speech_commands/no/f953e1af_nohash_3.wav
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/no/f953e1af_nohash_3.wav
- en: data/mini_speech_commands/stop/f632210f_nohash_0.wav
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: data/mini_speech_commands/stop/f632210f_nohash_0.wav
- en: You use `glob` again, this time showing it the pattern `'/*/*'` to list all
    the files in the subdirectories ❶. Then you randomly shuffle the returned list
    of filenames to reduce any bias in the training data ❷. You print the total number
    of files found, as well as the first five filenames. The output indicates that
    there are 8,000 WAV files in the dataset, and it gives you some idea of how the
    files are named—for example, *f632210f_nohash_0.wav*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你再次使用`glob`，这次给它传入模式`'/*/*'`，以列出所有子目录中的文件❶。然后，你随机打乱返回的文件名列表，以减少训练数据中的偏差❷。你打印出找到的文件总数，以及前五个文件名。输出结果显示数据集中有8,000个WAV文件，并给出了一些文件命名的提示——例如，*f632210f_nohash_0.wav*。
- en: 'Next, take a look at some individual WAV files from the dataset:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，查看数据集中的一些单独的WAV文件：
- en: filepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav' ❶
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav' ❶
- en: rate, data = wavfile.read(filepath) ❷
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: rate, data = wavfile.read(filepath) ❷
- en: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
- en: filepath = 'data/mini_speech_commands/no/f953e1af_nohash_3.wav'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = 'data/mini_speech_commands/no/f953e1af_nohash_3.wav'
- en: rate, data = wavfile.read(filepath)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: rate, data = wavfile.read(filepath)
- en: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: print("rate = {}, data.shape = {}, data.dtype = {}".format(rate, data.shape,
    data.dtype))
- en: rate = 16000, data.shape = (13654,), data.dtype = int16
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: rate = 16000, data.shape = (13654,), data.dtype = int16
- en: rate = 16000, data.shape = (16000,), data.dtype = int16
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: rate = 16000, data.shape = (16000,), data.dtype = int16
- en: 'You set the name of a WAV file you want to look at ❶ and use the `wavefile`
    module from `scipy` to read data from the file ❷. Then you print the sampling
    rate, shape (number of samples), and type of the data. You do the same for a second
    WAV file. The output shows that the sampling rates of both the WAV files are 16,000,
    as expected, and that each sample is a 16-bit integer for both—also expected.
    However, the shape indicates the first file has only 13,654 samples, and this
    is a problem. To train the neural network, each WAV file needs to have the same
    length; in this case, you’d like each recording to be one second, or 16,000 samples,
    long. Unfortunately, not all the files in the dataset fit that standard. We’ll
    look at a solution to this problem shortly, but first, try plotting the data from
    one of these WAV files:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你设置了一个WAV文件的文件名，想要查看它❶，并使用`scipy`中的`wavefile`模块读取该文件的数据❷。然后，你打印出采样率、形状（样本数量）和数据类型。你对第二个WAV文件做了相同的操作。输出结果显示，两个WAV文件的采样率都为16,000，正如预期的那样，并且每个样本都是16位整数——也是预期中的情况。然而，形状显示，第一个文件只有13,654个样本，这是一个问题。为了训练神经网络，每个WAV文件都需要具有相同的长度；在这种情况下，你希望每个录音的长度为一秒，或者16,000个样本。遗憾的是，数据集中的并非所有文件都符合这个标准。我们稍后会看看如何解决这个问题，但首先，尝试绘制其中一个WAV文件的数据：
- en: filepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = 'data/mini_speech_commands/stop/f632210f_nohash_1.wav'
- en: rate, data = wavfile.read(filepath)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: rate, data = wavfile.read(filepath)
- en: ❶ plt.plot(data)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ plt.plot(data)
- en: '![](images/nsp-venkitachalam503045-g15001.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-g15001.jpg)'
- en: You use `matplotlib` to create a plot of the audio waveform ❶. The WAV files
    in this dataset contain 16-bit signed data, which can range from −32,768 to +32,767\.
    The y-axis of the plot shows you that the data in this file ranges from only around
    −10,000 to +7,500\. The plot’s x-axis also underscores that the data is short
    of the necessary 16,000 samples—the axis runs only to 14,000.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用`matplotlib`来创建音频波形的图表❶。这个数据集中的WAV文件包含16位有符号数据，范围从−32,768到+32,767。图表的y轴显示，该文件中的数据仅在大约−10,000到+7,500之间波动。图表的x轴还表明，数据缺少必要的16,000个样本——该轴仅显示到14,000。
- en: Cleaning the Data
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 清理数据
- en: You’ve seen that the dataset needs to be standardized so that each clip is one
    second long. This type of preparatory work is called *data cleaning*, and you
    can do it by padding the audio data with zeros until it reaches a length of 16,000
    samples. You should clean the data further by *normalizing* it—mapping the value
    of each sample from range [−32,768, +32,767] to range [−1, 1]. This type of normalization
    is crucial for machine learning, as keeping the input data small and uniform will
    help the training. (For the mathematically curious, large numbers in the inputs
    will cause problems in the convergence of gradient descent algorithms used to
    train the data.)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到，数据集需要进行标准化，以确保每段音频长度为1秒。这种类型的准备工作被称为*数据清理*，你可以通过用零填充音频数据，直到其长度达到16,000个样本来完成它。你应该进一步清理数据，通过*归一化*它——将每个样本的值从范围[−32,768,
    +32,767]映射到范围[−1, 1]。这种归一化对于机器学习至关重要，因为保持输入数据小且统一将有助于训练。（对于数学上感兴趣的人来说，输入数据中的大数字会导致训练过程中梯度下降算法的收敛问题。）
- en: As an example of data cleaning, here you apply both padding and normalization
    to the WAV file you viewed in the previous listing. Then you plot the results
    to confirm that the cleaning has worked.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据清理的一个示例，在这里你对之前查看过的WAV文件应用了填充和归一化操作。然后，你绘制结果以确认清理操作已经生效。
- en: ❶ padded_data = np.zeros((16000,), dtype=np.int16)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ padded_data = np.zeros((16000,), dtype=np.int16)
- en: ❷ padded_data[:data.shape[0]] = data
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ padded_data[:data.shape[0]] = data
- en: ❸ norm_data = np.array(padded_data/32768.0, dtype=np.float32)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ norm_data = np.array(padded_data/32768.0, dtype=np.float32)
- en: plt.plot(norm_data)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(norm_data)
- en: '![](images/nsp-venkitachalam503045-g15002.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-g15002.jpg)'
- en: You create a 16-bit `numpy` array of length 16,000 filled with zeros ❶. Then
    you use the array slice operator, `[:]`, to copy the contents of the too-short
    WAV file into the beginning of the array ❷. Here `data.shape[0]` gives you the
    number of samples in the original WAV file, since `data.shape` is a tuple in the
    form `(13654,)`. You now have one second of WAV data, consisting of the original
    audio data followed by a padding of zeros as needed. You next create a normalized
    version of the data by dividing the values in the array by 32,768, the maximum
    value a 16-bit integer could have ❸. Then you plot the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了一个长度为16,000的16位`numpy`数组，填充了零❶。然后，你使用数组切片操作符`[:]`将过短的WAV文件的内容复制到数组的开头❷。这里`data.shape[0]`给出了原始WAV文件的样本数，因为`data.shape`是形如`(13654,)`的元组。你现在得到了1秒钟的WAV数据，包含了原始音频数据，后面是按需填充的零。接下来，你通过将数组中的值除以32,768来创建一个归一化版本的数据，32,768是16位整数的最大值❸。然后，你绘制这些数据。
- en: The x-axis of the output shows that the data has been padded to extend to 16,000
    samples, with the values from around 14,000 to 16,000 all being zero. Also, the
    y-axis shows that the values have all been normalized to fall nicely within the
    range of (−1, 1).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图的x轴显示数据已经被填充，扩展到了16,000个样本，值从大约14,000到16,000的部分全是零。同时，y轴显示这些值已经被归一化，落在了(−1,
    1)的范围内。
- en: Looking at Spectrograms
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查看频谱图
- en: 'As we’ve discussed, you won’t be training your model on the raw data from the
    WAV files. Instead, you’ll generate spectrograms of the files and use them to
    train the model. Here’s an example of how to generate a spectrogram:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所讨论的，你不会直接使用WAV文件的原始数据来训练模型。相反，你将生成这些文件的频谱图，并使用它们来训练模型。下面是如何生成频谱图的一个示例：
- en: filepath = 'data/mini_speech_commands/yes/00f0204f_nohash_0.wav'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: filepath = 'data/mini_speech_commands/yes/00f0204f_nohash_0.wav'
- en: rate, data = wavfile.read(filepath)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: rate, data = wavfile.read(filepath)
- en: ❶ f, t, spec = scipy.signal.stft(data, fs=16000, nperseg=255,
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f, t, spec = scipy.signal.stft(data, fs=16000, nperseg=255,
- en: noverlap = 124, nfft=256)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: noverlap = 124, nfft=256)
- en: ❷ spec = np.abs(spec)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ spec = np.abs(spec)
- en: 'print("spec: min = {}, max = {}, shape = {}, dtype = {}".format(np.min(spec),'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("spec: min = {}, max = {}, shape = {}, dtype = {}".format(np.min(spec),'
- en: np.max(spec), spec.shape, spec.dtype))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: np.max(spec), spec.shape, spec.dtype))
- en: ❸ X = t * 129*124
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ X = t * 129*124
- en: ❹ plt.pcolormesh(X, f, spec)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ plt.pcolormesh(X, f, spec)
- en: 'spec: min = 0.0, max = 2089.085693359375, shape = (129, 124), dtype = float32'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'spec: min = 0.0, max = 2089.085693359375, shape = (129, 124), dtype = float32'
- en: '![](images/nsp-venkitachalam503045-g15003.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-g15003.jpg)'
- en: 'You pick an arbitrary WAV file from the *yes* subdirectory and extract its
    data using the `wavfile` module from `scipy`, as before. Then you use the `scipy.signal.stft()`
    function to compute the spectrogram of the data ❶. In this function call, `fs`
    is the sampling rate, `nperseg` is the length of each segment, and `noverlap`
    is the number of overlapping samples between consecutive segments. The `stft()`
    function returns a tuple comprising three members: `f`, an array of frequencies;
    `t`, an array of the time intervals mapped to the range [0.0, 1.0]; and `spec`,
    the STFT itself, a grid of 129×124 complex numbers (these dimensions are given
    as `shape` in the output). You use `np.abs()` to convert the complex numbers in
    `spec` into real numbers ❷. Then you print some information about the computed
    spectrogram. Next, you create an array `X` to hold the sample numbers corresponding
    to the time intervals ❸. You get these by multiplying `t` by the dimensions of
    the grid. Finally, you use the `pcolormesh()` method to plot the grid in `spec`,
    using the values in `X` as the grid’s x-axis and the values in `f` as the y-axis
    ❹.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你从*yes*子目录中挑选一个任意的WAV文件，并像之前一样使用`scipy`中的`wavfile`模块提取它的数据。然后，使用`scipy.signal.stft()`函数来计算数据的频谱图❶。在这个函数调用中，`fs`是采样率，`nperseg`是每个段的长度，`noverlap`是连续段之间的重叠样本数。`stft()`函数返回一个包含三个成员的元组：`f`，频率数组；`t`，映射到[0.0,
    1.0]范围的时间区间数组；`spec`，STFT本身，一个129×124的复数网格（这些维度在输出中给出了`shape`）。你使用`np.abs()`将`spec`中的复数转换为实数❷。然后，你打印一些关于计算出的频谱图的信息。接下来，你创建一个数组`X`来存储与时间区间对应的样本编号❸。你通过将`t`与网格的维度相乘来获得这些样本编号。最后，你使用`pcolormesh()`方法绘制`spec`中的网格，将`X`中的值作为网格的x轴，`f`中的值作为y轴❹。
- en: The output shows the spectrogram. This 129×124 grid of values (an image), and
    many more like it, will be the input for the neural network. The bright spots
    around 1,000 Hz and lower, starting around 4,000 samples in, are where the frequency
    content is most prominent, while darker areas represent less prominent frequencies.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示的是频谱图。这个129×124的数值网格（即图像），以及更多类似的图像，将作为神经网络的输入。从约4,000个样本点开始，1,000 Hz和更低频率的亮点区域，正是频率内容最为突出的地方，而较暗的区域则代表频率较为不明显的部分。
- en: NOTE Notice that the y-axis in the spectrogram images goes up to only about
    8,000 Hz. This is a consequence of the *sampling theorem* in digital signal processing,
    which states that the maximum frequency that can be accurately measured in a digitally
    sampled signal is half the sampling rate. In this case, that maximum frequency
    works out to 16,000/2 = 8,000 Hz.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请注意，频谱图中的y轴仅显示到大约8,000 Hz。这是数字信号处理中的*采样定理*的结果，该定理指出，在数字采样信号中，能够准确测量的最大频率是采样率的一半。在这种情况下，最大频率为16,000/2
    = 8,000 Hz。
- en: Training the Model
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练模型
- en: You’re now ready to turn your attention to training the ML model, and that largely
    means leaving behind Python libraries like `numpy` and `scipy` in favor of TensorFlow
    methods and data structures like `tf.Tensor` and `tf.data.Dataset`. You’ve been
    using `numpy` and `scipy` so far because they’ve provided a convenient way to
    explore the speech commands dataset, and in fact you could continue using them,
    but then you’d miss out on the optimization opportunities provided by TensorFlow,
    which is designed for large-scale ML systems. You’ll find that TensorFlow has
    near-identical functions for most of the computations you’ve done until now, so
    the transition will be smooth. For our purposes, when I refer to a *tensor* in
    the upcoming discussion, understand that it’s similar to talking about a `numpy`
    array.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好将注意力转向训练机器学习模型，这在很大程度上意味着你需要放弃像`numpy`和`scipy`这样的Python库，转而使用TensorFlow的方法和数据结构，例如`tf.Tensor`和`tf.data.Dataset`。你之所以一直使用`numpy`和`scipy`，是因为它们为你提供了一个方便的方式来探索语音命令数据集，实际上你可以继续使用它们，但那样你将错失TensorFlow所提供的优化机会，而TensorFlow是为大规模机器学习系统设计的。你会发现，TensorFlow对于你至今做的大部分计算都有几乎相同的函数，因此过渡会非常顺利。为了我们的讨论目的，当我提到*张量*时，可以理解为它类似于`numpy`数组。
- en: 'To train the ML model, you need to be able to extract the spectrogram and label
    ID (the spoken command) from the filepath of an input audio file. For that, first
    create a function that computes an STFT:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练机器学习模型，你需要能够从输入音频文件的文件路径中提取频谱图和标签ID（即口令）。为此，首先创建一个计算STFT的函数：
- en: 'def stft(x):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 'def stft(x):'
- en: ❶ f, t, spec = scipy.signal.stft(x.numpy(), fs=16000, nperseg=255,
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f, t, spec = scipy.signal.stft(x.numpy(), fs=16000, nperseg=255,
- en: noverlap=124, nfft=256)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: noverlap=124, nfft=256)
- en: ❷ return tf.convert_to_tensor(np.abs(spec))
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回 tf.convert_to_tensor(np.abs(spec))
- en: The function takes in `x`, the data extracted from a WAV file, and computes
    its STFT using `scipy`, as before ❶. Then you convert the returned `numpy` array
    to a `tf.Tensor` object and return the result ❷. There is, in fact, a TensorFlow
    method called `tf.signal.stft()` that’s similar to the `scipy.signal.stft()` method,
    so why not use it? The answer is that the TensorFlow method won’t be available
    on the Raspberry Pi, where you’ll be using the slimmed-down TensorFlow Lite interpreter.
    Any preprocessing you do during the training phase should be identical to any
    preprocessing you do during inference, so you need to ensure that you use the
    same functions in Colab as you’ll use on the Pi.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接收 `x`，即从 WAV 文件中提取的数据，并像之前的 ❶ 一样使用 `scipy` 计算其 STFT。然后，你将返回的 `numpy` 数组转换为
    `tf.Tensor` 对象并返回结果 ❷。事实上，TensorFlow 有一个与 `scipy.signal.stft()` 方法类似的函数 `tf.signal.stft()`，那么为什么不使用它呢？答案是，TensorFlow
    方法在 Raspberry Pi 上不可用，因为你将使用精简版的 TensorFlow Lite 解释器。你在训练阶段进行的任何预处理操作应当与推理阶段保持一致，因此你需要确保在
    Colab 中使用的函数与在 Pi 上使用的相同。
- en: Now you can make use of your `stft()` function in a helper function that extracts
    the spectrogram and label ID from a filepath.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在一个辅助函数中使用 `stft()` 函数，该函数从文件路径中提取频谱图和标签 ID。
- en: 'def get_spec_label_pair(filepath):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'def get_spec_label_pair(filepath):'
- en: '# read WAV file'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '# 读取 WAV 文件'
- en: file_data = tf.io.read_file(filepath)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: file_data = tf.io.read_file(filepath)
- en: data, rate = tf.audio.decode_wav(file_data)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: data, rate = tf.audio.decode_wav(file_data)
- en: data = tf.squeeze(data, axis=-1)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: data = tf.squeeze(data, axis=-1)
- en: '# add zero padding for N < 16000'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '# 为 N < 16000 添加零填充'
- en: ❶ zero_padding = tf.zeros([16000] - tf.shape(data), dtype=tf.float32)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ zero_padding = tf.zeros([16000] - tf.shape(data), dtype=tf.float32)
- en: '# combine data with zero padding'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将数据与零填充合并'
- en: ❷ padded_data = tf.concat([data, zero_padding], 0)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ padded_data = tf.concat([data, zero_padding], 0)
- en: '# compute spectrogram'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '# 计算频谱图'
- en: ❸ spec = tf.py_function(func=stft, inp=[padded_data], Tout=tf.float32)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ spec = tf.py_function(func=stft, inp=[padded_data], Tout=tf.float32)
- en: spec.set_shape((129, 124))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: spec.set_shape((129, 124))
- en: spec = tf.expand_dims(spec, -1)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: spec = tf.expand_dims(spec, -1)
- en: '# get label'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取标签'
- en: ❹ cmd = tf.strings.split(filepath, os.path.sep)[-2]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ cmd = tf.strings.split(filepath, os.path.sep)[-2]
- en: ❺ label_id = tf.argmax(tf.cast(cmd == commands, "uint32"))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ label_id = tf.argmax(tf.cast(cmd == commands, "uint32"))
- en: '# return tuple'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '# 返回元组'
- en: return (spec, label_id)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: return (spec, label_id)
- en: You start by reading the file using `tf.io.read_file()` and decoding the WAV
    format using the `tf.audio.decode_wav()` function. (The latter is comparable to
    the `scipy.io.wavfile.read()` function you used previously.) You then use `tf.squeeze()`
    to change the shape of the `data` tensor from (*N*, 1) to (*N*, ), which is required
    for functions coming ahead. Next, you create a tensor for zero-padding the data
    ❶. Tensors are immutable objects, however, so you can’t copy the WAV data directly
    into a tensor full of zeros, as you did earlier with `numpy` arrays. Instead,
    you create a tensor with the exact number of zeros you need to pad the data, and
    then you concatenate it with the data tensor ❷.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先使用 `tf.io.read_file()` 读取文件，并使用 `tf.audio.decode_wav()` 函数解码 WAV 格式。（后者与之前使用的
    `scipy.io.wavfile.read()` 函数类似。）然后，你使用 `tf.squeeze()` 将 `data` 张量的形状从 (*N*, 1)
    转换为 (*N*,)，这是后续函数所要求的形状。接下来，你创建一个张量来为数据进行零填充 ❶。然而，张量是不可变对象，因此你不能像之前那样直接将 WAV 数据复制到充满零的张量中。相反，你需要创建一个张量，包含你需要的零的数量，并将其与数据张量进行拼接
    ❷。
- en: You next use `tf.py_function()` to call the `stft()` function you defined earlier
    ❸. In this call, you also need to specify the input and the data type of the output.
    This is a common method for calling a non-TensorFlow function from TensorFlow.
    You then do some reshaping of the tensor returned by `stft()`. First you use `set_shape()`
    to reshape it to (129, 124), which is necessary because you’re going from a TensorFlow
    function to a Python function and back. Then you run `tf.expand_dims(spec, -1)`
    to add a third dimension, going from (129, 124) to (129, 124, 1). The extra dimension
    is needed for the neural network model you’ll be building. Finally, you extract
    the label (for example, `'no'`) associated with the filepath ❹ and convert the
    label string to the integer `label_id` ❺, which is the index of the string in
    your `commands` list.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你使用 `tf.py_function()` 调用之前定义的 `stft()` 函数 ❸。在这个调用中，你还需要指定输入和输出的数据类型。这是从
    TensorFlow 中调用非 TensorFlow 函数的常见方法。然后，你会对 `stft()` 返回的张量进行一些重塑。首先，使用 `set_shape()`
    将其重塑为 (129, 124)，这是必要的，因为你正在从一个 TensorFlow 函数切换到一个 Python 函数，然后再返回。接着，你运行 `tf.expand_dims(spec,
    -1)` 来添加第三个维度，将形状从 (129, 124) 改为 (129, 124, 1)。这个额外的维度是你将在构建神经网络模型时需要的。最后，你提取与文件路径对应的标签（例如，'no'）❹
    并将标签字符串转换为整数 `label_id` ❺，它是 `commands` 列表中字符串的索引。
- en: 'Next, you need to get the input files ready for training. Recall that you had
    8,000 audio files in the subdirectories and that you randomly shuffled their filepath
    strings and put them into a list called `wav_file_names`. Now you’ll partition
    the data into three: 80 percent, or 6,400 files, for training; 10 percent, or
    800 files, for validation; and the other 10 percent for testing. Such partitioning
    is a common practice in machine learning. Once the model is trained using training
    data, you can use the validation data to tweak the model’s accuracy by changing
    the hyperparameters. The testing data is used only for checking the final accuracy
    of the (tweaked) model.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要准备输入文件进行训练。回想一下，你在子目录中有 8000 个音频文件，并且已经随机打乱了它们的文件路径字符串，并将它们放入名为 `wav_file_names`
    的列表中。现在，你将数据分为三部分：80%，即 6400 个文件用于训练；10%，即 800 个文件用于验证；剩下的 10% 用于测试。这样的数据划分是机器学习中的常见做法。一旦使用训练数据训练了模型，你可以利用验证数据通过调整超参数来优化模型的准确性。测试数据仅用于检查（调整后的）模型的最终准确性。
- en: train_files = wav_file_names[:6400]
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: train_files = wav_file_names[:6400]
- en: val_files = wav_file_names[6400:7200]
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: val_files = wav_file_names[6400:7200]
- en: test_files = wav_file_names[7200:]
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: test_files = wav_file_names[7200:]
- en: 'Now you load the filepath strings into TensorFlow `Dataset` objects. These
    objects are critical to working with TensorFlow; they hold your input data and
    allow for data transformations, and all this can happen at a large scale:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你将文件路径字符串加载到 TensorFlow `Dataset` 对象中。这些对象对于使用 TensorFlow 至关重要；它们保存你的输入数据并允许进行数据转换，所有这些都可以在大规模下进行：
- en: train_ds = tf.data.Dataset.from_tensor_slices(train_files)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: train_ds = tf.data.Dataset.from_tensor_slices(train_files)
- en: val_ds = tf.data.Dataset.from_tensor_slices(val_files)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: val_ds = tf.data.Dataset.from_tensor_slices(val_files)
- en: test_ds = tf.data.Dataset.from_tensor_slices(test_files)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: test_ds = tf.data.Dataset.from_tensor_slices(test_files)
- en: 'Take a look at what you just created:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 看看你刚刚创建的内容：
- en: 'for val in train_ds.take(5):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `train_ds.take(5)` 中的每一个 `val`：
- en: print(val)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: print(val)
- en: tf.Tensor(b'data/mini_speech_commands/stop/b4aa9fef_nohash_2.wav', shape=(),
    dtype=string)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(b'data/mini_speech_commands/stop/b4aa9fef_nohash_2.wav', shape=(),
    dtype=string)
- en: tf.Tensor(b'data/mini_speech_commands/stop/962f27eb_nohash_0.wav', shape=(),
    dtype=string)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(b'data/mini_speech_commands/stop/962f27eb_nohash_0.wav', shape=(),
    dtype=string)
- en: --snip--
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: --snip--
- en: tf.Tensor(b'data/mini_speech_commands/left/cf87b736_nohash_1.wav', shape=(),
    dtype=string)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(b'data/mini_speech_commands/left/cf87b736_nohash_1.wav', shape=(),
    dtype=string)
- en: 'Each `Dataset` object contains a bunch of tensors of type `string`, each holding
    a filepath. What you really need, however, are the `(spec, label_id)` pairs corresponding
    to those filepaths. You create those here:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `Dataset` 对象包含一组 `string` 类型的张量，每个张量保存一个文件路径。然而，你真正需要的是与这些文件路径对应的 `(spec,
    label_id)` 配对。你将在这里创建这些配对：
- en: train_ds = train_ds.map(get_spec_label_pair)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: train_ds = train_ds.map(get_spec_label_pair)
- en: val_ds = val_ds.map(get_spec_label_pair)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: val_ds = val_ds.map(get_spec_label_pair)
- en: test_ds = test_ds.map(get_spec_label_pair)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: test_ds = test_ds.map(get_spec_label_pair)
- en: You use `map()` to apply your `get_spec_label_pair()` function to each `Dataset`
    object. This technique of mapping a function to a list of things is common in
    computing. Essentially, you’re going through each filepath in the `Dataset` object,
    calling `get_spec_label_pair()` on it, and storing the resulting `(spec, label_id)`
    pair in a new `Dataset` object.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用`map()`将`get_spec_label_pair()`函数应用到每个`Dataset`对象。这种将函数映射到事物列表的技巧在计算中很常见。本质上，你在`Dataset`对象中的每个文件路径上调用`get_spec_label_pair()`，并将得到的`(spec,
    label_id)`对存储到一个新的`Dataset`对象中。
- en: 'Now you further prepare the dataset for training by splitting it up into smaller
    batches:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你通过将数据集分割成更小的批次进一步准备数据集进行训练：
- en: batch_size = 64
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size = 64
- en: train_ds = train_ds.batch(batch_size)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: train_ds = train_ds.batch(batch_size)
- en: val_ds = val_ds.batch(batch_size)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: val_ds = val_ds.batch(batch_size)
- en: Here you set the training and validation datasets to have a batch size of 64\.
    This is a common technique for speeding up the training process. If you tried
    to work with all 6,400 training samples and 800 validation samples at once, it
    would require a huge amount of memory and would slow down the training.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你将训练集和验证集的批量大小设置为64。这是加速训练过程的常见技巧。如果你试图一次性处理所有6,400个训练样本和800个验证样本，将需要大量内存并且会拖慢训练速度。
- en: 'Now you’re finally ready to create your neural network model:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你终于准备好创建你的神经网络模型了：
- en: ❶ input_shape = (129, 124, 1)
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ input_shape = (129, 124, 1)
- en: ❷ num_labels = 8
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ num_labels = 8
- en: norm_layer = preprocessing.Normalization()
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: norm_layer = preprocessing.Normalization()
- en: '❸ norm_layer.adapt(train_ds.map(lambda x, _: x))'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ norm_layer.adapt(train_ds.map(lambda x, _: x))'
- en: ❹ model = `models`.Sequential([
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ model = `models`.Sequential([
- en: layers.Input(shape=input_shape),
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Input(shape=input_shape),
- en: preprocessing.Resizing(32, 32),
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: preprocessing.Resizing(32, 32),
- en: norm_layer,
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: norm_layer,
- en: layers.Conv2D(32, 3, activation='relu'),
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Conv2D(32, 3, activation='relu'),
- en: layers.Conv2D(64, 3, activation='relu'),
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Conv2D(64, 3, activation='relu'),
- en: layers.MaxPooling2D(),
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: layers.MaxPooling2D(),
- en: layers.Dropout(0.25),
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Dropout(0.25),
- en: layers.Flatten(),
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Flatten(),
- en: layers.Dense(128, activation='relu'),
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Dense(128, activation='relu'),
- en: layers.Dropout(0.5),
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Dropout(0.5),
- en: layers.Dense(num_labels),
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: layers.Dense(num_labels),
- en: '])'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '])'
- en: ❺ model.summary()
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ model.summary()
- en: 'Model: "sequential_3"'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '模型: "sequential_3"'
- en: _____________________________________________________________________
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: _____________________________________________________________________
- en: 'Layer (type)                      Output Shape            Param #'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 层（类型）                      输出形状            参数数量
- en: =====================================================================
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: =====================================================================
- en: resizing_3 (Resizing)            (None, 32, 32, 1)        0
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: resizing_3 (Resizing)            (None, 32, 32, 1)        0
- en: normalization_3 (Normalization)  (None, 32, 32, 1)        3
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: normalization_3 (Normalization)  (None, 32, 32, 1)        3
- en: conv2d_5 (Conv2D)                (None, 30, 30, 32)       320
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: conv2d_5 (Conv2D)                (None, 30, 30, 32)       320
- en: conv2d_6 (Conv2D)                (None, 28, 28, 64)       18496
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: conv2d_6 (Conv2D)                (None, 28, 28, 64)       18496
- en: max_pooling2d_3 (MaxPooling2D)   (None, 14, 14, 64)       0
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: max_pooling2d_3 (MaxPooling2D)   (None, 14, 14, 64)       0
- en: dropout_6 (Dropout)              (None, 14, 14, 64)       0
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: dropout_6 (Dropout)              (None, 14, 14, 64)       0
- en: flatten_3 (Flatten)              (None, 12544)            0
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: flatten_3 (Flatten)              (None, 12544)            0
- en: dense_6 (Dense)                  (None, 128)              1605760
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: dense_6 (Dense)                  (None, 128)              1605760
- en: dropout_7 (Dropout)              (None, 128)              0
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: dropout_7 (Dropout)              (None, 128)              0
- en: dense_7 (Dense)                  (None, 8)                1032
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: dense_7 (Dense)                  (None, 8)                1032
- en: =====================================================================
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: =====================================================================
- en: 'Total params: 1,625,611'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '总参数: 1,625,611'
- en: 'Trainable params: 1,625,608'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '可训练参数: 1,625,608'
- en: 'Non-trainable params: 3'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '非可训练参数: 3'
- en: 'You set the shape of the input into the first layer of the model ❶ and then
    set the number of labels ❷, which will be the number of units in the model’s output
    layer. Next, you set up a normalization layer for the spectrogram data. This will
    scale and shift the data to a distribution centered on 1 with a standard deviation
    of 1\. This is a common practice in ML that improves training. Don’t let the `lambda`
    scare you ❸. All it’s doing is defining an anonymous function that picks out just
    the spectrogram from each `(spec, label_id)` pair in the training dataset. The
    `x, _: x` is just saying to ignore the second element in the pair and return only
    the first element.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '你设置了模型第一层的输入形状 ❶，然后设置标签的数量 ❷，这将是模型输出层的单位数。接下来，你为频谱图数据设置了归一化层。这将把数据缩放并偏移到以 1
    为中心，标准差为 1 的分布。这是机器学习中的常见做法，可以提高训练效果。不要让`lambda`吓到你 ❸。它所做的只是定义一个匿名函数，从训练数据集中的每个`(spec,
    label_id)`对中提取出频谱图。`x, _: x`只是表示忽略该对中的第二个元素，仅返回第一个元素。'
- en: You next create the neural network model, one layer at a time ❹. The layers
    correspond to the architecture we viewed earlier in [Figure 15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1).
    Finally, you print out a summary of the model ❺, which is shown in the output.
    The summary tells you all the layers in the model, the shape of the output tensor
    at each stage, and the number of trainable parameters in each layer.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你逐层创建神经网络模型 ❹。这些层对应于我们之前在[图 15-1](nsp-venkitachalam503045-0030.xhtml#fig15-1)中看到的架构。最后，你打印出模型的摘要
    ❺，该摘要显示在输出中。摘要告诉你模型中的所有层、每个阶段输出张量的形状以及每层可训练参数的数量。
- en: 'Now you need to compile the model. The compilation step sets the optimizer,
    the loss function, and the data collection metrics for the model:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你需要编译模型。编译步骤会为模型设置优化器、损失函数和数据收集指标：
- en: model.compile(
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: model.compile(
- en: optimizer=tf.keras.optimizers.Adam(),
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: optimizer=tf.keras.optimizers.Adam(),
- en: loss=`tf.keras.losses`.SparseCategoricalCrossentropy(from_logits=True),
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: loss=`tf.keras.losses`.SparseCategoricalCrossentropy(from_logits=True),
- en: metrics=['accuracy'],
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: metrics=['accuracy'],
- en: )
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: A *loss function* is a function used to measure how well a neural network is
    doing by comparing the output of the model to the known correct training data.
    An *optimizer* is a method used to adjust the trainable parameters in a model
    to reduce the losses. In this case, you’re using an optimizer of type `Adam` and
    a loss function of type `SparseCategoricalCrossentropy`. You also get set up to
    collect some accuracy metrics, which you’ll use to check how the training went.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*是用来衡量神经网络表现的函数，通过将模型的输出与已知的正确训练数据进行比较。*优化器*是用来调整模型中可训练参数的方法，以减少损失。在这个例子中，你使用了`Adam`类型的优化器和`SparseCategoricalCrossentropy`类型的损失函数。你还设置了准确率指标，用于检查训练效果。'
- en: 'Next, you train the model:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你开始训练模型：
- en: EPOCHS = 10
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: EPOCHS = 10
- en: history = model.fit(
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: history = model.fit(
- en: train_ds,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: train_ds,
- en: validation_data=val_ds,
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data=val_ds,
- en: epochs=EPOCHS,
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: epochs=EPOCHS,
- en: callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=15), ❶
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=15), ❶
- en: )
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: Epoch 1/10
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 1/10
- en: '100/100 [==============================] - 38s 371ms/step - loss: 1.7219 -
    accuracy: 0.3700'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '100/100 [==============================] - 38s 371ms/step - loss: 1.7219 -
    accuracy: 0.3700'
- en: '- val_loss: 1.2672 - val_accuracy: 0.5763'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '- val_loss: 1.2672 - val_accuracy: 0.5763'
- en: Epoch 2/10
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 2/10
- en: '100/100 [==============================] - 37s 368ms/step - loss: 1.1791 -
    accuracy: 0.5756'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '100/100 [==============================] - 37s 368ms/step - loss: 1.1791 -
    accuracy: 0.5756'
- en: '- val_loss: 0.9616 - val_accuracy: 0.6650'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '- val_loss: 0.9616 - val_accuracy: 0.6650'
- en: --snip--
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: --snip--
- en: Epoch 10/10
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 10/10
- en: '100/100 [==============================] - 39s 388ms/step - loss: 0.3897 -
    accuracy: 0.8639'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '100/100 [==============================] - 39s 388ms/step - loss: 0.3897 -
    accuracy: 0.8639'
- en: '- val_loss: 0.4766 - val_accuracy: 0.8450'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '- val_loss: 0.4766 - val_accuracy: 0.8450'
- en: You train the model by passing the training dataset `train_ds` into `model.fit()`.
    You also specify the validation dataset `val_ds`, which is used to evaluate how
    accurate the model is. The training takes place over 10 *epochs*. During each
    epoch, the complete set of training data is shown to the neural network. The data
    is randomly shuffled each time, so training across multiple epochs allows the
    model to learn better. You use the `callback` option ❶ to set up a function to
    exit the training if it turns out that the training loss is no longer decreasing
    with each epoch.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过将训练数据集 `train_ds` 传入 `model.fit()` 来训练模型。你还指定了验证数据集 `val_ds`，用于评估模型的准确性。训练会进行
    10 个 *周期*。每个周期，神经网络都会看到完整的训练数据集。每次数据都会随机打乱，因此跨多个周期进行训练可以帮助模型更好地学习。你使用 `callback`
    选项 ❶ 设置一个函数，如果发现训练损失在每个周期后不再下降，就退出训练。
- en: Running this Colab cell will take some time. The progress will be shown on the
    screen as the training is in process. Looking at the output, the `val_accuracy`
    listed under Epoch 10 shows that the model was about 85 percent accurate at running
    inference on the validation data by the end of the training. (The `val_accuracy`
    metric corresponds to the validation data, while `accuracy` corresponds to the
    training data.)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此 Colab 单元格需要一些时间。训练过程中的进度将显示在屏幕上。从输出结果来看，`val_accuracy` 在第 10 个周期下显示，模型在训练结束时对验证数据的推理准确率大约为
    85%。(`val_accuracy` 指标对应于验证数据，而 `accuracy` 指标对应于训练数据。)
- en: 'Now you can try the model by running inference on the testing portion of the
    data:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以通过在数据的测试部分运行推理来尝试模型：
- en: test_audio = []
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: test_audio = []
- en: test_labels = []
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels = []
- en: '❶ for audio, label in test_ds:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ for audio, label in test_ds:'
- en: test_audio.append(audio.numpy())
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: test_audio.append(audio.numpy())
- en: test_labels.append(label.numpy())
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels.append(label.numpy())
- en: ❷ test_audio = np.array(test_audio)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ test_audio = np.array(test_audio)
- en: test_labels = np.array(test_labels)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: test_labels = np.array(test_labels)
- en: ❸ y_pred = np.argmax(model.predict(test_audio), axis=1)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ y_pred = np.argmax(model.predict(test_audio), axis=1)
- en: y_true = test_labels
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: y_true = test_labels
- en: ❹ test_acc = sum(y_pred == y_true) / len(y_true)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ test_acc = sum(y_pred == y_true) / len(y_true)
- en: 'print(f''Test set accuracy: {test_acc:.0%}'')'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(f''测试集准确率: {test_acc:.0%}'')'
- en: 25/25 [==============================] - 1s 35ms/step
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 25/25 [==============================] - 1s 35ms/step
- en: 'Test set accuracy: 84%'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '测试集准确率: 84%'
- en: You first fill in two lists, `test_audio` and `test_labels`, by iterating through
    the test dataset `test_ds` ❶. Then you create `numpy` arrays from these lists
    ❷ and run inference on the data ❸. You compute the test accuracy by summing up
    the number of times the predictions matched the true value and dividing them by
    the total number of items ❹. The output shows an accuracy of 84 percent. Not perfect,
    but good enough for this project.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先通过遍历测试数据集 `test_ds` 填充两个列表 `test_audio` 和 `test_labels` ❶。然后将这些列表转换为 `numpy`
    数组 ❷，并在数据上运行推理 ❸。你通过计算预测与真实值匹配的次数并将其除以总数来计算测试准确率 ❹。输出结果显示准确率为 84%。虽然不是完美的，但对这个项目来说已经足够好。
- en: Exporting the Model to the Pi
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导出模型到 Raspberry Pi
- en: 'Congratulations! You have a fully trained machine learning model. Now you need
    to get it from Colab onto your Raspberry Pi. The first step is to save it:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经训练好了一个完整的机器学习模型。现在你需要将它从 Colab 转移到你的 Raspberry Pi 上。第一步是保存模型：
- en: model.save('audioml.sav')
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: model.save('audioml.sav')
- en: 'This saves the model to a file on the cloud called *audioml.sav*. Next, convert
    that file to the TensorFlow Lite format so you can use it on your Pi:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把模型保存到云端的一个名为 *audioml.sav* 的文件中。接下来，将该文件转换为 TensorFlow Lite 格式，这样你就可以在 Raspberry
    Pi 上使用它：
- en: ❶ converter = tf.lite.TFLiteConverter.from_saved_model('audioml.sav')
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ converter = tf.lite.TFLiteConverter.from_saved_model('audioml.sav')
- en: ❷ tflite_model = converter.convert()
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ tflite_model = converter.convert()
- en: '❸ with open(''audioml.tflite'', ''wb'') as f:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ with open(''audioml.tflite'', ''wb'') as f:'
- en: f.write(tflite_model)
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(tflite_model)
- en: 'You create a `TFLiteConverter` object, passing in the saved model filename ❶.
    Then you do the conversion ❷ and write the simplified TensorFlow model to a file
    called *audioml.tflite* ❸. Now you need to download this *.tflite* file from Colab
    onto your computer. Running the following snippet will give you a browser prompt
    to save the *.tflite* file:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了一个 `TFLiteConverter` 对象，并传入保存的模型文件名 ❶。然后进行转换 ❷，并将简化后的 TensorFlow 模型写入名为
    *audioml.tflite* 的文件 ❸。现在你需要从 Colab 下载这个 *.tflite* 文件到你的电脑。运行以下代码片段会弹出一个浏览器提示，让你保存
    *.tflite* 文件：
- en: from google.colab import files
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: from google.colab import files
- en: files.download('audioml.tflite')
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: files.download('audioml.tflite')
- en: Once you have the file, you can transfer it to your Raspberry Pi using SSH as
    we’ve discussed in other chapters.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了文件，就可以像我们在其他章节中讨论过的那样，通过 SSH 将其传输到你的 Raspberry Pi。
- en: '[Using the Model on the Raspberry Pi](nsp-venkitachalam503045-0008.xhtml#rbh1704)'
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[在 Raspberry Pi 上使用模型](nsp-venkitachalam503045-0008.xhtml#rbh1704)'
- en: Now we’ll turn our attention to the Raspberry Pi portion of the code. This code
    uses parallel processing to take in audio data from the microphone, prepare that
    data for your trained ML model, and show the data to the model to perform inference.
    As usual, you can write the code on your local machine and then transfer it to
    your Pi via SSH. To view the complete code, see [“The Complete Code”](nsp-venkitachalam503045-0030.xhtml#ah1708)
    on [page 389](nsp-venkitachalam503045-0030.xhtml#p389). You can also download
    the code from [https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将注意力转向 Raspberry Pi 部分的代码。此代码使用并行处理从麦克风获取音频数据，将数据准备好供你训练好的 ML 模型使用，并将数据传递给模型进行推理。像往常一样，你可以在本地计算机上编写代码，然后通过
    SSH 将其传输到 Pi 上。要查看完整代码，请参见 [“完整代码”](nsp-venkitachalam503045-0030.xhtml#ah1708)
    第 389 页 (nsp-venkitachalam503045-0030.xhtml#p389)。你也可以从 [https://github.com/mkvenkit/pp2e/tree/main/audioml](https://github.com/mkvenkit/pp2e/tree/main/audioml)
    下载代码。
- en: Setting Up
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'Start by importing the required modules:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入所需的模块：
- en: from scipy.io import wavfile
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.io import wavfile
- en: from scipy import signal
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy import signal
- en: import numpy as np
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import argparse
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: import argparse
- en: import pyaudio
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: import pyaudio
- en: import wave
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: import wave
- en: import time
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: import time
- en: from tflite_runtime.interpreter import Interpreter
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: from tflite_runtime.interpreter import Interpreter
- en: from multiprocessing import Process, Queue
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: from multiprocessing import Process, Queue
- en: 'Next, you initialize some parameters that are defined as global variables:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将初始化一些作为全局变量定义的参数：
- en: VERBOSE_DEBUG = False
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: VERBOSE_DEBUG = False
- en: CHUNK = 4000
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: CHUNK = 4000
- en: FORMAT = pyaudio.paInt16
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: FORMAT = pyaudio.paInt16
- en: CHANNELS = 1
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: CHANNELS = 1
- en: SAMPLE_RATE = 16000
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: SAMPLE_RATE = 16000
- en: RECORD_SECONDS = 1
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: RECORD_SECONDS = 1
- en: NCHUNKS = int((SAMPLE_RATE * RECORD_SECONDS) / CHUNK)
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: NCHUNKS = int((SAMPLE_RATE * RECORD_SECONDS) / CHUNK)
- en: ND = 2 * CHUNK * NCHUNKS
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ND = 2 * CHUNK * NCHUNKS
- en: NDH = ND // 2
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: NDH = ND // 2
- en: device index of microphone
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 麦克风的设备索引
- en: ❶ dev_index = -1
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ dev_index = -1
- en: '`VERBOSE_DEBUG` is a flag you’ll use in many places in the code. For now, you
    set it to `False`, but if set to `True` (via a command line option), it will print
    out a lot of debugging information.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`VERBOSE_DEBUG` 是你将在代码中多个地方使用的一个标志。现在，你将其设置为 `False`，但如果设置为 `True`（通过命令行选项），它将输出大量调试信息。'
- en: NOTE I’ve omitted the `print()` statements for debugging from the code listings
    that follow. You can find them in the full code listing and on the book’s GitHub
    repository.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我省略了后续代码清单中的 `print()` 调试语句。你可以在完整代码清单和书籍的 GitHub 仓库中找到它们。
- en: The next global variables are for working with the audio input. `CHUNK` sets
    the number of data samples read at a time using `PyAudio`, and `FORMAT` specifies
    that the audio data will consist of 16-bit integers. You set `CHANNELS` to `1`,
    since you’ll be using a single-channel microphone, and `SAMPLE_RATE` to `16000`
    for consistency with the ML training data. `RECORD_SECONDS` indicates that you’ll
    be grouping the audio into one-second increments (which you’ll stitch together
    into overlapping two-second clips, as discussed earlier). You calculate the number
    of chunks in each one-second recording as `NCHUNKS`. You’ll use `ND` and `NDH`
    to implement the overlapping technique—more on that later.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一些用于处理音频输入的全局变量。`CHUNK` 设置每次读取的数据样本数量，使用的是 `PyAudio`，`FORMAT` 指定音频数据将由 16
    位整数构成。你将 `CHANNELS` 设置为 `1`，因为你将使用单通道麦克风，并将 `SAMPLE_RATE` 设置为 `16000`，以确保与 ML
    训练数据的一致性。`RECORD_SECONDS` 表示你将音频分为每秒一段（稍后会将其拼接成重叠的两秒片段，如前所述）。你将计算每秒录音中的块数，即 `NCHUNKS`。你将使用
    `ND` 和 `NDH` 来实现重叠技术——稍后会详细讲解。
- en: Finally, you initialize the device index number of the microphone to `-1` ❶.
    You’ll need to update this value at the command line once you know your microphone’s
    index. Here’s a function to help you figure that out. You’ll be able to call this
    function as a command line option.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将麦克风的设备索引初始化为`-1` ❶。一旦知道麦克风的索引，你需要在命令行中更新该值。这里有一个函数可以帮助你找出麦克风的索引。你可以将这个函数作为命令行选项调用。
- en: 'def list_devices():'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 'def list_devices():'
- en: '"""list pyaudio devices"""'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '"""列出 pyaudio 设备"""'
- en: '# initialize pyaudio'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '# 初始化 pyaudio'
- en: ❶ p = pyaudio.PyAudio()
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ p = pyaudio.PyAudio()
- en: '# get device list'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取设备列表'
- en: index = None
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: index = None
- en: ❷ nDevices = p.get_device_count()
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ nDevices = p.get_device_count()
- en: print('\naudioml.py:\nFound the following input devices:')
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: print('\naudioml.py:\n发现以下输入设备：')
- en: '❸ for i in range(nDevices):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ for i in range(nDevices):'
- en: deviceInfo = p.get_device_info_by_index(i)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: deviceInfo = p.get_device_info_by_index(i)
- en: 'if deviceInfo[''maxInputChannels''] > 0:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 'if deviceInfo[''maxInputChannels''] > 0:'
- en: print(deviceInfo['index'], deviceInfo['name'],
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: print(deviceInfo['index'], deviceInfo['name'],
- en: deviceInfo['defaultSampleRate'])
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: deviceInfo['defaultSampleRate'])
- en: '# clean up'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '# 清理工作'
- en: ❹ p.terminate()
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ p.terminate()
- en: You initialize `PyAudio` ❶ and get a count of the audio devices it detects ❷.
    Then you iterate through the devices ❸. For each one, you retrieve information
    about the device using `get_device_info_by_index()` and print out devices with
    one or more input channels—that is, microphones. You finish by cleaning up `PyAudio`
    ❹.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你初始化了`PyAudio`❶并获取它检测到的音频设备数量❷。然后你遍历设备❸。对于每个设备，你使用`get_device_info_by_index()`获取设备信息，并打印出具有一个或多个输入通道的设备——即麦克风。最后，你清理`PyAudio`❹。
- en: 'Here’s what a typical output of the function looks like:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是该函数的典型输出：
- en: 'audioml.py:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 'audioml.py:'
- en: 'Found the following input devices:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 找到了以下输入设备：
- en: '1 Mico: USB Audio (hw:3,0) 16000.0'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '1 Mico: USB Audio (hw:3,0) 16000.0'
- en: This indicates there’s an input device called Mico with a default sample rate
    of 16,000 and an index of `1`.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示有一个名为Mico的输入设备，默认采样率为16,000，索引为`1`。
- en: Taking In Audio Data
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 接收音频数据
- en: 'One of the main tasks for the Pi is to continuously take in the audio input
    from the microphone and break it up into clips that you can run inference on.
    You create a `get_live_input()` function for this purpose. It takes in the `interpreter`
    object needed to work with the TensorFlow Lite model. Here’s the start of the
    function:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Pi的主要任务之一是持续地从麦克风接收音频输入，并将其分割成可以进行推理的片段。为此，你创建了一个`get_live_input()`函数。该函数接收一个`interpreter`对象，用于与TensorFlow
    Lite模型进行交互。下面是该函数的开始部分：
- en: 'def get_live_input(interpreter):'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 'def get_live_input(interpreter):'
- en: '# create a queue object'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '# 创建队列对象'
- en: ❶ dataq = Queue()
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ dataq = Queue()
- en: '# start inference process'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '# 启动推理进程'
- en: ❷ proc = Process(target = inference_process, args=(dataq, interpreter))
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ proc = Process(target = inference_process, args=(dataq, interpreter))
- en: proc.start()
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: proc.start()
- en: As we discussed in [“How It Works,”](nsp-venkitachalam503045-0030.xhtml#ah1702)
    you’ll need to use separate processes for reading the audio data and doing the
    inference to avoid missing any input. You create a `multiprocessing.``Queue` object
    that the processes will use to communicate with each other ❶. Then you create
    the inference process using `multiprocessing.``Process()` ❷. You specify the name
    of the handler function for the process as `inference_process`, which takes the
    `dataq` and `interpreter` objects as arguments (we’ll view this function later).
    You next start the process so the inference will run parallel to the data capture.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“工作原理”](nsp-venkitachalam503045-0030.xhtml#ah1702)中讨论的那样，为了避免漏掉任何输入，你需要使用不同的进程来读取音频数据和进行推理。你创建了一个`multiprocessing.Queue`对象，供这些进程之间进行通信❶。然后你创建了推理进程，使用`multiprocessing.Process()`❷。你指定了进程的处理函数为`inference_process`，该函数以`dataq`和`interpreter`对象作为参数（稍后我们将查看此函数）。接下来，你启动进程，使推理与数据捕获并行运行。
- en: You continue the `get_live_input()` function by initializing `PyAudio:`
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 你继续`get_live_input()`函数，初始化`PyAudio`：
- en: '# initialize pyaudio'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '# 初始化pyaudio'
- en: ❶ p = pyaudio.PyAudio()
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ p = pyaudio.PyAudio()
- en: print('opening stream...')
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: print('正在打开流...')
- en: ❷ stream = p.open(format = FORMAT,
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ stream = p.open(format = FORMAT,
- en: channels = CHANNELS,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: channels = CHANNELS,
- en: rate = SAMPLE_RATE,
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: rate = SAMPLE_RATE,
- en: input = True,
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: input = True,
- en: frames_per_buffer = CHUNK,
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: frames_per_buffer = CHUNK,
- en: input_device_index = dev_index)
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: input_device_index = dev_index)
- en: '# discard first 1 second'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '# 丢弃前1秒'
- en: '❸ for i in range(0, NCHUNKS):'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ for i in range(0, NCHUNKS):'
- en: data = stream.read(CHUNK, exception_on_overflow = False)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: data = stream.read(CHUNK, exception_on_overflow = False)
- en: You create a `PyAudio` object `p` ❶ and open an audio input stream ❷, using
    some of your global variables as parameters. Then you discard the first one second
    of data ❸. This is to disregard any spurious data that comes in when the microphone
    is enabled for the first time.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了一个`PyAudio`对象`p`❶并打开一个音频输入流❷，使用一些全局变量作为参数。然后，你丢弃了前1秒的数据❸。这样做是为了忽略麦克风第一次启用时可能出现的无效数据。
- en: 'Now you’re ready to start reading the data:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你准备好开始读取数据了：
- en: '# count for gathering two frames at a time'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '# 计数，用于每次收集两个帧'
- en: ❶ count = 0
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ count = 0
- en: ❷ inference_data = np.zeros((ND,), dtype=np.int16)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ inference_data = np.zeros((ND,), dtype=np.int16)
- en: print("Listening...")
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: print("正在监听...")
- en: 'try:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 'try:'
- en: '❸ while True:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ while True:'
- en: chunks = []
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: chunks = []
- en: '❹ for i in range(0, NCHUNKS):'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ for i in range(0, NCHUNKS):'
- en: data = stream.read(CHUNK, exception_on_overflow = False)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: data = stream.read(CHUNK, exception_on_overflow = False)
- en: chunks.append(data)
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: chunks.append(data)
- en: '# process data'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '# 处理数据'
- en: buffer = b''.join(chunks)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: buffer = b''.join(chunks)
- en: ❺ audio_data = np.frombuffer(buffer, dtype=np.int16)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ audio_data = np.frombuffer(buffer, dtype=np.int16)
- en: You initialize `count` to `0` ❶. You’ll use this variable to keep track of the
    number of one-second frames of audio data read in. Then you initialize a 16-bit
    array `inference_data` with zeros ❷. It has `ND` elements, which corresponds to
    two seconds of audio. You next enter a `while` loop to process the audio data
    continuously ❸. In it, you use a `for` loop ❹ to read in one second of audio data,
    one chunk at a time, appending those chunks to the list `chunks`. Once you have
    a full second of data, you convert it into a `numpy` array ❺.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 你将`count`初始化为`0` ❶。你将使用这个变量来跟踪读取的每一秒音频数据的数量。接下来，你初始化一个全为零的16位数组`inference_data`
    ❷。它有`ND`个元素，对应两秒钟的音频数据。然后你进入一个`while`循环，持续处理音频数据 ❸。在这个循环中，你使用一个`for`循环 ❹，每次读取一秒钟的音频数据，一次处理一个数据块，并将这些数据块附加到`chunks`列表中。一旦你得到完整的一秒数据，就将其转换成一个`numpy`数组
    ❺。
- en: Next, still within the `while` loop started in the previous listing, you implement
    the technique we discussed in [“How It Works”](nsp-venkitachalam503045-0030.xhtml#ah1702)
    to create overlapping two-second audio clips. You get help from your `NDH` global
    variable.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，仍然在之前列出的`while`循环内，你实现了我们在[“它是如何工作的”](nsp-venkitachalam503045-0030.xhtml#ah1702)中讨论的技术，创建了重叠的两秒音频片段。你从全局变量`NDH`中获取帮助。
- en: 'if count == 0:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 'if count == 0:'
- en: '# set first half'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置前半部分'
- en: ❶ inference_data[:NDH] = audio_data
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ inference_data[:NDH] = audio_data
- en: count += 1
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: count += 1
- en: 'elif count == 1:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif count == 1:'
- en: '# set second half'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置后半部分'
- en: ❷ inference_data[NDH:] = audio_data
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ inference_data[NDH:] = audio_data
- en: '# add data to queue'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将数据添加到队列'
- en: ❸ dataq.put(inference_data)
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ dataq.put(inference_data)
- en: count += 1
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: count += 1
- en: 'else:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: '# move second half to first half'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将后半部分移动到前半部分'
- en: ❹ inference_data[:NDH] = inference_data[NDH:]
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ inference_data[:NDH] = inference_data[NDH:]
- en: '# set second half'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置后半部分'
- en: ❺ inference_data[NDH:] = audio_data
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ inference_data[NDH:] = audio_data
- en: '# add data to queue'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将数据添加到队列'
- en: ❻ dataq.put(inference_data)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ dataq.put(inference_data)
- en: The very first time a one-second frame is read in, it’s stored in the first
    half of `inference_data` ❶. The next frame that comes in is stored in the second
    half of `inference_data` ❷. Now you have a full two seconds of audio data, so
    you put `inference_data` into the queue for the inference process to pick it up
    ❸. For every subsequent frame, the second half of the data is moved to the first
    half of `inference_data` ❹, the new data is set to the second half ❺, and `inference_data`
    is added to the queue ❻. This creates the desired one-second overlap between each
    consecutive two-second audio clip.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次读取一秒钟的音频帧时，将其存储在`inference_data`的前半部分 ❶。接下来的帧存储在`inference_data`的后半部分 ❷。现在你有了完整的两秒钟音频数据，因此将`inference_data`放入队列，供推理过程提取
    ❸。对于随后的每一帧，后半部分的数据将被移动到`inference_data`的前半部分 ❹，新数据会设置到后半部分 ❺，然后`inference_data`会被加入队列
    ❻。这样就形成了每个连续的两秒音频片段之间的一秒重叠。
- en: 'The `while` loop occurs inside a `try` block. To exit the loop, you just need
    to press CTRL-C and trigger the following `except` block:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环发生在`try`块内部。要退出循环，只需按CTRL-C并触发以下`except`块：'
- en: 'except KeyboardInterrupt:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 'except KeyboardInterrupt:'
- en: print("exiting...")
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: print("正在退出...")
- en: stream.stop_stream()
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: stream.stop_stream()
- en: stream.close()
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: stream.close()
- en: p.terminate()
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: p.terminate()
- en: This `except` block performs some basic cleanup by stopping and closing the
    stream and by terminating `PyAudio`.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`except`块执行一些基本的清理操作，停止并关闭流，并终止`PyAudio`。
- en: Preparing the Audio Data
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备音频数据
- en: 'Next, you’ll create a few functions to prepare the audio data for inference.
    First is `process_audio_data()`, which takes in a raw two-second clip of audio
    data pulled from the queue and extracts the most interesting one second of audio
    from it, based on peak amplitude. We’ll look at this function across several listings:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将创建几个函数来准备音频数据进行推理。第一个是`process_audio_data()`，它接收从队列中提取的原始两秒音频数据，并根据峰值幅度提取出最有趣的一秒音频。我们将在接下来的几段代码中查看这个函数：
- en: 'def process_audio_data(waveform):'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 'def process_audio_data(waveform):'
- en: '# compute peak to peak based on scaling by max 16-bit value'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '# 通过最大16位值缩放计算峰值-峰值'
- en: ❶ PTP = np.ptp(waveform / 32768.0)
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ PTP = np.ptp(waveform / 32768.0)
- en: '# return None if too silent'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '# 如果过于安静则返回 None'
- en: '❷ if PTP < 0.3:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ if PTP < 0.3:'
- en: return []
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: return []
- en: You want to skip doing any inference on the microphone audio input if nobody
    is talking. There will always be some noise in the environment, however, so you
    can’t simply look for the signal to be 0\. Instead, you’ll skip inference if the
    peak-to-peak amplitude (the difference between the highest value and the lowest
    value) of the audio is below a certain threshold. For this, you first divide the
    audio by `32768` to normalize it to a range of (−1, 1), and you pass the result
    to `np.ptp()` to get the peak-to-peak amplitude ❶. The normalization makes it
    easier to express the threshold as a fraction. You return an empty list (which
    will bypass the inference process) if the peak-to-peak amplitude is below `0.3`
    ❷. You may need to adjust this threshold value depending on the noise level of
    your environment.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有人在说话，你不希望对麦克风音频输入进行推理。然而，环境中总会有一些噪音，因此你不能仅仅查看信号是否为0。相反，如果音频的峰值到峰值幅度（即最高值和最低值之间的差）低于某个阈值，你将跳过推理过程。为此，首先将音频除以`32768`，将其归一化到（−1,
    1）范围内，然后将结果传递给`np.ptp()`以获得峰值到峰值幅度❶。归一化使得可以更容易地将阈值表示为一个分数。如果峰值到峰值幅度低于`0.3`❷，你将返回一个空列表（这将跳过推理过程）。你可能需要根据环境噪音水平调整该阈值。
- en: 'The `process_audio_data()` function continues with another technique for normalizing
    any audio data that won’t be skipped:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_audio_data()` 函数继续使用另一种技术来归一化任何不会被跳过的音频数据：'
- en: '# normalize audio'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '# 音频归一化'
- en: wabs = np.abs(waveform)
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: wabs = np.abs(waveform)
- en: wmax = np.max(wabs)
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: wmax = np.max(wabs)
- en: ❶ waveform = waveform / wmax
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ waveform = waveform / wmax
- en: '# compute peak to peak based on normalized waveform'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '# 基于归一化波形计算峰值到峰值'
- en: ❷ PTP = np.ptp(waveform)
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ PTP = np.ptp(waveform)
- en: '# scale and center'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '# 缩放和居中'
- en: ❸ waveform = 2.0*(waveform - np.min(waveform))/PTP – 1
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ waveform = 2.0*(waveform - np.min(waveform))/PTP – 1
- en: When you normalized the data before skipping quiet audio samples, you divided
    the audio by 32,768, the maximum possible value of a 16-bit signed integer. In
    most cases, however, the peak amplitude of the audio data will be well below this
    value. Now you want to normalize the audio such that its maximum amplitude, whatever
    that may be, is scaled to 1\. To do this, you first determine the peak amplitude
    in the audio signal and then divide the signal by that amplitude value ❶. Then
    you compute the new peak-to-peak value of the normalized audio ❷ and use this
    value to scale and center the data ❸. Specifically, the expression `(waveform
    – np.min(waveform))/PTP` will scale the waveform values to the range (0, 1). Multiplying
    this by 2 and subtracting 1 will put the values in the range (−1, 1), which is
    what you need.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在跳过安静的音频样本之前对数据进行归一化时，你将音频除以了32,768，这是16位有符号整数的最大可能值。然而，在大多数情况下，音频数据的峰值幅度将远低于该值。现在，你希望将音频归一化，使其最大幅度无论是多少，都被缩放到1。为此，你首先确定音频信号的峰值幅度，然后将信号除以该幅度值❶。接着，你计算归一化音频的新的峰值到峰值幅度❷，并使用该值来缩放和居中数据❸。具体来说，表达式`(waveform
    – np.min(waveform))/PTP`将把波形值缩放到（0, 1）范围。将其乘以2并减去1将把值放置在（−1, 1）范围内，这是你需要的。
- en: 'The next part of the function extracts one second of audio from the data:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的下一部分从数据中提取1秒钟的音频：
- en: '# extract 16000 len (1 second) of data'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '# 提取16000个采样点（1秒）数据'
- en: ❶ max_index = np.argmax(waveform)
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ max_index = np.argmax(waveform)
- en: ❷ start_index = max(0, max_index-8000)
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ start_index = max(0, max_index-8000)
- en: ❸ end_index = min(max_index+8000, waveform.shape[0])
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ end_index = min(max_index+8000, waveform.shape[0])
- en: ❹ waveform = waveform[start_index:end_index]
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ waveform = waveform[start_index:end_index]
- en: '# padding for files with less than 16000 samples'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '# 对于少于16000个采样点的文件进行填充'
- en: waveform_padded = np.zeros((16000,))
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: waveform_padded = np.zeros((16000,))
- en: waveform_padded[:waveform.shape[0]] = waveform
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: waveform_padded[:waveform.shape[0]] = waveform
- en: return waveform_padded
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: return waveform_padded
- en: You want to make sure you’re getting the most interesting one second of the
    data, so you find the array index where the audio amplitude is at the maximum
    ❶. Then you try to grab 8,000 values before ❷ and after ❸ this index to get a
    full second of data, using `max()` and `min()` to ensure that the start and end
    indices don’t fall out of range of the original clip. You use slicing to extract
    the relevant audio data ❹. Because of the `max()` and `min()` operations, you
    may end up with less than 16,000 samples, but the neural network strictly requires
    each input to be 16,000 samples long. To address this problem, you pad the data
    with zeros, using the same `numpy` techniques you saw during training. Then you
    return the result.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要确保你获取的是数据中最有趣的那一秒，所以你找到音频幅度最大的位置索引❶。然后，你尝试在这个索引前❷和后❸各抓取8,000个值，以获取完整的一秒数据，并使用`max()`和`min()`来确保起始和结束索引不会超出原始音频的范围。你使用切片来提取相关的音频数据❹。由于`max()`和`min()`操作，最终你可能会得到少于16,000个样本，但神经网络严格要求每个输入必须是16,000个样本长。为了解决这个问题，你用零填充数据，采用你在训练中看到的相同`numpy`技巧。然后，你返回结果。
- en: '[Figure 15-7](nsp-venkitachalam503045-0030.xhtml#fig15-7) summarizes the `process_audio_data()`
    function by showing an example waveform at the various stages of processing.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-7](nsp-venkitachalam503045-0030.xhtml#fig15-7)通过展示在不同处理阶段的示例波形，总结了`process_audio_data()`函数。'
- en: '![](images/nsp-venkitachalam503045-f15007.jpg)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](images/nsp-venkitachalam503045-f15007.jpg)'
- en: 'Figure 15-7: The audio preparation process at various stages'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15-7：音频准备过程的不同阶段
- en: The top waveform in [Figure 15-7](nsp-venkitachalam503045-0030.xhtml#fig15-7)
    shows the unprocessed audio. The second waveform shows the audio with the values
    normalized to range (−1, 1). The third waveform shows the audio after a shift
    and scale—notice on the y-axis how the waveform now fills the entire (−1, 1) range.
    The fourth waveform consists of 16,000 samples extracted from the third one, centered
    on the peak amplitude.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15-7](nsp-venkitachalam503045-0030.xhtml#fig15-7)中的上方波形展示了未经处理的音频。第二个波形展示了音频经过归一化处理后的值，范围为（-1，1）。第三个波形展示了音频经过平移和缩放后的效果—注意在y轴上，波形现在填满了整个（-1，1）的范围。第四个波形由从第三个波形中提取的16,000个样本组成，中心在最大幅度的位置。'
- en: 'Next, you need a `get_spectrogram()` function for computing the spectrogram
    of the audio data:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要一个`get_spectrogram()`函数来计算音频数据的频谱图：
- en: 'def get_spectrogram(waveform):'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 定义get_spectrogram(waveform)函数：
- en: ❶ waveform_padded = process_audio_data(waveform)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ waveform_padded = process_audio_data(waveform)
- en: '❷ if not len(waveform_padded):'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果波形数据为空：
- en: return []
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: return []
- en: '# compute spectrogram'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '# 计算频谱图'
- en: ❸ f, t, Zxx = signal.stft(waveform_padded, fs=16000, nperseg=255,
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ f, t, Zxx = signal.stft(waveform_padded, fs=16000, nperseg=255,
- en: noverlap = 124, nfft=256)
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: noverlap = 124, nfft=256)
- en: '# output is complex, so take abs value'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '# 输出是复数值，所以取绝对值'
- en: ❹ spectrogram = np.abs(Zxx)
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ spectrogram = np.abs(Zxx)
- en: return spectrogram
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: return spectrogram
- en: You call your `process_audio_data()` function to prepare the audio ❶. If the
    function returns an empty list (because the audio is too quiet), `get_spectrogram()`
    returns an empty list as well ❷. Next, you compute the spectrogram with `signal`.`stft()`
    from `scipy`, exactly as you did when training the model ❸. You then calculate
    the absolute value of the STFT ❹ to convert from complex numbers—again, as you
    did during training—and return the result.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 你调用`process_audio_data()`函数来准备音频❶。如果该函数返回一个空列表（因为音频太安静），`get_spectrogram()`也会返回一个空列表❷。接下来，你使用`scipy`中的`signal.stft()`计算音频的频谱图，正如你在训练模型时做的那样❸。然后你计算STFT的绝对值❹，以从复数转换为实数—这也是你在训练时做的，并返回结果。
- en: Running Inference
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行推理
- en: 'The heart of this project is using your trained model to run inference on the
    incoming audio data and identify any spoken commands. Recall that this occurs
    in a separate process from the code for taking in audio data from the microphone.
    Here’s the handler function that coordinates this process:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的核心是使用你训练好的模型对传入的音频数据进行推理，并识别出任何语音命令。回想一下，这个过程与从麦克风获取音频数据的代码是分开的。下面是协调这一过程的处理函数：
- en: 'def inference_process(dataq, interpreter):'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 'def inference_process(dataq, interpreter):'
- en: success = False
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: success = False
- en: 'while True:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 'while True:'
- en: '❶ if not dataq.empty():'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果数据队列不为空：
- en: '# get data from queue'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '# 从队列获取数据'
- en: ❷ inference_data = dataq.get()
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ inference_data = dataq.get()
- en: '# run inference only if previous one was not successful'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '# 只有在上一个处理不成功时才运行推理'
- en: '❸ if not success:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果处理不成功：
- en: success = run_inference(inference_data, interpreter)
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: success = run_inference(inference_data, interpreter)
- en: 'else:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: '# skipping, reset flag for next time'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '# 跳过，重置标志以便下次使用'
- en: ❹ success = False
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ success = False
- en: The inference process runs continuously inside a `while`. Within this loop,
    you check if there’s any data in the queue ❶, and if so, you retrieve it ❷. Then
    you run inference on it with the `run_inference()` function, which we’ll look
    at next, but only if the `success` flag is `False` ❸. This flag keeps you from
    responding to the same speech command twice. Recall that because of the overlap
    technique, the second half of one audio clip will be repeated as the first half
    of the next clip. This lets you catch any audio commands that might be split across
    two frames, but it means that once you have a successful inference, you should
    skip the next element in the queue because it will have a portion of the audio
    from the previous element. When you do a skip like this, you reset `success` to
    `False` ❹ to start running inference again on the next piece of data that comes
    in.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程在`while`循环内持续运行。在这个循环中，你会检查队列中是否有数据 ❶，如果有，你就将其取出 ❷。然后，你使用`run_inference()`函数对其进行推理，我们接下来会看这个函数，但只有在`success`标志为`False`时
    ❸，才会进行推理。这个标志防止你对同一个语音命令作出两次响应。回想一下，由于重叠技术的使用，一个音频片段的后半部分会在下一个片段中作为前半部分重复出现。这允许你捕捉到可能跨越两个帧的音频命令，但这也意味着，一旦推理成功，你应该跳过队列中的下一个元素，因为它包含了前一个元素的一部分音频。当你进行这样的跳过时，你需要将`success`重置为`False`
    ❹，以便重新开始对下一个数据片段进行推理。
- en: 'Now let’s look at the `run_inference()` function, where the inference is actually
    carried out:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下`run_inference()`函数，推理实际上是在这个函数中进行的：
- en: 'def run_inference(waveform, interpreter):'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run_inference(waveform, interpreter):'
- en: '# get spectrogram data'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取谱图数据'
- en: ❶ spectrogram = get_spectrogram(waveform)
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ spectrogram = get_spectrogram(waveform)
- en: 'if not len(spectrogram):'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not len(spectrogram):'
- en: return False
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: return False
- en: '# get input and output tensors details'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取输入和输出张量的详细信息'
- en: ❷ input_details = interpreter.get_input_details()
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ input_details = interpreter.get_input_details()
- en: ❸ output_details = interpreter.get_output_details()
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ output_details = interpreter.get_output_details()
- en: 'The function takes in the raw audio data (`waveform`) for interacting with
    your TensorFlow Lite model (`interpreter`). You call `get_spectrogram()` to process
    the audio and generate the spectrogram ❶, and if the audio was too quiet, you
    return `False`. Then you get the input ❷ and output ❸ details from the TensorFlow
    Lite interpreter. These tell you what the model is expecting as input and what
    you can expect from it as output. This is what `input_details` looks like:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受原始音频数据（`waveform`），以与TensorFlow Lite模型（`interpreter`）进行交互。你调用`get_spectrogram()`来处理音频并生成谱图
    ❶，如果音频太安静，则返回`False`。然后，你从TensorFlow Lite解释器获取输入 ❷ 和输出 ❸ 详细信息。这些信息告诉你模型期望的输入是什么，以及你可以期待的输出是什么。这就是`input_details`的样子：
- en: '[{''name'': ''serving_default_input_5:0'', ''index'': 0, ''shape'': array([1,
    129, 124,   1]),'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[{''name'': ''serving_default_input_5:0'', ''index'': 0, ''shape'': array([1,
    129, 124,   1]),'
- en: '''shape_signature'': array([ -1, 129, 124,   1]), ''dtype'': <class ''numpy.float32''>,'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '''shape_signature'': array([ -1, 129, 124,   1]), ''dtype'': <class ''numpy.float32''>,'
- en: '''quantization'': (0.0, 0), ''quantization_parameters'': {''scales'': array([],
    dtype=float32),'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '''quantization'': (0.0, 0), ''quantization_parameters'': {''scales'': array([],
    dtype=float32),'
- en: '''zero_points'': array([], dtype=int32), ''quantized_dimension'': 0}, ''sparsity_parameters'':
    {}}]'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '''zero_points'': array([], dtype=int32), ''quantized_dimension'': 0}, ''sparsity_parameters'':
    {}}]'
- en: 'Notice that `input_details` is an array with a dictionary inside it. The `''shape''`
    entry is especially of interest: `array([1, 129, 124, 1])`. You’ve already ensured
    that your spectrogram, which will be the input to the interpreter, is shaped to
    this value. The `''index''` entry is just the index of the tensor in the tensor
    list inside the interpreter, and `''dtype''` is the expected data type of the
    input, which in this case is `float32`, a signed 32-bit float. You’ll need to
    reference both `''index''` and `''dtype''` later in the `run_inference()` function.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`input_details`是一个包含字典的数组。`'shape'`条目尤其重要：`array([1, 129, 124, 1])`。你已经确保了你的谱图（将作为输入传递给解释器）的形状符合这个值。`'index'`条目只是张量在解释器内部张量列表中的索引，`'dtype'`是输入的预期数据类型，在本例中是`float32`，即一个带符号的32位浮动点数。你稍后将在`run_inference()`函数中引用`'index'`和`'dtype'`。
- en: 'Here’s `output_details`:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`output_details`：
- en: '[{''name'': ''StatefulPartitionedCall:0'', ''index'': 17, ''shape'': array([1,
    8]), ''shape_signature'':'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[{''name'': ''StatefulPartitionedCall:0'', ''index'': 17, ''shape'': array([1,
    8]), ''shape_signature'':'
- en: 'array([-1,  8]), ''dtype'': <class ''numpy.float32''>, ''quantization'': (0.0,
    0),'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 'array([-1,  8]), ''dtype'': <class ''numpy.float32''>, ''quantization'': (0.0,
    0),'
- en: '''quantization_parameters'': {''scales'': array([], dtype=float32), ''zero_points'':'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '''quantization_parameters'': {''scales'': array([], dtype=float32), ''zero_points'':'
- en: 'array([], dtype=int32), ''quantized_dimension'': 0}, ''sparsity_parameters'':
    {}}]'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 'array([], dtype=int32), ''quantized_dimension'': 0}, ''sparsity_parameters'':
    {}}]'
- en: Notice the `'shape'` entry in this dictionary. It shows that the output will
    be an array of shape (1, 8). The shape corresponds to the label IDs of the eight
    speech commands.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这个字典中的`'shape'`条目。它显示输出将是一个形状为(1, 8)的数组。该形状对应于八个语音命令的标签ID。
- en: 'You continue the `run_inference()` function by actually running inference on
    the input data:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过实际运行推理来继续`run_inference()`函数：
- en: '# set input'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置输入'
- en: ❶ input_data = spectrogram.astype(np.float32)
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ input_data = spectrogram.astype(np.float32)
- en: ❷ interpreter.set_tensor(input_details[0]['index'], input_data)
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ interpreter.set_tensor(input_details[0]['index'], input_data)
- en: '# run interpreter'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '# 运行解释器'
- en: print("running inference...")
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: print("正在运行推理...")
- en: ❸ interpreter.invoke()
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ interpreter.invoke()
- en: '# get output'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取输出'
- en: ❹ output_data = interpreter.get_tensor(output_details[0]['index'])
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ output_data = interpreter.get_tensor(output_details[0]['index'])
- en: ❺ yvals = output_data[0]
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ yvals = output_data[0]
- en: print(yvals)
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: print(yvals)
- en: 'First you convert the spectrogram data to 32-bit floating point values ❶. Recall
    that your audio data started as 16-bit integers. The scaling and other processing
    operations converted the data to 64-bit floats, but as you saw in `input_details`,
    the TensorFlow Lite model requires 32-bit floats, which is the reason for the
    conversion. You next set the input value to the appropriate tensor inside the
    interpreter ❷. Here the `[0]` accesses the first (and only) element in `input_details`,
    which as you saw is a dictionary, and `[''index'']` retrieves the value under
    that key in the dictionary to specify which tensor you’re setting. You run inference
    on the input using the `invoke()` method ❸. Then you retrieve the output tensor
    using similar indexing to the input ❹ and get the output itself by extracting
    the first element from the `output_data` array ❺. (Since you provided only one
    input, you expect only one output.) Here’s an example of what `yvals` looks like:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将谱图数据转换为32位浮点值❶。回想一下，你的音频数据最初是16位整数。缩放和其他处理操作将数据转换为64位浮动数，但正如你在`input_details`中看到的，TensorFlow
    Lite模型要求使用32位浮点数，这就是转换的原因。接下来，你将输入值设置为解释器内部适当的张量❷。这里的`[0]`访问`input_details`中的第一个（也是唯一一个）元素，它是一个字典，`['index']`从字典中获取该键下的值，以指定你要设置的张量。你使用`invoke()`方法对输入进行推理❸。然后，你使用类似的索引方式从`output_data`数组中检索输出张量❹，并通过提取`output_data`数组中的第一个元素来获取输出本身❺。（由于你只提供了一个输入，因此预计只有一个输出。）这是`yvals`的一个示例：
- en: '[  6.640185  -26.032831  -26.028618  8.746256  62.545185  -0.5698182  -15.045679  -29.140179 ]'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 6.640185 -26.032831 -26.028618 8.746256 62.545185 -0.5698182 -15.045679 -29.140179
    ]'
- en: 'These eight numbers correspond to the eight commands you trained the model
    with. The values indicate the likelihood of the input data being each word. In
    this particular array, the value at index `4` is by far the largest, so that’s
    what the neural network is predicting as the most probable answer. Here’s how
    you interpret the result:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 这八个数字对应于你用来训练模型的八个命令。这些值表示输入数据为每个词的可能性。在这个特定的数组中，索引`4`处的值最大，因此神经网络预测这是最可能的答案。以下是如何解释结果：
- en: '# Important! This should exactly match training labels/ids.'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '# 重要！这应该与训练标签/ID完全匹配。'
- en: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
- en: print(">>> " + commands[np.argmax(output_data[0])].upper())
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: print(">>> " + commands[np.argmax(output_data[0])].upper())
- en: You define a `commands` list in the same order as you used during training.
    It’s important to keep the order consistent across training and inference, or
    you’ll end up misinterpreting the results! Then you use `np.``argmax()` to get
    the index of the highest value in the output data and use that index to pick up
    the corresponding string from `commands`.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 你定义了一个`commands`列表，顺序与训练时使用的顺序相同。保持训练和推理中的顺序一致非常重要，否则你可能会误解结果！然后，你使用`np.argmax()`来获取输出数据中最大值的索引，并利用该索引从`commands`中获取相应的字符串。
- en: Writing the main() Function
  id: totrans-555
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编写`main()`函数
- en: 'Now let’s look at the `main()` function, which brings everything together:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看`main()`函数，它将所有内容整合在一起：
- en: 'def main():'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: '# globals set in this function'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '# 此函数中设置的全局变量'
- en: global VERBOSE_DEBUG
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: global VERBOSE_DEBUG
- en: '# create parser'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '# 创建解析器'
- en: descStr = "This program does ML inference on audio data."
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: descStr = "这个程序对音频数据进行机器学习推理。"
- en: parser = argparse.ArgumentParser(description=descStr)
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: parser = argparse.ArgumentParser(description=descStr)
- en: '# add a mutually exclusive group'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '# 添加一个互斥组'
- en: ❶ group = parser.add_mutually_exclusive_group(required=True)
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ group = parser.add_mutually_exclusive_group(required=True)
- en: '# add mutually exclusive arguments'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '# 添加互斥参数'
- en: ❷ group.add_argument('--list', action='store_true', required=False)
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ group.add_argument('--list', action='store_true', required=False)
- en: ❸ group.add_argument('--input', dest='wavfile_name', required=False)
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ group.add_argument('--input', dest='wavfile_name', required=False)
- en: ❹ group.add_argument('--index', dest='index', required=False)
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ group.add_argument('--index', dest='index', required=False)
- en: '# add other arguments'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '# 添加其他参数'
- en: ❺ parser.add_argument('--verbose', action='store_true', required=False)
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ parser.add_argument('--verbose', action='store_true', required=False)
- en: '# parse args'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '# 解析参数'
- en: args = parser.parse_args()
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: args = parser.parse_args()
- en: You start by setting `VERBOSE_DEBUG` as a global, since you’ll be setting it
    in this function and don’t want it to be treated as a local variable. Then you
    create a familiar `argparse.ArgumentParser` object and add a mutually exclusive
    group to the parser ❶, since some of your command line options won’t be compatible
    with each other. Those are the `--list` option ❷, which will list all the `PyAudio`
    devices so you can get your microphone’s index number; the `--input` option ❸,
    which lets you specify a WAV file to use as input instead of live data from the
    microphone (useful for testing); and the `--index` option ❹, which starts capturing
    audio and running inference using the microphone with the specified index. You
    also add the non–mutually exclusive `--verbose` option ❺ to print out detailed
    debug information as the program is run.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先设置 `VERBOSE_DEBUG` 为全局变量，因为你将在此函数中设置它，不希望它被当作局部变量。然后你创建一个常见的 `argparse.ArgumentParser`
    对象，并为解析器添加一个互斥组 ❶，因为你的某些命令行选项不能相互兼容。包括 `--list` 选项 ❷，该选项列出所有 `PyAudio` 设备，以便你可以获取麦克风的索引号；`--input`
    选项 ❸，该选项允许你指定一个 WAV 文件作为输入，而不是来自麦克风的实时数据（用于测试）；以及 `--index` 选项 ❹，该选项启动音频捕捉并使用指定的麦克风索引进行推理。你还添加了一个非互斥的
    `--verbose` 选项 ❺，它会在程序运行时打印详细的调试信息。
- en: 'Next, you create the TensorFlow Lite interpreter so you can use the ML model:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你创建 TensorFlow Lite 解释器，以便可以使用 ML 模型：
- en: '# load TF Lite model'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '# 加载 TF Lite 模型'
- en: interpreter = Interpreter('audioml.tflite')
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: interpreter = Interpreter('audioml.tflite')
- en: interpreter.allocate_tensors()
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: interpreter.allocate_tensors()
- en: Here you create an `Interpreter` object, passing it the *audioml.tflite* file
    with the model you created during training. Then you call `allocate_tensors()`
    to prepare the necessary tensors for running the inference.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你创建一个 `Interpreter` 对象，传递给它包含你在训练期间创建的模型的 *audioml.tflite* 文件。然后，你调用 `allocate_tensors()`
    准备运行推理所需的张量。
- en: 'The `main()` function finishes with branches for the different command line
    arguments:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`函数根据不同的命令行参数完成分支：'
- en: '# check verbose flag'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '# 检查详细标志'
- en: 'if args.verbose:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 'if args.verbose:'
- en: VERBOSE_DEBUG = True
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: VERBOSE_DEBUG = True
- en: '# test WAV file'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '# 测试 WAV 文件'
- en: 'if args.wavfile_name:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 'if args.wavfile_name:'
- en: ❶ wavfile_name = args.wavfile_name
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ wavfile_name = args.wavfile_name
- en: '# get audio data'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取音频数据'
- en: ❷ rate, waveform = wavfile.read(wavfile_name)
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ rate, waveform = wavfile.read(wavfile_name)
- en: '# run inference'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '# 运行推理'
- en: ❸ run_inference(waveform, interpreter)
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ run_inference(waveform, interpreter)
- en: 'elif args.list:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif args.list:'
- en: '# list devices'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '# 列出设备'
- en: ❹ list_devices()
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ list_devices()
- en: 'else:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: '# store device index'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '# 存储设备索引'
- en: ❺ dev_index = int(args.index)
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ dev_index = int(args.index)
- en: '# get live audio'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取实时音频'
- en: ❻ get_live_input(interpreter)
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ get_live_input(interpreter)
- en: print("done.")
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: print("完成。")
- en: If the `--input` command line option is used, you get the name of the WAV file
    ❶ and read its contents ❷. The resulting data is passed along for inference ❸.
    If the `--list` option is used, you call your `list_devices()` function ❹. If
    the `--index` option is used, you parse the device index ❺ and start processing
    live audio by calling the `get_live_input()` function ❻.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用了 `--input` 命令行选项，你将获得 WAV 文件的名称 ❶ 并读取其内容 ❷。结果数据将传递进行推理 ❸。如果使用了 `--list`
    选项，你将调用 `list_devices()` 函数 ❹。如果使用了 `--index` 选项，你将解析设备索引 ❺ 并通过调用 `get_live_input()`
    函数 ❻ 开始处理实时音频。
- en: '[Running the Speech Recognition System](nsp-venkitachalam503045-0008.xhtml#rah1705)'
  id: totrans-600
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[运行语音识别系统](nsp-venkitachalam503045-0008.xhtml#rah1705)'
- en: To run the project, gather your Python code and the *audioml.tflite* file into
    a folder on your Pi. For testing, you can also download the *right.wav* file from
    the book’s GitHub repository and add that to the folder. You can work with your
    Pi via SSH, as explained in [Appendix B](nsp-venkitachalam503045-0032.xhtml#appb).
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行该项目，将你的 Python 代码和 *audioml.tflite* 文件放入 Pi 上的一个文件夹中。为了测试，你也可以从书籍的 GitHub
    仓库下载 *right.wav* 文件并将其添加到文件夹中。你可以通过 SSH 操作 Pi，具体操作请参考 [附录 B](nsp-venkitachalam503045-0032.xhtml#appb)。
- en: 'First, try using the `--input` command line option to run inference on a WAV
    file:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尝试使用 `--input` 命令行选项对 WAV 文件进行推理：
- en: $ `sudo python audioml.py --input right.wav`
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: $ `sudo python audioml.py --input right.wav`
- en: 'Here’s the output:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: running inference...
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 正在运行推理...
- en: '[  6.640185  -26.032831  -26.028618    8.746256   62.545185   -0.5698182'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '[  6.640185  -26.032831  -26.028618    8.746256   62.545185   -0.5698182'
- en: -15.045679  -29.140179 ]
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: -15.045679  -29.140179 ]
- en: ❶ >>> RIGHT
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ >>> 右
- en: 'run_inference: 0.029174549999879673s'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 'run_inference: 0.029174549999879673秒'
- en: done.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。
- en: Notice that the program has correctly identified the *right* command recorded
    on the WAV file ❶.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，程序已经正确识别了录制在WAV文件中的*right*命令，位置在❶。
- en: 'Now plug your microphone into the Pi and use the `--list` option to determine
    its index number, as shown here:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将你的麦克风插入Pi，并使用`--list`选项来确定其索引号，如下所示：
- en: $ `sudo python audioml.py --list`
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: $ `sudo python audioml.py --list`
- en: 'Your output should be similar to the following:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出应与以下类似：
- en: 'audioml.py:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 'audioml.py:'
- en: 'Found the following input devices:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 找到以下输入设备：
- en: '1 Mico: USB Audio (hw:3,0) 16000.0'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '1 Mico: USB Audio (hw:3,0) 16000.0'
- en: done.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。
- en: 'In this example, the microphone has index `1`. Use that number to run the `--index`
    command to do some live speech detection! Here’s an example run:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，麦克风的索引是`1`。使用这个数字运行`--index`命令来进行实时语音检测！以下是一个示例运行：
- en: $ `sudo python audioml.py --index 1`
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: $ `sudo python audioml.py --index 1`
- en: --`snip`--
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: --`snip`--
- en: opening stream...
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 正在打开流...
- en: Listening...
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 正在监听...
- en: running inference...
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 正在运行推理...
- en: '[-2.647918    0.17592785 -3.3615346   6.6812882   4.472283   -3.7535028'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '[-2.647918    0.17592785 -3.3615346   6.6812882   4.472283   -3.7535028'
- en: 1.2349942   1.8546474 ]
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2349942   1.8546474 ]
- en: ❶ >>> LEFT
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ >>> 左
- en: 'run_inference: 0.03520956500142347s'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 'run_inference: 0.03520956500142347秒'
- en: running inference...
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 正在运行推理...
- en: '[-2.7683923 -5.9614644 -8.532391   6.906795  19.197264  -4.0255833'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[-2.7683923 -5.9614644 -8.532391   6.906795  19.197264  -4.0255833'
- en: 1.7236844 -4.374415 ]
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 1.7236844 -4.374415 ]
- en: ❷ >>> RIGHT
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ >>> 右
- en: 'run_inference: 0.03026762299850816s'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 'run_inference: 0.03026762299850816秒'
- en: --`snip`--
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: --`snip`--
- en: ^C
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: ^C
- en: KeyboardInterrupt
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: KeyboardInterrupt
- en: exiting...
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 正在退出...
- en: done.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。
- en: After starting the program and getting the “Listening . . .” prompt, I spoke
    the words *left* and *right*. The output at ❶ and ❷ indicates that the program
    was able to identify the commands correctly.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动程序并看到“Listening …”提示后，我说出了“*left*”和“*right*”这两个词。❶和❷处的输出表明程序能够正确识别这些命令。
- en: Try running the program with the `--verbose` option to see more information
    about how it’s working. Also, try speaking different commands in quick succession
    to verify whether the multiprocessing and overlapping techniques are working.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用`--verbose`选项运行程序，查看更多关于其工作方式的信息。同时，尝试快速连续地说出不同的命令，以验证多进程和重叠技术是否有效。
- en: '[Summary](nsp-venkitachalam503045-0008.xhtml#rah1706)'
  id: totrans-641
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[总结](nsp-venkitachalam503045-0008.xhtml#rah1706)'
- en: This chapter introduced you to the world of machine learning. You learned how
    to train a deep neural network to recognize speech commands using the TensorFlow
    framework, and you converted the resulting model to a TensorFlow Lite format for
    use on a resource-constrained Raspberry Pi. You also learned about spectrograms
    and the importance of processing input data before ML training. You practiced
    using Python multiprocessing, reading USB microphone input on a Raspberry Pi using
    `PyAudio`, and running a TensorFlow Lite interpreter for ML inference.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向你介绍了机器学习的世界。你学习了如何使用TensorFlow框架训练一个深度神经网络来识别语音命令，并将结果模型转换为TensorFlow Lite格式以在资源受限的树莓派上使用。你还了解了频谱图及其在机器学习训练中处理输入数据的重要性。你实践了使用Python多进程技术，使用`PyAudio`读取树莓派上的USB麦克风输入，以及运行TensorFlow
    Lite解释器进行机器学习推理。
- en: '[Experiments!](nsp-venkitachalam503045-0008.xhtml#rah1707)'
  id: totrans-643
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[实验！](nsp-venkitachalam503045-0008.xhtml#rah1707)'
- en: '1\. Now that you know how to process speech commands on a Raspberry Pi, you
    can build an assistive device that responds to those commands by doing more than
    printing out the identified words. For example, you could use the commands *left*,
    *right*, *up*, *down*, *stop*, and *go* to control a camera (or laser!) mounted
    on a pan/tilt mount. Hint: you''ll need to retrain the ML model with just these
    six commands. You’ll also need to get a two-axis pan/tilt bracket with two servo
    motors attached. The servos will be connected to the Raspberry Pi and controlled
    based on the inference results.'
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1. 现在你知道如何在树莓派上处理语音命令，你可以构建一个响应这些命令的辅助设备，不仅仅是打印出识别到的词。例如，你可以使用*left*、*right*、*up*、*down*、*stop*
    和 *go* 命令来控制安装在云台上的相机（或激光器！）。提示：你需要用这六个命令重新训练机器学习模型。你还需要获取一个带有两个伺服电机的双轴云台支架。伺服电机将连接到树莓派，并根据推理结果进行控制。
- en: 2\. Read about the *mel spectrogram*, a variant of the spectrogram you used
    for this project that’s better suited for human speech data.
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 阅读关于*mel频谱图*的内容，这是一种比你在本项目中使用的频谱图更适合人类语音数据的变体。
- en: 3\. Try modifying the neural network by adding or removing some layers. For
    example, remove the second Conv2D layer. See how the changes affect the training
    accuracy of the model and the inference accuracy on the Pi.
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 尝试通过添加或删除一些层来修改神经网络。例如，删除第二个Conv2D层。观察这些变化如何影响模型的训练准确性以及在Pi上的推理准确性。
- en: 4\. This project used an ad hoc neural network, but there are also pretrained
    neural networks available that you could leverage. For example, read up on MobileNet
    V2\. What changes are needed to adapt your project to use this network instead?
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4\. 这个项目使用了一个临时神经网络，但也有可用的预训练神经网络，你可以利用它们。例如，了解MobileNet V2。为了使用这个网络，你的项目需要做哪些改动？
- en: '[The Complete Code](nsp-venkitachalam503045-0008.xhtml#rah1708)'
  id: totrans-648
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[完整代码](nsp-venkitachalam503045-0008.xhtml#rah1708)'
- en: Here’s a complete listing of the code that goes on the Raspberry Pi, including
    the `print()` statements for verbose debugging. The Google Colab notebook code
    can be found at [https://github.com/mkvenkit/pp2e/blob/main/audioml/audioml.ipynb](https://github.com/mkvenkit/pp2e/blob/main/audioml/audioml.ipynb).
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是用于树莓派的完整代码清单，包括用于详细调试的`print()`语句。Google Colab笔记本代码可以在[https://github.com/mkvenkit/pp2e/blob/main/audioml/audioml.ipynb](https://github.com/mkvenkit/pp2e/blob/main/audioml/audioml.ipynb)找到。
- en: '"""'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: simple_audio.py
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: simple_audio.py
- en: This programs collects audio data from an I2S mic on the Raspberry Pi
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序从树莓派上的I2S麦克风收集音频数据
- en: and runs the TensorFlow Lite interpreter on a per-build model.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 并在每个构建模型上运行TensorFlow Lite解释器。
- en: 'Author: Mahesh Venkitachalam'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：Mahesh Venkitachalam
- en: '"""'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: from scipy.io import wavfile
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy.io import wavfile
- en: from scipy import signal
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: from scipy import signal
- en: import numpy as np
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import argparse
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: import argparse
- en: import pyaudio
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: import pyaudio
- en: import wave
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: import wave
- en: import time
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: import time
- en: from tflite_runtime.interpreter import Interpreter
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: from tflite_runtime.interpreter import Interpreter
- en: from multiprocessing import Process, Queue
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: from multiprocessing import Process, Queue
- en: VERBOSE_DEBUG = False
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: VERBOSE_DEBUG = False
- en: CHUNK = 4000                # choose a value divisible by SAMPLE_RATE
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: CHUNK = 4000                # 选择一个能被SAMPLE_RATE整除的值
- en: FORMAT = pyaudio.paInt16
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: FORMAT = pyaudio.paInt16
- en: CHANNELS = 1
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: CHANNELS = 1
- en: SAMPLE_RATE = 16000
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: SAMPLE_RATE = 16000
- en: RECORD_SECONDS = 1
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: RECORD_SECONDS = 1
- en: NCHUNKS = int((SAMPLE_RATE * RECORD_SECONDS) / CHUNK)
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: NCHUNKS = int((SAMPLE_RATE * RECORD_SECONDS) / CHUNK)
- en: ND = 2 * SAMPLE_RATE * RECORD_SECONDS
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: ND = 2 * SAMPLE_RATE * RECORD_SECONDS
- en: NDH = ND // 2
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: NDH = ND // 2
- en: device index of microphone
  id: totrans-674
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 麦克风的设备索引
- en: dev_index = -1
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: dev_index = -1
- en: 'def list_devices():'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 'def list_devices():'
- en: '"""list pyaudio devices"""'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '"""列出pyaudio设备"""'
- en: '# initialize pyaudio'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '# 初始化pyaudio'
- en: p = pyaudio.PyAudio()
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: p = pyaudio.PyAudio()
- en: '# get device list'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取设备列表而得到重复结果'
- en: index = None
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: index = None
- en: nDevices = p.get_device_count()
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: nDevices = p.get_device_count()
- en: print('\naudioml.py:\nFound the following input devices:')
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: print('\naudioml.py:\n找到以下输入设备：')
- en: 'for i in range(nDevices):'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(nDevices):'
- en: deviceInfo = p.get_device_info_by_index(i)
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: deviceInfo = p.get_device_info_by_index(i)
- en: 'if deviceInfo[''maxInputChannels''] > 0:'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 'if deviceInfo[''maxInputChannels''] > 0:'
- en: print(deviceInfo['index'], deviceInfo['name'], deviceInfo['defaultSampleRate'])
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: print(deviceInfo['index'], deviceInfo['name'], deviceInfo['defaultSampleRate'])
- en: '# clean up'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '# 清理'
- en: p.terminate()
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: p.terminate()
- en: 'def inference_process(dataq, interpreter):'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 'def inference_process(dataq, interpreter):'
- en: '"""infererence process handler"""'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '"""推理过程处理器"""'
- en: success = False
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: success = False
- en: 'while True:'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 'while True:'
- en: 'if not dataq.empty():'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not dataq.empty():'
- en: '# get data from queue'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '# 从队列获取数据'
- en: inference_data = dataq.get()
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: inference_data = dataq.get()
- en: '# run inference only if previous one was not success'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '# 仅在上一个推理失败时才进行推理'
- en: '# otherwise we will get duplicate results because of'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '# 否则，我们会因为'
- en: '# overlap in input data'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '# 输入数据的重叠'
- en: 'if not success:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not success:'
- en: success = run_inference(inference_data, interpreter)
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: success = run_inference(inference_data, interpreter)
- en: 'else:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: '# skipping, reset flag for next time'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '# 跳过，重置标志以便下次使用'
- en: success = False
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: success = False
- en: 'def process_audio_data(waveform):'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 'def process_audio_data(waveform):'
- en: '"""Process audio input.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '"""处理音频输入。'
- en: This function takes in raw audio data from a WAV file and does scaling
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数从WAV文件中接收原始音频数据并进行缩放
- en: and padding to 16000 length.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 并将长度填充到16000。
- en: '"""'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: 'if VERBOSE_DEBUG:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: print("waveform:", waveform.shape, waveform.dtype, type(waveform))
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: print("waveform:", waveform.shape, waveform.dtype, type(waveform))
- en: print(waveform[:5])
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: print(waveform[:5])
- en: '# compute peak to peak based on scaling by max 16-bit value'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '# 基于最大16位值的缩放计算峰值到峰值'
- en: PTP = np.ptp(waveform / 32768.0)
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: PTP = np.ptp(waveform / 32768.0)
- en: 'if VERBOSE_DEBUG:'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: 'print("peak-to-peak (16 bit scaling): {}".format(PTP))'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: print("峰值-峰值（16位缩放）：{}".format(PTP))
- en: '# return None if too silent'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '# 如果音频过于安静，则返回None'
- en: 'if PTP < 0.3:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 'if PTP < 0.3:'
- en: return []
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: return []
- en: '# normalize audio'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '# 归一化音频'
- en: wabs = np.abs(waveform)
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: wabs = np.abs(waveform)
- en: wmax = np.max(wabs)
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: wmax = np.max(wabs)
- en: waveform = waveform / wmax
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: waveform = waveform / wmax
- en: '# compute peak to peak based on normalized waveform'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '# 基于归一化波形计算峰值到峰值'
- en: PTP = np.ptp(waveform)
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: PTP = np.ptp(waveform)
- en: 'if VERBOSE_DEBUG:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: 'print("peak-to-peak (after normalize): {}".format(PTP))'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("peak-to-peak (after normalize): {}".format(PTP))'
- en: print("After normalization:")
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: print("After normalization:")
- en: print("waveform:", waveform.shape, waveform.dtype, type(waveform))
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: print("waveform:", waveform.shape, waveform.dtype, type(waveform))
- en: print(waveform[:5])
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: print(waveform[:5])
- en: '# scale and center'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '# 缩放和居中'
- en: waveform = 2.0*(waveform - np.min(waveform))/PTP - 1
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: waveform = 2.0*(waveform - np.min(waveform))/PTP - 1
- en: '# extract 16000 len (1 second) of data'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '# 提取16000个长度（1秒）的数据'
- en: max_index = np.argmax(waveform)
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: max_index = np.argmax(waveform)
- en: start_index = max(0, max_index-8000)
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: start_index = max(0, max_index-8000)
- en: end_index = min(max_index+8000, waveform.shape[0])
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: end_index = min(max_index+8000, waveform.shape[0])
- en: waveform = waveform[start_index:end_index]
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: waveform = waveform[start_index:end_index]
- en: '# padding for files with less than 16000 samples'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: '# 为少于16000个样本的文件进行填充'
- en: 'if VERBOSE_DEBUG:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: print("After padding:")
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: print("After padding:")
- en: waveform_padded = np.zeros((16000,))
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: waveform_padded = np.zeros((16000,))
- en: waveform_padded[:waveform.shape[0]] = waveform
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: waveform_padded[:waveform.shape[0]] = waveform
- en: 'if VERBOSE_DEBUG:'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: print("waveform_padded:", waveform_padded.shape,
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: print("waveform_padded:", waveform_padded.shape,
- en: waveform_padded.dtype, type(waveform_padded))
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: waveform_padded.dtype, type(waveform_padded))
- en: print(waveform_padded[:5])
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: print(waveform_padded[:5])
- en: return waveform_padded
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: return waveform_padded
- en: 'def get_spectrogram(waveform):'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 'def get_spectrogram(waveform):'
- en: '"""computes spectrogram from audio data"""'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '"""从音频数据计算频谱图"""'
- en: waveform_padded = process_audio_data(waveform)
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: waveform_padded = process_audio_data(waveform)
- en: 'if not len(waveform_padded):'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not len(waveform_padded):'
- en: return []
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: return []
- en: '# compute spectrogram'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '# 计算频谱图'
- en: f, t, Zxx = signal.stft(waveform_padded, fs=16000, nperseg=255,
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: f, t, Zxx = signal.stft(waveform_padded, fs=16000, nperseg=255,
- en: noverlap = 124, nfft=256)
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: noverlap = 124, nfft=256)
- en: '# output is complex, so take abs value'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '# 输出是复数，因此取其绝对值'
- en: spectrogram = np.abs(Zxx)
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: spectrogram = np.abs(Zxx)
- en: 'if VERBOSE_DEBUG:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: print("spectrogram:", spectrogram.shape, type(spectrogram))
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: print("spectrogram:", spectrogram.shape, type(spectrogram))
- en: print(spectrogram[0, 0])
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: print(spectrogram[0, 0])
- en: return spectrogram
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: return spectrogram
- en: 'def run_inference(waveform, interpreter):'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run_inference(waveform, interpreter):'
- en: '# start timing'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '# 开始计时'
- en: start = time.perf_counter()
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: start = time.perf_counter()
- en: '# get spectrogram data'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取频谱图数据'
- en: spectrogram = get_spectrogram(waveform)
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: spectrogram = get_spectrogram(waveform)
- en: 'if not len(spectrogram):'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 'if not len(spectrogram):'
- en: 'if VERBOSE_DEBUG:'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: print("Too silent. Skipping...")
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: print("太安静，跳过...")
- en: return False
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: return False
- en: 'if VERBOSE_DEBUG:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: 'print("spectrogram: %s, %s, %s" % (type(spectrogram),'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("spectrogram: %s, %s, %s" % (type(spectrogram),'
- en: spectrogram.dtype, spectrogram.shape))
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: spectrogram.dtype, spectrogram.shape))
- en: '# get input and output tensors details'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取输入和输出张量的详细信息'
- en: input_details = interpreter.get_input_details()
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: input_details = interpreter.get_input_details()
- en: output_details = interpreter.get_output_details()
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: output_details = interpreter.get_output_details()
- en: 'if VERBOSE_DEBUG:'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: 'print("input_details: {}".format(input_details))'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("input_details: {}".format(input_details))'
- en: 'print("output_details: {}".format(output_details))'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("output_details: {}".format(output_details))'
- en: '# reshape spectrogram to match interpreter requirement'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '# 重塑频谱图以匹配推理器要求'
- en: spectrogram = np.reshape(spectrogram, (-1, spectrogram.shape[0],
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: spectrogram = np.reshape(spectrogram, (-1, spectrogram.shape[0],
- en: spectrogram.shape[1], 1))
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: spectrogram.shape[1], 1))
- en: '# set input'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置输入'
- en: input_data = spectrogram.astype(np.float32)
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: input_data = spectrogram.astype(np.float32)
- en: interpreter.set_tensor(input_details[0]['index'], input_data)
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: interpreter.set_tensor(input_details[0]['index'], input_data)
- en: '# run interpreter'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: '# 运行推理器'
- en: print("running inference...")
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: print("running inference...")
- en: interpreter.invoke()
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: interpreter.invoke()
- en: '# get output'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取输出'
- en: output_data = interpreter.get_tensor(output_details[0]['index'])
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: output_data = interpreter.get_tensor(output_details[0]['index'])
- en: yvals = output_data[0]
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: yvals = output_data[0]
- en: 'if VERBOSE_DEBUG:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 'if VERBOSE_DEBUG:'
- en: print(output_data)
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: print(output_data)
- en: print(yvals)
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: print(yvals)
- en: '# Important! This should exactly match training labels/ids.'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '# 重要！这应该与训练标签/ID完全匹配。'
- en: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: commands = ['up', 'no', 'stop', 'left', 'right', 'go', 'down', 'yes']
- en: print(">>> " + commands[np.argmax(output_data[0])].upper())
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: print(">>> " + commands[np.argmax(output_data[0])].upper())
- en: '# stop timing'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '# 停止计时'
- en: end = time.perf_counter()
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: end = time.perf_counter()
- en: 'print("run_inference: {}s".format(end - start))'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("run_inference: {}s".format(end - start))'
- en: '# return success'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '# 返回成功'
- en: return True
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: return True
- en: 'def get_live_input(interpreter):'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 'def get_live_input(interpreter):'
- en: '"""this function gets live input from the microphone'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '"""此函数从麦克风获取实时输入'
- en: and runs inference on it"""
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 并在其上运行推理"""
- en: '# create a queue object'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: '# 创建队列对象'
- en: dataq = Queue()
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: dataq = Queue()
- en: '# start inference process'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '# 开始推理过程'
- en: proc = Process(target = inference_process, args=(dataq, interpreter))
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: proc = Process(target = inference_process, args=(dataq, interpreter))
- en: proc.start()
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: proc.start()
- en: '# initialize pyaudio'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '# 初始化 pyaudio'
- en: p = pyaudio.PyAudio()
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: p = pyaudio.PyAudio()
- en: print('opening stream...')
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: print('正在打开流...')
- en: stream = p.open(format = FORMAT,
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: stream = p.open(format = FORMAT,
- en: channels = CHANNELS,
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: channels = CHANNELS,
- en: rate = SAMPLE_RATE,
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: rate = SAMPLE_RATE,
- en: input = True,
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: input = True,
- en: frames_per_buffer = CHUNK,
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: frames_per_buffer = CHUNK,
- en: input_device_index = dev_index)
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: input_device_index = dev_index)
- en: '# discard first 1 second'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '# 丢弃前 1 秒'
- en: 'for i in range(0, NCHUNKS):'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(0, NCHUNKS):'
- en: data = stream.read(CHUNK, exception_on_overflow = False)
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: data = stream.read(CHUNK, exception_on_overflow = False)
- en: '# count for gathering two frames at a time'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '# 用于一次收集两帧数据的计数'
- en: count = 0
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: count = 0
- en: inference_data = np.zeros((ND,), dtype=np.int16)
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: inference_data = np.zeros((ND,), dtype=np.int16)
- en: print("Listening...")
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: print("正在监听...")
- en: 'try:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 'try:'
- en: 'while True:'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 'while True:'
- en: '# print("Listening...")'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '# print("正在监听...")'
- en: chunks = []
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: chunks = []
- en: 'for i in range(0, NCHUNKS):'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(0, NCHUNKS):'
- en: data = stream.read(CHUNK, exception_on_overflow = False)
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: data = stream.read(CHUNK, exception_on_overflow = False)
- en: chunks.append(data)
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: chunks.append(data)
- en: '# process data'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: '# 处理数据'
- en: buffer = b''.join(chunks)
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: buffer = b''.join(chunks)
- en: audio_data = np.frombuffer(buffer, dtype=np.int16)
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: audio_data = np.frombuffer(buffer, dtype=np.int16)
- en: 'if count == 0:'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 'if count == 0:'
- en: '# set first half'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置前半部分'
- en: inference_data[:NDH] = audio_data
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: inference_data[:NDH] = audio_data
- en: count += 1
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: count += 1
- en: 'elif count == 1:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif count == 1:'
- en: '# set second half'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置后半部分'
- en: inference_data[NDH:] = audio_data
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: inference_data[NDH:] = audio_data
- en: '# add data to queue'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将数据添加到队列'
- en: dataq.put(inference_data)
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: dataq.put(inference_data)
- en: count += 1
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: count += 1
- en: 'else:'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: '# move second half to first half'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将后半部分移至前半部分'
- en: inference_data[:NDH] = inference_data[NDH:]
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: inference_data[:NDH] = inference_data[NDH:]
- en: '# set second half'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: '# 设置后半部分'
- en: inference_data[NDH:] = audio_data
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: inference_data[NDH:] = audio_data
- en: '# add data to queue'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: '# 将数据添加到队列'
- en: dataq.put(inference_data)
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: dataq.put(inference_data)
- en: '# print("queue: {}".format(dataq.qsize()))'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '# print("队列大小: {}".format(dataq.qsize()))'
- en: 'except KeyboardInterrupt:'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 'except KeyboardInterrupt:'
- en: print("exiting...")
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: print("正在退出...")
- en: stream.stop_stream()
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: stream.stop_stream()
- en: stream.close()
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: stream.close()
- en: p.terminate()
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: p.terminate()
- en: 'def main():'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: '"""main function for the program"""'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: '"""程序的主函数"""'
- en: '# globals set in this function'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '# 在此函数中设置全局变量'
- en: global VERBOSE_DEBUG
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: global VERBOSE_DEBUG
- en: '# create parser'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '# 创建解析器'
- en: descStr = "This program does ML inference on audio data."
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: descStr = "该程序对音频数据进行机器学习推理。"
- en: parser = argparse.ArgumentParser(description=descStr)
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: parser = argparse.ArgumentParser(description=descStr)
- en: '# add a mutually exclusive group'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '# 添加互斥组'
- en: group = parser.add_mutually_exclusive_group(required=True)
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: group = parser.add_mutually_exclusive_group(required=True)
- en: '# add mutually exclusive arguments'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '# 添加互斥参数'
- en: group.add_argument('--list', action='store_true', required=False)
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: group.add_argument('--list', action='store_true', required=False)
- en: group.add_argument('--input', dest='wavfile_name', required=False)
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: group.add_argument('--input', dest='wavfile_name', required=False)
- en: group.add_argument('--index', dest='index', required=False)
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: group.add_argument('--index', dest='index', required=False)
- en: '# add other arguments'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '# 添加其他参数'
- en: parser.add_argument('--verbose', action='store_true', required=False)
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: parser.add_argument('--verbose', action='store_true', required=False)
- en: '# parse args'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '# 解析参数'
- en: args = parser.parse_args()
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: args = parser.parse_args()
- en: '# load TF Lite model'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: '# 加载 TF Lite 模型'
- en: interpreter = Interpreter('audioml.tflite')
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: interpreter = Interpreter('audioml.tflite')
- en: interpreter.allocate_tensors()
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: interpreter.allocate_tensors()
- en: '# check verbose flag'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '# 检查详细标志'
- en: 'if args.verbose:'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 'if args.verbose:'
- en: VERBOSE_DEBUG = True
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: VERBOSE_DEBUG = True
- en: '# test WAV file'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '# 测试 WAV 文件'
- en: 'if args.wavfile_name:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 'if args.wavfile_name:'
- en: wavfile_name = args.wavfile_name
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: wavfile_name = args.wavfile_name
- en: '# get audio data'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取音频数据'
- en: rate, waveform = wavfile.read(wavfile_name)
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: rate, waveform = wavfile.read(wavfile_name)
- en: '# run inference'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '# 运行推理'
- en: run_inference(waveform, interpreter)
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: run_inference(waveform, interpreter)
- en: 'elif args.list:'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 'elif args.list:'
- en: '# list devices'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '# 列出设备'
- en: list_devices()
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: list_devices()
- en: 'else:'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: '# store device index'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: '# 存储设备索引'
- en: dev_index = int(args.index)
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: dev_index = int(args.index)
- en: '# get live audio'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '# 获取实时音频'
- en: get_live_input(interpreter)
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: get_live_input(interpreter)
- en: print("done.")
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: print("完成。")
- en: main method
  id: totrans-899
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: main 方法
- en: 'if __name__ == ''__main__'':'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 'if __name__ == ''__main__'':'
- en: main()
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: main()
