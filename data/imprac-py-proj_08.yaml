- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8'
- en: COUNTING SYLLABLES FOR HAIKU POETRY**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计数俳句诗歌的音节**
- en: '![image](../images/common01.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/common01.jpg)'
- en: Poetry may be the supreme form of literature. It is, as Coleridge put it, “the
    best words in the best order.” The poet must—with great brevity—tell a story,
    promote an idea, describe a scene, or evoke intensity of feeling, all while obeying
    strict rules on rhythm and rhyme, style and structure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 诗歌可能是文学的最高形式。正如柯尔律治所说，“最好的词语按最好的顺序排列。”诗人必须以极简的语言讲述故事，传播思想，描绘场景，或唤起强烈的情感，同时遵循严格的节奏和押韵规则、风格和结构。
- en: 'Computers love rules and structure and even have the potential to evoke emotions.
    In his 1996 book *Virtual Muse: Experiments in Computer Poetry*, author Charles
    Hartman describes early attempts to write algorithms that could mimic human poetry.
    To quote Hartman, “The complexity of poetic interaction, the tricky dance among
    poet and text and reader, causes a game of hesitation. In this game, a properly
    programmed computer has a chance to slip in some interesting moves.”'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机喜欢规则和结构，甚至有潜力激发情感。在1996年出版的《虚拟缪斯：计算机诗歌实验》一书中，作者查尔斯·哈特曼描述了早期尝试编写能够模仿人类诗歌的算法。引用哈特曼的话说：“诗歌互动的复杂性，诗人、文本和读者之间微妙的舞蹈，导致了一场犹豫的博弈。在这场博弈中，经过适当编程的计算机有机会插入一些有趣的动作。”
- en: The early programs Hartman describes could, at best, produce bad beatnik poetry.
    The goal at the time was to “introduce calculated bits of mechanized anarchy into
    the language, put the results back into the contingent world where language lives,
    and see how the dust settles.” As we have touched on in several chapters, context
    is the weak link in programming things like proper-name anagrams and null ciphers.
    To write computer poems that pass the “supreme” test of literature, you cannot
    ignore context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 哈特曼描述的早期程序，充其量只能产生糟糕的嬉皮诗。那时的目标是“将计算出的机械化无序引入语言，将结果放回语言存在的具体世界，看看尘土如何落定。”正如我们在几个章节中提到的，上下文是编程中弱的一环，比如专有名词的字谜和空白密码。为了写出能够通过文学“至高”测试的计算机诗歌，你不能忽视上下文。
- en: Getting a computer to simulate this most human of human endeavors is an intriguing
    challenge—and certainly not one we can pass up. In this chapter and the next,
    you’ll teach your computer how to generate a traditional form of Japanese poetry
    called *haiku*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让计算机模拟这一最具人类特质的活动是一个引人入胜的挑战——而且无疑是我们无法放弃的挑战。在本章及下一章中，你将教你的计算机生成一种传统的日本诗歌形式——*俳句*。
- en: '**Japanese Haiku**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**日本俳句**'
- en: Haiku consist of three lines of five, seven, and five syllables, respectively.
    The poems rarely rhyme, and the subject matter usually addresses the natural world—mainly
    the seasons—either directly or indirectly. If done properly, a haiku can immerse
    you in the scene, as if evoking a memory.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 俳句由三行组成，分别为五个、七个和五个音节。这些诗歌很少押韵，题材通常直接或间接地涉及自然世界——主要是季节。如果做得好，一首俳句能够让你沉浸在场景中，就像唤起一段记忆一样。
- en: I’ve provided three example haiku here. The first is by the master Buson (1715–1783),
    the second by the master Issa (1763–1828), and the third by yours truly, based
    on memories of childhood road trips.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里提供了三首俳句范例。第一首是大师布孙（1715–1783）创作的，第二首是大师一茶（1763–1828）创作的，第三首是我自己创作的，基于儿时公路旅行的回忆。
- en: Standing still at dusk
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 黄昏时静立
- en: Listen . . . in far distances
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 听啊。。。在遥远的地方
- en: The song of froglings!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 青蛙的歌声！
- en: '*—Buson*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*—布孙*'
- en: Good friend grasshopper
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好朋友蝉蜕
- en: Will you play the caretaker
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你会当看护者吗
- en: For my little grave?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为我的小墓地吗？
- en: '*—Issa*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*—一茶*'
- en: Faraway cloudbanks
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 远方的云层
- en: That I let myself pretend
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我让自己假装
- en: Are distant mountains
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 远山的景象
- en: '*—Vaughan*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*—沃恩*'
- en: 'Because of its evocative nature, every haiku has a built-in “exploitable gap”
    for the programmer. This is summed up nicely by Peter Beilenson in his 1955 book
    *Japanese Haiku*: “the haiku is not expected to be always a complete or even a
    clear statement. The reader is supposed to add to the words his own associations
    and imagery, and thus to become a co-creator of his own pleasure in the poem.”
    Hartman adds, “The reader’s mind works most actively on sparse materials. We draw
    the clearest constellations from the fewest stars. So, the nonsense factor is
    low for a tiny collocation of words that can be imbued with imagistic significance.”
    To put it simply, it’s harder to mess up a short poem. Readers always assume the
    poet had a point and will make one up themselves if they can’t find it.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其唤起性特征，每一首俳句都内置了程序员可以“利用的空隙”。这一点彼得·贝伦森在他1955年出版的书《日本俳句》中有一句话总结得很好：“俳句并不总是期待成为一个完整的，甚至是清晰的陈述。读者应该将自己的联想和意象添加到文字中，从而成为自己在诗中愉悦的共同创作者。”哈特曼补充道：“读者的大脑在稀疏的材料上最为活跃。我们从最少的星星中画出最清晰的星座。因此，对于一小组可以赋予意象意义的词语来说，胡说八道的因素很低。”简单来说，搞砸一首短诗更为困难。读者总是认为诗人有某种意图，并且如果他们找不到，就会自己构造一个。
- en: Despite this advantage, training your computer to write poetry is no mean feat,
    and you’ll need two whole chapters to get it done. In this chapter, you’ll write
    a program that counts the number of syllables in words and phrases so that you
    can honor the syllabic structure of the haiku. In [Chapter 9](ch09.xhtml#ch09),
    you’ll use a technique called *Markov chain analysis* to capture the essence of
    haiku—the elusive evocative component—and transform existing poems into something
    new and, occasionally, arguably better.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这个优势，训练计算机写诗并非易事，你将需要两个完整的章节才能完成。在本章中，你将编写一个程序来计算单词和短语中的音节数，以便你能够遵循俳句的音节结构。在[第9章](ch09.xhtml#ch09)，你将使用一种叫做*马尔可夫链分析*的技术来捕捉俳句的本质——那种难以捉摸的唤起性成分——并将现有的诗歌转变为新的，有时甚至是更好的作品。
- en: '**Project #15: Counting Syllables**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**项目 #15：音节计数**'
- en: Counting syllables in English is difficult. The problem lies in, as Charles
    Hartman put it, the quirky spelling and tangled linguistic history of English.
    For example, a word like *aged* may be one syllable or two depending on whether
    it describes a man or a cheese. How can a program count syllables accurately without
    degenerating into an endless list of special cases?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算英语音节是困难的。问题在于，正如查尔斯·哈特曼所说，英语的拼写古怪且语言历史错综复杂。例如，一个词像*aged*（年老的）根据它是描述一个人还是奶酪，可能是一个音节，也可能是两个音节。一个程序如何在不沦为无休止的特殊情况列表的情况下，准确地计算音节呢？
- en: 'The answer is that it can’t, at least not without a “cheat sheet.” Fortunately,
    these cheat sheets exist, thanks to a branch of science known as *natural language
    processing* (*NLP*), which deals with interactions between the precise and structured
    language of computers and the nuanced, frequently ambiguous “natural” language
    used by humans. Example uses for NLP include machine translations, spam detection,
    comprehension of search engine questions, and predictive text recognition for
    cell phone users. The biggest impact of NLP is yet to come: the mining of vast
    volumes of previously unusable, poorly structured data and engaging in seamless
    conversations with our computer overlords.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是它无法做到，至少没有“备忘单”。幸运的是，借助于一门叫做*自然语言处理*（*NLP*）的科学分支，这些“备忘单”存在了，它处理的是计算机的精确结构化语言与人类使用的含糊且常常模糊的“自然”语言之间的互动。NLP的示例应用包括机器翻译、垃圾邮件检测、搜索引擎问题理解和手机用户的预测文本识别。NLP的最大影响还未到来：即挖掘大量以前无法使用的、结构不良的数据，并与我们的计算机“霸主”进行无缝对话。
- en: 'In this chapter, you’ll use an NLP dataset to help count syllables in words
    or phrases. You’ll also write code that finds words that are missing from this
    dataset and then helps you build a supporting dictionary. Finally, you’ll write
    a program to help you check your syllable-counting code. In [Chapter 9](ch09.xhtml#ch09),
    you’ll use this syllable-counting algorithm as a module in a program that helps
    you computationally produce the highest achievement in literature: poetry.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用一个NLP数据集来帮助计算单词或短语中的音节数量。你还将编写代码，找到缺失于此数据集的单词，并帮助你建立一个辅助字典。最后，你将编写一个程序，帮助你检查你的音节计数代码。在[第9章](ch09.xhtml#ch09)，你将把这个音节计数算法作为模块，嵌入到一个程序中，帮助你在计算上创作文学中的最高成就：诗歌。
- en: '**THE OBJECTIVE**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**'
- en: Write a Python program that counts the number of syllables in an English word
    or phrase.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个 Python 程序，统计英语单词或短语中的音节数。
- en: '**The Strategy**'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**策略**'
- en: For you and me, counting syllables is easy. Place the back of your hand just
    below your chin and start talking. Every time your chin hits your hand you’ve
    spoken a syllable. Computers don’t have hands or chins, but every vowel sound
    represents a syllable—and computers can count vowel sounds. It’s not easy, however,
    as there isn’t a simple rule for doing this. Some vowels in written language are
    silent, such as the *e* in *like*, and some combine to make a single sound, such
    as the *oo* in *moo*. Luckily, the number of words in the English language isn’t
    infinite. Fairly exhaustive lists are available that include much of the information
    you need.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你和我来说，数音节很简单。将手背放在下巴下方开始说话，每次下巴碰到手时，你就说了一个音节。电脑没有手或下巴，但每个元音音素代表一个音节——而计算机可以数元音音素。然而，这并不容易，因为没有简单的规则来执行这一操作。有些书面语言中的元音是哑音，比如
    *like* 中的 *e*，而有些元音组合成一个声音，比如 *moo* 中的 *oo*。幸运的是，英语中的单词数量不是无限的。已经有相当详尽的单词列表，其中包含你需要的很多信息。
- en: A *corpus* is a fancy name for a body of text. In [Chapter 9](ch09.xhtml#ch09),
    you’ll use a *training corpus*—composed of haiku—that teaches Python how to write
    new haiku. In this chapter, you’ll use this same corpus to extract syllable counts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*语料库* 是文本的一个高级名称。在[第 9 章](ch09.xhtml#ch09)中，你将使用一个由俳句组成的*训练语料库*，它教会 Python
    如何写新的俳句。在本章中，你将使用这个相同的语料库来提取音节计数。'
- en: Your syllable counter should evaluate both phrases and individual words, since
    you will ultimately use it to count the syllables in entire *lines* in a haiku.
    The program will take some text as an input, count the number of syllables in
    each word, and return the total syllable count. You’ll also have to deal with
    things like punctuation, whitespace, and missing words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你的音节计数器应评估短语和单个单词，因为最终你将使用它来统计俳句中整个*行*的音节数。该程序将接收一些文本作为输入，统计每个单词的音节数，并返回总音节数。你还需要处理标点符号、空格和缺失的单词等问题。
- en: 'The primary steps you need to follow are:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要遵循的主要步骤是：
- en: Download a large corpus with syllable-count information.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一个包含音节计数信息的大型语料库。
- en: Compare the syllable-count corpus to the haiku-training corpus and identify
    all the words missing from the syllable-count corpus.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将音节计数语料库与俳句训练语料库进行比较，找出所有缺失在音节计数语料库中的单词。
- en: Build a dictionary of the missing words and their syllable counts.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个缺失单词及其音节数的字典。
- en: Write a program that uses both the syllable-count corpus and the missing-words
    dictionary to count syllables in the training corpus.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个程序，使用音节计数语料库和缺失单词字典来统计训练语料库中的音节数。
- en: Write a program that checks the syllable-counting program against updates of
    the training corpus.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个程序，将音节计数程序与更新后的训练语料库进行比对。
- en: '***Using a Corpus***'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用语料库***'
- en: The Natural Language Toolkit (NLTK) is a popular suite of programs and libraries
    for working with human language data in Python. It was created in 2001 as part
    of a computational linguistics course in the Department of Computer and Information
    Science at the University of Pennsylvania. Development and expansion have continued
    with the help of dozens of contributors. To learn more, check out the official
    NLTK website at *[http://www.nltk.org/](http://www.nltk.org/)*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言工具包（NLTK）是一个流行的程序和库套件，专门用于处理 Python 中的自然语言数据。它于 2001 年作为宾夕法尼亚大学计算机与信息科学系的一门计算语言学课程的一部分创建。随着众多贡献者的帮助，它不断发展和扩展。要了解更多信息，请访问官方
    NLTK 网站 *[http://www.nltk.org/](http://www.nltk.org/)*。
- en: For this project, you will use NLTK to access the Carnegie Mellon University
    Pronouncing Dictionary (CMUdict). This corpus contains almost 125,000 words mapped
    to their pronunciations. It is machine readable and useful for tasks such as speech
    recognition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用 NLTK 访问卡内基梅隆大学发音词典（CMUdict）。该语料库包含近 125,000 个单词及其发音映射。它是机器可读的，适用于语音识别等任务。
- en: '***Installing NLTK***'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***安装 NLTK***'
- en: 'You can find instructions for installing NLTK on Unix, Windows, and macOS at
    *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*. If you
    are using Windows, I suggest you start by opening Windows Command Prompt or PowerShell
    and trying to install with `pip`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *[http://www.nltk.org/install.html](http://www.nltk.org/install.html)*
    上找到安装 NLTK 的说明，适用于 Unix、Windows 和 macOS。如果你使用的是 Windows，我建议你先打开 Windows 命令提示符或
    PowerShell，并尝试使用 `pip` 进行安装：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can check the installation by opening the Python interactive shell and
    typing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过打开 Python 交互式 shell 并输入以下内容来检查安装：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you don’t get an error, you’re good to go. Otherwise, follow the instructions
    on the website just cited.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有遇到错误，那么你就可以继续了。否则，请按照网站上提供的说明操作。
- en: '***Downloading CMUdict***'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***下载 CMUdict***'
- en: 'To get access to CMUdict (or any of the other NLTK corpora), you have to download
    it. You can do this using the handy NLTK Downloader. Once you’ve installed NLTK,
    enter the following into the Python shell:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 CMUdict（或任何其他 NLTK 语料库），你需要下载它。你可以使用便捷的 NLTK 下载器来实现这一点。安装 NLTK 后，在 Python
    shell 中输入以下内容：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The NLTK Downloader window ([Figure 8-1](ch08.xhtml#ch08fig1)) should now be
    open. Click the **Corpora** tab near the top, then click **cmudict** in the Identifier
    column. Next, scroll to the bottom of the window and set the Download Directory;
    I used the default, *C:\nltk_data*. Finally, click the **Download** button to
    load CMUdict.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 下载器窗口（[图 8-1](ch08.xhtml#ch08fig1)）应该已经打开。点击顶部的 **Corpora** 标签，然后在标识符列中点击
    **cmudict**。接着，滚动到窗口底部并设置下载目录；我使用了默认目录，*C:\nltk_data*。最后，点击 **下载** 按钮加载 CMUdict。
- en: '![image](../images/f0149-01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/f0149-01.jpg)'
- en: '*Figure 8-1: The NLTK Downloader window with cmudict selected for download*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-1：NLTK 下载器窗口，已选择 cmudict 进行下载*'
- en: 'When CMUdict has finished downloading, exit the Downloader and enter the following
    into the Python interactive shell:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CMUdict 下载完成后，退出下载器并在 Python 交互式 shell 中输入以下内容：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you don’t encounter an error, then the corpus has been successfully downloaded.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有遇到错误，则表示语料库已成功下载。
- en: '***Counting Sounds Instead of Syllables***'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***计算声音而不是音节***'
- en: The CMUdict corpus breaks words into sets of *phonemes*—perceptually distinct
    *units* of sound in a specified language—and marks vowels for lexical stress using
    numbers (0, 1, and 2). The CMUdict corpus marks every vowel with one, and only
    one, of these numbers, so you can use the numbers to identify the vowels in a
    word.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: CMUdict 语料库将单词分解为一组 *音素* ——在特定语言中感知上不同的 *声音单元* ——并通过数字（0、1 和 2）标记元音的词汇重音。CMUdict
    语料库为每个元音标记一个且仅一个这些数字，因此你可以使用这些数字来识别单词中的元音。
- en: 'Looking at words as a set of phonemes will help you sidestep a few problems.
    For one, CMUdict will not include vowels in the written word that are unpronounced.
    For example, here’s how CMUdict sees the word *scarecrow*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词视为音素集合将帮助你避免一些问题。首先，CMUdict 不会包括书写中未发音的元音。例如，下面是 CMUdict 如何表示单词 *scarecrow*：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each item with a numerical suffix represents a pronounced vowel. Note that the
    silent *e* at the end of *scare* is correctly omitted.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个带有数字后缀的项目表示一个发音元音。注意，*scare* 结尾的静音 *e* 被正确省略了。
- en: 'Second, sometimes multiple and consecutive written vowels are pronounced as
    just a single phoneme. For example, this is how CMUdict represents *house*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，有时多个连续的书写元音会被发音为一个单一音素。例如，CMUdict 是如何表示 *house* 的：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note how the corpus treats the written double vowels *ou* as a single vowel,
    `'AW1'`, for pronunciation purposes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意语料库如何将书写中的双元音 *ou* 作为一个单一元音 `'AW1'` 来处理，用于发音目的。
- en: '***Handling Words with Multiple Pronunciations***'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***处理有多重发音的词语***'
- en: 'As I mention in the introduction, some words have multiple distinct pronunciations;
    *aged* and *learned* are just two examples:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在介绍中提到的，某些单词有多种不同的发音；*aged* 和 *learned* 就是两个例子：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note the nested lists. The corpus recognizes that both words can be pronounced
    with one or two syllables. This means it will return more than one syllable count
    for certain words, something you will have to account for in your code.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意嵌套列表。语料库识别到这两个词可以用一个或两个音节发音。这意味着它会返回多个音节数，对于某些词，你需要在代码中考虑这一点。
- en: '**Managing Missing Words**'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**管理缺失的词语**'
- en: CMUdict is very useful, but with a corpus, a word is either there or not. It
    took only seconds to find more than 50 words—like *dewdrop*, *bathwater*, *dusky*,
    *ridgeline*, *storks*, *dragonfly*, *beggar*, and *archways*—missing from CMUdict
    in a 1,500-word test case. So, one of your strategies should be to check CMUdict
    for missing words and then address any omissions by making a corpus for your corpus!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: CMUdict 非常有用，但在语料库中，一个词要么存在，要么不存在。仅仅几秒钟，我就找到了50多个词——比如 *dewdrop*、*bathwater*、*dusky*、*ridgeline*、*storks*、*dragonfly*、*beggar*
    和 *archways*——在一个1500词的测试用例中缺失。因此，你的策略之一应该是检查 CMUdict 中缺失的词，并通过为你的语料库创建一个语料库来解决这些遗漏！
- en: '***The Training Corpus***'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***训练语料库***'
- en: In [Chapter 9](ch09.xhtml#ch09), you’ll use a training corpus of several hundred
    haiku to “teach” your program how to write new ones. But you can’t count on CMUdict
    to contain all the words in this corpus because some will be Japanese words, like
    *sake*. And as you already saw, even some common English words are missing from
    CMUdict.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)，你将使用数百首俳句作为训练语料库来“教”你的程序如何创作新的俳句。但是你不能指望CMUdict包含这个语料库中的所有单词，因为一些单词是日语词汇，比如*sake*。正如你已经看到的那样，甚至一些常见的英语单词在CMUdict中也缺失。
- en: So the first order of business is to check all the words in the training corpus
    for membership in CMUdict. To do this, you’ll need to download the training corpus,
    called *train.txt*, from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*.
    Keep it in the same folder as all the Python programs from this chapter. The file
    contains slightly under 300 haiku that have been randomly duplicated around 20
    times to ensure a robust training set.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首要任务是检查训练语料库中的所有单词是否存在于CMUdict中。为此，你需要从 *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    下载训练语料库，名为*train.txt*。将它和本章中的所有Python程序保存在同一个文件夹里。该文件包含略低于300首俳句，并且为了确保训练集的可靠性，这些俳句被随机重复了大约20次。
- en: Once you find words that aren’t in CMUdict, you’ll write a script to help you
    prepare a Python dictionary that uses words as keys and syllable counts as values;
    then you’ll save this dictionary to a file that can support CMUdict in the syllable-counting
    program.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到不在CMUdict中的单词，你将编写一个脚本，帮助你准备一个Python字典，将单词作为键，音节计数作为值；然后你将把这个字典保存到一个文件中，以便在音节计数程序中支持CMUdict。
- en: '***The Missing Words Code***'
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***缺失单词的代码***'
- en: The code in this section will find words missing from CMUdict, help you prepare
    a dictionary of the words and their syllable counts, and save the dictionary to
    a file. You can download the code from *[https://nostarch.com/impracticalpython/](https://nostarch.com/impracticalpython/)*
    as *missing_words_finder.py*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码将查找CMUdict中缺失的单词，帮助你准备一个包含单词及其音节计数的字典，并将字典保存到文件中。你可以从 *[https://nostarch.com/impracticalpython/](https://nostarch.com/impracticalpython/)*
    下载代码，文件名为 *missing_words_finder.py*。
- en: '**Importing Modules, Loading CMUdict, and Defining the main() Function**'
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**导入模块、加载CMUdict并定义main()函数**'
- en: '[Listing 8-1](ch08.xhtml#ch08list1) imports modules, loads CMUdict, and defines
    the `main()` function that runs the program.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-1](ch08.xhtml#ch08list1) 导入模块，加载CMUdict，并定义运行程序的`main()`函数。'
- en: '*missing_words_finder.py,* part 1'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py,* 第1部分'
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 8-1: Imports modules, loads CMUdict, and defines* main()'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：导入模块，加载CMUdict，并定义* main()'
- en: You start with some familiar imports and a few new ones. The `pprint` module
    lets you “pretty print” your dictionary of missing words in an easy-to-read format
    ➊. You’ll write out this same dictionary as persistent data using JavaScript Object
    Notation (`json`), a text-based way for computers to exchange data that works
    well with Python data structures; it’s part of the standard library, standardized
    across multiple languages, and the data is secure and human readable. Finish by
    importing the CMUdict corpus.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你从一些熟悉的导入开始，并添加几个新的模块。`pprint`模块允许你以易于阅读的格式“美化打印”缺失单词的字典 ➊。你将使用JavaScript对象表示法（`json`）将这个字典写出作为持久化数据，`json`是一种基于文本的计算机数据交换方式，能够很好地与Python数据结构配合使用；它是标准库的一部分，在多个编程语言中都得到标准化，数据安全且易于阅读。最后，导入CMUdict语料库。
- en: Next, call the `cmudict` module’s `dict()` method to turn the corpus into a
    dictionary with the words as keys and their phonemes as values ➋.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`cmudict`模块的`dict()`方法，将语料库转化为一个字典，字典的键是单词，值是它们的音素 ➋。
- en: Define the `main()` function that will call functions to load the training corpus,
    find missing words in CMUdict, build a dictionary with the words and their syllable
    counts, and save the results ➌. You’ll define these functions after defining `main()`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`main()`函数，该函数将调用其他函数来加载训练语料库，查找CMUdict中缺失的单词，构建包含单词及其音节计数的字典，并保存结果 ➌。你将在定义`main()`后定义这些函数。
- en: Call the function to load the haiku-training corpus and assign the returned
    set to a variable named `haiku` ➍. Then call the function that will find the missing
    words and return them as a set ➎. Using sets removes duplicate words that you
    don’t need. The `cmudict_missing()` function will also display the number of missing
    words and some other statistics.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 调用该函数加载俳句训练语料库，并将返回的集合赋值给名为`haiku`的变量 ➍。然后调用一个函数，查找缺失的单词并将它们作为集合返回 ➎。使用集合可以去除不需要的重复单词。`cmudict_missing()`函数还会显示缺失单词的数量和一些其他统计信息。
- en: Now, ask the user if they want to manually build a dictionary to address the
    missing words and assign their input to the `build_dict` variable ➏. If they want
    to stop, exit the program; otherwise, call a function to build the dictionary
    ➐ and then another one to save the dictionary. Note that the user isn’t restricted
    to pressing `y` if they want to continue, though that’s the prompt.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，询问用户是否想手动构建一个字典以处理缺失的单词，并将他们的输入赋值给`build_dict`变量 ➏。如果他们想停止，退出程序；否则，调用一个函数来构建字典
    ➐，然后再调用另一个函数保存字典。请注意，用户并不一定非得按`y`键继续，尽管这是提示。
- en: '**Loading the Training Corpus and Finding Missing Words**'
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**加载训练语料库并寻找缺失的单词**'
- en: '[Listing 8-2](ch08.xhtml#ch08list2) loads and prepares the training corpus,
    compares its contents to CMUdict, and keeps track of the differences. These tasks
    are divided between two functions.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-2](ch08.xhtml#ch08list2)加载并准备训练语料库，将其内容与CMUdict进行比较，并跟踪差异。这些任务由两个函数分工完成。'
- en: '*missing_words_finder.py,* part 2'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py*，第二部分'
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 8-2: Defines functions to load the corpus and finds words missing
    from CMUdict*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-2: 定义加载语料库的函数并查找缺失的单词*'
- en: Define a function to read in the words from the haiku-training corpus ➊. The
    haiku in *train.txt* have been duplicated many times, plus the original haiku
    contain duplicate words, like *moon*, *mountain*, and *the*. There’s no point
    in evaluating a word more than once, so load the words as a set to remove repeats
    ➋. You also need to replace hyphens with spaces. Hyphens are popular in haiku,
    but you need to separate the words on either side in order to check for them in
    CMUdict. End the function by returning the `haiku` set ➌.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个函数来读取来自haiku训练语料库中的单词 ➊。*train.txt*中的haiku已经被重复多次，且原始haiku包含了重复的单词，如*moon*、*mountain*和*the*。没有必要多次评估一个单词，因此将这些单词加载为集合来去重
    ➋。你还需要用空格替换连字符。连字符在haiku中很常见，但你需要将它们两侧的单词分开，以便在CMUdict中进行检查。函数结束时返回`haiku`集合 ➌。
- en: It’s now time to find missing words. Define a function, `cmudict_missing()`,
    that takes as an argument a sequence—in this case, the set of words returned by
    the `load_haiku()` function. Start an empty set called `exceptions` to hold any
    missing words ➍. Loop through each word in the haiku set, converting it to lowercase
    and stripping any leading or trailing punctuation. Note that you don’t want to
    remove interior punctuation other than hyphens because CMUdict recognizes words
    like *wouldn’t*. Possessive words typically aren’t in a corpus, so remove the
    trailing *’s*, since this won’t affect the syllable count.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候找出缺失的单词了。定义一个函数`cmudict_missing()`，它以一个序列作为参数——在这种情况下，是`load_haiku()`函数返回的单词集。创建一个空的`exceptions`集合来保存任何缺失的单词
    ➍。遍历每个haiku单词集中的单词，将其转换为小写，并去除任何前后标点。请注意，你不想去除除了连字符之外的内部标点，因为CMUdict识别诸如*wouldn’t*之类的单词。拥有所有格的单词通常不在语料库中，因此去掉尾部的*’s*，因为这不会影响音节数的统计。
- en: '**NOTE**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Be careful of curly apostrophes (’) produced by word-processing software.
    These are different from the straight apostrophes (*`''`*) used in simple text
    editors and shells and may not be recognized by CMUdict. If you add new words
    to either the training or JSON files, be sure to use a straight apostrophe for
    contractions or possessive nouns.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*小心使用文本处理软件生成的弯引号（’）。这些与简单文本编辑器和命令行中使用的直引号（*`''`*）不同，CMUdict可能无法识别这些弯引号。如果你向训练文件或JSON文件中添加新单词，请务必使用直引号来表示缩写或所有格名词。*'
- en: If the word isn’t found in CMUdict, add it to `exceptions` ➎. Print these words
    as a check, along with some basic information ➏, like how many unique words, how
    many missing words, and what percentage of the training corpus are members of
    CMUdict. Set the percent value precision to one decimal place ➐. End the function
    by returning the set of exceptions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在CMUdict中找不到该单词，将其添加到`exceptions` ➎。打印这些单词作为检查，并提供一些基本信息 ➏，如有多少个唯一单词、缺失单词的数量以及训练语料库中有多少百分比的单词是CMUdict的成员。将百分比的精度设置为小数点后一位
    ➐。函数结束时返回异常单词集。
- en: '**Building a Dictionary of Missing Words**'
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**构建缺失词字典**'
- en: '[Listing 8-3](ch08.xhtml#ch08list3) continues the *missing_words_finder.py*
    code, now supplementing CMUdict by assigning syllable counts to the missing words
    as values in a Python dictionary. Since the number of missing words should be
    relatively small, the user can assign the counts manually, so write the code to
    help them interact with the program.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-3](ch08.xhtml#ch08list3) 继续了 *missing_words_finder.py* 代码，现在通过将音节数作为值分配给缺失的单词，来补充
    CMUdict 数据。由于缺失的单词数量应该相对较小，用户可以手动分配这些音节数，因此编写代码帮助用户与程序交互。'
- en: '*missing_words_finder.py,* part 3'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py*，第 3 部分'
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 8-3: Allows the user to manually count syllables and builds a dictionary*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-3：允许用户手动计算音节并构建字典*'
- en: Start by defining a function that takes the set of exceptions returned by the
    `cmudict_missing()` function as an argument ➊. Immediately assign an empty dictionary
    to a variable named `missing_words` ➋. Let the user know that if they make a mistake,
    they’ll have a chance to fix it later; then, use a `for` and `while` loop to go
    through the set of missing words and present each word to the user, asking for
    the number of syllables as input. The word will be the dictionary key, and the
    `num_sylls` variable will become its value ➌. If the input is a digit ➍, break
    out of the loop. Otherwise, warn the user and let the `while` loop request input
    again. If the input passes, add the value to the dictionary as an integer ➎.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从定义一个函数开始，该函数接受由 `cmudict_missing()` 函数返回的异常集合作为参数 ➊。立即将一个空字典赋值给名为 `missing_words`
    的变量 ➋。让用户知道如果他们犯了错误，会有机会稍后修正；然后，使用 `for` 和 `while` 循环遍历缺失的单词集，向用户展示每个单词，询问音节数作为输入。单词将是字典的键，`num_sylls`
    变量将成为它的值 ➌。若输入是数字 ➍，则跳出循环。否则，警告用户并让 `while` 循环再次请求输入。如果输入通过，便将该值作为整数添加到字典中 ➎。
- en: Use `pprint` to display each key/value pair on a separate line, as a check.
    The `width` parameter acts as a newline argument ➏.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pprint` 来单独显示每个键/值对，作为检查。`width` 参数作为换行符号 ➏。
- en: Give the user the opportunity to make last-minute changes to the `missing_words`
    dictionary before saving it as a file ➐. Use triple quotes to present the options
    menu, followed by a `while` loop to keep the options active until the user is
    ready to save ➑. The three options are exiting, which invokes the `break` command;
    adding a new word or changing the syllable count for an existing word, which requires
    the word and syllable count as input; and removing an entry, which uses the dictionary
    `pop()` function ➒. Adding the `None` argument to `pop()` means the program won’t
    raise a `KeyError` if the user enters a word that’s not in the dictionary.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给用户提供机会在保存 `missing_words` 字典为文件之前进行最后修改 ➐。使用三引号展示选项菜单，然后用 `while` 循环保持选项活跃，直到用户准备好保存
    ➑。三个选项是退出，触发 `break` 命令；添加新单词或更改现有单词的音节数，需要输入单词和音节数；以及删除条目，使用字典的 `pop()` 函数 ➒。将
    `None` 参数添加到 `pop()` 中意味着如果用户输入的单词不在字典中，程序不会抛出 `KeyError`。
- en: Finish by giving the user a last look at the dictionary, in the event changes
    were made ➓, and then return it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，给用户一个最后查看字典的机会，以防有更改 ➓，然后返回字典。
- en: '**Saving the Missing Words Dictionary**'
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**保存缺失单词字典**'
- en: '*Persistent data* is data that is preserved after a program terminates. To
    make the `missing_words` dictionary available for use in the *count_syllables.py*
    program you’ll write later in this chapter, you need to save it to a file. [Listing
    8-4](ch08.xhtml#ch08list4) does just that.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*持久数据* 是在程序终止后仍然保存的数据。为了在后面章节中编写的 *count_syllables.py* 程序中使用 `missing_words`
    字典，你需要将它保存到文件中。[列表 8-4](ch08.xhtml#ch08list4) 就是完成这一操作的代码。'
- en: '*missing_words_finder.py,* part 4'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*missing_words_finder.py*，第 4 部分'
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '*Listing 8-4: Saves missing-words dictionary to a file and calls* main()'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-4：将缺失单词字典保存到文件并调用* main()'
- en: Use `json` to save the dictionary. Define a new function that takes the set
    of missing words as an argument ➊. Assign the `missing_words` dictionary to a
    new variable named `json_string` ➋; then, open a file with a *.json* extension
    ➌, write the `json` variable, and close the file. Display the name of the file
    as a reminder to the user ➍. End with the code that lets the program be run as
    a module or in stand-alone mode ➎.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `json` 来保存字典。定义一个新函数，接受缺失单词集合作为参数 ➊。将 `missing_words` 字典赋值给一个名为 `json_string`
    的新变量 ➋；然后，打开一个以 *.json* 为扩展名的文件 ➌，写入 `json` 变量，并关闭文件。提醒用户文件名 ➍。最后，编写代码，让程序可以作为模块或独立模式运行
    ➎。
- en: 'The `json.dumps()` method serializes the `missing_words` dictionary into a
    string. *Serialization* is the process of converting data into a more transmittable
    or storable format. For example:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`json.dumps()`方法将`missing_words`字典序列化为一个字符串。*序列化*是将数据转换为更易传输或存储格式的过程。例如：'
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the serialized dictionary is bound by single quotes, making it a string.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，序列化的字典被单引号包围，使其成为一个字符串。
- en: I’ve provided a partial output from *missing_words_finder.py* here. The list
    of missing words at the top and the manual syllable counts at the bottom have
    both been shortened for brevity.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里提供了*missing_words_finder.py*的部分输出。顶部的缺失单词列表和底部的手动音节计数都已缩短，以便简洁。
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Don’t worry—you won’t have to assign all the syllable counts. The *missing_words.json*
    file is complete and ready for download when you need it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——你不需要为所有单词分配音节计数。*missing_words.json*文件已经完成，并且在需要时可以下载。
- en: '**NOTE**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*For words that have multiple pronunciations, like* jagged *or* our*, you can
    force the program to use the one you prefer by manually opening the* missing_words.json
    *file and adding the key/value pair at any location. I did this with the word*
    sake *so that it uses the two-syllable Japanese pronunciation. Because word membership
    is checked in this file first, it will override the CMUdict value.*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于有多种发音的单词，比如* jagged *或* our*，你可以通过手动打开*missing_words.json*文件并在任意位置添加键/值对，强制程序使用你喜欢的发音。我就是通过这种方式调整了单词*sake*，让它使用两音节的日语发音。因为该文件首先会检查单词的成员资格，它会覆盖CMUdict中的值。*'
- en: Now that you’ve addressed the holes in CMUdict, you’re ready to write the code
    that counts syllables. In [Chapter 9](ch09.xhtml#ch09), you’ll use this code as
    a module in the *markov_haiku.py* program.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经解决了CMUdict中的漏洞，你可以编写计数字节数的代码了。在[第9章](ch09.xhtml#ch09)，你将把这段代码作为模块在*markov_haiku.py*程序中使用。
- en: '**The Count Syllables Code**'
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**计数字节数的代码**'
- en: This section contains the code for the *count_syllables.py* program. You’ll
    also need the *missing_words.json* file you created in the previous section. You
    can download both from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*.
    Keep them together in the same folder.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了*count_syllables.py*程序的代码。你还需要在上一节中创建的*missing_words.json*文件。你可以从*[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*下载这两个文件。将它们保存在同一个文件夹中。
- en: '***Prepping, Loading, and Counting***'
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***准备、加载和计数***'
- en: '[Listing 8-5](ch08.xhtml#ch08list5) imports the necessary modules, loads the
    CMUdict and missing-words dictionaries, and defines a function that will count
    the syllables in a given word or phrase.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-5](ch08.xhtml#ch08list5)导入必要的模块，加载CMUdict和缺失单词字典，并定义一个函数来计数字节数。'
- en: '*count_syllables.py,* part 1'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*count_syllables.py*，第1部分'
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Listing 8-5: Imports modules, loads dictionaries, and counts syllables*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-5：导入模块、加载词典并计数字节数*'
- en: After some familiar imports, load the *missing_words.json* file that contains
    all the words and syllable counts missing from CMUdict. Using `json.load()` restores
    the dictionary that was stored as a string. Next, turn the CMUdict corpus into
    a dictionary using the `dict()` method ➊.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入一些熟悉的模块后，加载包含所有CMUdict中缺失的单词和音节计数的*missing_words.json*文件。使用`json.load()`恢复存储为字符串的字典。接下来，使用`dict()`方法将CMUdict语料库转化为字典➊。
- en: Define a function called `count_syllables()` to count syllables. It should take
    both words *and* phrases, because you’ll ultimately want to pass it lines from
    a haiku. Prep the words as you did previously in the *missing_words_finder.py*
    program ➋.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为`count_syllables()`的函数来计数字节数。它应该能够接受单词*和*短语，因为你最终会希望将它传递给俳句的每一行。按照之前在*missing_words_finder.py*程序中做的那样准备单词➋。
- en: 'Assign a `num_sylls` variable to hold the syllable count and set it to `0`
    ➌. Now start looping through the input words, stripping punctuation and *’s* from
    the ends. Note that you can get tripped up by the format of the apostrophe, so
    two versions are supplied: one with a straight apostrophe and one with a curly
    apostrophe ➍. Next, check whether the word is a member of the small dictionary
    of missing words. If the word is found, add the dictionary value for the word
    to `num_sylls` ➎. Otherwise, start looking through the phonemes, which represent
    a value in CMUdict; for each phoneme, look through the strings that make it up
    ➏. If you find a digit at the end of the string, then you know that phoneme is
    a vowel. To illustrate using the word *aged*, only the first string (highlighted
    in gray here) ends with a digit, so the word contains one vowel:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 分配一个`num_sylls`变量来保存音节计数，并将其初始化为`0` ➌。然后开始循环输入的单词，去掉末尾的标点符号和*'s*。注意，撇号的格式可能会让你感到困扰，因此提供了两种版本：一种是直撇号，另一种是弯撇号
    ➍。接下来，检查单词是否是缺失单词的小字典中的成员。如果找到了该单词，则将该单词在字典中的值加到`num_sylls`中 ➎。否则，开始查看音素，这些音素在
    CMUdict 中表示一个值；对于每个音素，查看它所组成的字符串 ➏。如果在字符串的末尾找到数字，那么你就知道这个音素是元音。以单词*aged*为例，只有第一个字符串（在此处以灰色突出显示）末尾有数字，因此该单词包含一个元音：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that you use the first value (`[0]`) in case there are multiple pronunciations;
    remember that CMUdict represents each pronunciation in a nested list. This may
    result in the occasional error, as the proper choice will depend on context.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果有多个发音，你使用第一个值（`[0]`）；记住，CMUdict 将每个发音表示为一个嵌套列表。这可能会偶尔导致错误，因为正确的选择取决于上下文。
- en: Check whether the end of the phoneme has a digit, and if it does, add `1` to
    `num_sylls` ➐. Finally, return the total syllable count for the word or phrase
    ➑.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 检查音素的末尾是否有数字，如果有，则将`1`加到`num_sylls`中 ➐。最后，返回单词或短语的总音节数 ➑。
- en: '***Defining the main() Function***'
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***定义 main() 函数***'
- en: Completing the program, [Listing 8-6](ch08.xhtml#ch08list6) defines and runs
    the `main()` function. The program will call this function when the program is
    run in stand-alone mode—for example, to spot-check a word or phrase—but it won’t
    be called if you import `syllable_counter` as a module.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成程序后，[清单 8-6](ch08.xhtml#ch08list6) 定义并运行了 `main()` 函数。程序在以独立模式运行时会调用此函数——例如，用于检查单词或短语——但如果你将
    `syllable_counter` 作为模块导入，则不会调用此函数。
- en: '*count_syllables.py,* part 2'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*count_syllables.py,* 第 2 部分'
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*Listing 8-6: Defines and calls the* main() *function*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-6：定义并调用* main() *函数*'
- en: Define the `main()` function and then start a `while` loop ➊. Ask the user to
    input a word or phrase ➋. If the user presses ENTER with no input, the program
    exits ➌. Otherwise, start a `try`-`except` block so the program won’t crash if
    a user enters a word not found in either dictionary ➍. An exception should be
    raised only in stand-alone mode, as you have already prepared the program to run
    on the haiku-training corpus with no exceptions. Within this block, the `count_syllables()`
    function is called and passed the input, and then the results are displayed in
    the interactive shell. End with the standard code that lets the program run stand-alone
    or as a module in another program ➎.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`main()`函数并开始`while`循环 ➊。提示用户输入一个单词或短语 ➋。如果用户按下 ENTER 键但没有输入任何内容，程序将退出 ➌。否则，启动一个`try`-`except`块，以防用户输入的单词在字典中找不到而导致程序崩溃
    ➍。只有在独立模式下，才应引发异常，因为你已经准备好让程序在没有异常的情况下运行在 haiku-training 语料库上。在此块内，调用`count_syllables()`函数并传递输入，然后在交互式命令行中显示结果。最后使用标准代码，使程序能够作为独立程序运行或作为其他程序中的模块
    ➎。
- en: '**A Program to Check Your Program**'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**检查你的程序**'
- en: You have carefully tailored the syllable-counting program to ensure it will
    work with the training corpus. As you continue with the haiku program, you may
    want to add a poem or two to this corpus, but adding new haiku might introduce
    a new word that isn’t in either the CMUdict or your exceptions dictionary. Before
    you go back and rebuild the exceptions dictionary, check whether you really need
    to do so.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经精心调整了音节计数程序，确保它能与训练语料库配合使用。在继续进行 haiku 程序时，你可能想要将一两首诗加入到这个语料库中，但添加新的 haiku
    可能会引入一个新的单词，而这个单词不在 CMUdict 或你的异常字典中。在你回去重新构建异常字典之前，请检查是否真的需要这样做。
- en: '[Listing 8-7](ch08.xhtml#ch08list7) will automatically count the syllables
    in each word in your training corpus and display any word(s) on which it failed.
    You can download this program from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    as *test_count_syllables_w_full_corpus.py*. Keep it in the same folder as *count_syllables.py*,
    *train.txt*, and *missing_words.json*.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-7](ch08.xhtml#ch08list7)将自动计算训练语料库中每个单词的音节数，并显示失败的单词。你可以从* [https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*下载这个程序，文件名为*test_count_syllables_w_full_corpus.py*。将其保存在与*count_syllables.py*、*train.txt*和*missing_words.json*相同的文件夹中。'
- en: '*test_count_syllables_w_full_corpus.py*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*test_count_syllables_w_full_corpus.py*'
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '*Listing 8-7: Attempts to count syllables in words in a training corpus and
    lists all failures*'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-7：尝试计算训练语料库中单词的音节数，并列出所有失败的情况*'
- en: Open your updated *train.txt* training corpus and load it as a set to remove
    duplicates ➊. Start an empty list, called `missing`, to hold any new words for
    which syllables can’t be counted ➋. Words in `missing` won’t be in CMUdict or
    in your `missing_words` dictionary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 打开更新后的*train.txt*训练语料库，并将其加载为集合，以删除重复项 ➊。创建一个空列表，命名为`missing`，用于存储无法计数音节的新单词
    ➋。`missing`中的单词将不会出现在CMUdict或你的`missing_words`字典中。
- en: Loop through the words in the new training corpus ➌ and use a `try-except` block
    to handle the `KeyError` that will be raised if *count_syllables.py* can’t find
    the word ➍. Append this word to the `missing` list and then display the list ➎.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历新训练语料库中的单词 ➌，并使用`try-except`块处理当*count_syllables.py*找不到单词时抛出的`KeyError` ➍。将该单词追加到`missing`列表中，然后显示该列表
    ➎。
- en: If the program displays an empty list, then all the words in the new haiku are
    already present in either CMUdict or *missing_words.json*, so you don’t need to
    make any adjustments. Otherwise, you have the choice of manually adding the words
    to the *missing_words.json* file or rerunning *missing_words_finder.py* to rebuild
    *missing_words.json*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果程序显示一个空列表，则说明新哈库中的所有单词已经存在于CMUdict或*missing_words.json*中，因此不需要进行任何调整。否则，你可以选择手动将单词添加到*missing_words.json*文件中，或者重新运行*missing_words_finder.py*来重建*missing_words.json*。
- en: '**Summary**'
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you’ve learned how to download NLTK and use one of its datasets,
    the Carnegie Mellon Pronouncing Dictionary (CMUdict). You checked the CMUdict
    dataset against a training corpus of haiku and built a supporting Python dictionary
    for any missing words. You saved this Python dictionary as persistent data using
    JavaScript Object Notation (JSON). Finally, you wrote a program that can count
    syllables. In [Chapter 9](ch09.xhtml#ch09), you’ll use your syllable-counting
    program to help you generate novel haiku.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经学习了如何下载NLTK并使用其中一个数据集——卡内基梅隆发音词典（CMUdict）。你将CMUdict数据集与哈库训练语料库进行了比对，并为任何缺失的单词构建了一个支持的Python字典。你使用JavaScript对象表示法（JSON）将这个Python字典保存为持久数据。最后，你编写了一个可以计算音节的程序。在[第9章](ch09.xhtml#ch09)中，你将使用你的音节计数程序帮助生成新的哈库诗。
- en: '**Further Reading**'
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: '*Virtual Muse: Experiments in Computer Poetry* (Wesleyan University Press,
    1996) by Charles O. Hartman is an engaging look at the early collaboration between
    humans and computers to write poetry.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*虚拟缪斯：计算机诗歌实验*（卫斯理大学出版社，1996年）由查尔斯·O·哈特曼（Charles O. Hartman）编写，是一本生动有趣的书，探讨了人类与计算机在早期合作写诗的过程。'
- en: '*Natural Language Processing with Python: Analyzing Text with the Natural Language
    Toolkit* (O’Reilly, 2009) by Steven Bird, Ewan Klein, and Edward Loper is an accessible
    introduction to NLP using Python, with lots of exercises and useful integration
    with the NLTK website. A new version of the book, updated for Python 3 and NLTK
    3, is available online at *[http://www.nltk.org/book/](http://www.nltk.org/book/)*.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 《使用Python进行自然语言处理：使用自然语言工具包分析文本》（O'Reilly，2009）由史蒂文·伯德（Steven Bird）、厄恩·克莱因（Ewan
    Klein）和爱德华·洛珀（Edward Loper）编写，是一本易于入门的NLP书籍，使用Python进行操作，包含大量练习和有用的与NLTK网站的集成。该书的新版已更新至Python
    3和NLTK 3，并可在线访问，网址为* [http://www.nltk.org/book/](http://www.nltk.org/book/)*。
- en: “The Growing Importance of Natural Language Processing” by Stephen F. DeAngelis
    is a *Wired* magazine article on the expanding role of NLP in big data. An online
    version is available at *[https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/](https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/)*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 斯蒂芬·F·德安吉利斯（Stephen F. DeAngelis）的《自然语言处理的重要性日益增长》是*Wired*杂志上一篇关于自然语言处理（NLP）在大数据中日益扩展角色的文章。在线版本可以在*
    [https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/](https://www.wired.com/insights/2014/02/growing-importance-natural-language-processing/)*上查看。
- en: '**Practice Project: Syllable Counter vs. Dictionary File**'
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**实践项目：音节计数器与词典文件**'
- en: 'Write a Python program that lets you test *count_syllables.py* (or any other
    syllable-counting Python code) against a dictionary file. After allowing the user
    to specify how many words to check, choose the words at random and display a listing
    of each word and its syllable count on separate lines. The output should look
    similar to this printout:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个 Python 程序，允许你测试 *count_syllables.py*（或任何其他音节计数 Python 代码）与词典文件的对比。在允许用户指定要检查多少个单词后，随机选择单词并在不同的行上显示每个单词及其音节数。输出应类似于以下打印结果：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Downloadable dictionary files are listed in [Table 2-1](ch02.xhtml#ch02tab1)
    on [page 20](ch02.xhtml#page_20). You can find a solution in the appendix that
    can be downloaded from *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    as *test_count_syllables_w_dict.py*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可下载的词典文件列在[表格 2-1](ch02.xhtml#ch02tab1)中，位于[第 20 页](ch02.xhtml#page_20)。你可以在附录中找到解决方案，并且可以从
    *[https://www.nostarch.com/impracticalpython/](https://www.nostarch.com/impracticalpython/)*
    下载，文件名为 *test_count_syllables_w_dict.py*。
