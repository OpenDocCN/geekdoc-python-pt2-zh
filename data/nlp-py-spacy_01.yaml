- en: '**1'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1**'
- en: HOW NATURAL LANGUAGE PROCESSING WORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理是如何工作的**'
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/comm1.jpg)'
- en: In the 19th century, explorers discovered *rongorongo*, a system of mysterious
    glyphs on the island of Rapa Nui (commonly known as Easter Island). Researchers
    have never succeeded in decoding rongorongo inscriptions or even figuring out
    whether those inscriptions are writing or proto-writing (pictographic symbols
    that convey information but are language independent). Moreover, although we know
    that the creators of the glyphs also erected Moai, the large statues of human
    figures for which the island is most famous, the builders’ motivations remain
    unclear. We can only speculate.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在19世纪，探险者发现了*rongorongo*，这是一种神秘的象形文字系统，位于复活节岛（通常被称为复活节岛）。研究人员从未成功破译rongorongo铭文，甚至没有弄清楚这些铭文是书写还是原始书写（传达信息但与语言无关的图形符号）。此外，虽然我们知道铭文的创造者也建造了摩艾像，这些大大的雕像是复活节岛最著名的象征，但建造者的动机仍然不明。我们只能推测。
- en: If you don’t understand people’s writing—or the way in which they describe things—you
    most likely won’t understand the other aspects of their life, including what they
    do and why they do it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不了解人们的写作方式——或者他们描述事物的方式——你很可能就无法理解他们生活中的其他方面，包括他们做什么以及为什么这么做。
- en: '*Natural language processing (NLP)* is a subfield of artificial intelligence
    that tries to process and analyze natural language data. It includes teaching
    machines to interact with humans in a natural language (a language that developed
    naturally through use). By creating machine learning algorithms designed to work
    with unknown datasets much larger than those two dozen tablets found on Rapa Nui,
    data scientists can learn how we use language. They can also do more than simply
    decipher ancient inscriptions.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理（NLP）* 是人工智能的一个子领域，旨在处理和分析自然语言数据。它包括教机器用自然语言与人类互动（自然语言是通过使用自然发展出来的语言）。通过创建旨在处理比复活节岛上发现的那二十几块石板更大的未知数据集的机器学习算法，数据科学家可以了解我们如何使用语言。它们还能做的不仅仅是解读古代铭文。'
- en: Today, you can use algorithms to observe languages whose semantics and grammar
    rules are well known (unlike the rongorongo inscriptions), and then build applications
    that can programmatically “understand” utterances in that language. Businesses
    can use these applications to relieve humans from boring, monotonous tasks. For
    example, an app might take food orders or answer recurring customer questions
    requesting technical support.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，你可以使用算法来观察那些语义和语法规则已经被很好理解的语言（不同于rongorongo铭文），然后构建能够“理解”该语言发音的应用程序。企业可以利用这些应用程序帮助人类摆脱枯燥、单调的任务。例如，一个应用程序可以接受食物订单或回答客户经常提出的技术支持问题。
- en: Not surprisingly, generating and understanding natural language are the most
    promising and yet challenging tasks involved in NLP. In this book, you’ll use
    the Python programming language to build a natural language processor with spaCy,
    the leading open source Python library for natural language processing. But before
    you get started, this chapter outlines what goes on behind the scenes of building
    a natural language processor.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，生成和理解自然语言是NLP中最具前景且最具挑战性的任务。在本书中，你将使用Python编程语言，通过spaCy（一个领先的开源Python自然语言处理库）来构建一个自然语言处理器。但在开始之前，本章将概述构建自然语言处理器时幕后发生的事情。
- en: '**How Can Computers Understand Language?**'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**计算机如何理解语言？**'
- en: If computers are just emotionless machines, how is it possible to train them
    to understand human language and respond properly? Well, machines can’t understand
    natural language natively. If you want your computer to perform computational
    operations on language data, you need a system that can translate natural language
    words into numbers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算机只是没有情感的机器，那怎么可能训练它们理解人类语言并作出恰当的回应呢？事实上，机器无法本地理解自然语言。如果你希望你的计算机对语言数据进行计算操作，你需要一个能够将自然语言单词转换为数字的系统。
- en: '***Mapping Words and Numbers with Word Embedding***'
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***通过词嵌入映射单词和数字***'
- en: '*Word embedding* is the technique that assigns words to numbers. In word embedding,
    you map words to vectors of real numbers that distribute the meaning of each word
    between the coordinates of the corresponding word vector. Words with similar meanings
    should be nearby in such a vector space, allowing you to determine the meaning
    of a word by the company it keeps.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*词嵌入*是一种将单词与数字关联的技术。在词嵌入中，你将单词映射到实数向量，这些向量将每个单词的意义分布到对应词向量的坐标上。具有相似意义的单词应当在这种向量空间中彼此接近，从而让你通过单词的“邻居”来确定其含义。'
- en: 'The following is a fragment of such an implementation:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此类实现的一个片段：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This fragment maps the words “the,” “and,” “of,” “to,” and “in” to the coordinates
    that follow it. If you represented these coordinates graphically, the words that
    are closer in meaning would be closer in the graph as well. (But this doesn’t
    mean that you can expect the closer-in-meaning words to be grouped together in
    a textual representation like the one whose fragment is shown here. The textual
    representation of a word vector space usually starts with the most common words,
    such as “the,” “and,” and so on. This is the way word vector space generators
    lay out words.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段将单词“the”，“and”，“of”，“to”和“in”映射到它后面的坐标。如果你用图形方式表示这些坐标，那么具有相似意义的单词也会在图形中靠得更近。（但这并不意味着你可以期望在像这里展示的片段这样的文本表示中，相似意义的单词会聚集在一起。词向量空间的文本表示通常以最常见的单词开头，比如“the”，“and”等。这是词向量空间生成器布局单词的方式。）
- en: '**NOTE**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*A graphical representation of a multidimensional vector space can be implemented
    in the form of a 2D or a 3D projection. To prepare such a projection, you can
    use the first two or three principal components (or coordinates) of a vector,
    respectively. We’ll return to this concept in [Chapter 5](../Text/ch05.xhtml#ch05).*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*多维向量空间的图形表示可以通过二维或三维投影的形式来实现。为了准备这样的投影，你可以分别使用向量的前两个或三个主成分（或坐标）。我们将在[第5章](../Text/ch05.xhtml#ch05)回到这一概念。*'
- en: Once you have a matrix that maps words to numeric vectors, you can perform arithmetic
    on those vectors. For example, you can determine the *semantic similarity* (synonymy)
    of words, sentences, and even entire documents. You might use this information
    to programmatically determine what a text is about, for example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了将单词映射到数字向量的矩阵，你就可以对这些向量进行算术运算。例如，你可以确定单词、句子，甚至整个文档的*语义相似性*（同义性）。
- en: Mathematically, determining the semantic similarity between two words is reduced
    to calculating the cosine similarity between the corresponding vectors, or to
    calculating the cosine of the angle between the vectors. Although a complete explanation
    of calculating semantic similarity is outside the scope of this book, [Chapter
    5](../Text/ch05.xhtml#ch05) will cover working with word vectors in more detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，确定两个单词之间的语义相似性就变成了计算它们对应向量之间的余弦相似性，或者计算向量之间的夹角余弦。尽管计算语义相似性的完整解释超出了本书的范围，[第5章](../Text/ch05.xhtml#ch05)将更详细地讲解如何处理词向量。
- en: '***Using Machine Learning for Natural Language Processing***'
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***使用机器学习进行自然语言处理***'
- en: You can generate the numbers to put in the vectors using a machine learning
    algorithm. *Machine learning*, a subfield of artificial intelligence, creates
    computer systems that can automatically learn from data without being explicitly
    programmed. Machine learning algorithms can make predictions about new data, learn
    to recognize images and speech, classify photos and text documents, automate controls,
    and aid in game development.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用机器学习算法来生成用于填充向量的数字。*机器学习*是人工智能的一个子领域，它创建可以从数据中自动学习的计算机系统，而无需明确编程。机器学习算法可以对新数据做出预测，学习识别图像和语音，分类照片和文本文件，自动化控制，并协助游戏开发。
- en: Machine learning lets computers accomplish tasks that would be difficult, if
    not impossible, for them to do otherwise. If you wanted to, say, program a machine
    to play chess using a traditional programming approach in which you explicitly
    specify what the algorithm should do in every context, imagine how many `if...else`
    conditions you’d need to define. Even if you succeed, users of such an application
    will quickly discover weak points in your logic that they can take advantage of
    during the game until you make necessary corrections in the code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习使计算机能够完成那些否则非常困难，甚至不可能完成的任务。举个例子，如果你想用传统的编程方法为机器编写下象棋程序，其中你明确指定算法在每种情境下应该做什么，想象一下你需要定义多少个
    `if...else` 条件。即便你成功了，使用这种应用程序的用户很快就会发现你逻辑中的弱点，并利用这些弱点在游戏中获胜，直到你在代码中做出必要的修正。
- en: In contrast, applications built on machine learning algorithms don’t rely on
    predefined logic but use the capability to learn from past experience instead.
    Thus, a machine learning–based chess application looks for positions it remembers
    from the previous games and makes the move that leads to the best position. It
    stores this past experience in a statistical model, which is discussed in “[What
    Is a Statistical Model in NLP?](../Text/ch01.xhtml#lev5)” on [page 8](../Text/ch01.xhtml#page_8).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于机器学习算法构建的应用程序并不依赖于预定义的逻辑，而是利用从过去经验中学习的能力。因此，基于机器学习的国际象棋应用会寻找它从之前的游戏中记住的位置，并做出能达到最佳位置的移动。它将这些过去的经验存储在一个统计模型中，具体内容请参见
    “[什么是NLP中的统计模型？](../Text/ch01.xhtml#lev5)” 以及 [第8页](../Text/ch01.xhtml#page_8)。
- en: 'In spaCy, aside from generating word vectors, machine learning allows you to
    accomplish three tasks: *syntactic dependency parsing* (determining the relationships
    between words in a sentence), *part-of-speech tagging* (identifying nouns, verbs,
    and other parts of speech), and *named entity recognition* (sorting proper nouns
    into categories like people, organizations, and locations). We’ll discuss all
    of these at length in the following chapters.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，除了生成词向量外，机器学习还可以帮助你完成三个任务：*句法依赖解析*（确定句子中单词之间的关系）、*词性标注*（识别名词、动词及其他词类）和*命名实体识别*（将专有名词分为人名、组织名和地点等类别）。我们将在接下来的章节中详细讨论这些内容。
- en: 'The life cycle of a typical machine learning system has three steps: model
    training, testing, and making predictions.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的机器学习系统的生命周期包括三个步骤：模型训练、测试和预测。
- en: Model Training
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型训练
- en: In the first stage, you train a model by feeding your algorithm a large body
    of data. For these algorithms to give you reliable results, you must provide a
    sufficiently large volume of input—significantly more than the rongorongo tablets,
    for instance. When it comes to NLP, platforms like Wikipedia and Google News contain
    enough text to feed virtually any machine learning algorithm. But if you wanted
    to build a model specific to your particular use case, you might make it learn,
    for example, from customers using your site.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，你通过向算法输入大量数据来训练模型。为了让这些算法给出可靠的结果，你必须提供足够多的输入数据——比如比rongorongo铭文多得多。当涉及到NLP时，像Wikipedia和Google
    News这样的平台就包含了足够的文本，几乎可以喂养任何机器学习算法。但如果你想构建一个特定于你用例的模型，你可能会让它从使用你网站的客户那里学习。
- en: '[Figure 1-1](../Text/ch01.xhtml#ch01fig01) provides a high-level depiction
    of the model training stage.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-1](../Text/ch01.xhtml#ch01fig01)提供了模型训练阶段的高层次描述。'
- en: '![image](../Images/fig1-1.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-1.jpg)'
- en: '*Figure 1-1: Generating a statistical model with a machine learning algorithm
    using a large volume of text data as input*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-1：使用大量文本数据作为输入，通过机器学习算法生成统计模型*'
- en: Your model processes large volumes of text data to understand which words share
    characteristics; then it creates word vectors for those words that reflect those
    shared characteristics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型处理大量的文本数据，以理解哪些单词具有共同的特征；然后它为这些单词创建反映这些共享特征的词向量。
- en: As you’ll learn in “[What Is a Statistical Model in NLP?](../Text/ch01.xhtml#lev5)”
    on [page 8](../Text/ch01.xhtml#page_8), such a word vector space is not the only
    component of a statistical model built for NLP. The actual structure is typically
    more complicated, providing a way to extract linguistic features for each word
    depending on the context in which it appears.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在 “[什么是NLP中的统计模型？](../Text/ch01.xhtml#lev5)” 和 [第8页](../Text/ch01.xhtml#page_8)
    中学到的那样，这样的词向量空间并不是为NLP构建的统计模型的唯一组成部分。实际的结构通常更加复杂，它提供了一种根据上下文提取每个单词语言特征的方法。
- en: In [Chapter 10](../Text/ch10.xhtml#ch10), you’ll learn how to train an already
    existing, pretrained model with new examples and a blank one from scratch.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](../Text/ch10.xhtml#ch10)中，你将学习如何使用新示例训练一个已经存在的、预训练的模型，以及如何从头开始训练一个空白模型。
- en: Testing
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 测试
- en: Once you’ve trained the model, you can optionally test it to find out how well
    it will perform. To test the model, you feed it text it hasn’t seen before to
    check whether it can successfully identify the semantic similarities and other
    features learned during the training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好了模型，你可以选择性地对其进行测试，以了解它的表现如何。测试模型时，你将输入它之前未见过的文本，检查它是否能成功识别训练过程中学到的语义相似性和其他特征。
- en: Making Predictions
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 进行预测
- en: If everything works as expected, you can use the model to make predictions in
    your NLP application. For example, you can use it to predict a dependency tree
    structure over the text you input, as depicted in [Figure 1-2](../Text/ch01.xhtml#ch01fig02).
    A *dependency tree structure* represents the relationships between the words in
    a sentence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按预期工作，你可以使用该模型在你的NLP应用中进行预测。例如，你可以使用它预测你输入的文本的依赖树结构，如[图1-2](../Text/ch01.xhtml#ch01fig02)所示。*依赖树结构*表示句子中单词之间的关系。
- en: '![image](../Images/fig1-2.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-2.jpg)'
- en: '*Figure 1-2: Predicting a dependency tree structure for an utterance using
    a statistical model*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-2：使用统计模型预测发话的依赖树结构*'
- en: Visually, we can represent a dependency tree using arcs of different lengths
    to connect syntactically related pairs of words. For example, the one shown here
    tells us that the verb “sent” agrees with the pronoun “she.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉上，我们可以使用不同长度的弧线来表示依赖树，连接语法上相关的单词对。例如，这里展示的依赖树告诉我们动词“sent”与代词“she”相匹配。
- en: '***Why Use Machine Learning for Natural Language Processing?***'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***为什么在自然语言处理中使用机器学习？***'
- en: Your algorithm’s predictions aren’t statements of fact; they’re typically calculated
    with a degree of certainty. To achieve a higher degree of accuracy, you’ll need
    to implement more complicated algorithms, which are less efficient and less practical
    to implement. Usually, people strive to achieve a reasonable balance between accuracy
    and performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你的算法预测并不是事实陈述；它们通常是根据一定的确定性计算得出的。为了实现更高的准确度，你需要实现更复杂的算法，这些算法效率较低，且实施起来不太实用。通常，人们会努力在准确性和性能之间找到合理的平衡。
- en: Because machine learning models can’t predict perfectly, you might wonder whether
    machine learning is the best approach for building the models used in NLP applications.
    In other words, is there a more reliable approach based on strictly defined rules,
    similar to the one used by compilers and interpreters for processing programming
    languages? The quick answer is no. Here’s why.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型无法做到完美预测，你可能会想知道机器学习是否是用于构建自然语言处理（NLP）应用中模型的最佳方法。换句话说，是否存在一种基于严格定义规则的更可靠方法，类似于编译器和解释器处理编程语言时使用的方法？简短的回答是否定的。原因如下。
- en: To begin with, a programming language contains a relatively small number of
    words. For example, the Java programming language consists of 61 reserved words,
    each of which has a predefined meaning in the language.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，编程语言包含的单词数量相对较少。例如，Java编程语言包含61个保留字，每个保留字在该语言中都有预定义的含义。
- en: By contrast, the *Oxford English Dictionary,* released in 1989, contains 171,476
    entries for words in current use. In 2010, a team of researchers at Harvard University
    and Google counted about 1,022,000 words in a body of digitized texts containing
    approximately 4 percent of all books ever published. The study estimated that
    the language would grow by several thousand words a year. Assigning each word
    to a corresponding number would take too long.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*《牛津英语词典》* 1989年发布，收录了当前使用的171,476个单词。2010年，哈佛大学和谷歌的一个研究团队统计了约1,022,000个单词，这些单词出现在包含约4%已出版图书的数字化文本中。该研究估计，语言每年会增长几千个单词。为每个单词分配一个对应的数字将需要太长时间。
- en: But even if you tried to do it, you’d find it impossible, for several reasons,
    to determine the number of words used in a natural language. First of all, it’s
    unclear what really counts as an individual word. For example, should we count
    the verb “count” as one word, or two, or more? In one scenario, “count” might
    mean “to have value or importance.” In a different scenario, it might mean, “to
    say numbers one after another.” Of course, “count” can also be a noun.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使你尝试去做，你也会发现，出于几个原因，确定自然语言中使用的单词数量几乎是不可能的。首先，什么算作一个独立的单词并不清楚。例如，我们应该把动词“count”算作一个单词，还是两个，或更多呢？在一种情况下，“count”可能意味着“有价值或重要性”。在另一种情况下，它可能意味着“一个接一个地说数字”。当然，“count”也可以是一个名词。
- en: Should we count inflections—plural form of nouns, verb tenses, and so on—as
    separate entities, too? Should we count *loanwords* (words adopted from foreign
    languages), scientific terms, slang, and abbreviations? Evidently, the vocabulary
    of a natural language is defined loosely, because it’s hard to figure out which
    groups of words to include. In a programming language like Java, an attempt to
    include an unknown word in your code will force the compiler to interrupt processing
    with an error.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否也应该把屈折变化——名词的复数形式、动词时态等——当作独立的实体来计算呢？我们是否应该计算*借词*（从外语中借用的单词）、科学术语、俚语和缩写？显然，自然语言的词汇定义很松散，因为很难弄清楚应该包括哪些单词组。在像Java这样的编程语言中，试图在代码中包括一个未知单词会迫使编译器中断处理并报错。
- en: 'A similar situation exists for formal rules. Like its vocabulary, many formal
    rules of a natural language are defined loosely. Some cause controversy, like
    *split infinitives*, a grammatical construction in which an adverb is placed between
    the infinitive verb and its preposition. Here is an example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于形式规则，也存在类似的情况。像它的词汇一样，许多自然语言的形式规则定义得很松散。有些规则会引发争议，比如*分裂不定式*，这是一种语法结构，其中副词被放置在不定式动词和它的介词之间。以下是一个例子：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this example, the adverb “programmatically” separates the preposition and
    infinitive “to extract.” Those who believe that split infinitives are incorrect
    could suggest rewriting the sentence as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，副词“programmatically”将介词和不定式“to extract”分开。那些认为分裂不定式不正确的人可能会建议将句子改写如下：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But regardless of how you feel about split infinitives, your NLP application
    should understand both sentences equally well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但无论你对分裂不定式有什么看法，你的NLP应用程序应该能同等好地理解这两个句子。
- en: 'In contrast, a computer program that processes code written in a programming
    language isn’t designed to handle this kind of problem. The reason is that the
    formal rules for a programming language are strictly defined, leaving no possibility
    for discrepancy. For example, consider the following statement, written in the
    SQL programming language, which you might use to insert data into a database table:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，处理编程语言代码的计算机程序并未设计来处理这种问题。原因在于，编程语言的正式规则被严格定义，不留任何歧义。例如，考虑以下用SQL编程语言编写的语句，你可能会用它来向数据库表中插入数据：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The statement is fairly self-explanatory. Even if you don’t know SQL, you can
    easily guess that the statement is supposed to insert three values into table
    1.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个陈述相当直白。即使你不知道SQL，你也能很容易猜到，这个语句应该是向表1中插入三个值。
- en: 'Now, imagine that you change it as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你把它修改成如下：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From the standpoint of an English-speaking reader, the second statement should
    have the same meaning as the first one. After all, if you read it like an English
    sentence, it still makes sense. But if you try to execute it in a SQL tool, you’ll
    end up with the error `missing INTO keyword`. That’s because a SQL parser—like
    any other parser used in a programming language—relies on hardcoded rules, which
    means you must specify exactly what you want it to do in a way it expects. In
    this case, the SQL parser expects to see the keyword `INTO` right after the keyword
    `INSERT` without any other possible options.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从讲英语读者的角度来看，第二个陈述应该与第一个陈述具有相同的意义。毕竟，如果你像读英语句子那样去读，它仍然是有意义的。但如果你尝试在SQL工具中执行它，你将会遇到错误`missing
    INTO keyword`。那是因为SQL解析器——像任何其他编程语言中使用的解析器——依赖于硬编码的规则，这意味着你必须精确地按照它预期的方式指定你希望它执行的操作。在这个例子中，SQL解析器期望在关键字`INSERT`之后紧跟着看到`INTO`关键字，而没有任何其他可能的选项。
- en: Needless to say, such restrictions are impossible in a natural language. Taking
    all these differences into account, it’s fairly obvious that it would be inefficient
    or even impossible to define a set of formal rules that would specify a computational
    model for a natural language in the way we do for programming languages.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，这种限制在自然语言中是无法实现的。考虑到所有这些差异，显而易见的是，像我们为编程语言定义计算模型那样，定义一套正式规则来指定自然语言的计算模型是低效甚至不可能的。
- en: Instead of a rule-based approach, we use an approach that is based on observations.
    Rather than encoding a language by assigning each word to a predetermined number,
    machine learning algorithms generate statistical models that detect patterns in
    large volumes of language data and then make predictions about the syntactic structure
    in new, previously unseen text data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不使用基于规则的方法，而是采用基于观察的方法。机器学习算法通过生成统计模型来检测大量语言数据中的模式，并对新出现的、以前未见过的文本数据的句法结构做出预测，而不是通过为每个单词分配一个预定的数字来编码语言。
- en: '[Figure 1-3](../Text/ch01.xhtml#ch01fig03) summarizes how language processing
    works for natural languages and programming languages, respectively.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-3](../Text/ch01.xhtml#ch01fig03)总结了自然语言和编程语言处理的工作流程。'
- en: A natural language processing system uses an underlying statistical model to
    make predictions about the meaning of input text and then generates an appropriate
    response. In contrast, a compiler processing programming code applies a set of
    strictly defined rules.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理系统使用一个基础统计模型来预测输入文本的意义，然后生成适当的响应。相比之下，处理编程代码的编译器则应用一套严格定义的规则。
- en: '![image](../Images/fig1-3.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-3.jpg)'
- en: '*Figure 1-3: On the left, a basic workflow for processing natural language;
    on the right, a basic workflow for processing a programming language*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-3：左侧是处理自然语言的基本工作流程，右侧是处理编程语言的基本工作流程*'
- en: '**What Is a Statistical Model in NLP?**'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是自然语言处理中的统计模型？**'
- en: In NLP, a *statistical model* contains estimates for the probability distribution
    of linguistic units, such as words and phrases, allowing you to assign linguistic
    features to them. In probability theory and statistics, a *probability distribution*
    for a particular variable is a table of values that maps all of the possible outcomes
    of that variable to their probabilities of occurrence in an experiment. [Table
    1-1](../Text/ch01.xhtml#ch01tab01) illustrates what a probability distribution
    over part-of-speech tags for the word “count” might look like for a given sentence.
    (Remember that an individual word might act as more than one part of speech, depending
    on the context in which it appears.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，*统计模型*包含了语言单位（如单词和短语）概率分布的估计，从而使你能够为这些单位分配语言特征。在概率论和统计学中，某一变量的*概率分布*是一个数值表，它将该变量的所有可能结果映射到在实验中发生这些结果的概率。[表
    1-1](../Text/ch01.xhtml#ch01tab01)展示了“count”一词在给定句子中可能的词性标签概率分布。（记住，一个单词可能在不同的语境中充当多个词性。）
- en: '**Table 1-1:** An Example of a Probability Distribution for a Linguistic Unit
    in a Context'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1-1：** 语境中语言单位的概率分布示例'
- en: '| **VERB** | **NOUN** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **动词** | **名词** |'
- en: '| --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 78% | 22% |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 78% | 22% |'
- en: Of course, you’ll get other figures for the word “count” used in another context.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你会在另一种语境中看到“count”一词的其他图示。
- en: Statistical language modeling is vital to many natural language processing tasks,
    such as natural language generating and natural language understanding. For this
    reason, a statistical model lies at the heart of virtually any NLP application.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 统计语言建模对于许多自然语言处理任务至关重要，比如自然语言生成和自然语言理解。因此，统计模型几乎是所有自然语言处理应用的核心。
- en: '[Figure 1-4](../Text/ch01.xhtml#ch01fig04) provides a conceptual depiction
    of how an NLP application uses a statistical model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-4](../Text/ch01.xhtml#ch01fig04)提供了自然语言处理应用如何使用统计模型的概念性描述。'
- en: '![image](../Images/fig1-4.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-4.jpg)'
- en: '*Figure 1-4: A high-level conceptual view of an NLP application’s architecture*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-4：自然语言处理应用架构的高层次概念图*'
- en: The application interacts with spaCy’s API, which abstracts the underlying statistical
    model. The statistical model contains information like word vectors and linguistic
    annotations. The linguistic annotations might include features such as part-of-speech
    tags and syntactic annotations. The statistical model also includes a set of machine
    learning algorithms that can extract the necessary pieces of information from
    the stored data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序与spaCy的API进行交互，后者抽象了底层的统计模型。统计模型包含诸如词向量和语言学注释等信息。语言学注释可能包括如词性标签和句法注释等特征。统计模型还包含一组机器学习算法，可以从存储的数据中提取必要的信息。
- en: In real systems, a model’s data is typically stored in a binary format. Binary
    data doesn’t look friendly to humans, but it’s a machine’s best friend because
    it’s easy to store and loads quickly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际系统中，模型的数据通常以二进制格式存储。二进制数据对人类来说不太友好，但它是机器的最佳伙伴，因为它容易存储并且加载速度快。
- en: '***Neural Network Models***'
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***神经网络模型***'
- en: The statistical models used in NLP tools like spaCy for syntactic dependency
    parsing, part-of-speech tagging, and named entity recognition are neural network
    models. A *neural network* is a set of prediction algorithms. It consists of a
    large number of simple processing elements, like neurons in a brain, that interact
    by sending and receiving signals to and from neighboring nodes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP工具（如spaCy）中用于句法依赖解析、词性标注和命名实体识别的统计模型是神经网络模型。一个*神经网络*是一组预测算法。它由大量简单的处理单元组成，类似于大脑中的神经元，这些单元通过发送和接收信号与邻近节点进行交互。
- en: Typically, nodes in a neural network are grouped into layers, including an input
    layer, an output layer, and one or more hidden layers in between. Every node in
    a layer (except the output layer) connects to every node in the successive layer
    through a connection. A connection has a weight value associated with it. During
    the training process, the algorithm adjusts the weights to minimize the error
    it makes in its predictions. This architecture enables a neural network to recognize
    patterns, even in complex data inputs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，节点通常被分组到各个层次中，包括输入层、输出层和位于中间的一个或多个隐藏层。每个层中的节点（除输出层外）通过连接与后续层的每个节点相连。每个连接都有一个与之相关的权重值。在训练过程中，算法会调整权重值，以最小化其预测中的误差。这种架构使得神经网络能够识别模式，甚至在复杂的数据输入中也能做到。
- en: Conceptually, we can represent a neural network as shown in [Figure 1-5](../Text/ch01.xhtml#ch01fig05).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们可以如[图 1-5](../Text/ch01.xhtml#ch01fig05)所示表示神经网络。
- en: '![image](../Images/fig1-5.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-5.jpg)'
- en: '*Figure 1-5: A conceptual depiction of the neural network layout and operations
    at one node*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-5：神经网络布局和一个节点操作的概念性图示*'
- en: When a signal comes in, it’s multiplied by a weight value, which is a real number.
    The input and weight values passed on to a neural network generally come from
    the word vectors generated during the network training.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当信号传入时，它会乘以一个权重值，这是一个实数。传递到神经网络的输入和权重值通常来自网络训练期间生成的词向量。
- en: The neural network adds the results of the multiplications together for each
    node; then it passes the sum on to an activation function. The activation function
    generates a result that typically ranges from 0 to 1, thus producing a new signal
    that is passed on to each node in the successive layer, or, in the case of the
    output layer, an output parameter. Usually, the output layer has as many nodes
    as the number of possible distinct outputs for the given algorithm. For example,
    a neural network implemented for a part-of-speech tagger should have as many nodes
    in the output layer as the number of part-of-speech tags supported by the system,
    as illustrated in [Figure 1-6](../Text/ch01.xhtml#ch01fig06).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络将每个节点的乘法结果相加，然后将总和传递给激活函数。激活函数生成的结果通常在0到1之间，从而产生一个新的信号，传递给后续层中的每个节点，或者在输出层的情况下，传递给输出参数。通常，输出层的节点数量等于给定算法的可能输出结果的数量。例如，用于词性标注的神经网络应该在输出层有与系统支持的词性标签数量相等的节点，正如[图
    1-6](../Text/ch01.xhtml#ch01fig06)所示。
- en: '![image](../Images/fig1-6.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-6.jpg)'
- en: '*Figure 1-6: A simplified depiction of the part-of-speech tagging process*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-6：简化的词性标注过程图示*'
- en: The part-of-speech tagger then outputs a probability distribution over all possible
    parts of speech for a given word in a given context.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，词性标注器会输出给定单词在特定上下文中的所有可能词性的概率分布。
- en: '***Convolutional Neural Networks for NLP***'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***卷积神经网络用于自然语言处理（NLP）***'
- en: The architecture of a real neural network model can be quite complex; it’s formed
    by a number of different layers. Thus, the neural network model used in spaCy
    is a *convolutional neural network (CNN)* that includes a convolutional layer,
    which is shared between the part-of-speech tagger, dependency parser, and named
    entity recognizer. The convolutional layer applies *a set of detection filters
    to regions of input data to test for the presence of specific features.*
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个真实的神经网络模型的架构可能相当复杂，它由多个不同的层组成。因此，spaCy中使用的神经网络模型是一个*卷积神经网络（CNN）*，它包括一个卷积层，该卷积层在词性标注器、依赖解析器和命名实体识别器之间共享。卷积层将*a组检测滤波器应用于输入数据的区域，以测试是否存在特定特征。*
- en: '*As an example, let’s look at how a CNN might work for the part-of-speech tagging
    task when performed on the sentence in the previous example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*作为一个例子，让我们看看卷积神经网络（CNN）在前面示例中执行词性标注任务时的工作原理：*'
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Instead of analyzing each word on its own, the convolutional layer first breaks
    the sentence into chunks. You can consider a sentence in NLP as a matrix in which
    each row represents a word in the form of a vector. So if each word vector had
    300 dimensions and your sentence was five words long, you’d get a 5 × 300 matrix.
    The convolutional layer might use a detection filter size of three, applied to
    three consecutive words, thus having a tiling region size of 3 × 300\. This should
    provide sufficient context for making a decision on what part-of-speech tag each
    word is.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层并不是单独分析每个词，而是首先将句子划分为多个块。你可以将NLP中的句子视为一个矩阵，其中每一行表示一个词的向量形式。所以，如果每个词向量有300维，而你的句子有五个词，那么你就会得到一个5
    × 300的矩阵。卷积层可能使用一个大小为三的检测滤波器，应用于三个连续的词，从而得到一个3 × 300的平铺区域大小。这应该能提供足够的上下文信息，帮助做出每个词的词性标注决定。
- en: The operation of a part-of-speech tagging using the convolutional approach is
    depicted in [Figure 1-7](../Text/ch01.xhtml#ch01fig07).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积方法进行词性标注的操作如[图1-7](../Text/ch01.xhtml#ch01fig07)所示。
- en: '![image](../Images/fig1-7.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-7.jpg)'
- en: '*Figure 1-7: A conceptual look at how the convolutional approach works for
    an NLP task*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-7：卷积方法在NLP任务中如何工作的概念性展示*'
- en: In the preceding example, the most challenging task for the tagger is to determine
    what part-of-speech the word “count” is. The problem is that this word can be
    either a verb or a noun, depending on the context. But this task becomes a breeze
    when the tagger sees the chunk that includes the “we count on” word combination.
    In that context, it becomes clear that the word “count” can be only a verb.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，标注器面临的最具挑战性的任务是确定“count”一词的词性。问题在于，这个词可以是动词，也可以是名词，具体取决于上下文。但是，当标注器看到包含“we
    count on”这个词组时，这个任务就变得轻松了。在这种情况下，很明显，“count”一词只能是动词。
- en: A detailed look under the hood of the convolutional architecture is beyond the
    scope of this book. To learn more about the neural network model architecture
    behind statistical models used in spaCy, check out the “Neural Network Model Architecture”
    section in spaCy’s API documentation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解卷积架构的内部工作原理超出了本书的范围。如需了解有关spaCy中使用的统计模型背后的神经网络模型架构的更多信息，请查看spaCy API文档中的“神经网络模型架构”部分。
- en: '**What Is Still on You**'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**仍需你自己处理的部分**'
- en: As you learned in the preceding section, spaCy uses neural models for syntactic
    dependency parsing, part-of-speech tagging, and named entity recognition. Because
    spaCy provides these functions for you, what’s left for you to do as the developer
    of an NLP application?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在前一节中学到的，spaCy使用神经模型进行句法依赖分析、词性标注和命名实体识别。由于spaCy为你提供了这些功能，那么作为NLP应用程序的开发者，你需要做的还有什么呢？
- en: 'One thing spaCy can’t do for you is recognize the user’s intent. For example,
    suppose you sell clothes and your online application that takes orders has received
    the following request from a user:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy不能为你做的一件事是识别用户的意图。例如，假设你在卖衣服，你的在线应用程序接收到了用户的以下请求：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The application should recognize that the user intends to place an order for
    a pair of jeans.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序应该能够识别用户打算订购一条牛仔裤。
- en: If you use spaCy to perform syntactic dependency parsing for the utterance,
    you’ll get the result shown in [Figure 1-8](../Text/ch01.xhtml#ch01fig08).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用spaCy对这个语句执行句法依赖分析，你将得到[图1-8](../Text/ch01.xhtml#ch01fig08)中所示的结果。
- en: '![image](../Images/fig1-8.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-8.jpg)'
- en: '*Figure 1-8: The dependency tree for the sample utterance*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1-8：样本语句的依赖树*'
- en: Notice that spaCy doesn’t mark anything as the user’s intent in the generated
    tree. In fact, it would be strange if it did so. The reason is that spaCy doesn’t
    know how you’ve implemented your application’s logic and what kind of intent you
    expect to see in particular. Which words to consider the key terms for the task
    of intent recognition is entirely up to you.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，spaCy在生成的语法树中并没有标记任何内容作为用户的意图。事实上，如果它做了这样的标记，反而显得很奇怪。原因在于，spaCy并不知道你是如何实现应用逻辑的，也不知道你具体期待看到哪种意图。选择哪些词作为任务中的关键术语完全由你决定。
- en: 'To extract the meaning from an utterance or a discourse, you need to understand
    the following key aspects: keywords, context, and meaning transition.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从话语或言论中提取意义，你需要理解以下关键方面：关键词、上下文和意义转换。
- en: '***Keywords***'
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***关键词***'
- en: You can use the results of the syntactic dependency parse to choose the most
    important words for meaning recognition. In the “I want to order a pair of jeans.”
    example, the keywords are probably “order” and “jeans.”
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以利用句法依存解析的结果来选择对意义识别最重要的词汇。在“I want to order a pair of jeans.”这个例子中，关键词可能是“order”和“jeans”。
- en: Normally, the transitive verb plus its direct object work well for composing
    the intent. But in this particular example, it’s a bit more complicated. You’ll
    need to navigate the dependency tree and extract “order” (the transitive verb)
    and “jeans” (the object of the preposition related to the direct object “pair”).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，及物动词和它的直接宾语能够很好地构成意图。但在这个特定的例子中，情况有点复杂。你需要遍历依存树，提取“order”（及物动词）和“jeans”（与直接宾语“pair”相关的介词的宾语）。
- en: '***Context***'
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***上下文***'
- en: 'Context can matter when selecting keywords, because the same phrase might have
    different meanings when interpreted in different applications. Suppose you have
    the following utterance to treat:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择关键词时，上下文可能很重要，因为相同的短语在不同应用中可能有不同的含义。假设你需要处理以下的言论：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Depending on the context, this statement might be either a request to subscribe
    to a newspaper or a request to deliver the issue to the door. In the first case,
    the keywords might be “want” and “newspaper.” In the latter case, the keywords
    might be “delivered” and “door.”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文，这句话可能是请求订阅一份报纸，或者请求将报纸送到门口。在第一种情况下，关键词可能是“want”和“newspaper”。在后一种情况下，关键词可能是“delivered”和“door”。
- en: '***Meaning Transition***'
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***意义转换***'
- en: 'Often, people use more than one sentence to express even a very straightforward
    intent. As an example, consider the following discourse:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们会用不止一句话来表达一个非常简单的意图。比如，考虑以下话语：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this discourse, the words reflecting the intent expressed appear in two different
    sentences, as illustrated in [Figure 1-9](../Text/ch01.xhtml#ch01fig09).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论述中，反映意图的词语出现在两个不同的句子中，如[图 1-9](../Text/ch01.xhtml#ch01fig09)所示。
- en: '![image](../Images/fig1-9.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig1-9.jpg)'
- en: '*Figure 1-9: Recognizing the intent of the discourse*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1-9：识别话语的意图*'
- en: 'As you might guess, the words “want” and “jeans” best describe the intent of
    this discourse. The following are the general steps to finding keywords that best
    describe the user’s intent in this particular example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，“want”和“jeans”这两个词最能描述这段话的意图。以下是找到最佳描述用户意图的关键词的一般步骤：
- en: Within the discourse, find a transitive verb in the present tense.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在话语中，找到一个现在时的及物动词。
- en: Find the direct object of the transitive verb found in step 1.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出步骤1中发现的及物动词的直接宾语。
- en: If the direct object found in the previous step is a pro-form, find its antecedent
    in a previous sentence.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在上一步中找到的直接宾语是代词形式，找到它在前文中的先行词。
- en: With spaCy, you can easily implement these steps programmatically. We’ll describe
    this process in detail in [Chapter 8](../Text/ch08.xhtml#ch08).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用spaCy，你可以轻松地以编程方式实现这些步骤。我们将在[第8章](../Text/ch08.xhtml#ch08)中详细描述这一过程。
- en: '**Summary**'
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this chapter, you learned the basics of natural language processing. You
    now know that, unlike humans, machines use vector–based representations of words,
    which allow you to perform math on natural language units, including words, sentences,
    and documents.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学到了自然语言处理的基础知识。你现在知道，与人类不同，机器使用基于向量的单词表示方法，这使你能够对自然语言单元（包括单词、句子和文档）进行数学运算。
- en: You learned that word vectors are implemented in statistical models based on
    the neural network architecture. Then you learned about the tasks that are still
    left up to you as an NLP application developer.*
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你学到的词向量是基于神经网络架构的统计模型实现的。接着，你了解了作为一个NLP应用开发者，仍然需要你自己完成的任务。*
