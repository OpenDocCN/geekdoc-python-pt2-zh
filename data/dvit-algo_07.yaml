- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Language
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 语言
- en: '![](Images/circleart.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/circleart.png)'
- en: In this chapter, we step into the messy world of human language. We’ll start
    by discussing the differences between language and math that make language algorithms
    difficult. We’ll continue by building a space insertion algorithm that can take
    any text in any language and insert spaces wherever they’re missing. After that,
    we’ll build a phrase completion algorithm that can imitate the style of a writer
    and find the most fitting next word in a phrase.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将进入人类语言的复杂世界。我们将首先讨论语言与数学之间的差异，这些差异使得语言算法变得困难。接下来，我们将构建一个空格插入算法，它能够处理任何语言的文本，并在缺少空格的地方插入空格。之后，我们将构建一个短语补全算法，能够模仿作家的风格，并找到短语中最合适的下一个单词。
- en: 'The algorithms in this chapter rely heavily on two tools that we haven’t used
    before: list comprehensions and corpuses. *List comprehensions* enable us to quickly
    generate lists using the logic of loops and iterations. They’re optimized to run
    very quickly in Python and they’re easy to write concisely, but they can be hard
    to read and their syntax takes some getting used to. A *corpus* is a body of text
    that will “teach” our algorithm the language and style we want it to use.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的算法 heavily 依赖于我们之前没有使用过的两种工具：列表推导式和语料库。*列表推导式*使我们能够利用循环和迭代的逻辑快速生成列表。它们在
    Python 中经过优化，运行非常快，而且容易简洁地编写，但它们可能难以阅读，且其语法需要一些时间来适应。*语料库*是指一组文本，它将“教”我们的算法我们希望它使用的语言和风格。
- en: Why Language Algorithms Are Hard
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么语言算法如此困难
- en: The application of algorithmic thinking to language goes back at least as far
    as Descartes, who noticed that although there are infinite numbers, anyone with
    a rudimentary understanding of arithmetic knows how to create or interpret a number
    they’ve never encountered before. For example, maybe you’ve never encountered
    the number 14,326—never counted that high, never read a financial report about
    that many dollars, never mashed exactly those keys on the keyboard. And yet I’m
    confident that you can easily grasp exactly how high it is, what numbers are higher
    or lower than it, and how to manipulate it in equations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将算法思维应用到语言上，至少可以追溯到笛卡尔，他注意到，尽管数字是无限的，但任何具备基本算术知识的人都知道如何创建或解释一个他们从未遇到过的数字。例如，也许你从未遇到过数字14,326——从未数过那么高的数，没看过涉及这么多美元的财务报告，也没在键盘上敲过正好那几个键。但我敢肯定，你可以轻松理解这个数字有多大，哪些数字比它大，哪些比它小，以及如何在方程中操作它。
- en: The algorithm that lets us easily understand hitherto unimagined numbers is
    simply a combination of the 10 digits (0–9), memorized in order, and the place
    system. We know that 14,326 is one higher than 14,325 because the digit 6 comes
    one after the digit 5 in order, they occupy the same place in their respective
    numbers, and the digits in all the other places are the same. Knowing the digits
    and the place system enables us to instantly have an idea of how 14,326 is similar
    to 14,325 and how both are larger than 12 and smaller than 1,000,000\. We can
    also understand at a glance that 14,326 is similar to 4,326 in some respects but
    differs greatly in size.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们轻松理解前所未见的数字的算法，实际上只是将10个数字（0–9）按顺序记住，并结合使用位置系统。我们知道14,326比14,325大一个单位，因为数字6紧跟在数字5后面，它们在各自的数字中占据相同的位置，其他所有位置的数字都是一样的。知道这些数字和位置系统让我们能立刻了解14,326与14,325的相似之处，以及它们都大于12且小于1,000,000。我们还可以一眼看出，14,326在某些方面与4,326相似，但在大小上有很大的区别。
- en: Language is not the same. If you are learning English and you see the word *stage*
    for the first time, you cannot reliably reason about its meaning simply by noting
    its similarity to *stale* or *stake* or *state* or *stave* or *stade* or *sage*,
    even though those words differ from *stage* about as much as 14,326 does from
    14,325\. Nor can you reliably suppose that a bacterium is larger than an elk because
    of the number of syllables and characters in the words. Even supposedly reliable
    rules of language, like adding *s* to form plurals in English, can lead us badly
    astray when we infer that the word “princes” refers to less of something than
    the word “princess.”
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 语言则不同。如果你正在学习英语，当你第一次看到单词*stage*时，你不能仅凭它与*stale*、*stake*、*state*、*stave*、*stade*
    或 *sage* 的相似性来推断其含义，尽管这些词与*stage*的区别就像14,326与14,325的区别一样。你也不能仅凭单词的音节数和字符数来推测细菌比麋鹿要大。即使是那些看似可靠的语言规则，比如在英语中通过加*s*来构成复数，当我们推测“princes”比“princess”少一些某物时，也会把我们引入歧途。
- en: In order to use algorithms with language, we must either make language simpler,
    so that the short mathematical algorithms we have explored so far can reliably
    work with it, or make our algorithms smarter, so that they can deal with the messy
    complexity of human language as it has developed naturally. We’ll do the latter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用算法处理语言，我们必须让语言变得更简单，这样我们迄今为止探索的简短数学算法就可以可靠地与之配合使用，或者让我们的算法更智能，这样它们就能够处理人类语言自然发展所带来的混乱复杂性。我们将选择后者。
- en: Space Insertion
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空格插入
- en: Imagine that you are the chief algorithm officer at a large old company that
    has a warehouse full of handwritten paper records. The chief record digitization
    officer has been conducting a long-term project of scanning those paper records
    to image files, and then using text recognition technology to convert the images
    to text that can be easily stored in the company’s databases. However, some of
    the handwriting on the records is awful and the text recognition technology is
    imperfect, so the final digital text that is extracted from a paper record is
    sometimes incorrect. You’ve been given only the digitized text and you’re asked
    to find a way to correct the mistakes without referring to the paper originals.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一个大型老公司中的首席算法官，该公司拥有一个满是手写纸质记录的仓库。首席记录数字化官员一直在进行一个长期项目，将这些纸质记录扫描成图像文件，然后使用文本识别技术将图像转换为可以轻松存储在公司数据库中的文本。然而，部分手写记录非常难以辨认，而文本识别技术也不完美，因此从纸质记录中提取的最终数字文本有时会出现错误。你只收到了数字化后的文本，要求你找到一种方法来纠正这些错误，而不参考纸质原件。
- en: 'Suppose that you read the first digitized sentence into Python and find that
    it’s a quote from G. K. Chesterton: “The one perfectly divine thing, the one glimpse
    of God’s paradise given on earth, is to fight a losing battle—and not lose it.”
    You take this imperfectly digitized text and store it in a variable called `text`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你将第一个数字化的句子读取到Python中，并发现它是G. K. Chesterton的名言：“唯一完全神圣的事情，唯一在地球上看到的上帝乐园的片刻，是与一场失败的战斗作斗争——而且没有失去它。”你将这段不完全数字化的文本存储在一个叫做`text`的变量中：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You’ll notice that this text is in English, and while the spelling of each
    word is correct, there are missing spaces throughout: `oneperfectly` should actually
    be `one perfectly`, `paradisegiven` should be `paradise given`, and so on. (Missing
    a space is uncommon for humans, but text recognition technology often makes this
    kind of mistake.) In order to do your job, you’ll have to insert spaces at the
    appropriate spots in this text. For a fluent English speaker, this task may not
    seem difficult to do manually. However, imagine that you need to do it quickly
    for millions of scanned pages—you will obviously need to write an algorithm that
    can do it for you.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这段文本是英文的，虽然每个单词的拼写都是正确的，但其中缺少了空格：`oneperfectly`实际上应该是`one perfectly`，`paradisegiven`应该是`paradise
    given`，等等。（缺少空格对于人类来说不常见，但文本识别技术经常会犯这种错误。）为了完成你的任务，你需要在文本中的适当位置插入空格。对于一位流利的英语使用者来说，这个任务手动完成可能不难。然而，想象一下你需要为数百万页扫描文件快速完成这一任务——显然，你需要编写一个能够为你自动完成这一任务的算法。
- en: Defining a Word List and Finding Words
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义单词列表并查找单词
- en: 'The first thing we will do is teach our algorithm some English words. This
    isn’t very hard: we can define a list called `word_list` and populate it with
    words. Let’s start with just a few words:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的事情是教我们的算法一些英语单词。这并不困难：我们可以定义一个叫做`word_list`的列表，并用单词填充它。我们先从几个单词开始：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this chapter, we’ll create and manipulate lists using list comprehensions,
    which you’ll probably like after you get used to them. The following is a very
    simple list comprehension that creates a copy of our `word_list`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将使用列表推导式来创建和操作列表，相信你习惯之后会喜欢这种方式。以下是一个非常简单的列表推导式，它创建了我们`word_list`的副本：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see that the syntax `for word in word_list` is very similar to the
    syntax for a `for` loop. But we don’t need a colon or extra lines. In this case,
    the list comprehension is as simple as possible, just specifying that we want
    each word in `word_list` to be in our new list, `word_list_copy`. This may not
    be so useful, but we can concisely add logic to make it more useful. For example,
    if we want to find every word in our word list that contains the letter *n*, all
    it takes is the simple addition of an `if` statement:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，语法`for word in word_list`与`for`循环的语法非常相似。但我们不需要冒号或额外的行。在这种情况下，列表推导式尽可能简单，只是指定我们希望`word_list`中的每个单词都出现在我们新的列表`word_list_copy`中。这可能没什么用，但我们可以简洁地添加逻辑使它更有用。例如，如果我们想找到`word_list`中每个包含字母*n*的单词，只需简单地添加一个`if`语句：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can run `print(has_n)` to see that the result is what we expect:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行`print(has_n)`来查看结果是否如我们所料：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Later in the chapter, you’ll see more complex list comprehensions, including
    some that have nested loops. However, all of them follow the same basic pattern:
    a `for` loop specifying iteration, with optional `if` statements describing the
    logic of what we want to select for our final list output.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，你将看到更复杂的列表推导式，包括一些包含嵌套循环的例子。然而，它们都遵循相同的基本模式：一个`for`循环指定迭代，附加的`if`语句描述我们想要选择的最终列表输出的逻辑。
- en: 'We’ll use Python’s `re` module to access text manipulation tools. One of `re`’s
    useful functions is `finditer()`, which can search our text to find the location
    of any word in our `word_list`. We use `finditer()` in a list comprehension like
    so:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python的`re`模块来访问文本处理工具。`re`模块的一个有用函数是`finditer()`，它可以在我们的文本中搜索，找到`word_list`中任何单词的位置。我们可以像下面这样在列表推导式中使用`finditer()`：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That line is a little dense, so take a moment to make sure you understand it.
    We’re defining a variable called `locs`, short for “locations”; this variable
    will contain the locations in the text of every word in our word list. We’ll use
    a list comprehension to get this list of locations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行有点密集，花点时间确保你理解它。我们定义了一个名为`locs`的变量，`locs`是“位置”的缩写；这个变量将包含文本中`word_list`中每个单词的位置。我们将使用列表推导式来获得这个位置列表。
- en: The list comprehension takes place inside the square brackets (`[]`). We use
    `for word in word_list` to iterate over every word in our `word_list`. For each
    word, we call `re.finditer()`, which finds the selected word in our text and returns
    a list of every location where that word occurs. We iterate over these locations,
    and each individual location is stored in `m`. When we access `m.start()` and
    `m.end()`, we’ll get the location in the text of the beginning and end of the
    word, respectively. Notice—and get used to—the order of the `for` loops, since
    some people find it the opposite of the order they expected.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表推导式发生在方括号（`[]`）内。我们使用`for word in word_list`来迭代`word_list`中的每个单词。对于每个单词，我们调用`re.finditer()`，它会在我们的文本中找到该单词，并返回该单词出现的每个位置的列表。我们迭代这些位置，每个位置存储在`m`中。当我们访问`m.start()`和`m.end()`时，我们会分别得到该单词在文本中开始和结束的位置。注意——并习惯于——`for`循环的顺序，因为有些人会觉得它与他们预期的顺序相反。
- en: 'The whole list comprehension is enveloped by `list(set())`. This is a convenient
    way to get a list that contains only unique values with no duplicates. Our list
    comprehension alone might have multiple identical elements, but converting it
    to a set automatically removes duplicates, and then converting it back to a list
    puts it in the format we want: a list of unique word locations. You can run `print(locs)`
    to see the result of the whole operation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 整个列表推导式被`list(set())`包裹住。这是一种方便的方法，可以得到一个仅包含唯一值且没有重复项的列表。我们的列表推导式可能包含多个相同的元素，但将其转换为集合会自动去除重复项，然后再将其转换回列表，得到我们想要的格式：一个包含唯一单词位置的列表。你可以运行`print(locs)`来查看整个操作的结果：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In Python, ordered pairs like these are called *tuples*, and these tuples show
    the locations of each word from `word_list` in our text. For example, when we
    run `text[17:23]` (using the numbers from the third tuple in the preceding list),
    we find that it’s `divine`. Here, `d` is the 17th character of our text, `i` is
    the 18th character of our text, and so on until `e`, the final letter of `divine`,
    is the 22nd character of our text, so the tuple is rounded off with 23\. You can
    check that the other tuples also refer to the locations of words in our `word_list`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，像这样的有序对被称为*元组*，这些元组显示了`word_list`中每个单词在文本中的位置。例如，当我们运行`text[17:23]`（使用前面列表中第三个元组的数字时），我们发现它是`divine`。这里，`d`是我们文本中的第17个字符，`i`是第18个字符，依此类推，直到`e`，`divine`的最后一个字母，是我们文本中的第22个字符，因此元组以23结尾。你可以检查其他元组，它们也指示了`word_list`中单词的位置。
- en: 'Notice that `text[4:7]` is `one`, and `text[7:16]` is `perfectly`. The end
    of the word `one` runs into the beginning of the word `perfectly` without any
    intervening space. If we hadn’t noticed that immediately by reading the text,
    we could have caught it by looking at the tuples (4, 7) and (7, 16) in our `locs`
    variable: since 7 is the second element of (4, 7) and also the first element of
    (7, 16), we know that one word ends in the same index where another word begins.
    In order to find places where we need to insert spaces, we’ll look for cases like
    this: where the end of one valid word is at the same place as the beginning of
    another valid word.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`text[4:7]`是`one`，而`text[7:16]`是`perfectly`。单词`one`的结尾与单词`perfectly`的开头相接，没有任何间隔的空格。如果我们没有通过阅读文本立即注意到这一点，我们可以通过查看`locs`变量中的元组(4,
    7)和(7, 16)来捕捉到这一点：由于7是(4, 7)的第二个元素，同时也是(7, 16)的第一个元素，我们知道一个单词的结尾恰好与另一个单词的开头相同。为了找到需要插入空格的位置，我们将寻找这样的情况：一个有效单词的结尾恰好与另一个有效单词的开头重合。
- en: Dealing with Compound Words
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理复合词
- en: Unfortunately, two valid words appearing together without a space is not conclusive
    evidence that a space is missing. Consider the word *butterfly*. We know that
    *butter* is a valid word and *fly* is a valid word, but we can’t necessarily conclude
    that *butterfly* was written in error, because *butterfly* is also a valid word.
    So we need to check not only for valid words that appear together without a space
    but also for valid words that, when mashed together without a space, do not together
    form another valid word. This means that in our text, we need to check whether
    `oneperfectly` is a word, whether `paradisegiven` is a word, and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，两个有效单词并排出现而没有空格，并不能作为缺少空格的确凿证据。考虑单词*butterfly*。我们知道*butter*是一个有效单词，*fly*也是一个有效单词，但我们不能简单地得出*butterfly*是错误拼写的结论，因为*butterfly*也是一个有效单词。所以我们不仅需要检查那些没有空格而并排出现的有效单词，还需要检查那些没有空格并排在一起后，组合成另一个无效单词的情况。这意味着在我们的文本中，我们需要检查`oneperfectly`是否是一个有效单词，`paradisegiven`是否是一个有效单词，等等。
- en: In order to check this, we need to find all the spaces in our text. We can look
    at all the substrings between two consecutive spaces and call those potential
    words. If a potential word is not in our word list, then we’ll conclude that it’s
    invalid. We can check each invalid word to see whether it’s made up of a combination
    of two smaller words; if it is, we’ll conclude that there’s a missing space and
    add it back in, right between the two valid words that have combined to form the
    invalid word.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这一点，我们需要找到文本中的所有空格。我们可以查看两个连续空格之间的所有子字符串，并将其称为潜在单词。如果一个潜在单词不在我们的单词列表中，那么我们将认为它是无效的。我们可以检查每个无效单词，看看它是否由两个较小的单词组合而成；如果是，我们将认为缺少了一个空格，并将其添加回去，正好插入在两个有效单词之间，它们合并成了无效单词。
- en: Checking Between Existing Spaces for Potential Words
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查现有空格之间的潜在单词
- en: 'We can use `re.finditer()` again to find all the spaces in our text, which
    we’ll store in a variable called `spacestarts`. We’ll also add two more elements
    to our `spacestarts` variable: one to represent the location of the beginning
    of the text and one to represent the location of the end. This ensures that we
    find every potential word, since words at the very beginning and end will be the
    only words that are not between spaces. We also add a line that sorts the `spacestarts`
    list:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用`re.finditer()`来查找文本中的所有空格，并将它们存储在名为`spacestarts`的变量中。我们还将向`spacestarts`变量添加两个元素：一个表示文本开始位置，另一个表示文本结束位置。这样可以确保我们找到每个潜在的单词，因为文本最开始和最末尾的单词是唯一不在空格之间的单词。我们还添加了一行来对`spacestarts`列表进行排序：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The list `spacestarts` records the locations of the spaces in our text. We
    got these locations by using a list comprehension and the `re.finditer()` tool.
    In this case, `re.finditer()` finds the location of every space in the text and
    stores it in a list, which refers to each individual element as `m`. For each
    of those `m` elements, which are spaces, we get the location where the space begins
    by using the `start()` function. We are looking for potential words between those
    spaces. It will be useful to have another list that records the locations of characters
    that come just after a space; these will be the locations of the first character
    of each potential word. We’ll call that list `spacestarts_affine`, since in technical
    terms, this new list is an affine transformation of the `spacestarts` list. *Affine*
    is often used to refer to linear transformations, such as adding 1 to each location,
    which we’ll do here. We’ll also sort this list:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 `spacestarts` 记录了文本中空格的位置。我们通过使用列表推导式和 `re.finditer()` 工具获得这些位置。在这种情况下，`re.finditer()`
    查找文本中每个空格的位置并将其存储在一个列表中，每个元素被称为 `m`。对于每个 `m` 元素，也就是空格，我们通过使用 `start()` 函数获取空格的起始位置。我们正在寻找那些空格之间的潜在单词。我们还需要另一个列表，记录空格后紧跟的字符位置，这些将是每个潜在单词第一个字符的位置。我们将这个列表称为
    `spacestarts_affine`，因为从技术角度来说，这个新列表是 `spacestarts` 列表的仿射变换。*仿射* 通常用于指代线性变换，比如在每个位置上加1，我们将在这里这么做。我们还将对这个列表进行排序：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we can get all the substrings that are between two spaces:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以获取所有位于两个空格之间的子字符串：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The variable we’re creating here is called `between_spaces`, and it’s a list
    of tuples of the form (*<location of beginning of substring>*, *<location of end
    of substring>*), like (17, 23). The way we get these tuples is through a list
    comprehension. This list comprehension iterates over `k`. In this case, `k` takes
    on the values of integers between 0 and one less than the length of the `spacestarts`
    list. For each `k`, we will generate one tuple. The first element of the tuple
    is `spacestarts[k]+1`, which is one position after the location of each space.
    The second element of the tuple is `spacestarts[k+1]`, which is the location of
    the next space in the text. This way, our final output contains tuples that indicate
    the beginning and end of each substring between spaces.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里创建的变量叫做 `between_spaces`，它是一个包含元组的列表，元组的形式是 (*<子字符串起始位置>*, *<子字符串结束位置>*)，像
    (17, 23)。我们通过列表推导式来获取这些元组。这个列表推导式会遍历 `k`。在这种情况下，`k` 取值为从 0 到 `spacestarts` 列表长度减一之间的整数。对于每个
    `k`，我们将生成一个元组。元组的第一个元素是 `spacestarts[k]+1`，它是每个空格位置之后的位置。第二个元素是 `spacestarts[k+1]`，它是文本中下一个空格的位置。通过这种方式，我们最终输出的将是指示空格之间每个子字符串起始和结束位置的元组。
- en: 'Now, consider all of the potential words that are between spaces, and find
    the ones that are not valid (not in our word list):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑所有位于空格之间的潜在单词，并找出那些无效的单词（不在我们的单词列表中）：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Looking at `between_spaces_notvalid`, we can see that it’s a list of the locations
    of all invalid potential words in our text:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 `between_spaces_notvalid`，我们可以看到它是一个包含所有无效潜在单词位置的列表：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Our code thinks that all these locations refer to invalid words. However, if
    you look at some of the words referred to here, they look pretty valid. For example,
    `text[103:106]` outputs the valid word `and`. The reason our code thinks that
    `and` is an invalid word is that it isn’t in our word list. Of course, we could
    add it to our word list manually and continue using that approach as we need our
    code to recognize words. But remember that we want this space insertion algorithm
    to work for millions of pages of scanned text, and they may contain many thousands
    of unique words. It would be helpful if we could import a word list that already
    contained a substantial body of valid English words. Such a collection of words
    is referred to as a *corpus.*
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码认为所有这些位置都指向无效的单词。然而，如果你查看这里提到的一些单词，它们看起来是相当有效的。例如，`text[103:106]` 输出了有效单词
    `and`。我们的代码认为 `and` 是无效单词的原因是它不在我们的单词列表中。当然，我们可以手动将其添加到我们的单词列表中，并继续使用这种方法，因为我们需要让代码识别单词。但是请记住，我们希望这个空格插入算法能够处理数百万页扫描的文本，而这些文本可能包含成千上万个独特的单词。如果我们能够导入一个已经包含大量有效英语单词的单词列表，那将会很有帮助。这样的单词集合被称为
    *语料库*。
- en: Using an Imported Corpus to Check for Valid Words
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用导入的语料库来检查有效单词
- en: 'Luckily, there are existing Python modules that allow us to import a full corpus
    with just a few lines. First, we need to download the corpus:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些现成的Python模块可以让我们只用几行代码就能导入完整的语料库。首先，我们需要下载语料库：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We’ve downloaded a corpus called `brown` from the module called `nltk`. Next,
    we’ll import the corpus:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从名为`nltk`的模块中下载了一个名为`brown`的语料库。接下来，我们将导入这个语料库：
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have imported the corpus and converted its collection of words into a Python
    list. Before we use this new `word_list`, however, we should do some cleanup to
    remove what it thinks are words but are actually punctuation marks:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经导入了语料库，并将其中的单词集合转换成了一个Python列表。然而，在使用这个新的`word_list`之前，我们应该进行一些清理，去除它认为是单词但实际上是标点符号的部分：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'These lines use the `remove()` and `replace()` functions to replace punctuation
    with empty strings and then remove the empty strings. Now that we have a suitable
    word list, we’ll be able to recognize invalid words more accurately. We can rerun
    our check for invalid words using our new `word_list` and get better results:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行使用了`remove()`和`replace()`函数，将标点符号替换为空字符串，并去除空字符串。现在我们有了一个合适的单词列表，我们将能够更准确地识别无效单词。我们可以使用新的`word_list`重新运行无效单词检查，并获得更好的结果：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When we print the list `between_spaces_notvalid`, we get a shorter and more
    accurate list:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印列表`between_spaces_notvalid`时，得到的是一个更短且更准确的列表：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have found the invalid potential words in our text, we’ll check
    in our word list for words that could be combined to form those invalid words.
    We can begin by looking for words that start just after a space. These words could
    be the first half of an invalid word:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了文本中无效的潜在单词，我们将在单词列表中查找可能组合成这些无效单词的单词。我们可以从查找那些在空格后开始的单词入手。这些单词可能是无效单词的前半部分：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our list comprehension iterates over every element of our `locs` variable, which
    contains the location of every word in the text. It checks whether `locs[0]`,
    the beginning of the word, is in `spacestarts_affine`, a list containing the characters
    that come just after a space. Then it checks whether `loc[1]` is not in `spacestarts`,
    which checks whether the word ends where a space begins. If a word starts after
    a space and doesn’t end at the same place as a space, we put it in our `partial_words`
    variable, because this could be a word that needs to have a space inserted after
    it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列表推导式会遍历`locs`变量中的每个元素，该变量包含文本中每个单词的位置。它会检查`locs[0]`（单词的起始位置）是否在`spacestarts_affine`中，`spacestarts_affine`是一个包含紧接着空格后的字符的列表。接着，它会检查`loc[1]`是否不在`spacestarts`中，`spacestarts`检查单词是否在空格开始的位置结束。如果一个单词在空格后开始，并且不在空格的位置结束，我们将它放入`partial_words`变量中，因为这可能是一个需要在其后插入空格的单词。
- en: 'Next, let’s look for words that end with a space. These could be the second
    half of an invalid word. To find them, we make some small changes to the previous
    logic:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来查找以空格结尾的单词。这些单词可能是无效单词的后半部分。为了找到它们，我们对之前的逻辑做了一些小的调整：
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we can start inserting spaces.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始插入空格。
- en: Finding First and Second Halves of Potential Words
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找潜在单词的前半部分和后半部分
- en: 'Let’s start by inserting a space into `oneperfectly`. We’ll define a variable
    called `loc` that stores the location of `oneperfectly` in our text:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`oneperfectly`开始插入空格。我们将定义一个名为`loc`的变量，存储`oneperfectly`在文本中的位置：
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We now need to check whether any of the words in `partial_words` could be the
    first half of `oneperfectly`. For a valid word to be the first half of `oneperfectly`,
    it would have to have the same beginning location in the text , but not the same
    ending location, as `oneperfectly`. We’ll write a list comprehension that finds
    the ending location of every valid word that begins at the same location as `oneperfectly`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要检查`partial_words`中的任何单词是否可能是`oneperfectly`的前半部分。要成为`oneperfectly`的前半部分，一个有效的单词必须在文本中与`oneperfectly`有相同的起始位置，但结束位置不同。我们将编写一个列表推导式，找到每个有效单词的结束位置，这些单词与`oneperfectly`的起始位置相同：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We’ve specified `loc2[0] == loc[0]`, which says that our valid word must start
    at the same place as `oneperfectly`. We’ve also specified `(loc2[1]-loc[0])>1`,
    which ensures that the valid word we find is more than one character long. This
    is not strictly necessary, but it can help us avoid false positives. Think of
    words like *avoid*, *aside*, *along*, *irate*, and *iconic*, in which the first
    letter could be considered a word on its own but probably shouldn’t be.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了`loc2[0] == loc[0]`，这意味着我们的有效单词必须与`oneperfectly`在同一位置开始。我们还指定了`(loc2[1]-loc[0])>1`，这确保我们找到的有效单词长度大于一个字符。这不是严格必要的，但它可以帮助我们避免误报。想想像*avoid*、*aside*、*along*、*irate*和*iconic*这样的单词，其中第一个字母本身可能被认为是一个单词，但可能不应该这么处理。
- en: 'Our list `endsofbeginnings` should include the ending location of every valid
    word that begins at the same place as `oneperfectly`. Let’s use a list comprehension
    to create a similar variable, called `beginningsofends`, that will find the beginning
    location of every valid word that ends at the same place as `oneperfectly`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列表`endsofbeginnings`应该包含每个有效单词的结束位置，这些单词的开始位置与`oneperfectly`相同。我们可以使用列表推导式创建一个类似的变量，叫做`beginningsofends`，它会找到每个有效单词的开始位置，这些单词的结束位置与`oneperfectly`相同：
- en: '[PRE21]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We’ve specified `loc2[1] == loc[1]`, which says that our valid word must end
    at the same place as `oneperfectly`. We’ve also specified `(loc2[1]-loc[0])>1`,
    which ensures that the valid word we find is more than one character long, just
    as we did before.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了`loc2[1] == loc[1]`，这意味着我们的有效单词必须和`oneperfectly`在同一位置结束。我们还指定了`(loc2[1]-loc[0])>1`，这确保我们找到的有效单词长度大于一个字符，就像我们之前所做的那样。
- en: 'We’re almost home; we just need to find whether any locations are contained
    in both `endsofbeginnings` and `beginningsofends`. If there are, that means that
    our invalid word is indeed a combination of two valid words without a space. We
    can use the `intersection()` function to find all elements that are shared by
    both lists:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快到了；我们只需要找出是否有任何位置同时出现在`endsofbeginnings`和`beginningsofends`中。如果有，说明我们的无效单词确实是由两个有效单词组成的，且中间没有空格。我们可以使用`intersection()`函数来找到两个列表中共享的所有元素：
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We use the `list(set())` syntax again; just like before, it’s to make sure
    that our list contains only unique values, with no duplicates. We call the result
    `pivot`. It’s possible that `pivot` will contain more than one element. This would
    mean that there are more than two possible combinations of valid words that could
    compose our invalid word. If this happens, we’ll have to decide which combination
    is the one the original writer intended. This cannot be done with certainty. For
    example, consider the invalid word *choosespain*. It’s possible that this invalid
    word is from a travel brochure for Iberia (“Choose Spain!”), but it’s also possible
    that it’s from a description of a masochist (“chooses pain”). Because of the huge
    quantity of words in our language and the numerous ways they can be combined,
    sometimes we can’t be certain which is right. A more sophisticated approach would
    take into account context—whether other words around *choosespain* tend to be
    about olives and bullfighting or about whips and superfluous dentist appointments.
    Such an approach would be difficult to do well and impossible to do perfectly,
    illustrating again the difficulty of language algorithms in general. In our case,
    we’ll take the smallest element of `pivot`, not because this is certainly the
    correct one, but just because we have to take one:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用`list(set())`语法；和之前一样，这是为了确保我们的列表只包含唯一值，没有重复。我们将结果称为`pivot`。有可能`pivot`会包含多个元素，这意味着有多个可能的有效单词组合可以构成我们的无效单词。如果发生这种情况，我们必须决定哪个组合是原作者的意图。这无法确定。例如，考虑无效单词*choosespain*。有可能这个无效单词来自伊比利亚航空的旅行宣传单（“Choose
    Spain!”），但也可能来自描述一个受虐狂（“chooses pain”）。由于我们语言中单词的数量庞大，且它们可以以多种方式组合，有时我们无法确定哪个是正确的。一个更复杂的方法会考虑上下文——*choosespain*周围的其他单词是否倾向于讲橄榄和斗牛，或者是鞭子和多余的牙医预约。这样的做法很难做到好，而且不可能做到完美，这再次说明了语言算法的难度。在我们的情况下，我们将选择`pivot`中最小的元素，不是因为它一定是正确的，而仅仅因为我们必须选一个：
- en: '[PRE23]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can write one line that replaces our invalid word with the two
    valid component words plus a space:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以写一行代码，用两个有效的组成单词加一个空格来替换我们无效的单词：
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If we print this new text, we can see that it has correctly inserted a space
    into the misspelling `oneperfectly`, though it hasn’t yet inserted spaces in the
    rest of the misspellings.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出这个新的文本，我们可以看到它已经正确地在拼写错误的`oneperfectly`中插入了空格，尽管它还没有在其他拼写错误中插入空格。
- en: '[PRE25]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can put all this together into one beautiful function, shown in [Listing
    8-1](#listing8-1). This function will use a `for` loop to insert spaces into every
    instance of two valid words running together to become an invalid word.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有这些功能结合成一个漂亮的函数，如[Listing 8-1](#listing8-1)所示。这个函数将使用一个`for`循环，在每一对有效单词组合成无效单词的地方插入空格。
- en: '[PRE26]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 8-1:](#listinganchor8-1) A function that inserts spaces into texts,
    combining much of the code in the chapter so far'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 8-1:](#listinganchor8-1) 一个插入空格的函数，结合了本章中大部分代码'
- en: 'Then we can define any text and call our function as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以定义任何文本并调用我们的函数，如下所示：
- en: '[PRE27]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We see the output just as we expect, with spaces inserted perfectly:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的输出正如我们预期的那样，空格已完美插入：
- en: '[PRE28]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We’ve created an algorithm that can correctly insert spaces into English text.
    One thing to consider is whether you can do the same for other languages. You
    can—as long as you read in a good, appropriate corpus for the language you’re
    working with to define the `word_list`, the function we defined and called in
    this example can correctly insert spaces into text in any language. It can even
    correct a text in a language you’ve never studied or even heard of. Try different
    corpuses, different languages, and different texts to see what kind of results
    you can get, and you’ll get a glimpse of the power of language algorithms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个可以正确在英语文本中插入空格的算法。需要考虑的一点是，你是否也能在其他语言中做到这一点。可以的——只要你读取适合目标语言的良好语料库，定义我们在此示例中定义并调用的`word_list`，该函数就可以正确地在任何语言的文本中插入空格。它甚至可以纠正你从未学过或听说过的语言中的文本。尝试不同的语料库、不同的语言和不同的文本，看看你能得到什么样的结果，你就能一窥语言算法的强大功能。
- en: Phrase Completion
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 短语补全
- en: Imagine that you are doing algorithm consulting work for a startup that is trying
    to add features to a search engine they are building. They want to add phrase
    completion so that they can provide search suggestions to users. For example,
    when a user types in `peanut``butter and`, a search suggestion feature might suggest
    adding the word `jelly`. When a user types in `squash`, the search engine could
    suggest both `court` and `soup`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在为一个初创公司提供算法咨询服务，该公司正在构建搜索引擎并计划添加一些新功能。他们想要添加短语补全功能，以便能够为用户提供搜索建议。例如，当用户输入`peanut``butter
    and`时，搜索建议功能可能会建议添加单词`jelly`。当用户输入`squash`时，搜索引擎可能会同时建议`court`和`soup`。
- en: Building this feature is simple. We’ll start with a corpus, just like we did
    with our space checker. In this case, we’re interested not only in the individual
    words of our corpus but also in how the words fit together, so we’ll compile lists
    of n-grams from our corpus. An *n-gram* is simply a collection of *n* words that
    appear together. For example, the phrase “Reality is not always probable, or likely”
    is made up of seven words once spoken by the great Jorge Luis Borges. A 1-gram
    is an individual word, so the 1-grams of this phrase are *reality*, *is*, *not*,
    *always*, *probable*, *or*, and *likely*. The 2-grams are every string of two
    words that appear together, including *reality**is*, *is not*, *not always*, *always
    probable*, and so on. The 3-grams are *reality is not*, *is not always*, and so
    on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个功能很简单。我们将从一个语料库开始，就像我们在空间检查器中做的那样。在这种情况下，我们不仅关注语料库中的单个单词，还关注单词是如何组合在一起的，因此我们将从语料库中编译n-gram列表。*n-gram*仅仅是指一组*连续出现的n*个单词。例如，句子“Reality
    is not always probable, or likely”由七个单词组成，这是伟大的Jorge Luis Borges曾经说过的话。1-gram是单个单词，所以这个句子的1-grams是*reality*、*is*、*not*、*always*、*probable*、*or*和*likely*。2-gram是每一对连续的两个单词，包括*reality**is*、*is
    not*、*not always*、*always probable*，等等。3-gram是*reality is not*、*is not always*，依此类推。
- en: Tokenizing and Getting N-grams
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词和获取n-gram
- en: 'We’ll use a Python module called `nltk` to make n-gram collection easy. We’ll
    first tokenize our text. *Tokenizing* simply means splitting a string into its
    component words, ignoring punctuation. For example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`nltk`的Python模块来简化n-gram的收集。我们首先对文本进行分词。*分词*仅仅是指将一个字符串拆分成其组成的单词，忽略标点符号。例如：
- en: '[PRE29]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result we see is this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的结果是：
- en: '[PRE30]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can tokenize and get the n-grams from our text as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对文本进行分词并获得n-gram，如下所示：
- en: '[PRE31]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Alternatively, we can put all the n-grams in a list called `grams`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将所有的n-gram放入一个名为`grams`的列表中：
- en: '[PRE32]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In this case, we have gotten a tokenization and a list of n-grams for a short
    one-sentence text. However, in order to have an all-purpose phrase completion
    tool, we’ll need a considerably larger corpus. The `brown` corpus we used for
    space insertion won’t work because it consists of single words and so we can’t
    get its n-grams.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们已经获得了一个短句文本的分词和n-grams列表。然而，为了拥有一个通用的短语补全工具，我们需要一个相当大的语料库。我们用于空间插入的`brown`语料库不适用，因为它只包含单个单词，因此我们无法获取它的n-grams。
- en: 'One corpus we could use is a collection of literary texts made available online
    by Google’s Peter Norvig at [http://norvig.com/big.txt](http://norvig.com/big.txt).
    For the examples in this chapter, I downloaded a file of Shakespeare’s complete
    works, available for free online at *[http://www.gutenberg.org/files/100/100-0.txt](http://www.gutenberg.org/files/100/100-0.txt)*,
    and then removed the Project Gutenberg boilerplate text on the top. You could
    also use the complete works of Mark Twain, available at *[http://www.gutenberg.org/cache/epub/3200/pg3200.txt](http://www.gutenberg.org/cache/epub/3200/pg3200.txt)*.
    Read a corpus into Python as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用的一个语料库是由Google的Peter Norvig提供的在线文学文本合集，网址为[http://norvig.com/big.txt](http://norvig.com/big.txt)。在本章的示例中，我下载了莎士比亚全集的文件，它可以在*
    [http://www.gutenberg.org/files/100/100-0.txt](http://www.gutenberg.org/files/100/100-0.txt)*免费获取，然后删除了Project
    Gutenberg的页眉文本。你也可以使用马克·吐温的全集，网址为* [http://www.gutenberg.org/cache/epub/3200/pg3200.txt](http://www.gutenberg.org/cache/epub/3200/pg3200.txt)*。你可以通过以下方式将语料库读取到Python中：
- en: '[PRE33]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we used the `requests` module to directly read a text file containing
    the collected works of Shakespeare from a website where it’s being hosted, and
    then read it into our Python session in a variable called `text`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`requests`模块直接从托管网站读取包含莎士比亚全集的文本文件，然后将其读取到名为`text`的Python变量中。
- en: 'After reading in your chosen corpus, rerun the code that created the `grams`
    variable. Here it is with the new definition of the `text` variable:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取所选语料库后，重新运行创建`grams`变量的代码。以下是新定义的`text`变量的代码：
- en: '[PRE34]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Our Strategy
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的策略
- en: Our strategy for generating search suggestions is simple. When a user types
    in a search, we check how many words are in their search. In other words, a user
    enters an n-gram and we determine what *n* is. When a user searches for an n-gram,
    we are helping them add to their search, so we will want to suggest an *n* + 1-gram.
    We’ll search our corpus and find all *n* + 1-grams whose first *n* elements match
    our n-gram. For example, a user might search for `crane`, a 1-gram, and our corpus
    might contain the 2-grams `crane feather`, `crane operator`, and `crane neck`.
    Each is a potential search suggestion we could offer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成搜索建议的策略很简单。当用户输入搜索内容时，我们检查搜索中包含多少个单词。换句话说，用户输入一个n-gram，我们确定*n*的值。当用户搜索n-gram时，我们帮助他们扩展搜索，因此我们将建议一个*n*
    + 1-gram。我们将在语料库中搜索并找到所有首个*n*元素与我们的n-gram匹配的*n* + 1-grams。例如，用户可能搜索`crane`（起重机），一个1-gram，而我们的语料库可能包含2-gram
    `crane feather`（起重机羽毛）、`crane operator`（起重机操作员）和`crane neck`（起重机颈部）。每一个都是我们可以提供的潜在搜索建议。
- en: We could stop there, providing every *n* + 1-gram whose first *n* elements matched
    the *n* + 1-gram the user had entered. However, not all suggestions are equally
    good. For example, if we are working for a custom engine that searches through
    manuals for industrial construction equipment, it’s likely that `crane operator`
    will be a more relevant, useful suggestion than `crane feather`. The simplest
    way to determine which *n* + 1-gram is the best suggestion is to offer theone
    that appears most often in our corpus.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在此停止，提供每个首个*n*元素与用户输入的*n* + 1-gram匹配的所有*n* + 1-gram。然而，并不是所有的建议都是同等优秀的。例如，如果我们为一个搜索工业建筑设备手册的定制引擎工作，那么`crane
    operator`（起重机操作员）可能比`crane feather`（起重机羽毛）更相关、更有用。确定哪个*n* + 1-gram是最佳建议的最简单方法是提供在我们的语料库中出现频率最高的那个。
- en: 'Thus, our full algorithm: a user searches for an n-gram, we find all *n* +
    1-grams whose first *n* elements match the user’s n-gram, and we recommend the
    matching *n* + 1-gram that appears most frequently in the corpus.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的完整算法是：用户搜索一个n-gram，我们找到所有首个*n*元素与用户的n-gram匹配的*n* + 1-grams，并推荐在语料库中最常出现的匹配*n*
    + 1-gram。
- en: Finding Candidate *n* + 1-grams
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找候选*n* + 1-grams
- en: 'In order to find the *n* + 1-grams that will constitute our search suggestions,
    we need to know how long the user’s search term is. Suppose the search term is
    `life is a`, meaning that we’re looking for suggestions for how to complete the
    phrase “life is a . . .”. We can use the following simple lines to get the length
    of our search term:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到将构成我们搜索建议的*n* + 1-grams，我们需要知道用户搜索词的长度。假设搜索词是`life is a`，意思是我们正在寻找如何完成短语“life
    is a . . .”的建议。我们可以使用以下简单的代码来获取搜索词的长度：
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that we know the length of the search term, we know *n*—it’s 3\. Remember
    that we’ll be returning the most frequent *n* + 1-grams (4-grams) to the user.
    So we need to take into account the different frequencies of different *n* + 1-grams.
    We’ll use a function called `Counter()`, which will count the number of occurrences
    of each *n* + 1-gram in our collection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道搜索词的长度，知道了*n*的值——它是3。记住，我们将返回最频繁的*n* + 1-gram（即4-gram）给用户。所以我们需要考虑不同*n*
    + 1-gram的频率差异。我们将使用一个名为`Counter()`的函数，它会计算每个*n* + 1-gram在我们语料库中出现的次数。
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This line has selected only the *n* + 1-grams from our `grams` variable. Applying
    the `Counter()` function creates a list of tuples. Each tuple has an *n* + 1-gram
    as its first element and the frequency of that *n* + 1-gram in our corpus as its
    second element. For example, we can print the first element of `counted_grams`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行仅从我们的`grams`变量中选择了*n* + 1-gram。应用`Counter()`函数会创建一个元组列表。每个元组的第一个元素是*n* +
    1-gram，第二个元素是该*n* + 1-gram在我们语料库中的出现频率。例如，我们可以打印`counted_grams`的第一个元素：
- en: '[PRE37]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output shows us the first *n* + 1-gram in our corpus and tells us that
    it appears only once in the entire corpus:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们语料库中的第一个*n* + 1-gram，并告诉我们它在整个语料库中仅出现一次：
- en: '[PRE38]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This n-gram is the beginning of Shakespeare’s Sonnet 1\. It’s fun to look at
    some of the interesting 4-grams we can randomly find in Shakespeare’s works. For
    example, if you run `print(list(counted_grams)[10])`, you can see that the 10th
    4-gram in Shakespeare’s works is “rose might never die.” If you run `print(list(counted_grams)[240000])`,
    you can see that the 240,000th n-gram is “I shall command all.” The 323,002nd
    is “far more glorious star” and the 328,004th is “crack my arms asunder.” But
    we want to do phrase completion, not just *n* + 1-gram browsing. We need to find
    the subset of *n* + 1-grams whose first *n* elements match our search term. We
    can do that as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个n-gram是莎士比亚《第一首十四行诗》的开头。看看我们在莎士比亚作品中随机找到的一些有趣的4-grams非常有趣。例如，如果你运行`print(list(counted_grams)[10])`，你可以看到莎士比亚作品中的第十个4-gram是“rose
    might never die”。如果你运行`print(list(counted_grams)[240000])`，你可以看到第240,000个n-gram是“I
    shall command all”。第323,002个是“far more glorious star”，第328,004个是“crack my arms
    asunder”。但我们要做的是短语补全，而不仅仅是*n* + 1-gram浏览。我们需要找到一个*n* + 1-grams的子集，其前*n*个元素与我们的搜索词匹配。我们可以通过以下方式来做到这一点：
- en: '[PRE39]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This list comprehension iterates over every *n* + 1-gram and calls each element
    as it does so. For each element, it checks whether `element[0][:-1]==tuple(split_term)`.
    The left side of this equality, `element[0][:-1]`, simply takes the first *n*
    elements of each *n* + 1-gram: the `[:-1]` is a handy way to disregard the last
    element of a list. The right side of the equality, `tuple(split_term)`, is the
    n-gram we’re searching for (“life is a”). So we’re checking for *n* + 1-grams
    whose first *n* elements are the same as our n-gram of interest. Whichever terms
    match are stored in our final output, called `matching_terms`.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表推导式会遍历每个*n* + 1-gram，并在此过程中调用每个元素。对于每个元素，它检查`element[0][:-1]==tuple(split_term)`。这个等式的左边，`element[0][:-1]`，简单地获取每个*n*
    + 1-gram的前*n*个元素：`[:-1]`是一个方便的方式来忽略列表的最后一个元素。等式的右边，`tuple(split_term)`，是我们要搜索的n-gram（“life
    is a”）。所以我们在检查那些前*n*个元素与我们感兴趣的n-gram相同的*n* + 1-grams。所有匹配的词汇都存储在我们的最终输出中，名为`matching_terms`。
- en: Selecting a Phrase Based on Frequency
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于频率选择短语
- en: 'Our `matching_terms` list has everything we need to finish the job; it consists
    of *n* + 1-grams whose first *n* elements match the search term, and it includes
    their frequencies in our corpus. As long as there is at least one element in the
    matching terms list, we can find the element that occurs most frequently in the
    corpus and suggest it to the user as the completed phrase. The following snippet
    gets the job done:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`matching_terms`列表包含完成任务所需的所有内容；它由那些前*n*个元素与搜索词匹配的*n* + 1-grams组成，并包括它们在我们语料库中的频率。只要匹配的词汇列表中至少有一个元素，我们就可以找到在语料库中出现最频繁的元素，并将其建议给用户作为完整的短语。以下代码片段可以完成这个任务：
- en: '[PRE40]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In this snippet, we started by defining `frequencies`, a list containing the
    frequency of every *n* + 1-gram in our corpus that matches the search term. Then,
    we used the `numpy` module’s `max()` function to find the highest of those frequencies.
    We used another list comprehension to get the first *n* + 1-gram that occurs with
    the highest frequency in the corpus, and finally we created a `combined_term`,
    a string that puts together all of the words in that search term, with spaces
    separating the words.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们首先定义了`frequencies`，它是一个包含我们语料库中每个与搜索词匹配的*n* + 1-gram频率的列表。接着，我们使用了`numpy`模块的`max()`函数来找到这些频率中的最大值。然后，我们使用了另一个列表推导式来获取语料库中出现频率最高的第一个*n*
    + 1-gram，最后我们创建了一个`combined_term`，它是一个将搜索词中所有单词连接在一起的字符串，单词之间用空格分隔。
- en: Finally, we can put all of our code together in a function, shown in [Listing
    8-2](#listing8-2).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将所有代码整合成一个函数，如[列表 8-2](#listing8-2)所示。
- en: '[PRE41]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[Listing 8-2:](#listinganchor8-2) A function that provides search suggestions
    by taking an n-gram and returning the most likely n + 1-gram that starts with
    the input n-gram'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-2:](#listinganchor8-2) 一个通过输入一个n-gram并返回最可能的以该n-gram开头的n + 1-gram的搜索建议函数'
- en: 'When we call our function, we pass an n-gram as the argument, and the function
    returns an *n* + 1-gram. We call it as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用我们的函数时，我们传入一个n-gram作为参数，函数将返回一个*n* + 1-gram。我们按如下方式调用它：
- en: '[PRE42]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: And you can see that the suggestion is `life is a tedious`, which is the most
    common 4-gram that Shakespeare used that started with the words `life is a` (tied
    with two other 4-grams). Shakespeare used this 4-gram only once, in *Cymbeline*,
    when Imogen says, “I see a man’s life is a tedious one.” In *King Lear*, Edgar
    tells Gloucester “Thy life is a miracle” (or “Thy life’s a miracle,” depending
    on which text you use), so that 4-gram would also be a valid completion of our
    phrase.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，建议是`life is a tedious`，这是莎士比亚最常用的以`life is a`开头的4-gram（与其他两个4-gram并列）。莎士比亚只在一次使用了这个4-gram，出现在《辛白林》中，当伊莫金说：“我看到一个人的生命是乏味的。”在《李尔王》中，埃德加对格洛斯特说：“你的生命是一个奇迹”（或者根据不同版本是“你的生命是奇迹”），所以这个4-gram也可以作为我们短语的有效补全。
- en: 'We can have some fun by trying a different corpus and seeing how the results
    differ. Let’s use the corpus of Mark Twain’s collected works:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用不同的语料库，看看结果有何不同。让我们使用马克·吐温的全集语料库：
- en: '[PRE43]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With this new corpus, we can check for search suggestions again:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新的语料库，我们可以再次检查搜索建议：
- en: '[PRE44]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In this case, the completed phrase is `life is a failure`, indicating a difference
    between the two text corpuses, and maybe also a difference between the style and
    attitude of Shakespeare and those of Mark Twain. You can also try other search
    terms. For example, `I love` is completed by `you` if we use Mark Twain’s corpus,
    and `thee` if we use Shakespeare’s corpus, showing a difference in style across
    the centuries and ocean, if not a difference in ideas. Try another corpus and
    some other phrases and see how your phrases get completed. If you use a corpus
    written in another language, you can do phrase completion for languages you don’t
    even speak using the exact function we just wrote.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，补全后的短语是`life is a failure`，这表明了两个文本语料库之间的差异，也许还暗示了莎士比亚和马克·吐温在风格和态度上的不同。你也可以尝试其他搜索词。例如，如果使用马克·吐温的语料库，`I
    love`会被补全为`you`，而如果使用莎士比亚的语料库，它会被补全为`thee`，这展示了跨越几个世纪和大洋的风格差异，尽管观点可能没有太大不同。尝试使用另一个语料库和其他短语，看看你的短语如何被补全。如果你使用的是另一种语言写成的语料库，使用我们刚才编写的函数，你甚至可以对你不懂的语言进行短语补全。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed algorithms that can be used to work with human
    language. We started with a space insertion algorithm that can correct incorrectly
    scanned texts, and we continued with a phrase completion algorithm that can add
    words to input phrases to match the content and style of a text corpus. The approaches
    we took to these algorithms are similar to the approaches that work for other
    types of language algorithms, including spell checkers and intent parsers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了可以用于处理人类语言的算法。我们从一个空格插入算法开始，该算法可以修正扫描错误的文本，接着我们讨论了一个短语补全算法，它可以向输入的短语中添加单词，以匹配文本语料库的内容和风格。我们采取的这些算法方法类似于适用于其他类型语言算法的方法，包括拼写检查器和意图解析器。
- en: In the next chapter, we’ll explore machine learning, a powerful and growing
    field that every good algorithm-smith should be familiar with. We’ll focus on
    a machine learning algorithm called *decision trees*, which are simple, flexible,
    accurate, and interpretable models that can take you far on your journey through
    algorithms and life.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨机器学习，这是一门强大且不断发展的领域，每一个优秀的算法工程师都应该熟悉它。我们将重点介绍一种名为*决策树*的机器学习算法，它是一种简单、灵活、准确且可解释的模型，可以在你踏上算法和人生的旅程中帮助你走得更远。
