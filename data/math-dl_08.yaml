- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8'
- en: MATRIX CALCULUS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵微积分**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: '[Chapter 7](ch07.xhtml#ch07) introduced us to differential calculus. In this
    chapter, we’ll discuss *matrix calculus*, which extends differentiation to functions
    involving vectors and matrices.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.xhtml#ch07)向我们介绍了微分学。在这一章，我们将讨论*矩阵微积分*，它将微分扩展到涉及向量和矩阵的函数。'
- en: Deep learning works extensively with vectors and matrices, so it makes sense
    to develop a notation and approach to representing derivatives involving these
    objects. That’s what matrix calculus gives us. We saw a hint of this at the end
    of [Chapter 7](ch07.xhtml#ch07), when we introduced the gradient to represent
    the derivative of a scalar function of a vector—a function that accepts a vector
    argument and returns a scalar, *f*(***x***).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习广泛使用向量和矩阵，因此开发一种表示涉及这些对象的导数的符号和方法是有意义的。这正是矩阵微积分为我们提供的内容。在[第7章](ch07.xhtml#ch07)的结尾，我们引入了梯度来表示向量的标量函数的导数——一个接受向量参数并返回标量的函数，*f*(***x***)。
- en: We’ll start with the table of matrix calculus derivatives and their definitions.
    Next, we’ll examine some identities involving matrix derivatives. Mathematicians
    love identities; however, to preserve our sanity, we’ll only consider a handful.
    Some special matrices come out of matrix calculus, namely the Jacobian and Hessian.
    You’ll run into both of these matrices during your sojourn through deep learning,
    so we’ll consider them next in the context of optimization. Recall that training
    a neural network is, fundamentally, an optimization problem, so understanding
    what these special matrices represent and how we use them is especially important.
    We’ll close the chapter with some examples of matrix derivatives.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从矩阵微积分导数及其定义的表格开始。接下来，我们将研究一些涉及矩阵导数的恒等式。数学家喜欢恒等式；然而，为了保持理智，我们只会考虑其中的一小部分。一些特殊的矩阵是矩阵微积分的产物，特别是雅可比矩阵和海森矩阵。在你深入深度学习的过程中，你会遇到这两个矩阵，因此我们接下来会在优化的背景下讨论它们。回想一下，训练神经网络本质上是一个优化问题，因此理解这些特殊矩阵的含义以及如何使用它们尤为重要。我们将以一些矩阵导数的例子结束这一章。
- en: The Formulas
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公式
- en: '[Table 8-1](ch08.xhtml#ch08tab01) summarizes the matrix calculus derivatives
    we’ll explore in this chapter. These are the ones commonly used in practice.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)总结了我们将在本章中探讨的矩阵微积分导数。这些是实践中常用的导数。'
- en: '**Table 8-1:** Matrix Calculus Derivatives'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 8-1：** 矩阵微积分导数'
- en: '|  | **Scalar** | **Vector** | **Matrix** |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|  | **标量** | **向量** | **矩阵** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Scalar** | ∂*f*/∂*x* | ∂***f***/∂*x* | ∂***F***/∂*x* |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **标量** | ∂*f*/∂*x* | ∂***f***/∂*x* | ∂***F***/∂*x* |'
- en: '| **Vector** | ∂*f*/∂***x*** | ∂*f*/∂***x*** | — |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **向量** | ∂*f*/∂***x*** | ∂*f*/∂***x*** | — |'
- en: '| **Matrix** | ∂***f***/∂***X*** | — | — |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **矩阵** | ∂***f***/∂***X*** | — | — |'
- en: 'The columns of [Table 8-1](ch08.xhtml#ch08tab01) represent the function, meaning
    the return value. Notice we use three versions of the letter f: regular, bold,
    and capital. We use *f* if the return value is a scalar, ***f*** if a vector,
    and ***F*** if a matrix. The rows of [Table 8-1](ch08.xhtml#ch08tab01) are the
    variables the derivatives are calculated with respect to. The same notation applies:
    *x* is a scalar, ***x*** is a vector, and ***X*** is a matrix.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)的列表示函数，意味着返回值。注意，我们使用了三种版本的字母f：常规、粗体和大写字母。如果返回值是标量，则使用*f*；如果是向量，则使用***f***；如果是矩阵，则使用***F***。
    [表 8-1](ch08.xhtml#ch08tab01)的行表示导数是相对于哪些变量计算的。相同的符号规则适用：*x*是标量，***x***是向量，***X***是矩阵。'
- en: '[Table 8-1](ch08.xhtml#ch08tab01) defines six derivatives, but there are nine
    cells in the table. While possible to define, the remaining derivatives are not
    standardized or used often enough to make covering them worthwhile. That’s good
    for us, as the six are enough of a challenge for our mathematical brains.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)定义了六个导数，但表格中有九个单元格。虽然可以定义其余的导数，但由于它们不够标准化或不常用，所以不值得讨论它们。对我们来说，这很好，因为这六个已经足够挑战我们的数学思维了。'
- en: The first derivative in [Table 8-1](ch08.xhtml#ch08tab01), the one in the upper
    left, is the normal derivative of [Chapter 7](ch07.xhtml#ch07), a function producing
    a scalar with respect to a scalar. (Refer to [Chapter 7](ch07.xhtml#ch07) for
    everything you need to know about standard differentiation.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)中的第一个导数，位于左上角，是[第7章](ch07.xhtml#ch07)的标准导数，一个生成标量的函数，关于标量的导数。（有关标准微分的所有内容，请参阅[第7章](ch07.xhtml#ch07)。）'
- en: We’ll cover the remaining five derivatives in the sections below. We define
    each one in terms of scalar derivatives. We’ll first show the definition and then
    explain what the notation represents. The definition will help you build a model
    in your head of what the derivative is. I suspect that by the end of this section
    you’ll be predicting the definitions in advance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面的章节中讲解剩下的五个导数。我们将每个导数定义为标量导数的形式。我们首先展示定义，然后解释该符号的含义。定义将帮助你在脑海中建立导数的模型。我猜想，在本节结束时，你可能会提前预测出这些定义。
- en: Before we start, however, there is a complication we should discuss. Matrix
    calculus is notation heavy, but there’s no universal agreement on the notation.
    We’ve seen this before with the many ways to indicate differentiation. For matrix
    calculus, the two approaches are *numerator layout* or *denominator layout*. Specific
    disciplines seem to favor one over the other, though exceptions are almost the
    norm, as is mixing notations. For deep learning, a nonscientific perusal on my
    part seems to indicate a slight preference for numerator layout, so that’s what
    we’ll use here. Just be aware that there are two forms out there. One is typically
    the transpose of the other.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始之前，有一个复杂问题需要讨论。矩阵微积分符号较为繁琐，但并没有一个统一的符号体系。我们以前在讨论微分表示时也见过这种情况。对于矩阵微积分，有两种符号表示方法，分别是*分子布局*和*分母布局*。不同学科似乎偏好其中一种，尽管几乎常见的是混用符号和例外情况。对于深度学习，根据我非科学的粗略观察，似乎略偏向使用分子布局，因此我们这里也将采用这种布局。请注意，这两种布局都有存在，且其中一种通常是另一种的转置形式。
- en: A Vector Function by a Scalar Argument
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个接受标量参数的向量函数
- en: 'A vector function accepting a scalar argument is our first derivative; see
    [Table 8-1](ch08.xhtml#ch08tab01), second column of the first row. We write such
    a function as ***f***(*x*) to indicate a scalar argument, *x*, and a vector output,
    ***f***. Functions like ***f*** take a scalar and map it to a multidimensional
    vector:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个接受标量参数的向量函数是我们的第一个导数；参见[表 8-1](ch08.xhtml#ch08tab01)，第一行第二列。我们将这样的函数写为***f***(*x*)，以表示标量参数*x*和向量输出***f***。像***f***这样的函数接受一个标量并将其映射到一个多维向量：
- en: '***f*** : ℝ → ℝ^(*m*)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***f*** : ℝ → ℝ^(*m*)'
- en: Here, *m* is the number of elements in the output vector. Functions like ***f***
    are known as *vector-valued functions* with scalar arguments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m*是输出向量中的元素数量。像***f***这样的函数被称为*向量值函数*，其参数为标量。
- en: A parametric curve in 3D space is an excellent example of such a function. Those
    functions are often written as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 三维空间中的参数曲线是这类函数的一个极好例子。这些函数通常写作
- en: '![Image](Images/195equ01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/195equ01.jpg)'
- en: where ![Image](Images/195equ02b.jpg), ![Image](Images/195equ02a.jpg), and ![Image](Images/195equ03.jpg)
    are unit vectors in the x, y, and z directions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/195equ02b.jpg)，![Image](Images/195equ02a.jpg) 和 ![Image](Images/195equ03.jpg)
    是x、y和z方向上的单位向量。
- en: '[Figure 8-1](ch08.xhtml#ch08fig01) shows a plot of a 3D parametric curve,'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-1](ch08.xhtml#ch08fig01)显示了一个三维参数曲线的图像，'
- en: '![Image](Images/08equ01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ01.jpg)'
- en: where, as *t* varies, the three axis values also vary to trace out the spiral.
    Here, each value of *t* specifies a single point in 3D space.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，随着*t*的变化，三个轴的值也随之变化，从而描绘出螺旋形曲线。每个*t*的值都指定了三维空间中的一个点。
- en: '![image](Images/08fig01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig01.jpg)'
- en: '*Figure 8-1: A 3D parametric curve*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-1：三维参数曲线*'
- en: In matrix calculus notation, we don’t write ***f*** as shown in [Equation 8.1](ch08.xhtml#ch08equ01).
    Instead, we write ***f*** as a column vector of the functions,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵微积分符号中，我们不像[方程 8.1](ch08.xhtml#ch08equ01)那样写***f***。相反，我们将***f***表示为函数的列向量，
- en: '![Image](Images/195equ04.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/195equ04.jpg)'
- en: and, in general,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，一般情况下，
- en: '![Image](Images/196equ01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ01.jpg)'
- en: for ***f*** with *n* elements.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*n*个元素的***f***。
- en: 'The derivative of ***f***(*x*) is known as the *tangent vector*. What does
    the derivative look like? Since ***f*** is a vector, we might expect the derivative
    of ***f*** to be the derivatives of the functions representing each element of
    ***f***, and we’d be right:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '***f***(*x*)的导数被称为*切向量*。导数长什么样呢？由于***f***是一个向量，我们可能会预期***f***的导数是表示***f***每个元素的函数的导数，而且我们是对的：'
- en: '![Image](Images/196equ02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ02.jpg)'
- en: 'Let’s look at a simple example. First, we will define ***f***(*x*), and then
    the derivative:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个简单的例子。首先，我们将定义***f***(*x*)，然后是导数：
- en: '![Image](Images/196equ03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ03.jpg)'
- en: Here, each element of ∂***f***/∂*x* is the derivative of the corresponding function
    in ***f***.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，∂***f***/∂*x*的每个元素是对应函数的导数，且该函数属于***f***。
- en: A Scalar Function by a Vector Argument
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量参数的标量函数
- en: 'In [Chapter 7](ch07.xhtml#ch07), we learned that a function accepting a vector
    input but returning a scalar is a scalar field:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第七章](ch07.xhtml#ch07)中，我们了解到，接受向量输入但返回标量的函数是一个标量场：
- en: '*f* : ℝ^(*m*) → ℝ'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* : ℝ^(*m*) → ℝ'
- en: We also learned that the derivative of this function is the gradient. In matrix
    calculus notation, we write ∂*f*/∂***x*** for *f*(***x***) as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到这个函数的导数是梯度。在矩阵微积分符号中，我们将∂*f*/∂***x***写作*f*(***x***)的导数如下：
- en: '![Image](Images/196equ04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/196equ04.jpg)'
- en: where ***x*** = [*x*[0] *x*[1] ... *x*[*m*–1]]^⊤ is a vector of variables, and
    *f* is a function of those variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，***x*** = [*x*[0] *x*[1] ... *x*[*m*–1]]^⊤ 是一个变量向量，而*f*是这些变量的函数。
- en: Notice, because we decided to use the numerator layout approach, ∂*f*/∂***x***
    is written as a *row* vector. So, to be true to our notation, we have to write
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们决定使用分子布局方法，∂*f*/∂***x***被写成*行*向量。所以，为了保持我们的符号一致性，我们必须写成：
- en: '![Image](Images/197equ01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/197equ01.jpg)'
- en: turning the row vector into a column vector to match the gradient. Remember
    that ▽ is the symbol for gradient; we saw an example of the gradient in Chapter
    7.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将行向量转为列向量，以匹配梯度。记住，▽是梯度的符号；我们在第七章看到过梯度的一个例子。
- en: A Vector Function by a Vector
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量函数的向量形式
- en: If the derivative of a vector-valued function by a scalar produces a column
    vector, and the derivative of a scalar function by a vector results in a row vector,
    does the derivative of a vector-valued function by a vector produce a matrix?
    The answer is yes. In this case, we’re contemplating ∂***f***/∂***x*** for ***f***(***x***),
    a function that accepts a vector input and returns a vector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个向量值函数对标量的导数产生列向量，而一个标量函数对向量的导数产生行向量，那么一个向量值函数对向量的导数是否会产生一个矩阵呢？答案是肯定的。在这种情况下，我们正在考虑∂***f***/∂***x***，这是一个接受向量输入并返回向量的函数。
- en: 'The numerator layout convention gave us a column vector for the derivative
    of ***f***(*x*), implying we need a row for each of the functions in ***f***.
    Similarly, the derivative of *f*(***x***) produced a row vector. Therefore, merging
    the two gives us the derivative of ***f***(***x***):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分子布局惯例为***f***(*x*)的导数提供了一个列向量，这意味着我们需要为***f***中的每个函数提供一行。同样，*f*(***x***)的导数产生了一个行向量。因此，合并这两个部分就得到了***f***(***x***)的导数：
- en: '![Image](Images/08equ02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ02.jpg)'
- en: 'This is for a function, ***f***, returning an *n*-element vector and accepting
    an *m-*element vector, ***x***, as its argument:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对于一个函数，***f***，它返回一个*n*元素的向量，并接受一个*m*元素的向量***x***作为输入参数：
- en: '*f* : ℝ^(*m*) → ℝ^(*n*)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* : ℝ^(*m*) → ℝ^(*n*)'
- en: Each row of ***f*** is a scalar function of ***x***, for example, *f*[0](***x***).
    Therefore, we can write [Equation 8.2](ch08.xhtml#ch08equ02) as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '***f***的每一行都是***x***的标量函数，例如，*f*[0](***x***)。因此，我们可以将[方程 8.2](ch08.xhtml#ch08equ02)写成：'
- en: '![Image](Images/08equ03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ03.jpg)'
- en: This gives us the matrix as a collection of gradients, one for each scalar function
    in ***f***. We’ll return to this matrix later in the chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了一个矩阵，它是梯度的集合，每个标量函数在***f***中都有一个梯度。我们将在本章稍后返回到这个矩阵。
- en: A Matrix Function by a Scalar
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标量参数的矩阵函数
- en: 'If ***f***(*x*) is a function accepting a scalar argument but returning a vector,
    then we’d be correct in assuming ***F***(*x*) can be thought of as a function
    accepting a scalar argument but returning a matrix:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果***f***(*x*)是一个接受标量参数但返回向量的函数，那么我们可以正确地假设***F***(*x*)可以被看作是一个接受标量参数但返回矩阵的函数：
- en: '***F*** : ℝ → ℝ^(*n*×*m*)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***F*** : ℝ → ℝ^(*n*×*m*)'
- en: 'For example, assume ***F*** is an *n* × *m* matrix of scalar functions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设***F***是一个*n* × *m*的标量函数矩阵：
- en: '![Image](Images/198equ01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/198equ01.jpg)'
- en: 'The derivative with respect to the argument, *x*, is straightforward:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数*x*的导数是直接的：
- en: '![Image](Images/198equ02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/198equ02.jpg)'
- en: As we saw above, the derivative of a vector-valued function by a scalar is called
    the tangent vector. By analogy, then, the derivative of a matrix-valued function
    by a scalar is the *tangent matrix*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，标量对向量值函数的导数被称为切向量。类比来说，标量对矩阵值函数的导数就是*切向矩阵*。
- en: A Scalar Function by a Matrix
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量参数的标量函数
- en: 'Now let’s consider *f*(***X***), a function accepting a matrix and returning
    a scalar:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑*f*(***X***)，一个接受矩阵并返回标量的函数：
- en: '*f* : ℝ^(*n*×*m*) → ℝ'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* : ℝ^(*n*×*m*) → ℝ'
- en: We’d be correct in thinking that the derivative of *f* with respect to the matrix,
    ***X***, is itself a matrix. However, to be true to our numerator layout convention,
    the resulting matrix is not arranged like ***X***, but instead like ***X***^⊤,
    the transpose of ***X***.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以正确地认为，*f* 对矩阵 ***X*** 的导数本身是一个矩阵。然而，为了符合我们的分子布局惯例，得到的矩阵并不是像 ***X*** 那样排列的，而是像
    ***X***^⊤，即 ***X*** 的转置。
- en: Why use the transpose of ***X*** instead of ***X*** itself? To answer the question,
    we need to look back to how we defined ∂*f*/∂***x***. There, even though ***x***
    is a column vector, according to standard convention, we said the derivative is
    a *row* vector. We used ***x***^⊤ as the ordering. Therefore, to be consistent,
    we need to arrange ∂*f*/∂***X*** by the transpose of ***X*** and change the columns
    of ***X*** into rows in the derivative. As a result, we have the following definition.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用 ***X*** 的转置而不是 ***X*** 本身？为了解答这个问题，我们需要回顾一下我们如何定义 ∂*f*/∂***x***。在那里，尽管
    ***x*** 是列向量，但按照标准惯例，我们说导数是一个*行*向量。我们使用了 ***x***^⊤ 作为排列顺序。因此，为了保持一致，我们需要按 ***X***
    的转置来排列 ∂*f*/∂***X***，并将 ***X*** 的列转换为导数中的行。结果，我们得出了以下定义。
- en: '![Image](Images/08equ04.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ04.jpg)'
- en: This is an *m* × *n* output matrix for the *n* × *m* input matrix, ***X***.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 *m* × *n* 输出矩阵，对应 *n* × *m* 输入矩阵 ***X***。
- en: '[Equation 8.4](ch08.xhtml#ch08equ04) defines the *gradient matrix*, which,
    for matrices, plays a role similar to that of the gradient, ▽*f*(**x**). [Equation
    8.4](ch08.xhtml#ch08equ04) also completes our collection of matrix calculus derivatives.
    Let’s move on to consider some matrix derivative identities.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 8.4](ch08.xhtml#ch08equ04)定义了*梯度矩阵*，它在矩阵中发挥着类似于梯度 ▽*f*(**x**) 的作用。[方程式
    8.4](ch08.xhtml#ch08equ04)还完整了我们的矩阵微积分导数集合。接下来，让我们考虑一些矩阵导数恒等式。'
- en: The Identities
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恒等式
- en: Matrix calculus involves scalars, vectors, matrices, and functions thereof,
    which themselves return scalars, vectors, or matrices, implying many identities
    and relationships exist. However, here we’ll concentrate on basic identities showing
    the relationship between matrix calculus and the differential calculus of [Chapter
    7](ch07.xhtml#ch07).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵微积分涉及标量、向量、矩阵及其函数，这些函数返回标量、向量或矩阵，意味着存在许多恒等式和关系。然而，在这里我们将集中于基本恒等式，展示矩阵微积分与[第7章](ch07.xhtml#ch07)微分微积分之间的关系。
- en: Each of the following subsections presents identities related to the specific
    type of derivative indicated. The identities cover fundamental relationships and,
    when applicable, the chain rule. In all cases, the results follow the numerator
    layout scheme we’ve used throughout the chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下每个子节介绍了与特定类型的导数相关的恒等式。这些恒等式涵盖了基本关系，并在适用时包含链式法则。在所有情况下，结果都遵循我们在本章中使用的分子布局方案。
- en: A Scalar Function by a Vector
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 由向量构成的标量函数
- en: We begin with identities related to a scalar function with a vector input. If
    not otherwise specified, *f* and *g* are functions of a vector, ***x***, and return
    a scalar. A constant vector that doesn’t depend on ***x*** is given as ***a***,
    and *a* denotes a scalar constant.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从与向量输入的标量函数相关的恒等式开始。如果没有其他说明，*f* 和 *g* 是 ***x*** 的函数，返回标量。一个不依赖于 ***x*** 的常量向量用
    ***a*** 表示，*a* 表示标量常数。
- en: 'The basic rules are intuitive:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基本规则是直观的：
- en: '![Image](Images/08equ05.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ05.jpg)'
- en: and
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Image](Images/08equ06.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ06.jpg)'
- en: These show that multiplication by a scalar constant acts as it did in [Chapter
    7](ch07.xhtml#ch07), as does the linearity of the partial derivative.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表明，标量常数的乘法与[第7章](ch07.xhtml#ch07)中的作用相同，偏导数的线性性也如此。
- en: 'The product rule also works as we expect:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 乘积法则也如我们预期的那样适用：
- en: '![Image](Images/08equ07.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ07.jpg)'
- en: Let’s pause here and remind ourselves of the inputs and outputs for the equations
    above. We know that the derivative of a scalar function by a vector argument is
    a row vector in our notation. So, [Equation 8.5](ch08.xhtml#ch08equ05) returns
    a row vector multiplied by a scalar—each element of the derivative is multiplied
    by *a*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在此暂停一下，提醒自己以上方程的输入和输出。我们知道，标量函数对向量参数的导数在我们的符号中是一个行向量。所以，[方程式 8.5](ch08.xhtml#ch08equ05)返回一个乘以标量的行向量——导数的每个元素都乘以
    *a*。
- en: As differentiation is a linear operator, it distributes over addition, so [Equation
    8.6](ch08.xhtml#ch08equ06) delivers two terms, each a row vector generated by
    the respective derivative.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微分是一个线性算子，它对加法分配，因此[方程式 8.6](ch08.xhtml#ch08equ06)给出了两个项，每个项都是由相应导数生成的行向量。
- en: For [Equation 8.7](ch08.xhtml#ch08equ07), the product rule, the result again
    includes two terms. In each case, the derivative returns a row vector, which is
    multiplied by a scalar function value, either *f*(***x***) or *g*(***x***). Therefore,
    the output of [Equation 8.7](ch08.xhtml#ch08equ07) is also a row vector.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[方程 8.7](ch08.xhtml#ch08equ07)，乘积法则，结果再次包括两项。在每种情况下，导数返回一个行向量，并与标量函数值相乘，函数值为*f*(***x***)或*g*(***x***)。因此，[方程
    8.7](ch08.xhtml#ch08equ07)的输出也是一个行向量。
- en: The scalar-by-vector chain rule becomes
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 标量与向量的链式法则变为
- en: '![Image](Images/08equ08.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ08.jpg)'
- en: where *f*(*g*) returns a scalar and accepts a scalar argument, while *g*(***x***)
    returns a scalar and accepts a vector argument. The end result is a row vector.
    Let’s work through a complete example to demonstrate.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*f*(*g*)返回一个标量并接受标量参数，而*g*(***x***)返回一个标量并接受向量参数。最终结果是一个行向量。让我们通过一个完整的例子来演示。
- en: We have a vector, ***x*** = [*x*[0], *x*[1], *x*[2]]^⊤; a function of that vector
    written in component form, *g*(***x***) = *x*[0] + *x*[1]*x*[2]; and a function
    of *g*, *f*(*g*) = *g*². According to [Equation 8.8](ch08.xhtml#ch08equ08), the
    derivative of *f* with respect to ***x*** is
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个向量，***x*** = [*x*[0], *x*[1], *x*[2]]^⊤；一个该向量的分量形式函数，*g*(***x***) = *x*[0]
    + *x*[1]*x*[2]；以及一个关于*g*的函数，*f*(*g*) = *g*²。根据[方程 8.8](ch08.xhtml#ch08equ08)，*f*对***x***的导数是
- en: '![Image](Images/200equ01.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/200equ01.jpg)'
- en: To check our result, we can work through from *g*(***x***) = *x*[0] + *x*[1]*x*[2]
    and *f*(*g*) = *g*² to find *f*(***x***) directly via substitution. Doing this
    gives us
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的结果，我们可以从*g*(***x***) = *x*[0] + *x*[1]*x*[2] 和 *f*(*g*) = *g*²开始，直接通过替代法得到*f*(***x***)。这样做得到了
- en: '![Image](Images/201equ01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/201equ01.jpg)'
- en: from which we get
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们得到
- en: '![Image](Images/201equ02.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/201equ02.jpg)'
- en: which matches the result we found using the chain rule. Of course, in this simple
    example, it was easier to work through the substitution before taking the derivative,
    but we proved our case all the same.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们使用链式法则找到的结果匹配。当然，在这个简单的例子中，先进行替代再求导更容易，但我们同样证明了我们的推理。
- en: We’re not entirely through with scalar-by-vector identities, however. The dot
    product takes two vectors and produces a scalar, so it fits with the functional
    form we’re working with, even though the arguments to the dot product are vectors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完全结束标量与向量的恒等式。点积接受两个向量并产生一个标量，因此它与我们正在使用的函数形式相符，尽管点积的参数是向量。
- en: 'For example, consider this result:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下结果：
- en: '![Image](Images/08equ09.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ09.jpg)'
- en: Here, we have the derivative of the dot product between ***x*** and a vector
    ***a*** that does not depend on ***x***.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们得到了***x***和一个不依赖于***x***的向量***a***的点积的导数。
- en: 'We can expand on [Equation 8.9](ch08.xhtml#ch08equ09), replacing ***x*** with
    a vector-valued function, ***f(x***):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以扩展[方程 8.9](ch08.xhtml#ch08equ09)，将***x***替换为一个向量值函数，***f(x***):'
- en: '![Image](Images/08equ10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ10.jpg)'
- en: What’s the form of this result? Assume *f* accepts an *m*-element input and
    returns an *n*-element vector output. Likewise, assume ***a*** to be an *n*-element
    vector. From [Equation 8.2](ch08.xhtml#ch08equ02), we know the derivative ∂***f***/∂***x***
    to be an *n* × *m* matrix. Therefore, the final result is a (1 × *n*) × (*n* ×
    *m*) → 1 × *m* row vector. Good! We know the derivative of a scalar function by
    a vector should be a row vector when using the numerator layout convention.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果的形式是什么？假设*f*接受一个*m*维的输入并返回一个*n*维的向量输出。同样，假设***a***是一个*n*维的向量。从[方程 8.2](ch08.xhtml#ch08equ02)中，我们知道∂***f***/∂***x***是一个*n*
    × *m*的矩阵。因此，最终结果是一个(1 × *n*) × (*n* × *m*) → 1 × *m*的行向量。很好！我们知道，当使用分子布局约定时，标量函数对向量的导数应该是一个行向量。
- en: Finally, the derivative of the dot product of two vector-valued functions, ***f***
    and ***g***, is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，两个向量值函数***f***和***g***的点积的导数是
- en: '![Image](Images/08equ11.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ11.jpg)'
- en: If [Equation 8.10](ch08.xhtml#ch08equ10) is a row vector, then the sum of two
    terms like it is also a row vector.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果[方程 8.10](ch08.xhtml#ch08equ10)是一个行向量，那么类似的两个项的和也是一个行向量。
- en: A Vector Function by a Scalar
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过标量的向量函数
- en: 'Vector-by-scalar differentiation, [Table 8-1](ch08.xhtml#ch08tab01), first
    row, second column, is less common in machine learning, so we’ll only examine
    a few identities. The first are multiplications by constants:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 向量与标量的微分，[表 8-1](ch08.xhtml#ch08tab01)，第一行第二列，在机器学习中比较少见，因此我们只检查几个恒等式。第一个是常数乘法：
- en: '![Image](Images/202equ01.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/202equ01.jpg)'
- en: Note, we can multiply on the left by a matrix, as the derivative is a column
    vector.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以通过矩阵在左侧相乘，因为导数是一个列向量。
- en: The sum rule still applies,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 求和法则仍然适用，
- en: '![Image](Images/202equ02.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/202equ02.jpg)'
- en: as does the chain rule,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如同链式法则，
- en: '![Image](Images/08equ12.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ12.jpg)'
- en: '[Equation 8.12](ch08.xhtml#ch08equ12) is correct, since the derivative of a
    vector by a scalar is a column vector, and the derivative of a vector by a vector
    is a matrix. Therefore, multiplying the matrix on the right by a column vector
    returns a column vector, as expected.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 8.12](ch08.xhtml#ch08equ12)是正确的，因为一个向量对标量的导数是一个列向量，而一个向量对向量的导数是一个矩阵。因此，将矩阵右乘列向量返回一个列向量，正如预期的那样。'
- en: 'Two other derivatives involving dot products with respect to a scalar are worth
    knowing about. The first is similar to [Equation 8.11](ch08.xhtml#ch08equ11) but
    with two vector-valued functions of a scalar:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个关于与标量相乘的点积的导数值得了解。第一个类似于[方程 8.11](ch08.xhtml#ch08equ11)，但涉及两个标量的向量值函数：
- en: '![Image](Images/202equ03.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/202equ03.jpg)'
- en: 'The second derivative concerns the composition of *f*(***g***) and ***g***(*x*)
    with respect to *x*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个导数涉及*f*(***g***)和***g***(*x*)关于*x*的复合：
- en: '![Image](Images/202equ04.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/202equ04.jpg)'
- en: which is the dot product of a row vector and a column vector.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是行向量与列向量的点积。
- en: A Vector Function by a Vector
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量函数与向量的乘积
- en: 'The derivatives of vector-valued functions with vector arguments are common
    in physics and engineering. In machine learning, they show up during backpropagation,
    for example, at the derivative of the loss function. Let’s begin with some straightforward
    identities:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 带有向量值函数和向量参数的导数在物理和工程中很常见。在机器学习中，它们在反向传播过程中出现，例如，在损失函数的导数中。让我们从一些简单的恒等式开始：
- en: '![Image](Images/203equ01.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/203equ01.jpg)'
- en: and
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: '![Image](Images/203equ02.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/203equ02.jpg)'
- en: where the result is the sum of two matrices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中结果是两个矩阵的和。
- en: 'The chain rule is next and works as it did above for scalar-by-vector and vector-by-scalar
    derivatives:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是链式法则，它与标量对向量和向量对标量导数的情况一样有效：
- en: '![Image](Images/203equ03.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/203equ03.jpg)'
- en: with the result being the product of two matrices.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个矩阵的乘积。
- en: A Scalar Function by a Matrix
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标量函数与矩阵的乘积
- en: 'For functions of a matrix, ***X***, returning a scalar, we have the usual form
    for the sum rule:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于返回标量的矩阵函数***X***，我们有通常形式的求和法则：
- en: '![Image](Images/203equ04.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/203equ04.jpg)'
- en: with the result being the sum of two matrices. Recall, if ***X*** is an *n*
    × *m* matrix, the derivative in numerator layout notation is an *m* × *n* matrix.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个矩阵的和。回想一下，如果***X***是一个*n* × *m*矩阵，则分子布局符号下的导数是一个*m* × *n*矩阵。
- en: 'The product rule also works as expected:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 乘积法则同样按预期工作：
- en: '![Image](Images/203equ05.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/203equ05.jpg)'
- en: 'However, the chain rule is different. It depends on *f(g*), a scalar function
    accepting a scalar input, and *g*(***X***), a scalar function accepting a matrix
    input. With this restriction, the form of the chain rule looks familiar:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，链式法则有所不同。它依赖于*f(g*)，一个接受标量输入的标量函数，以及*g*(***X***)，一个接受矩阵输入的标量函数。在这个限制下，链式法则的形式看起来很熟悉：
- en: '![Image](Images/08equ13.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ13.jpg)'
- en: 'Let’s see [Equation 8.13](ch08.xhtml#ch08equ13) in action. First, we need ***X***,
    a 2 × 2 matrix:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看[方程 8.13](ch08.xhtml#ch08equ13)的实际应用。首先，我们需要***X***，一个2 × 2矩阵：
- en: '![Image](Images/204equ01.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/204equ01.jpg)'
- en: Next, we need ![Image](Images/204equ02.jpg) and *g*(***X***) = *x*[0]*x*[3]
    + *x*[1]*x*[2]. Notice, while *g*(***X***) accepts a matrix input, the result
    is a scalar calculated from the values of the matrix.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要 ![Image](Images/204equ02.jpg) 和 *g*(***X***) = *x*[0]*x*[3] + *x*[1]*x*[2]。注意，虽然*g*(***X***)接受矩阵输入，但结果是从矩阵的值计算得到的标量。
- en: To apply the chain rule, we need two derivatives,
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用链式法则，我们需要两个导数，
- en: '![Image](Images/204equ03.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/204equ03.jpg)'
- en: where we are again using numerator layout for the result.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们再次使用分子布局表示结果。
- en: To find the overall result, we calculate
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到最终结果，我们计算
- en: '![Image](Images/204equ04.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/204equ04.jpg)'
- en: To check, we combine the functions to write a single function, ![Image](Images/204equ05.jpg),
    and calculate the derivative using the standard chain rule for each element of
    the resulting matrix. This gives us
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查，我们将函数合并成一个单一函数，![Image](Images/204equ05.jpg)，并使用标准的链式法则计算每个结果矩阵元素的导数。这给出了
- en: '![Image](Images/204equ06.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/204equ06.jpg)'
- en: matching the previous result.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的结果一致。
- en: We have our definitions and identities. Let’s revisit the derivative of a vector-valued
    function with a vector argument, as the resulting matrix is special. We’ll encounter
    it frequently in deep learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了定义和恒等式。让我们回顾一下向量值函数对于向量参数的导数，因为得到的矩阵是特殊的。在深度学习中，我们将频繁遇到它。
- en: Jacobians and Hessians
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jacobians 和 Hessians
- en: '[Equation 8.2](ch08.xhtml#ch08equ02) defined the derivative of a vector-valued
    function, ***f***, with respect to a vector, ***x***:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[公式 8.2](ch08.xhtml#ch08equ02)定义了向量值函数***f***相对于向量***x***的导数：'
- en: '![Image](Images/08equ14.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ14.jpg)'
- en: This derivative is known as the *Jacobian matrix*, ***J***, or simply the *Jacobian*,
    and you’ll encounter it from time to time in the deep learning literature, especially
    during discussions of gradient descent and other optimization algorithms used
    in training models. The Jacobian sometimes has a subscript to indicate the variable
    it is with respect to; for example, ***J******[x]*** if with respect to ***x***.
    When the context is clear, we’ll often neglect the subscript.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数被称为 *Jacobian 矩阵*，***J***，或简称 *Jacobian*，你将会在深度学习文献中时常遇到它，尤其是在讨论梯度下降和训练模型时使用的其他优化算法时。Jacobian
    有时会带下标，以指示它是相对于哪个变量的；例如，***J******[x]*** 如果是相对于***x***的。在上下文清晰的情况下，我们通常会忽略下标。
- en: In this section, we’ll discuss the Jacobian and what it means. Then we’ll introduce
    another matrix, the *Hessian matrix* (or just the *Hessian*), which is based on
    the Jacobian, and learn how to use it in optimization problems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 Jacobian 及其含义。接着，我们将介绍另一个矩阵，即基于 Jacobian 的 *Hessian 矩阵*（或简称 *Hessian*），并学习如何在优化问题中使用它。
- en: 'The essence of this section is the following: the Jacobian is the generalization
    of the first derivative, and the Hessian is the generalization of the second derivative.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的核心内容如下：Jacobian 是一阶导数的推广，Hessian 是二阶导数的推广。
- en: Concerning Jacobians
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于 Jacobians
- en: 'We saw previously that we can think of [Equation 8.14](ch08.xhtml#ch08equ14)
    as a stack of transposed gradient vectors ([Equation 8.3](ch08.xhtml#ch08equ03)):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到过，可以将[公式 8.14](ch08.xhtml#ch08equ14)视为梯度向量的转置堆叠（[公式 8.3](ch08.xhtml#ch08equ03)）：
- en: '![Image](Images/205equ01.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/205equ01.jpg)'
- en: Viewing the Jacobian as a stack of gradients gives us a clue as to what it represents.
    Recall, the gradient of a scalar field, a function accepting a vector argument
    and returning a scalar, points in the direction of the maximum change in the function.
    Similarly, the Jacobian gives us information about how the vector-valued function
    behaves in the vicinity of some point, ***x******[p]***. The Jacobian is to vector-valued
    functions of vectors what the gradient is to scalar-valued functions of vectors;
    it tells us about how the function changes for a small change in the position
    of ***x******[p]***.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Jacobian 视为梯度的堆叠为我们提供了它所表示内容的线索。回想一下，标量场的梯度，即接受向量参数并返回标量的函数，指向函数变化最大方向。类似地，Jacobian
    向我们提供了关于向量值函数在某个点附近（***x******[p]***）如何变化的信息。Jacobian 对于向量值函数而言，就像梯度对于标量值函数一样；它告诉我们函数在***x******[p]***位置发生小变化时的变化情况。
- en: One way to think of the Jacobian is as a generalization of the more specific
    cases we encountered in [Chapter 7](ch07.xhtml#ch07). [Table 8-2](ch08.xhtml#ch08tab02)
    shows the relationship between the function and what its derivative measures.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种理解 Jacobian 的方式是将其视为我们在[第 7 章](ch07.xhtml#ch07)遇到的更具体情况的推广。[表 8-2](ch08.xhtml#ch08tab02)展示了函数及其导数测量之间的关系。
- en: '**Table 8-2:** The Relationship Between Jacobians, Gradients, and Slopes'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 8-2：** Jacobian、梯度和斜率之间的关系'
- en: '| **Function** | **Derivative** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **函数** | **导数** |'
- en: '| --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ***f***(***x***) | ∂***f***/∂***x***, Jacobian matrix |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ***f***(***x***) | ∂***f***/∂***x***，Jacobian 矩阵 |'
- en: '| *f*(***x***) | ∂*f*/∂***x***, gradient vector |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| *f*(***x***) | ∂*f*/∂***x***，梯度向量 |'
- en: '| *f*(*x*) | *df*/*dx*, slope |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| *f*(*x*) | *df*/*dx*，斜率 |'
- en: 'The Jacobian matrix is the most general of the three. If we limit the function
    to a scalar, then the Jacobian matrix becomes the gradient vector (row vector
    in numerator layout). If we limit the function and argument to scalars, the gradient
    becomes the slope. In a sense, they all indicate the same thing: how the function
    is changing around a point in space.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Jacobian 矩阵是这三者中最为一般的。如果我们将函数限制为标量，那么 Jacobian 矩阵就变成了梯度向量（分子布局中的行向量）。如果我们将函数和参数限制为标量，梯度就变成了斜率。从某种意义上讲，它们都表示相同的含义：函数在空间中的某一点附近是如何变化的。
- en: The Jacobian has many uses. I’ll present two examples. The first is from systems
    of differential equations. The second uses Newton’s method to find the roots of
    a vector-valued function. We’ll see Jacobians again when we discuss backpropagation,
    as that requires calculating derivatives of a vector-valued function with respect
    to a vector.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 雅可比矩阵有很多用途。我将举两个例子。第一个例子来自微分方程系统。第二个例子使用牛顿法来找到向量值函数的根。当我们讨论反向传播时，我们将再次看到雅可比矩阵，因为那时需要计算一个向量值函数相对于另一个向量的导数。
- en: Autonomous Differential Equations
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自治微分方程
- en: A *differential equation* combines derivatives and function values in one equation.
    Differential equations show up everywhere in physics and engineering. Our example
    comes from the theory of *autonomous systems*, which are differential equations
    where the independent variable does not appear on the right-hand side. For instance,
    if the system consists of values of the function and first derivatives with respect
    to time, *t*, there is no *t* explicit in the equations governing the system.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*微分方程* 将导数和函数值结合在一个方程中。微分方程在物理学和工程学中无处不在。我们的例子来自 *自治系统* 理论，这是一类微分方程，其中自变量不出现在方程的右侧。例如，如果系统由函数值和关于时间
    *t* 的一阶导数组成，则在描述该系统的方程中不会显式出现 *t*。'
- en: The previous paragraph is just for background; you don’t need to memorize it.
    Working with systems of autonomous differential equations ultimately leads to
    the Jacobian, which is our goal. We can view the system as a vector-valued function,
    and we’ll use the Jacobian to characterize the critical points of that system
    (the points where the derivative is zero). We worked with critical points of functions
    in [Chapter 7](ch07.xhtml#ch07).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段只是背景介绍；你不需要记住它。处理自治微分方程系统最终会引出雅可比矩阵，这是我们的目标。我们可以将系统视为一个向量值函数，并且我们将使用雅可比矩阵来描述该系统的临界点（导数为零的点）。我们在[第七章](ch07.xhtml#ch07)中处理过函数的临界点。
- en: 'For example, let’s explore the following system of equations:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们探索以下方程系统：
- en: '![Image](Images/206equ01.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/206equ01.jpg)'
- en: This system includes two functions, *x*(*t*) and *y*(*t*), and they are coupled
    so that the rate of change of *x*(*t*) depends on the value of *x* and the value
    of *y*, and vice versa.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统包含了两个函数，*x*(*t*) 和 *y*(*t*)，它们是耦合的，*x*(*t*) 的变化率依赖于 *x* 和 *y* 的值，反之亦然。
- en: 'We’ll view the system as a single, vector-valued function:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将系统视为一个单一的向量值函数：
- en: '![Image](Images/08equ15.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ15.jpg)'
- en: where we replace *x* with *x*[0] and *y* with *x*[1].
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *x* 替换为 *x*[0]，将 *y* 替换为 *x*[1]。
- en: The system that ***f*** represents has critical points at locations where ***f***
    = **0**, with **0** being the 2 × 1 dimensional zero vector. The critical points
    are
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '***f*** 表示的系统在 ***f*** = **0** 的位置有临界点，其中 **0** 是 2 × 1 维度的零向量。临界点是'
- en: '![Image](Images/08equ16.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ16.jpg)'
- en: where substitution into ***f*** shows that each point returns the zero vector.
    For the time being, assume we were given the critical points, and now we want
    to characterize them.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其中代入 ***f*** 证明每个点都返回零向量。暂时假设我们已知临界点，现在我们要对它们进行表征。
- en: 'To characterize a critical point, we will need the Jacobian matrix that ***f***
    generates:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要描述临界点，我们需要 ***f*** 生成的雅可比矩阵：
- en: '![Image](Images/08equ17.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ17.jpg)'
- en: Since the Jacobian describes how a function behaves in the vicinity of a point,
    we can use it to characterize the critical points. In [Chapter 7](ch07.xhtml#ch07),
    we used the derivative to tell us whether a point was a minimum or maximum of
    a function. For the Jacobian, we use the eigenvalues of ***J*** in much the same
    way to talk about the type and stability of critical points.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于雅可比矩阵描述了一个函数在某个点附近的行为，我们可以用它来表征临界点。在[第七章](ch07.xhtml#ch07)中，我们使用导数来判断一个点是函数的最小值点还是最大值点。对于雅可比矩阵，我们使用
    ***J*** 的特征值来以类似的方式讨论临界点的类型和稳定性。
- en: 'First, let’s find the Jacobian at each of the critical points:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们找到每个临界点的雅可比矩阵：
- en: '![Image](Images/207equ01.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/207equ01.jpg)'
- en: 'We can use NumPy to get the eigenvalues of the Jacobians:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 NumPy 获取雅可比矩阵的特征值：
- en: '>>> import numpy as np'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import numpy as np'
- en: '>>> np.linalg.eig([[4,0],[0,2]])[0]'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.linalg.eig([[4,0],[0,2]])[0]'
- en: array([4., 2.])
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: array([4., 2.])
- en: '>>> np.linalg.eig([[2,0],[1,-2]])[0]'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.linalg.eig([[2,0],[1,-2]])[0]'
- en: array([-2., 2.])
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: array([-2., 2.])
- en: '>>> np.linalg.eig([[0,-4],[2,-4]])[0]'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.linalg.eig([[0,-4],[2,-4]])[0]'
- en: array([-2.+2.j, -2.-2.j])
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: array([-2.+2.j, -2.-2.j])
- en: We encountered np.linalg.eig in [Chapter 6](ch06.xhtml#ch06). The eigenvalues
    are the first values that eig returns, hence the [0] subscript to the function
    call.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](ch06.xhtml#ch06)中遇到了np.linalg.eig。特征值是eig返回的第一个值，因此函数调用中的[0]下标。
- en: For critical points of a system of autonomous differential equations, the eigenvalues
    indicate the points’ type and stability. If both eigenvalues are real and have
    the same sign, the critical point is a node. If the eigenvalues are less than
    zero, the node is stable; otherwise, it is unstable. You can think of a stable
    node as a pit; if you’re near it, you’ll fall into it. An unstable node is like
    a hill; if you move away from the top, the critical point, you’ll fall off. The
    first critical point, ***c***[0], has positive, real eigenvalues; therefore, it
    represents an unstable node.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自治微分方程系统的临界点，特征值表明了这些点的类型和稳定性。如果两个特征值都是实数且符号相同，则临界点是一个节点。如果特征值小于零，则节点是稳定的；否则，它是不稳定的。你可以把稳定的节点看作一个坑；如果你接近它，就会掉进去。一个不稳定的节点就像一个山丘；如果你远离顶点——即临界点——你就会掉下来。第一个临界点***c***[0]具有正的实特征值；因此，它表示一个不稳定的节点。
- en: If the eigenvalues of the Jacobian are real but of opposite signs, the critical
    point is a saddle point. We discussed saddle points in [Chapter 7](ch07.xhtml#ch07).
    A saddle point is ultimately unstable, but in two dimensions, there’s a direction
    where you can “fall into” the saddle and a direction where you can “fall off”
    the saddle. Some researchers believe most minima found when training deep neural
    networks are really saddle points of the loss function. We see that critical point
    ***c***[1] is a saddle point, since the eigenvalues are real with opposite signs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果雅可比矩阵的特征值是实数且符号相反，那么临界点就是鞍点。我们在[第7章](ch07.xhtml#ch07)中讨论过鞍点。鞍点最终是不稳定的，但在二维中，存在一个方向可以“进入”鞍点，另一个方向则可以“离开”鞍点。一些研究人员认为，在训练深度神经网络时找到的大多数最小值实际上是损失函数的鞍点。我们看到，临界点***c***[1]是一个鞍点，因为其特征值是实数且符号相反。
- en: Finally, the eigenvalues of ***c***[2] are complex. Complex eigenvalues indicate
    a spiral (also called a focus). If the real part of the eigenvalues is less than
    zero, the spiral is stable; otherwise, it is unstable. As the eigenvalues are
    complex conjugates of each other, the signs of the real parts must be the same;
    one can’t be positive while the other is negative. For ***c***[2], the real parts
    are negative, so ***c***[2] indicates a stable spiral.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，***c***[2]的特征值是复数。复特征值表明存在一个螺旋（也称为焦点）。如果特征值的实部小于零，则螺旋是稳定的；否则，它是不稳定的。由于特征值是互为共轭的复数，其实部的符号必须相同；不能一个为正另一个为负。对于***c***[2]，实部为负，因此***c***[2]表示一个稳定的螺旋。
- en: Newton’s Method
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 牛顿法
- en: I presented the critical points of [Equation 8.15](ch08.xhtml#ch08equ15) by
    fiat. The system is easy enough that we can solve for the critical points algebraically,
    but that might not generally be the case. One classic method for finding the roots
    of a function (the places where it returns zero) is known as *Newton’s method*.
    This is an iterative method using the first derivative and an initial guess to
    zero in on the root. Let’s look at the method in one dimension and then extend
    it to two. We’ll see that moving to two or more dimensions requires the use of
    the Jacobian.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过推导给出了[方程8.15](ch08.xhtml#ch08equ15)的临界点。这个系统足够简单，我们可以代数地求解临界点，但这在一般情况下不一定成立。寻找一个函数根（即返回零的地方）的一种经典方法就是*牛顿法*。这是一种迭代法，通过使用一阶导数和初始猜测来逼近根。我们首先来看一维情况，然后扩展到二维。我们会看到，进入两维或更多维度需要使用雅可比矩阵。
- en: 'Let’s use Newton’s method to find the square root of 2\. To do that, we need
    an equation such that ![Image](Images/208equ01.jpg). A moment’s thought gives
    us one: *f*(*x*) = 2 − *x*². Clearly, when ![Image](Images/208equ02.jpg).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用牛顿法来求解2的平方根。为此，我们需要一个方程，使得![Image](Images/208equ01.jpg)。经过一番思考，我们得到了一个：*f*(*x*)
    = 2 − *x*²。显然，当![Image](Images/208equ02.jpg)时。
- en: The governing equation for Newton’s method in one dimension is
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿法在一维的控制方程是
- en: '![Image](Images/08equ18.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ18.jpg)'
- en: where *x*[0] is some initial guess at the solution.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x*[0] 是对解的一个初始猜测。
- en: We substitute *x*[0] for *x[n]* on the right-hand side of [Equation 8.18](ch08.xhtml#ch08equ18)
    to find *x*[1]. We then repeat using *x*[1] on the right-hand side to get *x*[2],
    and so on until we see little change in *x[n]*. At that point, if our initial
    guess is reasonable, we have the value we’re looking for. Newton’s method converges
    quickly, so for typical examples, we only need a handful of iterations. Of course,
    we have powerful computers at our fingertips, so we’ll use them instead of working
    by hand. The Python code we need is in [Listing 8-1](ch08.xhtml#ch08ex01).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*x*[0]代入[方程 8.18](ch08.xhtml#ch08equ18)的右侧，以找到*x*[1]。然后，我们重复使用*x*[1]代入右侧，得到*x*[2]，以此类推，直到我们看到*x[n]*几乎没有变化。此时，如果我们的初始猜测是合理的，那么我们就得到了我们想要的值。牛顿法收敛得很快，因此对于典型的例子，我们只需要进行几次迭代。当然，我们有强大的计算机可以使用，所以我们将利用它们，而不是手动计算。我们需要的Python代码在[清单
    8-1](ch08.xhtml#ch08ex01)中。
- en: import numpy as np
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: 'def f(x):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f(x):'
- en: return 2.0 - x*x
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: return 2.0 - x*x
- en: 'def d(x):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 'def d(x):'
- en: return -2.0*x
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: return -2.0*x
- en: x = 1.0
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: x = 1.0
- en: 'for i in range(5):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(5):'
- en: x = x - f(x)/d(x)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: x = x - f(x)/d(x)
- en: 'print("%2d: %0.16f" % (i+1,x))'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%2d: %0.16f" % (i+1,x))'
- en: print("NumPy says sqrt(2) = %0.16f for a deviation of %0.16f" %
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: print("NumPy表示sqrt(2) = %0.16f，偏差为 %0.16f" %
- en: (np.sqrt(2), np.abs(np.sqrt(2)-x)))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: (np.sqrt(2), np.abs(np.sqrt(2)-x)))
- en: '*Listing 8-1: Finding* ![Image](Images/209equ01.jpg) *via Newton’s method*'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：通过牛顿法寻找* ![图片](Images/209equ01.jpg) *的平方根*'
- en: '[Listing 8-1](ch08.xhtml#ch08ex01) defines two functions. The first, f(x),
    returns the function value for a given x. The second, d(x), returns the derivative
    at x. If *f*(*x*) = 2 − *x*², then *f*′(*x*) = −2*x*.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-1](ch08.xhtml#ch08ex01)定义了两个函数。第一个，f(x)，返回给定x的函数值。第二个，d(x)，返回x的导数。如果*f*(*x*)
    = 2 − *x*²，那么*f*′(*x*) = −2*x*。'
- en: Our initial guess is *x* = 1.0\. We iterate [Equation 8.18](ch08.xhtml#ch08equ18)
    five times, printing the current estimate of the square root of 2 each time. Finally,
    we use NumPy to calculate the *true* value and see how far we are from it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始猜测是*x* = 1.0。我们迭代[方程 8.18](ch08.xhtml#ch08equ18)五次，每次打印当前估算的2的平方根。最后，我们使用NumPy计算*真实*值，看看与真实值的差距有多大。
- en: Running [Listing 8-1](ch08.xhtml#ch08ex01) produces
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 运行[清单 8-1](ch08.xhtml#ch08ex01)会产生
- en: '1: 1.5000000000000000'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 1.5000000000000000'
- en: '2: 1.4166666666666667'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 1.4166666666666667'
- en: '3: 1.4142156862745099'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 1.4142156862745099'
- en: '4: 1.4142135623746899'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '4: 1.4142135623746899'
- en: '5: 1.4142135623730951'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '5: 1.4142135623730951'
- en: NumPy says sqrt(2) = 1.4142135623730951 for a
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy表示sqrt(2) = 1.4142135623730951，偏差为
- en: deviation of 0.0000000000000000
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差为0.0000000000000000
- en: which is impressive; we get ![Image](Images/209equ02.jpg) to 16 decimals in
    only five iterations.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这令人印象深刻；我们仅用了五次迭代，就将![图片](Images/209equ02.jpg)计算到了16位小数。
- en: To extend Newton’s method to vector-valued functions of vectors, like [Equation
    8.15](ch08.xhtml#ch08equ15), we replace the reciprocal of the derivative with
    the inverse of the Jacobian. Why the inverse? Recall, for a diagonal matrix, the
    inverse is the reciprocal of the diagonal elements. If we view the scalar derivative
    as a 1 × 1 matrix, then the reciprocal and inverse are the same. [Equation 8.18](ch08.xhtml#ch08equ18)
    is already using the inverse of the Jacobian, albeit one for a 1 × 1 matrix. Therefore,
    we’ll iterate
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将牛顿法扩展到向量值函数，像[方程 8.15](ch08.xhtml#ch08equ15)那样，我们将导数的倒数替换为雅可比矩阵的逆。为什么是逆矩阵？回想一下，对于一个对角矩阵，逆矩阵就是对角线元素的倒数。如果我们把标量导数看作一个1×1矩阵，那么倒数和逆矩阵是相同的。[方程
    8.18](ch08.xhtml#ch08equ18)已经使用了雅可比矩阵的逆，尽管它是一个1×1矩阵。因此，我们将迭代
- en: '![Image](Images/08equ19.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ19.jpg)'
- en: for a suitable initial value, ***x***[0], and the inverse of the Jacobian evaluated
    at *x[n]*. Let’s use Newton’s method to find the critical points of [Equation
    8.15](ch08.xhtml#ch08equ15).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于合适的初始值，***x***[0]，以及在*x[n]*处评估的雅可比矩阵的逆。我们使用牛顿法来找到[方程 8.15](ch08.xhtml#ch08equ15)的临界点。
- en: Before we can write some Python code, we need the inverse of the Jacobian, [Equation
    8.17](ch08.xhtml#ch08equ17). The inverse of a 2 × 2 matrix,
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以编写一些Python代码之前，我们需要雅可比矩阵的逆，[方程 8.17](ch08.xhtml#ch08equ17)。一个2×2矩阵的逆，
- en: '![Image](Images/210equ01.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/210equ01.jpg)'
- en: is
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: '![Image](Images/210equ02.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/210equ02.jpg)'
- en: assuming the determinant is not zero. The determinant of ***A*** is *ad* − *bc*.
    Therefore, the inverse of [Equation 8.17](ch08.xhtml#ch08equ17) is
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 假设行列式不为零。***A***的行列式为*ad* − *bc*。因此，[方程 8.17](ch08.xhtml#ch08equ17)的逆是
- en: '![Image](Images/210equ03.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/210equ03.jpg)'
- en: Now we can write our code. The result is [Listing 8-2](ch08.xhtml#ch08ex02).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写我们的代码了。结果见[清单 8-2](ch08.xhtml#ch08ex02)。
- en: import numpy as np
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: 'def f(x):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f(x):'
- en: x0,x1 = x[0,0],x[1,0]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: x0,x1 = x[0,0],x[1,0]
- en: return np.array([[4*x0-2*x0*x1],[2*x1+x0*x1-2*x1**2]])
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: return np.array([[4*x0-2*x0*x1],[2*x1+x0*x1-2*x1**2]])
- en: 'def JI(x):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'def JI(x):'
- en: x0,x1 = x[0,0],x[1,0]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: x0,x1 = x[0,0],x[1,0]
- en: d = (4-2*x1)*(2-x0-4*x1)+2*x0*x1
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: d = (4-2*x1)*(2-x0-4*x1)+2*x0*x1
- en: return (1/d)*np.array([[2-x0-4*x1,2*x0],[-x1,4-2*x0]])
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: return (1/d)*np.array([[2-x0-4*x1,2*x0],[-x1,4-2*x0]])
- en: 'x0 = float(input("x0: "))'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'x0 = float(input("x0: "))'
- en: 'x1 = float(input("x1: "))'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'x1 = float(input("x1: "))'
- en: ❶ x = np.array([[x0],[x1]])
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x = np.array([[x0],[x1]])
- en: N = 20
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: N = 20
- en: 'for i in range(N):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(N):'
- en: ❷ x = x - JI(x) @ f(x)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x = x - JI(x) @ f(x)
- en: 'if (i > (N-10)):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (i > (N-10)):'
- en: 'print("%4d: (%0.8f, %0.8f)" % (i, x[0,0],x[1,0]))'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("%4d: (%0.8f, %0.8f)" % (i, x[0,0],x[1,0]))'
- en: '*Listing 8-2: Newton’s method in 2D*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 8-2：二维中的牛顿法*'
- en: '[Listing 8-2](ch08.xhtml#ch08ex02) echoes [Listing 8-1](ch08.xhtml#ch08ex01)
    for the 1D case. We have f(x) to calculate the function value for a given input
    vector and JI(x) to give us the value of the inverse Jacobian at ***x***. Notice
    that f(x) returns a column vector and JI(x) returns a 2 × 2 matrix.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-2](ch08.xhtml#ch08ex02) 与 [列表 8-1](ch08.xhtml#ch08ex01) 在一维情况下是相同的。我们使用
    f(x) 来计算给定输入向量的函数值，使用 JI(x) 来得到 ***x*** 处逆雅可比矩阵的值。注意，f(x) 返回一个列向量，而 JI(x) 返回一个
    2 × 2 的矩阵。'
- en: The code first asks the user for initial guesses, x0 and x1. These are formed
    into the initial vector, x. Note that we explicitly form x as a column vector
    ❶.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先要求用户输入初始猜测，x0 和 x1。这些值将形成初始向量 x。请注意，我们明确将 x 作为列向量 ❶。
- en: The implementation of [Equation 8.19](ch08.xhtml#ch08equ19) comes next ❷. The
    inverse Jacobian is a 2 × 2 matrix that we multiply on the right by the function
    value, a 2 × 1 column vector, using NumPy’s matrix multiplication operator, @.
    The result is a 2 × 1 column vector subtracted from the current value of x, itself
    a 2 × 1 column vector. If the loop is within 10 iterations of completion, the
    current value is printed at the console.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来实现[方程 8.19](ch08.xhtml#ch08equ19)。逆雅可比矩阵是一个 2 × 2 的矩阵，我们通过 NumPy 的矩阵乘法运算符
    @ 将其与函数值（一个 2 × 1 的列向量）相乘。结果是一个 2 × 1 的列向量，从当前的 x 值（也是一个 2 × 1 的列向量）中减去。如果循环在
    10 次迭代内完成，当前值将打印在控制台上。
- en: Does [Listing 8-2](ch08.xhtml#ch08ex02) work? Let’s run it and see if we can
    find initial guesses leading to each of the critical points ([Equation 8.16](ch08.xhtml#ch08equ16)).
    For an initial guess of ![Image](Images/211equ01.jpg), we get
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8-2](ch08.xhtml#ch08ex02) 是否有效？让我们运行它，看看是否能找到指向每个临界点的初始猜测（[方程 8.16](ch08.xhtml#ch08equ16)）。对于初始猜测
    ![图片](Images/211equ01.jpg)，我们得到'
- en: '11: (0.00004807, -1.07511237)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '11: (0.00004807, -1.07511237)'
- en: '12: (0.00001107, -0.61452262)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '12: (0.00001107, -0.61452262)'
- en: '13: (0.00000188, -0.27403667)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '13: (0.00000188, -0.27403667)'
- en: '14: (0.00000019, -0.07568702)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '14: (0.00000019, -0.07568702)'
- en: '15: (0.00000001, -0.00755378)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '15: (0.00000001, -0.00755378)'
- en: '16: (0.00000000, -0.00008442)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '16: (0.00000000, -0.00008442)'
- en: '17: (0.00000000, -0.00000001)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '17: (0.00000000, -0.00000001)'
- en: '18: (0.00000000, -0.00000000)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '18: (0.00000000, -0.00000000)'
- en: '19: (0.00000000, -0.00000000)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '19: (0.00000000, -0.00000000)'
- en: which is the first critical point of [Equation 8.15](ch08.xhtml#ch08equ15).
    To find the two remaining critical points, we need to pick our initial guesses
    with some care. Some guesses explode, while many lead back to the zero vector.
    However, some trial and error gives us
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 [方程 8.15](ch08.xhtml#ch08equ15) 的第一个临界点。为了找到剩下的两个临界点，我们需要小心地选择初始猜测。有些猜测会爆炸，而很多会导致返回零向量。然而，通过一些试探和错误，我们得到了
- en: '![Image](Images/211equ02.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/211equ02.jpg)'
- en: showing that Newton’s method can find the critical points of [Equation 8.15](ch08.xhtml#ch08equ15).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明牛顿法能够找到 [方程 8.15](ch08.xhtml#ch08equ15) 的临界点。
- en: We started this section with a system of differential equations that we interpreted
    as a vector-valued function. We then used the Jacobian to characterize the critical
    points of that system. Next, we used the Jacobian a second time to locate the
    system’s critical points via Newton’s method. We could do this because the Jacobian
    is the generalization of the gradient to vector-valued functions, and the gradient
    itself is a generalization of the first derivative of a scalar function. As mentioned
    above, we’ll see Jacobians again when we discuss backpropagation in [Chapter 10](ch10.xhtml#ch10).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一组常微分方程开始，将其解释为一个向量值函数。然后，我们使用雅可比矩阵来表征该系统的临界点。接下来，我们第二次使用雅可比矩阵通过牛顿法找到系统的临界点。我们之所以能做到这一点，是因为雅可比矩阵是梯度在向量值函数上的推广，而梯度本身则是标量函数一阶导数的推广。如上所述，我们将在
    [第10章](ch10.xhtml#ch10) 讨论反向传播时再次看到雅可比矩阵。
- en: Concerning Hessians
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于 Hessian 矩阵
- en: If the Jacobian matrix is like the first derivative of a function of one variable,
    then the *Hessian matrix* is like the second derivative. In this case, we’re restricted
    to scalar fields, functions returning a scalar value for a vector input. Let’s
    start with the definition and go from there. For the function *f*(***x***), the
    Hessian is defined as
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果雅可比矩阵像一元函数的导数，那么 *Hessian 矩阵* 就像二阶导数。在这种情况下，我们只限制于标量场，即对向量输入返回标量值的函数。让我们从定义开始并继续。对于函数
    *f*(***x***)，Hessian 被定义为
- en: '![Image](Images/08equ20.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ20.jpg)'
- en: where ***x*** = [*x*[0] *x*[1] . . . *x*[*n*–1]]^⊤.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***x*** = [*x*[0] *x*[1] . . . *x*[*n*–1]]^⊤。
- en: Looking at [Equation 8.20](ch08.xhtml#ch08equ20) tells us that the Hessian is
    a square matrix. Moreover, it’s a symmetric matrix implying ***H*** = ***H***^⊤.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[公式 8.20](ch08.xhtml#ch08equ20)可以知道 Hessian 是一个方阵。此外，它是一个对称矩阵，意味着 ***H***
    = ***H***^⊤。
- en: 'The Hessian is the Jacobian of the gradient of a scalar field:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 是标量场梯度的雅可比矩阵：
- en: '***H**[f]* = ***J***(▽*f*)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '***H**[f]* = ***J***(▽*f*)'
- en: 'Let’s see this with an example. Consider this function:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看这个。考虑这个函数：
- en: '![Image](Images/212equ01.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/212equ01.jpg)'
- en: 'If we use the definition of the Hessian in [Equation 8.20](ch08.xhtml#ch08equ20)
    directly, we see that ![Image](Images/212equ02.jpg) because ∂*f*/∂*x*[0] = 4*x*[0]
    + *x*[2]. Similar calculations give us the rest of the Hessian matrix:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们直接使用[公式 8.20](ch08.xhtml#ch08equ20)定义 Hessian，看到的是 ![图片](Images/212equ02.jpg)，因为
    ∂*f*/∂*x*[0] = 4*x*[0] + *x*[2]。类似的计算给出了 Hessian 矩阵的其余部分：
- en: '![Image](Images/212equ03.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/212equ03.jpg)'
- en: In this case, the Hessian is constant, not a function of ***x***, because the
    highest power of a variable in *f*(***x***) is 2.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Hessian 是常数，而不是 *x* 的函数，因为 *f*(***x***) 中变量的最高次方是 2。
- en: The gradient of *f*(***x***), using our column vector definition, is
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们列向量定义的 *f*(***x***) 的梯度为
- en: '![Image](Images/212equ04.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/212equ04.jpg)'
- en: with the Jacobian of the gradient giving the following, which is identical to
    the matrix we found by direct use of [Equation 8.20](ch08.xhtml#ch08equ20).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的雅可比矩阵给出了以下结果，这与我们通过直接使用[公式 8.20](ch08.xhtml#ch08equ20)找到的矩阵相同。
- en: '![Image](Images/212equ05.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/212equ05.jpg)'
- en: Minima and Maxima
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最小值和最大值
- en: 'We saw in [Chapter 7](ch07.xhtml#ch07) that we could use the second derivative
    to test whether critical points of a function were minima (*f*′′ > 0) or maxima
    (*f*′′ < 0). We’ll see in the next section how we can use critical points in optimization
    problems. For now, let’s use the Hessian to find critical points by considering
    its eigenvalues. We’ll continue with the example above. The Hessian matrix is
    3 × 3, meaning there are three (or fewer) eigenvalues. Again, we’ll save time
    and use NumPy to tell us what they are:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 7 章](ch07.xhtml#ch07)中看到，我们可以通过二阶导数来测试一个函数的临界点是否是最小值 (*f*′′ > 0) 或最大值
    (*f*′′ < 0)。我们将在下一节看到如何在优化问题中使用临界点。现在，让我们使用 Hessian 通过考虑其特征值来找到临界点。我们将继续使用上面的例子。Hessian
    矩阵是 3 × 3，意味着有三个（或更少）特征值。再次，我们将节省时间并使用 NumPy 来告诉我们它们是什么：
- en: '>>> np.linalg.eig([[4,0,1],[0,-2,3],[1,3,0]])[0]'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> np.linalg.eig([[4,0,1],[0,-2,3],[1,3,0]])[0]'
- en: array([ 4.34211128, 1.86236874, -4.20448002])
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: array([ 4.34211128, 1.86236874, -4.20448002])
- en: Two of the three eigenvalues are positive, and one is negative. If all three
    were positive, the critical point would be a minimum. Likewise, if all three were
    negative, the critical point would be a maximum. Notice that the minimum/maximum
    label is the opposite of the sign, just like the single-variable case. However,
    if at least one eigenvalue is positive and another negative, which is the case
    with our example, the critical point is a saddle point.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 三个特征值中有两个是正的，一个是负的。如果三个都是正的，则临界点是最小值。同样，如果三个都是负的，则临界点是最大值。请注意，最小值/最大值标签与符号相反，就像单变量的情况一样。然而，如果至少一个特征值为正，另一个为负（正如我们示例中的情况），那么临界点是鞍点。
- en: It seems natural to ask whether the Hessian of a vector-valued function, ***f***(***x***),
    exists. After all, we can calculate the Jacobian of such a function; we did so
    above to show that the Hessian is the Jacobian of the gradient.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 自然会问，向量值函数的 Hessian 是否存在。毕竟，我们可以计算这种函数的雅可比矩阵；我们在上面已经展示了 Hessian 是梯度的雅可比矩阵。
- en: 'It is possible to extend the Hessian to a vector-valued function. However,
    the result is no longer a matrix, but an order-3 tensor. To see this is so, consider
    the definition of a vector-valued function:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 Hessian 扩展到向量值函数。然而，结果不再是一个矩阵，而是一个三阶张量。为了证明这一点，考虑向量值函数的定义：
- en: '![Image](Images/213equ01.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/213equ01.jpg)'
- en: We can think of a vector-valued function, a vector field, as a vector of scalar
    functions of a vector. We could calculate the Hessian of each of the *m* functions
    in ***f*** to get a vector of matrices,
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将向量值函数、向量场，视为一个标量函数的向量，每个标量函数是一个向量。我们可以计算每个***f***中的*m*个函数的Hessian，从而得到一个矩阵的向量，
- en: '![Image](Images/213equ02.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/213equ02.jpg)'
- en: 'but a vector of matrices is a 3D object. Think of an RGB image: a 3D array
    made up of three 2D images, one each for the red, green, and blue channels. Therefore,
    while possible to define and calculate, the Hessian of a vector-valued function
    is beyond our current scope.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，矩阵的向量是一个三维对象。可以想象一个RGB图像：一个由三个二维图像组成的三维数组，每个图像代表红色、绿色和蓝色通道。因此，尽管可以定义和计算，但向量值函数的Hessian超出了我们当前的讨论范围。
- en: Optimization
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化
- en: In deep learning, you’re most likely to see the Hessian in reference to optimization.
    Training a neural network is, to a first approximation, an optimization problem—the
    goal is to find the weights and biases leading to a minimum in the loss function
    landscape.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，您最可能看到Hessian与优化相关。训练神经网络，在初步的近似下，是一个优化问题——目标是找到导致损失函数最小值的权重和偏置。
- en: In [Chapter 7](ch07.xhtml#ch07), we saw that the gradient provides information
    on how to move toward a minimum. An optimization algorithm, like gradient descent,
    the subject of [Chapter 11](ch11.xhtml#ch11), uses the gradient as a guide. As
    the gradient is a first derivative of the loss function, algorithms based solely
    on the gradient are known as *first-order optimization methods*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml#ch07)中，我们看到梯度提供了如何向最小值移动的信息。像梯度下降这样的优化算法，[第11章](ch11.xhtml#ch11)的主题，利用梯度作为指导。由于梯度是损失函数的一阶导数，基于梯度的算法被称为*一阶优化方法*。
- en: The Hessian provides information beyond the gradient. As a second derivative,
    the Hessian contains information about how the loss landscape’s gradient is changing,
    that is, its curvature. An analogy from physics might help here. A particle’s
    motion in one dimension is described by some function of time, *x*(*t*). The first
    derivative, the velocity, is *dx*/*dt* = *v*(*t*). The velocity tells us how quickly
    the position is changing in time. However, the velocity might change with time,
    so its derivative, *dv*/*dt* = *a*(*t*), is the acceleration. And, if the velocity
    is the first derivative of the position, then the acceleration is the second,
    *d*²*x*/*dt*² = *a*(*t*). Similarly, the second derivative of the loss function,
    the Hessian, provides information on how the gradient is changing. Optimization
    algorithms using the Hessian, or an approximation of it, are known as *second-order
    optimization methods*.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵提供了比梯度更多的信息。作为二阶导数，Hessian包含了关于损失函数的梯度变化的信息，也就是它的曲率。这里一个物理学的类比可能有帮助。一个粒子在一维空间中的运动由某个时间函数*x*(*t*)描述。它的导数，即速度，是*dx*/*dt*
    = *v*(*t*)。速度告诉我们位置变化的快慢。然而，速度可能随时间变化，因此它的导数，*dv*/*dt* = *a*(*t*)，就是加速度。如果速度是位置的一阶导数，那么加速度就是二阶导数，*d*²*x*/*dt*²
    = *a*(*t*)。类似地，损失函数的二阶导数——Hessian，提供了梯度变化的信息。使用Hessian或其近似的优化算法被称为*二阶优化方法*。
- en: Let’s start with an example in one dimension. We have a function, *f*(*x*),
    and we’re currently at some *x*[0]. We want to move from this position to a new
    position, *x*[1], closer to a minimum of *f*(*x*). A first-order algorithm will
    use the gradient, here the derivative, as a guide, since we know moving in the
    direction opposite to the derivative will move us toward a lower function value.
    Therefore, for some step size, call it η (eta), we can write
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个一维的例子开始。我们有一个函数*f*(*x*)，当前处于某个*x*[0]的位置。我们希望从这个位置移动到一个新位置*x*[1]，更接近*f*(*x*)的最小值。一个一阶算法将使用梯度，这里是导数，作为指导，因为我们知道沿着与导数方向相反的方向移动将使我们更接近更低的函数值。因此，对于某个步长，记作η（eta），我们可以写成
- en: '*x*[1] = *x*[0] − η *f*′(*x*[0])'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[1] = *x*[0] − η *f*′(*x*[0])'
- en: This will move us from *x*[0] toward *x*[1], which is closer to the minimum
    of *f*(*x*), assuming the minimum exists.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们从*x*[0]移动到*x*[1]，即更接近*f*(*x*)的最小值，假设最小值存在。
- en: The equation above makes sense, so why think about a second-order method? The
    second-order method comes into play when we move from *f*(*x*) to *f*(*x*). Now
    we have a gradient, not just a derivative, and the landscape of *f*(***x***) around
    some point can be more complex. The general form of gradient descent is
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程是有道理的，那么为什么还要考虑二阶方法呢？二阶方法在我们从 *f*(*x*) 转向 *f*(*x*) 时发挥作用。现在我们有梯度，而不仅仅是导数，*f*(***x***)
    在某些点周围的形态可能更为复杂。梯度下降法的一般形式是
- en: '***x***[1] = ***x***[0] − η▽*f*(***x***[0])'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '***x***[1] = ***x***[0] − η▽*f*(***x***[0])'
- en: 'but the information in the Hessian can be of assistance. To see how, we first
    need to introduce the idea of a *Taylor series expansion*, a way of approximating
    an arbitrary function as the sum of a series of terms. We use Taylor series frequently
    in physics and engineering to simplify complex functions in the vicinity of a
    specific point. We also often use them to calculate values of *transcendental
    functions* (functions that can’t be written as a finite set of basic algebra operations).
    For example, it’s likely that when you use cos(x) in a programming language, the
    result is generated by a Taylor series expansion with a sufficient number of terms
    to get the cosine to 32- or 64-bit floating-point precision:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 但是赫西矩阵中的信息可以提供帮助。为了理解这一点，我们首先需要介绍 *泰勒级数展开* 的概念，这是一种将任意函数近似为一系列项之和的方法。我们在物理和工程中经常使用泰勒级数来简化特定点附近的复杂函数。我们也常用它们来计算
    *超越函数*（不能用有限的代数运算表示的函数）的值。例如，当你在编程语言中使用 cos(x) 时，结果通常是通过泰勒级数展开得到的，并使用足够多的项以确保余弦值达到
    32 位或 64 位浮点精度：
- en: '![Image](Images/215equ01.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/215equ01.jpg)'
- en: In general, to approximate a function, *f*(*x*), in the vicinity of a point,
    *x* = *a*, the Taylor series expansion is
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，要在点 *x* = *a* 附近逼近函数 *f*(*x*)，我们使用泰勒级数展开式：
- en: '![Image](Images/215equ02.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/215equ02.jpg)'
- en: where *f*^((*k*))(*a*) is the *k*th derivative of *f*(*x*) evaluated at point
    *a*.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f*^((*k*))(*a*) 是 *f*(*x*) 在点 *a* 处的第 *k* 阶导数。
- en: A linear approximation of *f*(*x*) around *x* = *a* is
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*) 在 *x* = *a* 处的线性近似是'
- en: '*f*(*x*) ≈ *f*(*a*) + *f*′(*a*)(*x* - *a*)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*) ≈ *f*(*a*) + *f*′(*a*)(*x* - *a*)'
- en: while a quadratic approximation of *f*(*x*) becomes
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 而 *f*(*x*) 的二次近似变为
- en: '![Image](Images/08equ21.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ21.jpg)'
- en: where we see the linear approximation using the first derivative and the quadratic
    using the first and second derivatives of *f*(*x*). A first-order optimization
    algorithm uses the linear approximation, while a second-order one uses the quadratic
    approximation.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到线性近似使用了一阶导数，二次近似使用了一阶和二阶导数的泰勒展开。一个一阶优化算法使用线性近似，而一个二阶优化算法则使用二次近似。
- en: Moving from a scalar function of a scalar, *f*(*x*), to a scalar function of
    a vector, *f*(***x***), changes the first derivative to a gradient and the second
    derivative to the Hessian matrix,
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 从标量函数 *f*(*x*) 转变为向量函数 *f*(***x***)，会将一阶导数转化为梯度，二阶导数转化为赫西矩阵，
- en: '![Image](Images/215equ03.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/215equ03.jpg)'
- en: with ***H***[*f*](***a***) the Hessian matrix for *f*(***x***) evaluated at
    the point ***a***. The products’ order changes to make the dimensions work out
    properly, as we now have vectors and a matrix to deal with.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***H***[*f*](***a***) 是 *f*(***x***) 在点 ***a*** 处的赫西矩阵。乘积的顺序发生变化，以使得维度正确，因为我们现在要处理的是向量和矩阵。
- en: For example, if ***x*** has *n* elements, then *f*(***a***) is a scalar; the
    gradient at ***a*** is an *n*-element column vector multiplying (***x*** − ***a***)^⊤,
    an *n*-element row vector, producing a scalar; and the last term is 1 × *n* times
    *n* × *n* times *n* × 1, leading to 1 × *n* times *n* × 1, which is also a scalar.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 ***x*** 有 *n* 个元素，那么 *f*(***a***) 是一个标量；在 ***a*** 处的梯度是一个 *n* 元素的列向量，乘以
    (***x*** − ***a***)^⊤，一个 *n* 元素的行向量，得到一个标量；最后一项是 1 × *n* 次 *n* × *n* 次 *n* × 1，最终得到
    1 × *n* 次 *n* × 1，也就是一个标量。
- en: To use the Taylor series expansions for optimization, to find the minimum of
    *f*, we can use Newton’s method in much the same way used in [Equation 8.18](ch08.xhtml#ch08equ18).
    First, we rewrite [Equation 8.21](ch08.xhtml#ch08equ21) to change our viewpoint
    to one of a displacement (Δ*x*) from a current position (*x*). [Equation 8.21](ch08.xhtml#ch08equ21)
    then becomes
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用泰勒级数展开进行优化，找到 *f* 的最小值，我们可以使用牛顿法，方法与 [方程 8.18](ch08.xhtml#ch08equ18) 中使用的类似。首先，我们重写
    [方程 8.21](ch08.xhtml#ch08equ21)，将视角转变为从当前位置 (*x*) 发生位移 (Δ*x*)。然后，方程 [8.21](ch08.xhtml#ch08equ21)
    变为
- en: '![Image](Images/08equ22.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ22.jpg)'
- en: '[Equation 8.22](ch08.xhtml#ch08equ22) is a parabola in Δ*x*, and we’re using
    it as a stand-in for the more complex shape of *f* in the region of *x* + Δ*x*.
    To find the minimum of [Equation 8.22](ch08.xhtml#ch08equ22), we take the derivative
    and set it to zero, then solve for Δ*x*. The derivative gives'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 8.22](ch08.xhtml#ch08equ22) 是 Δ*x* 的抛物线，我们将其作为 *f* 在 *x* + Δ*x* 区域中更复杂形状的代替。为了找到
    [方程 8.22](ch08.xhtml#ch08equ22) 的最小值，我们对其求导并设其为零，然后解出 Δ*x*。导数给出'
- en: '![Image](Images/216equ01.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/216equ01.jpg)'
- en: which, if set to zero, leads to
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设为零，则得到
- en: '![Image](Images/08equ23.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ23.jpg)'
- en: '[Equation 8.23](ch08.xhtml#ch08equ23) tells us the offset from a current position,
    *x*, that would lead to the minimum of *f*(*x*) if *f*(*x*) were actually a parabola.
    In reality, *f*(*x*) isn’t a parabola, so the Δ*x* of [Equation 8.23](ch08.xhtml#ch08equ23)
    isn’t the actual offset to the minimum of *f*(*x*). However, since the Taylor
    series expansion used the actual slope, *f*′(*x*), and curvature, *f*′′(*x*),
    of *f*(*x*) at *x*, the offset of [Equation 8.23](ch08.xhtml#ch08equ23) is a better
    estimate of the actual minimum of *f*(*x*) than the linear approximation, assuming
    there is a minimum.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 8.23](ch08.xhtml#ch08equ23) 告诉我们当前位点 *x* 的偏移量，这个偏移量如果 *f*(*x*) 实际上是一个抛物线的话，会导致
    *f*(*x*) 的最小值。实际上，*f*(*x*) 并不是抛物线，因此 [方程 8.23](ch08.xhtml#ch08equ23) 中的 Δ*x* 不是
    *f*(*x*) 的最小值的实际偏移量。然而，由于泰勒级数展开使用了 *f*(*x*) 在 *x* 处的实际斜率 *f*′(*x*) 和曲率 *f*′′(*x*)，因此
    [方程 8.23](ch08.xhtml#ch08equ23) 的偏移量比线性近似更能准确估计 *f*(*x*) 的实际最小值，假设存在最小值。'
- en: 'If we go from *x* to *x* + Δ*x*, there’s no reason why we can’t then use [Equation
    8.23](ch08.xhtml#ch08equ23) a second time, calling the new position *x*. Thinking
    like this leads to an equation we can iterate:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从 *x* 移动到 *x* + Δ*x*，那么没有理由不能再次使用 [方程 8.23](ch08.xhtml#ch08equ23)，将新的位置称为
    *x*。这样思考会导致一个可以迭代的方程：
- en: '![Image](Images/08equ24.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ24.jpg)'
- en: for *x*[0], some initial starting point.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *x*[0]，某个初始起点。
- en: We can work out all of the above for scalar functions with vector arguments,
    *f*(***x***), which are the kind we most often encounter in deep learning via
    the loss function. [Equation 8.24](ch08.xhtml#ch08equ24) becomes
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对具有向量参数的标量函数 *f*(***x***) 进行上述所有推导，这种类型的函数通常是我们在深度学习中通过损失函数遇到的。[方程 8.24](ch08.xhtml#ch08equ24)
    变为
- en: '![Image](Images/216equ02.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/216equ02.jpg)'
- en: where the reciprocal of the second derivative becomes the inverse of the Hessian
    matrix evaluated at ***x**[n]*.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，二阶导数的倒数变为在 ***x**[n]* 处求得的海森矩阵的逆。
- en: Excellent! We have an algorithm we can use to rapidly find the minimum of a
    function like *f*(***x***). We saw above that Newton’s method converges quickly,
    so using it to minimize a loss function also should converge quickly, faster than
    gradient descent, which only considers the first derivative.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们有了一个算法，可以用来快速找到像 *f*(***x***) 这样的函数的最小值。我们在上面看到，牛顿法收敛得很快，因此使用它来最小化损失函数也应该很快收敛，甚至比仅考虑一阶导数的梯度下降法更快。
- en: If this is the case, why do we use gradient descent to train neural networks
    instead of Newton’s method?
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，为什么我们使用梯度下降来训练神经网络，而不是使用牛顿法呢？
- en: There are several reasons. First, we haven’t discussed issues arising from the
    Hessian’s applicability, issues related to the Hessian being a positive definite
    matrix. A symmetric matrix is positive definite if all its eigenvalues are positive.
    Near saddle points, the Hessian might not be positive definite, which can cause
    the update rule to move away from the minimum. As you might expect with a simple
    algorithm like Newton’s method, some variations try to address issues like this,
    but even if problems with the eigenvalues of the Hessian are addressed, the computational
    burden of using the Hessian for updating network parameters is what stops Newton’s
    algorithm in its tracks.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因。首先，我们还没有讨论海森矩阵适用性的问题，即海森矩阵是正定矩阵的相关问题。一个对称矩阵是正定的，当且仅当其所有特征值都是正的。在鞍点附近，海森矩阵可能不是正定的，这可能导致更新规则远离最小值。正如你可能预期的那样，像牛顿法这样简单的算法，一些变种试图解决这类问题，但即使解决了海森矩阵特征值的问题，使用海森矩阵来更新网络参数的计算负担，也正是阻止牛顿法前进的原因。
- en: Every time the network’s weights and biases are updated, the Hessian changes,
    requiring it and its inverse to be calculated again. Think of the number of minibatches
    used during network training. For even one minibatch, there are *k* parameters
    in the network, where *k* is easily in the millions to even billions. The Hessian
    is a *k* × *k* symmetric and positive definite matrix. Inverting the Hessian typically
    uses Cholesky decomposition, which is more efficient than other methods but is
    still an 𝒪(*k*³) algorithm. The *big-O* notation indicates that the algorithm’s
    resource use scales as the cube of the size of the matrix in time, memory, or
    both. This means doubling the number of parameters in the network increases the
    computational time to invert the Hessian by a factor of 2³ = 8 while tripling
    the number of parameters requires some 3³ = 27 times as much effort, and quadrupling
    some 4³ = 64 times as much. And this is to say nothing about storing the *k*²
    elements of the Hessian matrix, all floating-point values.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 每次网络的权重和偏置更新时，海森矩阵都会发生变化，需要重新计算它及其逆矩阵。想想网络训练过程中使用的小批量数。即使只有一个小批量，网络中也有 *k* 个参数，其中
    *k* 轻松达到百万甚至数十亿级别。海森矩阵是一个 *k* × *k* 的对称正定矩阵。逆矩阵的计算通常使用 Cholesky 分解，这比其他方法更高效，但仍然是一个
    𝒪(*k*³) 算法。*大-O* 符号表示该算法的资源使用随着矩阵大小的立方而变化，无论是在时间、内存还是两者之间。这意味着，网络中参数数量翻倍时，逆海森矩阵的计算时间增加
    2³ = 8 倍，而三倍参数需要大约 3³ = 27 倍的计算量，四倍参数则需要大约 4³ = 64 倍的计算量。而且这还不包括存储海森矩阵中 *k*² 个浮点数值。
- en: The computation necessary to use Newton’s method with even modest deep networks
    is staggering. Gradient-based, first-order optimization methods are about all
    we can use for training neural networks.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 使用牛顿法进行计算，即使对于简单的深度网络，所需的计算量也是惊人的。基于梯度的、一级优化方法几乎是我们训练神经网络时唯一能用的工具。
- en: '**NOTE**'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*This statement is perhaps a bit premature. Recent work in the area of* neuroevolution
    *has demonstrated that evolutionary algorithms can successfully train deep models.
    My experimentation with swarm optimization techniques and neural networks lends
    credence to this approach as well.*'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个说法或许有些过早。近期关于* 神经进化 *的研究表明，进化算法能够成功地训练深度模型。我在群体优化技术和神经网络方面的实验也为这种方法提供了支持。*'
- en: That first-order methods work as well as they do seems, for now, to be a very
    happy accident.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一级方法之所以如此有效，现在看来，似乎是一个非常幸运的巧合。
- en: Some Examples of Matrix Calculus Derivatives
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些矩阵微积分导数的例子
- en: We conclude the chapter with some examples similar to the kinds of derivatives
    we commonly find in deep learning.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一些类似于在深度学习中常见的导数例子来结束本章。
- en: Derivative of Element-Wise Operations
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逐元素操作的导数
- en: Let’s begin with the derivative of element-wise operations, which includes things
    like adding two vectors together. Consider
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从逐元素操作的导数开始，这包括像将两个向量相加之类的操作。考虑
- en: '![Image](Images/217equ01.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/217equ01.jpg)'
- en: which is the straightforward addition of two vectors, element by element. What
    does ∂***f***/∂***a***, the Jacobian of ***f***, look like? From the definition,
    we have
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是两个向量逐元素相加的简单操作。∂***f***/∂***a***，即 ***f*** 的雅可比矩阵，看起来像什么？根据定义，我们得到
- en: '![Image](Images/218equ01.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/218equ01.jpg)'
- en: but *f*[0] only depends on *a*[0], while *f*[1] depends on *a*[1], and so on.
    Therefore, all derivatives ∂*f[i]*/∂*a[j]* for *i* ≠ *j* are zero. This removes
    all the off-diagonal elements of the matrix, leaving
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 *f*[0] 仅依赖于 *a*[0]，而 *f*[1] 依赖于 *a*[1]，以此类推。因此，所有 ∂*f[i]*/∂*a[j]* 当 *i* ≠
    *j* 时都为零。这去除了矩阵的所有非对角元素，留下
- en: '![Image](Images/218equ02.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/218equ02.jpg)'
- en: since ∂*f*[*i*]/∂*a*[*i*] = 1 for all *i*. Similarly, ∂***f***/∂***b*** = ***I***
    as well. Also, if we change from addition to subtraction, ∂***f***/∂***a*** =
    ***I***, but ∂***f***/∂***b*** = −***I***.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ∂*f*[*i*]/∂*a*[*i*] = 1 对所有 *i* 都成立。同样，∂***f***/∂***b*** = ***I*** 也是如此。而且，如果我们将加法改为减法，∂***f***/∂***a***
    = ***I***，但是 ∂***f***/∂***b*** = −***I***。
- en: If the operator is element-wise multiplication of ***a*** and ***b***, ***f***
    = ***a*** ⊗ ***b***, then we get the following, where the diag(***x***) notation
    means the *n* elements of vector ***x*** along the diagonal of an *n* × *n* matrix
    that is zero elsewhere.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作符是 ***a*** 和 ***b*** 的逐元素乘法，***f*** = ***a*** ⊗ ***b***，那么我们得到以下结果，其中 diag(***x***)
    表示向量 ***x*** 的 *n* 个元素沿对角线排列在一个 *n* × *n* 的矩阵中，其余元素为零。
- en: '![Image](Images/218equ03.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/218equ03.jpg)'
- en: Derivative of the Activation Function
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活函数的导数
- en: Let’s find the derivative of the weights and bias value for a single node of
    a hidden layer in a feedforward network. Recall, the inputs to the node are the
    outputs of the previous layer, ***x***, multiplied term by term by the weights,
    ***w***, and summed along with the bias value, *b*, a scalar. The result, a scalar,
    is passed to the activation function to produce the output value for the node.
    Here, we’re using the *rectified linear unit (ReLU)* which returns its argument
    if the argument is positive. If the argument is negative, ReLU returns zero. We
    can write this process as
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出前馈网络中单个节点的权重和偏置值的导数。回想一下，节点的输入是上一层的输出***x***，与权重***w***逐项相乘并与偏置值*b*（标量）相加。结果是一个标量，传递到激活函数中，生成节点的输出值。在这里，我们使用的是*修正线性单元（ReLU）*，当参数为正时，它返回参数值；当参数为负时，ReLU返回零。我们可以将这个过程写为
- en: '![Image](Images/08equ25.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ25.jpg)'
- en: In order to implement backpropagation, we need the derivatives of [Equation
    8.25](ch08.xhtml#ch08equ25) with respect to ***w*** and *b*. Let’s see how to
    find them.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现反向传播，我们需要[方程 8.25](ch08.xhtml#ch08equ25)关于***w***和*b*的导数。让我们来看一下如何找到它们。
- en: We begin by considering the pieces of [Equation 8.25](ch08.xhtml#ch08equ25).
    For example, from [Equation 8.9](ch08.xhtml#ch08equ09), we know the derivative
    of the dot product with respect to ***w*** is
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑[方程 8.25](ch08.xhtml#ch08equ25)的各个部分。例如，来自[方程 8.9](ch08.xhtml#ch08equ09)，我们知道点积关于***w***的导数是
- en: '![Image](Images/219equ01.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ01.jpg)'
- en: where we have taken advantage of the fact that the dot product is commutative,
    ***w*** • ***x*** = ***x*** • ***w***. Also, since *b* does not depend on ***w***,
    we have
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们利用了点积是交换律的事实，***w*** • ***x*** = ***x*** • ***w***。此外，由于*b*不依赖于***w***，我们有
- en: '![Image](Images/08equ26.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ26.jpg)'
- en: What about the derivative of ReLU? By definition,
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 那么ReLU的导数呢？根据定义，
- en: '![Image](Images/219equ02.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ02.jpg)'
- en: implying that
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 暗示着
- en: '![Image](Images/08equ27.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ27.jpg)'
- en: since ∂*z*/∂*z* = 1.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 由于∂*z*/∂*z* = 1。
- en: To find the derivatives of [Equation 8.25](ch08.xhtml#ch08equ25) with respect
    to ***w*** and *b*, we need the chain rule and the results above. Let’s start
    with ***w***. The chain rule tells us how
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到[方程 8.25](ch08.xhtml#ch08equ25)关于***w***和*b*的导数，我们需要链式法则和上述结果。让我们从***w***开始。链式法则告诉我们如何
- en: '![Image](Images/219equ03.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ03.jpg)'
- en: with *z* = ***w*** • ***x*** + *b* and *y* = ReLU(*z*).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*z* = ***w*** • ***x*** + *b* 和 *y* = ReLU(*z*)。
- en: We know ∂*y*/∂*z*; it’s the two cases above for the ReLU, [Equation 8.27](ch08.xhtml#ch08equ27).
    So now we have
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道∂*y*/∂*z*；它是上述ReLU的两种情况，[方程 8.27](ch08.xhtml#ch08equ27)。因此，现在我们有
- en: '![Image](Images/219equ04.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ04.jpg)'
- en: and we know ∂*z*/∂***w*** = ***x***^⊤; it’s [Equation 8.26](ch08.xhtml#ch08equ26).
    Therefore, our final result is
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道∂*z*/∂***w*** = ***x***^⊤；它是[方程 8.26](ch08.xhtml#ch08equ26)。因此，我们的最终结果是
- en: '![Image](Images/220equ01.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/220equ01.jpg)'
- en: where we’ve replaced *z* with ***w*** • ***x*** + *b*.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们将*z*替换为***w*** • ***x*** + *b*。
- en: We follow much the same procedure to find ∂*y*/∂*b*, as
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照类似的过程来找出∂*y*/∂*b*，如
- en: '![Image](Images/220equ02.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/220equ02.jpg)'
- en: but ∂*y*/∂*z* is 0 or 1, depending on the *z*’s sign. Likewise, ∂*z*/∂*b* =
    1, which leads to
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 但∂*y*/∂*z*为0或1，具体取决于*z*的符号。同样，∂*z*/∂*b* = 1，这导致
- en: '![Image](Images/220equ03.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/220equ03.jpg)'
- en: Summary
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this dense chapter, we learned about matrix calculus, including working with
    derivatives of functions involving vectors and matrices. We worked through the
    definitions and discussed some identities. We then introduced the Jacobian and
    Hessian matrices as analogs for first and second derivatives and learned how to
    use them in optimization problems. Training a deep neural network is, fundamentally,
    an optimization problem, so the potential utility of the Jacobian and Hessian
    is clear, even if the latter can’t be easily used for large neural networks. We
    ended the chapter with some examples for derivatives of expressions found in deep
    learning.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，我们学习了矩阵微积分，包括处理涉及向量和矩阵的函数的导数。我们逐步讲解了定义并讨论了一些恒等式。然后我们介绍了雅可比矩阵和海森矩阵，作为一阶和二阶导数的类比，并学习了如何在优化问题中使用它们。训练深度神经网络本质上是一个优化问题，因此雅可比矩阵和海森矩阵的潜在用途是显而易见的，尽管后者在大型神经网络中无法轻易使用。我们以一些深度学习中出现的表达式的导数例子结束了这一章节。
- en: This concludes the general mathematics portion of the book. We’ll now turn our
    attention to using what we’ve learned to understand the workings of deep neural
    networks. Let’s begin with a discussion of how data flows through a neural network
    model.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了本书的数学基础部分。接下来，我们将把注意力转向如何运用所学的知识来理解深度神经网络的工作原理。让我们从讨论数据如何在神经网络模型中流动开始。
