- en: '**8'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**8'
- en: MATRIX CALCULUS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵微积分**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: '[Chapter 7](ch07.xhtml#ch07) introduced us to differential calculus. In this
    chapter, we’ll discuss *matrix calculus*, which extends differentiation to functions
    involving vectors and matrices.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 7 章](ch07.xhtml#ch07)向我们介绍了微分学。在本章中，我们将讨论*矩阵微积分*，它将微分扩展到涉及向量和矩阵的函数。'
- en: Deep learning works extensively with vectors and matrices, so it makes sense
    to develop a notation and approach to representing derivatives involving these
    objects. That’s what matrix calculus gives us. We saw a hint of this at the end
    of [Chapter 7](ch07.xhtml#ch07), when we introduced the gradient to represent
    the derivative of a scalar function of a vector—a function that accepts a vector
    argument and returns a scalar, *f*(***x***).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习广泛使用向量和矩阵，因此开发一种表示涉及这些对象的导数的符号和方法是有意义的。这就是矩阵微积分所提供的东西。在[第 7 章](ch07.xhtml#ch07)的结尾，我们引入了梯度来表示标量函数关于向量的导数——这是一个接受向量参数并返回标量的函数，*f*(***x***)。
- en: We’ll start with the table of matrix calculus derivatives and their definitions.
    Next, we’ll examine some identities involving matrix derivatives. Mathematicians
    love identities; however, to preserve our sanity, we’ll only consider a handful.
    Some special matrices come out of matrix calculus, namely the Jacobian and Hessian.
    You’ll run into both of these matrices during your sojourn through deep learning,
    so we’ll consider them next in the context of optimization. Recall that training
    a neural network is, fundamentally, an optimization problem, so understanding
    what these special matrices represent and how we use them is especially important.
    We’ll close the chapter with some examples of matrix derivatives.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从矩阵微积分导数及其定义的表格开始。接下来，我们将研究一些涉及矩阵导数的恒等式。数学家喜欢恒等式；然而，为了保持理智，我们只考虑少数几个。在矩阵微积分中，出现了一些特殊矩阵，即雅可比矩阵和海森矩阵。在你深入学习深度学习的过程中，你会遇到这两种矩阵，因此我们将在优化的背景下讨论它们。回想一下，训练神经网络本质上是一个优化问题，因此理解这些特殊矩阵的含义以及我们如何使用它们尤其重要。我们将以一些矩阵导数的例子结束本章。
- en: The Formulas
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公式
- en: '[Table 8-1](ch08.xhtml#ch08tab01) summarizes the matrix calculus derivatives
    we’ll explore in this chapter. These are the ones commonly used in practice.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)总结了我们将在本章中探讨的矩阵微积分导数。这些是实践中常用的导数。'
- en: '**Table 8-1:** Matrix Calculus Derivatives'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 8-1：** 矩阵微积分导数'
- en: '|  | **Scalar** | **Vector** | **Matrix** |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|  | **标量** | **向量** | **矩阵** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Scalar** | ∂*f*/∂*x* | ∂***f***/∂*x* | ∂***F***/∂*x* |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **标量** | ∂*f*/∂*x* | ∂***f***/∂*x* | ∂***F***/∂*x* |'
- en: '| **Vector** | ∂*f*/∂***x*** | ∂*f*/∂***x*** | — |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **向量** | ∂*f*/∂***x*** | ∂*f*/∂***x*** | — |'
- en: '| **Matrix** | ∂***f***/∂***X*** | — | — |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **矩阵** | ∂***f***/∂***X*** | — | — |'
- en: 'The columns of [Table 8-1](ch08.xhtml#ch08tab01) represent the function, meaning
    the return value. Notice we use three versions of the letter f: regular, bold,
    and capital. We use *f* if the return value is a scalar, ***f*** if a vector,
    and ***F*** if a matrix. The rows of [Table 8-1](ch08.xhtml#ch08tab01) are the
    variables the derivatives are calculated with respect to. The same notation applies:
    *x* is a scalar, ***x*** is a vector, and ***X*** is a matrix.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)的列表示函数，意味着返回值。请注意，我们使用了三种版本的字母f：普通、粗体和大写。我们使用*f*表示返回值为标量，***f***表示返回值为向量，***F***表示返回值为矩阵。[表
    8-1](ch08.xhtml#ch08tab01)的行表示导数是相对于哪些变量计算的。相同的符号规则适用：*x*是标量，***x***是向量，***X***是矩阵。'
- en: '[Table 8-1](ch08.xhtml#ch08tab01) defines six derivatives, but there are nine
    cells in the table. While possible to define, the remaining derivatives are not
    standardized or used often enough to make covering them worthwhile. That’s good
    for us, as the six are enough of a challenge for our mathematical brains.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)定义了六个导数，但表格中有九个单元格。虽然可以定义其余的导数，但它们并未标准化，也不常用，因此不值得覆盖它们。对我们来说，这是个好事，因为这六个已经足够挑战我们的数学大脑了。'
- en: The first derivative in [Table 8-1](ch08.xhtml#ch08tab01), the one in the upper
    left, is the normal derivative of [Chapter 7](ch07.xhtml#ch07), a function producing
    a scalar with respect to a scalar. (Refer to [Chapter 7](ch07.xhtml#ch07) for
    everything you need to know about standard differentiation.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8-1](ch08.xhtml#ch08tab01)中的第一个导数，即左上角的那个，是[第 7 章](ch07.xhtml#ch07)中的标准导数，表示关于标量的标量函数的导数。（参阅[第
    7 章](ch07.xhtml#ch07)了解所有关于标准微分的内容。）'
- en: We’ll cover the remaining five derivatives in the sections below. We define
    each one in terms of scalar derivatives. We’ll first show the definition and then
    explain what the notation represents. The definition will help you build a model
    in your head of what the derivative is. I suspect that by the end of this section
    you’ll be predicting the definitions in advance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面的章节中介绍其余五个导数。我们将以标量导数的形式定义每一个。我们将首先展示定义，然后解释符号的含义。定义将帮助你在脑海中建立起对导数的模型。我猜测在本节结束时，你将能够预见到这些定义。
- en: Before we start, however, there is a complication we should discuss. Matrix
    calculus is notation heavy, but there’s no universal agreement on the notation.
    We’ve seen this before with the many ways to indicate differentiation. For matrix
    calculus, the two approaches are *numerator layout* or *denominator layout*. Specific
    disciplines seem to favor one over the other, though exceptions are almost the
    norm, as is mixing notations. For deep learning, a nonscientific perusal on my
    part seems to indicate a slight preference for numerator layout, so that’s what
    we’ll use here. Just be aware that there are two forms out there. One is typically
    the transpose of the other.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始之前，有一个问题需要讨论。矩阵微积分的符号较为复杂，而且在符号的使用上没有统一的标准。我们以前已经看到过，表示微分的方法有很多种。对于矩阵微积分，有两种方法——*分子布局*和*分母布局*。不同学科似乎偏好其中一种，尽管例外几乎成了常态，符号混合也很常见。就深度学习而言，我个人的一些非正式观察显示，*分子布局*稍微更为流行，所以我们在这里将采用这种布局。只是要注意，市面上有两种符号体系，其中一种通常是另一种的转置。
- en: A Vector Function by a Scalar Argument
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 由标量参数定义的向量函数
- en: 'A vector function accepting a scalar argument is our first derivative; see
    [Table 8-1](ch08.xhtml#ch08tab01), second column of the first row. We write such
    a function as ***f***(*x*) to indicate a scalar argument, *x*, and a vector output,
    ***f***. Functions like ***f*** take a scalar and map it to a multidimensional
    vector:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接受标量参数的向量函数是我们的第一个导数；参见 [表 8-1](ch08.xhtml#ch08tab01)，第一行第二列。我们将这样的函数写作 ***f***(*x*)，以表示标量参数
    *x* 和向量输出 ***f***。像 ***f*** 这样的函数接受标量并将其映射到多维向量：
- en: '***f*** : ℝ → ℝ^(*m*)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***f*** : ℝ → ℝ^(*m*)'
- en: Here, *m* is the number of elements in the output vector. Functions like ***f***
    are known as *vector-valued functions* with scalar arguments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m* 是输出向量中元素的数量。像 ***f*** 这样的函数被称为具有标量参数的*向量值函数*。
- en: A parametric curve in 3D space is an excellent example of such a function. Those
    functions are often written as
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 三维空间中的参数曲线是此类函数的一个优秀例子。这些函数通常写作
- en: '![Image](Images/195equ01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/195equ01.jpg)'
- en: where ![Image](Images/195equ02b.jpg), ![Image](Images/195equ02a.jpg), and ![Image](Images/195equ03.jpg)
    are unit vectors in the x, y, and z directions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![Image](Images/195equ02b.jpg), ![Image](Images/195equ02a.jpg), 和 ![Image](Images/195equ03.jpg)
    是 x、y 和 z 方向上的单位向量。
- en: '[Figure 8-1](ch08.xhtml#ch08fig01) shows a plot of a 3D parametric curve,'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-1](ch08.xhtml#ch08fig01) 显示了一个三维参数曲线的图像，'
- en: '![Image](Images/08equ01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ01.jpg)'
- en: where, as *t* varies, the three axis values also vary to trace out the spiral.
    Here, each value of *t* specifies a single point in 3D space.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，随着 *t* 的变化，三个轴的值也会变化，从而描绘出螺旋曲线。每个 *t* 值指定三维空间中的一个点。
- en: '![image](Images/08fig01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/08fig01.jpg)'
- en: '*Figure 8-1: A 3D parametric curve*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8-1：三维参数曲线*'
- en: In matrix calculus notation, we don’t write ***f*** as shown in [Equation 8.1](ch08.xhtml#ch08equ01).
    Instead, we write ***f*** as a column vector of the functions,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵微积分符号中，我们不会像在 [方程 8.1](ch08.xhtml#ch08equ01) 中那样写 ***f***。相反，我们将 ***f***
    写作函数的列向量，
- en: '![Image](Images/195equ04.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/195equ04.jpg)'
- en: and, in general,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，
- en: '![Image](Images/196equ01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ01.jpg)'
- en: for ***f*** with *n* elements.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 *n* 个元素的 ***f***。
- en: 'The derivative of ***f***(*x*) is known as the *tangent vector*. What does
    the derivative look like? Since ***f*** is a vector, we might expect the derivative
    of ***f*** to be the derivatives of the functions representing each element of
    ***f***, and we’d be right:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '***f***(*x*) 的导数被称为*切向量*。导数是什么样的？由于 ***f*** 是一个向量，我们可能期望 ***f*** 的导数是代表每个元素的函数的导数，事实也的确如此：'
- en: '![Image](Images/196equ02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ02.jpg)'
- en: 'Let’s look at a simple example. First, we will define ***f***(*x*), and then
    the derivative:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。首先，我们将定义 ***f***(*x*)，然后是导数：
- en: '![Image](Images/196equ03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ03.jpg)'
- en: Here, each element of ∂***f***/∂*x* is the derivative of the corresponding function
    in ***f***.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，∂***f***/∂*x* 的每个元素是 ***f*** 中相应函数的导数。
- en: A Scalar Function by a Vector Argument
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过向量参数的标量函数
- en: 'In [Chapter 7](ch07.xhtml#ch07), we learned that a function accepting a vector
    input but returning a scalar is a scalar field:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第七章](ch07.xhtml#ch07)中，我们学到了一个接受向量输入但返回标量的函数是标量场：
- en: '*f* : ℝ^(*m*) → ℝ'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* : ℝ^(*m*) → ℝ'
- en: We also learned that the derivative of this function is the gradient. In matrix
    calculus notation, we write ∂*f*/∂***x*** for *f*(***x***) as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到了该函数的导数是梯度。在矩阵微积分符号中，我们写作 ∂*f*/∂***x***，表示 *f*(***x***)。
- en: '![Image](Images/196equ04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/196equ04.jpg)'
- en: where ***x*** = [*x*[0] *x*[1] ... *x*[*m*–1]]^⊤ is a vector of variables, and
    *f* is a function of those variables.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***x*** = [*x*[0] *x*[1] ... *x*[*m*–1]]^⊤ 是一个变量向量，*f* 是这些变量的函数。
- en: Notice, because we decided to use the numerator layout approach, ∂*f*/∂***x***
    is written as a *row* vector. So, to be true to our notation, we have to write
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们决定使用分子布局方法，∂*f*/∂***x*** 被写作一个 *行* 向量。因此，为了符合我们的符号记法，我们必须写作
- en: '![Image](Images/197equ01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/197equ01.jpg)'
- en: turning the row vector into a column vector to match the gradient. Remember
    that ▽ is the symbol for gradient; we saw an example of the gradient in Chapter
    7.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将行向量转化为列向量，以与梯度匹配。记住，▽是梯度的符号；我们在第七章中看到过梯度的例子。
- en: A Vector Function by a Vector
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过向量的向量函数
- en: If the derivative of a vector-valued function by a scalar produces a column
    vector, and the derivative of a scalar function by a vector results in a row vector,
    does the derivative of a vector-valued function by a vector produce a matrix?
    The answer is yes. In this case, we’re contemplating ∂***f***/∂***x*** for ***f***(***x***),
    a function that accepts a vector input and returns a vector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过标量对向量值函数求导得到列向量，而通过向量对标量函数求导得到行向量，那么通过向量对向量值函数求导是否得到矩阵？答案是肯定的。在这种情况下，我们考虑的是
    ∂***f***/∂***x***，这是一个接受向量输入并返回向量的函数。
- en: 'The numerator layout convention gave us a column vector for the derivative
    of ***f***(*x*), implying we need a row for each of the functions in ***f***.
    Similarly, the derivative of *f*(***x***) produced a row vector. Therefore, merging
    the two gives us the derivative of ***f***(***x***):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分子布局约定给了我们 ***f***(*x*) 导数的列向量，这意味着我们需要为 ***f*** 中的每个函数准备一行。类似地，*f*(***x***)
    的导数生成了一个行向量。因此，合并这两者就得到了 ***f***(***x***) 的导数：
- en: '![Image](Images/08equ02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ02.jpg)'
- en: 'This is for a function, ***f***, returning an *n*-element vector and accepting
    an *m-*element vector, ***x***, as its argument:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对于一个函数 ***f*** 的情况，返回一个 *n* 元素的向量并接受一个 *m* 元素的向量 ***x*** 作为其自变量：
- en: '*f* : ℝ^(*m*) → ℝ^(*n*)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* : ℝ^(*m*) → ℝ^(*n*)'
- en: Each row of ***f*** is a scalar function of ***x***, for example, *f*[0](***x***).
    Therefore, we can write [Equation 8.2](ch08.xhtml#ch08equ02) as
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '***f*** 的每一行都是一个 ***x*** 的标量函数，例如，*f*[0](***x***)。因此，我们可以将[方程 8.2](ch08.xhtml#ch08equ02)写作'
- en: '![Image](Images/08equ03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ03.jpg)'
- en: This gives us the matrix as a collection of gradients, one for each scalar function
    in ***f***. We’ll return to this matrix later in the chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了一个矩阵，作为每个标量函数在 ***f*** 中梯度的集合。我们将在本章稍后讨论这个矩阵。
- en: A Matrix Function by a Scalar
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过标量的矩阵函数
- en: 'If ***f***(*x*) is a function accepting a scalar argument but returning a vector,
    then we’d be correct in assuming ***F***(*x*) can be thought of as a function
    accepting a scalar argument but returning a matrix:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ***f***(*x*) 是一个接受标量参数但返回向量的函数，那么我们可以正确地假设 ***F***(*x*) 可以看作是一个接受标量参数并返回矩阵的函数：
- en: '***F*** : ℝ → ℝ^(*n*×*m*)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***F*** : ℝ → ℝ^(*n*×*m*)'
- en: 'For example, assume ***F*** is an *n* × *m* matrix of scalar functions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 ***F*** 是一个 *n* × *m* 的标量函数矩阵：
- en: '![Image](Images/198equ01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/198equ01.jpg)'
- en: 'The derivative with respect to the argument, *x*, is straightforward:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自变量 *x* 的导数，计算过程很简单：
- en: '![Image](Images/198equ02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/198equ02.jpg)'
- en: As we saw above, the derivative of a vector-valued function by a scalar is called
    the tangent vector. By analogy, then, the derivative of a matrix-valued function
    by a scalar is the *tangent matrix*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们上面所看到的，通过标量对向量值函数求导，得到的是切向量。类比地，通过标量对矩阵值函数求导，得到的是 *切向矩阵*。
- en: A Scalar Function by a Matrix
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过矩阵的标量函数
- en: 'Now let’s consider *f*(***X***), a function accepting a matrix and returning
    a scalar:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑 *f*(***X***)，一个接受矩阵并返回标量的函数：
- en: '*f* : ℝ^(*n*×*m*) → ℝ'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* : ℝ^(*n*×*m*) → ℝ'
- en: We’d be correct in thinking that the derivative of *f* with respect to the matrix,
    ***X***, is itself a matrix. However, to be true to our numerator layout convention,
    the resulting matrix is not arranged like ***X***, but instead like ***X***^⊤,
    the transpose of ***X***.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为*F*关于矩阵***X***的导数本身就是一个矩阵，这样的理解是正确的。然而，为了符合我们的分子排列惯例，结果矩阵并不是像***X***那样排列，而是像***X***^⊤，即***X***的转置。
- en: Why use the transpose of ***X*** instead of ***X*** itself? To answer the question,
    we need to look back to how we defined ∂*f*/∂***x***. There, even though ***x***
    is a column vector, according to standard convention, we said the derivative is
    a *row* vector. We used ***x***^⊤ as the ordering. Therefore, to be consistent,
    we need to arrange ∂*f*/∂***X*** by the transpose of ***X*** and change the columns
    of ***X*** into rows in the derivative. As a result, we have the following definition.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用***X***的转置而不是***X***本身？为了回答这个问题，我们需要回顾一下我们如何定义∂*f*/∂***x***。即使***x***是列向量，根据标准惯例，我们表示导数是一个*行*向量。我们使用了***x***^⊤作为排列方式。因此，为了保持一致性，我们需要将∂*f*/∂***X***按照***X***的转置排列，并将***X***的列转化为导数中的行。结果，我们得到了以下定义。
- en: '![Image](Images/08equ04.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ04.jpg)'
- en: This is an *m* × *n* output matrix for the *n* × *m* input matrix, ***X***.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个*m* × *n*的输出矩阵，针对*m* × *n*的输入矩阵***X***。
- en: '[Equation 8.4](ch08.xhtml#ch08equ04) defines the *gradient matrix*, which,
    for matrices, plays a role similar to that of the gradient, ▽*f*(**x**). [Equation
    8.4](ch08.xhtml#ch08equ04) also completes our collection of matrix calculus derivatives.
    Let’s move on to consider some matrix derivative identities.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式8.4](ch08.xhtml#ch08equ04)定义了*梯度矩阵*，对于矩阵，它起着类似于梯度▽*f*(**x**)的作用。[方程式8.4](ch08.xhtml#ch08equ04)还完整地列出了我们的矩阵微积分导数。接下来，我们将考虑一些矩阵导数恒等式。'
- en: The Identities
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 等式
- en: Matrix calculus involves scalars, vectors, matrices, and functions thereof,
    which themselves return scalars, vectors, or matrices, implying many identities
    and relationships exist. However, here we’ll concentrate on basic identities showing
    the relationship between matrix calculus and the differential calculus of [Chapter
    7](ch07.xhtml#ch07).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵微积分涉及标量、向量、矩阵及其函数，这些函数自身返回标量、向量或矩阵，意味着存在许多恒等式和关系。然而，在这里，我们将集中于展示矩阵微积分与[第7章](ch07.xhtml#ch07)的微分微积分之间关系的基本恒等式。
- en: Each of the following subsections presents identities related to the specific
    type of derivative indicated. The identities cover fundamental relationships and,
    when applicable, the chain rule. In all cases, the results follow the numerator
    layout scheme we’ve used throughout the chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下每个子节展示了与所指示的特定类型的导数相关的恒等式。这些恒等式涵盖了基本关系，并在适用时涉及链式法则。在所有情况下，结果都遵循我们在本章中使用的分子排列方案。
- en: A Scalar Function by a Vector
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标量函数与向量
- en: We begin with identities related to a scalar function with a vector input. If
    not otherwise specified, *f* and *g* are functions of a vector, ***x***, and return
    a scalar. A constant vector that doesn’t depend on ***x*** is given as ***a***,
    and *a* denotes a scalar constant.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从与标量函数和向量输入相关的恒等式开始。如果没有特别说明，*f*和*g*是关于向量***x***的函数，并返回一个标量。一个不依赖于***x***的常量向量表示为***a***，而*a*表示一个标量常数。
- en: 'The basic rules are intuitive:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基本规则是直观的：
- en: '![Image](Images/08equ05.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ05.jpg)'
- en: and
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: '![Image](Images/08equ06.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ06.jpg)'
- en: These show that multiplication by a scalar constant acts as it did in [Chapter
    7](ch07.xhtml#ch07), as does the linearity of the partial derivative.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表明，乘以标量常数的运算与在[第7章](ch07.xhtml#ch07)中的表现一致，偏导数的线性性质也同样如此。
- en: 'The product rule also works as we expect:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 乘积法则也像我们预期的那样有效：
- en: '![Image](Images/08equ07.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ07.jpg)'
- en: Let’s pause here and remind ourselves of the inputs and outputs for the equations
    above. We know that the derivative of a scalar function by a vector argument is
    a row vector in our notation. So, [Equation 8.5](ch08.xhtml#ch08equ05) returns
    a row vector multiplied by a scalar—each element of the derivative is multiplied
    by *a*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里稍作停顿，提醒自己上面方程的输入和输出。我们知道，标量函数关于向量变量的导数在我们的符号中是一个行向量。因此，[方程式8.5](ch08.xhtml#ch08equ05)返回的是一个行向量乘以一个标量——导数的每个元素都乘以*a*。
- en: As differentiation is a linear operator, it distributes over addition, so [Equation
    8.6](ch08.xhtml#ch08equ06) delivers two terms, each a row vector generated by
    the respective derivative.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微分是一个线性算子，它对加法分配，因此[方程式8.6](ch08.xhtml#ch08equ06)会产生两个项，每个项都是由各自的导数生成的行向量。
- en: For [Equation 8.7](ch08.xhtml#ch08equ07), the product rule, the result again
    includes two terms. In each case, the derivative returns a row vector, which is
    multiplied by a scalar function value, either *f*(***x***) or *g*(***x***). Therefore,
    the output of [Equation 8.7](ch08.xhtml#ch08equ07) is also a row vector.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 [方程 8.7](ch08.xhtml#ch08equ07)，乘积法则，结果再次包含两个项。在每种情况下，导数返回一个行向量，该向量乘以一个标量函数值，可能是
    *f*(***x***) 或 *g*(***x***)。因此，[方程 8.7](ch08.xhtml#ch08equ07) 的输出也是一个行向量。
- en: The scalar-by-vector chain rule becomes
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 标量对向量的链式法则变为
- en: '![Image](Images/08equ08.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ08.jpg)'
- en: where *f*(*g*) returns a scalar and accepts a scalar argument, while *g*(***x***)
    returns a scalar and accepts a vector argument. The end result is a row vector.
    Let’s work through a complete example to demonstrate.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*g*) 返回一个标量并接受一个标量参数，而 *g*(***x***) 返回一个标量并接受一个向量参数。最终结果是一个行向量。让我们通过一个完整的例子来演示。'
- en: We have a vector, ***x*** = [*x*[0], *x*[1], *x*[2]]^⊤; a function of that vector
    written in component form, *g*(***x***) = *x*[0] + *x*[1]*x*[2]; and a function
    of *g*, *f*(*g*) = *g*². According to [Equation 8.8](ch08.xhtml#ch08equ08), the
    derivative of *f* with respect to ***x*** is
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个向量，***x*** = [*x*[0], *x*[1], *x*[2]]^⊤；该向量的一个分量形式的函数，*g*(***x***) = *x*[0]
    + *x*[1]*x*[2]；以及 *g* 的一个函数，*f*(*g*) = *g*²。根据 [方程 8.8](ch08.xhtml#ch08equ08)，*f*
    对 ***x*** 的导数是
- en: '![Image](Images/200equ01.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/200equ01.jpg)'
- en: To check our result, we can work through from *g*(***x***) = *x*[0] + *x*[1]*x*[2]
    and *f*(*g*) = *g*² to find *f*(***x***) directly via substitution. Doing this
    gives us
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的结果，我们可以从 *g*(***x***) = *x*[0] + *x*[1]*x*[2] 和 *f*(*g*) = *g*² 开始，通过代入直接求出
    *f*(***x***)。这样做得出的结果是
- en: '![Image](Images/201equ01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/201equ01.jpg)'
- en: from which we get
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们得到
- en: '![Image](Images/201equ02.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/201equ02.jpg)'
- en: which matches the result we found using the chain rule. Of course, in this simple
    example, it was easier to work through the substitution before taking the derivative,
    but we proved our case all the same.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们使用链式法则得到的结果相匹配。当然，在这个简单的例子中，先进行代入再求导更容易，但我们一样证明了我们的结论。
- en: We’re not entirely through with scalar-by-vector identities, however. The dot
    product takes two vectors and produces a scalar, so it fits with the functional
    form we’re working with, even though the arguments to the dot product are vectors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完全解决标量对向量的恒等式。点积操作接受两个向量并产生一个标量，因此它适配我们正在处理的函数形式，尽管点积的参数是向量。
- en: 'For example, consider this result:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑这个结果：
- en: '![Image](Images/08equ09.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ09.jpg)'
- en: Here, we have the derivative of the dot product between ***x*** and a vector
    ***a*** that does not depend on ***x***.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有 ***x*** 和一个与 ***x*** 无关的向量 ***a*** 之间的点积的导数。
- en: 'We can expand on [Equation 8.9](ch08.xhtml#ch08equ09), replacing ***x*** with
    a vector-valued function, ***f(x***):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以扩展 [方程 8.9](ch08.xhtml#ch08equ09)，将 ***x*** 替换为向量值函数 ***f(x***):'
- en: '![Image](Images/08equ10.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ10.jpg)'
- en: What’s the form of this result? Assume *f* accepts an *m*-element input and
    returns an *n*-element vector output. Likewise, assume ***a*** to be an *n*-element
    vector. From [Equation 8.2](ch08.xhtml#ch08equ02), we know the derivative ∂***f***/∂***x***
    to be an *n* × *m* matrix. Therefore, the final result is a (1 × *n*) × (*n* ×
    *m*) → 1 × *m* row vector. Good! We know the derivative of a scalar function by
    a vector should be a row vector when using the numerator layout convention.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果的形式是什么？假设 *f* 接受一个 *m* 元素的输入并返回一个 *n* 元素的向量输出。同样，假设 ***a*** 是一个 *n* 元素的向量。从
    [方程 8.2](ch08.xhtml#ch08equ02)，我们知道导数 ∂***f***/∂***x*** 是一个 *n* × *m* 矩阵。因此，最终结果是一个
    (1 × *n*) × (*n* × *m*) → 1 × *m* 的行向量。很好！我们知道，当使用分子布局约定时，标量函数对向量的导数应该是一个行向量。
- en: Finally, the derivative of the dot product of two vector-valued functions, ***f***
    and ***g***, is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，两个向量值函数 ***f*** 和 ***g*** 的点积的导数是
- en: '![Image](Images/08equ11.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ11.jpg)'
- en: If [Equation 8.10](ch08.xhtml#ch08equ10) is a row vector, then the sum of two
    terms like it is also a row vector.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 [方程 8.10](ch08.xhtml#ch08equ10) 是一个行向量，那么像它这样的两个项的和也是一个行向量。
- en: A Vector Function by a Scalar
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标量乘以向量的函数
- en: 'Vector-by-scalar differentiation, [Table 8-1](ch08.xhtml#ch08tab01), first
    row, second column, is less common in machine learning, so we’ll only examine
    a few identities. The first are multiplications by constants:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 向量对标量的微分，[表 8-1](ch08.xhtml#ch08tab01)，第一行，第二列，在机器学习中较少见，因此我们只会检查一些恒等式。首先是常数乘法：
- en: '![Image](Images/202equ01.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/202equ01.jpg)'
- en: Note, we can multiply on the left by a matrix, as the derivative is a column
    vector.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以通过左乘矩阵，因为导数是一个列向量。
- en: The sum rule still applies,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 求和规则仍然适用，
- en: '![Image](Images/202equ02.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/202equ02.jpg)'
- en: as does the chain rule,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就像链式法则一样，
- en: '![Image](Images/08equ12.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ12.jpg)'
- en: '[Equation 8.12](ch08.xhtml#ch08equ12) is correct, since the derivative of a
    vector by a scalar is a column vector, and the derivative of a vector by a vector
    is a matrix. Therefore, multiplying the matrix on the right by a column vector
    returns a column vector, as expected.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 8.12](ch08.xhtml#ch08equ12)是正确的，因为标量对向量的导数是一个列向量，而向量对向量的导数是一个矩阵。因此，将右侧的矩阵与列向量相乘会返回一个列向量，正如预期的那样。'
- en: 'Two other derivatives involving dot products with respect to a scalar are worth
    knowing about. The first is similar to [Equation 8.11](ch08.xhtml#ch08equ11) but
    with two vector-valued functions of a scalar:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另有两个涉及标量的点积导数是值得了解的。第一个与[方程 8.11](ch08.xhtml#ch08equ11)相似，但有两个标量值函数：
- en: '![Image](Images/202equ03.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/202equ03.jpg)'
- en: 'The second derivative concerns the composition of *f*(***g***) and ***g***(*x*)
    with respect to *x*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次导数涉及*f*(***g***)和***g***(*x*)关于*x*的合成：
- en: '![Image](Images/202equ04.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/202equ04.jpg)'
- en: which is the dot product of a row vector and a column vector.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个行向量和列向量的点积。
- en: A Vector Function by a Vector
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量函数与向量的关系
- en: 'The derivatives of vector-valued functions with vector arguments are common
    in physics and engineering. In machine learning, they show up during backpropagation,
    for example, at the derivative of the loss function. Let’s begin with some straightforward
    identities:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 向量值函数与向量参数的导数在物理学和工程学中很常见。在机器学习中，它们通常出现在反向传播中，例如，在损失函数的导数中。让我们从一些简单的恒等式开始：
- en: '![Image](Images/203equ01.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/203equ01.jpg)'
- en: and
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Image](Images/203equ02.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/203equ02.jpg)'
- en: where the result is the sum of two matrices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中结果是两个矩阵的和。
- en: 'The chain rule is next and works as it did above for scalar-by-vector and vector-by-scalar
    derivatives:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是链式法则，它的作用与上述标量对向量和向量对标量的导数相同：
- en: '![Image](Images/203equ03.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/203equ03.jpg)'
- en: with the result being the product of two matrices.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个矩阵的乘积。
- en: A Scalar Function by a Matrix
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个标量函数与矩阵的关系
- en: 'For functions of a matrix, ***X***, returning a scalar, we have the usual form
    for the sum rule:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于返回标量的矩阵函数***X***，我们有求和规则的常规形式：
- en: '![Image](Images/203equ04.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/203equ04.jpg)'
- en: with the result being the sum of two matrices. Recall, if ***X*** is an *n*
    × *m* matrix, the derivative in numerator layout notation is an *m* × *n* matrix.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个矩阵的和。回想一下，如果***X***是一个*n* × *m*的矩阵，那么分子布局符号中的导数是一个*m* × *n*的矩阵。
- en: 'The product rule also works as expected:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 乘积法则也按预期工作：
- en: '![Image](Images/203equ05.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/203equ05.jpg)'
- en: 'However, the chain rule is different. It depends on *f(g*), a scalar function
    accepting a scalar input, and *g*(***X***), a scalar function accepting a matrix
    input. With this restriction, the form of the chain rule looks familiar:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，链式法则有所不同。它依赖于*f(g*)，一个接受标量输入的标量函数，以及*g*(***X***)，一个接受矩阵输入的标量函数。在这个限制下，链式法则的形式看起来很熟悉：
- en: '![Image](Images/08equ13.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08equ13.jpg)'
- en: 'Let’s see [Equation 8.13](ch08.xhtml#ch08equ13) in action. First, we need ***X***,
    a 2 × 2 matrix:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下[方程 8.13](ch08.xhtml#ch08equ13)的应用。首先，我们需要***X***，一个2 × 2的矩阵：
- en: '![Image](Images/204equ01.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/204equ01.jpg)'
- en: Next, we need ![Image](Images/204equ02.jpg) and *g*(***X***) = *x*[0]*x*[3]
    + *x*[1]*x*[2]. Notice, while *g*(***X***) accepts a matrix input, the result
    is a scalar calculated from the values of the matrix.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要![图片](Images/204equ02.jpg)和*g*(***X***) = *x*[0]*x*[3] + *x*[1]*x*[2]。注意，虽然*g*(***X***)接受矩阵输入，但结果是一个标量，由矩阵的值计算得出。
- en: To apply the chain rule, we need two derivatives,
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用链式法则，我们需要两个导数，
- en: '![Image](Images/204equ03.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/204equ03.jpg)'
- en: where we are again using numerator layout for the result.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用分子布局来表示结果。
- en: To find the overall result, we calculate
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到整体结果，我们计算
- en: '![Image](Images/204equ04.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/204equ04.jpg)'
- en: To check, we combine the functions to write a single function, ![Image](Images/204equ05.jpg),
    and calculate the derivative using the standard chain rule for each element of
    the resulting matrix. This gives us
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证，我们将这些函数结合起来写成一个单一的函数，![图片](Images/204equ05.jpg)，并使用标准的链式法则对结果矩阵中的每个元素进行求导。这会给我们
- en: '![Image](Images/204equ06.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/204equ06.jpg)'
- en: matching the previous result.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的结果相匹配。
- en: We have our definitions and identities. Let’s revisit the derivative of a vector-valued
    function with a vector argument, as the resulting matrix is special. We’ll encounter
    it frequently in deep learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了定义和恒等式。让我们重新审视一下带有向量参数的向量值函数的导数，因为得到的矩阵是特殊的。我们将在深度学习中频繁遇到它。
- en: Jacobians and Hessians
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 雅可比矩阵与赫西矩阵
- en: '[Equation 8.2](ch08.xhtml#ch08equ02) defined the derivative of a vector-valued
    function, ***f***, with respect to a vector, ***x***:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程8.2](ch08.xhtml#ch08equ02)定义了带有向量参数***x***的向量值函数***f***的导数：'
- en: '![Image](Images/08equ14.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ14.jpg)'
- en: This derivative is known as the *Jacobian matrix*, ***J***, or simply the *Jacobian*,
    and you’ll encounter it from time to time in the deep learning literature, especially
    during discussions of gradient descent and other optimization algorithms used
    in training models. The Jacobian sometimes has a subscript to indicate the variable
    it is with respect to; for example, ***J******[x]*** if with respect to ***x***.
    When the context is clear, we’ll often neglect the subscript.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个导数被称为*雅可比矩阵*，***J***，或简称*雅可比*，你会在深度学习文献中时常遇到它，特别是在讨论梯度下降和其他用于训练模型的优化算法时。雅可比矩阵有时会带有下标，表示它是相对于某个变量的；例如，***J******[x]***
    表示相对于***x***。当上下文清晰时，我们通常会省略下标。
- en: In this section, we’ll discuss the Jacobian and what it means. Then we’ll introduce
    another matrix, the *Hessian matrix* (or just the *Hessian*), which is based on
    the Jacobian, and learn how to use it in optimization problems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论雅可比矩阵及其含义。然后我们将介绍另一个矩阵——*赫西矩阵*（或简称*赫西*），它基于雅可比矩阵，并学习如何在优化问题中使用它。
- en: 'The essence of this section is the following: the Jacobian is the generalization
    of the first derivative, and the Hessian is the generalization of the second derivative.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的核心内容如下：雅可比矩阵是第一导数的概括，赫西矩阵是第二导数的概括。
- en: Concerning Jacobians
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于雅可比矩阵
- en: 'We saw previously that we can think of [Equation 8.14](ch08.xhtml#ch08equ14)
    as a stack of transposed gradient vectors ([Equation 8.3](ch08.xhtml#ch08equ03)):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，我们可以将[方程8.14](ch08.xhtml#ch08equ14)看作是转置梯度向量的堆叠（[方程8.3](ch08.xhtml#ch08equ03)）：
- en: '![Image](Images/205equ01.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/205equ01.jpg)'
- en: Viewing the Jacobian as a stack of gradients gives us a clue as to what it represents.
    Recall, the gradient of a scalar field, a function accepting a vector argument
    and returning a scalar, points in the direction of the maximum change in the function.
    Similarly, the Jacobian gives us information about how the vector-valued function
    behaves in the vicinity of some point, ***x******[p]***. The Jacobian is to vector-valued
    functions of vectors what the gradient is to scalar-valued functions of vectors;
    it tells us about how the function changes for a small change in the position
    of ***x******[p]***.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 将雅可比矩阵看作是梯度堆叠，给了我们一些线索，帮助理解它的意义。回想一下，标量场的梯度，一个接受向量参数并返回标量的函数，指向函数最大变化的方向。类似地，雅可比矩阵为我们提供了有关向量值函数在某一点***x******[p]***附近行为的信息。雅可比矩阵对于向量值函数和向量的关系，就像梯度对于标量值函数和向量的关系；它告诉我们函数如何随着***x******[p]***位置的小变化而变化。
- en: One way to think of the Jacobian is as a generalization of the more specific
    cases we encountered in [Chapter 7](ch07.xhtml#ch07). [Table 8-2](ch08.xhtml#ch08tab02)
    shows the relationship between the function and what its derivative measures.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种理解雅可比矩阵的方法是将其视为我们在[第7章](ch07.xhtml#ch07)中遇到的更具体情况的概括。[表8-2](ch08.xhtml#ch08tab02)展示了函数与其导数所度量的关系。
- en: '**Table 8-2:** The Relationship Between Jacobians, Gradients, and Slopes'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**表8-2：** 雅可比矩阵、梯度和斜率之间的关系'
- en: '| **Function** | **Derivative** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **函数** | **导数** |'
- en: '| --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ***f***(***x***) | ∂***f***/∂***x***, Jacobian matrix |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ***f***(***x***) | ∂***f***/∂***x***, 雅可比矩阵 |'
- en: '| *f*(***x***) | ∂*f*/∂***x***, gradient vector |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| *f*(***x***) | ∂*f*/∂***x***，梯度向量 |'
- en: '| *f*(*x*) | *df*/*dx*, slope |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| *f*(*x*) | *df*/*dx*，斜率 |'
- en: 'The Jacobian matrix is the most general of the three. If we limit the function
    to a scalar, then the Jacobian matrix becomes the gradient vector (row vector
    in numerator layout). If we limit the function and argument to scalars, the gradient
    becomes the slope. In a sense, they all indicate the same thing: how the function
    is changing around a point in space.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 雅可比矩阵是三者中最一般的。如果我们将函数限制为标量，则雅可比矩阵变为梯度向量（分子形式的行向量）。如果我们将函数和参数都限制为标量，梯度则变为斜率。从某种意义上说，它们都表示相同的东西：函数在空间中某一点周围如何变化。
- en: The Jacobian has many uses. I’ll present two examples. The first is from systems
    of differential equations. The second uses Newton’s method to find the roots of
    a vector-valued function. We’ll see Jacobians again when we discuss backpropagation,
    as that requires calculating derivatives of a vector-valued function with respect
    to a vector.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 雅可比矩阵有许多用途。这里我将展示两个例子。第一个来自微分方程系统。第二个使用牛顿法来寻找向量值函数的根。当我们讨论反向传播时，还会再次看到雅可比矩阵，因为反向传播需要计算相对于向量的向量值函数的导数。
- en: Autonomous Differential Equations
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自治微分方程
- en: A *differential equation* combines derivatives and function values in one equation.
    Differential equations show up everywhere in physics and engineering. Our example
    comes from the theory of *autonomous systems*, which are differential equations
    where the independent variable does not appear on the right-hand side. For instance,
    if the system consists of values of the function and first derivatives with respect
    to time, *t*, there is no *t* explicit in the equations governing the system.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*微分方程*将导数和函数值结合在一个方程中。微分方程在物理学和工程学中随处可见。我们的例子来自*自治系统*的理论，它是微分方程的一种形式，其中独立变量不出现在方程的右侧。例如，如果系统由函数的值和关于时间
    *t* 的一阶导数组成，则系统的方程中没有显式的 *t*。'
- en: The previous paragraph is just for background; you don’t need to memorize it.
    Working with systems of autonomous differential equations ultimately leads to
    the Jacobian, which is our goal. We can view the system as a vector-valued function,
    and we’ll use the Jacobian to characterize the critical points of that system
    (the points where the derivative is zero). We worked with critical points of functions
    in [Chapter 7](ch07.xhtml#ch07).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的段落只是背景介绍；你不需要记住它。处理自治微分方程系统最终会导致雅可比矩阵，这就是我们的目标。我们可以将系统看作一个向量值函数，我们将使用雅可比矩阵来描述该系统的临界点（即导数为零的点）。我们在[第7章](ch07.xhtml#ch07)中处理了函数的临界点。
- en: 'For example, let’s explore the following system of equations:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们探讨以下方程组：
- en: '![Image](Images/206equ01.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/206equ01.jpg)'
- en: This system includes two functions, *x*(*t*) and *y*(*t*), and they are coupled
    so that the rate of change of *x*(*t*) depends on the value of *x* and the value
    of *y*, and vice versa.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统包括两个函数，*x*(*t*) 和 *y*(*t*)，它们是耦合的，即 *x*(*t*) 的变化率依赖于 *x* 和 *y* 的值，反之亦然。
- en: 'We’ll view the system as a single, vector-valued function:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将系统视为一个单一的向量值函数：
- en: '![Image](Images/08equ15.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ15.jpg)'
- en: where we replace *x* with *x*[0] and *y* with *x*[1].
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 *x* 替换为 *x*[0]，将 *y* 替换为 *x*[1]。
- en: The system that ***f*** represents has critical points at locations where ***f***
    = **0**, with **0** being the 2 × 1 dimensional zero vector. The critical points
    are
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '***f***表示的系统在***f*** = **0**的地方具有临界点，其中**0**是2 × 1维的零向量。临界点是'
- en: '![Image](Images/08equ16.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ16.jpg)'
- en: where substitution into ***f*** shows that each point returns the zero vector.
    For the time being, assume we were given the critical points, and now we want
    to characterize them.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其中将每个点代入***f***后得到零向量。暂时假设我们已经得到了临界点，现在我们想要描述它们。
- en: 'To characterize a critical point, we will need the Jacobian matrix that ***f***
    generates:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述一个临界点，我们需要***f***生成的雅可比矩阵：
- en: '![Image](Images/08equ17.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ17.jpg)'
- en: Since the Jacobian describes how a function behaves in the vicinity of a point,
    we can use it to characterize the critical points. In [Chapter 7](ch07.xhtml#ch07),
    we used the derivative to tell us whether a point was a minimum or maximum of
    a function. For the Jacobian, we use the eigenvalues of ***J*** in much the same
    way to talk about the type and stability of critical points.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于雅可比矩阵描述了函数在某一点附近的行为，因此我们可以用它来描述临界点。在[第7章](ch07.xhtml#ch07)中，我们使用导数来判断一个点是函数的最小值还是最大值。对于雅可比矩阵，我们通过使用***J***的特征值来以类似的方式讨论临界点的类型和稳定性。
- en: 'First, let’s find the Jacobian at each of the critical points:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在每个临界点处求雅可比矩阵：
- en: '![Image](Images/207equ01.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/207equ01.jpg)'
- en: 'We can use NumPy to get the eigenvalues of the Jacobians:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NumPy来获得雅可比矩阵的特征值：
- en: '[PRE0]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We encountered `np.linalg.eig` in [Chapter 6](ch06.xhtml#ch06). The eigenvalues
    are the first values that `eig` returns, hence the `[0]` subscript to the function
    call.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](ch06.xhtml#ch06)中遇到了`np.linalg.eig`。特征值是`eig`返回的第一个值，因此函数调用中有`[0]`的下标。
- en: For critical points of a system of autonomous differential equations, the eigenvalues
    indicate the points’ type and stability. If both eigenvalues are real and have
    the same sign, the critical point is a node. If the eigenvalues are less than
    zero, the node is stable; otherwise, it is unstable. You can think of a stable
    node as a pit; if you’re near it, you’ll fall into it. An unstable node is like
    a hill; if you move away from the top, the critical point, you’ll fall off. The
    first critical point, ***c***[0], has positive, real eigenvalues; therefore, it
    represents an unstable node.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组自治微分方程的临界点，特征值表明了点的类型和稳定性。如果两个特征值是实数并且符号相同，那么临界点是一个节点。如果特征值小于零，那么节点是稳定的；否则，它是不稳定的。你可以将稳定节点看作是一个坑；如果你接近它，你会掉进去。一个不稳定的节点像一座山；如果你从顶部偏离临界点，你会掉下来。第一个临界点***c***[0]的特征值为正实数，因此它代表一个不稳定的节点。
- en: If the eigenvalues of the Jacobian are real but of opposite signs, the critical
    point is a saddle point. We discussed saddle points in [Chapter 7](ch07.xhtml#ch07).
    A saddle point is ultimately unstable, but in two dimensions, there’s a direction
    where you can “fall into” the saddle and a direction where you can “fall off”
    the saddle. Some researchers believe most minima found when training deep neural
    networks are really saddle points of the loss function. We see that critical point
    ***c***[1] is a saddle point, since the eigenvalues are real with opposite signs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果雅可比矩阵的特征值是实数但符号相反，那么临界点就是鞍点。我们在[第7章](ch07.xhtml#ch07)中讨论了鞍点。鞍点最终是不稳定的，但在二维空间中，有一个方向你可以“掉进”鞍点，另一个方向则可以“从”鞍点“掉出来”。一些研究者认为，在训练深度神经网络时，找到的大多数最小值实际上是损失函数的鞍点。我们看到临界点***c***[1]是一个鞍点，因为特征值是实数且符号相反。
- en: Finally, the eigenvalues of ***c***[2] are complex. Complex eigenvalues indicate
    a spiral (also called a focus). If the real part of the eigenvalues is less than
    zero, the spiral is stable; otherwise, it is unstable. As the eigenvalues are
    complex conjugates of each other, the signs of the real parts must be the same;
    one can’t be positive while the other is negative. For ***c***[2], the real parts
    are negative, so ***c***[2] indicates a stable spiral.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，***c***[2]的特征值是复数。复特征值表示一个螺旋（也叫焦点）。如果复特征值的实部小于零，那么螺旋是稳定的；否则，它是不稳定的。由于特征值是彼此的复共轭，实部的符号必须相同；不能有一个是正的，而另一个是负的。对于***c***[2]，实部是负的，因此***c***[2]表示一个稳定的螺旋。
- en: Newton’s Method
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 牛顿法
- en: I presented the critical points of [Equation 8.15](ch08.xhtml#ch08equ15) by
    fiat. The system is easy enough that we can solve for the critical points algebraically,
    but that might not generally be the case. One classic method for finding the roots
    of a function (the places where it returns zero) is known as *Newton’s method*.
    This is an iterative method using the first derivative and an initial guess to
    zero in on the root. Let’s look at the method in one dimension and then extend
    it to two. We’ll see that moving to two or more dimensions requires the use of
    the Jacobian.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过公理性的方法给出了[方程8.15](ch08.xhtml#ch08equ15)的临界点。系统足够简单，我们可以通过代数方法求解临界点，但通常情况下情况并非如此。一个经典的求解函数根的方法（即函数值为零的点）被称为*牛顿法*。这是一种使用一阶导数和初始猜测值来逼近根的迭代方法。我们先看一下它在一维空间中的应用，然后再扩展到二维空间。我们将看到，进入二维或更高维空间时，需要使用雅可比矩阵。
- en: 'Let’s use Newton’s method to find the square root of 2\. To do that, we need
    an equation such that ![Image](Images/208equ01.jpg). A moment’s thought gives
    us one: *f*(*x*) = 2 − *x*². Clearly, when ![Image](Images/208equ02.jpg).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用牛顿法来求解2的平方根。为此，我们需要一个方程，使得![Image](Images/208equ01.jpg)。稍加思考，我们得到了一个方程：*f*(*x*)
    = 2 − *x*²。显然，当![Image](Images/208equ02.jpg)。
- en: The governing equation for Newton’s method in one dimension is
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿法在一维中的控制方程是
- en: '![Image](Images/08equ18.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ18.jpg)'
- en: where *x*[0] is some initial guess at the solution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x*[0] 是对解的某个初始猜测值。
- en: We substitute *x*[0] for *x[n]* on the right-hand side of [Equation 8.18](ch08.xhtml#ch08equ18)
    to find *x*[1]. We then repeat using *x*[1] on the right-hand side to get *x*[2],
    and so on until we see little change in *x[n]*. At that point, if our initial
    guess is reasonable, we have the value we’re looking for. Newton’s method converges
    quickly, so for typical examples, we only need a handful of iterations. Of course,
    we have powerful computers at our fingertips, so we’ll use them instead of working
    by hand. The Python code we need is in [Listing 8-1](ch08.xhtml#ch08ex01).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将*x*[0]代入[方程 8.18](ch08.xhtml#ch08equ18)右侧来求得*x*[1]。然后我们重复使用*x*[1]代入右侧来得到*x*[2]，以此类推，直到我们看到*x[n]*的变化非常小。此时，如果我们的初始猜测合理，我们就得到了我们想要的值。牛顿法收敛很快，所以对于典型的例子，我们只需要几次迭代。当然，我们手头有强大的计算机，所以我们会使用它们，而不是手工计算。我们需要的Python代码在[清单
    8-1](ch08.xhtml#ch08ex01)中。
- en: '[PRE1]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 8-1: Finding* ![Image](Images/209equ01.jpg) *via Newton’s method*'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-1：通过牛顿法求解* ![Image](Images/209equ01.jpg) *的值*'
- en: '[Listing 8-1](ch08.xhtml#ch08ex01) defines two functions. The first, `f(x)`,
    returns the function value for a given `x`. The second, `d(x)`, returns the derivative
    at `x`. If *f*(*x*) = 2 − *x*², then *f*′(*x*) = −2*x*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-1](ch08.xhtml#ch08ex01)定义了两个函数。第一个，`f(x)`，返回给定`x`的函数值。第二个，`d(x)`，返回`x`处的导数。如果*f*(*x*)
    = 2 − *x*²，则*f*′(*x*) = −2*x*。'
- en: Our initial guess is *x* = 1.0\. We iterate [Equation 8.18](ch08.xhtml#ch08equ18)
    five times, printing the current estimate of the square root of 2 each time. Finally,
    we use NumPy to calculate the *true* value and see how far we are from it.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始猜测是 *x* = 1.0。我们对[方程 8.18](ch08.xhtml#ch08equ18)进行五次迭代，每次打印当前的平方根2的估计值。最后，我们使用NumPy计算*真实*值，并查看我们与真实值的偏差。
- en: Running [Listing 8-1](ch08.xhtml#ch08ex01) produces
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 运行[清单 8-1](ch08.xhtml#ch08ex01)产生了
- en: '[PRE2]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: which is impressive; we get ![Image](Images/209equ02.jpg) to 16 decimals in
    only five iterations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人印象深刻；我们仅用五次迭代就得到了 ![Image](Images/209equ02.jpg)，精确到16位小数。
- en: To extend Newton’s method to vector-valued functions of vectors, like [Equation
    8.15](ch08.xhtml#ch08equ15), we replace the reciprocal of the derivative with
    the inverse of the Jacobian. Why the inverse? Recall, for a diagonal matrix, the
    inverse is the reciprocal of the diagonal elements. If we view the scalar derivative
    as a 1 × 1 matrix, then the reciprocal and inverse are the same. [Equation 8.18](ch08.xhtml#ch08equ18)
    is already using the inverse of the Jacobian, albeit one for a 1 × 1 matrix. Therefore,
    we’ll iterate
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将牛顿法扩展到向量值函数的向量问题（如[方程 8.15](ch08.xhtml#ch08equ15)），我们用雅可比矩阵的逆替代了导数的倒数。为什么是逆矩阵？回想一下，对于一个对角矩阵，逆矩阵是对角元素的倒数。如果我们将标量导数视为一个1
    × 1矩阵，那么倒数和逆矩阵是相同的。[方程 8.18](ch08.xhtml#ch08equ18)已经使用了雅可比矩阵的逆，尽管它是一个1 × 1矩阵的逆。因此，我们将进行迭代
- en: '![Image](Images/08equ19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ19.jpg)'
- en: for a suitable initial value, ***x***[0], and the inverse of the Jacobian evaluated
    at *x[n]*. Let’s use Newton’s method to find the critical points of [Equation
    8.15](ch08.xhtml#ch08equ15).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于合适的初始值，***x***[0]，以及在*x[n]*处计算的雅可比矩阵的逆。让我们使用牛顿法来找到[方程 8.15](ch08.xhtml#ch08equ15)的临界点。
- en: Before we can write some Python code, we need the inverse of the Jacobian, [Equation
    8.17](ch08.xhtml#ch08equ17). The inverse of a 2 × 2 matrix,
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们编写Python代码之前，我们需要[方程 8.17](ch08.xhtml#ch08equ17)中的雅可比矩阵的逆。一个2 × 2矩阵的逆，
- en: '![Image](Images/210equ01.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/210equ01.jpg)'
- en: is
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: '![Image](Images/210equ02.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/210equ02.jpg)'
- en: assuming the determinant is not zero. The determinant of ***A*** is *ad* − *bc*.
    Therefore, the inverse of [Equation 8.17](ch08.xhtml#ch08equ17) is
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 假设行列式不为零。***A***的行列式是*ad* − *bc*。因此，[方程 8.17](ch08.xhtml#ch08equ17)的逆矩阵是
- en: '![Image](Images/210equ03.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/210equ03.jpg)'
- en: Now we can write our code. The result is [Listing 8-2](ch08.xhtml#ch08ex02).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写代码了。结果见[清单 8-2](ch08.xhtml#ch08ex02)。
- en: '[PRE3]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Listing 8-2: Newton’s method in 2D*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8-2：二维牛顿法*'
- en: '[Listing 8-2](ch08.xhtml#ch08ex02) echoes [Listing 8-1](ch08.xhtml#ch08ex01)
    for the 1D case. We have `f(x)` to calculate the function value for a given input
    vector and `JI(x)` to give us the value of the inverse Jacobian at ***x***. Notice
    that `f(x)` returns a column vector and `JI(x)` returns a 2 × 2 matrix.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-2](ch08.xhtml#ch08ex02)是对1D情况的[清单 8-1](ch08.xhtml#ch08ex01)的回显。我们有`f(x)`来计算给定输入向量的函数值，`JI(x)`来提供在***x***处的雅可比矩阵的逆。注意，`f(x)`返回一个列向量，`JI(x)`返回一个2
    × 2矩阵。'
- en: The code first asks the user for initial guesses, `x0` and `x1`. These are formed
    into the initial vector, `x`. Note that we explicitly form `x` as a column vector
    ❶.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先要求用户输入初始猜测值，`x0`和`x1`。这些值会形成初始向量`x`。注意，我们显式地将`x`形成列向量 ❶。
- en: The implementation of [Equation 8.19](ch08.xhtml#ch08equ19) comes next ❷. The
    inverse Jacobian is a 2 × 2 matrix that we multiply on the right by the function
    value, a 2 × 1 column vector, using NumPy’s matrix multiplication operator, `@`.
    The result is a 2 × 1 column vector subtracted from the current value of `x`,
    itself a 2 × 1 column vector. If the loop is within 10 iterations of completion,
    the current value is printed at the console.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是[方程 8.19](ch08.xhtml#ch08equ19)的实现 ❷。逆雅可比矩阵是一个 2 × 2 的矩阵，我们将其与函数值（一个 2 ×
    1 的列向量）相乘，使用 NumPy 的矩阵乘法运算符 `@`。结果是一个 2 × 1 的列向量，从当前的 `x` 值（也是一个 2 × 1 的列向量）中减去。如果循环在
    10 次迭代内完成，当前值会在控制台打印出来。
- en: Does [Listing 8-2](ch08.xhtml#ch08ex02) work? Let’s run it and see if we can
    find initial guesses leading to each of the critical points ([Equation 8.16](ch08.xhtml#ch08equ16)).
    For an initial guess of ![Image](Images/211equ01.jpg), we get
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8-2](ch08.xhtml#ch08ex02)是否有效？让我们运行它，看看是否能找到初始猜测，从而找到每个关键点（[方程 8.16](ch08.xhtml#ch08equ16)）。对于初始猜测
    ![Image](Images/211equ01.jpg)，我们得到'
- en: '[PRE4]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which is the first critical point of [Equation 8.15](ch08.xhtml#ch08equ15).
    To find the two remaining critical points, we need to pick our initial guesses
    with some care. Some guesses explode, while many lead back to the zero vector.
    However, some trial and error gives us
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[方程 8.15](ch08.xhtml#ch08equ15)的第一个关键点。为了找到剩余的两个关键点，我们需要小心选择初始猜测。一些猜测会爆炸，而许多猜测会导致返回零向量。然而，通过一些试错法，我们得到了
- en: '![Image](Images/211equ02.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/211equ02.jpg)'
- en: showing that Newton’s method can find the critical points of [Equation 8.15](ch08.xhtml#ch08equ15).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明牛顿法可以找到[方程 8.15](ch08.xhtml#ch08equ15)的关键点。
- en: We started this section with a system of differential equations that we interpreted
    as a vector-valued function. We then used the Jacobian to characterize the critical
    points of that system. Next, we used the Jacobian a second time to locate the
    system’s critical points via Newton’s method. We could do this because the Jacobian
    is the generalization of the gradient to vector-valued functions, and the gradient
    itself is a generalization of the first derivative of a scalar function. As mentioned
    above, we’ll see Jacobians again when we discuss backpropagation in [Chapter 10](ch10.xhtml#ch10).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个微分方程系统开始，并将其解释为一个向量值函数。然后我们使用雅可比矩阵来表征该系统的关键点。接下来，我们第二次使用雅可比矩阵通过牛顿法定位系统的关键点。我们之所以能够这样做，是因为雅可比矩阵是梯度对向量值函数的推广，而梯度本身是标量函数一阶导数的推广。如上所述，当我们讨论反向传播时，还会看到雅可比矩阵，[第
    10 章](ch10.xhtml#ch10)中会详细介绍。
- en: Concerning Hessians
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于 Hessian
- en: If the Jacobian matrix is like the first derivative of a function of one variable,
    then the *Hessian matrix* is like the second derivative. In this case, we’re restricted
    to scalar fields, functions returning a scalar value for a vector input. Let’s
    start with the definition and go from there. For the function *f*(***x***), the
    Hessian is defined as
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果雅可比矩阵类似于单变量函数的第一导数，那么*Hessian 矩阵*就像第二导数。在这种情况下，我们限制在标量场上，即对于向量输入返回标量值的函数。让我们从定义开始，然后从那里出发。对于函数
    *f*(***x***)，Hessian 定义为
- en: '![Image](Images/08equ20.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ20.jpg)'
- en: where ***x*** = [*x*[0] *x*[1] . . . *x*[*n*–1]]^⊤.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***x*** = [*x*[0] *x*[1] . . . *x*[*n*–1]]^⊤。
- en: Looking at [Equation 8.20](ch08.xhtml#ch08equ20) tells us that the Hessian is
    a square matrix. Moreover, it’s a symmetric matrix implying ***H*** = ***H***^⊤.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[方程 8.20](ch08.xhtml#ch08equ20)告诉我们 Hessian 是一个方阵。此外，它是对称矩阵，意味着 ***H*** =
    ***H***^⊤。
- en: 'The Hessian is the Jacobian of the gradient of a scalar field:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 是标量场梯度的雅可比矩阵：
- en: '***H**[f]* = ***J***(▽*f*)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '***H**[f]* = ***J***(▽*f*)'
- en: 'Let’s see this with an example. Consider this function:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来看这个。考虑这个函数：
- en: '![Image](Images/212equ01.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/212equ01.jpg)'
- en: 'If we use the definition of the Hessian in [Equation 8.20](ch08.xhtml#ch08equ20)
    directly, we see that ![Image](Images/212equ02.jpg) because ∂*f*/∂*x*[0] = 4*x*[0]
    + *x*[2]. Similar calculations give us the rest of the Hessian matrix:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们直接使用[方程 8.20](ch08.xhtml#ch08equ20)中对 Hessian 的定义，我们可以看到![Image](Images/212equ02.jpg)，因为
    ∂*f*/∂*x*[0] = 4*x*[0] + *x*[2]。类似的计算给出了其余的 Hessian 矩阵：
- en: '![Image](Images/212equ03.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/212equ03.jpg)'
- en: In this case, the Hessian is constant, not a function of ***x***, because the
    highest power of a variable in *f*(***x***) is 2.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Hessian 是常数，而不是 *x* 的函数，因为 *f*(***x***) 中变量的最高次幂是 2。
- en: The gradient of *f*(***x***), using our column vector definition, is
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列向量定义的 *f*(***x***) 的梯度是
- en: '![Image](Images/212equ04.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/212equ04.jpg)'
- en: with the Jacobian of the gradient giving the following, which is identical to
    the matrix we found by direct use of [Equation 8.20](ch08.xhtml#ch08equ20).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的雅可比矩阵给出了以下内容，这与我们通过直接使用[公式8.20](ch08.xhtml#ch08equ20)找到的矩阵是相同的。
- en: '![Image](Images/212equ05.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/212equ05.jpg)'
- en: Minima and Maxima
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最小值和最大值
- en: 'We saw in [Chapter 7](ch07.xhtml#ch07) that we could use the second derivative
    to test whether critical points of a function were minima (*f*′′ > 0) or maxima
    (*f*′′ < 0). We’ll see in the next section how we can use critical points in optimization
    problems. For now, let’s use the Hessian to find critical points by considering
    its eigenvalues. We’ll continue with the example above. The Hessian matrix is
    3 × 3, meaning there are three (or fewer) eigenvalues. Again, we’ll save time
    and use NumPy to tell us what they are:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第7章](ch07.xhtml#ch07)中看到，我们可以利用二阶导数来测试一个函数的临界点是最小值（*f*′′ > 0）还是最大值（*f*′′
    < 0）。我们将在下一节看到如何在优化问题中使用临界点。现在，让我们通过考虑海森矩阵的特征值来使用海森矩阵找到临界点。我们将继续使用上面的例子。海森矩阵是3
    × 3矩阵，意味着它有三个（或更少）特征值。为了节省时间，我们将使用NumPy来告诉我们它们是什么：
- en: '[PRE5]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Two of the three eigenvalues are positive, and one is negative. If all three
    were positive, the critical point would be a minimum. Likewise, if all three were
    negative, the critical point would be a maximum. Notice that the minimum/maximum
    label is the opposite of the sign, just like the single-variable case. However,
    if at least one eigenvalue is positive and another negative, which is the case
    with our example, the critical point is a saddle point.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 三个特征值中有两个是正的，一个是负的。如果三个特征值都是正的，那么临界点将是最小值。同样，如果三个特征值都是负的，临界点将是最大值。请注意，最小值/最大值的标签与符号相反，就像单变量的情况一样。然而，如果至少有一个特征值是正的，另一个是负的（正如我们例子中的情况），那么临界点就是鞍点。
- en: It seems natural to ask whether the Hessian of a vector-valued function, ***f***(***x***),
    exists. After all, we can calculate the Jacobian of such a function; we did so
    above to show that the Hessian is the Jacobian of the gradient.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，我们会问，**向量值函数** ***f***（***x***）的海森矩阵是否存在。毕竟，我们可以计算这样的函数的雅可比矩阵；我们之前也这样做了，来展示海森矩阵是梯度的雅可比矩阵。
- en: 'It is possible to extend the Hessian to a vector-valued function. However,
    the result is no longer a matrix, but an order-3 tensor. To see this is so, consider
    the definition of a vector-valued function:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将海森矩阵扩展到向量值函数。然而，结果不再是一个矩阵，而是一个三阶张量。为了证明这一点，考虑向量值函数的定义：
- en: '![Image](Images/213equ01.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/213equ01.jpg)'
- en: We can think of a vector-valued function, a vector field, as a vector of scalar
    functions of a vector. We could calculate the Hessian of each of the *m* functions
    in ***f*** to get a vector of matrices,
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把一个向量值函数（一个向量场）看作是一个由向量的标量函数组成的向量。我们可以计算***f***中每个*m*个函数的海森矩阵，得到一个矩阵向量，
- en: '![Image](Images/213equ02.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/213equ02.jpg)'
- en: 'but a vector of matrices is a 3D object. Think of an RGB image: a 3D array
    made up of three 2D images, one each for the red, green, and blue channels. Therefore,
    while possible to define and calculate, the Hessian of a vector-valued function
    is beyond our current scope.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 但一个矩阵向量是一个三维对象。想象一下RGB图像：一个由三个二维图像组成的三维数组，分别表示红色、绿色和蓝色通道。因此，虽然可以定义和计算，但向量值函数的海森矩阵超出了我们当前的讨论范围。
- en: Optimization
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化
- en: In deep learning, you’re most likely to see the Hessian in reference to optimization.
    Training a neural network is, to a first approximation, an optimization problem—the
    goal is to find the weights and biases leading to a minimum in the loss function
    landscape.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，你最常见到海森矩阵是与优化相关的。训练神经网络从大体上来说就是一个优化问题——目标是找到能够最小化损失函数的权重和偏置。
- en: In [Chapter 7](ch07.xhtml#ch07), we saw that the gradient provides information
    on how to move toward a minimum. An optimization algorithm, like gradient descent,
    the subject of [Chapter 11](ch11.xhtml#ch11), uses the gradient as a guide. As
    the gradient is a first derivative of the loss function, algorithms based solely
    on the gradient are known as *first-order optimization methods*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml#ch07)中，我们看到梯度提供了如何朝着最小值移动的信息。优化算法，如梯度下降法，[第11章](ch11.xhtml#ch11)的主题，利用梯度作为指导。因为梯度是损失函数的一阶导数，所以完全基于梯度的算法被称为*一阶优化方法*。
- en: The Hessian provides information beyond the gradient. As a second derivative,
    the Hessian contains information about how the loss landscape’s gradient is changing,
    that is, its curvature. An analogy from physics might help here. A particle’s
    motion in one dimension is described by some function of time, *x*(*t*). The first
    derivative, the velocity, is *dx*/*dt* = *v*(*t*). The velocity tells us how quickly
    the position is changing in time. However, the velocity might change with time,
    so its derivative, *dv*/*dt* = *a*(*t*), is the acceleration. And, if the velocity
    is the first derivative of the position, then the acceleration is the second,
    *d*²*x*/*dt*² = *a*(*t*). Similarly, the second derivative of the loss function,
    the Hessian, provides information on how the gradient is changing. Optimization
    algorithms using the Hessian, or an approximation of it, are known as *second-order
    optimization methods*.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 矩阵提供了比梯度更丰富的信息。作为二阶导数，Hessian 包含了关于损失函数梯度变化的信息，也就是其曲率。这里的物理类比可能有帮助。一个粒子在一维空间中的运动可以通过时间的某个函数
    *x*(*t*) 来描述。其一阶导数，速度，是 *dx*/*dt* = *v*(*t*)。速度告诉我们位置随时间变化的快慢。然而，速度可能随时间变化，因此其导数
    *dv*/*dt* = *a*(*t*) 就是加速度。如果速度是位置的一阶导数，那么加速度就是二阶导数，*d*²*x*/*dt*² = *a*(*t*)。类似地，损失函数的二阶导数，即
    Hessian，提供了梯度变化的信息。使用 Hessian 或其近似值的优化算法被称为*二阶优化方法*。
- en: Let’s start with an example in one dimension. We have a function, *f*(*x*),
    and we’re currently at some *x*[0]. We want to move from this position to a new
    position, *x*[1], closer to a minimum of *f*(*x*). A first-order algorithm will
    use the gradient, here the derivative, as a guide, since we know moving in the
    direction opposite to the derivative will move us toward a lower function value.
    Therefore, for some step size, call it η (eta), we can write
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个一维的例子开始。我们有一个函数 *f*(*x*)，并且当前在某个 *x*[0] 处。我们希望从这个位置移动到一个新的位置 *x*[1]，接近
    *f*(*x*) 的最小值。一个一阶算法会使用梯度（这里是导数）作为指导，因为我们知道，沿着导数的反方向移动会使函数值变小。因此，对于某个步长，记作 η（希腊字母
    eta），我们可以写成
- en: '*x*[1] = *x*[0] − η *f*′(*x*[0])'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[1] = *x*[0] − η *f*′(*x*[0])'
- en: This will move us from *x*[0] toward *x*[1], which is closer to the minimum
    of *f*(*x*), assuming the minimum exists.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把我们从 *x*[0] 移动到 *x*[1]，假设最小值存在的话，*x*[1] 会更接近 *f*(*x*) 的最小值。
- en: The equation above makes sense, so why think about a second-order method? The
    second-order method comes into play when we move from *f*(*x*) to *f*(*x*). Now
    we have a gradient, not just a derivative, and the landscape of *f*(***x***) around
    some point can be more complex. The general form of gradient descent is
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程是有意义的，那么为什么还要考虑二阶方法呢？二阶方法的作用是在我们从 *f*(*x*) 移动到 *f*(*x*) 时起作用。现在我们有了梯度，而不仅仅是导数，并且
    *f*(***x***) 在某个点附近的形态可能会更加复杂。梯度下降的通用形式是
- en: '***x***[1] = ***x***[0] − η▽*f*(***x***[0])'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '***x***[1] = ***x***[0] − η▽*f*(***x***[0])'
- en: 'but the information in the Hessian can be of assistance. To see how, we first
    need to introduce the idea of a *Taylor series expansion*, a way of approximating
    an arbitrary function as the sum of a series of terms. We use Taylor series frequently
    in physics and engineering to simplify complex functions in the vicinity of a
    specific point. We also often use them to calculate values of *transcendental
    functions* (functions that can’t be written as a finite set of basic algebra operations).
    For example, it’s likely that when you use `cos(x)` in a programming language,
    the result is generated by a Taylor series expansion with a sufficient number
    of terms to get the cosine to 32- or 64-bit floating-point precision:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，Hessian 矩阵中的信息是有帮助的。为了了解其作用，我们首先需要引入*泰勒级数展开*的概念，这是一种将任意函数近似为一系列项之和的方法。在物理学和工程学中，我们经常使用泰勒级数来简化某个特定点附近的复杂函数。我们也经常用它们来计算*超越函数*的值（无法通过有限的基本代数运算来表示的函数）。例如，当你在编程语言中使用`cos(x)`时，结果可能是通过泰勒级数展开生成的，所用的项数足以让余弦函数达到32位或64位浮点精度：
- en: '![Image](Images/215equ01.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/215equ01.jpg)'
- en: In general, to approximate a function, *f*(*x*), in the vicinity of a point,
    *x* = *a*, the Taylor series expansion is
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，为了在某个点 *x* = *a* 附近近似函数 *f*(*x*)，泰勒级数展开是
- en: '![Image](Images/215equ02.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/215equ02.jpg)'
- en: where *f*^((*k*))(*a*) is the *k*th derivative of *f*(*x*) evaluated at point
    *a*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f*^((*k*))(*a*) 是 *f*(*x*) 在点 *a* 处的 *k* 阶导数。
- en: A linear approximation of *f*(*x*) around *x* = *a* is
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *x* = *a* 附近，*f*(*x*) 的线性近似是
- en: '*f*(*x*) ≈ *f*(*a*) + *f*′(*a*)(*x* - *a*)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*) ≈ *f*(*a*) + *f*′(*a*)(*x* - *a*)'
- en: while a quadratic approximation of *f*(*x*) becomes
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 而*f*(*x*)的二次近似变为
- en: '![Image](Images/08equ21.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ21.jpg)'
- en: where we see the linear approximation using the first derivative and the quadratic
    using the first and second derivatives of *f*(*x*). A first-order optimization
    algorithm uses the linear approximation, while a second-order one uses the quadratic
    approximation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到使用一阶导数的线性近似和使用*f*(*x*)的一阶和二阶导数的二次近似。一阶优化算法使用线性近似，而二阶优化算法使用二次近似。
- en: Moving from a scalar function of a scalar, *f*(*x*), to a scalar function of
    a vector, *f*(***x***), changes the first derivative to a gradient and the second
    derivative to the Hessian matrix,
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 从标量函数*f*(*x*)到向量的标量函数*f*(***x***)的转换，将一阶导数变为梯度，二阶导数变为赫塞矩阵，
- en: '![Image](Images/215equ03.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/215equ03.jpg)'
- en: with ***H***[*f*](***a***) the Hessian matrix for *f*(***x***) evaluated at
    the point ***a***. The products’ order changes to make the dimensions work out
    properly, as we now have vectors and a matrix to deal with.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***H***[*f*](***a***)是*f*(***x***)在点***a***处求值的赫塞矩阵。为了使维度匹配，乘积的顺序发生了变化，因为我们现在需要处理向量和矩阵。
- en: For example, if ***x*** has *n* elements, then *f*(***a***) is a scalar; the
    gradient at ***a*** is an *n*-element column vector multiplying (***x*** − ***a***)^⊤,
    an *n*-element row vector, producing a scalar; and the last term is 1 × *n* times
    *n* × *n* times *n* × 1, leading to 1 × *n* times *n* × 1, which is also a scalar.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果***x***有*n*个元素，则*f*(***a***)是一个标量；在***a***处的梯度是一个*n*维的列向量乘以(***x*** − ***a***)^⊤，一个*n*维的行向量，产生一个标量；最后一项是1
    × *n*乘以*n* × *n*乘以*n* × 1，结果是1 × *n*乘以*n* × 1，这也是一个标量。
- en: To use the Taylor series expansions for optimization, to find the minimum of
    *f*, we can use Newton’s method in much the same way used in [Equation 8.18](ch08.xhtml#ch08equ18).
    First, we rewrite [Equation 8.21](ch08.xhtml#ch08equ21) to change our viewpoint
    to one of a displacement (Δ*x*) from a current position (*x*). [Equation 8.21](ch08.xhtml#ch08equ21)
    then becomes
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用泰勒级数展开进行优化，找到*f*的最小值，我们可以像在[方程式 8.18](ch08.xhtml#ch08equ18)中一样使用牛顿法。首先，我们将[方程式
    8.21](ch08.xhtml#ch08equ21)重新写为从当前位置(*x*)的位移(Δ*x*)的角度来看。然后，[方程式 8.21](ch08.xhtml#ch08equ21)变为
- en: '![Image](Images/08equ22.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ22.jpg)'
- en: '[Equation 8.22](ch08.xhtml#ch08equ22) is a parabola in Δ*x*, and we’re using
    it as a stand-in for the more complex shape of *f* in the region of *x* + Δ*x*.
    To find the minimum of [Equation 8.22](ch08.xhtml#ch08equ22), we take the derivative
    and set it to zero, then solve for Δ*x*. The derivative gives'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 8.22](ch08.xhtml#ch08equ22)是Δ*x*的抛物线，我们将其作为*f*在*x* + Δ*x*区域内更复杂形状的替代。为了找到[方程式
    8.22](ch08.xhtml#ch08equ22)的最小值，我们对其求导并令其为零，然后解出Δ*x*。导数给出了'
- en: '![Image](Images/216equ01.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/216equ01.jpg)'
- en: which, if set to zero, leads to
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设为零，则得到
- en: '![Image](Images/08equ23.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ23.jpg)'
- en: '[Equation 8.23](ch08.xhtml#ch08equ23) tells us the offset from a current position,
    *x*, that would lead to the minimum of *f*(*x*) if *f*(*x*) were actually a parabola.
    In reality, *f*(*x*) isn’t a parabola, so the Δ*x* of [Equation 8.23](ch08.xhtml#ch08equ23)
    isn’t the actual offset to the minimum of *f*(*x*). However, since the Taylor
    series expansion used the actual slope, *f*′(*x*), and curvature, *f*′′(*x*),
    of *f*(*x*) at *x*, the offset of [Equation 8.23](ch08.xhtml#ch08equ23) is a better
    estimate of the actual minimum of *f*(*x*) than the linear approximation, assuming
    there is a minimum.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 8.23](ch08.xhtml#ch08equ23)告诉我们当前坐标*x*到达*f*(*x*)最小值的偏移量，假设*f*(*x*)是一个抛物线。实际上，*f*(*x*)并不是抛物线，所以[方程式
    8.23](ch08.xhtml#ch08equ23)中的Δ*x*并不是*f*(*x*)最小值的实际偏移量。然而，由于泰勒级数展开使用了*f*(*x*)在*x*处的实际斜率*f*′(*x*)和曲率*f*′′(*x*)，因此[方程式
    8.23](ch08.xhtml#ch08equ23)的偏移量比线性近似更接近*f*(*x*)的实际最小值，假设存在最小值。'
- en: 'If we go from *x* to *x* + Δ*x*, there’s no reason why we can’t then use [Equation
    8.23](ch08.xhtml#ch08equ23) a second time, calling the new position *x*. Thinking
    like this leads to an equation we can iterate:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从*x*移动到*x* + Δ*x*，那就没有理由不能再次使用[方程式 8.23](ch08.xhtml#ch08equ23)，将新位置称为*x*。这样思考会导致一个可以迭代的方程：
- en: '![Image](Images/08equ24.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ24.jpg)'
- en: for *x*[0], some initial starting point.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*x*[0]，某个初始起点。
- en: We can work out all of the above for scalar functions with vector arguments,
    *f*(***x***), which are the kind we most often encounter in deep learning via
    the loss function. [Equation 8.24](ch08.xhtml#ch08equ24) becomes
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以针对带有向量参数的标量函数 *f*(***x***) 计算上面的内容，这类函数是我们在深度学习中通过损失函数最常遇到的类型。[公式 8.24](ch08.xhtml#ch08equ24)
    变为：
- en: '![Image](Images/216equ02.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/216equ02.jpg)'
- en: where the reciprocal of the second derivative becomes the inverse of the Hessian
    matrix evaluated at ***x**[n]*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，二阶导数的倒数变成了在 ***x**[n]* 处求得的 Hessian 矩阵的逆。
- en: Excellent! We have an algorithm we can use to rapidly find the minimum of a
    function like *f*(***x***). We saw above that Newton’s method converges quickly,
    so using it to minimize a loss function also should converge quickly, faster than
    gradient descent, which only considers the first derivative.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们有一个可以快速找到类似 *f*(***x***) 函数最小值的算法。我们在上面看到，牛顿法收敛速度很快，因此用它来最小化损失函数也应该会迅速收敛，比只考虑一阶导数的梯度下降法要快。
- en: If this is the case, why do we use gradient descent to train neural networks
    instead of Newton’s method?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，为什么我们不使用牛顿法来训练神经网络，而选择梯度下降法呢？
- en: There are several reasons. First, we haven’t discussed issues arising from the
    Hessian’s applicability, issues related to the Hessian being a positive definite
    matrix. A symmetric matrix is positive definite if all its eigenvalues are positive.
    Near saddle points, the Hessian might not be positive definite, which can cause
    the update rule to move away from the minimum. As you might expect with a simple
    algorithm like Newton’s method, some variations try to address issues like this,
    but even if problems with the eigenvalues of the Hessian are addressed, the computational
    burden of using the Hessian for updating network parameters is what stops Newton’s
    algorithm in its tracks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因。首先，我们没有讨论 Hessian 适用性带来的问题，尤其是 Hessian 作为正定矩阵的问题。一个对称矩阵是正定的，若它的所有特征值都为正。在鞍点附近，Hessian
    可能不是正定的，这会导致更新规则偏离最小值。正如你可能会预料的那样，像牛顿法这样的简单算法中，一些变体尝试解决这类问题，但即便解决了 Hessian 特征值的问题，使用
    Hessian 更新网络参数的计算负担也正是牛顿法无法继续进行的原因。
- en: Every time the network’s weights and biases are updated, the Hessian changes,
    requiring it and its inverse to be calculated again. Think of the number of minibatches
    used during network training. For even one minibatch, there are *k* parameters
    in the network, where *k* is easily in the millions to even billions. The Hessian
    is a *k* × *k* symmetric and positive definite matrix. Inverting the Hessian typically
    uses Cholesky decomposition, which is more efficient than other methods but is
    still an 𝒪(*k*³) algorithm. The *big-O* notation indicates that the algorithm’s
    resource use scales as the cube of the size of the matrix in time, memory, or
    both. This means doubling the number of parameters in the network increases the
    computational time to invert the Hessian by a factor of 2³ = 8 while tripling
    the number of parameters requires some 3³ = 27 times as much effort, and quadrupling
    some 4³ = 64 times as much. And this is to say nothing about storing the *k*²
    elements of the Hessian matrix, all floating-point values.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 每次网络的权重和偏置被更新时，Hessian 矩阵都会发生变化，需要重新计算它及其逆矩阵。考虑到网络训练过程中使用的小批量数据量，即使是一个小批量，网络中也有
    *k* 个参数，其中 *k* 很容易达到百万甚至数十亿级别。Hessian 是一个 *k* × *k* 的对称正定矩阵。求逆 Hessian 通常使用 Cholesky
    分解，这比其他方法更高效，但仍然是一个 𝒪(*k*³) 的算法。*大-O* 符号表示算法的资源消耗随着矩阵大小的立方（时间、内存或两者）而增长。这意味着如果网络中的参数数量翻倍，求逆
    Hessian 的计算时间将增加 2³ = 8 倍，而三倍数量的参数需要约 3³ = 27 倍的计算工作量，四倍则需要大约 4³ = 64 倍。而且这还没有提到存储
    Hessian 矩阵中的 *k*² 个元素，所有这些都是浮动的数值。
- en: The computation necessary to use Newton’s method with even modest deep networks
    is staggering. Gradient-based, first-order optimization methods are about all
    we can use for training neural networks.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于中等规模的深度网络，使用牛顿法的计算量也是惊人的。基于梯度的一阶优化方法几乎是我们训练神经网络时唯一可以使用的。
- en: '**NOTE**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*This statement is perhaps a bit premature. Recent work in the area of* neuroevolution
    *has demonstrated that evolutionary algorithms can successfully train deep models.
    My experimentation with swarm optimization techniques and neural networks lends
    credence to this approach as well.*'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*这个说法可能有些过早。最近在* 神经进化 *领域的研究表明，进化算法可以成功地训练深度模型。我自己在群体优化技术和神经网络方面的实验也为这种方法提供了证据。*'
- en: That first-order methods work as well as they do seems, for now, to be a very
    happy accident.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶方法如此有效，似乎目前为止是一个非常幸运的偶然。
- en: Some Examples of Matrix Calculus Derivatives
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵微积分导数的一些例子
- en: We conclude the chapter with some examples similar to the kinds of derivatives
    we commonly find in deep learning.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以一些类似于深度学习中常见导数的例子结束本章。
- en: Derivative of Element-Wise Operations
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 元素级操作的导数
- en: Let’s begin with the derivative of element-wise operations, which includes things
    like adding two vectors together. Consider
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从元素级操作的导数开始，包括像将两个向量相加这样的操作。考虑
- en: '![Image](Images/217equ01.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/217equ01.jpg)'
- en: which is the straightforward addition of two vectors, element by element. What
    does ∂***f***/∂***a***, the Jacobian of ***f***, look like? From the definition,
    we have
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是将两个向量按元素相加。那么，∂***f***/∂***a***，即 ***f*** 的雅可比矩阵，是什么样子的呢？根据定义，我们有
- en: '![Image](Images/218equ01.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/218equ01.jpg)'
- en: but *f*[0] only depends on *a*[0], while *f*[1] depends on *a*[1], and so on.
    Therefore, all derivatives ∂*f[i]*/∂*a[j]* for *i* ≠ *j* are zero. This removes
    all the off-diagonal elements of the matrix, leaving
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 *f*[0] 只依赖于 *a*[0]，而 *f*[1] 依赖于 *a*[1]，以此类推。因此，所有 ∂*f[i]*/∂*a[j]*，对于 *i*
    ≠ *j* 都是零。这就去除了矩阵的所有非对角元素，剩下的是
- en: '![Image](Images/218equ02.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/218equ02.jpg)'
- en: since ∂*f*[*i*]/∂*a*[*i*] = 1 for all *i*. Similarly, ∂***f***/∂***b*** = ***I***
    as well. Also, if we change from addition to subtraction, ∂***f***/∂***a*** =
    ***I***, but ∂***f***/∂***b*** = −***I***.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 ∂*f*[*i*]/∂*a*[*i*] = 1 对所有 *i* 都成立。类似地，∂***f***/∂***b*** = ***I***。此外，如果我们将加法改为减法，∂***f***/∂***a***
    = ***I***，但 ∂***f***/∂***b*** = −***I***。
- en: If the operator is element-wise multiplication of ***a*** and ***b***, ***f***
    = ***a*** ⊗ ***b***, then we get the following, where the diag(***x***) notation
    means the *n* elements of vector ***x*** along the diagonal of an *n* × *n* matrix
    that is zero elsewhere.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作是 ***a*** 和 ***b*** 的元素级乘法，***f*** = ***a*** ⊗ ***b***，那么我们得到如下结果，其中 diag(***x***)
    表示向量 ***x*** 的 *n* 个元素沿对角线排列在一个 *n* × *n* 的矩阵中，其他位置为零。
- en: '![Image](Images/218equ03.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/218equ03.jpg)'
- en: Derivative of the Activation Function
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 激活函数的导数
- en: Let’s find the derivative of the weights and bias value for a single node of
    a hidden layer in a feedforward network. Recall, the inputs to the node are the
    outputs of the previous layer, ***x***, multiplied term by term by the weights,
    ***w***, and summed along with the bias value, *b*, a scalar. The result, a scalar,
    is passed to the activation function to produce the output value for the node.
    Here, we’re using the *rectified linear unit (ReLU)* which returns its argument
    if the argument is positive. If the argument is negative, ReLU returns zero. We
    can write this process as
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出前馈网络中隐藏层单个节点的权重和偏置的导数。回顾一下，节点的输入是前一层的输出 ***x***，与权重 ***w*** 按项相乘并加上偏置值
    *b*（一个标量）。结果是一个标量，传递给激活函数以产生节点的输出值。在这里，我们使用的是 *修正线性单元（ReLU）*，如果输入为正，则返回该输入值；如果输入为负，则返回零。我们可以将这个过程写成
- en: '![Image](Images/08equ25.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ25.jpg)'
- en: In order to implement backpropagation, we need the derivatives of [Equation
    8.25](ch08.xhtml#ch08equ25) with respect to ***w*** and *b*. Let’s see how to
    find them.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现反向传播，我们需要[方程 8.25](ch08.xhtml#ch08equ25)对***w***和 *b* 的导数。让我们看看如何找到它们。
- en: We begin by considering the pieces of [Equation 8.25](ch08.xhtml#ch08equ25).
    For example, from [Equation 8.9](ch08.xhtml#ch08equ09), we know the derivative
    of the dot product with respect to ***w*** is
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑[方程 8.25](ch08.xhtml#ch08equ25)的各个部分。例如，从[方程 8.9](ch08.xhtml#ch08equ09)中，我们知道点积对***w***的导数是
- en: '![Image](Images/219equ01.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ01.jpg)'
- en: where we have taken advantage of the fact that the dot product is commutative,
    ***w*** • ***x*** = ***x*** • ***w***. Also, since *b* does not depend on ***w***,
    we have
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们利用了点积是交换律的事实，***w*** • ***x*** = ***x*** • ***w***。另外，由于 *b* 不依赖于 ***w***，我们有
- en: '![Image](Images/08equ26.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ26.jpg)'
- en: What about the derivative of ReLU? By definition,
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 ReLU 的导数是什么样子的呢？根据定义，
- en: '![Image](Images/219equ02.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ02.jpg)'
- en: implying that
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '![Image](Images/08equ27.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/08equ27.jpg)'
- en: since ∂*z*/∂*z* = 1.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 ∂*z*/∂*z* = 1。
- en: To find the derivatives of [Equation 8.25](ch08.xhtml#ch08equ25) with respect
    to ***w*** and *b*, we need the chain rule and the results above. Let’s start
    with ***w***. The chain rule tells us how
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到[方程 8.25](ch08.xhtml#ch08equ25)对***w***和 *b* 的导数，我们需要链式法则和上面的结果。我们从 ***w***
    开始。链式法则告诉我们如何
- en: '![Image](Images/219equ03.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ03.jpg)'
- en: with *z* = ***w*** • ***x*** + *b* and *y* = ReLU(*z*).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = ***w*** • ***x*** + *b* 和 *y* = ReLU(*z*)。'
- en: We know ∂*y*/∂*z*; it’s the two cases above for the ReLU, [Equation 8.27](ch08.xhtml#ch08equ27).
    So now we have
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 ∂*y*/∂*z*；它是 ReLU 的上述两种情况，[方程 8.27](ch08.xhtml#ch08equ27)。所以现在我们有
- en: '![Image](Images/219equ04.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/219equ04.jpg)'
- en: and we know ∂*z*/∂***w*** = ***x***^⊤; it’s [Equation 8.26](ch08.xhtml#ch08equ26).
    Therefore, our final result is
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 ∂*z*/∂***w*** = ***x***^⊤；它是[方程 8.26](ch08.xhtml#ch08equ26)。因此，我们的最终结果是
- en: '![Image](Images/220equ01.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/220equ01.jpg)'
- en: where we’ve replaced *z* with ***w*** • ***x*** + *b*.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将 *z* 替换为 ***w*** • ***x*** + *b*。
- en: We follow much the same procedure to find ∂*y*/∂*b*, as
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用相同的程序来找到 ∂*y*/∂*b*，如
- en: '![Image](Images/220equ02.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/220equ02.jpg)'
- en: but ∂*y*/∂*z* is 0 or 1, depending on the *z*’s sign. Likewise, ∂*z*/∂*b* =
    1, which leads to
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 但 ∂*y*/∂*z* 是 0 或 1，取决于 *z* 的符号。同样，∂*z*/∂*b* = 1，这导致
- en: '![Image](Images/220equ03.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![Image](Images/220equ03.jpg)'
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this dense chapter, we learned about matrix calculus, including working with
    derivatives of functions involving vectors and matrices. We worked through the
    definitions and discussed some identities. We then introduced the Jacobian and
    Hessian matrices as analogs for first and second derivatives and learned how to
    use them in optimization problems. Training a deep neural network is, fundamentally,
    an optimization problem, so the potential utility of the Jacobian and Hessian
    is clear, even if the latter can’t be easily used for large neural networks. We
    ended the chapter with some examples for derivatives of expressions found in deep
    learning.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一密集章节中，我们学习了矩阵微积分，包括处理涉及向量和矩阵的函数的导数。我们通过了定义并讨论了一些恒等式。然后，我们引入了雅可比矩阵和海森矩阵，作为一阶和二阶导数的类比，并学习了如何在优化问题中使用它们。训练深度神经网络，实质上是一个优化问题，因此雅可比矩阵和海森矩阵的潜在用途显而易见，即使后者在大型神经网络中不易使用。我们以一些深度学习中常见的导数表达式为例，结束了本章。
- en: This concludes the general mathematics portion of the book. We’ll now turn our
    attention to using what we’ve learned to understand the workings of deep neural
    networks. Let’s begin with a discussion of how data flows through a neural network
    model.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的数学部分到此为止。接下来，我们将把注意力转向利用所学内容理解深度神经网络的工作原理。我们从讨论数据如何在神经网络模型中流动开始。
