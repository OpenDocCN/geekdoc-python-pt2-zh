- en: '**4'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4'
- en: EXTRACTING AND USING LINGUISTIC FEATURES**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 提取和使用语言特征**
- en: '![Image](../Images/comm1.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/comm1.jpg)'
- en: In the previous chapters, you learned how to access linguistic features, such
    as part-of-speech tags, syntactic dependencies, and named entities, as part of
    the text processing pipeline. This chapter will show you how to use part-of-speech
    tags and syntactic dependency labels to extract and generate text, allowing you
    to build question-asking chatbots, locate specific phrases in a text, and more.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你已经学习了如何访问语言特征，如词性标签、句法依赖关系和命名实体，这些都是文本处理管道的一部分。本章将展示如何使用词性标签和句法依赖标签来提取和生成文本，从而帮助你构建提问的聊天机器人、在文本中定位特定短语等。
- en: Almost every NLP application needs to extract specific information from a text
    and generate new text that is relevant to a particular situation. For example,
    a chatbot must be able carry on a conversation with a user, which means it must
    be able to identify specific parts of a user’s text and then generate its own
    appropriate response. Let’s look at how to do all of that using linguistic features.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每一个自然语言处理（NLP）应用都需要从文本中提取特定的信息，并生成与特定情境相关的新文本。例如，一个聊天机器人必须能够与用户进行对话，这意味着它必须能够识别用户文本中的特定部分，然后生成相应的回复。让我们来看看如何利用语言特征做到这一切。
- en: '**Extracting and Generating Text with Part-of-Speech Tags**'
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用词性标签提取和生成文本**'
- en: Part-of-speech tags can help you retrieve specific kinds of information from
    a text, and they can also help you generate entirely new sentences based on a
    submitted one. In this section, we’ll introduce you to some new part-of-speech
    tags, write a script that finds phrases describing amounts of money, and transform
    statements into questions. For a list of common part-of-speech tags used in spaCy
    for English models, refer to [Table 2-1](../Text/ch02.xhtml#ch02tab01) on [page
    22](../Text/ch02.xhtml#page_22).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标签可以帮助你从文本中提取特定的信息，它们还可以帮助你基于给定的句子生成全新的句子。在这一部分，我们将介绍一些新的词性标签，编写一个脚本来查找描述金额的短语，并将陈述句转换成疑问句。有关
    spaCy 中用于英语模型的常见词性标签的列表，请参阅[表 2-1](../Text/ch02.xhtml#ch02tab01)在[第 22 页](../Text/ch02.xhtml#page_22)上的内容。
- en: '***Numeric, Symbolic, and Punctuation Tags***'
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***数字、符号和标点标签***'
- en: 'In addition to part-of-speech tags for nouns, verbs, and other words in a sentence,
    spaCy has tags for symbols, numbers, and punctuation marks. Let’s look at these
    by processing the following sentence:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了句子中名词、动词和其他词的词性标签外，spaCy 还为符号、数字和标点符号提供了标签。让我们通过处理以下句子来看看这些标签：
- en: The firm earned $1.5 million in 2017.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司在2017年赚取了150万美元。
- en: 'To begin, let’s extract the coarse-grained part-of-speech features from the
    tokens in the sentence to see how spaCy distinguishes between different part-of-speech
    categories. We can do this with the following script:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从句子中的词元提取粗粒度的词性特征，以查看 spaCy 如何区分不同的词性类别。我们可以使用以下脚本来实现：
- en: '>>> import spacy'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> import spacy'
- en: '>>> nlp = spacy.load(''en'')'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> nlp = spacy.load(''en'')'
- en: '>>> doc = nlp(u"The firm earned $1.5 million in 2017.")'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u"该公司在2017年赚取了150万美元。")'
- en: '>>> for token in doc:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for token in doc:'
- en: '...   print(token.text, ➊token.pos_, ➋spacy.explain(token.pos_))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '...   print(token.text, ➊token.pos_, ➋spacy.explain(token.pos_))'
- en: '...'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: We create a Doc object for the submitted sentence and then output the coarse-grained
    part-of-speech tags ➊. We also use the spacy.explain() function, which returns
    a description for a given linguistic feature ➋.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为提交的句子创建了一个 Doc 对象，然后输出粗粒度的词性标签 ➊。我们还使用了 spacy.explain() 函数，它会返回给定语言特征的描述
    ➋。
- en: 'The output should look as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果应该如下所示：
- en: The     DET   determiner
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: The     DET   限定词
- en: firm    NOUN  noun
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: firm    NOUN  名词
- en: earned  VERB  verb
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: earned  VERB  动词
- en: $       SYM   symbol
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $       SYM   符号
- en: 1.5     NUM   numeral
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 1.5     NUM   数字
- en: million NUM   numeral
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: million NUM   数字
- en: in      ADP   adposition
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: in      ADP   介词
- en: 2017    NUM   numeral
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2017    NUM   数字
- en: .       PUNCT punctuation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: .       PUNCT 标点符号
- en: Notice that the coarse-grained tagger distinguishes numerals, symbols, and punctuation
    marks as individual categories. As you can see, it even recognizes “million” spelled
    out.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，粗粒度标签器将数字、符号和标点符号作为独立类别进行区分。正如你所看到的，它甚至能够识别出“million”这个单词的拼写。
- en: 'Now, for the sake of comparison, let’s output both coarse-grained and fine-grained
    part-of-speech tags for this sample sentence along with a description column for
    the fine-grained tags:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了进行比较，我们将输出这个示例句子的粗粒度和细粒度词性标签，并附上细粒度标签的描述列：
- en: '>>> for token in doc:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for token in doc:'
- en: '...   print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '...   print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))'
- en: 'The output should look as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: The     DET   DT  determiner
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: The     DET   DT  限定词
- en: firm    NOUN  NN  noun, singular
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: firm    NOUN  NN  名词，单数
- en: earned  VERB  VBD verb, past tense
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: earned  VERB  VBD  动词，过去式
- en: $       SYM   $   symbol, currency
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: $       SYM   $   符号，货币
- en: 1.5     NUM   CD  cardinal number
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1.5     NUM   CD  基数词
- en: million NUM   CD  cardinal number
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: million NUM   CD  基数词
- en: in      ADP   IN  conjunction, subordinating or preposition
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: in      ADP   IN  连词，附属或介词
- en: 2017    NUM   CD  cardinal number
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 2017    NUM   CD  基数词
- en: .       PUNCT .   punctuation mark, sentence closer
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: .       PUNCT .   标点符号，句末符号
- en: The second and third columns contain the coarse-grained and fine-grained part-of-speech
    tags, respectively. The fourth column gives descriptions of the fine-grained tags
    provided in the third column.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第二列和第三列分别包含粗粒度和细粒度的词性标签。第四列给出了第三列中细粒度标签的描述。
- en: 'The fine-grained tagging divides each category into subcategories. For example,
    the coarse-grained category SYM (symbols) has three fine-grained subcategories.
    These are $ for currency symbols, # for the number sign, and SYM for all the other
    symbols, such as +, −, ×, ÷, =. This sub-dividing can be useful when you need
    to distinguish between different types of symbols. For example, you might be processing
    articles about math and want your script to recognize symbols commonly found in
    math formulas. Or you might be writing a script that needs to recognize currency
    symbols in financial reports.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度标注将每个类别划分为子类别。例如，粗粒度类别 SYM（符号）有三个细粒度子类别。它们分别是：$表示货币符号，#表示数字符号，SYM表示所有其他符号，如
    +、−、×、÷、=。这种细分在你需要区分不同类型的符号时非常有用。例如，你可能正在处理数学文章，想让脚本识别数学公式中常见的符号。或者，你可能在编写一个脚本，需要识别财务报告中的货币符号。
- en: '**NOTE**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '*Because spaCy’s part-of-speech tagger relies on a token’s context to generate
    its label, you might get different labels for tokens used in unusual contexts.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*因为 spaCy 的词性标注器依赖于标记的上下文来生成其标签，所以在不寻常的上下文中，你可能会得到不同的标签。*'
- en: Now let’s look at how we can take advantage of these specific part-of-speech
    tags to extract and generate text.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何利用这些特定的词性标签来提取和生成文本。
- en: '***Extracting Descriptions of Money***'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***提取货币描述***'
- en: Suppose you’re developing an application for processing financial reports that
    must extract necessary pieces of information from long, boring texts. In practice,
    financial reports can be quite large, but all you really need are the figures.
    In particular, you’re interested in phrases that refer to an amount of money and
    start with a currency symbol. For example, your script should pick out the phrase
    “$1.5 million” from the previous sample sentence, but not “2017”.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在开发一个用于处理财务报告的应用程序，该程序必须从冗长、无聊的文本中提取必要的信息。在实践中，财务报告可能非常庞大，但你真正需要的只是数字。特别是，你感兴趣的是指代金额的短语，并且这些短语以货币符号开头。例如，你的脚本应该从前面的示例句子中提取出短语“$1.5百万”，而不是“2017”。
- en: 'The following script illustrates how you might extract this phrase from the
    sentence, relying on the tokens’ part-of-speech tags only. You can save this script
    to a file and then run it or execute the code from within a Python session:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本演示了如何仅依赖标记的词性标签从句子中提取这个短语。你可以将这个脚本保存到文件中，然后运行它，或者在 Python 会话中执行代码：
- en: import spacy
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc = nlp(u"The firm earned $1.5 million in 2017.")
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u"公司在2017年赚取了150万美元。")
- en: phrase = ''
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: phrase = ''
- en: '➊ for token in doc:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ for token in doc:'
- en: '➋ if token.tag_ == ''$'':'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ if token.tag_ == ''$'':'
- en: phrase = token.text
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: phrase = token.text
- en: i = token.i+1
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: i = token.i+1
- en: '➌ while doc[i].tag_ == ''CD'':'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '➌ while doc[i].tag_ == ''CD'':'
- en: phrase += doc[i].text + ' '
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: phrase += doc[i].text + ' '
- en: i += 1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: i += 1
- en: ➍ break
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ break
- en: phrase = phrase[:-1]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: phrase = phrase[:-1]
- en: print(phrase)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: print(phrase)
- en: We iterate over the sentence’s tokens ➊, searching for a token whose fine-grained
    part-of-speech tag is $ ➋. This tag indicates a currency symbol, and it typically
    starts a phrase that refers to an amount of money. Once we find a currency symbol,
    we start composing the phrase by checking whether the tokens that follow the currency
    symbol in the sentence are numbers. To do this, we implement a while loop in which
    we pick up the tokens located to the right of the currency symbol and check them
    for the CD tag, which is the cardinal number fine-grained part-of-speech-tag ➌.
    When we reach a nonnumeric token, we quit the while loop and break the for loop
    ➍ that iterates over the sentence’s tokens.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历句子的每个词 ➊，搜索一个精细词性标签为$的词 ➋。这个标签表示一个货币符号，通常开始一个表示金额的短语。一旦找到货币符号，我们就开始通过检查紧随其后的词是否为数字来构建短语。为此，我们实现了一个
    while 循环，在该循环中我们取出位于货币符号右侧的词，并检查它们是否有CD标签，这个标签是基数数字的精细词性标签 ➌。当我们遇到一个非数字词时，就退出
    while 循环并中断遍历句子词的 for 循环 ➍。
- en: 'When we run the script, the output should look as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行脚本时，输出应如下所示：
- en: $1.5 million
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $1.5 million
- en: This is exactly the kind of output we are looking for.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们想要的输出结果。
- en: Keep in mind that a currency symbol assigned to the $ fine-grained part-of-speech
    tag might not necessarily be “$”. The part-of-speech tag might label other common
    currency symbols, such as £ and €. For example, the preceding script would recognize
    the phrase “£1.500.000”.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，分配给$的精细词性标签不一定是“$”。这个词性标签可能会标记其他常见的货币符号，比如£和€。例如，前面的脚本会识别短语“£1.500.000”。
- en: '***Try This***'
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: 'We wrote this script to extract a single phrase referring to an amount of money
    from the submitted sentence. Once the script finds the phrase, it completes its
    execution. But in practice, you might have a sentence that has more than one such
    phrase, as in the following example: “The firm earned $1.5 million in 2017, in
    comparison with $1.2 million in 2016.”'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了这个脚本，以从提交的句子中提取一个表示金额的短语。一旦脚本找到了该短语，它就会完成执行。但在实际应用中，可能会有包含多个此类短语的句子，如以下示例：“公司在2017年赚取了150万美元，与2016年相比，赚取了120万美元。”
- en: Modify the script so it extracts every phrase that refers to an amount of money
    within a sentence. To accomplish this, remove the break statement to prevent the
    loop from ending after it finds the first occurrence of the phrase of interest.
    Then move the code that’s responsible for preparing and printing a found phrase
    (the last two lines of the script) into the loop, so you can invoke these two
    lines for every phrase of interest found in the submitted sentence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 修改脚本，使其能够提取句子中每个表示金额的短语。为了实现这一点，移除 break 语句，以防止在找到第一个感兴趣的短语后就结束循环。然后将负责准备和打印已找到短语的代码（脚本的最后两行）移动到循环中，这样就可以为每个在提交句子中找到的感兴趣的短语调用这两行代码。
- en: '***Turning Statements into Questions***'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***将陈述转化为问题***'
- en: Suppose your NLP application must be able to generate a question from a submitted
    statement. For example, one way chatbots maintain conversations with the user
    is by asking the user a confirmatory question. When a user says, “I am sure,”
    the chatbot might ask something like, “Are you really sure?” To do this, the chatbot
    must be able to generate a relevant question.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的 NLP 应用必须能够根据提交的陈述生成一个问题。例如，聊天机器人维持与用户对话的一种方式是通过向用户提出确认性问题。当用户说“我确定”时，聊天机器人可能会问类似“你真的确定吗？”的问题。为了做到这一点，聊天机器人必须能够生成相关的问题。
- en: 'Let’s say the user’s submitted sentence is this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户提交的句子是这样的：
- en: I can promise it is worth your time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以保证这值得你花时间。
- en: 'This sentence contains several verbs and pronouns, each with different morphologies.
    To see this more clearly, let’s look at the part-of-speech tags spaCy assigned
    to the tokens in this sentence:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子包含了多个动词和代词，每个词的形态不同。为了更清楚地查看这一点，我们来看一下 spaCy 为这个句子中的每个词分配的词性标签：
- en: '>>> doc = nlp(u"I can promise it is worth your time.")'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u"我可以保证这值得你花时间。")'
- en: '>>> for token in doc:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for token in doc:'
- en: '...   print(token.text, token.pos_, token.tag_)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '...   print(token.text, token.pos_, token.tag_)'
- en: '...'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: 'We print the tokens, their coarse-grained part-of-speech tags, and their fine-grained
    part-of-speech-tags, producing the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打印出这些词、它们的粗粒度词性标签以及精细词性标签，生成以下输出：
- en: I       PRON PRP
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: I       代词  PRP
- en: can     VERB MD
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: can     动词  MD
- en: promise VERB VB
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: promise 动词  VB
- en: it      PRON PRP
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: it      代词  PRP
- en: is      VERB VBZ
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: is      动词  VBZ
- en: worth   ADJ  JJ
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: worth   形容词  JJ
- en: your    ADJ  PRP$
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: your    ADJ  PRP$
- en: time    NOUN NN
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: time    NOUN NN
- en: .      PUNCT .
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: .      PUNCT .
- en: From the fine-grained part-of-speech tags, you can distinguish between the morphological
    categories of the verbs and pronouns present in the sentence. For example, the
    fine-grained part-of-speech tag PRP marks personal pronouns and PRP$ marks possessive
    pronouns, allowing you to distinguish between these two types of pronouns programmatically.
    We’ll need this information when working on this example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从细粒度的词性标签中，你可以区分句子中动词和代词的形态类别。例如，细粒度的词性标签PRP标记个人代词，而PRP$标记所有格代词，这使你能够在程序中区分这两种代词。我们在处理这个示例时需要这些信息。
- en: 'A confirmatory question to the sentence discussed here might be as follows
    (another statement would require another confirmatory question, of course):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里讨论的句子，确认问题可能如下所示（当然，另一个陈述需要另一个确认问题）：
- en: Can you really promise it is worth my time?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Can you really promise it is worth my time?
- en: 'From a human perspective, forming this question from the statement looks pretty
    straightforward: you change the order of some words, alter the pronouns accordingly,
    and add the adverbial modifier “really” to the main verb (the one that comes right
    after the subject). But how can you accomplish all these operations programmatically?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类的角度来看，从陈述中形成这个问题看起来相当简单：你只需更改一些单词的顺序，相应地修改代词，并将副词修饰语“really”添加到主谓之间（即紧跟主语后的动词）。但如何以编程的方式实现这些操作呢？
- en: Let’s look at some part-of-speech tags. In the sample sentence, the verbs involved
    in forming the question are “can” and “promise”. The fine-grained part-of-speech
    tags mark the first one, “can”, as a modal auxiliary verb and the second one as
    a verb in the base form. Notice that in the preceding confirmatory question, the
    modal auxiliary verb has switched places with the personal pronoun, a process
    called *inversion*. We’ll have to implement this in the script.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些词性标签。在示例句子中，构成问题的动词是“can”和“promise”。细粒度的词性标签将第一个动词“can”标记为情态助动词，将第二个动词“promise”标记为基本形式的动词。请注意，在前面的确认问题中，情态助动词与个人代词交换了位置，这个过程叫做*倒装*。我们需要在脚本中实现这一点。
- en: When it comes to the pronouns, the chatbot should follow a pattern common to
    regular conversations. [Table 4-1](../Text/ch04.xhtml#ch04tab01) summarizes the
    use of pronouns in such an application.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及代词时，聊天机器人应遵循常见的对话模式。[表 4-1](../Text/ch04.xhtml#ch04tab01)总结了在此类应用中代词的使用。
- en: '**Table 4-1:** The Use of Pronouns in a Chatbot'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 4-1：** 聊天机器人中代词的使用'
- en: '|  | **Personal pronouns** | **Possessive pronouns** |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | **个人代词** | **所有格代词** |'
- en: '| --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| chatbot | I, me | my, mine |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| chatbot | I, me | my, mine |'
- en: '| user | you | your, yours |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| user | you | your, yours |'
- en: In other words, a chatbot refers to itself as “I” or “me,” and it refers to
    a user as “you.”
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，聊天机器人会将自己称为“I”或“me”，并将用户称为“you”。
- en: 'The following steps outline what we need to do to generate a question from
    the original statement:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了我们需要做什么，以便从原始陈述中生成一个问题：
- en: Change the order of words in the original sentence from “subject + modal auxiliary
    verb + infinitive verb” to “modal auxiliary verb + subject + infinitive verb.”
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始句子中的单词顺序从“主语 + 情态助动词 + 不定式动词”更改为“情态助动词 + 主语 + 不定式动词”。
- en: Replace the personal pronoun “I” (the sentence’s subject) with “you.”
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将个人代词“I”（句子的主语）替换为“you”。
- en: Replace the possessive pronoun “your” with “my.”
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有格代词“your”替换为“my”。
- en: Place the adverbial modifier “really” before the verb “promise” to emphasize
    the latter.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将副词修饰语“really”放在动词“promise”之前，以强调后者。
- en: Replace the punctuation mark “.” with “?” at the end of the sentence.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将句末的标点符号“.”替换为“?”。
- en: 'The following script implements these steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本实现了这些步骤：
- en: import spacy
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: nlp = spacy.load('en')
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: doc = nlp(u"I can promise it is worth your time.")
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(u"我可以保证这值得你的时间。")
- en: sent = ''
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: sent = ''
- en: 'for i,token in enumerate(doc):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,token in enumerate(doc):'
- en: '➊ if token.tag_ == ''PRP'' and doc[i+1].tag_ == ''MD'' and doc[i+2].tag_ ==
    ''VB'':'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ if token.tag_ == ''PRP'' and doc[i+1].tag_ == ''MD'' and doc[i+2].tag_ ==
    ''VB'':'
- en: ➋ sent = doc[i+1].text.capitalize() + ' ' + doc[i].text
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ sent = doc[i+1].text.capitalize() + ' ' + doc[i].text
- en: sent = sent + ' ' + ➌doc[i+2:].text
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: sent = sent + ' ' + ➌doc[i+2:].text
- en: ➍ break
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ break
- en: '#By now, you should have: ''Can I promise it is worth your time.'''
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#到现在为止，你应该已经有了：“Can I promise it is worth your time.”'
- en: '#Retokenization'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#重标记'
- en: ➎ doc=nlp(sent)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ doc=nlp(sent)
- en: 'for i,token in enumerate(doc):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,token in enumerate(doc):'
- en: '➏ if token.tag_ == ''PRP'' and token.text == ''I'':'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '➏ if token.tag_ == ''PRP'' and token.text == ''I'':'
- en: sent = doc[:i].text + ' you ' + doc[i+1:].text
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: sent = doc[:i].text + ' you ' + doc[i+1:].text
- en: break
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '#By now, you should have: ''Can you promise it is worth your time.'''
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#到此为止，你应该得到：''Can you promise it is worth your time.'''
- en: doc=nlp(sent)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: doc=nlp(sent)
- en: 'for i,token in enumerate(doc):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,token in enumerate(doc):'
- en: '➐ if token.tag_ == ''PRP$'' and token.text == ''your'':'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '➐ if token.tag_ == ''PRP$'' and token.text == ''your'':'
- en: sent = doc[:i].text + ' my ' + doc[i+1:].text
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: sent = doc[:i].text + ' my ' + doc[i+1:].text
- en: break
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '#By now, you should have: ''Can you promise it is worth my time.'''
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '#到此为止，你应该得到：''Can you promise it is worth my time.'''
- en: doc=nlp(sent)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: doc=nlp(sent)
- en: 'for i,token in enumerate(doc):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,token in enumerate(doc):'
- en: 'if token.tag_ == ''VB'':'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 'if token.tag_ == ''VB'':'
- en: ➑ sent = doc[:i].text + ' really ' + doc[i:].text
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ sent = doc[:i].text + ' really ' + doc[i:].text
- en: break
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '#By now, you should have: ''Can you really promise it is worth my time.'''
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '#到此为止，你应该得到：''Can you really promise it is worth my time.'''
- en: doc=nlp(sent)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: doc=nlp(sent)
- en: ➒ sent = doc[:len(doc)-1].text + '?'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ sent = doc[:len(doc)-1].text + '?'
- en: '#Finally, you should have: ''Can you really promise it is worth my time?'''
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#最后，你应该得到：''Can you really promise it is worth my time?'''
- en: print(sent)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: print(sent)
- en: We perform the first four steps in separate for loops. First, we iterate over
    the tokens in the sentence and change the order of the subject and verb to make
    the sentence a question. In this example, we’re looking for the modal auxiliary
    verb (tagged MD) that follows a personal pronoun and is followed by an infinitive
    verb ➊. Once we find this sequence of words, we move the modal auxiliary verb
    immediately before the personal pronoun, placing it at the beginning of the sentence
    ➋.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前四个步骤分开执行。首先，我们遍历句子中的词元并改变主语和动词的顺序，使句子变为一个疑问句。在这个例子中，我们正在寻找跟在人称代词后面的情态助动词（标记为
    MD），并且后面跟着一个不定式动词 ➊。找到这一系列词后，我们将情态助动词移到人称代词之前，放在句子的开头 ➋。
- en: To compose a new sentence, we use a technique known in Python as *slicing* that
    allows us to extract a subsequence from a sequence object, such as a string or
    a list, by specifying the start and end indices. In this case, we can apply slicing
    to a Doc object to extract a given subsequence of tokens from it. For example,
    slice doc[2:] will contain the doc’s tokens starting from the token at index 2
    through the end of the doc, which in this case, is “promise it is worth your time.”
    ➌. Once we move the modal verb to a new position, we exit the for loop ➍.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构造一个新句子，我们使用在 Python 中称为 *切片* 的技术，它允许我们通过指定开始和结束索引，从序列对象中提取子序列，比如字符串或列表。在这个例子中，我们可以对
    Doc 对象应用切片，从中提取出给定的词元子序列。例如，切片 doc[2:] 将包含从索引 2 开始直到文档结尾的词元，在这种情况下，就是“promise
    it is worth your time.” ➌。一旦我们将情态动词移动到新位置，我们就退出 for 循环 ➍。
- en: You might wonder why we don’t just use the personal pronoun and auxiliary modal
    verb’s indices to perform inversion. Because we know the personal pronoun is at
    index 0 and the modal verb is at index 1, why do we have to use a loop that iterates
    over the entire set of tokens to find the modal verb’s position? Won’t the verb
    always follow the subject and so be the second word in the sentence?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么我们不直接使用人称代词和助动情态动词的索引来执行倒装呢？因为我们知道人称代词在索引 0 处，情态动词在索引 1 处，为什么还要使用一个遍历所有词元的循环来找到情态动词的位置呢？动词不总是跟随主语并且是句子中的第二个词吗？
- en: The fact is that a sentence doesn’t always start with the subject. For example,
    what if the sentence were “Sure enough, I can promise it is worth your time.”?
    In that case, the script would know to omit the first two words and start processing
    with the subject.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，句子并不总是以主语开始。例如，如果句子是 “Sure enough, I can promise it is worth your time.”
    呢？在这种情况下，脚本会知道省略前两个词，并从主语开始处理。
- en: As a result of the inversion, we get the new sentence as a string. To make this
    sentence available for further processing, we need to obtain a Doc object for
    it ➎.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于倒装，我们得到了新的句子作为字符串。为了让这个句子可用于进一步处理，我们需要为其获取一个 Doc 对象 ➎。
- en: Next, we create a new for loop that will replace the personal pronoun “I” with
    the personal pronoun “you.” To do this, we search for personal pronouns (tagged
    PRP). If the personal pronoun is “I,” we replace it with “you” ➏. Then we quit
    the for loop.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个新的 for 循环，将人称代词 “I” 替换为人称代词 “you”。为此，我们搜索人称代词（标记为 PRP）。如果人称代词是 “I”，我们将其替换为
    “you” ➏。然后我们退出 for 循环。
- en: We repeat this process to replace the possessive pronoun “your” with “my” by
    searching for the PRP$ tag ➐. Then, in a new for loop, we find a verb in the infinitive
    form and insert the adverbial modifier “really” before it ➑.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程，通过搜索PRP$标签 ➐来将所有物主代词“your”替换为“my”。然后，在一个新的for循环中，我们找到不定式形式的动词，并在它前面插入副词修饰语“really”
    ➑。
- en: Finally, we replace the sentence’s period with a question mark. This is the
    only step where we don’t need to use a loop. The reason is that in all possible
    sentences, the period and the question mark go at the end of a sentence, so we
    can reliably find them using their indices with len(doc)-1 ➒.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将句子的句号替换为问号。这是唯一一个不需要使用循环的步骤。原因是，在所有可能的句子中，句号和问号都位于句子的末尾，因此我们可以通过使用它们的索引来可靠地找到它们，方法是len(doc)-1
    ➒。
- en: 'When we run this code, we should get the following output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，我们应该得到以下输出：
- en: Can you really promise it is worth my time?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你真的能保证这是值得我花时间做的吗？
- en: 'This script is a good start, but it won’t work with every submitted statement.
    For example, the statement might contain a personal pronoun other than “I,” but
    our script doesn’t explicitly check for that. Also, some sentences don’t contain
    auxiliary verbs, like the sentence “I love eating ice cream.” In those cases,
    we’d have to use the word “do” to form the question instead of a word like “can”
    or “should,” like this: “Do you really love eating ice cream?” But if the sentence
    contains the verb “to be,” as in the sentence “I am sleepy,” we’d have to move
    that verb to the front, like this: “Are you sleepy?”'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这段脚本是一个不错的开始，但它不能适用于每个提交的句子。例如，句子可能包含“我”以外的其他人称代词，但我们的脚本并没有明确检查这一点。此外，一些句子不包含助动词，比如“我喜欢吃冰淇淋”这个句子。在这些情况下，我们必须使用“do”来构成疑问句，而不是像“can”或“should”这样的词，像这样：“你真的喜欢吃冰淇淋吗？”但是，如果句子中包含动词“to
    be”，例如“我困了”，我们就必须将动词移到句首，像这样：“你困吗？”
- en: A real implementation of this chatbot would have to be able to choose the appropriate
    option for a submitted sentence. You’ll see a “do” example in “[Deciding What
    Question a Chatbot Should Ask](../Text/ch04.xhtml#lev53)” on [page 56](../Text/ch04.xhtml#page_56).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个聊天机器人的真实实现必须能够为提交的句子选择合适的选项。你将在[“确定聊天机器人应提问的问题”](../Text/ch04.xhtml#lev53)中看到一个“do”示例，位于[第56页](../Text/ch04.xhtml#page_56)。
- en: '***Try This***'
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试看***'
- en: Examining the script from “[Turning Statements into Questions](../Text/ch04.xhtml#lev49)”,
    you might notice that some blocks of code in it look very similar, containing
    repetitive operations. In every step, you make a replacement in the sentence and
    then re-tokenize it. That means you might try to generalize the code, putting
    repetitive operations in a single function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细检查[“将陈述句转化为疑问句”](../Text/ch04.xhtml#lev49)中的脚本，你可能会注意到其中一些代码块看起来非常相似，包含重复的操作。在每个步骤中，你都会对句子进行替换，然后重新标记。这意味着你可能需要尝试将代码进行概括，将重复的操作放入一个单一的函数中。
- en: Before writing such a function, take some time to understand what parameters
    it will need to take to perform the text-manipulation operations you see in the
    script. In particular, you’ll need to explicitly specify what token you’re searching
    for and what operation you want to perform on it by either replacing it with another
    token or adding a token before it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写这样的函数之前，花些时间理解它需要哪些参数才能执行你在脚本中看到的文本处理操作。特别是，你需要明确指定你要搜索的标记，以及你希望对其执行的操作，无论是将其替换为另一个标记，还是在它之前添加一个标记。
- en: Once you define this function, you can write the main code that invokes it,
    implementing the same functionality as the original script.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了这个函数，你就可以编写调用它的主代码，实现与原始脚本相同的功能。
- en: '**Using Syntactic Dependency Labels in Text Processing**'
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**在文本处理中过滤句法依赖标签**'
- en: As you learned in “[Extracting and Generating Text with Part-of-Speech Tags](../Text/ch04.xhtml#lev45)”
    on [page 48](../Text/ch04.xhtml#page_48), part-of-speech tags are a powerful tool
    for smart text processing. But in practice, you might need to know more about
    a sentence’s tokens to process it intelligently.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[“通过词性标记提取和生成文本”](../Text/ch04.xhtml#lev45)中学习的那样，词性标记是智能文本处理的有力工具。但在实践中，你可能需要了解更多关于句子的标记，以便智能地处理它。
- en: For example, you might need to know whether a personal pronoun is the subject
    of a sentence or a grammatical object. Sometimes, this task is easy. The personal
    pronouns “I,” “he,” “she,” “they,” and “we” will almost always be the subject.
    When used as an object, “I” turns into “me,” as in “A postman brought me a letter.”
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可能需要知道一个人称代词是句子的主语还是语法上的宾语。有时，这个任务很简单。人称代词“I”，“he”，“she”，“they”和“we”几乎总是主语。作为宾语时，“I”变成了“me”，例如在句子“A
    postman brought me a letter.”中。
- en: 'But this might not be as clear when it comes to some other personal pronouns,
    such as “you” or “it,” which look the same whether they’re used as subjects or
    objects. Consider the following two sentences: “I know you. You know me.” In the
    first sentence, “you” is the direct object of the verb “know.” In the second sentence,
    “you” is the verb’s subject.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 但当涉及到其他一些人称代词时，情况可能不那么明确，例如“you”或“it”，这些代词无论作为主语还是宾语时形式都相同。考虑以下两个句子：“I know
    you. You know me.” 在第一个句子中，“you”是动词“know”的直接宾语。在第二个句子中，“you”是动词的主语。
- en: Let’s solve this problem using syntactic dependency labels and part-of-speech
    tags. Then we’ll apply syntactic dependency labels to build a better version of
    the question-asking chatbot.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用句法依存关系标签和词性标签来解决这个问题。然后，我们将应用句法依存关系标签来构建一个更好的问答聊天机器人。
- en: '***Distinguishing Subjects from Objects***'
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***区分主语和宾语***'
- en: To programmatically determine the role of a pronoun like “you” or “it” in a
    given sentence, you need to check the dependency label assigned to it. By using
    part-of-speech tags in conjunction with dependency labels, you can get much more
    information about the tokens of a sentence.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要程序化地确定像“you”或“it”这样的代词在特定句子中的角色，你需要检查分配给它的依存关系标签。通过结合使用词性标签和依存关系标签，你可以获得更多关于句子中单词的信息。
- en: 'Let’s return to the sentence in the previous example and look at the results
    of the dependency parsing performed on it:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到前面的句子，看看对它进行依存句法分析后的结果：
- en: '>>> doc = nlp(u"I can promise it is worth your time.")'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> doc = nlp(u"我可以保证这值得你的时间。")'
- en: '>>> for token in doc:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> for token in doc:'
- en: '...   print(token.text, token.pos_, token.tag_, token.dep_, spacy.explain(token.dep_))'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '...   print(token.text, token.pos_, token.tag_, token.dep_, spacy.explain(token.dep_))'
- en: 'We extract the part-of-speech tags, the dependency labels, and the description
    for the dependency labels:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取词性标签、依存关系标签以及依存关系标签的描述：
- en: I       PRON  PRP  nsubj     nominal subject
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: I       PRON  PRP  nsubj     主语
- en: can     VERB  MD   aux       auxiliary
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: can     VERB  MD   aux       auxiliary
- en: promise VERB  VB   ROOT      None
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: promise VERB  VB   ROOT      无
- en: it      PRON  PRP  nsubj     nominal subject
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: it      PRON  PRP  nsubj     主语
- en: is      VERB  VBZ  ccomp     clausal complement
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: is      VERB  VBZ  ccomp     从句补语
- en: worth   ADJ   JJ   acomp     adjectival complement
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: worth   ADJ   JJ   acomp     形容词性补语
- en: your    ADJ   PRP$ poss      possession modifier
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: your    ADJ   PRP$ poss      所有格修饰词
- en: time    NOUN  NN   npadvmod  noun phrase as adverbial modifier
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: time    NOUN  NN   npadvmod  作状语的名词短语
- en: .       PUNCT .    punct     punctuation
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: .       PUNCT .    punct     标点符号
- en: The second and third columns contain the coarse-grained and fine-grained part-of-speech
    tags, respectively. The fourth column contains the dependency labels, and the
    fifth column contains descriptions for those dependency labels.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第二列和第三列分别包含粗粒度和细粒度的词性标签。第四列包含依存关系标签，第五列包含这些依存关系标签的描述。
- en: Combining part-of-speech tags and dependency labels can give you a better picture
    of the grammatical role of each token in a sentence—more so than just part-of-speech
    tags or dependency labels alone. For instance, in this example, the part-of-speech
    tag VBZ assigned to the token “is” indicates a verb in the third person singular
    present, whereas the dependency label ccomp, assigned to the same token, indicates
    that “is” is a *clausal complement* (a dependent clause with an internal subject).
    In this example, “is” is a clausal complement of the verb “promise” with the internal
    subject “it.”
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结合词性标签和依存关系标签可以更清晰地了解句子中每个词的语法角色——比单纯的词性标签或依存关系标签更具信息性。例如，在这个例子中，分配给“is”的词性标签VBZ表示第三人称单数现在时动词，而分配给同一词的依存关系标签ccomp则表示“is”是一个*从句补语*（带有内部主语的依赖子句）。在这个例子中，“is”是动词“promise”的从句补语，内部主语是“it”。
- en: 'To figure out the role of “you” in “I know you. You know me.”, we’d check the
    following list of part-of-speech tags and dependency labels assigned to the tokens:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弄清楚句子“I know you. You know me.”中“you”的角色，我们需要检查以下词性标签和依存关系标签：
- en: I     PRON  PRP  nsubj  nominal subject
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: I     PRON  PRP  nsubj  名词性主语
- en: know  VERB  VBP  ROOT   None
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: know  VERB  VBP  ROOT   无
- en: you   PRON  PRP  dobj   direct object
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: you   PRON  PRP  dobj   直接宾语
- en: .     PUNCT .    punct  punctuation
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: .     PUNCT .    标点符号  标点符号
- en: You   PRON  PRP  nsubj  nominal subject
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: You   PRON  PRP  nsubj  名词性主语
- en: know  VERB  VBP  ROOT   None
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: know  VERB  VBP  ROOT   无
- en: me    PRON  PRP  dobj   direct object
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: me    PRON  PRP  dobj   直接宾语
- en: .     PUNCT .    Punct  punctuation
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: .     PUNCT .    标点符号  标点符号
- en: 'In both cases, “you” is assigned the same part-of-speech tags: PRON and PRP
    (coarse-grained and fine-grained, respectively). But the two cases have different
    dependency labels: dobj in the first sentence and nsubj in the second.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，“你”被分配了相同的词性标签：PRON 和 PRP（粗粒度和细粒度）。但这两种情况的依赖标签不同：第一个句子的标签是 dobj，第二个句子的标签是
    nsubj。
- en: '***Deciding What Question a Chatbot Should Ask***'
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***决定聊天机器人应提什么问题***'
- en: 'Sometimes, you might need to navigate a sentence’s dependency tree to extract
    necessary information. For example, consider the following conversation between
    a chatbot and its user:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你可能需要浏览一个句子的依赖树，以提取必要的信息。例如，考虑以下聊天机器人与用户之间的对话：
- en: 'User: I want an apple.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：我想要一个苹果。
- en: 'Bot: Do you want a red apple?'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人：你想要一个红色的苹果吗？
- en: 'User: I want a green apple.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：我想要一个绿色的苹果。
- en: 'Bot: Why do you want a green one?'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人：你为什么想要一个绿色的？
- en: The chatbot is able to continue the conversation by asking questions. But notice
    that the presence or absence of an adjectival modifier for the noun “apple” plays
    a key role in deciding what type of question it should ask.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人通过提问能够继续对话。但请注意，“苹果”这个名词是否有形容词修饰语对于决定它应该问什么类型的问题起着关键作用。
- en: 'There are two basic types of questions in English: yes/no questions and information
    questions. Yes/no questions, like the one we generated in the example discussed
    in “[Turning Statements into Questions](../Text/ch04.xhtml#lev49)” on [page 51](../Text/ch04.xhtml#page_51),
    can have only two possible answers: yes or no. To form a question of this type,
    you place a modal auxiliary verb before the subject and the main verb after the
    subject. For example: “Could you modify it?”'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 英语中有两种基本问题类型：是/否问题和信息性问题。像我们在 “[将陈述转为问题](../Text/ch04.xhtml#lev49)”（第 51 页）中讨论的例子那样的是/否问题只有两个可能的答案：是或否。要形成这种类型的问题，你将情态助动词置于主语之前，主语后面跟主谓动词。例如：“你能修改它吗？”
- en: 'Information questions are supposed to be answered with more information than
    just yes or no. They begin with a question word, such as “what,” “where,” “when,”
    “why,” or “how.” After the question word, the process of forming an information
    question is the same as for yes/no questions. For example: “What do you think
    about it?”'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 信息性问题应该通过提供比简单的“是”或“否”更多的信息来回答。它们通常以疑问词开头，如“什么”、“哪里”、“何时”、“为什么”或“怎么”。在疑问词之后，信息性问题的构成与是/否问题相同。例如：“你怎么看待这个？”
- en: In the first case in the preceding apple example, the chatbot asks a yes/no
    question. In the second case, when the user modifies the noun “apple” with the
    adjective “green,” the chatbot asks an information question.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面苹果例子的第一个案例中，聊天机器人提出了一个是/否问题。在第二个案例中，当用户用形容词“绿色”修饰名词“苹果”时，聊天机器人则提出了一个信息性问题。
- en: The flowchart in [Figure 4-1](../Text/ch04.xhtml#ch04fig01) summarizes this
    approach.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](../Text/ch04.xhtml#ch04fig01)中的流程图总结了这种方法。'
- en: '![image](../Images/fig4-1.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/fig4-1.jpg)'
- en: '*Figure 4-1: The presence of a modifier in the input sentence determines what
    question the chatbot asks.*'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4-1：输入句子中是否有修饰语决定了聊天机器人应该提什么问题。*'
- en: The following script simply analyzes a submitted sentence to decide what kind
    of question to ask and then forms the proper question. We’ll walk through the
    code in separate sections, but you should save the entire program in a single
    file called *question.py*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本仅分析提交的句子，以决定应该提什么类型的问题，然后形成正确的问题。我们将分别讲解代码，但你应该将整个程序保存为一个名为 *question.py*
    的文件。
- en: 'Begin by importing the sys module, which provides functionality for accepting
    a sentence for processing as an argument:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入 sys 模块，它提供了将句子作为参数传递给程序进行处理的功能：
- en: import spacy
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: import spacy
- en: import sys
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: import sys
- en: This is an improvement from the previous scripts where we hardcoded the sentence
    to analyze. Now users can submit their own sentences as input.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对之前脚本的改进，以前我们硬编码了需要分析的句子。现在用户可以提交他们自己的句子作为输入。
- en: 'Next, we define a function that recognizes and extracts any noun chunk that
    is a direct object from a submitted doc. For example, if you submit a doc that
    contains the sentence “I want a green apple.”, it will return the chunk “a green
    apple”:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，该函数识别并提取提交的文档中任何作为直接宾语的名词短语。例如，如果你提交的文档包含句子 “I want a green apple.”，它会返回名词短语
    “a green apple”：
- en: 'def find_chunk(doc):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 find_chunk(doc) 函数：
- en: chunk = ''
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: chunk = ''
- en: '➊ for i,token in enumerate(doc):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 对于 i, token 在 enumerate(doc) 中：
- en: '➋ if token.dep_ == ''dobj'':'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 token.dep_ == 'dobj'：
- en: ➌ shift = len([w for w in token.children])
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ shift = len([w for w in token.children])
- en: '➍ #print([w for w in token.children])'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ #print([w for w in token.children])'
- en: ➎ chunk = doc[i-shift:i+1]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ chunk = doc[i-shift:i+1]
- en: break
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: return chunk
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 chunk
- en: We iterate over the tokens in the submitted sentence ➊ and look for the one
    that acts as a direct object by checking whether its dependency tag is dobj ➋.
    In the sentence “I want a green apple.”, The direct object is the noun “apple.”
    Once we’ve found the direct object, we need to determine its syntactic children
    ➌, because they form the chunk that we’ll use to decide what kind of question
    to ask. For debugging purposes, we might also want to look at the children of
    the direct object ➍.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历提交句子中的 token ➊，并通过检查它的依赖标签是否为 dobj 来寻找作为直接宾语的 token ➋。在句子 “I want a green
    apple.” 中，直接宾语是名词 “apple”。一旦我们找到了直接宾语，我们需要确定它的语法子节点 ➌，因为它们构成了我们用来决定提问类型的 chunk。为了调试，我们可能还需要查看直接宾语的子节点
    ➍。
- en: 'To extract the chunk, we slice the Doc object, calculating the start and the
    end indices of the slice as follows: the start index is the index of the direct
    object minus the number of its syntactic children. As you might guess, this is
    the index of the leftmost child. The end index is the index of the direct object
    plus one, so the last token included in the chunk is the direct object ➎.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取 chunk，我们切片 Doc 对象，计算切片的起始和结束索引，方法如下：起始索引是直接宾语的索引减去其语法子节点的数量。你可能猜到，这就是最左边子节点的索引。结束索引是直接宾语的索引加一，所以
    chunk 中包含的最后一个 token 就是直接宾语 ➎。
- en: For simplicity, the algorithm implemented in this script assumes that a direct
    object has only leftward children. In fact, this isn’t always the case. For example,
    in the following sentence, “I want to touch a wall painted green.”, we’ll need
    to check the left and right children of the direct object “wall.” Also, because
    “green” is not a direct child of “wall,” we’ll need to walk the dependency tree
    to determine that “green” is a modifier of “wall.” We’ll discuss premodifiers
    and postmodifiers in more depth in [Chapter 6](../Text/ch06.xhtml#ch06).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，本文所实现的算法假设直接宾语只有向左的子节点。实际上，情况并非总是如此。例如，在以下句子中，“I want to touch a wall painted
    green.”，我们需要检查直接宾语 “wall” 的左右子节点。此外，由于 “green” 不是 “wall” 的直接子节点，我们需要遍历依赖树来确定 “green”
    是 “wall” 的修饰语。我们将在[第六章](../Text/ch06.xhtml#ch06)中更深入地讨论前修饰语和后修饰语。
- en: 'The following function examines the chunk and decides what kind of question
    the chatbot should ask:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数检查 chunk 并决定聊天机器人应该问哪种类型的问题：
- en: 'def determine_question_type(chunk):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 determine_question_type(chunk) 函数：
- en: ➊ question_type = 'yesno'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ question_type = 'yesno'
- en: 'for token in chunk:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 token 在 chunk 中：
- en: '➋ if token.dep_ == ''amod'':'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 如果 token.dep_ == 'amod'：
- en: ➌ question_type = 'info'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ question_type = 'info'
- en: return question_type
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 question_type
- en: We initialize the question_type variable to a value of yesno, which represents
    the yes/no question type ➊. Then, in the submitted chunk, we search for a token
    tagged amod, which stands for adjectival modifier ➋. If we find it, we set the
    question_type variable to 'info', which represents the information question type
    ➌.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 question_type 变量初始化为 yesno，表示是/否问题类型 ➊。然后，在提交的 chunk 中，我们搜索标记为 amod 的 token，表示形容词修饰语
    ➋。如果找到，我们将 question_type 变量设置为 'info'，表示信息性问题类型 ➌。
- en: 'Once we’ve determined what question type to use, the following function generates
    a question from the submitted sentence:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了要使用的提问类型，以下函数会根据提交的句子生成问题：
- en: 'def generate_question(doc, question_type):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 generate_question(doc, question_type) 函数：
- en: sent = ''
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: sent = ''
- en: 'for i,token in enumerate(doc):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 i, token 在 enumerate(doc) 中：
- en: 'if token.tag_ == ''PRP'' and doc[i+1].tag_ == ''VBP'':'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.tag_ == 'PRP' 且 doc[i+1].tag_ == 'VBP'：
- en: sent = 'do ' + doc[i].text
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: sent = 'do ' + doc[i].text
- en: sent = sent + ' ' + doc[i+1:].text
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: sent = sent + ' ' + doc[i+1:].text
- en: break
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: doc=nlp(sent)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(sent)
- en: 'for i,token in enumerate(doc):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 i, token 在 enumerate(doc) 中：
- en: 'if token.tag_ == ''PRP'' and token.text == ''I'':'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 token.tag_ == 'PRP' 且 token.text == 'I'：
- en: sent = doc[:i].text + ' you ' + doc[i+1:].text
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: sent = doc[:i].text + ' you ' + doc[i+1:].text
- en: break
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: doc=nlp(sent)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: doc = nlp(sent)
- en: '➊ if question_type == ''info'':'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 如果 question_type == 'info'：
- en: 'for i,token in enumerate(doc):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,token in enumerate(doc):'
- en: 'if token.dep_ == ''dobj'':'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'if token.dep_ == ''dobj'':'
- en: sent = 'why ' + doc[:i].text + ' one ' + doc[i+1:].text
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: sent = '为什么 ' + doc[:i].text + ' 一个 ' + doc[i+1:].text
- en: break
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '➋ if question_type == ''yesno'':'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '➋ if question_type == ''yesno'':'
- en: 'for i,token in enumerate(doc):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i,token in enumerate(doc):'
- en: 'if token.dep_ == ''dobj'':'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 'if token.dep_ == ''dobj'':'
- en: ➌ sent = doc[:i-1].text + ' a red ' + doc[i:].text
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ sent = doc[:i-1].text + ' a red ' + doc[i:].text
- en: break
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: doc=nlp(sent)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: doc=nlp(sent)
- en: sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: sent = doc[0].text.capitalize() +' ' + doc[1:len(doc)-1].text + '?'
- en: return sent
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: return sent
- en: In a sequence of for loops, we convert the submitted statement into a question
    by performing inversion and changing the personal pronouns. In this example, because
    there is no modal auxiliary verb in the statement, we add the verb “do” before
    the personal pronoun to form the question. (Remember that this will only work
    with certain sentences; in a more complete implementation, we’d have to programmatically
    figure out which processing approach to take.)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在一系列的for循环中，我们通过倒装和更改人称代词将提交的陈述句转换为疑问句。在这个例子中，由于句子中没有情态助动词，我们在人称代词前加上动词“do”来形成疑问句。（请记住，这仅适用于某些句子；在更完整的实现中，我们必须程序性地决定采用哪种处理方法。）
- en: If question_type is set to info, we add the word “why” to the beginning of the
    question ➊. If the question_type variable is set to yesno ➋, we insert an adjective
    to modify the direct object in the question. In this example, we’ve hardcoded
    the adjective for the sake of simplicity. We’ve chosen the adjective “red”, ➌
    which might sound strange in certain sentences. For example, we can say, “Do you
    want a red orange?” but not “Do you want a red idea?” In a better implementation
    of this chatbot, we could find a way to programmatically determine a suitable
    adjective to modify the direct object. We’ll come back to that topic in [Chapter
    6](../Text/ch06.xhtml#ch06).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果question_type设置为info，我们会在问题的开头加上“为什么”一词 ➊。如果question_type变量设置为yesno ➋，我们会插入一个形容词来修饰问题中的直接宾语。在这个例子中，为了简化，我们硬编码了形容词。我们选择了形容词“红色的”，
    ➌ 这个在某些句子中可能听起来有些奇怪。例如，我们可以说：“你想要一个红色的橙子吗？”但不能说：“你想要一个红色的想法吗？”在更好的聊天机器人实现中，我们可以找到一种方法，根据上下文程序性地确定合适的形容词来修饰直接宾语。我们将在[第六章](../Text/ch06.xhtml#ch06)中回到这个话题。
- en: Notice also that the algorithm used here assumes that a submitted sentence ends
    with a punctuation mark, such as “.” or “!”.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意这里使用的算法假设提交的句子以标点符号结尾，例如“。”或“！”。
- en: 'Now that we’ve defined all the functions, here is the main block of the script:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了所有的函数，以下是脚本的主要部分：
- en: '➊ if len(sys.argv) > 1:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '➊ if len(sys.argv) > 1:'
- en: sent = sys.argv[1]
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: sent = sys.argv[1]
- en: nlp = spacy.load('en')
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: nlp = spacy.load('en')
- en: ➋ doc = nlp(sent)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ doc = nlp(sent)
- en: ➌ chunk = find_chunk(doc)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ chunk = find_chunk(doc)
- en: '➍ if str(chunk) == '''':'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ if str(chunk) == '''':'
- en: print('The sentence does not contain a direct object.')
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: print('句子不包含直接宾语。')
- en: sys.exit()
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: sys.exit()
- en: ➎ question_type = determine_question_type(chunk)
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ question_type = determine_question_type(chunk)
- en: ➏ question = generate_question(doc, question_type)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ question = generate_question(doc, question_type)
- en: print(question)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: print(question)
- en: 'else:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: print('You did not submit a sentence!')
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: print('你没有提交句子！')
- en: First, we check whether a user has passed a sentence in as a command line argument
    ➊. If a sentence has been submitted, we apply spaCy’s pipeline to it, creating
    a Doc object instance ➋.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查用户是否通过命令行参数传递了一个句子 ➊。若提交了句子，我们应用spaCy的管道处理它，创建一个Doc对象实例 ➋。
- en: We then send the doc to the find_chunk function, which should return a noun
    chunk containing a direct object, such as “a green apple”, for further processing
    ➌. If there is no such noun chunk in the submitted sentence ➍, we’ll receive the
    message “The sentence does not contain a direct object.”
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将文档传递给find_chunk函数，该函数应该返回一个包含直接宾语的名词短语，例如“一个绿色的苹果”，以便进一步处理 ➌。如果提交的句子中没有这样的名词短语
    ➍，我们会收到“句子不包含直接宾语。”的提示信息。
- en: Next, we pass the chunk we just extracted to the determine_question_type function,
    which determines which question to ask based on the chunk’s structure ➎.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将刚刚提取的chunk传递给determine_question_type函数，该函数根据chunk的结构确定要提出的问题类型 ➎。
- en: Finally, we pass the submitted sentence and the type of question to the generate_question
    function, which will generate an appropriate question and return it as a string
    ➏.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将提交的句子和问题类型传递给generate_question函数，该函数将生成一个适当的问题并以字符串形式返回 ➏。
- en: 'The script’s output depends on the specific sentence submitted. Here are some
    possible variants:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的输出取决于提交的具体句子。以下是一些可能的变体：
- en: ➊ $ python question.py 'I want a green apple.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ $ python question.py 'I want a green apple.'
- en: Why do you want a green one?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么想要一个绿色的？
- en: ➋ $ python question.py 'I want an apple.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ $ python question.py 'I want an apple.'
- en: Do you want a red apple?
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要一个红色的苹果吗？
- en: ➌ $ python question.py 'I want...'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ $ python question.py 'I want...'
- en: The sentence does not contain a direct object.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 该句子不包含直接宾语。
- en: ➍ $ python question.py
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ $ python question.py
- en: You did not submit a sentence!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你没有提交句子！
- en: If we submit a sentence that contains an adjectival modifier, such as “green”
    for a direct object like “apple”, the script should generate an information question
    ➊.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们提交一个包含形容词修饰语的句子，例如“green”作为“apple”这样的直接宾语的修饰词，脚本应生成一个信息性疑问句 ➊。
- en: If the sentence contains a direct object without an adjectival modifier, the
    script should respond with a yes/no question ➋.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果句子包含没有形容词修饰语的直接宾语，脚本应该生成是/否疑问句 ➋。
- en: If we submit a sentence with no direct object, the script should recognize this
    at once and ask us to resubmit ➌.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们提交的句子没有直接宾语，脚本应该立刻识别并要求我们重新提交 ➌。
- en: Finally, if we forget to submit a sentence, the script should respond with an
    appropriate message ➍.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们忘记提交句子，脚本应该返回一个适当的消息 ➍。
- en: '***Try This***'
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '***试试这个***'
- en: As noted earlier, the script discussed in the preceding section won’t work with
    all sentences. The script adds “do” to form a question, which works only with
    sentences that contain no auxiliary modal verb.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，前面章节讨论的脚本并不适用于所有句子。该脚本通过添加“do”来构成疑问句，这只适用于不含有助动词的句子。
- en: Enhance the functionality of this script so it can also work with statements
    containing modal auxiliary verbs. For example, given the following statement,
    “I might want a green apple,” the script should generate “Why might you want a
    green one?” For details on how to turn a statement containing a modal auxiliary
    verb into a question, refer to “[Turning Statements into Questions](../Text/ch04.xhtml#lev49)”
    on [page 51](../Text/ch04.xhtml#page_51).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 增强该脚本的功能，使其也能处理包含情态助动词的陈述句。例如，针对以下陈述句：“I might want a green apple”，脚本应生成“Why
    might you want a green one？”有关如何将包含情态助动词的陈述句转为疑问句，请参考[《将陈述句转为疑问句》](../Text/ch04.xhtml#lev49)（见[第51页](../Text/ch04.xhtml#page_51)）。
- en: '**Summary**'
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**总结**'
- en: Linguistic features are at the heart of all NLP tasks. This chapter taught you
    some techniques for smart text processing and text generation with linguistic
    features. You learned how to extract phrases of a certain type (say, those that
    refer to an amount of money), and then wrote a script using dependency labels
    and part-of-speech tags that generated a meaningful response to the sentence submitted
    by a user.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学特征是所有NLP任务的核心。本章教会了你一些利用语言学特征进行智能文本处理和文本生成的技巧。你学会了如何提取某一类型的短语（例如那些指代金额的短语），然后编写了一个脚本，使用依赖标签和词性标签生成对用户提交的句子作出有意义的回应。
- en: We’ll return to linguistic features in [Chapter 6](../Text/ch06.xhtml#ch06)
    where you’ll implement them in more complex scenarios.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第6章](../Text/ch06.xhtml#ch06)中回到语言学特征，您将在更复杂的场景中实现它们。
