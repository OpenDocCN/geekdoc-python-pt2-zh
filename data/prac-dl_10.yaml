- en: '**10'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**10'
- en: EXPERIMENTS WITH NEURAL NETWORKS**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络实验**
- en: '![image](Images/common.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/common.jpg)'
- en: 'In [Chapter 9](ch09.xhtml#ch09), we discussed the theory behind neural networks.
    In this chapter, we’ll trade equations for code and run a number of experiments
    designed to increase our intuition regarding the essential parameters of neural
    networks: architecture and activation functions, batch size, base learning rate,
    training set size, L2 regularization, momentum, weight initialization, feature
    ordering, and the precision of the weights and biases.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 9 章](ch09.xhtml#ch09) 中，我们讨论了神经网络背后的理论。在本章中，我们将用代码替代方程式，进行一系列实验，以增强我们对神经网络基本参数的直觉：架构与激活函数、批量大小、基础学习率、训练集大小、L2
    正则化、动量、权重初始化、特征排序，以及权重和偏差的精度。
- en: To save space and eliminate tedious repetition, we won’t show the specific code
    for each experiment. In most cases, the code is only trivially different from
    the previous example; we’re usually changing only the particular argument to the
    `MLPClassifier` constructor we’re interested in. The code for each experiment
    is included in the set of files associated with this book, and we’ll list the
    network parameters and the name of the file. When necessary, we’ll provide code
    to clarify a particular approach. We’ll show the code for the first experiment
    in its entirety.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间并消除冗长的重复，我们不会展示每个实验的具体代码。在大多数情况下，代码与之前的示例仅有微小的不同；我们通常只会改变 `MLPClassifier`
    构造函数中的特定参数。每个实验的代码包含在本书相关文件集中，我们将列出网络参数和文件名。必要时，我们将提供代码以阐明某个特定方法。我们将展示第一个实验的完整代码。
- en: Our Dataset
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的数据集
- en: We’ll be working with the MNIST dataset’s vector form, which we assembled in
    [Chapter 5](ch05.xhtml#ch05). Recall that this dataset consists of 28×28 pixel
    8-bit grayscale images of handwritten digits, [0,9]. In vector form, each 28 ×
    28 image is unraveled into a vector of 28 × 28 = 784 elements, all bytes ([0,255]).
    The unraveling lays each row end to end. Therefore, each sample has 784 elements
    and an associated label. The training set has 60,000 samples, while the test set
    has 10,000\. For our experiments, we won’t use all of the data in the training
    set. This is to help illustrate the effect of network parameters and to keep our
    training times reasonable. Refer back to [Figure 5-3](ch05.xhtml#ch5fig3) for
    representative MNIST digits.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 MNIST 数据集的向量形式，该形式在 [第 5 章](ch05.xhtml#ch05) 中已汇总。回顾一下，该数据集由 28×28 像素的
    8 位灰度手写数字图像组成，范围为 [0,9]。在向量形式中，每个 28 × 28 的图像被展开成一个包含 28 × 28 = 784 个元素的向量，所有字节的范围为
    [0,255]。展开操作将每一行按顺序排列。因此，每个样本包含 784 个元素及其相关标签。训练集包含 60,000 个样本，测试集包含 10,000 个样本。在我们的实验中，我们不会使用训练集中的所有数据。这是为了帮助展示网络参数的效果，并保持训练时间在合理范围内。请参考
    [图 5-3](ch05.xhtml#ch5fig3) 以查看代表性的 MNIST 数字。
- en: The MLPClassifier Class
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLPClassifier 类
- en: 'The `MLPClassifier` class follows the same format as the other sklearn classifiers.
    There is a constructor and the expected methods: `fit` for training, `score` for
    applying the classifier to test data, and `predict` to make a prediction on unknown
    inputs. We’ll also use `predict_proba` to return the actual predicted per class
    probabilities. The constructor has many options:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`MLPClassifier` 类遵循与其他 sklearn 分类器相同的格式。它有一个构造函数和预期的方法：`fit` 用于训练，`score` 用于将分类器应用于测试数据，`predict`
    用于对未知输入进行预测。我们还将使用 `predict_proba` 来返回每个类别的实际预测概率。构造函数有许多选项：'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we’ve provided the default values for each parameter. See the sklearn documentation
    page at [http://scikit-learn.org/](http://scikit-learn.org/) for a complete description
    of each parameter. We’ll set some of these to specific values, and others will
    be changed for the experiments while still others are relevant in only specific
    situations. The key parameters we’ll work with are in [Table 10-1](ch10.xhtml#ch10tab1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了每个参数的默认值。请参阅 [http://scikit-learn.org/](http://scikit-learn.org/) 上的
    sklearn 文档页面，以获取每个参数的完整描述。我们将一些参数设置为特定的值，其他的将在实验中进行调整，还有一些参数仅在特定情况下相关。我们将使用的关键参数列在
    [表 10-1](ch10.xhtml#ch10tab1) 中。
- en: The following set of experiments explores the effect of various `MLPClassifier`
    parameters. As mentioned, we’ll show all the code used for the first experiment,
    understanding that only small changes are needed to perform the other experiments.
    At times, we’ll show little code snippets to make the change concrete.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一组实验探讨了各种 `MLPClassifier` 参数的影响。如前所述，我们将展示第一个实验所用的所有代码，理解其余实验只需进行小的更改即可。有时，我们将展示一些小的代码片段，以使更改更加具体。
- en: '**Table 10-1:** Important `MLPClassifier` Constructor Keywords and Our Default
    Values for Them'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-1:** 重要的`MLPClassifier`构造函数关键词及其默认值'
- en: '| **Keyword** | **Description** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **关键词** | **描述** |'
- en: '| --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `hidden_layer_sizes` | Tuple giving the hidden layer sizes |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `hidden_layer_sizes` | 隐藏层大小的元组 |'
- en: '| `activation` | Activation function type; for example, ReLU |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `activation` | 激活函数类型；例如，ReLU |'
- en: '| `alpha` | L2 parameter—we called it *λ* (lambda) |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `alpha` | L2参数—我们称之为*λ*（lambda） |'
- en: '| `batch_size` | Minibatch size |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `batch_size` | 小批量大小 |'
- en: '| `learning_rate_init` | The learning rate, *η* (eta) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `learning_rate_init` | 学习率，*η*（eta） |'
- en: '| `max_iter` | Number of training epochs |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `max_iter` | 训练的轮数 |'
- en: '| `warm_start` | Continue training or start again |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `warm_start` | 继续训练或重新开始 |'
- en: '| `momentum` | Momentum |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `momentum` | 动量 |'
- en: '| `solver` | Solver algorithm ("sgd") |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `solver` | 求解算法（"sgd"） |'
- en: '| `nesterovs_momentum` | Use Nesterov momentum (False) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `nesterovs_momentum` | 是否使用Nesterov动量（默认：False） |'
- en: '| `early_stopping` | Use early stopping (False) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `early_stopping` | 是否使用提前停止（默认：False） |'
- en: '| `learning_rate` | Learning rate schedule ("constant") |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `learning_rate` | 学习率计划（"constant"） |'
- en: '| `tol` | Stop early if loss change < tol (1e-8) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `tol` | 如果损失变化小于tol，则提前停止（1e-8） |'
- en: '| `verbose` | Output to console while training (False) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `verbose` | 训练过程中是否输出到控制台（默认：False） |'
- en: Architecture and Activation Functions
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构和激活函数
- en: 'When designing a neural network, we immediately face two fundamental questions:
    what architecture and what activation function? These are arguably the most important
    deciding factors for a model’s success. Let’s explore what happens when we train
    a model using different architectures and activation functions while holding the
    training dataset fixed.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计神经网络时，我们首先面临两个基本问题：选择什么架构和什么激活函数？这无疑是决定模型成功的最重要因素。让我们探讨一下，当保持训练数据集固定时，使用不同的架构和激活函数进行训练时会发生什么。
- en: The Code
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码
- en: As promised, for this first experiment we’ll show the code in its entirety,
    starting with the helper functions in [Listing 10-1](ch10.xhtml#ch10lis1).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如所承诺，对于这个首次实验，我们将展示完整代码，从[Listing 10-1](ch10.xhtml#ch10lis1)中的辅助函数开始。
- en: import numpy as np
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import time
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: import time
- en: from sklearn.neural_network import MLPClassifier
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neural_network import MLPClassifier
- en: 'def run(x_train, y_train, x_test, y_test, clf):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'def run(x_train, y_train, x_test, y_test, clf):'
- en: s = time.time()
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: s = time.time()
- en: (*\pagebreak*)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (*\pagebreak*)
- en: clf.fit(x_train, y_train)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: clf.fit(x_train, y_train)
- en: e = time.time()-s
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: e = time.time()-s
- en: loss = clf.loss_
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: loss = clf.loss_
- en: weights = clf.coefs_
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: weights = clf.coefs_
- en: biases = clf.intercepts_
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: biases = clf.intercepts_
- en: params = 0
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: params = 0
- en: 'for w in weights:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'for w in weights:'
- en: params += w.shape[0]*w.shape[1]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: params += w.shape[0]*w.shape[1]
- en: 'for b in biases:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'for b in biases:'
- en: params += b.shape[0]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: params += b.shape[0]
- en: return [clf.score(x_test, y_test), loss, params, e]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: return [clf.score(x_test, y_test), loss, params, e]
- en: 'def nn(layers, act):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'def nn(layers, act):'
- en: return MLPClassifier(solver="sgd", verbose=False, tol=1e-8,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: return MLPClassifier(solver="sgd", verbose=False, tol=1e-8,
- en: nesterovs_momentum=False, early_stopping=False,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: nesterovs_momentum=False, early_stopping=False,
- en: learning_rate_init=0.001, momentum=0.9, max_iter=200,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: learning_rate_init=0.001, momentum=0.9, max_iter=200,
- en: hidden_layer_sizes=layers, activation=act)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: hidden_layer_sizes=layers, activation=act)
- en: '*Listing 10-1: Helper functions for experimenting with the architecture and
    activation function. See* mnist_nn_experiments.py.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 10-1: 用于实验架构和激活函数的辅助函数。请参见* mnist_nn_experiments.py。'
- en: '[Listing 10-1](ch10.xhtml#ch10lis1) imports the usual modules and then defines
    two helper functions, `run` and `nn`. Starting with `nn`, we see that all it does
    is return an instance of `MLPClassifier` using the hidden layer sizes and the
    given activation function type.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 10-1](ch10.xhtml#ch10lis1)导入常用的模块，然后定义了两个辅助函数，`run`和`nn`。从`nn`开始，我们看到它所做的只是使用隐藏层大小和给定的激活函数类型返回一个`MLPClassifier`实例。'
- en: The hidden layer sizes are given as a tuple, where each element is the number
    of nodes in the corresponding layer. Recall that sklearn works with only fully
    connected layers, so a single number is all we need to specify the size. The input
    samples given for training determine the size of the input layer. Here the input
    samples are vectors representing the digit images, so there are 28 × 28 = 784
    nodes in the input layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的大小以元组的形式给出，其中每个元素表示相应层的节点数。回想一下，sklearn只使用全连接层，因此我们只需要一个数字来指定大小。用于训练的输入样本决定了输入层的大小。这里，输入样本是表示数字图像的向量，因此输入层有28
    × 28 = 784个节点。
- en: What about the output layer? It’s not specified explicitly because it depends
    on the number of classes in the training labels. The MNIST dataset has 10 classes,
    so there will be 10 nodes in the output layer. When the `predict_proba` method
    is called to get an output probability, sklearn applies a softmax over the 10
    outputs. If the model is binary, meaning the only class labels are 0 and 1, then
    there is only one output node, a logistic (sigmoid), representing the probability
    of belonging to class 1.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层怎么样？它没有明确指定，因为它取决于训练标签中的类别数量。MNIST 数据集有 10 个类别，所以输出层将有 10 个节点。当调用 `predict_proba`
    方法获取输出概率时，sklearn 会对这 10 个输出应用 softmax。如果模型是二分类的，即只有 0 和 1 这两个类别标签，那么只有一个输出节点，一个逻辑回归（sigmoid），表示属于类别
    1 的概率。
- en: Now let’s look at the parameters we passed in to `MLPClassifier`. First, we
    explicitly state that we want to use the SGD solver. The solver is the approach
    used to modify the weights and biases during training. All the solvers use backprop
    to calculate the gradients; how we use those gradients varies. Plain vanilla SGD
    is good enough for us right now.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看传递给 `MLPClassifier` 的参数。首先，我们明确声明我们希望使用 SGD 解算器。解算器是训练过程中用于调整权重和偏差的方法。所有解算器都使用反向传播来计算梯度；我们如何使用这些梯度是不同的。普通的
    SGD 就足够我们现在使用了。
- en: Next, we set a low tolerance so that we’ll train the requested number of epochs
    (`max_iter`). We also turn off Nesterov momentum (a variant of standard momentum)
    and early stopping (generally useful but not desired here).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置了较低的容忍度，以便训练所请求的轮数（`max_iter`）。我们还关闭了 Nesterov 动量（标准动量的变种）和提前停止（虽然通常很有用，但在这里不需要）。
- en: The initial learning rate is set to the default value of 0.001, as is the value
    of standard momentum, 0.9\. The number of epochs is arbitrarily set to 200 (the
    default), but we’ll explore this more in the experiments that follow. Please indulge
    your curiosity at all times and see what changing these values does to things.
    For consistency’s sake, we’ll use these values as defaults throughout unless they
    are the parameters we want to experiment with.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 初始学习率设置为默认值 0.001，标准动量值为 0.9。训练的轮数被任意设置为 200（默认值），但我们将在接下来的实验中进一步探讨这个问题。请随时保持好奇心，看看改变这些值会对结果产生什么影响。为了保持一致性，除非我们想要实验的参数，否则我们将在整个过程中将这些值作为默认值。
- en: The other helper function in [Listing 10-1](ch10.xhtml#ch10lis1) is `run`. This
    function will train and test the classifier object it’s passed using the standard
    sklearn `fit` and `score` methods. It also does some other things that we have
    not seen before.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 10-1](ch10.xhtml#ch10lis1)中，另一个辅助函数是 `run`。这个函数将使用标准的 sklearn `fit` 和 `score`
    方法训练和测试传入的分类器对象。它还做了一些我们之前没有见过的其他事情。
- en: In particular, after timing how long training takes, we extract the final training
    loss value, the network weights, and the network biases from the `MLPClassifier`
    object so that we can return them. The `MLPClassifier` class minimizes the log-loss,
    which we described in [Chapter 9](ch09.xhtml#ch09). We store the log-loss in the
    `loss_` member variable. The size of this value, and how it changes during training,
    gives us a clue as to how well the network is learning. In general, the smaller
    the log-loss, the better the network is doing. As you explore neural networks
    more and more, you’ll begin to develop intuition for what a good loss value is
    and whether the training process is learning quickly or not by how rapidly the
    loss changes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，在计时训练所需的时间后，我们从 `MLPClassifier` 对象中提取最终的训练损失值、网络权重和网络偏差，以便返回它们。`MLPClassifier`
    类最小化对数损失（log-loss），我们在[第 9 章](ch09.xhtml#ch09)中描述过。我们将对数损失存储在 `loss_` 成员变量中。这个值的大小以及它在训练过程中的变化，能给我们一个关于网络学习得怎么样的线索。通常来说，对数损失越小，网络的表现越好。随着你对神经网络的探索越来越深入，你将开始培养出对什么是良好损失值的直觉，并且通过损失变化的速度来判断训练过程是学习得快还是慢。
- en: The weights and biases are stored in the `coefs_` and `intercepts_` member variables.
    These are lists of NumPy matrices (weights) and vectors (biases), respectively.
    Here we use them to calculate the number of parameters in the network by summing
    the number of elements in each matrix and vector. This is what the two small loops
    in the `run` function do. Finally, we return all this information, including the
    score against the test set, to the `main` function. The `main` function is shown
    in [Listing 10-2](ch10.xhtml#ch10lis2).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏置存储在 `coefs_` 和 `intercepts_` 成员变量中。这些分别是 NumPy 矩阵（权重）和向量（偏置）的列表。在这里，我们使用它们通过求和每个矩阵和向量中的元素数量来计算网络中的参数数量。这正是
    `run` 函数中两个小循环所做的。最后，我们将包括对测试集的评分在内的所有信息返回给 `main` 函数。`main` 函数见 [清单 10-2](ch10.xhtml#ch10lis2)。
- en: 'def main():'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'def main():'
- en: x_train = np.load("mnist_train_vectors.npy").astype("float64")/256.0
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = np.load("mnist_train_vectors.npy").astype("float64")/256.0
- en: y_train = np.load("mnist_train_labels.npy")
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = np.load("mnist_train_labels.npy")
- en: x_test = np.load("mnist_test_vectors.npy").astype("float64")/256.0
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = np.load("mnist_test_vectors.npy").astype("float64")/256.0
- en: y_test = np.load("mnist_test_labels.npy")
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = np.load("mnist_test_labels.npy")
- en: N = 1000
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: N = 1000
- en: x_train = x_train[:N]
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = x_train[:N]
- en: y_train = y_train[:N]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = y_train[:N]
- en: x_test  = x_test[:N]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: x_test = x_test[:N]
- en: y_test  = y_test[:N]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: y_test = y_test[:N]
- en: layers = [
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: layers = [
- en: (1,), (500,), (800,), (1000,), (2000,), (3000,),
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (1,), (500,), (800,), (1000,), (2000,), (3000,),
- en: (1000,500), (3000,1500),
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (1000,500), (3000,1500),
- en: (2,2,2), (1000,500,250), (2000,1000,500),
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (2,2,2), (1000,500,250), (2000,1000,500),
- en: ']'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: 'for act in ["relu", "logistic", "tanh"]:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 'for act in ["relu", "logistic", "tanh"]:'
- en: print("%s:" % act)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: print("%s:" % act)
- en: 'for layer in layers:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'for layer in layers:'
- en: scores = []
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: scores = []
- en: loss = []
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: loss = []
- en: tm = []
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: tm = []
- en: 'for i in range(10):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: s,l,params,e = run(x_train, y_train, x_test, y_test,
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: s,l,params,e = run(x_train, y_train, x_test, y_test,
- en: nn(layer,act))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: nn(layer,act))
- en: scores.append(s)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: scores.append(s)
- en: loss.append(l)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: loss.append(l)
- en: tm.append(e)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: tm.append(e)
- en: s = np.array(scores)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: s = np.array(scores)
- en: l = np.array(loss)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: l = np.array(loss)
- en: t = np.array(tm)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: t = np.array(tm)
- en: n = np.sqrt(s.shape[0])
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: n = np.sqrt(s.shape[0])
- en: 'print("    layers: %14s, score= %0.4f +/- %0.4f,'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'print("    layers: %14s, score= %0.4f +/- %0.4f,'
- en: loss = %0.4f +/- %0.4f (params = %6d, time = %0.2f s)" % \
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: loss = %0.4f +/- %0.4f (params = %6d, time = %0.2f s)" % \
- en: (str(layer), s.mean(), s.std()/n, l.mean(),
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: (str(layer), s.mean(), s.std()/n, l.mean(),
- en: l.std()/n, params, t.mean()))
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: l.std()/n, params, t.mean()))
- en: '*Listing 10-2: The `main` function for experimenting with the architecture
    and activation function. See* mnist_nn_experiments.py.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 10-2：用于实验架构和激活函数的 `main` 函数。参见* mnist_nn_experiments.py。'
- en: We first load the MNIST train and test data stored in `x_train` (samples) and
    `y_train` (labels), and `x_test` and `y_test`. Notice that we divide the samples
    by 256.0 to make them floats in the range [0,1). This normalization is the only
    preprocessing we’ll do in this chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载存储在 `x_train`（样本）和 `y_train`（标签）、`x_test` 和 `y_test` 中的 MNIST 训练和测试数据。请注意，我们将样本除以
    256.0，使其转换为范围 [0,1) 内的浮点数。这是本章唯一的预处理步骤。
- en: As the full training set has 60,000 samples and we want to run many training
    sessions, we’ll use only the first 1,000 samples for training. We’ll likewise
    keep the first 1,000 test samples. Our goal in this chapter is to see relative
    differences as we change parameters, not to build the best model possible, so
    we’ll sacrifice the quality of the model to get results in a reasonable timeframe.
    With 1,000 training samples, we’ll have only 100 instances of each digit type,
    on average. We’ll vary the number of training samples for specific experiments.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于完整的训练集包含 60,000 个样本，而我们想进行多次训练，所以我们只使用前 1,000 个样本进行训练。同样，我们将保留前 1,000 个测试样本。本章的目标是通过改变参数来观察相对差异，而不是构建最佳的模型，因此我们会牺牲模型质量，以便在合理的时间内得到结果。使用
    1,000 个训练样本时，我们平均每个数字类型只有 100 个实例。对于特定实验，我们会改变训练样本的数量。
- en: The `layers` list holds the different architectures we’ll explore. Ultimately,
    we’ll pass these values to the `hidden_layer_sizes` argument of the `MLPClassifier`
    constructor. Notice that we’ll examine architectures ranging from a single hidden
    layer with a single node to three hidden layers with up to 2,000 nodes per layer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`layers` 列表包含了我们将要探索的不同架构。最终，我们会将这些值传递给 `MLPClassifier` 构造函数的 `hidden_layer_sizes`
    参数。请注意，我们将会检查从一个包含单个节点的单一隐藏层到三个隐藏层（每个隐藏层最多有 2,000 个节点）的架构。'
- en: 'The `main` loop runs over three activation function types: rectified linear
    unit, logistic (sigmoid) unit, and the hyperbolic tangent. We’ll train a model
    for each combination of activation function type and architecture (`layers`).
    Moreover, since we know neural network training is stochastic, we’ll train 10
    models for each combination and report the mean and standard error of the mean,
    so we’re not thrown off by a particularly bad model that isn’t representative.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 循环运行三个激活函数类型：修正线性单元（ReLU）、逻辑（Sigmoid）单元和双曲正切（Tanh）。我们将为每种激活函数类型和架构（`layers`）的组合训练一个模型。此外，由于我们知道神经网络训练是随机的，我们将为每个组合训练
    10 个模型，并报告均值和标准误差，这样就不会被一个特别差的、不具有代表性的模型影响。'
- en: '**Note** *When you run the code in the experiments that follow, you’ll likely
    generate warning messages from sklearn like this one:*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**注** *当你运行下面实验中的代码时，你很可能会收到来自 sklearn 的警告消息，类似于这个：*'
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*The messages are sklearn’s way of telling you that the number of training
    iterations completed before sklearn felt that the network had converged to a good
    set of weights.The warnings are safe to ignore and can be disabled completely
    by adding `-W ignore` to the command line when you run the code; for example:*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些消息是 sklearn 通知您，在训练迭代完成之前，sklearn 认为网络已经收敛到一个良好的权重集。警告是可以安全忽略的，并且可以通过在运行代码时在命令行添加
    `-W ignore` 来完全禁用；例如：*'
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Results
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Running this code takes several hours to complete, and produces output with
    lines that look something like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码需要几小时才能完成，并且产生的输出包含如下所示的行：
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This tells us that using a ReLU activation function, and an architecture with
    two hidden layers of 3,000 and 1,500 nodes each, the models had an average score
    of 88.2 percent and an average final training loss of 0.21 (remember that lower
    is better). It also tells us that the neural network had a total of nearly 6.9
    million parameters and took, on average, a little more than four minutes to train.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，使用 ReLU 激活函数，并且架构具有两个隐藏层（分别为 3,000 和 1,500 个节点），模型的平均得分为 88.2%，最终训练损失的平均值为
    0.21（记住，越低越好）。这还告诉我们，神经网络总共有将近 690 万个参数，训练时间平均略超过四分钟。
- en: '[Table 10-2](ch10.xhtml#ch10tab2) summarizes the scores for the various network
    architectures and activation function types.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-2](ch10.xhtml#ch10tab2) 总结了不同网络架构和激活函数类型的得分。'
- en: '**Table 10-2:** Mean Score (mean ± SE) on the MNIST Test Set as a Function
    of the Architecture and Activation Function Type'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-2：** 作为架构和激活函数类型的函数，MNIST 测试集的平均得分（均值 ± 标准误差）'
- en: '| Architecture | ReLU | Tanh | Logistic (sigmoid) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | ReLU | Tanh | Logistic (sigmoid) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 0.2066 ± 0.0046 | 0.2192 ± 0.0047 | 0.1718 ± 0.0118 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.2066 ± 0.0046 | 0.2192 ± 0.0047 | 0.1718 ± 0.0118 |'
- en: '| 500 | 0.8616 ± 0.0014 | 0.8576 ± 0.0011 | 0.6645 ± 0.0029 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 0.8616 ± 0.0014 | 0.8576 ± 0.0011 | 0.6645 ± 0.0029 |'
- en: '| 800 | 0.8669 ± 0.0014 | 0.8612 ± 0.0011 | 0.6841 ± 0.0030 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 800 | 0.8669 ± 0.0014 | 0.8612 ± 0.0011 | 0.6841 ± 0.0030 |'
- en: '| 1000 | 0.8670 ± 0.001 | 0.8592 ± 0.0014 | 0.6874 ± 0.0028 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 0.8670 ± 0.001 | 0.8592 ± 0.0014 | 0.6874 ± 0.0028 |'
- en: '| 2000 | 0.8682 ± 0.0008 | 0.8630 ± 0.0012 | 0.7092 ± 0.0029 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 0.8682 ± 0.0008 | 0.8630 ± 0.0012 | 0.7092 ± 0.0029 |'
- en: '| 3000 | 0.8691 ± 0.0005 | 0.8652 ± 0.0011 | 0.7088 ± 0.0024 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 3000 | 0.8691 ± 0.0005 | 0.8652 ± 0.0011 | 0.7088 ± 0.0024 |'
- en: '| 1000; 500 | 0.8779 ± 0.0011 | 0.8720 ± 0.0011 | 0.1184 ± 0.0033 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 1000; 500 | 0.8779 ± 0.0011 | 0.8720 ± 0.0011 | 0.1184 ± 0.0033 |'
- en: '| 3000; 1500 | 0.8822 ± 0.0007 | 0.8758 ± 0.0009 | 0.1221 ± 0.0001 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 3000; 1500 | 0.8822 ± 0.0007 | 0.8758 ± 0.0009 | 0.1221 ± 0.0001 |'
- en: '| 1000; 500; 250 | 0.8829 ± 0.0011 | 0.8746 ± 0.0012 | 0.1220 ± 0.0000 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 1000; 500; 250 | 0.8829 ± 0.0011 | 0.8746 ± 0.0012 | 0.1220 ± 0.0000 |'
- en: '| 2000; 1000; 500 | 0.8850 ± 0.0007 | 0.8771 ± 0.0010 | 0.1220 ± 0.0000 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 2000; 1000; 500 | 0.8850 ± 0.0007 | 0.8771 ± 0.0010 | 0.1220 ± 0.0000 |'
- en: In each case, we show the mean score on the reduced test set averaged over the
    10 models trained (plus or minus the standard error of the mean). There is quite
    a bit of information in this table, so let’s look at it carefully.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们展示了在减少的测试集上的平均得分，得分是对 10 个训练模型的平均值（加减标准误差）。这张表中包含了大量信息，因此我们需要仔细分析。
- en: If we look at the activation type, we immediately see something is off. The
    results for the logistic activation function show improved scores as the single
    hidden layer gets larger, something we might expect to see, but when we move to
    more than one hidden layer, the network fails to train. We know that it was unable
    to train because the scores on the test set are abysmal. If you check the output,
    you’ll see that the loss values do not go down. If the loss value does not decrease
    while training proceeds, something is wrong.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下激活类型，我们会立刻发现某些地方不对。逻辑激活函数的结果显示，随着单隐藏层的增大，得分有所提高，这是我们可能预期的，但当我们增加隐藏层数量超过一个时，网络无法训练。我们知道它无法训练，因为测试集上的得分非常糟糕。如果你检查输出，你会看到损失值并没有下降。如果在训练过程中，损失值没有减少，那么就说明出现了问题。
- en: It’s not immediately evident why training failed for the logistic activation
    function case. One possibility is a bug in sklearn, but this is rather unlikely
    given how widely used the toolkit is. The most likely culprit has to do with network
    initialization. The sklearn toolkit uses the standard, commonly used initialization
    schemes we discussed in [Chapter 8](ch08.xhtml#ch08). But these are tailored for
    ReLU and tanh activation functions and may not be performing well for the logistic
    case.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么逻辑激活函数的训练失败并不立即显现。一个可能性是sklearn中的错误，但考虑到该工具包的广泛使用，这种情况不太可能。最可能的原因与网络初始化有关。sklearn工具包使用了我们在[第8章](ch08.xhtml#ch08)中讨论过的标准、常用的初始化方案。但这些方案是针对ReLU和tanh激活函数设计的，可能不适合逻辑激活函数。
- en: For our purposes, we can view this failure as a glaring sign that the logistic
    activation function is not a good one to use for the hidden layers. Sadly, this
    is precisely the activation function that was widely used throughout much of the
    early history of neural networks, so we were shooting ourselves in the foot from
    the beginning. No wonder it took so long for neural networks to finally find their
    proper place! From here on out, we’ll ignore the logistic activation function
    results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们可以将这个失败视为一个显著的信号，表明逻辑激活函数并不是适合用于隐藏层的激活函数。可悲的是，这正是神经网络早期历史中广泛使用的激活函数，所以我们从一开始就给自己设置了障碍。难怪神经网络要花这么长时间才最终找到了适合的位置！从现在开始，我们将忽略逻辑激活函数的结果。
- en: 'Consider again the scores for the single hidden layer networks (see [Table
    10-2](ch10.xhtml#ch10tab2), rows 1–6). For the ReLU and tanh activation functions,
    we see a steady improvement in the performance of the networks. Also, note that
    in each case, the ReLU activation function slightly outperforms tanh for the same
    number of nodes in the hidden layer, though these differences are likely not statistically
    significant with only 10 models per architecture. Still, it follows a general
    observation prevalent in the community: ReLU is preferred to tanh.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑单隐藏层网络的得分（见[表10-2](ch10.xhtml#ch10tab2)，第1–6行）。对于ReLU和tanh激活函数，我们看到网络性能有了稳定的改善。此外，注意到在每种情况下，对于相同数量的隐藏层节点，ReLU激活函数的表现稍微优于tanh，尽管这类差异在每种架构只有10个模型的情况下，可能在统计上并不显著。尽管如此，这符合一个在社区中普遍存在的观察：ReLU比tanh更受偏爱。
- en: If we look at the remaining rows of [Table 10-2](ch10.xhtml#ch10tab2), we see
    that adding a second and even third hidden layer continues to improve the test
    scores but with diminishing returns. This is also a widely experienced phenomenon
    that we should look at a little more closely. In particular, we should consider
    the number of parameters in the models of [Table 10-2](ch10.xhtml#ch10tab2). This
    makes the comparison a bit unfair. If, instead, we train models that have closely
    matched numbers of parameters, then we can more fairly compare the performance
    of the models. Any differences in performance we see can be plausibly attributed
    to the number of layers used since the overall number of parameters will be virtually
    the same.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下[表10-2](ch10.xhtml#ch10tab2)的其余行，我们会看到，添加第二个甚至第三个隐藏层继续改善测试得分，但随着层数增加，效果递减。这也是一个广泛经历的现象，我们应该更仔细地研究一下。特别是，我们应该考虑[表10-2](ch10.xhtml#ch10tab2)中模型的参数数量。这使得比较变得有些不公平。如果我们训练的模型具有大致相同数量的参数，那么我们就能更公平地比较模型的表现。我们所看到的任何性能差异都可以合理地归因于使用的层数，因为整体参数数量几乎是一样的。
- en: By modifying the layers array in [Listing 10-2](ch10.xhtml#ch10lis2), we can
    train multiple versions of the architectures shown in [Table 10-3](ch10.xhtml#ch10tab3).
    The number of nodes per layer was selected to parallel the overall number of parameters
    in the models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改[清单 10-2](ch10.xhtml#ch10lis2)中的层数组，我们可以训练[表 10-3](ch10.xhtml#ch10tab3)中显示的架构的多个版本。每层的节点数量是根据模型中整体参数数量来选择的。
- en: '**Table 10-3:** Model Architectures Tested to Produce [Figure 10-1](ch10.xhtml#ch10fig1)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-3：** 为生成[图 10-1](ch10.xhtml#ch10fig1)而测试的模型架构'
- en: '| Architecture | Number of parameters |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 参数数量 |'
- en: '| --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1000 | 795,010 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 795,010 |'
- en: '| 2000 | 1,590,010 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 1,590,010 |'
- en: '| 4000 | 3,180,010 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 4000 | 3,180,010 |'
- en: '| 8000 | 6,360,010 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 8000 | 6,360,010 |'
- en: '| 700; 350 | 798,360 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 700; 350 | 798,360 |'
- en: '| 1150; 575 | 1,570,335 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 1150; 575 | 1,570,335 |'
- en: '| 1850; 925 | 3,173,685 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 1850; 925 | 3,173,685 |'
- en: '| 2850; 1425 | 6,314,185 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 2850; 1425 | 6,314,185 |'
- en: '| 660; 330; 165 | 792,505 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 660; 330; 165 | 792,505 |'
- en: '| 1080; 540; 270 | 1,580,320 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 1080; 540; 270 | 1,580,320 |'
- en: '| 1714; 857; 429 | 3,187,627 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 1714; 857; 429 | 3,187,627 |'
- en: '| 2620; 1310; 655 | 6,355,475 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 2620; 1310; 655 | 6,355,475 |'
- en: Where did the magic numbers in [Table 10-3](ch10.xhtml#ch10tab3) come from?
    We first picked the single-layer sizes we wanted to test. We then determined the
    number of parameters in models with those architectures. Next, we crafted two-layer
    architectures using the rules of thumb from [Chapter 8](ch08.xhtml#ch08) so that
    the number of parameters in those models will be close to the corresponding number
    of parameters in the single-layer models. Finally, we repeated the process for
    three-layer models. Doing things this way lets us compare the performance of the
    models for very similar numbers of parameters. In essence, we’re fixing the number
    of parameters in the model and altering only the way they interact with each other.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，[表 10-3](ch10.xhtml#ch10tab3)中的魔法数字是从哪里来的呢？我们首先选择了想要测试的单层尺寸。然后，我们确定了这些架构中模型的参数数量。接下来，我们使用[第
    8 章](ch08.xhtml#ch08)中的经验法则构建了两层架构，使得这些模型中的参数数量接近单层模型中的对应数量。最后，我们对三层模型重复了这个过程。这样做使我们能够比较具有相似参数数量的模型性能。实质上，我们固定了模型中的参数数量，仅改变它们之间的交互方式。
- en: Training models as we did in [Listing 10-2](ch10.xhtml#ch10lis2), but this time
    averaging 25 models instead of just 10, gives us [Figure 10-1](ch10.xhtml#ch10fig1).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们在[清单 10-2](ch10.xhtml#ch10lis2)中做的那样训练模型，但这次平均了 25 个模型而不是仅仅 10 个，得到了[图 10-1](ch10.xhtml#ch10fig1)。
- en: '![image](Images/10fig01.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig01.jpg)'
- en: '*Figure 10-1: Scores (mean ± E) on the MNIST test set for the architectures
    of [Table 10-3](ch10.xhtml#ch10tab3) as a function of the number of parameters
    in the network*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-1：基于网络中参数数量的架构在 MNIST 测试集上的得分（均值 ± E），参考[表 10-3](ch10.xhtml#ch10tab3)*'
- en: Let’s parse [Figure 10-1](ch10.xhtml#ch10fig1). First, note that the x-axis,
    the number of parameters in the model, is given in millions. Second, we can compare
    the three lines going vertically as those models all have similar numbers of parameters.
    The legend tells us which plot represents models with one, two, or three hidden
    layers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解析[图 10-1](ch10.xhtml#ch10fig1)。首先，注意横轴是模型中的参数数量，以百万为单位。其次，我们可以通过比较三条竖直的线来判断，这些模型的参数数量相似。图例告诉我们哪个图表示具有一层、两层或三层隐藏层的模型。
- en: 'Looking at the leftmost points, representing the smallest models in each case,
    we see that changing from a single layer to two layers gives us a jump in model
    performance. Also, moving from two layers to three results in another, smaller
    rise. This repeats for all the layer sizes moving left to right. We’ll address
    the dip in performance between the two largest models for single- and double-layer
    architectures in a bit. Fixing the number of parameters but increasing the depth
    of the network (number of layers) results in better performance. We might be tempted
    here to say, “Go deep, not wide,” but there will be cases where this doesn’t work.
    Still, it’s worth remembering: more layers can help, not just a wider layer with
    more nodes.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 看看最左侧的点，代表每种情况中最小的模型，我们看到从单层到两层的变化给我们带来了模型性能的跃升。同样，从两层到三层的变化则带来了另一个较小的提升。这种情况在从左到右的所有层次大小中都会出现。稍后我们会讨论单层和双层架构中两个最大模型之间性能下降的问题。固定参数数量，但增加网络的深度（层数），会带来更好的性能。我们可能会在这里忍不住说：“深度比宽度更重要”，但也有情况会不适用。尽管如此，值得记住的是：更多的层次有助于提升性能，而不仅仅是增加更多节点的宽层。
- en: What about the dip for the largest models in the one- and two-layer cases? These
    are the rightmost points of [Figure 10-1](ch10.xhtml#ch10fig1). Recall, the models
    used to make the plot were trained with only 1,000 samples each. For the largest
    models, there likely wasn’t enough data to adequately train such a wide model.
    If we were to increase the number of training samples, which we can do because
    we have 60,000 to choose from for MNIST, we might see the dip go away. I’ll leave
    this as an exercise for the reader.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在一层和两层情况下，最大的模型的性能下降如何呢？这些点是[图10-1](ch10.xhtml#ch10fig1)中的最右边部分。回想一下，绘制这个图的模型每个只用了1,000个样本进行训练。对于最大的模型，可能没有足够的数据来充分训练这样一个大规模的模型。如果我们增加训练样本的数量，因为MNIST数据集有60,000个样本可供选择，或许就能看到这个性能下降消失。我把这个作为练习留给读者。
- en: Batch Size
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量大小
- en: Let’s now turn our attention to how batch size affects training. Recall that
    here *batch size* means minibatch size, a subset of the full training set used
    in the forward pass to calculate the average loss over the minibatch. From this
    loss, we use backprop to update the weights and biases. Processing a single minibatch,
    then, results in a single gradient-descent step—a single update to the parameters
    of the network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们关注批量大小如何影响训练。回想一下，这里所说的*批量大小*指的是小批量大小，即在前向传播中用于计算小批量的平均损失的训练集子集。通过这个损失，我们使用反向传播来更新权重和偏置。处理一个小批量，因此，会导致一次梯度下降步骤——即网络参数的单次更新。
- en: We’ll train a fixed-size subset of MNIST for a set number of epochs with different
    minibatch sizes to see how that affects the final test scores. Before we do that,
    however, we need to understand, for epochs and minibatches, the process sklearn
    uses to train a neural network.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对MNIST数据集的一个固定大小子集进行训练，设定一定的训练轮次，并使用不同的小批量大小，看看这如何影响最终的测试分数。然而，在此之前，我们需要了解，对于训练轮次和小批量，sklearn在训练神经网络时所使用的过程。
- en: 'Let’s look briefly at the actual sklearn source code for the `MLPClassifier`
    class, in the `_fit_stochastic` method, found at [https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py).
    Understanding that this method is an internal one and might change from version
    to version, we see code that looks like this:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要看一下sklearn中`MLPClassifier`类的实际源代码，在`_fit_stochastic`方法中，代码位于[https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py](https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py)。理解到这个方法是内部方法，且可能会随着版本的更新而改变，我们看到的代码如下所示：
- en: '[PRE4]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There are two `for` loops, the first over the number of epochs (`max_iter`),
    and the second over the number of minibatches present in the training data. The
    `gen_batches` function returns minibatches from the training set. In reality,
    it returns slice indices with `X[batch_slice]` returning the actual training samples,
    but the effect is the same. The calls to `_backprop` and `update_params` complete
    the gradient descent step for the current minibatch.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个`for`循环，第一个是遍历训练轮次（`max_iter`），第二个是遍历训练数据中的小批量数量。`gen_batches`函数从训练集返回小批量数据。实际上，它返回的是切片索引，通过`X[batch_slice]`来返回实际的训练样本，但效果是一样的。调用`_backprop`和`update_params`完成当前小批量的梯度下降步骤。
- en: An *epoch* is a full pass through the minibatches present in the training set.
    The minibatches themselves are groupings of the training data so that looping
    over the minibatches uses all the samples in the training set once. If the number
    of training samples is not an integer multiple of the minibatch size, the final
    minibatch will be smaller than expected, but that will not affect training in
    the long run.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*训练轮次*是指对训练集中的小批量数据进行完整的遍历。小批量本身是训练数据的分组，因此遍历小批量会使用训练集中的所有样本一次。如果训练样本的数量不是小批量大小的整数倍，那么最后一个小批量会比预期的要小，但这不会影响训练的长期效果。
- en: We can view this graphically as in [Figure 10-2](ch10.xhtml#ch10fig2), where
    we see how an epoch is built from the minibatches in the training set. In [Figure
    10-2](ch10.xhtml#ch10fig2), the entire training set is represented as the epoch
    with *n* samples. A minibatch has *m* samples, as indicated. The last minibatch
    is smaller than the rest to indicate that the *n*/*m* might not be an integer.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过[图10-2](ch10.xhtml#ch10fig2)以图形化的方式查看这一点，在那里我们可以看到一个训练轮次是如何由训练集中的小批量组成的。在[图10-2](ch10.xhtml#ch10fig2)中，整个训练集被表示为包含*n*个样本的一个训练轮次。一个小批量有*m*个样本，如图所示。最后一个小批量比其他的要小，这表示*n*/*m*可能不是整数。
- en: '![image](Images/10fig02.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig02.jpg)'
- en: '*Figure 10-2: The relationship between epochs (*n*), minibatches (*m*), and
    samples* {x[*0*],x[*1*], …,x[*n-1*]}'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-2：周期数（*n*）、小批量（*m*）和样本（{x[*0*],x[*1*], …,x[*n-1*]}）之间的关系*'
- en: '[Figure 10-2](ch10.xhtml#ch10fig2) also implies that the order of the samples
    in the training set is essential, which is why we shuffled the datasets when we
    made them. The sklearn toolkit will also rearrange the samples after every epoch
    during training if desired. As long as a minibatch is, statistically, a random
    sample from the training set as a whole, things should be okay. If the minibatch
    is not, then it might give a biased view of the gradient direction during backprop.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-2](ch10.xhtml#ch10fig2) 也意味着训练集中的样本顺序至关重要，这就是我们在创建数据集时打乱样本顺序的原因。如果需要，sklearn
    工具包还会在每个训练周期后重新排列样本。只要小批量在统计上是从整个训练集中随机选取的样本，通常情况应该没问题。如果小批量不是这样，那么在反向传播过程中它可能会给出一个有偏的梯度方向。'
- en: Our minibatch experiment will fix the number of MNIST training samples at 16,384
    while we vary the minibatch size. We’ll also fix the number of epochs at 100\.
    The scores we report are the mean and standard error for five different runs of
    the same model, each with a different random initialization. The `MLPClassifier`
    object is therefore instantiated via
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小批量实验将固定 MNIST 训练样本数为 16,384，同时变化小批量大小。我们还将固定训练周期数为 100。我们报告的分数是五次不同运行的平均值和标准误差，每次运行都有不同的随机初始化。因此，`MLPClassifier`
    对象是通过以下方式实例化的：
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code indicates that all of the models have two hidden layers of 1,000 and
    500 nodes, respectively, making the architecture of the entire network 784-1000-500-10
    when adding in the nodes of the input and output layers. The only parameter that
    varies when defining a network is the `batch_size`. We’ll use the batch sizes
    in [Table 10-4](ch10.xhtml#ch10tab4) along with the number of gradient descent
    steps taken for each epoch (see [Figure 10-2](ch10.xhtml#ch10fig2)).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码表明，所有的模型都有两个隐藏层，分别包含 1,000 和 500 个节点，整体网络架构为 784-1000-500-10，包含输入层和输出层的节点数。当定义网络时，唯一变化的参数是
    `batch_size`。我们将使用 [表 10-4](ch10.xhtml#ch10tab4) 中的小批量大小，并结合每个周期的梯度下降步数（见 [图 10-2](ch10.xhtml#ch10fig2)）。
- en: '**Table 10-4:** Minibatch Sizes and the Corresponding Number of Gradient Descent
    Steps per Epoch'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-4：** 小批量大小与每个周期梯度下降步数的对应关系'
- en: '| Minibatch size | SGD steps per epoch |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 小批量大小 | 每个周期的 SGD 步数 |'
- en: '| --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 2 | 8,192 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8,192 |'
- en: '| 4 | 4,096 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4,096 |'
- en: '| 8 | 2,048 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2,048 |'
- en: '| 16 | 1,024 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 1,024 |'
- en: '| 32 | 512 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 512 |'
- en: '| 64 | 256 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 256 |'
- en: '| 128 | 128 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 128 |'
- en: '| 256 | 64 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 64 |'
- en: '| 512 | 32 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 32 |'
- en: '| 1,024 | 16 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 1,024 | 16 |'
- en: '| 2,048 | 8 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 2,048 | 8 |'
- en: '| 4,096 | 4 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 4,096 | 4 |'
- en: '| 8,192 | 2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 8,192 | 2 |'
- en: '| 16,384 | 1 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 16,384 | 1 |'
- en: When the minibatch size is 2, over 8,000 gradient descent steps will be taken
    per epoch, but when the minibatch size is 8,192, only 2 gradient descent steps
    are taken. Fixing the number of epochs should favor a smaller minibatch size since
    there will be correspondingly more gradient descent steps, implying more opportunity
    to move toward the optimal set of network parameters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当小批量大小为 2 时，每个周期将进行超过 8,000 次梯度下降步骤，但当小批量大小为 8,192 时，仅进行 2 次梯度下降步骤。固定训练周期数应该有利于选择较小的小批量大小，因为相应地会有更多的梯度下降步骤，这意味着有更多机会朝着最优的网络参数集前进。
- en: '[Figure 10-3](ch10.xhtml#ch10fig3) plots the mean score as a function of the
    minibatch size. The code that generated the data for the plot is in the *mnist_nn_experiments
    _batch_size.py* file. The plotting code itself is in *mnist_nn_experiments_batch
    _size_plot.py*. The curve that concerns us for the moment is the one using circles.
    We’ll explain the square symbol curve shortly.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-3](ch10.xhtml#ch10fig3) 显示了小批量大小对平均分数的影响。生成该图的数据代码位于 *mnist_nn_experiments_batch_size.py*
    文件中。绘图代码本身位于 *mnist_nn_experiments_batch_size_plot.py* 文件中。此刻我们关注的曲线是使用圆圈标记的那条。我们稍后会解释使用方形符号的曲线。'
- en: '![image](Images/10fig03.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig03.jpg)'
- en: '*Figure 10-3: Average score on the MNIST test set as a function of minibatch
    size (mean* ± *SE) for a fixed number of epochs (100) regardless of the minibatch
    size (circles) or a fixed number of minibatches (squares)*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-3：固定周期数（100）下，MNIST 测试集上平均分数与小批量大小（均值* ± *标准误差）之间的关系，无论小批量大小（圆圈）还是固定小批量数（方形）*'
- en: 'Here we’ve fixed the number of epochs at 100, so by varying the minibatch size,
    we vary the number of gradient steps: the larger the minibatch, the *fewer* gradient
    steps we take. Because the minibatch is larger, the steps themselves are based
    on a more faithful representation of the actual gradient direction; however, the
    number of steps is reduced because there are fewer minibatches per epoch, leading
    to poorer convergence: we are not reaching a good minimum of the loss function.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将迭代次数固定为100，因此通过改变小批量大小，我们改变了梯度步骤的数量：小批量越大，梯度步骤就越少。由于小批量更大，步骤本身基于对实际梯度方向的更真实表示；然而，由于每个迭代中的小批量较少，步骤数量减少，导致收敛变差：我们没有达到损失函数的良好最小值。
- en: A more “fair” test might be to see what happens when we adjust the number of
    epochs so that the number of *minibatches* examined is constant regardless of
    the minibatch size. One way to do that is to note that the number of minibatches
    per epoch is *n*/*m*, where *n* is the number of training samples, and *m* is
    the number of minibatches. If we call the overall number of minibatches we want
    to run *M*, then, to hold it fixed, we need to set the number of *epochs* to
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更“公平”的测试可能是，观察当我们调整迭代次数，使得检查的小批量数量在任何情况下都是恒定时会发生什么。实现这一点的一种方法是注意到每个迭代中的小批量数量是
    *n*/*m*，其中 *n* 是训练样本数，*m* 是小批量数量。如果我们将希望执行的总体小批量数量称为 *M*，那么为了保持其恒定，我们需要设置迭代次数为
- en: '![image](Images/235equ01.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/235equ01.jpg)'
- en: so that regardless of *m*, we perform a total of *M* gradient descent steps
    during training.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，无论 *m* 为何，我们在训练过程中都会执行 *M* 次梯度下降步骤。
- en: Let’s keep the same set of minibatches but alter the number of epochs according
    to the preceding equation. We need to select *M*, the overall number of minibatches
    (gradient descent steps). Let’s set it to *M* = 8,192 so that the number of epochs
    is an integer in each case. When the minibatch size is 2, we use one epoch to
    get 8,192 minibatches. And when the minibatch size is 16,384 (*n* is still also
    16,384 samples), we get 8,192 epochs. If we do this, we get a completely different
    set of results, the square symbol curve in [Figure 10-3](ch10.xhtml#ch10fig3),
    where we see that the mean score is pretty much a constant representing the constant
    number of gradient descent updates performed during training. When the minibatch
    size is small, corresponding to points near 0 in [Figure 10-3](ch10.xhtml#ch10fig3),
    we do see a degradation in performance, but after a certain minibatch size, the
    performance levels off, reflecting the constant number of gradient descent updates
    combined with a reasonable estimate of the true gradient from using a large enough
    minibatch.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们保持相同的小批量集合，但根据前面的公式调整迭代次数。我们需要选择 *M*，即总体的小批量数量（梯度下降步骤）。我们将其设置为 *M* = 8,192，以便每种情况下迭代次数都是整数。当小批量大小为2时，我们使用一个迭代来获取8,192个小批量。而当小批量大小为16,384（*n*
    仍然是16,384个样本）时，我们得到8,192个迭代。如果我们这样做，我们会得到完全不同的结果，正如在[图10-3](ch10.xhtml#ch10fig3)中看到的方形符号曲线一样，我们看到平均得分几乎是一个常数，代表了训练过程中执行的恒定梯度下降更新次数。当小批量大小较小时，对应于[图10-3](ch10.xhtml#ch10fig3)中的接近0的点，我们确实看到性能下降，但在达到一定的小批量大小后，性能趋于平稳，反映出梯度下降更新的恒定次数，并且通过使用足够大的小批量，合理地估计了真实的梯度。
- en: For the set of base neural network parameters, specifically for a fixed learning
    rate, fixing the number of epochs results in reduced performance because of the
    design of sklearn. Fixing the number of minibatches examined results in mainly
    constant performance.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组基础神经网络参数，特别是对于固定学习率，固定迭代次数会导致性能下降，这是由于 sklearn 的设计所致。固定检查的小批量数量会导致性能保持基本恒定。
- en: Base Learning Rate
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础学习率
- en: 'In [Chapter 9](ch09.xhtml#ch09), we introduced the basic equation for updating
    the weights of a neural network during training:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)中，我们介绍了训练过程中更新神经网络权重的基本公式：
- en: '*w* ← *w* – *η*Δ*w*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*w* ← *w* – *η*Δ*w*'
- en: Here *η* (eta) is the learning rate, the parameter that controls the step size
    based on the gradient value, *Δw*. In sklearn, *η* is specified via the `learning`
    `_rate_init` parameter. During training, the learning rate is often reduced, so
    that the step sizes get smaller the closer we get to the training minimum (hopefully!).
    For our experiments here, however, we’re using a constant learning rate, so whatever
    value we set `learning_rate_init` to persists throughout the entire training session.
    Let’s see how this value affects learning.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *η*（希腊字母 eta）表示学习率，是基于梯度值 *Δw* 控制步长的参数。在 sklearn 中，*η* 通过 `learning` `_rate_init`
    参数指定。在训练过程中，学习率通常会降低，以便步长在接近训练最小值时逐渐变小（希望如此！）。然而，在我们的实验中，我们使用的是恒定的学习率，因此我们设置的
    `learning_rate_init` 值在整个训练过程中都保持不变。让我们看看这个值如何影响学习过程。
- en: For this experiment, we fix the minibatch size at 64 samples and the architecture
    to (1000,500), meaning two hidden layers with 1,000 and 500 nodes, respectively.
    We then look at two main effects. The first is what we get when we fix the number
    of epochs regardless of the base learning rate. In this case, we’ll always take
    a set number of gradient descent steps during training. The second case fixes
    the *product* of the base learning rate and the number of epochs. This case is
    interesting because it looks at the effect on the test score of fewer large steps
    versus many small steps. The code for these experiments is in *mnist_experiments_base_lr.py*.
    The training set is the first 20,000 MNIST samples.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将小批量大小固定为64个样本，架构设置为（1000,500），即有两个隐藏层，分别包含1000和500个节点。接着，我们观察两个主要效果。第一个是固定训练轮次时，不考虑基础学习率的影响。在这种情况下，我们在训练期间始终进行固定数量的梯度下降步骤。第二种情况固定基础学习率与训练轮次的乘积。这种情况很有趣，因为它研究了较少的大步与更多的小步对测试成绩的影响。这些实验的代码在
    *mnist_experiments_base_lr.py* 中。训练集使用的是前20,000个MNIST样本。
- en: 'The first experiment fixes the epochs at 50 and loops over different base learning
    rates:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验固定训练轮次为50，并对不同的基础学习率进行循环测试：
- en: '[PRE6]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The second uses the same base learning rates but varies the number of epochs
    so that in each case the product of the base learning rate and epochs is 1.5\.
    This leads to the following number of epochs matched to the preceding base learning
    rates:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个实验使用相同的基础学习率，但调整训练轮次，使得每种情况中基础学习率与训练轮次的乘积都为1.5。这将导致以下与之前基础学习率相匹配的训练轮次数：
- en: '[PRE7]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Running the two experiments takes some time. When they’re complete, we can plot
    the test score as a function of the base learning rate size. Doing this gives
    us [Figure 10-4](ch10.xhtml#ch10fig4).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这两个实验需要一些时间。完成后，我们可以将测试成绩与基础学习率大小的关系绘制成图。通过这种方式，我们得到了 [图 10-4](ch10.xhtml#ch10fig4)。
- en: '![image](Images/10fig04.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig04.jpg)'
- en: '*Figure 10-4: MNIST test scores as a function of the base learning rate. The
    circles represent the fixed epochs case. The squares are the fixed product of
    the base learning rate and the epochs case.*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-4：MNIST 测试成绩与基础学习率的关系。圆圈代表固定训练轮次的情况。方框表示基础学习率与训练轮次乘积固定的情况。*'
- en: '[Figure 10-4](ch10.xhtml#ch10fig4) shows two plots. In the first plot, using
    circles, the number of epochs was fixed at 50\. Fixing the number of epochs fixes
    the number of gradient descent steps taken during training. We then vary the learning
    rate. The larger the learning rate, the bigger the steps we’ll take.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-4](ch10.xhtml#ch10fig4) 显示了两个图。第一个图中，使用圆圈表示，训练轮次固定为50。固定训练轮次意味着训练期间梯度下降步骤的数量固定。然后，我们调整学习率。学习率越大，步骤越大。'
- en: Imagine walking over a football field, attempting to get to the very center
    from one of the corners in a limited number of steps. If we take large steps,
    we might move over a lot of ground quickly, but we won’t be able to zero in on
    the center because we’ll keep stepping past it. If we take tiny steps, we’ll cover
    only a short distance from the corner toward the center. We might be on track,
    but since we’re allowed only a certain number steps, we can’t reach the center.
    Intuitively, we can perhaps convince ourselves that there is a sweet spot where
    the step size and the number of steps we get to take combine to get us to the
    center.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你走过一个足球场，试图从一个角落走到正中心，且步数有限。如果我们迈大步，可能很快就走过了很多地面，但我们无法精确地走到中心，因为我们会一直走过它。如果我们迈小步，从角落到中心的距离就很短。虽然我们可能在正确的轨道上，但由于步数有限，我们无法到达中心。直观上，我们或许可以说，存在一个最佳步长与步数的组合，能够让我们到达中心。
- en: We see this effect in the circle plot of [Figure 10-4](ch10.xhtml#ch10fig4).
    The leftmost point represents the case of tiny steps. We do relatively poorly
    because we haven’t traversed enough of the error space to find the minimum. Similarly,
    the rightmost point represents taking very large steps. We do poorly because we
    keep stepping past the minimum. The best score happens when the number of steps
    we get to make and the size of those steps work together to move us to the minimum.
    In the figure, this happens when the base learning rate is 0.1.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 10-4](ch10.xhtml#ch10fig4)中的圆形图中看到了这种效果。最左边的点代表了微小步伐的情况。我们表现较差，因为我们没有遍历足够的误差空间来找到最小值。类似地，最右边的点代表了采取非常大步伐的情况。我们表现不佳，因为我们总是跨越最小值。最佳得分出现在我们可以采取的步数和步伐大小配合得当时，将我们推向最小值。在图中，这发生在基础学习率为
    0.1 时。
- en: Now let’s look at the square symbol plot in [Figure 10-4](ch10.xhtml#ch10fig4).
    This plot comes from the scores found when the product of the base learning rate
    and the number of epochs is constant, meaning small learning rates will run for
    a large number of epochs. For the most part, the test scores are the same for
    all base learning rates except the very largest. In our walking over the football
    field thought experiment, the square symbol plot corresponds to taking a few large
    steps or very many small steps. We can imagine both approaches getting us close
    to the center of the field, at least until our step size is too large to let us
    land at the center.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下[图 10-4](ch10.xhtml#ch10fig4)中的方形符号图。这张图来自于当基础学习率和训练周期数的乘积保持恒定时得到的得分，这意味着较小的学习率将会运行很长的训练周期。大部分情况下，除了最大值之外，所有基础学习率的测试得分都差不多。在我们的足球场思维实验中，方形符号图对应的是采取少数几步大步伐或非常多的小步伐。我们可以想象这两种方法都会让我们接近场地的中心，至少在我们的步伐大小适当时，能让我们到达中心。
- en: Some readers might be objecting at this point. If we compare the first three
    points of both the circle and square plots in [Figure 10-4](ch10.xhtml#ch10fig4),
    we see a large gap. For the circles, the performance improves as the base learning
    rate increases. For the squares, however, the performance remains high and constant
    regardless of the base learning rate. For the circles, we trained for 50 epochs,
    always. This is a more significant number of epochs than were used for the squares
    plot for the corresponding base learning rates. This means that in the circles’
    case, we stomped around quite a bit after we got near the center of the field.
    For the case of the squares, however, we limited the number of epochs, so we stopped
    walking when we were near the center of the field, hence the improved performance.
    This implies that we need to adjust the number of epochs (gradient descent steps
    taken) to match the learning rate so that we get near to the minimum of the loss
    function quickly, without a lot of stomping around, but not so quickly that we
    are taking large steps that won’t let us converge on the minimum.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一些读者此时可能会提出异议。如果我们比较[图 10-4](ch10.xhtml#ch10fig4)中圆形和方形图的前三个点，会看到一个较大的差距。对于圆形图，性能随着基础学习率的增加而提升。然而，对于方形图，性能无论基础学习率如何都保持较高且恒定。对于圆形图，我们始终训练了
    50 个周期。这是一个比对应基础学习率下方形图使用的训练周期更多的周期数。这意味着在圆形图的情况下，我们在接近场地中心后走了相当多的步。相反，对于方形图，我们限制了训练周期数，所以在接近场地中心时就停止了，这也是性能得到提升的原因。这意味着我们需要调整训练周期数（即梯度下降的步数），以匹配学习率，从而快速接近损失函数的最小值，而不会走太多冤枉路，但也不能太快，以至于步伐过大，导致无法收敛到最小值。
- en: Thus far we’ve been holding the learning rate constant throughout training.
    Because of space considerations, we can’t fully explore the effect of changing
    the learning rate during training. Still, we can at least use our football field
    thought experiment to help us visualize why changing the learning rate during
    training makes sense. Recall, the network is initialized intelligently but randomly.
    This means we start somewhere on the field at random. The odds are low that this
    arbitrary position is near the center, the minimum of the error surface, so we
    do need to apply gradient descent to move us closer to the center. At first, we
    might as well take significant steps to move quickly through the field. Since
    we are following the gradient, this moves us toward the center. If we keep taking
    large steps, however, we might overshoot the center. After taking a few large
    steps, we might think it wise to start taking smaller steps, believing that we
    are now closer to our goal of reaching the center. The more we walk, the smaller
    our steps so we can get as close to the center as possible. This is why the learning
    rate is typically reduced during training.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直保持学习率在训练过程中不变。由于空间限制，我们无法全面探讨在训练过程中更改学习率的影响。不过，我们至少可以利用我们的足球场思维实验来帮助我们理解为何在训练过程中更改学习率是合理的。回想一下，网络是通过智能但随机的方式初始化的。这意味着我们在场地上的某个位置随机开始。这个随机位置离场地中心（即误差表面的最小值）可能很远，因此我们需要应用梯度下降法将我们带向中心。最开始，我们可以采取较大的步伐，以便快速穿过场地。由于我们跟随梯度，这样就能朝着中心移动。然而，如果我们继续采取大步伐，可能会越过中心。走了几步大步之后，我们可能会认为开始采取较小的步伐比较明智，因为我们认为现在已经离目标中心更近了。我们走得越远，步伐就越小，以便尽可能接近中心。这就是为什么在训练过程中学习率通常会逐渐减小的原因。
- en: Training Set Size
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练集大小
- en: We’ve mentioned that the number of samples in the training set affects performance
    significantly. Let’s use the MNIST data to quantify this assertion. For this experiment,
    we’ll vary the number of training set samples while adjusting the number of epochs
    so that in each case, we take (approximately) 1,000 gradient descent steps during
    training. The code for this experiment is in *mnist_nn_experiments_samples.py*.
    In all cases, the minibatch size is 100, and the architecture of the network has
    two hidden layers of 1,000 and 500 nodes, respectively. [Figure 10-5](ch10.xhtml#ch10fig5)
    shows the results of this experiment.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，训练集中的样本数量会显著影响性能。让我们使用 MNIST 数据来量化这个说法。在这个实验中，我们将变化训练集的样本数量，同时调整训练轮数（epoch），确保在每种情况下，我们在训练期间大约进行
    1,000 次梯度下降步骤。这个实验的代码在 *mnist_nn_experiments_samples.py* 中。所有情况下，最小批量大小为 100，网络架构有两个隐藏层，分别包含
    1,000 和 500 个节点。[图 10-5](ch10.xhtml#ch10fig5) 显示了该实验的结果。
- en: '![image](Images/10fig05.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig05.jpg)'
- en: '*Figure 10-5: MNIST test scores as a function of the number of training samples*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-5：MNIST 测试分数与训练样本数量的关系*'
- en: '[Figure 10-5](ch10.xhtml#ch10fig5) is particularly satisfying because it shows
    exactly what we’d expect to see. If we have too little training data, we cannot
    learn to generalize well because we’re training the model with a very sparse sample
    from the parent distribution. As we add more and more training data, we’d expect
    a potentially rapid rise in the performance of the network since the training
    set is a better and better sample of the parent distribution we’re asking the
    model to learn.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](ch10.xhtml#ch10fig5) 特别令人满意，因为它展示了我们预期看到的结果。如果训练数据太少，我们无法很好地学习到泛化能力，因为我们用来自父分布的非常稀疏的样本来训练模型。随着训练数据的不断增加，我们预计网络的性能会迅速提高，因为训练集变得越来越接近我们希望模型学习的父分布。'
- en: '[Figure 10-5](ch10.xhtml#ch10fig5) shows that increasing the training set size
    results in diminishing returns. Moving from 1,000 to 5,000 training set samples
    results in a substantial improvement in performance, but moving from 5,000 to
    even 10,000 samples gives us only a small performance boost, and further increases
    in the training set size level off at some ceiling performance. We can think of
    this level region as having reached some capacity—that the model has pretty much
    learned all it will learn from the dataset. At this point, we might think of enlarging
    the network architecture to see if we get a jump in test set scores provided we
    have enough training samples available.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](ch10.xhtml#ch10fig5)显示，增大训练集的大小会导致收益递减。从1,000个训练样本增加到5,000个训练样本，性能有了显著提升，但从5,000个样本增加到10,000个样本时，性能提升变得很小，进一步增加训练集大小的效果逐渐趋于平缓，达到某个性能上限。我们可以将这个水平区域视为模型已经从数据集中学到了它能学到的东西。此时，我们可能会考虑扩展网络架构，看看是否能在有足够训练样本的情况下提升测试集的得分。'
- en: L2 Regularization
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L2 正则化
- en: In [Chapter 9](ch09.xhtml#ch09), we discussed regularization techniques that
    improve network generalization, including L2 regularization. We saw that L2 regularization,
    which adds a new term to the loss function during training, is functionally equivalent
    to weight decay and penalizes the network during training if the weights get large.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#ch09)中，我们讨论了提高网络泛化能力的正则化技术，包括L2正则化。我们看到，L2正则化在训练过程中向损失函数中添加了一个新项，本质上等同于权重衰减，如果权重过大，训练时会对网络进行惩罚。
- en: In sklearn, the parameter controlling the strength of L2 regularization is `alpha`.
    If this parameter is 0, there is no L2 regularization, while the regularization
    increases in intensity as `alpha` increases. Let’s explore the effect of L2 regularization
    on our MNIST networks.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在sklearn中，控制L2正则化强度的参数是`alpha`。如果此参数为0，则没有L2正则化；而随着`alpha`的增加，正则化的强度也会增强。让我们来探索L2正则化对MNIST网络的影响。
- en: For this experiment, we’ll fix the minibatch size at 64\. We’ll also set the
    momentum to 0 so that the effect we see is due to L2 regularization alone. Finally,
    we’ll use a smaller network with two hidden layers of 100 and 50 nodes each and
    a small training set of the first 3,000 MNIST samples. The code is in *mnist_nn_experiments_L2.py*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们将小批量大小固定为64。我们还将动量设置为0，这样我们看到的效果就完全是L2正则化的影响。最后，我们将使用一个包含100和50个节点的两层隐藏网络，并选取前3,000个MNIST样本作为一个小的训练集。代码位于*mnist_nn_experiments_L2.py*。
- en: Unlike the previous experiments, in this case, we’d like to evaluate the test
    data after each training epoch so that we can watch the network learn over the
    training process. If it is learning, the error on the test set will go down as
    the number of training epochs increases. We know that sklearn will loop over all
    the minibatches in the dataset for one epoch, so we can set the number of training
    epochs to 1\. However, if we set `max_iter` to 1 and then call the `fit` method,
    the next time we call `fit`, we’ll start over with a newly initialized network.
    This won’t help us at all; we need to preserve the weights and biases between
    calls to `fit`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的实验不同，在这种情况下，我们希望在每个训练周期后评估测试数据，以便我们可以观察网络在训练过程中的学习情况。如果网络正在学习，随着训练周期数的增加，测试集上的误差应该会下降。我们知道，sklearn
    会在一个周期内遍历数据集中的所有小批量数据，因此我们可以将训练周期数设置为1。然而，如果我们将`max_iter`设置为1并调用`fit`方法，下次调用`fit`时，网络将重新初始化开始训练，这样并不能帮助我们；我们需要在多次调用`fit`之间保留权重和偏置。
- en: Fortunately for us, the creators of sklearn thought ahead and added the `warm_start`
    parameter. If this parameter is set to `True`, a call to `fit` will *not* re-initialize
    the network but will use the existing weights and biases. If we set `max_iter`
    to 1 and `warm_start` to `True`, we’ll be able to watch the network learn by calling
    `score` after each epoch of training. Calling `score` gives us the accuracy on
    the test data. If we want the error, the value we need to track is 1 – `score`.
    This is the value we plot as a function of epoch. The `alpha` values we’ll plot
    are
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，幸运的是，sklearn 的创建者考虑得很周到，加入了`warm_start`参数。如果将此参数设置为`True`，调用`fit`时将*不会*重新初始化网络，而是使用现有的权重和偏置。如果我们将`max_iter`设置为1并将`warm_start`设置为`True`，我们就可以通过在每个训练周期后调用`score`来观察网络的学习过程。调用`score`会给出测试数据上的准确度。如果我们想要误差，需要跟踪的值是1
    – `score`。这是我们绘制的随周期变化的值。我们将绘制的`alpha`值是
- en: '[PRE8]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We’ve made these rather large compared to the default so we can see the effect.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与默认设置相比，我们将这些参数设置得相当大，以便观察其效果。
- en: 'Focusing on the test error only, the code for evaluating a single epoch is:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 仅关注测试误差，评估单个周期的代码如下：
- en: '[PRE9]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, `fit` is called to perform one epoch of training. Then we calculate the
    error on the test set and store it in `val_err`. Setting `warm_start` to `True`
    after calling `fit` ensures that the first call to `epoch` will properly initialize
    the network, but subsequent calls will keep the weights and biases from the previous
    call.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，调用`fit`进行一次训练周期。然后，我们计算测试集上的误差，并将其存储在`val_err`中。调用`fit`后将`warm_start`设置为`True`，确保第一次调用`epoch`时会正确初始化网络，但随后的调用将保持上一次调用的权重和偏置。
- en: 'Training then happens in a simple loop:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程发生在一个简单的循环中：
- en: '[PRE10]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This loop collects the per epoch results and returns them to the `main` function,
    which itself loops over the *α* values we’re interested in.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环收集每个周期的结果，并将它们返回给`main`函数，该函数本身对我们感兴趣的*α*值进行循环。
- en: Let’s run this code and plot `val_err`, the test error, as a function of the
    number of epochs for each `alpha`. [Figure 10-6](ch10.xhtml#ch10fig6) is the result.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这段代码，并绘制`val_err`，即测试误差，作为每个`alpha`的训练周期数的函数。[图 10-6](ch10.xhtml#ch10fig6)是结果。
- en: '![image](Images/10fig06.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig06.jpg)'
- en: '*Figure 10-6: MNIST test error as a function of training epoch for different
    values of α*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-6：不同α值下训练周期的MNIST测试误差*'
- en: The first thing we notice in [Figure 10-6](ch10.xhtml#ch10fig6) is that any
    nonzero value for *α* produces a lower test error compared to not using L2 regularization
    at all. We can conclude that L2 regularization is helpful. The different *α* values
    all result in approximately the same test error, but larger values are slightly
    more effective and reach a lower test error sooner. Compare *α* = 0.1 to *α* =
    0.4, for example.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-6](ch10.xhtml#ch10fig6)中，我们首先注意到任何非零的*α*值都能比不使用L2正则化时产生更低的测试误差。我们可以得出结论，L2正则化是有帮助的。不同的*α*值结果的测试误差大致相同，但较大的值略微更有效，并且能更快地达到更低的测试误差。例如，可以比较*α*
    = 0.1与*α* = 0.4。
- en: 'Notice that larger *α* values seem noisier: the plot is thicker as the error
    jumps around more, relative to the smaller *α* values. To understand this, think
    about the total loss minimized during training. When *α* is large, we’re placing
    more importance on the L2 term relative to the network’s error over the minibatch.
    This means that when we ask the network to adjust the weights and biases during
    backprop, it’ll be more strongly affected by the magnitude of the parameters of
    the network than the training data itself. Because the network is focusing less
    on reducing the loss due to the training data, we might expect the per epoch test
    error to vary more.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，较大的*α*值似乎更嘈杂：相较于较小的*α*值，图表更粗，因为误差的波动更大。要理解这一点，可以思考在训练过程中最小化的总损失。当*α*较大时，我们在L2项相对于网络在小批次上的误差上赋予更多的权重。这意味着，当我们要求网络在反向传播过程中调整权重和偏置时，它将更多地受到网络参数大小的影响，而不是训练数据本身的影响。由于网络对减少训练数据导致的损失关注较少，我们可能会预期每个周期的测试误差会有更大的波动。
- en: Momentum
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动量
- en: Momentum alters the weight update during training by adding in a fraction of
    the gradient value used to update the weight in the previous minibatch. The fraction
    is specified as a multiplier on the previous gradient value, [0,1]. We covered
    momentum in [Chapter 9](ch09.xhtml#ch09).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 动量通过将上一个小批次中用于更新权重的梯度值的一部分加到当前训练中的权重更新中，从而改变训练过程中的权重更新。这个部分通过对上一个梯度值的乘数来指定，范围为[0,1]。我们在[第9章](ch09.xhtml#ch09)中讨论了动量。
- en: 'Let’s see how changing this parameter affects training. In this case, the setup
    for the experiment is simple. It’s identical to the approach used previously for
    L2 regularization, but instead of fixing the momentum parameter (*μ*) and varying
    the L2 weight (*α*), we’ll fix *α* = 0.0001 and vary *μ*. All the other parts
    remain the same: training by single epochs, the network configuration, and so
    forth. See the file *mnist_nn_experiments_momentum.py*.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看改变这个参数如何影响训练。在这种情况下，实验的设置很简单。它与之前用于L2正则化的设置相同，但不同之处在于，我们固定动量参数（*μ*），而变化L2权重（*α*）。我们将*α*
    = 0.0001，并变化*μ*。其他部分保持不变：按单个周期进行训练，网络配置等。请参见文件*mnist_nn_experiments_momentum.py*。
- en: 'We’ll explore these momentum values:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索这些动量值：
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: They range from no momentum term (*μ* = 0) to a large momentum term (*μ* = 0.99).
    Running the experiment produces [Figure 10-7](ch10.xhtml#ch10fig7).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的范围从没有动量项（*μ* = 0）到一个较大的动量项（*μ* = 0.99）。运行实验生成了[图 10-7](ch10.xhtml#ch10fig7)。
- en: In [Figure 10-7](ch10.xhtml#ch10fig7), we see three distinct regions. The first,
    represented by no momentum or relatively small momentum values (*μ* = 0.3, *μ*
    = 0.5), shows the highest test set error. The second shows improvement with moderate
    momentum values (*μ* = 0.7, *μ* = 0.9), including the “standard” (sklearn default)
    value of 0.9\. In this case, however, a large momentum of 0.99 lowers the test
    set error from about 7.5 percent to about 6 percent. Momentum helps and should
    be used, especially with values near the standard of 0.9\. In practice, people
    seldom seem to alter the momentum much, but as this example shows, sometimes it
    makes a big difference to the results.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-7](ch10.xhtml#ch10fig7)中，我们看到三个不同的区域。第一个区域表示没有动量或相对较小的动量值（*μ* = 0.3,
    *μ* = 0.5），此时测试集误差最大。第二个区域则显示了中等动量值（*μ* = 0.7, *μ* = 0.9）的改进，包括“标准”（sklearn 默认）值
    0.9。然而，在这种情况下，较大的动量值 0.99 使测试集误差从大约 7.5% 降低到了大约 6%。动量的确有效，尤其是在接近标准值 0.9 时应该使用。实际上，人们很少改变动量的值，但正如这个例子所示，有时它对结果的影响非常大。
- en: '![image](Images/10fig07.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig07.jpg)'
- en: '*Figure 10-7: MNIST test error as a function of training epoch for different
    values of μ*'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10-7：不同 μ 值下，MNIST 测试误差随训练轮次的变化*'
- en: Note that we severely limited the training set to a mere 3,000 samples, about
    300 per digit, which likely made momentum matter more because the training set
    was a small and less complete of a sample of the parent distribution we want the
    model to learn. Increasing the training set size to 30,000 results in a different,
    and more typical, ordering of the plot, where a momentum of 0.9 is the best option.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将训练集限制在仅 3,000 个样本左右，每个数字约 300 个样本，这可能使得动量的影响更为显著，因为训练集较小，且未能充分代表我们希望模型学习的母体分布。将训练集大小增加到
    30,000 后，图表的顺序发生了变化，呈现出更典型的趋势，此时 0.9 的动量值是最优的选择。
- en: Weight Initialization
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Once treated rather cavalierly, the initial set of values used for the weights
    and biases of a network is now known to be extremely important. The simple experiment
    of this section shows this plainly.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经被视为不重要的网络权重和偏置的初始值，现在被认为是极其重要的。本节中的简单实验清楚地展示了这一点。
- en: The sklearn toolkit initializes the weights and biases of a neural network by
    calling the `_init_coef` method of the `MLPClassifier` class. This method selects
    weights and biases randomly according to the Glorot algorithm we discussed in
    [Chapter 9](ch09.xhtml#ch09). This algorithm sets the weights and biases to values
    sampled uniformly from the range
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn 工具包通过调用 `MLPClassifier` 类的 `_init_coef` 方法来初始化神经网络的权重和偏置。该方法根据我们在[第
    9 章](ch09.xhtml#ch09)中讨论的 Glorot 算法随机选择权重和偏置。该算法将权重和偏置设置为从范围内均匀采样的值。
- en: '![image](Images/243equ01.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/243equ01.jpg)'
- en: where *f*[*in*] is the number of inputs and *f*[*out*] is the number of outputs
    for the current layer being initialized. If the activation function is a sigmoid,
    *A* = 2; otherwise, *A* = 6.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f*[*in*] 是当前层输入的数量，*f*[*out*] 是当前层输出的数量。如果激活函数是 Sigmoid，*A* = 2；否则，*A* =
    6。
- en: If we play a little trick, we can change the way that sklearn initializes the
    network and thereby experiment with alternative initialization schemes. The trick
    uses Python’s object-oriented programming abilities. If we make a subclass of
    `MLPClassifier`, let’s call it simply `Classifier`, we can override the `_init_coef`
    method with our own. Python also allows us to add new member variables to a class
    instance arbitrarily, which gives us all we need.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们玩个小把戏，我们可以改变 sklearn 初始化网络的方式，从而尝试不同的初始化方案。这个小把戏利用了 Python 的面向对象编程特性。如果我们创建一个
    `MLPClassifier` 的子类，称之为 `Classifier`，我们可以用自己的方式覆盖 `_init_coef` 方法。Python 还允许我们随意地向类实例添加新的成员变量，这为我们提供了所需的一切。
- en: The remainder of the experiment follows the format of the previous sections.
    We’ll ultimately plot the test error by epoch of the MNIST digits trained on a
    subset of the full data for different initialization approaches. The model itself
    will use the first 6,000 training samples, a minibatch size of 64, a constant
    learning rate of 0.01, a momentum of 0.9, an L2 regularization parameter of 0.2,
    and an architecture with two hidden layers of 100 and 50 nodes each. See *mnist_nn_experiments_init.py*
    for this experiment’s code.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的其余部分遵循前面章节的格式。我们最终将绘制在一个子集数据上训练的 MNIST 数字测试误差随训练轮次的变化，使用不同的初始化方法。模型本身将使用前
    6,000 个训练样本，迷你批次大小为 64，恒定学习率为 0.01，动量为 0.9，L2 正则化参数为 0.2，架构包括两层隐藏层，分别为 100 和 50
    个节点。请参阅 *mnist_nn_experiments_init.py* 了解此实验的代码。
- en: We’ll test four new weight initialization schemes along with the standard Glorot
    approach of sklearn. The schemes are shown in [Table 10-5](ch10.xhtml#ch10tab5).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试四种新的权重初始化方案，并与 sklearn 的标准 Glorot 方法进行比较。这些方案显示在 [表 10-5](ch10.xhtml#ch10tab5)
    中。
- en: '**Table 10-5:** Weight Initialization Schemes'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-5：** 权重初始化方案'
- en: '| Name | Equation | Description |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 方程式 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Glorot | ![Image](Images/244equ01.jpg) | sklearn default |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Glorot | ![Image](Images/244equ01.jpg) | sklearn 默认 |'
- en: '| He | ![Image](Images/244equ02.jpg) | He initialization for ReLU |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| He | ![Image](Images/244equ02.jpg) | ReLU 的 He 初始化 |'
- en: '| Xavier | ![Image](Images/244equ03.jpg) | Alternate Xavier |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Xavier | ![Image](Images/244equ03.jpg) | 交替 Xavier |'
- en: '| Uniform | 0.01(*U*(0,1)-0.5) | Classic small uniform |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 均匀 | 0.01(*U*(0,1)-0.5) | 经典的小均匀 |'
- en: '| Gaussian | 0.005*N*(0,1) | Classic small Gaussian |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 高斯 | 0.005*N*(0,1) | 经典的小高斯 |'
- en: Recall that *N*(0,1) refers to a sample from a bell curve with a mean of 0 and
    a standard deviation of 1 while *U*(0,1) refers to a sample drawn uniformly from
    [0,1), meaning all values in that range are equally likely except 1.0\. Each of
    the new initialization methods sets the bias values to 0, always. However, sklearn’s
    Glorot implementation sets the bias values in the same way it sets the weights.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，*N*(0,1) 表示从均值为 0、标准差为 1 的钟形曲线中抽取的样本，而 *U*(0,1) 表示从 [0,1) 区间中均匀抽取的样本，意味着该范围内的所有值都有相同的概率，除了
    1.0。每种新的初始化方法都会将偏置值始终设置为 0。然而，sklearn 的 Glorot 实现会像设置权重一样设置偏置值。
- en: '**Note** *As mentioned in [Chapter 9](ch09.xhtml#ch09) , both* Xavier *and*
    Glorot *refer to the same person, Xavier Glorot. We’re differentiating here because
    the form we’re calling* Xavier *is referred to as such in other machine learning
    toolkits like Caffe, and the equation used is different from the equation used
    in the original paper.*'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** *如 [第 9 章](ch09.xhtml#ch09) 所述，* Xavier *和* Glorot *指的是同一个人，Xavier Glorot。我们在这里区分是因为我们调用的*
    Xavier *在其他机器学习工具包中（如 Caffe）被称为 Xavier，并且使用的方程与原论文中使用的方程不同。*'
- en: 'This all sounds nice and neat, but how to implement it in code? First, we define
    a new Python class, `Classifier`, which is a subclass of `MLPClassifier`. As a
    subclass, the new class immediately inherits all the functionality of the superclass
    (`MLPClassifier`) while allowing us the freedom to override any of the superclass
    methods with our own implementation. We simply need to define our own version
    of `_init_coef` with the same arguments and return values. In code, it looks like
    this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切听起来不错且整洁，但如何在代码中实现呢？首先，我们定义一个新的 Python 类，`Classifier`，它是 `MLPClassifier`
    的子类。作为子类，新的类立即继承了超类（`MLPClassifier`）的所有功能，同时允许我们自由地使用我们自己的实现来覆盖任何超类的方法。我们只需要定义我们自己版本的
    `_init_coef`，并确保它具有相同的参数和返回值。代码如下所示：
- en: '[PRE12]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The initialization we perform depends on the value of `init_scheme`. This is
    a new member variable that we use to select the initialization method (see [Table
    10-6](ch10.xhtml#ch10tab6)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的初始化依赖于 `init_scheme` 的值。这是一个新的成员变量，用于选择初始化方法（请参见 [表 10-6](ch10.xhtml#ch10tab6)）。
- en: '**Table 10-6:** Initialization Scheme and `init_scheme` Value'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10-6：** 初始化方案和 `init_scheme` 值'
- en: '| Value | Initialization method |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 值 | 初始化方法 |'
- en: '| --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | sklearn default |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 0 | sklearn 默认 |'
- en: '| 1 | Classic small uniform |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 经典的小均匀 |'
- en: '| 2 | Classic small Gaussian |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 经典的小高斯 |'
- en: '| 3 | He initialization |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 3 | He 初始化 |'
- en: '| 4 | Alternate Xavier |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 交替 Xavier |'
- en: We set the variable immediately after creating the `Classifier` object.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在创建 `Classifier` 对象后立即设置该变量。
- en: 'We know that training a network more than once results in slightly different
    performance because of the way the network is initialized. Therefore, training
    a single network for each initialization type will likely lead to a wrong view
    of how well the initialization performs because we might hit a bad set of initial
    weights and biases. To mitigate this, we need to train multiple versions of the
    network and report the average performance. Since we want to plot the test error
    as a function of the training epoch, we need to track the test error at each epoch
    for each training of each initialization scheme. This suggests a three-dimensional
    array:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，训练一个网络多次会导致略有不同的性能，因为网络初始化的方式不同。因此，每种初始化类型训练一个网络可能会导致对初始化性能的错误看法，因为我们可能会遇到一组不理想的初始权重和偏置。为了解决这个问题，我们需要训练多个版本的网络，并报告平均性能。由于我们想要绘制训练轮次与测试误差的关系图，因此我们需要跟踪每个初始化方案的每次训练在每一轮的测试误差。这就需要一个三维数组：
- en: '[PRE13]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have `trainings` trainings of each initialization type (`init_types`) for
    a maximum of `epochs` epochs.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每种初始化类型（`init_types`）进行`trainings`次训练，最多训练`epochs`个周期。
- en: 'With all of this in place, the generation and storage of the actual experiment
    output is straightforward, if rather slow, taking the better part of a day to
    run:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一切准备好之后，实际实验结果的生成和存储变得相对直接，虽然速度较慢，运行起来大约需要一天的时间：
- en: '[PRE14]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here `nn` is the classifier instance to train, `init_scheme` sets the initialization
    scheme to use, and `run` is the function we defined earlier to train and test
    the network incrementally.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`nn`是要训练的分类器实例，`init_scheme`设置要使用的初始化方案，而`run`是我们之前定义的用于增量训练和测试网络的函数。
- en: If we set the number of training sessions to 10, the number of epochs to 4,000,
    and plot the mean test error per epoch, we get [Figure 10-8](ch10.xhtml#ch10fig8).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将训练次数设为10，训练周期设为4,000，并绘制每个周期的平均测试误差，我们得到[图10-8](ch10.xhtml#ch10fig8)。
- en: '![image](Images/10fig08.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig08.jpg)'
- en: '*Figure 10-8: MNIST test error as a function of training epoch for different
    weight initialization methods (mean over 10 training runs)*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-8：不同权重初始化方法下的MNIST测试误差与训练周期的关系（10次训练运行的平均值）*'
- en: Let’s understand what the figure is showing us. The five initialization approaches
    are marked, each pointing to one of the five curves in the figure. The curves
    themselves are familiar to us by now; they show the test set error as a function
    of the training epoch. In this case, the value plotted for each curve is the average
    over 10 training runs of the same network architecture initialized with the same
    approach but different random values.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来理解这个图表所展示的内容。五种初始化方法被标注出来，每个标记指向图中的一条曲线。我们现在已经很熟悉这些曲线，它们展示了训练周期的函数下，测试集的误差。在这种情况下，每条曲线的值是10次训练运行的平均值，训练网络架构使用相同的初始化方法，但随机值不同。
- en: We immediately see two distinct groups of results. On the top, we have the test
    error for the classic initialization approaches using small uniform or normally
    distributed values (Gaussian). On the bottom, we have the results for the more
    principled initialization in current use. Even this basic experiment shows the
    effectiveness of modern initialization approaches quite clearly. Recall, the classic
    approaches were part of the reason neural networks had a bad name a few decades
    ago. Networks were finicky and difficult to train in large part because of improper
    initialization.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立刻可以看到两组不同的结果。顶部是使用小的均匀分布或正态分布（高斯）的经典初始化方法的测试误差。底部是当前使用的更为原理化的初始化方法的结果。即便是这个基础实验，也清楚地显示出现代初始化方法的有效性。回想一下，经典的初始化方法曾是神经网络几十年前口碑差的部分原因。网络不稳定且难以训练，很大一部分原因是由于初始化不当。
- en: Looking at the bottom set of results, we see that for this experiment, there
    is little difference between the sklearn default initialization, which we are
    calling *Glorot*, and the initialization approach of He. The two plots are virtually
    identical. The plot labeled *Xavier* is slightly worse at first , but toward the
    end of our training runs matches the other two. Sklearn is using a good initialization
    strategy.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 看底部的结果，我们可以看到在这个实验中，sklearn默认的初始化方法（我们称之为*Glorot*）与He初始化方法几乎没有区别。这两张图几乎完全相同。标记为*Xavier*的曲线起初稍差一些，但在训练的后期与另外两条曲线相匹配。Sklearn使用了一种不错的初始化策略。
- en: 'The plot also shows us something else. For the classic initialization approaches,
    we see the test set error level off and remain more or less constant. For the
    modern initialization approaches, we observe the test error increase slightly
    with the training epoch. This is particularly true for the Glorot and He methods.
    This increase is a telltale sign of overfitting: as we keep training, the model
    stops learning general features of the parent distribution and starts to focus
    on specific features of the training set. We didn’t plot the training set error,
    but it would be going down even as the test set error starts to rise. The lowest
    test set error is at about 1,200 epochs. Ideally, this would be where we stop
    training because we have the most reliable evidence that the model is in a good
    place to correctly predict new, unseen inputs. Further training tends to degrade
    model generalization.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 该图还向我们展示了其他一些信息。对于经典初始化方法，我们看到测试集误差趋于平稳并保持相对恒定。而对于现代初始化方法，我们观察到随着训练轮次的增加，测试误差略有上升。这对于
    Glorot 和 He 方法尤为明显。这种增加是过拟合的明显信号：随着训练的进行，模型停止学习父分布的普遍特征，而开始专注于训练集的特定特征。我们没有绘制训练集误差，但即使测试集误差开始上升，训练集误差仍会下降。最低的测试集误差出现在大约
    1,200 次训练周期时。理想情况下，我们应该在此时停止训练，因为这时模型最有可能正确地预测新的、未见过的输入。进一步训练往往会降低模型的泛化能力。
- en: Why did the increase in the test error happen? The likely cause of this effect
    is too small of a training set, only 6,000 samples. Also, the model architecture
    is not very large, with only 100 and 50 nodes in the hidden layers.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么测试误差会增加？这种现象可能的原因是训练集太小，仅有 6,000 个样本。此外，模型架构也不大，隐藏层只有 100 和 50 个节点。
- en: This section dramatically demonstrates the benefit of using current, state-of-the-art
    network initialization. When we explore convolutional neural networks in [Chapter
    12](ch12.xhtml#ch12), we’ll use these approaches exclusively.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 本节生动地展示了使用当前最先进的网络初始化方法的好处。当我们在[第 12 章](ch12.xhtml#ch12)中探讨卷积神经网络时，我们将专门使用这些方法。
- en: Feature Ordering
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征排序
- en: We’ll end our MNIST experiments with a bit of fun that we’ll return to again
    when we’re exploring convolutional neural networks. All of the experiments so
    far use the MNIST digits as a vector made by laying the rows of the digit images
    end to end. When we do this, we know that the elements of the vector are related
    to each other in a way that will reconstruct the digit should we take the vector
    and reshape it into a 28 × 28 element array. This means, except for the end of
    one row and the beginning of the next, that the pixels in the row are still part
    of the digit—the spatial relationship of the components of the image is preserved.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一点有趣的实验结束我们的 MNIST 实验，之后我们在探讨卷积神经网络时会再次提到这一点。到目前为止，所有的实验都使用将数字图像的每一行拼接在一起形成的向量。当我们这样做时，我们知道该向量的元素是相互关联的，这样一来，如果我们将向量重新塑形为一个
    28 × 28 的数组，就能重构出数字。这意味着，除了每一行的结尾和下一行的开头，行中的像素仍然是数字的一部分——图像组件的空间关系得到了保留。
- en: However, if we scramble the pixels of the image, but always scramble the pixels
    in the same way, we’ll destroy the local spatial relationship between the pixels.
    This local relationship is what we use when we look at the image to decide what
    digit it represents. We look for the top part of a 5 to be a straight line segment
    and the bottom portion to curve on the right side, and so forth.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们打乱图像的像素，但始终以相同的方式打乱像素，我们将破坏像素之间的局部空间关系。我们在查看图像时会用到这种局部关系来判断它代表的是哪个数字。例如，我们会观察到数字
    5 的上部分是直线段，而下部分则在右侧有一个弯曲，等等。
- en: Look at [Figure 7-3](ch07.xhtml#ch7fig3). The figure shows MNIST digit images
    on the top row and what the same digit images look like after scrambling (bottom).
    In [Chapter 7](ch07.xhtml#ch07), we showed that this scrambling does not affect
    the accuracy of classic machine learning models; the models consider the inputs
    holistically, not by local spatial relationships as we do. Is this true for neural
    networks as well? Also, if true, will the network learn as quickly with the scrambled
    inputs as it does with the original images? Let’s find out.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请看一下[图 7-3](ch07.xhtml#ch7fig3)。该图展示了 MNIST 数字图像在上行，以及这些数字图像经过打乱后的样子（下行）。在[第
    7 章](ch07.xhtml#ch07)中，我们展示了这种打乱并不会影响经典机器学习模型的准确性；这些模型是整体地考虑输入，而不是像我们一样通过局部空间关系来分析。那么，这对于神经网络也是如此吗？此外，如果是这样，网络在使用打乱后的输入时，学习的速度会像使用原始图像时一样快吗？让我们来看看。
- en: The code for this experiment is found in *mnist_nn_experiments_scrambled .py*,
    where we simply define our now expected neural network model
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 该实验的代码可以在*mnist_nn_experiments_scrambled.py*中找到，在该代码中，我们简单地定义了我们现在预期的神经网络模型
- en: '[PRE15]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: and train it on the first 6,000 MNIST digit samples—first as usual, and then
    using the scrambled versions. We compute the test set error as a function of the
    epoch and average the results over 10 runs before plotting. The result is [Figure
    10-9](ch10.xhtml#ch10fig9).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 并在前6000个MNIST数字样本上进行训练——首先像往常一样，然后使用混乱版本。我们计算测试集误差与训练轮次的关系，并在10次运行中求平均，最后绘制结果。这就是[图10-9](ch10.xhtml#ch10fig9)所示的结果。
- en: '![image](Images/10fig09.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![image](Images/10fig09.jpg)'
- en: '*Figure 10-9: MNIST test error as a function of training epoch for scrambled
    and unscrambled digits*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10-9：混乱和未混乱数字的训练轮次与MNIST测试错误之间的关系*'
- en: In the figure, we see the answer to our earlier questions. First, yes, traditional
    neural networks do interpret their input vectors holistically, like the classic
    models. Second, yes, the network learns just as rapidly with the scrambled digits
    as it does with the unscrambled ones. The difference between the scrambled and
    unscrambled curves in [Figure 10-9](ch10.xhtml#ch10fig9) is not statistically
    significant.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们看到了之前问题的答案。首先，的确，传统神经网络会整体性地解读它们的输入向量，就像经典模型一样。其次，网络在处理混乱数字时的学习速度与处理未混乱数字时相同。在[图10-9](ch10.xhtml#ch10fig9)中，混乱和未混乱曲线的差异在统计上不显著。
- en: These results indicate that (traditional) neural networks “understand” their
    inputs in their entirety and do not look for local spatial relationships. We’ll
    see a different outcome to this experiment when we work with convolutional neural
    networks ([Chapter 12](ch12.xhtml#ch12)). It’s precisely this lack of spatial
    awareness (assuming images as inputs) that limited neural networks for so long
    and led to the development of convolutional neural networks, which are spatially
    aware.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，（传统的）神经网络“全面理解”它们的输入，而不会寻找局部空间关系。当我们使用卷积神经网络时（见[第12章](ch12.xhtml#ch12)），我们将看到不同的实验结果。正是这种缺乏空间意识（假设输入为图像）长期限制了神经网络的发展，并促成了卷积神经网络的出现，后者具有空间感知能力。
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the concepts developed in [Chapters 8](ch08.xhtml#ch08)
    and [9](ch09.xhtml#ch09) via experiments with the MNIST dataset. By varying key
    parameters associated with the network architecture and gradient descent learning
    process, we increased our intuition as to how the parameters influence the overall
    performance of the network. Space considerations prevented us from thoroughly
    exploring all the `MLPClassifier` options, so I encourage you to experiment more
    on your own. In particular, experiment with using the different solvers, Nesterov
    momentum, early stopping, and, particularly crucial for training convolutional
    neural networks, nonconstant learning rates.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过对MNIST数据集的实验，探索了在[第8章](ch08.xhtml#ch08)和[第9章](ch09.xhtml#ch09)中开发的概念。通过调整与网络架构和梯度下降学习过程相关的关键参数，我们加深了对这些参数如何影响网络整体性能的直觉。由于篇幅限制，我们未能全面探讨所有`MLPClassifier`选项，因此鼓励你自己做更多实验。特别是，尝试使用不同的求解器、Nesterov动量、早期停止，以及对训练卷积神经网络至关重要的非常规学习率。
- en: The next chapter explores techniques and metrics for evaluating the performance
    of machine learning models. This interlude before we jump to convolutional neural
    networks will supply us with tools we can use to help understand the performance
    of more advanced model types.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将探讨评估机器学习模型性能的技术和指标。在我们进入卷积神经网络之前的这一插曲将为我们提供一些工具，帮助我们理解更先进模型类型的性能。
